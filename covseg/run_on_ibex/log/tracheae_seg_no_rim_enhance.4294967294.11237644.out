Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: True test_id 0
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9412 # image files with weight 9372
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2471 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9372
Using 4 GPUs
best_f1 is: 0.5925981802070853
Going to train epochs [13-42]
Training epoch 13
	step [1/196], loss=84.4124
	step [2/196], loss=96.1513
	step [3/196], loss=78.1559
	step [4/196], loss=84.7339
	step [5/196], loss=90.7423
	step [6/196], loss=83.2799
	step [7/196], loss=64.5408
	step [8/196], loss=92.9880
	step [9/196], loss=89.7066
	step [10/196], loss=76.7637
	step [11/196], loss=80.8138
	step [12/196], loss=88.5014
	step [13/196], loss=70.1487
	step [14/196], loss=99.9323
	step [15/196], loss=111.0963
	step [16/196], loss=77.2392
	step [17/196], loss=80.1434
	step [18/196], loss=76.4781
	step [19/196], loss=63.1906
	step [20/196], loss=87.0379
	step [21/196], loss=92.6226
	step [22/196], loss=82.3014
	step [23/196], loss=90.5003
	step [24/196], loss=97.5175
	step [25/196], loss=88.9442
	step [26/196], loss=91.6800
	step [27/196], loss=90.3576
	step [28/196], loss=94.3125
	step [29/196], loss=99.6976
	step [30/196], loss=73.3766
	step [31/196], loss=102.7550
	step [32/196], loss=82.2889
	step [33/196], loss=86.0717
	step [34/196], loss=101.0095
	step [35/196], loss=90.1429
	step [36/196], loss=84.5461
	step [37/196], loss=68.8494
	step [38/196], loss=93.8596
	step [39/196], loss=78.9115
	step [40/196], loss=87.3267
	step [41/196], loss=76.6173
	step [42/196], loss=73.9836
	step [43/196], loss=80.0202
	step [44/196], loss=84.0521
	step [45/196], loss=77.7755
	step [46/196], loss=89.0742
	step [47/196], loss=65.6736
	step [48/196], loss=91.7026
	step [49/196], loss=78.1523
	step [50/196], loss=88.2363
	step [51/196], loss=82.2373
	step [52/196], loss=84.1766
	step [53/196], loss=74.2675
	step [54/196], loss=66.9006
	step [55/196], loss=106.4589
	step [56/196], loss=76.0439
	step [57/196], loss=105.6336
	step [58/196], loss=101.8576
	step [59/196], loss=80.6554
	step [60/196], loss=88.5443
	step [61/196], loss=80.4310
	step [62/196], loss=68.0192
	step [63/196], loss=90.8225
	step [64/196], loss=82.7204
	step [65/196], loss=81.0185
	step [66/196], loss=83.7438
	step [67/196], loss=101.3816
	step [68/196], loss=75.2022
	step [69/196], loss=87.1479
	step [70/196], loss=81.5767
	step [71/196], loss=87.1372
	step [72/196], loss=100.0105
	step [73/196], loss=80.5912
	step [74/196], loss=73.0346
	step [75/196], loss=81.8964
	step [76/196], loss=81.9356
	step [77/196], loss=92.1984
	step [78/196], loss=89.9744
	step [79/196], loss=103.0884
	step [80/196], loss=74.6640
	step [81/196], loss=90.9614
	step [82/196], loss=87.3500
	step [83/196], loss=73.5741
	step [84/196], loss=100.8571
	step [85/196], loss=69.3986
	step [86/196], loss=98.3030
	step [87/196], loss=82.3997
	step [88/196], loss=82.7200
	step [89/196], loss=97.7611
	step [90/196], loss=92.7327
	step [91/196], loss=74.6723
	step [92/196], loss=94.5724
	step [93/196], loss=92.3321
	step [94/196], loss=91.4366
	step [95/196], loss=78.5084
	step [96/196], loss=107.8751
	step [97/196], loss=78.0146
	step [98/196], loss=99.1612
	step [99/196], loss=84.7163
	step [100/196], loss=83.0767
	step [101/196], loss=71.5296
	step [102/196], loss=86.2275
	step [103/196], loss=88.9935
	step [104/196], loss=72.7014
	step [105/196], loss=75.5348
	step [106/196], loss=94.8014
	step [107/196], loss=90.9628
	step [108/196], loss=90.8076
	step [109/196], loss=75.9888
	step [110/196], loss=93.6665
	step [111/196], loss=58.2420
	step [112/196], loss=86.4320
	step [113/196], loss=92.5141
	step [114/196], loss=91.2050
	step [115/196], loss=72.4458
	step [116/196], loss=86.4243
	step [117/196], loss=75.6163
	step [118/196], loss=78.5148
	step [119/196], loss=86.3017
	step [120/196], loss=86.5836
	step [121/196], loss=78.7355
	step [122/196], loss=97.5208
	step [123/196], loss=87.9291
	step [124/196], loss=83.4933
	step [125/196], loss=93.0851
	step [126/196], loss=82.2819
	step [127/196], loss=72.3882
	step [128/196], loss=83.1909
	step [129/196], loss=82.2784
	step [130/196], loss=85.9655
	step [131/196], loss=103.6306
	step [132/196], loss=87.3636
	step [133/196], loss=85.2354
	step [134/196], loss=79.2220
	step [135/196], loss=73.6371
	step [136/196], loss=79.3230
	step [137/196], loss=99.3852
	step [138/196], loss=78.8219
	step [139/196], loss=77.2517
	step [140/196], loss=69.9362
	step [141/196], loss=66.2426
	step [142/196], loss=98.2301
	step [143/196], loss=80.8122
	step [144/196], loss=93.0851
	step [145/196], loss=93.7952
	step [146/196], loss=92.3531
	step [147/196], loss=71.9028
	step [148/196], loss=86.8909
	step [149/196], loss=71.4211
	step [150/196], loss=85.3867
	step [151/196], loss=86.3315
	step [152/196], loss=84.2084
	step [153/196], loss=98.5371
	step [154/196], loss=80.0595
	step [155/196], loss=77.0101
	step [156/196], loss=62.6736
	step [157/196], loss=73.7442
	step [158/196], loss=78.3126
	step [159/196], loss=73.9835
	step [160/196], loss=78.0957
	step [161/196], loss=80.5289
	step [162/196], loss=88.2233
	step [163/196], loss=77.9616
	step [164/196], loss=85.2996
	step [165/196], loss=80.9890
	step [166/196], loss=78.1694
	step [167/196], loss=76.0738
	step [168/196], loss=76.2541
	step [169/196], loss=93.6582
	step [170/196], loss=85.0334
	step [171/196], loss=84.1026
	step [172/196], loss=75.9176
	step [173/196], loss=72.0308
	step [174/196], loss=69.7153
	step [175/196], loss=73.0162
	step [176/196], loss=93.1092
	step [177/196], loss=78.3384
	step [178/196], loss=87.4493
	step [179/196], loss=88.7887
	step [180/196], loss=78.5421
	step [181/196], loss=82.7820
	step [182/196], loss=74.7734
	step [183/196], loss=80.4128
	step [184/196], loss=83.7039
	step [185/196], loss=86.7723
	step [186/196], loss=89.6891
	step [187/196], loss=99.3248
	step [188/196], loss=67.0910
	step [189/196], loss=83.0935
	step [190/196], loss=92.4238
	step [191/196], loss=73.4593
	step [192/196], loss=77.4470
	step [193/196], loss=59.2897
	step [194/196], loss=80.4158
	step [195/196], loss=82.1519
	step [196/196], loss=9.5362
	Evaluating
	loss=0.0262, precision=0.4112, recall=0.9141, f1=0.5673
Training epoch 14
	step [1/196], loss=79.1743
	step [2/196], loss=97.5418
	step [3/196], loss=77.1808
	step [4/196], loss=68.5272
	step [5/196], loss=82.3078
	step [6/196], loss=80.0037
	step [7/196], loss=89.5845
	step [8/196], loss=79.4915
	step [9/196], loss=90.3215
	step [10/196], loss=82.2247
	step [11/196], loss=90.3627
	step [12/196], loss=84.3399
	step [13/196], loss=89.1059
	step [14/196], loss=81.2031
	step [15/196], loss=82.6662
	step [16/196], loss=83.1398
	step [17/196], loss=95.2629
	step [18/196], loss=77.0417
	step [19/196], loss=92.8629
	step [20/196], loss=92.4729
	step [21/196], loss=92.3542
	step [22/196], loss=76.7001
	step [23/196], loss=82.8513
	step [24/196], loss=67.1987
	step [25/196], loss=68.4976
	step [26/196], loss=66.7018
	step [27/196], loss=106.0027
	step [28/196], loss=75.3689
	step [29/196], loss=70.3205
	step [30/196], loss=69.7797
	step [31/196], loss=82.2419
	step [32/196], loss=69.3160
	step [33/196], loss=87.2257
	step [34/196], loss=84.6565
	step [35/196], loss=73.3664
	step [36/196], loss=87.5753
	step [37/196], loss=84.1571
	step [38/196], loss=82.7200
	step [39/196], loss=80.4429
	step [40/196], loss=105.0461
	step [41/196], loss=82.5307
	step [42/196], loss=80.3325
	step [43/196], loss=81.5512
	step [44/196], loss=76.4940
	step [45/196], loss=76.7895
	step [46/196], loss=78.4165
	step [47/196], loss=78.9478
	step [48/196], loss=95.2985
	step [49/196], loss=106.4031
	step [50/196], loss=84.5104
	step [51/196], loss=75.0120
	step [52/196], loss=91.9237
	step [53/196], loss=81.4703
	step [54/196], loss=89.4917
	step [55/196], loss=66.8939
	step [56/196], loss=92.3106
	step [57/196], loss=77.4948
	step [58/196], loss=87.9885
	step [59/196], loss=75.7163
	step [60/196], loss=85.3183
	step [61/196], loss=88.1484
	step [62/196], loss=67.5564
	step [63/196], loss=86.7611
	step [64/196], loss=81.5172
	step [65/196], loss=83.0004
	step [66/196], loss=83.6717
	step [67/196], loss=78.1693
	step [68/196], loss=80.1980
	step [69/196], loss=109.4109
	step [70/196], loss=71.5357
	step [71/196], loss=72.6500
	step [72/196], loss=78.8502
	step [73/196], loss=68.6589
	step [74/196], loss=83.6940
	step [75/196], loss=91.9032
	step [76/196], loss=103.1218
	step [77/196], loss=85.3867
	step [78/196], loss=84.0073
	step [79/196], loss=88.6834
	step [80/196], loss=92.5118
	step [81/196], loss=83.7066
	step [82/196], loss=84.5277
	step [83/196], loss=75.8644
	step [84/196], loss=69.5532
	step [85/196], loss=86.3081
	step [86/196], loss=75.4186
	step [87/196], loss=83.4615
	step [88/196], loss=92.2674
	step [89/196], loss=91.1803
	step [90/196], loss=105.3319
	step [91/196], loss=82.8166
	step [92/196], loss=77.1352
	step [93/196], loss=75.3723
	step [94/196], loss=86.3360
	step [95/196], loss=85.7573
	step [96/196], loss=80.9446
	step [97/196], loss=80.9011
	step [98/196], loss=85.5457
	step [99/196], loss=90.8812
	step [100/196], loss=94.3180
	step [101/196], loss=87.9446
	step [102/196], loss=86.1466
	step [103/196], loss=81.9508
	step [104/196], loss=70.0839
	step [105/196], loss=96.8602
	step [106/196], loss=84.5925
	step [107/196], loss=77.9132
	step [108/196], loss=89.6089
	step [109/196], loss=81.9791
	step [110/196], loss=86.7722
	step [111/196], loss=95.6467
	step [112/196], loss=79.6709
	step [113/196], loss=93.7235
	step [114/196], loss=76.2458
	step [115/196], loss=78.4349
	step [116/196], loss=84.3746
	step [117/196], loss=90.8578
	step [118/196], loss=79.2773
	step [119/196], loss=70.5072
	step [120/196], loss=87.9110
	step [121/196], loss=85.9657
	step [122/196], loss=76.4973
	step [123/196], loss=85.8758
	step [124/196], loss=96.7205
	step [125/196], loss=88.1900
	step [126/196], loss=77.9486
	step [127/196], loss=84.6258
	step [128/196], loss=88.9410
	step [129/196], loss=95.5016
	step [130/196], loss=97.0245
	step [131/196], loss=90.2809
	step [132/196], loss=89.6639
	step [133/196], loss=75.4184
	step [134/196], loss=66.9473
	step [135/196], loss=93.7733
	step [136/196], loss=87.2454
	step [137/196], loss=70.4828
	step [138/196], loss=82.0420
	step [139/196], loss=86.6093
	step [140/196], loss=69.1393
	step [141/196], loss=71.8371
	step [142/196], loss=73.2678
	step [143/196], loss=81.0342
	step [144/196], loss=87.6559
	step [145/196], loss=75.1346
	step [146/196], loss=84.9361
	step [147/196], loss=99.4006
	step [148/196], loss=74.6161
	step [149/196], loss=65.3821
	step [150/196], loss=73.3051
	step [151/196], loss=75.9826
	step [152/196], loss=80.0087
	step [153/196], loss=73.2669
	step [154/196], loss=77.9569
	step [155/196], loss=70.9662
	step [156/196], loss=76.3822
	step [157/196], loss=72.3959
	step [158/196], loss=88.7813
	step [159/196], loss=72.3055
	step [160/196], loss=84.0016
	step [161/196], loss=81.8715
	step [162/196], loss=90.8820
	step [163/196], loss=82.2170
	step [164/196], loss=89.7736
	step [165/196], loss=74.9644
	step [166/196], loss=64.0446
	step [167/196], loss=79.5315
	step [168/196], loss=82.0856
	step [169/196], loss=80.9233
	step [170/196], loss=69.4640
	step [171/196], loss=74.2390
	step [172/196], loss=75.2973
	step [173/196], loss=79.1633
	step [174/196], loss=89.4041
	step [175/196], loss=72.1283
	step [176/196], loss=82.5702
	step [177/196], loss=82.4260
	step [178/196], loss=76.8637
	step [179/196], loss=79.4024
	step [180/196], loss=83.4555
	step [181/196], loss=73.1305
	step [182/196], loss=90.3500
	step [183/196], loss=99.7374
	step [184/196], loss=72.3254
	step [185/196], loss=72.0471
	step [186/196], loss=102.6438
	step [187/196], loss=76.4174
	step [188/196], loss=104.4095
	step [189/196], loss=81.2168
	step [190/196], loss=72.2918
	step [191/196], loss=90.2153
	step [192/196], loss=78.6416
	step [193/196], loss=77.1384
	step [194/196], loss=94.5734
	step [195/196], loss=77.1517
	step [196/196], loss=18.5187
	Evaluating
	loss=0.0234, precision=0.3760, recall=0.9346, f1=0.5362
Training epoch 15
	step [1/196], loss=91.7718
	step [2/196], loss=84.1257
	step [3/196], loss=86.1090
	step [4/196], loss=72.5242
	step [5/196], loss=72.8730
	step [6/196], loss=67.4856
	step [7/196], loss=73.4162
	step [8/196], loss=74.9716
	step [9/196], loss=93.1923
	step [10/196], loss=68.0310
	step [11/196], loss=89.0999
	step [12/196], loss=80.1818
	step [13/196], loss=91.2141
	step [14/196], loss=71.7293
	step [15/196], loss=87.8346
	step [16/196], loss=68.6866
	step [17/196], loss=78.4151
	step [18/196], loss=54.7745
	step [19/196], loss=93.0817
	step [20/196], loss=83.0733
	step [21/196], loss=87.1409
	step [22/196], loss=74.3357
	step [23/196], loss=83.1366
	step [24/196], loss=91.3609
	step [25/196], loss=80.2253
	step [26/196], loss=69.2401
	step [27/196], loss=78.3333
	step [28/196], loss=93.1366
	step [29/196], loss=69.9946
	step [30/196], loss=90.5660
	step [31/196], loss=67.2060
	step [32/196], loss=79.0816
	step [33/196], loss=63.3754
	step [34/196], loss=82.5947
	step [35/196], loss=82.4279
	step [36/196], loss=93.6322
	step [37/196], loss=94.0027
	step [38/196], loss=79.9134
	step [39/196], loss=71.1502
	step [40/196], loss=69.1533
	step [41/196], loss=99.3630
	step [42/196], loss=82.7382
	step [43/196], loss=94.6842
	step [44/196], loss=95.4625
	step [45/196], loss=79.2049
	step [46/196], loss=88.5162
	step [47/196], loss=85.6808
	step [48/196], loss=82.4992
	step [49/196], loss=72.7188
	step [50/196], loss=87.8550
	step [51/196], loss=92.8498
	step [52/196], loss=94.3055
	step [53/196], loss=89.0093
	step [54/196], loss=99.1266
	step [55/196], loss=86.8420
	step [56/196], loss=75.0694
	step [57/196], loss=86.7230
	step [58/196], loss=93.9082
	step [59/196], loss=68.9227
	step [60/196], loss=86.0272
	step [61/196], loss=81.2087
	step [62/196], loss=78.1668
	step [63/196], loss=76.8875
	step [64/196], loss=71.6371
	step [65/196], loss=98.5038
	step [66/196], loss=66.7257
	step [67/196], loss=75.5196
	step [68/196], loss=100.1203
	step [69/196], loss=64.1743
	step [70/196], loss=69.8385
	step [71/196], loss=89.1664
	step [72/196], loss=76.4379
	step [73/196], loss=94.3069
	step [74/196], loss=76.0811
	step [75/196], loss=76.3408
	step [76/196], loss=88.3961
	step [77/196], loss=82.2618
	step [78/196], loss=79.1548
	step [79/196], loss=77.5595
	step [80/196], loss=79.7863
	step [81/196], loss=84.0349
	step [82/196], loss=61.6595
	step [83/196], loss=91.1149
	step [84/196], loss=81.6518
	step [85/196], loss=70.7993
	step [86/196], loss=94.9184
	step [87/196], loss=64.0828
	step [88/196], loss=77.3254
	step [89/196], loss=99.0694
	step [90/196], loss=90.0599
	step [91/196], loss=73.8845
	step [92/196], loss=76.2233
	step [93/196], loss=79.5352
	step [94/196], loss=82.4766
	step [95/196], loss=76.7973
	step [96/196], loss=84.2889
	step [97/196], loss=96.9154
	step [98/196], loss=82.4604
	step [99/196], loss=71.1665
	step [100/196], loss=90.2010
	step [101/196], loss=88.8763
	step [102/196], loss=87.5796
	step [103/196], loss=76.8170
	step [104/196], loss=90.0218
	step [105/196], loss=85.6862
	step [106/196], loss=77.5366
	step [107/196], loss=86.2523
	step [108/196], loss=76.1945
	step [109/196], loss=104.6060
	step [110/196], loss=72.2253
	step [111/196], loss=81.4870
	step [112/196], loss=67.1846
	step [113/196], loss=71.9988
	step [114/196], loss=76.6642
	step [115/196], loss=67.0740
	step [116/196], loss=95.3089
	step [117/196], loss=89.7715
	step [118/196], loss=70.7857
	step [119/196], loss=76.5648
	step [120/196], loss=93.9071
	step [121/196], loss=77.0649
	step [122/196], loss=76.1323
	step [123/196], loss=96.0250
	step [124/196], loss=77.1306
	step [125/196], loss=98.2597
	step [126/196], loss=101.9310
	step [127/196], loss=69.3704
	step [128/196], loss=69.9078
	step [129/196], loss=79.1277
	step [130/196], loss=95.9131
	step [131/196], loss=83.1839
	step [132/196], loss=100.0061
	step [133/196], loss=84.3827
	step [134/196], loss=86.4403
	step [135/196], loss=74.2132
	step [136/196], loss=76.5893
	step [137/196], loss=91.3621
	step [138/196], loss=74.1808
	step [139/196], loss=69.0428
	step [140/196], loss=76.5447
	step [141/196], loss=65.1435
	step [142/196], loss=68.2005
	step [143/196], loss=99.3277
	step [144/196], loss=76.8293
	step [145/196], loss=91.0098
	step [146/196], loss=85.1349
	step [147/196], loss=71.5625
	step [148/196], loss=76.7500
	step [149/196], loss=70.7982
	step [150/196], loss=99.6577
	step [151/196], loss=69.0345
	step [152/196], loss=79.3834
	step [153/196], loss=74.2966
	step [154/196], loss=79.5115
	step [155/196], loss=80.4417
	step [156/196], loss=81.2516
	step [157/196], loss=81.4254
	step [158/196], loss=92.4833
	step [159/196], loss=80.9816
	step [160/196], loss=94.3573
	step [161/196], loss=77.9580
	step [162/196], loss=75.9940
	step [163/196], loss=92.7619
	step [164/196], loss=95.7801
	step [165/196], loss=80.8847
	step [166/196], loss=83.3983
	step [167/196], loss=87.7280
	step [168/196], loss=89.0176
	step [169/196], loss=77.2385
	step [170/196], loss=88.3522
	step [171/196], loss=70.4901
	step [172/196], loss=87.3326
	step [173/196], loss=98.4142
	step [174/196], loss=92.1579
	step [175/196], loss=78.0602
	step [176/196], loss=87.7834
	step [177/196], loss=77.4524
	step [178/196], loss=80.8146
	step [179/196], loss=87.0364
	step [180/196], loss=82.5717
	step [181/196], loss=97.2897
	step [182/196], loss=71.6905
	step [183/196], loss=84.7927
	step [184/196], loss=84.7433
	step [185/196], loss=100.8508
	step [186/196], loss=91.1004
	step [187/196], loss=70.6520
	step [188/196], loss=79.1195
	step [189/196], loss=75.3074
	step [190/196], loss=82.1509
	step [191/196], loss=78.4655
	step [192/196], loss=83.5132
	step [193/196], loss=59.8809
	step [194/196], loss=73.9931
	step [195/196], loss=83.8245
	step [196/196], loss=17.7979
	Evaluating
	loss=0.0223, precision=0.4099, recall=0.9168, f1=0.5665
Training epoch 16
	step [1/196], loss=79.4419
	step [2/196], loss=88.6755
	step [3/196], loss=93.0267
	step [4/196], loss=75.7535
	step [5/196], loss=70.6871
	step [6/196], loss=95.8345
	step [7/196], loss=77.9413
	step [8/196], loss=79.0861
	step [9/196], loss=80.4042
	step [10/196], loss=75.5520
	step [11/196], loss=89.3748
	step [12/196], loss=80.5124
	step [13/196], loss=81.8793
	step [14/196], loss=57.6003
	step [15/196], loss=96.0638
	step [16/196], loss=86.3777
	step [17/196], loss=80.4646
	step [18/196], loss=84.2953
	step [19/196], loss=75.6933
	step [20/196], loss=79.0904
	step [21/196], loss=96.7855
	step [22/196], loss=85.3290
	step [23/196], loss=78.1948
	step [24/196], loss=88.9796
	step [25/196], loss=73.4151
	step [26/196], loss=85.2822
	step [27/196], loss=78.7546
	step [28/196], loss=66.5065
	step [29/196], loss=80.2579
	step [30/196], loss=87.7962
	step [31/196], loss=73.2374
	step [32/196], loss=90.8356
	step [33/196], loss=81.5077
	step [34/196], loss=71.2368
	step [35/196], loss=82.3502
	step [36/196], loss=85.9748
	step [37/196], loss=60.1363
	step [38/196], loss=70.2828
	step [39/196], loss=75.6453
	step [40/196], loss=99.3643
	step [41/196], loss=78.8042
	step [42/196], loss=99.6159
	step [43/196], loss=88.2655
	step [44/196], loss=83.2800
	step [45/196], loss=75.5426
	step [46/196], loss=93.4932
	step [47/196], loss=64.7874
	step [48/196], loss=68.3068
	step [49/196], loss=87.9747
	step [50/196], loss=76.4298
	step [51/196], loss=77.6891
	step [52/196], loss=69.1892
	step [53/196], loss=75.3717
	step [54/196], loss=74.9869
	step [55/196], loss=88.9939
	step [56/196], loss=86.5422
	step [57/196], loss=70.2102
	step [58/196], loss=77.0100
	step [59/196], loss=81.7023
	step [60/196], loss=105.2364
	step [61/196], loss=89.3769
	step [62/196], loss=82.3626
	step [63/196], loss=77.7485
	step [64/196], loss=79.4758
	step [65/196], loss=89.4253
	step [66/196], loss=96.2813
	step [67/196], loss=77.3212
	step [68/196], loss=79.3487
	step [69/196], loss=75.0585
	step [70/196], loss=100.3081
	step [71/196], loss=89.4640
	step [72/196], loss=86.0644
	step [73/196], loss=96.1657
	step [74/196], loss=78.0400
	step [75/196], loss=82.4830
	step [76/196], loss=64.7589
	step [77/196], loss=60.2921
	step [78/196], loss=66.8508
	step [79/196], loss=80.1340
	step [80/196], loss=88.6315
	step [81/196], loss=105.5286
	step [82/196], loss=75.2116
	step [83/196], loss=70.7924
	step [84/196], loss=79.7655
	step [85/196], loss=84.7420
	step [86/196], loss=69.2341
	step [87/196], loss=79.7140
	step [88/196], loss=82.2320
	step [89/196], loss=88.3015
	step [90/196], loss=77.4393
	step [91/196], loss=77.4327
	step [92/196], loss=61.5691
	step [93/196], loss=68.0575
	step [94/196], loss=86.9449
	step [95/196], loss=85.2142
	step [96/196], loss=86.9935
	step [97/196], loss=96.1124
	step [98/196], loss=60.0486
	step [99/196], loss=92.8310
	step [100/196], loss=80.6552
	step [101/196], loss=67.0834
	step [102/196], loss=87.9412
	step [103/196], loss=76.5223
	step [104/196], loss=83.4373
	step [105/196], loss=81.9138
	step [106/196], loss=65.2550
	step [107/196], loss=92.2599
	step [108/196], loss=82.0330
	step [109/196], loss=72.2421
	step [110/196], loss=91.4023
	step [111/196], loss=78.9680
	step [112/196], loss=99.9425
	step [113/196], loss=96.2326
	step [114/196], loss=74.8205
	step [115/196], loss=74.7565
	step [116/196], loss=77.6427
	step [117/196], loss=58.8301
	step [118/196], loss=89.1819
	step [119/196], loss=75.3801
	step [120/196], loss=79.4057
	step [121/196], loss=91.5836
	step [122/196], loss=74.8961
	step [123/196], loss=75.8051
	step [124/196], loss=81.9339
	step [125/196], loss=89.5958
	step [126/196], loss=75.5396
	step [127/196], loss=88.2931
	step [128/196], loss=84.6058
	step [129/196], loss=72.6843
	step [130/196], loss=94.4621
	step [131/196], loss=93.9426
	step [132/196], loss=79.8684
	step [133/196], loss=83.6327
	step [134/196], loss=109.1400
	step [135/196], loss=72.6347
	step [136/196], loss=77.4036
	step [137/196], loss=78.9780
	step [138/196], loss=97.0677
	step [139/196], loss=99.6939
	step [140/196], loss=73.7702
	step [141/196], loss=73.9635
	step [142/196], loss=73.3338
	step [143/196], loss=66.6573
	step [144/196], loss=69.8658
	step [145/196], loss=81.8386
	step [146/196], loss=73.1083
	step [147/196], loss=82.0527
	step [148/196], loss=78.2202
	step [149/196], loss=87.9300
	step [150/196], loss=97.1390
	step [151/196], loss=91.5411
	step [152/196], loss=59.1549
	step [153/196], loss=68.9455
	step [154/196], loss=80.2134
	step [155/196], loss=83.2200
	step [156/196], loss=85.3198
	step [157/196], loss=80.5123
	step [158/196], loss=90.0508
	step [159/196], loss=71.6238
	step [160/196], loss=85.8309
	step [161/196], loss=66.4523
	step [162/196], loss=86.1610
	step [163/196], loss=96.3127
	step [164/196], loss=79.5167
	step [165/196], loss=94.6606
	step [166/196], loss=67.7039
	step [167/196], loss=78.5242
	step [168/196], loss=79.7819
	step [169/196], loss=68.2332
	step [170/196], loss=73.9363
	step [171/196], loss=83.1466
	step [172/196], loss=78.5157
	step [173/196], loss=91.4661
	step [174/196], loss=92.1938
	step [175/196], loss=76.8675
	step [176/196], loss=81.1560
	step [177/196], loss=78.3834
	step [178/196], loss=83.6612
	step [179/196], loss=86.6906
	step [180/196], loss=83.0025
	step [181/196], loss=67.7205
	step [182/196], loss=82.6232
	step [183/196], loss=101.6514
	step [184/196], loss=68.7824
	step [185/196], loss=80.6008
	step [186/196], loss=82.2245
	step [187/196], loss=82.2658
	step [188/196], loss=67.2845
	step [189/196], loss=82.7971
	step [190/196], loss=71.1812
	step [191/196], loss=85.8465
	step [192/196], loss=70.0276
	step [193/196], loss=63.1035
	step [194/196], loss=97.7447
	step [195/196], loss=93.5631
	step [196/196], loss=21.3085
	Evaluating
	loss=0.0231, precision=0.3273, recall=0.9516, f1=0.4871
Training epoch 17
	step [1/196], loss=81.2502
	step [2/196], loss=77.0165
	step [3/196], loss=73.5118
	step [4/196], loss=77.3974
	step [5/196], loss=83.4177
	step [6/196], loss=71.7531
	step [7/196], loss=73.6787
	step [8/196], loss=74.0818
	step [9/196], loss=71.4749
	step [10/196], loss=83.1131
	step [11/196], loss=78.4129
	step [12/196], loss=64.7162
	step [13/196], loss=58.3224
	step [14/196], loss=82.1538
	step [15/196], loss=59.3943
	step [16/196], loss=99.2261
	step [17/196], loss=67.0449
	step [18/196], loss=87.1233
	step [19/196], loss=78.4146
	step [20/196], loss=81.0456
	step [21/196], loss=77.3222
	step [22/196], loss=77.4219
	step [23/196], loss=83.2219
	step [24/196], loss=78.2333
	step [25/196], loss=96.5894
	step [26/196], loss=83.1933
	step [27/196], loss=89.5763
	step [28/196], loss=93.2474
	step [29/196], loss=91.0864
	step [30/196], loss=78.3670
	step [31/196], loss=85.7099
	step [32/196], loss=81.6523
	step [33/196], loss=77.0643
	step [34/196], loss=69.6458
	step [35/196], loss=99.2633
	step [36/196], loss=103.5059
	step [37/196], loss=89.2689
	step [38/196], loss=78.9931
	step [39/196], loss=82.2656
	step [40/196], loss=70.2582
	step [41/196], loss=76.4376
	step [42/196], loss=74.2496
	step [43/196], loss=92.4008
	step [44/196], loss=80.3826
	step [45/196], loss=81.7539
	step [46/196], loss=75.0868
	step [47/196], loss=84.5325
	step [48/196], loss=72.7152
	step [49/196], loss=85.3310
	step [50/196], loss=58.8744
	step [51/196], loss=99.3888
	step [52/196], loss=79.9173
	step [53/196], loss=96.3841
	step [54/196], loss=73.9719
	step [55/196], loss=79.3336
	step [56/196], loss=60.2458
	step [57/196], loss=94.5228
	step [58/196], loss=92.9106
	step [59/196], loss=81.6847
	step [60/196], loss=92.9498
	step [61/196], loss=85.1434
	step [62/196], loss=105.7477
	step [63/196], loss=87.7589
	step [64/196], loss=84.3197
	step [65/196], loss=82.7689
	step [66/196], loss=89.5386
	step [67/196], loss=74.7700
	step [68/196], loss=74.5910
	step [69/196], loss=69.1597
	step [70/196], loss=82.9316
	step [71/196], loss=92.1680
	step [72/196], loss=88.4838
	step [73/196], loss=84.7235
	step [74/196], loss=81.8214
	step [75/196], loss=86.1278
	step [76/196], loss=75.6055
	step [77/196], loss=71.6634
	step [78/196], loss=80.6574
	step [79/196], loss=78.3774
	step [80/196], loss=80.5372
	step [81/196], loss=77.8069
	step [82/196], loss=83.3408
	step [83/196], loss=85.9837
	step [84/196], loss=91.8219
	step [85/196], loss=77.9169
	step [86/196], loss=66.7289
	step [87/196], loss=80.4687
	step [88/196], loss=85.0040
	step [89/196], loss=74.7761
	step [90/196], loss=65.7221
	step [91/196], loss=66.7550
	step [92/196], loss=99.6983
	step [93/196], loss=82.5692
	step [94/196], loss=84.8053
	step [95/196], loss=80.8945
	step [96/196], loss=85.6094
	step [97/196], loss=85.0873
	step [98/196], loss=71.6040
	step [99/196], loss=71.8888
	step [100/196], loss=67.0749
	step [101/196], loss=95.5144
	step [102/196], loss=63.4480
	step [103/196], loss=88.5677
	step [104/196], loss=94.9854
	step [105/196], loss=92.1110
	step [106/196], loss=70.7289
	step [107/196], loss=98.4458
	step [108/196], loss=90.9809
	step [109/196], loss=84.4922
	step [110/196], loss=82.9320
	step [111/196], loss=69.6079
	step [112/196], loss=97.7781
	step [113/196], loss=84.8495
	step [114/196], loss=91.3618
	step [115/196], loss=86.9799
	step [116/196], loss=77.8958
	step [117/196], loss=98.3977
	step [118/196], loss=92.5575
	step [119/196], loss=69.8609
	step [120/196], loss=76.3302
	step [121/196], loss=80.8889
	step [122/196], loss=86.0794
	step [123/196], loss=78.1927
	step [124/196], loss=86.9881
	step [125/196], loss=77.9144
	step [126/196], loss=82.9429
	step [127/196], loss=82.0753
	step [128/196], loss=73.9233
	step [129/196], loss=77.5248
	step [130/196], loss=91.4692
	step [131/196], loss=74.8840
	step [132/196], loss=87.0714
	step [133/196], loss=75.4621
	step [134/196], loss=61.6214
	step [135/196], loss=71.5216
	step [136/196], loss=75.9421
	step [137/196], loss=85.7747
	step [138/196], loss=85.8711
	step [139/196], loss=70.9683
	step [140/196], loss=67.1145
	step [141/196], loss=66.1365
	step [142/196], loss=71.6337
	step [143/196], loss=94.5288
	step [144/196], loss=80.8434
	step [145/196], loss=74.1246
	step [146/196], loss=74.1788
	step [147/196], loss=82.4285
	step [148/196], loss=70.0753
	step [149/196], loss=73.8650
	step [150/196], loss=75.9797
	step [151/196], loss=87.1529
	step [152/196], loss=83.0706
	step [153/196], loss=56.3739
	step [154/196], loss=64.0179
	step [155/196], loss=73.1737
	step [156/196], loss=63.6485
	step [157/196], loss=90.9146
	step [158/196], loss=73.7022
	step [159/196], loss=90.8126
	step [160/196], loss=81.6987
	step [161/196], loss=77.2674
	step [162/196], loss=74.0011
	step [163/196], loss=91.2943
	step [164/196], loss=74.2138
	step [165/196], loss=72.0038
	step [166/196], loss=95.5022
	step [167/196], loss=72.6606
	step [168/196], loss=74.6421
	step [169/196], loss=89.8858
	step [170/196], loss=87.4049
	step [171/196], loss=71.0977
	step [172/196], loss=67.5145
	step [173/196], loss=68.4570
	step [174/196], loss=106.6027
	step [175/196], loss=88.5807
	step [176/196], loss=79.4331
	step [177/196], loss=96.9125
	step [178/196], loss=66.6517
	step [179/196], loss=85.0784
	step [180/196], loss=79.0653
	step [181/196], loss=86.1142
	step [182/196], loss=73.4729
	step [183/196], loss=70.2297
	step [184/196], loss=75.6268
	step [185/196], loss=68.5546
	step [186/196], loss=73.7077
	step [187/196], loss=97.7934
	step [188/196], loss=83.3071
	step [189/196], loss=66.7718
	step [190/196], loss=70.1020
	step [191/196], loss=94.3820
	step [192/196], loss=72.3683
	step [193/196], loss=77.8700
	step [194/196], loss=87.8234
	step [195/196], loss=84.2719
	step [196/196], loss=16.4197
	Evaluating
	loss=0.0191, precision=0.3938, recall=0.9369, f1=0.5545
Training epoch 18
	step [1/196], loss=80.2668
	step [2/196], loss=89.1064
	step [3/196], loss=81.2241
	step [4/196], loss=74.8635
	step [5/196], loss=85.4013
	step [6/196], loss=87.0658
	step [7/196], loss=80.1796
	step [8/196], loss=92.1903
	step [9/196], loss=86.6761
	step [10/196], loss=75.6931
	step [11/196], loss=90.4495
	step [12/196], loss=83.2525
	step [13/196], loss=65.3743
	step [14/196], loss=91.7592
	step [15/196], loss=84.3746
	step [16/196], loss=95.8332
	step [17/196], loss=74.2259
	step [18/196], loss=94.9927
	step [19/196], loss=81.9336
	step [20/196], loss=89.3990
	step [21/196], loss=77.0384
	step [22/196], loss=97.6051
	step [23/196], loss=50.0102
	step [24/196], loss=55.3688
	step [25/196], loss=90.3075
	step [26/196], loss=75.3790
	step [27/196], loss=64.6591
	step [28/196], loss=82.2484
	step [29/196], loss=85.9292
	step [30/196], loss=93.8006
	step [31/196], loss=93.7030
	step [32/196], loss=76.9913
	step [33/196], loss=73.2088
	step [34/196], loss=84.4813
	step [35/196], loss=77.6480
	step [36/196], loss=73.9538
	step [37/196], loss=75.2890
	step [38/196], loss=79.0137
	step [39/196], loss=69.8684
	step [40/196], loss=77.4226
	step [41/196], loss=95.6987
	step [42/196], loss=80.1377
	step [43/196], loss=86.5917
	step [44/196], loss=80.3730
	step [45/196], loss=80.9089
	step [46/196], loss=80.7894
	step [47/196], loss=82.5964
	step [48/196], loss=64.9321
	step [49/196], loss=72.4340
	step [50/196], loss=85.5114
	step [51/196], loss=69.8024
	step [52/196], loss=75.8109
	step [53/196], loss=86.0630
	step [54/196], loss=78.0117
	step [55/196], loss=81.8805
	step [56/196], loss=72.2346
	step [57/196], loss=99.6451
	step [58/196], loss=79.7934
	step [59/196], loss=81.1307
	step [60/196], loss=72.9154
	step [61/196], loss=91.6762
	step [62/196], loss=68.5086
	step [63/196], loss=85.5803
	step [64/196], loss=75.9850
	step [65/196], loss=63.6864
	step [66/196], loss=69.5618
	step [67/196], loss=74.8965
	step [68/196], loss=71.1351
	step [69/196], loss=69.4211
	step [70/196], loss=67.1240
	step [71/196], loss=75.3726
	step [72/196], loss=73.8891
	step [73/196], loss=79.5930
	step [74/196], loss=76.6172
	step [75/196], loss=73.4413
	step [76/196], loss=80.5875
	step [77/196], loss=80.1225
	step [78/196], loss=69.2766
	step [79/196], loss=71.2227
	step [80/196], loss=88.1499
	step [81/196], loss=86.6768
	step [82/196], loss=74.6143
	step [83/196], loss=65.9863
	step [84/196], loss=52.8214
	step [85/196], loss=84.2995
	step [86/196], loss=91.8212
	step [87/196], loss=64.6154
	step [88/196], loss=73.6161
	step [89/196], loss=95.1947
	step [90/196], loss=79.4641
	step [91/196], loss=84.6222
	step [92/196], loss=86.4825
	step [93/196], loss=86.5140
	step [94/196], loss=76.1222
	step [95/196], loss=74.5463
	step [96/196], loss=95.4905
	step [97/196], loss=74.6911
	step [98/196], loss=65.0009
	step [99/196], loss=96.1509
	step [100/196], loss=91.4494
	step [101/196], loss=91.4846
	step [102/196], loss=86.7081
	step [103/196], loss=77.3792
	step [104/196], loss=93.6093
	step [105/196], loss=77.0209
	step [106/196], loss=75.1352
	step [107/196], loss=74.8416
	step [108/196], loss=84.2985
	step [109/196], loss=81.1226
	step [110/196], loss=103.5395
	step [111/196], loss=66.9540
	step [112/196], loss=99.1267
	step [113/196], loss=80.2141
	step [114/196], loss=82.6007
	step [115/196], loss=77.8568
	step [116/196], loss=73.8784
	step [117/196], loss=69.6699
	step [118/196], loss=72.0287
	step [119/196], loss=77.0530
	step [120/196], loss=69.4317
	step [121/196], loss=95.0587
	step [122/196], loss=85.7600
	step [123/196], loss=79.5945
	step [124/196], loss=69.8841
	step [125/196], loss=72.1492
	step [126/196], loss=78.5815
	step [127/196], loss=76.6634
	step [128/196], loss=102.4018
	step [129/196], loss=87.1385
	step [130/196], loss=78.9055
	step [131/196], loss=90.9574
	step [132/196], loss=82.7746
	step [133/196], loss=71.7189
	step [134/196], loss=78.0968
	step [135/196], loss=76.5111
	step [136/196], loss=65.1313
	step [137/196], loss=84.5388
	step [138/196], loss=78.9856
	step [139/196], loss=82.5785
	step [140/196], loss=78.8557
	step [141/196], loss=77.8164
	step [142/196], loss=89.3180
	step [143/196], loss=58.6280
	step [144/196], loss=68.8147
	step [145/196], loss=80.0805
	step [146/196], loss=71.1762
	step [147/196], loss=79.7416
	step [148/196], loss=78.9480
	step [149/196], loss=66.8948
	step [150/196], loss=59.9923
	step [151/196], loss=92.7415
	step [152/196], loss=85.6846
	step [153/196], loss=72.3126
	step [154/196], loss=79.8202
	step [155/196], loss=56.9286
	step [156/196], loss=58.4936
	step [157/196], loss=81.6874
	step [158/196], loss=89.9083
	step [159/196], loss=72.4688
	step [160/196], loss=73.9527
	step [161/196], loss=85.5489
	step [162/196], loss=75.7495
	step [163/196], loss=100.2465
	step [164/196], loss=76.0832
	step [165/196], loss=82.4795
	step [166/196], loss=85.6830
	step [167/196], loss=86.3689
	step [168/196], loss=82.3709
	step [169/196], loss=78.0753
	step [170/196], loss=79.2818
	step [171/196], loss=71.4020
	step [172/196], loss=103.9297
	step [173/196], loss=87.7757
	step [174/196], loss=74.1271
	step [175/196], loss=92.4523
	step [176/196], loss=75.6896
	step [177/196], loss=70.6840
	step [178/196], loss=70.0708
	step [179/196], loss=103.8984
	step [180/196], loss=63.6253
	step [181/196], loss=84.3891
	step [182/196], loss=83.2133
	step [183/196], loss=77.0445
	step [184/196], loss=65.9977
	step [185/196], loss=75.5870
	step [186/196], loss=78.7745
	step [187/196], loss=70.7018
	step [188/196], loss=87.1903
	step [189/196], loss=76.7582
	step [190/196], loss=68.9437
	step [191/196], loss=93.0094
	step [192/196], loss=76.8879
	step [193/196], loss=75.6317
	step [194/196], loss=80.3995
	step [195/196], loss=76.7290
	step [196/196], loss=15.4047
	Evaluating
	loss=0.0174, precision=0.3973, recall=0.9227, f1=0.5554
Training epoch 19
	step [1/196], loss=85.9038
	step [2/196], loss=66.2900
	step [3/196], loss=63.0836
	step [4/196], loss=82.2017
	step [5/196], loss=58.7315
	step [6/196], loss=64.9308
	step [7/196], loss=78.2996
	step [8/196], loss=87.1620
	step [9/196], loss=96.1238
	step [10/196], loss=64.9257
	step [11/196], loss=104.9771
	step [12/196], loss=77.6963
	step [13/196], loss=82.6249
	step [14/196], loss=80.0559
	step [15/196], loss=81.4086
	step [16/196], loss=64.0751
	step [17/196], loss=69.5515
	step [18/196], loss=70.3731
	step [19/196], loss=78.2459
	step [20/196], loss=77.6356
	step [21/196], loss=72.5240
	step [22/196], loss=89.0174
	step [23/196], loss=81.1682
	step [24/196], loss=78.3303
	step [25/196], loss=74.7643
	step [26/196], loss=82.3675
	step [27/196], loss=86.9122
	step [28/196], loss=84.8843
	step [29/196], loss=82.6196
	step [30/196], loss=72.3828
	step [31/196], loss=85.7284
	step [32/196], loss=83.9934
	step [33/196], loss=62.2001
	step [34/196], loss=76.7311
	step [35/196], loss=86.0780
	step [36/196], loss=93.7198
	step [37/196], loss=79.3698
	step [38/196], loss=67.7316
	step [39/196], loss=73.1425
	step [40/196], loss=83.6938
	step [41/196], loss=88.5513
	step [42/196], loss=60.0581
	step [43/196], loss=73.9704
	step [44/196], loss=67.1327
	step [45/196], loss=67.0234
	step [46/196], loss=95.5788
	step [47/196], loss=83.5355
	step [48/196], loss=77.2018
	step [49/196], loss=71.2440
	step [50/196], loss=81.8126
	step [51/196], loss=88.7710
	step [52/196], loss=76.1143
	step [53/196], loss=74.1901
	step [54/196], loss=101.8123
	step [55/196], loss=79.9210
	step [56/196], loss=80.9623
	step [57/196], loss=82.6639
	step [58/196], loss=75.9128
	step [59/196], loss=75.5784
	step [60/196], loss=74.4363
	step [61/196], loss=62.8486
	step [62/196], loss=96.6099
	step [63/196], loss=85.6684
	step [64/196], loss=73.1980
	step [65/196], loss=86.3934
	step [66/196], loss=82.3488
	step [67/196], loss=90.1828
	step [68/196], loss=79.1787
	step [69/196], loss=73.2526
	step [70/196], loss=86.8353
	step [71/196], loss=66.7472
	step [72/196], loss=73.9737
	step [73/196], loss=78.0257
	step [74/196], loss=88.2660
	step [75/196], loss=65.7540
	step [76/196], loss=86.6674
	step [77/196], loss=92.6059
	step [78/196], loss=74.9516
	step [79/196], loss=76.5228
	step [80/196], loss=85.0813
	step [81/196], loss=59.6430
	step [82/196], loss=67.3443
	step [83/196], loss=78.5217
	step [84/196], loss=71.3252
	step [85/196], loss=76.6987
	step [86/196], loss=77.6864
	step [87/196], loss=71.4580
	step [88/196], loss=78.8493
	step [89/196], loss=79.7450
	step [90/196], loss=78.7290
	step [91/196], loss=76.3239
	step [92/196], loss=73.1446
	step [93/196], loss=67.7461
	step [94/196], loss=59.8707
	step [95/196], loss=76.3398
	step [96/196], loss=98.1390
	step [97/196], loss=74.2491
	step [98/196], loss=108.2433
	step [99/196], loss=75.0547
	step [100/196], loss=65.1057
	step [101/196], loss=79.1229
	step [102/196], loss=69.4961
	step [103/196], loss=85.4508
	step [104/196], loss=80.1581
	step [105/196], loss=80.7260
	step [106/196], loss=80.0333
	step [107/196], loss=81.3316
	step [108/196], loss=82.7089
	step [109/196], loss=81.0027
	step [110/196], loss=78.1139
	step [111/196], loss=73.7895
	step [112/196], loss=88.4164
	step [113/196], loss=86.6416
	step [114/196], loss=78.3488
	step [115/196], loss=67.7081
	step [116/196], loss=78.5388
	step [117/196], loss=97.7180
	step [118/196], loss=90.0890
	step [119/196], loss=103.0703
	step [120/196], loss=72.0384
	step [121/196], loss=82.4468
	step [122/196], loss=75.0059
	step [123/196], loss=77.0874
	step [124/196], loss=85.5479
	step [125/196], loss=62.3772
	step [126/196], loss=75.4897
	step [127/196], loss=82.0297
	step [128/196], loss=77.9455
	step [129/196], loss=66.3928
	step [130/196], loss=70.9184
	step [131/196], loss=63.5455
	step [132/196], loss=89.0552
	step [133/196], loss=70.1189
	step [134/196], loss=70.1946
	step [135/196], loss=83.8003
	step [136/196], loss=74.9064
	step [137/196], loss=69.3997
	step [138/196], loss=62.8430
	step [139/196], loss=58.5690
	step [140/196], loss=80.1362
	step [141/196], loss=69.5863
	step [142/196], loss=86.4518
	step [143/196], loss=85.4548
	step [144/196], loss=93.9117
	step [145/196], loss=75.2028
	step [146/196], loss=94.6463
	step [147/196], loss=75.0882
	step [148/196], loss=66.4996
	step [149/196], loss=73.5262
	step [150/196], loss=92.5071
	step [151/196], loss=80.4929
	step [152/196], loss=70.3254
	step [153/196], loss=59.2376
	step [154/196], loss=73.9592
	step [155/196], loss=93.0340
	step [156/196], loss=81.5013
	step [157/196], loss=77.7808
	step [158/196], loss=85.9417
	step [159/196], loss=72.9586
	step [160/196], loss=82.4370
	step [161/196], loss=88.4761
	step [162/196], loss=83.1191
	step [163/196], loss=89.6974
	step [164/196], loss=86.2130
	step [165/196], loss=98.3926
	step [166/196], loss=72.2170
	step [167/196], loss=86.7203
	step [168/196], loss=63.0366
	step [169/196], loss=81.1094
	step [170/196], loss=58.4153
	step [171/196], loss=72.3765
	step [172/196], loss=92.9540
	step [173/196], loss=78.7941
	step [174/196], loss=81.8761
	step [175/196], loss=87.7723
	step [176/196], loss=70.8924
	step [177/196], loss=67.3268
	step [178/196], loss=75.3463
	step [179/196], loss=96.2979
	step [180/196], loss=94.6500
	step [181/196], loss=86.8353
	step [182/196], loss=92.5425
	step [183/196], loss=83.3870
	step [184/196], loss=75.8412
	step [185/196], loss=79.3301
	step [186/196], loss=97.9756
	step [187/196], loss=77.9558
	step [188/196], loss=67.0560
	step [189/196], loss=80.4782
	step [190/196], loss=82.9713
	step [191/196], loss=90.2336
	step [192/196], loss=81.9753
	step [193/196], loss=81.7482
	step [194/196], loss=71.0507
	step [195/196], loss=73.7228
	step [196/196], loss=13.6907
	Evaluating
	loss=0.0168, precision=0.3672, recall=0.9314, f1=0.5267
Training epoch 20
	step [1/196], loss=69.4465
	step [2/196], loss=79.5607
	step [3/196], loss=79.0734
	step [4/196], loss=88.7281
	step [5/196], loss=75.5786
	step [6/196], loss=68.0822
	step [7/196], loss=79.4735
	step [8/196], loss=69.9402
	step [9/196], loss=72.3447
	step [10/196], loss=73.0714
	step [11/196], loss=81.4064
	step [12/196], loss=86.7627
	step [13/196], loss=69.4270
	step [14/196], loss=55.6676
	step [15/196], loss=75.5956
	step [16/196], loss=66.0353
	step [17/196], loss=73.1736
	step [18/196], loss=87.1069
	step [19/196], loss=82.2425
	step [20/196], loss=72.2727
	step [21/196], loss=72.3323
	step [22/196], loss=78.0571
	step [23/196], loss=100.0473
	step [24/196], loss=90.4335
	step [25/196], loss=68.0331
	step [26/196], loss=80.1353
	step [27/196], loss=66.9271
	step [28/196], loss=73.3076
	step [29/196], loss=90.0363
	step [30/196], loss=84.1040
	step [31/196], loss=67.7334
	step [32/196], loss=74.3614
	step [33/196], loss=54.4230
	step [34/196], loss=85.0986
	step [35/196], loss=64.5910
	step [36/196], loss=77.8016
	step [37/196], loss=76.6706
	step [38/196], loss=79.1572
	step [39/196], loss=92.6709
	step [40/196], loss=76.9275
	step [41/196], loss=70.8372
	step [42/196], loss=80.8887
	step [43/196], loss=97.0554
	step [44/196], loss=73.8532
	step [45/196], loss=71.1389
	step [46/196], loss=65.2667
	step [47/196], loss=73.4677
	step [48/196], loss=87.7548
	step [49/196], loss=73.7061
	step [50/196], loss=70.1479
	step [51/196], loss=75.1663
	step [52/196], loss=76.5742
	step [53/196], loss=73.3580
	step [54/196], loss=88.7499
	step [55/196], loss=76.2342
	step [56/196], loss=72.8531
	step [57/196], loss=78.1388
	step [58/196], loss=59.7286
	step [59/196], loss=79.6404
	step [60/196], loss=78.1968
	step [61/196], loss=75.6448
	step [62/196], loss=93.4348
	step [63/196], loss=83.6063
	step [64/196], loss=76.4305
	step [65/196], loss=79.0201
	step [66/196], loss=57.1336
	step [67/196], loss=90.5232
	step [68/196], loss=74.6160
	step [69/196], loss=68.7078
	step [70/196], loss=81.1913
	step [71/196], loss=88.0135
	step [72/196], loss=81.3596
	step [73/196], loss=88.7435
	step [74/196], loss=85.1912
	step [75/196], loss=63.0361
	step [76/196], loss=80.1386
	step [77/196], loss=70.1312
	step [78/196], loss=81.0002
	step [79/196], loss=115.7242
	step [80/196], loss=72.5978
	step [81/196], loss=77.9667
	step [82/196], loss=65.7929
	step [83/196], loss=75.1063
	step [84/196], loss=63.4414
	step [85/196], loss=70.3225
	step [86/196], loss=85.1250
	step [87/196], loss=84.5269
	step [88/196], loss=98.5718
	step [89/196], loss=70.7025
	step [90/196], loss=67.6504
	step [91/196], loss=77.2751
	step [92/196], loss=79.7867
	step [93/196], loss=72.9050
	step [94/196], loss=68.9885
	step [95/196], loss=78.3801
	step [96/196], loss=78.7673
	step [97/196], loss=90.7485
	step [98/196], loss=89.1534
	step [99/196], loss=78.9176
	step [100/196], loss=82.1426
	step [101/196], loss=89.2668
	step [102/196], loss=87.2959
	step [103/196], loss=68.9274
	step [104/196], loss=81.5071
	step [105/196], loss=70.8825
	step [106/196], loss=60.6243
	step [107/196], loss=76.8689
	step [108/196], loss=74.4556
	step [109/196], loss=80.2267
	step [110/196], loss=94.3118
	step [111/196], loss=77.9857
	step [112/196], loss=76.5592
	step [113/196], loss=84.6937
	step [114/196], loss=85.2713
	step [115/196], loss=79.6879
	step [116/196], loss=78.8860
	step [117/196], loss=84.6921
	step [118/196], loss=76.4350
	step [119/196], loss=87.9013
	step [120/196], loss=83.5569
	step [121/196], loss=89.4875
	step [122/196], loss=73.8602
	step [123/196], loss=82.7960
	step [124/196], loss=85.6884
	step [125/196], loss=67.3881
	step [126/196], loss=81.5864
	step [127/196], loss=76.8313
	step [128/196], loss=81.2373
	step [129/196], loss=87.1849
	step [130/196], loss=76.8842
	step [131/196], loss=82.9875
	step [132/196], loss=84.7144
	step [133/196], loss=88.7343
	step [134/196], loss=73.8411
	step [135/196], loss=72.2928
	step [136/196], loss=91.0323
	step [137/196], loss=76.6741
	step [138/196], loss=82.2578
	step [139/196], loss=79.5747
	step [140/196], loss=77.8607
	step [141/196], loss=89.5798
	step [142/196], loss=82.2820
	step [143/196], loss=82.3999
	step [144/196], loss=76.0950
	step [145/196], loss=75.9633
	step [146/196], loss=84.9770
	step [147/196], loss=78.3055
	step [148/196], loss=83.5300
	step [149/196], loss=84.8294
	step [150/196], loss=70.5225
	step [151/196], loss=86.3922
	step [152/196], loss=93.1594
	step [153/196], loss=82.1546
	step [154/196], loss=92.5385
	step [155/196], loss=68.2093
	step [156/196], loss=74.8130
	step [157/196], loss=84.9663
	step [158/196], loss=66.6712
	step [159/196], loss=68.2946
	step [160/196], loss=70.2589
	step [161/196], loss=75.2190
	step [162/196], loss=94.0914
	step [163/196], loss=71.5603
	step [164/196], loss=67.3580
	step [165/196], loss=83.4302
	step [166/196], loss=81.6104
	step [167/196], loss=83.1926
	step [168/196], loss=62.9455
	step [169/196], loss=76.3521
	step [170/196], loss=78.3822
	step [171/196], loss=81.7044
	step [172/196], loss=74.8747
	step [173/196], loss=63.3900
	step [174/196], loss=71.7139
	step [175/196], loss=70.3113
	step [176/196], loss=86.2342
	step [177/196], loss=84.9164
	step [178/196], loss=94.5023
	step [179/196], loss=67.7214
	step [180/196], loss=71.8907
	step [181/196], loss=62.5594
	step [182/196], loss=69.9740
	step [183/196], loss=78.9797
	step [184/196], loss=74.4293
	step [185/196], loss=64.3744
	step [186/196], loss=90.1023
	step [187/196], loss=85.6718
	step [188/196], loss=71.6459
	step [189/196], loss=77.7316
	step [190/196], loss=86.3888
	step [191/196], loss=73.9468
	step [192/196], loss=71.8613
	step [193/196], loss=79.3687
	step [194/196], loss=70.0041
	step [195/196], loss=71.8369
	step [196/196], loss=11.7412
	Evaluating
	loss=0.0164, precision=0.3751, recall=0.9091, f1=0.5311
Training epoch 21
	step [1/196], loss=82.5045
	step [2/196], loss=94.1349
	step [3/196], loss=56.3793
	step [4/196], loss=73.4711
	step [5/196], loss=88.4153
	step [6/196], loss=67.2244
	step [7/196], loss=89.8532
	step [8/196], loss=78.5993
	step [9/196], loss=71.6854
	step [10/196], loss=87.3638
	step [11/196], loss=74.6792
	step [12/196], loss=80.0620
	step [13/196], loss=73.3218
	step [14/196], loss=88.9316
	step [15/196], loss=70.8046
	step [16/196], loss=86.5136
	step [17/196], loss=93.8041
	step [18/196], loss=78.0358
	step [19/196], loss=71.6820
	step [20/196], loss=82.8283
	step [21/196], loss=86.7508
	step [22/196], loss=81.2133
	step [23/196], loss=72.4203
	step [24/196], loss=71.6231
	step [25/196], loss=82.2305
	step [26/196], loss=80.8046
	step [27/196], loss=79.7893
	step [28/196], loss=90.5352
	step [29/196], loss=73.8709
	step [30/196], loss=75.0519
	step [31/196], loss=76.3213
	step [32/196], loss=79.0089
	step [33/196], loss=74.2221
	step [34/196], loss=82.9194
	step [35/196], loss=78.6927
	step [36/196], loss=78.6766
	step [37/196], loss=81.8130
	step [38/196], loss=58.7881
	step [39/196], loss=84.2399
	step [40/196], loss=99.1377
	step [41/196], loss=56.1521
	step [42/196], loss=67.9874
	step [43/196], loss=73.9648
	step [44/196], loss=90.9935
	step [45/196], loss=93.9992
	step [46/196], loss=67.7230
	step [47/196], loss=85.3031
	step [48/196], loss=89.0228
	step [49/196], loss=86.3322
	step [50/196], loss=69.4057
	step [51/196], loss=72.1503
	step [52/196], loss=84.1920
	step [53/196], loss=72.5353
	step [54/196], loss=74.9201
	step [55/196], loss=85.8960
	step [56/196], loss=75.8841
	step [57/196], loss=82.1588
	step [58/196], loss=77.4971
	step [59/196], loss=73.5339
	step [60/196], loss=83.8230
	step [61/196], loss=72.4943
	step [62/196], loss=65.6273
	step [63/196], loss=71.3916
	step [64/196], loss=66.8757
	step [65/196], loss=82.9460
	step [66/196], loss=93.8918
	step [67/196], loss=65.8208
	step [68/196], loss=75.6580
	step [69/196], loss=58.9745
	step [70/196], loss=73.3001
	step [71/196], loss=60.0085
	step [72/196], loss=71.4539
	step [73/196], loss=70.1347
	step [74/196], loss=88.0277
	step [75/196], loss=82.7455
	step [76/196], loss=87.6613
	step [77/196], loss=84.9008
	step [78/196], loss=64.3883
	step [79/196], loss=77.1530
	step [80/196], loss=63.1928
	step [81/196], loss=74.2478
	step [82/196], loss=77.8452
	step [83/196], loss=87.0385
	step [84/196], loss=86.0955
	step [85/196], loss=82.0759
	step [86/196], loss=67.3320
	step [87/196], loss=94.7011
	step [88/196], loss=83.2694
	step [89/196], loss=82.7379
	step [90/196], loss=82.2681
	step [91/196], loss=83.0846
	step [92/196], loss=82.4156
	step [93/196], loss=80.6774
	step [94/196], loss=65.0275
	step [95/196], loss=87.4625
	step [96/196], loss=79.4430
	step [97/196], loss=65.5058
	step [98/196], loss=89.4663
	step [99/196], loss=83.1483
	step [100/196], loss=63.2531
	step [101/196], loss=74.2699
	step [102/196], loss=80.1046
	step [103/196], loss=74.5799
	step [104/196], loss=85.8024
	step [105/196], loss=80.2646
	step [106/196], loss=80.5546
	step [107/196], loss=74.7827
	step [108/196], loss=71.3919
	step [109/196], loss=83.3487
	step [110/196], loss=80.4853
	step [111/196], loss=80.8261
	step [112/196], loss=74.2653
	step [113/196], loss=69.9384
	step [114/196], loss=75.2815
	step [115/196], loss=79.1131
	step [116/196], loss=72.0079
	step [117/196], loss=65.9553
	step [118/196], loss=73.2134
	step [119/196], loss=76.3204
	step [120/196], loss=76.6253
	step [121/196], loss=70.3271
	step [122/196], loss=91.9139
	step [123/196], loss=88.9759
	step [124/196], loss=81.8284
	step [125/196], loss=97.2471
	step [126/196], loss=85.5876
	step [127/196], loss=83.2602
	step [128/196], loss=61.2476
	step [129/196], loss=57.7439
	step [130/196], loss=82.0784
	step [131/196], loss=88.5817
	step [132/196], loss=70.1149
	step [133/196], loss=73.2928
	step [134/196], loss=73.1286
	step [135/196], loss=70.4965
	step [136/196], loss=72.6499
	step [137/196], loss=76.7993
	step [138/196], loss=70.8375
	step [139/196], loss=75.1788
	step [140/196], loss=90.1254
	step [141/196], loss=63.9474
	step [142/196], loss=76.4481
	step [143/196], loss=87.0954
	step [144/196], loss=86.1625
	step [145/196], loss=74.9559
	step [146/196], loss=75.7400
	step [147/196], loss=73.8003
	step [148/196], loss=69.3754
	step [149/196], loss=72.3176
	step [150/196], loss=83.7231
	step [151/196], loss=72.3923
	step [152/196], loss=77.6784
	step [153/196], loss=66.1190
	step [154/196], loss=86.1356
	step [155/196], loss=72.6611
	step [156/196], loss=79.0637
	step [157/196], loss=77.0603
	step [158/196], loss=80.4503
	step [159/196], loss=78.9465
	step [160/196], loss=76.1665
	step [161/196], loss=66.7678
	step [162/196], loss=88.4969
	step [163/196], loss=54.6285
	step [164/196], loss=65.1976
	step [165/196], loss=75.6601
	step [166/196], loss=76.6752
	step [167/196], loss=68.6144
	step [168/196], loss=70.0519
	step [169/196], loss=83.4442
	step [170/196], loss=79.8046
	step [171/196], loss=87.0376
	step [172/196], loss=70.1594
	step [173/196], loss=76.9090
	step [174/196], loss=54.7406
	step [175/196], loss=92.2503
	step [176/196], loss=71.3308
	step [177/196], loss=66.6261
	step [178/196], loss=73.2643
	step [179/196], loss=62.7706
	step [180/196], loss=69.9053
	step [181/196], loss=90.9343
	step [182/196], loss=71.5601
	step [183/196], loss=79.4251
	step [184/196], loss=74.4241
	step [185/196], loss=83.3335
	step [186/196], loss=86.1307
	step [187/196], loss=84.3686
	step [188/196], loss=75.0490
	step [189/196], loss=76.0561
	step [190/196], loss=73.3915
	step [191/196], loss=78.1672
	step [192/196], loss=73.8773
	step [193/196], loss=80.0364
	step [194/196], loss=69.1849
	step [195/196], loss=76.1809
	step [196/196], loss=18.6016
	Evaluating
	loss=0.0128, precision=0.4814, recall=0.9047, f1=0.6284
saving model as: 0_saved_model.pth
Training epoch 22
	step [1/196], loss=83.6279
	step [2/196], loss=87.4724
	step [3/196], loss=82.1209
	step [4/196], loss=73.9969
	step [5/196], loss=77.8359
	step [6/196], loss=92.2217
	step [7/196], loss=83.8220
	step [8/196], loss=67.7700
	step [9/196], loss=90.2141
	step [10/196], loss=78.5136
	step [11/196], loss=62.6872
	step [12/196], loss=65.9246
	step [13/196], loss=77.7402
	step [14/196], loss=64.2141
	step [15/196], loss=84.6244
	step [16/196], loss=85.1988
	step [17/196], loss=87.0208
	step [18/196], loss=77.1322
	step [19/196], loss=73.5495
	step [20/196], loss=65.3118
	step [21/196], loss=80.0735
	step [22/196], loss=69.7731
	step [23/196], loss=66.3594
	step [24/196], loss=79.5053
	step [25/196], loss=80.5548
	step [26/196], loss=79.3385
	step [27/196], loss=79.5657
	step [28/196], loss=79.1621
	step [29/196], loss=78.8381
	step [30/196], loss=70.6338
	step [31/196], loss=74.0204
	step [32/196], loss=74.3911
	step [33/196], loss=77.4647
	step [34/196], loss=94.8294
	step [35/196], loss=89.8737
	step [36/196], loss=93.6804
	step [37/196], loss=84.3314
	step [38/196], loss=74.1092
	step [39/196], loss=67.2163
	step [40/196], loss=82.7346
	step [41/196], loss=57.8045
	step [42/196], loss=76.6303
	step [43/196], loss=54.1890
	step [44/196], loss=66.0166
	step [45/196], loss=67.5378
	step [46/196], loss=78.1978
	step [47/196], loss=75.9389
	step [48/196], loss=72.9735
	step [49/196], loss=84.5262
	step [50/196], loss=67.2934
	step [51/196], loss=75.7884
	step [52/196], loss=72.2509
	step [53/196], loss=71.4086
	step [54/196], loss=51.5714
	step [55/196], loss=75.9869
	step [56/196], loss=77.5335
	step [57/196], loss=67.1802
	step [58/196], loss=78.0918
	step [59/196], loss=80.0107
	step [60/196], loss=72.3171
	step [61/196], loss=72.9072
	step [62/196], loss=83.4914
	step [63/196], loss=81.4061
	step [64/196], loss=77.3458
	step [65/196], loss=71.3299
	step [66/196], loss=81.0844
	step [67/196], loss=82.6401
	step [68/196], loss=82.9278
	step [69/196], loss=75.0689
	step [70/196], loss=77.4791
	step [71/196], loss=69.2607
	step [72/196], loss=84.2762
	step [73/196], loss=82.4230
	step [74/196], loss=83.6166
	step [75/196], loss=73.0904
	step [76/196], loss=69.3187
	step [77/196], loss=64.0702
	step [78/196], loss=74.1828
	step [79/196], loss=84.8867
	step [80/196], loss=85.0968
	step [81/196], loss=90.1552
	step [82/196], loss=85.7501
	step [83/196], loss=80.2146
	step [84/196], loss=70.7028
	step [85/196], loss=83.8163
	step [86/196], loss=77.2869
	step [87/196], loss=94.3586
	step [88/196], loss=68.7278
	step [89/196], loss=81.2985
	step [90/196], loss=76.3561
	step [91/196], loss=84.4693
	step [92/196], loss=71.9402
	step [93/196], loss=81.3403
	step [94/196], loss=73.9379
	step [95/196], loss=53.1139
	step [96/196], loss=72.6717
	step [97/196], loss=72.0635
	step [98/196], loss=78.4824
	step [99/196], loss=82.1368
	step [100/196], loss=83.3797
	step [101/196], loss=70.3848
	step [102/196], loss=78.7404
	step [103/196], loss=84.4603
	step [104/196], loss=65.2397
	step [105/196], loss=81.7220
	step [106/196], loss=80.6400
	step [107/196], loss=84.8803
	step [108/196], loss=73.0366
	step [109/196], loss=66.8585
	step [110/196], loss=62.5352
	step [111/196], loss=87.5461
	step [112/196], loss=89.6281
	step [113/196], loss=79.5336
	step [114/196], loss=99.9560
	step [115/196], loss=77.0904
	step [116/196], loss=84.5652
	step [117/196], loss=72.6383
	step [118/196], loss=84.7043
	step [119/196], loss=66.5218
	step [120/196], loss=76.8190
	step [121/196], loss=83.1688
	step [122/196], loss=86.3032
	step [123/196], loss=60.6708
	step [124/196], loss=69.4391
	step [125/196], loss=86.7016
	step [126/196], loss=89.1242
	step [127/196], loss=79.9845
	step [128/196], loss=68.2291
	step [129/196], loss=78.6211
	step [130/196], loss=77.7686
	step [131/196], loss=69.7570
	step [132/196], loss=62.4967
	step [133/196], loss=78.3386
	step [134/196], loss=86.7576
	step [135/196], loss=75.6579
	step [136/196], loss=93.2624
	step [137/196], loss=79.5502
	step [138/196], loss=77.7756
	step [139/196], loss=62.6209
	step [140/196], loss=84.3860
	step [141/196], loss=65.1196
	step [142/196], loss=83.5165
	step [143/196], loss=70.8254
	step [144/196], loss=77.4717
	step [145/196], loss=78.7163
	step [146/196], loss=76.8296
	step [147/196], loss=60.1859
	step [148/196], loss=78.5563
	step [149/196], loss=92.0329
	step [150/196], loss=80.0008
	step [151/196], loss=82.4771
	step [152/196], loss=55.4992
	step [153/196], loss=81.2494
	step [154/196], loss=70.5105
	step [155/196], loss=85.5804
	step [156/196], loss=64.6404
	step [157/196], loss=66.8701
	step [158/196], loss=64.7948
	step [159/196], loss=83.6818
	step [160/196], loss=73.7755
	step [161/196], loss=79.2133
	step [162/196], loss=90.3155
	step [163/196], loss=76.8647
	step [164/196], loss=91.8482
	step [165/196], loss=72.8479
	step [166/196], loss=82.1116
	step [167/196], loss=60.2532
	step [168/196], loss=68.5621
	step [169/196], loss=87.1333
	step [170/196], loss=57.5473
	step [171/196], loss=85.7833
	step [172/196], loss=83.8198
	step [173/196], loss=92.2468
	step [174/196], loss=71.1564
	step [175/196], loss=59.8618
	step [176/196], loss=84.7033
	step [177/196], loss=73.3572
	step [178/196], loss=62.0614
	step [179/196], loss=74.1524
	step [180/196], loss=82.5879
	step [181/196], loss=61.9527
	step [182/196], loss=72.1037
	step [183/196], loss=71.2632
	step [184/196], loss=67.1530
	step [185/196], loss=88.5013
	step [186/196], loss=78.6049
	step [187/196], loss=65.0212
	step [188/196], loss=82.3935
	step [189/196], loss=74.5029
	step [190/196], loss=74.4313
	step [191/196], loss=66.6183
	step [192/196], loss=88.9534
	step [193/196], loss=83.3868
	step [194/196], loss=68.8407
	step [195/196], loss=86.0576
	step [196/196], loss=14.8169
	Evaluating
	loss=0.0116, precision=0.5084, recall=0.8856, f1=0.6460
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/196], loss=75.6571
	step [2/196], loss=81.4226
	step [3/196], loss=64.3081
	step [4/196], loss=72.0643
	step [5/196], loss=72.8635
	step [6/196], loss=79.4999
	step [7/196], loss=75.8070
	step [8/196], loss=70.2471
	step [9/196], loss=57.1807
	step [10/196], loss=63.5829
	step [11/196], loss=83.9456
	step [12/196], loss=62.4741
	step [13/196], loss=78.8950
	step [14/196], loss=65.5724
	step [15/196], loss=67.7381
	step [16/196], loss=76.2674
	step [17/196], loss=80.8226
	step [18/196], loss=70.5789
	step [19/196], loss=91.8650
	step [20/196], loss=77.1305
	step [21/196], loss=75.4325
	step [22/196], loss=69.3148
	step [23/196], loss=80.8493
	step [24/196], loss=64.1560
	step [25/196], loss=82.5463
	step [26/196], loss=66.4669
	step [27/196], loss=75.1006
	step [28/196], loss=70.6277
	step [29/196], loss=79.2890
	step [30/196], loss=65.1202
	step [31/196], loss=68.4374
	step [32/196], loss=83.1977
	step [33/196], loss=62.6189
	step [34/196], loss=58.5680
	step [35/196], loss=79.6385
	step [36/196], loss=72.2698
	step [37/196], loss=72.5471
	step [38/196], loss=78.4410
	step [39/196], loss=82.2255
	step [40/196], loss=79.6185
	step [41/196], loss=76.9040
	step [42/196], loss=80.8709
	step [43/196], loss=58.5412
	step [44/196], loss=58.7157
	step [45/196], loss=83.2583
	step [46/196], loss=83.0034
	step [47/196], loss=99.3848
	step [48/196], loss=71.4334
	step [49/196], loss=92.2954
	step [50/196], loss=90.4986
	step [51/196], loss=94.2698
	step [52/196], loss=101.4771
	step [53/196], loss=86.0527
	step [54/196], loss=70.4031
	step [55/196], loss=76.0047
	step [56/196], loss=65.1000
	step [57/196], loss=65.8945
	step [58/196], loss=79.0464
	step [59/196], loss=85.0728
	step [60/196], loss=81.4356
	step [61/196], loss=90.8211
	step [62/196], loss=88.3072
	step [63/196], loss=91.0191
	step [64/196], loss=81.3375
	step [65/196], loss=70.6747
	step [66/196], loss=77.4707
	step [67/196], loss=82.1071
	step [68/196], loss=80.1026
	step [69/196], loss=90.0187
	step [70/196], loss=66.7298
	step [71/196], loss=90.1933
	step [72/196], loss=70.7806
	step [73/196], loss=89.1090
	step [74/196], loss=61.8388
	step [75/196], loss=71.8751
	step [76/196], loss=73.7758
	step [77/196], loss=75.8289
	step [78/196], loss=82.2117
	step [79/196], loss=58.4209
	step [80/196], loss=87.1163
	step [81/196], loss=84.7687
	step [82/196], loss=72.2038
	step [83/196], loss=74.2690
	step [84/196], loss=72.3778
	step [85/196], loss=74.8507
	step [86/196], loss=73.7305
	step [87/196], loss=81.3474
	step [88/196], loss=72.8980
	step [89/196], loss=63.8941
	step [90/196], loss=76.0142
	step [91/196], loss=69.4007
	step [92/196], loss=79.3721
	step [93/196], loss=73.9528
	step [94/196], loss=74.8893
	step [95/196], loss=84.0055
	step [96/196], loss=89.7769
	step [97/196], loss=77.0222
	step [98/196], loss=74.2409
	step [99/196], loss=75.9043
	step [100/196], loss=62.2652
	step [101/196], loss=84.0070
	step [102/196], loss=63.6443
	step [103/196], loss=80.7976
	step [104/196], loss=80.7679
	step [105/196], loss=68.8986
	step [106/196], loss=69.7968
	step [107/196], loss=79.6400
	step [108/196], loss=86.6325
	step [109/196], loss=59.7186
	step [110/196], loss=67.1061
	step [111/196], loss=87.3693
	step [112/196], loss=67.2097
	step [113/196], loss=80.6587
	step [114/196], loss=67.7466
	step [115/196], loss=73.7591
	step [116/196], loss=75.5171
	step [117/196], loss=94.2672
	step [118/196], loss=65.6007
	step [119/196], loss=92.6786
	step [120/196], loss=76.4773
	step [121/196], loss=69.1430
	step [122/196], loss=81.0399
	step [123/196], loss=79.5949
	step [124/196], loss=68.9448
	step [125/196], loss=81.4266
	step [126/196], loss=79.4375
	step [127/196], loss=69.1638
	step [128/196], loss=76.4906
	step [129/196], loss=58.6734
	step [130/196], loss=72.8574
	step [131/196], loss=83.4918
	step [132/196], loss=74.8494
	step [133/196], loss=74.4217
	step [134/196], loss=78.3333
	step [135/196], loss=73.4979
	step [136/196], loss=68.6900
	step [137/196], loss=75.8741
	step [138/196], loss=56.2406
	step [139/196], loss=77.2555
	step [140/196], loss=65.5113
	step [141/196], loss=59.1826
	step [142/196], loss=88.8123
	step [143/196], loss=63.4994
	step [144/196], loss=70.0494
	step [145/196], loss=76.0034
	step [146/196], loss=92.7353
	step [147/196], loss=81.8259
	step [148/196], loss=89.0685
	step [149/196], loss=61.3091
	step [150/196], loss=70.0031
	step [151/196], loss=69.3748
	step [152/196], loss=84.5707
	step [153/196], loss=64.0802
	step [154/196], loss=81.6671
	step [155/196], loss=83.0585
	step [156/196], loss=74.2693
	step [157/196], loss=75.4473
	step [158/196], loss=72.0462
	step [159/196], loss=85.2068
	step [160/196], loss=61.7923
	step [161/196], loss=74.6586
	step [162/196], loss=87.6652
	step [163/196], loss=78.0889
	step [164/196], loss=75.0908
	step [165/196], loss=86.2168
	step [166/196], loss=71.6640
	step [167/196], loss=70.8178
	step [168/196], loss=86.8426
	step [169/196], loss=86.2004
	step [170/196], loss=66.4143
	step [171/196], loss=79.3169
	step [172/196], loss=82.8511
	step [173/196], loss=80.7596
	step [174/196], loss=74.6337
	step [175/196], loss=76.0960
	step [176/196], loss=71.3316
	step [177/196], loss=67.9801
	step [178/196], loss=93.7650
	step [179/196], loss=67.8446
	step [180/196], loss=73.2701
	step [181/196], loss=84.9991
	step [182/196], loss=70.5005
	step [183/196], loss=69.6344
	step [184/196], loss=75.5289
	step [185/196], loss=67.3045
	step [186/196], loss=81.6681
	step [187/196], loss=75.4881
	step [188/196], loss=75.8286
	step [189/196], loss=58.2333
	step [190/196], loss=67.0442
	step [191/196], loss=93.4109
	step [192/196], loss=74.4740
	step [193/196], loss=78.2999
	step [194/196], loss=81.4420
	step [195/196], loss=77.9471
	step [196/196], loss=15.0287
	Evaluating
	loss=0.0125, precision=0.3987, recall=0.9360, f1=0.5592
Training epoch 24
	step [1/196], loss=79.0280
	step [2/196], loss=83.4558
	step [3/196], loss=80.1682
	step [4/196], loss=80.4803
	step [5/196], loss=86.8846
	step [6/196], loss=87.6501
	step [7/196], loss=87.7274
	step [8/196], loss=76.7397
	step [9/196], loss=74.9737
	step [10/196], loss=74.6370
	step [11/196], loss=73.4553
	step [12/196], loss=79.6096
	step [13/196], loss=83.5985
	step [14/196], loss=57.1154
	step [15/196], loss=67.8504
	step [16/196], loss=78.4875
	step [17/196], loss=62.9883
	step [18/196], loss=87.2204
	step [19/196], loss=71.8788
	step [20/196], loss=74.6427
	step [21/196], loss=102.4562
	step [22/196], loss=71.2474
	step [23/196], loss=86.9633
	step [24/196], loss=79.3543
	step [25/196], loss=77.0557
	step [26/196], loss=83.6085
	step [27/196], loss=64.6501
	step [28/196], loss=75.6625
	step [29/196], loss=94.1019
	step [30/196], loss=58.5339
	step [31/196], loss=88.7381
	step [32/196], loss=55.6834
	step [33/196], loss=75.4952
	step [34/196], loss=73.0330
	step [35/196], loss=85.2753
	step [36/196], loss=62.4307
	step [37/196], loss=61.6814
	step [38/196], loss=71.1748
	step [39/196], loss=69.1450
	step [40/196], loss=69.2577
	step [41/196], loss=85.8058
	step [42/196], loss=82.6798
	step [43/196], loss=76.8503
	step [44/196], loss=68.6931
	step [45/196], loss=65.9360
	step [46/196], loss=58.7904
	step [47/196], loss=70.5861
	step [48/196], loss=92.0654
	step [49/196], loss=81.4555
	step [50/196], loss=88.2887
	step [51/196], loss=72.4815
	step [52/196], loss=70.3996
	step [53/196], loss=66.7821
	step [54/196], loss=60.3686
	step [55/196], loss=96.0275
	step [56/196], loss=64.0542
	step [57/196], loss=78.4632
	step [58/196], loss=77.5168
	step [59/196], loss=79.2598
	step [60/196], loss=90.9826
	step [61/196], loss=53.9497
	step [62/196], loss=91.1129
	step [63/196], loss=72.0354
	step [64/196], loss=84.3559
	step [65/196], loss=77.3822
	step [66/196], loss=64.2353
	step [67/196], loss=75.5673
	step [68/196], loss=87.7173
	step [69/196], loss=62.2054
	step [70/196], loss=71.0540
	step [71/196], loss=74.4137
	step [72/196], loss=77.3565
	step [73/196], loss=85.3544
	step [74/196], loss=65.6648
	step [75/196], loss=68.6991
	step [76/196], loss=69.0042
	step [77/196], loss=74.1784
	step [78/196], loss=80.7436
	step [79/196], loss=77.2010
	step [80/196], loss=72.2353
	step [81/196], loss=89.2599
	step [82/196], loss=75.8985
	step [83/196], loss=80.0039
	step [84/196], loss=76.7008
	step [85/196], loss=66.0286
	step [86/196], loss=84.2321
	step [87/196], loss=65.2638
	step [88/196], loss=74.2926
	step [89/196], loss=75.1922
	step [90/196], loss=78.0765
	step [91/196], loss=68.0840
	step [92/196], loss=81.3895
	step [93/196], loss=74.2114
	step [94/196], loss=79.0416
	step [95/196], loss=67.6476
	step [96/196], loss=77.3373
	step [97/196], loss=81.6897
	step [98/196], loss=88.0673
	step [99/196], loss=81.3381
	step [100/196], loss=65.2886
	step [101/196], loss=73.3009
	step [102/196], loss=71.0044
	step [103/196], loss=67.9020
	step [104/196], loss=75.2425
	step [105/196], loss=53.9904
	step [106/196], loss=75.7679
	step [107/196], loss=67.4194
	step [108/196], loss=74.5990
	step [109/196], loss=67.2376
	step [110/196], loss=83.0423
	step [111/196], loss=81.8339
	step [112/196], loss=78.4209
	step [113/196], loss=65.9109
	step [114/196], loss=72.7403
	step [115/196], loss=79.6837
	step [116/196], loss=83.7790
	step [117/196], loss=77.3406
	step [118/196], loss=71.5286
	step [119/196], loss=92.1027
	step [120/196], loss=76.0921
	step [121/196], loss=74.7207
	step [122/196], loss=79.6397
	step [123/196], loss=68.7929
	step [124/196], loss=82.6735
	step [125/196], loss=74.9589
	step [126/196], loss=76.7862
	step [127/196], loss=65.0931
	step [128/196], loss=69.1453
	step [129/196], loss=68.0567
	step [130/196], loss=75.6309
	step [131/196], loss=79.6105
	step [132/196], loss=85.5957
	step [133/196], loss=65.5453
	step [134/196], loss=72.2698
	step [135/196], loss=74.8169
	step [136/196], loss=71.3986
	step [137/196], loss=75.6916
	step [138/196], loss=68.2454
	step [139/196], loss=78.4796
	step [140/196], loss=70.6165
	step [141/196], loss=75.9901
	step [142/196], loss=82.5223
	step [143/196], loss=70.7824
	step [144/196], loss=73.7776
	step [145/196], loss=71.2999
	step [146/196], loss=81.9676
	step [147/196], loss=77.3979
	step [148/196], loss=63.2795
	step [149/196], loss=68.8478
	step [150/196], loss=67.6100
	step [151/196], loss=55.3308
	step [152/196], loss=78.0610
	step [153/196], loss=66.3923
	step [154/196], loss=85.5220
	step [155/196], loss=84.6614
	step [156/196], loss=58.3922
	step [157/196], loss=70.4595
	step [158/196], loss=68.5555
	step [159/196], loss=74.1230
	step [160/196], loss=71.6391
	step [161/196], loss=76.3366
	step [162/196], loss=73.7008
	step [163/196], loss=65.5409
	step [164/196], loss=68.8252
	step [165/196], loss=69.6810
	step [166/196], loss=91.5231
	step [167/196], loss=70.0098
	step [168/196], loss=93.3524
	step [169/196], loss=62.0629
	step [170/196], loss=64.8481
	step [171/196], loss=101.9418
	step [172/196], loss=85.3003
	step [173/196], loss=82.2644
	step [174/196], loss=85.8105
	step [175/196], loss=69.4566
	step [176/196], loss=67.0095
	step [177/196], loss=74.1491
	step [178/196], loss=66.1720
	step [179/196], loss=61.6156
	step [180/196], loss=90.7257
	step [181/196], loss=71.0364
	step [182/196], loss=81.8893
	step [183/196], loss=63.8029
	step [184/196], loss=74.2128
	step [185/196], loss=71.4360
	step [186/196], loss=65.6398
	step [187/196], loss=71.9739
	step [188/196], loss=77.8591
	step [189/196], loss=97.1398
	step [190/196], loss=80.3584
	step [191/196], loss=88.1119
	step [192/196], loss=62.3577
	step [193/196], loss=55.8382
	step [194/196], loss=72.0124
	step [195/196], loss=79.5119
	step [196/196], loss=20.0335
	Evaluating
	loss=0.0136, precision=0.3674, recall=0.8951, f1=0.5210
Training epoch 25
	step [1/196], loss=72.7266
	step [2/196], loss=75.3649
	step [3/196], loss=74.1813
	step [4/196], loss=67.9283
	step [5/196], loss=78.3035
	step [6/196], loss=70.8531
	step [7/196], loss=90.6125
	step [8/196], loss=79.9179
	step [9/196], loss=90.9680
	step [10/196], loss=63.5681
	step [11/196], loss=66.8008
	step [12/196], loss=86.8331
	step [13/196], loss=69.1564
	step [14/196], loss=90.7199
	step [15/196], loss=60.8923
	step [16/196], loss=61.7664
	step [17/196], loss=82.7551
	step [18/196], loss=64.6670
	step [19/196], loss=69.7640
	step [20/196], loss=76.9818
	step [21/196], loss=75.1928
	step [22/196], loss=69.4162
	step [23/196], loss=73.5064
	step [24/196], loss=80.4046
	step [25/196], loss=65.4380
	step [26/196], loss=82.3418
	step [27/196], loss=70.1232
	step [28/196], loss=91.5059
	step [29/196], loss=72.4243
	step [30/196], loss=70.1777
	step [31/196], loss=65.6862
	step [32/196], loss=68.7116
	step [33/196], loss=70.2590
	step [34/196], loss=72.7888
	step [35/196], loss=78.5283
	step [36/196], loss=77.5970
	step [37/196], loss=84.7134
	step [38/196], loss=77.9052
	step [39/196], loss=73.9515
	step [40/196], loss=61.3343
	step [41/196], loss=70.6591
	step [42/196], loss=66.2197
	step [43/196], loss=74.1170
	step [44/196], loss=79.5070
	step [45/196], loss=87.5207
	step [46/196], loss=64.9763
	step [47/196], loss=76.2086
	step [48/196], loss=85.0553
	step [49/196], loss=69.7712
	step [50/196], loss=60.7274
	step [51/196], loss=65.4367
	step [52/196], loss=70.4707
	step [53/196], loss=83.0835
	step [54/196], loss=73.9184
	step [55/196], loss=68.5008
	step [56/196], loss=67.1760
	step [57/196], loss=74.9182
	step [58/196], loss=65.3774
	step [59/196], loss=70.9376
	step [60/196], loss=72.1563
	step [61/196], loss=73.9546
	step [62/196], loss=75.3260
	step [63/196], loss=67.5766
	step [64/196], loss=75.5536
	step [65/196], loss=61.6764
	step [66/196], loss=67.3991
	step [67/196], loss=78.7451
	step [68/196], loss=100.3667
	step [69/196], loss=67.8389
	step [70/196], loss=76.2640
	step [71/196], loss=66.1944
	step [72/196], loss=57.8812
	step [73/196], loss=72.0512
	step [74/196], loss=62.3311
	step [75/196], loss=71.1870
	step [76/196], loss=78.0464
	step [77/196], loss=82.0046
	step [78/196], loss=84.6571
	step [79/196], loss=76.2341
	step [80/196], loss=59.0834
	step [81/196], loss=70.9864
	step [82/196], loss=81.2041
	step [83/196], loss=59.3910
	step [84/196], loss=63.2983
	step [85/196], loss=79.4579
	step [86/196], loss=95.3918
	step [87/196], loss=82.4056
	step [88/196], loss=71.8945
	step [89/196], loss=89.5354
	step [90/196], loss=87.0806
	step [91/196], loss=77.4283
	step [92/196], loss=54.1055
	step [93/196], loss=81.3126
	step [94/196], loss=78.0454
	step [95/196], loss=87.0350
	step [96/196], loss=71.6356
	step [97/196], loss=82.6949
	step [98/196], loss=76.0624
	step [99/196], loss=76.6454
	step [100/196], loss=78.0173
	step [101/196], loss=75.5347
	step [102/196], loss=71.6764
	step [103/196], loss=76.0618
	step [104/196], loss=80.9839
	step [105/196], loss=72.0670
	step [106/196], loss=68.2604
	step [107/196], loss=65.1704
	step [108/196], loss=67.4631
	step [109/196], loss=70.8726
	step [110/196], loss=83.4691
	step [111/196], loss=75.5852
	step [112/196], loss=65.4686
	step [113/196], loss=68.2278
	step [114/196], loss=64.6387
	step [115/196], loss=65.1110
	step [116/196], loss=70.6664
	step [117/196], loss=75.5927
	step [118/196], loss=94.9273
	step [119/196], loss=75.0124
	step [120/196], loss=48.2778
	step [121/196], loss=89.4057
	step [122/196], loss=70.6485
	step [123/196], loss=71.0068
	step [124/196], loss=85.0551
	step [125/196], loss=84.2187
	step [126/196], loss=70.5066
	step [127/196], loss=69.5591
	step [128/196], loss=65.8126
	step [129/196], loss=69.3340
	step [130/196], loss=79.6492
	step [131/196], loss=76.8850
	step [132/196], loss=71.1619
	step [133/196], loss=77.2226
	step [134/196], loss=89.9789
	step [135/196], loss=57.4255
	step [136/196], loss=75.7016
	step [137/196], loss=85.5430
	step [138/196], loss=68.1489
	step [139/196], loss=85.2834
	step [140/196], loss=75.8347
	step [141/196], loss=82.4472
	step [142/196], loss=81.4620
	step [143/196], loss=79.5309
	step [144/196], loss=73.1729
	step [145/196], loss=71.4036
	step [146/196], loss=75.8218
	step [147/196], loss=77.8808
	step [148/196], loss=75.0946
	step [149/196], loss=89.3620
	step [150/196], loss=83.3264
	step [151/196], loss=69.5860
	step [152/196], loss=74.3313
	step [153/196], loss=87.1105
	step [154/196], loss=87.5148
	step [155/196], loss=70.2491
	step [156/196], loss=82.0183
	step [157/196], loss=63.2491
	step [158/196], loss=81.5260
	step [159/196], loss=69.5386
	step [160/196], loss=52.2440
	step [161/196], loss=67.7782
	step [162/196], loss=65.5183
	step [163/196], loss=75.4397
	step [164/196], loss=75.8060
	step [165/196], loss=72.2786
	step [166/196], loss=75.6849
	step [167/196], loss=69.8152
	step [168/196], loss=63.9930
	step [169/196], loss=73.0084
	step [170/196], loss=67.2043
	step [171/196], loss=48.4398
	step [172/196], loss=77.2699
	step [173/196], loss=81.3373
	step [174/196], loss=100.2260
	step [175/196], loss=84.2031
	step [176/196], loss=70.4841
	step [177/196], loss=85.4082
	step [178/196], loss=66.0420
	step [179/196], loss=65.6973
	step [180/196], loss=67.7617
	step [181/196], loss=94.1619
	step [182/196], loss=68.3121
	step [183/196], loss=79.6254
	step [184/196], loss=76.2933
	step [185/196], loss=83.7161
	step [186/196], loss=83.8402
	step [187/196], loss=68.3019
	step [188/196], loss=76.7525
	step [189/196], loss=76.3452
	step [190/196], loss=67.1752
	step [191/196], loss=78.0304
	step [192/196], loss=90.0195
	step [193/196], loss=65.3519
	step [194/196], loss=84.8446
	step [195/196], loss=84.8589
	step [196/196], loss=22.5945
	Evaluating
	loss=0.0175, precision=0.2949, recall=0.9130, f1=0.4458
Training epoch 26
	step [1/196], loss=73.3448
	step [2/196], loss=80.4457
	step [3/196], loss=62.1493
	step [4/196], loss=73.6661
	step [5/196], loss=71.0095
	step [6/196], loss=74.1633
	step [7/196], loss=66.4816
	step [8/196], loss=78.0640
	step [9/196], loss=71.0269
	step [10/196], loss=79.4037
	step [11/196], loss=72.8166
	step [12/196], loss=67.6233
	step [13/196], loss=93.4730
	step [14/196], loss=79.1227
	step [15/196], loss=56.4031
	step [16/196], loss=80.5710
	step [17/196], loss=69.9189
	step [18/196], loss=65.2021
	step [19/196], loss=62.7560
	step [20/196], loss=77.5630
	step [21/196], loss=81.4304
	step [22/196], loss=76.9380
	step [23/196], loss=58.0548
	step [24/196], loss=85.1918
	step [25/196], loss=71.8929
	step [26/196], loss=59.1487
	step [27/196], loss=69.6633
	step [28/196], loss=68.2193
	step [29/196], loss=77.0129
	step [30/196], loss=82.1970
	step [31/196], loss=82.0752
	step [32/196], loss=80.8807
	step [33/196], loss=72.8408
	step [34/196], loss=84.3849
	step [35/196], loss=75.7753
	step [36/196], loss=55.2584
	step [37/196], loss=56.9858
	step [38/196], loss=66.2419
	step [39/196], loss=75.2921
	step [40/196], loss=64.8343
	step [41/196], loss=66.8572
	step [42/196], loss=78.7289
	step [43/196], loss=59.1443
	step [44/196], loss=83.3562
	step [45/196], loss=69.2577
	step [46/196], loss=75.7827
	step [47/196], loss=71.1846
	step [48/196], loss=71.8517
	step [49/196], loss=84.3107
	step [50/196], loss=76.0460
	step [51/196], loss=63.6508
	step [52/196], loss=70.0763
	step [53/196], loss=79.3982
	step [54/196], loss=77.6602
	step [55/196], loss=77.9642
	step [56/196], loss=67.8535
	step [57/196], loss=67.5854
	step [58/196], loss=78.5117
	step [59/196], loss=66.5169
	step [60/196], loss=70.1287
	step [61/196], loss=86.7281
	step [62/196], loss=80.3246
	step [63/196], loss=67.3796
	step [64/196], loss=74.4641
	step [65/196], loss=74.3470
	step [66/196], loss=75.8824
	step [67/196], loss=67.4161
	step [68/196], loss=71.5170
	step [69/196], loss=78.8595
	step [70/196], loss=70.9854
	step [71/196], loss=64.1577
	step [72/196], loss=67.1868
	step [73/196], loss=57.2683
	step [74/196], loss=85.3714
	step [75/196], loss=78.3109
	step [76/196], loss=70.9208
	step [77/196], loss=85.5257
	step [78/196], loss=68.1043
	step [79/196], loss=78.2405
	step [80/196], loss=82.2357
	step [81/196], loss=86.4815
	step [82/196], loss=66.2339
	step [83/196], loss=74.8991
	step [84/196], loss=83.7633
	step [85/196], loss=76.8606
	step [86/196], loss=74.3745
	step [87/196], loss=58.1175
	step [88/196], loss=72.1939
	step [89/196], loss=73.3461
	step [90/196], loss=58.6628
	step [91/196], loss=83.4524
	step [92/196], loss=74.8311
	step [93/196], loss=70.1469
	step [94/196], loss=80.7920
	step [95/196], loss=73.5340
	step [96/196], loss=79.5097
	step [97/196], loss=79.6704
	step [98/196], loss=87.8360
	step [99/196], loss=85.5837
	step [100/196], loss=81.7704
	step [101/196], loss=81.3781
	step [102/196], loss=77.3448
	step [103/196], loss=76.3406
	step [104/196], loss=84.1213
	step [105/196], loss=71.5284
	step [106/196], loss=81.3503
	step [107/196], loss=72.0970
	step [108/196], loss=71.4476
	step [109/196], loss=60.6055
	step [110/196], loss=76.9175
	step [111/196], loss=74.7504
	step [112/196], loss=75.7891
	step [113/196], loss=83.3675
	step [114/196], loss=79.0282
	step [115/196], loss=70.6012
	step [116/196], loss=72.2920
	step [117/196], loss=78.5203
	step [118/196], loss=83.0544
	step [119/196], loss=88.2609
	step [120/196], loss=69.2614
	step [121/196], loss=80.9247
	step [122/196], loss=71.6329
	step [123/196], loss=75.8825
	step [124/196], loss=68.3203
	step [125/196], loss=74.9198
	step [126/196], loss=91.3142
	step [127/196], loss=62.5034
	step [128/196], loss=82.6254
	step [129/196], loss=82.0511
	step [130/196], loss=85.2141
	step [131/196], loss=68.6410
	step [132/196], loss=68.0098
	step [133/196], loss=66.0216
	step [134/196], loss=76.4268
	step [135/196], loss=68.4821
	step [136/196], loss=82.2867
	step [137/196], loss=50.2362
	step [138/196], loss=64.3344
	step [139/196], loss=75.1253
	step [140/196], loss=77.0462
	step [141/196], loss=77.4694
	step [142/196], loss=55.1768
	step [143/196], loss=68.5887
	step [144/196], loss=71.0107
	step [145/196], loss=75.2086
	step [146/196], loss=63.2764
	step [147/196], loss=70.4386
	step [148/196], loss=70.1608
	step [149/196], loss=79.1628
	step [150/196], loss=72.6860
	step [151/196], loss=80.0694
	step [152/196], loss=69.0219
	step [153/196], loss=66.2979
	step [154/196], loss=78.9938
	step [155/196], loss=75.7051
	step [156/196], loss=66.3411
	step [157/196], loss=75.2451
	step [158/196], loss=73.0202
	step [159/196], loss=70.0714
	step [160/196], loss=83.0863
	step [161/196], loss=65.0427
	step [162/196], loss=71.0986
	step [163/196], loss=69.3893
	step [164/196], loss=73.9841
	step [165/196], loss=68.5455
	step [166/196], loss=74.5067
	step [167/196], loss=67.4134
	step [168/196], loss=70.5287
	step [169/196], loss=63.2078
	step [170/196], loss=77.6929
	step [171/196], loss=51.4984
	step [172/196], loss=74.5676
	step [173/196], loss=81.0127
	step [174/196], loss=82.4919
	step [175/196], loss=58.0989
	step [176/196], loss=60.6968
	step [177/196], loss=79.7925
	step [178/196], loss=84.6812
	step [179/196], loss=92.3485
	step [180/196], loss=91.6140
	step [181/196], loss=74.9117
	step [182/196], loss=63.7808
	step [183/196], loss=78.0632
	step [184/196], loss=68.8218
	step [185/196], loss=91.2978
	step [186/196], loss=69.9617
	step [187/196], loss=66.7835
	step [188/196], loss=74.0971
	step [189/196], loss=92.7384
	step [190/196], loss=66.4192
	step [191/196], loss=61.3604
	step [192/196], loss=85.1162
	step [193/196], loss=69.2872
	step [194/196], loss=61.1533
	step [195/196], loss=91.7412
	step [196/196], loss=23.4116
	Evaluating
	loss=0.0108, precision=0.4469, recall=0.8942, f1=0.5960
Training epoch 27
	step [1/196], loss=68.3003
	step [2/196], loss=84.4216
	step [3/196], loss=63.9058
	step [4/196], loss=62.8901
	step [5/196], loss=67.9577
	step [6/196], loss=79.4973
	step [7/196], loss=92.1141
	step [8/196], loss=85.6088
	step [9/196], loss=70.2740
	step [10/196], loss=74.4455
	step [11/196], loss=73.9150
	step [12/196], loss=74.2309
	step [13/196], loss=75.3644
	step [14/196], loss=71.3315
	step [15/196], loss=70.1006
	step [16/196], loss=51.7357
	step [17/196], loss=74.2136
	step [18/196], loss=70.5362
	step [19/196], loss=67.2663
	step [20/196], loss=76.4852
	step [21/196], loss=75.8458
	step [22/196], loss=79.7896
	step [23/196], loss=80.3827
	step [24/196], loss=76.6241
	step [25/196], loss=75.2894
	step [26/196], loss=67.1689
	step [27/196], loss=73.8261
	step [28/196], loss=70.8577
	step [29/196], loss=82.4434
	step [30/196], loss=71.2612
	step [31/196], loss=54.5793
	step [32/196], loss=69.0570
	step [33/196], loss=68.3241
	step [34/196], loss=71.4351
	step [35/196], loss=75.8434
	step [36/196], loss=69.1578
	step [37/196], loss=62.1685
	step [38/196], loss=88.6107
	step [39/196], loss=71.8790
	step [40/196], loss=61.7241
	step [41/196], loss=68.1734
	step [42/196], loss=93.8932
	step [43/196], loss=67.3527
	step [44/196], loss=56.3233
	step [45/196], loss=75.5888
	step [46/196], loss=75.2727
	step [47/196], loss=81.1738
	step [48/196], loss=82.7117
	step [49/196], loss=74.5919
	step [50/196], loss=66.1208
	step [51/196], loss=79.3296
	step [52/196], loss=58.8622
	step [53/196], loss=94.5885
	step [54/196], loss=72.3383
	step [55/196], loss=78.6021
	step [56/196], loss=71.9183
	step [57/196], loss=65.9329
	step [58/196], loss=82.5171
	step [59/196], loss=79.0020
	step [60/196], loss=65.6430
	step [61/196], loss=76.4947
	step [62/196], loss=68.8270
	step [63/196], loss=94.3749
	step [64/196], loss=70.8463
	step [65/196], loss=83.1960
	step [66/196], loss=88.3297
	step [67/196], loss=64.2674
	step [68/196], loss=64.3151
	step [69/196], loss=68.4616
	step [70/196], loss=62.6225
	step [71/196], loss=72.4492
	step [72/196], loss=84.0213
	step [73/196], loss=60.1990
	step [74/196], loss=71.6307
	step [75/196], loss=67.3192
	step [76/196], loss=75.4810
	step [77/196], loss=77.8688
	step [78/196], loss=56.9361
	step [79/196], loss=72.1722
	step [80/196], loss=77.2357
	step [81/196], loss=58.2342
	step [82/196], loss=75.5124
	step [83/196], loss=71.7449
	step [84/196], loss=70.9012
	step [85/196], loss=80.6890
	step [86/196], loss=70.9778
	step [87/196], loss=75.1006
	step [88/196], loss=61.0652
	step [89/196], loss=77.1243
	step [90/196], loss=70.5910
	step [91/196], loss=71.7087
	step [92/196], loss=69.5054
	step [93/196], loss=74.4472
	step [94/196], loss=73.8837
	step [95/196], loss=81.0359
	step [96/196], loss=77.0558
	step [97/196], loss=70.2553
	step [98/196], loss=74.3733
	step [99/196], loss=80.6100
	step [100/196], loss=76.8773
	step [101/196], loss=70.8513
	step [102/196], loss=76.9737
	step [103/196], loss=63.9883
	step [104/196], loss=62.2059
	step [105/196], loss=76.2041
	step [106/196], loss=88.7186
	step [107/196], loss=83.7015
	step [108/196], loss=75.7080
	step [109/196], loss=75.3234
	step [110/196], loss=59.8604
	step [111/196], loss=64.0773
	step [112/196], loss=76.9693
	step [113/196], loss=68.5292
	step [114/196], loss=57.0432
	step [115/196], loss=67.8146
	step [116/196], loss=79.4897
	step [117/196], loss=78.9914
	step [118/196], loss=84.8377
	step [119/196], loss=64.7764
	step [120/196], loss=65.2735
	step [121/196], loss=81.7742
	step [122/196], loss=75.1186
	step [123/196], loss=59.4155
	step [124/196], loss=68.1329
	step [125/196], loss=89.4719
	step [126/196], loss=67.7949
	step [127/196], loss=62.8311
	step [128/196], loss=65.5056
	step [129/196], loss=75.9294
	step [130/196], loss=74.9822
	step [131/196], loss=60.5204
	step [132/196], loss=101.9413
	step [133/196], loss=84.7090
	step [134/196], loss=74.0044
	step [135/196], loss=62.0289
	step [136/196], loss=71.8062
	step [137/196], loss=71.8373
	step [138/196], loss=75.7005
	step [139/196], loss=78.1208
	step [140/196], loss=65.8570
	step [141/196], loss=61.3307
	step [142/196], loss=82.1533
	step [143/196], loss=76.3990
	step [144/196], loss=71.7588
	step [145/196], loss=64.1418
	step [146/196], loss=68.5499
	step [147/196], loss=63.1616
	step [148/196], loss=64.2562
	step [149/196], loss=70.9793
	step [150/196], loss=86.1344
	step [151/196], loss=68.0440
	step [152/196], loss=83.8672
	step [153/196], loss=77.4557
	step [154/196], loss=73.1291
	step [155/196], loss=72.8257
	step [156/196], loss=87.4748
	step [157/196], loss=73.0442
	step [158/196], loss=59.9973
	step [159/196], loss=79.7510
	step [160/196], loss=63.5957
	step [161/196], loss=74.5879
	step [162/196], loss=73.1726
	step [163/196], loss=62.4663
	step [164/196], loss=96.8587
	step [165/196], loss=70.0298
	step [166/196], loss=83.0866
	step [167/196], loss=89.1147
	step [168/196], loss=75.1920
	step [169/196], loss=69.0518
	step [170/196], loss=72.6677
	step [171/196], loss=91.1150
	step [172/196], loss=69.1061
	step [173/196], loss=61.8884
	step [174/196], loss=80.2921
	step [175/196], loss=66.3987
	step [176/196], loss=80.9305
	step [177/196], loss=68.7315
	step [178/196], loss=83.9100
	step [179/196], loss=71.9872
	step [180/196], loss=58.0927
	step [181/196], loss=75.8932
	step [182/196], loss=65.3263
	step [183/196], loss=56.1973
	step [184/196], loss=73.3715
	step [185/196], loss=90.0200
	step [186/196], loss=62.4083
	step [187/196], loss=65.8968
	step [188/196], loss=74.7344
	step [189/196], loss=86.8096
	step [190/196], loss=78.9976
	step [191/196], loss=71.2244
	step [192/196], loss=77.3266
	step [193/196], loss=61.3512
	step [194/196], loss=68.5307
	step [195/196], loss=63.2000
	step [196/196], loss=16.3808
	Evaluating
	loss=0.0104, precision=0.4339, recall=0.9091, f1=0.5875
Training epoch 28
	step [1/196], loss=58.4633
	step [2/196], loss=67.1550
	step [3/196], loss=66.7654
	step [4/196], loss=61.2150
	step [5/196], loss=67.2552
	step [6/196], loss=85.5025
	step [7/196], loss=68.1646
	step [8/196], loss=69.3408
	step [9/196], loss=80.7325
	step [10/196], loss=61.1948
	step [11/196], loss=61.8441
	step [12/196], loss=68.2404
	step [13/196], loss=69.6197
	step [14/196], loss=84.9804
	step [15/196], loss=76.9863
	step [16/196], loss=63.3682
	step [17/196], loss=77.9111
	step [18/196], loss=61.9308
	step [19/196], loss=68.7122
	step [20/196], loss=72.1373
	step [21/196], loss=63.2033
	step [22/196], loss=74.1407
	step [23/196], loss=82.8271
	step [24/196], loss=68.2191
	step [25/196], loss=74.4763
	step [26/196], loss=74.7780
	step [27/196], loss=80.1938
	step [28/196], loss=83.4058
	step [29/196], loss=59.3717
	step [30/196], loss=59.2082
	step [31/196], loss=86.5550
	step [32/196], loss=65.5156
	step [33/196], loss=69.7435
	step [34/196], loss=78.8839
	step [35/196], loss=61.6127
	step [36/196], loss=66.7548
	step [37/196], loss=70.8058
	step [38/196], loss=68.9297
	step [39/196], loss=62.8324
	step [40/196], loss=58.8144
	step [41/196], loss=73.7998
	step [42/196], loss=76.0666
	step [43/196], loss=66.7116
	step [44/196], loss=75.4051
	step [45/196], loss=77.5779
	step [46/196], loss=72.1415
	step [47/196], loss=63.4966
	step [48/196], loss=76.0984
	step [49/196], loss=94.4371
	step [50/196], loss=80.4633
	step [51/196], loss=79.7085
	step [52/196], loss=63.5189
	step [53/196], loss=65.6328
	step [54/196], loss=79.2934
	step [55/196], loss=77.6101
	step [56/196], loss=78.9789
	step [57/196], loss=68.0664
	step [58/196], loss=71.7115
	step [59/196], loss=59.2440
	step [60/196], loss=73.7281
	step [61/196], loss=83.0558
	step [62/196], loss=56.7928
	step [63/196], loss=70.9297
	step [64/196], loss=70.2530
	step [65/196], loss=77.5320
	step [66/196], loss=69.6244
	step [67/196], loss=73.9813
	step [68/196], loss=69.9344
	step [69/196], loss=61.9465
	step [70/196], loss=93.6200
	step [71/196], loss=90.5458
	step [72/196], loss=66.6801
	step [73/196], loss=64.9942
	step [74/196], loss=79.4987
	step [75/196], loss=78.2142
	step [76/196], loss=70.9323
	step [77/196], loss=61.9829
	step [78/196], loss=85.9190
	step [79/196], loss=82.4546
	step [80/196], loss=73.9140
	step [81/196], loss=70.8391
	step [82/196], loss=82.5402
	step [83/196], loss=55.0101
	step [84/196], loss=74.0065
	step [85/196], loss=80.6368
	step [86/196], loss=57.9782
	step [87/196], loss=64.1062
	step [88/196], loss=64.9338
	step [89/196], loss=73.6271
	step [90/196], loss=80.0027
	step [91/196], loss=61.6347
	step [92/196], loss=92.3232
	step [93/196], loss=63.4706
	step [94/196], loss=68.6210
	step [95/196], loss=70.7467
	step [96/196], loss=77.6607
	step [97/196], loss=85.5753
	step [98/196], loss=83.8039
	step [99/196], loss=72.9760
	step [100/196], loss=75.4055
	step [101/196], loss=72.5323
	step [102/196], loss=72.1371
	step [103/196], loss=75.8599
	step [104/196], loss=70.7274
	step [105/196], loss=65.8573
	step [106/196], loss=65.9291
	step [107/196], loss=72.1770
	step [108/196], loss=91.2171
	step [109/196], loss=63.0313
	step [110/196], loss=82.7939
	step [111/196], loss=71.2994
	step [112/196], loss=66.5595
	step [113/196], loss=69.8800
	step [114/196], loss=58.0558
	step [115/196], loss=80.1232
	step [116/196], loss=85.2625
	step [117/196], loss=92.5154
	step [118/196], loss=63.2811
	step [119/196], loss=76.0448
	step [120/196], loss=87.0157
	step [121/196], loss=65.6845
	step [122/196], loss=74.7430
	step [123/196], loss=82.0054
	step [124/196], loss=69.1390
	step [125/196], loss=66.7772
	step [126/196], loss=70.8778
	step [127/196], loss=73.2990
	step [128/196], loss=67.2035
	step [129/196], loss=63.4129
	step [130/196], loss=87.1231
	step [131/196], loss=62.9711
	step [132/196], loss=69.4563
	step [133/196], loss=73.5253
	step [134/196], loss=86.6460
	step [135/196], loss=64.6275
	step [136/196], loss=79.2743
	step [137/196], loss=78.3689
	step [138/196], loss=67.5130
	step [139/196], loss=62.7169
	step [140/196], loss=82.2839
	step [141/196], loss=70.0287
	step [142/196], loss=64.3357
	step [143/196], loss=83.5694
	step [144/196], loss=75.9846
	step [145/196], loss=64.5569
	step [146/196], loss=54.8001
	step [147/196], loss=75.2643
	step [148/196], loss=84.1126
	step [149/196], loss=92.2813
	step [150/196], loss=56.2224
	step [151/196], loss=74.0125
	step [152/196], loss=61.9937
	step [153/196], loss=65.4139
	step [154/196], loss=70.5082
	step [155/196], loss=71.5026
	step [156/196], loss=76.0549
	step [157/196], loss=67.3872
	step [158/196], loss=76.2221
	step [159/196], loss=83.0115
	step [160/196], loss=63.7924
	step [161/196], loss=74.5153
	step [162/196], loss=82.1075
	step [163/196], loss=65.8653
	step [164/196], loss=82.3543
	step [165/196], loss=81.8978
	step [166/196], loss=71.2064
	step [167/196], loss=67.2840
	step [168/196], loss=76.8391
	step [169/196], loss=69.4543
	step [170/196], loss=68.3096
	step [171/196], loss=75.0269
	step [172/196], loss=62.8358
	step [173/196], loss=74.4375
	step [174/196], loss=76.3077
	step [175/196], loss=79.1694
	step [176/196], loss=89.6832
	step [177/196], loss=65.4902
	step [178/196], loss=66.9006
	step [179/196], loss=82.3652
	step [180/196], loss=63.0718
	step [181/196], loss=75.5875
	step [182/196], loss=82.4491
	step [183/196], loss=74.9383
	step [184/196], loss=72.0978
	step [185/196], loss=74.9882
	step [186/196], loss=59.9216
	step [187/196], loss=70.3513
	step [188/196], loss=69.7211
	step [189/196], loss=65.2932
	step [190/196], loss=73.3545
	step [191/196], loss=79.6095
	step [192/196], loss=64.7750
	step [193/196], loss=75.3601
	step [194/196], loss=59.2897
	step [195/196], loss=81.6476
	step [196/196], loss=18.0554
	Evaluating
	loss=0.0083, precision=0.5100, recall=0.9043, f1=0.6522
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/196], loss=71.4618
	step [2/196], loss=72.0596
	step [3/196], loss=70.8378
	step [4/196], loss=69.1253
	step [5/196], loss=56.5929
	step [6/196], loss=57.8769
	step [7/196], loss=91.2096
	step [8/196], loss=63.0249
	step [9/196], loss=72.7030
	step [10/196], loss=66.2845
	step [11/196], loss=84.0705
	step [12/196], loss=64.0172
	step [13/196], loss=84.6169
	step [14/196], loss=74.0780
	step [15/196], loss=75.1061
	step [16/196], loss=86.8990
	step [17/196], loss=77.2709
	step [18/196], loss=72.7400
	step [19/196], loss=63.7566
	step [20/196], loss=77.6797
	step [21/196], loss=79.4470
	step [22/196], loss=66.9284
	step [23/196], loss=63.2192
	step [24/196], loss=70.6833
	step [25/196], loss=70.6035
	step [26/196], loss=83.9134
	step [27/196], loss=92.5619
	step [28/196], loss=68.8354
	step [29/196], loss=69.6953
	step [30/196], loss=68.1194
	step [31/196], loss=60.7375
	step [32/196], loss=84.9494
	step [33/196], loss=69.8270
	step [34/196], loss=98.1113
	step [35/196], loss=52.8436
	step [36/196], loss=77.5927
	step [37/196], loss=69.2038
	step [38/196], loss=75.4536
	step [39/196], loss=76.5999
	step [40/196], loss=72.1800
	step [41/196], loss=90.3186
	step [42/196], loss=73.8702
	step [43/196], loss=64.5462
	step [44/196], loss=76.2914
	step [45/196], loss=68.7146
	step [46/196], loss=60.9347
	step [47/196], loss=76.3427
	step [48/196], loss=68.0754
	step [49/196], loss=83.7536
	step [50/196], loss=80.3750
	step [51/196], loss=50.6405
	step [52/196], loss=93.6883
	step [53/196], loss=74.6673
	step [54/196], loss=75.6297
	step [55/196], loss=71.6726
	step [56/196], loss=53.2874
	step [57/196], loss=92.2867
	step [58/196], loss=63.3930
	step [59/196], loss=75.3712
	step [60/196], loss=63.1750
	step [61/196], loss=68.6686
	step [62/196], loss=66.9109
	step [63/196], loss=62.9731
	step [64/196], loss=80.2768
	step [65/196], loss=65.4507
	step [66/196], loss=65.9778
	step [67/196], loss=71.6894
	step [68/196], loss=69.7650
	step [69/196], loss=67.6102
	step [70/196], loss=70.7278
	step [71/196], loss=66.4322
	step [72/196], loss=69.7439
	step [73/196], loss=80.2104
	step [74/196], loss=69.6525
	step [75/196], loss=79.0524
	step [76/196], loss=73.5165
	step [77/196], loss=82.3483
	step [78/196], loss=73.3925
	step [79/196], loss=58.2309
	step [80/196], loss=86.9600
	step [81/196], loss=60.5172
	step [82/196], loss=76.2433
	step [83/196], loss=64.5844
	step [84/196], loss=54.8490
	step [85/196], loss=81.9875
	step [86/196], loss=71.3557
	step [87/196], loss=82.2597
	step [88/196], loss=70.0105
	step [89/196], loss=73.3183
	step [90/196], loss=63.9736
	step [91/196], loss=62.3879
	step [92/196], loss=76.4793
	step [93/196], loss=73.1026
	step [94/196], loss=57.0445
	step [95/196], loss=73.7980
	step [96/196], loss=80.8348
	step [97/196], loss=67.3140
	step [98/196], loss=73.4745
	step [99/196], loss=87.7874
	step [100/196], loss=74.4426
	step [101/196], loss=87.6124
	step [102/196], loss=67.7912
	step [103/196], loss=79.1292
	step [104/196], loss=68.9260
	step [105/196], loss=59.1490
	step [106/196], loss=70.5265
	step [107/196], loss=71.8438
	step [108/196], loss=67.0798
	step [109/196], loss=72.3439
	step [110/196], loss=80.7492
	step [111/196], loss=77.5047
	step [112/196], loss=52.4635
	step [113/196], loss=76.0877
	step [114/196], loss=68.1219
	step [115/196], loss=78.1337
	step [116/196], loss=69.4728
	step [117/196], loss=69.8467
	step [118/196], loss=73.4448
	step [119/196], loss=71.3453
	step [120/196], loss=70.9243
	step [121/196], loss=73.6442
	step [122/196], loss=63.0165
	step [123/196], loss=57.8754
	step [124/196], loss=64.3289
	step [125/196], loss=67.3733
	step [126/196], loss=81.5582
	step [127/196], loss=72.7925
	step [128/196], loss=72.0841
	step [129/196], loss=82.9261
	step [130/196], loss=72.9772
	step [131/196], loss=68.9517
	step [132/196], loss=68.5135
	step [133/196], loss=73.7707
	step [134/196], loss=65.2293
	step [135/196], loss=55.7805
	step [136/196], loss=80.7374
	step [137/196], loss=68.8021
	step [138/196], loss=75.4527
	step [139/196], loss=73.7468
	step [140/196], loss=67.7481
	step [141/196], loss=63.4378
	step [142/196], loss=69.8952
	step [143/196], loss=85.7559
	step [144/196], loss=83.1045
	step [145/196], loss=65.9061
	step [146/196], loss=60.5400
	step [147/196], loss=74.3002
	step [148/196], loss=81.3601
	step [149/196], loss=80.2635
	step [150/196], loss=79.1740
	step [151/196], loss=67.5436
	step [152/196], loss=74.3949
	step [153/196], loss=79.2537
	step [154/196], loss=65.5608
	step [155/196], loss=55.0130
	step [156/196], loss=70.6108
	step [157/196], loss=62.9491
	step [158/196], loss=66.6215
	step [159/196], loss=62.3720
	step [160/196], loss=65.0846
	step [161/196], loss=79.1985
	step [162/196], loss=85.2102
	step [163/196], loss=73.1016
	step [164/196], loss=73.7162
	step [165/196], loss=64.3082
	step [166/196], loss=72.8550
	step [167/196], loss=66.0012
	step [168/196], loss=68.0138
	step [169/196], loss=69.8121
	step [170/196], loss=75.2910
	step [171/196], loss=63.7027
	step [172/196], loss=82.1343
	step [173/196], loss=66.6923
	step [174/196], loss=57.0679
	step [175/196], loss=79.8342
	step [176/196], loss=80.8299
	step [177/196], loss=83.8293
	step [178/196], loss=73.9074
	step [179/196], loss=68.2365
	step [180/196], loss=76.4321
	step [181/196], loss=63.0589
	step [182/196], loss=52.7977
	step [183/196], loss=72.7569
	step [184/196], loss=73.2740
	step [185/196], loss=66.6744
	step [186/196], loss=72.3716
	step [187/196], loss=66.2675
	step [188/196], loss=63.0997
	step [189/196], loss=59.9612
	step [190/196], loss=82.3560
	step [191/196], loss=75.0195
	step [192/196], loss=62.8902
	step [193/196], loss=70.6367
	step [194/196], loss=63.5005
	step [195/196], loss=65.6110
	step [196/196], loss=14.2736
	Evaluating
	loss=0.0098, precision=0.4499, recall=0.9225, f1=0.6048
Training epoch 30
	step [1/196], loss=76.2994
	step [2/196], loss=64.1856
	step [3/196], loss=62.3532
	step [4/196], loss=75.8160
	step [5/196], loss=69.4797
	step [6/196], loss=74.7939
	step [7/196], loss=77.5417
	step [8/196], loss=66.8834
	step [9/196], loss=57.5637
	step [10/196], loss=61.7033
	step [11/196], loss=66.6170
	step [12/196], loss=75.8038
	step [13/196], loss=68.6007
	step [14/196], loss=69.8611
	step [15/196], loss=76.9796
	step [16/196], loss=68.0354
	step [17/196], loss=76.6972
	step [18/196], loss=56.3941
	step [19/196], loss=73.8859
	step [20/196], loss=85.0606
	step [21/196], loss=61.9684
	step [22/196], loss=61.8861
	step [23/196], loss=71.0290
	step [24/196], loss=67.3124
	step [25/196], loss=71.6817
	step [26/196], loss=66.5983
	step [27/196], loss=67.7238
	step [28/196], loss=96.5701
	step [29/196], loss=55.4576
	step [30/196], loss=67.1351
	step [31/196], loss=77.6096
	step [32/196], loss=58.8867
	step [33/196], loss=67.8232
	step [34/196], loss=75.8189
	step [35/196], loss=65.1393
	step [36/196], loss=79.5067
	step [37/196], loss=60.8741
	step [38/196], loss=69.9434
	step [39/196], loss=70.1803
	step [40/196], loss=49.7675
	step [41/196], loss=66.3436
	step [42/196], loss=71.9283
	step [43/196], loss=77.3457
	step [44/196], loss=84.8071
	step [45/196], loss=60.2177
	step [46/196], loss=72.7194
	step [47/196], loss=58.5636
	step [48/196], loss=83.1417
	step [49/196], loss=93.6376
	step [50/196], loss=69.5048
	step [51/196], loss=78.7736
	step [52/196], loss=69.9944
	step [53/196], loss=67.7337
	step [54/196], loss=86.4531
	step [55/196], loss=61.5276
	step [56/196], loss=88.2095
	step [57/196], loss=70.1914
	step [58/196], loss=54.1518
	step [59/196], loss=57.7561
	step [60/196], loss=64.0541
	step [61/196], loss=75.9212
	step [62/196], loss=66.2512
	step [63/196], loss=68.9839
	step [64/196], loss=77.4484
	step [65/196], loss=63.6575
	step [66/196], loss=60.5490
	step [67/196], loss=73.5004
	step [68/196], loss=71.1500
	step [69/196], loss=61.9409
	step [70/196], loss=72.7431
	step [71/196], loss=67.9921
	step [72/196], loss=74.0997
	step [73/196], loss=72.5022
	step [74/196], loss=81.2942
	step [75/196], loss=59.9965
	step [76/196], loss=67.6269
	step [77/196], loss=71.1189
	step [78/196], loss=63.5888
	step [79/196], loss=71.7612
	step [80/196], loss=75.5990
	step [81/196], loss=65.8782
	step [82/196], loss=70.6772
	step [83/196], loss=60.3350
	step [84/196], loss=86.7935
	step [85/196], loss=63.4853
	step [86/196], loss=61.5913
	step [87/196], loss=61.9834
	step [88/196], loss=63.5041
	step [89/196], loss=69.2338
	step [90/196], loss=79.3697
	step [91/196], loss=63.4274
	step [92/196], loss=64.7781
	step [93/196], loss=97.0077
	step [94/196], loss=58.7912
	step [95/196], loss=67.0922
	step [96/196], loss=64.7823
	step [97/196], loss=72.8708
	step [98/196], loss=86.5208
	step [99/196], loss=63.8555
	step [100/196], loss=83.2198
	step [101/196], loss=70.0418
	step [102/196], loss=65.2563
	step [103/196], loss=89.3396
	step [104/196], loss=86.2941
	step [105/196], loss=75.6518
	step [106/196], loss=67.8397
	step [107/196], loss=77.2062
	step [108/196], loss=76.8468
	step [109/196], loss=52.7509
	step [110/196], loss=72.4757
	step [111/196], loss=68.0258
	step [112/196], loss=56.7561
	step [113/196], loss=83.3285
	step [114/196], loss=74.0546
	step [115/196], loss=57.0556
	step [116/196], loss=76.3564
	step [117/196], loss=66.5741
	step [118/196], loss=71.5791
	step [119/196], loss=67.3821
	step [120/196], loss=69.5973
	step [121/196], loss=71.8485
	step [122/196], loss=85.4214
	step [123/196], loss=80.2532
	step [124/196], loss=75.4303
	step [125/196], loss=71.2983
	step [126/196], loss=68.7440
	step [127/196], loss=59.0686
	step [128/196], loss=67.7676
	step [129/196], loss=56.6176
	step [130/196], loss=80.5407
	step [131/196], loss=53.9197
	step [132/196], loss=71.2523
	step [133/196], loss=69.1283
	step [134/196], loss=62.6931
	step [135/196], loss=65.4834
	step [136/196], loss=79.7720
	step [137/196], loss=56.8972
	step [138/196], loss=80.0256
	step [139/196], loss=64.4796
	step [140/196], loss=74.7294
	step [141/196], loss=75.4572
	step [142/196], loss=67.5801
	step [143/196], loss=64.4830
	step [144/196], loss=63.8337
	step [145/196], loss=75.3116
	step [146/196], loss=88.3239
	step [147/196], loss=78.2267
	step [148/196], loss=66.9582
	step [149/196], loss=74.9278
	step [150/196], loss=82.1514
	step [151/196], loss=59.8521
	step [152/196], loss=93.4022
	step [153/196], loss=71.4990
	step [154/196], loss=61.5718
	step [155/196], loss=68.7006
	step [156/196], loss=69.3667
	step [157/196], loss=74.4447
	step [158/196], loss=66.4933
	step [159/196], loss=85.9737
	step [160/196], loss=83.3357
	step [161/196], loss=77.4316
	step [162/196], loss=77.2458
	step [163/196], loss=60.6319
	step [164/196], loss=59.6336
	step [165/196], loss=73.0843
	step [166/196], loss=67.9405
	step [167/196], loss=84.6272
	step [168/196], loss=69.4245
	step [169/196], loss=70.7497
	step [170/196], loss=83.7299
	step [171/196], loss=74.7058
	step [172/196], loss=75.1568
	step [173/196], loss=69.7016
	step [174/196], loss=87.1623
	step [175/196], loss=67.9623
	step [176/196], loss=66.4859
	step [177/196], loss=79.9677
	step [178/196], loss=72.8349
	step [179/196], loss=79.7892
	step [180/196], loss=79.1584
	step [181/196], loss=60.3299
	step [182/196], loss=52.8496
	step [183/196], loss=77.0204
	step [184/196], loss=82.0834
	step [185/196], loss=90.5398
	step [186/196], loss=74.8940
	step [187/196], loss=67.0349
	step [188/196], loss=71.1721
	step [189/196], loss=63.4060
	step [190/196], loss=47.4099
	step [191/196], loss=77.6475
	step [192/196], loss=70.5876
	step [193/196], loss=57.5821
	step [194/196], loss=69.4743
	step [195/196], loss=76.4722
	step [196/196], loss=22.5934
	Evaluating
	loss=0.0110, precision=0.3874, recall=0.9258, f1=0.5463
Training epoch 31
	step [1/196], loss=90.9158
	step [2/196], loss=76.8236
	step [3/196], loss=78.4430
	step [4/196], loss=83.1029
	step [5/196], loss=73.3461
	step [6/196], loss=54.0620
	step [7/196], loss=65.4995
	step [8/196], loss=63.9273
	step [9/196], loss=84.3888
	step [10/196], loss=66.3719
	step [11/196], loss=72.6905
	step [12/196], loss=64.7910
	step [13/196], loss=72.0488
	step [14/196], loss=83.3061
	step [15/196], loss=67.9194
	step [16/196], loss=66.5779
	step [17/196], loss=76.6953
	step [18/196], loss=62.2128
	step [19/196], loss=62.0562
	step [20/196], loss=67.1886
	step [21/196], loss=61.5620
	step [22/196], loss=75.2054
	step [23/196], loss=73.1337
	step [24/196], loss=71.4403
	step [25/196], loss=69.4967
	step [26/196], loss=65.2860
	step [27/196], loss=72.5080
	step [28/196], loss=88.7667
	step [29/196], loss=63.9377
	step [30/196], loss=70.9544
	step [31/196], loss=82.5381
	step [32/196], loss=64.3180
	step [33/196], loss=60.2760
	step [34/196], loss=79.8679
	step [35/196], loss=70.4575
	step [36/196], loss=91.1380
	step [37/196], loss=66.0062
	step [38/196], loss=64.5409
	step [39/196], loss=54.7627
	step [40/196], loss=66.6999
	step [41/196], loss=57.7592
	step [42/196], loss=68.7059
	step [43/196], loss=68.7672
	step [44/196], loss=67.6792
	step [45/196], loss=68.8990
	step [46/196], loss=64.9830
	step [47/196], loss=71.0129
	step [48/196], loss=65.5222
	step [49/196], loss=76.1232
	step [50/196], loss=60.8014
	step [51/196], loss=74.7555
	step [52/196], loss=89.9264
	step [53/196], loss=95.3276
	step [54/196], loss=78.7559
	step [55/196], loss=66.4838
	step [56/196], loss=61.5751
	step [57/196], loss=78.1463
	step [58/196], loss=59.7210
	step [59/196], loss=73.7090
	step [60/196], loss=80.0146
	step [61/196], loss=74.1686
	step [62/196], loss=55.6636
	step [63/196], loss=76.1597
	step [64/196], loss=71.2108
	step [65/196], loss=56.3944
	step [66/196], loss=66.5009
	step [67/196], loss=71.5119
	step [68/196], loss=68.8883
	step [69/196], loss=64.3651
	step [70/196], loss=72.3383
	step [71/196], loss=77.5997
	step [72/196], loss=59.5165
	step [73/196], loss=62.8699
	step [74/196], loss=70.0587
	step [75/196], loss=73.3048
	step [76/196], loss=64.0662
	step [77/196], loss=76.9809
	step [78/196], loss=57.3534
	step [79/196], loss=68.3309
	step [80/196], loss=84.1451
	step [81/196], loss=69.0175
	step [82/196], loss=66.3403
	step [83/196], loss=67.7536
	step [84/196], loss=60.4968
	step [85/196], loss=68.3794
	step [86/196], loss=61.0471
	step [87/196], loss=78.7876
	step [88/196], loss=68.6867
	step [89/196], loss=76.1489
	step [90/196], loss=59.4674
	step [91/196], loss=81.7457
	step [92/196], loss=62.4466
	step [93/196], loss=70.0233
	step [94/196], loss=59.7987
	step [95/196], loss=74.3758
	step [96/196], loss=72.1309
	step [97/196], loss=72.4067
	step [98/196], loss=60.4390
	step [99/196], loss=71.0244
	step [100/196], loss=79.0678
	step [101/196], loss=72.9203
	step [102/196], loss=62.3917
	step [103/196], loss=69.9991
	step [104/196], loss=80.0298
	step [105/196], loss=75.9231
	step [106/196], loss=71.8333
	step [107/196], loss=58.2317
	step [108/196], loss=68.8038
	step [109/196], loss=81.9314
	step [110/196], loss=62.4360
	step [111/196], loss=73.7841
	step [112/196], loss=74.6253
	step [113/196], loss=65.0024
	step [114/196], loss=76.6332
	step [115/196], loss=63.6430
	step [116/196], loss=59.4346
	step [117/196], loss=80.2906
	step [118/196], loss=65.1559
	step [119/196], loss=74.0303
	step [120/196], loss=77.4028
	step [121/196], loss=63.8837
	step [122/196], loss=66.7775
	step [123/196], loss=69.3576
	step [124/196], loss=59.4120
	step [125/196], loss=62.5312
	step [126/196], loss=67.8060
	step [127/196], loss=68.0312
	step [128/196], loss=69.2339
	step [129/196], loss=70.8769
	step [130/196], loss=85.4470
	step [131/196], loss=70.7951
	step [132/196], loss=72.6657
	step [133/196], loss=82.2488
	step [134/196], loss=77.5778
	step [135/196], loss=80.6896
	step [136/196], loss=78.6020
	step [137/196], loss=68.8916
	step [138/196], loss=63.2972
	step [139/196], loss=59.9529
	step [140/196], loss=63.6278
	step [141/196], loss=74.8423
	step [142/196], loss=66.1964
	step [143/196], loss=83.8198
	step [144/196], loss=79.8863
	step [145/196], loss=74.2878
	step [146/196], loss=67.3374
	step [147/196], loss=75.5965
	step [148/196], loss=62.6235
	step [149/196], loss=65.0531
	step [150/196], loss=75.2522
	step [151/196], loss=82.0734
	step [152/196], loss=77.4147
	step [153/196], loss=58.5693
	step [154/196], loss=77.7953
	step [155/196], loss=60.6722
	step [156/196], loss=79.6898
	step [157/196], loss=79.5388
	step [158/196], loss=65.5466
	step [159/196], loss=63.2242
	step [160/196], loss=79.2606
	step [161/196], loss=70.3438
	step [162/196], loss=59.2738
	step [163/196], loss=77.3177
	step [164/196], loss=75.3927
	step [165/196], loss=66.8465
	step [166/196], loss=62.4253
	step [167/196], loss=70.4918
	step [168/196], loss=77.9337
	step [169/196], loss=63.6802
	step [170/196], loss=73.3400
	step [171/196], loss=77.8397
	step [172/196], loss=49.5045
	step [173/196], loss=71.0981
	step [174/196], loss=73.2405
	step [175/196], loss=70.1928
	step [176/196], loss=75.0130
	step [177/196], loss=63.4021
	step [178/196], loss=69.8477
	step [179/196], loss=74.1338
	step [180/196], loss=82.1936
	step [181/196], loss=62.4269
	step [182/196], loss=74.2573
	step [183/196], loss=68.9066
	step [184/196], loss=70.4660
	step [185/196], loss=70.0400
	step [186/196], loss=52.8489
	step [187/196], loss=65.5821
	step [188/196], loss=52.7598
	step [189/196], loss=84.7345
	step [190/196], loss=63.4364
	step [191/196], loss=63.8580
	step [192/196], loss=78.3462
	step [193/196], loss=71.1700
	step [194/196], loss=69.8644
	step [195/196], loss=62.3405
	step [196/196], loss=9.9662
	Evaluating
	loss=0.0079, precision=0.4962, recall=0.8920, f1=0.6377
Training epoch 32
	step [1/196], loss=49.6766
	step [2/196], loss=76.7171
	step [3/196], loss=63.2361
	step [4/196], loss=81.9560
	step [5/196], loss=84.1531
	step [6/196], loss=68.8256
	step [7/196], loss=64.5984
	step [8/196], loss=70.0992
	step [9/196], loss=66.6319
	step [10/196], loss=76.8239
	step [11/196], loss=79.8988
	step [12/196], loss=82.1913
	step [13/196], loss=55.6534
	step [14/196], loss=96.5158
	step [15/196], loss=65.2495
	step [16/196], loss=62.6740
	step [17/196], loss=61.2058
	step [18/196], loss=69.3934
	step [19/196], loss=72.2878
	step [20/196], loss=75.0060
	step [21/196], loss=65.0078
	step [22/196], loss=74.4643
	step [23/196], loss=71.2265
	step [24/196], loss=59.2264
	step [25/196], loss=56.7374
	step [26/196], loss=70.3858
	step [27/196], loss=75.7308
	step [28/196], loss=83.8883
	step [29/196], loss=64.4433
	step [30/196], loss=65.9806
	step [31/196], loss=69.3901
	step [32/196], loss=83.8405
	step [33/196], loss=82.1219
	step [34/196], loss=70.9613
	step [35/196], loss=72.3841
	step [36/196], loss=78.5509
	step [37/196], loss=68.7574
	step [38/196], loss=57.0034
	step [39/196], loss=70.5899
	step [40/196], loss=63.0436
	step [41/196], loss=79.5315
	step [42/196], loss=65.3265
	step [43/196], loss=71.3095
	step [44/196], loss=81.8877
	step [45/196], loss=60.8642
	step [46/196], loss=73.3823
	step [47/196], loss=72.6519
	step [48/196], loss=71.5442
	step [49/196], loss=83.2864
	step [50/196], loss=48.9675
	step [51/196], loss=65.3993
	step [52/196], loss=74.9736
	step [53/196], loss=76.2558
	step [54/196], loss=78.1531
	step [55/196], loss=68.2659
	step [56/196], loss=85.5083
	step [57/196], loss=77.5919
	step [58/196], loss=65.0017
	step [59/196], loss=69.7903
	step [60/196], loss=62.8795
	step [61/196], loss=59.3539
	step [62/196], loss=78.3632
	step [63/196], loss=68.3679
	step [64/196], loss=62.2995
	step [65/196], loss=56.6921
	step [66/196], loss=61.8129
	step [67/196], loss=61.8799
	step [68/196], loss=59.9086
	step [69/196], loss=67.8471
	step [70/196], loss=49.9870
	step [71/196], loss=80.5439
	step [72/196], loss=76.0885
	step [73/196], loss=77.5788
	step [74/196], loss=63.9476
	step [75/196], loss=82.2917
	step [76/196], loss=68.8677
	step [77/196], loss=66.1693
	step [78/196], loss=60.4750
	step [79/196], loss=73.6951
	step [80/196], loss=65.2964
	step [81/196], loss=65.8630
	step [82/196], loss=61.6899
	step [83/196], loss=76.0182
	step [84/196], loss=57.1712
	step [85/196], loss=64.6131
	step [86/196], loss=62.4746
	step [87/196], loss=68.8504
	step [88/196], loss=72.5540
	step [89/196], loss=63.2667
	step [90/196], loss=67.2321
	step [91/196], loss=71.4631
	step [92/196], loss=82.4836
	step [93/196], loss=80.2645
	step [94/196], loss=71.5977
	step [95/196], loss=74.0688
	step [96/196], loss=64.2190
	step [97/196], loss=78.8305
	step [98/196], loss=79.7022
	step [99/196], loss=67.8884
	step [100/196], loss=68.5999
	step [101/196], loss=68.8425
	step [102/196], loss=74.6146
	step [103/196], loss=65.8929
	step [104/196], loss=70.4703
	step [105/196], loss=84.8470
	step [106/196], loss=72.9371
	step [107/196], loss=60.7581
	step [108/196], loss=68.8278
	step [109/196], loss=49.6700
	step [110/196], loss=67.0045
	step [111/196], loss=89.8535
	step [112/196], loss=65.8767
	step [113/196], loss=72.4094
	step [114/196], loss=65.4726
	step [115/196], loss=78.5234
	step [116/196], loss=58.0646
	step [117/196], loss=92.3277
	step [118/196], loss=60.1384
	step [119/196], loss=73.5380
	step [120/196], loss=72.9199
	step [121/196], loss=76.4910
	step [122/196], loss=87.9583
	step [123/196], loss=47.9835
	step [124/196], loss=77.8764
	step [125/196], loss=64.9505
	step [126/196], loss=64.3377
	step [127/196], loss=66.6357
	step [128/196], loss=72.1651
	step [129/196], loss=74.1811
	step [130/196], loss=53.3396
	step [131/196], loss=58.0448
	step [132/196], loss=62.3873
	step [133/196], loss=72.5154
	step [134/196], loss=67.8365
	step [135/196], loss=66.2830
	step [136/196], loss=66.3859
	step [137/196], loss=80.1656
	step [138/196], loss=66.7036
	step [139/196], loss=64.8919
	step [140/196], loss=68.8455
	step [141/196], loss=76.9133
	step [142/196], loss=62.8995
	step [143/196], loss=62.8282
	step [144/196], loss=64.9223
	step [145/196], loss=78.2396
	step [146/196], loss=78.5888
	step [147/196], loss=83.2382
	step [148/196], loss=59.7264
	step [149/196], loss=62.9891
	step [150/196], loss=58.8393
	step [151/196], loss=60.6010
	step [152/196], loss=62.0555
	step [153/196], loss=73.9285
	step [154/196], loss=65.9448
	step [155/196], loss=70.6233
	step [156/196], loss=68.1160
	step [157/196], loss=60.6016
	step [158/196], loss=68.2227
	step [159/196], loss=81.4340
	step [160/196], loss=73.4147
	step [161/196], loss=81.1055
	step [162/196], loss=67.9510
	step [163/196], loss=70.1967
	step [164/196], loss=59.2555
	step [165/196], loss=71.9001
	step [166/196], loss=71.4639
	step [167/196], loss=66.3102
	step [168/196], loss=73.1729
	step [169/196], loss=69.3682
	step [170/196], loss=62.1405
	step [171/196], loss=51.0827
	step [172/196], loss=69.0006
	step [173/196], loss=81.5010
	step [174/196], loss=66.9865
	step [175/196], loss=77.9435
	step [176/196], loss=77.9839
	step [177/196], loss=70.0030
	step [178/196], loss=69.8207
	step [179/196], loss=63.9945
	step [180/196], loss=74.4510
	step [181/196], loss=71.0418
	step [182/196], loss=64.8509
	step [183/196], loss=67.3145
	step [184/196], loss=74.7853
	step [185/196], loss=58.2078
	step [186/196], loss=66.5518
	step [187/196], loss=68.0122
	step [188/196], loss=62.8123
	step [189/196], loss=64.3072
	step [190/196], loss=58.6276
	step [191/196], loss=76.9099
	step [192/196], loss=70.3361
	step [193/196], loss=82.7085
	step [194/196], loss=82.7664
	step [195/196], loss=66.0310
	step [196/196], loss=13.7867
	Evaluating
	loss=0.0121, precision=0.3514, recall=0.9062, f1=0.5064
Training epoch 33
	step [1/196], loss=65.9186
	step [2/196], loss=68.7685
	step [3/196], loss=72.9529
	step [4/196], loss=63.1775
	step [5/196], loss=78.9926
	step [6/196], loss=60.1737
	step [7/196], loss=54.3627
	step [8/196], loss=67.5606
	step [9/196], loss=64.7918
	step [10/196], loss=73.7534
	step [11/196], loss=73.8916
	step [12/196], loss=65.1309
	step [13/196], loss=64.9816
	step [14/196], loss=79.2281
	step [15/196], loss=79.5562
	step [16/196], loss=59.3813
	step [17/196], loss=70.6695
	step [18/196], loss=55.4762
	step [19/196], loss=69.7617
	step [20/196], loss=69.4776
	step [21/196], loss=60.4690
	step [22/196], loss=67.3530
	step [23/196], loss=73.8377
	step [24/196], loss=79.6654
	step [25/196], loss=56.7949
	step [26/196], loss=53.7660
	step [27/196], loss=54.3066
	step [28/196], loss=73.6416
	step [29/196], loss=58.7353
	step [30/196], loss=72.2711
	step [31/196], loss=77.1240
	step [32/196], loss=67.7837
	step [33/196], loss=64.1508
	step [34/196], loss=73.8245
	step [35/196], loss=75.4223
	step [36/196], loss=76.4492
	step [37/196], loss=54.0017
	step [38/196], loss=61.0406
	step [39/196], loss=80.0401
	step [40/196], loss=70.9836
	step [41/196], loss=55.2235
	step [42/196], loss=52.7021
	step [43/196], loss=60.2344
	step [44/196], loss=77.6083
	step [45/196], loss=70.3525
	step [46/196], loss=65.8968
	step [47/196], loss=75.1735
	step [48/196], loss=53.6755
	step [49/196], loss=66.3724
	step [50/196], loss=63.2168
	step [51/196], loss=77.3577
	step [52/196], loss=64.4150
	step [53/196], loss=74.9344
	step [54/196], loss=66.3446
	step [55/196], loss=63.5715
	step [56/196], loss=84.7307
	step [57/196], loss=64.2326
	step [58/196], loss=68.1133
	step [59/196], loss=75.3480
	step [60/196], loss=55.9913
	step [61/196], loss=67.0233
	step [62/196], loss=65.3645
	step [63/196], loss=79.7091
	step [64/196], loss=71.4794
	step [65/196], loss=75.6459
	step [66/196], loss=68.7365
	step [67/196], loss=60.7393
	step [68/196], loss=63.7719
	step [69/196], loss=63.0579
	step [70/196], loss=76.5223
	step [71/196], loss=69.1033
	step [72/196], loss=66.4278
	step [73/196], loss=64.3045
	step [74/196], loss=63.2332
	step [75/196], loss=67.0905
	step [76/196], loss=67.7784
	step [77/196], loss=67.8804
	step [78/196], loss=78.9032
	step [79/196], loss=83.5404
	step [80/196], loss=80.6070
	step [81/196], loss=63.3630
	step [82/196], loss=90.3677
	step [83/196], loss=57.8038
	step [84/196], loss=64.4510
	step [85/196], loss=61.2639
	step [86/196], loss=77.4508
	step [87/196], loss=63.9663
	step [88/196], loss=67.6703
	step [89/196], loss=70.8167
	step [90/196], loss=77.7183
	step [91/196], loss=65.1578
	step [92/196], loss=93.1439
	step [93/196], loss=79.9668
	step [94/196], loss=74.3619
	step [95/196], loss=76.0351
	step [96/196], loss=68.9678
	step [97/196], loss=74.3635
	step [98/196], loss=71.1306
	step [99/196], loss=66.4858
	step [100/196], loss=64.9510
	step [101/196], loss=81.0288
	step [102/196], loss=79.9115
	step [103/196], loss=70.0036
	step [104/196], loss=65.9918
	step [105/196], loss=69.9798
	step [106/196], loss=72.3145
	step [107/196], loss=79.2621
	step [108/196], loss=72.4902
	step [109/196], loss=74.0123
	step [110/196], loss=76.8849
	step [111/196], loss=64.7856
	step [112/196], loss=69.2464
	step [113/196], loss=61.4434
	step [114/196], loss=54.2156
	step [115/196], loss=73.1538
	step [116/196], loss=66.2737
	step [117/196], loss=76.7237
	step [118/196], loss=78.6039
	step [119/196], loss=82.6713
	step [120/196], loss=71.0901
	step [121/196], loss=73.3354
	step [122/196], loss=78.3222
	step [123/196], loss=55.1036
	step [124/196], loss=64.9524
	step [125/196], loss=69.7457
	step [126/196], loss=81.7359
	step [127/196], loss=75.0487
	step [128/196], loss=57.8312
	step [129/196], loss=75.5162
	step [130/196], loss=59.4014
	step [131/196], loss=68.9133
	step [132/196], loss=67.0388
	step [133/196], loss=69.2054
	step [134/196], loss=61.1171
	step [135/196], loss=70.4241
	step [136/196], loss=80.7154
	step [137/196], loss=63.6387
	step [138/196], loss=86.6896
	step [139/196], loss=54.2883
	step [140/196], loss=86.1994
	step [141/196], loss=65.8876
	step [142/196], loss=65.0861
	step [143/196], loss=70.1837
	step [144/196], loss=69.9872
	step [145/196], loss=78.2318
	step [146/196], loss=52.9974
	step [147/196], loss=66.5266
	step [148/196], loss=69.6980
	step [149/196], loss=80.8302
	step [150/196], loss=67.1729
	step [151/196], loss=73.3869
	step [152/196], loss=68.7993
	step [153/196], loss=74.0602
	step [154/196], loss=74.6062
	step [155/196], loss=55.7510
	step [156/196], loss=71.9474
	step [157/196], loss=65.2399
	step [158/196], loss=72.5253
	step [159/196], loss=76.8245
	step [160/196], loss=82.0858
	step [161/196], loss=75.8367
	step [162/196], loss=65.8565
	step [163/196], loss=74.3226
	step [164/196], loss=61.6577
	step [165/196], loss=75.1233
	step [166/196], loss=74.4994
	step [167/196], loss=70.9885
	step [168/196], loss=78.5295
	step [169/196], loss=65.1304
	step [170/196], loss=48.0374
	step [171/196], loss=70.1834
	step [172/196], loss=64.9790
	step [173/196], loss=67.9446
	step [174/196], loss=64.9435
	step [175/196], loss=69.6300
	step [176/196], loss=67.4519
	step [177/196], loss=64.6517
	step [178/196], loss=85.4616
	step [179/196], loss=67.1906
	step [180/196], loss=64.0999
	step [181/196], loss=66.5236
	step [182/196], loss=63.8167
	step [183/196], loss=64.2677
	step [184/196], loss=54.3373
	step [185/196], loss=68.3225
	step [186/196], loss=71.6458
	step [187/196], loss=69.9703
	step [188/196], loss=77.0770
	step [189/196], loss=57.0569
	step [190/196], loss=80.7113
	step [191/196], loss=49.0526
	step [192/196], loss=80.7385
	step [193/196], loss=69.3836
	step [194/196], loss=54.3047
	step [195/196], loss=61.5425
	step [196/196], loss=18.3155
	Evaluating
	loss=0.0088, precision=0.4529, recall=0.9104, f1=0.6049
Training epoch 34
	step [1/196], loss=82.8955
	step [2/196], loss=76.9823
	step [3/196], loss=76.9844
	step [4/196], loss=59.9754
	step [5/196], loss=83.0660
	step [6/196], loss=71.7103
	step [7/196], loss=61.7733
	step [8/196], loss=67.9394
	step [9/196], loss=61.7911
	step [10/196], loss=82.6051
	step [11/196], loss=63.2768
	step [12/196], loss=75.0313
	step [13/196], loss=64.0242
	step [14/196], loss=65.3927
	step [15/196], loss=63.2230
	step [16/196], loss=65.1633
	step [17/196], loss=87.7463
	step [18/196], loss=65.5465
	step [19/196], loss=72.4440
	step [20/196], loss=67.3228
	step [21/196], loss=62.8037
	step [22/196], loss=61.3379
	step [23/196], loss=80.7543
	step [24/196], loss=58.7237
	step [25/196], loss=60.5320
	step [26/196], loss=69.6712
	step [27/196], loss=79.1702
	step [28/196], loss=83.2425
	step [29/196], loss=66.7263
	step [30/196], loss=74.1700
	step [31/196], loss=82.4911
	step [32/196], loss=68.3688
	step [33/196], loss=63.2898
	step [34/196], loss=59.0711
	step [35/196], loss=53.8047
	step [36/196], loss=56.3005
	step [37/196], loss=73.7242
	step [38/196], loss=67.8045
	step [39/196], loss=67.4365
	step [40/196], loss=81.6000
	step [41/196], loss=73.3526
	step [42/196], loss=72.2793
	step [43/196], loss=64.9052
	step [44/196], loss=62.3542
	step [45/196], loss=62.7143
	step [46/196], loss=76.2825
	step [47/196], loss=70.9939
	step [48/196], loss=69.1837
	step [49/196], loss=60.8708
	step [50/196], loss=77.3661
	step [51/196], loss=66.8709
	step [52/196], loss=52.6519
	step [53/196], loss=84.6029
	step [54/196], loss=64.2869
	step [55/196], loss=64.6993
	step [56/196], loss=62.6825
	step [57/196], loss=69.6097
	step [58/196], loss=92.4484
	step [59/196], loss=86.6331
	step [60/196], loss=75.7869
	step [61/196], loss=63.1201
	step [62/196], loss=66.4362
	step [63/196], loss=69.9705
	step [64/196], loss=56.7161
	step [65/196], loss=66.2374
	step [66/196], loss=81.0980
	step [67/196], loss=77.0743
	step [68/196], loss=74.1732
	step [69/196], loss=68.5944
	step [70/196], loss=80.1769
	step [71/196], loss=65.1169
	step [72/196], loss=66.4482
	step [73/196], loss=58.0096
	step [74/196], loss=72.9549
	step [75/196], loss=60.7811
	step [76/196], loss=78.1344
	step [77/196], loss=64.9667
	step [78/196], loss=65.1043
	step [79/196], loss=76.3677
	step [80/196], loss=62.9252
	step [81/196], loss=55.2703
	step [82/196], loss=64.7381
	step [83/196], loss=62.0069
	step [84/196], loss=64.8572
	step [85/196], loss=68.7149
	step [86/196], loss=79.0137
	step [87/196], loss=72.6583
	step [88/196], loss=67.0323
	step [89/196], loss=69.8843
	step [90/196], loss=68.3037
	step [91/196], loss=67.3680
	step [92/196], loss=62.8086
	step [93/196], loss=81.4678
	step [94/196], loss=71.4932
	step [95/196], loss=69.6760
	step [96/196], loss=74.7543
	step [97/196], loss=52.5801
	step [98/196], loss=82.8680
	step [99/196], loss=72.9211
	step [100/196], loss=71.7882
	step [101/196], loss=87.1821
	step [102/196], loss=70.4315
	step [103/196], loss=71.1789
	step [104/196], loss=64.0615
	step [105/196], loss=70.7807
	step [106/196], loss=70.5263
	step [107/196], loss=61.9242
	step [108/196], loss=67.5200
	step [109/196], loss=69.6853
	step [110/196], loss=53.0613
	step [111/196], loss=77.4967
	step [112/196], loss=65.1275
	step [113/196], loss=54.8521
	step [114/196], loss=67.7144
	step [115/196], loss=75.4637
	step [116/196], loss=63.1699
	step [117/196], loss=74.1988
	step [118/196], loss=81.8335
	step [119/196], loss=62.2129
	step [120/196], loss=57.2670
	step [121/196], loss=67.4929
	step [122/196], loss=62.0785
	step [123/196], loss=68.0442
	step [124/196], loss=53.1439
	step [125/196], loss=71.6628
	step [126/196], loss=68.9765
	step [127/196], loss=76.9984
	step [128/196], loss=58.0136
	step [129/196], loss=54.2231
	step [130/196], loss=72.0070
	step [131/196], loss=73.6993
	step [132/196], loss=55.3693
	step [133/196], loss=77.2938
	step [134/196], loss=80.4214
	step [135/196], loss=66.9665
	step [136/196], loss=58.0511
	step [137/196], loss=74.1159
	step [138/196], loss=58.3430
	step [139/196], loss=71.9791
	step [140/196], loss=62.8078
	step [141/196], loss=79.7528
	step [142/196], loss=41.5503
	step [143/196], loss=63.8236
	step [144/196], loss=77.3703
	step [145/196], loss=65.7992
	step [146/196], loss=60.0333
	step [147/196], loss=51.5403
	step [148/196], loss=74.1874
	step [149/196], loss=74.8624
	step [150/196], loss=85.6016
	step [151/196], loss=67.6897
	step [152/196], loss=72.6162
	step [153/196], loss=63.4593
	step [154/196], loss=71.8735
	step [155/196], loss=74.6864
	step [156/196], loss=55.8515
	step [157/196], loss=74.0808
	step [158/196], loss=63.1572
	step [159/196], loss=78.6285
	step [160/196], loss=59.0061
	step [161/196], loss=57.4479
	step [162/196], loss=57.1563
	step [163/196], loss=71.8424
	step [164/196], loss=62.2145
	step [165/196], loss=70.2709
	step [166/196], loss=67.8540
	step [167/196], loss=68.0330
	step [168/196], loss=53.8688
	step [169/196], loss=76.5916
	step [170/196], loss=72.0964
	step [171/196], loss=64.2829
	step [172/196], loss=69.3057
	step [173/196], loss=59.1065
	step [174/196], loss=55.7451
	step [175/196], loss=50.7037
	step [176/196], loss=83.9565
	step [177/196], loss=62.4847
	step [178/196], loss=79.6600
	step [179/196], loss=64.0318
	step [180/196], loss=64.0032
	step [181/196], loss=77.4619
	step [182/196], loss=66.4783
	step [183/196], loss=83.2687
	step [184/196], loss=68.9498
	step [185/196], loss=71.2945
	step [186/196], loss=61.7940
	step [187/196], loss=60.2715
	step [188/196], loss=73.1048
	step [189/196], loss=56.9196
	step [190/196], loss=71.8791
	step [191/196], loss=76.2579
	step [192/196], loss=63.5164
	step [193/196], loss=73.1684
	step [194/196], loss=70.9381
	step [195/196], loss=66.8517
	step [196/196], loss=15.5313
	Evaluating
	loss=0.0083, precision=0.4458, recall=0.9138, f1=0.5993
Training epoch 35
	step [1/196], loss=60.5006
	step [2/196], loss=70.8056
	step [3/196], loss=76.0147
	step [4/196], loss=60.2448
	step [5/196], loss=56.3269
	step [6/196], loss=60.5046
	step [7/196], loss=60.7414
	step [8/196], loss=77.7264
	step [9/196], loss=83.9364
	step [10/196], loss=68.3143
	step [11/196], loss=90.6612
	step [12/196], loss=72.0590
	step [13/196], loss=66.8792
	step [14/196], loss=78.9098
	step [15/196], loss=63.0921
	step [16/196], loss=74.3974
	step [17/196], loss=65.7810
	step [18/196], loss=66.8815
	step [19/196], loss=64.0678
	step [20/196], loss=90.2388
	step [21/196], loss=82.0155
	step [22/196], loss=87.1098
	step [23/196], loss=78.8353
	step [24/196], loss=84.7474
	step [25/196], loss=59.2081
	step [26/196], loss=75.8309
	step [27/196], loss=62.8422
	step [28/196], loss=50.4503
	step [29/196], loss=72.0323
	step [30/196], loss=58.2671
	step [31/196], loss=67.6480
	step [32/196], loss=63.9720
	step [33/196], loss=58.7350
	step [34/196], loss=51.1628
	step [35/196], loss=66.3451
	step [36/196], loss=67.8227
	step [37/196], loss=74.9088
	step [38/196], loss=68.2313
	step [39/196], loss=61.7574
	step [40/196], loss=73.6583
	step [41/196], loss=66.0918
	step [42/196], loss=71.4028
	step [43/196], loss=56.2678
	step [44/196], loss=65.5958
	step [45/196], loss=66.7855
	step [46/196], loss=61.8547
	step [47/196], loss=72.2020
	step [48/196], loss=77.3773
	step [49/196], loss=74.4628
	step [50/196], loss=78.9295
	step [51/196], loss=75.1464
	step [52/196], loss=59.9696
	step [53/196], loss=63.3737
	step [54/196], loss=60.6197
	step [55/196], loss=65.6024
	step [56/196], loss=80.6275
	step [57/196], loss=70.2370
	step [58/196], loss=59.3884
	step [59/196], loss=63.8859
	step [60/196], loss=75.5620
	step [61/196], loss=64.2992
	step [62/196], loss=60.6307
	step [63/196], loss=72.4697
	step [64/196], loss=62.6176
	step [65/196], loss=69.8453
	step [66/196], loss=66.0310
	step [67/196], loss=63.0885
	step [68/196], loss=66.7529
	step [69/196], loss=59.5502
	step [70/196], loss=74.3815
	step [71/196], loss=74.1801
	step [72/196], loss=75.4491
	step [73/196], loss=64.3154
	step [74/196], loss=73.9841
	step [75/196], loss=82.5282
	step [76/196], loss=71.8422
	step [77/196], loss=63.0054
	step [78/196], loss=61.1115
	step [79/196], loss=75.2077
	step [80/196], loss=74.7935
	step [81/196], loss=64.7715
	step [82/196], loss=91.6624
	step [83/196], loss=58.0615
	step [84/196], loss=67.0346
	step [85/196], loss=76.1224
	step [86/196], loss=78.5887
	step [87/196], loss=58.2458
	step [88/196], loss=68.7325
	step [89/196], loss=65.5393
	step [90/196], loss=66.7751
	step [91/196], loss=69.1699
	step [92/196], loss=62.7635
	step [93/196], loss=61.9679
	step [94/196], loss=60.0901
	step [95/196], loss=60.9423
	step [96/196], loss=76.7527
	step [97/196], loss=64.6762
	step [98/196], loss=68.5710
	step [99/196], loss=70.0133
	step [100/196], loss=75.7845
	step [101/196], loss=78.7456
	step [102/196], loss=57.8967
	step [103/196], loss=58.8093
	step [104/196], loss=66.6778
	step [105/196], loss=61.1330
	step [106/196], loss=77.6853
	step [107/196], loss=70.5156
	step [108/196], loss=63.2158
	step [109/196], loss=76.9382
	step [110/196], loss=60.8045
	step [111/196], loss=71.8041
	step [112/196], loss=61.8183
	step [113/196], loss=72.3252
	step [114/196], loss=72.6252
	step [115/196], loss=65.0465
	step [116/196], loss=76.1512
	step [117/196], loss=78.3543
	step [118/196], loss=71.1075
	step [119/196], loss=78.2079
	step [120/196], loss=71.3696
	step [121/196], loss=66.8666
	step [122/196], loss=65.3926
	step [123/196], loss=72.5235
	step [124/196], loss=67.7118
	step [125/196], loss=74.9076
	step [126/196], loss=67.2694
	step [127/196], loss=78.7485
	step [128/196], loss=74.6408
	step [129/196], loss=65.2326
	step [130/196], loss=66.6694
	step [131/196], loss=70.8791
	step [132/196], loss=68.6403
	step [133/196], loss=66.6489
	step [134/196], loss=57.0995
	step [135/196], loss=60.9304
	step [136/196], loss=61.1517
	step [137/196], loss=81.7994
	step [138/196], loss=89.4733
	step [139/196], loss=63.2311
	step [140/196], loss=66.8038
	step [141/196], loss=68.5713
	step [142/196], loss=56.5972
	step [143/196], loss=60.3669
	step [144/196], loss=51.2033
	step [145/196], loss=72.9793
	step [146/196], loss=59.9742
	step [147/196], loss=68.8731
	step [148/196], loss=79.1260
	step [149/196], loss=65.6073
	step [150/196], loss=73.8108
	step [151/196], loss=72.6150
	step [152/196], loss=66.6286
	step [153/196], loss=56.9888
	step [154/196], loss=51.6256
	step [155/196], loss=80.5135
	step [156/196], loss=73.0333
	step [157/196], loss=58.7629
	step [158/196], loss=67.2245
	step [159/196], loss=52.0969
	step [160/196], loss=66.9230
	step [161/196], loss=59.2470
	step [162/196], loss=73.1851
	step [163/196], loss=67.0453
	step [164/196], loss=62.6043
	step [165/196], loss=78.1515
	step [166/196], loss=54.4694
	step [167/196], loss=77.8739
	step [168/196], loss=60.7752
	step [169/196], loss=66.4482
	step [170/196], loss=77.8851
	step [171/196], loss=60.7332
	step [172/196], loss=64.6169
	step [173/196], loss=73.1084
	step [174/196], loss=70.6394
	step [175/196], loss=59.5729
	step [176/196], loss=63.1841
	step [177/196], loss=74.5228
	step [178/196], loss=65.6586
	step [179/196], loss=64.3517
	step [180/196], loss=64.5783
	step [181/196], loss=68.1290
	step [182/196], loss=79.2199
	step [183/196], loss=77.6508
	step [184/196], loss=59.8872
	step [185/196], loss=60.6052
	step [186/196], loss=60.9807
	step [187/196], loss=67.7825
	step [188/196], loss=67.2567
	step [189/196], loss=82.7048
	step [190/196], loss=60.4134
	step [191/196], loss=57.9371
	step [192/196], loss=67.9969
	step [193/196], loss=53.9599
	step [194/196], loss=74.7626
	step [195/196], loss=69.7717
	step [196/196], loss=9.5026
	Evaluating
	loss=0.0089, precision=0.4231, recall=0.8935, f1=0.5743
Training epoch 36
	step [1/196], loss=70.9655
	step [2/196], loss=65.8294
	step [3/196], loss=72.3083
	step [4/196], loss=68.9920
	step [5/196], loss=62.4839
	step [6/196], loss=66.9697
	step [7/196], loss=56.1939
	step [8/196], loss=77.7397
	step [9/196], loss=68.6903
	step [10/196], loss=59.5988
	step [11/196], loss=63.4642
	step [12/196], loss=68.3443
	step [13/196], loss=75.1633
	step [14/196], loss=68.6588
	step [15/196], loss=84.1461
	step [16/196], loss=69.0461
	step [17/196], loss=64.6101
	step [18/196], loss=64.5884
	step [19/196], loss=75.7002
	step [20/196], loss=81.4949
	step [21/196], loss=78.0999
	step [22/196], loss=51.4358
	step [23/196], loss=56.2588
	step [24/196], loss=67.9044
	step [25/196], loss=73.6687
	step [26/196], loss=61.3794
	step [27/196], loss=77.5983
	step [28/196], loss=80.2147
	step [29/196], loss=56.4463
	step [30/196], loss=71.7509
	step [31/196], loss=76.3862
	step [32/196], loss=61.1510
	step [33/196], loss=67.1385
	step [34/196], loss=63.9015
	step [35/196], loss=72.3927
	step [36/196], loss=68.4515
	step [37/196], loss=66.6076
	step [38/196], loss=60.5990
	step [39/196], loss=71.4889
	step [40/196], loss=70.7969
	step [41/196], loss=78.2059
	step [42/196], loss=72.1669
	step [43/196], loss=63.9242
	step [44/196], loss=53.1524
	step [45/196], loss=66.8115
	step [46/196], loss=62.9507
	step [47/196], loss=78.3672
	step [48/196], loss=76.5329
	step [49/196], loss=62.2876
	step [50/196], loss=61.3094
	step [51/196], loss=57.7623
	step [52/196], loss=56.9908
	step [53/196], loss=69.8009
	step [54/196], loss=57.1401
	step [55/196], loss=72.6997
	step [56/196], loss=59.9606
	step [57/196], loss=65.3959
	step [58/196], loss=69.5687
	step [59/196], loss=67.5092
	step [60/196], loss=68.4804
	step [61/196], loss=68.6482
	step [62/196], loss=50.3481
	step [63/196], loss=68.4844
	step [64/196], loss=76.8919
	step [65/196], loss=57.4559
	step [66/196], loss=70.1922
	step [67/196], loss=76.7211
	step [68/196], loss=65.7150
	step [69/196], loss=63.6405
	step [70/196], loss=67.7652
	step [71/196], loss=69.9371
	step [72/196], loss=61.9694
	step [73/196], loss=63.6936
	step [74/196], loss=72.6300
	step [75/196], loss=59.0292
	step [76/196], loss=62.7871
	step [77/196], loss=65.0206
	step [78/196], loss=61.6083
	step [79/196], loss=68.4685
	step [80/196], loss=65.2526
	step [81/196], loss=73.3942
	step [82/196], loss=72.9090
	step [83/196], loss=65.0491
	step [84/196], loss=75.0274
	step [85/196], loss=68.0226
	step [86/196], loss=65.9425
	step [87/196], loss=54.2525
	step [88/196], loss=66.7449
	step [89/196], loss=59.4252
	step [90/196], loss=67.0165
	step [91/196], loss=89.1200
	step [92/196], loss=63.3391
	step [93/196], loss=59.5111
	step [94/196], loss=67.0497
	step [95/196], loss=66.7768
	step [96/196], loss=58.1816
	step [97/196], loss=76.0733
	step [98/196], loss=57.0215
	step [99/196], loss=77.1332
	step [100/196], loss=57.9058
	step [101/196], loss=64.6214
	step [102/196], loss=69.9801
	step [103/196], loss=72.3694
	step [104/196], loss=60.9610
	step [105/196], loss=64.1850
	step [106/196], loss=63.2800
	step [107/196], loss=56.6032
	step [108/196], loss=62.1144
	step [109/196], loss=65.5752
	step [110/196], loss=69.0385
	step [111/196], loss=76.3319
	step [112/196], loss=65.2332
	step [113/196], loss=82.5830
	step [114/196], loss=73.9948
	step [115/196], loss=74.1268
	step [116/196], loss=58.6680
	step [117/196], loss=60.8452
	step [118/196], loss=62.3591
	step [119/196], loss=63.2163
	step [120/196], loss=56.9823
	step [121/196], loss=64.4324
	step [122/196], loss=73.6855
	step [123/196], loss=69.5910
	step [124/196], loss=73.7489
	step [125/196], loss=81.1593
	step [126/196], loss=69.8278
	step [127/196], loss=56.2504
	step [128/196], loss=80.4118
	step [129/196], loss=65.8392
	step [130/196], loss=68.7318
	step [131/196], loss=71.0950
	step [132/196], loss=60.9200
	step [133/196], loss=63.9753
	step [134/196], loss=55.9529
	step [135/196], loss=63.2207
	step [136/196], loss=76.8127
	step [137/196], loss=74.3674
	step [138/196], loss=77.4887
	step [139/196], loss=76.1762
	step [140/196], loss=77.3983
	step [141/196], loss=63.5711
	step [142/196], loss=63.9538
	step [143/196], loss=71.4796
	step [144/196], loss=55.6099
	step [145/196], loss=76.8681
	step [146/196], loss=61.0041
	step [147/196], loss=63.0270
	step [148/196], loss=61.4980
	step [149/196], loss=48.0466
	step [150/196], loss=74.7042
	step [151/196], loss=73.7010
	step [152/196], loss=68.4806
	step [153/196], loss=68.3863
	step [154/196], loss=60.1000
	step [155/196], loss=77.4930
	step [156/196], loss=57.4603
	step [157/196], loss=62.6333
	step [158/196], loss=52.2143
	step [159/196], loss=74.3044
	step [160/196], loss=69.8223
	step [161/196], loss=59.7563
	step [162/196], loss=72.5205
	step [163/196], loss=67.9631
	step [164/196], loss=73.2321
	step [165/196], loss=61.7855
	step [166/196], loss=63.0964
	step [167/196], loss=72.2678
	step [168/196], loss=67.3052
	step [169/196], loss=76.8150
	step [170/196], loss=69.2768
	step [171/196], loss=63.8929
	step [172/196], loss=61.8753
	step [173/196], loss=73.3802
	step [174/196], loss=64.3838
	step [175/196], loss=59.0046
	step [176/196], loss=67.3564
	step [177/196], loss=72.3189
	step [178/196], loss=70.1641
	step [179/196], loss=69.8296
	step [180/196], loss=72.0618
	step [181/196], loss=58.4887
	step [182/196], loss=97.0941
	step [183/196], loss=57.4054
	step [184/196], loss=70.5216
	step [185/196], loss=62.8753
	step [186/196], loss=69.3466
	step [187/196], loss=87.1170
	step [188/196], loss=74.1832
	step [189/196], loss=65.9571
	step [190/196], loss=64.4353
	step [191/196], loss=54.9050
	step [192/196], loss=79.1657
	step [193/196], loss=65.5008
	step [194/196], loss=74.6075
	step [195/196], loss=52.8125
	step [196/196], loss=18.5312
	Evaluating
	loss=0.0095, precision=0.3982, recall=0.9146, f1=0.5548
Training epoch 37
	step [1/196], loss=68.2874
	step [2/196], loss=73.8076
	step [3/196], loss=57.3557
	step [4/196], loss=72.8514
	step [5/196], loss=65.6917
	step [6/196], loss=67.4939
	step [7/196], loss=80.8781
	step [8/196], loss=64.9653
	step [9/196], loss=49.4627
	step [10/196], loss=70.5561
	step [11/196], loss=70.5035
	step [12/196], loss=68.6409
	step [13/196], loss=70.7796
	step [14/196], loss=76.8298
	step [15/196], loss=61.0447
	step [16/196], loss=66.4193
	step [17/196], loss=59.1453
	step [18/196], loss=59.3129
	step [19/196], loss=72.0246
	step [20/196], loss=60.0474
	step [21/196], loss=77.9599
	step [22/196], loss=64.2447
	step [23/196], loss=59.7091
	step [24/196], loss=65.3833
	step [25/196], loss=77.3779
	step [26/196], loss=63.0488
	step [27/196], loss=69.4867
	step [28/196], loss=73.8910
	step [29/196], loss=68.6295
	step [30/196], loss=68.0549
	step [31/196], loss=59.9535
	step [32/196], loss=77.6135
	step [33/196], loss=59.8447
	step [34/196], loss=82.5934
	step [35/196], loss=60.4088
	step [36/196], loss=68.5543
	step [37/196], loss=79.9069
	step [38/196], loss=82.2778
	step [39/196], loss=66.3755
	step [40/196], loss=67.7088
	step [41/196], loss=71.5501
	step [42/196], loss=60.2956
	step [43/196], loss=74.6742
	step [44/196], loss=69.0973
	step [45/196], loss=59.7079
	step [46/196], loss=77.9011
	step [47/196], loss=55.3628
	step [48/196], loss=79.4730
	step [49/196], loss=59.0909
	step [50/196], loss=73.2379
	step [51/196], loss=56.4617
	step [52/196], loss=80.5887
	step [53/196], loss=66.1130
	step [54/196], loss=58.7016
	step [55/196], loss=62.2443
	step [56/196], loss=61.0942
	step [57/196], loss=55.7234
	step [58/196], loss=73.0685
	step [59/196], loss=56.9295
	step [60/196], loss=50.2160
	step [61/196], loss=63.5734
	step [62/196], loss=56.9408
	step [63/196], loss=72.4842
	step [64/196], loss=71.3008
	step [65/196], loss=60.1278
	step [66/196], loss=72.4360
	step [67/196], loss=70.5163
	step [68/196], loss=64.3911
	step [69/196], loss=60.3154
	step [70/196], loss=68.8422
	step [71/196], loss=64.2537
	step [72/196], loss=73.4566
	step [73/196], loss=58.9794
	step [74/196], loss=66.3964
	step [75/196], loss=71.9876
	step [76/196], loss=69.1495
	step [77/196], loss=55.1541
	step [78/196], loss=71.4910
	step [79/196], loss=54.2076
	step [80/196], loss=68.7414
	step [81/196], loss=72.3490
	step [82/196], loss=68.8849
	step [83/196], loss=51.6024
	step [84/196], loss=56.1059
	step [85/196], loss=70.2468
	step [86/196], loss=68.6218
	step [87/196], loss=53.1141
	step [88/196], loss=74.6598
	step [89/196], loss=73.4271
	step [90/196], loss=57.0905
	step [91/196], loss=54.8181
	step [92/196], loss=80.3425
	step [93/196], loss=76.6011
	step [94/196], loss=57.7688
	step [95/196], loss=61.6573
	step [96/196], loss=64.3052
	step [97/196], loss=70.9261
	step [98/196], loss=55.4056
	step [99/196], loss=66.7151
	step [100/196], loss=64.9133
	step [101/196], loss=81.8129
	step [102/196], loss=79.0760
	step [103/196], loss=70.9297
	step [104/196], loss=67.1278
	step [105/196], loss=64.1860
	step [106/196], loss=58.5163
	step [107/196], loss=87.1495
	step [108/196], loss=52.4664
	step [109/196], loss=80.6239
	step [110/196], loss=76.2826
	step [111/196], loss=66.6008
	step [112/196], loss=66.2841
	step [113/196], loss=76.5553
	step [114/196], loss=64.0029
	step [115/196], loss=59.9309
	step [116/196], loss=60.1314
	step [117/196], loss=68.3587
	step [118/196], loss=81.1596
	step [119/196], loss=65.9399
	step [120/196], loss=79.2390
	step [121/196], loss=67.9938
	step [122/196], loss=58.8993
	step [123/196], loss=68.3973
	step [124/196], loss=59.6157
	step [125/196], loss=55.0087
	step [126/196], loss=73.4159
	step [127/196], loss=72.7842
	step [128/196], loss=58.7254
	step [129/196], loss=73.4993
	step [130/196], loss=72.1555
	step [131/196], loss=68.4712
	step [132/196], loss=75.7927
	step [133/196], loss=66.0262
	step [134/196], loss=66.5665
	step [135/196], loss=72.6872
	step [136/196], loss=54.9887
	step [137/196], loss=71.0553
	step [138/196], loss=55.9594
	step [139/196], loss=62.6460
	step [140/196], loss=51.8947
	step [141/196], loss=60.2968
	step [142/196], loss=61.9149
	step [143/196], loss=50.4888
	step [144/196], loss=64.9122
	step [145/196], loss=61.7803
	step [146/196], loss=68.6776
	step [147/196], loss=64.0187
	step [148/196], loss=57.9101
	step [149/196], loss=81.0759
	step [150/196], loss=72.0622
	step [151/196], loss=73.4113
	step [152/196], loss=58.3738
	step [153/196], loss=66.6110
	step [154/196], loss=61.5229
	step [155/196], loss=56.7914
	step [156/196], loss=70.2646
	step [157/196], loss=69.0135
	step [158/196], loss=64.0049
	step [159/196], loss=81.1885
	step [160/196], loss=60.6283
	step [161/196], loss=59.0002
	step [162/196], loss=70.9456
	step [163/196], loss=63.9046
	step [164/196], loss=67.3291
	step [165/196], loss=67.3145
	step [166/196], loss=56.1973
	step [167/196], loss=66.4545
	step [168/196], loss=65.4133
	step [169/196], loss=64.0319
	step [170/196], loss=57.0138
	step [171/196], loss=83.5861
	step [172/196], loss=74.4400
	step [173/196], loss=66.1080
	step [174/196], loss=68.5018
	step [175/196], loss=66.9922
	step [176/196], loss=95.4724
	step [177/196], loss=70.8219
	step [178/196], loss=64.6932
	step [179/196], loss=65.2952
	step [180/196], loss=64.3248
	step [181/196], loss=70.5375
	step [182/196], loss=68.8137
	step [183/196], loss=73.1539
	step [184/196], loss=70.8949
	step [185/196], loss=76.9757
	step [186/196], loss=62.6008
	step [187/196], loss=74.2381
	step [188/196], loss=68.3523
	step [189/196], loss=64.1483
	step [190/196], loss=69.9514
	step [191/196], loss=72.8266
	step [192/196], loss=55.8592
	step [193/196], loss=72.7746
	step [194/196], loss=59.8034
	step [195/196], loss=52.9390
	step [196/196], loss=13.4307
	Evaluating
	loss=0.0084, precision=0.4396, recall=0.9090, f1=0.5926
Training epoch 38
	step [1/196], loss=76.4631
	step [2/196], loss=88.0561
	step [3/196], loss=65.5578
	step [4/196], loss=67.3871
	step [5/196], loss=51.6723
	step [6/196], loss=80.1783
	step [7/196], loss=75.2880
	step [8/196], loss=61.5184
	step [9/196], loss=53.1029
	step [10/196], loss=61.6096
	step [11/196], loss=55.4240
	step [12/196], loss=50.1119
	step [13/196], loss=80.6823
	step [14/196], loss=74.9073
	step [15/196], loss=71.7080
	step [16/196], loss=73.7438
	step [17/196], loss=68.1634
	step [18/196], loss=63.1563
	step [19/196], loss=62.9001
	step [20/196], loss=72.3461
	step [21/196], loss=66.7782
	step [22/196], loss=77.1212
	step [23/196], loss=64.8326
	step [24/196], loss=55.7845
	step [25/196], loss=79.0739
	step [26/196], loss=74.0931
	step [27/196], loss=68.4188
	step [28/196], loss=53.3610
	step [29/196], loss=67.5164
	step [30/196], loss=75.1516
	step [31/196], loss=65.4312
	step [32/196], loss=75.0346
	step [33/196], loss=63.7829
	step [34/196], loss=52.3952
	step [35/196], loss=79.4341
	step [36/196], loss=55.0914
	step [37/196], loss=68.1948
	step [38/196], loss=62.2985
	step [39/196], loss=62.7814
	step [40/196], loss=55.3005
	step [41/196], loss=70.2082
	step [42/196], loss=59.9128
	step [43/196], loss=58.2489
	step [44/196], loss=70.7042
	step [45/196], loss=65.5617
	step [46/196], loss=59.7944
	step [47/196], loss=63.6436
	step [48/196], loss=64.6369
	step [49/196], loss=66.8345
	step [50/196], loss=55.2012
	step [51/196], loss=59.1472
	step [52/196], loss=77.8217
	step [53/196], loss=71.3234
	step [54/196], loss=70.2392
	step [55/196], loss=57.1690
	step [56/196], loss=63.3616
	step [57/196], loss=72.5600
	step [58/196], loss=67.0539
	step [59/196], loss=63.8676
	step [60/196], loss=64.9699
	step [61/196], loss=66.9527
	step [62/196], loss=59.0162
	step [63/196], loss=66.7646
	step [64/196], loss=65.4316
	step [65/196], loss=65.1382
	step [66/196], loss=67.0848
	step [67/196], loss=69.7633
	step [68/196], loss=73.6671
	step [69/196], loss=62.7151
	step [70/196], loss=61.7364
	step [71/196], loss=74.3698
	step [72/196], loss=55.1386
	step [73/196], loss=64.9338
	step [74/196], loss=66.1702
	step [75/196], loss=59.6779
	step [76/196], loss=72.4317
	step [77/196], loss=62.3021
	step [78/196], loss=64.2350
	step [79/196], loss=79.3511
	step [80/196], loss=56.5864
	step [81/196], loss=62.8058
	step [82/196], loss=62.7246
	step [83/196], loss=66.4234
	step [84/196], loss=68.4702
	step [85/196], loss=60.0611
	step [86/196], loss=77.5478
	step [87/196], loss=65.0479
	step [88/196], loss=58.4591
	step [89/196], loss=54.7657
	step [90/196], loss=62.3956
	step [91/196], loss=63.0554
	step [92/196], loss=61.2124
	step [93/196], loss=74.7219
	step [94/196], loss=66.1061
	step [95/196], loss=75.7363
	step [96/196], loss=68.5909
	step [97/196], loss=70.9330
	step [98/196], loss=72.1002
	step [99/196], loss=52.4592
	step [100/196], loss=67.6918
	step [101/196], loss=81.1902
	step [102/196], loss=61.3814
	step [103/196], loss=63.8728
	step [104/196], loss=75.9314
	step [105/196], loss=58.9547
	step [106/196], loss=82.0025
	step [107/196], loss=70.6889
	step [108/196], loss=59.3245
	step [109/196], loss=55.5307
	step [110/196], loss=73.4396
	step [111/196], loss=70.9644
	step [112/196], loss=57.4734
	step [113/196], loss=71.5851
	step [114/196], loss=76.5585
	step [115/196], loss=54.4719
	step [116/196], loss=67.6199
	step [117/196], loss=63.7090
	step [118/196], loss=64.4313
	step [119/196], loss=58.6473
	step [120/196], loss=60.3403
	step [121/196], loss=68.6002
	step [122/196], loss=62.4758
	step [123/196], loss=55.8305
	step [124/196], loss=63.4001
	step [125/196], loss=65.6577
	step [126/196], loss=69.5611
	step [127/196], loss=54.6661
	step [128/196], loss=64.9652
	step [129/196], loss=76.7842
	step [130/196], loss=67.6244
	step [131/196], loss=66.5269
	step [132/196], loss=63.3695
	step [133/196], loss=55.8048
	step [134/196], loss=72.8228
	step [135/196], loss=78.1436
	step [136/196], loss=64.8810
	step [137/196], loss=49.0821
	step [138/196], loss=57.8150
	step [139/196], loss=66.0064
	step [140/196], loss=67.7228
	step [141/196], loss=80.5105
	step [142/196], loss=79.6285
	step [143/196], loss=79.3623
	step [144/196], loss=64.2134
	step [145/196], loss=59.8168
	step [146/196], loss=53.9181
	step [147/196], loss=60.6187
	step [148/196], loss=69.7545
	step [149/196], loss=75.8548
	step [150/196], loss=66.3161
	step [151/196], loss=63.0210
	step [152/196], loss=67.5379
	step [153/196], loss=59.9209
	step [154/196], loss=63.1142
	step [155/196], loss=70.9047
	step [156/196], loss=77.2459
	step [157/196], loss=45.1245
	step [158/196], loss=80.3313
	step [159/196], loss=61.2653
	step [160/196], loss=65.7282
	step [161/196], loss=61.4961
	step [162/196], loss=70.7782
	step [163/196], loss=73.0731
	step [164/196], loss=61.2066
	step [165/196], loss=56.0064
	step [166/196], loss=71.3699
	step [167/196], loss=79.9112
	step [168/196], loss=67.2347
	step [169/196], loss=72.0815
	step [170/196], loss=64.5756
	step [171/196], loss=69.5444
	step [172/196], loss=73.4855
	step [173/196], loss=55.9483
	step [174/196], loss=67.2817
	step [175/196], loss=54.7336
	step [176/196], loss=84.1215
	step [177/196], loss=63.7531
	step [178/196], loss=71.3253
	step [179/196], loss=61.1615
	step [180/196], loss=55.6485
	step [181/196], loss=69.5466
	step [182/196], loss=60.0008
	step [183/196], loss=59.9695
	step [184/196], loss=68.9646
	step [185/196], loss=62.7678
	step [186/196], loss=80.6278
	step [187/196], loss=61.4189
	step [188/196], loss=69.3291
	step [189/196], loss=65.3438
	step [190/196], loss=76.1986
	step [191/196], loss=73.0243
	step [192/196], loss=58.1074
	step [193/196], loss=67.0958
	step [194/196], loss=72.4842
	step [195/196], loss=72.9097
	step [196/196], loss=13.9992
	Evaluating
	loss=0.0091, precision=0.4101, recall=0.8962, f1=0.5627
Training epoch 39
	step [1/196], loss=56.0958
	step [2/196], loss=68.4051
	step [3/196], loss=54.3089
	step [4/196], loss=58.3672
	step [5/196], loss=61.1453
	step [6/196], loss=72.2508
	step [7/196], loss=76.4557
	step [8/196], loss=66.6243
	step [9/196], loss=63.9237
	step [10/196], loss=63.6939
	step [11/196], loss=75.8214
	step [12/196], loss=71.7180
	step [13/196], loss=64.6512
	step [14/196], loss=51.9092
	step [15/196], loss=61.7029
	step [16/196], loss=49.3165
	step [17/196], loss=65.4454
	step [18/196], loss=66.4121
	step [19/196], loss=65.7954
	step [20/196], loss=55.6800
	step [21/196], loss=72.0371
	step [22/196], loss=69.2684
	step [23/196], loss=70.2403
	step [24/196], loss=56.6351
	step [25/196], loss=67.4704
	step [26/196], loss=58.7711
	step [27/196], loss=70.2365
	step [28/196], loss=67.6383
	step [29/196], loss=66.7513
	step [30/196], loss=76.4056
	step [31/196], loss=54.3083
	step [32/196], loss=61.9885
	step [33/196], loss=72.4081
	step [34/196], loss=73.7021
	step [35/196], loss=63.3210
	step [36/196], loss=74.6620
	step [37/196], loss=52.6721
	step [38/196], loss=69.4458
	step [39/196], loss=55.7067
	step [40/196], loss=66.9984
	step [41/196], loss=73.0602
	step [42/196], loss=56.4659
	step [43/196], loss=64.3008
	step [44/196], loss=67.8515
	step [45/196], loss=52.9175
	step [46/196], loss=66.4769
	step [47/196], loss=70.3925
	step [48/196], loss=69.3974
	step [49/196], loss=67.6797
	step [50/196], loss=64.6949
	step [51/196], loss=77.5684
	step [52/196], loss=67.5866
	step [53/196], loss=71.5393
	step [54/196], loss=65.3782
	step [55/196], loss=72.7812
	step [56/196], loss=61.4807
	step [57/196], loss=53.4053
	step [58/196], loss=60.1997
	step [59/196], loss=63.8543
	step [60/196], loss=45.5843
	step [61/196], loss=64.7131
	step [62/196], loss=71.3881
	step [63/196], loss=56.8897
	step [64/196], loss=60.3630
	step [65/196], loss=69.2929
	step [66/196], loss=60.6233
	step [67/196], loss=54.9855
	step [68/196], loss=77.8331
	step [69/196], loss=73.6529
	step [70/196], loss=64.8370
	step [71/196], loss=76.7747
	step [72/196], loss=61.2024
	step [73/196], loss=72.1905
	step [74/196], loss=73.7869
	step [75/196], loss=72.5072
	step [76/196], loss=69.0938
	step [77/196], loss=68.9217
	step [78/196], loss=79.7954
	step [79/196], loss=73.2469
	step [80/196], loss=71.9307
	step [81/196], loss=50.9227
	step [82/196], loss=68.5195
	step [83/196], loss=59.7244
	step [84/196], loss=75.5722
	step [85/196], loss=64.0617
	step [86/196], loss=73.0412
	step [87/196], loss=58.1254
	step [88/196], loss=66.5464
	step [89/196], loss=56.6126
	step [90/196], loss=63.0401
	step [91/196], loss=76.9410
	step [92/196], loss=62.1297
	step [93/196], loss=57.8266
	step [94/196], loss=53.6028
	step [95/196], loss=72.7105
	step [96/196], loss=61.9322
	step [97/196], loss=75.6241
	step [98/196], loss=70.6567
	step [99/196], loss=65.0927
	step [100/196], loss=69.5916
	step [101/196], loss=64.1901
	step [102/196], loss=61.0142
	step [103/196], loss=77.3022
	step [104/196], loss=50.0785
	step [105/196], loss=66.7949
	step [106/196], loss=83.1324
	step [107/196], loss=63.3663
	step [108/196], loss=55.3050
	step [109/196], loss=58.2042
	step [110/196], loss=66.5121
	step [111/196], loss=67.2916
	step [112/196], loss=70.3048
	step [113/196], loss=70.6100
	step [114/196], loss=66.6062
	step [115/196], loss=76.7748
	step [116/196], loss=71.4623
	step [117/196], loss=47.2971
	step [118/196], loss=68.4171
	step [119/196], loss=66.5862
	step [120/196], loss=55.1216
	step [121/196], loss=73.7700
	step [122/196], loss=81.2600
	step [123/196], loss=55.5261
	step [124/196], loss=62.2384
	step [125/196], loss=73.8996
	step [126/196], loss=67.1742
	step [127/196], loss=64.8729
	step [128/196], loss=61.5258
	step [129/196], loss=71.2945
	step [130/196], loss=65.5693
	step [131/196], loss=78.1506
	step [132/196], loss=51.1954
	step [133/196], loss=68.5451
	step [134/196], loss=60.4725
	step [135/196], loss=81.5175
	step [136/196], loss=73.8835
	step [137/196], loss=56.8518
	step [138/196], loss=74.0758
	step [139/196], loss=60.9653
	step [140/196], loss=53.6443
	step [141/196], loss=54.2581
	step [142/196], loss=75.5928
	step [143/196], loss=71.1590
	step [144/196], loss=73.5926
	step [145/196], loss=62.9986
	step [146/196], loss=78.2720
	step [147/196], loss=56.5067
	step [148/196], loss=57.8979
	step [149/196], loss=67.1646
	step [150/196], loss=61.4824
	step [151/196], loss=66.5708
	step [152/196], loss=75.3021
	step [153/196], loss=61.3104
	step [154/196], loss=53.0901
	step [155/196], loss=56.5205
	step [156/196], loss=60.6490
	step [157/196], loss=78.9045
	step [158/196], loss=61.3093
	step [159/196], loss=72.0575
	step [160/196], loss=49.8096
	step [161/196], loss=54.1532
	step [162/196], loss=69.9480
	step [163/196], loss=55.7526
	step [164/196], loss=73.6201
	step [165/196], loss=56.9604
	step [166/196], loss=54.6961
	step [167/196], loss=73.0518
	step [168/196], loss=64.6827
	step [169/196], loss=64.3194
	step [170/196], loss=53.5086
	step [171/196], loss=73.3406
	step [172/196], loss=68.4883
	step [173/196], loss=53.6895
	step [174/196], loss=88.9239
	step [175/196], loss=78.2595
	step [176/196], loss=76.3290
	step [177/196], loss=80.4028
	step [178/196], loss=78.5353
	step [179/196], loss=58.5155
	step [180/196], loss=76.5891
	step [181/196], loss=62.9420
	step [182/196], loss=60.3714
	step [183/196], loss=54.5127
	step [184/196], loss=72.6183
	step [185/196], loss=70.2663
	step [186/196], loss=57.8087
	step [187/196], loss=77.2783
	step [188/196], loss=54.9873
	step [189/196], loss=76.3270
	step [190/196], loss=75.6206
	step [191/196], loss=69.6900
	step [192/196], loss=52.9282
	step [193/196], loss=63.1333
	step [194/196], loss=69.5465
	step [195/196], loss=67.9492
	step [196/196], loss=18.3649
	Evaluating
	loss=0.0077, precision=0.4431, recall=0.8783, f1=0.5891
Training epoch 40
	step [1/196], loss=51.8551
	step [2/196], loss=79.9108
	step [3/196], loss=75.7915
	step [4/196], loss=60.3939
	step [5/196], loss=71.6887
	step [6/196], loss=63.4148
	step [7/196], loss=62.5216
	step [8/196], loss=68.2292
	step [9/196], loss=56.6426
	step [10/196], loss=78.3838
	step [11/196], loss=63.5633
	step [12/196], loss=71.6415
	step [13/196], loss=60.8440
	step [14/196], loss=69.4155
	step [15/196], loss=74.9088
	step [16/196], loss=66.9756
	step [17/196], loss=76.2955
	step [18/196], loss=63.3731
	step [19/196], loss=62.8407
	step [20/196], loss=61.7112
	step [21/196], loss=67.4847
	step [22/196], loss=59.8038
	step [23/196], loss=66.4153
	step [24/196], loss=60.3285
	step [25/196], loss=73.6449
	step [26/196], loss=57.4883
	step [27/196], loss=60.0523
	step [28/196], loss=67.0350
	step [29/196], loss=65.7512
	step [30/196], loss=65.9600
	step [31/196], loss=80.1095
	step [32/196], loss=82.7527
	step [33/196], loss=71.7983
	step [34/196], loss=73.8254
	step [35/196], loss=64.8292
	step [36/196], loss=65.0652
	step [37/196], loss=54.4819
	step [38/196], loss=59.9387
	step [39/196], loss=67.4654
	step [40/196], loss=66.2728
	step [41/196], loss=61.9181
	step [42/196], loss=71.3621
	step [43/196], loss=67.0876
	step [44/196], loss=69.8156
	step [45/196], loss=74.5057
	step [46/196], loss=84.9208
	step [47/196], loss=68.5341
	step [48/196], loss=71.7118
	step [49/196], loss=51.9557
	step [50/196], loss=62.4797
	step [51/196], loss=61.5617
	step [52/196], loss=73.5972
	step [53/196], loss=56.9846
	step [54/196], loss=56.3040
	step [55/196], loss=58.7959
	step [56/196], loss=63.5743
	step [57/196], loss=72.0170
	step [58/196], loss=73.9026
	step [59/196], loss=60.9604
	step [60/196], loss=55.2037
	step [61/196], loss=62.4301
	step [62/196], loss=49.8810
	step [63/196], loss=65.7898
	step [64/196], loss=56.6801
	step [65/196], loss=70.9111
	step [66/196], loss=62.2760
	step [67/196], loss=62.7355
	step [68/196], loss=73.9774
	step [69/196], loss=72.0004
	step [70/196], loss=58.7612
	step [71/196], loss=68.3455
	step [72/196], loss=70.0402
	step [73/196], loss=60.9630
	step [74/196], loss=60.2622
	step [75/196], loss=73.9262
	step [76/196], loss=68.0648
	step [77/196], loss=74.3605
	step [78/196], loss=74.7852
	step [79/196], loss=78.4559
	step [80/196], loss=70.2178
	step [81/196], loss=70.0626
	step [82/196], loss=62.5456
	step [83/196], loss=60.1731
	step [84/196], loss=60.9831
	step [85/196], loss=57.8429
	step [86/196], loss=69.1992
	step [87/196], loss=62.6762
	step [88/196], loss=63.5659
	step [89/196], loss=69.4833
	step [90/196], loss=61.4568
	step [91/196], loss=62.4931
	step [92/196], loss=74.9824
	step [93/196], loss=64.4532
	step [94/196], loss=62.3443
	step [95/196], loss=67.8149
	step [96/196], loss=55.1777
	step [97/196], loss=49.6382
	step [98/196], loss=73.1456
	step [99/196], loss=54.0425
	step [100/196], loss=67.5073
	step [101/196], loss=53.3897
	step [102/196], loss=63.0992
	step [103/196], loss=66.0473
	step [104/196], loss=54.7998
	step [105/196], loss=59.6043
	step [106/196], loss=71.7440
	step [107/196], loss=69.9037
	step [108/196], loss=60.9979
	step [109/196], loss=63.5474
	step [110/196], loss=64.0155
	step [111/196], loss=67.8239
	step [112/196], loss=62.9192
	step [113/196], loss=62.2098
	step [114/196], loss=51.9008
	step [115/196], loss=73.1463
	step [116/196], loss=66.0040
	step [117/196], loss=60.8672
	step [118/196], loss=69.9039
	step [119/196], loss=66.0967
	step [120/196], loss=56.4159
	step [121/196], loss=62.2906
	step [122/196], loss=62.4592
	step [123/196], loss=63.8815
	step [124/196], loss=64.4848
	step [125/196], loss=70.6691
	step [126/196], loss=74.1463
	step [127/196], loss=48.9563
	step [128/196], loss=53.4541
	step [129/196], loss=66.9785
	step [130/196], loss=73.9416
	step [131/196], loss=67.0648
	step [132/196], loss=60.3375
	step [133/196], loss=72.0506
	step [134/196], loss=63.9675
	step [135/196], loss=64.0053
	step [136/196], loss=67.2499
	step [137/196], loss=68.3997
	step [138/196], loss=73.8701
	step [139/196], loss=73.7976
	step [140/196], loss=65.1971
	step [141/196], loss=76.9667
	step [142/196], loss=49.7963
	step [143/196], loss=61.7262
	step [144/196], loss=66.7408
	step [145/196], loss=84.0984
	step [146/196], loss=70.9494
	step [147/196], loss=53.5181
	step [148/196], loss=59.9829
	step [149/196], loss=61.9608
	step [150/196], loss=69.1287
	step [151/196], loss=85.5000
	step [152/196], loss=67.1383
	step [153/196], loss=42.4632
	step [154/196], loss=55.8080
	step [155/196], loss=75.7708
	step [156/196], loss=43.8955
	step [157/196], loss=55.7103
	step [158/196], loss=64.9099
	step [159/196], loss=65.8620
	step [160/196], loss=57.1524
	step [161/196], loss=62.1530
	step [162/196], loss=62.0758
	step [163/196], loss=68.7203
	step [164/196], loss=65.7651
	step [165/196], loss=56.6929
	step [166/196], loss=67.9855
	step [167/196], loss=71.3109
	step [168/196], loss=59.5682
	step [169/196], loss=65.7399
	step [170/196], loss=72.5112
	step [171/196], loss=69.3532
	step [172/196], loss=71.4255
	step [173/196], loss=64.3099
	step [174/196], loss=48.5703
	step [175/196], loss=66.1201
	step [176/196], loss=68.1251
	step [177/196], loss=62.8593
	step [178/196], loss=58.5415
	step [179/196], loss=59.6964
	step [180/196], loss=62.2066
	step [181/196], loss=52.9076
	step [182/196], loss=59.6906
	step [183/196], loss=76.8197
	step [184/196], loss=58.8867
	step [185/196], loss=70.7576
	step [186/196], loss=78.5817
	step [187/196], loss=70.2538
	step [188/196], loss=58.8359
	step [189/196], loss=61.9066
	step [190/196], loss=52.9855
	step [191/196], loss=63.7977
	step [192/196], loss=64.6901
	step [193/196], loss=71.2189
	step [194/196], loss=56.8873
	step [195/196], loss=64.3172
	step [196/196], loss=13.0179
	Evaluating
	loss=0.0073, precision=0.4778, recall=0.8818, f1=0.6198
Training epoch 41
	step [1/196], loss=57.4956
	step [2/196], loss=62.7100
	step [3/196], loss=69.2211
	step [4/196], loss=57.9376
	step [5/196], loss=72.5330
	step [6/196], loss=65.8286
	step [7/196], loss=70.3595
	step [8/196], loss=59.0446
	step [9/196], loss=67.2115
	step [10/196], loss=58.2367
	step [11/196], loss=77.0983
	step [12/196], loss=67.2452
	step [13/196], loss=73.0909
	step [14/196], loss=51.8401
	step [15/196], loss=64.8643
	step [16/196], loss=60.8314
	step [17/196], loss=63.3224
	step [18/196], loss=56.4684
	step [19/196], loss=55.8029
	step [20/196], loss=50.3952
	step [21/196], loss=70.8917
	step [22/196], loss=75.2619
	step [23/196], loss=59.7828
	step [24/196], loss=60.6953
	step [25/196], loss=60.6094
	step [26/196], loss=77.7625
	step [27/196], loss=61.9520
	step [28/196], loss=78.7299
	step [29/196], loss=69.5068
	step [30/196], loss=82.3137
	step [31/196], loss=54.4826
	step [32/196], loss=64.7612
	step [33/196], loss=60.5913
	step [34/196], loss=51.4012
	step [35/196], loss=63.7725
	step [36/196], loss=52.6365
	step [37/196], loss=62.4789
	step [38/196], loss=73.8650
	step [39/196], loss=49.4821
	step [40/196], loss=85.6386
	step [41/196], loss=61.5454
	step [42/196], loss=68.2288
	step [43/196], loss=58.4912
	step [44/196], loss=64.1209
	step [45/196], loss=65.7594
	step [46/196], loss=64.1123
	step [47/196], loss=66.7278
	step [48/196], loss=72.4869
	step [49/196], loss=75.7745
	step [50/196], loss=67.6548
	step [51/196], loss=47.1242
	step [52/196], loss=65.3052
	step [53/196], loss=65.2611
	step [54/196], loss=57.7028
	step [55/196], loss=60.6182
	step [56/196], loss=69.9419
	step [57/196], loss=55.3527
	step [58/196], loss=65.9688
	step [59/196], loss=56.6383
	step [60/196], loss=73.3458
	step [61/196], loss=74.4575
	step [62/196], loss=78.1513
	step [63/196], loss=67.4680
	step [64/196], loss=62.8416
	step [65/196], loss=70.0744
	step [66/196], loss=50.9021
	step [67/196], loss=62.8199
	step [68/196], loss=55.9571
	step [69/196], loss=65.1135
	step [70/196], loss=68.2623
	step [71/196], loss=77.4940
	step [72/196], loss=65.5271
	step [73/196], loss=68.0326
	step [74/196], loss=61.2315
	step [75/196], loss=67.1150
	step [76/196], loss=63.0590
	step [77/196], loss=69.8318
	step [78/196], loss=64.7213
	step [79/196], loss=63.1190
	step [80/196], loss=76.7070
	step [81/196], loss=84.6203
	step [82/196], loss=64.5306
	step [83/196], loss=46.1805
	step [84/196], loss=55.7391
	step [85/196], loss=61.3976
	step [86/196], loss=67.1418
	step [87/196], loss=53.4838
	step [88/196], loss=58.5354
	step [89/196], loss=62.6247
	step [90/196], loss=62.5144
	step [91/196], loss=49.7003
	step [92/196], loss=62.4662
	step [93/196], loss=69.2040
	step [94/196], loss=54.1687
	step [95/196], loss=67.1688
	step [96/196], loss=56.2242
	step [97/196], loss=67.2304
	step [98/196], loss=61.2293
	step [99/196], loss=82.1554
	step [100/196], loss=63.3909
	step [101/196], loss=71.0055
	step [102/196], loss=64.3025
	step [103/196], loss=76.0799
	step [104/196], loss=54.4641
	step [105/196], loss=67.7535
	step [106/196], loss=64.2956
	step [107/196], loss=79.3364
	step [108/196], loss=62.0331
	step [109/196], loss=72.5862
	step [110/196], loss=70.7735
	step [111/196], loss=61.9268
	step [112/196], loss=61.4292
	step [113/196], loss=59.8702
	step [114/196], loss=74.0747
	step [115/196], loss=65.9832
	step [116/196], loss=72.6976
	step [117/196], loss=76.5703
	step [118/196], loss=65.9510
	step [119/196], loss=67.2510
	step [120/196], loss=56.3550
	step [121/196], loss=69.1530
	step [122/196], loss=53.7766
	step [123/196], loss=60.4896
	step [124/196], loss=63.4273
	step [125/196], loss=69.9808
	step [126/196], loss=79.4971
	step [127/196], loss=67.0375
	step [128/196], loss=59.4850
	step [129/196], loss=60.9098
	step [130/196], loss=64.9704
	step [131/196], loss=76.0126
	step [132/196], loss=62.9410
	step [133/196], loss=64.6653
	step [134/196], loss=73.9033
	step [135/196], loss=56.6859
	step [136/196], loss=71.9173
	step [137/196], loss=54.6166
	step [138/196], loss=56.0546
	step [139/196], loss=58.1383
	step [140/196], loss=60.2900
	step [141/196], loss=72.3470
	step [142/196], loss=73.2263
	step [143/196], loss=59.4734
	step [144/196], loss=61.6144
	step [145/196], loss=74.5024
	step [146/196], loss=58.4818
	step [147/196], loss=58.0960
	step [148/196], loss=59.4903
	step [149/196], loss=57.8714
	step [150/196], loss=67.8722
	step [151/196], loss=56.9272
	step [152/196], loss=69.7599
	step [153/196], loss=66.1838
	step [154/196], loss=58.9718
	step [155/196], loss=66.5711
	step [156/196], loss=51.6587
	step [157/196], loss=71.5850
	step [158/196], loss=68.8891
	step [159/196], loss=63.0840
	step [160/196], loss=71.9012
	step [161/196], loss=60.9901
	step [162/196], loss=66.9779
	step [163/196], loss=67.4551
	step [164/196], loss=72.2287
	step [165/196], loss=70.3907
	step [166/196], loss=66.2545
	step [167/196], loss=61.1837
	step [168/196], loss=64.9012
	step [169/196], loss=66.6649
	step [170/196], loss=62.4093
	step [171/196], loss=60.4068
	step [172/196], loss=56.9571
	step [173/196], loss=62.6984
	step [174/196], loss=69.9790
	step [175/196], loss=62.1933
	step [176/196], loss=72.7414
	step [177/196], loss=59.2351
	step [178/196], loss=55.5725
	step [179/196], loss=73.1363
	step [180/196], loss=60.0060
	step [181/196], loss=70.6955
	step [182/196], loss=50.4010
	step [183/196], loss=76.4361
	step [184/196], loss=54.7906
	step [185/196], loss=56.3346
	step [186/196], loss=52.3772
	step [187/196], loss=61.7520
	step [188/196], loss=67.7892
	step [189/196], loss=49.2369
	step [190/196], loss=58.8252
	step [191/196], loss=55.6533
	step [192/196], loss=67.4666
	step [193/196], loss=62.6292
	step [194/196], loss=60.7776
	step [195/196], loss=57.9365
	step [196/196], loss=10.6514
	Evaluating
	loss=0.0079, precision=0.4350, recall=0.8874, f1=0.5838
Training epoch 42
	step [1/196], loss=74.7516
	step [2/196], loss=79.8719
	step [3/196], loss=60.9720
	step [4/196], loss=41.8775
	step [5/196], loss=67.8272
	step [6/196], loss=70.7743
	step [7/196], loss=68.9305
	step [8/196], loss=57.3415
	step [9/196], loss=47.5975
	step [10/196], loss=80.9692
	step [11/196], loss=61.8594
	step [12/196], loss=62.0387
	step [13/196], loss=55.8061
	step [14/196], loss=67.1546
	step [15/196], loss=81.5497
	step [16/196], loss=68.8650
	step [17/196], loss=69.6890
	step [18/196], loss=60.0530
	step [19/196], loss=54.0921
	step [20/196], loss=55.5355
	step [21/196], loss=69.2773
	step [22/196], loss=57.1982
	step [23/196], loss=60.3341
	step [24/196], loss=65.8818
	step [25/196], loss=60.5751
	step [26/196], loss=62.5402
	step [27/196], loss=48.1194
	step [28/196], loss=55.2018
	step [29/196], loss=67.9616
	step [30/196], loss=69.7652
	step [31/196], loss=53.1284
	step [32/196], loss=60.0846
	step [33/196], loss=58.1149
	step [34/196], loss=65.5516
	step [35/196], loss=59.5620
	step [36/196], loss=58.7649
	step [37/196], loss=55.0646
	step [38/196], loss=74.7962
	step [39/196], loss=63.4285
	step [40/196], loss=70.0881
	step [41/196], loss=60.1882
	step [42/196], loss=65.4422
	step [43/196], loss=74.2697
	step [44/196], loss=70.0122
	step [45/196], loss=67.6500
	step [46/196], loss=61.4269
	step [47/196], loss=53.3134
	step [48/196], loss=68.2991
	step [49/196], loss=67.2631
	step [50/196], loss=49.9940
	step [51/196], loss=65.4456
	step [52/196], loss=59.0821
	step [53/196], loss=66.1032
	step [54/196], loss=57.1317
	step [55/196], loss=61.8499
	step [56/196], loss=64.6649
	step [57/196], loss=73.3712
	step [58/196], loss=64.3023
	step [59/196], loss=68.5164
	step [60/196], loss=66.1631
	step [61/196], loss=63.8309
	step [62/196], loss=68.6733
	step [63/196], loss=58.9785
	step [64/196], loss=72.6160
	step [65/196], loss=65.1259
	step [66/196], loss=49.8694
	step [67/196], loss=39.0653
	step [68/196], loss=65.3344
	step [69/196], loss=72.0533
	step [70/196], loss=65.4521
	step [71/196], loss=70.1473
	step [72/196], loss=67.6955
	step [73/196], loss=59.1191
	step [74/196], loss=67.4744
	step [75/196], loss=61.8642
	step [76/196], loss=62.2572
	step [77/196], loss=59.7325
	step [78/196], loss=71.5328
	step [79/196], loss=51.4568
	step [80/196], loss=61.8897
	step [81/196], loss=59.6271
	step [82/196], loss=74.6125
	step [83/196], loss=65.3881
	step [84/196], loss=66.9437
	step [85/196], loss=55.6526
	step [86/196], loss=60.4155
	step [87/196], loss=83.6133
	step [88/196], loss=67.7396
	step [89/196], loss=74.8849
	step [90/196], loss=47.4821
	step [91/196], loss=57.3305
	step [92/196], loss=54.2265
	step [93/196], loss=58.4321
	step [94/196], loss=58.2116
	step [95/196], loss=63.9896
	step [96/196], loss=64.1531
	step [97/196], loss=71.6181
	step [98/196], loss=45.3578
	step [99/196], loss=52.6704
	step [100/196], loss=85.1109
	step [101/196], loss=65.6967
	step [102/196], loss=80.9126
	step [103/196], loss=50.0191
	step [104/196], loss=65.1402
	step [105/196], loss=58.7037
	step [106/196], loss=76.4052
	step [107/196], loss=85.3270
	step [108/196], loss=73.1917
	step [109/196], loss=60.2920
	step [110/196], loss=67.4754
	step [111/196], loss=70.0718
	step [112/196], loss=65.5757
	step [113/196], loss=83.6249
	step [114/196], loss=70.8408
	step [115/196], loss=64.9198
	step [116/196], loss=57.7017
	step [117/196], loss=69.6600
	step [118/196], loss=61.8946
	step [119/196], loss=59.1046
	step [120/196], loss=64.1311
	step [121/196], loss=74.2530
	step [122/196], loss=62.6178
	step [123/196], loss=60.8369
	step [124/196], loss=66.1551
	step [125/196], loss=70.3037
	step [126/196], loss=68.3545
	step [127/196], loss=57.5766
	step [128/196], loss=68.2972
	step [129/196], loss=50.9232
	step [130/196], loss=66.8935
	step [131/196], loss=62.4872
	step [132/196], loss=56.6416
	step [133/196], loss=69.5673
	step [134/196], loss=80.5397
	step [135/196], loss=63.6172
	step [136/196], loss=75.7128
	step [137/196], loss=63.0342
	step [138/196], loss=67.8751
	step [139/196], loss=61.7631
	step [140/196], loss=76.7238
	step [141/196], loss=62.7640
	step [142/196], loss=67.7764
	step [143/196], loss=55.8657
	step [144/196], loss=67.1991
	step [145/196], loss=65.4902
	step [146/196], loss=50.8543
	step [147/196], loss=61.8047
	step [148/196], loss=56.0613
	step [149/196], loss=52.5593
	step [150/196], loss=73.9793
	step [151/196], loss=74.1868
	step [152/196], loss=54.0628
	step [153/196], loss=57.0864
	step [154/196], loss=65.0707
	step [155/196], loss=68.0608
	step [156/196], loss=65.5452
	step [157/196], loss=50.8757
	step [158/196], loss=65.7094
	step [159/196], loss=64.9328
	step [160/196], loss=73.0316
	step [161/196], loss=62.6178
	step [162/196], loss=66.3391
	step [163/196], loss=54.7242
	step [164/196], loss=67.1504
	step [165/196], loss=64.5647
	step [166/196], loss=62.3758
	step [167/196], loss=71.8452
	step [168/196], loss=58.8367
	step [169/196], loss=65.7516
	step [170/196], loss=55.4241
	step [171/196], loss=71.5105
	step [172/196], loss=63.2712
	step [173/196], loss=56.5174
	step [174/196], loss=61.5284
	step [175/196], loss=71.0459
	step [176/196], loss=65.2079
	step [177/196], loss=66.6847
	step [178/196], loss=75.9828
	step [179/196], loss=64.9948
	step [180/196], loss=63.5564
	step [181/196], loss=64.0573
	step [182/196], loss=70.2639
	step [183/196], loss=62.9933
	step [184/196], loss=57.4543
	step [185/196], loss=60.2761
	step [186/196], loss=67.5146
	step [187/196], loss=61.8667
	step [188/196], loss=63.5029
	step [189/196], loss=58.8705
	step [190/196], loss=58.6395
	step [191/196], loss=58.1168
	step [192/196], loss=74.2470
	step [193/196], loss=48.5400
	step [194/196], loss=69.0984
	step [195/196], loss=54.8899
	step [196/196], loss=13.2111
	Evaluating
	loss=0.0078, precision=0.4529, recall=0.9195, f1=0.6069
Training finished
best_f1: 0.6521579800889953
directing: Y rim_enhanced: True test_id 0
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15948 # image files with weight 15917
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4124 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/332], loss=370.3301
	step [2/332], loss=296.8805
	step [3/332], loss=244.4166
	step [4/332], loss=267.6763
	step [5/332], loss=254.9795
	step [6/332], loss=265.0385
	step [7/332], loss=204.1759
	step [8/332], loss=232.7916
	step [9/332], loss=210.7886
	step [10/332], loss=227.8556
	step [11/332], loss=205.9772
	step [12/332], loss=216.4364
	step [13/332], loss=239.3740
	step [14/332], loss=212.9179
	step [15/332], loss=185.8347
	step [16/332], loss=205.8673
	step [17/332], loss=205.9098
	step [18/332], loss=202.6114
	step [19/332], loss=191.1767
	step [20/332], loss=175.9193
	step [21/332], loss=183.6901
	step [22/332], loss=199.8528
	step [23/332], loss=174.2213
	step [24/332], loss=190.2624
	step [25/332], loss=181.6893
	step [26/332], loss=180.8914
	step [27/332], loss=184.8386
	step [28/332], loss=197.7527
	step [29/332], loss=168.2870
	step [30/332], loss=155.8119
	step [31/332], loss=187.9185
	step [32/332], loss=180.8359
	step [33/332], loss=186.8441
	step [34/332], loss=182.2721
	step [35/332], loss=167.3064
	step [36/332], loss=170.6674
	step [37/332], loss=168.5518
	step [38/332], loss=172.9080
	step [39/332], loss=178.5847
	step [40/332], loss=173.6755
	step [41/332], loss=159.9073
	step [42/332], loss=155.2077
	step [43/332], loss=158.7020
	step [44/332], loss=186.0754
	step [45/332], loss=169.1831
	step [46/332], loss=189.2374
	step [47/332], loss=151.1082
	step [48/332], loss=170.0322
	step [49/332], loss=160.9197
	step [50/332], loss=175.2696
	step [51/332], loss=174.4064
	step [52/332], loss=145.7038
	step [53/332], loss=167.8768
	step [54/332], loss=175.8491
	step [55/332], loss=162.6621
	step [56/332], loss=167.9028
	step [57/332], loss=165.1672
	step [58/332], loss=159.1707
	step [59/332], loss=149.5161
	step [60/332], loss=174.7180
	step [61/332], loss=158.5053
	step [62/332], loss=147.9996
	step [63/332], loss=154.3300
	step [64/332], loss=158.0267
	step [65/332], loss=185.3593
	step [66/332], loss=174.2494
	step [67/332], loss=175.7579
	step [68/332], loss=166.0755
	step [69/332], loss=179.7802
	step [70/332], loss=163.2398
	step [71/332], loss=163.6566
	step [72/332], loss=158.4469
	step [73/332], loss=170.3943
	step [74/332], loss=144.6962
	step [75/332], loss=159.5183
	step [76/332], loss=162.8085
	step [77/332], loss=162.2915
	step [78/332], loss=164.1479
	step [79/332], loss=139.5373
	step [80/332], loss=143.3521
	step [81/332], loss=158.4813
	step [82/332], loss=144.3127
	step [83/332], loss=147.6812
	step [84/332], loss=153.0255
	step [85/332], loss=172.0365
	step [86/332], loss=147.6676
	step [87/332], loss=158.8291
	step [88/332], loss=160.3594
	step [89/332], loss=154.2948
	step [90/332], loss=146.5590
	step [91/332], loss=150.6024
	step [92/332], loss=166.3716
	step [93/332], loss=152.5423
	step [94/332], loss=157.6105
	step [95/332], loss=167.6685
	step [96/332], loss=146.3392
	step [97/332], loss=162.8195
	step [98/332], loss=146.3877
	step [99/332], loss=134.7940
	step [100/332], loss=153.3675
	step [101/332], loss=151.0708
	step [102/332], loss=154.1076
	step [103/332], loss=163.4375
	step [104/332], loss=160.6300
	step [105/332], loss=154.7804
	step [106/332], loss=129.2368
	step [107/332], loss=143.7590
	step [108/332], loss=143.6137
	step [109/332], loss=157.2614
	step [110/332], loss=143.1993
	step [111/332], loss=143.6415
	step [112/332], loss=133.1624
	step [113/332], loss=135.1162
	step [114/332], loss=147.3306
	step [115/332], loss=137.7697
	step [116/332], loss=133.9900
	step [117/332], loss=130.1082
	step [118/332], loss=135.0967
	step [119/332], loss=146.6084
	step [120/332], loss=153.4734
	step [121/332], loss=146.7428
	step [122/332], loss=138.4397
	step [123/332], loss=147.6599
	step [124/332], loss=138.8522
	step [125/332], loss=149.0836
	step [126/332], loss=166.7671
	step [127/332], loss=136.1303
	step [128/332], loss=140.2897
	step [129/332], loss=142.0379
	step [130/332], loss=117.5702
	step [131/332], loss=131.7722
	step [132/332], loss=125.9066
	step [133/332], loss=162.2332
	step [134/332], loss=147.1770
	step [135/332], loss=129.8737
	step [136/332], loss=145.4189
	step [137/332], loss=148.2515
	step [138/332], loss=137.8917
	step [139/332], loss=157.6541
	step [140/332], loss=145.0733
	step [141/332], loss=125.8560
	step [142/332], loss=147.9280
	step [143/332], loss=140.2073
	step [144/332], loss=132.4294
	step [145/332], loss=169.2588
	step [146/332], loss=142.4956
	step [147/332], loss=143.0832
	step [148/332], loss=152.8360
	step [149/332], loss=136.8856
	step [150/332], loss=155.0585
	step [151/332], loss=132.6718
	step [152/332], loss=127.8477
	step [153/332], loss=133.4521
	step [154/332], loss=131.0485
	step [155/332], loss=128.3823
	step [156/332], loss=130.3272
	step [157/332], loss=144.6244
	step [158/332], loss=151.1446
	step [159/332], loss=149.0144
	step [160/332], loss=134.7122
	step [161/332], loss=140.9242
	step [162/332], loss=131.7989
	step [163/332], loss=134.4676
	step [164/332], loss=142.6404
	step [165/332], loss=137.7476
	step [166/332], loss=147.5815
	step [167/332], loss=143.0334
	step [168/332], loss=153.5586
	step [169/332], loss=129.0542
	step [170/332], loss=145.1062
	step [171/332], loss=124.8757
	step [172/332], loss=157.9175
	step [173/332], loss=142.1052
	step [174/332], loss=148.6932
	step [175/332], loss=128.2473
	step [176/332], loss=148.4091
	step [177/332], loss=136.2137
	step [178/332], loss=120.2913
	step [179/332], loss=131.7354
	step [180/332], loss=152.4618
	step [181/332], loss=147.6854
	step [182/332], loss=104.8774
	step [183/332], loss=121.5091
	step [184/332], loss=147.0347
	step [185/332], loss=126.1942
	step [186/332], loss=138.6966
	step [187/332], loss=154.8672
	step [188/332], loss=146.0492
	step [189/332], loss=135.1836
	step [190/332], loss=138.3340
	step [191/332], loss=118.9349
	step [192/332], loss=136.1533
	step [193/332], loss=140.7828
	step [194/332], loss=122.4999
	step [195/332], loss=126.5919
	step [196/332], loss=139.3113
	step [197/332], loss=149.6595
	step [198/332], loss=125.5076
	step [199/332], loss=139.6787
	step [200/332], loss=126.2784
	step [201/332], loss=145.0443
	step [202/332], loss=117.9982
	step [203/332], loss=123.5024
	step [204/332], loss=126.2532
	step [205/332], loss=129.7423
	step [206/332], loss=123.1295
	step [207/332], loss=129.5350
	step [208/332], loss=122.8451
	step [209/332], loss=143.8000
	step [210/332], loss=117.5791
	step [211/332], loss=138.1046
	step [212/332], loss=133.1557
	step [213/332], loss=159.3494
	step [214/332], loss=129.2543
	step [215/332], loss=116.5585
	step [216/332], loss=124.7088
	step [217/332], loss=141.1245
	step [218/332], loss=115.5218
	step [219/332], loss=147.8971
	step [220/332], loss=115.6896
	step [221/332], loss=111.8875
	step [222/332], loss=127.7479
	step [223/332], loss=113.3168
	step [224/332], loss=144.0960
	step [225/332], loss=117.0222
	step [226/332], loss=126.7064
	step [227/332], loss=132.4469
	step [228/332], loss=121.7511
	step [229/332], loss=116.7504
	step [230/332], loss=128.2265
	step [231/332], loss=143.5324
	step [232/332], loss=118.6439
	step [233/332], loss=135.7938
	step [234/332], loss=133.8174
	step [235/332], loss=113.4969
	step [236/332], loss=130.7478
	step [237/332], loss=126.3730
	step [238/332], loss=120.6891
	step [239/332], loss=110.4663
	step [240/332], loss=154.1214
	step [241/332], loss=121.9660
	step [242/332], loss=102.0061
	step [243/332], loss=123.7157
	step [244/332], loss=129.6689
	step [245/332], loss=128.2894
	step [246/332], loss=131.2114
	step [247/332], loss=135.9793
	step [248/332], loss=123.3276
	step [249/332], loss=134.9704
	step [250/332], loss=118.8496
	step [251/332], loss=139.0410
	step [252/332], loss=129.5670
	step [253/332], loss=125.4759
	step [254/332], loss=116.5895
	step [255/332], loss=134.3464
	step [256/332], loss=139.7067
	step [257/332], loss=112.9368
	step [258/332], loss=145.0628
	step [259/332], loss=135.6594
	step [260/332], loss=117.0388
	step [261/332], loss=133.4775
	step [262/332], loss=126.8527
	step [263/332], loss=129.3997
	step [264/332], loss=134.2974
	step [265/332], loss=116.8773
	step [266/332], loss=139.0848
	step [267/332], loss=140.8879
	step [268/332], loss=125.2938
	step [269/332], loss=123.8821
	step [270/332], loss=126.8659
	step [271/332], loss=136.2227
	step [272/332], loss=141.6900
	step [273/332], loss=113.9079
	step [274/332], loss=121.4556
	step [275/332], loss=124.5019
	step [276/332], loss=109.0744
	step [277/332], loss=123.1371
	step [278/332], loss=128.3424
	step [279/332], loss=117.0768
	step [280/332], loss=132.9055
	step [281/332], loss=150.7317
	step [282/332], loss=122.6941
	step [283/332], loss=126.2280
	step [284/332], loss=104.2633
	step [285/332], loss=128.2160
	step [286/332], loss=102.2923
	step [287/332], loss=108.6027
	step [288/332], loss=121.4337
	step [289/332], loss=102.8679
	step [290/332], loss=118.7416
	step [291/332], loss=123.9125
	step [292/332], loss=106.0504
	step [293/332], loss=112.8480
	step [294/332], loss=119.6376
	step [295/332], loss=112.5356
	step [296/332], loss=114.8168
	step [297/332], loss=124.0341
	step [298/332], loss=123.4039
	step [299/332], loss=125.2106
	step [300/332], loss=113.0885
	step [301/332], loss=114.3123
	step [302/332], loss=129.6784
	step [303/332], loss=132.1803
	step [304/332], loss=129.1173
	step [305/332], loss=130.4599
	step [306/332], loss=127.1749
	step [307/332], loss=125.8035
	step [308/332], loss=136.9719
	step [309/332], loss=118.0689
	step [310/332], loss=140.7197
	step [311/332], loss=101.4047
	step [312/332], loss=134.7479
	step [313/332], loss=129.1727
	step [314/332], loss=119.3343
	step [315/332], loss=117.1365
	step [316/332], loss=106.9409
	step [317/332], loss=137.0651
	step [318/332], loss=119.7028
	step [319/332], loss=119.3580
	step [320/332], loss=112.8085
	step [321/332], loss=115.9413
	step [322/332], loss=122.2890
	step [323/332], loss=124.8378
	step [324/332], loss=107.9595
	step [325/332], loss=115.4672
	step [326/332], loss=115.3236
	step [327/332], loss=109.3218
	step [328/332], loss=122.5248
	step [329/332], loss=124.0760
	step [330/332], loss=125.5493
	step [331/332], loss=108.5395
	step [332/332], loss=73.0478
	Evaluating
	loss=0.2411, precision=0.3023, recall=0.9276, f1=0.4560
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/332], loss=104.7874
	step [2/332], loss=117.5762
	step [3/332], loss=102.6651
	step [4/332], loss=128.0616
	step [5/332], loss=127.2046
	step [6/332], loss=128.0586
	step [7/332], loss=101.1871
	step [8/332], loss=110.9255
	step [9/332], loss=121.3064
	step [10/332], loss=112.2567
	step [11/332], loss=113.8867
	step [12/332], loss=135.0402
	step [13/332], loss=136.9821
	step [14/332], loss=112.5387
	step [15/332], loss=132.1649
	step [16/332], loss=103.4401
	step [17/332], loss=119.2665
	step [18/332], loss=109.4184
	step [19/332], loss=107.1089
	step [20/332], loss=152.6042
	step [21/332], loss=111.7265
	step [22/332], loss=123.5772
	step [23/332], loss=122.8323
	step [24/332], loss=136.5755
	step [25/332], loss=126.2327
	step [26/332], loss=104.8074
	step [27/332], loss=134.6847
	step [28/332], loss=143.0594
	step [29/332], loss=108.3857
	step [30/332], loss=108.6969
	step [31/332], loss=114.2100
	step [32/332], loss=137.1535
	step [33/332], loss=132.0078
	step [34/332], loss=111.1198
	step [35/332], loss=98.2216
	step [36/332], loss=94.3003
	step [37/332], loss=123.9118
	step [38/332], loss=128.7102
	step [39/332], loss=140.8644
	step [40/332], loss=105.1610
	step [41/332], loss=109.1671
	step [42/332], loss=116.6123
	step [43/332], loss=119.9003
	step [44/332], loss=117.4650
	step [45/332], loss=117.8502
	step [46/332], loss=109.3739
	step [47/332], loss=114.9230
	step [48/332], loss=120.5294
	step [49/332], loss=122.9133
	step [50/332], loss=115.8097
	step [51/332], loss=119.4908
	step [52/332], loss=113.2680
	step [53/332], loss=108.1348
	step [54/332], loss=120.1988
	step [55/332], loss=124.0950
	step [56/332], loss=123.6434
	step [57/332], loss=118.6125
	step [58/332], loss=129.2750
	step [59/332], loss=102.0436
	step [60/332], loss=108.7372
	step [61/332], loss=112.6577
	step [62/332], loss=121.2617
	step [63/332], loss=92.4911
	step [64/332], loss=109.2213
	step [65/332], loss=114.4794
	step [66/332], loss=108.1938
	step [67/332], loss=130.5682
	step [68/332], loss=125.9122
	step [69/332], loss=115.6634
	step [70/332], loss=101.7338
	step [71/332], loss=96.9064
	step [72/332], loss=111.0068
	step [73/332], loss=130.2923
	step [74/332], loss=118.6832
	step [75/332], loss=103.6192
	step [76/332], loss=117.2280
	step [77/332], loss=88.4272
	step [78/332], loss=132.7162
	step [79/332], loss=98.3060
	step [80/332], loss=102.3131
	step [81/332], loss=126.5654
	step [82/332], loss=98.2372
	step [83/332], loss=111.6765
	step [84/332], loss=105.7393
	step [85/332], loss=116.8924
	step [86/332], loss=110.6596
	step [87/332], loss=109.4494
	step [88/332], loss=116.5741
	step [89/332], loss=94.3500
	step [90/332], loss=117.3315
	step [91/332], loss=129.4101
	step [92/332], loss=120.5058
	step [93/332], loss=116.3294
	step [94/332], loss=128.3225
	step [95/332], loss=109.3474
	step [96/332], loss=121.6172
	step [97/332], loss=136.2085
	step [98/332], loss=117.9472
	step [99/332], loss=94.3683
	step [100/332], loss=108.6112
	step [101/332], loss=109.7852
	step [102/332], loss=115.3280
	step [103/332], loss=120.3038
	step [104/332], loss=120.5693
	step [105/332], loss=92.4284
	step [106/332], loss=117.2734
	step [107/332], loss=113.3731
	step [108/332], loss=113.4882
	step [109/332], loss=126.1238
	step [110/332], loss=87.2758
	step [111/332], loss=93.0374
	step [112/332], loss=116.1123
	step [113/332], loss=110.0812
	step [114/332], loss=114.2447
	step [115/332], loss=126.4265
	step [116/332], loss=120.0916
	step [117/332], loss=126.6454
	step [118/332], loss=128.7962
	step [119/332], loss=119.4595
	step [120/332], loss=118.7771
	step [121/332], loss=112.9627
	step [122/332], loss=117.2130
	step [123/332], loss=106.9723
	step [124/332], loss=118.3254
	step [125/332], loss=100.6706
	step [126/332], loss=113.0852
	step [127/332], loss=122.5614
	step [128/332], loss=105.2891
	step [129/332], loss=153.5953
	step [130/332], loss=100.3478
	step [131/332], loss=104.7405
	step [132/332], loss=124.8168
	step [133/332], loss=100.2032
	step [134/332], loss=126.2604
	step [135/332], loss=129.7820
	step [136/332], loss=118.3234
	step [137/332], loss=98.4324
	step [138/332], loss=102.2289
	step [139/332], loss=124.5827
	step [140/332], loss=105.2128
	step [141/332], loss=100.2772
	step [142/332], loss=112.8558
	step [143/332], loss=125.6706
	step [144/332], loss=109.9455
	step [145/332], loss=107.9132
	step [146/332], loss=110.1837
	step [147/332], loss=113.2864
	step [148/332], loss=117.8920
	step [149/332], loss=120.0410
	step [150/332], loss=93.1072
	step [151/332], loss=112.1592
	step [152/332], loss=113.6529
	step [153/332], loss=103.8067
	step [154/332], loss=115.6139
	step [155/332], loss=120.1778
	step [156/332], loss=101.5173
	step [157/332], loss=86.4054
	step [158/332], loss=118.5387
	step [159/332], loss=102.2662
	step [160/332], loss=104.3418
	step [161/332], loss=113.9546
	step [162/332], loss=112.2606
	step [163/332], loss=109.1301
	step [164/332], loss=109.6437
	step [165/332], loss=102.5339
	step [166/332], loss=111.5225
	step [167/332], loss=96.6161
	step [168/332], loss=120.5726
	step [169/332], loss=130.9501
	step [170/332], loss=111.3062
	step [171/332], loss=119.9905
	step [172/332], loss=109.2382
	step [173/332], loss=93.2297
	step [174/332], loss=112.9884
	step [175/332], loss=108.5041
	step [176/332], loss=119.9204
	step [177/332], loss=105.6916
	step [178/332], loss=109.4016
	step [179/332], loss=115.2199
	step [180/332], loss=106.9510
	step [181/332], loss=114.7933
	step [182/332], loss=113.6767
	step [183/332], loss=113.9918
	step [184/332], loss=86.2655
	step [185/332], loss=108.4756
	step [186/332], loss=84.8882
	step [187/332], loss=96.3984
	step [188/332], loss=110.0272
	step [189/332], loss=98.1358
	step [190/332], loss=117.8015
	step [191/332], loss=105.3535
	step [192/332], loss=113.9071
	step [193/332], loss=104.4424
	step [194/332], loss=93.2925
	step [195/332], loss=95.7259
	step [196/332], loss=124.5829
	step [197/332], loss=93.7625
	step [198/332], loss=93.6655
	step [199/332], loss=87.8543
	step [200/332], loss=98.4484
	step [201/332], loss=97.1174
	step [202/332], loss=105.8028
	step [203/332], loss=96.7413
	step [204/332], loss=110.0964
	step [205/332], loss=97.6089
	step [206/332], loss=113.9028
	step [207/332], loss=93.2165
	step [208/332], loss=100.2060
	step [209/332], loss=102.8036
	step [210/332], loss=116.9669
	step [211/332], loss=106.0147
	step [212/332], loss=106.0833
	step [213/332], loss=94.0261
	step [214/332], loss=106.0424
	step [215/332], loss=100.0859
	step [216/332], loss=103.4072
	step [217/332], loss=96.4557
	step [218/332], loss=98.8478
	step [219/332], loss=116.6241
	step [220/332], loss=94.9641
	step [221/332], loss=111.7435
	step [222/332], loss=95.6341
	step [223/332], loss=118.2497
	step [224/332], loss=106.3247
	step [225/332], loss=98.2537
	step [226/332], loss=123.7604
	step [227/332], loss=89.7946
	step [228/332], loss=117.2725
	step [229/332], loss=107.3331
	step [230/332], loss=97.1133
	step [231/332], loss=119.4037
	step [232/332], loss=115.5164
	step [233/332], loss=107.0807
	step [234/332], loss=103.8444
	step [235/332], loss=121.1421
	step [236/332], loss=91.0042
	step [237/332], loss=125.1300
	step [238/332], loss=99.2034
	step [239/332], loss=88.4781
	step [240/332], loss=106.0332
	step [241/332], loss=84.4053
	step [242/332], loss=110.9931
	step [243/332], loss=98.8218
	step [244/332], loss=135.4564
	step [245/332], loss=95.2071
	step [246/332], loss=100.6241
	step [247/332], loss=119.1725
	step [248/332], loss=96.2897
	step [249/332], loss=89.4932
	step [250/332], loss=103.0977
	step [251/332], loss=111.5716
	step [252/332], loss=119.4675
	step [253/332], loss=80.5645
	step [254/332], loss=111.9101
	step [255/332], loss=111.9424
	step [256/332], loss=125.3913
	step [257/332], loss=108.9551
	step [258/332], loss=96.7666
	step [259/332], loss=104.6903
	step [260/332], loss=113.4239
	step [261/332], loss=100.7437
	step [262/332], loss=104.9191
	step [263/332], loss=112.5772
	step [264/332], loss=83.5352
	step [265/332], loss=96.1309
	step [266/332], loss=129.6738
	step [267/332], loss=93.2562
	step [268/332], loss=99.7911
	step [269/332], loss=102.8671
	step [270/332], loss=121.1581
	step [271/332], loss=103.5145
	step [272/332], loss=102.9298
	step [273/332], loss=110.7050
	step [274/332], loss=89.8648
	step [275/332], loss=108.9998
	step [276/332], loss=96.6448
	step [277/332], loss=97.1389
	step [278/332], loss=90.5757
	step [279/332], loss=91.9404
	step [280/332], loss=116.3642
	step [281/332], loss=93.3979
	step [282/332], loss=105.2094
	step [283/332], loss=116.6751
	step [284/332], loss=100.7292
	step [285/332], loss=107.8828
	step [286/332], loss=94.3987
	step [287/332], loss=86.2697
	step [288/332], loss=109.2553
	step [289/332], loss=95.9268
	step [290/332], loss=107.9217
	step [291/332], loss=94.0513
	step [292/332], loss=101.8948
	step [293/332], loss=102.0361
	step [294/332], loss=86.3096
	step [295/332], loss=107.5141
	step [296/332], loss=115.6012
	step [297/332], loss=115.2041
	step [298/332], loss=101.5048
	step [299/332], loss=97.0591
	step [300/332], loss=112.1222
	step [301/332], loss=107.0460
	step [302/332], loss=93.0230
	step [303/332], loss=130.8756
	step [304/332], loss=94.6950
	step [305/332], loss=109.7372
	step [306/332], loss=90.5164
	step [307/332], loss=119.9448
	step [308/332], loss=101.0926
	step [309/332], loss=106.9064
	step [310/332], loss=96.2001
	step [311/332], loss=87.8857
	step [312/332], loss=98.6117
	step [313/332], loss=88.0674
	step [314/332], loss=110.4477
	step [315/332], loss=104.2063
	step [316/332], loss=98.9636
	step [317/332], loss=95.5280
	step [318/332], loss=118.2698
	step [319/332], loss=118.6886
	step [320/332], loss=89.9449
	step [321/332], loss=96.3802
	step [322/332], loss=129.2390
	step [323/332], loss=104.7926
	step [324/332], loss=90.6336
	step [325/332], loss=122.8822
	step [326/332], loss=109.9172
	step [327/332], loss=95.3162
	step [328/332], loss=107.3737
	step [329/332], loss=106.0758
	step [330/332], loss=116.9362
	step [331/332], loss=111.8280
	step [332/332], loss=62.6318
	Evaluating
	loss=0.1414, precision=0.3844, recall=0.8991, f1=0.5386
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/332], loss=105.7814
	step [2/332], loss=113.5491
	step [3/332], loss=108.8514
	step [4/332], loss=98.4013
	step [5/332], loss=95.1051
	step [6/332], loss=106.0008
	step [7/332], loss=102.6996
	step [8/332], loss=101.5514
	step [9/332], loss=103.8993
	step [10/332], loss=105.3812
	step [11/332], loss=97.8655
	step [12/332], loss=99.8079
	step [13/332], loss=106.4386
	step [14/332], loss=86.0617
	step [15/332], loss=106.5088
	step [16/332], loss=125.1432
	step [17/332], loss=102.2636
	step [18/332], loss=95.2917
	step [19/332], loss=103.5644
	step [20/332], loss=97.1880
	step [21/332], loss=98.6097
	step [22/332], loss=120.3946
	step [23/332], loss=88.2431
	step [24/332], loss=112.5452
	step [25/332], loss=97.8746
	step [26/332], loss=90.0692
	step [27/332], loss=87.4180
	step [28/332], loss=97.2230
	step [29/332], loss=97.1003
	step [30/332], loss=105.5438
	step [31/332], loss=82.6866
	step [32/332], loss=100.4129
	step [33/332], loss=107.0644
	step [34/332], loss=81.8145
	step [35/332], loss=116.3111
	step [36/332], loss=84.9659
	step [37/332], loss=98.9789
	step [38/332], loss=103.2616
	step [39/332], loss=85.4557
	step [40/332], loss=92.6829
	step [41/332], loss=94.4724
	step [42/332], loss=97.1799
	step [43/332], loss=92.4076
	step [44/332], loss=96.5974
	step [45/332], loss=99.2098
	step [46/332], loss=106.1494
	step [47/332], loss=97.9357
	step [48/332], loss=116.0761
	step [49/332], loss=99.9859
	step [50/332], loss=105.1746
	step [51/332], loss=106.7712
	step [52/332], loss=96.2676
	step [53/332], loss=111.5633
	step [54/332], loss=81.8387
	step [55/332], loss=93.3369
	step [56/332], loss=92.6832
	step [57/332], loss=105.3611
	step [58/332], loss=90.0945
	step [59/332], loss=105.7253
	step [60/332], loss=117.0683
	step [61/332], loss=91.3531
	step [62/332], loss=111.5768
	step [63/332], loss=107.0103
	step [64/332], loss=109.3511
	step [65/332], loss=96.2742
	step [66/332], loss=97.1825
	step [67/332], loss=93.8314
	step [68/332], loss=101.3714
	step [69/332], loss=112.9603
	step [70/332], loss=93.1931
	step [71/332], loss=101.1026
	step [72/332], loss=110.0853
	step [73/332], loss=127.7300
	step [74/332], loss=89.1935
	step [75/332], loss=102.5092
	step [76/332], loss=96.1967
	step [77/332], loss=111.0291
	step [78/332], loss=84.3818
	step [79/332], loss=125.4284
	step [80/332], loss=95.8075
	step [81/332], loss=106.9907
	step [82/332], loss=111.8948
	step [83/332], loss=97.0945
	step [84/332], loss=99.0226
	step [85/332], loss=92.9090
	step [86/332], loss=94.4046
	step [87/332], loss=92.3368
	step [88/332], loss=96.4452
	step [89/332], loss=84.4245
	step [90/332], loss=95.6771
	step [91/332], loss=103.7018
	step [92/332], loss=109.4973
	step [93/332], loss=117.2594
	step [94/332], loss=105.2970
	step [95/332], loss=105.0985
	step [96/332], loss=83.9257
	step [97/332], loss=89.9051
	step [98/332], loss=91.2746
	step [99/332], loss=92.2286
	step [100/332], loss=100.3347
	step [101/332], loss=97.2682
	step [102/332], loss=91.0381
	step [103/332], loss=88.7840
	step [104/332], loss=127.8301
	step [105/332], loss=94.2643
	step [106/332], loss=102.5723
	step [107/332], loss=88.1221
	step [108/332], loss=100.4388
	step [109/332], loss=108.3092
	step [110/332], loss=102.3395
	step [111/332], loss=91.3731
	step [112/332], loss=100.0940
	step [113/332], loss=109.6018
	step [114/332], loss=109.2972
	step [115/332], loss=94.4735
	step [116/332], loss=90.6116
	step [117/332], loss=102.4006
	step [118/332], loss=93.9476
	step [119/332], loss=89.2626
	step [120/332], loss=105.9969
	step [121/332], loss=93.6935
	step [122/332], loss=96.2472
	step [123/332], loss=105.0164
	step [124/332], loss=105.4197
	step [125/332], loss=83.7630
	step [126/332], loss=84.3729
	step [127/332], loss=76.6519
	step [128/332], loss=105.1305
	step [129/332], loss=80.1095
	step [130/332], loss=84.8902
	step [131/332], loss=115.0591
	step [132/332], loss=95.8201
	step [133/332], loss=103.1733
	step [134/332], loss=94.5651
	step [135/332], loss=108.5617
	step [136/332], loss=87.9248
	step [137/332], loss=92.7981
	step [138/332], loss=92.3058
	step [139/332], loss=96.3163
	step [140/332], loss=97.3698
	step [141/332], loss=97.5419
	step [142/332], loss=91.7799
	step [143/332], loss=104.1600
	step [144/332], loss=111.9999
	step [145/332], loss=100.7354
	step [146/332], loss=113.7120
	step [147/332], loss=116.5302
	step [148/332], loss=85.3665
	step [149/332], loss=86.0839
	step [150/332], loss=89.5571
	step [151/332], loss=99.5823
	step [152/332], loss=92.6930
	step [153/332], loss=108.4839
	step [154/332], loss=115.5784
	step [155/332], loss=110.0167
	step [156/332], loss=93.7334
	step [157/332], loss=93.2047
	step [158/332], loss=111.4143
	step [159/332], loss=109.4813
	step [160/332], loss=86.7397
	step [161/332], loss=91.4282
	step [162/332], loss=100.3926
	step [163/332], loss=104.4837
	step [164/332], loss=103.5943
	step [165/332], loss=101.2794
	step [166/332], loss=102.0917
	step [167/332], loss=106.8626
	step [168/332], loss=97.2250
	step [169/332], loss=100.0916
	step [170/332], loss=87.6617
	step [171/332], loss=100.1000
	step [172/332], loss=107.3110
	step [173/332], loss=95.5098
	step [174/332], loss=92.7984
	step [175/332], loss=94.0604
	step [176/332], loss=108.3719
	step [177/332], loss=100.3280
	step [178/332], loss=91.9636
	step [179/332], loss=93.7633
	step [180/332], loss=114.9791
	step [181/332], loss=99.3419
	step [182/332], loss=94.0040
	step [183/332], loss=104.3905
	step [184/332], loss=84.7986
	step [185/332], loss=105.5526
	step [186/332], loss=79.1183
	step [187/332], loss=108.8368
	step [188/332], loss=85.4733
	step [189/332], loss=97.4485
	step [190/332], loss=86.8080
	step [191/332], loss=110.2773
	step [192/332], loss=86.4929
	step [193/332], loss=93.9790
	step [194/332], loss=78.6755
	step [195/332], loss=82.2230
	step [196/332], loss=84.5962
	step [197/332], loss=90.1733
	step [198/332], loss=123.6525
	step [199/332], loss=103.5285
	step [200/332], loss=104.6491
	step [201/332], loss=104.5353
	step [202/332], loss=95.5103
	step [203/332], loss=93.7228
	step [204/332], loss=86.5768
	step [205/332], loss=100.9502
	step [206/332], loss=97.4185
	step [207/332], loss=84.0671
	step [208/332], loss=98.6250
	step [209/332], loss=93.3046
	step [210/332], loss=83.7536
	step [211/332], loss=84.9167
	step [212/332], loss=75.4535
	step [213/332], loss=89.1040
	step [214/332], loss=93.1059
	step [215/332], loss=104.8694
	step [216/332], loss=103.7494
	step [217/332], loss=106.0154
	step [218/332], loss=97.2523
	step [219/332], loss=86.6163
	step [220/332], loss=96.5063
	step [221/332], loss=92.2804
	step [222/332], loss=90.8279
	step [223/332], loss=111.2680
	step [224/332], loss=119.6928
	step [225/332], loss=84.4412
	step [226/332], loss=113.8316
	step [227/332], loss=113.6663
	step [228/332], loss=106.1622
	step [229/332], loss=80.0721
	step [230/332], loss=102.1153
	step [231/332], loss=92.9304
	step [232/332], loss=88.4234
	step [233/332], loss=97.2271
	step [234/332], loss=104.6458
	step [235/332], loss=114.4734
	step [236/332], loss=81.6179
	step [237/332], loss=90.5307
	step [238/332], loss=82.3550
	step [239/332], loss=100.7344
	step [240/332], loss=90.6617
	step [241/332], loss=93.1195
	step [242/332], loss=80.6641
	step [243/332], loss=99.5379
	step [244/332], loss=94.3612
	step [245/332], loss=86.2815
	step [246/332], loss=86.9818
	step [247/332], loss=73.7010
	step [248/332], loss=90.7979
	step [249/332], loss=99.6572
	step [250/332], loss=88.2907
	step [251/332], loss=102.1750
	step [252/332], loss=112.2106
	step [253/332], loss=105.2341
	step [254/332], loss=75.3484
	step [255/332], loss=89.0259
	step [256/332], loss=106.5096
	step [257/332], loss=106.3546
	step [258/332], loss=110.0438
	step [259/332], loss=76.4308
	step [260/332], loss=114.8850
	step [261/332], loss=106.7271
	step [262/332], loss=100.9306
	step [263/332], loss=120.6962
	step [264/332], loss=88.1732
	step [265/332], loss=78.8207
	step [266/332], loss=79.9486
	step [267/332], loss=104.0299
	step [268/332], loss=91.6106
	step [269/332], loss=88.1481
	step [270/332], loss=102.0293
	step [271/332], loss=87.3520
	step [272/332], loss=101.9446
	step [273/332], loss=94.5737
	step [274/332], loss=110.9520
	step [275/332], loss=98.4325
	step [276/332], loss=105.5280
	step [277/332], loss=85.1233
	step [278/332], loss=82.7296
	step [279/332], loss=105.8898
	step [280/332], loss=105.6866
	step [281/332], loss=94.5820
	step [282/332], loss=103.3150
	step [283/332], loss=97.5399
	step [284/332], loss=90.6568
	step [285/332], loss=90.6959
	step [286/332], loss=98.8206
	step [287/332], loss=103.8227
	step [288/332], loss=97.1507
	step [289/332], loss=88.3542
	step [290/332], loss=101.8697
	step [291/332], loss=107.9583
	step [292/332], loss=103.5683
	step [293/332], loss=74.0121
	step [294/332], loss=89.8572
	step [295/332], loss=112.3123
	step [296/332], loss=112.1948
	step [297/332], loss=85.7939
	step [298/332], loss=95.5262
	step [299/332], loss=98.3903
	step [300/332], loss=96.9149
	step [301/332], loss=91.2686
	step [302/332], loss=106.2274
	step [303/332], loss=113.3571
	step [304/332], loss=92.6688
	step [305/332], loss=77.4777
	step [306/332], loss=99.6691
	step [307/332], loss=79.0676
	step [308/332], loss=112.0154
	step [309/332], loss=93.8748
	step [310/332], loss=100.0905
	step [311/332], loss=97.8238
	step [312/332], loss=89.0598
	step [313/332], loss=96.3124
	step [314/332], loss=100.6534
	step [315/332], loss=81.5380
	step [316/332], loss=100.6092
	step [317/332], loss=113.7331
	step [318/332], loss=98.3121
	step [319/332], loss=80.5350
	step [320/332], loss=89.3252
	step [321/332], loss=104.7886
	step [322/332], loss=83.4547
	step [323/332], loss=76.9464
	step [324/332], loss=94.3037
	step [325/332], loss=95.2714
	step [326/332], loss=84.7319
	step [327/332], loss=93.0350
	step [328/332], loss=101.1380
	step [329/332], loss=113.4594
	step [330/332], loss=81.1678
	step [331/332], loss=79.2865
	step [332/332], loss=77.8789
	Evaluating
	loss=0.0946, precision=0.3094, recall=0.9207, f1=0.4631
Training epoch 4
	step [1/332], loss=93.1639
	step [2/332], loss=102.1114
	step [3/332], loss=110.4833
	step [4/332], loss=89.4069
	step [5/332], loss=86.3235
	step [6/332], loss=82.2900
	step [7/332], loss=91.9312
	step [8/332], loss=88.3872
	step [9/332], loss=85.1119
	step [10/332], loss=76.2308
	step [11/332], loss=110.2991
	step [12/332], loss=106.7769
	step [13/332], loss=102.8108
	step [14/332], loss=86.2384
	step [15/332], loss=81.2167
	step [16/332], loss=101.3024
	step [17/332], loss=109.4065
	step [18/332], loss=97.2457
	step [19/332], loss=105.6289
	step [20/332], loss=95.0932
	step [21/332], loss=77.1797
	step [22/332], loss=103.5968
	step [23/332], loss=104.1587
	step [24/332], loss=74.7067
	step [25/332], loss=85.0630
	step [26/332], loss=90.5919
	step [27/332], loss=88.6717
	step [28/332], loss=91.7888
	step [29/332], loss=103.3010
	step [30/332], loss=104.9979
	step [31/332], loss=91.2746
	step [32/332], loss=88.4986
	step [33/332], loss=79.5982
	step [34/332], loss=93.0878
	step [35/332], loss=87.8911
	step [36/332], loss=94.3342
	step [37/332], loss=101.0190
	step [38/332], loss=82.5224
	step [39/332], loss=103.1531
	step [40/332], loss=109.5912
	step [41/332], loss=89.1275
	step [42/332], loss=89.3952
	step [43/332], loss=78.5972
	step [44/332], loss=93.1417
	step [45/332], loss=81.7592
	step [46/332], loss=93.4400
	step [47/332], loss=78.4617
	step [48/332], loss=77.6082
	step [49/332], loss=99.2264
	step [50/332], loss=95.2355
	step [51/332], loss=92.6837
	step [52/332], loss=103.8421
	step [53/332], loss=92.3268
	step [54/332], loss=78.0002
	step [55/332], loss=90.3203
	step [56/332], loss=93.8826
	step [57/332], loss=86.7764
	step [58/332], loss=95.3698
	step [59/332], loss=102.6993
	step [60/332], loss=96.6956
	step [61/332], loss=106.3472
	step [62/332], loss=77.6692
	step [63/332], loss=87.0460
	step [64/332], loss=103.9064
	step [65/332], loss=90.4507
	step [66/332], loss=100.9750
	step [67/332], loss=89.3116
	step [68/332], loss=92.5723
	step [69/332], loss=98.1980
	step [70/332], loss=92.7979
	step [71/332], loss=77.0744
	step [72/332], loss=95.6487
	step [73/332], loss=90.9557
	step [74/332], loss=91.4725
	step [75/332], loss=94.0137
	step [76/332], loss=79.1446
	step [77/332], loss=93.8530
	step [78/332], loss=75.7379
	step [79/332], loss=84.2563
	step [80/332], loss=97.3212
	step [81/332], loss=92.3755
	step [82/332], loss=80.2475
	step [83/332], loss=71.9375
	step [84/332], loss=80.7144
	step [85/332], loss=96.8713
	step [86/332], loss=88.2316
	step [87/332], loss=82.1660
	step [88/332], loss=107.6752
	step [89/332], loss=70.5143
	step [90/332], loss=95.4668
	step [91/332], loss=111.1519
	step [92/332], loss=93.7294
	step [93/332], loss=78.7343
	step [94/332], loss=101.4918
	step [95/332], loss=77.4726
	step [96/332], loss=66.6185
	step [97/332], loss=99.7808
	step [98/332], loss=85.4129
	step [99/332], loss=94.3913
	step [100/332], loss=71.3652
	step [101/332], loss=79.7326
	step [102/332], loss=100.6658
	step [103/332], loss=94.1391
	step [104/332], loss=93.8119
	step [105/332], loss=86.2625
	step [106/332], loss=81.3358
	step [107/332], loss=80.8070
	step [108/332], loss=100.6971
	step [109/332], loss=96.6336
	step [110/332], loss=102.8047
	step [111/332], loss=94.0745
	step [112/332], loss=83.6029
	step [113/332], loss=87.7338
	step [114/332], loss=102.0922
	step [115/332], loss=96.6987
	step [116/332], loss=92.8637
	step [117/332], loss=90.7442
	step [118/332], loss=104.3023
	step [119/332], loss=108.1848
	step [120/332], loss=87.4715
	step [121/332], loss=86.3942
	step [122/332], loss=86.7820
	step [123/332], loss=84.0541
	step [124/332], loss=78.0366
	step [125/332], loss=80.4681
	step [126/332], loss=102.8828
	step [127/332], loss=103.3728
	step [128/332], loss=84.0051
	step [129/332], loss=107.8167
	step [130/332], loss=70.8625
	step [131/332], loss=90.1281
	step [132/332], loss=91.8238
	step [133/332], loss=121.2923
	step [134/332], loss=101.4342
	step [135/332], loss=102.1215
	step [136/332], loss=97.9862
	step [137/332], loss=109.9873
	step [138/332], loss=99.0202
	step [139/332], loss=77.2325
	step [140/332], loss=99.2953
	step [141/332], loss=92.3270
	step [142/332], loss=73.1602
	step [143/332], loss=86.2487
	step [144/332], loss=95.9853
	step [145/332], loss=80.8262
	step [146/332], loss=99.0643
	step [147/332], loss=88.2300
	step [148/332], loss=111.1372
	step [149/332], loss=79.2547
	step [150/332], loss=92.0224
	step [151/332], loss=103.9376
	step [152/332], loss=96.2105
	step [153/332], loss=81.8270
	step [154/332], loss=93.7725
	step [155/332], loss=88.2274
	step [156/332], loss=86.7960
	step [157/332], loss=101.9875
	step [158/332], loss=81.0001
	step [159/332], loss=73.6731
	step [160/332], loss=99.7408
	step [161/332], loss=80.2470
	step [162/332], loss=82.3081
	step [163/332], loss=97.0927
	step [164/332], loss=80.6516
	step [165/332], loss=91.1833
	step [166/332], loss=97.4559
	step [167/332], loss=96.4569
	step [168/332], loss=83.8512
	step [169/332], loss=89.6954
	step [170/332], loss=87.0063
	step [171/332], loss=90.9937
	step [172/332], loss=100.0501
	step [173/332], loss=88.9417
	step [174/332], loss=99.5336
	step [175/332], loss=79.5725
	step [176/332], loss=81.3006
	step [177/332], loss=84.7679
	step [178/332], loss=106.4045
	step [179/332], loss=80.3227
	step [180/332], loss=83.4517
	step [181/332], loss=93.3677
	step [182/332], loss=94.0094
	step [183/332], loss=98.4992
	step [184/332], loss=88.5864
	step [185/332], loss=94.5832
	step [186/332], loss=101.1033
	step [187/332], loss=88.0770
	step [188/332], loss=102.6739
	step [189/332], loss=84.7350
	step [190/332], loss=88.6132
	step [191/332], loss=98.5970
	step [192/332], loss=93.8784
	step [193/332], loss=89.5776
	step [194/332], loss=101.8145
	step [195/332], loss=79.8477
	step [196/332], loss=85.5533
	step [197/332], loss=101.6507
	step [198/332], loss=84.2692
	step [199/332], loss=82.9515
	step [200/332], loss=107.8163
	step [201/332], loss=86.7348
	step [202/332], loss=89.0326
	step [203/332], loss=96.7082
	step [204/332], loss=98.5765
	step [205/332], loss=79.0668
	step [206/332], loss=90.5943
	step [207/332], loss=96.2487
	step [208/332], loss=102.0061
	step [209/332], loss=110.2450
	step [210/332], loss=89.8496
	step [211/332], loss=103.5543
	step [212/332], loss=76.7657
	step [213/332], loss=71.9687
	step [214/332], loss=94.6005
	step [215/332], loss=100.2796
	step [216/332], loss=90.1708
	step [217/332], loss=86.0157
	step [218/332], loss=95.0152
	step [219/332], loss=97.1678
	step [220/332], loss=84.2333
	step [221/332], loss=81.8321
	step [222/332], loss=91.2110
	step [223/332], loss=93.1301
	step [224/332], loss=83.4978
	step [225/332], loss=97.2241
	step [226/332], loss=113.1180
	step [227/332], loss=97.6205
	step [228/332], loss=99.9168
	step [229/332], loss=100.6434
	step [230/332], loss=119.0581
	step [231/332], loss=86.2301
	step [232/332], loss=94.6349
	step [233/332], loss=86.1015
	step [234/332], loss=96.7800
	step [235/332], loss=89.7549
	step [236/332], loss=95.7029
	step [237/332], loss=94.2005
	step [238/332], loss=89.9488
	step [239/332], loss=101.5522
	step [240/332], loss=88.4211
	step [241/332], loss=89.7975
	step [242/332], loss=84.6165
	step [243/332], loss=94.9078
	step [244/332], loss=79.4977
	step [245/332], loss=80.8333
	step [246/332], loss=85.7916
	step [247/332], loss=90.3252
	step [248/332], loss=88.6647
	step [249/332], loss=93.2157
	step [250/332], loss=91.3988
	step [251/332], loss=71.7519
	step [252/332], loss=87.8982
	step [253/332], loss=83.5776
	step [254/332], loss=83.6357
	step [255/332], loss=100.5599
	step [256/332], loss=72.8565
	step [257/332], loss=117.1322
	step [258/332], loss=90.7183
	step [259/332], loss=105.2392
	step [260/332], loss=87.9287
	step [261/332], loss=79.5893
	step [262/332], loss=88.1659
	step [263/332], loss=103.2090
	step [264/332], loss=64.6009
	step [265/332], loss=85.6381
	step [266/332], loss=80.0576
	step [267/332], loss=90.5556
	step [268/332], loss=77.0770
	step [269/332], loss=92.0274
	step [270/332], loss=94.0328
	step [271/332], loss=90.0432
	step [272/332], loss=93.6423
	step [273/332], loss=76.1804
	step [274/332], loss=87.2694
	step [275/332], loss=110.3095
	step [276/332], loss=72.2846
	step [277/332], loss=92.4358
	step [278/332], loss=75.9215
	step [279/332], loss=59.0305
	step [280/332], loss=86.6122
	step [281/332], loss=95.1104
	step [282/332], loss=96.1415
	step [283/332], loss=105.8918
	step [284/332], loss=105.2878
	step [285/332], loss=95.4605
	step [286/332], loss=92.9613
	step [287/332], loss=90.0854
	step [288/332], loss=81.9671
	step [289/332], loss=111.8068
	step [290/332], loss=90.5201
	step [291/332], loss=82.8255
	step [292/332], loss=83.0245
	step [293/332], loss=70.9903
	step [294/332], loss=88.2779
	step [295/332], loss=84.6518
	step [296/332], loss=98.3595
	step [297/332], loss=92.4480
	step [298/332], loss=91.1669
	step [299/332], loss=91.9197
	step [300/332], loss=80.3152
	step [301/332], loss=100.5270
	step [302/332], loss=88.4280
	step [303/332], loss=78.9914
	step [304/332], loss=73.7397
	step [305/332], loss=88.7318
	step [306/332], loss=82.2375
	step [307/332], loss=75.8400
	step [308/332], loss=113.6197
	step [309/332], loss=85.0099
	step [310/332], loss=95.3264
	step [311/332], loss=91.3933
	step [312/332], loss=83.2393
	step [313/332], loss=98.2148
	step [314/332], loss=91.2216
	step [315/332], loss=94.6488
	step [316/332], loss=87.4002
	step [317/332], loss=102.8298
	step [318/332], loss=94.5649
	step [319/332], loss=86.4109
	step [320/332], loss=93.5953
	step [321/332], loss=85.7414
	step [322/332], loss=68.1664
	step [323/332], loss=95.8311
	step [324/332], loss=93.3874
	step [325/332], loss=87.8341
	step [326/332], loss=87.8823
	step [327/332], loss=92.2995
	step [328/332], loss=92.6907
	step [329/332], loss=72.6284
	step [330/332], loss=98.7297
	step [331/332], loss=98.4179
	step [332/332], loss=61.5549
	Evaluating
	loss=0.0654, precision=0.3232, recall=0.9177, f1=0.4780
Training epoch 5
	step [1/332], loss=78.8954
	step [2/332], loss=101.7790
	step [3/332], loss=95.2258
	step [4/332], loss=94.6971
	step [5/332], loss=99.3881
	step [6/332], loss=82.3048
	step [7/332], loss=90.5339
	step [8/332], loss=79.4863
	step [9/332], loss=98.3535
	step [10/332], loss=102.0501
	step [11/332], loss=100.9795
	step [12/332], loss=82.1329
	step [13/332], loss=71.7836
	step [14/332], loss=84.8868
	step [15/332], loss=90.6493
	step [16/332], loss=79.2851
	step [17/332], loss=93.1590
	step [18/332], loss=89.0164
	step [19/332], loss=84.8108
	step [20/332], loss=83.0089
	step [21/332], loss=96.8377
	step [22/332], loss=93.8529
	step [23/332], loss=103.0571
	step [24/332], loss=80.7293
	step [25/332], loss=83.4504
	step [26/332], loss=91.5217
	step [27/332], loss=84.9229
	step [28/332], loss=83.8571
	step [29/332], loss=73.8273
	step [30/332], loss=113.0111
	step [31/332], loss=74.4179
	step [32/332], loss=103.7653
	step [33/332], loss=79.2590
	step [34/332], loss=92.8001
	step [35/332], loss=81.8737
	step [36/332], loss=107.3217
	step [37/332], loss=77.9076
	step [38/332], loss=82.9503
	step [39/332], loss=92.9403
	step [40/332], loss=89.0248
	step [41/332], loss=79.4512
	step [42/332], loss=93.0636
	step [43/332], loss=111.7707
	step [44/332], loss=79.2134
	step [45/332], loss=74.6946
	step [46/332], loss=93.8630
	step [47/332], loss=102.4970
	step [48/332], loss=75.5547
	step [49/332], loss=96.0790
	step [50/332], loss=82.0921
	step [51/332], loss=81.7631
	step [52/332], loss=66.6450
	step [53/332], loss=98.7834
	step [54/332], loss=92.8036
	step [55/332], loss=70.8151
	step [56/332], loss=72.7274
	step [57/332], loss=82.6097
	step [58/332], loss=78.8700
	step [59/332], loss=93.9373
	step [60/332], loss=82.8176
	step [61/332], loss=88.2384
	step [62/332], loss=91.9197
	step [63/332], loss=81.2996
	step [64/332], loss=91.3850
	step [65/332], loss=93.8407
	step [66/332], loss=95.2741
	step [67/332], loss=85.0244
	step [68/332], loss=73.2406
	step [69/332], loss=108.1008
	step [70/332], loss=64.5697
	step [71/332], loss=77.0341
	step [72/332], loss=82.0582
	step [73/332], loss=83.1220
	step [74/332], loss=87.1974
	step [75/332], loss=107.2797
	step [76/332], loss=87.6524
	step [77/332], loss=101.8998
	step [78/332], loss=94.7812
	step [79/332], loss=112.5782
	step [80/332], loss=91.7946
	step [81/332], loss=75.5826
	step [82/332], loss=76.1690
	step [83/332], loss=94.6196
	step [84/332], loss=71.9964
	step [85/332], loss=59.9374
	step [86/332], loss=88.7896
	step [87/332], loss=97.5735
	step [88/332], loss=93.9548
	step [89/332], loss=84.5641
	step [90/332], loss=90.2675
	step [91/332], loss=94.6567
	step [92/332], loss=99.4596
	step [93/332], loss=93.5415
	step [94/332], loss=75.7421
	step [95/332], loss=75.9491
	step [96/332], loss=102.9062
	step [97/332], loss=78.2874
	step [98/332], loss=86.0419
	step [99/332], loss=93.8737
	step [100/332], loss=84.7172
	step [101/332], loss=93.1599
	step [102/332], loss=103.1588
	step [103/332], loss=87.4745
	step [104/332], loss=106.9733
	step [105/332], loss=71.7694
	step [106/332], loss=74.2561
	step [107/332], loss=79.8875
	step [108/332], loss=89.3819
	step [109/332], loss=76.9518
	step [110/332], loss=79.5353
	step [111/332], loss=82.3259
	step [112/332], loss=72.1402
	step [113/332], loss=78.2487
	step [114/332], loss=100.2753
	step [115/332], loss=84.1714
	step [116/332], loss=85.8815
	step [117/332], loss=98.8317
	step [118/332], loss=81.8558
	step [119/332], loss=90.5241
	step [120/332], loss=100.2655
	step [121/332], loss=88.5743
	step [122/332], loss=89.4881
	step [123/332], loss=69.1532
	step [124/332], loss=74.6501
	step [125/332], loss=82.8102
	step [126/332], loss=78.9461
	step [127/332], loss=82.4645
	step [128/332], loss=81.0918
	step [129/332], loss=86.7599
	step [130/332], loss=97.9111
	step [131/332], loss=87.4971
	step [132/332], loss=93.7117
	step [133/332], loss=80.7852
	step [134/332], loss=94.7682
	step [135/332], loss=86.5965
	step [136/332], loss=76.7010
	step [137/332], loss=91.4670
	step [138/332], loss=100.6368
	step [139/332], loss=110.4879
	step [140/332], loss=66.8202
	step [141/332], loss=72.3864
	step [142/332], loss=91.0227
	step [143/332], loss=82.7041
	step [144/332], loss=87.6492
	step [145/332], loss=77.4420
	step [146/332], loss=91.6704
	step [147/332], loss=84.9468
	step [148/332], loss=99.3594
	step [149/332], loss=80.4547
	step [150/332], loss=95.3948
	step [151/332], loss=74.9101
	step [152/332], loss=76.0182
	step [153/332], loss=102.8152
	step [154/332], loss=97.3758
	step [155/332], loss=73.8084
	step [156/332], loss=86.3679
	step [157/332], loss=63.4681
	step [158/332], loss=88.1115
	step [159/332], loss=87.6654
	step [160/332], loss=76.6458
	step [161/332], loss=72.8773
	step [162/332], loss=93.1359
	step [163/332], loss=90.5869
	step [164/332], loss=88.5174
	step [165/332], loss=92.3697
	step [166/332], loss=78.4503
	step [167/332], loss=66.7174
	step [168/332], loss=95.2494
	step [169/332], loss=84.9285
	step [170/332], loss=86.0242
	step [171/332], loss=101.2557
	step [172/332], loss=89.0238
	step [173/332], loss=101.4131
	step [174/332], loss=76.1296
	step [175/332], loss=75.6195
	step [176/332], loss=107.4941
	step [177/332], loss=89.9546
	step [178/332], loss=78.8675
	step [179/332], loss=91.6657
	step [180/332], loss=84.6868
	step [181/332], loss=77.2413
	step [182/332], loss=76.1944
	step [183/332], loss=117.4352
	step [184/332], loss=72.9520
	step [185/332], loss=120.9158
	step [186/332], loss=84.7328
	step [187/332], loss=100.1908
	step [188/332], loss=83.9377
	step [189/332], loss=76.1655
	step [190/332], loss=103.6747
	step [191/332], loss=90.7641
	step [192/332], loss=89.7332
	step [193/332], loss=86.5435
	step [194/332], loss=74.1922
	step [195/332], loss=68.3111
	step [196/332], loss=101.3584
	step [197/332], loss=89.9956
	step [198/332], loss=84.1885
	step [199/332], loss=104.8543
	step [200/332], loss=70.7065
	step [201/332], loss=98.1376
	step [202/332], loss=96.1487
	step [203/332], loss=83.7061
	step [204/332], loss=93.5987
	step [205/332], loss=96.5888
	step [206/332], loss=81.6388
	step [207/332], loss=74.3811
	step [208/332], loss=88.7142
	step [209/332], loss=108.0582
	step [210/332], loss=97.0297
	step [211/332], loss=84.0673
	step [212/332], loss=91.4643
	step [213/332], loss=87.3008
	step [214/332], loss=91.0183
	step [215/332], loss=66.6682
	step [216/332], loss=97.7623
	step [217/332], loss=82.7580
	step [218/332], loss=79.4114
	step [219/332], loss=101.1622
	step [220/332], loss=79.2200
	step [221/332], loss=72.9399
	step [222/332], loss=97.5631
	step [223/332], loss=82.5060
	step [224/332], loss=81.8012
	step [225/332], loss=95.2984
	step [226/332], loss=86.9514
	step [227/332], loss=90.0820
	step [228/332], loss=88.4369
	step [229/332], loss=98.0422
	step [230/332], loss=85.3168
	step [231/332], loss=88.0706
	step [232/332], loss=77.5182
	step [233/332], loss=93.8921
	step [234/332], loss=88.0766
	step [235/332], loss=95.0397
	step [236/332], loss=93.1133
	step [237/332], loss=103.9774
	step [238/332], loss=90.9516
	step [239/332], loss=86.7162
	step [240/332], loss=91.4885
	step [241/332], loss=89.0240
	step [242/332], loss=93.0922
	step [243/332], loss=100.6685
	step [244/332], loss=109.8137
	step [245/332], loss=78.3380
	step [246/332], loss=85.6119
	step [247/332], loss=84.6119
	step [248/332], loss=83.1340
	step [249/332], loss=76.5586
	step [250/332], loss=84.4615
	step [251/332], loss=94.0141
	step [252/332], loss=90.6735
	step [253/332], loss=77.1526
	step [254/332], loss=89.2206
	step [255/332], loss=85.7283
	step [256/332], loss=95.5492
	step [257/332], loss=79.1951
	step [258/332], loss=89.6266
	step [259/332], loss=71.3730
	step [260/332], loss=70.2426
	step [261/332], loss=87.8468
	step [262/332], loss=90.7802
	step [263/332], loss=72.4404
	step [264/332], loss=78.0684
	step [265/332], loss=71.5877
	step [266/332], loss=78.0886
	step [267/332], loss=73.6533
	step [268/332], loss=92.9755
	step [269/332], loss=67.1822
	step [270/332], loss=88.0454
	step [271/332], loss=88.2660
	step [272/332], loss=83.1843
	step [273/332], loss=67.5626
	step [274/332], loss=82.0656
	step [275/332], loss=81.4521
	step [276/332], loss=85.5044
	step [277/332], loss=96.2380
	step [278/332], loss=77.9204
	step [279/332], loss=103.4893
	step [280/332], loss=74.0040
	step [281/332], loss=88.3722
	step [282/332], loss=80.8012
	step [283/332], loss=76.4757
	step [284/332], loss=97.3136
	step [285/332], loss=81.0433
	step [286/332], loss=93.0532
	step [287/332], loss=102.6239
	step [288/332], loss=86.2576
	step [289/332], loss=93.8300
	step [290/332], loss=85.7827
	step [291/332], loss=84.4182
	step [292/332], loss=88.2954
	step [293/332], loss=90.7543
	step [294/332], loss=86.4622
	step [295/332], loss=69.7832
	step [296/332], loss=81.5970
	step [297/332], loss=70.8617
	step [298/332], loss=92.6505
	step [299/332], loss=94.0029
	step [300/332], loss=73.3703
	step [301/332], loss=83.3521
	step [302/332], loss=90.5615
	step [303/332], loss=88.8179
	step [304/332], loss=97.1483
	step [305/332], loss=75.1263
	step [306/332], loss=81.9291
	step [307/332], loss=81.2738
	step [308/332], loss=73.0998
	step [309/332], loss=98.8684
	step [310/332], loss=80.6515
	step [311/332], loss=85.5246
	step [312/332], loss=90.3136
	step [313/332], loss=79.6977
	step [314/332], loss=88.7004
	step [315/332], loss=67.7608
	step [316/332], loss=118.4545
	step [317/332], loss=78.6844
	step [318/332], loss=77.3760
	step [319/332], loss=82.7200
	step [320/332], loss=79.0776
	step [321/332], loss=87.5424
	step [322/332], loss=83.8747
	step [323/332], loss=99.0896
	step [324/332], loss=96.6632
	step [325/332], loss=102.8697
	step [326/332], loss=78.5384
	step [327/332], loss=98.0594
	step [328/332], loss=66.2604
	step [329/332], loss=73.5313
	step [330/332], loss=67.6235
	step [331/332], loss=94.3759
	step [332/332], loss=67.2005
	Evaluating
	loss=0.0465, precision=0.3715, recall=0.9167, f1=0.5288
Training epoch 6
	step [1/332], loss=90.9987
	step [2/332], loss=78.1739
	step [3/332], loss=102.2218
	step [4/332], loss=72.6142
	step [5/332], loss=89.6820
	step [6/332], loss=88.0803
	step [7/332], loss=82.0846
	step [8/332], loss=76.9680
	step [9/332], loss=86.7999
	step [10/332], loss=94.6136
	step [11/332], loss=77.3342
	step [12/332], loss=68.6591
	step [13/332], loss=95.6741
	step [14/332], loss=68.2687
	step [15/332], loss=84.3762
	step [16/332], loss=95.8235
	step [17/332], loss=93.8796
	step [18/332], loss=84.1313
	step [19/332], loss=88.0514
	step [20/332], loss=83.0209
	step [21/332], loss=74.8897
	step [22/332], loss=84.7347
	step [23/332], loss=92.2281
	step [24/332], loss=81.2994
	step [25/332], loss=87.3763
	step [26/332], loss=98.2696
	step [27/332], loss=113.1446
	step [28/332], loss=93.1501
	step [29/332], loss=87.6568
	step [30/332], loss=71.7953
	step [31/332], loss=84.8650
	step [32/332], loss=79.1874
	step [33/332], loss=89.6058
	step [34/332], loss=75.9530
	step [35/332], loss=99.0682
	step [36/332], loss=58.5712
	step [37/332], loss=80.5250
	step [38/332], loss=75.0338
	step [39/332], loss=68.3815
	step [40/332], loss=104.2800
	step [41/332], loss=87.5699
	step [42/332], loss=79.1003
	step [43/332], loss=96.7322
	step [44/332], loss=97.2852
	step [45/332], loss=84.7729
	step [46/332], loss=81.2080
	step [47/332], loss=73.0505
	step [48/332], loss=84.5590
	step [49/332], loss=93.7025
	step [50/332], loss=71.8282
	step [51/332], loss=82.1772
	step [52/332], loss=72.1328
	step [53/332], loss=94.5497
	step [54/332], loss=88.5317
	step [55/332], loss=99.7604
	step [56/332], loss=84.4144
	step [57/332], loss=81.5742
	step [58/332], loss=88.4726
	step [59/332], loss=85.0693
	step [60/332], loss=73.1803
	step [61/332], loss=91.4396
	step [62/332], loss=82.5758
	step [63/332], loss=60.0956
	step [64/332], loss=94.0504
	step [65/332], loss=85.2422
	step [66/332], loss=107.2878
	step [67/332], loss=97.3628
	step [68/332], loss=81.4671
	step [69/332], loss=80.4716
	step [70/332], loss=81.1669
	step [71/332], loss=85.6706
	step [72/332], loss=90.9699
	step [73/332], loss=83.7643
	step [74/332], loss=75.0457
	step [75/332], loss=94.3396
	step [76/332], loss=93.0469
	step [77/332], loss=83.6896
	step [78/332], loss=74.2139
	step [79/332], loss=74.3046
	step [80/332], loss=91.0408
	step [81/332], loss=80.9410
	step [82/332], loss=76.9866
	step [83/332], loss=85.2917
	step [84/332], loss=68.1231
	step [85/332], loss=93.3957
	step [86/332], loss=80.9557
	step [87/332], loss=74.0736
	step [88/332], loss=88.3608
	step [89/332], loss=81.3345
	step [90/332], loss=74.1664
	step [91/332], loss=71.4862
	step [92/332], loss=82.5294
	step [93/332], loss=87.2219
	step [94/332], loss=85.3417
	step [95/332], loss=71.6486
	step [96/332], loss=72.8432
	step [97/332], loss=90.8941
	step [98/332], loss=79.5248
	step [99/332], loss=86.5827
	step [100/332], loss=84.2275
	step [101/332], loss=100.2101
	step [102/332], loss=79.6307
	step [103/332], loss=70.0801
	step [104/332], loss=74.1428
	step [105/332], loss=92.0671
	step [106/332], loss=81.0603
	step [107/332], loss=101.4932
	step [108/332], loss=80.2865
	step [109/332], loss=89.7249
	step [110/332], loss=67.9727
	step [111/332], loss=85.7937
	step [112/332], loss=78.4852
	step [113/332], loss=67.6044
	step [114/332], loss=68.5478
	step [115/332], loss=79.0120
	step [116/332], loss=96.2173
	step [117/332], loss=82.2922
	step [118/332], loss=78.3565
	step [119/332], loss=63.4777
	step [120/332], loss=83.2058
	step [121/332], loss=75.3603
	step [122/332], loss=97.0351
	step [123/332], loss=73.1474
	step [124/332], loss=99.7025
	step [125/332], loss=85.3875
	step [126/332], loss=75.7378
	step [127/332], loss=86.6660
	step [128/332], loss=72.7891
	step [129/332], loss=91.1331
	step [130/332], loss=102.8874
	step [131/332], loss=72.9958
	step [132/332], loss=88.7209
	step [133/332], loss=101.0388
	step [134/332], loss=75.8425
	step [135/332], loss=76.2143
	step [136/332], loss=92.9310
	step [137/332], loss=96.7252
	step [138/332], loss=85.2769
	step [139/332], loss=79.0063
	step [140/332], loss=89.8091
	step [141/332], loss=84.6741
	step [142/332], loss=89.6366
	step [143/332], loss=88.2158
	step [144/332], loss=90.5509
	step [145/332], loss=83.2255
	step [146/332], loss=70.5029
	step [147/332], loss=97.4192
	step [148/332], loss=75.6749
	step [149/332], loss=69.9682
	step [150/332], loss=82.3666
	step [151/332], loss=82.8213
	step [152/332], loss=90.6798
	step [153/332], loss=93.0468
	step [154/332], loss=79.9910
	step [155/332], loss=71.5248
	step [156/332], loss=90.4940
	step [157/332], loss=74.1806
	step [158/332], loss=77.2442
	step [159/332], loss=90.8638
	step [160/332], loss=92.4831
	step [161/332], loss=105.4357
	step [162/332], loss=77.3495
	step [163/332], loss=90.8751
	step [164/332], loss=75.4965
	step [165/332], loss=79.7471
	step [166/332], loss=76.1996
	step [167/332], loss=91.6661
	step [168/332], loss=80.7050
	step [169/332], loss=88.1081
	step [170/332], loss=71.4551
	step [171/332], loss=90.8428
	step [172/332], loss=68.4585
	step [173/332], loss=81.9269
	step [174/332], loss=93.4751
	step [175/332], loss=81.8436
	step [176/332], loss=100.9778
	step [177/332], loss=66.2667
	step [178/332], loss=78.3856
	step [179/332], loss=84.8340
	step [180/332], loss=87.1197
	step [181/332], loss=88.8120
	step [182/332], loss=87.5816
	step [183/332], loss=77.9936
	step [184/332], loss=76.0289
	step [185/332], loss=98.5995
	step [186/332], loss=83.4544
	step [187/332], loss=102.9841
	step [188/332], loss=77.6244
	step [189/332], loss=87.7473
	step [190/332], loss=69.8809
	step [191/332], loss=72.9432
	step [192/332], loss=79.0975
	step [193/332], loss=94.9374
	step [194/332], loss=94.8305
	step [195/332], loss=72.3436
	step [196/332], loss=79.1523
	step [197/332], loss=74.9292
	step [198/332], loss=92.1091
	step [199/332], loss=89.6781
	step [200/332], loss=96.1553
	step [201/332], loss=79.7523
	step [202/332], loss=93.7926
	step [203/332], loss=85.7984
	step [204/332], loss=87.6281
	step [205/332], loss=88.3531
	step [206/332], loss=64.5610
	step [207/332], loss=76.5063
	step [208/332], loss=88.2175
	step [209/332], loss=86.5888
	step [210/332], loss=65.4007
	step [211/332], loss=89.5420
	step [212/332], loss=94.1714
	step [213/332], loss=87.0277
	step [214/332], loss=79.7169
	step [215/332], loss=86.6062
	step [216/332], loss=82.0276
	step [217/332], loss=61.3271
	step [218/332], loss=77.7925
	step [219/332], loss=88.8694
	step [220/332], loss=78.1748
	step [221/332], loss=77.6707
	step [222/332], loss=82.9124
	step [223/332], loss=93.6580
	step [224/332], loss=85.3572
	step [225/332], loss=91.6584
	step [226/332], loss=77.2528
	step [227/332], loss=82.0148
	step [228/332], loss=85.5953
	step [229/332], loss=70.0124
	step [230/332], loss=74.6657
	step [231/332], loss=71.9205
	step [232/332], loss=80.5041
	step [233/332], loss=76.1542
	step [234/332], loss=86.6271
	step [235/332], loss=85.9174
	step [236/332], loss=87.1966
	step [237/332], loss=76.8418
	step [238/332], loss=95.0282
	step [239/332], loss=94.6771
	step [240/332], loss=79.1171
	step [241/332], loss=73.7949
	step [242/332], loss=90.3250
	step [243/332], loss=90.3471
	step [244/332], loss=84.5995
	step [245/332], loss=83.6851
	step [246/332], loss=84.7665
	step [247/332], loss=107.1545
	step [248/332], loss=88.0331
	step [249/332], loss=76.8505
	step [250/332], loss=74.0485
	step [251/332], loss=97.3065
	step [252/332], loss=76.7314
	step [253/332], loss=96.2650
	step [254/332], loss=81.7578
	step [255/332], loss=67.7593
	step [256/332], loss=88.9104
	step [257/332], loss=77.7915
	step [258/332], loss=84.6099
	step [259/332], loss=73.6572
	step [260/332], loss=90.8096
	step [261/332], loss=84.0019
	step [262/332], loss=76.2552
	step [263/332], loss=91.2086
	step [264/332], loss=79.4715
	step [265/332], loss=85.0921
	step [266/332], loss=78.3718
	step [267/332], loss=83.0796
	step [268/332], loss=82.3572
	step [269/332], loss=92.6944
	step [270/332], loss=72.1287
	step [271/332], loss=84.8013
	step [272/332], loss=81.6638
	step [273/332], loss=78.4351
	step [274/332], loss=87.5610
	step [275/332], loss=85.7838
	step [276/332], loss=65.7599
	step [277/332], loss=96.7365
	step [278/332], loss=63.4737
	step [279/332], loss=77.5881
	step [280/332], loss=76.4214
	step [281/332], loss=85.8767
	step [282/332], loss=70.4833
	step [283/332], loss=88.0801
	step [284/332], loss=80.8615
	step [285/332], loss=86.2824
	step [286/332], loss=76.1292
	step [287/332], loss=99.4489
	step [288/332], loss=89.5269
	step [289/332], loss=90.6046
	step [290/332], loss=90.7844
	step [291/332], loss=83.3163
	step [292/332], loss=90.5862
	step [293/332], loss=88.1322
	step [294/332], loss=83.1566
	step [295/332], loss=79.3310
	step [296/332], loss=93.0738
	step [297/332], loss=84.6461
	step [298/332], loss=89.6099
	step [299/332], loss=90.9950
	step [300/332], loss=84.1476
	step [301/332], loss=98.2861
	step [302/332], loss=67.9554
	step [303/332], loss=77.2164
	step [304/332], loss=80.9063
	step [305/332], loss=88.4854
	step [306/332], loss=83.2395
	step [307/332], loss=83.4150
	step [308/332], loss=75.4862
	step [309/332], loss=99.5127
	step [310/332], loss=64.0990
	step [311/332], loss=78.1271
	step [312/332], loss=81.9251
	step [313/332], loss=81.0788
	step [314/332], loss=71.1268
	step [315/332], loss=100.7654
	step [316/332], loss=85.3907
	step [317/332], loss=100.1361
	step [318/332], loss=81.5179
	step [319/332], loss=94.9112
	step [320/332], loss=79.9601
	step [321/332], loss=98.8008
	step [322/332], loss=85.5070
	step [323/332], loss=71.6601
	step [324/332], loss=89.5876
	step [325/332], loss=101.6350
	step [326/332], loss=67.9874
	step [327/332], loss=90.9590
	step [328/332], loss=87.9058
	step [329/332], loss=90.8037
	step [330/332], loss=96.4987
	step [331/332], loss=99.9838
	step [332/332], loss=63.3879
	Evaluating
	loss=0.0362, precision=0.3319, recall=0.9233, f1=0.4883
Training epoch 7
	step [1/332], loss=75.2395
	step [2/332], loss=81.0288
	step [3/332], loss=82.2933
	step [4/332], loss=84.9473
	step [5/332], loss=94.6223
	step [6/332], loss=68.9917
	step [7/332], loss=96.0454
	step [8/332], loss=86.9337
	step [9/332], loss=101.7944
	step [10/332], loss=87.8634
	step [11/332], loss=78.9667
	step [12/332], loss=86.3216
	step [13/332], loss=83.8911
	step [14/332], loss=63.4191
	step [15/332], loss=90.1502
	step [16/332], loss=88.7195
	step [17/332], loss=83.4347
	step [18/332], loss=71.8815
	step [19/332], loss=99.7763
	step [20/332], loss=86.1218
	step [21/332], loss=85.2784
	step [22/332], loss=92.8094
	step [23/332], loss=80.2774
	step [24/332], loss=76.7420
	step [25/332], loss=74.5156
	step [26/332], loss=90.5726
	step [27/332], loss=91.0135
	step [28/332], loss=77.2410
	step [29/332], loss=79.7583
	step [30/332], loss=90.3182
	step [31/332], loss=61.8998
	step [32/332], loss=78.3994
	step [33/332], loss=71.6355
	step [34/332], loss=71.8959
	step [35/332], loss=75.4414
	step [36/332], loss=76.0002
	step [37/332], loss=73.7168
	step [38/332], loss=90.0782
	step [39/332], loss=80.5667
	step [40/332], loss=82.0308
	step [41/332], loss=80.8859
	step [42/332], loss=65.6604
	step [43/332], loss=95.1853
	step [44/332], loss=97.8899
	step [45/332], loss=98.4554
	step [46/332], loss=79.5199
	step [47/332], loss=87.5513
	step [48/332], loss=73.3405
	step [49/332], loss=68.4883
	step [50/332], loss=84.5791
	step [51/332], loss=78.1147
	step [52/332], loss=90.4051
	step [53/332], loss=86.1128
	step [54/332], loss=104.2658
	step [55/332], loss=84.9960
	step [56/332], loss=94.9215
	step [57/332], loss=85.7528
	step [58/332], loss=74.5184
	step [59/332], loss=81.9097
	step [60/332], loss=71.9654
	step [61/332], loss=86.1309
	step [62/332], loss=95.9734
	step [63/332], loss=84.1516
	step [64/332], loss=89.2229
	step [65/332], loss=96.6279
	step [66/332], loss=88.3064
	step [67/332], loss=90.4522
	step [68/332], loss=76.7547
	step [69/332], loss=79.7159
	step [70/332], loss=63.2241
	step [71/332], loss=67.6972
	step [72/332], loss=77.9714
	step [73/332], loss=84.6922
	step [74/332], loss=91.4839
	step [75/332], loss=90.9357
	step [76/332], loss=82.8321
	step [77/332], loss=83.0546
	step [78/332], loss=73.0239
	step [79/332], loss=79.2660
	step [80/332], loss=91.9971
	step [81/332], loss=125.4476
	step [82/332], loss=81.4810
	step [83/332], loss=106.2927
	step [84/332], loss=75.4331
	step [85/332], loss=75.8015
	step [86/332], loss=73.6628
	step [87/332], loss=70.3244
	step [88/332], loss=70.8342
	step [89/332], loss=71.1142
	step [90/332], loss=78.8939
	step [91/332], loss=103.9212
	step [92/332], loss=92.2845
	step [93/332], loss=69.8248
	step [94/332], loss=74.8887
	step [95/332], loss=77.6662
	step [96/332], loss=71.7941
	step [97/332], loss=71.0591
	step [98/332], loss=66.8433
	step [99/332], loss=76.7144
	step [100/332], loss=106.1051
	step [101/332], loss=95.8003
	step [102/332], loss=82.9839
	step [103/332], loss=94.2547
	step [104/332], loss=79.5074
	step [105/332], loss=89.5692
	step [106/332], loss=70.2719
	step [107/332], loss=86.8940
	step [108/332], loss=107.4988
	step [109/332], loss=104.2009
	step [110/332], loss=72.8072
	step [111/332], loss=86.5020
	step [112/332], loss=75.0521
	step [113/332], loss=70.7568
	step [114/332], loss=73.8357
	step [115/332], loss=71.8034
	step [116/332], loss=70.6029
	step [117/332], loss=85.2039
	step [118/332], loss=69.7031
	step [119/332], loss=88.8212
	step [120/332], loss=92.6886
	step [121/332], loss=85.9222
	step [122/332], loss=67.9707
	step [123/332], loss=83.1900
	step [124/332], loss=89.8722
	step [125/332], loss=75.5369
	step [126/332], loss=87.2568
	step [127/332], loss=94.8927
	step [128/332], loss=81.1135
	step [129/332], loss=85.2711
	step [130/332], loss=82.5320
	step [131/332], loss=92.2467
	step [132/332], loss=94.8241
	step [133/332], loss=67.8915
	step [134/332], loss=71.4114
	step [135/332], loss=83.1001
	step [136/332], loss=84.4166
	step [137/332], loss=71.3756
	step [138/332], loss=70.5164
	step [139/332], loss=85.8780
	step [140/332], loss=92.0254
	step [141/332], loss=81.1688
	step [142/332], loss=87.9806
	step [143/332], loss=87.7816
	step [144/332], loss=95.7077
	step [145/332], loss=69.3039
	step [146/332], loss=99.3332
	step [147/332], loss=76.7699
	step [148/332], loss=81.2458
	step [149/332], loss=101.5213
	step [150/332], loss=93.4225
	step [151/332], loss=91.1628
	step [152/332], loss=80.7208
	step [153/332], loss=67.8135
	step [154/332], loss=79.1408
	step [155/332], loss=58.7627
	step [156/332], loss=92.8373
	step [157/332], loss=84.7120
	step [158/332], loss=79.8107
	step [159/332], loss=73.1146
	step [160/332], loss=62.6067
	step [161/332], loss=101.5878
	step [162/332], loss=88.3495
	step [163/332], loss=70.7300
	step [164/332], loss=83.8989
	step [165/332], loss=87.1241
	step [166/332], loss=80.6720
	step [167/332], loss=72.8688
	step [168/332], loss=82.5171
	step [169/332], loss=96.6934
	step [170/332], loss=90.1801
	step [171/332], loss=72.5560
	step [172/332], loss=81.7188
	step [173/332], loss=81.7912
	step [174/332], loss=94.0619
	step [175/332], loss=82.4124
	step [176/332], loss=69.3665
	step [177/332], loss=76.1433
	step [178/332], loss=86.2235
	step [179/332], loss=76.4240
	step [180/332], loss=78.1536
	step [181/332], loss=85.7965
	step [182/332], loss=73.2853
	step [183/332], loss=70.8657
	step [184/332], loss=75.6900
	step [185/332], loss=76.9719
	step [186/332], loss=89.3925
	step [187/332], loss=72.9848
	step [188/332], loss=76.2463
	step [189/332], loss=77.6402
	step [190/332], loss=92.5580
	step [191/332], loss=81.9963
	step [192/332], loss=86.5344
	step [193/332], loss=74.6856
	step [194/332], loss=68.5416
	step [195/332], loss=75.5858
	step [196/332], loss=80.1297
	step [197/332], loss=78.2823
	step [198/332], loss=70.2566
	step [199/332], loss=99.8663
	step [200/332], loss=78.9639
	step [201/332], loss=71.2349
	step [202/332], loss=97.8378
	step [203/332], loss=68.5053
	step [204/332], loss=73.9891
	step [205/332], loss=89.0169
	step [206/332], loss=81.7099
	step [207/332], loss=101.5051
	step [208/332], loss=97.8433
	step [209/332], loss=88.9886
	step [210/332], loss=85.5399
	step [211/332], loss=91.2934
	step [212/332], loss=109.6367
	step [213/332], loss=79.0811
	step [214/332], loss=69.2050
	step [215/332], loss=87.2590
	step [216/332], loss=78.2057
	step [217/332], loss=79.5561
	step [218/332], loss=98.3487
	step [219/332], loss=79.2845
	step [220/332], loss=90.5728
	step [221/332], loss=69.1579
	step [222/332], loss=90.0642
	step [223/332], loss=66.3415
	step [224/332], loss=78.7125
	step [225/332], loss=90.4374
	step [226/332], loss=89.1493
	step [227/332], loss=80.6375
	step [228/332], loss=82.2522
	step [229/332], loss=95.3319
	step [230/332], loss=71.9034
	step [231/332], loss=79.1039
	step [232/332], loss=74.3572
	step [233/332], loss=75.3467
	step [234/332], loss=77.0107
	step [235/332], loss=77.4506
	step [236/332], loss=72.3511
	step [237/332], loss=73.8059
	step [238/332], loss=86.1924
	step [239/332], loss=82.7216
	step [240/332], loss=93.5513
	step [241/332], loss=83.8061
	step [242/332], loss=90.6777
	step [243/332], loss=97.1512
	step [244/332], loss=58.4364
	step [245/332], loss=79.3118
	step [246/332], loss=85.7877
	step [247/332], loss=87.1711
	step [248/332], loss=75.8375
	step [249/332], loss=78.2697
	step [250/332], loss=97.1874
	step [251/332], loss=70.5934
	step [252/332], loss=81.2794
	step [253/332], loss=78.1928
	step [254/332], loss=81.7666
	step [255/332], loss=112.2383
	step [256/332], loss=75.0612
	step [257/332], loss=72.9621
	step [258/332], loss=77.5617
	step [259/332], loss=75.8593
	step [260/332], loss=78.6518
	step [261/332], loss=105.0435
	step [262/332], loss=79.1449
	step [263/332], loss=89.2330
	step [264/332], loss=98.2842
	step [265/332], loss=83.2132
	step [266/332], loss=96.6843
	step [267/332], loss=83.3600
	step [268/332], loss=80.8914
	step [269/332], loss=75.7140
	step [270/332], loss=73.9406
	step [271/332], loss=60.8743
	step [272/332], loss=89.8434
	step [273/332], loss=66.8156
	step [274/332], loss=84.6514
	step [275/332], loss=81.3370
	step [276/332], loss=102.9482
	step [277/332], loss=78.5106
	step [278/332], loss=80.9161
	step [279/332], loss=90.5286
	step [280/332], loss=65.5863
	step [281/332], loss=81.4612
	step [282/332], loss=75.5641
	step [283/332], loss=92.5728
	step [284/332], loss=106.3624
	step [285/332], loss=85.8267
	step [286/332], loss=81.5678
	step [287/332], loss=80.3898
	step [288/332], loss=86.2796
	step [289/332], loss=74.8483
	step [290/332], loss=73.4179
	step [291/332], loss=73.9428
	step [292/332], loss=76.3075
	step [293/332], loss=91.1460
	step [294/332], loss=82.2024
	step [295/332], loss=77.4555
	step [296/332], loss=59.1669
	step [297/332], loss=85.9561
	step [298/332], loss=89.2633
	step [299/332], loss=78.7663
	step [300/332], loss=83.4569
	step [301/332], loss=61.6626
	step [302/332], loss=61.8080
	step [303/332], loss=67.8839
	step [304/332], loss=70.6574
	step [305/332], loss=79.2798
	step [306/332], loss=74.4851
	step [307/332], loss=70.9191
	step [308/332], loss=72.9169
	step [309/332], loss=91.0140
	step [310/332], loss=80.5719
	step [311/332], loss=89.6572
	step [312/332], loss=87.6495
	step [313/332], loss=78.2376
	step [314/332], loss=80.0795
	step [315/332], loss=88.8096
	step [316/332], loss=57.0866
	step [317/332], loss=95.3975
	step [318/332], loss=92.6820
	step [319/332], loss=79.5388
	step [320/332], loss=87.1125
	step [321/332], loss=64.3084
	step [322/332], loss=85.2491
	step [323/332], loss=97.8107
	step [324/332], loss=86.4348
	step [325/332], loss=77.8750
	step [326/332], loss=71.7843
	step [327/332], loss=84.5340
	step [328/332], loss=77.7756
	step [329/332], loss=74.1175
	step [330/332], loss=85.5133
	step [331/332], loss=80.6073
	step [332/332], loss=49.8452
	Evaluating
	loss=0.0280, precision=0.3586, recall=0.9285, f1=0.5174
Training epoch 8
	step [1/332], loss=76.1380
	step [2/332], loss=95.1353
	step [3/332], loss=71.3316
	step [4/332], loss=70.2770
	step [5/332], loss=86.6289
	step [6/332], loss=74.9316
	step [7/332], loss=78.9389
	step [8/332], loss=77.2379
	step [9/332], loss=71.8390
	step [10/332], loss=75.5595
	step [11/332], loss=99.6779
	step [12/332], loss=66.8791
	step [13/332], loss=83.8292
	step [14/332], loss=86.0699
	step [15/332], loss=89.0262
	step [16/332], loss=95.9214
	step [17/332], loss=75.9983
	step [18/332], loss=79.8843
	step [19/332], loss=82.4287
	step [20/332], loss=81.5006
	step [21/332], loss=74.7670
	step [22/332], loss=81.2025
	step [23/332], loss=74.8412
	step [24/332], loss=94.4855
	step [25/332], loss=79.4109
	step [26/332], loss=86.4888
	step [27/332], loss=93.6877
	step [28/332], loss=90.7508
	step [29/332], loss=80.4688
	step [30/332], loss=75.3802
	step [31/332], loss=84.5103
	step [32/332], loss=86.4884
	step [33/332], loss=75.9053
	step [34/332], loss=102.0734
	step [35/332], loss=73.7560
	step [36/332], loss=68.2175
	step [37/332], loss=66.1936
	step [38/332], loss=94.3170
	step [39/332], loss=90.7463
	step [40/332], loss=83.5497
	step [41/332], loss=86.2000
	step [42/332], loss=75.3225
	step [43/332], loss=88.3202
	step [44/332], loss=73.1438
	step [45/332], loss=74.6596
	step [46/332], loss=80.3374
	step [47/332], loss=91.2842
	step [48/332], loss=78.1394
	step [49/332], loss=67.1128
	step [50/332], loss=75.0405
	step [51/332], loss=86.3883
	step [52/332], loss=77.4084
	step [53/332], loss=77.0197
	step [54/332], loss=81.0391
	step [55/332], loss=78.5223
	step [56/332], loss=84.3463
	step [57/332], loss=98.8269
	step [58/332], loss=88.0845
	step [59/332], loss=89.3611
	step [60/332], loss=73.7134
	step [61/332], loss=81.4917
	step [62/332], loss=88.4674
	step [63/332], loss=83.9343
	step [64/332], loss=85.4481
	step [65/332], loss=79.8818
	step [66/332], loss=105.6776
	step [67/332], loss=89.5982
	step [68/332], loss=74.3447
	step [69/332], loss=87.6147
	step [70/332], loss=93.1035
	step [71/332], loss=90.4837
	step [72/332], loss=61.2913
	step [73/332], loss=76.8009
	step [74/332], loss=61.1399
	step [75/332], loss=88.1209
	step [76/332], loss=102.1540
	step [77/332], loss=64.6485
	step [78/332], loss=84.3921
	step [79/332], loss=72.9603
	step [80/332], loss=90.1464
	step [81/332], loss=78.8046
	step [82/332], loss=72.0704
	step [83/332], loss=68.2417
	step [84/332], loss=95.9948
	step [85/332], loss=76.1411
	step [86/332], loss=87.7062
	step [87/332], loss=74.5119
	step [88/332], loss=90.5406
	step [89/332], loss=86.0493
	step [90/332], loss=81.5792
	step [91/332], loss=78.2455
	step [92/332], loss=82.6258
	step [93/332], loss=73.1164
	step [94/332], loss=67.4299
	step [95/332], loss=81.6624
	step [96/332], loss=74.6460
	step [97/332], loss=90.5176
	step [98/332], loss=74.8707
	step [99/332], loss=96.4776
	step [100/332], loss=67.0004
	step [101/332], loss=96.2275
	step [102/332], loss=87.5638
	step [103/332], loss=87.3000
	step [104/332], loss=65.7211
	step [105/332], loss=78.4655
	step [106/332], loss=79.3355
	step [107/332], loss=66.7171
	step [108/332], loss=85.6846
	step [109/332], loss=83.9058
	step [110/332], loss=85.1123
	step [111/332], loss=86.8126
	step [112/332], loss=81.0116
	step [113/332], loss=85.7636
	step [114/332], loss=81.1770
	step [115/332], loss=73.2739
	step [116/332], loss=84.5475
	step [117/332], loss=86.2609
	step [118/332], loss=73.4758
	step [119/332], loss=87.0314
	step [120/332], loss=77.1159
	step [121/332], loss=75.6147
	step [122/332], loss=71.0979
	step [123/332], loss=86.5545
	step [124/332], loss=82.4614
	step [125/332], loss=73.2791
	step [126/332], loss=79.3498
	step [127/332], loss=66.5714
	step [128/332], loss=100.3582
	step [129/332], loss=85.7059
	step [130/332], loss=68.2036
	step [131/332], loss=71.2353
	step [132/332], loss=101.8997
	step [133/332], loss=75.6761
	step [134/332], loss=84.1738
	step [135/332], loss=76.6015
	step [136/332], loss=72.8239
	step [137/332], loss=71.4920
	step [138/332], loss=83.6325
	step [139/332], loss=86.0630
	step [140/332], loss=83.0781
	step [141/332], loss=62.8282
	step [142/332], loss=83.9686
	step [143/332], loss=97.4393
	step [144/332], loss=104.5077
	step [145/332], loss=67.0022
	step [146/332], loss=83.2020
	step [147/332], loss=72.3911
	step [148/332], loss=77.9680
	step [149/332], loss=90.8407
	step [150/332], loss=66.7626
	step [151/332], loss=79.8528
	step [152/332], loss=70.5846
	step [153/332], loss=86.7497
	step [154/332], loss=57.6733
	step [155/332], loss=69.4427
	step [156/332], loss=71.6732
	step [157/332], loss=84.4483
	step [158/332], loss=90.1884
	step [159/332], loss=73.8208
	step [160/332], loss=91.8759
	step [161/332], loss=63.0121
	step [162/332], loss=71.2530
	step [163/332], loss=66.3127
	step [164/332], loss=83.1344
	step [165/332], loss=73.9207
	step [166/332], loss=82.7587
	step [167/332], loss=87.7259
	step [168/332], loss=76.1161
	step [169/332], loss=76.4505
	step [170/332], loss=89.6633
	step [171/332], loss=74.9611
	step [172/332], loss=82.4318
	step [173/332], loss=71.2412
	step [174/332], loss=91.8541
	step [175/332], loss=66.5922
	step [176/332], loss=75.1199
	step [177/332], loss=102.8365
	step [178/332], loss=96.7681
	step [179/332], loss=77.7183
	step [180/332], loss=76.4121
	step [181/332], loss=67.7862
	step [182/332], loss=70.2204
	step [183/332], loss=76.0726
	step [184/332], loss=74.8047
	step [185/332], loss=68.8059
	step [186/332], loss=69.6542
	step [187/332], loss=77.5397
	step [188/332], loss=96.4965
	step [189/332], loss=61.5534
	step [190/332], loss=84.3957
	step [191/332], loss=86.4750
	step [192/332], loss=66.0621
	step [193/332], loss=92.8028
	step [194/332], loss=91.2901
	step [195/332], loss=76.8969
	step [196/332], loss=77.7112
	step [197/332], loss=79.7236
	step [198/332], loss=79.6911
	step [199/332], loss=69.7410
	step [200/332], loss=81.1275
	step [201/332], loss=75.6430
	step [202/332], loss=92.8592
	step [203/332], loss=78.4998
	step [204/332], loss=70.6671
	step [205/332], loss=87.5281
	step [206/332], loss=81.3467
	step [207/332], loss=69.3610
	step [208/332], loss=88.7505
	step [209/332], loss=78.5243
	step [210/332], loss=84.5833
	step [211/332], loss=87.8432
	step [212/332], loss=86.6045
	step [213/332], loss=91.0730
	step [214/332], loss=89.2704
	step [215/332], loss=76.1683
	step [216/332], loss=73.0863
	step [217/332], loss=83.8333
	step [218/332], loss=73.2716
	step [219/332], loss=89.6652
	step [220/332], loss=89.4466
	step [221/332], loss=89.3702
	step [222/332], loss=83.8866
	step [223/332], loss=67.2006
	step [224/332], loss=96.5506
	step [225/332], loss=83.8083
	step [226/332], loss=93.3890
	step [227/332], loss=81.2698
	step [228/332], loss=82.5938
	step [229/332], loss=99.5748
	step [230/332], loss=74.7741
	step [231/332], loss=81.1173
	step [232/332], loss=72.6271
	step [233/332], loss=85.3837
	step [234/332], loss=80.7538
	step [235/332], loss=85.8074
	step [236/332], loss=59.7154
	step [237/332], loss=82.2435
	step [238/332], loss=80.9981
	step [239/332], loss=83.1918
	step [240/332], loss=67.1961
	step [241/332], loss=65.8560
	step [242/332], loss=82.0268
	step [243/332], loss=82.1096
	step [244/332], loss=81.0319
	step [245/332], loss=111.2162
	step [246/332], loss=85.5481
	step [247/332], loss=80.8475
	step [248/332], loss=81.9345
	step [249/332], loss=79.0552
	step [250/332], loss=96.2405
	step [251/332], loss=66.9563
	step [252/332], loss=64.0835
	step [253/332], loss=87.0948
	step [254/332], loss=78.7395
	step [255/332], loss=57.3026
	step [256/332], loss=86.5496
	step [257/332], loss=82.3759
	step [258/332], loss=77.7993
	step [259/332], loss=74.4416
	step [260/332], loss=66.8013
	step [261/332], loss=64.0611
	step [262/332], loss=70.4632
	step [263/332], loss=79.9069
	step [264/332], loss=75.8042
	step [265/332], loss=91.9787
	step [266/332], loss=88.6589
	step [267/332], loss=85.3931
	step [268/332], loss=71.6574
	step [269/332], loss=93.0085
	step [270/332], loss=97.8709
	step [271/332], loss=73.0108
	step [272/332], loss=85.1156
	step [273/332], loss=80.7520
	step [274/332], loss=83.0029
	step [275/332], loss=81.4027
	step [276/332], loss=64.3583
	step [277/332], loss=81.3745
	step [278/332], loss=71.2244
	step [279/332], loss=74.1359
	step [280/332], loss=89.0436
	step [281/332], loss=91.8783
	step [282/332], loss=58.4020
	step [283/332], loss=72.6641
	step [284/332], loss=80.7241
	step [285/332], loss=102.1873
	step [286/332], loss=78.4746
	step [287/332], loss=91.8901
	step [288/332], loss=72.1085
	step [289/332], loss=70.0357
	step [290/332], loss=74.0049
	step [291/332], loss=62.8252
	step [292/332], loss=79.6882
	step [293/332], loss=80.3661
	step [294/332], loss=78.3950
	step [295/332], loss=77.7931
	step [296/332], loss=78.4197
	step [297/332], loss=68.4037
	step [298/332], loss=85.2920
	step [299/332], loss=85.6405
	step [300/332], loss=81.1717
	step [301/332], loss=87.8646
	step [302/332], loss=80.4210
	step [303/332], loss=74.6620
	step [304/332], loss=73.9474
	step [305/332], loss=86.3550
	step [306/332], loss=100.3132
	step [307/332], loss=56.3402
	step [308/332], loss=78.0080
	step [309/332], loss=65.5186
	step [310/332], loss=77.8347
	step [311/332], loss=103.9868
	step [312/332], loss=77.8143
	step [313/332], loss=81.2349
	step [314/332], loss=64.4140
	step [315/332], loss=63.9670
	step [316/332], loss=64.7901
	step [317/332], loss=77.6210
	step [318/332], loss=81.3885
	step [319/332], loss=78.7930
	step [320/332], loss=87.4374
	step [321/332], loss=77.8526
	step [322/332], loss=66.4347
	step [323/332], loss=100.8052
	step [324/332], loss=85.4962
	step [325/332], loss=54.3263
	step [326/332], loss=79.0178
	step [327/332], loss=95.0816
	step [328/332], loss=79.3552
	step [329/332], loss=81.4354
	step [330/332], loss=79.2724
	step [331/332], loss=87.5897
	step [332/332], loss=38.2765
	Evaluating
	loss=0.0232, precision=0.3532, recall=0.9296, f1=0.5120
Training epoch 9
	step [1/332], loss=91.3402
	step [2/332], loss=76.4696
	step [3/332], loss=87.8907
	step [4/332], loss=66.4502
	step [5/332], loss=81.4542
	step [6/332], loss=88.3774
	step [7/332], loss=53.4709
	step [8/332], loss=83.8062
	step [9/332], loss=79.6090
	step [10/332], loss=77.4928
	step [11/332], loss=95.9960
	step [12/332], loss=76.6882
	step [13/332], loss=58.5045
	step [14/332], loss=70.3600
	step [15/332], loss=60.7062
	step [16/332], loss=86.3613
	step [17/332], loss=94.8441
	step [18/332], loss=104.4722
	step [19/332], loss=87.1164
	step [20/332], loss=68.9374
	step [21/332], loss=66.7155
	step [22/332], loss=78.2726
	step [23/332], loss=83.8244
	step [24/332], loss=81.4108
	step [25/332], loss=90.1620
	step [26/332], loss=74.3347
	step [27/332], loss=95.0614
	step [28/332], loss=77.7892
	step [29/332], loss=83.4572
	step [30/332], loss=85.1703
	step [31/332], loss=63.0210
	step [32/332], loss=86.9925
	step [33/332], loss=70.2508
	step [34/332], loss=79.2852
	step [35/332], loss=76.2648
	step [36/332], loss=78.7663
	step [37/332], loss=67.2180
	step [38/332], loss=71.1666
	step [39/332], loss=64.9938
	step [40/332], loss=83.6828
	step [41/332], loss=69.2822
	step [42/332], loss=64.7093
	step [43/332], loss=78.0567
	step [44/332], loss=97.5690
	step [45/332], loss=70.7241
	step [46/332], loss=63.9448
	step [47/332], loss=98.1737
	step [48/332], loss=88.8487
	step [49/332], loss=86.7234
	step [50/332], loss=92.5692
	step [51/332], loss=75.5738
	step [52/332], loss=78.5357
	step [53/332], loss=73.1797
	step [54/332], loss=78.7890
	step [55/332], loss=71.0330
	step [56/332], loss=80.3109
	step [57/332], loss=87.9926
	step [58/332], loss=97.0651
	step [59/332], loss=68.3996
	step [60/332], loss=75.2407
	step [61/332], loss=58.9008
	step [62/332], loss=81.0016
	step [63/332], loss=95.5282
	step [64/332], loss=75.7003
	step [65/332], loss=77.6119
	step [66/332], loss=77.9159
	step [67/332], loss=77.6528
	step [68/332], loss=91.7903
	step [69/332], loss=73.1320
	step [70/332], loss=70.5623
	step [71/332], loss=66.6421
	step [72/332], loss=85.0178
	step [73/332], loss=79.9880
	step [74/332], loss=104.6423
	step [75/332], loss=99.7385
	step [76/332], loss=70.5431
	step [77/332], loss=85.8667
	step [78/332], loss=87.9557
	step [79/332], loss=83.8018
	step [80/332], loss=74.4637
	step [81/332], loss=72.0652
	step [82/332], loss=86.3269
	step [83/332], loss=82.4391
	step [84/332], loss=73.0716
	step [85/332], loss=71.8370
	step [86/332], loss=90.4956
	step [87/332], loss=60.1482
	step [88/332], loss=69.2688
	step [89/332], loss=78.1213
	step [90/332], loss=71.3882
	step [91/332], loss=75.3362
	step [92/332], loss=75.6226
	step [93/332], loss=76.3132
	step [94/332], loss=73.4319
	step [95/332], loss=77.3045
	step [96/332], loss=66.6544
	step [97/332], loss=80.6782
	step [98/332], loss=81.9999
	step [99/332], loss=62.8435
	step [100/332], loss=83.6133
	step [101/332], loss=76.6674
	step [102/332], loss=79.2364
	step [103/332], loss=83.2632
	step [104/332], loss=107.9255
	step [105/332], loss=85.7385
	step [106/332], loss=69.8600
	step [107/332], loss=77.8119
	step [108/332], loss=67.9814
	step [109/332], loss=84.2401
	step [110/332], loss=82.0046
	step [111/332], loss=74.5345
	step [112/332], loss=69.8479
	step [113/332], loss=80.3167
	step [114/332], loss=80.3097
	step [115/332], loss=86.2245
	step [116/332], loss=75.1786
	step [117/332], loss=79.2314
	step [118/332], loss=77.0400
	step [119/332], loss=76.9100
	step [120/332], loss=67.2866
	step [121/332], loss=78.2182
	step [122/332], loss=88.5152
	step [123/332], loss=89.3779
	step [124/332], loss=85.6056
	step [125/332], loss=72.9706
	step [126/332], loss=87.9962
	step [127/332], loss=86.5408
	step [128/332], loss=62.7207
	step [129/332], loss=72.6192
	step [130/332], loss=89.3736
	step [131/332], loss=85.2920
	step [132/332], loss=91.7979
	step [133/332], loss=75.6827
	step [134/332], loss=84.0350
	step [135/332], loss=85.0764
	step [136/332], loss=83.4174
	step [137/332], loss=81.3262
	step [138/332], loss=97.4330
	step [139/332], loss=72.6603
	step [140/332], loss=77.6810
	step [141/332], loss=72.7004
	step [142/332], loss=80.5079
	step [143/332], loss=81.8525
	step [144/332], loss=74.9565
	step [145/332], loss=72.1309
	step [146/332], loss=74.0877
	step [147/332], loss=88.7335
	step [148/332], loss=83.3303
	step [149/332], loss=72.7102
	step [150/332], loss=88.7704
	step [151/332], loss=71.4661
	step [152/332], loss=80.6365
	step [153/332], loss=94.0532
	step [154/332], loss=70.9040
	step [155/332], loss=80.2449
	step [156/332], loss=84.6967
	step [157/332], loss=72.5160
	step [158/332], loss=81.1941
	step [159/332], loss=74.6872
	step [160/332], loss=77.0965
	step [161/332], loss=60.4382
	step [162/332], loss=76.0248
	step [163/332], loss=81.0843
	step [164/332], loss=73.8962
	step [165/332], loss=61.6887
	step [166/332], loss=77.5109
	step [167/332], loss=80.6588
	step [168/332], loss=84.8178
	step [169/332], loss=80.5601
	step [170/332], loss=84.4099
	step [171/332], loss=74.2932
	step [172/332], loss=68.8452
	step [173/332], loss=67.0665
	step [174/332], loss=70.9176
	step [175/332], loss=84.2536
	step [176/332], loss=75.9104
	step [177/332], loss=57.4883
	step [178/332], loss=76.3832
	step [179/332], loss=80.2064
	step [180/332], loss=82.3031
	step [181/332], loss=78.5420
	step [182/332], loss=80.7714
	step [183/332], loss=81.8851
	step [184/332], loss=72.3917
	step [185/332], loss=74.5692
	step [186/332], loss=100.6989
	step [187/332], loss=78.9949
	step [188/332], loss=82.6773
	step [189/332], loss=76.0070
	step [190/332], loss=93.7910
	step [191/332], loss=94.1117
	step [192/332], loss=73.6322
	step [193/332], loss=64.2656
	step [194/332], loss=95.8636
	step [195/332], loss=94.2646
	step [196/332], loss=79.7217
	step [197/332], loss=75.8874
	step [198/332], loss=69.5674
	step [199/332], loss=70.7609
	step [200/332], loss=67.4331
	step [201/332], loss=74.9099
	step [202/332], loss=99.3681
	step [203/332], loss=76.7231
	step [204/332], loss=86.8962
	step [205/332], loss=83.4256
	step [206/332], loss=79.5843
	step [207/332], loss=90.7809
	step [208/332], loss=96.1863
	step [209/332], loss=74.6677
	step [210/332], loss=103.4201
	step [211/332], loss=86.5321
	step [212/332], loss=77.5196
	step [213/332], loss=76.4374
	step [214/332], loss=63.6459
	step [215/332], loss=69.1307
	step [216/332], loss=89.5654
	step [217/332], loss=92.4898
	step [218/332], loss=78.8845
	step [219/332], loss=95.8861
	step [220/332], loss=87.7417
	step [221/332], loss=88.5330
	step [222/332], loss=80.8836
	step [223/332], loss=75.2704
	step [224/332], loss=70.1636
	step [225/332], loss=86.7651
	step [226/332], loss=82.4844
	step [227/332], loss=74.4471
	step [228/332], loss=81.9662
	step [229/332], loss=86.2523
	step [230/332], loss=58.0734
	step [231/332], loss=74.4801
	step [232/332], loss=63.9973
	step [233/332], loss=65.7764
	step [234/332], loss=68.2148
	step [235/332], loss=79.0534
	step [236/332], loss=72.5314
	step [237/332], loss=78.0502
	step [238/332], loss=74.1195
	step [239/332], loss=70.3329
	step [240/332], loss=64.0488
	step [241/332], loss=81.3506
	step [242/332], loss=70.4882
	step [243/332], loss=87.8529
	step [244/332], loss=74.9586
	step [245/332], loss=79.7864
	step [246/332], loss=70.6812
	step [247/332], loss=63.3774
	step [248/332], loss=82.4520
	step [249/332], loss=82.6895
	step [250/332], loss=86.7048
	step [251/332], loss=93.7954
	step [252/332], loss=80.2299
	step [253/332], loss=79.8009
	step [254/332], loss=75.7349
	step [255/332], loss=83.1404
	step [256/332], loss=103.8524
	step [257/332], loss=89.0556
	step [258/332], loss=82.7255
	step [259/332], loss=60.9345
	step [260/332], loss=81.6305
	step [261/332], loss=91.8007
	step [262/332], loss=65.2733
	step [263/332], loss=70.5061
	step [264/332], loss=76.6788
	step [265/332], loss=79.9467
	step [266/332], loss=81.4542
	step [267/332], loss=76.7444
	step [268/332], loss=71.2407
	step [269/332], loss=81.4947
	step [270/332], loss=80.2899
	step [271/332], loss=73.1419
	step [272/332], loss=82.9297
	step [273/332], loss=81.6145
	step [274/332], loss=77.2112
	step [275/332], loss=74.2612
	step [276/332], loss=78.3168
	step [277/332], loss=83.5247
	step [278/332], loss=82.1807
	step [279/332], loss=69.6297
	step [280/332], loss=109.2629
	step [281/332], loss=69.4911
	step [282/332], loss=84.1716
	step [283/332], loss=62.3077
	step [284/332], loss=66.6682
	step [285/332], loss=101.9083
	step [286/332], loss=83.0909
	step [287/332], loss=84.0226
	step [288/332], loss=68.6960
	step [289/332], loss=87.9490
	step [290/332], loss=70.3643
	step [291/332], loss=108.1966
	step [292/332], loss=57.2528
	step [293/332], loss=66.3153
	step [294/332], loss=63.3172
	step [295/332], loss=61.3140
	step [296/332], loss=85.7873
	step [297/332], loss=85.9830
	step [298/332], loss=89.6484
	step [299/332], loss=81.6388
	step [300/332], loss=75.4160
	step [301/332], loss=69.3938
	step [302/332], loss=75.7087
	step [303/332], loss=71.6142
	step [304/332], loss=59.1629
	step [305/332], loss=91.1081
	step [306/332], loss=87.8956
	step [307/332], loss=86.7166
	step [308/332], loss=75.0406
	step [309/332], loss=86.4220
	step [310/332], loss=82.0556
	step [311/332], loss=86.3000
	step [312/332], loss=92.7794
	step [313/332], loss=76.4203
	step [314/332], loss=78.7220
	step [315/332], loss=75.1096
	step [316/332], loss=94.1486
	step [317/332], loss=87.7035
	step [318/332], loss=85.1429
	step [319/332], loss=76.7176
	step [320/332], loss=88.8065
	step [321/332], loss=89.2743
	step [322/332], loss=59.7146
	step [323/332], loss=83.5839
	step [324/332], loss=71.7818
	step [325/332], loss=73.2331
	step [326/332], loss=77.7142
	step [327/332], loss=66.5294
	step [328/332], loss=89.6896
	step [329/332], loss=73.8361
	step [330/332], loss=66.5263
	step [331/332], loss=75.4783
	step [332/332], loss=55.9279
	Evaluating
	loss=0.0192, precision=0.3520, recall=0.9159, f1=0.5086
Training epoch 10
	step [1/332], loss=95.5126
	step [2/332], loss=61.1528
	step [3/332], loss=54.9228
	step [4/332], loss=85.8553
	step [5/332], loss=68.1895
	step [6/332], loss=57.1470
	step [7/332], loss=79.9153
	step [8/332], loss=88.7994
	step [9/332], loss=97.4610
	step [10/332], loss=76.0523
	step [11/332], loss=88.4783
	step [12/332], loss=71.9713
	step [13/332], loss=77.9749
	step [14/332], loss=86.6663
	step [15/332], loss=78.8579
	step [16/332], loss=66.3629
	step [17/332], loss=80.5517
	step [18/332], loss=66.3503
	step [19/332], loss=67.0154
	step [20/332], loss=93.9366
	step [21/332], loss=64.9345
	step [22/332], loss=78.4692
	step [23/332], loss=86.6841
	step [24/332], loss=68.7639
	step [25/332], loss=79.3473
	step [26/332], loss=78.3231
	step [27/332], loss=86.5129
	step [28/332], loss=82.3753
	step [29/332], loss=58.5654
	step [30/332], loss=86.2801
	step [31/332], loss=74.7546
	step [32/332], loss=77.8952
	step [33/332], loss=91.5375
	step [34/332], loss=79.8376
	step [35/332], loss=75.0599
	step [36/332], loss=75.5846
	step [37/332], loss=80.7760
	step [38/332], loss=84.8788
	step [39/332], loss=76.1164
	step [40/332], loss=91.4631
	step [41/332], loss=87.2436
	step [42/332], loss=87.9761
	step [43/332], loss=92.6857
	step [44/332], loss=70.7986
	step [45/332], loss=87.9279
	step [46/332], loss=71.8370
	step [47/332], loss=99.4640
	step [48/332], loss=80.2338
	step [49/332], loss=90.3460
	step [50/332], loss=76.1829
	step [51/332], loss=74.8225
	step [52/332], loss=96.7124
	step [53/332], loss=74.0777
	step [54/332], loss=75.6502
	step [55/332], loss=67.6564
	step [56/332], loss=64.7163
	step [57/332], loss=76.8526
	step [58/332], loss=80.5949
	step [59/332], loss=89.4361
	step [60/332], loss=74.7140
	step [61/332], loss=65.4573
	step [62/332], loss=63.5021
	step [63/332], loss=83.6511
	step [64/332], loss=65.2949
	step [65/332], loss=78.6613
	step [66/332], loss=81.2049
	step [67/332], loss=61.3188
	step [68/332], loss=85.6792
	step [69/332], loss=77.9355
	step [70/332], loss=84.6271
	step [71/332], loss=75.9241
	step [72/332], loss=81.4316
	step [73/332], loss=80.1322
	step [74/332], loss=69.2971
	step [75/332], loss=71.1650
	step [76/332], loss=85.7466
	step [77/332], loss=88.6254
	step [78/332], loss=67.7767
	step [79/332], loss=73.7032
	step [80/332], loss=79.4248
	step [81/332], loss=69.2455
	step [82/332], loss=70.0394
	step [83/332], loss=77.0914
	step [84/332], loss=92.1403
	step [85/332], loss=89.6016
	step [86/332], loss=76.0448
	step [87/332], loss=85.4980
	step [88/332], loss=89.8515
	step [89/332], loss=67.7477
	step [90/332], loss=70.7864
	step [91/332], loss=82.1790
	step [92/332], loss=63.0572
	step [93/332], loss=92.5889
	step [94/332], loss=75.4298
	step [95/332], loss=73.1994
	step [96/332], loss=77.8868
	step [97/332], loss=78.2096
	step [98/332], loss=78.1052
	step [99/332], loss=70.1025
	step [100/332], loss=80.1673
	step [101/332], loss=83.4405
	step [102/332], loss=77.8463
	step [103/332], loss=80.7262
	step [104/332], loss=76.6016
	step [105/332], loss=84.7842
	step [106/332], loss=88.1535
	step [107/332], loss=74.5186
	step [108/332], loss=90.7692
	step [109/332], loss=76.2927
	step [110/332], loss=67.2490
	step [111/332], loss=80.8165
	step [112/332], loss=103.1494
	step [113/332], loss=86.5939
	step [114/332], loss=79.5642
	step [115/332], loss=95.2915
	step [116/332], loss=92.7555
	step [117/332], loss=90.4708
	step [118/332], loss=70.8652
	step [119/332], loss=71.6146
	step [120/332], loss=84.0006
	step [121/332], loss=83.7700
	step [122/332], loss=82.2712
	step [123/332], loss=84.9587
	step [124/332], loss=70.8037
	step [125/332], loss=81.5077
	step [126/332], loss=62.1067
	step [127/332], loss=68.9067
	step [128/332], loss=59.4796
	step [129/332], loss=88.8234
	step [130/332], loss=71.7931
	step [131/332], loss=72.2045
	step [132/332], loss=85.9070
	step [133/332], loss=81.5412
	step [134/332], loss=82.8616
	step [135/332], loss=103.0206
	step [136/332], loss=92.4425
	step [137/332], loss=80.7695
	step [138/332], loss=84.7545
	step [139/332], loss=89.3245
	step [140/332], loss=75.8559
	step [141/332], loss=85.0774
	step [142/332], loss=66.4777
	step [143/332], loss=57.2013
	step [144/332], loss=69.3288
	step [145/332], loss=94.4383
	step [146/332], loss=71.1226
	step [147/332], loss=80.9703
	step [148/332], loss=82.2127
	step [149/332], loss=75.4310
	step [150/332], loss=92.9904
	step [151/332], loss=76.6695
	step [152/332], loss=56.8688
	step [153/332], loss=76.6633
	step [154/332], loss=96.0562
	step [155/332], loss=71.3641
	step [156/332], loss=55.7582
	step [157/332], loss=79.9374
	step [158/332], loss=80.3696
	step [159/332], loss=68.8330
	step [160/332], loss=74.8422
	step [161/332], loss=86.1784
	step [162/332], loss=70.4848
	step [163/332], loss=84.3569
	step [164/332], loss=71.5356
	step [165/332], loss=95.5130
	step [166/332], loss=79.2692
	step [167/332], loss=79.5694
	step [168/332], loss=55.4336
	step [169/332], loss=91.9036
	step [170/332], loss=86.7671
	step [171/332], loss=80.5761
	step [172/332], loss=99.7313
	step [173/332], loss=93.0829
	step [174/332], loss=64.5521
	step [175/332], loss=82.2390
	step [176/332], loss=83.3998
	step [177/332], loss=69.8852
	step [178/332], loss=74.4030
	step [179/332], loss=66.0355
	step [180/332], loss=67.9230
	step [181/332], loss=90.0157
	step [182/332], loss=71.3947
	step [183/332], loss=86.4208
	step [184/332], loss=66.7297
	step [185/332], loss=69.3008
	step [186/332], loss=82.4808
	step [187/332], loss=72.4648
	step [188/332], loss=83.1998
	step [189/332], loss=82.8263
	step [190/332], loss=83.6261
	step [191/332], loss=65.2254
	step [192/332], loss=64.4985
	step [193/332], loss=93.4835
	step [194/332], loss=79.6007
	step [195/332], loss=62.6987
	step [196/332], loss=91.5333
	step [197/332], loss=81.7656
	step [198/332], loss=85.3154
	step [199/332], loss=85.8765
	step [200/332], loss=76.2516
	step [201/332], loss=69.3936
	step [202/332], loss=66.2024
	step [203/332], loss=70.6495
	step [204/332], loss=68.5594
	step [205/332], loss=71.4361
	step [206/332], loss=93.2256
	step [207/332], loss=64.8369
	step [208/332], loss=77.3841
	step [209/332], loss=84.1482
	step [210/332], loss=74.0358
	step [211/332], loss=80.2159
	step [212/332], loss=69.5725
	step [213/332], loss=67.8768
	step [214/332], loss=86.8715
	step [215/332], loss=94.7315
	step [216/332], loss=76.0229
	step [217/332], loss=88.7782
	step [218/332], loss=74.5089
	step [219/332], loss=78.4486
	step [220/332], loss=73.0678
	step [221/332], loss=74.8633
	step [222/332], loss=92.0694
	step [223/332], loss=68.8427
	step [224/332], loss=65.7567
	step [225/332], loss=79.7053
	step [226/332], loss=85.1408
	step [227/332], loss=68.7456
	step [228/332], loss=77.8997
	step [229/332], loss=73.8213
	step [230/332], loss=83.5959
	step [231/332], loss=63.5804
	step [232/332], loss=84.5473
	step [233/332], loss=80.0700
	step [234/332], loss=95.1257
	step [235/332], loss=95.6849
	step [236/332], loss=80.5825
	step [237/332], loss=79.7705
	step [238/332], loss=74.3895
	step [239/332], loss=76.6354
	step [240/332], loss=83.0230
	step [241/332], loss=66.5794
	step [242/332], loss=90.5576
	step [243/332], loss=83.5395
	step [244/332], loss=86.3328
	step [245/332], loss=65.8801
	step [246/332], loss=77.6503
	step [247/332], loss=82.3975
	step [248/332], loss=72.5393
	step [249/332], loss=81.4251
	step [250/332], loss=70.2209
	step [251/332], loss=77.0785
	step [252/332], loss=79.6072
	step [253/332], loss=69.1603
	step [254/332], loss=78.4414
	step [255/332], loss=67.5642
	step [256/332], loss=76.8963
	step [257/332], loss=79.6179
	step [258/332], loss=74.6135
	step [259/332], loss=82.6432
	step [260/332], loss=80.2406
	step [261/332], loss=94.1222
	step [262/332], loss=63.9855
	step [263/332], loss=81.1828
	step [264/332], loss=72.4571
	step [265/332], loss=77.0297
	step [266/332], loss=95.9902
	step [267/332], loss=68.8368
	step [268/332], loss=90.5168
	step [269/332], loss=119.9096
	step [270/332], loss=83.6118
	step [271/332], loss=69.0870
	step [272/332], loss=71.9998
	step [273/332], loss=89.9268
	step [274/332], loss=66.4499
	step [275/332], loss=82.1043
	step [276/332], loss=61.3088
	step [277/332], loss=88.0045
	step [278/332], loss=82.9617
	step [279/332], loss=82.6169
	step [280/332], loss=73.1397
	step [281/332], loss=71.0302
	step [282/332], loss=73.6961
	step [283/332], loss=73.8705
	step [284/332], loss=77.7713
	step [285/332], loss=64.2725
	step [286/332], loss=68.3345
	step [287/332], loss=89.9953
	step [288/332], loss=81.6855
	step [289/332], loss=55.8438
	step [290/332], loss=73.3608
	step [291/332], loss=81.9348
	step [292/332], loss=85.3626
	step [293/332], loss=56.1424
	step [294/332], loss=63.0686
	step [295/332], loss=71.9526
	step [296/332], loss=84.1914
	step [297/332], loss=83.0830
	step [298/332], loss=77.0615
	step [299/332], loss=76.7551
	step [300/332], loss=78.1400
	step [301/332], loss=63.6332
	step [302/332], loss=79.2802
	step [303/332], loss=89.7995
	step [304/332], loss=86.4010
	step [305/332], loss=71.2409
	step [306/332], loss=80.1896
	step [307/332], loss=81.7470
	step [308/332], loss=80.2797
	step [309/332], loss=76.2456
	step [310/332], loss=71.2068
	step [311/332], loss=71.3213
	step [312/332], loss=85.3302
	step [313/332], loss=80.1689
	step [314/332], loss=59.0174
	step [315/332], loss=72.8864
	step [316/332], loss=84.2273
	step [317/332], loss=98.1582
	step [318/332], loss=76.7906
	step [319/332], loss=72.6500
	step [320/332], loss=68.9068
	step [321/332], loss=71.0415
	step [322/332], loss=61.9934
	step [323/332], loss=79.4054
	step [324/332], loss=71.2092
	step [325/332], loss=68.2295
	step [326/332], loss=87.3517
	step [327/332], loss=84.7033
	step [328/332], loss=55.8083
	step [329/332], loss=68.8896
	step [330/332], loss=79.1223
	step [331/332], loss=83.5959
	step [332/332], loss=47.3705
	Evaluating
	loss=0.0171, precision=0.3390, recall=0.9053, f1=0.4933
Training epoch 11
	step [1/332], loss=79.9925
	step [2/332], loss=94.2279
	step [3/332], loss=85.9495
	step [4/332], loss=95.6291
	step [5/332], loss=71.9534
	step [6/332], loss=96.1490
	step [7/332], loss=84.3044
	step [8/332], loss=78.0215
	step [9/332], loss=87.0636
	step [10/332], loss=76.3121
	step [11/332], loss=63.3484
	step [12/332], loss=71.9666
	step [13/332], loss=73.0672
	step [14/332], loss=68.7338
	step [15/332], loss=76.1507
	step [16/332], loss=83.1769
	step [17/332], loss=90.6074
	step [18/332], loss=79.6568
	step [19/332], loss=76.2461
	step [20/332], loss=76.5797
	step [21/332], loss=66.9438
	step [22/332], loss=61.6994
	step [23/332], loss=81.5315
	step [24/332], loss=78.3943
	step [25/332], loss=75.0475
	step [26/332], loss=69.5996
	step [27/332], loss=65.0767
	step [28/332], loss=59.2901
	step [29/332], loss=80.8479
	step [30/332], loss=76.4977
	step [31/332], loss=68.3789
	step [32/332], loss=63.8718
	step [33/332], loss=68.5731
	step [34/332], loss=77.2113
	step [35/332], loss=92.3918
	step [36/332], loss=71.3886
	step [37/332], loss=78.5328
	step [38/332], loss=84.1137
	step [39/332], loss=75.1119
	step [40/332], loss=82.9404
	step [41/332], loss=87.3743
	step [42/332], loss=80.6407
	step [43/332], loss=72.6811
	step [44/332], loss=66.1890
	step [45/332], loss=85.4567
	step [46/332], loss=70.7570
	step [47/332], loss=85.4830
	step [48/332], loss=85.7157
	step [49/332], loss=59.0816
	step [50/332], loss=84.8481
	step [51/332], loss=63.6466
	step [52/332], loss=69.4002
	step [53/332], loss=69.0291
	step [54/332], loss=72.2096
	step [55/332], loss=78.0044
	step [56/332], loss=71.1566
	step [57/332], loss=77.4715
	step [58/332], loss=77.8903
	step [59/332], loss=76.6067
	step [60/332], loss=80.5402
	step [61/332], loss=84.9225
	step [62/332], loss=82.7108
	step [63/332], loss=69.1059
	step [64/332], loss=84.8403
	step [65/332], loss=96.3914
	step [66/332], loss=93.1374
	step [67/332], loss=77.7973
	step [68/332], loss=80.7864
	step [69/332], loss=89.8570
	step [70/332], loss=74.2677
	step [71/332], loss=81.1289
	step [72/332], loss=95.2512
	step [73/332], loss=72.4771
	step [74/332], loss=83.0508
	step [75/332], loss=83.7581
	step [76/332], loss=84.7646
	step [77/332], loss=84.7953
	step [78/332], loss=75.9877
	step [79/332], loss=85.5632
	step [80/332], loss=67.9684
	step [81/332], loss=88.3871
	step [82/332], loss=76.0501
	step [83/332], loss=70.6231
	step [84/332], loss=66.7162
	step [85/332], loss=71.9198
	step [86/332], loss=67.6915
	step [87/332], loss=86.9231
	step [88/332], loss=72.0732
	step [89/332], loss=77.3947
	step [90/332], loss=81.3534
	step [91/332], loss=72.3538
	step [92/332], loss=78.1000
	step [93/332], loss=83.2467
	step [94/332], loss=92.9825
	step [95/332], loss=77.4597
	step [96/332], loss=79.3193
	step [97/332], loss=68.0560
	step [98/332], loss=114.5966
	step [99/332], loss=81.2125
	step [100/332], loss=77.3087
	step [101/332], loss=81.1949
	step [102/332], loss=63.6385
	step [103/332], loss=79.5404
	step [104/332], loss=80.5743
	step [105/332], loss=74.5401
	step [106/332], loss=88.6650
	step [107/332], loss=77.6333
	step [108/332], loss=79.7049
	step [109/332], loss=65.0505
	step [110/332], loss=75.1282
	step [111/332], loss=76.1351
	step [112/332], loss=84.8543
	step [113/332], loss=86.9051
	step [114/332], loss=90.0635
	step [115/332], loss=90.3756
	step [116/332], loss=66.3209
	step [117/332], loss=90.0096
	step [118/332], loss=66.1805
	step [119/332], loss=93.7261
	step [120/332], loss=56.1397
	step [121/332], loss=68.7520
	step [122/332], loss=89.2965
	step [123/332], loss=56.9194
	step [124/332], loss=68.4266
	step [125/332], loss=73.3878
	step [126/332], loss=94.0717
	step [127/332], loss=77.8167
	step [128/332], loss=83.1150
	step [129/332], loss=85.6348
	step [130/332], loss=78.3723
	step [131/332], loss=81.0884
	step [132/332], loss=67.1600
	step [133/332], loss=74.9630
	step [134/332], loss=62.1035
	step [135/332], loss=89.7107
	step [136/332], loss=77.0547
	step [137/332], loss=69.3906
	step [138/332], loss=80.5463
	step [139/332], loss=84.5597
	step [140/332], loss=58.6497
	step [141/332], loss=70.9282
	step [142/332], loss=77.0373
	step [143/332], loss=54.5926
	step [144/332], loss=88.8730
	step [145/332], loss=73.7149
	step [146/332], loss=85.7737
	step [147/332], loss=71.3701
	step [148/332], loss=81.8219
	step [149/332], loss=69.6038
	step [150/332], loss=78.4583
	step [151/332], loss=78.1019
	step [152/332], loss=63.9235
	step [153/332], loss=83.5457
	step [154/332], loss=85.7492
	step [155/332], loss=70.8310
	step [156/332], loss=74.5587
	step [157/332], loss=72.0230
	step [158/332], loss=73.6494
	step [159/332], loss=95.5293
	step [160/332], loss=71.9561
	step [161/332], loss=60.3746
	step [162/332], loss=59.3045
	step [163/332], loss=72.8096
	step [164/332], loss=77.4024
	step [165/332], loss=94.2428
	step [166/332], loss=71.2296
	step [167/332], loss=80.3898
	step [168/332], loss=74.2775
	step [169/332], loss=78.1086
	step [170/332], loss=83.2105
	step [171/332], loss=70.7030
	step [172/332], loss=75.4094
	step [173/332], loss=68.1194
	step [174/332], loss=89.8290
	step [175/332], loss=69.0431
	step [176/332], loss=73.7316
	step [177/332], loss=69.6703
	step [178/332], loss=77.2289
	step [179/332], loss=88.0981
	step [180/332], loss=76.8801
	step [181/332], loss=66.5255
	step [182/332], loss=63.4737
	step [183/332], loss=80.5247
	step [184/332], loss=69.8771
	step [185/332], loss=80.3826
	step [186/332], loss=75.1844
	step [187/332], loss=86.8149
	step [188/332], loss=68.2504
	step [189/332], loss=77.9034
	step [190/332], loss=64.4697
	step [191/332], loss=81.5233
	step [192/332], loss=81.9314
	step [193/332], loss=78.8219
	step [194/332], loss=67.9056
	step [195/332], loss=84.8314
	step [196/332], loss=67.5774
	step [197/332], loss=87.8992
	step [198/332], loss=68.8705
	step [199/332], loss=80.3269
	step [200/332], loss=64.4882
	step [201/332], loss=81.8968
	step [202/332], loss=77.4717
	step [203/332], loss=68.7543
	step [204/332], loss=76.6280
	step [205/332], loss=76.9157
	step [206/332], loss=76.0313
	step [207/332], loss=67.8729
	step [208/332], loss=67.5044
	step [209/332], loss=65.0336
	step [210/332], loss=75.5937
	step [211/332], loss=75.4160
	step [212/332], loss=80.8006
	step [213/332], loss=58.9317
	step [214/332], loss=79.9834
	step [215/332], loss=84.4764
	step [216/332], loss=72.9216
	step [217/332], loss=62.9749
	step [218/332], loss=88.3738
	step [219/332], loss=69.9409
	step [220/332], loss=92.4516
	step [221/332], loss=84.5724
	step [222/332], loss=55.1837
	step [223/332], loss=74.8335
	step [224/332], loss=88.3315
	step [225/332], loss=74.5996
	step [226/332], loss=57.5097
	step [227/332], loss=80.4895
	step [228/332], loss=68.0656
	step [229/332], loss=85.8691
	step [230/332], loss=77.0726
	step [231/332], loss=81.0226
	step [232/332], loss=80.9466
	step [233/332], loss=60.5726
	step [234/332], loss=86.3648
	step [235/332], loss=79.5140
	step [236/332], loss=92.5385
	step [237/332], loss=75.3199
	step [238/332], loss=80.7769
	step [239/332], loss=84.3757
	step [240/332], loss=79.3296
	step [241/332], loss=70.6895
	step [242/332], loss=93.7517
	step [243/332], loss=93.9003
	step [244/332], loss=77.5418
	step [245/332], loss=78.4690
	step [246/332], loss=93.4902
	step [247/332], loss=75.8610
	step [248/332], loss=82.0705
	step [249/332], loss=75.8213
	step [250/332], loss=71.6898
	step [251/332], loss=92.2857
	step [252/332], loss=80.7913
	step [253/332], loss=63.2237
	step [254/332], loss=86.5297
	step [255/332], loss=65.3877
	step [256/332], loss=73.0894
	step [257/332], loss=57.0406
	step [258/332], loss=77.7677
	step [259/332], loss=76.2233
	step [260/332], loss=89.1743
	step [261/332], loss=66.0216
	step [262/332], loss=93.1300
	step [263/332], loss=73.7948
	step [264/332], loss=69.4314
	step [265/332], loss=79.5176
	step [266/332], loss=85.5515
	step [267/332], loss=82.0642
	step [268/332], loss=84.4217
	step [269/332], loss=88.1944
	step [270/332], loss=76.9643
	step [271/332], loss=82.0709
	step [272/332], loss=56.0345
	step [273/332], loss=65.7429
	step [274/332], loss=68.1777
	step [275/332], loss=82.0424
	step [276/332], loss=67.2609
	step [277/332], loss=72.8003
	step [278/332], loss=69.6402
	step [279/332], loss=69.9110
	step [280/332], loss=80.5150
	step [281/332], loss=82.4499
	step [282/332], loss=85.7732
	step [283/332], loss=66.4024
	step [284/332], loss=78.5804
	step [285/332], loss=74.0640
	step [286/332], loss=73.1303
	step [287/332], loss=71.1904
	step [288/332], loss=88.3705
	step [289/332], loss=84.5224
	step [290/332], loss=65.5691
	step [291/332], loss=69.6398
	step [292/332], loss=82.9951
	step [293/332], loss=78.5299
	step [294/332], loss=80.5066
	step [295/332], loss=75.1274
	step [296/332], loss=74.6745
	step [297/332], loss=81.4462
	step [298/332], loss=65.5778
	step [299/332], loss=71.4262
	step [300/332], loss=74.7531
	step [301/332], loss=79.1017
	step [302/332], loss=86.6951
	step [303/332], loss=79.6445
	step [304/332], loss=93.2973
	step [305/332], loss=62.7152
	step [306/332], loss=84.4571
	step [307/332], loss=72.8916
	step [308/332], loss=76.6521
	step [309/332], loss=75.8996
	step [310/332], loss=97.5643
	step [311/332], loss=67.6014
	step [312/332], loss=86.9832
	step [313/332], loss=72.8570
	step [314/332], loss=83.8097
	step [315/332], loss=60.9526
	step [316/332], loss=74.2397
	step [317/332], loss=80.2274
	step [318/332], loss=72.9277
	step [319/332], loss=78.4478
	step [320/332], loss=71.5947
	step [321/332], loss=71.5817
	step [322/332], loss=87.8951
	step [323/332], loss=63.6854
	step [324/332], loss=68.0392
	step [325/332], loss=63.6846
	step [326/332], loss=63.7041
	step [327/332], loss=87.3728
	step [328/332], loss=69.2754
	step [329/332], loss=81.6954
	step [330/332], loss=68.0974
	step [331/332], loss=75.6127
	step [332/332], loss=56.7944
	Evaluating
	loss=0.0143, precision=0.3594, recall=0.9228, f1=0.5173
Training epoch 12
	step [1/332], loss=82.6553
	step [2/332], loss=77.0840
	step [3/332], loss=82.3906
	step [4/332], loss=77.4495
	step [5/332], loss=70.4889
	step [6/332], loss=83.2076
	step [7/332], loss=79.3748
	step [8/332], loss=82.3974
	step [9/332], loss=69.6064
	step [10/332], loss=70.8247
	step [11/332], loss=63.1022
	step [12/332], loss=71.2932
	step [13/332], loss=86.8922
	step [14/332], loss=75.9146
	step [15/332], loss=78.6230
	step [16/332], loss=77.8376
	step [17/332], loss=81.3662
	step [18/332], loss=74.3919
	step [19/332], loss=63.4718
	step [20/332], loss=74.8667
	step [21/332], loss=56.6744
	step [22/332], loss=99.1793
	step [23/332], loss=66.0675
	step [24/332], loss=84.9465
	step [25/332], loss=78.7024
	step [26/332], loss=70.0978
	step [27/332], loss=83.4312
	step [28/332], loss=71.6445
	step [29/332], loss=88.9881
	step [30/332], loss=58.7329
	step [31/332], loss=64.5017
	step [32/332], loss=68.6551
	step [33/332], loss=69.4293
	step [34/332], loss=75.6542
	step [35/332], loss=71.1548
	step [36/332], loss=70.5841
	step [37/332], loss=89.6210
	step [38/332], loss=86.5178
	step [39/332], loss=69.1298
	step [40/332], loss=67.5004
	step [41/332], loss=75.8125
	step [42/332], loss=70.7056
	step [43/332], loss=78.6326
	step [44/332], loss=78.5154
	step [45/332], loss=74.6292
	step [46/332], loss=74.6391
	step [47/332], loss=82.4527
	step [48/332], loss=81.7038
	step [49/332], loss=90.0661
	step [50/332], loss=67.4832
	step [51/332], loss=84.5121
	step [52/332], loss=68.5905
	step [53/332], loss=55.6871
	step [54/332], loss=72.7207
	step [55/332], loss=81.4555
	step [56/332], loss=63.1763
	step [57/332], loss=65.6589
	step [58/332], loss=61.9879
	step [59/332], loss=81.3501
	step [60/332], loss=84.0527
	step [61/332], loss=77.3965
	step [62/332], loss=76.0585
	step [63/332], loss=94.7583
	step [64/332], loss=84.6695
	step [65/332], loss=80.5191
	step [66/332], loss=84.6103
	step [67/332], loss=72.8517
	step [68/332], loss=69.4099
	step [69/332], loss=85.8303
	step [70/332], loss=58.0997
	step [71/332], loss=73.3684
	step [72/332], loss=79.0555
	step [73/332], loss=77.1606
	step [74/332], loss=87.5227
	step [75/332], loss=71.1623
	step [76/332], loss=78.5743
	step [77/332], loss=71.2177
	step [78/332], loss=78.2177
	step [79/332], loss=80.0710
	step [80/332], loss=83.2447
	step [81/332], loss=90.4776
	step [82/332], loss=87.0794
	step [83/332], loss=72.8176
	step [84/332], loss=85.5924
	step [85/332], loss=69.5574
	step [86/332], loss=69.3236
	step [87/332], loss=85.1423
	step [88/332], loss=66.5853
	step [89/332], loss=81.7571
	step [90/332], loss=72.3890
	step [91/332], loss=87.5334
	step [92/332], loss=72.5791
	step [93/332], loss=77.5907
	step [94/332], loss=81.1498
	step [95/332], loss=78.9724
	step [96/332], loss=69.6370
	step [97/332], loss=72.2593
	step [98/332], loss=78.2976
	step [99/332], loss=66.9275
	step [100/332], loss=93.9333
	step [101/332], loss=92.0180
	step [102/332], loss=74.8468
	step [103/332], loss=71.1008
	step [104/332], loss=75.6022
	step [105/332], loss=87.6542
	step [106/332], loss=61.4366
	step [107/332], loss=85.1562
	step [108/332], loss=79.3965
	step [109/332], loss=78.3440
	step [110/332], loss=74.9700
	step [111/332], loss=65.2967
	step [112/332], loss=97.9547
	step [113/332], loss=78.1637
	step [114/332], loss=65.7272
	step [115/332], loss=69.3071
	step [116/332], loss=77.3947
	step [117/332], loss=66.7047
	step [118/332], loss=83.9808
	step [119/332], loss=76.2456
	step [120/332], loss=75.7185
	step [121/332], loss=54.9031
	step [122/332], loss=73.2011
	step [123/332], loss=69.4977
	step [124/332], loss=84.0482
	step [125/332], loss=55.8696
	step [126/332], loss=89.5946
	step [127/332], loss=91.5625
	step [128/332], loss=77.9620
	step [129/332], loss=76.2718
	step [130/332], loss=75.2825
	step [131/332], loss=82.0839
	step [132/332], loss=78.8406
	step [133/332], loss=82.6013
	step [134/332], loss=84.7832
	step [135/332], loss=86.2282
	step [136/332], loss=78.8378
	step [137/332], loss=62.5695
	step [138/332], loss=88.7906
	step [139/332], loss=84.9140
	step [140/332], loss=78.7750
	step [141/332], loss=91.3761
	step [142/332], loss=80.2017
	step [143/332], loss=75.2992
	step [144/332], loss=70.2382
	step [145/332], loss=69.5012
	step [146/332], loss=69.8105
	step [147/332], loss=71.4525
	step [148/332], loss=86.1925
	step [149/332], loss=93.4312
	step [150/332], loss=74.1538
	step [151/332], loss=76.2603
	step [152/332], loss=72.2179
	step [153/332], loss=83.1029
	step [154/332], loss=75.6736
	step [155/332], loss=84.6085
	step [156/332], loss=72.1961
	step [157/332], loss=86.7932
	step [158/332], loss=93.0467
	step [159/332], loss=77.9372
	step [160/332], loss=62.3547
	step [161/332], loss=64.7606
	step [162/332], loss=89.2163
	step [163/332], loss=53.0870
	step [164/332], loss=76.0943
	step [165/332], loss=65.7031
	step [166/332], loss=75.4409
	step [167/332], loss=66.4829
	step [168/332], loss=73.3148
	step [169/332], loss=69.2305
	step [170/332], loss=74.0398
	step [171/332], loss=81.6094
	step [172/332], loss=69.0712
	step [173/332], loss=66.2544
	step [174/332], loss=74.3049
	step [175/332], loss=77.7412
	step [176/332], loss=70.5162
	step [177/332], loss=73.3016
	step [178/332], loss=66.9056
	step [179/332], loss=71.8346
	step [180/332], loss=69.5608
	step [181/332], loss=71.0166
	step [182/332], loss=97.2272
	step [183/332], loss=62.6779
	step [184/332], loss=66.0697
	step [185/332], loss=72.5652
	step [186/332], loss=78.1909
	step [187/332], loss=73.7696
	step [188/332], loss=86.7203
	step [189/332], loss=85.0543
	step [190/332], loss=84.1797
	step [191/332], loss=77.7386
	step [192/332], loss=74.7389
	step [193/332], loss=73.9615
	step [194/332], loss=67.9945
	step [195/332], loss=80.0317
	step [196/332], loss=96.8549
	step [197/332], loss=74.9528
	step [198/332], loss=72.4515
	step [199/332], loss=79.2944
	step [200/332], loss=66.3996
	step [201/332], loss=71.4806
	step [202/332], loss=83.3451
	step [203/332], loss=75.4108
	step [204/332], loss=66.4570
	step [205/332], loss=80.1011
	step [206/332], loss=84.5092
	step [207/332], loss=78.1354
	step [208/332], loss=90.9702
	step [209/332], loss=89.9214
	step [210/332], loss=67.7132
	step [211/332], loss=72.1770
	step [212/332], loss=75.6295
	step [213/332], loss=73.0542
	step [214/332], loss=87.6471
	step [215/332], loss=72.1108
	step [216/332], loss=85.5995
	step [217/332], loss=62.5009
	step [218/332], loss=87.0856
	step [219/332], loss=72.2369
	step [220/332], loss=61.2305
	step [221/332], loss=91.4916
	step [222/332], loss=65.5110
	step [223/332], loss=62.5970
	step [224/332], loss=80.4205
	step [225/332], loss=81.6550
	step [226/332], loss=74.6481
	step [227/332], loss=80.3156
	step [228/332], loss=90.6480
	step [229/332], loss=69.3863
	step [230/332], loss=75.8382
	step [231/332], loss=49.8724
	step [232/332], loss=59.7045
	step [233/332], loss=73.3317
	step [234/332], loss=83.5887
	step [235/332], loss=59.9035
	step [236/332], loss=54.7061
	step [237/332], loss=67.9091
	step [238/332], loss=88.2806
	step [239/332], loss=77.0326
	step [240/332], loss=66.6861
	step [241/332], loss=67.2019
	step [242/332], loss=66.4301
	step [243/332], loss=69.8233
	step [244/332], loss=56.3770
	step [245/332], loss=75.0233
	step [246/332], loss=72.2556
	step [247/332], loss=68.4145
	step [248/332], loss=81.3051
	step [249/332], loss=65.1491
	step [250/332], loss=81.7399
	step [251/332], loss=66.7048
	step [252/332], loss=72.7934
	step [253/332], loss=82.1121
	step [254/332], loss=64.7272
	step [255/332], loss=66.1568
	step [256/332], loss=92.8397
	step [257/332], loss=74.8913
	step [258/332], loss=69.7844
	step [259/332], loss=88.3026
	step [260/332], loss=85.4141
	step [261/332], loss=74.7496
	step [262/332], loss=68.8200
	step [263/332], loss=72.1672
	step [264/332], loss=66.3502
	step [265/332], loss=84.6029
	step [266/332], loss=70.8495
	step [267/332], loss=83.9768
	step [268/332], loss=65.0266
	step [269/332], loss=90.4261
	step [270/332], loss=67.5747
	step [271/332], loss=80.4999
	step [272/332], loss=83.8913
	step [273/332], loss=85.2453
	step [274/332], loss=80.6790
	step [275/332], loss=75.8128
	step [276/332], loss=76.0955
	step [277/332], loss=72.3582
	step [278/332], loss=77.9935
	step [279/332], loss=86.1351
	step [280/332], loss=61.1067
	step [281/332], loss=64.3764
	step [282/332], loss=94.6317
	step [283/332], loss=75.8130
	step [284/332], loss=86.5802
	step [285/332], loss=78.7191
	step [286/332], loss=76.0212
	step [287/332], loss=86.7631
	step [288/332], loss=82.0632
	step [289/332], loss=85.5345
	step [290/332], loss=67.5548
	step [291/332], loss=77.1299
	step [292/332], loss=77.4197
	step [293/332], loss=90.5830
	step [294/332], loss=68.9296
	step [295/332], loss=69.8825
	step [296/332], loss=68.5932
	step [297/332], loss=68.0850
	step [298/332], loss=68.0449
	step [299/332], loss=68.6482
	step [300/332], loss=79.1799
	step [301/332], loss=76.6106
	step [302/332], loss=64.5117
	step [303/332], loss=71.2575
	step [304/332], loss=60.5172
	step [305/332], loss=72.1969
	step [306/332], loss=69.0649
	step [307/332], loss=77.6285
	step [308/332], loss=59.7314
	step [309/332], loss=62.5822
	step [310/332], loss=70.3727
	step [311/332], loss=75.5550
	step [312/332], loss=78.4609
	step [313/332], loss=69.2860
	step [314/332], loss=83.3585
	step [315/332], loss=82.4239
	step [316/332], loss=74.0680
	step [317/332], loss=82.6185
	step [318/332], loss=81.9031
	step [319/332], loss=81.7735
	step [320/332], loss=76.8316
	step [321/332], loss=79.7259
	step [322/332], loss=91.1107
	step [323/332], loss=60.8472
	step [324/332], loss=78.4066
	step [325/332], loss=95.5239
	step [326/332], loss=73.8950
	step [327/332], loss=66.7155
	step [328/332], loss=81.6428
	step [329/332], loss=93.4448
	step [330/332], loss=79.1217
	step [331/332], loss=68.9677
	step [332/332], loss=36.8815
	Evaluating
	loss=0.0140, precision=0.3223, recall=0.9224, f1=0.4777
Training epoch 13
	step [1/332], loss=80.9502
	step [2/332], loss=63.4496
	step [3/332], loss=58.6502
	step [4/332], loss=61.0323
	step [5/332], loss=64.9990
	step [6/332], loss=71.7487
	step [7/332], loss=78.6084
	step [8/332], loss=66.7838
	step [9/332], loss=70.0879
	step [10/332], loss=78.9665
	step [11/332], loss=74.7596
	step [12/332], loss=90.2194
	step [13/332], loss=76.1382
	step [14/332], loss=71.2626
	step [15/332], loss=87.9334
	step [16/332], loss=82.1316
	step [17/332], loss=81.4001
	step [18/332], loss=74.1089
	step [19/332], loss=74.3060
	step [20/332], loss=75.2610
	step [21/332], loss=77.2682
	step [22/332], loss=70.6672
	step [23/332], loss=81.5472
	step [24/332], loss=76.7042
	step [25/332], loss=84.0233
	step [26/332], loss=58.1167
	step [27/332], loss=67.4532
	step [28/332], loss=106.8526
	step [29/332], loss=77.9174
	step [30/332], loss=82.3241
	step [31/332], loss=67.5620
	step [32/332], loss=87.1983
	step [33/332], loss=67.6004
	step [34/332], loss=84.2770
	step [35/332], loss=72.0321
	step [36/332], loss=71.9453
	step [37/332], loss=75.6000
	step [38/332], loss=67.2088
	step [39/332], loss=84.3196
	step [40/332], loss=81.5672
	step [41/332], loss=95.8063
	step [42/332], loss=62.7132
	step [43/332], loss=52.3727
	step [44/332], loss=70.8557
	step [45/332], loss=82.7989
	step [46/332], loss=78.8063
	step [47/332], loss=71.7449
	step [48/332], loss=77.4930
	step [49/332], loss=84.8053
	step [50/332], loss=82.3835
	step [51/332], loss=74.3716
	step [52/332], loss=86.1502
	step [53/332], loss=89.8638
	step [54/332], loss=82.8451
	step [55/332], loss=75.2325
	step [56/332], loss=73.1002
	step [57/332], loss=81.1071
	step [58/332], loss=69.8018
	step [59/332], loss=73.4336
	step [60/332], loss=70.3426
	step [61/332], loss=83.5722
	step [62/332], loss=67.0794
	step [63/332], loss=83.1233
	step [64/332], loss=79.5520
	step [65/332], loss=77.4550
	step [66/332], loss=74.0400
	step [67/332], loss=75.3732
	step [68/332], loss=83.0065
	step [69/332], loss=81.1292
	step [70/332], loss=78.6367
	step [71/332], loss=72.1399
	step [72/332], loss=76.3041
	step [73/332], loss=76.5445
	step [74/332], loss=96.9952
	step [75/332], loss=103.6585
	step [76/332], loss=93.3417
	step [77/332], loss=89.7371
	step [78/332], loss=68.6342
	step [79/332], loss=76.1269
	step [80/332], loss=75.7625
	step [81/332], loss=91.4088
	step [82/332], loss=64.1736
	step [83/332], loss=78.5306
	step [84/332], loss=76.6871
	step [85/332], loss=76.0731
	step [86/332], loss=86.2114
	step [87/332], loss=80.4893
	step [88/332], loss=74.8284
	step [89/332], loss=71.6521
	step [90/332], loss=77.5185
	step [91/332], loss=72.5362
	step [92/332], loss=79.8205
	step [93/332], loss=74.9565
	step [94/332], loss=74.6656
	step [95/332], loss=67.3526
	step [96/332], loss=62.0617
	step [97/332], loss=81.1153
	step [98/332], loss=82.6080
	step [99/332], loss=73.3958
	step [100/332], loss=77.4919
	step [101/332], loss=61.0220
	step [102/332], loss=74.8132
	step [103/332], loss=83.6062
	step [104/332], loss=82.0466
	step [105/332], loss=64.1009
	step [106/332], loss=73.2763
	step [107/332], loss=74.5538
	step [108/332], loss=68.7179
	step [109/332], loss=63.0380
	step [110/332], loss=72.9550
	step [111/332], loss=70.1876
	step [112/332], loss=62.6315
	step [113/332], loss=62.0910
	step [114/332], loss=70.9781
	step [115/332], loss=66.4425
	step [116/332], loss=71.9646
	step [117/332], loss=80.9708
	step [118/332], loss=78.1231
	step [119/332], loss=70.2886
	step [120/332], loss=74.2964
	step [121/332], loss=61.1163
	step [122/332], loss=65.0795
	step [123/332], loss=75.2362
	step [124/332], loss=82.9054
	step [125/332], loss=80.7658
	step [126/332], loss=75.3378
	step [127/332], loss=80.0132
	step [128/332], loss=58.0822
	step [129/332], loss=79.7028
	step [130/332], loss=83.9531
	step [131/332], loss=89.6076
	step [132/332], loss=75.8074
	step [133/332], loss=67.4685
	step [134/332], loss=75.5800
	step [135/332], loss=83.4809
	step [136/332], loss=77.1867
	step [137/332], loss=73.4469
	step [138/332], loss=75.6406
	step [139/332], loss=65.3062
	step [140/332], loss=82.7440
	step [141/332], loss=77.1154
	step [142/332], loss=64.0372
	step [143/332], loss=65.2101
	step [144/332], loss=63.9549
	step [145/332], loss=73.8968
	step [146/332], loss=71.2562
	step [147/332], loss=70.0756
	step [148/332], loss=70.4892
	step [149/332], loss=70.5766
	step [150/332], loss=55.3354
	step [151/332], loss=66.2386
	step [152/332], loss=72.5255
	step [153/332], loss=79.8381
	step [154/332], loss=57.6355
	step [155/332], loss=54.5387
	step [156/332], loss=66.8991
	step [157/332], loss=74.8910
	step [158/332], loss=70.0588
	step [159/332], loss=93.7632
	step [160/332], loss=81.7238
	step [161/332], loss=64.3794
	step [162/332], loss=66.7364
	step [163/332], loss=71.7720
	step [164/332], loss=77.3282
	step [165/332], loss=71.5687
	step [166/332], loss=102.2330
	step [167/332], loss=68.9637
	step [168/332], loss=66.7671
	step [169/332], loss=60.9310
	step [170/332], loss=74.9855
	step [171/332], loss=75.6400
	step [172/332], loss=77.9832
	step [173/332], loss=61.3339
	step [174/332], loss=79.4670
	step [175/332], loss=75.6053
	step [176/332], loss=84.4372
	step [177/332], loss=82.1842
	step [178/332], loss=85.4531
	step [179/332], loss=78.0635
	step [180/332], loss=77.6530
	step [181/332], loss=79.9131
	step [182/332], loss=84.6950
	step [183/332], loss=64.3992
	step [184/332], loss=65.1741
	step [185/332], loss=85.5602
	step [186/332], loss=60.8053
	step [187/332], loss=69.9319
	step [188/332], loss=92.6526
	step [189/332], loss=53.0073
	step [190/332], loss=55.8745
	step [191/332], loss=75.9410
	step [192/332], loss=96.7961
	step [193/332], loss=85.7901
	step [194/332], loss=67.8111
	step [195/332], loss=86.3579
	step [196/332], loss=82.3767
	step [197/332], loss=62.5298
	step [198/332], loss=70.4310
	step [199/332], loss=79.0464
	step [200/332], loss=87.4460
	step [201/332], loss=59.4060
	step [202/332], loss=62.5584
	step [203/332], loss=79.2239
	step [204/332], loss=73.8706
	step [205/332], loss=88.3365
	step [206/332], loss=68.9122
	step [207/332], loss=72.9997
	step [208/332], loss=80.0945
	step [209/332], loss=73.5178
	step [210/332], loss=77.6429
	step [211/332], loss=86.5133
	step [212/332], loss=76.0892
	step [213/332], loss=91.8295
	step [214/332], loss=69.9183
	step [215/332], loss=73.8390
	step [216/332], loss=82.4474
	step [217/332], loss=74.0838
	step [218/332], loss=89.1645
	step [219/332], loss=77.1248
	step [220/332], loss=78.3357
	step [221/332], loss=77.9307
	step [222/332], loss=66.2283
	step [223/332], loss=75.2618
	step [224/332], loss=78.2404
	step [225/332], loss=68.4458
	step [226/332], loss=61.9133
	step [227/332], loss=58.2733
	step [228/332], loss=75.9490
	step [229/332], loss=72.7295
	step [230/332], loss=69.2128
	step [231/332], loss=70.5125
	step [232/332], loss=86.6150
	step [233/332], loss=59.8661
	step [234/332], loss=78.1293
	step [235/332], loss=78.0945
	step [236/332], loss=73.3418
	step [237/332], loss=82.7347
	step [238/332], loss=90.1760
	step [239/332], loss=70.7855
	step [240/332], loss=69.7376
	step [241/332], loss=73.6358
	step [242/332], loss=97.4215
	step [243/332], loss=64.4494
	step [244/332], loss=63.4429
	step [245/332], loss=63.8656
	step [246/332], loss=67.7996
	step [247/332], loss=82.4644
	step [248/332], loss=94.7661
	step [249/332], loss=74.6971
	step [250/332], loss=58.9318
	step [251/332], loss=58.8164
	step [252/332], loss=72.9398
	step [253/332], loss=63.4000
	step [254/332], loss=77.2022
	step [255/332], loss=70.5756
	step [256/332], loss=78.0233
	step [257/332], loss=65.5521
	step [258/332], loss=80.3750
	step [259/332], loss=64.5495
	step [260/332], loss=70.9914
	step [261/332], loss=76.1540
	step [262/332], loss=63.0728
	step [263/332], loss=62.1273
	step [264/332], loss=84.8502
	step [265/332], loss=57.7268
	step [266/332], loss=86.5800
	step [267/332], loss=64.4790
	step [268/332], loss=76.8766
	step [269/332], loss=82.6024
	step [270/332], loss=80.0511
	step [271/332], loss=70.0879
	step [272/332], loss=52.0009
	step [273/332], loss=67.6609
	step [274/332], loss=74.4079
	step [275/332], loss=89.4108
	step [276/332], loss=78.1214
	step [277/332], loss=80.1580
	step [278/332], loss=60.5222
	step [279/332], loss=90.2272
	step [280/332], loss=70.7473
	step [281/332], loss=76.6393
	step [282/332], loss=81.7513
	step [283/332], loss=82.8642
	step [284/332], loss=66.2050
	step [285/332], loss=69.8592
	step [286/332], loss=66.4533
	step [287/332], loss=75.3311
	step [288/332], loss=80.3822
	step [289/332], loss=79.9240
	step [290/332], loss=71.3129
	step [291/332], loss=77.0764
	step [292/332], loss=78.0162
	step [293/332], loss=72.2373
	step [294/332], loss=87.7753
	step [295/332], loss=74.1303
	step [296/332], loss=83.9517
	step [297/332], loss=80.6271
	step [298/332], loss=74.5527
	step [299/332], loss=65.3293
	step [300/332], loss=88.3542
	step [301/332], loss=84.4751
	step [302/332], loss=70.2257
	step [303/332], loss=79.2112
	step [304/332], loss=92.5260
	step [305/332], loss=59.1180
	step [306/332], loss=70.3508
	step [307/332], loss=67.1062
	step [308/332], loss=61.6356
	step [309/332], loss=80.1514
	step [310/332], loss=79.8811
	step [311/332], loss=75.2985
	step [312/332], loss=86.8808
	step [313/332], loss=68.5172
	step [314/332], loss=70.7431
	step [315/332], loss=59.1983
	step [316/332], loss=76.3266
	step [317/332], loss=72.3430
	step [318/332], loss=81.1283
	step [319/332], loss=60.3349
	step [320/332], loss=81.3287
	step [321/332], loss=51.7889
	step [322/332], loss=74.5324
	step [323/332], loss=59.8250
	step [324/332], loss=72.8950
	step [325/332], loss=79.6622
	step [326/332], loss=97.9111
	step [327/332], loss=75.2487
	step [328/332], loss=75.8584
	step [329/332], loss=79.0873
	step [330/332], loss=70.7022
	step [331/332], loss=73.4186
	step [332/332], loss=49.0883
	Evaluating
	loss=0.0132, precision=0.3290, recall=0.9108, f1=0.4834
Training epoch 14
	step [1/332], loss=81.1708
	step [2/332], loss=70.6320
	step [3/332], loss=85.5788
	step [4/332], loss=73.7719
	step [5/332], loss=76.9304
	step [6/332], loss=65.6882
	step [7/332], loss=73.4706
	step [8/332], loss=66.3443
	step [9/332], loss=60.3102
	step [10/332], loss=93.8339
	step [11/332], loss=64.4888
	step [12/332], loss=87.3108
	step [13/332], loss=74.3648
	step [14/332], loss=78.8029
	step [15/332], loss=57.7742
	step [16/332], loss=89.5919
	step [17/332], loss=79.4828
	step [18/332], loss=64.8519
	step [19/332], loss=74.3905
	step [20/332], loss=83.5092
	step [21/332], loss=65.6199
	step [22/332], loss=63.3946
	step [23/332], loss=78.1294
	step [24/332], loss=64.4998
	step [25/332], loss=65.6320
	step [26/332], loss=71.7420
	step [27/332], loss=92.9147
	step [28/332], loss=75.8956
	step [29/332], loss=72.0872
	step [30/332], loss=83.1359
	step [31/332], loss=71.9225
	step [32/332], loss=83.8223
	step [33/332], loss=71.9792
	step [34/332], loss=70.9003
	step [35/332], loss=91.7139
	step [36/332], loss=70.4902
	step [37/332], loss=72.2883
	step [38/332], loss=54.0491
	step [39/332], loss=89.3083
	step [40/332], loss=92.7397
	step [41/332], loss=67.2930
	step [42/332], loss=70.8412
	step [43/332], loss=61.4833
	step [44/332], loss=73.9191
	step [45/332], loss=76.6530
	step [46/332], loss=84.1497
	step [47/332], loss=74.5469
	step [48/332], loss=66.2385
	step [49/332], loss=76.1005
	step [50/332], loss=59.3117
	step [51/332], loss=72.0470
	step [52/332], loss=87.4589
	step [53/332], loss=62.5528
	step [54/332], loss=88.2639
	step [55/332], loss=64.3916
	step [56/332], loss=85.3400
	step [57/332], loss=74.1559
	step [58/332], loss=70.6467
	step [59/332], loss=63.5995
	step [60/332], loss=68.7587
	step [61/332], loss=85.8435
	step [62/332], loss=78.7561
	step [63/332], loss=67.2360
	step [64/332], loss=85.3581
	step [65/332], loss=73.6986
	step [66/332], loss=78.7060
	step [67/332], loss=73.4183
	step [68/332], loss=74.8689
	step [69/332], loss=88.4709
	step [70/332], loss=71.8124
	step [71/332], loss=78.4478
	step [72/332], loss=71.7885
	step [73/332], loss=84.5534
	step [74/332], loss=78.5436
	step [75/332], loss=66.2651
	step [76/332], loss=85.0143
	step [77/332], loss=59.5821
	step [78/332], loss=82.8260
	step [79/332], loss=69.0927
	step [80/332], loss=81.1654
	step [81/332], loss=89.1780
	step [82/332], loss=70.5385
	step [83/332], loss=63.3942
	step [84/332], loss=82.0210
	step [85/332], loss=74.9283
	step [86/332], loss=72.0466
	step [87/332], loss=62.8274
	step [88/332], loss=79.0908
	step [89/332], loss=73.7801
	step [90/332], loss=75.3356
	step [91/332], loss=76.9538
	step [92/332], loss=69.4662
	step [93/332], loss=71.9797
	step [94/332], loss=70.4419
	step [95/332], loss=61.2612
	step [96/332], loss=65.9498
	step [97/332], loss=75.0518
	step [98/332], loss=83.1659
	step [99/332], loss=70.1762
	step [100/332], loss=78.2516
	step [101/332], loss=80.0619
	step [102/332], loss=57.7551
	step [103/332], loss=99.0608
	step [104/332], loss=84.4640
	step [105/332], loss=77.9734
	step [106/332], loss=75.1202
	step [107/332], loss=75.2690
	step [108/332], loss=89.8045
	step [109/332], loss=54.4520
	step [110/332], loss=77.2978
	step [111/332], loss=69.2625
	step [112/332], loss=92.0187
	step [113/332], loss=68.6976
	step [114/332], loss=63.6775
	step [115/332], loss=75.3342
	step [116/332], loss=70.1398
	step [117/332], loss=74.4174
	step [118/332], loss=72.5656
	step [119/332], loss=83.1322
	step [120/332], loss=76.8309
	step [121/332], loss=59.3088
	step [122/332], loss=62.9075
	step [123/332], loss=69.4226
	step [124/332], loss=82.2564
	step [125/332], loss=73.6782
	step [126/332], loss=87.2288
	step [127/332], loss=67.6278
	step [128/332], loss=77.1057
	step [129/332], loss=63.8296
	step [130/332], loss=63.9396
	step [131/332], loss=51.1618
	step [132/332], loss=69.3747
	step [133/332], loss=101.8340
	step [134/332], loss=84.4817
	step [135/332], loss=88.8447
	step [136/332], loss=73.9652
	step [137/332], loss=70.5000
	step [138/332], loss=65.6687
	step [139/332], loss=69.6434
	step [140/332], loss=83.3081
	step [141/332], loss=56.6354
	step [142/332], loss=68.0000
	step [143/332], loss=63.1329
	step [144/332], loss=69.8012
	step [145/332], loss=69.8582
	step [146/332], loss=72.6250
	step [147/332], loss=78.9136
	step [148/332], loss=55.6562
	step [149/332], loss=69.8939
	step [150/332], loss=80.9825
	step [151/332], loss=75.6735
	step [152/332], loss=76.8075
	step [153/332], loss=68.0267
	step [154/332], loss=82.1521
	step [155/332], loss=83.2642
	step [156/332], loss=77.0407
	step [157/332], loss=87.2930
	step [158/332], loss=67.6146
	step [159/332], loss=76.9339
	step [160/332], loss=61.3997
	step [161/332], loss=93.1272
	step [162/332], loss=64.9377
	step [163/332], loss=74.9410
	step [164/332], loss=87.0115
	step [165/332], loss=58.8624
	step [166/332], loss=66.2498
	step [167/332], loss=74.5890
	step [168/332], loss=79.4155
	step [169/332], loss=72.4231
	step [170/332], loss=65.0993
	step [171/332], loss=77.6104
	step [172/332], loss=82.3903
	step [173/332], loss=79.7026
	step [174/332], loss=67.2807
	step [175/332], loss=60.6724
	step [176/332], loss=65.9241
	step [177/332], loss=86.3933
	step [178/332], loss=67.5938
	step [179/332], loss=91.9062
	step [180/332], loss=61.1632
	step [181/332], loss=73.6304
	step [182/332], loss=65.0317
	step [183/332], loss=74.5435
	step [184/332], loss=67.8089
	step [185/332], loss=62.1524
	step [186/332], loss=94.8493
	step [187/332], loss=68.8012
	step [188/332], loss=60.5252
	step [189/332], loss=74.7005
	step [190/332], loss=71.6360
	step [191/332], loss=82.5564
	step [192/332], loss=87.4369
	step [193/332], loss=85.4673
	step [194/332], loss=69.2766
	step [195/332], loss=88.3290
	step [196/332], loss=72.6388
	step [197/332], loss=74.7509
	step [198/332], loss=64.4079
	step [199/332], loss=89.6441
	step [200/332], loss=62.3939
	step [201/332], loss=84.9038
	step [202/332], loss=73.2315
	step [203/332], loss=64.6186
	step [204/332], loss=64.6346
	step [205/332], loss=61.4816
	step [206/332], loss=58.4304
	step [207/332], loss=61.3992
	step [208/332], loss=80.4193
	step [209/332], loss=74.6945
	step [210/332], loss=77.2966
	step [211/332], loss=92.6243
	step [212/332], loss=63.6659
	step [213/332], loss=57.2074
	step [214/332], loss=95.4548
	step [215/332], loss=56.3548
	step [216/332], loss=67.5476
	step [217/332], loss=69.7268
	step [218/332], loss=59.7864
	step [219/332], loss=67.3763
	step [220/332], loss=74.6507
	step [221/332], loss=82.8484
	step [222/332], loss=70.8114
	step [223/332], loss=77.2950
	step [224/332], loss=71.7154
	step [225/332], loss=84.8378
	step [226/332], loss=79.1785
	step [227/332], loss=73.6788
	step [228/332], loss=62.7918
	step [229/332], loss=93.1884
	step [230/332], loss=73.7944
	step [231/332], loss=77.0271
	step [232/332], loss=72.1873
	step [233/332], loss=58.7173
	step [234/332], loss=69.5164
	step [235/332], loss=72.9704
	step [236/332], loss=99.4715
	step [237/332], loss=74.5355
	step [238/332], loss=69.7049
	step [239/332], loss=70.2526
	step [240/332], loss=84.5813
	step [241/332], loss=75.2447
	step [242/332], loss=77.4695
	step [243/332], loss=61.8759
	step [244/332], loss=72.4907
	step [245/332], loss=90.4472
	step [246/332], loss=63.7718
	step [247/332], loss=75.1333
	step [248/332], loss=80.9560
	step [249/332], loss=73.9260
	step [250/332], loss=62.8843
	step [251/332], loss=90.5917
	step [252/332], loss=81.3008
	step [253/332], loss=68.2826
	step [254/332], loss=74.3434
	step [255/332], loss=68.5311
	step [256/332], loss=85.8158
	step [257/332], loss=72.8477
	step [258/332], loss=67.7352
	step [259/332], loss=78.7768
	step [260/332], loss=49.1771
	step [261/332], loss=73.5188
	step [262/332], loss=68.0305
	step [263/332], loss=67.2453
	step [264/332], loss=77.3854
	step [265/332], loss=71.1172
	step [266/332], loss=62.7757
	step [267/332], loss=74.7902
	step [268/332], loss=101.7339
	step [269/332], loss=80.9988
	step [270/332], loss=76.0560
	step [271/332], loss=84.2680
	step [272/332], loss=73.4803
	step [273/332], loss=82.3992
	step [274/332], loss=87.6953
	step [275/332], loss=75.0187
	step [276/332], loss=58.1097
	step [277/332], loss=77.3651
	step [278/332], loss=69.2494
	step [279/332], loss=73.9257
	step [280/332], loss=71.1456
	step [281/332], loss=79.5130
	step [282/332], loss=85.4323
	step [283/332], loss=56.1052
	step [284/332], loss=62.4817
	step [285/332], loss=76.5721
	step [286/332], loss=77.6696
	step [287/332], loss=70.6465
	step [288/332], loss=69.0839
	step [289/332], loss=70.6243
	step [290/332], loss=66.7099
	step [291/332], loss=67.5849
	step [292/332], loss=74.9487
	step [293/332], loss=75.4355
	step [294/332], loss=88.0073
	step [295/332], loss=81.4987
	step [296/332], loss=79.4583
	step [297/332], loss=60.3205
	step [298/332], loss=70.9452
	step [299/332], loss=65.9323
	step [300/332], loss=85.3158
	step [301/332], loss=85.6323
	step [302/332], loss=77.0510
	step [303/332], loss=70.5366
	step [304/332], loss=76.9563
	step [305/332], loss=67.3814
	step [306/332], loss=73.3831
	step [307/332], loss=93.4725
	step [308/332], loss=72.7736
	step [309/332], loss=70.0261
	step [310/332], loss=82.6251
	step [311/332], loss=70.7985
	step [312/332], loss=70.6892
	step [313/332], loss=63.2548
	step [314/332], loss=70.6231
	step [315/332], loss=86.4099
	step [316/332], loss=74.5227
	step [317/332], loss=85.1018
	step [318/332], loss=96.3163
	step [319/332], loss=85.4733
	step [320/332], loss=76.7730
	step [321/332], loss=73.3323
	step [322/332], loss=70.2757
	step [323/332], loss=68.6142
	step [324/332], loss=66.4766
	step [325/332], loss=69.9523
	step [326/332], loss=73.7002
	step [327/332], loss=83.9905
	step [328/332], loss=78.1770
	step [329/332], loss=65.4664
	step [330/332], loss=68.1735
	step [331/332], loss=68.6193
	step [332/332], loss=35.5638
	Evaluating
	loss=0.0119, precision=0.3562, recall=0.9155, f1=0.5128
Training epoch 15
	step [1/332], loss=73.4652
	step [2/332], loss=79.6171
	step [3/332], loss=57.9043
	step [4/332], loss=78.4780
	step [5/332], loss=60.3010
	step [6/332], loss=65.7283
	step [7/332], loss=65.6665
	step [8/332], loss=78.1467
	step [9/332], loss=78.9727
	step [10/332], loss=75.6059
	step [11/332], loss=78.8183
	step [12/332], loss=58.9272
	step [13/332], loss=55.2177
	step [14/332], loss=72.9941
	step [15/332], loss=70.0984
	step [16/332], loss=79.3117
	step [17/332], loss=77.0176
	step [18/332], loss=64.4781
	step [19/332], loss=101.4139
	step [20/332], loss=63.1899
	step [21/332], loss=73.2533
	step [22/332], loss=52.2309
	step [23/332], loss=72.3532
	step [24/332], loss=63.6568
	step [25/332], loss=51.1158
	step [26/332], loss=73.7718
	step [27/332], loss=66.0338
	step [28/332], loss=60.7191
	step [29/332], loss=64.0249
	step [30/332], loss=72.7327
	step [31/332], loss=64.4898
	step [32/332], loss=80.9167
	step [33/332], loss=75.3406
	step [34/332], loss=80.6361
	step [35/332], loss=64.7742
	step [36/332], loss=61.4373
	step [37/332], loss=75.8819
	step [38/332], loss=60.9519
	step [39/332], loss=67.6251
	step [40/332], loss=77.0421
	step [41/332], loss=72.4122
	step [42/332], loss=79.9929
	step [43/332], loss=80.3772
	step [44/332], loss=81.6337
	step [45/332], loss=71.9925
	step [46/332], loss=78.4987
	step [47/332], loss=95.5981
	step [48/332], loss=75.1384
	step [49/332], loss=66.9252
	step [50/332], loss=73.0098
	step [51/332], loss=83.1346
	step [52/332], loss=60.7730
	step [53/332], loss=64.3873
	step [54/332], loss=77.1284
	step [55/332], loss=55.6304
	step [56/332], loss=66.0079
	step [57/332], loss=70.3717
	step [58/332], loss=59.6668
	step [59/332], loss=78.3683
	step [60/332], loss=78.6344
	step [61/332], loss=81.6312
	step [62/332], loss=68.3012
	step [63/332], loss=99.7496
	step [64/332], loss=66.3319
	step [65/332], loss=80.0375
	step [66/332], loss=70.8955
	step [67/332], loss=68.3683
	step [68/332], loss=85.7763
	step [69/332], loss=79.1982
	step [70/332], loss=70.5385
	step [71/332], loss=62.7519
	step [72/332], loss=84.2403
	step [73/332], loss=74.5382
	step [74/332], loss=66.0211
	step [75/332], loss=105.9124
	step [76/332], loss=67.5500
	step [77/332], loss=75.7538
	step [78/332], loss=66.3990
	step [79/332], loss=76.2270
	step [80/332], loss=106.9280
	step [81/332], loss=77.6691
	step [82/332], loss=75.8440
	step [83/332], loss=71.3440
	step [84/332], loss=62.6176
	step [85/332], loss=83.8804
	step [86/332], loss=71.6112
	step [87/332], loss=69.1326
	step [88/332], loss=92.4559
	step [89/332], loss=84.4349
	step [90/332], loss=62.8551
	step [91/332], loss=73.1236
	step [92/332], loss=63.9030
	step [93/332], loss=55.2444
	step [94/332], loss=69.5428
	step [95/332], loss=86.2875
	step [96/332], loss=82.9052
	step [97/332], loss=78.2969
	step [98/332], loss=83.6097
	step [99/332], loss=68.8030
	step [100/332], loss=68.5117
	step [101/332], loss=62.8655
	step [102/332], loss=83.4429
	step [103/332], loss=78.9415
	step [104/332], loss=84.5144
	step [105/332], loss=70.9217
	step [106/332], loss=74.6150
	step [107/332], loss=69.0200
	step [108/332], loss=88.4568
	step [109/332], loss=66.1426
	step [110/332], loss=69.3393
	step [111/332], loss=88.1071
	step [112/332], loss=71.8422
	step [113/332], loss=84.6362
	step [114/332], loss=69.2747
	step [115/332], loss=87.8472
	step [116/332], loss=77.5392
	step [117/332], loss=71.9921
	step [118/332], loss=66.6796
	step [119/332], loss=60.8824
	step [120/332], loss=75.6774
	step [121/332], loss=74.8629
	step [122/332], loss=54.3593
	step [123/332], loss=51.8601
	step [124/332], loss=66.5639
	step [125/332], loss=64.2574
	step [126/332], loss=70.2621
	step [127/332], loss=77.0778
	step [128/332], loss=84.4145
	step [129/332], loss=72.9798
	step [130/332], loss=84.2719
	step [131/332], loss=74.3917
	step [132/332], loss=61.5717
	step [133/332], loss=73.5602
	step [134/332], loss=71.5316
	step [135/332], loss=86.8506
	step [136/332], loss=59.3414
	step [137/332], loss=76.3521
	step [138/332], loss=68.1962
	step [139/332], loss=67.2930
	step [140/332], loss=68.0674
	step [141/332], loss=91.6467
	step [142/332], loss=83.9431
	step [143/332], loss=91.9176
	step [144/332], loss=71.2829
	step [145/332], loss=75.6404
	step [146/332], loss=66.5816
	step [147/332], loss=78.2759
	step [148/332], loss=84.6746
	step [149/332], loss=81.1605
	step [150/332], loss=77.9173
	step [151/332], loss=72.6376
	step [152/332], loss=73.0439
	step [153/332], loss=61.0273
	step [154/332], loss=74.6335
	step [155/332], loss=74.9597
	step [156/332], loss=70.5545
	step [157/332], loss=68.6601
	step [158/332], loss=72.2718
	step [159/332], loss=74.2574
	step [160/332], loss=71.8068
	step [161/332], loss=72.7356
	step [162/332], loss=67.8737
	step [163/332], loss=76.4479
	step [164/332], loss=68.4843
	step [165/332], loss=71.2087
	step [166/332], loss=92.2434
	step [167/332], loss=56.4721
	step [168/332], loss=77.4899
	step [169/332], loss=58.2964
	step [170/332], loss=59.1470
	step [171/332], loss=74.1125
	step [172/332], loss=61.4216
	step [173/332], loss=72.4772
	step [174/332], loss=81.9740
	step [175/332], loss=68.6189
	step [176/332], loss=70.2581
	step [177/332], loss=64.7230
	step [178/332], loss=61.4468
	step [179/332], loss=97.4756
	step [180/332], loss=63.5441
	step [181/332], loss=83.2517
	step [182/332], loss=73.5872
	step [183/332], loss=77.3225
	step [184/332], loss=61.6758
	step [185/332], loss=77.4797
	step [186/332], loss=83.8920
	step [187/332], loss=69.6564
	step [188/332], loss=64.2728
	step [189/332], loss=74.6297
	step [190/332], loss=75.7105
	step [191/332], loss=75.5668
	step [192/332], loss=80.5213
	step [193/332], loss=82.9336
	step [194/332], loss=80.7103
	step [195/332], loss=82.7977
	step [196/332], loss=71.5311
	step [197/332], loss=60.0980
	step [198/332], loss=51.6656
	step [199/332], loss=77.5376
	step [200/332], loss=91.8324
	step [201/332], loss=75.0659
	step [202/332], loss=51.6094
	step [203/332], loss=64.6845
	step [204/332], loss=75.8094
	step [205/332], loss=69.4764
	step [206/332], loss=82.7092
	step [207/332], loss=79.5128
	step [208/332], loss=62.7181
	step [209/332], loss=63.1188
	step [210/332], loss=74.5888
	step [211/332], loss=73.1881
	step [212/332], loss=78.8616
	step [213/332], loss=80.7314
	step [214/332], loss=62.7526
	step [215/332], loss=82.6839
	step [216/332], loss=66.1901
	step [217/332], loss=65.7417
	step [218/332], loss=83.3768
	step [219/332], loss=86.9481
	step [220/332], loss=83.8363
	step [221/332], loss=75.7879
	step [222/332], loss=75.9523
	step [223/332], loss=74.1651
	step [224/332], loss=77.9039
	step [225/332], loss=69.0188
	step [226/332], loss=63.6630
	step [227/332], loss=75.0606
	step [228/332], loss=82.5174
	step [229/332], loss=70.7316
	step [230/332], loss=67.2896
	step [231/332], loss=84.5030
	step [232/332], loss=60.0852
	step [233/332], loss=72.0820
	step [234/332], loss=76.5574
	step [235/332], loss=69.6434
	step [236/332], loss=82.0706
	step [237/332], loss=67.3203
	step [238/332], loss=54.1678
	step [239/332], loss=80.6341
	step [240/332], loss=79.8876
	step [241/332], loss=76.4095
	step [242/332], loss=76.1329
	step [243/332], loss=61.9656
	step [244/332], loss=73.0809
	step [245/332], loss=73.3374
	step [246/332], loss=70.6618
	step [247/332], loss=59.1434
	step [248/332], loss=96.0932
	step [249/332], loss=66.1388
	step [250/332], loss=79.0378
	step [251/332], loss=86.5921
	step [252/332], loss=71.6473
	step [253/332], loss=67.7547
	step [254/332], loss=78.7645
	step [255/332], loss=62.8391
	step [256/332], loss=77.5319
	step [257/332], loss=79.4948
	step [258/332], loss=78.8439
	step [259/332], loss=52.2062
	step [260/332], loss=76.0341
	step [261/332], loss=74.6594
	step [262/332], loss=78.7606
	step [263/332], loss=69.2012
	step [264/332], loss=66.7013
	step [265/332], loss=67.9635
	step [266/332], loss=47.0337
	step [267/332], loss=89.4555
	step [268/332], loss=73.8262
	step [269/332], loss=75.7020
	step [270/332], loss=86.5537
	step [271/332], loss=71.2193
	step [272/332], loss=77.5633
	step [273/332], loss=71.1678
	step [274/332], loss=64.8743
	step [275/332], loss=79.4932
	step [276/332], loss=82.4212
	step [277/332], loss=76.2786
	step [278/332], loss=68.6539
	step [279/332], loss=68.3251
	step [280/332], loss=76.6656
	step [281/332], loss=73.2180
	step [282/332], loss=81.5355
	step [283/332], loss=70.3244
	step [284/332], loss=75.8339
	step [285/332], loss=70.9789
	step [286/332], loss=70.6162
	step [287/332], loss=59.8151
	step [288/332], loss=68.2666
	step [289/332], loss=74.6483
	step [290/332], loss=71.3517
	step [291/332], loss=87.0705
	step [292/332], loss=73.1165
	step [293/332], loss=69.5368
	step [294/332], loss=85.5429
	step [295/332], loss=76.4292
	step [296/332], loss=67.6710
	step [297/332], loss=63.2646
	step [298/332], loss=60.8165
	step [299/332], loss=78.1443
	step [300/332], loss=75.7026
	step [301/332], loss=65.6692
	step [302/332], loss=83.1188
	step [303/332], loss=70.0075
	step [304/332], loss=85.5680
	step [305/332], loss=69.0435
	step [306/332], loss=79.5642
	step [307/332], loss=81.9854
	step [308/332], loss=67.2749
	step [309/332], loss=70.8144
	step [310/332], loss=65.3937
	step [311/332], loss=79.1260
	step [312/332], loss=71.7670
	step [313/332], loss=77.5660
	step [314/332], loss=65.5703
	step [315/332], loss=81.3404
	step [316/332], loss=65.1039
	step [317/332], loss=72.8335
	step [318/332], loss=69.4181
	step [319/332], loss=74.0644
	step [320/332], loss=66.2843
	step [321/332], loss=72.3229
	step [322/332], loss=52.5795
	step [323/332], loss=82.0476
	step [324/332], loss=94.9672
	step [325/332], loss=66.6181
	step [326/332], loss=81.5187
	step [327/332], loss=69.9309
	step [328/332], loss=73.4587
	step [329/332], loss=67.1758
	step [330/332], loss=71.2109
	step [331/332], loss=73.3021
	step [332/332], loss=43.0103
	Evaluating
	loss=0.0102, precision=0.3643, recall=0.9270, f1=0.5231
Training epoch 16
	step [1/332], loss=77.0533
	step [2/332], loss=57.0857
	step [3/332], loss=72.7386
	step [4/332], loss=67.8204
	step [5/332], loss=58.3713
	step [6/332], loss=80.6085
	step [7/332], loss=85.5624
	step [8/332], loss=64.5980
	step [9/332], loss=70.1522
	step [10/332], loss=77.7286
	step [11/332], loss=68.0588
	step [12/332], loss=84.1017
	step [13/332], loss=77.8791
	step [14/332], loss=75.9948
	step [15/332], loss=65.4926
	step [16/332], loss=68.8117
	step [17/332], loss=76.9191
	step [18/332], loss=66.7309
	step [19/332], loss=73.2770
	step [20/332], loss=85.5211
	step [21/332], loss=60.4711
	step [22/332], loss=72.0016
	step [23/332], loss=75.6661
	step [24/332], loss=76.0193
	step [25/332], loss=70.5685
	step [26/332], loss=68.7653
	step [27/332], loss=71.0230
	step [28/332], loss=59.4459
	step [29/332], loss=61.7806
	step [30/332], loss=69.4466
	step [31/332], loss=71.8356
	step [32/332], loss=78.4927
	step [33/332], loss=66.7233
	step [34/332], loss=78.0895
	step [35/332], loss=79.3110
	step [36/332], loss=64.9594
	step [37/332], loss=70.3969
	step [38/332], loss=75.2732
	step [39/332], loss=70.7817
	step [40/332], loss=64.4393
	step [41/332], loss=63.9467
	step [42/332], loss=64.4462
	step [43/332], loss=74.0465
	step [44/332], loss=87.6321
	step [45/332], loss=66.2386
	step [46/332], loss=62.7919
	step [47/332], loss=70.5536
	step [48/332], loss=70.2361
	step [49/332], loss=78.2984
	step [50/332], loss=76.8249
	step [51/332], loss=69.4622
	step [52/332], loss=68.1606
	step [53/332], loss=57.4919
	step [54/332], loss=73.6693
	step [55/332], loss=68.8264
	step [56/332], loss=71.7987
	step [57/332], loss=78.5679
	step [58/332], loss=70.0602
	step [59/332], loss=75.0864
	step [60/332], loss=81.6468
	step [61/332], loss=66.5193
	step [62/332], loss=69.1195
	step [63/332], loss=74.2617
	step [64/332], loss=74.9134
	step [65/332], loss=64.3955
	step [66/332], loss=61.1788
	step [67/332], loss=76.8704
	step [68/332], loss=63.6618
	step [69/332], loss=65.9433
	step [70/332], loss=69.8680
	step [71/332], loss=74.1684
	step [72/332], loss=66.8361
	step [73/332], loss=86.0843
	step [74/332], loss=82.1067
	step [75/332], loss=72.3621
	step [76/332], loss=77.3377
	step [77/332], loss=66.9303
	step [78/332], loss=74.0683
	step [79/332], loss=74.4577
	step [80/332], loss=59.8811
	step [81/332], loss=58.9035
	step [82/332], loss=71.3388
	step [83/332], loss=60.0562
	step [84/332], loss=84.4275
	step [85/332], loss=72.6347
	step [86/332], loss=66.8127
	step [87/332], loss=60.5418
	step [88/332], loss=86.6427
	step [89/332], loss=67.8725
	step [90/332], loss=89.6621
	step [91/332], loss=82.9483
	step [92/332], loss=48.2991
	step [93/332], loss=75.2820
	step [94/332], loss=80.9401
	step [95/332], loss=92.6762
	step [96/332], loss=85.2332
	step [97/332], loss=65.9729
	step [98/332], loss=64.6029
	step [99/332], loss=66.5045
	step [100/332], loss=83.1136
	step [101/332], loss=76.2504
	step [102/332], loss=64.6512
	step [103/332], loss=70.3217
	step [104/332], loss=94.5817
	step [105/332], loss=62.6514
	step [106/332], loss=84.4323
	step [107/332], loss=69.2276
	step [108/332], loss=75.6373
	step [109/332], loss=76.1541
	step [110/332], loss=62.2036
	step [111/332], loss=65.0855
	step [112/332], loss=66.9296
	step [113/332], loss=61.3446
	step [114/332], loss=61.2386
	step [115/332], loss=63.9331
	step [116/332], loss=78.9803
	step [117/332], loss=88.0876
	step [118/332], loss=87.4631
	step [119/332], loss=65.4461
	step [120/332], loss=70.9845
	step [121/332], loss=75.1135
	step [122/332], loss=79.4916
	step [123/332], loss=54.1617
	step [124/332], loss=68.2656
	step [125/332], loss=76.5989
	step [126/332], loss=58.3490
	step [127/332], loss=68.4925
	step [128/332], loss=69.4137
	step [129/332], loss=81.4173
	step [130/332], loss=76.0314
	step [131/332], loss=73.4287
	step [132/332], loss=74.7458
	step [133/332], loss=69.4598
	step [134/332], loss=80.0286
	step [135/332], loss=83.4149
	step [136/332], loss=69.9595
	step [137/332], loss=90.3238
	step [138/332], loss=69.0806
	step [139/332], loss=72.5936
	step [140/332], loss=66.2554
	step [141/332], loss=71.8870
	step [142/332], loss=61.4462
	step [143/332], loss=65.6427
	step [144/332], loss=59.9332
	step [145/332], loss=66.6476
	step [146/332], loss=75.7996
	step [147/332], loss=85.0233
	step [148/332], loss=83.9868
	step [149/332], loss=60.9649
	step [150/332], loss=65.6831
	step [151/332], loss=67.5515
	step [152/332], loss=71.0829
	step [153/332], loss=62.2368
	step [154/332], loss=63.2262
	step [155/332], loss=66.9015
	step [156/332], loss=70.1677
	step [157/332], loss=80.5931
	step [158/332], loss=73.5030
	step [159/332], loss=68.6119
	step [160/332], loss=76.2447
	step [161/332], loss=72.1448
	step [162/332], loss=84.0133
	step [163/332], loss=86.7097
	step [164/332], loss=56.3826
	step [165/332], loss=66.2982
	step [166/332], loss=79.0414
	step [167/332], loss=95.3421
	step [168/332], loss=82.0725
	step [169/332], loss=67.9448
	step [170/332], loss=83.6953
	step [171/332], loss=71.6738
	step [172/332], loss=76.7607
	step [173/332], loss=63.4811
	step [174/332], loss=82.1059
	step [175/332], loss=67.9445
	step [176/332], loss=80.1182
	step [177/332], loss=69.4245
	step [178/332], loss=61.1749
	step [179/332], loss=68.2090
	step [180/332], loss=69.1090
	step [181/332], loss=102.6031
	step [182/332], loss=64.5310
	step [183/332], loss=76.1083
	step [184/332], loss=88.5688
	step [185/332], loss=69.9597
	step [186/332], loss=80.4304
	step [187/332], loss=59.2808
	step [188/332], loss=61.6504
	step [189/332], loss=67.3734
	step [190/332], loss=77.9788
	step [191/332], loss=81.0013
	step [192/332], loss=80.1747
	step [193/332], loss=74.6678
	step [194/332], loss=79.1892
	step [195/332], loss=78.5022
	step [196/332], loss=63.7067
	step [197/332], loss=88.5748
	step [198/332], loss=73.0394
	step [199/332], loss=70.6154
	step [200/332], loss=84.2741
	step [201/332], loss=65.9327
	step [202/332], loss=73.2978
	step [203/332], loss=74.5849
	step [204/332], loss=74.1630
	step [205/332], loss=62.7462
	step [206/332], loss=67.0203
	step [207/332], loss=69.4783
	step [208/332], loss=86.4846
	step [209/332], loss=67.5793
	step [210/332], loss=75.1288
	step [211/332], loss=56.3482
	step [212/332], loss=92.6096
	step [213/332], loss=56.3170
	step [214/332], loss=56.4225
	step [215/332], loss=65.2340
	step [216/332], loss=67.8705
	step [217/332], loss=76.1623
	step [218/332], loss=76.4033
	step [219/332], loss=79.6085
	step [220/332], loss=70.9321
	step [221/332], loss=86.2029
	step [222/332], loss=62.8062
	step [223/332], loss=80.4057
	step [224/332], loss=63.9071
	step [225/332], loss=59.6497
	step [226/332], loss=89.8163
	step [227/332], loss=77.9373
	step [228/332], loss=71.7737
	step [229/332], loss=70.7743
	step [230/332], loss=76.1896
	step [231/332], loss=78.3880
	step [232/332], loss=59.4475
	step [233/332], loss=70.2231
	step [234/332], loss=66.1209
	step [235/332], loss=60.4291
	step [236/332], loss=66.1399
	step [237/332], loss=80.6888
	step [238/332], loss=64.3330
	step [239/332], loss=74.0638
	step [240/332], loss=70.4266
	step [241/332], loss=67.7342
	step [242/332], loss=78.1893
	step [243/332], loss=70.2153
	step [244/332], loss=76.0072
	step [245/332], loss=60.0344
	step [246/332], loss=62.8020
	step [247/332], loss=80.3389
	step [248/332], loss=81.1817
	step [249/332], loss=74.9691
	step [250/332], loss=92.4815
	step [251/332], loss=72.6772
	step [252/332], loss=69.8939
	step [253/332], loss=72.9534
	step [254/332], loss=76.0130
	step [255/332], loss=75.0336
	step [256/332], loss=63.4016
	step [257/332], loss=64.8713
	step [258/332], loss=75.2444
	step [259/332], loss=72.7862
	step [260/332], loss=63.4100
	step [261/332], loss=85.6463
	step [262/332], loss=77.0758
	step [263/332], loss=72.5176
	step [264/332], loss=71.1063
	step [265/332], loss=66.4339
	step [266/332], loss=68.8039
	step [267/332], loss=61.1723
	step [268/332], loss=80.0130
	step [269/332], loss=73.1760
	step [270/332], loss=72.1486
	step [271/332], loss=67.1182
	step [272/332], loss=71.3113
	step [273/332], loss=79.0285
	step [274/332], loss=69.6558
	step [275/332], loss=68.5410
	step [276/332], loss=61.6028
	step [277/332], loss=63.7499
	step [278/332], loss=74.2074
	step [279/332], loss=81.1527
	step [280/332], loss=58.0693
	step [281/332], loss=79.6656
	step [282/332], loss=74.3105
	step [283/332], loss=71.3032
	step [284/332], loss=66.8186
	step [285/332], loss=64.2651
	step [286/332], loss=77.9950
	step [287/332], loss=81.0890
	step [288/332], loss=68.1152
	step [289/332], loss=66.8645
	step [290/332], loss=89.6580
	step [291/332], loss=100.6962
	step [292/332], loss=81.3598
	step [293/332], loss=66.5169
	step [294/332], loss=66.3735
	step [295/332], loss=57.9897
	step [296/332], loss=80.6959
	step [297/332], loss=80.6597
	step [298/332], loss=80.6629
	step [299/332], loss=76.5884
	step [300/332], loss=75.1545
	step [301/332], loss=73.7717
	step [302/332], loss=74.6465
	step [303/332], loss=80.2725
	step [304/332], loss=78.6958
	step [305/332], loss=71.8050
	step [306/332], loss=74.0643
	step [307/332], loss=77.4695
	step [308/332], loss=92.4544
	step [309/332], loss=61.7023
	step [310/332], loss=69.6385
	step [311/332], loss=59.7876
	step [312/332], loss=76.4125
	step [313/332], loss=71.3677
	step [314/332], loss=82.1352
	step [315/332], loss=78.8419
	step [316/332], loss=73.3201
	step [317/332], loss=62.0429
	step [318/332], loss=82.4190
	step [319/332], loss=51.3639
	step [320/332], loss=80.6554
	step [321/332], loss=79.7713
	step [322/332], loss=70.7856
	step [323/332], loss=73.9876
	step [324/332], loss=61.8447
	step [325/332], loss=73.9222
	step [326/332], loss=80.9890
	step [327/332], loss=58.4483
	step [328/332], loss=76.8942
	step [329/332], loss=73.1569
	step [330/332], loss=76.0728
	step [331/332], loss=63.2819
	step [332/332], loss=30.1899
	Evaluating
	loss=0.0088, precision=0.4181, recall=0.8958, f1=0.5701
saving model as: 0_saved_model.pth
Training epoch 17
	step [1/332], loss=85.5627
	step [2/332], loss=74.6978
	step [3/332], loss=63.9804
	step [4/332], loss=55.9660
	step [5/332], loss=80.7208
	step [6/332], loss=72.9578
	step [7/332], loss=74.5559
	step [8/332], loss=78.8883
	step [9/332], loss=78.7950
	step [10/332], loss=66.1806
	step [11/332], loss=55.0877
	step [12/332], loss=84.4200
	step [13/332], loss=63.9630
	step [14/332], loss=49.9358
	step [15/332], loss=75.3316
	step [16/332], loss=72.3556
	step [17/332], loss=80.8391
	step [18/332], loss=65.1360
	step [19/332], loss=68.3082
	step [20/332], loss=66.5437
	step [21/332], loss=58.0800
	step [22/332], loss=73.3199
	step [23/332], loss=76.9022
	step [24/332], loss=61.2740
	step [25/332], loss=63.8390
	step [26/332], loss=66.5235
	step [27/332], loss=79.9622
	step [28/332], loss=78.7336
	step [29/332], loss=65.9142
	step [30/332], loss=71.3201
	step [31/332], loss=51.3914
	step [32/332], loss=82.2830
	step [33/332], loss=73.4963
	step [34/332], loss=66.7997
	step [35/332], loss=75.2565
	step [36/332], loss=82.7334
	step [37/332], loss=83.7020
	step [38/332], loss=72.5527
	step [39/332], loss=68.2948
	step [40/332], loss=66.7788
	step [41/332], loss=81.8475
	step [42/332], loss=62.5538
	step [43/332], loss=88.7583
	step [44/332], loss=67.1530
	step [45/332], loss=62.5626
	step [46/332], loss=64.3492
	step [47/332], loss=76.4356
	step [48/332], loss=65.7654
	step [49/332], loss=78.4710
	step [50/332], loss=67.0943
	step [51/332], loss=61.6905
	step [52/332], loss=71.9084
	step [53/332], loss=52.0688
	step [54/332], loss=72.3764
	step [55/332], loss=61.9048
	step [56/332], loss=72.4933
	step [57/332], loss=62.1000
	step [58/332], loss=66.6375
	step [59/332], loss=65.7970
	step [60/332], loss=58.5925
	step [61/332], loss=84.2334
	step [62/332], loss=78.8437
	step [63/332], loss=75.6506
	step [64/332], loss=73.8960
	step [65/332], loss=62.8213
	step [66/332], loss=74.3309
	step [67/332], loss=63.9809
	step [68/332], loss=69.5840
	step [69/332], loss=70.1392
	step [70/332], loss=74.8106
	step [71/332], loss=66.1080
	step [72/332], loss=77.4594
	step [73/332], loss=66.5704
	step [74/332], loss=68.9719
	step [75/332], loss=59.5230
	step [76/332], loss=72.9231
	step [77/332], loss=59.2670
	step [78/332], loss=79.5585
	step [79/332], loss=65.7731
	step [80/332], loss=67.1887
	step [81/332], loss=74.2648
	step [82/332], loss=58.9644
	step [83/332], loss=76.3457
	step [84/332], loss=60.6503
	step [85/332], loss=60.1843
	step [86/332], loss=72.6814
	step [87/332], loss=78.3352
	step [88/332], loss=85.3025
	step [89/332], loss=76.4207
	step [90/332], loss=75.6125
	step [91/332], loss=79.7312
	step [92/332], loss=63.5128
	step [93/332], loss=64.1533
	step [94/332], loss=57.5478
	step [95/332], loss=78.7187
	step [96/332], loss=56.3087
	step [97/332], loss=67.2432
	step [98/332], loss=64.2383
	step [99/332], loss=58.9946
	step [100/332], loss=82.6309
	step [101/332], loss=83.1409
	step [102/332], loss=78.7934
	step [103/332], loss=78.4529
	step [104/332], loss=65.4104
	step [105/332], loss=77.7577
	step [106/332], loss=70.3090
	step [107/332], loss=65.2114
	step [108/332], loss=76.0217
	step [109/332], loss=82.9142
	step [110/332], loss=72.5313
	step [111/332], loss=76.1819
	step [112/332], loss=61.1255
	step [113/332], loss=70.6044
	step [114/332], loss=78.0312
	step [115/332], loss=69.9126
	step [116/332], loss=65.2633
	step [117/332], loss=60.9114
	step [118/332], loss=59.7438
	step [119/332], loss=66.0185
	step [120/332], loss=78.4043
	step [121/332], loss=75.5650
	step [122/332], loss=63.7177
	step [123/332], loss=70.0596
	step [124/332], loss=74.0858
	step [125/332], loss=71.1807
	step [126/332], loss=74.1078
	step [127/332], loss=76.6522
	step [128/332], loss=81.2811
	step [129/332], loss=87.0667
	step [130/332], loss=75.2912
	step [131/332], loss=73.3395
	step [132/332], loss=70.1554
	step [133/332], loss=80.9807
	step [134/332], loss=85.8254
	step [135/332], loss=80.8349
	step [136/332], loss=70.7720
	step [137/332], loss=76.0488
	step [138/332], loss=70.4181
	step [139/332], loss=68.8586
	step [140/332], loss=49.3792
	step [141/332], loss=69.1773
	step [142/332], loss=75.7809
	step [143/332], loss=85.9132
	step [144/332], loss=63.3407
	step [145/332], loss=69.6363
	step [146/332], loss=75.1504
	step [147/332], loss=63.6743
	step [148/332], loss=84.3400
	step [149/332], loss=64.0325
	step [150/332], loss=75.2150
	step [151/332], loss=72.8215
	step [152/332], loss=62.7296
	step [153/332], loss=91.9257
	step [154/332], loss=76.7021
	step [155/332], loss=86.3369
	step [156/332], loss=72.0180
	step [157/332], loss=66.8682
	step [158/332], loss=76.0754
	step [159/332], loss=70.1821
	step [160/332], loss=72.1508
	step [161/332], loss=85.4148
	step [162/332], loss=68.4060
	step [163/332], loss=71.5472
	step [164/332], loss=72.6541
	step [165/332], loss=58.9013
	step [166/332], loss=65.6164
	step [167/332], loss=72.3410
	step [168/332], loss=79.8212
	step [169/332], loss=78.4010
	step [170/332], loss=71.9599
	step [171/332], loss=58.3194
	step [172/332], loss=82.2139
	step [173/332], loss=61.8261
	step [174/332], loss=81.1236
	step [175/332], loss=58.6963
	step [176/332], loss=69.7310
	step [177/332], loss=65.9313
	step [178/332], loss=71.8217
	step [179/332], loss=62.6161
	step [180/332], loss=57.9712
	step [181/332], loss=69.1662
	step [182/332], loss=71.5625
	step [183/332], loss=88.7283
	step [184/332], loss=75.2418
	step [185/332], loss=68.5191
	step [186/332], loss=82.5139
	step [187/332], loss=89.0398
	step [188/332], loss=68.4567
	step [189/332], loss=66.0061
	step [190/332], loss=75.4444
	step [191/332], loss=60.1014
	step [192/332], loss=65.0868
	step [193/332], loss=86.8607
	step [194/332], loss=69.6529
	step [195/332], loss=79.5323
	step [196/332], loss=86.9813
	step [197/332], loss=59.0430
	step [198/332], loss=76.8087
	step [199/332], loss=53.4450
	step [200/332], loss=72.9252
	step [201/332], loss=88.7572
	step [202/332], loss=85.3216
	step [203/332], loss=74.1502
	step [204/332], loss=76.7779
	step [205/332], loss=58.1884
	step [206/332], loss=80.9167
	step [207/332], loss=88.8582
	step [208/332], loss=71.5695
	step [209/332], loss=55.2966
	step [210/332], loss=74.0199
	step [211/332], loss=80.9429
	step [212/332], loss=72.3367
	step [213/332], loss=73.7968
	step [214/332], loss=60.2907
	step [215/332], loss=71.3291
	step [216/332], loss=62.9647
	step [217/332], loss=76.1112
	step [218/332], loss=64.3335
	step [219/332], loss=67.1546
	step [220/332], loss=74.7754
	step [221/332], loss=50.4797
	step [222/332], loss=71.8518
	step [223/332], loss=83.0250
	step [224/332], loss=74.3936
	step [225/332], loss=61.1481
	step [226/332], loss=60.7254
	step [227/332], loss=68.7031
	step [228/332], loss=73.9650
	step [229/332], loss=62.3906
	step [230/332], loss=66.5517
	step [231/332], loss=61.6205
	step [232/332], loss=87.5993
	step [233/332], loss=68.7289
	step [234/332], loss=69.7139
	step [235/332], loss=79.1102
	step [236/332], loss=61.9709
	step [237/332], loss=56.0832
	step [238/332], loss=61.1761
	step [239/332], loss=62.5401
	step [240/332], loss=68.2607
	step [241/332], loss=73.3851
	step [242/332], loss=73.1392
	step [243/332], loss=63.2434
	step [244/332], loss=76.4770
	step [245/332], loss=78.0634
	step [246/332], loss=71.3955
	step [247/332], loss=72.0751
	step [248/332], loss=74.3962
	step [249/332], loss=75.0246
	step [250/332], loss=83.2232
	step [251/332], loss=69.4572
	step [252/332], loss=65.1037
	step [253/332], loss=87.0494
	step [254/332], loss=67.3491
	step [255/332], loss=73.4369
	step [256/332], loss=83.3434
	step [257/332], loss=80.9902
	step [258/332], loss=75.9608
	step [259/332], loss=73.8212
	step [260/332], loss=74.6623
	step [261/332], loss=76.3984
	step [262/332], loss=71.9258
	step [263/332], loss=90.3114
	step [264/332], loss=83.8174
	step [265/332], loss=64.4816
	step [266/332], loss=60.8737
	step [267/332], loss=63.3336
	step [268/332], loss=77.0758
	step [269/332], loss=67.9117
	step [270/332], loss=72.0116
	step [271/332], loss=67.0015
	step [272/332], loss=68.9739
	step [273/332], loss=66.4210
	step [274/332], loss=65.0746
	step [275/332], loss=85.7846
	step [276/332], loss=78.3248
	step [277/332], loss=71.9607
	step [278/332], loss=80.8958
	step [279/332], loss=77.9855
	step [280/332], loss=70.5754
	step [281/332], loss=68.9382
	step [282/332], loss=64.8140
	step [283/332], loss=67.1892
	step [284/332], loss=65.5455
	step [285/332], loss=62.6119
	step [286/332], loss=80.9014
	step [287/332], loss=91.1093
	step [288/332], loss=90.9059
	step [289/332], loss=52.2506
	step [290/332], loss=71.2902
	step [291/332], loss=78.7346
	step [292/332], loss=88.2072
	step [293/332], loss=67.0834
	step [294/332], loss=71.3732
	step [295/332], loss=71.9623
	step [296/332], loss=78.4256
	step [297/332], loss=80.8415
	step [298/332], loss=82.2523
	step [299/332], loss=58.1628
	step [300/332], loss=73.6846
	step [301/332], loss=65.9544
	step [302/332], loss=79.8031
	step [303/332], loss=71.6532
	step [304/332], loss=71.2374
	step [305/332], loss=73.4799
	step [306/332], loss=67.5881
	step [307/332], loss=69.1368
	step [308/332], loss=64.6519
	step [309/332], loss=77.2824
	step [310/332], loss=54.2134
	step [311/332], loss=69.0147
	step [312/332], loss=70.1497
	step [313/332], loss=88.7736
	step [314/332], loss=81.2528
	step [315/332], loss=67.0118
	step [316/332], loss=80.0011
	step [317/332], loss=70.7013
	step [318/332], loss=59.9172
	step [319/332], loss=68.4311
	step [320/332], loss=71.9061
	step [321/332], loss=62.6762
	step [322/332], loss=64.0740
	step [323/332], loss=67.5069
	step [324/332], loss=57.8625
	step [325/332], loss=73.6688
	step [326/332], loss=70.1657
	step [327/332], loss=76.0141
	step [328/332], loss=76.5520
	step [329/332], loss=71.0094
	step [330/332], loss=72.3520
	step [331/332], loss=64.3575
	step [332/332], loss=44.9249
	Evaluating
	loss=0.0122, precision=0.2961, recall=0.9199, f1=0.4480
Training epoch 18
	step [1/332], loss=90.3803
	step [2/332], loss=75.7755
	step [3/332], loss=80.0947
	step [4/332], loss=75.1517
	step [5/332], loss=82.9163
	step [6/332], loss=77.1175
	step [7/332], loss=55.5419
	step [8/332], loss=68.9963
	step [9/332], loss=81.7367
	step [10/332], loss=79.1072
	step [11/332], loss=72.7915
	step [12/332], loss=69.6422
	step [13/332], loss=64.7025
	step [14/332], loss=66.5609
	step [15/332], loss=59.4729
	step [16/332], loss=93.2351
	step [17/332], loss=69.5727
	step [18/332], loss=64.2141
	step [19/332], loss=68.0061
	step [20/332], loss=68.4322
	step [21/332], loss=72.5873
	step [22/332], loss=68.8627
	step [23/332], loss=68.0121
	step [24/332], loss=69.2351
	step [25/332], loss=66.0969
	step [26/332], loss=77.2749
	step [27/332], loss=77.2013
	step [28/332], loss=86.2641
	step [29/332], loss=70.1090
	step [30/332], loss=83.9863
	step [31/332], loss=73.4975
	step [32/332], loss=67.7793
	step [33/332], loss=76.1800
	step [34/332], loss=74.2512
	step [35/332], loss=73.8077
	step [36/332], loss=81.0734
	step [37/332], loss=70.5506
	step [38/332], loss=72.1841
	step [39/332], loss=73.6909
	step [40/332], loss=70.2912
	step [41/332], loss=81.2326
	step [42/332], loss=80.2183
	step [43/332], loss=48.6734
	step [44/332], loss=65.5878
	step [45/332], loss=71.3652
	step [46/332], loss=68.8291
	step [47/332], loss=62.5569
	step [48/332], loss=68.8487
	step [49/332], loss=79.8937
	step [50/332], loss=64.7585
	step [51/332], loss=66.2548
	step [52/332], loss=67.5820
	step [53/332], loss=65.7914
	step [54/332], loss=63.1622
	step [55/332], loss=60.3392
	step [56/332], loss=69.3372
	step [57/332], loss=65.9306
	step [58/332], loss=73.1286
	step [59/332], loss=74.9756
	step [60/332], loss=83.4179
	step [61/332], loss=76.4943
	step [62/332], loss=65.8801
	step [63/332], loss=63.7562
	step [64/332], loss=72.3275
	step [65/332], loss=65.5832
	step [66/332], loss=79.5829
	step [67/332], loss=87.7774
	step [68/332], loss=92.9559
	step [69/332], loss=65.9355
	step [70/332], loss=58.8637
	step [71/332], loss=82.6479
	step [72/332], loss=69.8396
	step [73/332], loss=74.3596
	step [74/332], loss=62.9402
	step [75/332], loss=73.4906
	step [76/332], loss=74.5495
	step [77/332], loss=57.5556
	step [78/332], loss=74.7365
	step [79/332], loss=68.0295
	step [80/332], loss=83.2399
	step [81/332], loss=78.4180
	step [82/332], loss=74.8901
	step [83/332], loss=74.0157
	step [84/332], loss=66.0445
	step [85/332], loss=76.5009
	step [86/332], loss=76.4101
	step [87/332], loss=73.6875
	step [88/332], loss=73.4893
	step [89/332], loss=67.8500
	step [90/332], loss=66.3913
	step [91/332], loss=86.2237
	step [92/332], loss=56.8637
	step [93/332], loss=62.3554
	step [94/332], loss=62.5140
	step [95/332], loss=56.5829
	step [96/332], loss=62.0336
	step [97/332], loss=56.1422
	step [98/332], loss=59.8874
	step [99/332], loss=71.9346
	step [100/332], loss=65.3731
	step [101/332], loss=64.1904
	step [102/332], loss=80.0460
	step [103/332], loss=87.1397
	step [104/332], loss=72.4785
	step [105/332], loss=58.5748
	step [106/332], loss=79.4911
	step [107/332], loss=80.4680
	step [108/332], loss=63.6656
	step [109/332], loss=75.5958
	step [110/332], loss=80.9257
	step [111/332], loss=71.7885
	step [112/332], loss=57.9721
	step [113/332], loss=73.6193
	step [114/332], loss=77.4633
	step [115/332], loss=69.2223
	step [116/332], loss=73.4850
	step [117/332], loss=83.9628
	step [118/332], loss=72.7932
	step [119/332], loss=85.7695
	step [120/332], loss=71.4302
	step [121/332], loss=67.8854
	step [122/332], loss=74.2952
	step [123/332], loss=70.1393
	step [124/332], loss=100.0676
	step [125/332], loss=57.7381
	step [126/332], loss=69.1739
	step [127/332], loss=55.6723
	step [128/332], loss=73.4328
	step [129/332], loss=70.4090
	step [130/332], loss=88.8509
	step [131/332], loss=72.0896
	step [132/332], loss=71.7317
	step [133/332], loss=74.1846
	step [134/332], loss=66.5394
	step [135/332], loss=81.7329
	step [136/332], loss=64.4150
	step [137/332], loss=68.3122
	step [138/332], loss=74.3703
	step [139/332], loss=65.1198
	step [140/332], loss=84.2766
	step [141/332], loss=79.3924
	step [142/332], loss=64.7434
	step [143/332], loss=79.5782
	step [144/332], loss=68.9270
	step [145/332], loss=70.5516
	step [146/332], loss=72.1957
	step [147/332], loss=77.7061
	step [148/332], loss=81.2927
	step [149/332], loss=65.9218
	step [150/332], loss=61.1525
	step [151/332], loss=79.4737
	step [152/332], loss=55.5934
	step [153/332], loss=67.9048
	step [154/332], loss=62.8585
	step [155/332], loss=66.7689
	step [156/332], loss=59.9938
	step [157/332], loss=71.4619
	step [158/332], loss=80.0426
	step [159/332], loss=67.3634
	step [160/332], loss=70.6465
	step [161/332], loss=68.3812
	step [162/332], loss=73.8589
	step [163/332], loss=65.7666
	step [164/332], loss=67.6477
	step [165/332], loss=62.4406
	step [166/332], loss=67.0437
	step [167/332], loss=78.6123
	step [168/332], loss=66.9701
	step [169/332], loss=79.4271
	step [170/332], loss=83.9101
	step [171/332], loss=63.9533
	step [172/332], loss=63.5786
	step [173/332], loss=74.2840
	step [174/332], loss=70.5168
	step [175/332], loss=59.5829
	step [176/332], loss=73.1529
	step [177/332], loss=69.2614
	step [178/332], loss=64.3466
	step [179/332], loss=65.5359
	step [180/332], loss=74.0681
	step [181/332], loss=84.9539
	step [182/332], loss=73.0403
	step [183/332], loss=71.7454
	step [184/332], loss=58.5282
	step [185/332], loss=56.9228
	step [186/332], loss=59.7583
	step [187/332], loss=58.4101
	step [188/332], loss=68.9681
	step [189/332], loss=81.5527
	step [190/332], loss=66.4635
	step [191/332], loss=71.5233
	step [192/332], loss=52.5924
	step [193/332], loss=61.4518
	step [194/332], loss=78.3957
	step [195/332], loss=59.1906
	step [196/332], loss=61.6835
	step [197/332], loss=62.0344
	step [198/332], loss=75.6177
	step [199/332], loss=78.3141
	step [200/332], loss=67.1890
	step [201/332], loss=73.5529
	step [202/332], loss=92.2697
	step [203/332], loss=66.5054
	step [204/332], loss=74.3897
	step [205/332], loss=77.3505
	step [206/332], loss=60.5659
	step [207/332], loss=83.1976
	step [208/332], loss=73.4921
	step [209/332], loss=68.2414
	step [210/332], loss=75.4971
	step [211/332], loss=76.0595
	step [212/332], loss=70.5180
	step [213/332], loss=69.1872
	step [214/332], loss=66.6482
	step [215/332], loss=58.0630
	step [216/332], loss=54.8561
	step [217/332], loss=73.1875
	step [218/332], loss=89.7434
	step [219/332], loss=78.7393
	step [220/332], loss=75.1101
	step [221/332], loss=85.4662
	step [222/332], loss=65.0746
	step [223/332], loss=73.6331
	step [224/332], loss=65.8719
	step [225/332], loss=74.1954
	step [226/332], loss=75.5121
	step [227/332], loss=71.5696
	step [228/332], loss=77.3072
	step [229/332], loss=66.6653
	step [230/332], loss=67.7327
	step [231/332], loss=73.3451
	step [232/332], loss=63.1085
	step [233/332], loss=55.1941
	step [234/332], loss=83.2073
	step [235/332], loss=72.8316
	step [236/332], loss=72.3660
	step [237/332], loss=79.8334
	step [238/332], loss=77.9318
	step [239/332], loss=69.2386
	step [240/332], loss=93.8298
	step [241/332], loss=92.1277
	step [242/332], loss=64.8687
	step [243/332], loss=75.9339
	step [244/332], loss=93.4108
	step [245/332], loss=70.8889
	step [246/332], loss=64.3663
	step [247/332], loss=62.8458
	step [248/332], loss=73.9047
	step [249/332], loss=60.3163
	step [250/332], loss=79.2128
	step [251/332], loss=62.2175
	step [252/332], loss=63.6187
	step [253/332], loss=81.1161
	step [254/332], loss=61.1615
	step [255/332], loss=84.9884
	step [256/332], loss=77.0768
	step [257/332], loss=71.5129
	step [258/332], loss=62.0658
	step [259/332], loss=74.3565
	step [260/332], loss=80.2976
	step [261/332], loss=75.0330
	step [262/332], loss=66.2836
	step [263/332], loss=56.4697
	step [264/332], loss=76.1329
	step [265/332], loss=65.6561
	step [266/332], loss=60.3372
	step [267/332], loss=75.4260
	step [268/332], loss=62.0209
	step [269/332], loss=63.7033
	step [270/332], loss=76.8397
	step [271/332], loss=86.7404
	step [272/332], loss=60.7031
	step [273/332], loss=65.6601
	step [274/332], loss=67.7564
	step [275/332], loss=64.3839
	step [276/332], loss=68.7812
	step [277/332], loss=59.2213
	step [278/332], loss=71.4542
	step [279/332], loss=69.7434
	step [280/332], loss=59.0598
	step [281/332], loss=54.3021
	step [282/332], loss=63.2327
	step [283/332], loss=54.4945
	step [284/332], loss=84.9783
	step [285/332], loss=64.3113
	step [286/332], loss=68.2801
	step [287/332], loss=55.1477
	step [288/332], loss=77.2289
	step [289/332], loss=69.0084
	step [290/332], loss=64.9878
	step [291/332], loss=74.6098
	step [292/332], loss=54.8215
	step [293/332], loss=81.6826
	step [294/332], loss=61.8022
	step [295/332], loss=68.9785
	step [296/332], loss=69.8949
	step [297/332], loss=54.7333
	step [298/332], loss=78.4151
	step [299/332], loss=70.7151
	step [300/332], loss=52.1712
	step [301/332], loss=76.9442
	step [302/332], loss=71.3112
	step [303/332], loss=63.7982
	step [304/332], loss=78.7592
	step [305/332], loss=74.9674
	step [306/332], loss=64.8626
	step [307/332], loss=49.8904
	step [308/332], loss=77.9816
	step [309/332], loss=71.6272
	step [310/332], loss=77.4800
	step [311/332], loss=62.0363
	step [312/332], loss=81.4041
	step [313/332], loss=66.0213
	step [314/332], loss=62.2646
	step [315/332], loss=64.7077
	step [316/332], loss=60.4307
	step [317/332], loss=73.1556
	step [318/332], loss=70.1865
	step [319/332], loss=73.0629
	step [320/332], loss=59.7828
	step [321/332], loss=73.3114
	step [322/332], loss=64.0160
	step [323/332], loss=61.8382
	step [324/332], loss=83.2116
	step [325/332], loss=72.3858
	step [326/332], loss=76.8229
	step [327/332], loss=77.0931
	step [328/332], loss=61.8850
	step [329/332], loss=75.6893
	step [330/332], loss=54.6184
	step [331/332], loss=68.9221
	step [332/332], loss=43.2011
	Evaluating
	loss=0.0095, precision=0.3256, recall=0.9050, f1=0.4789
Training epoch 19
	step [1/332], loss=64.5629
	step [2/332], loss=53.7604
	step [3/332], loss=66.3506
	step [4/332], loss=84.6880
	step [5/332], loss=80.1524
	step [6/332], loss=69.8733
	step [7/332], loss=70.6145
	step [8/332], loss=50.0466
	step [9/332], loss=56.7062
	step [10/332], loss=63.1618
	step [11/332], loss=65.9297
	step [12/332], loss=67.9683
	step [13/332], loss=52.9486
	step [14/332], loss=51.6635
	step [15/332], loss=69.5795
	step [16/332], loss=68.3122
	step [17/332], loss=79.4287
	step [18/332], loss=59.6335
	step [19/332], loss=57.1256
	step [20/332], loss=66.1304
	step [21/332], loss=87.6903
	step [22/332], loss=67.9392
	step [23/332], loss=74.1828
	step [24/332], loss=53.6197
	step [25/332], loss=85.0904
	step [26/332], loss=76.9395
	step [27/332], loss=67.4588
	step [28/332], loss=70.7106
	step [29/332], loss=68.1690
	step [30/332], loss=74.0960
	step [31/332], loss=73.3897
	step [32/332], loss=70.2619
	step [33/332], loss=64.3267
	step [34/332], loss=65.9172
	step [35/332], loss=66.9023
	step [36/332], loss=61.3389
	step [37/332], loss=83.1692
	step [38/332], loss=78.3666
	step [39/332], loss=67.9683
	step [40/332], loss=75.8163
	step [41/332], loss=73.6168
	step [42/332], loss=74.9310
	step [43/332], loss=69.8497
	step [44/332], loss=79.8203
	step [45/332], loss=78.7283
	step [46/332], loss=71.4535
	step [47/332], loss=77.6238
	step [48/332], loss=77.0770
	step [49/332], loss=65.2162
	step [50/332], loss=73.5617
	step [51/332], loss=72.5636
	step [52/332], loss=59.9281
	step [53/332], loss=84.7445
	step [54/332], loss=73.1107
	step [55/332], loss=68.1626
	step [56/332], loss=87.0195
	step [57/332], loss=66.0170
	step [58/332], loss=82.9082
	step [59/332], loss=66.4460
	step [60/332], loss=67.4590
	step [61/332], loss=70.7703
	step [62/332], loss=61.4605
	step [63/332], loss=80.1865
	step [64/332], loss=77.2324
	step [65/332], loss=64.6018
	step [66/332], loss=64.5722
	step [67/332], loss=68.0660
	step [68/332], loss=72.0963
	step [69/332], loss=66.8483
	step [70/332], loss=72.8670
	step [71/332], loss=65.2148
	step [72/332], loss=62.1187
	step [73/332], loss=56.1671
	step [74/332], loss=53.3046
	step [75/332], loss=80.8959
	step [76/332], loss=69.2980
	step [77/332], loss=68.4797
	step [78/332], loss=82.6778
	step [79/332], loss=46.0836
	step [80/332], loss=62.5862
	step [81/332], loss=74.2295
	step [82/332], loss=56.3300
	step [83/332], loss=62.6631
	step [84/332], loss=79.1662
	step [85/332], loss=73.2239
	step [86/332], loss=65.1721
	step [87/332], loss=56.8854
	step [88/332], loss=81.4545
	step [89/332], loss=77.2775
	step [90/332], loss=93.3728
	step [91/332], loss=61.9064
	step [92/332], loss=72.3069
	step [93/332], loss=66.3950
	step [94/332], loss=61.7630
	step [95/332], loss=62.8994
	step [96/332], loss=64.0899
	step [97/332], loss=77.9821
	step [98/332], loss=74.5713
	step [99/332], loss=82.8335
	step [100/332], loss=59.9945
	step [101/332], loss=80.7992
	step [102/332], loss=71.4483
	step [103/332], loss=78.2897
	step [104/332], loss=65.9872
	step [105/332], loss=62.3238
	step [106/332], loss=75.8474
	step [107/332], loss=73.1525
	step [108/332], loss=57.0659
	step [109/332], loss=76.5670
	step [110/332], loss=67.0893
	step [111/332], loss=67.3781
	step [112/332], loss=42.9249
	step [113/332], loss=82.5941
	step [114/332], loss=68.9462
	step [115/332], loss=71.2657
	step [116/332], loss=78.6798
	step [117/332], loss=66.2168
	step [118/332], loss=72.7266
	step [119/332], loss=66.1737
	step [120/332], loss=63.6518
	step [121/332], loss=57.7955
	step [122/332], loss=58.3094
	step [123/332], loss=65.5859
	step [124/332], loss=68.0323
	step [125/332], loss=69.7983
	step [126/332], loss=70.3806
	step [127/332], loss=75.1453
	step [128/332], loss=64.5904
	step [129/332], loss=68.7949
	step [130/332], loss=63.2895
	step [131/332], loss=64.8187
	step [132/332], loss=66.8115
	step [133/332], loss=59.5027
	step [134/332], loss=70.2969
	step [135/332], loss=71.1679
	step [136/332], loss=74.8881
	step [137/332], loss=78.0124
	step [138/332], loss=74.9262
	step [139/332], loss=68.8169
	step [140/332], loss=89.5318
	step [141/332], loss=59.6881
	step [142/332], loss=88.4239
	step [143/332], loss=61.8472
	step [144/332], loss=63.1087
	step [145/332], loss=70.6876
	step [146/332], loss=60.4540
	step [147/332], loss=64.9544
	step [148/332], loss=71.3562
	step [149/332], loss=69.8533
	step [150/332], loss=61.1062
	step [151/332], loss=81.9027
	step [152/332], loss=79.2201
	step [153/332], loss=71.2936
	step [154/332], loss=78.2800
	step [155/332], loss=67.7838
	step [156/332], loss=71.5033
	step [157/332], loss=78.5687
	step [158/332], loss=78.6164
	step [159/332], loss=72.1096
	step [160/332], loss=79.5140
	step [161/332], loss=69.7000
	step [162/332], loss=67.0152
	step [163/332], loss=82.2838
	step [164/332], loss=81.7389
	step [165/332], loss=50.5442
	step [166/332], loss=84.1875
	step [167/332], loss=79.7204
	step [168/332], loss=64.9440
	step [169/332], loss=73.6754
	step [170/332], loss=63.4827
	step [171/332], loss=73.2586
	step [172/332], loss=64.8276
	step [173/332], loss=71.1816
	step [174/332], loss=47.6168
	step [175/332], loss=70.3268
	step [176/332], loss=58.6226
	step [177/332], loss=79.7899
	step [178/332], loss=65.3968
	step [179/332], loss=67.3865
	step [180/332], loss=64.5587
	step [181/332], loss=72.4515
	step [182/332], loss=79.7349
	step [183/332], loss=70.4328
	step [184/332], loss=88.1405
	step [185/332], loss=64.4433
	step [186/332], loss=75.6082
	step [187/332], loss=78.8899
	step [188/332], loss=68.7101
	step [189/332], loss=73.4573
	step [190/332], loss=64.0857
	step [191/332], loss=78.0437
	step [192/332], loss=66.2297
	step [193/332], loss=62.6909
	step [194/332], loss=71.0952
	step [195/332], loss=72.9002
	step [196/332], loss=73.2907
	step [197/332], loss=69.6136
	step [198/332], loss=61.2017
	step [199/332], loss=73.4897
	step [200/332], loss=62.3872
	step [201/332], loss=79.1247
	step [202/332], loss=71.5016
	step [203/332], loss=70.6147
	step [204/332], loss=69.6018
	step [205/332], loss=57.8182
	step [206/332], loss=79.5794
	step [207/332], loss=62.9510
	step [208/332], loss=65.5179
	step [209/332], loss=63.8968
	step [210/332], loss=77.4082
	step [211/332], loss=65.5091
	step [212/332], loss=59.1879
	step [213/332], loss=50.2197
	step [214/332], loss=64.5238
	step [215/332], loss=49.2796
	step [216/332], loss=64.3166
	step [217/332], loss=72.3645
	step [218/332], loss=69.4093
	step [219/332], loss=93.1969
	step [220/332], loss=69.1665
	step [221/332], loss=70.7967
	step [222/332], loss=67.5746
	step [223/332], loss=74.8139
	step [224/332], loss=77.3356
	step [225/332], loss=68.3579
	step [226/332], loss=86.1693
	step [227/332], loss=80.9525
	step [228/332], loss=63.0436
	step [229/332], loss=72.8618
	step [230/332], loss=74.1108
	step [231/332], loss=74.7400
	step [232/332], loss=83.9154
	step [233/332], loss=79.2820
	step [234/332], loss=56.3719
	step [235/332], loss=59.5359
	step [236/332], loss=67.0236
	step [237/332], loss=75.3744
	step [238/332], loss=63.4730
	step [239/332], loss=70.3932
	step [240/332], loss=60.4360
	step [241/332], loss=80.8566
	step [242/332], loss=69.4736
	step [243/332], loss=61.4841
	step [244/332], loss=68.8844
	step [245/332], loss=72.5937
	step [246/332], loss=61.8124
	step [247/332], loss=76.3543
	step [248/332], loss=75.2513
	step [249/332], loss=67.6746
	step [250/332], loss=75.5960
	step [251/332], loss=78.8015
	step [252/332], loss=76.4801
	step [253/332], loss=64.7476
	step [254/332], loss=55.7671
	step [255/332], loss=78.8229
	step [256/332], loss=66.2988
	step [257/332], loss=64.5822
	step [258/332], loss=53.8885
	step [259/332], loss=60.6485
	step [260/332], loss=76.1234
	step [261/332], loss=79.6571
	step [262/332], loss=85.7207
	step [263/332], loss=68.3525
	step [264/332], loss=69.4997
	step [265/332], loss=73.2075
	step [266/332], loss=84.0463
	step [267/332], loss=55.4832
	step [268/332], loss=77.8293
	step [269/332], loss=53.9393
	step [270/332], loss=71.7109
	step [271/332], loss=58.4410
	step [272/332], loss=58.6212
	step [273/332], loss=64.5992
	step [274/332], loss=62.4770
	step [275/332], loss=61.7872
	step [276/332], loss=66.1322
	step [277/332], loss=61.6962
	step [278/332], loss=78.2655
	step [279/332], loss=65.6890
	step [280/332], loss=67.3365
	step [281/332], loss=66.5824
	step [282/332], loss=66.7573
	step [283/332], loss=65.9605
	step [284/332], loss=69.1843
	step [285/332], loss=64.3121
	step [286/332], loss=95.9882
	step [287/332], loss=58.2252
	step [288/332], loss=53.4203
	step [289/332], loss=62.7252
	step [290/332], loss=76.9115
	step [291/332], loss=73.6411
	step [292/332], loss=82.9108
	step [293/332], loss=75.7233
	step [294/332], loss=80.9763
	step [295/332], loss=64.7468
	step [296/332], loss=62.4453
	step [297/332], loss=81.6558
	step [298/332], loss=79.0916
	step [299/332], loss=63.6247
	step [300/332], loss=63.1173
	step [301/332], loss=72.6858
	step [302/332], loss=76.7572
	step [303/332], loss=62.2790
	step [304/332], loss=68.6883
	step [305/332], loss=66.6758
	step [306/332], loss=82.0807
	step [307/332], loss=72.1408
	step [308/332], loss=73.1355
	step [309/332], loss=57.9611
	step [310/332], loss=70.8658
	step [311/332], loss=71.6833
	step [312/332], loss=68.8587
	step [313/332], loss=55.6432
	step [314/332], loss=61.5394
	step [315/332], loss=81.8689
	step [316/332], loss=86.9608
	step [317/332], loss=75.6072
	step [318/332], loss=80.5016
	step [319/332], loss=71.3036
	step [320/332], loss=80.9138
	step [321/332], loss=76.7450
	step [322/332], loss=56.6879
	step [323/332], loss=65.8726
	step [324/332], loss=80.1531
	step [325/332], loss=81.8557
	step [326/332], loss=64.2834
	step [327/332], loss=67.0140
	step [328/332], loss=67.2284
	step [329/332], loss=74.1118
	step [330/332], loss=66.0002
	step [331/332], loss=49.7161
	step [332/332], loss=33.4385
	Evaluating
	loss=0.0073, precision=0.3922, recall=0.9050, f1=0.5472
Training epoch 20
	step [1/332], loss=53.3989
	step [2/332], loss=71.4284
	step [3/332], loss=64.2178
	step [4/332], loss=64.3829
	step [5/332], loss=81.8166
	step [6/332], loss=70.2617
	step [7/332], loss=79.0493
	step [8/332], loss=60.5216
	step [9/332], loss=69.9663
	step [10/332], loss=78.3890
	step [11/332], loss=61.0967
	step [12/332], loss=54.4903
	step [13/332], loss=61.9038
	step [14/332], loss=64.7119
	step [15/332], loss=80.3122
	step [16/332], loss=67.5445
	step [17/332], loss=55.0858
	step [18/332], loss=64.2774
	step [19/332], loss=60.8818
	step [20/332], loss=76.9987
	step [21/332], loss=69.6153
	step [22/332], loss=60.3687
	step [23/332], loss=83.1874
	step [24/332], loss=74.8218
	step [25/332], loss=68.9776
	step [26/332], loss=63.4866
	step [27/332], loss=65.1210
	step [28/332], loss=74.7963
	step [29/332], loss=63.2036
	step [30/332], loss=64.2985
	step [31/332], loss=59.2576
	step [32/332], loss=75.7530
	step [33/332], loss=77.8743
	step [34/332], loss=71.7803
	step [35/332], loss=71.9736
	step [36/332], loss=56.9962
	step [37/332], loss=82.5057
	step [38/332], loss=78.2634
	step [39/332], loss=60.0834
	step [40/332], loss=56.2410
	step [41/332], loss=72.7647
	step [42/332], loss=62.7368
	step [43/332], loss=75.6315
	step [44/332], loss=87.4565
	step [45/332], loss=64.4983
	step [46/332], loss=71.3420
	step [47/332], loss=57.3539
	step [48/332], loss=63.1692
	step [49/332], loss=73.7113
	step [50/332], loss=87.9693
	step [51/332], loss=56.9390
	step [52/332], loss=58.0827
	step [53/332], loss=72.3367
	step [54/332], loss=72.1303
	step [55/332], loss=81.6031
	step [56/332], loss=69.4378
	step [57/332], loss=73.0866
	step [58/332], loss=65.0208
	step [59/332], loss=69.0942
	step [60/332], loss=62.9644
	step [61/332], loss=63.5165
	step [62/332], loss=57.1501
	step [63/332], loss=55.8213
	step [64/332], loss=65.1467
	step [65/332], loss=53.9215
	step [66/332], loss=57.5843
	step [67/332], loss=69.0609
	step [68/332], loss=55.8221
	step [69/332], loss=64.9668
	step [70/332], loss=67.3518
	step [71/332], loss=68.6841
	step [72/332], loss=76.3601
	step [73/332], loss=66.8250
	step [74/332], loss=60.5960
	step [75/332], loss=72.1921
	step [76/332], loss=70.3858
	step [77/332], loss=61.4212
	step [78/332], loss=63.7313
	step [79/332], loss=74.2969
	step [80/332], loss=79.5015
	step [81/332], loss=74.3823
	step [82/332], loss=78.6979
	step [83/332], loss=65.9502
	step [84/332], loss=79.1082
	step [85/332], loss=70.5278
	step [86/332], loss=88.6473
	step [87/332], loss=71.8446
	step [88/332], loss=61.3590
	step [89/332], loss=72.9965
	step [90/332], loss=69.1815
	step [91/332], loss=68.1827
	step [92/332], loss=69.7876
	step [93/332], loss=58.1006
	step [94/332], loss=69.0907
	step [95/332], loss=67.8896
	step [96/332], loss=57.8359
	step [97/332], loss=61.0543
	step [98/332], loss=64.7175
	step [99/332], loss=60.1777
	step [100/332], loss=58.8613
	step [101/332], loss=68.3975
	step [102/332], loss=75.4388
	step [103/332], loss=60.7670
	step [104/332], loss=67.0322
	step [105/332], loss=61.5789
	step [106/332], loss=61.1890
	step [107/332], loss=70.2549
	step [108/332], loss=70.5468
	step [109/332], loss=72.2849
	step [110/332], loss=72.6044
	step [111/332], loss=61.1078
	step [112/332], loss=56.2414
	step [113/332], loss=61.0466
	step [114/332], loss=84.1722
	step [115/332], loss=79.2869
	step [116/332], loss=61.8138
	step [117/332], loss=66.6552
	step [118/332], loss=83.3504
	step [119/332], loss=71.2842
	step [120/332], loss=59.2038
	step [121/332], loss=68.9373
	step [122/332], loss=68.8844
	step [123/332], loss=54.8107
	step [124/332], loss=75.9907
	step [125/332], loss=83.2723
	step [126/332], loss=77.3344
	step [127/332], loss=59.6218
	step [128/332], loss=81.9153
	step [129/332], loss=72.1035
	step [130/332], loss=76.9349
	step [131/332], loss=68.9948
	step [132/332], loss=68.4741
	step [133/332], loss=76.9017
	step [134/332], loss=83.4891
	step [135/332], loss=60.2646
	step [136/332], loss=63.4116
	step [137/332], loss=48.9022
	step [138/332], loss=66.8523
	step [139/332], loss=69.7034
	step [140/332], loss=58.4556
	step [141/332], loss=80.2761
	step [142/332], loss=77.9464
	step [143/332], loss=66.1848
	step [144/332], loss=77.8008
	step [145/332], loss=64.5622
	step [146/332], loss=59.2469
	step [147/332], loss=60.5624
	step [148/332], loss=69.9383
	step [149/332], loss=67.6826
	step [150/332], loss=58.5327
	step [151/332], loss=69.3161
	step [152/332], loss=62.5767
	step [153/332], loss=76.0276
	step [154/332], loss=85.3960
	step [155/332], loss=75.7280
	step [156/332], loss=72.6817
	step [157/332], loss=72.7682
	step [158/332], loss=51.7431
	step [159/332], loss=65.6900
	step [160/332], loss=77.0232
	step [161/332], loss=64.5323
	step [162/332], loss=51.3577
	step [163/332], loss=66.6056
	step [164/332], loss=68.4363
	step [165/332], loss=54.3606
	step [166/332], loss=66.4954
	step [167/332], loss=66.7289
	step [168/332], loss=54.6969
	step [169/332], loss=86.5131
	step [170/332], loss=75.1014
	step [171/332], loss=62.9370
	step [172/332], loss=58.7319
	step [173/332], loss=62.1671
	step [174/332], loss=71.9417
	step [175/332], loss=60.4609
	step [176/332], loss=59.2859
	step [177/332], loss=61.4703
	step [178/332], loss=56.3378
	step [179/332], loss=66.1104
	step [180/332], loss=84.0351
	step [181/332], loss=76.7429
	step [182/332], loss=90.4614
	step [183/332], loss=77.5251
	step [184/332], loss=67.5119
	step [185/332], loss=72.1893
	step [186/332], loss=62.3180
	step [187/332], loss=65.7210
	step [188/332], loss=76.4005
	step [189/332], loss=61.9487
	step [190/332], loss=67.5455
	step [191/332], loss=53.0446
	step [192/332], loss=74.5963
	step [193/332], loss=80.9113
	step [194/332], loss=79.3676
	step [195/332], loss=75.0324
	step [196/332], loss=78.1102
	step [197/332], loss=62.9558
	step [198/332], loss=78.4200
	step [199/332], loss=71.0531
	step [200/332], loss=76.1115
	step [201/332], loss=78.8697
	step [202/332], loss=64.4969
	step [203/332], loss=75.2450
	step [204/332], loss=78.9918
	step [205/332], loss=65.5935
	step [206/332], loss=73.1850
	step [207/332], loss=63.2233
	step [208/332], loss=70.2918
	step [209/332], loss=67.6672
	step [210/332], loss=70.8283
	step [211/332], loss=68.3276
	step [212/332], loss=75.4749
	step [213/332], loss=64.5995
	step [214/332], loss=77.4597
	step [215/332], loss=63.2151
	step [216/332], loss=79.8542
	step [217/332], loss=79.9947
	step [218/332], loss=71.1160
	step [219/332], loss=67.0455
	step [220/332], loss=58.7211
	step [221/332], loss=70.4306
	step [222/332], loss=75.8199
	step [223/332], loss=73.5384
	step [224/332], loss=66.3877
	step [225/332], loss=70.1453
	step [226/332], loss=72.2603
	step [227/332], loss=71.8350
	step [228/332], loss=72.2514
	step [229/332], loss=67.6892
	step [230/332], loss=71.0297
	step [231/332], loss=70.5286
	step [232/332], loss=56.6258
	step [233/332], loss=66.4779
	step [234/332], loss=76.1455
	step [235/332], loss=72.5824
	step [236/332], loss=65.9819
	step [237/332], loss=67.6986
	step [238/332], loss=58.8903
	step [239/332], loss=60.9563
	step [240/332], loss=62.4859
	step [241/332], loss=63.4622
	step [242/332], loss=63.1825
	step [243/332], loss=66.2083
	step [244/332], loss=66.7692
	step [245/332], loss=64.2307
	step [246/332], loss=75.7145
	step [247/332], loss=71.9949
	step [248/332], loss=59.4018
	step [249/332], loss=83.9894
	step [250/332], loss=61.1463
	step [251/332], loss=65.4399
	step [252/332], loss=70.9361
	step [253/332], loss=59.8127
	step [254/332], loss=65.2159
	step [255/332], loss=68.2406
	step [256/332], loss=64.3476
	step [257/332], loss=78.8571
	step [258/332], loss=66.2140
	step [259/332], loss=66.0687
	step [260/332], loss=68.7807
	step [261/332], loss=62.4074
	step [262/332], loss=67.6627
	step [263/332], loss=81.4693
	step [264/332], loss=72.4542
	step [265/332], loss=61.7303
	step [266/332], loss=72.7636
	step [267/332], loss=54.6482
	step [268/332], loss=62.5496
	step [269/332], loss=62.5694
	step [270/332], loss=69.1745
	step [271/332], loss=74.2430
	step [272/332], loss=52.8007
	step [273/332], loss=73.0282
	step [274/332], loss=66.0496
	step [275/332], loss=58.2110
	step [276/332], loss=65.7184
	step [277/332], loss=58.7975
	step [278/332], loss=93.8107
	step [279/332], loss=69.6063
	step [280/332], loss=67.1401
	step [281/332], loss=61.6723
	step [282/332], loss=82.2146
	step [283/332], loss=81.0732
	step [284/332], loss=74.3382
	step [285/332], loss=73.5818
	step [286/332], loss=59.9717
	step [287/332], loss=72.0262
	step [288/332], loss=67.7477
	step [289/332], loss=63.9150
	step [290/332], loss=66.9768
	step [291/332], loss=72.3385
	step [292/332], loss=51.2244
	step [293/332], loss=81.8563
	step [294/332], loss=67.6434
	step [295/332], loss=78.0740
	step [296/332], loss=61.6651
	step [297/332], loss=76.8020
	step [298/332], loss=67.8046
	step [299/332], loss=67.6616
	step [300/332], loss=63.5880
	step [301/332], loss=61.7377
	step [302/332], loss=81.8644
	step [303/332], loss=84.8543
	step [304/332], loss=60.2613
	step [305/332], loss=81.3732
	step [306/332], loss=85.4758
	step [307/332], loss=68.1960
	step [308/332], loss=61.8848
	step [309/332], loss=69.8204
	step [310/332], loss=72.4494
	step [311/332], loss=68.1361
	step [312/332], loss=80.6824
	step [313/332], loss=66.5622
	step [314/332], loss=80.2369
	step [315/332], loss=66.9353
	step [316/332], loss=66.6480
	step [317/332], loss=65.9286
	step [318/332], loss=66.4948
	step [319/332], loss=76.3935
	step [320/332], loss=75.8417
	step [321/332], loss=66.3718
	step [322/332], loss=57.8169
	step [323/332], loss=64.9373
	step [324/332], loss=61.8097
	step [325/332], loss=85.8069
	step [326/332], loss=91.0525
	step [327/332], loss=76.9466
	step [328/332], loss=62.3357
	step [329/332], loss=66.3756
	step [330/332], loss=72.5686
	step [331/332], loss=66.0321
	step [332/332], loss=40.3143
	Evaluating
	loss=0.0078, precision=0.3674, recall=0.9130, f1=0.5239
Training epoch 21
	step [1/332], loss=79.7763
	step [2/332], loss=74.4192
	step [3/332], loss=69.7537
	step [4/332], loss=64.9324
	step [5/332], loss=52.7753
	step [6/332], loss=69.7508
	step [7/332], loss=57.7013
	step [8/332], loss=61.1331
	step [9/332], loss=54.2345
	step [10/332], loss=43.8869
	step [11/332], loss=62.9414
	step [12/332], loss=56.0845
	step [13/332], loss=58.6957
	step [14/332], loss=58.2764
	step [15/332], loss=72.3781
	step [16/332], loss=74.0842
	step [17/332], loss=67.3930
	step [18/332], loss=80.2176
	step [19/332], loss=70.6940
	step [20/332], loss=71.8001
	step [21/332], loss=67.2412
	step [22/332], loss=66.7157
	step [23/332], loss=68.2956
	step [24/332], loss=70.1108
	step [25/332], loss=65.4985
	step [26/332], loss=79.8528
	step [27/332], loss=79.5087
	step [28/332], loss=72.4868
	step [29/332], loss=54.9759
	step [30/332], loss=64.5554
	step [31/332], loss=60.5266
	step [32/332], loss=59.9689
	step [33/332], loss=66.5504
	step [34/332], loss=75.9049
	step [35/332], loss=65.9761
	step [36/332], loss=76.0029
	step [37/332], loss=75.4119
	step [38/332], loss=68.0450
	step [39/332], loss=61.8144
	step [40/332], loss=54.4986
	step [41/332], loss=71.8672
	step [42/332], loss=77.5297
	step [43/332], loss=93.0283
	step [44/332], loss=76.8501
	step [45/332], loss=72.9091
	step [46/332], loss=74.5066
	step [47/332], loss=73.0524
	step [48/332], loss=59.0557
	step [49/332], loss=51.9903
	step [50/332], loss=83.3530
	step [51/332], loss=65.8495
	step [52/332], loss=63.5555
	step [53/332], loss=55.5017
	step [54/332], loss=64.1240
	step [55/332], loss=62.9093
	step [56/332], loss=66.2843
	step [57/332], loss=66.3780
	step [58/332], loss=67.0696
	step [59/332], loss=72.3234
	step [60/332], loss=48.1104
	step [61/332], loss=62.3605
	step [62/332], loss=62.4190
	step [63/332], loss=76.2197
	step [64/332], loss=79.7507
	step [65/332], loss=62.7608
	step [66/332], loss=67.0096
	step [67/332], loss=76.8752
	step [68/332], loss=58.8917
	step [69/332], loss=70.1323
	step [70/332], loss=73.2989
	step [71/332], loss=75.4668
	step [72/332], loss=78.7147
	step [73/332], loss=67.8526
	step [74/332], loss=79.2206
	step [75/332], loss=60.5051
	step [76/332], loss=72.7282
	step [77/332], loss=61.1155
	step [78/332], loss=82.4026
	step [79/332], loss=74.6107
	step [80/332], loss=72.7414
	step [81/332], loss=69.3210
	step [82/332], loss=54.9709
	step [83/332], loss=76.6861
	step [84/332], loss=68.5763
	step [85/332], loss=80.9079
	step [86/332], loss=74.2319
	step [87/332], loss=62.4968
	step [88/332], loss=69.5180
	step [89/332], loss=57.3397
	step [90/332], loss=70.1074
	step [91/332], loss=61.9636
	step [92/332], loss=68.9090
	step [93/332], loss=43.8230
	step [94/332], loss=81.8868
	step [95/332], loss=57.3054
	step [96/332], loss=49.9346
	step [97/332], loss=76.3703
	step [98/332], loss=69.5726
	step [99/332], loss=68.6499
	step [100/332], loss=69.2704
	step [101/332], loss=56.7575
	step [102/332], loss=61.4485
	step [103/332], loss=85.3655
	step [104/332], loss=56.3092
	step [105/332], loss=60.1278
	step [106/332], loss=67.9641
	step [107/332], loss=82.0536
	step [108/332], loss=71.3439
	step [109/332], loss=70.9433
	step [110/332], loss=83.4225
	step [111/332], loss=75.5373
	step [112/332], loss=80.1603
	step [113/332], loss=70.4334
	step [114/332], loss=64.4873
	step [115/332], loss=74.8642
	step [116/332], loss=72.9821
	step [117/332], loss=68.1067
	step [118/332], loss=67.1607
	step [119/332], loss=67.4409
	step [120/332], loss=69.7073
	step [121/332], loss=65.8069
	step [122/332], loss=80.5425
	step [123/332], loss=69.3495
	step [124/332], loss=61.8804
	step [125/332], loss=71.4421
	step [126/332], loss=81.2039
	step [127/332], loss=70.5504
	step [128/332], loss=63.5835
	step [129/332], loss=71.5614
	step [130/332], loss=65.6339
	step [131/332], loss=63.7322
	step [132/332], loss=60.9303
	step [133/332], loss=59.5778
	step [134/332], loss=87.4765
	step [135/332], loss=68.0828
	step [136/332], loss=69.6163
	step [137/332], loss=61.9846
	step [138/332], loss=60.0745
	step [139/332], loss=75.7605
	step [140/332], loss=63.9323
	step [141/332], loss=59.2504
	step [142/332], loss=66.8374
	step [143/332], loss=69.1002
	step [144/332], loss=75.3219
	step [145/332], loss=84.0294
	step [146/332], loss=67.4761
	step [147/332], loss=77.2423
	step [148/332], loss=66.9049
	step [149/332], loss=73.9431
	step [150/332], loss=69.8542
	step [151/332], loss=47.9834
	step [152/332], loss=61.2272
	step [153/332], loss=62.6925
	step [154/332], loss=71.8297
	step [155/332], loss=70.2350
	step [156/332], loss=63.6820
	step [157/332], loss=62.2600
	step [158/332], loss=65.0254
	step [159/332], loss=57.7022
	step [160/332], loss=72.2688
	step [161/332], loss=67.4362
	step [162/332], loss=80.4645
	step [163/332], loss=65.2412
	step [164/332], loss=75.3837
	step [165/332], loss=62.8998
	step [166/332], loss=69.6746
	step [167/332], loss=71.9212
	step [168/332], loss=73.3382
	step [169/332], loss=57.7708
	step [170/332], loss=74.6577
	step [171/332], loss=82.8124
	step [172/332], loss=49.5255
	step [173/332], loss=76.9656
	step [174/332], loss=87.4717
	step [175/332], loss=59.9935
	step [176/332], loss=58.7184
	step [177/332], loss=67.7931
	step [178/332], loss=92.0348
	step [179/332], loss=67.5087
	step [180/332], loss=68.5173
	step [181/332], loss=60.8524
	step [182/332], loss=66.2704
	step [183/332], loss=73.9275
	step [184/332], loss=71.2130
	step [185/332], loss=44.7609
	step [186/332], loss=74.7424
	step [187/332], loss=74.4616
	step [188/332], loss=64.7479
	step [189/332], loss=70.4586
	step [190/332], loss=54.4791
	step [191/332], loss=68.1594
	step [192/332], loss=75.2725
	step [193/332], loss=62.7586
	step [194/332], loss=69.9723
	step [195/332], loss=53.6573
	step [196/332], loss=64.4244
	step [197/332], loss=74.6557
	step [198/332], loss=51.8559
	step [199/332], loss=74.1000
	step [200/332], loss=66.5382
	step [201/332], loss=71.3187
	step [202/332], loss=56.9779
	step [203/332], loss=59.2434
	step [204/332], loss=65.3316
	step [205/332], loss=63.2464
	step [206/332], loss=65.9620
	step [207/332], loss=57.7994
	step [208/332], loss=73.5519
	step [209/332], loss=69.4459
	step [210/332], loss=68.4756
	step [211/332], loss=68.3012
	step [212/332], loss=75.5312
	step [213/332], loss=74.3917
	step [214/332], loss=61.4406
	step [215/332], loss=67.7647
	step [216/332], loss=65.9488
	step [217/332], loss=67.4319
	step [218/332], loss=53.2784
	step [219/332], loss=67.9442
	step [220/332], loss=52.1682
	step [221/332], loss=67.5277
	step [222/332], loss=53.7848
	step [223/332], loss=71.1980
	step [224/332], loss=80.3893
	step [225/332], loss=85.9386
	step [226/332], loss=75.6167
	step [227/332], loss=58.5927
	step [228/332], loss=68.2368
	step [229/332], loss=69.6271
	step [230/332], loss=71.0629
	step [231/332], loss=68.4200
	step [232/332], loss=51.7890
	step [233/332], loss=57.6200
	step [234/332], loss=68.6518
	step [235/332], loss=77.1399
	step [236/332], loss=73.1844
	step [237/332], loss=63.4451
	step [238/332], loss=66.8615
	step [239/332], loss=74.4751
	step [240/332], loss=68.2270
	step [241/332], loss=64.3286
	step [242/332], loss=58.2742
	step [243/332], loss=59.3615
	step [244/332], loss=88.7414
	step [245/332], loss=82.3598
	step [246/332], loss=65.3903
	step [247/332], loss=52.2113
	step [248/332], loss=72.2365
	step [249/332], loss=72.3974
	step [250/332], loss=72.4895
	step [251/332], loss=75.3714
	step [252/332], loss=60.3371
	step [253/332], loss=78.4898
	step [254/332], loss=86.2673
	step [255/332], loss=78.4686
	step [256/332], loss=70.5900
	step [257/332], loss=68.6474
	step [258/332], loss=60.4985
	step [259/332], loss=71.2191
	step [260/332], loss=51.3582
	step [261/332], loss=60.9483
	step [262/332], loss=67.8687
	step [263/332], loss=62.5816
	step [264/332], loss=69.6535
	step [265/332], loss=64.3278
	step [266/332], loss=77.6444
	step [267/332], loss=76.0912
	step [268/332], loss=64.0217
	step [269/332], loss=62.9616
	step [270/332], loss=76.2054
	step [271/332], loss=74.4653
	step [272/332], loss=79.4311
	step [273/332], loss=74.1722
	step [274/332], loss=72.2223
	step [275/332], loss=61.9918
	step [276/332], loss=79.5624
	step [277/332], loss=69.5094
	step [278/332], loss=61.8317
	step [279/332], loss=57.0577
	step [280/332], loss=67.2994
	step [281/332], loss=74.8357
	step [282/332], loss=64.4976
	step [283/332], loss=74.7945
	step [284/332], loss=68.2332
	step [285/332], loss=67.4521
	step [286/332], loss=76.6352
	step [287/332], loss=57.8073
	step [288/332], loss=63.0452
	step [289/332], loss=65.5146
	step [290/332], loss=62.6454
	step [291/332], loss=70.4734
	step [292/332], loss=82.2498
	step [293/332], loss=69.8200
	step [294/332], loss=46.9767
	step [295/332], loss=56.2316
	step [296/332], loss=51.0266
	step [297/332], loss=64.2983
	step [298/332], loss=69.8626
	step [299/332], loss=82.6240
	step [300/332], loss=65.6395
	step [301/332], loss=72.5971
	step [302/332], loss=72.8504
	step [303/332], loss=67.7823
	step [304/332], loss=61.6593
	step [305/332], loss=66.2836
	step [306/332], loss=57.1566
	step [307/332], loss=67.4624
	step [308/332], loss=74.6597
	step [309/332], loss=57.1624
	step [310/332], loss=75.3019
	step [311/332], loss=68.0420
	step [312/332], loss=72.4916
	step [313/332], loss=61.2667
	step [314/332], loss=56.4465
	step [315/332], loss=68.7127
	step [316/332], loss=47.2946
	step [317/332], loss=80.8682
	step [318/332], loss=68.6652
	step [319/332], loss=63.9827
	step [320/332], loss=54.0489
	step [321/332], loss=57.4757
	step [322/332], loss=77.3725
	step [323/332], loss=59.8167
	step [324/332], loss=60.0008
	step [325/332], loss=82.8259
	step [326/332], loss=71.1279
	step [327/332], loss=67.3948
	step [328/332], loss=74.1366
	step [329/332], loss=54.6216
	step [330/332], loss=68.1456
	step [331/332], loss=73.8070
	step [332/332], loss=31.2746
	Evaluating
	loss=0.0075, precision=0.4070, recall=0.9034, f1=0.5612
Training epoch 22
	step [1/332], loss=62.6478
	step [2/332], loss=73.9501
	step [3/332], loss=59.5333
	step [4/332], loss=90.3826
	step [5/332], loss=76.5898
	step [6/332], loss=75.4447
	step [7/332], loss=62.4043
	step [8/332], loss=69.7811
	step [9/332], loss=65.0564
	step [10/332], loss=63.4442
	step [11/332], loss=65.9472
	step [12/332], loss=61.3691
	step [13/332], loss=75.2680
	step [14/332], loss=68.5605
	step [15/332], loss=75.6171
	step [16/332], loss=63.8771
	step [17/332], loss=63.9290
	step [18/332], loss=86.4205
	step [19/332], loss=76.8303
	step [20/332], loss=62.5237
	step [21/332], loss=67.0529
	step [22/332], loss=73.3485
	step [23/332], loss=67.0649
	step [24/332], loss=69.7431
	step [25/332], loss=69.1939
	step [26/332], loss=68.7092
	step [27/332], loss=75.8492
	step [28/332], loss=87.9988
	step [29/332], loss=60.0623
	step [30/332], loss=56.5990
	step [31/332], loss=78.3286
	step [32/332], loss=68.5260
	step [33/332], loss=63.9948
	step [34/332], loss=65.0896
	step [35/332], loss=72.5170
	step [36/332], loss=68.5980
	step [37/332], loss=77.3015
	step [38/332], loss=62.7467
	step [39/332], loss=76.0050
	step [40/332], loss=78.3944
	step [41/332], loss=63.2408
	step [42/332], loss=84.8480
	step [43/332], loss=64.8378
	step [44/332], loss=74.3374
	step [45/332], loss=70.1753
	step [46/332], loss=75.4401
	step [47/332], loss=64.0022
	step [48/332], loss=86.3936
	step [49/332], loss=64.2012
	step [50/332], loss=72.5183
	step [51/332], loss=65.5315
	step [52/332], loss=56.3745
	step [53/332], loss=56.9282
	step [54/332], loss=78.6278
	step [55/332], loss=64.6795
	step [56/332], loss=54.2713
	step [57/332], loss=69.3394
	step [58/332], loss=78.3445
	step [59/332], loss=70.4765
	step [60/332], loss=65.2909
	step [61/332], loss=50.7213
	step [62/332], loss=67.8141
	step [63/332], loss=51.4989
	step [64/332], loss=62.5223
	step [65/332], loss=65.1740
	step [66/332], loss=75.5583
	step [67/332], loss=67.9325
	step [68/332], loss=77.3438
	step [69/332], loss=79.9099
	step [70/332], loss=73.4994
	step [71/332], loss=57.6704
	step [72/332], loss=80.1334
	step [73/332], loss=70.9982
	step [74/332], loss=67.6001
	step [75/332], loss=67.6456
	step [76/332], loss=53.4543
	step [77/332], loss=72.8862
	step [78/332], loss=68.1829
	step [79/332], loss=47.5883
	step [80/332], loss=66.5409
	step [81/332], loss=63.2211
	step [82/332], loss=54.5674
	step [83/332], loss=67.8401
	step [84/332], loss=59.8229
	step [85/332], loss=58.7260
	step [86/332], loss=67.0524
	step [87/332], loss=64.7688
	step [88/332], loss=65.5172
	step [89/332], loss=73.3711
	step [90/332], loss=65.2150
	step [91/332], loss=71.7381
	step [92/332], loss=69.2426
	step [93/332], loss=77.1682
	step [94/332], loss=63.2045
	step [95/332], loss=62.9986
	step [96/332], loss=51.2071
	step [97/332], loss=69.3789
	step [98/332], loss=66.5950
	step [99/332], loss=56.9030
	step [100/332], loss=69.2788
	step [101/332], loss=57.1170
	step [102/332], loss=73.6460
	step [103/332], loss=73.1679
	step [104/332], loss=53.6144
	step [105/332], loss=62.6461
	step [106/332], loss=62.2606
	step [107/332], loss=70.5210
	step [108/332], loss=65.0315
	step [109/332], loss=57.4217
	step [110/332], loss=73.8766
	step [111/332], loss=73.3090
	step [112/332], loss=77.1449
	step [113/332], loss=83.3735
	step [114/332], loss=69.8835
	step [115/332], loss=75.2031
	step [116/332], loss=62.0037
	step [117/332], loss=60.8640
	step [118/332], loss=51.1587
	step [119/332], loss=69.3946
	step [120/332], loss=58.3770
	step [121/332], loss=60.2427
	step [122/332], loss=69.7950
	step [123/332], loss=75.2164
	step [124/332], loss=84.7322
	step [125/332], loss=72.4371
	step [126/332], loss=70.0704
	step [127/332], loss=83.1106
	step [128/332], loss=62.6437
	step [129/332], loss=65.2410
	step [130/332], loss=67.2995
	step [131/332], loss=62.1117
	step [132/332], loss=56.2813
	step [133/332], loss=64.6171
	step [134/332], loss=75.6722
	step [135/332], loss=46.1742
	step [136/332], loss=72.4266
	step [137/332], loss=83.0360
	step [138/332], loss=59.9006
	step [139/332], loss=68.0361
	step [140/332], loss=63.7613
	step [141/332], loss=69.1361
	step [142/332], loss=76.8126
	step [143/332], loss=62.5838
	step [144/332], loss=64.6664
	step [145/332], loss=69.1228
	step [146/332], loss=55.1540
	step [147/332], loss=58.1443
	step [148/332], loss=63.8682
	step [149/332], loss=73.7783
	step [150/332], loss=53.9365
	step [151/332], loss=55.4233
	step [152/332], loss=84.8418
	step [153/332], loss=64.4358
	step [154/332], loss=67.2664
	step [155/332], loss=80.4656
	step [156/332], loss=58.8416
	step [157/332], loss=76.4797
	step [158/332], loss=72.6505
	step [159/332], loss=62.3456
	step [160/332], loss=69.8949
	step [161/332], loss=63.2747
	step [162/332], loss=68.7523
	step [163/332], loss=67.0023
	step [164/332], loss=70.4511
	step [165/332], loss=53.9745
	step [166/332], loss=59.9813
	step [167/332], loss=48.0442
	step [168/332], loss=65.6907
	step [169/332], loss=64.5376
	step [170/332], loss=50.0216
	step [171/332], loss=59.9893
	step [172/332], loss=71.5958
	step [173/332], loss=62.0267
	step [174/332], loss=61.4282
	step [175/332], loss=56.0368
	step [176/332], loss=62.3359
	step [177/332], loss=63.8163
	step [178/332], loss=55.2563
	step [179/332], loss=69.0823
	step [180/332], loss=55.1334
	step [181/332], loss=66.2737
	step [182/332], loss=69.7657
	step [183/332], loss=54.9382
	step [184/332], loss=73.7611
	step [185/332], loss=80.0479
	step [186/332], loss=66.5015
	step [187/332], loss=63.1938
	step [188/332], loss=70.7215
	step [189/332], loss=93.5660
	step [190/332], loss=55.3596
	step [191/332], loss=62.1509
	step [192/332], loss=62.5440
	step [193/332], loss=66.3198
	step [194/332], loss=60.6001
	step [195/332], loss=69.0525
	step [196/332], loss=58.6542
	step [197/332], loss=78.1897
	step [198/332], loss=67.9995
	step [199/332], loss=66.3083
	step [200/332], loss=71.7832
	step [201/332], loss=81.4895
	step [202/332], loss=71.9917
	step [203/332], loss=68.7564
	step [204/332], loss=59.5141
	step [205/332], loss=62.8837
	step [206/332], loss=63.6774
	step [207/332], loss=55.5268
	step [208/332], loss=64.2912
	step [209/332], loss=53.6169
	step [210/332], loss=61.5497
	step [211/332], loss=66.2540
	step [212/332], loss=51.8003
	step [213/332], loss=72.6022
	step [214/332], loss=74.7604
	step [215/332], loss=73.5007
	step [216/332], loss=67.9431
	step [217/332], loss=70.8509
	step [218/332], loss=60.5363
	step [219/332], loss=55.1135
	step [220/332], loss=51.0862
	step [221/332], loss=80.5709
	step [222/332], loss=59.3291
	step [223/332], loss=66.4362
	step [224/332], loss=80.0328
	step [225/332], loss=72.0709
	step [226/332], loss=60.7900
	step [227/332], loss=70.6007
	step [228/332], loss=68.1652
	step [229/332], loss=74.4774
	step [230/332], loss=61.8304
	step [231/332], loss=70.5020
	step [232/332], loss=59.7078
	step [233/332], loss=61.7428
	step [234/332], loss=80.4446
	step [235/332], loss=67.8085
	step [236/332], loss=55.5074
	step [237/332], loss=68.6133
	step [238/332], loss=66.8758
	step [239/332], loss=74.5101
	step [240/332], loss=73.5284
	step [241/332], loss=71.1909
	step [242/332], loss=54.1090
	step [243/332], loss=69.7227
	step [244/332], loss=77.4838
	step [245/332], loss=68.7242
	step [246/332], loss=71.6871
	step [247/332], loss=52.9146
	step [248/332], loss=55.1835
	step [249/332], loss=63.7309
	step [250/332], loss=56.2923
	step [251/332], loss=86.9687
	step [252/332], loss=70.1789
	step [253/332], loss=64.7028
	step [254/332], loss=61.0750
	step [255/332], loss=73.4273
	step [256/332], loss=54.7886
	step [257/332], loss=63.1643
	step [258/332], loss=55.5781
	step [259/332], loss=69.1982
	step [260/332], loss=67.1350
	step [261/332], loss=49.8492
	step [262/332], loss=64.4716
	step [263/332], loss=61.1774
	step [264/332], loss=78.4670
	step [265/332], loss=83.6790
	step [266/332], loss=63.9914
	step [267/332], loss=67.9577
	step [268/332], loss=68.5271
	step [269/332], loss=60.6051
	step [270/332], loss=64.9359
	step [271/332], loss=84.9222
	step [272/332], loss=81.5690
	step [273/332], loss=71.4027
	step [274/332], loss=66.2767
	step [275/332], loss=62.6331
	step [276/332], loss=64.8573
	step [277/332], loss=72.8967
	step [278/332], loss=64.3781
	step [279/332], loss=90.4920
	step [280/332], loss=73.8883
	step [281/332], loss=70.6607
	step [282/332], loss=61.4393
	step [283/332], loss=61.1139
	step [284/332], loss=64.6759
	step [285/332], loss=61.7706
	step [286/332], loss=66.2009
	step [287/332], loss=49.0609
	step [288/332], loss=69.5719
	step [289/332], loss=68.9676
	step [290/332], loss=57.5126
	step [291/332], loss=62.0523
	step [292/332], loss=56.5011
	step [293/332], loss=69.9817
	step [294/332], loss=73.5537
	step [295/332], loss=69.9102
	step [296/332], loss=63.0595
	step [297/332], loss=52.7571
	step [298/332], loss=47.4592
	step [299/332], loss=79.7935
	step [300/332], loss=61.4482
	step [301/332], loss=61.4085
	step [302/332], loss=69.8399
	step [303/332], loss=56.7846
	step [304/332], loss=64.5617
	step [305/332], loss=71.5555
	step [306/332], loss=70.5495
	step [307/332], loss=72.2042
	step [308/332], loss=64.2110
	step [309/332], loss=72.7380
	step [310/332], loss=73.4648
	step [311/332], loss=66.2933
	step [312/332], loss=81.9394
	step [313/332], loss=74.1424
	step [314/332], loss=68.1012
	step [315/332], loss=67.1157
	step [316/332], loss=66.8947
	step [317/332], loss=67.4380
	step [318/332], loss=72.1126
	step [319/332], loss=70.0905
	step [320/332], loss=69.7038
	step [321/332], loss=66.1046
	step [322/332], loss=62.1039
	step [323/332], loss=66.3689
	step [324/332], loss=65.2641
	step [325/332], loss=72.5790
	step [326/332], loss=55.1051
	step [327/332], loss=61.3305
	step [328/332], loss=71.1311
	step [329/332], loss=81.2318
	step [330/332], loss=71.4328
	step [331/332], loss=74.1701
	step [332/332], loss=35.7146
	Evaluating
	loss=0.0067, precision=0.4114, recall=0.8898, f1=0.5626
Training epoch 23
	step [1/332], loss=70.2091
	step [2/332], loss=68.1823
	step [3/332], loss=62.7343
	step [4/332], loss=69.4356
	step [5/332], loss=63.0461
	step [6/332], loss=60.6060
	step [7/332], loss=61.2919
	step [8/332], loss=63.9437
	step [9/332], loss=76.3507
	step [10/332], loss=66.8590
	step [11/332], loss=76.0744
	step [12/332], loss=85.8488
	step [13/332], loss=62.4790
	step [14/332], loss=70.5225
	step [15/332], loss=63.0906
	step [16/332], loss=65.3013
	step [17/332], loss=59.6793
	step [18/332], loss=47.4759
	step [19/332], loss=60.2178
	step [20/332], loss=60.5394
	step [21/332], loss=81.0429
	step [22/332], loss=77.4430
	step [23/332], loss=71.8961
	step [24/332], loss=56.0117
	step [25/332], loss=79.8402
	step [26/332], loss=64.3841
	step [27/332], loss=72.4436
	step [28/332], loss=52.4466
	step [29/332], loss=52.2538
	step [30/332], loss=67.4152
	step [31/332], loss=72.2909
	step [32/332], loss=69.1896
	step [33/332], loss=51.9849
	step [34/332], loss=56.4626
	step [35/332], loss=62.4268
	step [36/332], loss=62.7201
	step [37/332], loss=65.6176
	step [38/332], loss=71.6808
	step [39/332], loss=61.4541
	step [40/332], loss=53.6945
	step [41/332], loss=61.3364
	step [42/332], loss=66.0929
	step [43/332], loss=52.9664
	step [44/332], loss=46.6298
	step [45/332], loss=70.4132
	step [46/332], loss=64.3704
	step [47/332], loss=70.3939
	step [48/332], loss=67.0193
	step [49/332], loss=74.0111
	step [50/332], loss=59.6977
	step [51/332], loss=71.8677
	step [52/332], loss=65.6373
	step [53/332], loss=75.6138
	step [54/332], loss=73.2105
	step [55/332], loss=93.2711
	step [56/332], loss=58.1531
	step [57/332], loss=77.8186
	step [58/332], loss=76.3792
	step [59/332], loss=79.3516
	step [60/332], loss=67.5830
	step [61/332], loss=62.2716
	step [62/332], loss=67.0881
	step [63/332], loss=59.0147
	step [64/332], loss=52.0654
	step [65/332], loss=88.1647
	step [66/332], loss=70.1901
	step [67/332], loss=72.3533
	step [68/332], loss=60.3855
	step [69/332], loss=61.5951
	step [70/332], loss=74.5240
	step [71/332], loss=75.6395
	step [72/332], loss=71.3553
	step [73/332], loss=64.4455
	step [74/332], loss=71.1894
	step [75/332], loss=78.1163
	step [76/332], loss=66.4464
	step [77/332], loss=71.5451
	step [78/332], loss=69.9260
	step [79/332], loss=76.8354
	step [80/332], loss=63.3076
	step [81/332], loss=60.0354
	step [82/332], loss=68.6873
	step [83/332], loss=68.7373
	step [84/332], loss=74.2809
	step [85/332], loss=80.6763
	step [86/332], loss=69.7081
	step [87/332], loss=77.4995
	step [88/332], loss=66.8681
	step [89/332], loss=72.3711
	step [90/332], loss=57.8401
	step [91/332], loss=44.6802
	step [92/332], loss=77.5265
	step [93/332], loss=66.6773
	step [94/332], loss=67.0341
	step [95/332], loss=69.6872
	step [96/332], loss=74.3463
	step [97/332], loss=64.9878
	step [98/332], loss=78.5469
	step [99/332], loss=65.7567
	step [100/332], loss=83.6046
	step [101/332], loss=69.6576
	step [102/332], loss=72.4135
	step [103/332], loss=83.2819
	step [104/332], loss=72.9899
	step [105/332], loss=62.4432
	step [106/332], loss=72.5620
	step [107/332], loss=69.0410
	step [108/332], loss=71.6302
	step [109/332], loss=57.6881
	step [110/332], loss=49.8157
	step [111/332], loss=52.4557
	step [112/332], loss=72.1087
	step [113/332], loss=63.3070
	step [114/332], loss=61.9336
	step [115/332], loss=67.8778
	step [116/332], loss=57.8329
	step [117/332], loss=66.1088
	step [118/332], loss=71.5119
	step [119/332], loss=67.7154
	step [120/332], loss=69.5422
	step [121/332], loss=71.2866
	step [122/332], loss=65.3782
	step [123/332], loss=54.2109
	step [124/332], loss=69.8803
	step [125/332], loss=56.3670
	step [126/332], loss=68.8682
	step [127/332], loss=58.4831
	step [128/332], loss=62.2209
	step [129/332], loss=56.8642
	step [130/332], loss=58.5863
	step [131/332], loss=65.0595
	step [132/332], loss=71.0291
	step [133/332], loss=54.5508
	step [134/332], loss=75.6117
	step [135/332], loss=67.1821
	step [136/332], loss=63.9986
	step [137/332], loss=67.4655
	step [138/332], loss=62.3587
	step [139/332], loss=50.5844
	step [140/332], loss=62.1881
	step [141/332], loss=75.3208
	step [142/332], loss=68.7717
	step [143/332], loss=55.9128
	step [144/332], loss=59.1903
	step [145/332], loss=61.9585
	step [146/332], loss=58.7547
	step [147/332], loss=77.4928
	step [148/332], loss=77.6362
	step [149/332], loss=64.9297
	step [150/332], loss=76.9164
	step [151/332], loss=68.7771
	step [152/332], loss=55.6532
	step [153/332], loss=54.2744
	step [154/332], loss=65.0583
	step [155/332], loss=72.0797
	step [156/332], loss=74.7004
	step [157/332], loss=70.6123
	step [158/332], loss=70.2954
	step [159/332], loss=62.4086
	step [160/332], loss=57.3620
	step [161/332], loss=45.5494
	step [162/332], loss=61.7068
	step [163/332], loss=66.3892
	step [164/332], loss=67.0347
	step [165/332], loss=66.6484
	step [166/332], loss=80.1886
	step [167/332], loss=52.5130
	step [168/332], loss=80.9186
	step [169/332], loss=63.4829
	step [170/332], loss=70.5415
	step [171/332], loss=61.2503
	step [172/332], loss=60.4698
	step [173/332], loss=58.2412
	step [174/332], loss=64.5882
	step [175/332], loss=68.3409
	step [176/332], loss=71.4561
	step [177/332], loss=58.6981
	step [178/332], loss=72.5578
	step [179/332], loss=63.4863
	step [180/332], loss=67.5631
	step [181/332], loss=71.0358
	step [182/332], loss=74.3295
	step [183/332], loss=58.8612
	step [184/332], loss=73.2701
	step [185/332], loss=70.7075
	step [186/332], loss=61.1652
	step [187/332], loss=70.8485
	step [188/332], loss=72.8716
	step [189/332], loss=67.2556
	step [190/332], loss=58.0587
	step [191/332], loss=76.2151
	step [192/332], loss=54.8281
	step [193/332], loss=81.6442
	step [194/332], loss=63.0033
	step [195/332], loss=64.3273
	step [196/332], loss=54.1632
	step [197/332], loss=72.4963
	step [198/332], loss=75.3733
	step [199/332], loss=62.6286
	step [200/332], loss=85.0424
	step [201/332], loss=62.2724
	step [202/332], loss=57.6818
	step [203/332], loss=62.9677
	step [204/332], loss=66.4479
	step [205/332], loss=62.4239
	step [206/332], loss=68.4329
	step [207/332], loss=62.0381
	step [208/332], loss=70.9789
	step [209/332], loss=65.1371
	step [210/332], loss=64.9203
	step [211/332], loss=73.0803
	step [212/332], loss=66.0416
	step [213/332], loss=74.5905
	step [214/332], loss=50.9184
	step [215/332], loss=67.7165
	step [216/332], loss=64.7978
	step [217/332], loss=51.2542
	step [218/332], loss=72.2997
	step [219/332], loss=61.8411
	step [220/332], loss=86.6093
	step [221/332], loss=68.7320
	step [222/332], loss=63.0688
	step [223/332], loss=77.5667
	step [224/332], loss=58.9486
	step [225/332], loss=65.9211
	step [226/332], loss=67.1144
	step [227/332], loss=63.5270
	step [228/332], loss=59.7831
	step [229/332], loss=50.5410
	step [230/332], loss=58.3287
	step [231/332], loss=62.1392
	step [232/332], loss=72.9365
	step [233/332], loss=63.2536
	step [234/332], loss=64.5275
	step [235/332], loss=74.2273
	step [236/332], loss=49.8913
	step [237/332], loss=66.1648
	step [238/332], loss=57.4627
	step [239/332], loss=61.4057
	step [240/332], loss=52.8250
	step [241/332], loss=61.5190
	step [242/332], loss=59.1702
	step [243/332], loss=58.4165
	step [244/332], loss=67.0638
	step [245/332], loss=57.3773
	step [246/332], loss=63.1660
	step [247/332], loss=64.4908
	step [248/332], loss=75.2728
	step [249/332], loss=73.7404
	step [250/332], loss=63.0243
	step [251/332], loss=82.5700
	step [252/332], loss=64.6519
	step [253/332], loss=50.6423
	step [254/332], loss=67.4036
	step [255/332], loss=66.4803
	step [256/332], loss=69.2237
	step [257/332], loss=75.0372
	step [258/332], loss=64.3367
	step [259/332], loss=62.5007
	step [260/332], loss=66.7894
	step [261/332], loss=68.4347
	step [262/332], loss=62.7351
	step [263/332], loss=63.7579
	step [264/332], loss=64.9518
	step [265/332], loss=70.8373
	step [266/332], loss=46.1288
	step [267/332], loss=68.5065
	step [268/332], loss=59.9043
	step [269/332], loss=68.0298
	step [270/332], loss=73.2858
	step [271/332], loss=66.7781
	step [272/332], loss=68.6394
	step [273/332], loss=86.1526
	step [274/332], loss=73.9108
	step [275/332], loss=66.6438
	step [276/332], loss=69.8116
	step [277/332], loss=56.8421
	step [278/332], loss=67.6680
	step [279/332], loss=58.8219
	step [280/332], loss=75.9935
	step [281/332], loss=57.9337
	step [282/332], loss=61.1734
	step [283/332], loss=76.0631
	step [284/332], loss=58.6485
	step [285/332], loss=67.7191
	step [286/332], loss=49.3609
	step [287/332], loss=51.9711
	step [288/332], loss=70.8267
	step [289/332], loss=68.5732
	step [290/332], loss=67.1513
	step [291/332], loss=74.1262
	step [292/332], loss=51.7245
	step [293/332], loss=64.2460
	step [294/332], loss=59.8965
	step [295/332], loss=68.4165
	step [296/332], loss=66.5085
	step [297/332], loss=62.5874
	step [298/332], loss=88.7082
	step [299/332], loss=74.0820
	step [300/332], loss=67.3996
	step [301/332], loss=57.0179
	step [302/332], loss=64.7721
	step [303/332], loss=63.8772
	step [304/332], loss=72.8958
	step [305/332], loss=66.2257
	step [306/332], loss=64.9231
	step [307/332], loss=76.8775
	step [308/332], loss=69.4519
	step [309/332], loss=72.0179
	step [310/332], loss=70.1588
	step [311/332], loss=81.1608
	step [312/332], loss=68.8239
	step [313/332], loss=75.6530
	step [314/332], loss=64.4062
	step [315/332], loss=55.3412
	step [316/332], loss=73.5475
	step [317/332], loss=67.7648
	step [318/332], loss=53.5137
	step [319/332], loss=62.1441
	step [320/332], loss=75.9850
	step [321/332], loss=72.8646
	step [322/332], loss=57.0512
	step [323/332], loss=54.0471
	step [324/332], loss=71.2688
	step [325/332], loss=75.1518
	step [326/332], loss=65.9879
	step [327/332], loss=60.3648
	step [328/332], loss=70.1653
	step [329/332], loss=57.3543
	step [330/332], loss=62.9881
	step [331/332], loss=65.5049
	step [332/332], loss=36.3835
	Evaluating
	loss=0.0074, precision=0.3726, recall=0.9092, f1=0.5286
Training epoch 24
	step [1/332], loss=57.6544
	step [2/332], loss=61.5906
	step [3/332], loss=67.8601
	step [4/332], loss=65.5878
	step [5/332], loss=64.5146
	step [6/332], loss=72.8431
	step [7/332], loss=65.0567
	step [8/332], loss=63.2321
	step [9/332], loss=59.8696
	step [10/332], loss=61.5512
	step [11/332], loss=71.5255
	step [12/332], loss=69.6818
	step [13/332], loss=56.4865
	step [14/332], loss=57.7380
	step [15/332], loss=55.6660
	step [16/332], loss=83.3739
	step [17/332], loss=67.1334
	step [18/332], loss=61.0082
	step [19/332], loss=55.6778
	step [20/332], loss=75.2639
	step [21/332], loss=51.7533
	step [22/332], loss=65.6430
	step [23/332], loss=57.6046
	step [24/332], loss=75.2518
	step [25/332], loss=71.3353
	step [26/332], loss=70.8791
	step [27/332], loss=77.3430
	step [28/332], loss=59.9856
	step [29/332], loss=64.0186
	step [30/332], loss=72.3582
	step [31/332], loss=61.8005
	step [32/332], loss=63.6292
	step [33/332], loss=58.6982
	step [34/332], loss=59.2294
	step [35/332], loss=54.0658
	step [36/332], loss=48.6084
	step [37/332], loss=57.2788
	step [38/332], loss=62.3479
	step [39/332], loss=53.2422
	step [40/332], loss=70.7390
	step [41/332], loss=65.0320
	step [42/332], loss=82.5463
	step [43/332], loss=75.4849
	step [44/332], loss=56.0685
	step [45/332], loss=87.6800
	step [46/332], loss=52.4303
	step [47/332], loss=64.3552
	step [48/332], loss=61.5736
	step [49/332], loss=74.2793
	step [50/332], loss=73.8280
	step [51/332], loss=57.9768
	step [52/332], loss=74.5530
	step [53/332], loss=62.2571
	step [54/332], loss=72.6856
	step [55/332], loss=56.1366
	step [56/332], loss=85.0872
	step [57/332], loss=74.7530
	step [58/332], loss=76.7330
	step [59/332], loss=71.4686
	step [60/332], loss=66.0708
	step [61/332], loss=57.7628
	step [62/332], loss=69.1709
	step [63/332], loss=70.9021
	step [64/332], loss=69.8788
	step [65/332], loss=63.0151
	step [66/332], loss=58.6994
	step [67/332], loss=77.9666
	step [68/332], loss=60.0987
	step [69/332], loss=76.0579
	step [70/332], loss=60.9893
	step [71/332], loss=72.7177
	step [72/332], loss=70.7184
	step [73/332], loss=60.1710
	step [74/332], loss=54.9664
	step [75/332], loss=51.5053
	step [76/332], loss=61.5392
	step [77/332], loss=71.5563
	step [78/332], loss=64.1251
	step [79/332], loss=71.5547
	step [80/332], loss=57.9557
	step [81/332], loss=65.2004
	step [82/332], loss=56.1258
	step [83/332], loss=63.4233
	step [84/332], loss=77.3113
	step [85/332], loss=66.8631
	step [86/332], loss=61.7958
	step [87/332], loss=83.7157
	step [88/332], loss=66.5354
	step [89/332], loss=75.3981
	step [90/332], loss=66.5213
	step [91/332], loss=70.7170
	step [92/332], loss=75.5344
	step [93/332], loss=60.6787
	step [94/332], loss=61.1969
	step [95/332], loss=65.6682
	step [96/332], loss=67.0488
	step [97/332], loss=82.8189
	step [98/332], loss=67.0459
	step [99/332], loss=78.5110
	step [100/332], loss=92.3737
	step [101/332], loss=62.4583
	step [102/332], loss=64.0265
	step [103/332], loss=67.3227
	step [104/332], loss=65.6148
	step [105/332], loss=67.3409
	step [106/332], loss=68.5974
	step [107/332], loss=56.8370
	step [108/332], loss=80.3493
	step [109/332], loss=69.5663
	step [110/332], loss=69.0611
	step [111/332], loss=63.1674
	step [112/332], loss=69.6486
	step [113/332], loss=68.4858
	step [114/332], loss=62.6612
	step [115/332], loss=66.3492
	step [116/332], loss=61.0407
	step [117/332], loss=70.5413
	step [118/332], loss=53.8262
	step [119/332], loss=62.2344
	step [120/332], loss=68.9421
	step [121/332], loss=81.8638
	step [122/332], loss=57.6804
	step [123/332], loss=60.9412
	step [124/332], loss=63.7899
	step [125/332], loss=65.3208
	step [126/332], loss=81.1881
	step [127/332], loss=64.2051
	step [128/332], loss=50.1208
	step [129/332], loss=66.9038
	step [130/332], loss=78.3783
	step [131/332], loss=59.0955
	step [132/332], loss=83.3780
	step [133/332], loss=56.6497
	step [134/332], loss=64.3132
	step [135/332], loss=72.0272
	step [136/332], loss=55.6761
	step [137/332], loss=75.1511
	step [138/332], loss=68.5359
	step [139/332], loss=73.6535
	step [140/332], loss=57.2472
	step [141/332], loss=53.8192
	step [142/332], loss=50.3583
	step [143/332], loss=75.2586
	step [144/332], loss=59.8957
	step [145/332], loss=61.6256
	step [146/332], loss=74.3420
	step [147/332], loss=71.2740
	step [148/332], loss=78.1032
	step [149/332], loss=49.5225
	step [150/332], loss=71.2818
	step [151/332], loss=65.1413
	step [152/332], loss=72.4200
	step [153/332], loss=58.7231
	step [154/332], loss=70.9084
	step [155/332], loss=55.7260
	step [156/332], loss=72.5530
	step [157/332], loss=54.0874
	step [158/332], loss=61.6630
	step [159/332], loss=60.2959
	step [160/332], loss=71.7235
	step [161/332], loss=51.8549
	step [162/332], loss=58.5799
	step [163/332], loss=63.1669
	step [164/332], loss=63.7186
	step [165/332], loss=67.1571
	step [166/332], loss=55.4736
	step [167/332], loss=68.7299
	step [168/332], loss=47.8869
	step [169/332], loss=65.2222
	step [170/332], loss=49.4678
	step [171/332], loss=78.3003
	step [172/332], loss=61.7631
	step [173/332], loss=60.4180
	step [174/332], loss=72.9748
	step [175/332], loss=88.7758
	step [176/332], loss=73.9895
	step [177/332], loss=72.5506
	step [178/332], loss=63.1731
	step [179/332], loss=64.0830
	step [180/332], loss=83.3787
	step [181/332], loss=77.2710
	step [182/332], loss=71.7184
	step [183/332], loss=73.3947
	step [184/332], loss=54.5322
	step [185/332], loss=62.1040
	step [186/332], loss=67.7779
	step [187/332], loss=77.0644
	step [188/332], loss=60.0392
	step [189/332], loss=66.3243
	step [190/332], loss=51.6720
	step [191/332], loss=63.6768
	step [192/332], loss=71.2081
	step [193/332], loss=66.6185
	step [194/332], loss=63.3670
	step [195/332], loss=72.1815
	step [196/332], loss=55.1851
	step [197/332], loss=62.8510
	step [198/332], loss=56.2769
	step [199/332], loss=70.5111
	step [200/332], loss=61.3187
	step [201/332], loss=58.3940
	step [202/332], loss=68.4750
	step [203/332], loss=81.6761
	step [204/332], loss=62.8824
	step [205/332], loss=77.1691
	step [206/332], loss=67.8487
	step [207/332], loss=74.3166
	step [208/332], loss=62.5670
	step [209/332], loss=51.1328
	step [210/332], loss=57.3006
	step [211/332], loss=73.8691
	step [212/332], loss=53.2777
	step [213/332], loss=70.0249
	step [214/332], loss=61.7303
	step [215/332], loss=61.0771
	step [216/332], loss=63.0904
	step [217/332], loss=58.5975
	step [218/332], loss=70.2501
	step [219/332], loss=61.3523
	step [220/332], loss=66.5326
	step [221/332], loss=60.5578
	step [222/332], loss=55.8945
	step [223/332], loss=68.1332
	step [224/332], loss=76.9826
	step [225/332], loss=60.8182
	step [226/332], loss=71.9572
	step [227/332], loss=72.8735
	step [228/332], loss=78.8666
	step [229/332], loss=73.0255
	step [230/332], loss=62.2237
	step [231/332], loss=64.8934
	step [232/332], loss=69.0584
	step [233/332], loss=74.1201
	step [234/332], loss=63.4594
	step [235/332], loss=65.3963
	step [236/332], loss=72.8738
	step [237/332], loss=67.5064
	step [238/332], loss=69.0933
	step [239/332], loss=76.3125
	step [240/332], loss=74.2783
	step [241/332], loss=65.9299
	step [242/332], loss=57.9070
	step [243/332], loss=63.8160
	step [244/332], loss=62.8749
	step [245/332], loss=67.4480
	step [246/332], loss=62.0589
	step [247/332], loss=63.0346
	step [248/332], loss=60.9794
	step [249/332], loss=61.5690
	step [250/332], loss=60.5262
	step [251/332], loss=57.0536
	step [252/332], loss=49.6464
	step [253/332], loss=60.7382
	step [254/332], loss=63.4537
	step [255/332], loss=58.7457
	step [256/332], loss=67.7256
	step [257/332], loss=62.4242
	step [258/332], loss=78.0924
	step [259/332], loss=62.3372
	step [260/332], loss=89.1035
	step [261/332], loss=56.6642
	step [262/332], loss=65.7151
	step [263/332], loss=70.6443
	step [264/332], loss=54.6712
	step [265/332], loss=66.1817
	step [266/332], loss=77.3172
	step [267/332], loss=68.5642
	step [268/332], loss=53.0946
	step [269/332], loss=71.0408
	step [270/332], loss=64.6179
	step [271/332], loss=59.8593
	step [272/332], loss=67.9229
	step [273/332], loss=70.9037
	step [274/332], loss=61.1250
	step [275/332], loss=44.5938
	step [276/332], loss=58.5185
	step [277/332], loss=55.5421
	step [278/332], loss=73.5424
	step [279/332], loss=60.7069
	step [280/332], loss=60.8497
	step [281/332], loss=75.3588
	step [282/332], loss=53.0601
	step [283/332], loss=83.6570
	step [284/332], loss=69.5919
	step [285/332], loss=68.9105
	step [286/332], loss=58.4356
	step [287/332], loss=61.3585
	step [288/332], loss=60.7118
	step [289/332], loss=61.0013
	step [290/332], loss=53.5393
	step [291/332], loss=55.9934
	step [292/332], loss=66.8679
	step [293/332], loss=64.3971
	step [294/332], loss=55.1460
	step [295/332], loss=66.4193
	step [296/332], loss=65.4732
	step [297/332], loss=70.0342
	step [298/332], loss=50.7501
	step [299/332], loss=73.4533
	step [300/332], loss=69.0858
	step [301/332], loss=65.5334
	step [302/332], loss=54.9805
	step [303/332], loss=76.9892
	step [304/332], loss=68.8492
	step [305/332], loss=75.6800
	step [306/332], loss=62.7513
	step [307/332], loss=62.6898
	step [308/332], loss=53.6242
	step [309/332], loss=53.9761
	step [310/332], loss=59.0122
	step [311/332], loss=59.2328
	step [312/332], loss=70.8467
	step [313/332], loss=84.2659
	step [314/332], loss=47.4662
	step [315/332], loss=63.5049
	step [316/332], loss=64.6090
	step [317/332], loss=55.9340
	step [318/332], loss=64.8045
	step [319/332], loss=59.7383
	step [320/332], loss=53.2321
	step [321/332], loss=60.5393
	step [322/332], loss=62.8350
	step [323/332], loss=51.2958
	step [324/332], loss=72.7262
	step [325/332], loss=58.0962
	step [326/332], loss=66.8070
	step [327/332], loss=71.9511
	step [328/332], loss=61.4018
	step [329/332], loss=59.1095
	step [330/332], loss=64.2834
	step [331/332], loss=71.9881
	step [332/332], loss=33.1311
	Evaluating
	loss=0.0069, precision=0.3934, recall=0.9200, f1=0.5512
Training epoch 25
	step [1/332], loss=75.8604
	step [2/332], loss=68.6922
	step [3/332], loss=78.4293
	step [4/332], loss=76.1448
	step [5/332], loss=58.7364
	step [6/332], loss=78.3447
	step [7/332], loss=73.6749
	step [8/332], loss=52.1170
	step [9/332], loss=74.3147
	step [10/332], loss=60.8517
	step [11/332], loss=61.6657
	step [12/332], loss=53.6406
	step [13/332], loss=69.1222
	step [14/332], loss=66.7008
	step [15/332], loss=61.8851
	step [16/332], loss=69.5380
	step [17/332], loss=65.3973
	step [18/332], loss=67.1395
	step [19/332], loss=53.9986
	step [20/332], loss=69.4581
	step [21/332], loss=64.8004
	step [22/332], loss=56.7261
	step [23/332], loss=82.6546
	step [24/332], loss=61.1768
	step [25/332], loss=66.3915
	step [26/332], loss=55.3078
	step [27/332], loss=58.8277
	step [28/332], loss=70.6233
	step [29/332], loss=78.2518
	step [30/332], loss=59.0766
	step [31/332], loss=41.8471
	step [32/332], loss=63.4809
	step [33/332], loss=56.1414
	step [34/332], loss=64.2740
	step [35/332], loss=79.5486
	step [36/332], loss=54.9314
	step [37/332], loss=51.8879
	step [38/332], loss=75.6294
	step [39/332], loss=63.5730
	step [40/332], loss=53.2278
	step [41/332], loss=71.8718
	step [42/332], loss=69.3314
	step [43/332], loss=58.6237
	step [44/332], loss=62.4101
	step [45/332], loss=66.2119
	step [46/332], loss=63.4310
	step [47/332], loss=61.8259
	step [48/332], loss=73.2857
	step [49/332], loss=66.6143
	step [50/332], loss=58.1511
	step [51/332], loss=54.8324
	step [52/332], loss=61.4513
	step [53/332], loss=74.9387
	step [54/332], loss=57.1059
	step [55/332], loss=58.7840
	step [56/332], loss=65.9027
	step [57/332], loss=52.3822
	step [58/332], loss=62.4049
	step [59/332], loss=60.0875
	step [60/332], loss=55.7952
	step [61/332], loss=62.7901
	step [62/332], loss=64.1698
	step [63/332], loss=57.5950
	step [64/332], loss=87.0699
	step [65/332], loss=66.6161
	step [66/332], loss=71.5871
	step [67/332], loss=54.6571
	step [68/332], loss=64.3357
	step [69/332], loss=67.8318
	step [70/332], loss=70.4496
	step [71/332], loss=48.9742
	step [72/332], loss=70.2484
	step [73/332], loss=81.1964
	step [74/332], loss=88.2585
	step [75/332], loss=63.4783
	step [76/332], loss=67.2179
	step [77/332], loss=60.7373
	step [78/332], loss=55.3471
	step [79/332], loss=72.0966
	step [80/332], loss=46.6485
	step [81/332], loss=62.5529
	step [82/332], loss=55.5157
	step [83/332], loss=57.5898
	step [84/332], loss=51.9052
	step [85/332], loss=64.7692
	step [86/332], loss=66.9168
	step [87/332], loss=61.5764
	step [88/332], loss=58.7754
	step [89/332], loss=69.1042
	step [90/332], loss=69.0078
	step [91/332], loss=70.6209
	step [92/332], loss=68.5936
	step [93/332], loss=66.1761
	step [94/332], loss=51.6710
	step [95/332], loss=54.6909
	step [96/332], loss=67.9984
	step [97/332], loss=56.3114
	step [98/332], loss=51.4455
	step [99/332], loss=48.5646
	step [100/332], loss=68.3852
	step [101/332], loss=72.0842
	step [102/332], loss=63.9591
	step [103/332], loss=76.1531
	step [104/332], loss=63.1585
	step [105/332], loss=58.8391
	step [106/332], loss=66.2647
	step [107/332], loss=66.7521
	step [108/332], loss=72.9533
	step [109/332], loss=74.5455
	step [110/332], loss=63.5631
	step [111/332], loss=51.5796
	step [112/332], loss=67.2970
	step [113/332], loss=71.0917
	step [114/332], loss=68.9240
	step [115/332], loss=67.6247
	step [116/332], loss=70.6064
	step [117/332], loss=63.2740
	step [118/332], loss=64.8351
	step [119/332], loss=56.6313
	step [120/332], loss=55.3874
	step [121/332], loss=69.8527
	step [122/332], loss=59.2456
	step [123/332], loss=57.8004
	step [124/332], loss=63.7930
	step [125/332], loss=66.1596
	step [126/332], loss=78.6312
	step [127/332], loss=61.6206
	step [128/332], loss=49.4644
	step [129/332], loss=61.0905
	step [130/332], loss=68.5308
	step [131/332], loss=70.2535
	step [132/332], loss=65.3017
	step [133/332], loss=55.9659
	step [134/332], loss=60.3236
	step [135/332], loss=51.8408
	step [136/332], loss=68.0118
	step [137/332], loss=70.2181
	step [138/332], loss=58.2852
	step [139/332], loss=62.4013
	step [140/332], loss=80.6684
	step [141/332], loss=67.3337
	step [142/332], loss=56.0491
	step [143/332], loss=68.2541
	step [144/332], loss=78.3821
	step [145/332], loss=60.7402
	step [146/332], loss=56.2799
	step [147/332], loss=74.4156
	step [148/332], loss=64.3638
	step [149/332], loss=54.9285
	step [150/332], loss=69.7444
	step [151/332], loss=60.6233
	step [152/332], loss=63.9121
	step [153/332], loss=48.9372
	step [154/332], loss=63.3087
	step [155/332], loss=72.8261
	step [156/332], loss=79.9998
	step [157/332], loss=53.3640
	step [158/332], loss=74.1248
	step [159/332], loss=66.6871
	step [160/332], loss=64.7442
	step [161/332], loss=62.8105
	step [162/332], loss=62.4397
	step [163/332], loss=66.2749
	step [164/332], loss=64.9311
	step [165/332], loss=54.3548
	step [166/332], loss=68.4806
	step [167/332], loss=64.4465
	step [168/332], loss=75.0087
	step [169/332], loss=72.0276
	step [170/332], loss=71.4594
	step [171/332], loss=60.5227
	step [172/332], loss=76.2204
	step [173/332], loss=79.9261
	step [174/332], loss=56.4153
	step [175/332], loss=81.9955
	step [176/332], loss=72.9262
	step [177/332], loss=68.6370
	step [178/332], loss=73.0326
	step [179/332], loss=80.3179
	step [180/332], loss=67.6193
	step [181/332], loss=49.3082
	step [182/332], loss=62.1783
	step [183/332], loss=61.9853
	step [184/332], loss=66.2922
	step [185/332], loss=49.1335
	step [186/332], loss=67.2396
	step [187/332], loss=66.7909
	step [188/332], loss=54.3170
	step [189/332], loss=73.6217
	step [190/332], loss=60.4859
	step [191/332], loss=56.9822
	step [192/332], loss=72.6698
	step [193/332], loss=64.2805
	step [194/332], loss=64.1250
	step [195/332], loss=61.8091
	step [196/332], loss=74.9010
	step [197/332], loss=73.2726
	step [198/332], loss=59.9325
	step [199/332], loss=73.3691
	step [200/332], loss=71.7973
	step [201/332], loss=58.6132
	step [202/332], loss=61.2554
	step [203/332], loss=61.1488
	step [204/332], loss=74.9353
	step [205/332], loss=66.7852
	step [206/332], loss=77.2320
	step [207/332], loss=66.3600
	step [208/332], loss=59.0960
	step [209/332], loss=75.6584
	step [210/332], loss=72.8037
	step [211/332], loss=77.7980
	step [212/332], loss=63.4069
	step [213/332], loss=64.8766
	step [214/332], loss=66.9986
	step [215/332], loss=51.1853
	step [216/332], loss=72.7020
	step [217/332], loss=73.8656
	step [218/332], loss=60.6503
	step [219/332], loss=53.0339
	step [220/332], loss=74.2689
	step [221/332], loss=66.3066
	step [222/332], loss=58.9548
	step [223/332], loss=61.2944
	step [224/332], loss=65.2389
	step [225/332], loss=55.4488
	step [226/332], loss=65.5921
	step [227/332], loss=70.6256
	step [228/332], loss=76.0946
	step [229/332], loss=52.6915
	step [230/332], loss=70.8296
	step [231/332], loss=62.9676
	step [232/332], loss=54.7249
	step [233/332], loss=62.5904
	step [234/332], loss=64.5615
	step [235/332], loss=54.4732
	step [236/332], loss=68.9879
	step [237/332], loss=66.4097
	step [238/332], loss=56.7830
	step [239/332], loss=62.6559
	step [240/332], loss=63.7077
	step [241/332], loss=61.6512
	step [242/332], loss=78.5201
	step [243/332], loss=61.6186
	step [244/332], loss=59.4936
	step [245/332], loss=74.6338
	step [246/332], loss=62.5147
	step [247/332], loss=56.5158
	step [248/332], loss=67.4646
	step [249/332], loss=65.1805
	step [250/332], loss=55.1041
	step [251/332], loss=60.7989
	step [252/332], loss=83.4918
	step [253/332], loss=57.5607
	step [254/332], loss=70.3357
	step [255/332], loss=71.5437
	step [256/332], loss=53.1918
	step [257/332], loss=71.0946
	step [258/332], loss=55.5143
	step [259/332], loss=54.2318
	step [260/332], loss=64.9661
	step [261/332], loss=68.0198
	step [262/332], loss=71.9377
	step [263/332], loss=57.6451
	step [264/332], loss=64.1088
	step [265/332], loss=64.1143
	step [266/332], loss=53.6605
	step [267/332], loss=63.4878
	step [268/332], loss=80.5360
	step [269/332], loss=72.9321
	step [270/332], loss=79.5356
	step [271/332], loss=58.1452
	step [272/332], loss=60.9706
	step [273/332], loss=68.7814
	step [274/332], loss=66.3021
	step [275/332], loss=65.1078
	step [276/332], loss=68.5333
	step [277/332], loss=61.3559
	step [278/332], loss=55.2123
	step [279/332], loss=51.9966
	step [280/332], loss=56.6674
	step [281/332], loss=63.6343
	step [282/332], loss=49.6113
	step [283/332], loss=60.5020
	step [284/332], loss=58.1065
	step [285/332], loss=70.8812
	step [286/332], loss=71.5345
	step [287/332], loss=65.8736
	step [288/332], loss=53.1160
	step [289/332], loss=67.7270
	step [290/332], loss=59.8700
	step [291/332], loss=57.9817
	step [292/332], loss=46.1344
	step [293/332], loss=71.2907
	step [294/332], loss=64.5468
	step [295/332], loss=65.2658
	step [296/332], loss=64.5523
	step [297/332], loss=59.3109
	step [298/332], loss=57.4863
	step [299/332], loss=76.5359
	step [300/332], loss=66.4758
	step [301/332], loss=63.9051
	step [302/332], loss=88.5253
	step [303/332], loss=59.0233
	step [304/332], loss=65.8508
	step [305/332], loss=72.2044
	step [306/332], loss=54.1256
	step [307/332], loss=66.8584
	step [308/332], loss=69.2025
	step [309/332], loss=75.2866
	step [310/332], loss=60.9008
	step [311/332], loss=47.3255
	step [312/332], loss=66.3417
	step [313/332], loss=70.7083
	step [314/332], loss=48.9963
	step [315/332], loss=63.3185
	step [316/332], loss=66.9175
	step [317/332], loss=68.0974
	step [318/332], loss=62.9426
	step [319/332], loss=77.1139
	step [320/332], loss=72.9771
	step [321/332], loss=59.5663
	step [322/332], loss=56.8161
	step [323/332], loss=67.6007
	step [324/332], loss=61.4485
	step [325/332], loss=64.5744
	step [326/332], loss=66.1493
	step [327/332], loss=62.1254
	step [328/332], loss=66.0536
	step [329/332], loss=58.1966
	step [330/332], loss=79.6635
	step [331/332], loss=72.5562
	step [332/332], loss=33.5098
	Evaluating
	loss=0.0063, precision=0.4118, recall=0.8958, f1=0.5642
Training epoch 26
	step [1/332], loss=62.8890
	step [2/332], loss=71.1227
	step [3/332], loss=60.2329
	step [4/332], loss=69.8604
	step [5/332], loss=62.7248
	step [6/332], loss=77.3884
	step [7/332], loss=52.4219
	step [8/332], loss=70.7206
	step [9/332], loss=70.2552
	step [10/332], loss=68.7554
	step [11/332], loss=65.7047
	step [12/332], loss=75.1981
	step [13/332], loss=61.7651
	step [14/332], loss=95.7547
	step [15/332], loss=68.2081
	step [16/332], loss=65.5467
	step [17/332], loss=81.3024
	step [18/332], loss=65.2131
	step [19/332], loss=53.6555
	step [20/332], loss=62.2336
	step [21/332], loss=60.7613
	step [22/332], loss=62.5347
	step [23/332], loss=57.4564
	step [24/332], loss=50.8298
	step [25/332], loss=54.5440
	step [26/332], loss=55.2118
	step [27/332], loss=68.2030
	step [28/332], loss=63.5974
	step [29/332], loss=58.7351
	step [30/332], loss=69.5149
	step [31/332], loss=69.1726
	step [32/332], loss=53.2534
	step [33/332], loss=64.2553
	step [34/332], loss=39.3385
	step [35/332], loss=70.2566
	step [36/332], loss=77.3576
	step [37/332], loss=83.6498
	step [38/332], loss=47.0045
	step [39/332], loss=66.1577
	step [40/332], loss=74.1371
	step [41/332], loss=59.0077
	step [42/332], loss=74.3668
	step [43/332], loss=59.9733
	step [44/332], loss=66.2386
	step [45/332], loss=60.1188
	step [46/332], loss=52.9162
	step [47/332], loss=68.7169
	step [48/332], loss=72.9425
	step [49/332], loss=55.8497
	step [50/332], loss=58.0720
	step [51/332], loss=52.5272
	step [52/332], loss=71.8967
	step [53/332], loss=58.5124
	step [54/332], loss=68.5819
	step [55/332], loss=57.1788
	step [56/332], loss=83.6330
	step [57/332], loss=55.3275
	step [58/332], loss=58.1698
	step [59/332], loss=58.4353
	step [60/332], loss=68.4138
	step [61/332], loss=61.2960
	step [62/332], loss=61.3941
	step [63/332], loss=60.0535
	step [64/332], loss=60.8035
	step [65/332], loss=58.1616
	step [66/332], loss=57.9099
	step [67/332], loss=59.7569
	step [68/332], loss=68.5416
	step [69/332], loss=67.2348
	step [70/332], loss=63.8694
	step [71/332], loss=57.4655
	step [72/332], loss=71.5979
	step [73/332], loss=52.4061
	step [74/332], loss=45.4737
	step [75/332], loss=54.7172
	step [76/332], loss=78.8853
	step [77/332], loss=52.2458
	step [78/332], loss=63.8355
	step [79/332], loss=64.3653
	step [80/332], loss=61.0544
	step [81/332], loss=70.3058
	step [82/332], loss=78.2062
	step [83/332], loss=58.2505
	step [84/332], loss=67.4774
	step [85/332], loss=57.2894
	step [86/332], loss=69.5567
	step [87/332], loss=69.2100
	step [88/332], loss=51.8265
	step [89/332], loss=45.5708
	step [90/332], loss=60.5804
	step [91/332], loss=57.3226
	step [92/332], loss=76.9632
	step [93/332], loss=60.0960
	step [94/332], loss=44.5666
	step [95/332], loss=60.9372
	step [96/332], loss=62.9884
	step [97/332], loss=59.3453
	step [98/332], loss=70.1187
	step [99/332], loss=61.0504
	step [100/332], loss=61.5346
	step [101/332], loss=70.1288
	step [102/332], loss=74.3358
	step [103/332], loss=70.2924
	step [104/332], loss=58.4400
	step [105/332], loss=66.9053
	step [106/332], loss=79.3047
	step [107/332], loss=61.4378
	step [108/332], loss=58.5595
	step [109/332], loss=77.7183
	step [110/332], loss=56.0420
	step [111/332], loss=67.3079
	step [112/332], loss=62.2454
	step [113/332], loss=66.5381
	step [114/332], loss=67.5757
	step [115/332], loss=67.3184
	step [116/332], loss=54.4768
	step [117/332], loss=65.5352
	step [118/332], loss=71.5420
	step [119/332], loss=54.5530
	step [120/332], loss=55.0255
	step [121/332], loss=85.3999
	step [122/332], loss=69.9879
	step [123/332], loss=73.7168
	step [124/332], loss=66.1901
	step [125/332], loss=67.3534
	step [126/332], loss=61.9610
	step [127/332], loss=57.3877
	step [128/332], loss=64.0740
	step [129/332], loss=73.8801
	step [130/332], loss=71.4609
	step [131/332], loss=69.2831
	step [132/332], loss=60.8238
	step [133/332], loss=70.4307
	step [134/332], loss=56.1132
	step [135/332], loss=76.1729
	step [136/332], loss=74.2264
	step [137/332], loss=65.9376
	step [138/332], loss=74.0992
	step [139/332], loss=66.1524
	step [140/332], loss=69.6250
	step [141/332], loss=58.4061
	step [142/332], loss=58.4389
	step [143/332], loss=59.0993
	step [144/332], loss=66.0392
	step [145/332], loss=60.9310
	step [146/332], loss=55.3940
	step [147/332], loss=65.1504
	step [148/332], loss=61.3826
	step [149/332], loss=80.7353
	step [150/332], loss=54.5560
	step [151/332], loss=68.4374
	step [152/332], loss=58.0088
	step [153/332], loss=62.5486
	step [154/332], loss=73.4797
	step [155/332], loss=51.6846
	step [156/332], loss=66.3713
	step [157/332], loss=71.2194
	step [158/332], loss=60.0040
	step [159/332], loss=69.0478
	step [160/332], loss=66.7372
	step [161/332], loss=51.7137
	step [162/332], loss=62.3577
	step [163/332], loss=62.7845
	step [164/332], loss=61.4950
	step [165/332], loss=54.8093
	step [166/332], loss=57.9705
	step [167/332], loss=71.6238
	step [168/332], loss=84.6239
	step [169/332], loss=45.3082
	step [170/332], loss=69.8228
	step [171/332], loss=52.3039
	step [172/332], loss=65.2286
	step [173/332], loss=72.5804
	step [174/332], loss=69.3657
	step [175/332], loss=67.8866
	step [176/332], loss=63.5245
	step [177/332], loss=73.5547
	step [178/332], loss=43.0411
	step [179/332], loss=65.2979
	step [180/332], loss=69.3113
	step [181/332], loss=55.2862
	step [182/332], loss=62.7555
	step [183/332], loss=65.2140
	step [184/332], loss=76.5602
	step [185/332], loss=40.1032
	step [186/332], loss=67.9250
	step [187/332], loss=55.4322
	step [188/332], loss=67.4827
	step [189/332], loss=58.5121
	step [190/332], loss=58.9072
	step [191/332], loss=54.0815
	step [192/332], loss=47.8388
	step [193/332], loss=61.4850
	step [194/332], loss=64.1880
	step [195/332], loss=63.2417
	step [196/332], loss=70.1264
	step [197/332], loss=80.1059
	step [198/332], loss=69.5322
	step [199/332], loss=71.0667
	step [200/332], loss=75.6783
	step [201/332], loss=54.7437
	step [202/332], loss=65.7487
	step [203/332], loss=48.5824
	step [204/332], loss=52.7089
	step [205/332], loss=57.1275
	step [206/332], loss=63.2917
	step [207/332], loss=66.0443
	step [208/332], loss=68.9209
	step [209/332], loss=64.6945
	step [210/332], loss=54.7906
	step [211/332], loss=63.0778
	step [212/332], loss=71.3471
	step [213/332], loss=54.8279
	step [214/332], loss=56.8505
	step [215/332], loss=73.3532
	step [216/332], loss=64.4300
	step [217/332], loss=68.4398
	step [218/332], loss=46.7359
	step [219/332], loss=54.4464
	step [220/332], loss=53.1633
	step [221/332], loss=79.5086
	step [222/332], loss=61.8824
	step [223/332], loss=72.7706
	step [224/332], loss=57.2011
	step [225/332], loss=69.0380
	step [226/332], loss=55.3036
	step [227/332], loss=58.1057
	step [228/332], loss=68.9688
	step [229/332], loss=58.2141
	step [230/332], loss=70.7997
	step [231/332], loss=57.6955
	step [232/332], loss=62.5922
	step [233/332], loss=53.9151
	step [234/332], loss=57.7887
	step [235/332], loss=55.3464
	step [236/332], loss=60.0846
	step [237/332], loss=51.9171
	step [238/332], loss=55.5935
	step [239/332], loss=64.1448
	step [240/332], loss=68.9361
	step [241/332], loss=68.7895
	step [242/332], loss=52.7605
	step [243/332], loss=71.7150
	step [244/332], loss=69.8043
	step [245/332], loss=52.9837
	step [246/332], loss=69.3015
	step [247/332], loss=61.4518
	step [248/332], loss=56.0124
	step [249/332], loss=60.2299
	step [250/332], loss=69.8377
	step [251/332], loss=51.8897
	step [252/332], loss=57.7642
	step [253/332], loss=59.8353
	step [254/332], loss=53.7049
	step [255/332], loss=66.0077
	step [256/332], loss=60.1255
	step [257/332], loss=69.2479
	step [258/332], loss=56.8709
	step [259/332], loss=78.3923
	step [260/332], loss=61.2062
	step [261/332], loss=74.5096
	step [262/332], loss=70.1155
	step [263/332], loss=63.3516
	step [264/332], loss=67.7647
	step [265/332], loss=71.8318
	step [266/332], loss=70.7467
	step [267/332], loss=61.1915
	step [268/332], loss=64.7526
	step [269/332], loss=51.8278
	step [270/332], loss=56.0294
	step [271/332], loss=62.9141
	step [272/332], loss=66.7536
	step [273/332], loss=91.4759
	step [274/332], loss=58.7669
	step [275/332], loss=64.4837
	step [276/332], loss=73.5626
	step [277/332], loss=68.3926
	step [278/332], loss=63.6405
	step [279/332], loss=74.9419
	step [280/332], loss=61.6248
	step [281/332], loss=54.5844
	step [282/332], loss=66.5928
	step [283/332], loss=75.3141
	step [284/332], loss=79.2816
	step [285/332], loss=62.4193
	step [286/332], loss=58.4481
	step [287/332], loss=59.2319
	step [288/332], loss=53.1367
	step [289/332], loss=59.0740
	step [290/332], loss=60.4623
	step [291/332], loss=51.6245
	step [292/332], loss=53.5811
	step [293/332], loss=69.9507
	step [294/332], loss=55.8405
	step [295/332], loss=60.3442
	step [296/332], loss=57.3207
	step [297/332], loss=69.4202
	step [298/332], loss=68.9597
	step [299/332], loss=85.0272
	step [300/332], loss=72.5799
	step [301/332], loss=57.3315
	step [302/332], loss=67.6136
	step [303/332], loss=66.3672
	step [304/332], loss=59.1818
	step [305/332], loss=71.8785
	step [306/332], loss=64.7756
	step [307/332], loss=60.3940
	step [308/332], loss=77.5591
	step [309/332], loss=62.4295
	step [310/332], loss=70.9349
	step [311/332], loss=49.5810
	step [312/332], loss=54.1119
	step [313/332], loss=74.9502
	step [314/332], loss=64.1663
	step [315/332], loss=68.2950
	step [316/332], loss=69.8631
	step [317/332], loss=64.4253
	step [318/332], loss=67.4235
	step [319/332], loss=69.4897
	step [320/332], loss=62.4048
	step [321/332], loss=65.3246
	step [322/332], loss=74.8765
	step [323/332], loss=61.4045
	step [324/332], loss=59.7248
	step [325/332], loss=69.6827
	step [326/332], loss=62.2294
	step [327/332], loss=65.2181
	step [328/332], loss=70.8918
	step [329/332], loss=79.3942
	step [330/332], loss=71.1239
	step [331/332], loss=69.4249
	step [332/332], loss=34.4559
	Evaluating
	loss=0.0064, precision=0.3915, recall=0.9043, f1=0.5464
Training epoch 27
	step [1/332], loss=51.1410
	step [2/332], loss=63.1907
	step [3/332], loss=66.9719
	step [4/332], loss=48.0623
	step [5/332], loss=64.7813
	step [6/332], loss=66.6530
	step [7/332], loss=81.9032
	step [8/332], loss=64.7694
	step [9/332], loss=70.6023
	step [10/332], loss=73.8127
	step [11/332], loss=67.4995
	step [12/332], loss=69.5626
	step [13/332], loss=68.2758
	step [14/332], loss=70.5827
	step [15/332], loss=59.2546
	step [16/332], loss=63.0911
	step [17/332], loss=60.9889
	step [18/332], loss=50.5074
	step [19/332], loss=63.8129
	step [20/332], loss=73.7856
	step [21/332], loss=55.0861
	step [22/332], loss=58.6324
	step [23/332], loss=57.2829
	step [24/332], loss=60.5876
	step [25/332], loss=46.1583
	step [26/332], loss=75.1291
	step [27/332], loss=58.7437
	step [28/332], loss=53.2134
	step [29/332], loss=61.6454
	step [30/332], loss=56.4620
	step [31/332], loss=68.5650
	step [32/332], loss=64.1766
	step [33/332], loss=68.7414
	step [34/332], loss=50.9945
	step [35/332], loss=62.7151
	step [36/332], loss=57.0854
	step [37/332], loss=69.9361
	step [38/332], loss=64.0623
	step [39/332], loss=61.7703
	step [40/332], loss=59.1863
	step [41/332], loss=60.5022
	step [42/332], loss=69.8056
	step [43/332], loss=57.4396
	step [44/332], loss=49.3251
	step [45/332], loss=55.0380
	step [46/332], loss=62.8247
	step [47/332], loss=72.0405
	step [48/332], loss=67.0960
	step [49/332], loss=56.0734
	step [50/332], loss=61.4860
	step [51/332], loss=44.0453
	step [52/332], loss=54.0206
	step [53/332], loss=71.9263
	step [54/332], loss=65.3993
	step [55/332], loss=55.7730
	step [56/332], loss=83.3682
	step [57/332], loss=67.4842
	step [58/332], loss=67.0915
	step [59/332], loss=66.3333
	step [60/332], loss=79.7535
	step [61/332], loss=69.8427
	step [62/332], loss=50.5202
	step [63/332], loss=55.8323
	step [64/332], loss=56.9315
	step [65/332], loss=48.6058
	step [66/332], loss=54.7032
	step [67/332], loss=56.8216
	step [68/332], loss=58.3824
	step [69/332], loss=77.7859
	step [70/332], loss=69.2739
	step [71/332], loss=59.1794
	step [72/332], loss=64.4468
	step [73/332], loss=59.1899
	step [74/332], loss=52.4642
	step [75/332], loss=70.6154
	step [76/332], loss=62.4948
	step [77/332], loss=69.2511
	step [78/332], loss=55.5347
	step [79/332], loss=82.4922
	step [80/332], loss=60.6571
	step [81/332], loss=58.1433
	step [82/332], loss=61.8205
	step [83/332], loss=63.2852
	step [84/332], loss=57.5043
	step [85/332], loss=64.6559
	step [86/332], loss=71.7367
	step [87/332], loss=58.0531
	step [88/332], loss=56.0266
	step [89/332], loss=60.3829
	step [90/332], loss=68.4291
	step [91/332], loss=70.2976
	step [92/332], loss=79.7503
	step [93/332], loss=65.0967
	step [94/332], loss=59.8802
	step [95/332], loss=58.0230
	step [96/332], loss=63.7787
	step [97/332], loss=54.9083
	step [98/332], loss=50.4438
	step [99/332], loss=57.7114
	step [100/332], loss=53.0759
	step [101/332], loss=56.0660
	step [102/332], loss=55.9331
	step [103/332], loss=63.9216
	step [104/332], loss=72.5095
	step [105/332], loss=64.7034
	step [106/332], loss=64.2158
	step [107/332], loss=69.6337
	step [108/332], loss=56.7296
	step [109/332], loss=54.0252
	step [110/332], loss=57.0298
	step [111/332], loss=65.2356
	step [112/332], loss=60.4362
	step [113/332], loss=59.5498
	step [114/332], loss=67.7392
	step [115/332], loss=54.4681
	step [116/332], loss=66.8045
	step [117/332], loss=62.8405
	step [118/332], loss=67.4255
	step [119/332], loss=68.9571
	step [120/332], loss=48.9839
	step [121/332], loss=59.7830
	step [122/332], loss=60.6858
	step [123/332], loss=61.1989
	step [124/332], loss=75.2021
	step [125/332], loss=62.5415
	step [126/332], loss=59.9777
	step [127/332], loss=49.8500
	step [128/332], loss=57.7072
	step [129/332], loss=72.5737
	step [130/332], loss=60.5258
	step [131/332], loss=51.7032
	step [132/332], loss=64.6802
	step [133/332], loss=59.0034
	step [134/332], loss=49.1360
	step [135/332], loss=58.0219
	step [136/332], loss=52.6728
	step [137/332], loss=86.2149
	step [138/332], loss=61.7522
	step [139/332], loss=56.2268
	step [140/332], loss=69.1445
	step [141/332], loss=66.3812
	step [142/332], loss=79.0332
	step [143/332], loss=60.2161
	step [144/332], loss=60.2819
	step [145/332], loss=61.9758
	step [146/332], loss=68.2885
	step [147/332], loss=66.9542
	step [148/332], loss=79.2413
	step [149/332], loss=59.9877
	step [150/332], loss=58.1477
	step [151/332], loss=61.6748
	step [152/332], loss=67.6368
	step [153/332], loss=62.4221
	step [154/332], loss=48.8787
	step [155/332], loss=65.2317
	step [156/332], loss=68.6971
	step [157/332], loss=77.2220
	step [158/332], loss=67.9255
	step [159/332], loss=58.0399
	step [160/332], loss=44.5417
	step [161/332], loss=77.9573
	step [162/332], loss=69.4860
	step [163/332], loss=63.4160
	step [164/332], loss=73.6541
	step [165/332], loss=83.0693
	step [166/332], loss=61.1818
	step [167/332], loss=66.5520
	step [168/332], loss=56.5866
	step [169/332], loss=63.3736
	step [170/332], loss=60.7819
	step [171/332], loss=67.3357
	step [172/332], loss=86.1435
	step [173/332], loss=53.6336
	step [174/332], loss=65.5870
	step [175/332], loss=54.5427
	step [176/332], loss=61.0372
	step [177/332], loss=65.5835
	step [178/332], loss=50.0922
	step [179/332], loss=67.1524
	step [180/332], loss=56.9896
	step [181/332], loss=63.2886
	step [182/332], loss=57.5093
	step [183/332], loss=70.8301
	step [184/332], loss=60.0715
	step [185/332], loss=70.9363
	step [186/332], loss=45.2008
	step [187/332], loss=75.2722
	step [188/332], loss=80.0761
	step [189/332], loss=62.0367
	step [190/332], loss=55.8598
	step [191/332], loss=62.0065
	step [192/332], loss=67.9539
	step [193/332], loss=64.1040
	step [194/332], loss=65.4866
	step [195/332], loss=62.8835
	step [196/332], loss=59.6798
	step [197/332], loss=57.5123
	step [198/332], loss=74.1153
	step [199/332], loss=56.1030
	step [200/332], loss=68.4308
	step [201/332], loss=58.0212
	step [202/332], loss=75.3844
	step [203/332], loss=59.3043
	step [204/332], loss=69.5894
	step [205/332], loss=72.7027
	step [206/332], loss=70.6651
	step [207/332], loss=47.2826
	step [208/332], loss=52.4987
	step [209/332], loss=55.7171
	step [210/332], loss=57.4479
	step [211/332], loss=60.7460
	step [212/332], loss=54.6693
	step [213/332], loss=69.3694
	step [214/332], loss=68.6463
	step [215/332], loss=71.2454
	step [216/332], loss=73.9735
	step [217/332], loss=71.9139
	step [218/332], loss=85.2569
	step [219/332], loss=59.8768
	step [220/332], loss=64.5433
	step [221/332], loss=51.7670
	step [222/332], loss=76.7806
	step [223/332], loss=61.1802
	step [224/332], loss=61.0523
	step [225/332], loss=72.0379
	step [226/332], loss=65.9269
	step [227/332], loss=48.8374
	step [228/332], loss=62.3281
	step [229/332], loss=64.0548
	step [230/332], loss=64.0652
	step [231/332], loss=70.3048
	step [232/332], loss=72.2629
	step [233/332], loss=67.4315
	step [234/332], loss=73.1603
	step [235/332], loss=77.7516
	step [236/332], loss=67.0184
	step [237/332], loss=65.8733
	step [238/332], loss=76.5090
	step [239/332], loss=76.8156
	step [240/332], loss=42.2041
	step [241/332], loss=59.7044
	step [242/332], loss=86.8684
	step [243/332], loss=68.4446
	step [244/332], loss=55.2116
	step [245/332], loss=60.7544
	step [246/332], loss=63.4297
	step [247/332], loss=55.8408
	step [248/332], loss=55.0248
	step [249/332], loss=63.1564
	step [250/332], loss=55.6073
	step [251/332], loss=59.7165
	step [252/332], loss=55.2307
	step [253/332], loss=56.8121
	step [254/332], loss=63.3298
	step [255/332], loss=62.7797
	step [256/332], loss=56.0305
	step [257/332], loss=64.9934
	step [258/332], loss=55.4567
	step [259/332], loss=68.0133
	step [260/332], loss=62.0103
	step [261/332], loss=59.3599
	step [262/332], loss=70.9716
	step [263/332], loss=56.0985
	step [264/332], loss=56.0576
	step [265/332], loss=83.6227
	step [266/332], loss=65.4342
	step [267/332], loss=64.5185
	step [268/332], loss=57.7539
	step [269/332], loss=56.9847
	step [270/332], loss=61.2531
	step [271/332], loss=68.2188
	step [272/332], loss=57.7742
	step [273/332], loss=63.9064
	step [274/332], loss=54.0459
	step [275/332], loss=51.7030
	step [276/332], loss=55.4664
	step [277/332], loss=56.8887
	step [278/332], loss=62.6863
	step [279/332], loss=74.2949
	step [280/332], loss=56.8209
	step [281/332], loss=85.5467
	step [282/332], loss=70.4627
	step [283/332], loss=67.6627
	step [284/332], loss=61.0052
	step [285/332], loss=58.4298
	step [286/332], loss=64.0741
	step [287/332], loss=61.1856
	step [288/332], loss=51.9841
	step [289/332], loss=62.4209
	step [290/332], loss=54.5793
	step [291/332], loss=58.2764
	step [292/332], loss=57.3227
	step [293/332], loss=64.2339
	step [294/332], loss=62.8907
	step [295/332], loss=62.4616
	step [296/332], loss=60.3679
	step [297/332], loss=74.4517
	step [298/332], loss=60.4066
	step [299/332], loss=67.4472
	step [300/332], loss=61.9141
	step [301/332], loss=61.5397
	step [302/332], loss=65.0249
	step [303/332], loss=64.8835
	step [304/332], loss=57.0466
	step [305/332], loss=58.8092
	step [306/332], loss=59.3590
	step [307/332], loss=68.8699
	step [308/332], loss=54.5908
	step [309/332], loss=60.3999
	step [310/332], loss=62.5476
	step [311/332], loss=49.8200
	step [312/332], loss=60.5346
	step [313/332], loss=59.7962
	step [314/332], loss=56.1147
	step [315/332], loss=71.9528
	step [316/332], loss=55.3482
	step [317/332], loss=65.6709
	step [318/332], loss=75.1764
	step [319/332], loss=69.2281
	step [320/332], loss=63.0887
	step [321/332], loss=84.7754
	step [322/332], loss=69.8296
	step [323/332], loss=74.2558
	step [324/332], loss=72.2071
	step [325/332], loss=58.2169
	step [326/332], loss=61.8967
	step [327/332], loss=61.9265
	step [328/332], loss=50.6243
	step [329/332], loss=65.5492
	step [330/332], loss=58.7572
	step [331/332], loss=64.6495
	step [332/332], loss=35.2605
	Evaluating
	loss=0.0063, precision=0.3897, recall=0.9073, f1=0.5452
Training epoch 28
	step [1/332], loss=64.5568
	step [2/332], loss=48.7411
	step [3/332], loss=67.6384
	step [4/332], loss=66.9843
	step [5/332], loss=73.2284
	step [6/332], loss=58.2451
	step [7/332], loss=68.0864
	step [8/332], loss=75.6893
	step [9/332], loss=82.4754
	step [10/332], loss=70.3495
	step [11/332], loss=62.4880
	step [12/332], loss=57.7437
	step [13/332], loss=66.3204
	step [14/332], loss=69.3412
	step [15/332], loss=57.8070
	step [16/332], loss=73.8030
	step [17/332], loss=63.3354
	step [18/332], loss=58.6209
	step [19/332], loss=65.0017
	step [20/332], loss=48.2369
	step [21/332], loss=49.3230
	step [22/332], loss=56.5095
	step [23/332], loss=59.1961
	step [24/332], loss=65.9670
	step [25/332], loss=66.4592
	step [26/332], loss=54.9402
	step [27/332], loss=68.0493
	step [28/332], loss=60.1137
	step [29/332], loss=53.8659
	step [30/332], loss=60.9413
	step [31/332], loss=53.1013
	step [32/332], loss=79.5245
	step [33/332], loss=59.2955
	step [34/332], loss=52.5836
	step [35/332], loss=72.1898
	step [36/332], loss=75.2752
	step [37/332], loss=62.9310
	step [38/332], loss=67.7892
	step [39/332], loss=64.8260
	step [40/332], loss=64.2035
	step [41/332], loss=62.2459
	step [42/332], loss=61.4402
	step [43/332], loss=68.1015
	step [44/332], loss=67.1097
	step [45/332], loss=62.3829
	step [46/332], loss=50.6182
	step [47/332], loss=72.1512
	step [48/332], loss=55.9694
	step [49/332], loss=74.3706
	step [50/332], loss=54.9183
	step [51/332], loss=51.8184
	step [52/332], loss=66.7039
	step [53/332], loss=70.4676
	step [54/332], loss=68.2671
	step [55/332], loss=55.3338
	step [56/332], loss=61.1078
	step [57/332], loss=52.7945
	step [58/332], loss=69.2421
	step [59/332], loss=64.3119
	step [60/332], loss=74.6204
	step [61/332], loss=62.9648
	step [62/332], loss=70.2550
	step [63/332], loss=57.5801
	step [64/332], loss=66.2047
	step [65/332], loss=50.5978
	step [66/332], loss=73.5077
	step [67/332], loss=65.8218
	step [68/332], loss=67.6190
	step [69/332], loss=51.4878
	step [70/332], loss=73.7695
	step [71/332], loss=57.7317
	step [72/332], loss=62.7422
	step [73/332], loss=67.4440
	step [74/332], loss=75.4648
	step [75/332], loss=61.8211
	step [76/332], loss=73.2703
	step [77/332], loss=61.7366
	step [78/332], loss=62.8196
	step [79/332], loss=50.2299
	step [80/332], loss=58.0516
	step [81/332], loss=55.6202
	step [82/332], loss=64.9058
	step [83/332], loss=69.9932
	step [84/332], loss=58.2370
	step [85/332], loss=73.7552
	step [86/332], loss=68.4572
	step [87/332], loss=64.5165
	step [88/332], loss=66.3393
	step [89/332], loss=77.2743
	step [90/332], loss=67.0005
	step [91/332], loss=76.5593
	step [92/332], loss=67.9356
	step [93/332], loss=68.6694
	step [94/332], loss=57.6680
	step [95/332], loss=60.5742
	step [96/332], loss=48.1822
	step [97/332], loss=59.9248
	step [98/332], loss=68.0835
	step [99/332], loss=63.2818
	step [100/332], loss=68.6125
	step [101/332], loss=75.0140
	step [102/332], loss=64.5381
	step [103/332], loss=69.9930
	step [104/332], loss=56.2809
	step [105/332], loss=63.5660
	step [106/332], loss=68.1303
	step [107/332], loss=69.1828
	step [108/332], loss=69.2699
	step [109/332], loss=74.8637
	step [110/332], loss=64.7721
	step [111/332], loss=59.8966
	step [112/332], loss=64.8846
	step [113/332], loss=76.1206
	step [114/332], loss=64.4312
	step [115/332], loss=50.0276
	step [116/332], loss=58.5777
	step [117/332], loss=72.0686
	step [118/332], loss=58.5712
	step [119/332], loss=64.7866
	step [120/332], loss=62.4529
	step [121/332], loss=60.1667
	step [122/332], loss=66.8071
	step [123/332], loss=67.1017
	step [124/332], loss=68.2284
	step [125/332], loss=69.8580
	step [126/332], loss=67.5184
	step [127/332], loss=69.6170
	step [128/332], loss=67.5669
	step [129/332], loss=58.9257
	step [130/332], loss=69.4925
	step [131/332], loss=69.2035
	step [132/332], loss=77.6377
	step [133/332], loss=71.5239
	step [134/332], loss=67.3927
	step [135/332], loss=80.0005
	step [136/332], loss=56.9230
	step [137/332], loss=53.8539
	step [138/332], loss=69.3048
	step [139/332], loss=63.0909
	step [140/332], loss=59.3308
	step [141/332], loss=58.3540
	step [142/332], loss=52.8389
	step [143/332], loss=65.5164
	step [144/332], loss=75.3056
	step [145/332], loss=58.9314
	step [146/332], loss=54.6606
	step [147/332], loss=53.1174
	step [148/332], loss=48.7711
	step [149/332], loss=73.7491
	step [150/332], loss=62.4483
	step [151/332], loss=57.6380
	step [152/332], loss=66.9238
	step [153/332], loss=66.4884
	step [154/332], loss=65.2341
	step [155/332], loss=67.6650
	step [156/332], loss=49.0147
	step [157/332], loss=48.9025
	step [158/332], loss=61.4927
	step [159/332], loss=62.3358
	step [160/332], loss=61.8167
	step [161/332], loss=47.8993
	step [162/332], loss=59.0386
	step [163/332], loss=59.6350
	step [164/332], loss=69.6838
	step [165/332], loss=54.0761
	step [166/332], loss=76.7039
	step [167/332], loss=56.9355
	step [168/332], loss=63.3440
	step [169/332], loss=74.5496
	step [170/332], loss=73.7831
	step [171/332], loss=74.7716
	step [172/332], loss=74.5100
	step [173/332], loss=74.7345
	step [174/332], loss=71.9127
	step [175/332], loss=64.4238
	step [176/332], loss=56.6314
	step [177/332], loss=56.6725
	step [178/332], loss=74.6770
	step [179/332], loss=62.9553
	step [180/332], loss=61.1368
	step [181/332], loss=69.3175
	step [182/332], loss=40.8241
	step [183/332], loss=46.5690
	step [184/332], loss=64.2812
	step [185/332], loss=58.6707
	step [186/332], loss=46.4973
	step [187/332], loss=63.1103
	step [188/332], loss=60.4754
	step [189/332], loss=66.2708
	step [190/332], loss=72.7105
	step [191/332], loss=51.0967
	step [192/332], loss=63.9493
	step [193/332], loss=56.2816
	step [194/332], loss=58.4517
	step [195/332], loss=77.2157
	step [196/332], loss=65.7546
	step [197/332], loss=56.9125
	step [198/332], loss=51.7609
	step [199/332], loss=70.6979
	step [200/332], loss=58.0291
	step [201/332], loss=45.7659
	step [202/332], loss=57.6460
	step [203/332], loss=61.9077
	step [204/332], loss=56.0075
	step [205/332], loss=59.2073
	step [206/332], loss=62.2578
	step [207/332], loss=47.9337
	step [208/332], loss=69.8842
	step [209/332], loss=72.9468
	step [210/332], loss=55.1762
	step [211/332], loss=54.6136
	step [212/332], loss=56.1070
	step [213/332], loss=64.1428
	step [214/332], loss=59.5948
	step [215/332], loss=59.8707
	step [216/332], loss=70.4778
	step [217/332], loss=56.4726
	step [218/332], loss=54.6937
	step [219/332], loss=66.7467
	step [220/332], loss=77.5540
	step [221/332], loss=56.3533
	step [222/332], loss=59.4101
	step [223/332], loss=56.7104
	step [224/332], loss=62.6876
	step [225/332], loss=54.2953
	step [226/332], loss=63.2403
	step [227/332], loss=71.9842
	step [228/332], loss=67.8468
	step [229/332], loss=62.2129
	step [230/332], loss=52.0507
	step [231/332], loss=66.3609
	step [232/332], loss=56.6894
	step [233/332], loss=66.7381
	step [234/332], loss=60.3241
	step [235/332], loss=70.7461
	step [236/332], loss=67.2616
	step [237/332], loss=46.4741
	step [238/332], loss=52.9806
	step [239/332], loss=52.3669
	step [240/332], loss=58.7196
	step [241/332], loss=57.7643
	step [242/332], loss=62.4854
	step [243/332], loss=55.5806
	step [244/332], loss=56.1183
	step [245/332], loss=53.9800
	step [246/332], loss=73.9983
	step [247/332], loss=50.4242
	step [248/332], loss=61.7333
	step [249/332], loss=65.7568
	step [250/332], loss=55.9751
	step [251/332], loss=61.9878
	step [252/332], loss=59.0305
	step [253/332], loss=57.4190
	step [254/332], loss=59.5789
	step [255/332], loss=56.0301
	step [256/332], loss=47.0779
	step [257/332], loss=74.9268
	step [258/332], loss=75.5755
	step [259/332], loss=56.6487
	step [260/332], loss=57.9810
	step [261/332], loss=44.3868
	step [262/332], loss=76.7804
	step [263/332], loss=54.1667
	step [264/332], loss=64.4200
	step [265/332], loss=57.5547
	step [266/332], loss=52.2770
	step [267/332], loss=49.1032
	step [268/332], loss=72.9728
	step [269/332], loss=55.6078
	step [270/332], loss=68.2509
	step [271/332], loss=57.3111
	step [272/332], loss=58.4396
	step [273/332], loss=64.1037
	step [274/332], loss=68.1272
	step [275/332], loss=48.7321
	step [276/332], loss=45.3630
	step [277/332], loss=55.8207
	step [278/332], loss=53.3828
	step [279/332], loss=67.2831
	step [280/332], loss=64.9182
	step [281/332], loss=45.7017
	step [282/332], loss=65.5208
	step [283/332], loss=65.1743
	step [284/332], loss=63.8126
	step [285/332], loss=55.1123
	step [286/332], loss=69.5429
	step [287/332], loss=52.9285
	step [288/332], loss=50.9735
	step [289/332], loss=49.7190
	step [290/332], loss=56.9314
	step [291/332], loss=57.0714
	step [292/332], loss=69.8570
	step [293/332], loss=56.4861
	step [294/332], loss=67.0930
	step [295/332], loss=61.6034
	step [296/332], loss=48.3466
	step [297/332], loss=65.1948
	step [298/332], loss=45.9647
	step [299/332], loss=66.2402
	step [300/332], loss=52.0862
	step [301/332], loss=70.8310
	step [302/332], loss=48.5832
	step [303/332], loss=55.1774
	step [304/332], loss=67.0172
	step [305/332], loss=60.3962
	step [306/332], loss=63.7282
	step [307/332], loss=65.8683
	step [308/332], loss=61.7840
	step [309/332], loss=64.2863
	step [310/332], loss=54.6971
	step [311/332], loss=88.1432
	step [312/332], loss=63.9724
	step [313/332], loss=56.5316
	step [314/332], loss=69.3920
	step [315/332], loss=59.8654
	step [316/332], loss=51.4784
	step [317/332], loss=72.8721
	step [318/332], loss=59.4428
	step [319/332], loss=65.3783
	step [320/332], loss=81.7921
	step [321/332], loss=59.2980
	step [322/332], loss=59.5644
	step [323/332], loss=56.5846
	step [324/332], loss=58.6028
	step [325/332], loss=49.1053
	step [326/332], loss=64.9400
	step [327/332], loss=69.0576
	step [328/332], loss=62.3569
	step [329/332], loss=67.9348
	step [330/332], loss=63.4712
	step [331/332], loss=74.1935
	step [332/332], loss=40.0608
	Evaluating
	loss=0.0061, precision=0.3935, recall=0.9090, f1=0.5492
Training epoch 29
	step [1/332], loss=51.4524
	step [2/332], loss=75.6378
	step [3/332], loss=63.9985
	step [4/332], loss=46.2629
	step [5/332], loss=58.9101
	step [6/332], loss=68.2333
	step [7/332], loss=61.9403
	step [8/332], loss=61.6321
	step [9/332], loss=69.2928
	step [10/332], loss=59.4760
	step [11/332], loss=69.9026
	step [12/332], loss=68.2488
	step [13/332], loss=70.3244
	step [14/332], loss=61.8724
	step [15/332], loss=78.6502
	step [16/332], loss=58.4429
	step [17/332], loss=64.8251
	step [18/332], loss=50.0229
	step [19/332], loss=67.8434
	step [20/332], loss=61.0208
	step [21/332], loss=54.7845
	step [22/332], loss=68.9102
	step [23/332], loss=57.9308
	step [24/332], loss=49.1545
	step [25/332], loss=65.3007
	step [26/332], loss=56.5039
	step [27/332], loss=81.4170
	step [28/332], loss=57.1225
	step [29/332], loss=70.5877
	step [30/332], loss=67.2319
	step [31/332], loss=62.9481
	step [32/332], loss=80.8563
	step [33/332], loss=82.0568
	step [34/332], loss=58.2539
	step [35/332], loss=88.1679
	step [36/332], loss=65.2603
	step [37/332], loss=61.9432
	step [38/332], loss=64.1118
	step [39/332], loss=69.6114
	step [40/332], loss=56.7557
	step [41/332], loss=60.4092
	step [42/332], loss=66.1113
	step [43/332], loss=62.8733
	step [44/332], loss=63.5198
	step [45/332], loss=56.9633
	step [46/332], loss=50.2926
	step [47/332], loss=57.0143
	step [48/332], loss=59.7586
	step [49/332], loss=74.9443
	step [50/332], loss=54.2322
	step [51/332], loss=63.1110
	step [52/332], loss=78.2511
	step [53/332], loss=51.4734
	step [54/332], loss=68.9588
	step [55/332], loss=57.6177
	step [56/332], loss=65.2268
	step [57/332], loss=59.1794
	step [58/332], loss=55.5605
	step [59/332], loss=55.1294
	step [60/332], loss=46.8115
	step [61/332], loss=73.3818
	step [62/332], loss=69.2380
	step [63/332], loss=61.9922
	step [64/332], loss=65.5266
	step [65/332], loss=48.1807
	step [66/332], loss=71.8965
	step [67/332], loss=68.3034
	step [68/332], loss=60.3956
	step [69/332], loss=61.2023
	step [70/332], loss=69.4056
	step [71/332], loss=52.0854
	step [72/332], loss=53.2334
	step [73/332], loss=61.5540
	step [74/332], loss=57.0899
	step [75/332], loss=59.9770
	step [76/332], loss=62.0282
	step [77/332], loss=68.2700
	step [78/332], loss=76.3236
	step [79/332], loss=50.1478
	step [80/332], loss=47.9882
	step [81/332], loss=57.3680
	step [82/332], loss=51.1648
	step [83/332], loss=71.8836
	step [84/332], loss=62.3734
	step [85/332], loss=62.6582
	step [86/332], loss=66.0376
	step [87/332], loss=58.4749
	step [88/332], loss=62.2437
	step [89/332], loss=54.4985
	step [90/332], loss=62.0159
	step [91/332], loss=55.8259
	step [92/332], loss=60.8186
	step [93/332], loss=63.9332
	step [94/332], loss=66.3951
	step [95/332], loss=56.8993
	step [96/332], loss=60.3387
	step [97/332], loss=75.6815
	step [98/332], loss=48.8439
	step [99/332], loss=59.1125
	step [100/332], loss=77.7899
	step [101/332], loss=66.5313
	step [102/332], loss=51.5022
	step [103/332], loss=58.2903
	step [104/332], loss=68.9773
	step [105/332], loss=78.3336
	step [106/332], loss=55.9328
	step [107/332], loss=64.0777
	step [108/332], loss=61.7554
	step [109/332], loss=65.8075
	step [110/332], loss=65.7593
	step [111/332], loss=64.6243
	step [112/332], loss=74.8747
	step [113/332], loss=60.1947
	step [114/332], loss=65.9879
	step [115/332], loss=45.6747
	step [116/332], loss=59.7295
	step [117/332], loss=63.5112
	step [118/332], loss=57.1892
	step [119/332], loss=64.3765
	step [120/332], loss=58.4937
	step [121/332], loss=73.0606
	step [122/332], loss=58.0146
	step [123/332], loss=54.0627
	step [124/332], loss=63.6298
	step [125/332], loss=56.9417
	step [126/332], loss=73.0509
	step [127/332], loss=62.1153
	step [128/332], loss=76.1759
	step [129/332], loss=67.0192
	step [130/332], loss=58.3764
	step [131/332], loss=79.9294
	step [132/332], loss=47.3844
	step [133/332], loss=59.0437
	step [134/332], loss=54.3164
	step [135/332], loss=52.5882
	step [136/332], loss=63.6000
	step [137/332], loss=56.7156
	step [138/332], loss=58.8800
	step [139/332], loss=64.7740
	step [140/332], loss=63.3313
	step [141/332], loss=63.5466
	step [142/332], loss=65.2963
	step [143/332], loss=54.7797
	step [144/332], loss=52.8129
	step [145/332], loss=62.4374
	step [146/332], loss=49.2007
	step [147/332], loss=58.9661
	step [148/332], loss=57.0955
	step [149/332], loss=76.8104
	step [150/332], loss=69.8784
	step [151/332], loss=66.1049
	step [152/332], loss=51.2997
	step [153/332], loss=64.7806
	step [154/332], loss=65.1159
	step [155/332], loss=79.0272
	step [156/332], loss=75.9444
	step [157/332], loss=64.7369
	step [158/332], loss=67.5861
	step [159/332], loss=65.5057
	step [160/332], loss=63.3297
	step [161/332], loss=63.8601
	step [162/332], loss=65.3499
	step [163/332], loss=58.1804
	step [164/332], loss=64.9424
	step [165/332], loss=57.9417
	step [166/332], loss=69.4095
	step [167/332], loss=64.4110
	step [168/332], loss=55.3914
	step [169/332], loss=60.1101
	step [170/332], loss=59.3002
	step [171/332], loss=55.0079
	step [172/332], loss=66.2159
	step [173/332], loss=61.6698
	step [174/332], loss=59.5707
	step [175/332], loss=69.3188
	step [176/332], loss=78.8283
	step [177/332], loss=46.5205
	step [178/332], loss=45.4398
	step [179/332], loss=56.6716
	step [180/332], loss=71.9933
	step [181/332], loss=54.3429
	step [182/332], loss=69.5741
	step [183/332], loss=63.7595
	step [184/332], loss=58.5181
	step [185/332], loss=46.6192
	step [186/332], loss=71.4673
	step [187/332], loss=48.2832
	step [188/332], loss=56.8023
	step [189/332], loss=58.2327
	step [190/332], loss=65.0074
	step [191/332], loss=73.2272
	step [192/332], loss=54.2969
	step [193/332], loss=55.5950
	step [194/332], loss=64.8668
	step [195/332], loss=61.4614
	step [196/332], loss=55.7478
	step [197/332], loss=72.0896
	step [198/332], loss=66.4050
	step [199/332], loss=57.9803
	step [200/332], loss=49.7190
	step [201/332], loss=65.3601
	step [202/332], loss=54.4897
	step [203/332], loss=71.4351
	step [204/332], loss=63.2059
	step [205/332], loss=47.1440
	step [206/332], loss=68.5501
	step [207/332], loss=66.5302
	step [208/332], loss=57.3468
	step [209/332], loss=68.7916
	step [210/332], loss=49.8474
	step [211/332], loss=60.3098
	step [212/332], loss=43.7087
	step [213/332], loss=60.1010
	step [214/332], loss=65.0283
	step [215/332], loss=45.3961
	step [216/332], loss=67.7605
	step [217/332], loss=71.5919
	step [218/332], loss=61.5609
	step [219/332], loss=69.3235
	step [220/332], loss=63.3930
	step [221/332], loss=53.6752
	step [222/332], loss=44.4081
	step [223/332], loss=72.5427
	step [224/332], loss=54.8230
	step [225/332], loss=68.6586
	step [226/332], loss=76.1980
	step [227/332], loss=60.9568
	step [228/332], loss=53.2228
	step [229/332], loss=60.0332
	step [230/332], loss=62.4305
	step [231/332], loss=67.3061
	step [232/332], loss=68.7010
	step [233/332], loss=54.2411
	step [234/332], loss=74.0366
	step [235/332], loss=66.1324
	step [236/332], loss=52.5236
	step [237/332], loss=55.2409
	step [238/332], loss=71.1272
	step [239/332], loss=52.6464
	step [240/332], loss=75.0123
	step [241/332], loss=54.6765
	step [242/332], loss=60.8948
	step [243/332], loss=52.7022
	step [244/332], loss=49.7734
	step [245/332], loss=49.5915
	step [246/332], loss=54.5403
	step [247/332], loss=57.0498
	step [248/332], loss=64.3221
	step [249/332], loss=60.3177
	step [250/332], loss=53.6328
	step [251/332], loss=63.4645
	step [252/332], loss=76.5119
	step [253/332], loss=57.2496
	step [254/332], loss=56.3069
	step [255/332], loss=59.8899
	step [256/332], loss=64.8801
	step [257/332], loss=58.6311
	step [258/332], loss=55.9150
	step [259/332], loss=56.5497
	step [260/332], loss=59.3252
	step [261/332], loss=55.6187
	step [262/332], loss=53.7429
	step [263/332], loss=70.1398
	step [264/332], loss=67.8924
	step [265/332], loss=55.2232
	step [266/332], loss=61.5138
	step [267/332], loss=49.7291
	step [268/332], loss=63.8835
	step [269/332], loss=49.8551
	step [270/332], loss=72.3459
	step [271/332], loss=53.0013
	step [272/332], loss=65.7785
	step [273/332], loss=52.8116
	step [274/332], loss=67.8571
	step [275/332], loss=67.4256
	step [276/332], loss=68.0833
	step [277/332], loss=79.5443
	step [278/332], loss=82.5805
	step [279/332], loss=53.7493
	step [280/332], loss=46.1390
	step [281/332], loss=64.7483
	step [282/332], loss=66.5560
	step [283/332], loss=67.5676
	step [284/332], loss=53.7451
	step [285/332], loss=58.3893
	step [286/332], loss=52.5078
	step [287/332], loss=57.4733
	step [288/332], loss=56.9248
	step [289/332], loss=65.7853
	step [290/332], loss=53.8198
	step [291/332], loss=85.9531
	step [292/332], loss=54.6361
	step [293/332], loss=62.3344
	step [294/332], loss=61.1535
	step [295/332], loss=52.3349
	step [296/332], loss=61.9382
	step [297/332], loss=51.4300
	step [298/332], loss=54.7200
	step [299/332], loss=57.5098
	step [300/332], loss=60.4641
	step [301/332], loss=73.8755
	step [302/332], loss=51.8292
	step [303/332], loss=74.1874
	step [304/332], loss=55.4103
	step [305/332], loss=61.6769
	step [306/332], loss=57.6938
	step [307/332], loss=62.5335
	step [308/332], loss=47.4587
	step [309/332], loss=60.4130
	step [310/332], loss=64.0853
	step [311/332], loss=63.6446
	step [312/332], loss=72.5603
	step [313/332], loss=82.8159
	step [314/332], loss=52.5090
	step [315/332], loss=48.4472
	step [316/332], loss=69.7895
	step [317/332], loss=56.1816
	step [318/332], loss=56.2771
	step [319/332], loss=69.2956
	step [320/332], loss=72.8505
	step [321/332], loss=56.5639
	step [322/332], loss=58.6440
	step [323/332], loss=70.0988
	step [324/332], loss=63.4903
	step [325/332], loss=61.3918
	step [326/332], loss=54.0086
	step [327/332], loss=62.7721
	step [328/332], loss=63.3253
	step [329/332], loss=64.5335
	step [330/332], loss=54.2043
	step [331/332], loss=63.5521
	step [332/332], loss=39.5719
	Evaluating
	loss=0.0059, precision=0.3866, recall=0.9029, f1=0.5414
Training epoch 30
	step [1/332], loss=72.0407
	step [2/332], loss=72.5377
	step [3/332], loss=54.1649
	step [4/332], loss=67.3139
	step [5/332], loss=60.0890
	step [6/332], loss=50.5516
	step [7/332], loss=46.3671
	step [8/332], loss=61.0241
	step [9/332], loss=62.6319
	step [10/332], loss=56.9942
	step [11/332], loss=71.8758
	step [12/332], loss=63.8619
	step [13/332], loss=66.2973
	step [14/332], loss=63.0307
	step [15/332], loss=58.5442
	step [16/332], loss=60.9582
	step [17/332], loss=61.8063
	step [18/332], loss=66.6456
	step [19/332], loss=70.4142
	step [20/332], loss=68.2314
	step [21/332], loss=51.3408
	step [22/332], loss=61.5147
	step [23/332], loss=52.8510
	step [24/332], loss=73.5893
	step [25/332], loss=60.5354
	step [26/332], loss=71.2713
	step [27/332], loss=57.8771
	step [28/332], loss=64.7448
	step [29/332], loss=62.4768
	step [30/332], loss=65.8206
	step [31/332], loss=61.5791
	step [32/332], loss=59.3235
	step [33/332], loss=71.9928
	step [34/332], loss=51.9182
	step [35/332], loss=63.3800
	step [36/332], loss=48.5740
	step [37/332], loss=51.8481
	step [38/332], loss=64.5457
	step [39/332], loss=58.3726
	step [40/332], loss=52.3320
	step [41/332], loss=57.9799
	step [42/332], loss=56.0512
	step [43/332], loss=54.0765
	step [44/332], loss=72.7190
	step [45/332], loss=51.7449
	step [46/332], loss=76.4564
	step [47/332], loss=50.4662
	step [48/332], loss=73.8605
	step [49/332], loss=54.1373
	step [50/332], loss=59.6867
	step [51/332], loss=44.5336
	step [52/332], loss=74.0560
	step [53/332], loss=64.0622
	step [54/332], loss=62.6329
	step [55/332], loss=48.4235
	step [56/332], loss=65.3052
	step [57/332], loss=52.3515
	step [58/332], loss=59.0154
	step [59/332], loss=59.8839
	step [60/332], loss=66.3357
	step [61/332], loss=52.8720
	step [62/332], loss=51.9603
	step [63/332], loss=73.0698
	step [64/332], loss=54.8653
	step [65/332], loss=65.8681
	step [66/332], loss=65.8930
	step [67/332], loss=65.7940
	step [68/332], loss=75.3741
	step [69/332], loss=63.2816
	step [70/332], loss=60.2990
	step [71/332], loss=57.1038
	step [72/332], loss=48.0829
	step [73/332], loss=62.7897
	step [74/332], loss=60.6181
	step [75/332], loss=60.8953
	step [76/332], loss=66.3033
	step [77/332], loss=52.9675
	step [78/332], loss=49.8338
	step [79/332], loss=44.5633
	step [80/332], loss=58.0597
	step [81/332], loss=57.0745
	step [82/332], loss=62.7460
	step [83/332], loss=60.3990
	step [84/332], loss=56.8202
	step [85/332], loss=67.3221
	step [86/332], loss=51.5436
	step [87/332], loss=55.6735
	step [88/332], loss=67.1595
	step [89/332], loss=73.4542
	step [90/332], loss=75.0969
	step [91/332], loss=67.1708
	step [92/332], loss=59.2526
	step [93/332], loss=60.2798
	step [94/332], loss=53.4807
	step [95/332], loss=61.3063
	step [96/332], loss=57.6362
	step [97/332], loss=53.0191
	step [98/332], loss=57.6601
	step [99/332], loss=61.5991
	step [100/332], loss=46.5945
	step [101/332], loss=49.0808
	step [102/332], loss=46.9909
	step [103/332], loss=58.1321
	step [104/332], loss=66.4974
	step [105/332], loss=75.3820
	step [106/332], loss=72.3169
	step [107/332], loss=56.8205
	step [108/332], loss=74.5720
	step [109/332], loss=60.1615
	step [110/332], loss=56.3476
	step [111/332], loss=63.3883
	step [112/332], loss=85.7493
	step [113/332], loss=60.0086
	step [114/332], loss=57.8792
	step [115/332], loss=52.9551
	step [116/332], loss=59.2411
	step [117/332], loss=68.0187
	step [118/332], loss=49.5475
	step [119/332], loss=54.4979
	step [120/332], loss=59.6930
	step [121/332], loss=58.1043
	step [122/332], loss=60.5450
	step [123/332], loss=61.6592
	step [124/332], loss=58.1518
	step [125/332], loss=57.2128
	step [126/332], loss=54.6534
	step [127/332], loss=48.4011
	step [128/332], loss=52.8522
	step [129/332], loss=73.3677
	step [130/332], loss=51.5671
	step [131/332], loss=61.6929
	step [132/332], loss=66.9571
	step [133/332], loss=55.9508
	step [134/332], loss=62.1515
	step [135/332], loss=58.5859
	step [136/332], loss=64.2881
	step [137/332], loss=57.2903
	step [138/332], loss=67.2392
	step [139/332], loss=64.0237
	step [140/332], loss=62.5672
	step [141/332], loss=61.1530
	step [142/332], loss=60.5943
	step [143/332], loss=44.8913
	step [144/332], loss=56.6117
	step [145/332], loss=64.2457
	step [146/332], loss=43.8019
	step [147/332], loss=70.5857
	step [148/332], loss=68.9480
	step [149/332], loss=69.5233
	step [150/332], loss=61.8770
	step [151/332], loss=55.5485
	step [152/332], loss=59.0567
	step [153/332], loss=64.0279
	step [154/332], loss=68.5106
	step [155/332], loss=68.2629
	step [156/332], loss=61.7444
	step [157/332], loss=59.4818
	step [158/332], loss=60.8562
	step [159/332], loss=63.1953
	step [160/332], loss=69.1389
	step [161/332], loss=63.3018
	step [162/332], loss=66.8608
	step [163/332], loss=57.5283
	step [164/332], loss=64.2658
	step [165/332], loss=59.8457
	step [166/332], loss=55.8383
	step [167/332], loss=63.0061
	step [168/332], loss=53.3835
	step [169/332], loss=62.5523
	step [170/332], loss=55.8640
	step [171/332], loss=53.6791
	step [172/332], loss=59.8402
	step [173/332], loss=59.0785
	step [174/332], loss=65.5125
	step [175/332], loss=53.1768
	step [176/332], loss=50.6033
	step [177/332], loss=66.4143
	step [178/332], loss=52.0295
	step [179/332], loss=54.7080
	step [180/332], loss=50.8609
	step [181/332], loss=68.0012
	step [182/332], loss=57.5380
	step [183/332], loss=72.6527
	step [184/332], loss=59.4015
	step [185/332], loss=55.3318
	step [186/332], loss=58.9807
	step [187/332], loss=72.0859
	step [188/332], loss=52.4189
	step [189/332], loss=53.3831
	step [190/332], loss=61.9262
	step [191/332], loss=68.0743
	step [192/332], loss=68.8443
	step [193/332], loss=78.4805
	step [194/332], loss=64.7647
	step [195/332], loss=64.8312
	step [196/332], loss=59.1953
	step [197/332], loss=51.5005
	step [198/332], loss=64.5048
	step [199/332], loss=47.7093
	step [200/332], loss=65.1903
	step [201/332], loss=56.5268
	step [202/332], loss=64.0567
	step [203/332], loss=75.9535
	step [204/332], loss=57.2428
	step [205/332], loss=58.0833
	step [206/332], loss=54.4428
	step [207/332], loss=60.7801
	step [208/332], loss=61.4424
	step [209/332], loss=60.2092
	step [210/332], loss=61.0202
	step [211/332], loss=53.7579
	step [212/332], loss=63.8475
	step [213/332], loss=61.2037
	step [214/332], loss=65.4305
	step [215/332], loss=51.1829
	step [216/332], loss=66.8623
	step [217/332], loss=68.3778
	step [218/332], loss=54.7980
	step [219/332], loss=92.0507
	step [220/332], loss=54.9446
	step [221/332], loss=49.3427
	step [222/332], loss=51.8576
	step [223/332], loss=69.5237
	step [224/332], loss=47.2919
	step [225/332], loss=60.4896
	step [226/332], loss=50.2103
	step [227/332], loss=60.0174
	step [228/332], loss=58.1971
	step [229/332], loss=59.1991
	step [230/332], loss=63.5665
	step [231/332], loss=63.5150
	step [232/332], loss=67.0136
	step [233/332], loss=58.9331
	step [234/332], loss=52.5854
	step [235/332], loss=50.0986
	step [236/332], loss=75.5237
	step [237/332], loss=61.5249
	step [238/332], loss=59.2009
	step [239/332], loss=50.5696
	step [240/332], loss=75.8553
	step [241/332], loss=68.7372
	step [242/332], loss=44.7564
	step [243/332], loss=72.6621
	step [244/332], loss=68.3397
	step [245/332], loss=50.8889
	step [246/332], loss=62.7672
	step [247/332], loss=75.4682
	step [248/332], loss=47.4458
	step [249/332], loss=45.8722
	step [250/332], loss=72.1430
	step [251/332], loss=56.4800
	step [252/332], loss=44.5374
	step [253/332], loss=69.1539
	step [254/332], loss=60.7497
	step [255/332], loss=59.3854
	step [256/332], loss=65.4068
	step [257/332], loss=64.0992
	step [258/332], loss=53.6991
	step [259/332], loss=72.1767
	step [260/332], loss=61.1488
	step [261/332], loss=53.6046
	step [262/332], loss=69.9838
	step [263/332], loss=67.4048
	step [264/332], loss=59.5010
	step [265/332], loss=49.8992
	step [266/332], loss=61.3305
	step [267/332], loss=73.2365
	step [268/332], loss=68.8809
	step [269/332], loss=61.9130
	step [270/332], loss=67.2167
	step [271/332], loss=49.4074
	step [272/332], loss=63.7886
	step [273/332], loss=81.7023
	step [274/332], loss=52.8405
	step [275/332], loss=70.5284
	step [276/332], loss=51.2713
	step [277/332], loss=88.3297
	step [278/332], loss=80.4304
	step [279/332], loss=78.2735
	step [280/332], loss=59.8157
	step [281/332], loss=73.4859
	step [282/332], loss=60.2355
	step [283/332], loss=51.0401
	step [284/332], loss=55.9768
	step [285/332], loss=71.1113
	step [286/332], loss=57.8269
	step [287/332], loss=60.8271
	step [288/332], loss=63.0265
	step [289/332], loss=63.5678
	step [290/332], loss=65.2253
	step [291/332], loss=63.6960
	step [292/332], loss=80.2523
	step [293/332], loss=63.0018
	step [294/332], loss=52.9022
	step [295/332], loss=63.9281
	step [296/332], loss=71.1958
	step [297/332], loss=57.3709
	step [298/332], loss=62.2912
	step [299/332], loss=74.9157
	step [300/332], loss=64.3724
	step [301/332], loss=58.5668
	step [302/332], loss=67.8993
	step [303/332], loss=57.4349
	step [304/332], loss=68.4069
	step [305/332], loss=59.4726
	step [306/332], loss=60.5696
	step [307/332], loss=60.1186
	step [308/332], loss=69.0398
	step [309/332], loss=61.3081
	step [310/332], loss=64.5052
	step [311/332], loss=55.7263
	step [312/332], loss=69.9364
	step [313/332], loss=47.4788
	step [314/332], loss=53.0122
	step [315/332], loss=65.5991
	step [316/332], loss=62.8176
	step [317/332], loss=54.7109
	step [318/332], loss=56.6320
	step [319/332], loss=57.3408
	step [320/332], loss=65.6852
	step [321/332], loss=60.3547
	step [322/332], loss=62.4215
	step [323/332], loss=54.8327
	step [324/332], loss=64.5468
	step [325/332], loss=62.9025
	step [326/332], loss=55.0005
	step [327/332], loss=70.7467
	step [328/332], loss=56.9124
	step [329/332], loss=48.5813
	step [330/332], loss=76.4549
	step [331/332], loss=48.8036
	step [332/332], loss=31.8274
	Evaluating
	loss=0.0054, precision=0.4215, recall=0.8973, f1=0.5736
saving model as: 0_saved_model.pth
Training finished
best_f1: 0.5735634956622194
directing: Z rim_enhanced: True test_id 0
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12281 # image files with weight 12232
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3263 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/255], loss=240.3516
	step [2/255], loss=231.3298
	step [3/255], loss=246.7934
	step [4/255], loss=221.3268
	step [5/255], loss=234.6345
	step [6/255], loss=218.6280
	step [7/255], loss=214.5053
	step [8/255], loss=222.3101
	step [9/255], loss=241.9210
	step [10/255], loss=208.1107
	step [11/255], loss=229.4326
	step [12/255], loss=193.1857
	step [13/255], loss=225.4769
	step [14/255], loss=196.0609
	step [15/255], loss=193.6865
	step [16/255], loss=192.8668
	step [17/255], loss=198.6881
	step [18/255], loss=188.0292
	step [19/255], loss=187.2969
	step [20/255], loss=188.3456
	step [21/255], loss=187.4711
	step [22/255], loss=204.1255
	step [23/255], loss=171.0153
	step [24/255], loss=213.3186
	step [25/255], loss=198.3831
	step [26/255], loss=194.3837
	step [27/255], loss=190.4149
	step [28/255], loss=175.3978
	step [29/255], loss=169.4164
	step [30/255], loss=176.1825
	step [31/255], loss=180.4982
	step [32/255], loss=185.8538
	step [33/255], loss=180.1453
	step [34/255], loss=183.8061
	step [35/255], loss=191.5197
	step [36/255], loss=160.5983
	step [37/255], loss=172.3235
	step [38/255], loss=191.7424
	step [39/255], loss=166.1258
	step [40/255], loss=213.9183
	step [41/255], loss=176.4759
	step [42/255], loss=169.2786
	step [43/255], loss=175.4594
	step [44/255], loss=170.7405
	step [45/255], loss=165.9504
	step [46/255], loss=180.6942
	step [47/255], loss=179.8893
	step [48/255], loss=162.7658
	step [49/255], loss=176.5766
	step [50/255], loss=166.9450
	step [51/255], loss=167.4939
	step [52/255], loss=164.9170
	step [53/255], loss=174.5707
	step [54/255], loss=170.6863
	step [55/255], loss=166.1824
	step [56/255], loss=164.0276
	step [57/255], loss=169.3669
	step [58/255], loss=166.1289
	step [59/255], loss=148.5889
	step [60/255], loss=177.2189
	step [61/255], loss=161.5811
	step [62/255], loss=173.9817
	step [63/255], loss=182.0886
	step [64/255], loss=171.3636
	step [65/255], loss=165.1370
	step [66/255], loss=153.2847
	step [67/255], loss=168.9348
	step [68/255], loss=154.7931
	step [69/255], loss=143.6374
	step [70/255], loss=166.7220
	step [71/255], loss=166.7244
	step [72/255], loss=155.4424
	step [73/255], loss=146.5936
	step [74/255], loss=165.1387
	step [75/255], loss=151.1545
	step [76/255], loss=164.8362
	step [77/255], loss=150.3519
	step [78/255], loss=144.4722
	step [79/255], loss=151.9093
	step [80/255], loss=175.7034
	step [81/255], loss=155.6755
	step [82/255], loss=156.7167
	step [83/255], loss=147.4997
	step [84/255], loss=147.6497
	step [85/255], loss=163.6546
	step [86/255], loss=156.9733
	step [87/255], loss=153.4657
	step [88/255], loss=146.2548
	step [89/255], loss=169.4578
	step [90/255], loss=134.9050
	step [91/255], loss=153.9003
	step [92/255], loss=150.9130
	step [93/255], loss=147.6815
	step [94/255], loss=147.5750
	step [95/255], loss=160.6711
	step [96/255], loss=145.6609
	step [97/255], loss=156.0376
	step [98/255], loss=131.3306
	step [99/255], loss=163.2407
	step [100/255], loss=146.3834
	step [101/255], loss=145.6177
	step [102/255], loss=153.9788
	step [103/255], loss=152.1422
	step [104/255], loss=163.5638
	step [105/255], loss=134.0468
	step [106/255], loss=142.1981
	step [107/255], loss=151.4168
	step [108/255], loss=137.3774
	step [109/255], loss=147.0636
	step [110/255], loss=141.6156
	step [111/255], loss=144.7890
	step [112/255], loss=149.5841
	step [113/255], loss=140.7644
	step [114/255], loss=137.2223
	step [115/255], loss=150.8681
	step [116/255], loss=159.6896
	step [117/255], loss=142.0581
	step [118/255], loss=143.3150
	step [119/255], loss=148.5927
	step [120/255], loss=139.0196
	step [121/255], loss=142.5915
	step [122/255], loss=127.2434
	step [123/255], loss=147.6660
	step [124/255], loss=160.5077
	step [125/255], loss=144.7756
	step [126/255], loss=145.8451
	step [127/255], loss=135.4103
	step [128/255], loss=142.2907
	step [129/255], loss=137.9094
	step [130/255], loss=153.9328
	step [131/255], loss=142.2219
	step [132/255], loss=143.0061
	step [133/255], loss=140.3343
	step [134/255], loss=140.6632
	step [135/255], loss=154.1101
	step [136/255], loss=146.6034
	step [137/255], loss=161.4836
	step [138/255], loss=155.6990
	step [139/255], loss=153.7799
	step [140/255], loss=149.6722
	step [141/255], loss=146.9391
	step [142/255], loss=145.6222
	step [143/255], loss=137.8717
	step [144/255], loss=135.4709
	step [145/255], loss=139.6913
	step [146/255], loss=131.8191
	step [147/255], loss=142.5786
	step [148/255], loss=135.2963
	step [149/255], loss=145.4995
	step [150/255], loss=127.7464
	step [151/255], loss=130.1112
	step [152/255], loss=146.3156
	step [153/255], loss=142.2100
	step [154/255], loss=141.5062
	step [155/255], loss=135.3764
	step [156/255], loss=133.2546
	step [157/255], loss=150.5611
	step [158/255], loss=150.8770
	step [159/255], loss=135.7190
	step [160/255], loss=135.5559
	step [161/255], loss=140.5755
	step [162/255], loss=146.3913
	step [163/255], loss=120.1249
	step [164/255], loss=147.1159
	step [165/255], loss=134.5385
	step [166/255], loss=131.0352
	step [167/255], loss=147.8599
	step [168/255], loss=139.3417
	step [169/255], loss=146.8710
	step [170/255], loss=129.1311
	step [171/255], loss=138.5625
	step [172/255], loss=141.7947
	step [173/255], loss=130.1268
	step [174/255], loss=134.1447
	step [175/255], loss=130.6466
	step [176/255], loss=144.5564
	step [177/255], loss=126.8927
	step [178/255], loss=142.4678
	step [179/255], loss=125.1920
	step [180/255], loss=137.7360
	step [181/255], loss=154.3944
	step [182/255], loss=150.5995
	step [183/255], loss=126.3933
	step [184/255], loss=135.3180
	step [185/255], loss=133.0328
	step [186/255], loss=134.8962
	step [187/255], loss=151.7148
	step [188/255], loss=136.7104
	step [189/255], loss=134.3813
	step [190/255], loss=133.0697
	step [191/255], loss=131.1298
	step [192/255], loss=130.0572
	step [193/255], loss=135.5627
	step [194/255], loss=133.1534
	step [195/255], loss=151.0082
	step [196/255], loss=133.2961
	step [197/255], loss=146.6063
	step [198/255], loss=147.9857
	step [199/255], loss=124.7032
	step [200/255], loss=132.3930
	step [201/255], loss=132.3748
	step [202/255], loss=139.6890
	step [203/255], loss=144.9403
	step [204/255], loss=143.3250
	step [205/255], loss=118.7961
	step [206/255], loss=127.7812
	step [207/255], loss=136.4948
	step [208/255], loss=134.5232
	step [209/255], loss=134.5946
	step [210/255], loss=147.1862
	step [211/255], loss=120.3175
	step [212/255], loss=125.5493
	step [213/255], loss=131.0936
	step [214/255], loss=145.1349
	step [215/255], loss=133.4421
	step [216/255], loss=121.3062
	step [217/255], loss=135.4910
	step [218/255], loss=140.6936
	step [219/255], loss=120.2179
	step [220/255], loss=118.4638
	step [221/255], loss=143.1195
	step [222/255], loss=119.2917
	step [223/255], loss=132.3348
	step [224/255], loss=137.9653
	step [225/255], loss=117.8335
	step [226/255], loss=134.4535
	step [227/255], loss=131.2067
	step [228/255], loss=124.5597
	step [229/255], loss=120.6740
	step [230/255], loss=128.2811
	step [231/255], loss=128.8151
	step [232/255], loss=131.6206
	step [233/255], loss=131.2462
	step [234/255], loss=139.6841
	step [235/255], loss=145.8204
	step [236/255], loss=118.1990
	step [237/255], loss=124.4090
	step [238/255], loss=128.7631
	step [239/255], loss=138.1918
	step [240/255], loss=128.9566
	step [241/255], loss=134.2911
	step [242/255], loss=126.3312
	step [243/255], loss=132.7422
	step [244/255], loss=137.5148
	step [245/255], loss=122.5392
	step [246/255], loss=121.6479
	step [247/255], loss=128.5513
	step [248/255], loss=126.1647
	step [249/255], loss=130.3229
	step [250/255], loss=135.0897
	step [251/255], loss=132.2410
	step [252/255], loss=121.4634
	step [253/255], loss=128.7530
	step [254/255], loss=115.0310
	step [255/255], loss=102.5385
	Evaluating
	loss=0.3161, precision=0.2623, recall=0.9423, f1=0.4104
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/255], loss=128.8019
	step [2/255], loss=124.0720
	step [3/255], loss=132.7519
	step [4/255], loss=120.1339
	step [5/255], loss=125.8301
	step [6/255], loss=138.9601
	step [7/255], loss=119.8106
	step [8/255], loss=119.4338
	step [9/255], loss=122.3695
	step [10/255], loss=121.7659
	step [11/255], loss=113.3152
	step [12/255], loss=144.0692
	step [13/255], loss=121.5427
	step [14/255], loss=132.8044
	step [15/255], loss=124.9642
	step [16/255], loss=128.0174
	step [17/255], loss=124.3378
	step [18/255], loss=125.4744
	step [19/255], loss=117.4397
	step [20/255], loss=127.2583
	step [21/255], loss=129.7030
	step [22/255], loss=129.5581
	step [23/255], loss=120.6327
	step [24/255], loss=117.3493
	step [25/255], loss=123.7047
	step [26/255], loss=114.9373
	step [27/255], loss=125.1559
	step [28/255], loss=121.9957
	step [29/255], loss=139.4371
	step [30/255], loss=128.0038
	step [31/255], loss=127.4236
	step [32/255], loss=121.5163
	step [33/255], loss=124.7207
	step [34/255], loss=131.5984
	step [35/255], loss=112.9873
	step [36/255], loss=123.8131
	step [37/255], loss=125.5598
	step [38/255], loss=130.4991
	step [39/255], loss=128.9261
	step [40/255], loss=125.7403
	step [41/255], loss=136.0376
	step [42/255], loss=126.2523
	step [43/255], loss=121.1150
	step [44/255], loss=162.5338
	step [45/255], loss=122.4429
	step [46/255], loss=108.2333
	step [47/255], loss=122.6485
	step [48/255], loss=138.6389
	step [49/255], loss=118.7980
	step [50/255], loss=115.1649
	step [51/255], loss=130.8108
	step [52/255], loss=133.9848
	step [53/255], loss=125.1735
	step [54/255], loss=105.7058
	step [55/255], loss=123.2191
	step [56/255], loss=126.4126
	step [57/255], loss=135.1644
	step [58/255], loss=121.9769
	step [59/255], loss=131.1910
	step [60/255], loss=124.0798
	step [61/255], loss=118.5369
	step [62/255], loss=136.0369
	step [63/255], loss=119.4467
	step [64/255], loss=114.7333
	step [65/255], loss=126.1552
	step [66/255], loss=116.1742
	step [67/255], loss=127.8844
	step [68/255], loss=118.4184
	step [69/255], loss=124.3966
	step [70/255], loss=136.7357
	step [71/255], loss=114.6404
	step [72/255], loss=113.4712
	step [73/255], loss=114.5994
	step [74/255], loss=120.0262
	step [75/255], loss=123.7216
	step [76/255], loss=118.2959
	step [77/255], loss=134.9579
	step [78/255], loss=117.3230
	step [79/255], loss=125.9958
	step [80/255], loss=127.2520
	step [81/255], loss=104.9006
	step [82/255], loss=119.9505
	step [83/255], loss=121.5459
	step [84/255], loss=123.7058
	step [85/255], loss=131.0837
	step [86/255], loss=119.5012
	step [87/255], loss=120.6148
	step [88/255], loss=117.6674
	step [89/255], loss=113.8720
	step [90/255], loss=118.1696
	step [91/255], loss=103.1235
	step [92/255], loss=116.0268
	step [93/255], loss=128.6222
	step [94/255], loss=110.8933
	step [95/255], loss=114.9746
	step [96/255], loss=114.3042
	step [97/255], loss=115.8329
	step [98/255], loss=131.5529
	step [99/255], loss=114.9041
	step [100/255], loss=121.2338
	step [101/255], loss=122.7750
	step [102/255], loss=115.4038
	step [103/255], loss=129.0023
	step [104/255], loss=114.6575
	step [105/255], loss=125.6284
	step [106/255], loss=113.2336
	step [107/255], loss=107.6815
	step [108/255], loss=131.4458
	step [109/255], loss=123.9827
	step [110/255], loss=118.3819
	step [111/255], loss=119.0995
	step [112/255], loss=114.9068
	step [113/255], loss=114.7380
	step [114/255], loss=122.5490
	step [115/255], loss=126.6614
	step [116/255], loss=124.2808
	step [117/255], loss=119.1410
	step [118/255], loss=119.0946
	step [119/255], loss=113.0251
	step [120/255], loss=117.0981
	step [121/255], loss=127.1138
	step [122/255], loss=120.7780
	step [123/255], loss=123.9336
	step [124/255], loss=126.2237
	step [125/255], loss=114.5585
	step [126/255], loss=111.0694
	step [127/255], loss=124.1843
	step [128/255], loss=125.6575
	step [129/255], loss=127.3993
	step [130/255], loss=128.8803
	step [131/255], loss=110.3231
	step [132/255], loss=129.3622
	step [133/255], loss=108.0997
	step [134/255], loss=116.3336
	step [135/255], loss=110.6351
	step [136/255], loss=116.4468
	step [137/255], loss=114.5895
	step [138/255], loss=118.8657
	step [139/255], loss=110.3292
	step [140/255], loss=110.1441
	step [141/255], loss=117.2765
	step [142/255], loss=105.9610
	step [143/255], loss=127.5236
	step [144/255], loss=113.8334
	step [145/255], loss=107.2711
	step [146/255], loss=123.6442
	step [147/255], loss=131.2009
	step [148/255], loss=108.3267
	step [149/255], loss=96.3126
	step [150/255], loss=110.4982
	step [151/255], loss=124.4900
	step [152/255], loss=113.2125
	step [153/255], loss=110.7986
	step [154/255], loss=134.7526
	step [155/255], loss=116.7495
	step [156/255], loss=100.4306
	step [157/255], loss=114.3835
	step [158/255], loss=125.6958
	step [159/255], loss=119.4049
	step [160/255], loss=127.1853
	step [161/255], loss=107.5732
	step [162/255], loss=115.4471
	step [163/255], loss=110.1362
	step [164/255], loss=116.5604
	step [165/255], loss=122.8939
	step [166/255], loss=121.4887
	step [167/255], loss=115.9600
	step [168/255], loss=125.4009
	step [169/255], loss=122.5642
	step [170/255], loss=116.1133
	step [171/255], loss=123.7213
	step [172/255], loss=113.0896
	step [173/255], loss=119.8298
	step [174/255], loss=115.7486
	step [175/255], loss=102.4946
	step [176/255], loss=101.2811
	step [177/255], loss=116.9177
	step [178/255], loss=109.0907
	step [179/255], loss=120.3554
	step [180/255], loss=114.9935
	step [181/255], loss=122.0804
	step [182/255], loss=120.1107
	step [183/255], loss=108.1982
	step [184/255], loss=118.4921
	step [185/255], loss=108.4571
	step [186/255], loss=128.7262
	step [187/255], loss=116.0896
	step [188/255], loss=122.0175
	step [189/255], loss=124.2810
	step [190/255], loss=116.8235
	step [191/255], loss=105.6390
	step [192/255], loss=109.7485
	step [193/255], loss=124.7677
	step [194/255], loss=113.4333
	step [195/255], loss=108.8520
	step [196/255], loss=121.9465
	step [197/255], loss=117.1865
	step [198/255], loss=133.9881
	step [199/255], loss=102.1824
	step [200/255], loss=121.4641
	step [201/255], loss=119.5486
	step [202/255], loss=123.4031
	step [203/255], loss=93.3233
	step [204/255], loss=111.3268
	step [205/255], loss=124.5466
	step [206/255], loss=105.5966
	step [207/255], loss=119.0184
	step [208/255], loss=115.7530
	step [209/255], loss=115.9900
	step [210/255], loss=109.9271
	step [211/255], loss=113.9612
	step [212/255], loss=99.5466
	step [213/255], loss=97.8803
	step [214/255], loss=124.7708
	step [215/255], loss=117.5320
	step [216/255], loss=113.4470
	step [217/255], loss=108.6668
	step [218/255], loss=111.4864
	step [219/255], loss=115.7604
	step [220/255], loss=115.6379
	step [221/255], loss=120.5261
	step [222/255], loss=112.6435
	step [223/255], loss=98.8060
	step [224/255], loss=100.1901
	step [225/255], loss=117.6633
	step [226/255], loss=117.7791
	step [227/255], loss=110.9825
	step [228/255], loss=114.3282
	step [229/255], loss=94.5328
	step [230/255], loss=95.3689
	step [231/255], loss=104.7922
	step [232/255], loss=140.8258
	step [233/255], loss=120.4248
	step [234/255], loss=115.1386
	step [235/255], loss=114.4627
	step [236/255], loss=100.7502
	step [237/255], loss=103.6307
	step [238/255], loss=119.6531
	step [239/255], loss=109.7653
	step [240/255], loss=118.9142
	step [241/255], loss=126.1380
	step [242/255], loss=112.9569
	step [243/255], loss=118.5245
	step [244/255], loss=106.8587
	step [245/255], loss=109.6581
	step [246/255], loss=107.3916
	step [247/255], loss=113.3217
	step [248/255], loss=111.9349
	step [249/255], loss=91.7539
	step [250/255], loss=120.5253
	step [251/255], loss=114.7430
	step [252/255], loss=111.0215
	step [253/255], loss=103.2960
	step [254/255], loss=104.1081
	step [255/255], loss=85.1592
	Evaluating
	loss=0.2064, precision=0.3409, recall=0.9202, f1=0.4975
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/255], loss=114.7923
	step [2/255], loss=110.0213
	step [3/255], loss=120.7741
	step [4/255], loss=116.3474
	step [5/255], loss=100.2995
	step [6/255], loss=123.8334
	step [7/255], loss=99.0436
	step [8/255], loss=105.3298
	step [9/255], loss=105.4785
	step [10/255], loss=124.9930
	step [11/255], loss=112.9322
	step [12/255], loss=102.8333
	step [13/255], loss=102.5980
	step [14/255], loss=110.6880
	step [15/255], loss=107.9839
	step [16/255], loss=99.1383
	step [17/255], loss=100.1802
	step [18/255], loss=106.8671
	step [19/255], loss=105.3993
	step [20/255], loss=101.2421
	step [21/255], loss=105.1360
	step [22/255], loss=94.9968
	step [23/255], loss=95.8934
	step [24/255], loss=104.3965
	step [25/255], loss=109.2730
	step [26/255], loss=122.1371
	step [27/255], loss=99.0246
	step [28/255], loss=121.3638
	step [29/255], loss=119.0233
	step [30/255], loss=110.7381
	step [31/255], loss=101.5370
	step [32/255], loss=94.1313
	step [33/255], loss=105.7452
	step [34/255], loss=107.0092
	step [35/255], loss=117.5484
	step [36/255], loss=113.6840
	step [37/255], loss=105.8181
	step [38/255], loss=108.9309
	step [39/255], loss=96.3966
	step [40/255], loss=108.7006
	step [41/255], loss=116.3288
	step [42/255], loss=97.4378
	step [43/255], loss=109.4554
	step [44/255], loss=107.4328
	step [45/255], loss=125.8341
	step [46/255], loss=108.2828
	step [47/255], loss=120.6035
	step [48/255], loss=104.7864
	step [49/255], loss=106.4705
	step [50/255], loss=112.5495
	step [51/255], loss=124.1815
	step [52/255], loss=113.3786
	step [53/255], loss=101.8369
	step [54/255], loss=108.3662
	step [55/255], loss=107.1123
	step [56/255], loss=97.3066
	step [57/255], loss=104.5229
	step [58/255], loss=113.6216
	step [59/255], loss=114.9464
	step [60/255], loss=117.0022
	step [61/255], loss=123.3739
	step [62/255], loss=111.0486
	step [63/255], loss=99.2611
	step [64/255], loss=96.5422
	step [65/255], loss=131.0574
	step [66/255], loss=96.3218
	step [67/255], loss=108.5884
	step [68/255], loss=106.1174
	step [69/255], loss=110.5818
	step [70/255], loss=107.7440
	step [71/255], loss=113.5070
	step [72/255], loss=115.4125
	step [73/255], loss=97.5967
	step [74/255], loss=120.7111
	step [75/255], loss=110.9779
	step [76/255], loss=98.2564
	step [77/255], loss=101.5112
	step [78/255], loss=105.7526
	step [79/255], loss=103.9790
	step [80/255], loss=110.6767
	step [81/255], loss=104.5897
	step [82/255], loss=103.1298
	step [83/255], loss=103.4326
	step [84/255], loss=107.8497
	step [85/255], loss=106.6894
	step [86/255], loss=103.0405
	step [87/255], loss=92.2930
	step [88/255], loss=119.9109
	step [89/255], loss=108.6183
	step [90/255], loss=107.7502
	step [91/255], loss=103.3670
	step [92/255], loss=103.6475
	step [93/255], loss=85.0379
	step [94/255], loss=101.5904
	step [95/255], loss=93.8708
	step [96/255], loss=113.0093
	step [97/255], loss=112.4113
	step [98/255], loss=111.8101
	step [99/255], loss=112.4755
	step [100/255], loss=108.0355
	step [101/255], loss=107.9835
	step [102/255], loss=115.3713
	step [103/255], loss=102.4499
	step [104/255], loss=104.0819
	step [105/255], loss=90.4331
	step [106/255], loss=93.8489
	step [107/255], loss=103.6004
	step [108/255], loss=103.1611
	step [109/255], loss=107.9617
	step [110/255], loss=109.8906
	step [111/255], loss=99.2100
	step [112/255], loss=99.7673
	step [113/255], loss=105.7788
	step [114/255], loss=111.8438
	step [115/255], loss=105.1595
	step [116/255], loss=101.1435
	step [117/255], loss=102.7436
	step [118/255], loss=106.6366
	step [119/255], loss=113.9779
	step [120/255], loss=113.4766
	step [121/255], loss=114.3905
	step [122/255], loss=108.5102
	step [123/255], loss=106.3784
	step [124/255], loss=112.7586
	step [125/255], loss=102.1204
	step [126/255], loss=96.9514
	step [127/255], loss=121.7430
	step [128/255], loss=103.9883
	step [129/255], loss=96.9758
	step [130/255], loss=93.5147
	step [131/255], loss=98.2018
	step [132/255], loss=94.7217
	step [133/255], loss=106.4231
	step [134/255], loss=102.5600
	step [135/255], loss=99.3899
	step [136/255], loss=102.9424
	step [137/255], loss=93.6155
	step [138/255], loss=108.2693
	step [139/255], loss=101.5422
	step [140/255], loss=109.9642
	step [141/255], loss=102.5716
	step [142/255], loss=129.8587
	step [143/255], loss=121.0454
	step [144/255], loss=91.2910
	step [145/255], loss=89.6186
	step [146/255], loss=115.7133
	step [147/255], loss=102.8791
	step [148/255], loss=87.7403
	step [149/255], loss=103.8966
	step [150/255], loss=94.7343
	step [151/255], loss=104.2044
	step [152/255], loss=86.4827
	step [153/255], loss=108.1480
	step [154/255], loss=93.7036
	step [155/255], loss=95.5500
	step [156/255], loss=113.6684
	step [157/255], loss=89.1841
	step [158/255], loss=127.8210
	step [159/255], loss=106.1019
	step [160/255], loss=112.4635
	step [161/255], loss=106.4575
	step [162/255], loss=107.5123
	step [163/255], loss=112.6066
	step [164/255], loss=90.2399
	step [165/255], loss=107.4645
	step [166/255], loss=109.0256
	step [167/255], loss=86.0735
	step [168/255], loss=79.8580
	step [169/255], loss=93.9301
	step [170/255], loss=100.3082
	step [171/255], loss=101.3770
	step [172/255], loss=88.2251
	step [173/255], loss=92.6518
	step [174/255], loss=104.7302
	step [175/255], loss=97.9301
	step [176/255], loss=104.6657
	step [177/255], loss=87.7322
	step [178/255], loss=100.4243
	step [179/255], loss=94.9353
	step [180/255], loss=84.9964
	step [181/255], loss=94.7366
	step [182/255], loss=110.5829
	step [183/255], loss=92.4975
	step [184/255], loss=97.2512
	step [185/255], loss=88.4557
	step [186/255], loss=102.3742
	step [187/255], loss=104.6399
	step [188/255], loss=111.4464
	step [189/255], loss=112.4885
	step [190/255], loss=125.5704
	step [191/255], loss=104.9014
	step [192/255], loss=108.2575
	step [193/255], loss=99.6358
	step [194/255], loss=93.1945
	step [195/255], loss=91.3062
	step [196/255], loss=105.7934
	step [197/255], loss=94.5474
	step [198/255], loss=79.9401
	step [199/255], loss=93.5185
	step [200/255], loss=103.5418
	step [201/255], loss=102.3069
	step [202/255], loss=100.8635
	step [203/255], loss=106.0168
	step [204/255], loss=106.1470
	step [205/255], loss=107.8797
	step [206/255], loss=94.3679
	step [207/255], loss=117.0066
	step [208/255], loss=100.1884
	step [209/255], loss=100.8951
	step [210/255], loss=102.2899
	step [211/255], loss=96.6062
	step [212/255], loss=107.6148
	step [213/255], loss=100.4601
	step [214/255], loss=90.2718
	step [215/255], loss=100.4082
	step [216/255], loss=106.4528
	step [217/255], loss=92.5816
	step [218/255], loss=102.5792
	step [219/255], loss=102.9824
	step [220/255], loss=104.9959
	step [221/255], loss=98.4105
	step [222/255], loss=101.3498
	step [223/255], loss=102.3475
	step [224/255], loss=109.5840
	step [225/255], loss=111.5838
	step [226/255], loss=97.5888
	step [227/255], loss=105.2852
	step [228/255], loss=97.5726
	step [229/255], loss=103.0899
	step [230/255], loss=100.3999
	step [231/255], loss=103.0760
	step [232/255], loss=104.8542
	step [233/255], loss=94.8017
	step [234/255], loss=109.7351
	step [235/255], loss=88.3558
	step [236/255], loss=102.5868
	step [237/255], loss=115.5438
	step [238/255], loss=101.3919
	step [239/255], loss=84.6642
	step [240/255], loss=107.4404
	step [241/255], loss=92.6370
	step [242/255], loss=95.1383
	step [243/255], loss=107.6629
	step [244/255], loss=104.9317
	step [245/255], loss=92.8422
	step [246/255], loss=101.0352
	step [247/255], loss=104.8755
	step [248/255], loss=99.1911
	step [249/255], loss=117.0797
	step [250/255], loss=87.8511
	step [251/255], loss=104.9819
	step [252/255], loss=118.1094
	step [253/255], loss=106.8302
	step [254/255], loss=90.7563
	step [255/255], loss=89.3636
	Evaluating
	loss=0.1392, precision=0.2763, recall=0.9224, f1=0.4253
Training epoch 4
	step [1/255], loss=92.1758
	step [2/255], loss=106.9563
	step [3/255], loss=111.3326
	step [4/255], loss=97.0499
	step [5/255], loss=103.5833
	step [6/255], loss=92.6912
	step [7/255], loss=103.2480
	step [8/255], loss=91.5636
	step [9/255], loss=103.4001
	step [10/255], loss=112.1691
	step [11/255], loss=97.2432
	step [12/255], loss=116.1109
	step [13/255], loss=94.3493
	step [14/255], loss=98.8889
	step [15/255], loss=103.6975
	step [16/255], loss=100.9504
	step [17/255], loss=99.9746
	step [18/255], loss=97.7910
	step [19/255], loss=82.7738
	step [20/255], loss=97.6951
	step [21/255], loss=87.6971
	step [22/255], loss=95.0705
	step [23/255], loss=103.4275
	step [24/255], loss=97.6135
	step [25/255], loss=86.9170
	step [26/255], loss=97.5475
	step [27/255], loss=87.9468
	step [28/255], loss=102.8390
	step [29/255], loss=108.0983
	step [30/255], loss=103.6064
	step [31/255], loss=99.1503
	step [32/255], loss=84.6624
	step [33/255], loss=101.9529
	step [34/255], loss=106.8415
	step [35/255], loss=93.5024
	step [36/255], loss=99.3462
	step [37/255], loss=92.7378
	step [38/255], loss=105.4123
	step [39/255], loss=105.2176
	step [40/255], loss=96.2887
	step [41/255], loss=106.8823
	step [42/255], loss=97.8255
	step [43/255], loss=94.9977
	step [44/255], loss=95.6445
	step [45/255], loss=94.4667
	step [46/255], loss=95.1329
	step [47/255], loss=99.5148
	step [48/255], loss=97.4187
	step [49/255], loss=91.8447
	step [50/255], loss=104.6930
	step [51/255], loss=88.5547
	step [52/255], loss=102.8972
	step [53/255], loss=92.6568
	step [54/255], loss=103.4640
	step [55/255], loss=113.0183
	step [56/255], loss=83.1960
	step [57/255], loss=104.9407
	step [58/255], loss=97.0995
	step [59/255], loss=88.2118
	step [60/255], loss=96.5290
	step [61/255], loss=96.8106
	step [62/255], loss=93.8173
	step [63/255], loss=97.6875
	step [64/255], loss=85.3472
	step [65/255], loss=98.2507
	step [66/255], loss=104.3393
	step [67/255], loss=90.4953
	step [68/255], loss=97.5176
	step [69/255], loss=94.2807
	step [70/255], loss=108.0694
	step [71/255], loss=86.7213
	step [72/255], loss=105.1968
	step [73/255], loss=104.6885
	step [74/255], loss=108.9786
	step [75/255], loss=101.7765
	step [76/255], loss=92.2872
	step [77/255], loss=100.4869
	step [78/255], loss=100.5266
	step [79/255], loss=92.4070
	step [80/255], loss=101.9398
	step [81/255], loss=90.9964
	step [82/255], loss=101.3992
	step [83/255], loss=94.8000
	step [84/255], loss=96.5012
	step [85/255], loss=93.1807
	step [86/255], loss=95.2282
	step [87/255], loss=108.8040
	step [88/255], loss=88.1333
	step [89/255], loss=99.4097
	step [90/255], loss=100.6276
	step [91/255], loss=95.3658
	step [92/255], loss=97.6518
	step [93/255], loss=91.9191
	step [94/255], loss=99.9772
	step [95/255], loss=89.9092
	step [96/255], loss=98.3369
	step [97/255], loss=91.7624
	step [98/255], loss=96.0179
	step [99/255], loss=101.9416
	step [100/255], loss=95.9705
	step [101/255], loss=97.4649
	step [102/255], loss=96.7725
	step [103/255], loss=98.7425
	step [104/255], loss=97.9624
	step [105/255], loss=110.1735
	step [106/255], loss=90.6399
	step [107/255], loss=98.3607
	step [108/255], loss=99.3423
	step [109/255], loss=103.3560
	step [110/255], loss=95.6282
	step [111/255], loss=87.9962
	step [112/255], loss=105.2229
	step [113/255], loss=83.6952
	step [114/255], loss=108.4986
	step [115/255], loss=79.3078
	step [116/255], loss=85.2561
	step [117/255], loss=89.9216
	step [118/255], loss=108.8446
	step [119/255], loss=96.8632
	step [120/255], loss=102.9100
	step [121/255], loss=102.5843
	step [122/255], loss=90.0221
	step [123/255], loss=76.8177
	step [124/255], loss=96.3705
	step [125/255], loss=92.2623
	step [126/255], loss=88.7951
	step [127/255], loss=86.2372
	step [128/255], loss=84.0647
	step [129/255], loss=92.8075
	step [130/255], loss=95.5628
	step [131/255], loss=81.7401
	step [132/255], loss=86.8159
	step [133/255], loss=80.5441
	step [134/255], loss=94.1388
	step [135/255], loss=101.8789
	step [136/255], loss=97.8212
	step [137/255], loss=90.0370
	step [138/255], loss=96.8332
	step [139/255], loss=69.5030
	step [140/255], loss=97.0457
	step [141/255], loss=92.1953
	step [142/255], loss=110.3480
	step [143/255], loss=102.9373
	step [144/255], loss=92.1805
	step [145/255], loss=90.6330
	step [146/255], loss=91.7142
	step [147/255], loss=97.6967
	step [148/255], loss=90.8840
	step [149/255], loss=81.0681
	step [150/255], loss=91.6417
	step [151/255], loss=98.6662
	step [152/255], loss=90.4688
	step [153/255], loss=103.1852
	step [154/255], loss=88.0679
	step [155/255], loss=94.5791
	step [156/255], loss=93.4924
	step [157/255], loss=105.3134
	step [158/255], loss=84.4166
	step [159/255], loss=97.2122
	step [160/255], loss=99.6000
	step [161/255], loss=103.5040
	step [162/255], loss=109.6634
	step [163/255], loss=86.9284
	step [164/255], loss=104.6901
	step [165/255], loss=92.5202
	step [166/255], loss=86.3353
	step [167/255], loss=93.3503
	step [168/255], loss=111.7500
	step [169/255], loss=110.1694
	step [170/255], loss=96.3931
	step [171/255], loss=88.3257
	step [172/255], loss=95.2111
	step [173/255], loss=96.2112
	step [174/255], loss=84.2339
	step [175/255], loss=94.2053
	step [176/255], loss=90.1798
	step [177/255], loss=95.3157
	step [178/255], loss=98.8705
	step [179/255], loss=82.8884
	step [180/255], loss=89.2758
	step [181/255], loss=81.2122
	step [182/255], loss=83.0745
	step [183/255], loss=91.2731
	step [184/255], loss=101.6614
	step [185/255], loss=87.2453
	step [186/255], loss=106.9710
	step [187/255], loss=83.0037
	step [188/255], loss=105.3490
	step [189/255], loss=86.7727
	step [190/255], loss=98.0347
	step [191/255], loss=90.4644
	step [192/255], loss=97.9119
	step [193/255], loss=94.5553
	step [194/255], loss=94.2387
	step [195/255], loss=96.3046
	step [196/255], loss=76.3632
	step [197/255], loss=83.5673
	step [198/255], loss=98.5952
	step [199/255], loss=96.9508
	step [200/255], loss=94.7999
	step [201/255], loss=79.2002
	step [202/255], loss=93.2105
	step [203/255], loss=104.9298
	step [204/255], loss=111.0941
	step [205/255], loss=86.2316
	step [206/255], loss=94.7891
	step [207/255], loss=86.2354
	step [208/255], loss=97.0004
	step [209/255], loss=95.8906
	step [210/255], loss=98.0319
	step [211/255], loss=91.0775
	step [212/255], loss=96.0161
	step [213/255], loss=105.9100
	step [214/255], loss=80.1395
	step [215/255], loss=84.1094
	step [216/255], loss=99.1448
	step [217/255], loss=91.6682
	step [218/255], loss=102.3526
	step [219/255], loss=92.3463
	step [220/255], loss=91.2370
	step [221/255], loss=87.7628
	step [222/255], loss=90.0208
	step [223/255], loss=80.1275
	step [224/255], loss=96.6124
	step [225/255], loss=96.0089
	step [226/255], loss=91.9127
	step [227/255], loss=85.2841
	step [228/255], loss=89.3298
	step [229/255], loss=93.2809
	step [230/255], loss=92.2635
	step [231/255], loss=87.8810
	step [232/255], loss=90.8088
	step [233/255], loss=104.7876
	step [234/255], loss=106.1244
	step [235/255], loss=102.4288
	step [236/255], loss=96.8487
	step [237/255], loss=88.9080
	step [238/255], loss=100.9421
	step [239/255], loss=89.1872
	step [240/255], loss=87.3924
	step [241/255], loss=85.3452
	step [242/255], loss=80.4515
	step [243/255], loss=92.1986
	step [244/255], loss=88.3505
	step [245/255], loss=106.1332
	step [246/255], loss=92.6022
	step [247/255], loss=110.8930
	step [248/255], loss=94.0535
	step [249/255], loss=90.5883
	step [250/255], loss=89.9188
	step [251/255], loss=81.5275
	step [252/255], loss=101.0799
	step [253/255], loss=97.4317
	step [254/255], loss=87.5198
	step [255/255], loss=75.5659
	Evaluating
	loss=0.1002, precision=0.3168, recall=0.9251, f1=0.4719
Training epoch 5
	step [1/255], loss=94.2307
	step [2/255], loss=95.6255
	step [3/255], loss=100.2849
	step [4/255], loss=91.5690
	step [5/255], loss=85.5927
	step [6/255], loss=86.3043
	step [7/255], loss=94.3046
	step [8/255], loss=100.6196
	step [9/255], loss=85.5569
	step [10/255], loss=91.1395
	step [11/255], loss=86.1338
	step [12/255], loss=96.6697
	step [13/255], loss=102.2881
	step [14/255], loss=89.6766
	step [15/255], loss=74.2678
	step [16/255], loss=85.2485
	step [17/255], loss=89.1379
	step [18/255], loss=88.3490
	step [19/255], loss=93.7044
	step [20/255], loss=87.3624
	step [21/255], loss=101.7561
	step [22/255], loss=88.0982
	step [23/255], loss=93.0175
	step [24/255], loss=89.8023
	step [25/255], loss=97.3795
	step [26/255], loss=100.0423
	step [27/255], loss=83.3865
	step [28/255], loss=96.7378
	step [29/255], loss=115.5035
	step [30/255], loss=84.1438
	step [31/255], loss=90.4382
	step [32/255], loss=92.8735
	step [33/255], loss=91.7082
	step [34/255], loss=85.3799
	step [35/255], loss=101.3430
	step [36/255], loss=90.3898
	step [37/255], loss=83.2216
	step [38/255], loss=89.7800
	step [39/255], loss=91.9002
	step [40/255], loss=90.5501
	step [41/255], loss=101.8588
	step [42/255], loss=83.1906
	step [43/255], loss=102.5548
	step [44/255], loss=87.8573
	step [45/255], loss=89.2998
	step [46/255], loss=75.3222
	step [47/255], loss=87.8538
	step [48/255], loss=88.9629
	step [49/255], loss=110.7597
	step [50/255], loss=102.0779
	step [51/255], loss=82.3152
	step [52/255], loss=87.4282
	step [53/255], loss=92.3216
	step [54/255], loss=113.1264
	step [55/255], loss=85.1618
	step [56/255], loss=94.5963
	step [57/255], loss=104.4570
	step [58/255], loss=80.8810
	step [59/255], loss=81.6449
	step [60/255], loss=73.2185
	step [61/255], loss=92.8566
	step [62/255], loss=84.5974
	step [63/255], loss=102.3361
	step [64/255], loss=82.8715
	step [65/255], loss=96.4758
	step [66/255], loss=88.7812
	step [67/255], loss=77.0537
	step [68/255], loss=97.3664
	step [69/255], loss=86.1717
	step [70/255], loss=99.4528
	step [71/255], loss=101.7128
	step [72/255], loss=93.2576
	step [73/255], loss=87.0086
	step [74/255], loss=96.4095
	step [75/255], loss=86.3455
	step [76/255], loss=103.9839
	step [77/255], loss=87.3700
	step [78/255], loss=97.6621
	step [79/255], loss=78.5295
	step [80/255], loss=95.6617
	step [81/255], loss=95.5741
	step [82/255], loss=97.1505
	step [83/255], loss=71.1790
	step [84/255], loss=101.7794
	step [85/255], loss=96.5440
	step [86/255], loss=93.9077
	step [87/255], loss=79.8317
	step [88/255], loss=101.6177
	step [89/255], loss=86.3036
	step [90/255], loss=102.3745
	step [91/255], loss=87.6627
	step [92/255], loss=79.3693
	step [93/255], loss=99.7517
	step [94/255], loss=88.5049
	step [95/255], loss=102.6489
	step [96/255], loss=91.7710
	step [97/255], loss=91.8895
	step [98/255], loss=91.6498
	step [99/255], loss=86.0165
	step [100/255], loss=92.2100
	step [101/255], loss=82.2319
	step [102/255], loss=97.6814
	step [103/255], loss=83.0233
	step [104/255], loss=84.9893
	step [105/255], loss=80.2547
	step [106/255], loss=87.4051
	step [107/255], loss=81.5590
	step [108/255], loss=90.3281
	step [109/255], loss=98.3763
	step [110/255], loss=79.8148
	step [111/255], loss=96.7233
	step [112/255], loss=80.1250
	step [113/255], loss=95.8490
	step [114/255], loss=88.8527
	step [115/255], loss=85.8554
	step [116/255], loss=94.9851
	step [117/255], loss=91.5136
	step [118/255], loss=81.7008
	step [119/255], loss=89.5608
	step [120/255], loss=85.5883
	step [121/255], loss=90.7828
	step [122/255], loss=110.1667
	step [123/255], loss=97.3988
	step [124/255], loss=106.2893
	step [125/255], loss=81.8265
	step [126/255], loss=87.7193
	step [127/255], loss=79.0009
	step [128/255], loss=87.1925
	step [129/255], loss=75.9872
	step [130/255], loss=91.0029
	step [131/255], loss=82.4364
	step [132/255], loss=72.2951
	step [133/255], loss=82.2337
	step [134/255], loss=77.0413
	step [135/255], loss=78.3348
	step [136/255], loss=95.4659
	step [137/255], loss=78.1277
	step [138/255], loss=108.8267
	step [139/255], loss=87.2753
	step [140/255], loss=95.7771
	step [141/255], loss=96.7673
	step [142/255], loss=82.5962
	step [143/255], loss=81.0276
	step [144/255], loss=93.8818
	step [145/255], loss=92.2858
	step [146/255], loss=85.0848
	step [147/255], loss=91.4639
	step [148/255], loss=90.2913
	step [149/255], loss=103.1444
	step [150/255], loss=95.5336
	step [151/255], loss=83.4272
	step [152/255], loss=86.6661
	step [153/255], loss=90.8806
	step [154/255], loss=94.1854
	step [155/255], loss=92.3652
	step [156/255], loss=92.3235
	step [157/255], loss=99.8321
	step [158/255], loss=87.0430
	step [159/255], loss=86.0557
	step [160/255], loss=84.8980
	step [161/255], loss=80.3248
	step [162/255], loss=99.2280
	step [163/255], loss=78.3790
	step [164/255], loss=103.7982
	step [165/255], loss=86.2847
	step [166/255], loss=69.5293
	step [167/255], loss=86.3926
	step [168/255], loss=90.9170
	step [169/255], loss=93.1637
	step [170/255], loss=104.8043
	step [171/255], loss=82.5563
	step [172/255], loss=82.1471
	step [173/255], loss=98.3123
	step [174/255], loss=98.8901
	step [175/255], loss=89.8895
	step [176/255], loss=91.0459
	step [177/255], loss=97.4485
	step [178/255], loss=95.0103
	step [179/255], loss=78.1763
	step [180/255], loss=89.9199
	step [181/255], loss=74.1533
	step [182/255], loss=96.1189
	step [183/255], loss=72.8392
	step [184/255], loss=80.0978
	step [185/255], loss=83.2018
	step [186/255], loss=98.3845
	step [187/255], loss=77.3661
	step [188/255], loss=88.6454
	step [189/255], loss=93.7874
	step [190/255], loss=104.8461
	step [191/255], loss=87.8383
	step [192/255], loss=100.0936
	step [193/255], loss=102.0333
	step [194/255], loss=92.0789
	step [195/255], loss=88.4370
	step [196/255], loss=81.8597
	step [197/255], loss=77.2464
	step [198/255], loss=97.9166
	step [199/255], loss=77.7205
	step [200/255], loss=73.7792
	step [201/255], loss=100.1893
	step [202/255], loss=92.0240
	step [203/255], loss=105.4711
	step [204/255], loss=92.1619
	step [205/255], loss=95.3721
	step [206/255], loss=80.8489
	step [207/255], loss=86.4268
	step [208/255], loss=97.8567
	step [209/255], loss=90.6256
	step [210/255], loss=104.7900
	step [211/255], loss=90.0519
	step [212/255], loss=89.3820
	step [213/255], loss=98.3227
	step [214/255], loss=85.1005
	step [215/255], loss=88.2749
	step [216/255], loss=98.5836
	step [217/255], loss=82.1265
	step [218/255], loss=99.9834
	step [219/255], loss=95.6070
	step [220/255], loss=98.7222
	step [221/255], loss=62.7910
	step [222/255], loss=92.0806
	step [223/255], loss=95.2026
	step [224/255], loss=91.0148
	step [225/255], loss=94.9536
	step [226/255], loss=91.9344
	step [227/255], loss=85.5082
	step [228/255], loss=87.4577
	step [229/255], loss=75.2372
	step [230/255], loss=78.9124
	step [231/255], loss=91.4170
	step [232/255], loss=83.2884
	step [233/255], loss=85.4966
	step [234/255], loss=80.3572
	step [235/255], loss=103.0526
	step [236/255], loss=91.2193
	step [237/255], loss=88.9597
	step [238/255], loss=93.8709
	step [239/255], loss=86.3571
	step [240/255], loss=84.7869
	step [241/255], loss=91.6453
	step [242/255], loss=102.1736
	step [243/255], loss=80.7862
	step [244/255], loss=80.6312
	step [245/255], loss=78.1621
	step [246/255], loss=81.4177
	step [247/255], loss=70.0022
	step [248/255], loss=86.4321
	step [249/255], loss=77.0953
	step [250/255], loss=78.4162
	step [251/255], loss=84.4334
	step [252/255], loss=89.5468
	step [253/255], loss=96.9803
	step [254/255], loss=95.6858
	step [255/255], loss=68.8228
	Evaluating
	loss=0.0742, precision=0.3047, recall=0.9293, f1=0.4589
Training epoch 6
	step [1/255], loss=90.0220
	step [2/255], loss=82.8080
	step [3/255], loss=82.7124
	step [4/255], loss=87.7398
	step [5/255], loss=78.0869
	step [6/255], loss=85.8253
	step [7/255], loss=76.2804
	step [8/255], loss=83.8377
	step [9/255], loss=92.1263
	step [10/255], loss=78.3781
	step [11/255], loss=101.0021
	step [12/255], loss=82.7524
	step [13/255], loss=85.6828
	step [14/255], loss=89.1154
	step [15/255], loss=78.9880
	step [16/255], loss=110.9196
	step [17/255], loss=90.2878
	step [18/255], loss=109.0174
	step [19/255], loss=81.4982
	step [20/255], loss=80.8508
	step [21/255], loss=77.4598
	step [22/255], loss=80.6574
	step [23/255], loss=89.4587
	step [24/255], loss=75.9277
	step [25/255], loss=88.5667
	step [26/255], loss=96.1052
	step [27/255], loss=94.6683
	step [28/255], loss=81.6228
	step [29/255], loss=91.6327
	step [30/255], loss=76.2315
	step [31/255], loss=80.5744
	step [32/255], loss=91.3895
	step [33/255], loss=79.9672
	step [34/255], loss=84.8288
	step [35/255], loss=78.6578
	step [36/255], loss=82.8020
	step [37/255], loss=94.6928
	step [38/255], loss=78.9943
	step [39/255], loss=86.5482
	step [40/255], loss=82.3713
	step [41/255], loss=81.4276
	step [42/255], loss=98.8837
	step [43/255], loss=87.2594
	step [44/255], loss=83.9221
	step [45/255], loss=102.4290
	step [46/255], loss=86.8922
	step [47/255], loss=88.6193
	step [48/255], loss=90.8127
	step [49/255], loss=86.1812
	step [50/255], loss=86.3154
	step [51/255], loss=81.5323
	step [52/255], loss=86.0081
	step [53/255], loss=86.3663
	step [54/255], loss=75.9594
	step [55/255], loss=84.0562
	step [56/255], loss=94.5067
	step [57/255], loss=77.8422
	step [58/255], loss=77.5413
	step [59/255], loss=94.1696
	step [60/255], loss=74.3406
	step [61/255], loss=79.3758
	step [62/255], loss=89.0791
	step [63/255], loss=85.7215
	step [64/255], loss=86.5798
	step [65/255], loss=81.3061
	step [66/255], loss=90.4855
	step [67/255], loss=75.7435
	step [68/255], loss=93.2943
	step [69/255], loss=84.3276
	step [70/255], loss=81.4386
	step [71/255], loss=76.8012
	step [72/255], loss=95.2774
	step [73/255], loss=94.6921
	step [74/255], loss=81.7734
	step [75/255], loss=92.2826
	step [76/255], loss=92.1422
	step [77/255], loss=92.1427
	step [78/255], loss=75.4693
	step [79/255], loss=72.8172
	step [80/255], loss=83.1192
	step [81/255], loss=79.2789
	step [82/255], loss=91.6800
	step [83/255], loss=94.4680
	step [84/255], loss=90.1037
	step [85/255], loss=84.6182
	step [86/255], loss=83.3389
	step [87/255], loss=82.4160
	step [88/255], loss=83.1393
	step [89/255], loss=80.9350
	step [90/255], loss=111.2375
	step [91/255], loss=77.7447
	step [92/255], loss=89.4799
	step [93/255], loss=85.3251
	step [94/255], loss=96.1873
	step [95/255], loss=81.8621
	step [96/255], loss=90.9006
	step [97/255], loss=91.5694
	step [98/255], loss=81.5338
	step [99/255], loss=88.9686
	step [100/255], loss=79.7620
	step [101/255], loss=102.5257
	step [102/255], loss=88.6915
	step [103/255], loss=101.1418
	step [104/255], loss=77.1379
	step [105/255], loss=78.8025
	step [106/255], loss=80.8234
	step [107/255], loss=71.3630
	step [108/255], loss=81.3533
	step [109/255], loss=84.0296
	step [110/255], loss=97.7494
	step [111/255], loss=96.9092
	step [112/255], loss=77.5996
	step [113/255], loss=78.9493
	step [114/255], loss=89.8461
	step [115/255], loss=75.4294
	step [116/255], loss=89.2026
	step [117/255], loss=89.9046
	step [118/255], loss=89.5347
	step [119/255], loss=95.0299
	step [120/255], loss=87.2497
	step [121/255], loss=89.7182
	step [122/255], loss=90.6524
	step [123/255], loss=94.1407
	step [124/255], loss=99.9112
	step [125/255], loss=95.8934
	step [126/255], loss=86.0573
	step [127/255], loss=87.8802
	step [128/255], loss=74.7456
	step [129/255], loss=86.5156
	step [130/255], loss=95.6642
	step [131/255], loss=80.7804
	step [132/255], loss=88.5537
	step [133/255], loss=83.8261
	step [134/255], loss=86.6566
	step [135/255], loss=84.6239
	step [136/255], loss=86.1598
	step [137/255], loss=96.1195
	step [138/255], loss=81.6031
	step [139/255], loss=88.1730
	step [140/255], loss=98.3868
	step [141/255], loss=92.5842
	step [142/255], loss=82.6648
	step [143/255], loss=82.1024
	step [144/255], loss=91.5926
	step [145/255], loss=90.7179
	step [146/255], loss=91.5989
	step [147/255], loss=90.2007
	step [148/255], loss=91.7054
	step [149/255], loss=84.7630
	step [150/255], loss=94.0636
	step [151/255], loss=85.0129
	step [152/255], loss=80.8276
	step [153/255], loss=84.9781
	step [154/255], loss=83.9460
	step [155/255], loss=70.7819
	step [156/255], loss=77.9382
	step [157/255], loss=89.9596
	step [158/255], loss=92.0119
	step [159/255], loss=94.8311
	step [160/255], loss=75.4783
	step [161/255], loss=81.0250
	step [162/255], loss=83.1197
	step [163/255], loss=95.3318
	step [164/255], loss=87.3710
	step [165/255], loss=87.9126
	step [166/255], loss=87.6102
	step [167/255], loss=89.9726
	step [168/255], loss=109.6351
	step [169/255], loss=83.0318
	step [170/255], loss=69.6495
	step [171/255], loss=84.9281
	step [172/255], loss=96.4608
	step [173/255], loss=89.6136
	step [174/255], loss=97.4256
	step [175/255], loss=89.5953
	step [176/255], loss=90.0864
	step [177/255], loss=80.7566
	step [178/255], loss=94.2407
	step [179/255], loss=85.1419
	step [180/255], loss=91.7408
	step [181/255], loss=71.7668
	step [182/255], loss=86.8703
	step [183/255], loss=86.2480
	step [184/255], loss=95.7989
	step [185/255], loss=78.1959
	step [186/255], loss=99.3267
	step [187/255], loss=87.5879
	step [188/255], loss=86.3493
	step [189/255], loss=77.4003
	step [190/255], loss=85.7655
	step [191/255], loss=97.3581
	step [192/255], loss=92.9212
	step [193/255], loss=78.6949
	step [194/255], loss=83.2173
	step [195/255], loss=81.2249
	step [196/255], loss=80.1701
	step [197/255], loss=102.7512
	step [198/255], loss=77.2636
	step [199/255], loss=88.8828
	step [200/255], loss=83.6292
	step [201/255], loss=85.6650
	step [202/255], loss=85.6082
	step [203/255], loss=80.9397
	step [204/255], loss=75.1765
	step [205/255], loss=87.1733
	step [206/255], loss=86.4489
	step [207/255], loss=79.6464
	step [208/255], loss=109.5354
	step [209/255], loss=97.4589
	step [210/255], loss=89.1513
	step [211/255], loss=96.4348
	step [212/255], loss=79.0102
	step [213/255], loss=70.4978
	step [214/255], loss=82.1205
	step [215/255], loss=73.6882
	step [216/255], loss=81.1290
	step [217/255], loss=86.2461
	step [218/255], loss=90.3958
	step [219/255], loss=95.3094
	step [220/255], loss=79.8475
	step [221/255], loss=71.1245
	step [222/255], loss=81.2216
	step [223/255], loss=88.3374
	step [224/255], loss=97.0354
	step [225/255], loss=76.7770
	step [226/255], loss=78.5921
	step [227/255], loss=89.9526
	step [228/255], loss=70.6154
	step [229/255], loss=91.2153
	step [230/255], loss=79.1105
	step [231/255], loss=97.5151
	step [232/255], loss=97.2081
	step [233/255], loss=88.6703
	step [234/255], loss=78.6690
	step [235/255], loss=68.9069
	step [236/255], loss=78.5809
	step [237/255], loss=78.9665
	step [238/255], loss=84.1440
	step [239/255], loss=81.2267
	step [240/255], loss=80.2260
	step [241/255], loss=78.9491
	step [242/255], loss=83.3874
	step [243/255], loss=76.8840
	step [244/255], loss=85.7735
	step [245/255], loss=86.9869
	step [246/255], loss=73.1490
	step [247/255], loss=85.1625
	step [248/255], loss=87.7575
	step [249/255], loss=91.5198
	step [250/255], loss=72.9729
	step [251/255], loss=93.4054
	step [252/255], loss=87.9504
	step [253/255], loss=83.3726
	step [254/255], loss=75.6546
	step [255/255], loss=66.9248
	Evaluating
	loss=0.0542, precision=0.4067, recall=0.9181, f1=0.5637
saving model as: 0_saved_model.pth
Training epoch 7
	step [1/255], loss=95.6123
	step [2/255], loss=86.1878
	step [3/255], loss=79.0236
	step [4/255], loss=84.9411
	step [5/255], loss=85.7236
	step [6/255], loss=87.9174
	step [7/255], loss=82.2377
	step [8/255], loss=76.9032
	step [9/255], loss=79.8049
	step [10/255], loss=85.4674
	step [11/255], loss=103.4480
	step [12/255], loss=93.9984
	step [13/255], loss=91.4119
	step [14/255], loss=85.1396
	step [15/255], loss=76.2513
	step [16/255], loss=81.0274
	step [17/255], loss=97.6067
	step [18/255], loss=79.6158
	step [19/255], loss=98.4908
	step [20/255], loss=84.0328
	step [21/255], loss=84.2084
	step [22/255], loss=86.7969
	step [23/255], loss=82.7180
	step [24/255], loss=78.9598
	step [25/255], loss=79.0141
	step [26/255], loss=67.8564
	step [27/255], loss=93.0643
	step [28/255], loss=88.2458
	step [29/255], loss=101.7151
	step [30/255], loss=86.2600
	step [31/255], loss=68.3976
	step [32/255], loss=80.7004
	step [33/255], loss=86.4044
	step [34/255], loss=89.3549
	step [35/255], loss=72.5537
	step [36/255], loss=77.8303
	step [37/255], loss=90.8500
	step [38/255], loss=93.5784
	step [39/255], loss=86.4861
	step [40/255], loss=89.0758
	step [41/255], loss=77.8569
	step [42/255], loss=77.6605
	step [43/255], loss=78.4784
	step [44/255], loss=83.8456
	step [45/255], loss=86.2399
	step [46/255], loss=83.3603
	step [47/255], loss=78.6281
	step [48/255], loss=81.5034
	step [49/255], loss=81.8464
	step [50/255], loss=98.3581
	step [51/255], loss=97.4379
	step [52/255], loss=86.7150
	step [53/255], loss=91.3131
	step [54/255], loss=90.7601
	step [55/255], loss=73.5585
	step [56/255], loss=79.4624
	step [57/255], loss=86.8168
	step [58/255], loss=102.3193
	step [59/255], loss=86.7888
	step [60/255], loss=68.2446
	step [61/255], loss=83.2692
	step [62/255], loss=76.3054
	step [63/255], loss=89.1115
	step [64/255], loss=87.5421
	step [65/255], loss=89.0668
	step [66/255], loss=80.9867
	step [67/255], loss=74.3661
	step [68/255], loss=84.4736
	step [69/255], loss=80.9268
	step [70/255], loss=96.5631
	step [71/255], loss=86.9266
	step [72/255], loss=71.0549
	step [73/255], loss=73.8849
	step [74/255], loss=100.9713
	step [75/255], loss=87.4723
	step [76/255], loss=97.4289
	step [77/255], loss=76.2662
	step [78/255], loss=77.7432
	step [79/255], loss=79.5557
	step [80/255], loss=89.9339
	step [81/255], loss=89.1870
	step [82/255], loss=84.6307
	step [83/255], loss=95.5736
	step [84/255], loss=90.6733
	step [85/255], loss=93.2807
	step [86/255], loss=106.0764
	step [87/255], loss=85.4539
	step [88/255], loss=75.5328
	step [89/255], loss=84.8603
	step [90/255], loss=83.2849
	step [91/255], loss=75.9812
	step [92/255], loss=98.4279
	step [93/255], loss=84.9984
	step [94/255], loss=90.2199
	step [95/255], loss=91.9426
	step [96/255], loss=92.0199
	step [97/255], loss=81.5199
	step [98/255], loss=58.2723
	step [99/255], loss=73.6460
	step [100/255], loss=81.9004
	step [101/255], loss=87.8363
	step [102/255], loss=72.5364
	step [103/255], loss=83.4940
	step [104/255], loss=76.6375
	step [105/255], loss=92.9340
	step [106/255], loss=81.2299
	step [107/255], loss=73.8511
	step [108/255], loss=91.2525
	step [109/255], loss=84.2679
	step [110/255], loss=86.0871
	step [111/255], loss=86.7030
	step [112/255], loss=85.1901
	step [113/255], loss=79.1265
	step [114/255], loss=81.3155
	step [115/255], loss=82.7964
	step [116/255], loss=97.4402
	step [117/255], loss=78.6676
	step [118/255], loss=77.4129
	step [119/255], loss=89.6454
	step [120/255], loss=82.8162
	step [121/255], loss=74.0473
	step [122/255], loss=80.3785
	step [123/255], loss=75.4380
	step [124/255], loss=76.2755
	step [125/255], loss=83.8707
	step [126/255], loss=73.5886
	step [127/255], loss=81.9331
	step [128/255], loss=82.7888
	step [129/255], loss=71.4171
	step [130/255], loss=66.4446
	step [131/255], loss=96.1791
	step [132/255], loss=80.4499
	step [133/255], loss=94.0873
	step [134/255], loss=94.4553
	step [135/255], loss=67.9302
	step [136/255], loss=101.3319
	step [137/255], loss=82.1814
	step [138/255], loss=86.0746
	step [139/255], loss=82.8751
	step [140/255], loss=89.2178
	step [141/255], loss=86.7740
	step [142/255], loss=82.5880
	step [143/255], loss=90.3067
	step [144/255], loss=83.2394
	step [145/255], loss=94.6851
	step [146/255], loss=88.8750
	step [147/255], loss=74.0933
	step [148/255], loss=85.7158
	step [149/255], loss=90.9120
	step [150/255], loss=82.4064
	step [151/255], loss=82.7251
	step [152/255], loss=90.3222
	step [153/255], loss=100.1946
	step [154/255], loss=78.8775
	step [155/255], loss=93.1238
	step [156/255], loss=75.5851
	step [157/255], loss=90.5595
	step [158/255], loss=86.2277
	step [159/255], loss=87.0261
	step [160/255], loss=83.3656
	step [161/255], loss=87.8466
	step [162/255], loss=77.2845
	step [163/255], loss=77.7599
	step [164/255], loss=76.4661
	step [165/255], loss=74.7210
	step [166/255], loss=69.5684
	step [167/255], loss=83.0510
	step [168/255], loss=76.6059
	step [169/255], loss=81.9209
	step [170/255], loss=69.3795
	step [171/255], loss=87.5670
	step [172/255], loss=75.4690
	step [173/255], loss=80.8121
	step [174/255], loss=79.9570
	step [175/255], loss=90.9132
	step [176/255], loss=94.1357
	step [177/255], loss=92.0944
	step [178/255], loss=94.0141
	step [179/255], loss=74.5648
	step [180/255], loss=79.8555
	step [181/255], loss=87.8989
	step [182/255], loss=82.5594
	step [183/255], loss=84.1362
	step [184/255], loss=82.2017
	step [185/255], loss=81.4311
	step [186/255], loss=74.5914
	step [187/255], loss=82.6751
	step [188/255], loss=78.9398
	step [189/255], loss=93.6447
	step [190/255], loss=70.5395
	step [191/255], loss=87.4337
	step [192/255], loss=83.3493
	step [193/255], loss=85.5879
	step [194/255], loss=71.4617
	step [195/255], loss=67.6510
	step [196/255], loss=86.3240
	step [197/255], loss=104.4044
	step [198/255], loss=85.2415
	step [199/255], loss=84.3390
	step [200/255], loss=79.5967
	step [201/255], loss=82.6093
	step [202/255], loss=80.3715
	step [203/255], loss=100.9497
	step [204/255], loss=74.7920
	step [205/255], loss=80.7359
	step [206/255], loss=81.5726
	step [207/255], loss=74.2651
	step [208/255], loss=78.9739
	step [209/255], loss=72.3704
	step [210/255], loss=83.7653
	step [211/255], loss=85.9087
	step [212/255], loss=67.0041
	step [213/255], loss=64.0356
	step [214/255], loss=89.4113
	step [215/255], loss=93.8003
	step [216/255], loss=84.5895
	step [217/255], loss=76.6339
	step [218/255], loss=77.3290
	step [219/255], loss=69.2103
	step [220/255], loss=91.9369
	step [221/255], loss=90.9492
	step [222/255], loss=73.4748
	step [223/255], loss=89.8738
	step [224/255], loss=81.9079
	step [225/255], loss=79.9526
	step [226/255], loss=69.1154
	step [227/255], loss=87.5504
	step [228/255], loss=81.0752
	step [229/255], loss=80.5863
	step [230/255], loss=77.5897
	step [231/255], loss=82.1294
	step [232/255], loss=74.6797
	step [233/255], loss=79.5196
	step [234/255], loss=83.6821
	step [235/255], loss=79.5426
	step [236/255], loss=86.7973
	step [237/255], loss=89.9681
	step [238/255], loss=93.3780
	step [239/255], loss=65.2804
	step [240/255], loss=81.6970
	step [241/255], loss=70.6907
	step [242/255], loss=77.3163
	step [243/255], loss=73.8958
	step [244/255], loss=66.0964
	step [245/255], loss=86.1171
	step [246/255], loss=69.8662
	step [247/255], loss=82.2002
	step [248/255], loss=88.3686
	step [249/255], loss=68.1835
	step [250/255], loss=75.7187
	step [251/255], loss=99.8520
	step [252/255], loss=71.8271
	step [253/255], loss=88.4494
	step [254/255], loss=91.4787
	step [255/255], loss=69.7695
	Evaluating
	loss=0.0425, precision=0.3478, recall=0.9012, f1=0.5019
Training epoch 8
	step [1/255], loss=68.7267
	step [2/255], loss=85.5180
	step [3/255], loss=90.2653
	step [4/255], loss=78.5101
	step [5/255], loss=70.6437
	step [6/255], loss=83.8552
	step [7/255], loss=80.3401
	step [8/255], loss=69.2009
	step [9/255], loss=84.2409
	step [10/255], loss=84.0488
	step [11/255], loss=87.5835
	step [12/255], loss=85.2564
	step [13/255], loss=64.2209
	step [14/255], loss=90.5283
	step [15/255], loss=94.6196
	step [16/255], loss=79.3371
	step [17/255], loss=79.8577
	step [18/255], loss=84.0851
	step [19/255], loss=77.3075
	step [20/255], loss=88.1294
	step [21/255], loss=90.2268
	step [22/255], loss=89.3744
	step [23/255], loss=79.8430
	step [24/255], loss=91.6411
	step [25/255], loss=76.3870
	step [26/255], loss=81.0933
	step [27/255], loss=73.9780
	step [28/255], loss=82.7001
	step [29/255], loss=83.1036
	step [30/255], loss=87.7348
	step [31/255], loss=82.9663
	step [32/255], loss=82.2198
	step [33/255], loss=77.1279
	step [34/255], loss=86.5842
	step [35/255], loss=94.6523
	step [36/255], loss=82.9981
	step [37/255], loss=78.8411
	step [38/255], loss=80.9643
	step [39/255], loss=85.6186
	step [40/255], loss=81.9072
	step [41/255], loss=80.5803
	step [42/255], loss=82.7900
	step [43/255], loss=80.3426
	step [44/255], loss=78.3775
	step [45/255], loss=77.8700
	step [46/255], loss=87.5958
	step [47/255], loss=81.1821
	step [48/255], loss=82.5782
	step [49/255], loss=75.7766
	step [50/255], loss=72.3325
	step [51/255], loss=76.7624
	step [52/255], loss=87.0988
	step [53/255], loss=74.0109
	step [54/255], loss=72.3024
	step [55/255], loss=82.5056
	step [56/255], loss=81.4003
	step [57/255], loss=72.5062
	step [58/255], loss=80.5801
	step [59/255], loss=73.3305
	step [60/255], loss=83.8194
	step [61/255], loss=69.4731
	step [62/255], loss=76.0349
	step [63/255], loss=100.5213
	step [64/255], loss=87.5284
	step [65/255], loss=83.4552
	step [66/255], loss=68.3051
	step [67/255], loss=73.0213
	step [68/255], loss=76.1184
	step [69/255], loss=70.1893
	step [70/255], loss=77.1501
	step [71/255], loss=72.8394
	step [72/255], loss=86.0736
	step [73/255], loss=79.4414
	step [74/255], loss=71.2316
	step [75/255], loss=96.9896
	step [76/255], loss=83.9176
	step [77/255], loss=67.5246
	step [78/255], loss=98.5372
	step [79/255], loss=75.7196
	step [80/255], loss=82.9174
	step [81/255], loss=81.8788
	step [82/255], loss=75.8657
	step [83/255], loss=80.9162
	step [84/255], loss=82.4159
	step [85/255], loss=91.2570
	step [86/255], loss=84.4689
	step [87/255], loss=73.3760
	step [88/255], loss=68.4299
	step [89/255], loss=94.8300
	step [90/255], loss=76.4167
	step [91/255], loss=76.7614
	step [92/255], loss=90.5997
	step [93/255], loss=76.0276
	step [94/255], loss=76.1594
	step [95/255], loss=96.1867
	step [96/255], loss=84.8559
	step [97/255], loss=80.1407
	step [98/255], loss=83.5264
	step [99/255], loss=87.2223
	step [100/255], loss=93.2108
	step [101/255], loss=86.5953
	step [102/255], loss=78.6097
	step [103/255], loss=78.5223
	step [104/255], loss=73.4659
	step [105/255], loss=88.9201
	step [106/255], loss=74.2785
	step [107/255], loss=85.6926
	step [108/255], loss=88.5599
	step [109/255], loss=76.1025
	step [110/255], loss=77.4118
	step [111/255], loss=98.7961
	step [112/255], loss=77.9043
	step [113/255], loss=76.5155
	step [114/255], loss=83.3562
	step [115/255], loss=71.9374
	step [116/255], loss=99.4700
	step [117/255], loss=74.3637
	step [118/255], loss=85.4586
	step [119/255], loss=80.7316
	step [120/255], loss=90.5247
	step [121/255], loss=71.0663
	step [122/255], loss=84.2434
	step [123/255], loss=69.7269
	step [124/255], loss=67.4920
	step [125/255], loss=82.3534
	step [126/255], loss=99.6634
	step [127/255], loss=88.3674
	step [128/255], loss=82.8227
	step [129/255], loss=82.9683
	step [130/255], loss=78.6037
	step [131/255], loss=75.2875
	step [132/255], loss=80.1486
	step [133/255], loss=83.5552
	step [134/255], loss=86.0280
	step [135/255], loss=88.1452
	step [136/255], loss=73.8831
	step [137/255], loss=80.7955
	step [138/255], loss=83.0381
	step [139/255], loss=77.2126
	step [140/255], loss=76.4338
	step [141/255], loss=79.4027
	step [142/255], loss=78.6041
	step [143/255], loss=93.6328
	step [144/255], loss=80.0033
	step [145/255], loss=86.8851
	step [146/255], loss=69.6619
	step [147/255], loss=85.1765
	step [148/255], loss=86.8211
	step [149/255], loss=73.2509
	step [150/255], loss=86.7834
	step [151/255], loss=95.5759
	step [152/255], loss=76.1661
	step [153/255], loss=83.9486
	step [154/255], loss=77.2829
	step [155/255], loss=82.2490
	step [156/255], loss=67.4044
	step [157/255], loss=99.0527
	step [158/255], loss=62.1745
	step [159/255], loss=91.2131
	step [160/255], loss=86.3994
	step [161/255], loss=87.3747
	step [162/255], loss=88.2471
	step [163/255], loss=83.2698
	step [164/255], loss=93.7620
	step [165/255], loss=67.8859
	step [166/255], loss=78.6671
	step [167/255], loss=84.5595
	step [168/255], loss=82.1067
	step [169/255], loss=76.3172
	step [170/255], loss=82.7286
	step [171/255], loss=77.9330
	step [172/255], loss=69.2132
	step [173/255], loss=73.7257
	step [174/255], loss=93.8419
	step [175/255], loss=68.8609
	step [176/255], loss=80.5568
	step [177/255], loss=99.4962
	step [178/255], loss=70.9818
	step [179/255], loss=85.1284
	step [180/255], loss=82.0794
	step [181/255], loss=76.6195
	step [182/255], loss=90.3005
	step [183/255], loss=82.5241
	step [184/255], loss=81.3503
	step [185/255], loss=84.2433
	step [186/255], loss=87.3343
	step [187/255], loss=80.7593
	step [188/255], loss=78.6689
	step [189/255], loss=71.5590
	step [190/255], loss=67.2671
	step [191/255], loss=79.1540
	step [192/255], loss=83.2758
	step [193/255], loss=62.0945
	step [194/255], loss=87.4784
	step [195/255], loss=85.5767
	step [196/255], loss=81.1725
	step [197/255], loss=76.5327
	step [198/255], loss=76.7018
	step [199/255], loss=95.8708
	step [200/255], loss=78.5232
	step [201/255], loss=88.5263
	step [202/255], loss=71.4117
	step [203/255], loss=73.7545
	step [204/255], loss=82.4921
	step [205/255], loss=72.6492
	step [206/255], loss=80.1762
	step [207/255], loss=90.7485
	step [208/255], loss=78.3636
	step [209/255], loss=92.8464
	step [210/255], loss=79.1034
	step [211/255], loss=87.8607
	step [212/255], loss=71.4934
	step [213/255], loss=86.4839
	step [214/255], loss=85.3580
	step [215/255], loss=76.7601
	step [216/255], loss=78.5355
	step [217/255], loss=80.3018
	step [218/255], loss=86.6270
	step [219/255], loss=78.7523
	step [220/255], loss=93.1490
	step [221/255], loss=71.9311
	step [222/255], loss=73.0707
	step [223/255], loss=95.1397
	step [224/255], loss=84.3160
	step [225/255], loss=79.4929
	step [226/255], loss=75.7179
	step [227/255], loss=88.4207
	step [228/255], loss=71.1947
	step [229/255], loss=92.1845
	step [230/255], loss=66.2246
	step [231/255], loss=68.5374
	step [232/255], loss=82.8640
	step [233/255], loss=87.1767
	step [234/255], loss=79.0442
	step [235/255], loss=76.6728
	step [236/255], loss=83.6926
	step [237/255], loss=76.0737
	step [238/255], loss=82.9067
	step [239/255], loss=85.1614
	step [240/255], loss=78.0179
	step [241/255], loss=78.3598
	step [242/255], loss=81.2164
	step [243/255], loss=67.7184
	step [244/255], loss=88.6099
	step [245/255], loss=67.8896
	step [246/255], loss=86.8773
	step [247/255], loss=80.8970
	step [248/255], loss=85.7273
	step [249/255], loss=83.1519
	step [250/255], loss=80.3373
	step [251/255], loss=66.2297
	step [252/255], loss=76.4179
	step [253/255], loss=80.1135
	step [254/255], loss=64.0021
	step [255/255], loss=65.6985
	Evaluating
	loss=0.0356, precision=0.3623, recall=0.9076, f1=0.5179
Training epoch 9
	step [1/255], loss=77.5246
	step [2/255], loss=73.1458
	step [3/255], loss=88.6255
	step [4/255], loss=76.2773
	step [5/255], loss=90.1920
	step [6/255], loss=88.7972
	step [7/255], loss=82.3729
	step [8/255], loss=77.7907
	step [9/255], loss=77.1506
	step [10/255], loss=77.8399
	step [11/255], loss=80.3917
	step [12/255], loss=79.0982
	step [13/255], loss=79.5152
	step [14/255], loss=88.7873
	step [15/255], loss=79.2127
	step [16/255], loss=93.3526
	step [17/255], loss=66.3964
	step [18/255], loss=89.8520
	step [19/255], loss=74.6395
	step [20/255], loss=80.9845
	step [21/255], loss=75.1907
	step [22/255], loss=87.1483
	step [23/255], loss=70.7448
	step [24/255], loss=64.2563
	step [25/255], loss=74.9791
	step [26/255], loss=88.0303
	step [27/255], loss=97.3985
	step [28/255], loss=76.2790
	step [29/255], loss=83.1133
	step [30/255], loss=76.3970
	step [31/255], loss=64.7400
	step [32/255], loss=81.8537
	step [33/255], loss=90.6527
	step [34/255], loss=93.8552
	step [35/255], loss=92.8392
	step [36/255], loss=84.5609
	step [37/255], loss=82.2739
	step [38/255], loss=98.0880
	step [39/255], loss=87.1272
	step [40/255], loss=74.5150
	step [41/255], loss=79.5702
	step [42/255], loss=82.4063
	step [43/255], loss=88.1021
	step [44/255], loss=78.8792
	step [45/255], loss=75.0431
	step [46/255], loss=71.1856
	step [47/255], loss=83.9462
	step [48/255], loss=64.4715
	step [49/255], loss=80.6046
	step [50/255], loss=74.4052
	step [51/255], loss=70.9671
	step [52/255], loss=75.7556
	step [53/255], loss=84.7366
	step [54/255], loss=72.2429
	step [55/255], loss=68.0523
	step [56/255], loss=78.4384
	step [57/255], loss=90.8791
	step [58/255], loss=80.3751
	step [59/255], loss=84.8966
	step [60/255], loss=69.8270
	step [61/255], loss=84.4651
	step [62/255], loss=80.8285
	step [63/255], loss=93.5720
	step [64/255], loss=63.5518
	step [65/255], loss=81.5155
	step [66/255], loss=79.5799
	step [67/255], loss=72.5423
	step [68/255], loss=89.0690
	step [69/255], loss=75.6302
	step [70/255], loss=72.4445
	step [71/255], loss=74.5367
	step [72/255], loss=88.4552
	step [73/255], loss=97.7720
	step [74/255], loss=78.6707
	step [75/255], loss=76.4479
	step [76/255], loss=82.2298
	step [77/255], loss=76.2627
	step [78/255], loss=69.7881
	step [79/255], loss=73.8884
	step [80/255], loss=84.8869
	step [81/255], loss=90.0324
	step [82/255], loss=83.3631
	step [83/255], loss=65.6565
	step [84/255], loss=88.1143
	step [85/255], loss=70.0407
	step [86/255], loss=84.5385
	step [87/255], loss=85.9855
	step [88/255], loss=70.8373
	step [89/255], loss=70.8745
	step [90/255], loss=92.1154
	step [91/255], loss=97.0258
	step [92/255], loss=78.7754
	step [93/255], loss=76.3028
	step [94/255], loss=81.4293
	step [95/255], loss=73.9490
	step [96/255], loss=89.7684
	step [97/255], loss=75.5299
	step [98/255], loss=78.2021
	step [99/255], loss=90.1934
	step [100/255], loss=79.1505
	step [101/255], loss=73.9270
	step [102/255], loss=63.9851
	step [103/255], loss=86.9248
	step [104/255], loss=73.4950
	step [105/255], loss=74.2486
	step [106/255], loss=75.3565
	step [107/255], loss=89.3108
	step [108/255], loss=82.5553
	step [109/255], loss=80.4148
	step [110/255], loss=82.7692
	step [111/255], loss=67.8152
	step [112/255], loss=83.2770
	step [113/255], loss=76.6881
	step [114/255], loss=80.7487
	step [115/255], loss=85.1367
	step [116/255], loss=88.7339
	step [117/255], loss=79.8279
	step [118/255], loss=97.8892
	step [119/255], loss=88.8442
	step [120/255], loss=82.5965
	step [121/255], loss=87.0117
	step [122/255], loss=89.3115
	step [123/255], loss=63.6010
	step [124/255], loss=88.7218
	step [125/255], loss=91.4053
	step [126/255], loss=79.7816
	step [127/255], loss=81.9492
	step [128/255], loss=88.0147
	step [129/255], loss=69.1834
	step [130/255], loss=78.0090
	step [131/255], loss=71.4793
	step [132/255], loss=90.7665
	step [133/255], loss=78.4890
	step [134/255], loss=85.1869
	step [135/255], loss=77.5156
	step [136/255], loss=77.8451
	step [137/255], loss=77.4933
	step [138/255], loss=79.7659
	step [139/255], loss=92.2890
	step [140/255], loss=71.6806
	step [141/255], loss=79.9837
	step [142/255], loss=84.2166
	step [143/255], loss=73.9057
	step [144/255], loss=78.6461
	step [145/255], loss=87.4975
	step [146/255], loss=70.9030
	step [147/255], loss=92.2071
	step [148/255], loss=76.3460
	step [149/255], loss=71.4769
	step [150/255], loss=73.1639
	step [151/255], loss=86.8762
	step [152/255], loss=70.2225
	step [153/255], loss=87.4276
	step [154/255], loss=84.9337
	step [155/255], loss=71.9512
	step [156/255], loss=80.9464
	step [157/255], loss=86.0332
	step [158/255], loss=78.3594
	step [159/255], loss=91.1214
	step [160/255], loss=80.3374
	step [161/255], loss=85.0913
	step [162/255], loss=81.5264
	step [163/255], loss=70.0370
	step [164/255], loss=73.9220
	step [165/255], loss=74.9752
	step [166/255], loss=70.7949
	step [167/255], loss=91.1686
	step [168/255], loss=72.7613
	step [169/255], loss=71.3473
	step [170/255], loss=79.3845
	step [171/255], loss=74.9743
	step [172/255], loss=81.5639
	step [173/255], loss=97.0819
	step [174/255], loss=79.8684
	step [175/255], loss=79.8219
	step [176/255], loss=79.8392
	step [177/255], loss=76.9014
	step [178/255], loss=80.1256
	step [179/255], loss=80.6884
	step [180/255], loss=78.5470
	step [181/255], loss=77.5585
	step [182/255], loss=76.1166
	step [183/255], loss=88.3567
	step [184/255], loss=89.9019
	step [185/255], loss=76.9487
	step [186/255], loss=78.7134
	step [187/255], loss=78.8954
	step [188/255], loss=96.2876
	step [189/255], loss=76.4173
	step [190/255], loss=93.4465
	step [191/255], loss=72.9923
	step [192/255], loss=70.4358
	step [193/255], loss=77.5557
	step [194/255], loss=72.6703
	step [195/255], loss=70.5875
	step [196/255], loss=79.4444
	step [197/255], loss=81.5914
	step [198/255], loss=78.7039
	step [199/255], loss=82.5562
	step [200/255], loss=71.5402
	step [201/255], loss=88.9065
	step [202/255], loss=74.0735
	step [203/255], loss=79.1029
	step [204/255], loss=80.6425
	step [205/255], loss=86.5238
	step [206/255], loss=81.0605
	step [207/255], loss=70.8963
	step [208/255], loss=87.8895
	step [209/255], loss=69.6073
	step [210/255], loss=84.1413
	step [211/255], loss=79.0362
	step [212/255], loss=82.0465
	step [213/255], loss=78.8351
	step [214/255], loss=87.0553
	step [215/255], loss=87.3984
	step [216/255], loss=75.8573
	step [217/255], loss=82.0372
	step [218/255], loss=69.5238
	step [219/255], loss=88.6154
	step [220/255], loss=85.4593
	step [221/255], loss=87.0146
	step [222/255], loss=72.9836
	step [223/255], loss=87.6200
	step [224/255], loss=93.5893
	step [225/255], loss=63.4822
	step [226/255], loss=80.1628
	step [227/255], loss=63.6549
	step [228/255], loss=72.6636
	step [229/255], loss=74.8899
	step [230/255], loss=69.2827
	step [231/255], loss=69.6105
	step [232/255], loss=88.1963
	step [233/255], loss=84.1785
	step [234/255], loss=69.1263
	step [235/255], loss=69.5116
	step [236/255], loss=80.4851
	step [237/255], loss=64.5380
	step [238/255], loss=77.3396
	step [239/255], loss=75.9918
	step [240/255], loss=85.8248
	step [241/255], loss=83.2942
	step [242/255], loss=72.1044
	step [243/255], loss=68.5754
	step [244/255], loss=75.8333
	step [245/255], loss=84.3502
	step [246/255], loss=73.2390
	step [247/255], loss=68.6643
	step [248/255], loss=74.2342
	step [249/255], loss=65.2386
	step [250/255], loss=70.5364
	step [251/255], loss=72.9672
	step [252/255], loss=83.2605
	step [253/255], loss=73.8416
	step [254/255], loss=70.3702
	step [255/255], loss=61.3911
	Evaluating
	loss=0.0326, precision=0.3331, recall=0.9269, f1=0.4901
Training epoch 10
	step [1/255], loss=79.9671
	step [2/255], loss=88.1759
	step [3/255], loss=83.3648
	step [4/255], loss=88.8302
	step [5/255], loss=86.4867
	step [6/255], loss=96.2290
	step [7/255], loss=81.2173
	step [8/255], loss=79.0480
	step [9/255], loss=82.0713
	step [10/255], loss=73.4579
	step [11/255], loss=80.1528
	step [12/255], loss=90.9529
	step [13/255], loss=73.4853
	step [14/255], loss=70.2432
	step [15/255], loss=102.1863
	step [16/255], loss=74.6672
	step [17/255], loss=74.2033
	step [18/255], loss=82.5623
	step [19/255], loss=72.6699
	step [20/255], loss=95.9461
	step [21/255], loss=71.3857
	step [22/255], loss=66.5615
	step [23/255], loss=83.0491
	step [24/255], loss=61.4485
	step [25/255], loss=73.1073
	step [26/255], loss=75.7949
	step [27/255], loss=75.7096
	step [28/255], loss=67.6888
	step [29/255], loss=73.8817
	step [30/255], loss=76.3612
	step [31/255], loss=86.7611
	step [32/255], loss=79.8949
	step [33/255], loss=81.9955
	step [34/255], loss=70.2366
	step [35/255], loss=82.2013
	step [36/255], loss=85.6602
	step [37/255], loss=81.7752
	step [38/255], loss=81.1544
	step [39/255], loss=74.3109
	step [40/255], loss=81.7405
	step [41/255], loss=71.4615
	step [42/255], loss=84.2596
	step [43/255], loss=91.5739
	step [44/255], loss=77.2235
	step [45/255], loss=83.5123
	step [46/255], loss=71.5572
	step [47/255], loss=69.0254
	step [48/255], loss=75.9807
	step [49/255], loss=80.4677
	step [50/255], loss=90.7475
	step [51/255], loss=72.8875
	step [52/255], loss=74.3986
	step [53/255], loss=87.6874
	step [54/255], loss=72.4291
	step [55/255], loss=73.8313
	step [56/255], loss=77.3627
	step [57/255], loss=71.0937
	step [58/255], loss=80.9650
	step [59/255], loss=81.8939
	step [60/255], loss=74.3049
	step [61/255], loss=71.8713
	step [62/255], loss=72.6969
	step [63/255], loss=77.8067
	step [64/255], loss=84.2391
	step [65/255], loss=80.7669
	step [66/255], loss=70.7595
	step [67/255], loss=87.8778
	step [68/255], loss=86.0593
	step [69/255], loss=78.6400
	step [70/255], loss=83.4066
	step [71/255], loss=98.6183
	step [72/255], loss=84.0525
	step [73/255], loss=80.8578
	step [74/255], loss=82.8337
	step [75/255], loss=61.9215
	step [76/255], loss=73.5461
	step [77/255], loss=76.9130
	step [78/255], loss=85.4652
	step [79/255], loss=80.6292
	step [80/255], loss=77.7354
	step [81/255], loss=87.3320
	step [82/255], loss=82.7090
	step [83/255], loss=79.4703
	step [84/255], loss=72.3702
	step [85/255], loss=78.7673
	step [86/255], loss=68.8764
	step [87/255], loss=79.3365
	step [88/255], loss=70.6814
	step [89/255], loss=77.9613
	step [90/255], loss=70.7488
	step [91/255], loss=83.1191
	step [92/255], loss=76.7933
	step [93/255], loss=81.1190
	step [94/255], loss=75.9719
	step [95/255], loss=75.5244
	step [96/255], loss=74.6148
	step [97/255], loss=84.5856
	step [98/255], loss=69.5693
	step [99/255], loss=74.2242
	step [100/255], loss=78.7907
	step [101/255], loss=65.0028
	step [102/255], loss=81.2700
	step [103/255], loss=73.8249
	step [104/255], loss=85.8392
	step [105/255], loss=78.8877
	step [106/255], loss=77.5566
	step [107/255], loss=86.8511
	step [108/255], loss=87.9937
	step [109/255], loss=82.8156
	step [110/255], loss=69.8980
	step [111/255], loss=80.3248
	step [112/255], loss=95.2979
	step [113/255], loss=76.3599
	step [114/255], loss=83.4287
	step [115/255], loss=71.5917
	step [116/255], loss=71.0657
	step [117/255], loss=82.5451
	step [118/255], loss=79.4637
	step [119/255], loss=91.6485
	step [120/255], loss=68.2478
	step [121/255], loss=87.0443
	step [122/255], loss=86.9003
	step [123/255], loss=89.0712
	step [124/255], loss=77.2307
	step [125/255], loss=82.5930
	step [126/255], loss=83.4955
	step [127/255], loss=74.0484
	step [128/255], loss=68.2593
	step [129/255], loss=79.0200
	step [130/255], loss=78.0854
	step [131/255], loss=81.1212
	step [132/255], loss=79.0965
	step [133/255], loss=78.8262
	step [134/255], loss=89.8609
	step [135/255], loss=73.4623
	step [136/255], loss=84.0395
	step [137/255], loss=80.8813
	step [138/255], loss=82.7849
	step [139/255], loss=77.1957
	step [140/255], loss=88.7819
	step [141/255], loss=80.0382
	step [142/255], loss=70.3328
	step [143/255], loss=86.8823
	step [144/255], loss=72.2078
	step [145/255], loss=76.2246
	step [146/255], loss=72.5222
	step [147/255], loss=78.6684
	step [148/255], loss=64.8992
	step [149/255], loss=65.8698
	step [150/255], loss=69.9373
	step [151/255], loss=77.1707
	step [152/255], loss=79.9834
	step [153/255], loss=79.5520
	step [154/255], loss=96.4850
	step [155/255], loss=75.9259
	step [156/255], loss=71.5367
	step [157/255], loss=67.7797
	step [158/255], loss=74.0002
	step [159/255], loss=86.6021
	step [160/255], loss=73.2200
	step [161/255], loss=70.6603
	step [162/255], loss=77.0523
	step [163/255], loss=69.1235
	step [164/255], loss=85.4248
	step [165/255], loss=84.8225
	step [166/255], loss=74.9414
	step [167/255], loss=79.8412
	step [168/255], loss=95.3704
	step [169/255], loss=79.6242
	step [170/255], loss=86.4616
	step [171/255], loss=81.7514
	step [172/255], loss=77.5005
	step [173/255], loss=81.0186
	step [174/255], loss=81.9608
	step [175/255], loss=65.0813
	step [176/255], loss=79.8043
	step [177/255], loss=87.7974
	step [178/255], loss=73.5038
	step [179/255], loss=73.0042
	step [180/255], loss=67.7295
	step [181/255], loss=77.5861
	step [182/255], loss=71.6566
	step [183/255], loss=86.9900
	step [184/255], loss=69.9298
	step [185/255], loss=69.2854
	step [186/255], loss=93.2455
	step [187/255], loss=85.8549
	step [188/255], loss=84.8975
	step [189/255], loss=79.6058
	step [190/255], loss=72.8787
	step [191/255], loss=69.8160
	step [192/255], loss=64.2190
	step [193/255], loss=67.6572
	step [194/255], loss=72.0193
	step [195/255], loss=81.9416
	step [196/255], loss=66.2450
	step [197/255], loss=65.0747
	step [198/255], loss=86.0537
	step [199/255], loss=75.6112
	step [200/255], loss=83.8361
	step [201/255], loss=86.5987
	step [202/255], loss=77.4105
	step [203/255], loss=79.2560
	step [204/255], loss=66.6431
	step [205/255], loss=75.3823
	step [206/255], loss=65.8052
	step [207/255], loss=94.8228
	step [208/255], loss=73.1833
	step [209/255], loss=67.9296
	step [210/255], loss=67.1295
	step [211/255], loss=80.7721
	step [212/255], loss=72.1351
	step [213/255], loss=80.3155
	step [214/255], loss=87.4435
	step [215/255], loss=78.0533
	step [216/255], loss=71.5290
	step [217/255], loss=78.6382
	step [218/255], loss=68.0428
	step [219/255], loss=75.5328
	step [220/255], loss=72.1350
	step [221/255], loss=80.4013
	step [222/255], loss=71.1690
	step [223/255], loss=70.5867
	step [224/255], loss=77.5526
	step [225/255], loss=77.4453
	step [226/255], loss=66.4207
	step [227/255], loss=76.8600
	step [228/255], loss=79.7018
	step [229/255], loss=93.1864
	step [230/255], loss=85.7573
	step [231/255], loss=62.5577
	step [232/255], loss=83.5347
	step [233/255], loss=74.7231
	step [234/255], loss=74.6708
	step [235/255], loss=84.1274
	step [236/255], loss=80.8636
	step [237/255], loss=85.1437
	step [238/255], loss=71.6174
	step [239/255], loss=79.9286
	step [240/255], loss=76.2615
	step [241/255], loss=75.0629
	step [242/255], loss=74.7571
	step [243/255], loss=86.3300
	step [244/255], loss=64.8684
	step [245/255], loss=82.4760
	step [246/255], loss=84.2215
	step [247/255], loss=69.0178
	step [248/255], loss=80.2956
	step [249/255], loss=76.5569
	step [250/255], loss=74.7045
	step [251/255], loss=74.2972
	step [252/255], loss=74.8079
	step [253/255], loss=71.5526
	step [254/255], loss=72.0097
	step [255/255], loss=71.0422
	Evaluating
	loss=0.0252, precision=0.3411, recall=0.9174, f1=0.4973
Training epoch 11
	step [1/255], loss=71.4596
	step [2/255], loss=96.1814
	step [3/255], loss=74.4081
	step [4/255], loss=80.2760
	step [5/255], loss=77.4061
	step [6/255], loss=65.4985
	step [7/255], loss=82.4858
	step [8/255], loss=80.7316
	step [9/255], loss=80.6882
	step [10/255], loss=77.1067
	step [11/255], loss=81.3397
	step [12/255], loss=79.4966
	step [13/255], loss=73.3865
	step [14/255], loss=69.7866
	step [15/255], loss=68.5566
	step [16/255], loss=76.1006
	step [17/255], loss=67.8519
	step [18/255], loss=79.3520
	step [19/255], loss=72.1015
	step [20/255], loss=78.6133
	step [21/255], loss=85.3774
	step [22/255], loss=71.3726
	step [23/255], loss=89.6065
	step [24/255], loss=72.4680
	step [25/255], loss=77.5374
	step [26/255], loss=65.7749
	step [27/255], loss=81.6910
	step [28/255], loss=87.1485
	step [29/255], loss=79.4220
	step [30/255], loss=76.7679
	step [31/255], loss=76.6763
	step [32/255], loss=74.8763
	step [33/255], loss=79.8501
	step [34/255], loss=70.0721
	step [35/255], loss=77.6714
	step [36/255], loss=80.6155
	step [37/255], loss=70.4667
	step [38/255], loss=83.2513
	step [39/255], loss=81.7307
	step [40/255], loss=72.3652
	step [41/255], loss=74.9400
	step [42/255], loss=77.8917
	step [43/255], loss=80.2249
	step [44/255], loss=76.7793
	step [45/255], loss=74.3521
	step [46/255], loss=82.6690
	step [47/255], loss=83.4774
	step [48/255], loss=81.2739
	step [49/255], loss=65.6585
	step [50/255], loss=78.6659
	step [51/255], loss=89.7791
	step [52/255], loss=92.2025
	step [53/255], loss=76.7152
	step [54/255], loss=71.9442
	step [55/255], loss=85.7294
	step [56/255], loss=74.2924
	step [57/255], loss=70.0009
	step [58/255], loss=78.4566
	step [59/255], loss=80.7889
	step [60/255], loss=75.9696
	step [61/255], loss=82.0801
	step [62/255], loss=70.0213
	step [63/255], loss=75.7705
	step [64/255], loss=72.9518
	step [65/255], loss=89.3765
	step [66/255], loss=83.5109
	step [67/255], loss=69.2996
	step [68/255], loss=74.8200
	step [69/255], loss=77.6561
	step [70/255], loss=71.6928
	step [71/255], loss=74.1922
	step [72/255], loss=78.1493
	step [73/255], loss=80.4507
	step [74/255], loss=70.9208
	step [75/255], loss=79.3556
	step [76/255], loss=70.8669
	step [77/255], loss=85.9868
	step [78/255], loss=69.5014
	step [79/255], loss=83.4366
	step [80/255], loss=61.5213
	step [81/255], loss=61.9299
	step [82/255], loss=72.5455
	step [83/255], loss=77.7731
	step [84/255], loss=82.1387
	step [85/255], loss=90.6976
	step [86/255], loss=75.9355
	step [87/255], loss=82.7631
	step [88/255], loss=69.9507
	step [89/255], loss=77.9247
	step [90/255], loss=87.6558
	step [91/255], loss=76.7934
	step [92/255], loss=80.8054
	step [93/255], loss=77.8760
	step [94/255], loss=72.5323
	step [95/255], loss=86.4275
	step [96/255], loss=75.5410
	step [97/255], loss=97.0366
	step [98/255], loss=90.3722
	step [99/255], loss=69.0189
	step [100/255], loss=79.3071
	step [101/255], loss=69.0322
	step [102/255], loss=78.7955
	step [103/255], loss=82.0377
	step [104/255], loss=98.3084
	step [105/255], loss=62.2276
	step [106/255], loss=69.1849
	step [107/255], loss=71.3703
	step [108/255], loss=70.0983
	step [109/255], loss=72.1591
	step [110/255], loss=82.5618
	step [111/255], loss=85.9293
	step [112/255], loss=75.5892
	step [113/255], loss=86.5392
	step [114/255], loss=90.0734
	step [115/255], loss=68.9713
	step [116/255], loss=96.2881
	step [117/255], loss=78.8405
	step [118/255], loss=85.4814
	step [119/255], loss=74.2862
	step [120/255], loss=77.8895
	step [121/255], loss=93.8326
	step [122/255], loss=78.4382
	step [123/255], loss=64.5763
	step [124/255], loss=70.3210
	step [125/255], loss=73.2012
	step [126/255], loss=91.3637
	step [127/255], loss=65.7368
	step [128/255], loss=91.5931
	step [129/255], loss=75.4205
	step [130/255], loss=74.0266
	step [131/255], loss=75.8663
	step [132/255], loss=78.5996
	step [133/255], loss=91.6954
	step [134/255], loss=79.6015
	step [135/255], loss=67.5441
	step [136/255], loss=77.1245
	step [137/255], loss=75.3246
	step [138/255], loss=89.1390
	step [139/255], loss=70.9133
	step [140/255], loss=63.6992
	step [141/255], loss=77.0019
	step [142/255], loss=76.8637
	step [143/255], loss=67.3383
	step [144/255], loss=67.3328
	step [145/255], loss=88.0444
	step [146/255], loss=75.4270
	step [147/255], loss=75.9143
	step [148/255], loss=91.1260
	step [149/255], loss=69.4247
	step [150/255], loss=79.1009
	step [151/255], loss=79.4160
	step [152/255], loss=67.4799
	step [153/255], loss=77.4154
	step [154/255], loss=67.2758
	step [155/255], loss=78.1303
	step [156/255], loss=78.2956
	step [157/255], loss=80.2866
	step [158/255], loss=56.1818
	step [159/255], loss=66.9177
	step [160/255], loss=84.4724
	step [161/255], loss=74.0687
	step [162/255], loss=69.4923
	step [163/255], loss=69.1360
	step [164/255], loss=79.9756
	step [165/255], loss=74.1008
	step [166/255], loss=87.4538
	step [167/255], loss=80.8562
	step [168/255], loss=72.1357
	step [169/255], loss=77.0301
	step [170/255], loss=76.0551
	step [171/255], loss=80.4625
	step [172/255], loss=66.9926
	step [173/255], loss=65.9383
	step [174/255], loss=76.9276
	step [175/255], loss=80.6832
	step [176/255], loss=70.1163
	step [177/255], loss=70.9800
	step [178/255], loss=77.7917
	step [179/255], loss=79.0170
	step [180/255], loss=65.6656
	step [181/255], loss=68.9065
	step [182/255], loss=74.7011
	step [183/255], loss=63.0477
	step [184/255], loss=75.7444
	step [185/255], loss=65.6958
	step [186/255], loss=74.5264
	step [187/255], loss=77.7101
	step [188/255], loss=73.8963
	step [189/255], loss=72.4239
	step [190/255], loss=78.4855
	step [191/255], loss=91.1672
	step [192/255], loss=90.5791
	step [193/255], loss=72.4279
	step [194/255], loss=76.5716
	step [195/255], loss=78.6660
	step [196/255], loss=84.1058
	step [197/255], loss=79.9111
	step [198/255], loss=70.1976
	step [199/255], loss=74.3861
	step [200/255], loss=73.6288
	step [201/255], loss=72.8821
	step [202/255], loss=83.8613
	step [203/255], loss=76.4805
	step [204/255], loss=62.7797
	step [205/255], loss=78.8434
	step [206/255], loss=80.7010
	step [207/255], loss=76.6257
	step [208/255], loss=71.2203
	step [209/255], loss=71.8153
	step [210/255], loss=88.0394
	step [211/255], loss=74.1653
	step [212/255], loss=65.7605
	step [213/255], loss=83.3255
	step [214/255], loss=79.0094
	step [215/255], loss=72.0416
	step [216/255], loss=71.5152
	step [217/255], loss=70.0231
	step [218/255], loss=76.2991
	step [219/255], loss=79.7697
	step [220/255], loss=76.8369
	step [221/255], loss=69.4663
	step [222/255], loss=88.0448
	step [223/255], loss=70.7654
	step [224/255], loss=75.2457
	step [225/255], loss=70.8089
	step [226/255], loss=81.1174
	step [227/255], loss=73.4521
	step [228/255], loss=71.6399
	step [229/255], loss=72.8297
	step [230/255], loss=70.0548
	step [231/255], loss=83.5977
	step [232/255], loss=95.8307
	step [233/255], loss=84.7599
	step [234/255], loss=83.5374
	step [235/255], loss=72.9299
	step [236/255], loss=82.2692
	step [237/255], loss=73.2218
	step [238/255], loss=71.7055
	step [239/255], loss=79.0892
	step [240/255], loss=83.6754
	step [241/255], loss=66.7386
	step [242/255], loss=72.5038
	step [243/255], loss=70.5384
	step [244/255], loss=88.7324
	step [245/255], loss=84.2079
	step [246/255], loss=69.0307
	step [247/255], loss=81.9777
	step [248/255], loss=63.3769
	step [249/255], loss=73.1340
	step [250/255], loss=86.3738
	step [251/255], loss=84.9224
	step [252/255], loss=66.2240
	step [253/255], loss=58.0998
	step [254/255], loss=74.2853
	step [255/255], loss=62.8947
	Evaluating
	loss=0.0210, precision=0.3701, recall=0.9153, f1=0.5271
Training epoch 12
	step [1/255], loss=88.0635
	step [2/255], loss=73.8892
	step [3/255], loss=72.2358
	step [4/255], loss=65.4544
	step [5/255], loss=74.5415
	step [6/255], loss=76.9968
	step [7/255], loss=70.4228
	step [8/255], loss=75.1591
	step [9/255], loss=73.8803
	step [10/255], loss=80.8125
	step [11/255], loss=70.3485
	step [12/255], loss=70.9295
	step [13/255], loss=68.5468
	step [14/255], loss=75.9303
	step [15/255], loss=69.5640
	step [16/255], loss=75.7683
	step [17/255], loss=79.5869
	step [18/255], loss=79.9205
	step [19/255], loss=70.0783
	step [20/255], loss=84.7390
	step [21/255], loss=74.9795
	step [22/255], loss=72.4502
	step [23/255], loss=72.5843
	step [24/255], loss=77.9840
	step [25/255], loss=82.1700
	step [26/255], loss=75.6086
	step [27/255], loss=87.5766
	step [28/255], loss=70.1390
	step [29/255], loss=71.2406
	step [30/255], loss=72.8545
	step [31/255], loss=81.5898
	step [32/255], loss=92.0751
	step [33/255], loss=80.1524
	step [34/255], loss=73.1253
	step [35/255], loss=72.0426
	step [36/255], loss=64.8993
	step [37/255], loss=72.3733
	step [38/255], loss=72.5457
	step [39/255], loss=71.6492
	step [40/255], loss=73.9034
	step [41/255], loss=81.9787
	step [42/255], loss=79.4762
	step [43/255], loss=66.3548
	step [44/255], loss=70.3127
	step [45/255], loss=68.6201
	step [46/255], loss=74.9850
	step [47/255], loss=79.5692
	step [48/255], loss=76.8635
	step [49/255], loss=75.1544
	step [50/255], loss=64.4980
	step [51/255], loss=84.3952
	step [52/255], loss=83.0064
	step [53/255], loss=79.1689
	step [54/255], loss=89.3642
	step [55/255], loss=77.8329
	step [56/255], loss=71.9876
	step [57/255], loss=90.2149
	step [58/255], loss=72.0294
	step [59/255], loss=95.3503
	step [60/255], loss=87.3947
	step [61/255], loss=75.4736
	step [62/255], loss=67.1860
	step [63/255], loss=78.4542
	step [64/255], loss=70.1009
	step [65/255], loss=71.8105
	step [66/255], loss=86.8650
	step [67/255], loss=69.7923
	step [68/255], loss=68.7870
	step [69/255], loss=81.1827
	step [70/255], loss=74.3734
	step [71/255], loss=77.3768
	step [72/255], loss=71.9864
	step [73/255], loss=67.2412
	step [74/255], loss=83.7175
	step [75/255], loss=65.7854
	step [76/255], loss=71.7378
	step [77/255], loss=67.6250
	step [78/255], loss=64.2777
	step [79/255], loss=81.7379
	step [80/255], loss=68.7524
	step [81/255], loss=80.7250
	step [82/255], loss=72.8708
	step [83/255], loss=71.4255
	step [84/255], loss=74.7814
	step [85/255], loss=69.4958
	step [86/255], loss=64.1372
	step [87/255], loss=67.1496
	step [88/255], loss=67.8566
	step [89/255], loss=76.0835
	step [90/255], loss=90.8993
	step [91/255], loss=71.9679
	step [92/255], loss=72.0031
	step [93/255], loss=72.1958
	step [94/255], loss=81.7252
	step [95/255], loss=79.0949
	step [96/255], loss=72.4734
	step [97/255], loss=85.0175
	step [98/255], loss=75.5819
	step [99/255], loss=83.3085
	step [100/255], loss=89.1694
	step [101/255], loss=81.0147
	step [102/255], loss=90.9170
	step [103/255], loss=72.4435
	step [104/255], loss=64.9106
	step [105/255], loss=78.4053
	step [106/255], loss=67.1850
	step [107/255], loss=82.7153
	step [108/255], loss=69.0065
	step [109/255], loss=89.9342
	step [110/255], loss=75.5810
	step [111/255], loss=79.4355
	step [112/255], loss=70.7961
	step [113/255], loss=86.0967
	step [114/255], loss=75.2264
	step [115/255], loss=68.7927
	step [116/255], loss=77.9298
	step [117/255], loss=72.7120
	step [118/255], loss=76.9061
	step [119/255], loss=98.2492
	step [120/255], loss=63.0831
	step [121/255], loss=82.8995
	step [122/255], loss=80.0218
	step [123/255], loss=77.3353
	step [124/255], loss=66.3288
	step [125/255], loss=74.2282
	step [126/255], loss=73.9414
	step [127/255], loss=66.9072
	step [128/255], loss=84.9920
	step [129/255], loss=75.6789
	step [130/255], loss=78.4745
	step [131/255], loss=79.4555
	step [132/255], loss=78.2669
	step [133/255], loss=77.1721
	step [134/255], loss=86.9347
	step [135/255], loss=73.1872
	step [136/255], loss=77.0073
	step [137/255], loss=82.3673
	step [138/255], loss=67.4757
	step [139/255], loss=72.0445
	step [140/255], loss=65.4649
	step [141/255], loss=66.5321
	step [142/255], loss=85.6539
	step [143/255], loss=80.3085
	step [144/255], loss=81.7817
	step [145/255], loss=78.2559
	step [146/255], loss=82.4303
	step [147/255], loss=66.2862
	step [148/255], loss=78.7732
	step [149/255], loss=94.4533
	step [150/255], loss=71.8579
	step [151/255], loss=84.0768
	step [152/255], loss=89.8795
	step [153/255], loss=86.8706
	step [154/255], loss=64.0154
	step [155/255], loss=66.6300
	step [156/255], loss=77.1476
	step [157/255], loss=79.4928
	step [158/255], loss=73.4374
	step [159/255], loss=77.0054
	step [160/255], loss=67.0561
	step [161/255], loss=78.4642
	step [162/255], loss=77.7735
	step [163/255], loss=80.4136
	step [164/255], loss=76.2675
	step [165/255], loss=83.3147
	step [166/255], loss=94.7566
	step [167/255], loss=66.4460
	step [168/255], loss=72.7255
	step [169/255], loss=81.1617
	step [170/255], loss=74.6611
	step [171/255], loss=69.7307
	step [172/255], loss=79.6067
	step [173/255], loss=74.7745
	step [174/255], loss=66.0402
	step [175/255], loss=83.0926
	step [176/255], loss=50.8081
	step [177/255], loss=70.2946
	step [178/255], loss=67.1486
	step [179/255], loss=64.6758
	step [180/255], loss=87.8587
	step [181/255], loss=77.6943
	step [182/255], loss=64.6800
	step [183/255], loss=81.2257
	step [184/255], loss=63.1416
	step [185/255], loss=62.8466
	step [186/255], loss=85.9542
	step [187/255], loss=70.8462
	step [188/255], loss=68.1666
	step [189/255], loss=79.4366
	step [190/255], loss=72.0211
	step [191/255], loss=81.5045
	step [192/255], loss=82.2582
	step [193/255], loss=61.7182
	step [194/255], loss=66.4011
	step [195/255], loss=74.8591
	step [196/255], loss=81.6924
	step [197/255], loss=66.3671
	step [198/255], loss=69.7213
	step [199/255], loss=76.7177
	step [200/255], loss=72.8008
	step [201/255], loss=72.7174
	step [202/255], loss=62.5636
	step [203/255], loss=88.3855
	step [204/255], loss=63.9515
	step [205/255], loss=83.6862
	step [206/255], loss=75.8183
	step [207/255], loss=85.0024
	step [208/255], loss=81.6139
	step [209/255], loss=82.4221
	step [210/255], loss=84.7957
	step [211/255], loss=86.1643
	step [212/255], loss=90.7854
	step [213/255], loss=65.1744
	step [214/255], loss=80.1269
	step [215/255], loss=82.2453
	step [216/255], loss=83.7529
	step [217/255], loss=74.9811
	step [218/255], loss=76.7282
	step [219/255], loss=74.1739
	step [220/255], loss=71.8132
	step [221/255], loss=75.3977
	step [222/255], loss=67.5086
	step [223/255], loss=96.0344
	step [224/255], loss=87.7844
	step [225/255], loss=70.7889
	step [226/255], loss=71.1915
	step [227/255], loss=79.7173
	step [228/255], loss=76.0435
	step [229/255], loss=59.5523
	step [230/255], loss=84.1441
	step [231/255], loss=57.3916
	step [232/255], loss=85.6674
	step [233/255], loss=79.4014
	step [234/255], loss=80.6688
	step [235/255], loss=79.7044
	step [236/255], loss=70.1373
	step [237/255], loss=67.7388
	step [238/255], loss=81.9048
	step [239/255], loss=81.7124
	step [240/255], loss=78.2812
	step [241/255], loss=87.2146
	step [242/255], loss=71.3429
	step [243/255], loss=71.2974
	step [244/255], loss=72.2264
	step [245/255], loss=66.8357
	step [246/255], loss=74.9933
	step [247/255], loss=77.1911
	step [248/255], loss=83.7072
	step [249/255], loss=78.2866
	step [250/255], loss=73.0819
	step [251/255], loss=77.7277
	step [252/255], loss=73.9433
	step [253/255], loss=67.7651
	step [254/255], loss=74.1639
	step [255/255], loss=57.7272
	Evaluating
	loss=0.0197, precision=0.3880, recall=0.9107, f1=0.5442
Training epoch 13
	step [1/255], loss=67.5527
	step [2/255], loss=91.2991
	step [3/255], loss=68.3323
	step [4/255], loss=68.5178
	step [5/255], loss=72.4082
	step [6/255], loss=71.9269
	step [7/255], loss=80.9152
	step [8/255], loss=79.6558
	step [9/255], loss=74.8308
	step [10/255], loss=70.0546
	step [11/255], loss=73.4384
	step [12/255], loss=92.0442
	step [13/255], loss=71.8206
	step [14/255], loss=67.8996
	step [15/255], loss=72.0872
	step [16/255], loss=71.5932
	step [17/255], loss=77.0513
	step [18/255], loss=69.8196
	step [19/255], loss=80.0610
	step [20/255], loss=63.9027
	step [21/255], loss=69.8769
	step [22/255], loss=74.0307
	step [23/255], loss=73.5573
	step [24/255], loss=75.1946
	step [25/255], loss=85.6780
	step [26/255], loss=76.0983
	step [27/255], loss=93.2539
	step [28/255], loss=67.5894
	step [29/255], loss=81.6497
	step [30/255], loss=77.9076
	step [31/255], loss=83.0819
	step [32/255], loss=72.6054
	step [33/255], loss=85.1322
	step [34/255], loss=72.8271
	step [35/255], loss=84.4790
	step [36/255], loss=84.0976
	step [37/255], loss=71.9489
	step [38/255], loss=65.1932
	step [39/255], loss=75.1740
	step [40/255], loss=80.1568
	step [41/255], loss=79.5888
	step [42/255], loss=82.3450
	step [43/255], loss=70.6748
	step [44/255], loss=77.8167
	step [45/255], loss=77.8906
	step [46/255], loss=77.6663
	step [47/255], loss=73.4107
	step [48/255], loss=76.8489
	step [49/255], loss=66.9273
	step [50/255], loss=60.1728
	step [51/255], loss=67.1036
	step [52/255], loss=79.2985
	step [53/255], loss=79.4880
	step [54/255], loss=58.3486
	step [55/255], loss=85.6476
	step [56/255], loss=76.2832
	step [57/255], loss=72.7782
	step [58/255], loss=69.1006
	step [59/255], loss=83.9204
	step [60/255], loss=72.5308
	step [61/255], loss=77.1237
	step [62/255], loss=77.4077
	step [63/255], loss=78.8614
	step [64/255], loss=70.5857
	step [65/255], loss=80.0747
	step [66/255], loss=67.9950
	step [67/255], loss=63.0067
	step [68/255], loss=81.4257
	step [69/255], loss=61.3008
	step [70/255], loss=78.6826
	step [71/255], loss=74.2209
	step [72/255], loss=71.0762
	step [73/255], loss=93.1242
	step [74/255], loss=71.9458
	step [75/255], loss=74.5514
	step [76/255], loss=73.9528
	step [77/255], loss=55.0176
	step [78/255], loss=69.5641
	step [79/255], loss=64.0588
	step [80/255], loss=76.5804
	step [81/255], loss=71.6996
	step [82/255], loss=79.7962
	step [83/255], loss=73.1074
	step [84/255], loss=63.3525
	step [85/255], loss=66.5820
	step [86/255], loss=71.3586
	step [87/255], loss=87.7717
	step [88/255], loss=76.7165
	step [89/255], loss=69.2118
	step [90/255], loss=80.8286
	step [91/255], loss=63.4259
	step [92/255], loss=73.6362
	step [93/255], loss=81.8627
	step [94/255], loss=74.0095
	step [95/255], loss=69.7867
	step [96/255], loss=64.9796
	step [97/255], loss=86.6370
	step [98/255], loss=74.6503
	step [99/255], loss=81.5175
	step [100/255], loss=68.5454
	step [101/255], loss=84.8391
	step [102/255], loss=76.0620
	step [103/255], loss=81.6888
	step [104/255], loss=78.1718
	step [105/255], loss=82.5834
	step [106/255], loss=74.9053
	step [107/255], loss=75.1426
	step [108/255], loss=76.0917
	step [109/255], loss=77.8162
	step [110/255], loss=74.1920
	step [111/255], loss=78.7227
	step [112/255], loss=82.0009
	step [113/255], loss=78.4763
	step [114/255], loss=90.1075
	step [115/255], loss=82.7443
	step [116/255], loss=92.1307
	step [117/255], loss=77.1282
	step [118/255], loss=70.1415
	step [119/255], loss=71.4326
	step [120/255], loss=87.6596
	step [121/255], loss=92.7312
	step [122/255], loss=66.3158
	step [123/255], loss=69.5386
	step [124/255], loss=76.8979
	step [125/255], loss=75.4942
	step [126/255], loss=83.5392
	step [127/255], loss=68.3203
	step [128/255], loss=75.0935
	step [129/255], loss=85.1713
	step [130/255], loss=71.1155
	step [131/255], loss=82.2610
	step [132/255], loss=70.7251
	step [133/255], loss=72.3881
	step [134/255], loss=64.6444
	step [135/255], loss=69.2437
	step [136/255], loss=82.2979
	step [137/255], loss=67.0327
	step [138/255], loss=69.0576
	step [139/255], loss=77.9899
	step [140/255], loss=62.9886
	step [141/255], loss=67.3883
	step [142/255], loss=80.2480
	step [143/255], loss=69.6050
	step [144/255], loss=80.1562
	step [145/255], loss=79.8160
	step [146/255], loss=98.9119
	step [147/255], loss=78.7463
	step [148/255], loss=76.1383
	step [149/255], loss=73.4703
	step [150/255], loss=74.3130
	step [151/255], loss=83.0109
	step [152/255], loss=70.7538
	step [153/255], loss=72.7029
	step [154/255], loss=80.6252
	step [155/255], loss=78.1453
	step [156/255], loss=81.3113
	step [157/255], loss=65.2990
	step [158/255], loss=64.8447
	step [159/255], loss=69.6566
	step [160/255], loss=73.2182
	step [161/255], loss=88.8377
	step [162/255], loss=73.9144
	step [163/255], loss=78.1286
	step [164/255], loss=70.4537
	step [165/255], loss=70.4526
	step [166/255], loss=82.9389
	step [167/255], loss=86.8434
	step [168/255], loss=78.9466
	step [169/255], loss=67.7575
	step [170/255], loss=82.3026
	step [171/255], loss=74.2679
	step [172/255], loss=91.3216
	step [173/255], loss=73.3717
	step [174/255], loss=73.1201
	step [175/255], loss=73.8250
	step [176/255], loss=78.8917
	step [177/255], loss=83.2674
	step [178/255], loss=59.0089
	step [179/255], loss=71.9769
	step [180/255], loss=68.6395
	step [181/255], loss=67.2208
	step [182/255], loss=82.0766
	step [183/255], loss=77.7007
	step [184/255], loss=75.6100
	step [185/255], loss=68.3859
	step [186/255], loss=79.1583
	step [187/255], loss=65.5411
	step [188/255], loss=70.3946
	step [189/255], loss=71.9379
	step [190/255], loss=83.8595
	step [191/255], loss=69.4589
	step [192/255], loss=70.2859
	step [193/255], loss=74.8233
	step [194/255], loss=78.0634
	step [195/255], loss=76.5407
	step [196/255], loss=71.4215
	step [197/255], loss=74.6901
	step [198/255], loss=77.0653
	step [199/255], loss=66.5830
	step [200/255], loss=81.2987
	step [201/255], loss=79.0013
	step [202/255], loss=66.1445
	step [203/255], loss=66.7850
	step [204/255], loss=72.7247
	step [205/255], loss=78.8388
	step [206/255], loss=65.2195
	step [207/255], loss=83.0106
	step [208/255], loss=71.4453
	step [209/255], loss=69.1648
	step [210/255], loss=70.2494
	step [211/255], loss=75.9451
	step [212/255], loss=73.8745
	step [213/255], loss=69.6591
	step [214/255], loss=76.0585
	step [215/255], loss=75.9629
	step [216/255], loss=68.2727
	step [217/255], loss=92.4437
	step [218/255], loss=75.8199
	step [219/255], loss=92.0730
	step [220/255], loss=75.7188
	step [221/255], loss=78.2622
	step [222/255], loss=66.7857
	step [223/255], loss=70.3824
	step [224/255], loss=85.5323
	step [225/255], loss=69.1801
	step [226/255], loss=62.8093
	step [227/255], loss=70.7993
	step [228/255], loss=72.4620
	step [229/255], loss=76.9500
	step [230/255], loss=88.8081
	step [231/255], loss=79.4497
	step [232/255], loss=76.1004
	step [233/255], loss=73.4907
	step [234/255], loss=79.5723
	step [235/255], loss=68.0987
	step [236/255], loss=79.9874
	step [237/255], loss=73.8822
	step [238/255], loss=91.5766
	step [239/255], loss=72.5119
	step [240/255], loss=72.5596
	step [241/255], loss=74.0350
	step [242/255], loss=81.8914
	step [243/255], loss=89.2355
	step [244/255], loss=64.0415
	step [245/255], loss=67.1202
	step [246/255], loss=66.8394
	step [247/255], loss=76.4834
	step [248/255], loss=56.3632
	step [249/255], loss=70.1124
	step [250/255], loss=63.3531
	step [251/255], loss=67.5874
	step [252/255], loss=70.5375
	step [253/255], loss=63.0450
	step [254/255], loss=87.0153
	step [255/255], loss=56.1437
	Evaluating
	loss=0.0164, precision=0.4062, recall=0.9209, f1=0.5637
saving model as: 0_saved_model.pth
Training epoch 14
	step [1/255], loss=77.7092
	step [2/255], loss=79.7286
	step [3/255], loss=69.3780
	step [4/255], loss=80.5222
	step [5/255], loss=61.8451
	step [6/255], loss=82.9254
	step [7/255], loss=75.4309
	step [8/255], loss=74.1195
	step [9/255], loss=80.3946
	step [10/255], loss=73.1816
	step [11/255], loss=71.8870
	step [12/255], loss=78.1633
	step [13/255], loss=68.5886
	step [14/255], loss=73.9652
	step [15/255], loss=75.6593
	step [16/255], loss=77.5955
	step [17/255], loss=60.8289
	step [18/255], loss=74.9989
	step [19/255], loss=80.1496
	step [20/255], loss=70.1852
	step [21/255], loss=66.0945
	step [22/255], loss=76.0355
	step [23/255], loss=67.2166
	step [24/255], loss=72.8447
	step [25/255], loss=76.1925
	step [26/255], loss=62.2078
	step [27/255], loss=62.6212
	step [28/255], loss=75.0894
	step [29/255], loss=78.1129
	step [30/255], loss=80.6950
	step [31/255], loss=80.0021
	step [32/255], loss=90.3136
	step [33/255], loss=75.3141
	step [34/255], loss=82.9294
	step [35/255], loss=64.1561
	step [36/255], loss=74.1130
	step [37/255], loss=69.1582
	step [38/255], loss=83.7715
	step [39/255], loss=80.8340
	step [40/255], loss=74.4300
	step [41/255], loss=67.2007
	step [42/255], loss=65.2555
	step [43/255], loss=73.7406
	step [44/255], loss=77.1086
	step [45/255], loss=88.2239
	step [46/255], loss=80.2351
	step [47/255], loss=77.2369
	step [48/255], loss=68.3677
	step [49/255], loss=68.8904
	step [50/255], loss=81.3354
	step [51/255], loss=66.4017
	step [52/255], loss=66.7637
	step [53/255], loss=73.6066
	step [54/255], loss=74.7444
	step [55/255], loss=89.1610
	step [56/255], loss=73.2028
	step [57/255], loss=77.1065
	step [58/255], loss=69.4895
	step [59/255], loss=75.6034
	step [60/255], loss=70.7633
	step [61/255], loss=75.2821
	step [62/255], loss=67.1713
	step [63/255], loss=67.0105
	step [64/255], loss=83.1691
	step [65/255], loss=75.4348
	step [66/255], loss=64.9036
	step [67/255], loss=87.7005
	step [68/255], loss=68.9844
	step [69/255], loss=83.9641
	step [70/255], loss=77.2990
	step [71/255], loss=86.6192
	step [72/255], loss=70.5699
	step [73/255], loss=74.1748
	step [74/255], loss=68.3732
	step [75/255], loss=58.4570
	step [76/255], loss=62.5784
	step [77/255], loss=67.4767
	step [78/255], loss=72.1068
	step [79/255], loss=72.7161
	step [80/255], loss=82.2433
	step [81/255], loss=79.8209
	step [82/255], loss=76.8969
	step [83/255], loss=67.9862
	step [84/255], loss=68.7876
	step [85/255], loss=71.3924
	step [86/255], loss=61.5456
	step [87/255], loss=80.9370
	step [88/255], loss=77.1508
	step [89/255], loss=68.6039
	step [90/255], loss=72.3142
	step [91/255], loss=68.6842
	step [92/255], loss=83.5681
	step [93/255], loss=73.9820
	step [94/255], loss=71.0174
	step [95/255], loss=70.2931
	step [96/255], loss=59.7695
	step [97/255], loss=76.5622
	step [98/255], loss=72.5124
	step [99/255], loss=82.4727
	step [100/255], loss=79.2886
	step [101/255], loss=68.4932
	step [102/255], loss=69.1727
	step [103/255], loss=78.3112
	step [104/255], loss=57.1520
	step [105/255], loss=80.4604
	step [106/255], loss=59.7985
	step [107/255], loss=66.9004
	step [108/255], loss=87.9934
	step [109/255], loss=76.6825
	step [110/255], loss=82.3970
	step [111/255], loss=73.2178
	step [112/255], loss=70.2296
	step [113/255], loss=76.0108
	step [114/255], loss=69.8453
	step [115/255], loss=78.1029
	step [116/255], loss=81.3858
	step [117/255], loss=71.7808
	step [118/255], loss=76.8465
	step [119/255], loss=80.0817
	step [120/255], loss=83.3619
	step [121/255], loss=80.6559
	step [122/255], loss=86.6154
	step [123/255], loss=75.3533
	step [124/255], loss=78.6983
	step [125/255], loss=72.4652
	step [126/255], loss=71.9334
	step [127/255], loss=75.2278
	step [128/255], loss=77.9890
	step [129/255], loss=77.1764
	step [130/255], loss=71.9775
	step [131/255], loss=75.4796
	step [132/255], loss=83.6268
	step [133/255], loss=69.0755
	step [134/255], loss=67.2983
	step [135/255], loss=70.5572
	step [136/255], loss=87.3391
	step [137/255], loss=69.9807
	step [138/255], loss=66.9854
	step [139/255], loss=70.6167
	step [140/255], loss=70.2109
	step [141/255], loss=72.6641
	step [142/255], loss=60.8561
	step [143/255], loss=80.3027
	step [144/255], loss=85.1576
	step [145/255], loss=69.8742
	step [146/255], loss=73.6979
	step [147/255], loss=71.9867
	step [148/255], loss=76.4452
	step [149/255], loss=85.6916
	step [150/255], loss=66.7782
	step [151/255], loss=69.7987
	step [152/255], loss=69.7386
	step [153/255], loss=74.1256
	step [154/255], loss=71.0785
	step [155/255], loss=85.9566
	step [156/255], loss=74.0956
	step [157/255], loss=72.2025
	step [158/255], loss=77.6485
	step [159/255], loss=78.7292
	step [160/255], loss=76.7483
	step [161/255], loss=54.0190
	step [162/255], loss=79.7755
	step [163/255], loss=70.3362
	step [164/255], loss=72.1041
	step [165/255], loss=65.7499
	step [166/255], loss=73.2731
	step [167/255], loss=64.0634
	step [168/255], loss=76.0696
	step [169/255], loss=70.8187
	step [170/255], loss=75.2094
	step [171/255], loss=66.6219
	step [172/255], loss=71.6924
	step [173/255], loss=79.3526
	step [174/255], loss=75.9938
	step [175/255], loss=79.0080
	step [176/255], loss=76.9360
	step [177/255], loss=75.4530
	step [178/255], loss=66.7756
	step [179/255], loss=83.7280
	step [180/255], loss=71.1582
	step [181/255], loss=71.3444
	step [182/255], loss=58.5233
	step [183/255], loss=77.3189
	step [184/255], loss=65.6636
	step [185/255], loss=65.9696
	step [186/255], loss=81.4466
	step [187/255], loss=84.4762
	step [188/255], loss=74.3852
	step [189/255], loss=82.4144
	step [190/255], loss=91.7276
	step [191/255], loss=72.0916
	step [192/255], loss=80.5160
	step [193/255], loss=87.7418
	step [194/255], loss=74.0320
	step [195/255], loss=54.8566
	step [196/255], loss=83.1098
	step [197/255], loss=67.0941
	step [198/255], loss=76.3529
	step [199/255], loss=68.5567
	step [200/255], loss=84.3052
	step [201/255], loss=80.9604
	step [202/255], loss=70.6412
	step [203/255], loss=66.8357
	step [204/255], loss=60.5402
	step [205/255], loss=77.5787
	step [206/255], loss=68.2349
	step [207/255], loss=68.8835
	step [208/255], loss=64.5793
	step [209/255], loss=86.4051
	step [210/255], loss=68.4183
	step [211/255], loss=62.0456
	step [212/255], loss=68.5587
	step [213/255], loss=76.4554
	step [214/255], loss=76.2673
	step [215/255], loss=88.4319
	step [216/255], loss=88.0758
	step [217/255], loss=86.6832
	step [218/255], loss=79.5911
	step [219/255], loss=76.5702
	step [220/255], loss=71.8350
	step [221/255], loss=67.3438
	step [222/255], loss=68.1054
	step [223/255], loss=72.8035
	step [224/255], loss=86.4290
	step [225/255], loss=62.9925
	step [226/255], loss=80.9555
	step [227/255], loss=67.1150
	step [228/255], loss=72.2020
	step [229/255], loss=83.6470
	step [230/255], loss=93.4656
	step [231/255], loss=74.1939
	step [232/255], loss=79.7878
	step [233/255], loss=62.4769
	step [234/255], loss=83.3753
	step [235/255], loss=54.1978
	step [236/255], loss=72.1135
	step [237/255], loss=71.1331
	step [238/255], loss=80.2256
	step [239/255], loss=69.0412
	step [240/255], loss=76.1586
	step [241/255], loss=64.8764
	step [242/255], loss=76.1570
	step [243/255], loss=71.4441
	step [244/255], loss=64.0724
	step [245/255], loss=79.2918
	step [246/255], loss=84.3462
	step [247/255], loss=71.5186
	step [248/255], loss=69.3237
	step [249/255], loss=82.0515
	step [250/255], loss=72.3704
	step [251/255], loss=73.4646
	step [252/255], loss=74.7698
	step [253/255], loss=62.9495
	step [254/255], loss=81.1604
	step [255/255], loss=57.6421
	Evaluating
	loss=0.0156, precision=0.3627, recall=0.9180, f1=0.5200
Training epoch 15
	step [1/255], loss=70.2105
	step [2/255], loss=74.8111
	step [3/255], loss=64.0276
	step [4/255], loss=72.5083
	step [5/255], loss=66.0385
	step [6/255], loss=88.3470
	step [7/255], loss=74.7079
	step [8/255], loss=82.9950
	step [9/255], loss=77.1258
	step [10/255], loss=69.9673
	step [11/255], loss=61.5445
	step [12/255], loss=73.9933
	step [13/255], loss=74.0778
	step [14/255], loss=78.5259
	step [15/255], loss=64.6877
	step [16/255], loss=83.5528
	step [17/255], loss=58.8913
	step [18/255], loss=69.2568
	step [19/255], loss=76.4124
	step [20/255], loss=80.4630
	step [21/255], loss=79.3045
	step [22/255], loss=82.0042
	step [23/255], loss=76.1227
	step [24/255], loss=79.0344
	step [25/255], loss=65.8177
	step [26/255], loss=88.2080
	step [27/255], loss=68.1464
	step [28/255], loss=74.1171
	step [29/255], loss=72.6538
	step [30/255], loss=76.0653
	step [31/255], loss=67.3110
	step [32/255], loss=69.0806
	step [33/255], loss=69.0611
	step [34/255], loss=79.0193
	step [35/255], loss=66.1244
	step [36/255], loss=77.2947
	step [37/255], loss=64.7718
	step [38/255], loss=78.5266
	step [39/255], loss=74.0800
	step [40/255], loss=73.7956
	step [41/255], loss=74.0000
	step [42/255], loss=77.2264
	step [43/255], loss=60.5009
	step [44/255], loss=84.2256
	step [45/255], loss=67.3134
	step [46/255], loss=80.6108
	step [47/255], loss=71.8908
	step [48/255], loss=60.4490
	step [49/255], loss=76.8707
	step [50/255], loss=75.6812
	step [51/255], loss=70.2706
	step [52/255], loss=63.2299
	step [53/255], loss=63.3935
	step [54/255], loss=54.8150
	step [55/255], loss=86.1395
	step [56/255], loss=66.9153
	step [57/255], loss=72.3488
	step [58/255], loss=66.1846
	step [59/255], loss=69.9008
	step [60/255], loss=67.1783
	step [61/255], loss=81.2728
	step [62/255], loss=78.8188
	step [63/255], loss=74.1286
	step [64/255], loss=68.1012
	step [65/255], loss=76.3184
	step [66/255], loss=54.8989
	step [67/255], loss=74.8094
	step [68/255], loss=75.9340
	step [69/255], loss=67.4751
	step [70/255], loss=72.2812
	step [71/255], loss=75.1330
	step [72/255], loss=85.1705
	step [73/255], loss=68.5925
	step [74/255], loss=67.0481
	step [75/255], loss=68.5739
	step [76/255], loss=77.4974
	step [77/255], loss=74.1610
	step [78/255], loss=77.9597
	step [79/255], loss=62.2133
	step [80/255], loss=70.9134
	step [81/255], loss=78.3988
	step [82/255], loss=76.8364
	step [83/255], loss=57.5391
	step [84/255], loss=67.2978
	step [85/255], loss=71.7495
	step [86/255], loss=86.8357
	step [87/255], loss=76.6924
	step [88/255], loss=74.3262
	step [89/255], loss=63.5387
	step [90/255], loss=71.0883
	step [91/255], loss=71.4997
	step [92/255], loss=82.8831
	step [93/255], loss=74.4178
	step [94/255], loss=72.5182
	step [95/255], loss=65.5623
	step [96/255], loss=68.5994
	step [97/255], loss=92.5776
	step [98/255], loss=71.9883
	step [99/255], loss=59.5478
	step [100/255], loss=87.1315
	step [101/255], loss=80.1451
	step [102/255], loss=68.5331
	step [103/255], loss=58.3428
	step [104/255], loss=66.0006
	step [105/255], loss=78.4390
	step [106/255], loss=70.3284
	step [107/255], loss=83.9001
	step [108/255], loss=78.6550
	step [109/255], loss=75.1679
	step [110/255], loss=69.2838
	step [111/255], loss=73.2785
	step [112/255], loss=78.6269
	step [113/255], loss=61.9703
	step [114/255], loss=56.2553
	step [115/255], loss=78.5674
	step [116/255], loss=77.8241
	step [117/255], loss=63.4186
	step [118/255], loss=64.3418
	step [119/255], loss=64.7288
	step [120/255], loss=66.1337
	step [121/255], loss=79.2552
	step [122/255], loss=73.9321
	step [123/255], loss=78.4810
	step [124/255], loss=68.0584
	step [125/255], loss=66.1360
	step [126/255], loss=75.3840
	step [127/255], loss=57.1901
	step [128/255], loss=84.9322
	step [129/255], loss=72.0644
	step [130/255], loss=80.6010
	step [131/255], loss=68.5805
	step [132/255], loss=76.8337
	step [133/255], loss=70.4795
	step [134/255], loss=76.0900
	step [135/255], loss=74.4055
	step [136/255], loss=66.5628
	step [137/255], loss=78.5871
	step [138/255], loss=72.1853
	step [139/255], loss=82.1867
	step [140/255], loss=75.6138
	step [141/255], loss=75.3339
	step [142/255], loss=78.6910
	step [143/255], loss=66.7034
	step [144/255], loss=65.1488
	step [145/255], loss=73.6595
	step [146/255], loss=71.3235
	step [147/255], loss=80.4910
	step [148/255], loss=77.3232
	step [149/255], loss=75.2892
	step [150/255], loss=65.2137
	step [151/255], loss=83.3465
	step [152/255], loss=82.1743
	step [153/255], loss=91.0302
	step [154/255], loss=81.4947
	step [155/255], loss=69.1318
	step [156/255], loss=76.2530
	step [157/255], loss=86.3375
	step [158/255], loss=62.8103
	step [159/255], loss=66.5362
	step [160/255], loss=83.3658
	step [161/255], loss=72.6339
	step [162/255], loss=73.0786
	step [163/255], loss=73.0839
	step [164/255], loss=90.6429
	step [165/255], loss=80.5988
	step [166/255], loss=71.2006
	step [167/255], loss=78.4511
	step [168/255], loss=69.0921
	step [169/255], loss=66.0713
	step [170/255], loss=74.9981
	step [171/255], loss=71.1635
	step [172/255], loss=79.9450
	step [173/255], loss=77.6607
	step [174/255], loss=59.3195
	step [175/255], loss=75.2809
	step [176/255], loss=71.3424
	step [177/255], loss=72.5565
	step [178/255], loss=63.8546
	step [179/255], loss=65.2536
	step [180/255], loss=87.2796
	step [181/255], loss=61.8016
	step [182/255], loss=66.9537
	step [183/255], loss=72.9074
	step [184/255], loss=77.2825
	step [185/255], loss=67.8137
	step [186/255], loss=69.9641
	step [187/255], loss=61.5281
	step [188/255], loss=65.3282
	step [189/255], loss=66.9056
	step [190/255], loss=76.5220
	step [191/255], loss=83.1645
	step [192/255], loss=80.2882
	step [193/255], loss=75.1580
	step [194/255], loss=73.8493
	step [195/255], loss=71.9598
	step [196/255], loss=70.6796
	step [197/255], loss=71.2971
	step [198/255], loss=77.6510
	step [199/255], loss=77.1573
	step [200/255], loss=75.7315
	step [201/255], loss=65.9055
	step [202/255], loss=78.5395
	step [203/255], loss=63.9451
	step [204/255], loss=69.0901
	step [205/255], loss=76.7323
	step [206/255], loss=59.1008
	step [207/255], loss=74.9966
	step [208/255], loss=77.6694
	step [209/255], loss=78.1370
	step [210/255], loss=79.2857
	step [211/255], loss=79.5822
	step [212/255], loss=73.0329
	step [213/255], loss=74.4647
	step [214/255], loss=75.8071
	step [215/255], loss=68.5168
	step [216/255], loss=74.2190
	step [217/255], loss=57.0985
	step [218/255], loss=63.8537
	step [219/255], loss=72.9151
	step [220/255], loss=71.0315
	step [221/255], loss=64.8079
	step [222/255], loss=90.7232
	step [223/255], loss=73.0797
	step [224/255], loss=73.4877
	step [225/255], loss=76.8567
	step [226/255], loss=75.0708
	step [227/255], loss=78.2761
	step [228/255], loss=67.6558
	step [229/255], loss=71.1644
	step [230/255], loss=73.8568
	step [231/255], loss=70.8259
	step [232/255], loss=77.3304
	step [233/255], loss=63.9483
	step [234/255], loss=72.9794
	step [235/255], loss=69.4471
	step [236/255], loss=77.1934
	step [237/255], loss=80.2668
	step [238/255], loss=64.4035
	step [239/255], loss=84.2098
	step [240/255], loss=84.6099
	step [241/255], loss=74.9029
	step [242/255], loss=61.0416
	step [243/255], loss=79.4428
	step [244/255], loss=80.0984
	step [245/255], loss=76.1106
	step [246/255], loss=75.3272
	step [247/255], loss=64.4618
	step [248/255], loss=72.7476
	step [249/255], loss=72.0862
	step [250/255], loss=64.8283
	step [251/255], loss=70.5946
	step [252/255], loss=64.6819
	step [253/255], loss=81.4050
	step [254/255], loss=73.7821
	step [255/255], loss=57.2032
	Evaluating
	loss=0.0146, precision=0.4044, recall=0.9084, f1=0.5596
Training epoch 16
	step [1/255], loss=73.1776
	step [2/255], loss=68.6737
	step [3/255], loss=81.3254
	step [4/255], loss=74.6230
	step [5/255], loss=67.1252
	step [6/255], loss=74.2955
	step [7/255], loss=67.1328
	step [8/255], loss=73.0413
	step [9/255], loss=79.0323
	step [10/255], loss=77.6666
	step [11/255], loss=80.4969
	step [12/255], loss=77.1466
	step [13/255], loss=75.1354
	step [14/255], loss=71.7990
	step [15/255], loss=77.4443
	step [16/255], loss=65.1939
	step [17/255], loss=71.2511
	step [18/255], loss=80.3936
	step [19/255], loss=70.4307
	step [20/255], loss=77.9785
	step [21/255], loss=59.2363
	step [22/255], loss=75.7436
	step [23/255], loss=82.0127
	step [24/255], loss=90.4003
	step [25/255], loss=73.9496
	step [26/255], loss=73.3633
	step [27/255], loss=72.2614
	step [28/255], loss=71.4470
	step [29/255], loss=83.1908
	step [30/255], loss=73.9415
	step [31/255], loss=74.3632
	step [32/255], loss=75.3557
	step [33/255], loss=69.8422
	step [34/255], loss=82.1474
	step [35/255], loss=69.2680
	step [36/255], loss=69.8898
	step [37/255], loss=65.8956
	step [38/255], loss=75.9671
	step [39/255], loss=67.4555
	step [40/255], loss=82.1169
	step [41/255], loss=73.2189
	step [42/255], loss=95.2522
	step [43/255], loss=63.6993
	step [44/255], loss=62.4587
	step [45/255], loss=78.5564
	step [46/255], loss=86.1461
	step [47/255], loss=71.2466
	step [48/255], loss=63.5847
	step [49/255], loss=67.7712
	step [50/255], loss=70.6029
	step [51/255], loss=75.2058
	step [52/255], loss=49.4822
	step [53/255], loss=71.8753
	step [54/255], loss=59.9116
	step [55/255], loss=75.1637
	step [56/255], loss=57.9302
	step [57/255], loss=74.9475
	step [58/255], loss=65.0992
	step [59/255], loss=78.7791
	step [60/255], loss=67.4970
	step [61/255], loss=66.5186
	step [62/255], loss=72.7329
	step [63/255], loss=79.5363
	step [64/255], loss=72.5933
	step [65/255], loss=76.4490
	step [66/255], loss=65.9781
	step [67/255], loss=79.1593
	step [68/255], loss=59.9425
	step [69/255], loss=73.8273
	step [70/255], loss=65.0678
	step [71/255], loss=79.3978
	step [72/255], loss=85.5405
	step [73/255], loss=75.5247
	step [74/255], loss=71.9131
	step [75/255], loss=77.5267
	step [76/255], loss=94.4802
	step [77/255], loss=79.8610
	step [78/255], loss=69.1128
	step [79/255], loss=73.1379
	step [80/255], loss=68.8031
	step [81/255], loss=69.8353
	step [82/255], loss=60.8376
	step [83/255], loss=76.6938
	step [84/255], loss=74.1785
	step [85/255], loss=70.5827
	step [86/255], loss=68.8350
	step [87/255], loss=68.6600
	step [88/255], loss=66.7657
	step [89/255], loss=69.9572
	step [90/255], loss=76.3265
	step [91/255], loss=62.2778
	step [92/255], loss=75.0464
	step [93/255], loss=70.6175
	step [94/255], loss=81.6017
	step [95/255], loss=64.0555
	step [96/255], loss=84.0173
	step [97/255], loss=78.6496
	step [98/255], loss=76.1038
	step [99/255], loss=64.1846
	step [100/255], loss=71.4950
	step [101/255], loss=65.9129
	step [102/255], loss=60.4462
	step [103/255], loss=77.7157
	step [104/255], loss=78.2438
	step [105/255], loss=68.2275
	step [106/255], loss=75.0804
	step [107/255], loss=88.3744
	step [108/255], loss=67.9100
	step [109/255], loss=71.7172
	step [110/255], loss=73.0549
	step [111/255], loss=68.6262
	step [112/255], loss=77.6189
	step [113/255], loss=84.2460
	step [114/255], loss=66.9770
	step [115/255], loss=85.6283
	step [116/255], loss=88.6087
	step [117/255], loss=77.7510
	step [118/255], loss=63.6891
	step [119/255], loss=78.7534
	step [120/255], loss=74.8645
	step [121/255], loss=72.6438
	step [122/255], loss=65.6259
	step [123/255], loss=70.1687
	step [124/255], loss=57.8169
	step [125/255], loss=70.1073
	step [126/255], loss=73.6318
	step [127/255], loss=90.7046
	step [128/255], loss=69.9721
	step [129/255], loss=60.4220
	step [130/255], loss=62.5885
	step [131/255], loss=74.3614
	step [132/255], loss=88.8050
	step [133/255], loss=61.2769
	step [134/255], loss=71.8208
	step [135/255], loss=64.8393
	step [136/255], loss=66.0463
	step [137/255], loss=69.5647
	step [138/255], loss=79.4267
	step [139/255], loss=67.4339
	step [140/255], loss=68.0744
	step [141/255], loss=81.6946
	step [142/255], loss=65.8593
	step [143/255], loss=75.3519
	step [144/255], loss=64.7890
	step [145/255], loss=78.1972
	step [146/255], loss=61.7280
	step [147/255], loss=70.8374
	step [148/255], loss=78.2331
	step [149/255], loss=74.1156
	step [150/255], loss=74.7893
	step [151/255], loss=68.7652
	step [152/255], loss=77.8599
	step [153/255], loss=81.1533
	step [154/255], loss=70.7590
	step [155/255], loss=79.6605
	step [156/255], loss=61.1342
	step [157/255], loss=72.6982
	step [158/255], loss=84.9968
	step [159/255], loss=59.3395
	step [160/255], loss=72.1000
	step [161/255], loss=90.2061
	step [162/255], loss=82.9985
	step [163/255], loss=73.9730
	step [164/255], loss=66.7708
	step [165/255], loss=60.3190
	step [166/255], loss=72.2962
	step [167/255], loss=79.7470
	step [168/255], loss=77.2536
	step [169/255], loss=52.2125
	step [170/255], loss=58.4119
	step [171/255], loss=81.5298
	step [172/255], loss=74.0449
	step [173/255], loss=79.5412
	step [174/255], loss=69.0369
	step [175/255], loss=68.7134
	step [176/255], loss=73.8703
	step [177/255], loss=59.3367
	step [178/255], loss=70.1774
	step [179/255], loss=69.6346
	step [180/255], loss=72.1113
	step [181/255], loss=63.0764
	step [182/255], loss=62.7796
	step [183/255], loss=84.3281
	step [184/255], loss=74.9816
	step [185/255], loss=66.6039
	step [186/255], loss=60.3757
	step [187/255], loss=70.6694
	step [188/255], loss=75.3725
	step [189/255], loss=75.7932
	step [190/255], loss=67.3324
	step [191/255], loss=93.9933
	step [192/255], loss=63.6803
	step [193/255], loss=69.3565
	step [194/255], loss=72.3251
	step [195/255], loss=60.0455
	step [196/255], loss=75.1564
	step [197/255], loss=64.5808
	step [198/255], loss=72.9407
	step [199/255], loss=70.0990
	step [200/255], loss=66.0390
	step [201/255], loss=77.1458
	step [202/255], loss=58.6347
	step [203/255], loss=59.4404
	step [204/255], loss=81.7915
	step [205/255], loss=68.4933
	step [206/255], loss=65.3088
	step [207/255], loss=58.4621
	step [208/255], loss=70.2359
	step [209/255], loss=65.2742
	step [210/255], loss=83.5902
	step [211/255], loss=70.3037
	step [212/255], loss=68.5438
	step [213/255], loss=69.4092
	step [214/255], loss=67.3123
	step [215/255], loss=69.1447
	step [216/255], loss=73.8176
	step [217/255], loss=63.3938
	step [218/255], loss=55.1425
	step [219/255], loss=67.1061
	step [220/255], loss=81.3511
	step [221/255], loss=80.9742
	step [222/255], loss=84.9584
	step [223/255], loss=71.8826
	step [224/255], loss=75.8139
	step [225/255], loss=61.7504
	step [226/255], loss=82.0482
	step [227/255], loss=79.0159
	step [228/255], loss=77.0747
	step [229/255], loss=73.9656
	step [230/255], loss=75.4673
	step [231/255], loss=67.7125
	step [232/255], loss=76.6514
	step [233/255], loss=72.7369
	step [234/255], loss=69.8951
	step [235/255], loss=88.8632
	step [236/255], loss=74.5784
	step [237/255], loss=75.7743
	step [238/255], loss=73.3741
	step [239/255], loss=71.7775
	step [240/255], loss=77.9919
	step [241/255], loss=67.7185
	step [242/255], loss=70.0755
	step [243/255], loss=69.1673
	step [244/255], loss=66.4362
	step [245/255], loss=67.8608
	step [246/255], loss=83.7755
	step [247/255], loss=76.6334
	step [248/255], loss=64.3716
	step [249/255], loss=73.2236
	step [250/255], loss=66.1658
	step [251/255], loss=79.6663
	step [252/255], loss=59.7267
	step [253/255], loss=82.5806
	step [254/255], loss=61.1641
	step [255/255], loss=66.1706
	Evaluating
	loss=0.0141, precision=0.3499, recall=0.9208, f1=0.5071
Training epoch 17
	step [1/255], loss=70.9842
	step [2/255], loss=73.6665
	step [3/255], loss=79.9475
	step [4/255], loss=87.3541
	step [5/255], loss=72.7190
	step [6/255], loss=76.8874
	step [7/255], loss=62.8941
	step [8/255], loss=75.4277
	step [9/255], loss=78.9306
	step [10/255], loss=62.6899
	step [11/255], loss=69.3269
	step [12/255], loss=64.1168
	step [13/255], loss=61.1416
	step [14/255], loss=75.3238
	step [15/255], loss=67.6513
	step [16/255], loss=87.4683
	step [17/255], loss=82.7356
	step [18/255], loss=66.7571
	step [19/255], loss=71.0599
	step [20/255], loss=67.6432
	step [21/255], loss=73.5467
	step [22/255], loss=66.0882
	step [23/255], loss=65.3774
	step [24/255], loss=62.5278
	step [25/255], loss=71.6168
	step [26/255], loss=60.2574
	step [27/255], loss=65.0781
	step [28/255], loss=80.7082
	step [29/255], loss=80.2521
	step [30/255], loss=75.3755
	step [31/255], loss=89.7197
	step [32/255], loss=77.9976
	step [33/255], loss=63.4748
	step [34/255], loss=83.6664
	step [35/255], loss=76.7297
	step [36/255], loss=74.2151
	step [37/255], loss=75.7640
	step [38/255], loss=69.0693
	step [39/255], loss=67.5952
	step [40/255], loss=68.1061
	step [41/255], loss=77.8840
	step [42/255], loss=65.4829
	step [43/255], loss=69.3398
	step [44/255], loss=74.5928
	step [45/255], loss=66.9591
	step [46/255], loss=77.4746
	step [47/255], loss=80.2223
	step [48/255], loss=75.9964
	step [49/255], loss=73.1936
	step [50/255], loss=80.9265
	step [51/255], loss=64.1116
	step [52/255], loss=74.2487
	step [53/255], loss=75.8039
	step [54/255], loss=80.5195
	step [55/255], loss=68.8807
	step [56/255], loss=65.7875
	step [57/255], loss=71.2509
	step [58/255], loss=70.0521
	step [59/255], loss=72.4472
	step [60/255], loss=77.7764
	step [61/255], loss=63.3886
	step [62/255], loss=70.5958
	step [63/255], loss=76.4353
	step [64/255], loss=75.4634
	step [65/255], loss=62.5216
	step [66/255], loss=71.2260
	step [67/255], loss=50.3631
	step [68/255], loss=65.8112
	step [69/255], loss=71.7797
	step [70/255], loss=75.5023
	step [71/255], loss=61.9443
	step [72/255], loss=76.1874
	step [73/255], loss=68.1804
	step [74/255], loss=75.6624
	step [75/255], loss=69.2136
	step [76/255], loss=71.3162
	step [77/255], loss=67.7447
	step [78/255], loss=67.7046
	step [79/255], loss=68.2308
	step [80/255], loss=60.1488
	step [81/255], loss=61.7059
	step [82/255], loss=71.3871
	step [83/255], loss=68.0557
	step [84/255], loss=67.6953
	step [85/255], loss=66.6570
	step [86/255], loss=75.9466
	step [87/255], loss=69.6604
	step [88/255], loss=62.9442
	step [89/255], loss=69.4346
	step [90/255], loss=67.3290
	step [91/255], loss=78.0665
	step [92/255], loss=72.4125
	step [93/255], loss=73.0596
	step [94/255], loss=71.0170
	step [95/255], loss=78.2752
	step [96/255], loss=74.2158
	step [97/255], loss=65.0096
	step [98/255], loss=71.7595
	step [99/255], loss=75.6947
	step [100/255], loss=71.0551
	step [101/255], loss=75.0111
	step [102/255], loss=57.5111
	step [103/255], loss=71.8158
	step [104/255], loss=70.3646
	step [105/255], loss=68.6130
	step [106/255], loss=68.8890
	step [107/255], loss=68.1917
	step [108/255], loss=87.3127
	step [109/255], loss=57.1817
	step [110/255], loss=60.5883
	step [111/255], loss=56.7615
	step [112/255], loss=78.8645
	step [113/255], loss=78.1039
	step [114/255], loss=68.6146
	step [115/255], loss=81.4925
	step [116/255], loss=61.4437
	step [117/255], loss=72.3124
	step [118/255], loss=72.4499
	step [119/255], loss=72.9850
	step [120/255], loss=70.8334
	step [121/255], loss=69.1203
	step [122/255], loss=71.4069
	step [123/255], loss=84.7186
	step [124/255], loss=77.2597
	step [125/255], loss=68.1580
	step [126/255], loss=72.1241
	step [127/255], loss=83.0146
	step [128/255], loss=62.2761
	step [129/255], loss=69.1047
	step [130/255], loss=83.0774
	step [131/255], loss=76.1426
	step [132/255], loss=62.6430
	step [133/255], loss=73.2305
	step [134/255], loss=70.4613
	step [135/255], loss=75.9751
	step [136/255], loss=75.8806
	step [137/255], loss=65.5618
	step [138/255], loss=64.5641
	step [139/255], loss=82.9926
	step [140/255], loss=63.7024
	step [141/255], loss=86.2877
	step [142/255], loss=58.7472
	step [143/255], loss=73.6209
	step [144/255], loss=74.6649
	step [145/255], loss=72.6758
	step [146/255], loss=62.0969
	step [147/255], loss=67.6901
	step [148/255], loss=76.7239
	step [149/255], loss=69.5022
	step [150/255], loss=62.0273
	step [151/255], loss=68.0059
	step [152/255], loss=67.4471
	step [153/255], loss=68.8259
	step [154/255], loss=73.5537
	step [155/255], loss=79.0638
	step [156/255], loss=81.7574
	step [157/255], loss=75.3526
	step [158/255], loss=77.4724
	step [159/255], loss=74.2320
	step [160/255], loss=59.6376
	step [161/255], loss=67.5235
	step [162/255], loss=64.4405
	step [163/255], loss=77.6245
	step [164/255], loss=64.5287
	step [165/255], loss=65.8950
	step [166/255], loss=70.9277
	step [167/255], loss=80.3567
	step [168/255], loss=69.5758
	step [169/255], loss=67.6197
	step [170/255], loss=67.1120
	step [171/255], loss=76.7348
	step [172/255], loss=65.3322
	step [173/255], loss=75.1229
	step [174/255], loss=70.8274
	step [175/255], loss=62.3442
	step [176/255], loss=67.3096
	step [177/255], loss=67.2883
	step [178/255], loss=71.5908
	step [179/255], loss=70.0974
	step [180/255], loss=79.8665
	step [181/255], loss=71.2274
	step [182/255], loss=70.5246
	step [183/255], loss=62.2247
	step [184/255], loss=76.8322
	step [185/255], loss=68.7068
	step [186/255], loss=61.1095
	step [187/255], loss=72.4285
	step [188/255], loss=76.1851
	step [189/255], loss=61.1638
	step [190/255], loss=61.8845
	step [191/255], loss=68.3548
	step [192/255], loss=70.6443
	step [193/255], loss=73.2086
	step [194/255], loss=64.3016
	step [195/255], loss=64.7574
	step [196/255], loss=77.2566
	step [197/255], loss=68.8457
	step [198/255], loss=76.3784
	step [199/255], loss=67.6736
	step [200/255], loss=87.0218
	step [201/255], loss=80.6661
	step [202/255], loss=62.7078
	step [203/255], loss=77.8656
	step [204/255], loss=64.4430
	step [205/255], loss=69.2191
	step [206/255], loss=68.1526
	step [207/255], loss=75.1324
	step [208/255], loss=70.8419
	step [209/255], loss=78.2973
	step [210/255], loss=60.6130
	step [211/255], loss=68.5574
	step [212/255], loss=66.5097
	step [213/255], loss=58.1084
	step [214/255], loss=69.7566
	step [215/255], loss=73.1353
	step [216/255], loss=72.4325
	step [217/255], loss=67.7326
	step [218/255], loss=74.2899
	step [219/255], loss=68.0816
	step [220/255], loss=69.6943
	step [221/255], loss=71.2568
	step [222/255], loss=78.9068
	step [223/255], loss=67.5321
	step [224/255], loss=77.9194
	step [225/255], loss=78.9441
	step [226/255], loss=64.9745
	step [227/255], loss=71.0097
	step [228/255], loss=79.6331
	step [229/255], loss=75.4488
	step [230/255], loss=79.7220
	step [231/255], loss=60.9197
	step [232/255], loss=74.5049
	step [233/255], loss=76.5109
	step [234/255], loss=70.8321
	step [235/255], loss=73.2128
	step [236/255], loss=77.1268
	step [237/255], loss=76.4381
	step [238/255], loss=72.3649
	step [239/255], loss=57.1200
	step [240/255], loss=68.3500
	step [241/255], loss=61.8373
	step [242/255], loss=70.3527
	step [243/255], loss=84.7249
	step [244/255], loss=84.9029
	step [245/255], loss=60.7621
	step [246/255], loss=63.3133
	step [247/255], loss=54.6754
	step [248/255], loss=76.1081
	step [249/255], loss=77.2708
	step [250/255], loss=76.7773
	step [251/255], loss=70.0657
	step [252/255], loss=80.3485
	step [253/255], loss=70.5556
	step [254/255], loss=74.5392
	step [255/255], loss=56.8075
	Evaluating
	loss=0.0132, precision=0.3764, recall=0.9001, f1=0.5308
Training epoch 18
	step [1/255], loss=60.8665
	step [2/255], loss=77.7693
	step [3/255], loss=76.6569
	step [4/255], loss=74.5929
	step [5/255], loss=66.3900
	step [6/255], loss=80.6851
	step [7/255], loss=76.2448
	step [8/255], loss=66.0260
	step [9/255], loss=69.8336
	step [10/255], loss=66.2568
	step [11/255], loss=56.7004
	step [12/255], loss=65.8949
	step [13/255], loss=72.7446
	step [14/255], loss=92.8087
	step [15/255], loss=63.4626
	step [16/255], loss=67.8816
	step [17/255], loss=69.7115
	step [18/255], loss=62.9169
	step [19/255], loss=75.6914
	step [20/255], loss=78.8605
	step [21/255], loss=79.2912
	step [22/255], loss=64.0641
	step [23/255], loss=74.8768
	step [24/255], loss=68.0116
	step [25/255], loss=73.3336
	step [26/255], loss=66.3628
	step [27/255], loss=75.7763
	step [28/255], loss=75.5549
	step [29/255], loss=63.6491
	step [30/255], loss=65.1810
	step [31/255], loss=65.1651
	step [32/255], loss=74.7337
	step [33/255], loss=66.2517
	step [34/255], loss=64.3499
	step [35/255], loss=77.9333
	step [36/255], loss=72.4204
	step [37/255], loss=63.3610
	step [38/255], loss=77.1574
	step [39/255], loss=55.0635
	step [40/255], loss=66.0075
	step [41/255], loss=71.1716
	step [42/255], loss=73.0545
	step [43/255], loss=68.2302
	step [44/255], loss=73.0171
	step [45/255], loss=65.9851
	step [46/255], loss=79.3846
	step [47/255], loss=66.5601
	step [48/255], loss=66.9625
	step [49/255], loss=77.6771
	step [50/255], loss=69.8624
	step [51/255], loss=66.8878
	step [52/255], loss=70.1258
	step [53/255], loss=82.8483
	step [54/255], loss=69.7776
	step [55/255], loss=71.2318
	step [56/255], loss=71.5066
	step [57/255], loss=85.8585
	step [58/255], loss=64.5089
	step [59/255], loss=67.9633
	step [60/255], loss=63.6165
	step [61/255], loss=83.3067
	step [62/255], loss=68.1364
	step [63/255], loss=69.5542
	step [64/255], loss=66.5512
	step [65/255], loss=72.1935
	step [66/255], loss=74.9009
	step [67/255], loss=79.5971
	step [68/255], loss=57.6475
	step [69/255], loss=63.8310
	step [70/255], loss=78.9477
	step [71/255], loss=69.0574
	step [72/255], loss=67.5594
	step [73/255], loss=66.4801
	step [74/255], loss=79.6067
	step [75/255], loss=86.9689
	step [76/255], loss=73.9132
	step [77/255], loss=81.6243
	step [78/255], loss=61.5165
	step [79/255], loss=76.8941
	step [80/255], loss=70.3293
	step [81/255], loss=69.1795
	step [82/255], loss=77.9310
	step [83/255], loss=86.6274
	step [84/255], loss=79.2718
	step [85/255], loss=92.7889
	step [86/255], loss=80.2780
	step [87/255], loss=82.8187
	step [88/255], loss=77.2609
	step [89/255], loss=58.4413
	step [90/255], loss=64.5800
	step [91/255], loss=60.1575
	step [92/255], loss=76.2982
	step [93/255], loss=63.0396
	step [94/255], loss=65.7408
	step [95/255], loss=70.6941
	step [96/255], loss=74.8758
	step [97/255], loss=78.8689
	step [98/255], loss=67.8445
	step [99/255], loss=65.3806
	step [100/255], loss=84.9885
	step [101/255], loss=75.0356
	step [102/255], loss=62.9970
	step [103/255], loss=66.9883
	step [104/255], loss=76.9441
	step [105/255], loss=70.5717
	step [106/255], loss=63.5851
	step [107/255], loss=58.4712
	step [108/255], loss=64.7840
	step [109/255], loss=68.4125
	step [110/255], loss=74.6815
	step [111/255], loss=61.9665
	step [112/255], loss=71.1579
	step [113/255], loss=72.0073
	step [114/255], loss=68.7333
	step [115/255], loss=74.2344
	step [116/255], loss=70.2805
	step [117/255], loss=69.9629
	step [118/255], loss=67.7861
	step [119/255], loss=69.6432
	step [120/255], loss=74.7432
	step [121/255], loss=75.0773
	step [122/255], loss=73.5214
	step [123/255], loss=60.9602
	step [124/255], loss=67.0336
	step [125/255], loss=83.6256
	step [126/255], loss=76.9982
	step [127/255], loss=77.3791
	step [128/255], loss=77.3764
	step [129/255], loss=76.6800
	step [130/255], loss=58.0472
	step [131/255], loss=73.8656
	step [132/255], loss=71.1193
	step [133/255], loss=72.5658
	step [134/255], loss=76.4793
	step [135/255], loss=65.6848
	step [136/255], loss=56.4834
	step [137/255], loss=65.8134
	step [138/255], loss=69.7860
	step [139/255], loss=80.9856
	step [140/255], loss=68.7905
	step [141/255], loss=75.1770
	step [142/255], loss=64.3238
	step [143/255], loss=73.2777
	step [144/255], loss=73.3339
	step [145/255], loss=73.3845
	step [146/255], loss=76.7590
	step [147/255], loss=67.7783
	step [148/255], loss=69.6925
	step [149/255], loss=68.9562
	step [150/255], loss=72.5830
	step [151/255], loss=69.3501
	step [152/255], loss=81.9375
	step [153/255], loss=63.2643
	step [154/255], loss=78.7854
	step [155/255], loss=87.5235
	step [156/255], loss=72.9582
	step [157/255], loss=58.9783
	step [158/255], loss=67.5007
	step [159/255], loss=71.8242
	step [160/255], loss=63.3427
	step [161/255], loss=73.4257
	step [162/255], loss=65.9392
	step [163/255], loss=74.6446
	step [164/255], loss=63.7410
	step [165/255], loss=65.0751
	step [166/255], loss=72.4874
	step [167/255], loss=78.8899
	step [168/255], loss=71.0982
	step [169/255], loss=73.8404
	step [170/255], loss=62.8362
	step [171/255], loss=56.4429
	step [172/255], loss=82.8808
	step [173/255], loss=80.5045
	step [174/255], loss=65.2252
	step [175/255], loss=75.7290
	step [176/255], loss=66.3452
	step [177/255], loss=66.4283
	step [178/255], loss=74.6695
	step [179/255], loss=67.8893
	step [180/255], loss=74.2297
	step [181/255], loss=60.0908
	step [182/255], loss=70.0852
	step [183/255], loss=75.0173
	step [184/255], loss=63.0537
	step [185/255], loss=66.5617
	step [186/255], loss=81.4745
	step [187/255], loss=79.1152
	step [188/255], loss=61.9223
	step [189/255], loss=66.7598
	step [190/255], loss=73.2978
	step [191/255], loss=65.6995
	step [192/255], loss=63.8712
	step [193/255], loss=63.3209
	step [194/255], loss=58.8356
	step [195/255], loss=62.3615
	step [196/255], loss=79.8806
	step [197/255], loss=74.1652
	step [198/255], loss=55.4792
	step [199/255], loss=74.3662
	step [200/255], loss=59.3632
	step [201/255], loss=57.7495
	step [202/255], loss=75.5212
	step [203/255], loss=74.6011
	step [204/255], loss=73.0547
	step [205/255], loss=72.5205
	step [206/255], loss=75.2245
	step [207/255], loss=65.9832
	step [208/255], loss=68.4824
	step [209/255], loss=71.5151
	step [210/255], loss=78.1744
	step [211/255], loss=72.6696
	step [212/255], loss=72.0742
	step [213/255], loss=72.2880
	step [214/255], loss=65.9010
	step [215/255], loss=72.8154
	step [216/255], loss=71.5321
	step [217/255], loss=75.2570
	step [218/255], loss=73.9757
	step [219/255], loss=58.7310
	step [220/255], loss=72.0723
	step [221/255], loss=60.0319
	step [222/255], loss=78.0399
	step [223/255], loss=65.7498
	step [224/255], loss=76.0018
	step [225/255], loss=60.6723
	step [226/255], loss=73.9029
	step [227/255], loss=57.8851
	step [228/255], loss=74.3501
	step [229/255], loss=76.8052
	step [230/255], loss=82.1714
	step [231/255], loss=77.7533
	step [232/255], loss=60.9779
	step [233/255], loss=63.2860
	step [234/255], loss=69.9323
	step [235/255], loss=62.8649
	step [236/255], loss=78.2909
	step [237/255], loss=63.3339
	step [238/255], loss=69.6633
	step [239/255], loss=73.9665
	step [240/255], loss=70.3497
	step [241/255], loss=66.3684
	step [242/255], loss=72.7135
	step [243/255], loss=63.2794
	step [244/255], loss=57.6066
	step [245/255], loss=69.0026
	step [246/255], loss=57.5969
	step [247/255], loss=77.3743
	step [248/255], loss=64.2056
	step [249/255], loss=81.0233
	step [250/255], loss=59.7398
	step [251/255], loss=63.1635
	step [252/255], loss=75.6535
	step [253/255], loss=62.1462
	step [254/255], loss=71.1263
	step [255/255], loss=45.8311
	Evaluating
	loss=0.0141, precision=0.3130, recall=0.8955, f1=0.4639
Training epoch 19
	step [1/255], loss=64.6071
	step [2/255], loss=69.3900
	step [3/255], loss=69.6062
	step [4/255], loss=71.9531
	step [5/255], loss=69.5456
	step [6/255], loss=79.6500
	step [7/255], loss=74.1758
	step [8/255], loss=56.6143
	step [9/255], loss=81.0753
	step [10/255], loss=56.6227
	step [11/255], loss=75.9145
	step [12/255], loss=77.5072
	step [13/255], loss=66.6362
	step [14/255], loss=67.2520
	step [15/255], loss=71.3280
	step [16/255], loss=68.8975
	step [17/255], loss=63.7951
	step [18/255], loss=70.3987
	step [19/255], loss=69.4523
	step [20/255], loss=83.8951
	step [21/255], loss=80.8694
	step [22/255], loss=73.2873
	step [23/255], loss=70.7985
	step [24/255], loss=68.2880
	step [25/255], loss=65.7388
	step [26/255], loss=64.4664
	step [27/255], loss=60.5844
	step [28/255], loss=73.0235
	step [29/255], loss=58.7770
	step [30/255], loss=74.2680
	step [31/255], loss=67.5402
	step [32/255], loss=74.3586
	step [33/255], loss=76.3017
	step [34/255], loss=75.3156
	step [35/255], loss=71.4412
	step [36/255], loss=72.3334
	step [37/255], loss=64.2006
	step [38/255], loss=70.9943
	step [39/255], loss=71.8071
	step [40/255], loss=73.8315
	step [41/255], loss=58.8208
	step [42/255], loss=67.0310
	step [43/255], loss=71.7785
	step [44/255], loss=72.8057
	step [45/255], loss=69.7036
	step [46/255], loss=61.2282
	step [47/255], loss=78.5781
	step [48/255], loss=77.0985
	step [49/255], loss=75.5519
	step [50/255], loss=68.7404
	step [51/255], loss=55.1253
	step [52/255], loss=75.6571
	step [53/255], loss=85.3886
	step [54/255], loss=74.3342
	step [55/255], loss=73.6106
	step [56/255], loss=55.4683
	step [57/255], loss=72.6975
	step [58/255], loss=70.6073
	step [59/255], loss=77.1558
	step [60/255], loss=71.4581
	step [61/255], loss=70.0986
	step [62/255], loss=71.5929
	step [63/255], loss=75.6107
	step [64/255], loss=77.8249
	step [65/255], loss=65.5113
	step [66/255], loss=64.1356
	step [67/255], loss=75.3816
	step [68/255], loss=71.4748
	step [69/255], loss=70.4052
	step [70/255], loss=75.1982
	step [71/255], loss=81.0589
	step [72/255], loss=75.7863
	step [73/255], loss=66.4923
	step [74/255], loss=73.5601
	step [75/255], loss=71.1911
	step [76/255], loss=64.2073
	step [77/255], loss=60.4411
	step [78/255], loss=66.1379
	step [79/255], loss=81.6675
	step [80/255], loss=70.8204
	step [81/255], loss=68.3977
	step [82/255], loss=65.6752
	step [83/255], loss=78.5017
	step [84/255], loss=69.1859
	step [85/255], loss=62.8814
	step [86/255], loss=60.7560
	step [87/255], loss=84.0035
	step [88/255], loss=73.5975
	step [89/255], loss=64.2156
	step [90/255], loss=59.2275
	step [91/255], loss=75.0524
	step [92/255], loss=62.6868
	step [93/255], loss=72.3155
	step [94/255], loss=61.6581
	step [95/255], loss=60.6241
	step [96/255], loss=60.0714
	step [97/255], loss=72.4491
	step [98/255], loss=76.8603
	step [99/255], loss=56.6919
	step [100/255], loss=62.9492
	step [101/255], loss=77.4539
	step [102/255], loss=61.5060
	step [103/255], loss=68.7195
	step [104/255], loss=76.3463
	step [105/255], loss=69.8862
	step [106/255], loss=61.8707
	step [107/255], loss=64.2974
	step [108/255], loss=65.8089
	step [109/255], loss=56.1846
	step [110/255], loss=78.1077
	step [111/255], loss=60.0516
	step [112/255], loss=66.7388
	step [113/255], loss=67.5965
	step [114/255], loss=80.7743
	step [115/255], loss=65.5391
	step [116/255], loss=83.1905
	step [117/255], loss=65.4345
	step [118/255], loss=70.9133
	step [119/255], loss=73.8974
	step [120/255], loss=76.7012
	step [121/255], loss=69.3315
	step [122/255], loss=54.4397
	step [123/255], loss=73.9693
	step [124/255], loss=62.7815
	step [125/255], loss=64.6903
	step [126/255], loss=72.4992
	step [127/255], loss=77.5833
	step [128/255], loss=65.1954
	step [129/255], loss=77.5253
	step [130/255], loss=75.3840
	step [131/255], loss=82.3077
	step [132/255], loss=65.6741
	step [133/255], loss=79.8426
	step [134/255], loss=81.0393
	step [135/255], loss=67.7576
	step [136/255], loss=80.8723
	step [137/255], loss=71.2514
	step [138/255], loss=74.8488
	step [139/255], loss=64.9078
	step [140/255], loss=62.3184
	step [141/255], loss=69.3497
	step [142/255], loss=55.5381
	step [143/255], loss=68.7297
	step [144/255], loss=67.5453
	step [145/255], loss=60.8067
	step [146/255], loss=69.6830
	step [147/255], loss=59.8195
	step [148/255], loss=68.9847
	step [149/255], loss=84.6499
	step [150/255], loss=82.0694
	step [151/255], loss=66.4209
	step [152/255], loss=52.4304
	step [153/255], loss=70.1382
	step [154/255], loss=71.5420
	step [155/255], loss=81.7743
	step [156/255], loss=65.2522
	step [157/255], loss=68.9227
	step [158/255], loss=77.9149
	step [159/255], loss=68.9285
	step [160/255], loss=65.7741
	step [161/255], loss=61.3960
	step [162/255], loss=65.2151
	step [163/255], loss=61.3604
	step [164/255], loss=63.8649
	step [165/255], loss=66.8496
	step [166/255], loss=58.3983
	step [167/255], loss=69.1828
	step [168/255], loss=60.2561
	step [169/255], loss=69.8163
	step [170/255], loss=52.8961
	step [171/255], loss=72.5313
	step [172/255], loss=67.8337
	step [173/255], loss=66.4702
	step [174/255], loss=71.8297
	step [175/255], loss=86.0022
	step [176/255], loss=62.7471
	step [177/255], loss=61.6942
	step [178/255], loss=75.3314
	step [179/255], loss=66.5019
	step [180/255], loss=69.7243
	step [181/255], loss=73.2043
	step [182/255], loss=70.3642
	step [183/255], loss=82.7336
	step [184/255], loss=67.2001
	step [185/255], loss=61.0429
	step [186/255], loss=64.1097
	step [187/255], loss=72.2420
	step [188/255], loss=80.6497
	step [189/255], loss=64.4007
	step [190/255], loss=71.2360
	step [191/255], loss=59.6124
	step [192/255], loss=62.7058
	step [193/255], loss=63.5673
	step [194/255], loss=62.5563
	step [195/255], loss=57.2288
	step [196/255], loss=71.3995
	step [197/255], loss=71.7262
	step [198/255], loss=63.8768
	step [199/255], loss=74.7788
	step [200/255], loss=68.4276
	step [201/255], loss=73.2030
	step [202/255], loss=74.3096
	step [203/255], loss=68.4712
	step [204/255], loss=69.8970
	step [205/255], loss=69.1444
	step [206/255], loss=74.1241
	step [207/255], loss=71.8316
	step [208/255], loss=61.0068
	step [209/255], loss=56.1049
	step [210/255], loss=71.3033
	step [211/255], loss=63.3507
	step [212/255], loss=76.5132
	step [213/255], loss=65.0429
	step [214/255], loss=73.2023
	step [215/255], loss=61.4882
	step [216/255], loss=79.6100
	step [217/255], loss=78.5632
	step [218/255], loss=71.9306
	step [219/255], loss=56.4508
	step [220/255], loss=61.1159
	step [221/255], loss=70.7200
	step [222/255], loss=70.6940
	step [223/255], loss=70.1659
	step [224/255], loss=67.1243
	step [225/255], loss=69.7072
	step [226/255], loss=77.9795
	step [227/255], loss=64.9036
	step [228/255], loss=67.8400
	step [229/255], loss=78.7256
	step [230/255], loss=73.8201
	step [231/255], loss=63.2090
	step [232/255], loss=56.7451
	step [233/255], loss=73.3100
	step [234/255], loss=66.1874
	step [235/255], loss=76.4457
	step [236/255], loss=76.9984
	step [237/255], loss=73.9380
	step [238/255], loss=68.1233
	step [239/255], loss=65.4269
	step [240/255], loss=78.2038
	step [241/255], loss=63.6453
	step [242/255], loss=75.3035
	step [243/255], loss=74.9901
	step [244/255], loss=85.5285
	step [245/255], loss=74.2087
	step [246/255], loss=69.4311
	step [247/255], loss=76.2071
	step [248/255], loss=86.0725
	step [249/255], loss=75.0441
	step [250/255], loss=77.7627
	step [251/255], loss=68.1366
	step [252/255], loss=77.8867
	step [253/255], loss=82.2361
	step [254/255], loss=64.6353
	step [255/255], loss=51.0527
	Evaluating
	loss=0.0113, precision=0.3694, recall=0.9075, f1=0.5251
Training epoch 20
	step [1/255], loss=61.9583
	step [2/255], loss=81.1993
	step [3/255], loss=75.6251
	step [4/255], loss=67.8580
	step [5/255], loss=91.3210
	step [6/255], loss=75.8096
	step [7/255], loss=64.7213
	step [8/255], loss=74.4163
	step [9/255], loss=64.1379
	step [10/255], loss=86.1179
	step [11/255], loss=56.6295
	step [12/255], loss=68.7281
	step [13/255], loss=63.9630
	step [14/255], loss=90.3594
	step [15/255], loss=69.5259
	step [16/255], loss=65.1649
	step [17/255], loss=66.1154
	step [18/255], loss=70.1940
	step [19/255], loss=51.4036
	step [20/255], loss=62.5723
	step [21/255], loss=54.1655
	step [22/255], loss=70.4826
	step [23/255], loss=57.8420
	step [24/255], loss=69.7050
	step [25/255], loss=66.9619
	step [26/255], loss=72.1143
	step [27/255], loss=55.7205
	step [28/255], loss=76.2890
	step [29/255], loss=59.5769
	step [30/255], loss=63.4764
	step [31/255], loss=71.2133
	step [32/255], loss=70.8907
	step [33/255], loss=67.0522
	step [34/255], loss=69.1159
	step [35/255], loss=72.6842
	step [36/255], loss=77.0874
	step [37/255], loss=60.4379
	step [38/255], loss=75.8155
	step [39/255], loss=73.6472
	step [40/255], loss=73.2700
	step [41/255], loss=67.9145
	step [42/255], loss=65.4798
	step [43/255], loss=66.9816
	step [44/255], loss=54.8554
	step [45/255], loss=67.2280
	step [46/255], loss=71.3135
	step [47/255], loss=74.2407
	step [48/255], loss=66.9166
	step [49/255], loss=64.9568
	step [50/255], loss=73.6252
	step [51/255], loss=59.3477
	step [52/255], loss=82.1020
	step [53/255], loss=77.1263
	step [54/255], loss=61.4063
	step [55/255], loss=79.3138
	step [56/255], loss=75.0169
	step [57/255], loss=87.3847
	step [58/255], loss=67.1256
	step [59/255], loss=65.3822
	step [60/255], loss=75.0886
	step [61/255], loss=66.9813
	step [62/255], loss=66.3466
	step [63/255], loss=63.7654
	step [64/255], loss=67.2837
	step [65/255], loss=69.5228
	step [66/255], loss=58.1408
	step [67/255], loss=72.6159
	step [68/255], loss=70.7214
	step [69/255], loss=81.4794
	step [70/255], loss=70.0537
	step [71/255], loss=71.5234
	step [72/255], loss=66.1687
	step [73/255], loss=71.5851
	step [74/255], loss=78.6468
	step [75/255], loss=81.1912
	step [76/255], loss=67.4437
	step [77/255], loss=76.0027
	step [78/255], loss=67.8109
	step [79/255], loss=72.0153
	step [80/255], loss=63.7417
	step [81/255], loss=66.5770
	step [82/255], loss=76.0714
	step [83/255], loss=75.1726
	step [84/255], loss=60.1575
	step [85/255], loss=63.0039
	step [86/255], loss=63.3790
	step [87/255], loss=62.7018
	step [88/255], loss=83.9131
	step [89/255], loss=70.0401
	step [90/255], loss=66.5537
	step [91/255], loss=72.0583
	step [92/255], loss=66.3336
	step [93/255], loss=63.1855
	step [94/255], loss=77.2211
	step [95/255], loss=74.5336
	step [96/255], loss=62.2333
	step [97/255], loss=62.4811
	step [98/255], loss=70.0948
	step [99/255], loss=67.4013
	step [100/255], loss=73.4019
	step [101/255], loss=54.0230
	step [102/255], loss=73.7691
	step [103/255], loss=69.2869
	step [104/255], loss=60.2180
	step [105/255], loss=71.3397
	step [106/255], loss=72.3141
	step [107/255], loss=71.0449
	step [108/255], loss=68.6686
	step [109/255], loss=81.9996
	step [110/255], loss=66.8715
	step [111/255], loss=61.3777
	step [112/255], loss=65.1580
	step [113/255], loss=61.7656
	step [114/255], loss=58.3757
	step [115/255], loss=81.2491
	step [116/255], loss=59.3723
	step [117/255], loss=83.3952
	step [118/255], loss=60.9481
	step [119/255], loss=72.4404
	step [120/255], loss=72.4615
	step [121/255], loss=66.4402
	step [122/255], loss=65.3198
	step [123/255], loss=76.8526
	step [124/255], loss=67.0858
	step [125/255], loss=76.8092
	step [126/255], loss=60.7050
	step [127/255], loss=64.5760
	step [128/255], loss=77.6458
	step [129/255], loss=70.0234
	step [130/255], loss=75.4459
	step [131/255], loss=70.8314
	step [132/255], loss=57.4333
	step [133/255], loss=85.7398
	step [134/255], loss=66.7754
	step [135/255], loss=71.9330
	step [136/255], loss=54.3111
	step [137/255], loss=61.5844
	step [138/255], loss=68.9741
	step [139/255], loss=74.3311
	step [140/255], loss=61.0769
	step [141/255], loss=61.0688
	step [142/255], loss=76.4946
	step [143/255], loss=58.1739
	step [144/255], loss=69.7048
	step [145/255], loss=74.2744
	step [146/255], loss=58.5865
	step [147/255], loss=66.0179
	step [148/255], loss=72.4296
	step [149/255], loss=69.0376
	step [150/255], loss=54.8932
	step [151/255], loss=64.6830
	step [152/255], loss=75.4894
	step [153/255], loss=59.3438
	step [154/255], loss=62.7816
	step [155/255], loss=60.3872
	step [156/255], loss=63.0173
	step [157/255], loss=68.8024
	step [158/255], loss=62.1202
	step [159/255], loss=77.3094
	step [160/255], loss=63.0642
	step [161/255], loss=70.1693
	step [162/255], loss=75.4606
	step [163/255], loss=65.7535
	step [164/255], loss=78.2072
	step [165/255], loss=71.1295
	step [166/255], loss=77.0882
	step [167/255], loss=77.0471
	step [168/255], loss=60.1061
	step [169/255], loss=65.9229
	step [170/255], loss=81.2600
	step [171/255], loss=67.3745
	step [172/255], loss=63.9923
	step [173/255], loss=65.3582
	step [174/255], loss=53.7156
	step [175/255], loss=75.0050
	step [176/255], loss=75.4957
	step [177/255], loss=62.4320
	step [178/255], loss=57.7453
	step [179/255], loss=71.7598
	step [180/255], loss=67.8884
	step [181/255], loss=64.5685
	step [182/255], loss=77.7076
	step [183/255], loss=68.7521
	step [184/255], loss=60.4625
	step [185/255], loss=58.6469
	step [186/255], loss=66.9121
	step [187/255], loss=68.8575
	step [188/255], loss=77.4766
	step [189/255], loss=69.1232
	step [190/255], loss=72.9335
	step [191/255], loss=70.9516
	step [192/255], loss=62.6051
	step [193/255], loss=58.2235
	step [194/255], loss=63.2880
	step [195/255], loss=63.4790
	step [196/255], loss=74.0965
	step [197/255], loss=51.8135
	step [198/255], loss=67.9879
	step [199/255], loss=70.3849
	step [200/255], loss=74.2080
	step [201/255], loss=71.0033
	step [202/255], loss=76.9405
	step [203/255], loss=65.5806
	step [204/255], loss=64.9617
	step [205/255], loss=59.4895
	step [206/255], loss=72.6637
	step [207/255], loss=53.4352
	step [208/255], loss=69.0559
	step [209/255], loss=52.7727
	step [210/255], loss=69.9386
	step [211/255], loss=59.7113
	step [212/255], loss=55.1469
	step [213/255], loss=70.4943
	step [214/255], loss=64.7631
	step [215/255], loss=71.8792
	step [216/255], loss=65.8317
	step [217/255], loss=69.9978
	step [218/255], loss=60.2565
	step [219/255], loss=65.9993
	step [220/255], loss=75.0611
	step [221/255], loss=67.5980
	step [222/255], loss=78.2379
	step [223/255], loss=70.2791
	step [224/255], loss=58.7403
	step [225/255], loss=76.9357
	step [226/255], loss=85.8786
	step [227/255], loss=76.5939
	step [228/255], loss=70.3080
	step [229/255], loss=60.7409
	step [230/255], loss=56.3809
	step [231/255], loss=69.5363
	step [232/255], loss=78.6360
	step [233/255], loss=68.8208
	step [234/255], loss=76.6749
	step [235/255], loss=63.8404
	step [236/255], loss=75.5459
	step [237/255], loss=84.2523
	step [238/255], loss=70.2137
	step [239/255], loss=65.7185
	step [240/255], loss=69.4128
	step [241/255], loss=69.0088
	step [242/255], loss=74.3154
	step [243/255], loss=70.5225
	step [244/255], loss=68.6720
	step [245/255], loss=64.4235
	step [246/255], loss=75.3364
	step [247/255], loss=68.1418
	step [248/255], loss=69.7271
	step [249/255], loss=71.7974
	step [250/255], loss=78.7053
	step [251/255], loss=64.3598
	step [252/255], loss=66.0876
	step [253/255], loss=74.1413
	step [254/255], loss=77.2056
	step [255/255], loss=61.0774
	Evaluating
	loss=0.0107, precision=0.3671, recall=0.9098, f1=0.5232
Training epoch 21
	step [1/255], loss=66.0255
	step [2/255], loss=67.7993
	step [3/255], loss=77.1924
	step [4/255], loss=79.6908
	step [5/255], loss=63.8945
	step [6/255], loss=70.6140
	step [7/255], loss=63.0428
	step [8/255], loss=62.9439
	step [9/255], loss=67.2836
	step [10/255], loss=68.8461
	step [11/255], loss=63.2856
	step [12/255], loss=52.1561
	step [13/255], loss=80.1896
	step [14/255], loss=57.4715
	step [15/255], loss=79.8102
	step [16/255], loss=69.2620
	step [17/255], loss=62.4276
	step [18/255], loss=64.6075
	step [19/255], loss=73.1225
	step [20/255], loss=73.4118
	step [21/255], loss=50.5557
	step [22/255], loss=68.2646
	step [23/255], loss=65.3633
	step [24/255], loss=65.1581
	step [25/255], loss=56.8084
	step [26/255], loss=59.3864
	step [27/255], loss=66.7158
	step [28/255], loss=55.9808
	step [29/255], loss=73.9975
	step [30/255], loss=76.3427
	step [31/255], loss=74.2923
	step [32/255], loss=77.3745
	step [33/255], loss=62.2516
	step [34/255], loss=71.0853
	step [35/255], loss=60.8913
	step [36/255], loss=76.2523
	step [37/255], loss=70.9417
	step [38/255], loss=64.1485
	step [39/255], loss=70.3478
	step [40/255], loss=76.9783
	step [41/255], loss=62.8278
	step [42/255], loss=61.8619
	step [43/255], loss=76.0086
	step [44/255], loss=70.3602
	step [45/255], loss=64.7302
	step [46/255], loss=60.9970
	step [47/255], loss=69.6667
	step [48/255], loss=62.9675
	step [49/255], loss=76.5629
	step [50/255], loss=62.7632
	step [51/255], loss=69.9239
	step [52/255], loss=58.1279
	step [53/255], loss=72.6114
	step [54/255], loss=62.2750
	step [55/255], loss=69.0493
	step [56/255], loss=68.2771
	step [57/255], loss=60.3890
	step [58/255], loss=60.4462
	step [59/255], loss=77.2486
	step [60/255], loss=80.6229
	step [61/255], loss=77.4082
	step [62/255], loss=65.1469
	step [63/255], loss=77.7638
	step [64/255], loss=69.1210
	step [65/255], loss=68.9390
	step [66/255], loss=63.3879
	step [67/255], loss=60.2236
	step [68/255], loss=58.3477
	step [69/255], loss=68.7615
	step [70/255], loss=68.5012
	step [71/255], loss=57.1690
	step [72/255], loss=60.7338
	step [73/255], loss=57.2179
	step [74/255], loss=64.9927
	step [75/255], loss=70.8388
	step [76/255], loss=73.7418
	step [77/255], loss=55.5482
	step [78/255], loss=73.6835
	step [79/255], loss=57.0059
	step [80/255], loss=60.0080
	step [81/255], loss=61.9046
	step [82/255], loss=82.3007
	step [83/255], loss=64.0531
	step [84/255], loss=79.6333
	step [85/255], loss=69.7400
	step [86/255], loss=78.6998
	step [87/255], loss=74.3181
	step [88/255], loss=68.0213
	step [89/255], loss=80.8202
	step [90/255], loss=68.5625
	step [91/255], loss=77.9017
	step [92/255], loss=52.3043
	step [93/255], loss=75.0330
	step [94/255], loss=83.6000
	step [95/255], loss=61.0446
	step [96/255], loss=67.3714
	step [97/255], loss=48.9127
	step [98/255], loss=62.4042
	step [99/255], loss=58.0568
	step [100/255], loss=69.8508
	step [101/255], loss=60.8813
	step [102/255], loss=63.6815
	step [103/255], loss=64.5810
	step [104/255], loss=68.6436
	step [105/255], loss=87.9122
	step [106/255], loss=60.5587
	step [107/255], loss=69.6254
	step [108/255], loss=70.9656
	step [109/255], loss=73.0025
	step [110/255], loss=70.5373
	step [111/255], loss=62.5945
	step [112/255], loss=61.1200
	step [113/255], loss=61.9875
	step [114/255], loss=68.6306
	step [115/255], loss=53.6757
	step [116/255], loss=66.3551
	step [117/255], loss=69.8285
	step [118/255], loss=81.1545
	step [119/255], loss=70.9506
	step [120/255], loss=64.8744
	step [121/255], loss=74.6625
	step [122/255], loss=62.4251
	step [123/255], loss=54.9305
	step [124/255], loss=64.7944
	step [125/255], loss=59.2229
	step [126/255], loss=68.3987
	step [127/255], loss=61.6988
	step [128/255], loss=66.7635
	step [129/255], loss=65.3004
	step [130/255], loss=72.2589
	step [131/255], loss=68.4639
	step [132/255], loss=67.8351
	step [133/255], loss=62.5457
	step [134/255], loss=73.2684
	step [135/255], loss=71.2090
	step [136/255], loss=67.3661
	step [137/255], loss=75.9265
	step [138/255], loss=68.6711
	step [139/255], loss=63.5604
	step [140/255], loss=58.3918
	step [141/255], loss=65.7161
	step [142/255], loss=63.8089
	step [143/255], loss=79.0205
	step [144/255], loss=63.0455
	step [145/255], loss=76.2717
	step [146/255], loss=72.4666
	step [147/255], loss=74.1238
	step [148/255], loss=66.6703
	step [149/255], loss=52.8982
	step [150/255], loss=55.9840
	step [151/255], loss=71.0396
	step [152/255], loss=65.9533
	step [153/255], loss=71.9245
	step [154/255], loss=75.8112
	step [155/255], loss=84.8336
	step [156/255], loss=68.7946
	step [157/255], loss=75.7677
	step [158/255], loss=70.2933
	step [159/255], loss=74.0980
	step [160/255], loss=57.4165
	step [161/255], loss=67.9815
	step [162/255], loss=62.5231
	step [163/255], loss=52.7417
	step [164/255], loss=70.0899
	step [165/255], loss=73.4034
	step [166/255], loss=75.0408
	step [167/255], loss=67.7799
	step [168/255], loss=65.2168
	step [169/255], loss=60.1579
	step [170/255], loss=69.8302
	step [171/255], loss=76.2655
	step [172/255], loss=69.7923
	step [173/255], loss=64.6580
	step [174/255], loss=59.4979
	step [175/255], loss=56.3855
	step [176/255], loss=63.7315
	step [177/255], loss=61.7154
	step [178/255], loss=67.3237
	step [179/255], loss=68.0789
	step [180/255], loss=82.0601
	step [181/255], loss=76.6339
	step [182/255], loss=69.4717
	step [183/255], loss=68.7643
	step [184/255], loss=61.6130
	step [185/255], loss=71.6246
	step [186/255], loss=59.6513
	step [187/255], loss=68.0574
	step [188/255], loss=62.2836
	step [189/255], loss=59.7902
	step [190/255], loss=71.7066
	step [191/255], loss=70.4467
	step [192/255], loss=78.0106
	step [193/255], loss=65.6531
	step [194/255], loss=68.7467
	step [195/255], loss=79.9905
	step [196/255], loss=70.9254
	step [197/255], loss=65.8157
	step [198/255], loss=83.4194
	step [199/255], loss=66.2912
	step [200/255], loss=87.2784
	step [201/255], loss=60.9530
	step [202/255], loss=69.3332
	step [203/255], loss=74.6031
	step [204/255], loss=70.7115
	step [205/255], loss=80.1067
	step [206/255], loss=69.2952
	step [207/255], loss=79.7795
	step [208/255], loss=70.9194
	step [209/255], loss=69.0331
	step [210/255], loss=73.6360
	step [211/255], loss=69.4559
	step [212/255], loss=64.7393
	step [213/255], loss=84.6707
	step [214/255], loss=74.9514
	step [215/255], loss=66.8028
	step [216/255], loss=67.3114
	step [217/255], loss=69.1907
	step [218/255], loss=84.9473
	step [219/255], loss=68.4510
	step [220/255], loss=69.8388
	step [221/255], loss=71.4641
	step [222/255], loss=55.2486
	step [223/255], loss=74.8265
	step [224/255], loss=54.4709
	step [225/255], loss=81.4958
	step [226/255], loss=60.3297
	step [227/255], loss=69.9608
	step [228/255], loss=71.1037
	step [229/255], loss=58.0760
	step [230/255], loss=70.0695
	step [231/255], loss=69.2875
	step [232/255], loss=72.9654
	step [233/255], loss=78.0691
	step [234/255], loss=83.3768
	step [235/255], loss=70.4696
	step [236/255], loss=59.1232
	step [237/255], loss=63.7862
	step [238/255], loss=76.0666
	step [239/255], loss=60.9210
	step [240/255], loss=73.9116
	step [241/255], loss=78.5247
	step [242/255], loss=65.6679
	step [243/255], loss=83.0101
	step [244/255], loss=73.4442
	step [245/255], loss=70.7424
	step [246/255], loss=61.5638
	step [247/255], loss=71.8608
	step [248/255], loss=60.8515
	step [249/255], loss=59.1530
	step [250/255], loss=59.7667
	step [251/255], loss=65.3962
	step [252/255], loss=71.2448
	step [253/255], loss=59.8429
	step [254/255], loss=58.4198
	step [255/255], loss=48.8257
	Evaluating
	loss=0.0103, precision=0.3730, recall=0.9055, f1=0.5284
Training epoch 22
	step [1/255], loss=65.9055
	step [2/255], loss=67.9847
	step [3/255], loss=65.8357
	step [4/255], loss=69.0384
	step [5/255], loss=63.8499
	step [6/255], loss=59.5070
	step [7/255], loss=70.6805
	step [8/255], loss=63.9762
	step [9/255], loss=75.1251
	step [10/255], loss=56.4526
	step [11/255], loss=62.8321
	step [12/255], loss=72.0096
	step [13/255], loss=74.2949
	step [14/255], loss=82.0635
	step [15/255], loss=68.8436
	step [16/255], loss=70.9274
	step [17/255], loss=68.3771
	step [18/255], loss=78.5839
	step [19/255], loss=77.7266
	step [20/255], loss=68.3807
	step [21/255], loss=62.4040
	step [22/255], loss=65.0919
	step [23/255], loss=63.8696
	step [24/255], loss=71.4215
	step [25/255], loss=64.6967
	step [26/255], loss=89.5843
	step [27/255], loss=64.1099
	step [28/255], loss=78.7511
	step [29/255], loss=69.5755
	step [30/255], loss=55.8037
	step [31/255], loss=72.1676
	step [32/255], loss=70.3382
	step [33/255], loss=80.5073
	step [34/255], loss=59.6084
	step [35/255], loss=66.2816
	step [36/255], loss=65.7997
	step [37/255], loss=65.0473
	step [38/255], loss=56.2321
	step [39/255], loss=75.9570
	step [40/255], loss=64.8867
	step [41/255], loss=67.7253
	step [42/255], loss=58.8007
	step [43/255], loss=52.2764
	step [44/255], loss=59.4193
	step [45/255], loss=68.5040
	step [46/255], loss=67.3476
	step [47/255], loss=70.0806
	step [48/255], loss=59.1371
	step [49/255], loss=57.2129
	step [50/255], loss=63.8080
	step [51/255], loss=71.1477
	step [52/255], loss=66.2967
	step [53/255], loss=64.6928
	step [54/255], loss=70.9938
	step [55/255], loss=72.3648
	step [56/255], loss=59.1093
	step [57/255], loss=67.0693
	step [58/255], loss=72.4712
	step [59/255], loss=83.2763
	step [60/255], loss=68.9591
	step [61/255], loss=63.0110
	step [62/255], loss=63.7433
	step [63/255], loss=76.2696
	step [64/255], loss=76.5561
	step [65/255], loss=73.9966
	step [66/255], loss=61.6225
	step [67/255], loss=70.9690
	step [68/255], loss=69.5500
	step [69/255], loss=65.8002
	step [70/255], loss=54.1622
	step [71/255], loss=62.0327
	step [72/255], loss=80.3250
	step [73/255], loss=68.5207
	step [74/255], loss=67.6279
	step [75/255], loss=65.8478
	step [76/255], loss=65.5517
	step [77/255], loss=72.1387
	step [78/255], loss=71.6778
	step [79/255], loss=68.9690
	step [80/255], loss=60.4195
	step [81/255], loss=60.6695
	step [82/255], loss=73.4808
	step [83/255], loss=64.2363
	step [84/255], loss=70.0036
	step [85/255], loss=68.5531
	step [86/255], loss=67.4421
	step [87/255], loss=72.2298
	step [88/255], loss=60.1353
	step [89/255], loss=65.3212
	step [90/255], loss=59.6274
	step [91/255], loss=67.4221
	step [92/255], loss=66.8442
	step [93/255], loss=69.3233
	step [94/255], loss=74.3887
	step [95/255], loss=59.0263
	step [96/255], loss=68.2291
	step [97/255], loss=67.0140
	step [98/255], loss=66.9037
	step [99/255], loss=81.6936
	step [100/255], loss=79.8147
	step [101/255], loss=64.7036
	step [102/255], loss=72.5027
	step [103/255], loss=67.5811
	step [104/255], loss=71.8375
	step [105/255], loss=70.4552
	step [106/255], loss=66.0768
	step [107/255], loss=61.2898
	step [108/255], loss=73.4506
	step [109/255], loss=58.2060
	step [110/255], loss=64.7103
	step [111/255], loss=76.7367
	step [112/255], loss=65.8346
	step [113/255], loss=79.6019
	step [114/255], loss=55.3128
	step [115/255], loss=77.4232
	step [116/255], loss=63.9323
	step [117/255], loss=75.4716
	step [118/255], loss=65.2372
	step [119/255], loss=52.5079
	step [120/255], loss=69.1222
	step [121/255], loss=68.9252
	step [122/255], loss=73.1021
	step [123/255], loss=49.0405
	step [124/255], loss=68.4520
	step [125/255], loss=67.3535
	step [126/255], loss=57.5812
	step [127/255], loss=69.5450
	step [128/255], loss=63.3158
	step [129/255], loss=67.2060
	step [130/255], loss=59.4557
	step [131/255], loss=68.9486
	step [132/255], loss=72.7935
	step [133/255], loss=71.0203
	step [134/255], loss=73.9629
	step [135/255], loss=73.5220
	step [136/255], loss=55.4905
	step [137/255], loss=60.1609
	step [138/255], loss=61.6902
	step [139/255], loss=64.2933
	step [140/255], loss=57.6903
	step [141/255], loss=68.1092
	step [142/255], loss=69.6963
	step [143/255], loss=70.5512
	step [144/255], loss=75.3462
	step [145/255], loss=74.3233
	step [146/255], loss=61.5350
	step [147/255], loss=76.7003
	step [148/255], loss=64.8703
	step [149/255], loss=72.2790
	step [150/255], loss=70.5511
	step [151/255], loss=71.1986
	step [152/255], loss=59.8117
	step [153/255], loss=69.1741
	step [154/255], loss=62.7776
	step [155/255], loss=78.0872
	step [156/255], loss=70.0651
	step [157/255], loss=60.8793
	step [158/255], loss=67.4503
	step [159/255], loss=60.0128
	step [160/255], loss=68.8244
	step [161/255], loss=57.1078
	step [162/255], loss=75.9882
	step [163/255], loss=60.9754
	step [164/255], loss=64.3239
	step [165/255], loss=65.0577
	step [166/255], loss=57.4608
	step [167/255], loss=72.1235
	step [168/255], loss=65.6775
	step [169/255], loss=61.8539
	step [170/255], loss=67.2328
	step [171/255], loss=80.5024
	step [172/255], loss=75.5651
	step [173/255], loss=69.6812
	step [174/255], loss=65.0239
	step [175/255], loss=73.7802
	step [176/255], loss=60.1616
	step [177/255], loss=70.8320
	step [178/255], loss=68.7819
	step [179/255], loss=68.8010
	step [180/255], loss=68.3578
	step [181/255], loss=65.3100
	step [182/255], loss=67.7670
	step [183/255], loss=63.8744
	step [184/255], loss=59.1153
	step [185/255], loss=58.0133
	step [186/255], loss=73.4526
	step [187/255], loss=69.5766
	step [188/255], loss=64.8243
	step [189/255], loss=58.7538
	step [190/255], loss=59.6320
	step [191/255], loss=70.5349
	step [192/255], loss=56.7144
	step [193/255], loss=66.2237
	step [194/255], loss=68.5670
	step [195/255], loss=65.9660
	step [196/255], loss=76.2735
	step [197/255], loss=61.2288
	step [198/255], loss=59.7905
	step [199/255], loss=68.1998
	step [200/255], loss=65.2925
	step [201/255], loss=75.1213
	step [202/255], loss=81.2075
	step [203/255], loss=66.9209
	step [204/255], loss=67.3582
	step [205/255], loss=66.3402
	step [206/255], loss=67.9765
	step [207/255], loss=73.1918
	step [208/255], loss=66.3316
	step [209/255], loss=58.0749
	step [210/255], loss=69.1287
	step [211/255], loss=80.3957
	step [212/255], loss=57.5475
	step [213/255], loss=57.4722
	step [214/255], loss=71.6924
	step [215/255], loss=65.3196
	step [216/255], loss=79.4841
	step [217/255], loss=71.9730
	step [218/255], loss=81.0764
	step [219/255], loss=60.3941
	step [220/255], loss=82.6066
	step [221/255], loss=72.9259
	step [222/255], loss=72.3380
	step [223/255], loss=66.7947
	step [224/255], loss=79.5591
	step [225/255], loss=65.0723
	step [226/255], loss=64.9259
	step [227/255], loss=70.9053
	step [228/255], loss=72.8044
	step [229/255], loss=62.3005
	step [230/255], loss=74.0705
	step [231/255], loss=62.8379
	step [232/255], loss=60.2135
	step [233/255], loss=60.7868
	step [234/255], loss=69.5471
	step [235/255], loss=58.7732
	step [236/255], loss=68.2611
	step [237/255], loss=63.9749
	step [238/255], loss=68.1860
	step [239/255], loss=68.0430
	step [240/255], loss=71.4385
	step [241/255], loss=63.9101
	step [242/255], loss=62.6314
	step [243/255], loss=62.2310
	step [244/255], loss=67.8846
	step [245/255], loss=86.0784
	step [246/255], loss=72.1123
	step [247/255], loss=74.4253
	step [248/255], loss=76.0782
	step [249/255], loss=67.9101
	step [250/255], loss=66.3383
	step [251/255], loss=70.9046
	step [252/255], loss=61.6355
	step [253/255], loss=68.6630
	step [254/255], loss=70.7204
	step [255/255], loss=54.4330
	Evaluating
	loss=0.0100, precision=0.3863, recall=0.9143, f1=0.5432
Training epoch 23
	step [1/255], loss=63.2008
	step [2/255], loss=75.0146
	step [3/255], loss=68.4938
	step [4/255], loss=68.3804
	step [5/255], loss=63.9030
	step [6/255], loss=62.6717
	step [7/255], loss=82.9132
	step [8/255], loss=73.9092
	step [9/255], loss=87.2638
	step [10/255], loss=73.7618
	step [11/255], loss=58.1670
	step [12/255], loss=70.0493
	step [13/255], loss=74.8567
	step [14/255], loss=63.9878
	step [15/255], loss=67.8942
	step [16/255], loss=67.4448
	step [17/255], loss=72.8969
	step [18/255], loss=61.5238
	step [19/255], loss=76.7939
	step [20/255], loss=69.7210
	step [21/255], loss=67.1023
	step [22/255], loss=57.4701
	step [23/255], loss=71.0797
	step [24/255], loss=76.1076
	step [25/255], loss=58.9271
	step [26/255], loss=59.5776
	step [27/255], loss=72.9330
	step [28/255], loss=61.3218
	step [29/255], loss=65.6134
	step [30/255], loss=61.0320
	step [31/255], loss=71.2665
	step [32/255], loss=67.4261
	step [33/255], loss=68.9325
	step [34/255], loss=61.6320
	step [35/255], loss=70.8972
	step [36/255], loss=72.3243
	step [37/255], loss=65.5428
	step [38/255], loss=60.5891
	step [39/255], loss=65.9341
	step [40/255], loss=59.9122
	step [41/255], loss=79.8381
	step [42/255], loss=55.2645
	step [43/255], loss=60.4954
	step [44/255], loss=65.5692
	step [45/255], loss=59.9965
	step [46/255], loss=72.5072
	step [47/255], loss=67.1324
	step [48/255], loss=63.4439
	step [49/255], loss=64.5035
	step [50/255], loss=63.1827
	step [51/255], loss=81.1829
	step [52/255], loss=72.2015
	step [53/255], loss=60.1620
	step [54/255], loss=77.8324
	step [55/255], loss=65.6026
	step [56/255], loss=64.8898
	step [57/255], loss=76.7109
	step [58/255], loss=71.3197
	step [59/255], loss=67.9510
	step [60/255], loss=59.7436
	step [61/255], loss=67.7212
	step [62/255], loss=74.1146
	step [63/255], loss=66.2172
	step [64/255], loss=73.6734
	step [65/255], loss=80.1386
	step [66/255], loss=61.6518
	step [67/255], loss=71.7535
	step [68/255], loss=60.7075
	step [69/255], loss=61.9953
	step [70/255], loss=72.2566
	step [71/255], loss=73.1568
	step [72/255], loss=80.5049
	step [73/255], loss=60.1293
	step [74/255], loss=62.1910
	step [75/255], loss=59.4302
	step [76/255], loss=63.2236
	step [77/255], loss=62.9076
	step [78/255], loss=61.1441
	step [79/255], loss=65.2706
	step [80/255], loss=64.0179
	step [81/255], loss=72.1927
	step [82/255], loss=68.7792
	step [83/255], loss=76.6523
	step [84/255], loss=78.4854
	step [85/255], loss=63.1318
	step [86/255], loss=55.8759
	step [87/255], loss=67.2875
	step [88/255], loss=62.5997
	step [89/255], loss=82.3986
	step [90/255], loss=73.2035
	step [91/255], loss=70.8481
	step [92/255], loss=62.6154
	step [93/255], loss=69.2415
	step [94/255], loss=66.9082
	step [95/255], loss=62.5025
	step [96/255], loss=70.0671
	step [97/255], loss=56.6738
	step [98/255], loss=59.6705
	step [99/255], loss=68.8078
	step [100/255], loss=69.7035
	step [101/255], loss=76.5448
	step [102/255], loss=67.7563
	step [103/255], loss=64.2458
	step [104/255], loss=67.1350
	step [105/255], loss=68.1998
	step [106/255], loss=63.4819
	step [107/255], loss=60.1952
	step [108/255], loss=69.7344
	step [109/255], loss=63.4338
	step [110/255], loss=70.8950
	step [111/255], loss=74.8786
	step [112/255], loss=59.5064
	step [113/255], loss=61.4619
	step [114/255], loss=54.3086
	step [115/255], loss=80.9049
	step [116/255], loss=73.6866
	step [117/255], loss=52.0841
	step [118/255], loss=63.0182
	step [119/255], loss=64.9871
	step [120/255], loss=65.4401
	step [121/255], loss=67.6967
	step [122/255], loss=63.6127
	step [123/255], loss=68.1435
	step [124/255], loss=69.6859
	step [125/255], loss=73.7291
	step [126/255], loss=68.9518
	step [127/255], loss=66.8449
	step [128/255], loss=64.8621
	step [129/255], loss=67.7898
	step [130/255], loss=77.7495
	step [131/255], loss=61.5301
	step [132/255], loss=61.5966
	step [133/255], loss=76.5327
	step [134/255], loss=47.3381
	step [135/255], loss=61.6487
	step [136/255], loss=70.9608
	step [137/255], loss=62.3634
	step [138/255], loss=68.6487
	step [139/255], loss=63.1713
	step [140/255], loss=70.3295
	step [141/255], loss=65.1774
	step [142/255], loss=75.4128
	step [143/255], loss=81.7661
	step [144/255], loss=58.0333
	step [145/255], loss=72.9753
	step [146/255], loss=68.2520
	step [147/255], loss=74.4718
	step [148/255], loss=61.2643
	step [149/255], loss=72.2389
	step [150/255], loss=55.3246
	step [151/255], loss=61.6442
	step [152/255], loss=76.6212
	step [153/255], loss=75.1788
	step [154/255], loss=73.7025
	step [155/255], loss=63.3719
	step [156/255], loss=59.5997
	step [157/255], loss=59.4656
	step [158/255], loss=58.9421
	step [159/255], loss=59.1475
	step [160/255], loss=76.7792
	step [161/255], loss=71.5574
	step [162/255], loss=61.7045
	step [163/255], loss=66.0525
	step [164/255], loss=54.9449
	step [165/255], loss=78.6392
	step [166/255], loss=67.5712
	step [167/255], loss=61.4982
	step [168/255], loss=69.8100
	step [169/255], loss=69.9570
	step [170/255], loss=71.8999
	step [171/255], loss=64.5555
	step [172/255], loss=69.9396
	step [173/255], loss=67.7270
	step [174/255], loss=72.8902
	step [175/255], loss=70.4212
	step [176/255], loss=58.7403
	step [177/255], loss=63.4586
	step [178/255], loss=73.7124
	step [179/255], loss=72.6666
	step [180/255], loss=62.0578
	step [181/255], loss=58.5399
	step [182/255], loss=55.0763
	step [183/255], loss=80.6811
	step [184/255], loss=71.9835
	step [185/255], loss=78.9145
	step [186/255], loss=50.6027
	step [187/255], loss=74.4939
	step [188/255], loss=70.3498
	step [189/255], loss=67.1200
	step [190/255], loss=65.9903
	step [191/255], loss=67.6622
	step [192/255], loss=62.7194
	step [193/255], loss=82.0642
	step [194/255], loss=68.4371
	step [195/255], loss=73.4691
	step [196/255], loss=65.1393
	step [197/255], loss=61.4876
	step [198/255], loss=81.9689
	step [199/255], loss=51.4967
	step [200/255], loss=63.9064
	step [201/255], loss=65.0468
	step [202/255], loss=65.8840
	step [203/255], loss=80.2616
	step [204/255], loss=69.2926
	step [205/255], loss=59.8020
	step [206/255], loss=64.4168
	step [207/255], loss=61.7818
	step [208/255], loss=56.1990
	step [209/255], loss=59.8853
	step [210/255], loss=69.8124
	step [211/255], loss=50.8883
	step [212/255], loss=76.8496
	step [213/255], loss=60.3753
	step [214/255], loss=77.8338
	step [215/255], loss=67.7772
	step [216/255], loss=66.4216
	step [217/255], loss=67.7938
	step [218/255], loss=57.4789
	step [219/255], loss=75.6573
	step [220/255], loss=63.3978
	step [221/255], loss=66.3264
	step [222/255], loss=63.1104
	step [223/255], loss=66.4398
	step [224/255], loss=73.4328
	step [225/255], loss=57.8923
	step [226/255], loss=68.6266
	step [227/255], loss=60.9401
	step [228/255], loss=68.6726
	step [229/255], loss=62.3233
	step [230/255], loss=62.3807
	step [231/255], loss=66.8743
	step [232/255], loss=68.6367
	step [233/255], loss=60.0340
	step [234/255], loss=69.8379
	step [235/255], loss=50.4565
	step [236/255], loss=59.8638
	step [237/255], loss=71.7559
	step [238/255], loss=63.7093
	step [239/255], loss=45.4347
	step [240/255], loss=51.7880
	step [241/255], loss=64.3335
	step [242/255], loss=55.1043
	step [243/255], loss=60.2822
	step [244/255], loss=60.1345
	step [245/255], loss=83.8285
	step [246/255], loss=75.6698
	step [247/255], loss=65.3985
	step [248/255], loss=65.3722
	step [249/255], loss=76.1155
	step [250/255], loss=65.0286
	step [251/255], loss=64.2555
	step [252/255], loss=60.1433
	step [253/255], loss=62.5848
	step [254/255], loss=57.6522
	step [255/255], loss=58.5286
	Evaluating
	loss=0.0097, precision=0.3777, recall=0.9085, f1=0.5335
Training epoch 24
	step [1/255], loss=63.9055
	step [2/255], loss=60.0915
	step [3/255], loss=60.9199
	step [4/255], loss=53.1904
	step [5/255], loss=57.6360
	step [6/255], loss=67.7464
	step [7/255], loss=72.4370
	step [8/255], loss=60.5607
	step [9/255], loss=61.1998
	step [10/255], loss=67.8184
	step [11/255], loss=76.3358
	step [12/255], loss=52.4805
	step [13/255], loss=60.7731
	step [14/255], loss=62.8508
	step [15/255], loss=78.7907
	step [16/255], loss=60.5005
	step [17/255], loss=69.4038
	step [18/255], loss=70.0025
	step [19/255], loss=72.4154
	step [20/255], loss=68.0575
	step [21/255], loss=67.9553
	step [22/255], loss=57.7871
	step [23/255], loss=70.7824
	step [24/255], loss=54.9972
	step [25/255], loss=63.0826
	step [26/255], loss=55.3980
	step [27/255], loss=68.0964
	step [28/255], loss=61.9400
	step [29/255], loss=62.0841
	step [30/255], loss=71.5438
	step [31/255], loss=76.8870
	step [32/255], loss=67.7269
	step [33/255], loss=63.6470
	step [34/255], loss=65.2265
	step [35/255], loss=59.6111
	step [36/255], loss=84.6655
	step [37/255], loss=66.7737
	step [38/255], loss=60.7339
	step [39/255], loss=57.0385
	step [40/255], loss=61.6129
	step [41/255], loss=63.9210
	step [42/255], loss=68.7528
	step [43/255], loss=70.7013
	step [44/255], loss=55.7916
	step [45/255], loss=61.1542
	step [46/255], loss=63.7270
	step [47/255], loss=55.0729
	step [48/255], loss=58.6607
	step [49/255], loss=75.8231
	step [50/255], loss=70.7496
	step [51/255], loss=66.7055
	step [52/255], loss=64.8089
	step [53/255], loss=78.2512
	step [54/255], loss=61.8322
	step [55/255], loss=69.7597
	step [56/255], loss=64.1595
	step [57/255], loss=75.0285
	step [58/255], loss=70.6115
	step [59/255], loss=71.1693
	step [60/255], loss=65.4257
	step [61/255], loss=77.2855
	step [62/255], loss=67.2133
	step [63/255], loss=76.9287
	step [64/255], loss=69.9202
	step [65/255], loss=80.6497
	step [66/255], loss=54.1800
	step [67/255], loss=69.7266
	step [68/255], loss=59.5540
	step [69/255], loss=73.5080
	step [70/255], loss=68.9587
	step [71/255], loss=63.6426
	step [72/255], loss=68.5657
	step [73/255], loss=64.0487
	step [74/255], loss=56.3872
	step [75/255], loss=61.8090
	step [76/255], loss=69.8260
	step [77/255], loss=81.6692
	step [78/255], loss=61.6430
	step [79/255], loss=68.1719
	step [80/255], loss=60.7239
	step [81/255], loss=64.4454
	step [82/255], loss=70.2572
	step [83/255], loss=65.1657
	step [84/255], loss=59.5603
	step [85/255], loss=59.0530
	step [86/255], loss=60.9655
	step [87/255], loss=72.3408
	step [88/255], loss=63.3547
	step [89/255], loss=70.7025
	step [90/255], loss=66.3164
	step [91/255], loss=62.9726
	step [92/255], loss=57.0214
	step [93/255], loss=71.6853
	step [94/255], loss=66.0828
	step [95/255], loss=68.9196
	step [96/255], loss=64.2404
	step [97/255], loss=66.1268
	step [98/255], loss=70.6932
	step [99/255], loss=61.1078
	step [100/255], loss=68.2896
	step [101/255], loss=68.0485
	step [102/255], loss=62.3217
	step [103/255], loss=60.2583
	step [104/255], loss=67.7978
	step [105/255], loss=82.2395
	step [106/255], loss=83.9205
	step [107/255], loss=69.2159
	step [108/255], loss=69.6830
	step [109/255], loss=68.3496
	step [110/255], loss=62.4721
	step [111/255], loss=73.7068
	step [112/255], loss=69.0385
	step [113/255], loss=65.2438
	step [114/255], loss=60.4399
	step [115/255], loss=65.2103
	step [116/255], loss=74.6908
	step [117/255], loss=70.3932
	step [118/255], loss=58.8468
	step [119/255], loss=69.9460
	step [120/255], loss=59.6687
	step [121/255], loss=60.3050
	step [122/255], loss=72.3672
	step [123/255], loss=63.8629
	step [124/255], loss=65.9630
	step [125/255], loss=63.8485
	step [126/255], loss=62.7771
	step [127/255], loss=60.5396
	step [128/255], loss=72.0383
	step [129/255], loss=63.1921
	step [130/255], loss=54.2098
	step [131/255], loss=63.3443
	step [132/255], loss=70.8615
	step [133/255], loss=62.9430
	step [134/255], loss=63.7314
	step [135/255], loss=64.8776
	step [136/255], loss=63.1905
	step [137/255], loss=73.4446
	step [138/255], loss=52.8270
	step [139/255], loss=70.6031
	step [140/255], loss=63.3319
	step [141/255], loss=58.6474
	step [142/255], loss=68.7258
	step [143/255], loss=64.2032
	step [144/255], loss=73.1164
	step [145/255], loss=65.8514
	step [146/255], loss=65.6881
	step [147/255], loss=60.3637
	step [148/255], loss=71.1751
	step [149/255], loss=68.5880
	step [150/255], loss=61.9399
	step [151/255], loss=63.1871
	step [152/255], loss=57.8029
	step [153/255], loss=56.1560
	step [154/255], loss=69.6055
	step [155/255], loss=72.1511
	step [156/255], loss=62.1944
	step [157/255], loss=80.8882
	step [158/255], loss=69.5495
	step [159/255], loss=53.8792
	step [160/255], loss=70.1758
	step [161/255], loss=69.8527
	step [162/255], loss=67.4175
	step [163/255], loss=72.6875
	step [164/255], loss=70.0115
	step [165/255], loss=75.8584
	step [166/255], loss=61.0686
	step [167/255], loss=57.7587
	step [168/255], loss=62.5151
	step [169/255], loss=60.5178
	step [170/255], loss=60.0776
	step [171/255], loss=66.4083
	step [172/255], loss=63.4551
	step [173/255], loss=78.2046
	step [174/255], loss=73.9374
	step [175/255], loss=58.8802
	step [176/255], loss=65.2176
	step [177/255], loss=72.0049
	step [178/255], loss=68.8028
	step [179/255], loss=70.7335
	step [180/255], loss=63.3919
	step [181/255], loss=57.8116
	step [182/255], loss=66.3448
	step [183/255], loss=68.1737
	step [184/255], loss=59.9219
	step [185/255], loss=81.2276
	step [186/255], loss=77.7534
	step [187/255], loss=51.5404
	step [188/255], loss=69.5545
	step [189/255], loss=67.0088
	step [190/255], loss=77.8077
	step [191/255], loss=65.5853
	step [192/255], loss=75.3695
	step [193/255], loss=62.6840
	step [194/255], loss=62.6697
	step [195/255], loss=57.9074
	step [196/255], loss=83.8470
	step [197/255], loss=61.2243
	step [198/255], loss=65.0158
	step [199/255], loss=67.4626
	step [200/255], loss=69.3285
	step [201/255], loss=73.3516
	step [202/255], loss=67.7061
	step [203/255], loss=59.7753
	step [204/255], loss=72.8137
	step [205/255], loss=62.5436
	step [206/255], loss=58.8646
	step [207/255], loss=69.1415
	step [208/255], loss=61.8392
	step [209/255], loss=63.4777
	step [210/255], loss=48.9301
	step [211/255], loss=78.7572
	step [212/255], loss=68.5135
	step [213/255], loss=69.8890
	step [214/255], loss=74.1472
	step [215/255], loss=65.0674
	step [216/255], loss=66.3261
	step [217/255], loss=65.3481
	step [218/255], loss=55.6678
	step [219/255], loss=51.0604
	step [220/255], loss=70.4383
	step [221/255], loss=82.9879
	step [222/255], loss=63.6048
	step [223/255], loss=63.9405
	step [224/255], loss=58.4673
	step [225/255], loss=66.2249
	step [226/255], loss=65.1965
	step [227/255], loss=67.3512
	step [228/255], loss=70.1185
	step [229/255], loss=69.6191
	step [230/255], loss=70.5237
	step [231/255], loss=71.5338
	step [232/255], loss=57.4687
	step [233/255], loss=67.8414
	step [234/255], loss=67.8524
	step [235/255], loss=72.8099
	step [236/255], loss=57.6219
	step [237/255], loss=71.7507
	step [238/255], loss=82.3760
	step [239/255], loss=61.0495
	step [240/255], loss=72.1704
	step [241/255], loss=67.6421
	step [242/255], loss=63.1813
	step [243/255], loss=75.9886
	step [244/255], loss=62.2147
	step [245/255], loss=72.5284
	step [246/255], loss=68.8961
	step [247/255], loss=64.7808
	step [248/255], loss=62.7290
	step [249/255], loss=62.3899
	step [250/255], loss=68.5303
	step [251/255], loss=58.7409
	step [252/255], loss=66.7698
	step [253/255], loss=55.6692
	step [254/255], loss=71.6121
	step [255/255], loss=58.7063
	Evaluating
	loss=0.0101, precision=0.3533, recall=0.9173, f1=0.5102
Training epoch 25
	step [1/255], loss=56.7342
	step [2/255], loss=69.7189
	step [3/255], loss=69.6313
	step [4/255], loss=67.4784
	step [5/255], loss=65.5396
	step [6/255], loss=58.9869
	step [7/255], loss=69.4239
	step [8/255], loss=61.3886
	step [9/255], loss=54.2890
	step [10/255], loss=72.8105
	step [11/255], loss=73.0244
	step [12/255], loss=79.1136
	step [13/255], loss=60.9613
	step [14/255], loss=63.9802
	step [15/255], loss=67.5545
	step [16/255], loss=68.5473
	step [17/255], loss=72.3365
	step [18/255], loss=57.8661
	step [19/255], loss=78.3621
	step [20/255], loss=49.8695
	step [21/255], loss=63.2864
	step [22/255], loss=65.0264
	step [23/255], loss=76.3048
	step [24/255], loss=66.0193
	step [25/255], loss=57.4077
	step [26/255], loss=56.2141
	step [27/255], loss=66.1636
	step [28/255], loss=50.0964
	step [29/255], loss=63.2960
	step [30/255], loss=58.7831
	step [31/255], loss=46.8991
	step [32/255], loss=69.0429
	step [33/255], loss=65.3269
	step [34/255], loss=51.7099
	step [35/255], loss=64.1387
	step [36/255], loss=69.7233
	step [37/255], loss=62.7603
	step [38/255], loss=61.5335
	step [39/255], loss=67.6185
	step [40/255], loss=53.4488
	step [41/255], loss=85.8954
	step [42/255], loss=74.9192
	step [43/255], loss=82.1793
	step [44/255], loss=63.1701
	step [45/255], loss=64.8122
	step [46/255], loss=64.4569
	step [47/255], loss=64.0634
	step [48/255], loss=74.9832
	step [49/255], loss=79.8783
	step [50/255], loss=71.9293
	step [51/255], loss=66.3940
	step [52/255], loss=47.2880
	step [53/255], loss=59.7016
	step [54/255], loss=54.5072
	step [55/255], loss=62.4146
	step [56/255], loss=64.9052
	step [57/255], loss=81.2758
	step [58/255], loss=69.4484
	step [59/255], loss=76.1441
	step [60/255], loss=67.1447
	step [61/255], loss=64.1828
	step [62/255], loss=70.1691
	step [63/255], loss=76.5023
	step [64/255], loss=55.8299
	step [65/255], loss=70.0390
	step [66/255], loss=69.0908
	step [67/255], loss=59.9671
	step [68/255], loss=66.7543
	step [69/255], loss=60.0005
	step [70/255], loss=61.2047
	step [71/255], loss=63.3293
	step [72/255], loss=68.9895
	step [73/255], loss=81.5048
	step [74/255], loss=77.9288
	step [75/255], loss=70.7556
	step [76/255], loss=65.9109
	step [77/255], loss=74.6146
	step [78/255], loss=72.3893
	step [79/255], loss=62.6955
	step [80/255], loss=65.7966
	step [81/255], loss=61.5479
	step [82/255], loss=64.8914
	step [83/255], loss=70.7296
	step [84/255], loss=71.3820
	step [85/255], loss=67.3996
	step [86/255], loss=69.3089
	step [87/255], loss=68.8562
	step [88/255], loss=70.9725
	step [89/255], loss=59.3758
	step [90/255], loss=60.9194
	step [91/255], loss=65.8324
	step [92/255], loss=53.2351
	step [93/255], loss=68.8025
	step [94/255], loss=68.2277
	step [95/255], loss=68.5207
	step [96/255], loss=71.8832
	step [97/255], loss=60.9099
	step [98/255], loss=49.5285
	step [99/255], loss=64.9679
	step [100/255], loss=66.6971
	step [101/255], loss=62.3549
	step [102/255], loss=73.0170
	step [103/255], loss=60.9092
	step [104/255], loss=55.1173
	step [105/255], loss=61.8032
	step [106/255], loss=61.4995
	step [107/255], loss=74.9142
	step [108/255], loss=68.6237
	step [109/255], loss=64.8440
	step [110/255], loss=68.7424
	step [111/255], loss=56.3620
	step [112/255], loss=62.4447
	step [113/255], loss=74.4075
	step [114/255], loss=59.7909
	step [115/255], loss=70.2432
	step [116/255], loss=62.7327
	step [117/255], loss=58.8272
	step [118/255], loss=71.7870
	step [119/255], loss=69.7419
	step [120/255], loss=69.8965
	step [121/255], loss=60.4127
	step [122/255], loss=67.9299
	step [123/255], loss=64.3214
	step [124/255], loss=52.3593
	step [125/255], loss=73.6900
	step [126/255], loss=61.8560
	step [127/255], loss=54.2280
	step [128/255], loss=65.5495
	step [129/255], loss=67.5898
	step [130/255], loss=63.8509
	step [131/255], loss=68.5329
	step [132/255], loss=73.1885
	step [133/255], loss=87.1854
	step [134/255], loss=57.1312
	step [135/255], loss=65.1991
	step [136/255], loss=62.2106
	step [137/255], loss=57.2276
	step [138/255], loss=64.4951
	step [139/255], loss=70.3093
	step [140/255], loss=80.6147
	step [141/255], loss=70.6236
	step [142/255], loss=71.1282
	step [143/255], loss=60.7723
	step [144/255], loss=53.1036
	step [145/255], loss=63.4715
	step [146/255], loss=60.1825
	step [147/255], loss=72.6579
	step [148/255], loss=66.9948
	step [149/255], loss=57.1405
	step [150/255], loss=65.4617
	step [151/255], loss=64.7182
	step [152/255], loss=64.6098
	step [153/255], loss=56.6478
	step [154/255], loss=68.1890
	step [155/255], loss=60.8050
	step [156/255], loss=75.0573
	step [157/255], loss=65.6064
	step [158/255], loss=60.1776
	step [159/255], loss=56.0890
	step [160/255], loss=65.9143
	step [161/255], loss=63.3173
	step [162/255], loss=73.4549
	step [163/255], loss=76.0418
	step [164/255], loss=72.8566
	step [165/255], loss=58.9614
	step [166/255], loss=55.7934
	step [167/255], loss=58.4147
	step [168/255], loss=71.1579
	step [169/255], loss=57.9853
	step [170/255], loss=70.5180
	step [171/255], loss=68.9068
	step [172/255], loss=64.5956
	step [173/255], loss=57.3078
	step [174/255], loss=59.8248
	step [175/255], loss=53.7617
	step [176/255], loss=67.6196
	step [177/255], loss=50.6278
	step [178/255], loss=69.2668
	step [179/255], loss=62.3004
	step [180/255], loss=74.5583
	step [181/255], loss=72.4516
	step [182/255], loss=71.1624
	step [183/255], loss=66.3547
	step [184/255], loss=62.6055
	step [185/255], loss=62.3782
	step [186/255], loss=69.6121
	step [187/255], loss=69.4476
	step [188/255], loss=60.1345
	step [189/255], loss=75.6120
	step [190/255], loss=66.1482
	step [191/255], loss=59.4589
	step [192/255], loss=67.3162
	step [193/255], loss=74.5562
	step [194/255], loss=53.1137
	step [195/255], loss=53.8692
	step [196/255], loss=59.4174
	step [197/255], loss=69.1995
	step [198/255], loss=66.2506
	step [199/255], loss=67.9135
	step [200/255], loss=74.7558
	step [201/255], loss=54.7852
	step [202/255], loss=66.4147
	step [203/255], loss=63.8584
	step [204/255], loss=64.2553
	step [205/255], loss=65.7344
	step [206/255], loss=57.9995
	step [207/255], loss=76.6465
	step [208/255], loss=70.2005
	step [209/255], loss=57.9416
	step [210/255], loss=78.7562
	step [211/255], loss=72.4077
	step [212/255], loss=65.4813
	step [213/255], loss=60.3072
	step [214/255], loss=66.3183
	step [215/255], loss=69.8359
	step [216/255], loss=55.5136
	step [217/255], loss=54.7286
	step [218/255], loss=67.4048
	step [219/255], loss=70.8561
	step [220/255], loss=71.0313
	step [221/255], loss=79.6202
	step [222/255], loss=68.2228
	step [223/255], loss=65.1937
	step [224/255], loss=70.1919
	step [225/255], loss=71.2379
	step [226/255], loss=60.5734
	step [227/255], loss=66.5658
	step [228/255], loss=71.2435
	step [229/255], loss=69.7020
	step [230/255], loss=72.7640
	step [231/255], loss=68.7595
	step [232/255], loss=66.2720
	step [233/255], loss=59.9403
	step [234/255], loss=76.2786
	step [235/255], loss=69.0594
	step [236/255], loss=72.4906
	step [237/255], loss=66.6231
	step [238/255], loss=56.0936
	step [239/255], loss=66.5603
	step [240/255], loss=61.9522
	step [241/255], loss=65.1897
	step [242/255], loss=62.9919
	step [243/255], loss=74.2240
	step [244/255], loss=77.2910
	step [245/255], loss=76.6424
	step [246/255], loss=65.2749
	step [247/255], loss=80.9472
	step [248/255], loss=63.1679
	step [249/255], loss=58.6454
	step [250/255], loss=56.4920
	step [251/255], loss=53.0796
	step [252/255], loss=73.2897
	step [253/255], loss=72.5480
	step [254/255], loss=74.8654
	step [255/255], loss=45.4239
	Evaluating
	loss=0.0091, precision=0.3734, recall=0.9020, f1=0.5282
Training epoch 26
	step [1/255], loss=73.9668
	step [2/255], loss=64.4104
	step [3/255], loss=74.0078
	step [4/255], loss=67.3254
	step [5/255], loss=67.1732
	step [6/255], loss=60.2401
	step [7/255], loss=55.3041
	step [8/255], loss=68.7586
	step [9/255], loss=80.0731
	step [10/255], loss=72.1777
	step [11/255], loss=58.7092
	step [12/255], loss=45.2593
	step [13/255], loss=65.7502
	step [14/255], loss=73.1534
	step [15/255], loss=48.0694
	step [16/255], loss=63.2617
	step [17/255], loss=60.0888
	step [18/255], loss=78.0901
	step [19/255], loss=64.3776
	step [20/255], loss=56.6320
	step [21/255], loss=61.7021
	step [22/255], loss=63.4374
	step [23/255], loss=58.2492
	step [24/255], loss=69.7190
	step [25/255], loss=69.5394
	step [26/255], loss=69.8163
	step [27/255], loss=58.0003
	step [28/255], loss=68.7096
	step [29/255], loss=73.8724
	step [30/255], loss=75.0326
	step [31/255], loss=55.3174
	step [32/255], loss=60.2949
	step [33/255], loss=62.2326
	step [34/255], loss=66.3645
	step [35/255], loss=74.8839
	step [36/255], loss=70.6931
	step [37/255], loss=72.4893
	step [38/255], loss=71.6540
	step [39/255], loss=67.3421
	step [40/255], loss=61.4758
	step [41/255], loss=56.1626
	step [42/255], loss=75.8859
	step [43/255], loss=70.4677
	step [44/255], loss=65.9448
	step [45/255], loss=68.7386
	step [46/255], loss=46.8620
	step [47/255], loss=79.3931
	step [48/255], loss=63.2159
	step [49/255], loss=60.2296
	step [50/255], loss=69.8144
	step [51/255], loss=62.7342
	step [52/255], loss=66.9343
	step [53/255], loss=85.5865
	step [54/255], loss=49.5732
	step [55/255], loss=62.9793
	step [56/255], loss=67.4293
	step [57/255], loss=69.5072
	step [58/255], loss=65.9028
	step [59/255], loss=67.7964
	step [60/255], loss=70.1797
	step [61/255], loss=69.2983
	step [62/255], loss=60.6802
	step [63/255], loss=66.1025
	step [64/255], loss=62.8580
	step [65/255], loss=56.9009
	step [66/255], loss=69.9339
	step [67/255], loss=71.6446
	step [68/255], loss=65.2391
	step [69/255], loss=68.6591
	step [70/255], loss=64.3409
	step [71/255], loss=80.5639
	step [72/255], loss=62.6062
	step [73/255], loss=71.7440
	step [74/255], loss=83.0355
	step [75/255], loss=78.4720
	step [76/255], loss=63.7461
	step [77/255], loss=66.0908
	step [78/255], loss=53.7836
	step [79/255], loss=52.8894
	step [80/255], loss=74.3771
	step [81/255], loss=60.0169
	step [82/255], loss=63.3888
	step [83/255], loss=65.0026
	step [84/255], loss=64.7073
	step [85/255], loss=55.6914
	step [86/255], loss=54.9877
	step [87/255], loss=67.7417
	step [88/255], loss=64.6296
	step [89/255], loss=61.1727
	step [90/255], loss=62.9931
	step [91/255], loss=62.6246
	step [92/255], loss=58.0754
	step [93/255], loss=60.9699
	step [94/255], loss=62.8574
	step [95/255], loss=69.9523
	step [96/255], loss=71.3441
	step [97/255], loss=74.2472
	step [98/255], loss=69.0464
	step [99/255], loss=72.6906
	step [100/255], loss=72.6457
	step [101/255], loss=63.8028
	step [102/255], loss=57.0750
	step [103/255], loss=60.5563
	step [104/255], loss=57.3299
	step [105/255], loss=61.4582
	step [106/255], loss=63.3979
	step [107/255], loss=71.6656
	step [108/255], loss=68.4546
	step [109/255], loss=67.6042
	step [110/255], loss=62.1533
	step [111/255], loss=55.4523
	step [112/255], loss=69.0319
	step [113/255], loss=62.8201
	step [114/255], loss=60.7647
	step [115/255], loss=62.1464
	step [116/255], loss=69.9901
	step [117/255], loss=66.9374
	step [118/255], loss=67.9155
	step [119/255], loss=71.4051
	step [120/255], loss=71.8695
	step [121/255], loss=63.1761
	step [122/255], loss=57.9568
	step [123/255], loss=59.6700
	step [124/255], loss=66.8184
	step [125/255], loss=62.2068
	step [126/255], loss=66.6349
	step [127/255], loss=78.7254
	step [128/255], loss=60.4802
	step [129/255], loss=57.0015
	step [130/255], loss=76.7728
	step [131/255], loss=64.8731
	step [132/255], loss=63.1843
	step [133/255], loss=69.8268
	step [134/255], loss=65.6568
	step [135/255], loss=68.7654
	step [136/255], loss=56.9041
	step [137/255], loss=69.9232
	step [138/255], loss=65.5614
	step [139/255], loss=73.3018
	step [140/255], loss=67.0150
	step [141/255], loss=58.2546
	step [142/255], loss=70.7735
	step [143/255], loss=55.6968
	step [144/255], loss=62.7516
	step [145/255], loss=59.1019
	step [146/255], loss=72.9765
	step [147/255], loss=58.2425
	step [148/255], loss=58.6046
	step [149/255], loss=63.0569
	step [150/255], loss=59.3696
	step [151/255], loss=67.3465
	step [152/255], loss=64.5281
	step [153/255], loss=68.3080
	step [154/255], loss=55.9759
	step [155/255], loss=65.1469
	step [156/255], loss=61.4484
	step [157/255], loss=57.3918
	step [158/255], loss=56.6916
	step [159/255], loss=66.9592
	step [160/255], loss=81.1033
	step [161/255], loss=79.6333
	step [162/255], loss=62.0475
	step [163/255], loss=52.4613
	step [164/255], loss=55.5799
	step [165/255], loss=60.7164
	step [166/255], loss=71.7110
	step [167/255], loss=64.7845
	step [168/255], loss=80.7822
	step [169/255], loss=70.4689
	step [170/255], loss=66.9951
	step [171/255], loss=70.0371
	step [172/255], loss=79.8008
	step [173/255], loss=59.1359
	step [174/255], loss=48.3380
	step [175/255], loss=66.6049
	step [176/255], loss=57.5523
	step [177/255], loss=57.7657
	step [178/255], loss=58.0085
	step [179/255], loss=68.0779
	step [180/255], loss=53.9951
	step [181/255], loss=71.2900
	step [182/255], loss=56.5858
	step [183/255], loss=74.0425
	step [184/255], loss=54.6766
	step [185/255], loss=61.9132
	step [186/255], loss=64.3785
	step [187/255], loss=70.2084
	step [188/255], loss=66.9443
	step [189/255], loss=64.3876
	step [190/255], loss=59.8501
	step [191/255], loss=63.8026
	step [192/255], loss=72.3084
	step [193/255], loss=70.7975
	step [194/255], loss=64.4339
	step [195/255], loss=63.1795
	step [196/255], loss=67.0984
	step [197/255], loss=74.5640
	step [198/255], loss=59.6667
	step [199/255], loss=66.7275
	step [200/255], loss=57.2516
	step [201/255], loss=66.5565
	step [202/255], loss=59.7153
	step [203/255], loss=59.3224
	step [204/255], loss=61.5692
	step [205/255], loss=68.4529
	step [206/255], loss=73.7470
	step [207/255], loss=63.7454
	step [208/255], loss=59.5654
	step [209/255], loss=70.8211
	step [210/255], loss=60.4237
	step [211/255], loss=72.6857
	step [212/255], loss=55.7215
	step [213/255], loss=64.8447
	step [214/255], loss=63.5143
	step [215/255], loss=54.4415
	step [216/255], loss=64.9696
	step [217/255], loss=64.7642
	step [218/255], loss=64.9334
	step [219/255], loss=56.1792
	step [220/255], loss=65.2575
	step [221/255], loss=64.8955
	step [222/255], loss=70.5794
	step [223/255], loss=74.6008
	step [224/255], loss=73.9301
	step [225/255], loss=76.9023
	step [226/255], loss=64.6097
	step [227/255], loss=54.5238
	step [228/255], loss=64.0732
	step [229/255], loss=63.6396
	step [230/255], loss=64.4141
	step [231/255], loss=58.0603
	step [232/255], loss=66.6595
	step [233/255], loss=62.8714
	step [234/255], loss=64.0842
	step [235/255], loss=58.0411
	step [236/255], loss=68.9850
	step [237/255], loss=53.1980
	step [238/255], loss=60.8399
	step [239/255], loss=70.0732
	step [240/255], loss=53.3715
	step [241/255], loss=54.1670
	step [242/255], loss=61.0702
	step [243/255], loss=63.5257
	step [244/255], loss=69.1080
	step [245/255], loss=66.6711
	step [246/255], loss=70.4095
	step [247/255], loss=52.5160
	step [248/255], loss=75.8068
	step [249/255], loss=71.0157
	step [250/255], loss=77.6568
	step [251/255], loss=57.8885
	step [252/255], loss=67.2051
	step [253/255], loss=69.0392
	step [254/255], loss=64.4608
	step [255/255], loss=61.2156
	Evaluating
	loss=0.0090, precision=0.4041, recall=0.9077, f1=0.5592
Training epoch 27
	step [1/255], loss=63.2179
	step [2/255], loss=71.7988
	step [3/255], loss=51.0230
	step [4/255], loss=60.6495
	step [5/255], loss=60.8478
	step [6/255], loss=61.5824
	step [7/255], loss=53.8948
	step [8/255], loss=56.9489
	step [9/255], loss=62.4798
	step [10/255], loss=80.6769
	step [11/255], loss=62.2774
	step [12/255], loss=57.2847
	step [13/255], loss=58.5553
	step [14/255], loss=61.3116
	step [15/255], loss=58.4509
	step [16/255], loss=78.5682
	step [17/255], loss=65.2869
	step [18/255], loss=70.6957
	step [19/255], loss=73.4711
	step [20/255], loss=49.9097
	step [21/255], loss=56.7425
	step [22/255], loss=70.7212
	step [23/255], loss=73.8372
	step [24/255], loss=58.9148
	step [25/255], loss=67.3741
	step [26/255], loss=57.4359
	step [27/255], loss=65.1480
	step [28/255], loss=59.1339
	step [29/255], loss=62.6679
	step [30/255], loss=63.7476
	step [31/255], loss=65.7905
	step [32/255], loss=57.0111
	step [33/255], loss=53.4891
	step [34/255], loss=66.8135
	step [35/255], loss=58.5873
	step [36/255], loss=74.1952
	step [37/255], loss=71.7883
	step [38/255], loss=69.2947
	step [39/255], loss=57.3694
	step [40/255], loss=72.6428
	step [41/255], loss=62.5617
	step [42/255], loss=63.6431
	step [43/255], loss=64.9250
	step [44/255], loss=64.5038
	step [45/255], loss=73.6191
	step [46/255], loss=65.7124
	step [47/255], loss=48.6058
	step [48/255], loss=59.8406
	step [49/255], loss=56.7725
	step [50/255], loss=60.7142
	step [51/255], loss=63.3865
	step [52/255], loss=70.8485
	step [53/255], loss=62.6829
	step [54/255], loss=62.2487
	step [55/255], loss=70.0176
	step [56/255], loss=63.9795
	step [57/255], loss=83.5496
	step [58/255], loss=56.3660
	step [59/255], loss=71.1667
	step [60/255], loss=60.4769
	step [61/255], loss=65.1266
	step [62/255], loss=71.4325
	step [63/255], loss=70.2737
	step [64/255], loss=69.1902
	step [65/255], loss=57.0163
	step [66/255], loss=67.5340
	step [67/255], loss=66.4302
	step [68/255], loss=66.1872
	step [69/255], loss=51.1593
	step [70/255], loss=69.0211
	step [71/255], loss=67.7421
	step [72/255], loss=68.8564
	step [73/255], loss=73.7054
	step [74/255], loss=67.7466
	step [75/255], loss=52.5881
	step [76/255], loss=50.8896
	step [77/255], loss=59.6429
	step [78/255], loss=65.7680
	step [79/255], loss=69.8084
	step [80/255], loss=56.3812
	step [81/255], loss=67.5911
	step [82/255], loss=48.2480
	step [83/255], loss=57.4936
	step [84/255], loss=82.2383
	step [85/255], loss=74.1359
	step [86/255], loss=59.6557
	step [87/255], loss=65.5100
	step [88/255], loss=64.4037
	step [89/255], loss=68.7063
	step [90/255], loss=78.0364
	step [91/255], loss=57.4626
	step [92/255], loss=64.1197
	step [93/255], loss=60.8841
	step [94/255], loss=56.9154
	step [95/255], loss=72.6031
	step [96/255], loss=61.4342
	step [97/255], loss=72.0886
	step [98/255], loss=66.7409
	step [99/255], loss=74.2080
	step [100/255], loss=74.0597
	step [101/255], loss=69.5449
	step [102/255], loss=66.5907
	step [103/255], loss=64.6449
	step [104/255], loss=55.6043
	step [105/255], loss=73.7916
	step [106/255], loss=75.1667
	step [107/255], loss=61.8092
	step [108/255], loss=68.6899
	step [109/255], loss=52.6512
	step [110/255], loss=55.2883
	step [111/255], loss=63.1073
	step [112/255], loss=57.2604
	step [113/255], loss=65.1341
	step [114/255], loss=78.6756
	step [115/255], loss=53.7753
	step [116/255], loss=65.4458
	step [117/255], loss=57.2952
	step [118/255], loss=64.1513
	step [119/255], loss=64.9026
	step [120/255], loss=65.5333
	step [121/255], loss=59.1428
	step [122/255], loss=61.8219
	step [123/255], loss=60.0440
	step [124/255], loss=76.1296
	step [125/255], loss=57.5758
	step [126/255], loss=76.0064
	step [127/255], loss=68.1686
	step [128/255], loss=62.3625
	step [129/255], loss=69.7944
	step [130/255], loss=74.0235
	step [131/255], loss=62.5294
	step [132/255], loss=53.3499
	step [133/255], loss=57.4791
	step [134/255], loss=63.4537
	step [135/255], loss=60.9826
	step [136/255], loss=68.8012
	step [137/255], loss=62.4073
	step [138/255], loss=50.7220
	step [139/255], loss=55.4123
	step [140/255], loss=79.3559
	step [141/255], loss=62.8371
	step [142/255], loss=78.5900
	step [143/255], loss=56.5820
	step [144/255], loss=64.9983
	step [145/255], loss=64.4846
	step [146/255], loss=65.9934
	step [147/255], loss=56.9120
	step [148/255], loss=59.7214
	step [149/255], loss=65.1925
	step [150/255], loss=73.4611
	step [151/255], loss=68.0895
	step [152/255], loss=77.3519
	step [153/255], loss=70.8455
	step [154/255], loss=57.6133
	step [155/255], loss=64.6080
	step [156/255], loss=68.2666
	step [157/255], loss=73.4238
	step [158/255], loss=78.3007
	step [159/255], loss=62.7079
	step [160/255], loss=59.2665
	step [161/255], loss=63.8341
	step [162/255], loss=73.1978
	step [163/255], loss=60.7624
	step [164/255], loss=58.3294
	step [165/255], loss=54.1879
	step [166/255], loss=57.6062
	step [167/255], loss=57.9031
	step [168/255], loss=75.6910
	step [169/255], loss=65.8506
	step [170/255], loss=65.6405
	step [171/255], loss=68.1146
	step [172/255], loss=60.9797
	step [173/255], loss=62.5773
	step [174/255], loss=61.4420
	step [175/255], loss=60.6369
	step [176/255], loss=60.5946
	step [177/255], loss=56.8154
	step [178/255], loss=67.5182
	step [179/255], loss=60.2194
	step [180/255], loss=61.1871
	step [181/255], loss=67.3499
	step [182/255], loss=70.8129
	step [183/255], loss=63.6568
	step [184/255], loss=61.4381
	step [185/255], loss=54.9186
	step [186/255], loss=89.2016
	step [187/255], loss=65.5125
	step [188/255], loss=68.6191
	step [189/255], loss=67.6901
	step [190/255], loss=58.5794
	step [191/255], loss=67.9867
	step [192/255], loss=70.2502
	step [193/255], loss=69.9832
	step [194/255], loss=64.9712
	step [195/255], loss=68.5218
	step [196/255], loss=65.1130
	step [197/255], loss=52.7893
	step [198/255], loss=54.4668
	step [199/255], loss=66.2216
	step [200/255], loss=72.5672
	step [201/255], loss=54.8038
	step [202/255], loss=68.4889
	step [203/255], loss=62.5100
	step [204/255], loss=75.4392
	step [205/255], loss=56.0305
	step [206/255], loss=59.6941
	step [207/255], loss=56.2739
	step [208/255], loss=58.4835
	step [209/255], loss=71.6679
	step [210/255], loss=62.4217
	step [211/255], loss=62.5232
	step [212/255], loss=62.0382
	step [213/255], loss=65.2896
	step [214/255], loss=58.3107
	step [215/255], loss=73.7992
	step [216/255], loss=57.5706
	step [217/255], loss=63.4731
	step [218/255], loss=60.3536
	step [219/255], loss=55.5842
	step [220/255], loss=75.1664
	step [221/255], loss=57.8739
	step [222/255], loss=63.7363
	step [223/255], loss=71.0555
	step [224/255], loss=53.9342
	step [225/255], loss=76.8015
	step [226/255], loss=72.0198
	step [227/255], loss=69.3627
	step [228/255], loss=55.9858
	step [229/255], loss=61.9024
	step [230/255], loss=58.6057
	step [231/255], loss=67.0218
	step [232/255], loss=62.6109
	step [233/255], loss=64.4709
	step [234/255], loss=62.1778
	step [235/255], loss=60.6195
	step [236/255], loss=61.8036
	step [237/255], loss=68.9161
	step [238/255], loss=52.6406
	step [239/255], loss=66.7478
	step [240/255], loss=61.3683
	step [241/255], loss=61.0452
	step [242/255], loss=67.9182
	step [243/255], loss=53.6350
	step [244/255], loss=68.3199
	step [245/255], loss=66.7782
	step [246/255], loss=62.9429
	step [247/255], loss=70.3179
	step [248/255], loss=74.9357
	step [249/255], loss=70.0210
	step [250/255], loss=56.7516
	step [251/255], loss=65.2876
	step [252/255], loss=76.3349
	step [253/255], loss=70.7219
	step [254/255], loss=72.2366
	step [255/255], loss=44.9578
	Evaluating
	loss=0.0080, precision=0.4090, recall=0.9016, f1=0.5627
Training epoch 28
	step [1/255], loss=69.0766
	step [2/255], loss=68.5794
	step [3/255], loss=59.6968
	step [4/255], loss=71.6253
	step [5/255], loss=72.3412
	step [6/255], loss=64.9769
	step [7/255], loss=60.4645
	step [8/255], loss=55.3258
	step [9/255], loss=57.4829
	step [10/255], loss=69.5222
	step [11/255], loss=57.2868
	step [12/255], loss=65.1041
	step [13/255], loss=57.1890
	step [14/255], loss=57.6121
	step [15/255], loss=72.6467
	step [16/255], loss=59.9770
	step [17/255], loss=57.4271
	step [18/255], loss=74.6948
	step [19/255], loss=62.1909
	step [20/255], loss=65.9160
	step [21/255], loss=64.0588
	step [22/255], loss=60.6359
	step [23/255], loss=70.6110
	step [24/255], loss=69.7657
	step [25/255], loss=78.7657
	step [26/255], loss=76.1888
	step [27/255], loss=62.4207
	step [28/255], loss=66.3487
	step [29/255], loss=58.3665
	step [30/255], loss=67.3080
	step [31/255], loss=82.9497
	step [32/255], loss=64.6854
	step [33/255], loss=68.4185
	step [34/255], loss=72.6795
	step [35/255], loss=59.8355
	step [36/255], loss=77.2242
	step [37/255], loss=55.4698
	step [38/255], loss=68.1706
	step [39/255], loss=63.3490
	step [40/255], loss=58.5456
	step [41/255], loss=56.7107
	step [42/255], loss=59.7075
	step [43/255], loss=66.5037
	step [44/255], loss=64.9548
	step [45/255], loss=64.6683
	step [46/255], loss=65.5307
	step [47/255], loss=71.7051
	step [48/255], loss=76.4419
	step [49/255], loss=65.3587
	step [50/255], loss=67.9574
	step [51/255], loss=66.6490
	step [52/255], loss=70.9315
	step [53/255], loss=51.5163
	step [54/255], loss=60.9238
	step [55/255], loss=53.8077
	step [56/255], loss=61.5703
	step [57/255], loss=62.4296
	step [58/255], loss=58.7266
	step [59/255], loss=66.8465
	step [60/255], loss=54.7776
	step [61/255], loss=78.1397
	step [62/255], loss=58.1265
	step [63/255], loss=58.5142
	step [64/255], loss=65.6076
	step [65/255], loss=62.7905
	step [66/255], loss=58.1560
	step [67/255], loss=61.7304
	step [68/255], loss=55.2895
	step [69/255], loss=76.9536
	step [70/255], loss=58.1570
	step [71/255], loss=68.4864
	step [72/255], loss=69.7048
	step [73/255], loss=57.4818
	step [74/255], loss=61.2725
	step [75/255], loss=69.7176
	step [76/255], loss=60.5320
	step [77/255], loss=65.5363
	step [78/255], loss=76.0744
	step [79/255], loss=72.7722
	step [80/255], loss=67.7885
	step [81/255], loss=59.5165
	step [82/255], loss=72.9977
	step [83/255], loss=61.2833
	step [84/255], loss=57.7157
	step [85/255], loss=55.2512
	step [86/255], loss=53.9165
	step [87/255], loss=57.1122
	step [88/255], loss=69.1650
	step [89/255], loss=65.1635
	step [90/255], loss=55.2580
	step [91/255], loss=65.3068
	step [92/255], loss=56.0682
	step [93/255], loss=67.8322
	step [94/255], loss=58.8688
	step [95/255], loss=73.8092
	step [96/255], loss=60.0429
	step [97/255], loss=63.5368
	step [98/255], loss=59.7232
	step [99/255], loss=67.4626
	step [100/255], loss=66.7983
	step [101/255], loss=81.0600
	step [102/255], loss=59.8136
	step [103/255], loss=76.0343
	step [104/255], loss=56.8243
	step [105/255], loss=71.2723
	step [106/255], loss=78.7834
	step [107/255], loss=65.7139
	step [108/255], loss=68.1006
	step [109/255], loss=72.5900
	step [110/255], loss=84.9004
	step [111/255], loss=76.8137
	step [112/255], loss=58.6114
	step [113/255], loss=56.7608
	step [114/255], loss=53.5559
	step [115/255], loss=67.5221
	step [116/255], loss=60.9665
	step [117/255], loss=67.3006
	step [118/255], loss=59.5229
	step [119/255], loss=56.8122
	step [120/255], loss=66.2275
	step [121/255], loss=64.1814
	step [122/255], loss=56.8109
	step [123/255], loss=64.5101
	step [124/255], loss=62.1268
	step [125/255], loss=53.0011
	step [126/255], loss=55.9326
	step [127/255], loss=47.8302
	step [128/255], loss=67.6170
	step [129/255], loss=64.1152
	step [130/255], loss=59.1094
	step [131/255], loss=53.7569
	step [132/255], loss=51.3552
	step [133/255], loss=69.8397
	step [134/255], loss=69.2211
	step [135/255], loss=68.1414
	step [136/255], loss=50.8805
	step [137/255], loss=62.1409
	step [138/255], loss=68.0349
	step [139/255], loss=64.1925
	step [140/255], loss=70.0736
	step [141/255], loss=65.1072
	step [142/255], loss=59.4265
	step [143/255], loss=49.3120
	step [144/255], loss=69.2499
	step [145/255], loss=67.6262
	step [146/255], loss=54.2357
	step [147/255], loss=55.4834
	step [148/255], loss=53.7540
	step [149/255], loss=67.6674
	step [150/255], loss=58.5585
	step [151/255], loss=62.3807
	step [152/255], loss=63.7188
	step [153/255], loss=64.4116
	step [154/255], loss=58.6294
	step [155/255], loss=74.6682
	step [156/255], loss=57.7591
	step [157/255], loss=72.4420
	step [158/255], loss=56.3875
	step [159/255], loss=49.4033
	step [160/255], loss=68.9351
	step [161/255], loss=63.1800
	step [162/255], loss=68.6385
	step [163/255], loss=54.3521
	step [164/255], loss=71.5256
	step [165/255], loss=56.0052
	step [166/255], loss=64.6912
	step [167/255], loss=60.0101
	step [168/255], loss=65.6752
	step [169/255], loss=65.1244
	step [170/255], loss=64.2667
	step [171/255], loss=62.2829
	step [172/255], loss=52.9171
	step [173/255], loss=57.3744
	step [174/255], loss=73.5588
	step [175/255], loss=67.0993
	step [176/255], loss=68.2990
	step [177/255], loss=63.1826
	step [178/255], loss=73.6747
	step [179/255], loss=70.6286
	step [180/255], loss=59.4885
	step [181/255], loss=65.8184
	step [182/255], loss=65.7219
	step [183/255], loss=52.1057
	step [184/255], loss=65.2728
	step [185/255], loss=59.3351
	step [186/255], loss=54.7134
	step [187/255], loss=67.0039
	step [188/255], loss=46.8910
	step [189/255], loss=65.4983
	step [190/255], loss=58.2498
	step [191/255], loss=56.4372
	step [192/255], loss=65.2057
	step [193/255], loss=67.4579
	step [194/255], loss=62.6065
	step [195/255], loss=60.6442
	step [196/255], loss=77.8728
	step [197/255], loss=65.2397
	step [198/255], loss=58.4182
	step [199/255], loss=59.7186
	step [200/255], loss=58.4390
	step [201/255], loss=69.9235
	step [202/255], loss=51.2041
	step [203/255], loss=62.3698
	step [204/255], loss=73.0624
	step [205/255], loss=71.7010
	step [206/255], loss=65.5776
	step [207/255], loss=67.0630
	step [208/255], loss=60.6874
	step [209/255], loss=73.1876
	step [210/255], loss=64.1124
	step [211/255], loss=53.3850
	step [212/255], loss=56.9356
	step [213/255], loss=58.7673
	step [214/255], loss=82.7749
	step [215/255], loss=69.7766
	step [216/255], loss=66.9523
	step [217/255], loss=71.2203
	step [218/255], loss=59.3958
	step [219/255], loss=75.4389
	step [220/255], loss=73.4024
	step [221/255], loss=66.9011
	step [222/255], loss=55.7846
	step [223/255], loss=76.1255
	step [224/255], loss=69.7122
	step [225/255], loss=71.0982
	step [226/255], loss=66.8218
	step [227/255], loss=68.5309
	step [228/255], loss=66.9565
	step [229/255], loss=64.6359
	step [230/255], loss=59.5013
	step [231/255], loss=73.7385
	step [232/255], loss=67.2287
	step [233/255], loss=66.4073
	step [234/255], loss=68.0578
	step [235/255], loss=70.9086
	step [236/255], loss=55.7356
	step [237/255], loss=69.3834
	step [238/255], loss=68.3945
	step [239/255], loss=62.9051
	step [240/255], loss=63.7868
	step [241/255], loss=78.8397
	step [242/255], loss=61.9329
	step [243/255], loss=48.9013
	step [244/255], loss=69.7440
	step [245/255], loss=63.4996
	step [246/255], loss=55.5350
	step [247/255], loss=59.6404
	step [248/255], loss=56.0006
	step [249/255], loss=59.8909
	step [250/255], loss=76.9361
	step [251/255], loss=64.9531
	step [252/255], loss=63.2831
	step [253/255], loss=68.3549
	step [254/255], loss=48.4782
	step [255/255], loss=42.8363
	Evaluating
	loss=0.0082, precision=0.3962, recall=0.8933, f1=0.5489
Training epoch 29
	step [1/255], loss=60.6834
	step [2/255], loss=65.2346
	step [3/255], loss=58.7363
	step [4/255], loss=69.3559
	step [5/255], loss=63.7062
	step [6/255], loss=62.2823
	step [7/255], loss=72.9198
	step [8/255], loss=58.6943
	step [9/255], loss=59.6930
	step [10/255], loss=74.0691
	step [11/255], loss=64.2500
	step [12/255], loss=64.1692
	step [13/255], loss=63.5506
	step [14/255], loss=58.4159
	step [15/255], loss=75.6333
	step [16/255], loss=62.4954
	step [17/255], loss=59.4037
	step [18/255], loss=64.6720
	step [19/255], loss=65.7362
	step [20/255], loss=64.6252
	step [21/255], loss=54.2828
	step [22/255], loss=64.5239
	step [23/255], loss=52.0155
	step [24/255], loss=52.9854
	step [25/255], loss=51.7282
	step [26/255], loss=66.1412
	step [27/255], loss=51.8374
	step [28/255], loss=61.9563
	step [29/255], loss=58.0823
	step [30/255], loss=67.7522
	step [31/255], loss=71.9914
	step [32/255], loss=73.4988
	step [33/255], loss=69.7593
	step [34/255], loss=54.9520
	step [35/255], loss=62.4244
	step [36/255], loss=73.1075
	step [37/255], loss=54.8055
	step [38/255], loss=71.6887
	step [39/255], loss=54.7089
	step [40/255], loss=58.0055
	step [41/255], loss=62.6046
	step [42/255], loss=57.0861
	step [43/255], loss=64.3707
	step [44/255], loss=69.2291
	step [45/255], loss=59.5450
	step [46/255], loss=63.6381
	step [47/255], loss=53.1550
	step [48/255], loss=66.4059
	step [49/255], loss=54.5257
	step [50/255], loss=64.9647
	step [51/255], loss=62.2727
	step [52/255], loss=60.8994
	step [53/255], loss=63.0468
	step [54/255], loss=62.4811
	step [55/255], loss=47.9863
	step [56/255], loss=73.7845
	step [57/255], loss=60.8726
	step [58/255], loss=66.0993
	step [59/255], loss=68.8819
	step [60/255], loss=71.6799
	step [61/255], loss=69.8834
	step [62/255], loss=58.4979
	step [63/255], loss=63.5072
	step [64/255], loss=62.0625
	step [65/255], loss=63.1363
	step [66/255], loss=60.5281
	step [67/255], loss=69.2753
	step [68/255], loss=59.2978
	step [69/255], loss=63.2599
	step [70/255], loss=77.4328
	step [71/255], loss=63.3389
	step [72/255], loss=78.6548
	step [73/255], loss=62.0527
	step [74/255], loss=71.8166
	step [75/255], loss=62.2024
	step [76/255], loss=66.1340
	step [77/255], loss=53.6688
	step [78/255], loss=77.5682
	step [79/255], loss=67.8892
	step [80/255], loss=70.3072
	step [81/255], loss=66.8895
	step [82/255], loss=78.7013
	step [83/255], loss=71.1789
	step [84/255], loss=65.1071
	step [85/255], loss=66.4804
	step [86/255], loss=71.8038
	step [87/255], loss=71.3737
	step [88/255], loss=68.8195
	step [89/255], loss=53.2291
	step [90/255], loss=60.7003
	step [91/255], loss=56.1109
	step [92/255], loss=80.4146
	step [93/255], loss=63.3950
	step [94/255], loss=73.4230
	step [95/255], loss=58.3801
	step [96/255], loss=62.0635
	step [97/255], loss=69.4807
	step [98/255], loss=65.8941
	step [99/255], loss=67.3689
	step [100/255], loss=78.9652
	step [101/255], loss=57.8331
	step [102/255], loss=70.4467
	step [103/255], loss=66.0948
	step [104/255], loss=59.2009
	step [105/255], loss=64.8780
	step [106/255], loss=66.6333
	step [107/255], loss=61.5224
	step [108/255], loss=62.3627
	step [109/255], loss=66.9712
	step [110/255], loss=62.6624
	step [111/255], loss=55.2924
	step [112/255], loss=68.8364
	step [113/255], loss=59.3115
	step [114/255], loss=70.9357
	step [115/255], loss=59.0855
	step [116/255], loss=73.6180
	step [117/255], loss=65.6539
	step [118/255], loss=60.4060
	step [119/255], loss=69.7611
	step [120/255], loss=57.2382
	step [121/255], loss=63.7045
	step [122/255], loss=65.8303
	step [123/255], loss=48.3238
	step [124/255], loss=68.9882
	step [125/255], loss=53.0503
	step [126/255], loss=56.9966
	step [127/255], loss=59.3910
	step [128/255], loss=72.0376
	step [129/255], loss=60.6271
	step [130/255], loss=71.4582
	step [131/255], loss=64.5226
	step [132/255], loss=79.0291
	step [133/255], loss=71.3308
	step [134/255], loss=62.9713
	step [135/255], loss=61.8616
	step [136/255], loss=65.8539
	step [137/255], loss=73.5016
	step [138/255], loss=49.5794
	step [139/255], loss=52.1995
	step [140/255], loss=63.6495
	step [141/255], loss=58.9199
	step [142/255], loss=53.5493
	step [143/255], loss=78.5226
	step [144/255], loss=65.6052
	step [145/255], loss=75.0392
	step [146/255], loss=61.1044
	step [147/255], loss=68.7386
	step [148/255], loss=59.1612
	step [149/255], loss=62.9094
	step [150/255], loss=64.9413
	step [151/255], loss=52.3515
	step [152/255], loss=58.6040
	step [153/255], loss=57.1607
	step [154/255], loss=79.1349
	step [155/255], loss=68.1960
	step [156/255], loss=65.6343
	step [157/255], loss=57.0815
	step [158/255], loss=60.7058
	step [159/255], loss=70.0771
	step [160/255], loss=53.6900
	step [161/255], loss=57.4832
	step [162/255], loss=62.2060
	step [163/255], loss=64.1362
	step [164/255], loss=73.1802
	step [165/255], loss=60.7481
	step [166/255], loss=58.6301
	step [167/255], loss=66.3122
	step [168/255], loss=53.8532
	step [169/255], loss=62.7539
	step [170/255], loss=66.4422
	step [171/255], loss=70.6368
	step [172/255], loss=68.1194
	step [173/255], loss=61.2331
	step [174/255], loss=74.1186
	step [175/255], loss=59.2334
	step [176/255], loss=69.7516
	step [177/255], loss=65.4157
	step [178/255], loss=77.0953
	step [179/255], loss=70.1907
	step [180/255], loss=66.2563
	step [181/255], loss=73.3403
	step [182/255], loss=65.1594
	step [183/255], loss=61.0117
	step [184/255], loss=62.3746
	step [185/255], loss=68.5310
	step [186/255], loss=61.0854
	step [187/255], loss=56.6371
	step [188/255], loss=63.7320
	step [189/255], loss=71.6733
	step [190/255], loss=66.8107
	step [191/255], loss=55.1118
	step [192/255], loss=58.4841
	step [193/255], loss=64.4909
	step [194/255], loss=65.8382
	step [195/255], loss=68.4958
	step [196/255], loss=52.2034
	step [197/255], loss=51.1507
	step [198/255], loss=58.5373
	step [199/255], loss=53.1002
	step [200/255], loss=67.2370
	step [201/255], loss=59.5885
	step [202/255], loss=55.1629
	step [203/255], loss=61.2219
	step [204/255], loss=55.1430
	step [205/255], loss=68.5457
	step [206/255], loss=63.8857
	step [207/255], loss=62.8248
	step [208/255], loss=62.3171
	step [209/255], loss=63.3665
	step [210/255], loss=59.1582
	step [211/255], loss=67.6432
	step [212/255], loss=68.8110
	step [213/255], loss=65.9046
	step [214/255], loss=60.0233
	step [215/255], loss=58.2668
	step [216/255], loss=57.4426
	step [217/255], loss=63.3181
	step [218/255], loss=60.3322
	step [219/255], loss=65.1909
	step [220/255], loss=60.2967
	step [221/255], loss=72.3937
	step [222/255], loss=64.5435
	step [223/255], loss=68.4862
	step [224/255], loss=58.6410
	step [225/255], loss=58.4203
	step [226/255], loss=55.6335
	step [227/255], loss=66.7480
	step [228/255], loss=63.2152
	step [229/255], loss=59.3722
	step [230/255], loss=53.6105
	step [231/255], loss=75.2911
	step [232/255], loss=59.2249
	step [233/255], loss=53.1023
	step [234/255], loss=70.0470
	step [235/255], loss=74.0709
	step [236/255], loss=51.1306
	step [237/255], loss=54.7186
	step [238/255], loss=63.4539
	step [239/255], loss=78.7132
	step [240/255], loss=57.3067
	step [241/255], loss=46.0375
	step [242/255], loss=71.5418
	step [243/255], loss=61.5794
	step [244/255], loss=66.4370
	step [245/255], loss=62.3605
	step [246/255], loss=57.0452
	step [247/255], loss=63.6646
	step [248/255], loss=51.5454
	step [249/255], loss=58.9392
	step [250/255], loss=58.7953
	step [251/255], loss=60.6113
	step [252/255], loss=60.5038
	step [253/255], loss=67.0891
	step [254/255], loss=56.7349
	step [255/255], loss=54.6044
	Evaluating
	loss=0.0086, precision=0.3713, recall=0.8978, f1=0.5253
Training epoch 30
	step [1/255], loss=57.4535
	step [2/255], loss=62.7893
	step [3/255], loss=65.5661
	step [4/255], loss=53.3362
	step [5/255], loss=61.2715
	step [6/255], loss=61.1354
	step [7/255], loss=59.4482
	step [8/255], loss=67.3804
	step [9/255], loss=56.1010
	step [10/255], loss=63.8037
	step [11/255], loss=64.0626
	step [12/255], loss=50.5719
	step [13/255], loss=52.8616
	step [14/255], loss=55.6997
	step [15/255], loss=59.6681
	step [16/255], loss=63.9437
	step [17/255], loss=66.0083
	step [18/255], loss=70.0286
	step [19/255], loss=61.5704
	step [20/255], loss=56.8988
	step [21/255], loss=68.7820
	step [22/255], loss=67.9578
	step [23/255], loss=52.6302
	step [24/255], loss=57.9709
	step [25/255], loss=59.6094
	step [26/255], loss=62.5509
	step [27/255], loss=58.7779
	step [28/255], loss=71.0949
	step [29/255], loss=60.4434
	step [30/255], loss=59.7030
	step [31/255], loss=56.6865
	step [32/255], loss=71.7999
	step [33/255], loss=62.0631
	step [34/255], loss=47.1686
	step [35/255], loss=75.1809
	step [36/255], loss=67.6127
	step [37/255], loss=72.8509
	step [38/255], loss=66.0941
	step [39/255], loss=57.6798
	step [40/255], loss=60.8176
	step [41/255], loss=67.3403
	step [42/255], loss=58.8503
	step [43/255], loss=59.7382
	step [44/255], loss=57.1859
	step [45/255], loss=66.7758
	step [46/255], loss=63.0935
	step [47/255], loss=63.0392
	step [48/255], loss=60.0266
	step [49/255], loss=75.4698
	step [50/255], loss=64.9539
	step [51/255], loss=77.4804
	step [52/255], loss=60.4129
	step [53/255], loss=64.3021
	step [54/255], loss=66.3731
	step [55/255], loss=56.9875
	step [56/255], loss=70.2125
	step [57/255], loss=62.5200
	step [58/255], loss=49.7907
	step [59/255], loss=60.0222
	step [60/255], loss=63.9139
	step [61/255], loss=57.3793
	step [62/255], loss=70.2298
	step [63/255], loss=68.3193
	step [64/255], loss=66.2664
	step [65/255], loss=53.8086
	step [66/255], loss=68.0317
	step [67/255], loss=53.3005
	step [68/255], loss=63.9655
	step [69/255], loss=62.0893
	step [70/255], loss=46.5480
	step [71/255], loss=64.9100
	step [72/255], loss=69.6933
	step [73/255], loss=74.5485
	step [74/255], loss=49.3462
	step [75/255], loss=59.7362
	step [76/255], loss=58.3570
	step [77/255], loss=69.3285
	step [78/255], loss=68.9084
	step [79/255], loss=59.3793
	step [80/255], loss=64.8033
	step [81/255], loss=70.2719
	step [82/255], loss=62.9640
	step [83/255], loss=72.8259
	step [84/255], loss=66.2395
	step [85/255], loss=63.6589
	step [86/255], loss=67.3181
	step [87/255], loss=66.3205
	step [88/255], loss=55.7862
	step [89/255], loss=61.6688
	step [90/255], loss=72.8798
	step [91/255], loss=64.4730
	step [92/255], loss=65.1607
	step [93/255], loss=56.0725
	step [94/255], loss=61.9489
	step [95/255], loss=58.2871
	step [96/255], loss=55.8597
	step [97/255], loss=80.1282
	step [98/255], loss=66.6600
	step [99/255], loss=77.3115
	step [100/255], loss=64.0267
	step [101/255], loss=65.3050
	step [102/255], loss=52.3587
	step [103/255], loss=70.3053
	step [104/255], loss=57.9793
	step [105/255], loss=65.1869
	step [106/255], loss=59.3425
	step [107/255], loss=64.2849
	step [108/255], loss=72.4142
	step [109/255], loss=70.9713
	step [110/255], loss=69.6039
	step [111/255], loss=68.0546
	step [112/255], loss=52.6586
	step [113/255], loss=75.4212
	step [114/255], loss=62.1659
	step [115/255], loss=61.4243
	step [116/255], loss=61.8204
	step [117/255], loss=52.2524
	step [118/255], loss=58.1706
	step [119/255], loss=69.9291
	step [120/255], loss=51.4641
	step [121/255], loss=48.5724
	step [122/255], loss=59.5012
	step [123/255], loss=70.6477
	step [124/255], loss=61.6521
	step [125/255], loss=66.7959
	step [126/255], loss=63.5425
	step [127/255], loss=65.0505
	step [128/255], loss=67.4141
	step [129/255], loss=61.7326
	step [130/255], loss=56.5895
	step [131/255], loss=65.0568
	step [132/255], loss=59.1942
	step [133/255], loss=68.4763
	step [134/255], loss=63.3380
	step [135/255], loss=64.6332
	step [136/255], loss=62.2390
	step [137/255], loss=63.8814
	step [138/255], loss=57.6742
	step [139/255], loss=65.0619
	step [140/255], loss=58.2919
	step [141/255], loss=57.9500
	step [142/255], loss=70.3920
	step [143/255], loss=62.2259
	step [144/255], loss=65.1617
	step [145/255], loss=72.9530
	step [146/255], loss=72.2113
	step [147/255], loss=60.4871
	step [148/255], loss=73.7050
	step [149/255], loss=67.5303
	step [150/255], loss=57.8651
	step [151/255], loss=53.2701
	step [152/255], loss=71.8004
	step [153/255], loss=61.8564
	step [154/255], loss=62.5175
	step [155/255], loss=55.3287
	step [156/255], loss=60.1943
	step [157/255], loss=63.7934
	step [158/255], loss=55.2890
	step [159/255], loss=59.5817
	step [160/255], loss=58.7611
	step [161/255], loss=55.1802
	step [162/255], loss=68.2792
	step [163/255], loss=64.0740
	step [164/255], loss=69.3858
	step [165/255], loss=75.7455
	step [166/255], loss=60.2028
	step [167/255], loss=67.1008
	step [168/255], loss=62.1187
	step [169/255], loss=62.8542
	step [170/255], loss=58.5471
	step [171/255], loss=59.3058
	step [172/255], loss=71.3762
	step [173/255], loss=51.4466
	step [174/255], loss=54.2273
	step [175/255], loss=68.7324
	step [176/255], loss=64.2747
	step [177/255], loss=56.9416
	step [178/255], loss=59.1460
	step [179/255], loss=56.3186
	step [180/255], loss=77.2959
	step [181/255], loss=54.5879
	step [182/255], loss=47.5764
	step [183/255], loss=60.1199
	step [184/255], loss=62.4997
	step [185/255], loss=63.8513
	step [186/255], loss=57.5488
	step [187/255], loss=70.4510
	step [188/255], loss=60.9147
	step [189/255], loss=73.7785
	step [190/255], loss=68.3293
	step [191/255], loss=62.3595
	step [192/255], loss=65.5591
	step [193/255], loss=72.4420
	step [194/255], loss=67.0350
	step [195/255], loss=70.8113
	step [196/255], loss=68.9363
	step [197/255], loss=60.7140
	step [198/255], loss=68.4486
	step [199/255], loss=61.4132
	step [200/255], loss=64.0478
	step [201/255], loss=49.5603
	step [202/255], loss=64.3164
	step [203/255], loss=56.5291
	step [204/255], loss=58.9393
	step [205/255], loss=59.5197
	step [206/255], loss=65.9184
	step [207/255], loss=56.1976
	step [208/255], loss=62.3100
	step [209/255], loss=69.7159
	step [210/255], loss=55.0858
	step [211/255], loss=70.8007
	step [212/255], loss=71.0598
	step [213/255], loss=54.6262
	step [214/255], loss=66.7956
	step [215/255], loss=62.0123
	step [216/255], loss=55.7756
	step [217/255], loss=73.6076
	step [218/255], loss=64.1811
	step [219/255], loss=64.7373
	step [220/255], loss=60.7974
	step [221/255], loss=67.4682
	step [222/255], loss=56.5913
	step [223/255], loss=69.9386
	step [224/255], loss=66.6316
	step [225/255], loss=63.3608
	step [226/255], loss=75.6742
	step [227/255], loss=64.1721
	step [228/255], loss=57.9477
	step [229/255], loss=55.3358
	step [230/255], loss=53.5045
	step [231/255], loss=72.7437
	step [232/255], loss=57.7256
	step [233/255], loss=71.5261
	step [234/255], loss=54.9785
	step [235/255], loss=69.2800
	step [236/255], loss=55.6709
	step [237/255], loss=54.5037
	step [238/255], loss=81.9211
	step [239/255], loss=70.2555
	step [240/255], loss=50.2471
	step [241/255], loss=59.7876
	step [242/255], loss=71.4058
	step [243/255], loss=49.5606
	step [244/255], loss=57.2678
	step [245/255], loss=55.2705
	step [246/255], loss=57.3660
	step [247/255], loss=90.0920
	step [248/255], loss=62.3577
	step [249/255], loss=73.2606
	step [250/255], loss=67.8551
	step [251/255], loss=50.6096
	step [252/255], loss=74.9371
	step [253/255], loss=55.7494
	step [254/255], loss=71.5255
	step [255/255], loss=60.5377
	Evaluating
	loss=0.0088, precision=0.3654, recall=0.8916, f1=0.5183
Training finished
best_f1: 0.5637342226936225
directing: X rim_enhanced: True test_id 1
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2708 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/191], loss=634.2084
	step [2/191], loss=434.8246
	step [3/191], loss=352.3488
	step [4/191], loss=274.3252
	step [5/191], loss=251.5195
	step [6/191], loss=238.2382
	step [7/191], loss=200.6801
	step [8/191], loss=206.1939
	step [9/191], loss=221.2197
	step [10/191], loss=224.5754
	step [11/191], loss=232.6620
	step [12/191], loss=212.1111
	step [13/191], loss=216.2617
	step [14/191], loss=222.0741
	step [15/191], loss=194.4212
	step [16/191], loss=211.4942
	step [17/191], loss=188.1515
	step [18/191], loss=230.9800
	step [19/191], loss=195.5560
	step [20/191], loss=181.0580
	step [21/191], loss=180.5334
	step [22/191], loss=202.4350
	step [23/191], loss=192.8246
	step [24/191], loss=198.9465
	step [25/191], loss=169.0592
	step [26/191], loss=179.2491
	step [27/191], loss=181.4189
	step [28/191], loss=197.8374
	step [29/191], loss=189.6081
	step [30/191], loss=175.8533
	step [31/191], loss=176.3180
	step [32/191], loss=180.7477
	step [33/191], loss=189.1909
	step [34/191], loss=185.9633
	step [35/191], loss=189.6476
	step [36/191], loss=191.6071
	step [37/191], loss=172.5328
	step [38/191], loss=161.0580
	step [39/191], loss=179.9794
	step [40/191], loss=186.8850
	step [41/191], loss=184.9929
	step [42/191], loss=175.1635
	step [43/191], loss=193.3154
	step [44/191], loss=160.5814
	step [45/191], loss=186.3803
	step [46/191], loss=188.4227
	step [47/191], loss=157.3823
	step [48/191], loss=156.7617
	step [49/191], loss=175.7171
	step [50/191], loss=178.7401
	step [51/191], loss=160.1870
	step [52/191], loss=161.6404
	step [53/191], loss=159.5861
	step [54/191], loss=186.7764
	step [55/191], loss=153.9559
	step [56/191], loss=156.5837
	step [57/191], loss=169.6243
	step [58/191], loss=155.4231
	step [59/191], loss=164.2054
	step [60/191], loss=168.1892
	step [61/191], loss=170.9199
	step [62/191], loss=155.2950
	step [63/191], loss=167.1338
	step [64/191], loss=166.4127
	step [65/191], loss=152.5677
	step [66/191], loss=164.9646
	step [67/191], loss=170.5895
	step [68/191], loss=162.6170
	step [69/191], loss=183.7066
	step [70/191], loss=155.5692
	step [71/191], loss=150.7641
	step [72/191], loss=176.9170
	step [73/191], loss=138.9347
	step [74/191], loss=154.9963
	step [75/191], loss=160.9696
	step [76/191], loss=141.5581
	step [77/191], loss=141.3710
	step [78/191], loss=163.5420
	step [79/191], loss=132.2802
	step [80/191], loss=145.7587
	step [81/191], loss=150.0660
	step [82/191], loss=160.6197
	step [83/191], loss=175.6714
	step [84/191], loss=148.6995
	step [85/191], loss=152.6518
	step [86/191], loss=170.6114
	step [87/191], loss=163.6406
	step [88/191], loss=150.4657
	step [89/191], loss=152.8428
	step [90/191], loss=151.0675
	step [91/191], loss=146.3985
	step [92/191], loss=143.0312
	step [93/191], loss=139.5000
	step [94/191], loss=161.2082
	step [95/191], loss=146.2303
	step [96/191], loss=132.3837
	step [97/191], loss=145.9571
	step [98/191], loss=150.6034
	step [99/191], loss=149.6685
	step [100/191], loss=142.8726
	step [101/191], loss=146.3340
	step [102/191], loss=126.7621
	step [103/191], loss=154.2742
	step [104/191], loss=137.6698
	step [105/191], loss=159.6192
	step [106/191], loss=138.4145
	step [107/191], loss=143.5195
	step [108/191], loss=153.8410
	step [109/191], loss=159.9411
	step [110/191], loss=129.6756
	step [111/191], loss=152.4879
	step [112/191], loss=138.8493
	step [113/191], loss=135.9677
	step [114/191], loss=146.8573
	step [115/191], loss=137.8318
	step [116/191], loss=155.0225
	step [117/191], loss=147.1357
	step [118/191], loss=137.1168
	step [119/191], loss=133.2461
	step [120/191], loss=154.5549
	step [121/191], loss=134.0635
	step [122/191], loss=150.5031
	step [123/191], loss=130.8743
	step [124/191], loss=154.6553
	step [125/191], loss=130.4517
	step [126/191], loss=146.9028
	step [127/191], loss=154.2719
	step [128/191], loss=146.3131
	step [129/191], loss=128.0038
	step [130/191], loss=167.0951
	step [131/191], loss=138.4787
	step [132/191], loss=142.6179
	step [133/191], loss=150.8617
	step [134/191], loss=131.5300
	step [135/191], loss=132.9838
	step [136/191], loss=163.1519
	step [137/191], loss=124.4038
	step [138/191], loss=129.0218
	step [139/191], loss=123.8722
	step [140/191], loss=158.4548
	step [141/191], loss=131.8938
	step [142/191], loss=153.0974
	step [143/191], loss=135.7833
	step [144/191], loss=137.1277
	step [145/191], loss=129.5659
	step [146/191], loss=143.0272
	step [147/191], loss=139.0417
	step [148/191], loss=132.8624
	step [149/191], loss=130.4463
	step [150/191], loss=123.7409
	step [151/191], loss=140.7195
	step [152/191], loss=120.0656
	step [153/191], loss=141.9547
	step [154/191], loss=138.2941
	step [155/191], loss=149.1493
	step [156/191], loss=152.4446
	step [157/191], loss=155.5350
	step [158/191], loss=150.9148
	step [159/191], loss=140.6676
	step [160/191], loss=139.2478
	step [161/191], loss=147.8871
	step [162/191], loss=138.4866
	step [163/191], loss=137.7426
	step [164/191], loss=152.9411
	step [165/191], loss=140.6626
	step [166/191], loss=153.8990
	step [167/191], loss=128.7407
	step [168/191], loss=137.8423
	step [169/191], loss=148.1645
	step [170/191], loss=144.5036
	step [171/191], loss=149.6583
	step [172/191], loss=133.0274
	step [173/191], loss=127.9991
	step [174/191], loss=129.3288
	step [175/191], loss=134.5708
	step [176/191], loss=161.2439
	step [177/191], loss=140.3731
	step [178/191], loss=133.7747
	step [179/191], loss=137.5626
	step [180/191], loss=137.6839
	step [181/191], loss=150.0741
	step [182/191], loss=134.5396
	step [183/191], loss=124.2913
	step [184/191], loss=125.3011
	step [185/191], loss=145.7827
	step [186/191], loss=134.8036
	step [187/191], loss=130.9351
	step [188/191], loss=131.5361
	step [189/191], loss=136.3534
	step [190/191], loss=130.4666
	step [191/191], loss=56.2718
	Evaluating
	loss=0.3196, precision=0.2875, recall=0.8995, f1=0.4357
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/191], loss=129.6352
	step [2/191], loss=116.7624
	step [3/191], loss=144.2078
	step [4/191], loss=131.0165
	step [5/191], loss=133.3327
	step [6/191], loss=141.2636
	step [7/191], loss=145.7475
	step [8/191], loss=125.5181
	step [9/191], loss=132.7176
	step [10/191], loss=150.2529
	step [11/191], loss=129.4677
	step [12/191], loss=140.5753
	step [13/191], loss=120.3443
	step [14/191], loss=130.6796
	step [15/191], loss=134.7626
	step [16/191], loss=132.8624
	step [17/191], loss=122.1351
	step [18/191], loss=144.6939
	step [19/191], loss=113.9983
	step [20/191], loss=130.1030
	step [21/191], loss=121.8723
	step [22/191], loss=131.9198
	step [23/191], loss=121.3103
	step [24/191], loss=132.0421
	step [25/191], loss=145.6108
	step [26/191], loss=120.7176
	step [27/191], loss=138.6590
	step [28/191], loss=121.5203
	step [29/191], loss=140.5472
	step [30/191], loss=131.5270
	step [31/191], loss=124.0855
	step [32/191], loss=124.6577
	step [33/191], loss=132.3782
	step [34/191], loss=139.3621
	step [35/191], loss=144.9362
	step [36/191], loss=159.1616
	step [37/191], loss=127.5683
	step [38/191], loss=144.0814
	step [39/191], loss=127.6125
	step [40/191], loss=135.9939
	step [41/191], loss=129.7486
	step [42/191], loss=121.4156
	step [43/191], loss=130.7222
	step [44/191], loss=133.9142
	step [45/191], loss=122.8709
	step [46/191], loss=119.4019
	step [47/191], loss=120.6950
	step [48/191], loss=112.3551
	step [49/191], loss=132.8458
	step [50/191], loss=152.9818
	step [51/191], loss=137.1735
	step [52/191], loss=115.8721
	step [53/191], loss=131.9332
	step [54/191], loss=135.3421
	step [55/191], loss=137.7942
	step [56/191], loss=121.0245
	step [57/191], loss=142.8733
	step [58/191], loss=131.5624
	step [59/191], loss=127.8889
	step [60/191], loss=140.5755
	step [61/191], loss=121.4647
	step [62/191], loss=139.5999
	step [63/191], loss=121.0134
	step [64/191], loss=120.0797
	step [65/191], loss=126.0714
	step [66/191], loss=141.9112
	step [67/191], loss=126.3102
	step [68/191], loss=119.1642
	step [69/191], loss=132.5600
	step [70/191], loss=123.2904
	step [71/191], loss=130.4335
	step [72/191], loss=129.9149
	step [73/191], loss=126.7908
	step [74/191], loss=126.8949
	step [75/191], loss=136.9170
	step [76/191], loss=135.6499
	step [77/191], loss=123.8828
	step [78/191], loss=139.2613
	step [79/191], loss=125.9242
	step [80/191], loss=130.3280
	step [81/191], loss=117.0308
	step [82/191], loss=117.0312
	step [83/191], loss=111.5581
	step [84/191], loss=131.3109
	step [85/191], loss=110.4345
	step [86/191], loss=131.5993
	step [87/191], loss=128.0240
	step [88/191], loss=124.8694
	step [89/191], loss=137.4816
	step [90/191], loss=134.4874
	step [91/191], loss=130.0045
	step [92/191], loss=124.0669
	step [93/191], loss=123.9059
	step [94/191], loss=133.0490
	step [95/191], loss=122.9172
	step [96/191], loss=122.7543
	step [97/191], loss=133.1088
	step [98/191], loss=125.1086
	step [99/191], loss=134.5170
	step [100/191], loss=111.5966
	step [101/191], loss=125.0521
	step [102/191], loss=116.9827
	step [103/191], loss=114.7370
	step [104/191], loss=123.3615
	step [105/191], loss=120.1525
	step [106/191], loss=104.4170
	step [107/191], loss=126.6936
	step [108/191], loss=115.7056
	step [109/191], loss=117.2067
	step [110/191], loss=121.0221
	step [111/191], loss=118.8937
	step [112/191], loss=104.0459
	step [113/191], loss=133.0976
	step [114/191], loss=119.5649
	step [115/191], loss=109.0015
	step [116/191], loss=122.5070
	step [117/191], loss=120.3972
	step [118/191], loss=110.0544
	step [119/191], loss=114.2899
	step [120/191], loss=120.3504
	step [121/191], loss=132.1833
	step [122/191], loss=142.4044
	step [123/191], loss=128.3244
	step [124/191], loss=122.1951
	step [125/191], loss=117.2604
	step [126/191], loss=132.2383
	step [127/191], loss=125.2606
	step [128/191], loss=137.2840
	step [129/191], loss=125.7261
	step [130/191], loss=133.7513
	step [131/191], loss=109.3351
	step [132/191], loss=114.9943
	step [133/191], loss=110.9145
	step [134/191], loss=120.2509
	step [135/191], loss=100.9555
	step [136/191], loss=114.0449
	step [137/191], loss=113.5685
	step [138/191], loss=124.4006
	step [139/191], loss=115.2951
	step [140/191], loss=120.9000
	step [141/191], loss=124.2259
	step [142/191], loss=121.8760
	step [143/191], loss=128.8473
	step [144/191], loss=109.9808
	step [145/191], loss=126.1175
	step [146/191], loss=125.6142
	step [147/191], loss=133.9212
	step [148/191], loss=118.5237
	step [149/191], loss=111.8367
	step [150/191], loss=117.7795
	step [151/191], loss=125.4639
	step [152/191], loss=103.9526
	step [153/191], loss=106.4464
	step [154/191], loss=112.8718
	step [155/191], loss=111.0698
	step [156/191], loss=129.1898
	step [157/191], loss=118.0150
	step [158/191], loss=131.9244
	step [159/191], loss=116.0813
	step [160/191], loss=103.3485
	step [161/191], loss=118.2097
	step [162/191], loss=121.0597
	step [163/191], loss=100.9161
	step [164/191], loss=111.1204
	step [165/191], loss=126.0721
	step [166/191], loss=134.1216
	step [167/191], loss=115.8858
	step [168/191], loss=97.7987
	step [169/191], loss=104.6409
	step [170/191], loss=124.2717
	step [171/191], loss=99.8834
	step [172/191], loss=102.7736
	step [173/191], loss=108.5110
	step [174/191], loss=99.5090
	step [175/191], loss=99.7001
	step [176/191], loss=126.1926
	step [177/191], loss=115.6303
	step [178/191], loss=123.8491
	step [179/191], loss=104.6897
	step [180/191], loss=126.5565
	step [181/191], loss=122.7332
	step [182/191], loss=122.1957
	step [183/191], loss=107.6301
	step [184/191], loss=103.6966
	step [185/191], loss=135.9028
	step [186/191], loss=118.5622
	step [187/191], loss=110.2175
	step [188/191], loss=118.9131
	step [189/191], loss=122.0471
	step [190/191], loss=104.0879
	step [191/191], loss=45.4762
	Evaluating
	loss=0.2243, precision=0.4636, recall=0.9071, f1=0.6136
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/191], loss=114.8741
	step [2/191], loss=131.2759
	step [3/191], loss=106.0750
	step [4/191], loss=114.3916
	step [5/191], loss=106.5024
	step [6/191], loss=120.5809
	step [7/191], loss=105.0637
	step [8/191], loss=138.5499
	step [9/191], loss=132.9960
	step [10/191], loss=100.7891
	step [11/191], loss=125.1588
	step [12/191], loss=114.3854
	step [13/191], loss=112.0017
	step [14/191], loss=118.9920
	step [15/191], loss=131.1457
	step [16/191], loss=134.4320
	step [17/191], loss=110.5518
	step [18/191], loss=117.3932
	step [19/191], loss=111.0673
	step [20/191], loss=117.7311
	step [21/191], loss=110.5164
	step [22/191], loss=101.6334
	step [23/191], loss=106.7026
	step [24/191], loss=107.6149
	step [25/191], loss=102.0010
	step [26/191], loss=119.9159
	step [27/191], loss=106.2853
	step [28/191], loss=106.6470
	step [29/191], loss=113.6813
	step [30/191], loss=127.7121
	step [31/191], loss=110.8212
	step [32/191], loss=141.7853
	step [33/191], loss=103.4083
	step [34/191], loss=122.7176
	step [35/191], loss=113.3505
	step [36/191], loss=127.2765
	step [37/191], loss=104.7175
	step [38/191], loss=108.9289
	step [39/191], loss=122.8800
	step [40/191], loss=103.2017
	step [41/191], loss=120.1404
	step [42/191], loss=106.9247
	step [43/191], loss=111.1313
	step [44/191], loss=100.0546
	step [45/191], loss=118.7560
	step [46/191], loss=121.6596
	step [47/191], loss=107.9309
	step [48/191], loss=101.1181
	step [49/191], loss=121.8520
	step [50/191], loss=135.8611
	step [51/191], loss=123.5559
	step [52/191], loss=116.3745
	step [53/191], loss=109.2910
	step [54/191], loss=121.5423
	step [55/191], loss=105.6014
	step [56/191], loss=106.2619
	step [57/191], loss=105.8381
	step [58/191], loss=118.1549
	step [59/191], loss=124.9146
	step [60/191], loss=103.9230
	step [61/191], loss=112.7960
	step [62/191], loss=126.6052
	step [63/191], loss=98.9786
	step [64/191], loss=99.4192
	step [65/191], loss=117.7607
	step [66/191], loss=126.1474
	step [67/191], loss=96.1730
	step [68/191], loss=118.6547
	step [69/191], loss=112.2207
	step [70/191], loss=118.6931
	step [71/191], loss=117.7487
	step [72/191], loss=100.5629
	step [73/191], loss=92.3515
	step [74/191], loss=112.8710
	step [75/191], loss=107.0584
	step [76/191], loss=94.7166
	step [77/191], loss=114.1841
	step [78/191], loss=85.7137
	step [79/191], loss=105.0790
	step [80/191], loss=120.9689
	step [81/191], loss=119.4141
	step [82/191], loss=119.3407
	step [83/191], loss=102.2832
	step [84/191], loss=123.5905
	step [85/191], loss=105.0999
	step [86/191], loss=104.7301
	step [87/191], loss=94.9185
	step [88/191], loss=106.5994
	step [89/191], loss=116.3739
	step [90/191], loss=105.2452
	step [91/191], loss=119.5834
	step [92/191], loss=112.0825
	step [93/191], loss=115.2958
	step [94/191], loss=106.5070
	step [95/191], loss=105.2019
	step [96/191], loss=98.2785
	step [97/191], loss=121.4148
	step [98/191], loss=111.1591
	step [99/191], loss=99.9849
	step [100/191], loss=123.4586
	step [101/191], loss=119.1204
	step [102/191], loss=111.4623
	step [103/191], loss=122.5178
	step [104/191], loss=110.5211
	step [105/191], loss=117.7439
	step [106/191], loss=104.6151
	step [107/191], loss=99.7522
	step [108/191], loss=108.5712
	step [109/191], loss=108.5468
	step [110/191], loss=113.4527
	step [111/191], loss=125.4274
	step [112/191], loss=118.5976
	step [113/191], loss=127.6278
	step [114/191], loss=107.0288
	step [115/191], loss=130.7264
	step [116/191], loss=105.0098
	step [117/191], loss=111.1660
	step [118/191], loss=92.7185
	step [119/191], loss=118.8606
	step [120/191], loss=106.1307
	step [121/191], loss=122.9304
	step [122/191], loss=96.0188
	step [123/191], loss=105.4305
	step [124/191], loss=101.3806
	step [125/191], loss=106.7676
	step [126/191], loss=97.1399
	step [127/191], loss=103.0651
	step [128/191], loss=105.0885
	step [129/191], loss=88.1564
	step [130/191], loss=92.4797
	step [131/191], loss=109.0758
	step [132/191], loss=91.0120
	step [133/191], loss=97.7839
	step [134/191], loss=103.7068
	step [135/191], loss=122.6403
	step [136/191], loss=101.7612
	step [137/191], loss=99.9371
	step [138/191], loss=113.7551
	step [139/191], loss=124.7510
	step [140/191], loss=107.0825
	step [141/191], loss=102.4287
	step [142/191], loss=104.8453
	step [143/191], loss=113.3236
	step [144/191], loss=100.4122
	step [145/191], loss=106.5262
	step [146/191], loss=103.5465
	step [147/191], loss=118.6986
	step [148/191], loss=115.3312
	step [149/191], loss=117.8680
	step [150/191], loss=99.1603
	step [151/191], loss=98.9378
	step [152/191], loss=111.6885
	step [153/191], loss=107.1582
	step [154/191], loss=106.8972
	step [155/191], loss=112.1577
	step [156/191], loss=125.9148
	step [157/191], loss=97.7114
	step [158/191], loss=97.0171
	step [159/191], loss=106.6523
	step [160/191], loss=110.1720
	step [161/191], loss=110.9658
	step [162/191], loss=110.3612
	step [163/191], loss=106.1069
	step [164/191], loss=102.2488
	step [165/191], loss=99.9064
	step [166/191], loss=112.7154
	step [167/191], loss=105.7381
	step [168/191], loss=98.3036
	step [169/191], loss=98.3982
	step [170/191], loss=111.9149
	step [171/191], loss=119.3742
	step [172/191], loss=107.6130
	step [173/191], loss=104.3207
	step [174/191], loss=111.6767
	step [175/191], loss=117.1558
	step [176/191], loss=103.2077
	step [177/191], loss=115.8865
	step [178/191], loss=92.0792
	step [179/191], loss=117.3519
	step [180/191], loss=119.7571
	step [181/191], loss=105.4327
	step [182/191], loss=130.1941
	step [183/191], loss=89.8648
	step [184/191], loss=102.4134
	step [185/191], loss=97.1210
	step [186/191], loss=103.4911
	step [187/191], loss=82.9202
	step [188/191], loss=87.9089
	step [189/191], loss=102.8017
	step [190/191], loss=112.3529
	step [191/191], loss=43.9638
	Evaluating
	loss=0.1613, precision=0.4058, recall=0.9256, f1=0.5642
Training epoch 4
	step [1/191], loss=98.7921
	step [2/191], loss=101.8920
	step [3/191], loss=97.2198
	step [4/191], loss=94.6108
	step [5/191], loss=110.2440
	step [6/191], loss=97.3965
	step [7/191], loss=117.1061
	step [8/191], loss=121.2729
	step [9/191], loss=119.2532
	step [10/191], loss=110.4649
	step [11/191], loss=97.5946
	step [12/191], loss=103.2876
	step [13/191], loss=101.2066
	step [14/191], loss=88.2738
	step [15/191], loss=104.0667
	step [16/191], loss=125.0697
	step [17/191], loss=102.3844
	step [18/191], loss=103.3800
	step [19/191], loss=116.6707
	step [20/191], loss=110.0091
	step [21/191], loss=110.9286
	step [22/191], loss=93.9301
	step [23/191], loss=124.6538
	step [24/191], loss=103.1791
	step [25/191], loss=95.5255
	step [26/191], loss=91.7444
	step [27/191], loss=97.9150
	step [28/191], loss=98.2218
	step [29/191], loss=108.9341
	step [30/191], loss=98.4340
	step [31/191], loss=89.3112
	step [32/191], loss=92.8523
	step [33/191], loss=113.9529
	step [34/191], loss=114.2955
	step [35/191], loss=99.6691
	step [36/191], loss=95.2755
	step [37/191], loss=93.6951
	step [38/191], loss=100.0762
	step [39/191], loss=112.9758
	step [40/191], loss=113.5038
	step [41/191], loss=108.3717
	step [42/191], loss=116.1153
	step [43/191], loss=104.8129
	step [44/191], loss=101.1185
	step [45/191], loss=106.3370
	step [46/191], loss=99.5242
	step [47/191], loss=87.9099
	step [48/191], loss=112.5014
	step [49/191], loss=117.3185
	step [50/191], loss=96.3022
	step [51/191], loss=99.5728
	step [52/191], loss=104.0332
	step [53/191], loss=92.8168
	step [54/191], loss=107.2892
	step [55/191], loss=89.6440
	step [56/191], loss=99.6084
	step [57/191], loss=114.1258
	step [58/191], loss=111.9772
	step [59/191], loss=93.6662
	step [60/191], loss=95.2175
	step [61/191], loss=106.9748
	step [62/191], loss=94.3467
	step [63/191], loss=96.9138
	step [64/191], loss=106.2316
	step [65/191], loss=82.4697
	step [66/191], loss=99.5923
	step [67/191], loss=93.5918
	step [68/191], loss=99.2047
	step [69/191], loss=112.1779
	step [70/191], loss=102.3726
	step [71/191], loss=103.0368
	step [72/191], loss=112.7880
	step [73/191], loss=100.8681
	step [74/191], loss=87.6155
	step [75/191], loss=106.1996
	step [76/191], loss=102.3913
	step [77/191], loss=109.4588
	step [78/191], loss=108.1094
	step [79/191], loss=84.8145
	step [80/191], loss=103.0326
	step [81/191], loss=88.5897
	step [82/191], loss=89.1336
	step [83/191], loss=100.9641
	step [84/191], loss=95.6612
	step [85/191], loss=86.6215
	step [86/191], loss=108.1694
	step [87/191], loss=100.3265
	step [88/191], loss=97.9178
	step [89/191], loss=114.8257
	step [90/191], loss=90.1930
	step [91/191], loss=121.5321
	step [92/191], loss=90.8667
	step [93/191], loss=110.5462
	step [94/191], loss=98.3860
	step [95/191], loss=107.6944
	step [96/191], loss=81.2370
	step [97/191], loss=85.0476
	step [98/191], loss=105.9153
	step [99/191], loss=102.9620
	step [100/191], loss=96.2987
	step [101/191], loss=87.7164
	step [102/191], loss=106.2739
	step [103/191], loss=103.6302
	step [104/191], loss=107.1742
	step [105/191], loss=91.9576
	step [106/191], loss=99.4053
	step [107/191], loss=105.1823
	step [108/191], loss=105.9393
	step [109/191], loss=117.8981
	step [110/191], loss=84.4381
	step [111/191], loss=102.7989
	step [112/191], loss=78.2357
	step [113/191], loss=94.2319
	step [114/191], loss=114.1450
	step [115/191], loss=101.4307
	step [116/191], loss=99.5128
	step [117/191], loss=99.3846
	step [118/191], loss=87.0336
	step [119/191], loss=97.7658
	step [120/191], loss=118.1311
	step [121/191], loss=111.1811
	step [122/191], loss=102.5091
	step [123/191], loss=97.5056
	step [124/191], loss=110.2804
	step [125/191], loss=109.3405
	step [126/191], loss=99.6169
	step [127/191], loss=133.8589
	step [128/191], loss=111.5361
	step [129/191], loss=101.5212
	step [130/191], loss=97.8439
	step [131/191], loss=90.6542
	step [132/191], loss=104.4908
	step [133/191], loss=106.5376
	step [134/191], loss=93.1030
	step [135/191], loss=104.6527
	step [136/191], loss=114.1241
	step [137/191], loss=101.9894
	step [138/191], loss=131.2526
	step [139/191], loss=92.9237
	step [140/191], loss=93.5855
	step [141/191], loss=101.1608
	step [142/191], loss=87.0474
	step [143/191], loss=125.1436
	step [144/191], loss=95.6806
	step [145/191], loss=114.1322
	step [146/191], loss=112.4112
	step [147/191], loss=104.5545
	step [148/191], loss=106.5384
	step [149/191], loss=108.7259
	step [150/191], loss=77.5743
	step [151/191], loss=95.8220
	step [152/191], loss=99.9085
	step [153/191], loss=104.8187
	step [154/191], loss=88.5457
	step [155/191], loss=98.3289
	step [156/191], loss=93.8702
	step [157/191], loss=88.5701
	step [158/191], loss=93.3259
	step [159/191], loss=92.8665
	step [160/191], loss=83.5839
	step [161/191], loss=104.3986
	step [162/191], loss=103.3948
	step [163/191], loss=100.9927
	step [164/191], loss=119.0459
	step [165/191], loss=112.3642
	step [166/191], loss=113.5764
	step [167/191], loss=106.2430
	step [168/191], loss=100.8679
	step [169/191], loss=96.1746
	step [170/191], loss=95.5554
	step [171/191], loss=79.5385
	step [172/191], loss=110.4539
	step [173/191], loss=74.2593
	step [174/191], loss=101.6706
	step [175/191], loss=105.1024
	step [176/191], loss=117.5720
	step [177/191], loss=84.0798
	step [178/191], loss=119.7785
	step [179/191], loss=86.8749
	step [180/191], loss=103.0007
	step [181/191], loss=87.7862
	step [182/191], loss=104.3248
	step [183/191], loss=96.5910
	step [184/191], loss=104.9224
	step [185/191], loss=96.5589
	step [186/191], loss=98.2990
	step [187/191], loss=99.7197
	step [188/191], loss=85.0283
	step [189/191], loss=89.6127
	step [190/191], loss=93.4024
	step [191/191], loss=39.5599
	Evaluating
	loss=0.1190, precision=0.4633, recall=0.9107, f1=0.6142
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/191], loss=108.1428
	step [2/191], loss=92.3090
	step [3/191], loss=91.9050
	step [4/191], loss=92.1861
	step [5/191], loss=95.5012
	step [6/191], loss=103.2387
	step [7/191], loss=119.5404
	step [8/191], loss=77.1187
	step [9/191], loss=99.7006
	step [10/191], loss=104.0842
	step [11/191], loss=87.7956
	step [12/191], loss=82.7397
	step [13/191], loss=106.1954
	step [14/191], loss=94.2704
	step [15/191], loss=97.5663
	step [16/191], loss=120.5865
	step [17/191], loss=113.3411
	step [18/191], loss=99.3565
	step [19/191], loss=100.1504
	step [20/191], loss=86.4160
	step [21/191], loss=102.9995
	step [22/191], loss=104.9102
	step [23/191], loss=87.5882
	step [24/191], loss=83.3831
	step [25/191], loss=78.1808
	step [26/191], loss=93.6019
	step [27/191], loss=91.6602
	step [28/191], loss=84.6940
	step [29/191], loss=96.7610
	step [30/191], loss=98.2751
	step [31/191], loss=87.2965
	step [32/191], loss=87.3662
	step [33/191], loss=93.7508
	step [34/191], loss=85.3104
	step [35/191], loss=95.8632
	step [36/191], loss=105.7974
	step [37/191], loss=89.6753
	step [38/191], loss=102.8119
	step [39/191], loss=114.2534
	step [40/191], loss=116.2262
	step [41/191], loss=89.1964
	step [42/191], loss=114.1000
	step [43/191], loss=85.5583
	step [44/191], loss=102.6697
	step [45/191], loss=95.4843
	step [46/191], loss=89.9611
	step [47/191], loss=95.1018
	step [48/191], loss=103.3781
	step [49/191], loss=96.0666
	step [50/191], loss=90.1236
	step [51/191], loss=102.1364
	step [52/191], loss=95.0310
	step [53/191], loss=84.9648
	step [54/191], loss=95.1387
	step [55/191], loss=107.0950
	step [56/191], loss=104.0460
	step [57/191], loss=94.1721
	step [58/191], loss=90.7056
	step [59/191], loss=103.6326
	step [60/191], loss=83.5148
	step [61/191], loss=103.3463
	step [62/191], loss=92.8758
	step [63/191], loss=98.8297
	step [64/191], loss=80.1463
	step [65/191], loss=94.6500
	step [66/191], loss=88.3510
	step [67/191], loss=95.4062
	step [68/191], loss=100.5737
	step [69/191], loss=84.3672
	step [70/191], loss=87.5404
	step [71/191], loss=105.9227
	step [72/191], loss=102.0385
	step [73/191], loss=92.8357
	step [74/191], loss=95.8200
	step [75/191], loss=90.1143
	step [76/191], loss=100.6277
	step [77/191], loss=89.2472
	step [78/191], loss=90.6240
	step [79/191], loss=112.9451
	step [80/191], loss=107.7573
	step [81/191], loss=123.0340
	step [82/191], loss=97.8693
	step [83/191], loss=98.6945
	step [84/191], loss=112.3064
	step [85/191], loss=100.9953
	step [86/191], loss=95.9658
	step [87/191], loss=93.1788
	step [88/191], loss=101.5713
	step [89/191], loss=101.8460
	step [90/191], loss=90.2178
	step [91/191], loss=107.1700
	step [92/191], loss=113.2198
	step [93/191], loss=98.1140
	step [94/191], loss=89.9862
	step [95/191], loss=92.3429
	step [96/191], loss=88.8949
	step [97/191], loss=111.6984
	step [98/191], loss=98.5981
	step [99/191], loss=80.3644
	step [100/191], loss=87.2474
	step [101/191], loss=98.4839
	step [102/191], loss=103.1469
	step [103/191], loss=96.6685
	step [104/191], loss=84.2929
	step [105/191], loss=87.5236
	step [106/191], loss=109.5988
	step [107/191], loss=96.0211
	step [108/191], loss=99.2588
	step [109/191], loss=109.4374
	step [110/191], loss=99.6738
	step [111/191], loss=85.6715
	step [112/191], loss=105.8154
	step [113/191], loss=94.2325
	step [114/191], loss=103.9416
	step [115/191], loss=109.1062
	step [116/191], loss=97.2544
	step [117/191], loss=86.9819
	step [118/191], loss=97.2156
	step [119/191], loss=97.8064
	step [120/191], loss=96.3786
	step [121/191], loss=88.1660
	step [122/191], loss=94.6010
	step [123/191], loss=79.8566
	step [124/191], loss=85.6852
	step [125/191], loss=93.0647
	step [126/191], loss=81.0605
	step [127/191], loss=76.6557
	step [128/191], loss=103.6008
	step [129/191], loss=115.0570
	step [130/191], loss=92.7473
	step [131/191], loss=80.2551
	step [132/191], loss=90.2784
	step [133/191], loss=106.3988
	step [134/191], loss=113.8119
	step [135/191], loss=90.4997
	step [136/191], loss=104.3853
	step [137/191], loss=82.7493
	step [138/191], loss=79.7323
	step [139/191], loss=86.4719
	step [140/191], loss=101.6331
	step [141/191], loss=90.9414
	step [142/191], loss=107.5103
	step [143/191], loss=98.7995
	step [144/191], loss=102.5528
	step [145/191], loss=84.0737
	step [146/191], loss=98.2480
	step [147/191], loss=78.7778
	step [148/191], loss=104.1650
	step [149/191], loss=103.7938
	step [150/191], loss=77.2909
	step [151/191], loss=94.7046
	step [152/191], loss=96.0472
	step [153/191], loss=87.0569
	step [154/191], loss=84.6612
	step [155/191], loss=85.5462
	step [156/191], loss=101.6870
	step [157/191], loss=89.0843
	step [158/191], loss=88.4911
	step [159/191], loss=81.8551
	step [160/191], loss=96.5700
	step [161/191], loss=94.7923
	step [162/191], loss=94.9382
	step [163/191], loss=93.2230
	step [164/191], loss=92.9198
	step [165/191], loss=107.1362
	step [166/191], loss=83.6577
	step [167/191], loss=112.6564
	step [168/191], loss=92.5272
	step [169/191], loss=104.0168
	step [170/191], loss=86.5923
	step [171/191], loss=97.4751
	step [172/191], loss=116.6985
	step [173/191], loss=90.0169
	step [174/191], loss=126.3332
	step [175/191], loss=77.1458
	step [176/191], loss=92.7410
	step [177/191], loss=103.6270
	step [178/191], loss=89.9218
	step [179/191], loss=98.7435
	step [180/191], loss=114.5814
	step [181/191], loss=112.7008
	step [182/191], loss=109.2639
	step [183/191], loss=110.2478
	step [184/191], loss=95.3027
	step [185/191], loss=96.6685
	step [186/191], loss=99.2545
	step [187/191], loss=80.6608
	step [188/191], loss=90.8210
	step [189/191], loss=96.0472
	step [190/191], loss=96.3290
	step [191/191], loss=61.2555
	Evaluating
	loss=0.0955, precision=0.4700, recall=0.8972, f1=0.6168
saving model as: 1_saved_model.pth
Training epoch 6
	step [1/191], loss=105.6496
	step [2/191], loss=95.1082
	step [3/191], loss=88.5329
	step [4/191], loss=74.7651
	step [5/191], loss=81.1853
	step [6/191], loss=87.2301
	step [7/191], loss=104.2263
	step [8/191], loss=105.7379
	step [9/191], loss=103.5730
	step [10/191], loss=86.9831
	step [11/191], loss=98.7434
	step [12/191], loss=87.3601
	step [13/191], loss=95.2646
	step [14/191], loss=78.2991
	step [15/191], loss=84.2555
	step [16/191], loss=94.8161
	step [17/191], loss=100.0933
	step [18/191], loss=97.1561
	step [19/191], loss=92.9564
	step [20/191], loss=80.0115
	step [21/191], loss=78.6023
	step [22/191], loss=102.3413
	step [23/191], loss=80.0016
	step [24/191], loss=83.8985
	step [25/191], loss=85.0503
	step [26/191], loss=96.7887
	step [27/191], loss=82.7036
	step [28/191], loss=110.5569
	step [29/191], loss=87.4598
	step [30/191], loss=91.9146
	step [31/191], loss=112.2717
	step [32/191], loss=105.2725
	step [33/191], loss=96.1583
	step [34/191], loss=82.1094
	step [35/191], loss=106.3125
	step [36/191], loss=80.7515
	step [37/191], loss=91.6717
	step [38/191], loss=83.0233
	step [39/191], loss=94.8022
	step [40/191], loss=90.4820
	step [41/191], loss=88.8697
	step [42/191], loss=92.5052
	step [43/191], loss=97.3170
	step [44/191], loss=115.6779
	step [45/191], loss=88.9599
	step [46/191], loss=84.4233
	step [47/191], loss=88.5455
	step [48/191], loss=112.6008
	step [49/191], loss=83.5355
	step [50/191], loss=101.3975
	step [51/191], loss=110.0932
	step [52/191], loss=86.4629
	step [53/191], loss=102.7805
	step [54/191], loss=97.0784
	step [55/191], loss=85.5263
	step [56/191], loss=84.6749
	step [57/191], loss=109.1519
	step [58/191], loss=112.2555
	step [59/191], loss=85.4931
	step [60/191], loss=79.7705
	step [61/191], loss=94.3596
	step [62/191], loss=103.2937
	step [63/191], loss=78.1714
	step [64/191], loss=94.1258
	step [65/191], loss=90.3919
	step [66/191], loss=86.8569
	step [67/191], loss=92.9471
	step [68/191], loss=95.2431
	step [69/191], loss=107.4522
	step [70/191], loss=88.3269
	step [71/191], loss=91.9929
	step [72/191], loss=99.9217
	step [73/191], loss=97.2436
	step [74/191], loss=70.4881
	step [75/191], loss=84.0499
	step [76/191], loss=93.7227
	step [77/191], loss=76.7149
	step [78/191], loss=88.6910
	step [79/191], loss=90.9617
	step [80/191], loss=83.1551
	step [81/191], loss=90.9516
	step [82/191], loss=85.0908
	step [83/191], loss=95.1726
	step [84/191], loss=92.4728
	step [85/191], loss=87.3662
	step [86/191], loss=88.2376
	step [87/191], loss=88.6122
	step [88/191], loss=101.2946
	step [89/191], loss=109.2030
	step [90/191], loss=98.6430
	step [91/191], loss=88.0388
	step [92/191], loss=82.2890
	step [93/191], loss=81.5446
	step [94/191], loss=80.8311
	step [95/191], loss=94.3609
	step [96/191], loss=88.3728
	step [97/191], loss=102.3031
	step [98/191], loss=87.8247
	step [99/191], loss=80.6811
	step [100/191], loss=91.7278
	step [101/191], loss=93.5703
	step [102/191], loss=97.6308
	step [103/191], loss=105.8769
	step [104/191], loss=83.8979
	step [105/191], loss=72.5030
	step [106/191], loss=90.2126
	step [107/191], loss=90.7009
	step [108/191], loss=96.9957
	step [109/191], loss=70.4454
	step [110/191], loss=90.4439
	step [111/191], loss=95.9751
	step [112/191], loss=103.1661
	step [113/191], loss=89.6844
	step [114/191], loss=98.5880
	step [115/191], loss=104.8400
	step [116/191], loss=94.7210
	step [117/191], loss=89.5839
	step [118/191], loss=83.6582
	step [119/191], loss=86.3302
	step [120/191], loss=88.8279
	step [121/191], loss=111.3801
	step [122/191], loss=78.6231
	step [123/191], loss=112.2050
	step [124/191], loss=104.7691
	step [125/191], loss=114.7453
	step [126/191], loss=102.6524
	step [127/191], loss=94.3776
	step [128/191], loss=89.1369
	step [129/191], loss=90.2487
	step [130/191], loss=80.6735
	step [131/191], loss=94.7173
	step [132/191], loss=96.4759
	step [133/191], loss=73.9294
	step [134/191], loss=93.2706
	step [135/191], loss=86.2329
	step [136/191], loss=95.4920
	step [137/191], loss=85.4289
	step [138/191], loss=92.7933
	step [139/191], loss=88.0868
	step [140/191], loss=88.6425
	step [141/191], loss=89.1227
	step [142/191], loss=79.9735
	step [143/191], loss=97.7929
	step [144/191], loss=92.9909
	step [145/191], loss=78.2465
	step [146/191], loss=90.0242
	step [147/191], loss=97.6197
	step [148/191], loss=75.3558
	step [149/191], loss=88.2299
	step [150/191], loss=90.8138
	step [151/191], loss=96.6896
	step [152/191], loss=66.1623
	step [153/191], loss=94.9008
	step [154/191], loss=80.1103
	step [155/191], loss=110.8713
	step [156/191], loss=107.0661
	step [157/191], loss=75.8172
	step [158/191], loss=81.0480
	step [159/191], loss=93.5609
	step [160/191], loss=83.5214
	step [161/191], loss=87.0502
	step [162/191], loss=75.8904
	step [163/191], loss=116.4676
	step [164/191], loss=78.5527
	step [165/191], loss=90.9814
	step [166/191], loss=100.7688
	step [167/191], loss=79.4004
	step [168/191], loss=95.8214
	step [169/191], loss=91.6581
	step [170/191], loss=96.8560
	step [171/191], loss=115.1305
	step [172/191], loss=110.9472
	step [173/191], loss=95.6864
	step [174/191], loss=87.7202
	step [175/191], loss=80.6999
	step [176/191], loss=91.0468
	step [177/191], loss=85.8406
	step [178/191], loss=100.1621
	step [179/191], loss=88.2292
	step [180/191], loss=88.1146
	step [181/191], loss=96.6145
	step [182/191], loss=97.2437
	step [183/191], loss=88.4422
	step [184/191], loss=90.9146
	step [185/191], loss=102.3126
	step [186/191], loss=87.3453
	step [187/191], loss=91.8060
	step [188/191], loss=88.3324
	step [189/191], loss=95.9016
	step [190/191], loss=100.4552
	step [191/191], loss=40.1918
	Evaluating
	loss=0.0761, precision=0.4241, recall=0.9297, f1=0.5825
Training epoch 7
	step [1/191], loss=98.9754
	step [2/191], loss=83.5570
	step [3/191], loss=94.9912
	step [4/191], loss=99.1110
	step [5/191], loss=105.7468
	step [6/191], loss=92.7902
	step [7/191], loss=104.6641
	step [8/191], loss=82.8096
	step [9/191], loss=86.8985
	step [10/191], loss=95.9776
	step [11/191], loss=112.7569
	step [12/191], loss=84.4963
	step [13/191], loss=76.0085
	step [14/191], loss=75.7050
	step [15/191], loss=103.4527
	step [16/191], loss=87.2677
	step [17/191], loss=95.7150
	step [18/191], loss=88.7421
	step [19/191], loss=90.7930
	step [20/191], loss=64.9321
	step [21/191], loss=86.3941
	step [22/191], loss=91.0627
	step [23/191], loss=97.2387
	step [24/191], loss=91.5327
	step [25/191], loss=91.5182
	step [26/191], loss=75.8064
	step [27/191], loss=97.2870
	step [28/191], loss=78.2207
	step [29/191], loss=84.7214
	step [30/191], loss=99.5513
	step [31/191], loss=86.2400
	step [32/191], loss=91.2781
	step [33/191], loss=98.3206
	step [34/191], loss=72.5319
	step [35/191], loss=82.0789
	step [36/191], loss=83.2558
	step [37/191], loss=96.5114
	step [38/191], loss=99.3833
	step [39/191], loss=90.1831
	step [40/191], loss=101.8144
	step [41/191], loss=98.0570
	step [42/191], loss=93.7178
	step [43/191], loss=118.2377
	step [44/191], loss=87.7880
	step [45/191], loss=90.0927
	step [46/191], loss=102.2969
	step [47/191], loss=90.8167
	step [48/191], loss=82.7555
	step [49/191], loss=103.5972
	step [50/191], loss=82.4943
	step [51/191], loss=99.6034
	step [52/191], loss=103.3703
	step [53/191], loss=81.1824
	step [54/191], loss=103.3419
	step [55/191], loss=89.2950
	step [56/191], loss=79.8942
	step [57/191], loss=82.1026
	step [58/191], loss=75.5160
	step [59/191], loss=83.8246
	step [60/191], loss=103.8925
	step [61/191], loss=86.7135
	step [62/191], loss=73.8636
	step [63/191], loss=89.0204
	step [64/191], loss=82.3253
	step [65/191], loss=93.9923
	step [66/191], loss=93.4943
	step [67/191], loss=89.9401
	step [68/191], loss=86.2686
	step [69/191], loss=97.5316
	step [70/191], loss=98.1276
	step [71/191], loss=102.8034
	step [72/191], loss=83.0088
	step [73/191], loss=91.1255
	step [74/191], loss=84.6921
	step [75/191], loss=90.5551
	step [76/191], loss=84.2753
	step [77/191], loss=76.5000
	step [78/191], loss=93.9220
	step [79/191], loss=78.3253
	step [80/191], loss=90.3135
	step [81/191], loss=94.6704
	step [82/191], loss=87.1951
	step [83/191], loss=91.0484
	step [84/191], loss=84.5135
	step [85/191], loss=97.8932
	step [86/191], loss=81.3075
	step [87/191], loss=88.8028
	step [88/191], loss=91.4664
	step [89/191], loss=87.5569
	step [90/191], loss=83.2959
	step [91/191], loss=109.4810
	step [92/191], loss=89.6529
	step [93/191], loss=95.3633
	step [94/191], loss=91.9562
	step [95/191], loss=87.7922
	step [96/191], loss=86.2211
	step [97/191], loss=76.4668
	step [98/191], loss=90.7812
	step [99/191], loss=97.5030
	step [100/191], loss=107.4020
	step [101/191], loss=90.2351
	step [102/191], loss=84.7815
	step [103/191], loss=93.1592
	step [104/191], loss=85.8466
	step [105/191], loss=96.9687
	step [106/191], loss=80.7318
	step [107/191], loss=94.2467
	step [108/191], loss=89.7111
	step [109/191], loss=80.0632
	step [110/191], loss=96.2000
	step [111/191], loss=104.7632
	step [112/191], loss=82.4754
	step [113/191], loss=78.4821
	step [114/191], loss=75.1522
	step [115/191], loss=78.5449
	step [116/191], loss=83.0479
	step [117/191], loss=99.8300
	step [118/191], loss=95.9846
	step [119/191], loss=74.5014
	step [120/191], loss=84.9147
	step [121/191], loss=86.0859
	step [122/191], loss=79.9978
	step [123/191], loss=89.4031
	step [124/191], loss=88.2235
	step [125/191], loss=80.5275
	step [126/191], loss=93.8221
	step [127/191], loss=94.4411
	step [128/191], loss=108.8022
	step [129/191], loss=83.1612
	step [130/191], loss=81.5853
	step [131/191], loss=96.7143
	step [132/191], loss=100.6223
	step [133/191], loss=77.2879
	step [134/191], loss=81.1348
	step [135/191], loss=78.7820
	step [136/191], loss=83.4269
	step [137/191], loss=96.9547
	step [138/191], loss=99.0186
	step [139/191], loss=91.4366
	step [140/191], loss=105.0115
	step [141/191], loss=96.2935
	step [142/191], loss=83.8160
	step [143/191], loss=88.2173
	step [144/191], loss=88.8911
	step [145/191], loss=93.9182
	step [146/191], loss=80.0156
	step [147/191], loss=79.9020
	step [148/191], loss=89.2902
	step [149/191], loss=73.4914
	step [150/191], loss=94.6397
	step [151/191], loss=89.1268
	step [152/191], loss=76.6744
	step [153/191], loss=75.3786
	step [154/191], loss=85.0334
	step [155/191], loss=79.4810
	step [156/191], loss=88.8274
	step [157/191], loss=80.7335
	step [158/191], loss=80.7084
	step [159/191], loss=84.1646
	step [160/191], loss=96.0732
	step [161/191], loss=94.8820
	step [162/191], loss=97.0410
	step [163/191], loss=78.1998
	step [164/191], loss=104.3750
	step [165/191], loss=83.1361
	step [166/191], loss=86.8461
	step [167/191], loss=66.8331
	step [168/191], loss=94.4417
	step [169/191], loss=90.2892
	step [170/191], loss=93.2070
	step [171/191], loss=76.5329
	step [172/191], loss=79.1374
	step [173/191], loss=75.4409
	step [174/191], loss=99.5946
	step [175/191], loss=87.2426
	step [176/191], loss=89.1183
	step [177/191], loss=86.0756
	step [178/191], loss=87.8716
	step [179/191], loss=98.8255
	step [180/191], loss=97.2158
	step [181/191], loss=88.4234
	step [182/191], loss=81.3948
	step [183/191], loss=87.2900
	step [184/191], loss=76.7969
	step [185/191], loss=99.9050
	step [186/191], loss=86.4227
	step [187/191], loss=69.9642
	step [188/191], loss=77.4054
	step [189/191], loss=91.2748
	step [190/191], loss=78.7517
	step [191/191], loss=35.5606
	Evaluating
	loss=0.0606, precision=0.4930, recall=0.8888, f1=0.6342
saving model as: 1_saved_model.pth
Training epoch 8
	step [1/191], loss=83.4068
	step [2/191], loss=72.8339
	step [3/191], loss=77.8479
	step [4/191], loss=81.5507
	step [5/191], loss=89.4443
	step [6/191], loss=84.6572
	step [7/191], loss=73.0627
	step [8/191], loss=83.6720
	step [9/191], loss=81.6187
	step [10/191], loss=97.1263
	step [11/191], loss=78.9750
	step [12/191], loss=88.5858
	step [13/191], loss=63.4213
	step [14/191], loss=76.3827
	step [15/191], loss=90.2404
	step [16/191], loss=78.7819
	step [17/191], loss=89.2621
	step [18/191], loss=95.8624
	step [19/191], loss=97.0630
	step [20/191], loss=77.3216
	step [21/191], loss=88.9249
	step [22/191], loss=93.0843
	step [23/191], loss=77.4968
	step [24/191], loss=78.5602
	step [25/191], loss=90.1391
	step [26/191], loss=128.7859
	step [27/191], loss=103.9718
	step [28/191], loss=87.5032
	step [29/191], loss=86.2366
	step [30/191], loss=89.6535
	step [31/191], loss=89.4603
	step [32/191], loss=85.9663
	step [33/191], loss=98.1028
	step [34/191], loss=92.9779
	step [35/191], loss=83.6144
	step [36/191], loss=93.9261
	step [37/191], loss=91.0513
	step [38/191], loss=77.0700
	step [39/191], loss=84.6533
	step [40/191], loss=69.3680
	step [41/191], loss=86.4557
	step [42/191], loss=89.5328
	step [43/191], loss=84.2199
	step [44/191], loss=77.0247
	step [45/191], loss=88.2295
	step [46/191], loss=95.6703
	step [47/191], loss=86.2214
	step [48/191], loss=95.5366
	step [49/191], loss=95.3934
	step [50/191], loss=88.6486
	step [51/191], loss=84.1338
	step [52/191], loss=99.4892
	step [53/191], loss=84.1303
	step [54/191], loss=95.9633
	step [55/191], loss=85.6580
	step [56/191], loss=78.0218
	step [57/191], loss=86.7795
	step [58/191], loss=70.8662
	step [59/191], loss=88.5505
	step [60/191], loss=76.6106
	step [61/191], loss=79.2322
	step [62/191], loss=79.0213
	step [63/191], loss=94.4677
	step [64/191], loss=84.9350
	step [65/191], loss=106.4870
	step [66/191], loss=95.1264
	step [67/191], loss=74.7007
	step [68/191], loss=82.4644
	step [69/191], loss=92.1132
	step [70/191], loss=90.7565
	step [71/191], loss=97.9169
	step [72/191], loss=96.9267
	step [73/191], loss=77.4458
	step [74/191], loss=83.3840
	step [75/191], loss=77.9574
	step [76/191], loss=83.7122
	step [77/191], loss=73.6839
	step [78/191], loss=86.8737
	step [79/191], loss=86.4756
	step [80/191], loss=94.6193
	step [81/191], loss=79.2780
	step [82/191], loss=77.8898
	step [83/191], loss=84.0349
	step [84/191], loss=86.2027
	step [85/191], loss=64.3483
	step [86/191], loss=80.5248
	step [87/191], loss=86.2174
	step [88/191], loss=88.4027
	step [89/191], loss=74.3702
	step [90/191], loss=85.0790
	step [91/191], loss=84.5798
	step [92/191], loss=89.0862
	step [93/191], loss=79.4414
	step [94/191], loss=78.5088
	step [95/191], loss=83.3432
	step [96/191], loss=100.6613
	step [97/191], loss=76.9327
	step [98/191], loss=89.8826
	step [99/191], loss=87.5888
	step [100/191], loss=76.8447
	step [101/191], loss=87.9009
	step [102/191], loss=84.4358
	step [103/191], loss=77.5142
	step [104/191], loss=91.3316
	step [105/191], loss=64.9217
	step [106/191], loss=90.2069
	step [107/191], loss=96.1796
	step [108/191], loss=80.4546
	step [109/191], loss=91.0877
	step [110/191], loss=92.2710
	step [111/191], loss=78.5631
	step [112/191], loss=72.4783
	step [113/191], loss=83.5609
	step [114/191], loss=100.3091
	step [115/191], loss=85.6961
	step [116/191], loss=82.9025
	step [117/191], loss=82.4919
	step [118/191], loss=93.6186
	step [119/191], loss=93.2037
	step [120/191], loss=71.6336
	step [121/191], loss=89.8769
	step [122/191], loss=97.7245
	step [123/191], loss=106.5400
	step [124/191], loss=102.1889
	step [125/191], loss=100.8186
	step [126/191], loss=76.3476
	step [127/191], loss=85.1732
	step [128/191], loss=86.9269
	step [129/191], loss=81.1003
	step [130/191], loss=94.7041
	step [131/191], loss=85.3499
	step [132/191], loss=90.2115
	step [133/191], loss=78.5346
	step [134/191], loss=94.6612
	step [135/191], loss=103.0987
	step [136/191], loss=71.5564
	step [137/191], loss=107.5354
	step [138/191], loss=87.7377
	step [139/191], loss=111.2899
	step [140/191], loss=74.8202
	step [141/191], loss=82.5424
	step [142/191], loss=95.6660
	step [143/191], loss=99.6653
	step [144/191], loss=88.8956
	step [145/191], loss=84.9051
	step [146/191], loss=79.3384
	step [147/191], loss=85.5481
	step [148/191], loss=99.9781
	step [149/191], loss=82.5569
	step [150/191], loss=81.4817
	step [151/191], loss=88.0370
	step [152/191], loss=67.1608
	step [153/191], loss=91.0471
	step [154/191], loss=90.9303
	step [155/191], loss=95.3171
	step [156/191], loss=67.7094
	step [157/191], loss=73.8110
	step [158/191], loss=86.0566
	step [159/191], loss=96.0041
	step [160/191], loss=88.3825
	step [161/191], loss=80.1668
	step [162/191], loss=76.3158
	step [163/191], loss=93.3445
	step [164/191], loss=90.1933
	step [165/191], loss=95.9301
	step [166/191], loss=92.0638
	step [167/191], loss=92.7269
	step [168/191], loss=95.6116
	step [169/191], loss=82.7447
	step [170/191], loss=83.5457
	step [171/191], loss=83.7249
	step [172/191], loss=77.3909
	step [173/191], loss=85.1382
	step [174/191], loss=82.2176
	step [175/191], loss=78.4169
	step [176/191], loss=90.8090
	step [177/191], loss=90.0197
	step [178/191], loss=97.6636
	step [179/191], loss=88.1419
	step [180/191], loss=75.7383
	step [181/191], loss=85.2529
	step [182/191], loss=75.9124
	step [183/191], loss=98.9047
	step [184/191], loss=73.8960
	step [185/191], loss=79.9242
	step [186/191], loss=85.6082
	step [187/191], loss=84.0410
	step [188/191], loss=84.6297
	step [189/191], loss=104.4788
	step [190/191], loss=88.4695
	step [191/191], loss=40.8618
	Evaluating
	loss=0.0514, precision=0.4692, recall=0.9034, f1=0.6176
Training epoch 9
	step [1/191], loss=75.8388
	step [2/191], loss=81.7656
	step [3/191], loss=78.8352
	step [4/191], loss=62.0657
	step [5/191], loss=89.4078
	step [6/191], loss=78.4276
	step [7/191], loss=81.7420
	step [8/191], loss=69.1376
	step [9/191], loss=88.0387
	step [10/191], loss=92.6861
	step [11/191], loss=86.8241
	step [12/191], loss=96.1858
	step [13/191], loss=80.1643
	step [14/191], loss=97.2360
	step [15/191], loss=100.2954
	step [16/191], loss=92.3627
	step [17/191], loss=85.2211
	step [18/191], loss=85.6374
	step [19/191], loss=77.4003
	step [20/191], loss=79.7343
	step [21/191], loss=76.2943
	step [22/191], loss=82.1574
	step [23/191], loss=77.7756
	step [24/191], loss=88.7437
	step [25/191], loss=94.2828
	step [26/191], loss=81.3183
	step [27/191], loss=91.9006
	step [28/191], loss=82.6018
	step [29/191], loss=86.3655
	step [30/191], loss=87.8850
	step [31/191], loss=77.1191
	step [32/191], loss=90.1076
	step [33/191], loss=95.9975
	step [34/191], loss=66.0893
	step [35/191], loss=85.3015
	step [36/191], loss=82.2471
	step [37/191], loss=83.3485
	step [38/191], loss=83.1962
	step [39/191], loss=69.7012
	step [40/191], loss=81.4537
	step [41/191], loss=76.6328
	step [42/191], loss=97.7024
	step [43/191], loss=90.0197
	step [44/191], loss=86.2235
	step [45/191], loss=69.8131
	step [46/191], loss=86.4904
	step [47/191], loss=76.0777
	step [48/191], loss=88.1042
	step [49/191], loss=75.2763
	step [50/191], loss=73.7813
	step [51/191], loss=70.6890
	step [52/191], loss=78.5975
	step [53/191], loss=63.1892
	step [54/191], loss=94.2728
	step [55/191], loss=71.7983
	step [56/191], loss=82.8168
	step [57/191], loss=89.1744
	step [58/191], loss=76.9626
	step [59/191], loss=89.3679
	step [60/191], loss=79.6875
	step [61/191], loss=81.8138
	step [62/191], loss=69.6671
	step [63/191], loss=80.0104
	step [64/191], loss=79.3405
	step [65/191], loss=101.1252
	step [66/191], loss=100.9359
	step [67/191], loss=95.0918
	step [68/191], loss=75.2245
	step [69/191], loss=84.4227
	step [70/191], loss=77.1941
	step [71/191], loss=98.2490
	step [72/191], loss=76.4773
	step [73/191], loss=86.2135
	step [74/191], loss=62.8383
	step [75/191], loss=92.1004
	step [76/191], loss=95.7083
	step [77/191], loss=74.1817
	step [78/191], loss=90.6104
	step [79/191], loss=83.9788
	step [80/191], loss=88.7331
	step [81/191], loss=91.8745
	step [82/191], loss=81.0331
	step [83/191], loss=110.2801
	step [84/191], loss=105.2773
	step [85/191], loss=104.4001
	step [86/191], loss=85.2426
	step [87/191], loss=84.5505
	step [88/191], loss=91.2414
	step [89/191], loss=92.8467
	step [90/191], loss=91.8584
	step [91/191], loss=84.8701
	step [92/191], loss=75.9131
	step [93/191], loss=87.7941
	step [94/191], loss=84.0861
	step [95/191], loss=75.5905
	step [96/191], loss=89.0865
	step [97/191], loss=99.3302
	step [98/191], loss=85.0086
	step [99/191], loss=89.0514
	step [100/191], loss=94.0340
	step [101/191], loss=76.4619
	step [102/191], loss=73.5614
	step [103/191], loss=84.4582
	step [104/191], loss=92.7524
	step [105/191], loss=79.6815
	step [106/191], loss=89.6028
	step [107/191], loss=84.2179
	step [108/191], loss=104.0117
	step [109/191], loss=79.0917
	step [110/191], loss=62.4662
	step [111/191], loss=84.7449
	step [112/191], loss=63.5927
	step [113/191], loss=85.6497
	step [114/191], loss=81.2099
	step [115/191], loss=84.5562
	step [116/191], loss=87.2815
	step [117/191], loss=78.6069
	step [118/191], loss=73.6701
	step [119/191], loss=94.3496
	step [120/191], loss=70.1161
	step [121/191], loss=93.5084
	step [122/191], loss=84.0627
	step [123/191], loss=97.0714
	step [124/191], loss=83.4725
	step [125/191], loss=84.5749
	step [126/191], loss=84.6800
	step [127/191], loss=94.7775
	step [128/191], loss=96.2745
	step [129/191], loss=78.3604
	step [130/191], loss=76.9372
	step [131/191], loss=80.5873
	step [132/191], loss=89.5595
	step [133/191], loss=89.6496
	step [134/191], loss=89.2622
	step [135/191], loss=82.1892
	step [136/191], loss=96.0658
	step [137/191], loss=85.7186
	step [138/191], loss=81.0649
	step [139/191], loss=79.6078
	step [140/191], loss=81.3102
	step [141/191], loss=86.7680
	step [142/191], loss=81.5313
	step [143/191], loss=79.1960
	step [144/191], loss=79.4489
	step [145/191], loss=100.3061
	step [146/191], loss=84.0119
	step [147/191], loss=63.6225
	step [148/191], loss=91.8295
	step [149/191], loss=86.5455
	step [150/191], loss=110.5048
	step [151/191], loss=91.5066
	step [152/191], loss=94.9781
	step [153/191], loss=71.8291
	step [154/191], loss=78.3257
	step [155/191], loss=82.3234
	step [156/191], loss=85.0401
	step [157/191], loss=90.5713
	step [158/191], loss=77.2113
	step [159/191], loss=65.0205
	step [160/191], loss=92.1567
	step [161/191], loss=89.6726
	step [162/191], loss=72.5989
	step [163/191], loss=94.9882
	step [164/191], loss=77.7634
	step [165/191], loss=75.1935
	step [166/191], loss=73.9294
	step [167/191], loss=87.7418
	step [168/191], loss=82.5599
	step [169/191], loss=82.0158
	step [170/191], loss=82.5398
	step [171/191], loss=73.3861
	step [172/191], loss=96.5161
	step [173/191], loss=93.8128
	step [174/191], loss=87.0692
	step [175/191], loss=68.8797
	step [176/191], loss=73.6855
	step [177/191], loss=80.5897
	step [178/191], loss=77.7586
	step [179/191], loss=81.4727
	step [180/191], loss=79.1206
	step [181/191], loss=91.8826
	step [182/191], loss=88.0966
	step [183/191], loss=81.0299
	step [184/191], loss=105.6600
	step [185/191], loss=108.1253
	step [186/191], loss=85.4348
	step [187/191], loss=93.5399
	step [188/191], loss=79.2658
	step [189/191], loss=80.9715
	step [190/191], loss=75.4965
	step [191/191], loss=48.0244
	Evaluating
	loss=0.0423, precision=0.4754, recall=0.8919, f1=0.6202
Training epoch 10
	step [1/191], loss=64.8394
	step [2/191], loss=70.6233
	step [3/191], loss=85.1150
	step [4/191], loss=87.3410
	step [5/191], loss=79.3398
	step [6/191], loss=81.7552
	step [7/191], loss=90.1230
	step [8/191], loss=81.5922
	step [9/191], loss=64.5155
	step [10/191], loss=83.3620
	step [11/191], loss=91.4178
	step [12/191], loss=90.6338
	step [13/191], loss=77.8880
	step [14/191], loss=81.5290
	step [15/191], loss=76.1775
	step [16/191], loss=75.6172
	step [17/191], loss=94.8270
	step [18/191], loss=74.7582
	step [19/191], loss=102.5771
	step [20/191], loss=80.4456
	step [21/191], loss=74.3084
	step [22/191], loss=86.4018
	step [23/191], loss=71.1592
	step [24/191], loss=85.5242
	step [25/191], loss=85.1285
	step [26/191], loss=82.0339
	step [27/191], loss=78.9417
	step [28/191], loss=81.7630
	step [29/191], loss=78.7006
	step [30/191], loss=70.7592
	step [31/191], loss=76.5714
	step [32/191], loss=80.4701
	step [33/191], loss=82.3572
	step [34/191], loss=77.8316
	step [35/191], loss=60.7182
	step [36/191], loss=80.3081
	step [37/191], loss=85.9389
	step [38/191], loss=91.6502
	step [39/191], loss=87.5318
	step [40/191], loss=88.6978
	step [41/191], loss=79.2304
	step [42/191], loss=75.9664
	step [43/191], loss=79.3723
	step [44/191], loss=89.4928
	step [45/191], loss=98.8415
	step [46/191], loss=77.8924
	step [47/191], loss=98.6210
	step [48/191], loss=85.2884
	step [49/191], loss=99.7455
	step [50/191], loss=86.5070
	step [51/191], loss=95.3079
	step [52/191], loss=79.3492
	step [53/191], loss=85.1568
	step [54/191], loss=89.1034
	step [55/191], loss=86.3688
	step [56/191], loss=90.5781
	step [57/191], loss=81.2559
	step [58/191], loss=83.6577
	step [59/191], loss=92.5872
	step [60/191], loss=80.4742
	step [61/191], loss=79.3646
	step [62/191], loss=86.9640
	step [63/191], loss=66.8186
	step [64/191], loss=77.1264
	step [65/191], loss=64.0621
	step [66/191], loss=88.2981
	step [67/191], loss=78.4530
	step [68/191], loss=85.2002
	step [69/191], loss=71.3003
	step [70/191], loss=81.9727
	step [71/191], loss=69.3709
	step [72/191], loss=83.2466
	step [73/191], loss=70.9960
	step [74/191], loss=94.5052
	step [75/191], loss=77.1795
	step [76/191], loss=86.2245
	step [77/191], loss=82.6609
	step [78/191], loss=75.3149
	step [79/191], loss=76.2239
	step [80/191], loss=83.7444
	step [81/191], loss=86.7886
	step [82/191], loss=84.3141
	step [83/191], loss=78.4089
	step [84/191], loss=68.0428
	step [85/191], loss=99.4831
	step [86/191], loss=86.6420
	step [87/191], loss=90.7954
	step [88/191], loss=87.5983
	step [89/191], loss=94.0195
	step [90/191], loss=89.7869
	step [91/191], loss=69.1665
	step [92/191], loss=87.8179
	step [93/191], loss=91.3773
	step [94/191], loss=79.6578
	step [95/191], loss=92.1052
	step [96/191], loss=90.8143
	step [97/191], loss=76.7804
	step [98/191], loss=71.9154
	step [99/191], loss=85.3717
	step [100/191], loss=83.9880
	step [101/191], loss=89.1569
	step [102/191], loss=91.2708
	step [103/191], loss=77.5107
	step [104/191], loss=83.5309
	step [105/191], loss=72.5556
	step [106/191], loss=81.9867
	step [107/191], loss=85.0058
	step [108/191], loss=88.6371
	step [109/191], loss=100.3493
	step [110/191], loss=80.8067
	step [111/191], loss=95.4660
	step [112/191], loss=91.1775
	step [113/191], loss=82.7732
	step [114/191], loss=82.1884
	step [115/191], loss=67.3904
	step [116/191], loss=81.5523
	step [117/191], loss=84.9395
	step [118/191], loss=75.9236
	step [119/191], loss=81.9698
	step [120/191], loss=94.1350
	step [121/191], loss=89.5168
	step [122/191], loss=93.5613
	step [123/191], loss=87.7398
	step [124/191], loss=80.2572
	step [125/191], loss=97.4978
	step [126/191], loss=77.3778
	step [127/191], loss=80.1465
	step [128/191], loss=71.7943
	step [129/191], loss=75.7328
	step [130/191], loss=84.2009
	step [131/191], loss=96.4352
	step [132/191], loss=86.8661
	step [133/191], loss=74.8065
	step [134/191], loss=77.4603
	step [135/191], loss=75.6761
	step [136/191], loss=80.4066
	step [137/191], loss=79.4544
	step [138/191], loss=64.0364
	step [139/191], loss=78.1088
	step [140/191], loss=107.0600
	step [141/191], loss=77.6202
	step [142/191], loss=92.4864
	step [143/191], loss=80.6303
	step [144/191], loss=87.2260
	step [145/191], loss=68.5272
	step [146/191], loss=70.8349
	step [147/191], loss=74.5343
	step [148/191], loss=84.5304
	step [149/191], loss=81.1376
	step [150/191], loss=89.5604
	step [151/191], loss=92.5471
	step [152/191], loss=86.8440
	step [153/191], loss=82.6296
	step [154/191], loss=74.9566
	step [155/191], loss=81.1259
	step [156/191], loss=88.1928
	step [157/191], loss=94.3514
	step [158/191], loss=84.1159
	step [159/191], loss=71.9193
	step [160/191], loss=72.5971
	step [161/191], loss=90.2227
	step [162/191], loss=92.0875
	step [163/191], loss=104.3244
	step [164/191], loss=67.7384
	step [165/191], loss=81.7720
	step [166/191], loss=80.5659
	step [167/191], loss=86.1549
	step [168/191], loss=91.0028
	step [169/191], loss=80.0263
	step [170/191], loss=87.1878
	step [171/191], loss=81.7410
	step [172/191], loss=86.9353
	step [173/191], loss=93.0650
	step [174/191], loss=83.8560
	step [175/191], loss=64.6591
	step [176/191], loss=82.0080
	step [177/191], loss=101.9294
	step [178/191], loss=76.8052
	step [179/191], loss=81.8380
	step [180/191], loss=98.8083
	step [181/191], loss=94.0940
	step [182/191], loss=84.5713
	step [183/191], loss=56.0611
	step [184/191], loss=76.8536
	step [185/191], loss=67.4766
	step [186/191], loss=84.9255
	step [187/191], loss=90.3763
	step [188/191], loss=68.9319
	step [189/191], loss=76.7042
	step [190/191], loss=67.1231
	step [191/191], loss=38.2433
	Evaluating
	loss=0.0356, precision=0.5007, recall=0.8792, f1=0.6380
saving model as: 1_saved_model.pth
Training epoch 11
	step [1/191], loss=83.2746
	step [2/191], loss=63.8963
	step [3/191], loss=88.3103
	step [4/191], loss=86.4326
	step [5/191], loss=87.1656
	step [6/191], loss=87.8242
	step [7/191], loss=74.2626
	step [8/191], loss=81.0620
	step [9/191], loss=80.9497
	step [10/191], loss=81.4261
	step [11/191], loss=76.2755
	step [12/191], loss=77.1300
	step [13/191], loss=87.6885
	step [14/191], loss=78.9500
	step [15/191], loss=78.2244
	step [16/191], loss=91.1760
	step [17/191], loss=92.3595
	step [18/191], loss=104.6843
	step [19/191], loss=70.6962
	step [20/191], loss=95.4261
	step [21/191], loss=95.5913
	step [22/191], loss=78.8145
	step [23/191], loss=72.6063
	step [24/191], loss=63.5492
	step [25/191], loss=66.0670
	step [26/191], loss=72.2419
	step [27/191], loss=89.1101
	step [28/191], loss=87.1646
	step [29/191], loss=78.8550
	step [30/191], loss=72.3662
	step [31/191], loss=75.6835
	step [32/191], loss=68.9801
	step [33/191], loss=87.6074
	step [34/191], loss=79.8068
	step [35/191], loss=85.5702
	step [36/191], loss=90.0188
	step [37/191], loss=96.2298
	step [38/191], loss=74.6147
	step [39/191], loss=75.8001
	step [40/191], loss=72.8526
	step [41/191], loss=79.2882
	step [42/191], loss=70.7421
	step [43/191], loss=63.8207
	step [44/191], loss=94.9604
	step [45/191], loss=63.1042
	step [46/191], loss=74.0548
	step [47/191], loss=76.1011
	step [48/191], loss=79.1452
	step [49/191], loss=89.3893
	step [50/191], loss=62.3080
	step [51/191], loss=93.0235
	step [52/191], loss=87.6241
	step [53/191], loss=91.9063
	step [54/191], loss=82.6008
	step [55/191], loss=83.3416
	step [56/191], loss=72.5747
	step [57/191], loss=74.2886
	step [58/191], loss=83.6483
	step [59/191], loss=75.0972
	step [60/191], loss=96.6439
	step [61/191], loss=87.2310
	step [62/191], loss=80.1124
	step [63/191], loss=72.3543
	step [64/191], loss=78.0834
	step [65/191], loss=63.0982
	step [66/191], loss=78.0626
	step [67/191], loss=92.9865
	step [68/191], loss=85.3515
	step [69/191], loss=73.7794
	step [70/191], loss=83.5610
	step [71/191], loss=68.2690
	step [72/191], loss=75.5125
	step [73/191], loss=75.7408
	step [74/191], loss=83.6749
	step [75/191], loss=81.0520
	step [76/191], loss=77.2464
	step [77/191], loss=77.5103
	step [78/191], loss=78.3272
	step [79/191], loss=69.3636
	step [80/191], loss=93.2323
	step [81/191], loss=85.5790
	step [82/191], loss=98.0991
	step [83/191], loss=67.9221
	step [84/191], loss=78.6803
	step [85/191], loss=86.9027
	step [86/191], loss=92.2482
	step [87/191], loss=86.8448
	step [88/191], loss=87.6770
	step [89/191], loss=79.4987
	step [90/191], loss=76.7768
	step [91/191], loss=87.1331
	step [92/191], loss=90.5329
	step [93/191], loss=83.4584
	step [94/191], loss=92.7540
	step [95/191], loss=68.7717
	step [96/191], loss=81.3529
	step [97/191], loss=85.4689
	step [98/191], loss=85.4152
	step [99/191], loss=90.6617
	step [100/191], loss=88.0722
	step [101/191], loss=75.2690
	step [102/191], loss=85.2461
	step [103/191], loss=90.2488
	step [104/191], loss=83.0089
	step [105/191], loss=66.0434
	step [106/191], loss=94.0446
	step [107/191], loss=74.1860
	step [108/191], loss=77.3350
	step [109/191], loss=94.0728
	step [110/191], loss=67.4965
	step [111/191], loss=75.0997
	step [112/191], loss=89.6621
	step [113/191], loss=83.9997
	step [114/191], loss=73.3191
	step [115/191], loss=78.3008
	step [116/191], loss=80.9599
	step [117/191], loss=64.9729
	step [118/191], loss=84.5915
	step [119/191], loss=88.1451
	step [120/191], loss=90.5017
	step [121/191], loss=80.1440
	step [122/191], loss=82.5691
	step [123/191], loss=71.9184
	step [124/191], loss=74.4886
	step [125/191], loss=102.3502
	step [126/191], loss=88.5391
	step [127/191], loss=86.7390
	step [128/191], loss=77.3738
	step [129/191], loss=74.8227
	step [130/191], loss=80.1104
	step [131/191], loss=73.9015
	step [132/191], loss=78.2235
	step [133/191], loss=76.6662
	step [134/191], loss=72.9751
	step [135/191], loss=109.4077
	step [136/191], loss=87.9660
	step [137/191], loss=99.7562
	step [138/191], loss=84.0477
	step [139/191], loss=80.0372
	step [140/191], loss=80.5422
	step [141/191], loss=102.3918
	step [142/191], loss=92.6493
	step [143/191], loss=85.1351
	step [144/191], loss=74.2240
	step [145/191], loss=77.9465
	step [146/191], loss=89.8562
	step [147/191], loss=87.6456
	step [148/191], loss=89.2482
	step [149/191], loss=91.0220
	step [150/191], loss=77.1602
	step [151/191], loss=74.8029
	step [152/191], loss=85.7957
	step [153/191], loss=87.7078
	step [154/191], loss=104.0841
	step [155/191], loss=68.5834
	step [156/191], loss=79.6758
	step [157/191], loss=69.4877
	step [158/191], loss=68.8796
	step [159/191], loss=74.6581
	step [160/191], loss=81.1903
	step [161/191], loss=68.1837
	step [162/191], loss=81.4071
	step [163/191], loss=89.0964
	step [164/191], loss=96.3047
	step [165/191], loss=85.5018
	step [166/191], loss=73.2043
	step [167/191], loss=83.9683
	step [168/191], loss=66.7467
	step [169/191], loss=70.8427
	step [170/191], loss=78.9734
	step [171/191], loss=73.5079
	step [172/191], loss=76.8049
	step [173/191], loss=77.1157
	step [174/191], loss=74.9503
	step [175/191], loss=75.9024
	step [176/191], loss=69.4666
	step [177/191], loss=88.3897
	step [178/191], loss=78.0286
	step [179/191], loss=90.2243
	step [180/191], loss=90.1792
	step [181/191], loss=98.7943
	step [182/191], loss=76.4824
	step [183/191], loss=99.8531
	step [184/191], loss=84.8528
	step [185/191], loss=73.7646
	step [186/191], loss=81.7432
	step [187/191], loss=74.7389
	step [188/191], loss=69.7292
	step [189/191], loss=82.7646
	step [190/191], loss=82.6737
	step [191/191], loss=36.2044
	Evaluating
	loss=0.0299, precision=0.5177, recall=0.8856, f1=0.6535
saving model as: 1_saved_model.pth
Training epoch 12
	step [1/191], loss=98.9574
	step [2/191], loss=85.1019
	step [3/191], loss=91.0589
	step [4/191], loss=67.1026
	step [5/191], loss=96.4779
	step [6/191], loss=90.9330
	step [7/191], loss=90.7739
	step [8/191], loss=91.7152
	step [9/191], loss=84.5930
	step [10/191], loss=90.9264
	step [11/191], loss=75.8713
	step [12/191], loss=77.7109
	step [13/191], loss=64.6390
	step [14/191], loss=64.4479
	step [15/191], loss=73.8833
	step [16/191], loss=77.9155
	step [17/191], loss=77.8677
	step [18/191], loss=88.1643
	step [19/191], loss=86.3433
	step [20/191], loss=84.5407
	step [21/191], loss=62.5027
	step [22/191], loss=77.8485
	step [23/191], loss=91.6856
	step [24/191], loss=62.3550
	step [25/191], loss=87.4475
	step [26/191], loss=69.9388
	step [27/191], loss=83.5185
	step [28/191], loss=88.8055
	step [29/191], loss=94.2424
	step [30/191], loss=77.0330
	step [31/191], loss=75.6862
	step [32/191], loss=76.4698
	step [33/191], loss=78.1565
	step [34/191], loss=68.8582
	step [35/191], loss=74.7233
	step [36/191], loss=79.4012
	step [37/191], loss=68.4935
	step [38/191], loss=82.2154
	step [39/191], loss=71.1824
	step [40/191], loss=81.1236
	step [41/191], loss=95.0542
	step [42/191], loss=85.1191
	step [43/191], loss=88.3292
	step [44/191], loss=77.7852
	step [45/191], loss=93.5683
	step [46/191], loss=82.7406
	step [47/191], loss=82.7890
	step [48/191], loss=86.2407
	step [49/191], loss=81.9424
	step [50/191], loss=85.7696
	step [51/191], loss=56.4273
	step [52/191], loss=77.3981
	step [53/191], loss=82.1518
	step [54/191], loss=80.6644
	step [55/191], loss=76.2181
	step [56/191], loss=68.9377
	step [57/191], loss=72.3583
	step [58/191], loss=70.1017
	step [59/191], loss=79.8195
	step [60/191], loss=88.4364
	step [61/191], loss=86.2941
	step [62/191], loss=77.7091
	step [63/191], loss=80.0075
	step [64/191], loss=83.3468
	step [65/191], loss=80.9624
	step [66/191], loss=68.4170
	step [67/191], loss=80.4862
	step [68/191], loss=76.2102
	step [69/191], loss=75.9628
	step [70/191], loss=84.3447
	step [71/191], loss=89.5527
	step [72/191], loss=84.0957
	step [73/191], loss=94.3248
	step [74/191], loss=93.2202
	step [75/191], loss=86.8600
	step [76/191], loss=79.8316
	step [77/191], loss=99.1590
	step [78/191], loss=79.5396
	step [79/191], loss=70.3133
	step [80/191], loss=75.6072
	step [81/191], loss=66.5781
	step [82/191], loss=81.6079
	step [83/191], loss=92.9459
	step [84/191], loss=67.7067
	step [85/191], loss=82.2539
	step [86/191], loss=73.3327
	step [87/191], loss=82.9774
	step [88/191], loss=77.8266
	step [89/191], loss=84.1055
	step [90/191], loss=84.5205
	step [91/191], loss=76.4488
	step [92/191], loss=89.4845
	step [93/191], loss=96.3025
	step [94/191], loss=75.3531
	step [95/191], loss=73.8511
	step [96/191], loss=87.4383
	step [97/191], loss=67.5996
	step [98/191], loss=87.9358
	step [99/191], loss=80.0441
	step [100/191], loss=69.3859
	step [101/191], loss=97.5228
	step [102/191], loss=81.4059
	step [103/191], loss=76.0187
	step [104/191], loss=89.2871
	step [105/191], loss=82.2154
	step [106/191], loss=87.0950
	step [107/191], loss=64.6578
	step [108/191], loss=79.9781
	step [109/191], loss=66.3564
	step [110/191], loss=86.6575
	step [111/191], loss=59.1056
	step [112/191], loss=70.9280
	step [113/191], loss=94.4861
	step [114/191], loss=77.1824
	step [115/191], loss=78.0359
	step [116/191], loss=81.5385
	step [117/191], loss=76.0421
	step [118/191], loss=77.2632
	step [119/191], loss=75.2827
	step [120/191], loss=78.0787
	step [121/191], loss=71.1263
	step [122/191], loss=88.9380
	step [123/191], loss=82.3666
	step [124/191], loss=100.6907
	step [125/191], loss=96.6649
	step [126/191], loss=89.8920
	step [127/191], loss=68.1261
	step [128/191], loss=75.4349
	step [129/191], loss=72.7247
	step [130/191], loss=70.8908
	step [131/191], loss=90.1806
	step [132/191], loss=85.9822
	step [133/191], loss=80.9929
	step [134/191], loss=70.1421
	step [135/191], loss=73.9406
	step [136/191], loss=81.6398
	step [137/191], loss=75.9524
	step [138/191], loss=79.5315
	step [139/191], loss=92.2713
	step [140/191], loss=73.6047
	step [141/191], loss=72.8455
	step [142/191], loss=75.5741
	step [143/191], loss=72.9071
	step [144/191], loss=66.5374
	step [145/191], loss=76.1365
	step [146/191], loss=73.4930
	step [147/191], loss=82.4586
	step [148/191], loss=71.1056
	step [149/191], loss=81.2854
	step [150/191], loss=78.3114
	step [151/191], loss=79.5157
	step [152/191], loss=90.9125
	step [153/191], loss=82.7608
	step [154/191], loss=82.5368
	step [155/191], loss=83.5276
	step [156/191], loss=93.6698
	step [157/191], loss=72.6778
	step [158/191], loss=86.5447
	step [159/191], loss=75.4998
	step [160/191], loss=73.5849
	step [161/191], loss=72.0632
	step [162/191], loss=87.8979
	step [163/191], loss=83.9004
	step [164/191], loss=88.1637
	step [165/191], loss=72.5371
	step [166/191], loss=68.5035
	step [167/191], loss=70.9055
	step [168/191], loss=77.5844
	step [169/191], loss=76.3664
	step [170/191], loss=61.4629
	step [171/191], loss=80.9077
	step [172/191], loss=77.8880
	step [173/191], loss=81.9758
	step [174/191], loss=76.2001
	step [175/191], loss=93.5188
	step [176/191], loss=73.9471
	step [177/191], loss=91.5947
	step [178/191], loss=65.7254
	step [179/191], loss=75.4835
	step [180/191], loss=78.8973
	step [181/191], loss=76.8746
	step [182/191], loss=104.1402
	step [183/191], loss=73.1948
	step [184/191], loss=83.0464
	step [185/191], loss=76.1027
	step [186/191], loss=78.0075
	step [187/191], loss=85.5392
	step [188/191], loss=93.6625
	step [189/191], loss=78.8503
	step [190/191], loss=79.4822
	step [191/191], loss=28.4301
	Evaluating
	loss=0.0298, precision=0.4067, recall=0.9050, f1=0.5612
Training epoch 13
	step [1/191], loss=79.9858
	step [2/191], loss=70.3846
	step [3/191], loss=88.7357
	step [4/191], loss=75.0219
	step [5/191], loss=83.7757
	step [6/191], loss=83.8628
	step [7/191], loss=92.4563
	step [8/191], loss=74.7437
	step [9/191], loss=90.2980
	step [10/191], loss=84.1580
	step [11/191], loss=73.5156
	step [12/191], loss=76.7227
	step [13/191], loss=83.0769
	step [14/191], loss=96.4266
	step [15/191], loss=83.1584
	step [16/191], loss=82.6878
	step [17/191], loss=81.9713
	step [18/191], loss=87.9854
	step [19/191], loss=63.5627
	step [20/191], loss=91.9172
	step [21/191], loss=74.1070
	step [22/191], loss=104.4017
	step [23/191], loss=92.5912
	step [24/191], loss=80.8059
	step [25/191], loss=85.5706
	step [26/191], loss=76.4249
	step [27/191], loss=69.8792
	step [28/191], loss=88.5283
	step [29/191], loss=88.4687
	step [30/191], loss=69.4745
	step [31/191], loss=89.8339
	step [32/191], loss=85.6416
	step [33/191], loss=76.0955
	step [34/191], loss=73.8146
	step [35/191], loss=66.5845
	step [36/191], loss=68.8578
	step [37/191], loss=91.1709
	step [38/191], loss=79.3736
	step [39/191], loss=65.2815
	step [40/191], loss=78.4144
	step [41/191], loss=72.8170
	step [42/191], loss=85.1171
	step [43/191], loss=76.5640
	step [44/191], loss=92.0117
	step [45/191], loss=93.9583
	step [46/191], loss=82.4368
	step [47/191], loss=74.7610
	step [48/191], loss=84.9014
	step [49/191], loss=89.1479
	step [50/191], loss=81.0491
	step [51/191], loss=99.9074
	step [52/191], loss=81.1100
	step [53/191], loss=79.5857
	step [54/191], loss=75.2491
	step [55/191], loss=86.6944
	step [56/191], loss=79.1851
	step [57/191], loss=94.6007
	step [58/191], loss=61.9426
	step [59/191], loss=92.8087
	step [60/191], loss=94.9160
	step [61/191], loss=82.1437
	step [62/191], loss=73.2362
	step [63/191], loss=68.7537
	step [64/191], loss=81.4529
	step [65/191], loss=64.8836
	step [66/191], loss=77.3582
	step [67/191], loss=85.7598
	step [68/191], loss=78.3052
	step [69/191], loss=82.5035
	step [70/191], loss=76.5179
	step [71/191], loss=82.1884
	step [72/191], loss=84.7550
	step [73/191], loss=72.2398
	step [74/191], loss=84.8204
	step [75/191], loss=53.4633
	step [76/191], loss=98.4582
	step [77/191], loss=93.2773
	step [78/191], loss=76.7317
	step [79/191], loss=78.1509
	step [80/191], loss=70.1023
	step [81/191], loss=84.6758
	step [82/191], loss=88.2709
	step [83/191], loss=71.1584
	step [84/191], loss=83.3152
	step [85/191], loss=86.2122
	step [86/191], loss=81.5518
	step [87/191], loss=77.9477
	step [88/191], loss=84.5131
	step [89/191], loss=70.7391
	step [90/191], loss=77.3334
	step [91/191], loss=84.1246
	step [92/191], loss=68.1586
	step [93/191], loss=87.9091
	step [94/191], loss=92.6180
	step [95/191], loss=75.3899
	step [96/191], loss=79.8451
	step [97/191], loss=71.3642
	step [98/191], loss=66.3721
	step [99/191], loss=70.8205
	step [100/191], loss=71.9025
	step [101/191], loss=85.4173
	step [102/191], loss=89.3207
	step [103/191], loss=84.6869
	step [104/191], loss=87.5251
	step [105/191], loss=83.4810
	step [106/191], loss=75.0070
	step [107/191], loss=67.7745
	step [108/191], loss=78.2054
	step [109/191], loss=73.9350
	step [110/191], loss=79.2809
	step [111/191], loss=71.0372
	step [112/191], loss=71.0345
	step [113/191], loss=69.4853
	step [114/191], loss=85.4836
	step [115/191], loss=75.1332
	step [116/191], loss=78.0782
	step [117/191], loss=75.8869
	step [118/191], loss=80.7399
	step [119/191], loss=84.2823
	step [120/191], loss=70.8174
	step [121/191], loss=86.8083
	step [122/191], loss=77.7275
	step [123/191], loss=81.6369
	step [124/191], loss=70.8053
	step [125/191], loss=82.5897
	step [126/191], loss=77.5403
	step [127/191], loss=77.4363
	step [128/191], loss=82.9905
	step [129/191], loss=83.1173
	step [130/191], loss=80.5915
	step [131/191], loss=84.2821
	step [132/191], loss=73.3798
	step [133/191], loss=77.5026
	step [134/191], loss=58.4778
	step [135/191], loss=79.3863
	step [136/191], loss=81.6992
	step [137/191], loss=73.2988
	step [138/191], loss=72.0316
	step [139/191], loss=56.9029
	step [140/191], loss=86.9105
	step [141/191], loss=82.4537
	step [142/191], loss=64.9364
	step [143/191], loss=74.8815
	step [144/191], loss=68.8353
	step [145/191], loss=74.5335
	step [146/191], loss=62.0624
	step [147/191], loss=60.4814
	step [148/191], loss=85.4270
	step [149/191], loss=69.6546
	step [150/191], loss=69.9400
	step [151/191], loss=107.8202
	step [152/191], loss=88.5690
	step [153/191], loss=72.6532
	step [154/191], loss=90.3143
	step [155/191], loss=68.7064
	step [156/191], loss=73.0085
	step [157/191], loss=69.3796
	step [158/191], loss=73.2445
	step [159/191], loss=65.3339
	step [160/191], loss=77.2154
	step [161/191], loss=84.3921
	step [162/191], loss=84.3429
	step [163/191], loss=88.9012
	step [164/191], loss=82.7483
	step [165/191], loss=98.1929
	step [166/191], loss=72.5182
	step [167/191], loss=64.5618
	step [168/191], loss=75.7258
	step [169/191], loss=65.7107
	step [170/191], loss=69.9398
	step [171/191], loss=70.0412
	step [172/191], loss=97.4251
	step [173/191], loss=81.3789
	step [174/191], loss=75.3460
	step [175/191], loss=91.0246
	step [176/191], loss=72.7055
	step [177/191], loss=81.7533
	step [178/191], loss=86.7528
	step [179/191], loss=80.4805
	step [180/191], loss=75.4195
	step [181/191], loss=66.2369
	step [182/191], loss=77.2728
	step [183/191], loss=80.0827
	step [184/191], loss=73.1799
	step [185/191], loss=96.9619
	step [186/191], loss=65.4963
	step [187/191], loss=82.3509
	step [188/191], loss=83.8043
	step [189/191], loss=86.8425
	step [190/191], loss=72.6418
	step [191/191], loss=23.0817
	Evaluating
	loss=0.0296, precision=0.3902, recall=0.9009, f1=0.5446
Training epoch 14
	step [1/191], loss=68.5945
	step [2/191], loss=70.0765
	step [3/191], loss=95.9929
	step [4/191], loss=81.0151
	step [5/191], loss=99.6163
	step [6/191], loss=74.9065
	step [7/191], loss=74.3179
	step [8/191], loss=93.2286
	step [9/191], loss=79.1779
	step [10/191], loss=98.2749
	step [11/191], loss=77.4967
	step [12/191], loss=80.0204
	step [13/191], loss=83.2731
	step [14/191], loss=101.0861
	step [15/191], loss=89.3287
	step [16/191], loss=57.3446
	step [17/191], loss=62.1387
	step [18/191], loss=78.6201
	step [19/191], loss=78.3772
	step [20/191], loss=63.1937
	step [21/191], loss=78.5306
	step [22/191], loss=79.0386
	step [23/191], loss=82.3068
	step [24/191], loss=81.6734
	step [25/191], loss=88.9963
	step [26/191], loss=87.8669
	step [27/191], loss=99.6303
	step [28/191], loss=68.2453
	step [29/191], loss=94.8826
	step [30/191], loss=80.5249
	step [31/191], loss=64.3265
	step [32/191], loss=85.1798
	step [33/191], loss=75.1880
	step [34/191], loss=63.4784
	step [35/191], loss=74.9125
	step [36/191], loss=75.6646
	step [37/191], loss=72.2256
	step [38/191], loss=90.4899
	step [39/191], loss=78.3265
	step [40/191], loss=83.3572
	step [41/191], loss=74.5354
	step [42/191], loss=61.7580
	step [43/191], loss=64.0285
	step [44/191], loss=85.7032
	step [45/191], loss=86.0198
	step [46/191], loss=90.0632
	step [47/191], loss=73.7900
	step [48/191], loss=75.5197
	step [49/191], loss=76.1864
	step [50/191], loss=69.8439
	step [51/191], loss=86.1911
	step [52/191], loss=65.3767
	step [53/191], loss=80.4903
	step [54/191], loss=56.4715
	step [55/191], loss=75.4564
	step [56/191], loss=68.4703
	step [57/191], loss=80.4426
	step [58/191], loss=54.4061
	step [59/191], loss=66.0173
	step [60/191], loss=79.7061
	step [61/191], loss=83.2161
	step [62/191], loss=72.4526
	step [63/191], loss=84.3193
	step [64/191], loss=56.9838
	step [65/191], loss=94.1343
	step [66/191], loss=69.8106
	step [67/191], loss=78.3852
	step [68/191], loss=62.3360
	step [69/191], loss=90.3618
	step [70/191], loss=82.0918
	step [71/191], loss=69.7365
	step [72/191], loss=94.3411
	step [73/191], loss=70.4445
	step [74/191], loss=75.7156
	step [75/191], loss=75.5496
	step [76/191], loss=85.8969
	step [77/191], loss=88.3927
	step [78/191], loss=69.0946
	step [79/191], loss=86.0642
	step [80/191], loss=77.5089
	step [81/191], loss=73.3915
	step [82/191], loss=74.5975
	step [83/191], loss=85.9347
	step [84/191], loss=60.6576
	step [85/191], loss=89.4341
	step [86/191], loss=74.3072
	step [87/191], loss=91.2652
	step [88/191], loss=76.5085
	step [89/191], loss=100.3100
	step [90/191], loss=74.0294
	step [91/191], loss=78.0084
	step [92/191], loss=62.0082
	step [93/191], loss=86.0224
	step [94/191], loss=88.6319
	step [95/191], loss=79.5641
	step [96/191], loss=93.1471
	step [97/191], loss=74.9865
	step [98/191], loss=65.6855
	step [99/191], loss=87.0860
	step [100/191], loss=84.1456
	step [101/191], loss=78.9426
	step [102/191], loss=76.2599
	step [103/191], loss=85.3433
	step [104/191], loss=78.9799
	step [105/191], loss=70.3279
	step [106/191], loss=81.7768
	step [107/191], loss=77.1140
	step [108/191], loss=91.6204
	step [109/191], loss=74.6023
	step [110/191], loss=73.2406
	step [111/191], loss=74.8110
	step [112/191], loss=77.6745
	step [113/191], loss=90.3966
	step [114/191], loss=85.6356
	step [115/191], loss=79.7291
	step [116/191], loss=67.4534
	step [117/191], loss=72.7866
	step [118/191], loss=77.3444
	step [119/191], loss=84.4217
	step [120/191], loss=78.8229
	step [121/191], loss=86.3317
	step [122/191], loss=88.9076
	step [123/191], loss=73.0594
	step [124/191], loss=84.4428
	step [125/191], loss=73.0345
	step [126/191], loss=61.5699
	step [127/191], loss=86.9923
	step [128/191], loss=83.2956
	step [129/191], loss=72.0827
	step [130/191], loss=84.6082
	step [131/191], loss=73.0774
	step [132/191], loss=81.2434
	step [133/191], loss=65.6442
	step [134/191], loss=65.4433
	step [135/191], loss=81.3578
	step [136/191], loss=80.5343
	step [137/191], loss=71.8490
	step [138/191], loss=84.9588
	step [139/191], loss=71.8769
	step [140/191], loss=79.0506
	step [141/191], loss=86.9606
	step [142/191], loss=81.5908
	step [143/191], loss=80.9192
	step [144/191], loss=71.5488
	step [145/191], loss=80.7237
	step [146/191], loss=75.6889
	step [147/191], loss=70.1950
	step [148/191], loss=70.8593
	step [149/191], loss=86.1816
	step [150/191], loss=84.4364
	step [151/191], loss=86.3134
	step [152/191], loss=69.1231
	step [153/191], loss=87.0383
	step [154/191], loss=80.7845
	step [155/191], loss=79.8846
	step [156/191], loss=61.7383
	step [157/191], loss=72.9907
	step [158/191], loss=79.2673
	step [159/191], loss=93.4191
	step [160/191], loss=80.1734
	step [161/191], loss=63.5743
	step [162/191], loss=78.8051
	step [163/191], loss=70.6990
	step [164/191], loss=74.3783
	step [165/191], loss=72.3463
	step [166/191], loss=59.2221
	step [167/191], loss=77.7313
	step [168/191], loss=85.2420
	step [169/191], loss=64.9362
	step [170/191], loss=76.4367
	step [171/191], loss=92.7932
	step [172/191], loss=85.8320
	step [173/191], loss=75.9479
	step [174/191], loss=91.3162
	step [175/191], loss=82.2488
	step [176/191], loss=70.2817
	step [177/191], loss=86.1697
	step [178/191], loss=61.6443
	step [179/191], loss=81.5740
	step [180/191], loss=72.9739
	step [181/191], loss=67.5611
	step [182/191], loss=60.2291
	step [183/191], loss=70.9932
	step [184/191], loss=72.5209
	step [185/191], loss=83.4764
	step [186/191], loss=89.1797
	step [187/191], loss=75.3787
	step [188/191], loss=83.2758
	step [189/191], loss=73.1909
	step [190/191], loss=76.3147
	step [191/191], loss=29.7578
	Evaluating
	loss=0.0217, precision=0.4858, recall=0.8903, f1=0.6286
Training epoch 15
	step [1/191], loss=71.9743
	step [2/191], loss=83.8600
	step [3/191], loss=72.3222
	step [4/191], loss=74.4132
	step [5/191], loss=84.2649
	step [6/191], loss=72.2362
	step [7/191], loss=67.1035
	step [8/191], loss=76.6622
	step [9/191], loss=66.3471
	step [10/191], loss=79.1819
	step [11/191], loss=75.7998
	step [12/191], loss=82.1183
	step [13/191], loss=82.1439
	step [14/191], loss=75.2833
	step [15/191], loss=72.5337
	step [16/191], loss=73.4699
	step [17/191], loss=69.7274
	step [18/191], loss=69.0691
	step [19/191], loss=68.2206
	step [20/191], loss=73.3215
	step [21/191], loss=100.4571
	step [22/191], loss=88.4645
	step [23/191], loss=79.9052
	step [24/191], loss=74.5118
	step [25/191], loss=67.9011
	step [26/191], loss=74.9246
	step [27/191], loss=84.4524
	step [28/191], loss=90.7077
	step [29/191], loss=72.3622
	step [30/191], loss=78.1298
	step [31/191], loss=85.9189
	step [32/191], loss=73.1878
	step [33/191], loss=77.3943
	step [34/191], loss=76.4060
	step [35/191], loss=84.6743
	step [36/191], loss=67.4535
	step [37/191], loss=58.6753
	step [38/191], loss=67.3659
	step [39/191], loss=70.8701
	step [40/191], loss=94.9247
	step [41/191], loss=74.0697
	step [42/191], loss=74.6978
	step [43/191], loss=60.5788
	step [44/191], loss=97.7026
	step [45/191], loss=74.6747
	step [46/191], loss=94.6154
	step [47/191], loss=56.9175
	step [48/191], loss=81.3841
	step [49/191], loss=83.7467
	step [50/191], loss=84.1446
	step [51/191], loss=68.3123
	step [52/191], loss=79.8407
	step [53/191], loss=92.6862
	step [54/191], loss=80.9896
	step [55/191], loss=87.9423
	step [56/191], loss=66.3521
	step [57/191], loss=83.2518
	step [58/191], loss=79.2592
	step [59/191], loss=85.0206
	step [60/191], loss=72.1580
	step [61/191], loss=84.7978
	step [62/191], loss=78.9767
	step [63/191], loss=70.5306
	step [64/191], loss=86.6719
	step [65/191], loss=93.8479
	step [66/191], loss=74.2492
	step [67/191], loss=88.4435
	step [68/191], loss=71.9647
	step [69/191], loss=59.3382
	step [70/191], loss=75.2035
	step [71/191], loss=79.5897
	step [72/191], loss=72.8654
	step [73/191], loss=84.7599
	step [74/191], loss=92.3186
	step [75/191], loss=57.2077
	step [76/191], loss=70.2366
	step [77/191], loss=68.2085
	step [78/191], loss=71.3326
	step [79/191], loss=79.8583
	step [80/191], loss=76.4570
	step [81/191], loss=90.2168
	step [82/191], loss=74.9287
	step [83/191], loss=72.9489
	step [84/191], loss=70.9807
	step [85/191], loss=83.4242
	step [86/191], loss=85.7476
	step [87/191], loss=75.0623
	step [88/191], loss=80.5316
	step [89/191], loss=81.9848
	step [90/191], loss=80.5063
	step [91/191], loss=77.2364
	step [92/191], loss=79.4397
	step [93/191], loss=82.9025
	step [94/191], loss=71.4016
	step [95/191], loss=87.7200
	step [96/191], loss=72.6270
	step [97/191], loss=76.6848
	step [98/191], loss=81.4606
	step [99/191], loss=77.0185
	step [100/191], loss=67.2306
	step [101/191], loss=74.4160
	step [102/191], loss=68.6056
	step [103/191], loss=66.4525
	step [104/191], loss=78.2910
	step [105/191], loss=79.9950
	step [106/191], loss=78.1894
	step [107/191], loss=92.9068
	step [108/191], loss=75.4623
	step [109/191], loss=82.7040
	step [110/191], loss=76.3196
	step [111/191], loss=68.9068
	step [112/191], loss=75.9098
	step [113/191], loss=84.3361
	step [114/191], loss=91.4025
	step [115/191], loss=76.4337
	step [116/191], loss=82.4268
	step [117/191], loss=76.7475
	step [118/191], loss=86.3607
	step [119/191], loss=82.4399
	step [120/191], loss=86.4778
	step [121/191], loss=79.5225
	step [122/191], loss=70.9786
	step [123/191], loss=76.0847
	step [124/191], loss=69.1124
	step [125/191], loss=80.0751
	step [126/191], loss=65.4526
	step [127/191], loss=65.3294
	step [128/191], loss=76.7455
	step [129/191], loss=92.9986
	step [130/191], loss=76.8747
	step [131/191], loss=86.0614
	step [132/191], loss=80.7322
	step [133/191], loss=80.2723
	step [134/191], loss=82.1355
	step [135/191], loss=74.2052
	step [136/191], loss=72.1833
	step [137/191], loss=55.2540
	step [138/191], loss=88.2794
	step [139/191], loss=64.2895
	step [140/191], loss=82.6853
	step [141/191], loss=97.5785
	step [142/191], loss=72.5955
	step [143/191], loss=82.0721
	step [144/191], loss=82.3394
	step [145/191], loss=77.7134
	step [146/191], loss=74.4819
	step [147/191], loss=75.8967
	step [148/191], loss=95.9887
	step [149/191], loss=77.8318
	step [150/191], loss=82.8961
	step [151/191], loss=77.1317
	step [152/191], loss=84.5745
	step [153/191], loss=85.7486
	step [154/191], loss=69.4752
	step [155/191], loss=57.8900
	step [156/191], loss=71.9417
	step [157/191], loss=80.9343
	step [158/191], loss=68.8727
	step [159/191], loss=80.7192
	step [160/191], loss=80.0891
	step [161/191], loss=70.5848
	step [162/191], loss=86.7724
	step [163/191], loss=77.5251
	step [164/191], loss=73.8731
	step [165/191], loss=79.3238
	step [166/191], loss=68.9265
	step [167/191], loss=87.1895
	step [168/191], loss=71.4239
	step [169/191], loss=89.2315
	step [170/191], loss=68.4647
	step [171/191], loss=50.8984
	step [172/191], loss=89.3812
	step [173/191], loss=69.7198
	step [174/191], loss=69.7980
	step [175/191], loss=71.3928
	step [176/191], loss=80.1165
	step [177/191], loss=71.6459
	step [178/191], loss=74.2794
	step [179/191], loss=60.8187
	step [180/191], loss=66.7798
	step [181/191], loss=59.2002
	step [182/191], loss=76.3152
	step [183/191], loss=84.6341
	step [184/191], loss=67.8134
	step [185/191], loss=78.0284
	step [186/191], loss=84.4320
	step [187/191], loss=83.7796
	step [188/191], loss=97.1684
	step [189/191], loss=79.1114
	step [190/191], loss=61.9884
	step [191/191], loss=36.0488
	Evaluating
	loss=0.0251, precision=0.3643, recall=0.8833, f1=0.5159
Training epoch 16
	step [1/191], loss=62.5680
	step [2/191], loss=68.7361
	step [3/191], loss=70.8689
	step [4/191], loss=62.0339
	step [5/191], loss=64.1445
	step [6/191], loss=77.0665
	step [7/191], loss=72.2284
	step [8/191], loss=60.2515
	step [9/191], loss=82.1269
	step [10/191], loss=91.9575
	step [11/191], loss=70.1030
	step [12/191], loss=77.5955
	step [13/191], loss=69.8303
	step [14/191], loss=81.0570
	step [15/191], loss=72.5576
	step [16/191], loss=83.6345
	step [17/191], loss=79.4247
	step [18/191], loss=75.6924
	step [19/191], loss=66.8883
	step [20/191], loss=85.4192
	step [21/191], loss=93.5135
	step [22/191], loss=71.7538
	step [23/191], loss=61.8386
	step [24/191], loss=68.0638
	step [25/191], loss=80.2021
	step [26/191], loss=86.7925
	step [27/191], loss=71.0243
	step [28/191], loss=82.8208
	step [29/191], loss=84.0601
	step [30/191], loss=77.9942
	step [31/191], loss=82.3056
	step [32/191], loss=75.0707
	step [33/191], loss=90.3799
	step [34/191], loss=79.5588
	step [35/191], loss=65.7823
	step [36/191], loss=78.4418
	step [37/191], loss=81.5843
	step [38/191], loss=73.7193
	step [39/191], loss=88.7323
	step [40/191], loss=73.2786
	step [41/191], loss=94.1301
	step [42/191], loss=80.9013
	step [43/191], loss=81.1742
	step [44/191], loss=75.7270
	step [45/191], loss=76.1962
	step [46/191], loss=75.5413
	step [47/191], loss=89.4836
	step [48/191], loss=92.9803
	step [49/191], loss=60.6321
	step [50/191], loss=83.7944
	step [51/191], loss=75.7007
	step [52/191], loss=73.9066
	step [53/191], loss=55.6633
	step [54/191], loss=87.9893
	step [55/191], loss=70.1954
	step [56/191], loss=101.5145
	step [57/191], loss=63.4133
	step [58/191], loss=69.5067
	step [59/191], loss=93.4598
	step [60/191], loss=78.0932
	step [61/191], loss=89.8291
	step [62/191], loss=72.3580
	step [63/191], loss=78.0005
	step [64/191], loss=98.7136
	step [65/191], loss=74.3412
	step [66/191], loss=81.0490
	step [67/191], loss=88.6573
	step [68/191], loss=65.1448
	step [69/191], loss=65.7424
	step [70/191], loss=81.5918
	step [71/191], loss=75.8109
	step [72/191], loss=91.9220
	step [73/191], loss=63.3036
	step [74/191], loss=67.3261
	step [75/191], loss=91.4070
	step [76/191], loss=78.3166
	step [77/191], loss=71.3300
	step [78/191], loss=82.0947
	step [79/191], loss=83.9520
	step [80/191], loss=78.8376
	step [81/191], loss=72.7169
	step [82/191], loss=65.6995
	step [83/191], loss=73.3119
	step [84/191], loss=83.3626
	step [85/191], loss=68.7145
	step [86/191], loss=64.7052
	step [87/191], loss=64.4350
	step [88/191], loss=86.6241
	step [89/191], loss=67.8634
	step [90/191], loss=71.8267
	step [91/191], loss=81.3992
	step [92/191], loss=69.5036
	step [93/191], loss=79.8421
	step [94/191], loss=76.3086
	step [95/191], loss=85.5621
	step [96/191], loss=93.0231
	step [97/191], loss=77.1635
	step [98/191], loss=81.5906
	step [99/191], loss=65.3468
	step [100/191], loss=78.7603
	step [101/191], loss=78.6225
	step [102/191], loss=87.8052
	step [103/191], loss=79.4995
	step [104/191], loss=72.2070
	step [105/191], loss=76.2774
	step [106/191], loss=81.6285
	step [107/191], loss=73.6500
	step [108/191], loss=84.8012
	step [109/191], loss=92.4030
	step [110/191], loss=69.7410
	step [111/191], loss=81.0777
	step [112/191], loss=62.4813
	step [113/191], loss=69.2839
	step [114/191], loss=68.9017
	step [115/191], loss=80.5799
	step [116/191], loss=85.2346
	step [117/191], loss=76.3734
	step [118/191], loss=80.5216
	step [119/191], loss=72.5304
	step [120/191], loss=88.4582
	step [121/191], loss=79.0660
	step [122/191], loss=69.1983
	step [123/191], loss=81.3029
	step [124/191], loss=87.1354
	step [125/191], loss=81.0502
	step [126/191], loss=75.0703
	step [127/191], loss=84.0749
	step [128/191], loss=68.0940
	step [129/191], loss=68.7729
	step [130/191], loss=76.9328
	step [131/191], loss=92.7525
	step [132/191], loss=68.4774
	step [133/191], loss=71.9177
	step [134/191], loss=61.3866
	step [135/191], loss=97.7478
	step [136/191], loss=51.7331
	step [137/191], loss=70.4036
	step [138/191], loss=84.7720
	step [139/191], loss=77.6108
	step [140/191], loss=73.1840
	step [141/191], loss=89.8726
	step [142/191], loss=85.5877
	step [143/191], loss=71.4282
	step [144/191], loss=74.4836
	step [145/191], loss=56.1049
	step [146/191], loss=79.7525
	step [147/191], loss=78.3906
	step [148/191], loss=73.5255
	step [149/191], loss=91.0211
	step [150/191], loss=75.7533
	step [151/191], loss=65.5462
	step [152/191], loss=70.4004
	step [153/191], loss=69.6027
	step [154/191], loss=70.2149
	step [155/191], loss=69.7961
	step [156/191], loss=70.8681
	step [157/191], loss=69.6683
	step [158/191], loss=81.7723
	step [159/191], loss=68.5847
	step [160/191], loss=73.3076
	step [161/191], loss=60.6690
	step [162/191], loss=69.8447
	step [163/191], loss=74.9273
	step [164/191], loss=78.6614
	step [165/191], loss=62.8620
	step [166/191], loss=76.1368
	step [167/191], loss=82.1305
	step [168/191], loss=73.8628
	step [169/191], loss=82.1457
	step [170/191], loss=70.8496
	step [171/191], loss=77.7582
	step [172/191], loss=72.5944
	step [173/191], loss=75.3373
	step [174/191], loss=76.5215
	step [175/191], loss=58.2448
	step [176/191], loss=77.0844
	step [177/191], loss=84.5514
	step [178/191], loss=78.7261
	step [179/191], loss=67.9967
	step [180/191], loss=75.2333
	step [181/191], loss=71.9829
	step [182/191], loss=71.8151
	step [183/191], loss=82.3850
	step [184/191], loss=73.4494
	step [185/191], loss=72.0226
	step [186/191], loss=67.0861
	step [187/191], loss=67.1837
	step [188/191], loss=72.9577
	step [189/191], loss=77.6296
	step [190/191], loss=72.4108
	step [191/191], loss=43.7246
	Evaluating
	loss=0.0207, precision=0.3998, recall=0.8943, f1=0.5526
Training epoch 17
	step [1/191], loss=72.0506
	step [2/191], loss=75.3735
	step [3/191], loss=62.3815
	step [4/191], loss=84.9922
	step [5/191], loss=69.4024
	step [6/191], loss=86.1603
	step [7/191], loss=82.2924
	step [8/191], loss=76.3032
	step [9/191], loss=75.0296
	step [10/191], loss=72.6939
	step [11/191], loss=67.6265
	step [12/191], loss=55.5779
	step [13/191], loss=79.3055
	step [14/191], loss=81.5538
	step [15/191], loss=67.0500
	step [16/191], loss=73.7531
	step [17/191], loss=84.8800
	step [18/191], loss=63.2996
	step [19/191], loss=68.4820
	step [20/191], loss=75.5348
	step [21/191], loss=73.6059
	step [22/191], loss=79.4903
	step [23/191], loss=80.3424
	step [24/191], loss=76.8380
	step [25/191], loss=71.3174
	step [26/191], loss=91.3576
	step [27/191], loss=56.0656
	step [28/191], loss=85.7350
	step [29/191], loss=62.1707
	step [30/191], loss=82.1444
	step [31/191], loss=82.9297
	step [32/191], loss=80.6210
	step [33/191], loss=64.8078
	step [34/191], loss=91.6969
	step [35/191], loss=63.6434
	step [36/191], loss=90.3101
	step [37/191], loss=79.0064
	step [38/191], loss=62.4045
	step [39/191], loss=85.8536
	step [40/191], loss=68.9471
	step [41/191], loss=82.8555
	step [42/191], loss=90.5392
	step [43/191], loss=77.4196
	step [44/191], loss=56.0522
	step [45/191], loss=80.6196
	step [46/191], loss=69.6315
	step [47/191], loss=79.1217
	step [48/191], loss=83.6359
	step [49/191], loss=68.5090
	step [50/191], loss=71.1803
	step [51/191], loss=65.9226
	step [52/191], loss=72.5923
	step [53/191], loss=70.8132
	step [54/191], loss=75.0192
	step [55/191], loss=77.5359
	step [56/191], loss=80.0431
	step [57/191], loss=79.8499
	step [58/191], loss=58.4964
	step [59/191], loss=65.6551
	step [60/191], loss=75.3393
	step [61/191], loss=65.0669
	step [62/191], loss=78.9216
	step [63/191], loss=83.4480
	step [64/191], loss=63.6987
	step [65/191], loss=81.7714
	step [66/191], loss=65.4118
	step [67/191], loss=71.4113
	step [68/191], loss=76.8328
	step [69/191], loss=69.1744
	step [70/191], loss=92.9795
	step [71/191], loss=83.6160
	step [72/191], loss=85.9118
	step [73/191], loss=92.3184
	step [74/191], loss=83.1482
	step [75/191], loss=78.9324
	step [76/191], loss=87.6474
	step [77/191], loss=76.2756
	step [78/191], loss=71.2782
	step [79/191], loss=72.1087
	step [80/191], loss=80.2633
	step [81/191], loss=73.4230
	step [82/191], loss=70.5834
	step [83/191], loss=66.9549
	step [84/191], loss=65.1707
	step [85/191], loss=81.0538
	step [86/191], loss=78.0901
	step [87/191], loss=80.0259
	step [88/191], loss=59.2193
	step [89/191], loss=62.8670
	step [90/191], loss=76.9589
	step [91/191], loss=71.4711
	step [92/191], loss=62.8570
	step [93/191], loss=80.5316
	step [94/191], loss=62.2891
	step [95/191], loss=57.4301
	step [96/191], loss=73.4942
	step [97/191], loss=86.7512
	step [98/191], loss=66.6750
	step [99/191], loss=104.9203
	step [100/191], loss=67.6364
	step [101/191], loss=74.4260
	step [102/191], loss=58.2159
	step [103/191], loss=85.8217
	step [104/191], loss=74.7529
	step [105/191], loss=63.9168
	step [106/191], loss=96.4729
	step [107/191], loss=78.4358
	step [108/191], loss=64.1225
	step [109/191], loss=64.6925
	step [110/191], loss=65.3363
	step [111/191], loss=86.0675
	step [112/191], loss=70.5622
	step [113/191], loss=61.7946
	step [114/191], loss=80.4518
	step [115/191], loss=68.1390
	step [116/191], loss=80.9870
	step [117/191], loss=83.0060
	step [118/191], loss=60.8792
	step [119/191], loss=67.1656
	step [120/191], loss=87.2649
	step [121/191], loss=97.0795
	step [122/191], loss=66.2967
	step [123/191], loss=91.6326
	step [124/191], loss=70.7922
	step [125/191], loss=83.0096
	step [126/191], loss=63.0832
	step [127/191], loss=72.4282
	step [128/191], loss=74.2722
	step [129/191], loss=68.1220
	step [130/191], loss=80.0672
	step [131/191], loss=79.0461
	step [132/191], loss=78.3282
	step [133/191], loss=79.2018
	step [134/191], loss=86.7061
	step [135/191], loss=87.5622
	step [136/191], loss=66.3063
	step [137/191], loss=67.4734
	step [138/191], loss=92.5233
	step [139/191], loss=61.6523
	step [140/191], loss=66.4849
	step [141/191], loss=81.9175
	step [142/191], loss=62.1440
	step [143/191], loss=83.1827
	step [144/191], loss=72.9936
	step [145/191], loss=66.9625
	step [146/191], loss=99.7696
	step [147/191], loss=74.0718
	step [148/191], loss=69.8442
	step [149/191], loss=85.2754
	step [150/191], loss=81.9283
	step [151/191], loss=88.1155
	step [152/191], loss=79.4857
	step [153/191], loss=69.3612
	step [154/191], loss=74.4242
	step [155/191], loss=79.1635
	step [156/191], loss=77.5537
	step [157/191], loss=91.2263
	step [158/191], loss=80.7332
	step [159/191], loss=73.0316
	step [160/191], loss=67.7338
	step [161/191], loss=60.0684
	step [162/191], loss=67.7987
	step [163/191], loss=72.3916
	step [164/191], loss=62.4873
	step [165/191], loss=82.0491
	step [166/191], loss=57.0286
	step [167/191], loss=69.0014
	step [168/191], loss=69.3004
	step [169/191], loss=70.3164
	step [170/191], loss=86.6387
	step [171/191], loss=64.3316
	step [172/191], loss=89.0019
	step [173/191], loss=83.1055
	step [174/191], loss=75.3027
	step [175/191], loss=87.9653
	step [176/191], loss=92.0846
	step [177/191], loss=80.4390
	step [178/191], loss=80.9901
	step [179/191], loss=77.9764
	step [180/191], loss=66.9018
	step [181/191], loss=79.1119
	step [182/191], loss=77.4502
	step [183/191], loss=82.8327
	step [184/191], loss=84.6240
	step [185/191], loss=53.7566
	step [186/191], loss=78.8588
	step [187/191], loss=72.8927
	step [188/191], loss=80.2977
	step [189/191], loss=74.4062
	step [190/191], loss=79.9256
	step [191/191], loss=33.8653
	Evaluating
	loss=0.0166, precision=0.4999, recall=0.8788, f1=0.6373
Training epoch 18
	step [1/191], loss=70.2897
	step [2/191], loss=61.5769
	step [3/191], loss=76.2183
	step [4/191], loss=88.9742
	step [5/191], loss=76.6990
	step [6/191], loss=67.5225
	step [7/191], loss=67.8433
	step [8/191], loss=75.9675
	step [9/191], loss=72.9625
	step [10/191], loss=69.2358
	step [11/191], loss=64.6842
	step [12/191], loss=65.1607
	step [13/191], loss=77.3746
	step [14/191], loss=62.4074
	step [15/191], loss=92.6662
	step [16/191], loss=80.7687
	step [17/191], loss=67.5528
	step [18/191], loss=88.0301
	step [19/191], loss=74.4583
	step [20/191], loss=82.1857
	step [21/191], loss=85.6649
	step [22/191], loss=79.5930
	step [23/191], loss=87.2305
	step [24/191], loss=86.7374
	step [25/191], loss=70.3698
	step [26/191], loss=81.0392
	step [27/191], loss=75.5314
	step [28/191], loss=71.8972
	step [29/191], loss=72.7491
	step [30/191], loss=73.3791
	step [31/191], loss=61.9632
	step [32/191], loss=64.1235
	step [33/191], loss=69.5427
	step [34/191], loss=73.7004
	step [35/191], loss=77.4672
	step [36/191], loss=83.4256
	step [37/191], loss=62.1014
	step [38/191], loss=79.9169
	step [39/191], loss=69.3640
	step [40/191], loss=81.4640
	step [41/191], loss=65.5024
	step [42/191], loss=82.6619
	step [43/191], loss=73.3920
	step [44/191], loss=73.9327
	step [45/191], loss=67.8578
	step [46/191], loss=77.7474
	step [47/191], loss=74.4950
	step [48/191], loss=66.4707
	step [49/191], loss=75.1967
	step [50/191], loss=58.4286
	step [51/191], loss=59.9149
	step [52/191], loss=78.1609
	step [53/191], loss=68.6263
	step [54/191], loss=61.8903
	step [55/191], loss=59.3622
	step [56/191], loss=70.6733
	step [57/191], loss=83.0077
	step [58/191], loss=72.4627
	step [59/191], loss=97.5913
	step [60/191], loss=61.7621
	step [61/191], loss=86.4539
	step [62/191], loss=94.1824
	step [63/191], loss=65.2257
	step [64/191], loss=73.2673
	step [65/191], loss=72.3720
	step [66/191], loss=79.8118
	step [67/191], loss=71.6989
	step [68/191], loss=76.4173
	step [69/191], loss=89.6624
	step [70/191], loss=90.0096
	step [71/191], loss=59.8310
	step [72/191], loss=71.1465
	step [73/191], loss=88.3634
	step [74/191], loss=78.2964
	step [75/191], loss=67.9805
	step [76/191], loss=70.5839
	step [77/191], loss=78.3264
	step [78/191], loss=65.5148
	step [79/191], loss=67.9940
	step [80/191], loss=70.6037
	step [81/191], loss=73.0801
	step [82/191], loss=94.2694
	step [83/191], loss=73.6908
	step [84/191], loss=84.4418
	step [85/191], loss=83.5255
	step [86/191], loss=91.4471
	step [87/191], loss=93.2864
	step [88/191], loss=79.0036
	step [89/191], loss=94.8360
	step [90/191], loss=58.9374
	step [91/191], loss=70.6567
	step [92/191], loss=70.6259
	step [93/191], loss=77.6829
	step [94/191], loss=62.8278
	step [95/191], loss=74.1768
	step [96/191], loss=67.8062
	step [97/191], loss=87.8791
	step [98/191], loss=85.5499
	step [99/191], loss=68.0091
	step [100/191], loss=89.8672
	step [101/191], loss=72.5718
	step [102/191], loss=67.9325
	step [103/191], loss=83.5445
	step [104/191], loss=95.2645
	step [105/191], loss=74.7083
	step [106/191], loss=67.6529
	step [107/191], loss=70.6753
	step [108/191], loss=57.4284
	step [109/191], loss=65.2576
	step [110/191], loss=55.9213
	step [111/191], loss=75.4933
	step [112/191], loss=75.7325
	step [113/191], loss=78.2947
	step [114/191], loss=85.9030
	step [115/191], loss=63.9467
	step [116/191], loss=74.7579
	step [117/191], loss=77.1970
	step [118/191], loss=71.1132
	step [119/191], loss=61.1621
	step [120/191], loss=84.5420
	step [121/191], loss=97.8592
	step [122/191], loss=72.7612
	step [123/191], loss=68.8864
	step [124/191], loss=58.7082
	step [125/191], loss=70.9690
	step [126/191], loss=74.2245
	step [127/191], loss=72.9820
	step [128/191], loss=60.0374
	step [129/191], loss=76.9229
	step [130/191], loss=73.4062
	step [131/191], loss=71.0184
	step [132/191], loss=67.7289
	step [133/191], loss=71.1825
	step [134/191], loss=80.2644
	step [135/191], loss=62.4750
	step [136/191], loss=75.3987
	step [137/191], loss=67.8639
	step [138/191], loss=88.8252
	step [139/191], loss=70.7129
	step [140/191], loss=76.6525
	step [141/191], loss=75.4752
	step [142/191], loss=69.9289
	step [143/191], loss=57.1510
	step [144/191], loss=75.9880
	step [145/191], loss=73.3385
	step [146/191], loss=69.7891
	step [147/191], loss=58.9284
	step [148/191], loss=61.0036
	step [149/191], loss=67.2877
	step [150/191], loss=77.9673
	step [151/191], loss=71.9281
	step [152/191], loss=80.8488
	step [153/191], loss=65.1496
	step [154/191], loss=80.8823
	step [155/191], loss=88.6972
	step [156/191], loss=79.6030
	step [157/191], loss=63.4841
	step [158/191], loss=77.6341
	step [159/191], loss=76.8552
	step [160/191], loss=85.6637
	step [161/191], loss=81.2443
	step [162/191], loss=75.8607
	step [163/191], loss=94.5949
	step [164/191], loss=70.0607
	step [165/191], loss=90.4482
	step [166/191], loss=79.3908
	step [167/191], loss=89.0760
	step [168/191], loss=65.3721
	step [169/191], loss=59.7887
	step [170/191], loss=91.1546
	step [171/191], loss=79.8837
	step [172/191], loss=68.6622
	step [173/191], loss=90.0123
	step [174/191], loss=76.3933
	step [175/191], loss=76.8063
	step [176/191], loss=66.7951
	step [177/191], loss=76.3756
	step [178/191], loss=76.0953
	step [179/191], loss=75.0125
	step [180/191], loss=91.2756
	step [181/191], loss=76.6936
	step [182/191], loss=75.5126
	step [183/191], loss=72.7153
	step [184/191], loss=83.6956
	step [185/191], loss=78.9020
	step [186/191], loss=59.1613
	step [187/191], loss=66.1485
	step [188/191], loss=85.4834
	step [189/191], loss=77.8856
	step [190/191], loss=86.8776
	step [191/191], loss=43.6219
	Evaluating
	loss=0.0154, precision=0.4639, recall=0.8943, f1=0.6109
Training epoch 19
	step [1/191], loss=79.3902
	step [2/191], loss=68.4794
	step [3/191], loss=79.1341
	step [4/191], loss=67.8315
	step [5/191], loss=83.0560
	step [6/191], loss=60.9274
	step [7/191], loss=93.4601
	step [8/191], loss=75.4981
	step [9/191], loss=75.9550
	step [10/191], loss=71.3895
	step [11/191], loss=80.0098
	step [12/191], loss=66.4369
	step [13/191], loss=84.6126
	step [14/191], loss=76.6335
	step [15/191], loss=55.4052
	step [16/191], loss=63.3353
	step [17/191], loss=67.3110
	step [18/191], loss=70.3173
	step [19/191], loss=69.4750
	step [20/191], loss=77.2254
	step [21/191], loss=80.1220
	step [22/191], loss=64.9168
	step [23/191], loss=54.0058
	step [24/191], loss=66.5649
	step [25/191], loss=73.1876
	step [26/191], loss=96.6369
	step [27/191], loss=84.0192
	step [28/191], loss=69.3026
	step [29/191], loss=80.0419
	step [30/191], loss=92.6202
	step [31/191], loss=83.0634
	step [32/191], loss=58.9031
	step [33/191], loss=87.5709
	step [34/191], loss=82.0191
	step [35/191], loss=72.2998
	step [36/191], loss=65.4257
	step [37/191], loss=63.8517
	step [38/191], loss=75.6805
	step [39/191], loss=63.6284
	step [40/191], loss=73.5097
	step [41/191], loss=70.3606
	step [42/191], loss=66.8058
	step [43/191], loss=61.1197
	step [44/191], loss=66.9334
	step [45/191], loss=81.3913
	step [46/191], loss=73.3257
	step [47/191], loss=79.7806
	step [48/191], loss=64.6337
	step [49/191], loss=69.9802
	step [50/191], loss=67.9049
	step [51/191], loss=58.6441
	step [52/191], loss=80.9933
	step [53/191], loss=76.7246
	step [54/191], loss=69.1970
	step [55/191], loss=76.9234
	step [56/191], loss=64.8462
	step [57/191], loss=77.6266
	step [58/191], loss=87.0962
	step [59/191], loss=58.0403
	step [60/191], loss=60.6589
	step [61/191], loss=85.2922
	step [62/191], loss=73.4304
	step [63/191], loss=81.6056
	step [64/191], loss=66.3130
	step [65/191], loss=72.3342
	step [66/191], loss=79.6807
	step [67/191], loss=88.1964
	step [68/191], loss=71.5257
	step [69/191], loss=82.2226
	step [70/191], loss=66.6878
	step [71/191], loss=76.8416
	step [72/191], loss=78.1997
	step [73/191], loss=77.1136
	step [74/191], loss=78.8175
	step [75/191], loss=63.4666
	step [76/191], loss=76.1870
	step [77/191], loss=79.5676
	step [78/191], loss=85.6403
	step [79/191], loss=70.3560
	step [80/191], loss=67.2017
	step [81/191], loss=83.1962
	step [82/191], loss=77.0982
	step [83/191], loss=86.5261
	step [84/191], loss=64.7052
	step [85/191], loss=63.5646
	step [86/191], loss=66.7720
	step [87/191], loss=78.2741
	step [88/191], loss=72.1849
	step [89/191], loss=67.8208
	step [90/191], loss=78.8587
	step [91/191], loss=80.6059
	step [92/191], loss=67.2128
	step [93/191], loss=74.0081
	step [94/191], loss=72.9779
	step [95/191], loss=85.6179
	step [96/191], loss=69.4740
	step [97/191], loss=71.6753
	step [98/191], loss=81.3934
	step [99/191], loss=73.6967
	step [100/191], loss=91.3047
	step [101/191], loss=70.8085
	step [102/191], loss=61.5707
	step [103/191], loss=85.7714
	step [104/191], loss=88.4518
	step [105/191], loss=62.3806
	step [106/191], loss=61.6737
	step [107/191], loss=86.5525
	step [108/191], loss=98.8911
	step [109/191], loss=62.6027
	step [110/191], loss=70.0414
	step [111/191], loss=78.2194
	step [112/191], loss=82.8597
	step [113/191], loss=91.9691
	step [114/191], loss=78.7050
	step [115/191], loss=64.1706
	step [116/191], loss=85.9128
	step [117/191], loss=90.5057
	step [118/191], loss=76.9971
	step [119/191], loss=67.6413
	step [120/191], loss=73.5862
	step [121/191], loss=82.0513
	step [122/191], loss=68.4930
	step [123/191], loss=92.5804
	step [124/191], loss=53.0165
	step [125/191], loss=80.8363
	step [126/191], loss=64.3961
	step [127/191], loss=78.7176
	step [128/191], loss=77.8646
	step [129/191], loss=73.6334
	step [130/191], loss=68.9640
	step [131/191], loss=72.9108
	step [132/191], loss=81.0624
	step [133/191], loss=60.0782
	step [134/191], loss=66.3757
	step [135/191], loss=72.8778
	step [136/191], loss=81.6909
	step [137/191], loss=71.5560
	step [138/191], loss=71.3067
	step [139/191], loss=71.5844
	step [140/191], loss=61.8994
	step [141/191], loss=77.8807
	step [142/191], loss=70.1687
	step [143/191], loss=61.6269
	step [144/191], loss=58.8434
	step [145/191], loss=78.5374
	step [146/191], loss=75.1139
	step [147/191], loss=64.0513
	step [148/191], loss=81.4440
	step [149/191], loss=78.6760
	step [150/191], loss=72.0484
	step [151/191], loss=75.9863
	step [152/191], loss=71.4791
	step [153/191], loss=68.1661
	step [154/191], loss=67.3503
	step [155/191], loss=78.9545
	step [156/191], loss=72.0435
	step [157/191], loss=81.2010
	step [158/191], loss=70.2494
	step [159/191], loss=61.4687
	step [160/191], loss=65.6786
	step [161/191], loss=62.9894
	step [162/191], loss=77.7076
	step [163/191], loss=74.8310
	step [164/191], loss=76.1012
	step [165/191], loss=74.9011
	step [166/191], loss=79.5101
	step [167/191], loss=57.9226
	step [168/191], loss=67.7697
	step [169/191], loss=73.2949
	step [170/191], loss=71.5039
	step [171/191], loss=69.5907
	step [172/191], loss=71.4536
	step [173/191], loss=60.6193
	step [174/191], loss=73.6439
	step [175/191], loss=83.8829
	step [176/191], loss=72.0760
	step [177/191], loss=68.5019
	step [178/191], loss=76.1975
	step [179/191], loss=72.4420
	step [180/191], loss=54.2893
	step [181/191], loss=73.0956
	step [182/191], loss=70.6376
	step [183/191], loss=80.8271
	step [184/191], loss=62.0129
	step [185/191], loss=78.1735
	step [186/191], loss=75.5167
	step [187/191], loss=90.3047
	step [188/191], loss=76.7762
	step [189/191], loss=74.0345
	step [190/191], loss=65.9903
	step [191/191], loss=39.7026
	Evaluating
	loss=0.0145, precision=0.4453, recall=0.8952, f1=0.5948
Training epoch 20
	step [1/191], loss=67.9367
	step [2/191], loss=82.9056
	step [3/191], loss=100.2491
	step [4/191], loss=76.7467
	step [5/191], loss=49.7906
	step [6/191], loss=85.0196
	step [7/191], loss=76.6279
	step [8/191], loss=65.6601
	step [9/191], loss=72.6723
	step [10/191], loss=78.5234
	step [11/191], loss=77.9075
	step [12/191], loss=78.8182
	step [13/191], loss=75.2775
	step [14/191], loss=74.5050
	step [15/191], loss=55.1910
	step [16/191], loss=66.3835
	step [17/191], loss=57.2990
	step [18/191], loss=63.6026
	step [19/191], loss=71.1572
	step [20/191], loss=64.1881
	step [21/191], loss=77.5401
	step [22/191], loss=78.3284
	step [23/191], loss=89.2536
	step [24/191], loss=65.0825
	step [25/191], loss=71.0327
	step [26/191], loss=77.5296
	step [27/191], loss=63.4695
	step [28/191], loss=76.0508
	step [29/191], loss=63.7376
	step [30/191], loss=64.7313
	step [31/191], loss=58.6649
	step [32/191], loss=75.9954
	step [33/191], loss=74.8697
	step [34/191], loss=79.1809
	step [35/191], loss=79.2613
	step [36/191], loss=71.7596
	step [37/191], loss=89.9504
	step [38/191], loss=67.2348
	step [39/191], loss=82.3524
	step [40/191], loss=68.9843
	step [41/191], loss=71.9364
	step [42/191], loss=68.2506
	step [43/191], loss=68.3740
	step [44/191], loss=77.8734
	step [45/191], loss=62.3804
	step [46/191], loss=70.7429
	step [47/191], loss=91.4722
	step [48/191], loss=65.4091
	step [49/191], loss=68.3125
	step [50/191], loss=73.3691
	step [51/191], loss=80.1754
	step [52/191], loss=70.5963
	step [53/191], loss=79.7121
	step [54/191], loss=81.2908
	step [55/191], loss=80.5513
	step [56/191], loss=85.2553
	step [57/191], loss=64.3221
	step [58/191], loss=87.8490
	step [59/191], loss=77.0532
	step [60/191], loss=67.6931
	step [61/191], loss=74.6451
	step [62/191], loss=64.8001
	step [63/191], loss=72.4014
	step [64/191], loss=58.3175
	step [65/191], loss=82.9668
	step [66/191], loss=70.1760
	step [67/191], loss=59.3937
	step [68/191], loss=77.6996
	step [69/191], loss=70.3463
	step [70/191], loss=79.4368
	step [71/191], loss=72.6508
	step [72/191], loss=71.5868
	step [73/191], loss=66.7904
	step [74/191], loss=80.9988
	step [75/191], loss=74.4564
	step [76/191], loss=74.5070
	step [77/191], loss=83.3062
	step [78/191], loss=68.6887
	step [79/191], loss=75.3517
	step [80/191], loss=64.3862
	step [81/191], loss=80.5174
	step [82/191], loss=62.4225
	step [83/191], loss=86.1271
	step [84/191], loss=67.2073
	step [85/191], loss=64.9499
	step [86/191], loss=60.2626
	step [87/191], loss=57.6978
	step [88/191], loss=75.4834
	step [89/191], loss=84.8580
	step [90/191], loss=72.1336
	step [91/191], loss=77.1912
	step [92/191], loss=54.0025
	step [93/191], loss=73.2367
	step [94/191], loss=71.8822
	step [95/191], loss=65.7414
	step [96/191], loss=74.3474
	step [97/191], loss=74.0911
	step [98/191], loss=86.8767
	step [99/191], loss=79.1831
	step [100/191], loss=75.7041
	step [101/191], loss=74.8670
	step [102/191], loss=64.8764
	step [103/191], loss=70.2149
	step [104/191], loss=78.9702
	step [105/191], loss=74.2833
	step [106/191], loss=73.7225
	step [107/191], loss=80.8252
	step [108/191], loss=68.3939
	step [109/191], loss=76.5582
	step [110/191], loss=67.4341
	step [111/191], loss=80.6233
	step [112/191], loss=77.6535
	step [113/191], loss=74.5526
	step [114/191], loss=74.4249
	step [115/191], loss=81.5779
	step [116/191], loss=76.4771
	step [117/191], loss=66.7370
	step [118/191], loss=58.8200
	step [119/191], loss=51.3964
	step [120/191], loss=79.3433
	step [121/191], loss=69.1330
	step [122/191], loss=58.1698
	step [123/191], loss=80.3607
	step [124/191], loss=64.8992
	step [125/191], loss=67.2041
	step [126/191], loss=65.5300
	step [127/191], loss=74.3882
	step [128/191], loss=59.3685
	step [129/191], loss=72.7917
	step [130/191], loss=77.4677
	step [131/191], loss=66.5942
	step [132/191], loss=63.8166
	step [133/191], loss=84.2623
	step [134/191], loss=85.1916
	step [135/191], loss=68.0837
	step [136/191], loss=76.9982
	step [137/191], loss=86.5603
	step [138/191], loss=71.9116
	step [139/191], loss=63.7804
	step [140/191], loss=77.4963
	step [141/191], loss=87.5175
	step [142/191], loss=74.0966
	step [143/191], loss=77.4809
	step [144/191], loss=65.2096
	step [145/191], loss=87.8358
	step [146/191], loss=60.8836
	step [147/191], loss=61.1686
	step [148/191], loss=63.9844
	step [149/191], loss=61.0436
	step [150/191], loss=88.6951
	step [151/191], loss=71.0192
	step [152/191], loss=77.0473
	step [153/191], loss=75.0153
	step [154/191], loss=91.4158
	step [155/191], loss=85.3580
	step [156/191], loss=74.4138
	step [157/191], loss=69.0485
	step [158/191], loss=82.8759
	step [159/191], loss=82.0666
	step [160/191], loss=72.4298
	step [161/191], loss=75.0966
	step [162/191], loss=60.9140
	step [163/191], loss=63.9249
	step [164/191], loss=75.7795
	step [165/191], loss=63.9904
	step [166/191], loss=75.9183
	step [167/191], loss=63.8846
	step [168/191], loss=75.8613
	step [169/191], loss=70.4379
	step [170/191], loss=55.8516
	step [171/191], loss=55.8255
	step [172/191], loss=70.3760
	step [173/191], loss=92.0965
	step [174/191], loss=93.4962
	step [175/191], loss=64.9239
	step [176/191], loss=63.0183
	step [177/191], loss=75.3844
	step [178/191], loss=91.4823
	step [179/191], loss=85.2698
	step [180/191], loss=73.3718
	step [181/191], loss=74.8308
	step [182/191], loss=73.6424
	step [183/191], loss=79.6110
	step [184/191], loss=79.3621
	step [185/191], loss=79.9505
	step [186/191], loss=63.5863
	step [187/191], loss=77.9243
	step [188/191], loss=66.1690
	step [189/191], loss=67.1082
	step [190/191], loss=72.0275
	step [191/191], loss=28.8497
	Evaluating
	loss=0.0135, precision=0.4747, recall=0.9037, f1=0.6224
Training epoch 21
	step [1/191], loss=74.1933
	step [2/191], loss=66.9808
	step [3/191], loss=73.1104
	step [4/191], loss=82.9808
	step [5/191], loss=80.3055
	step [6/191], loss=77.3390
	step [7/191], loss=69.2035
	step [8/191], loss=72.6274
	step [9/191], loss=84.0927
	step [10/191], loss=75.2897
	step [11/191], loss=71.0959
	step [12/191], loss=67.2158
	step [13/191], loss=67.0129
	step [14/191], loss=64.7511
	step [15/191], loss=77.2661
	step [16/191], loss=69.3802
	step [17/191], loss=74.9207
	step [18/191], loss=85.4572
	step [19/191], loss=72.0821
	step [20/191], loss=77.4474
	step [21/191], loss=91.0114
	step [22/191], loss=69.1326
	step [23/191], loss=75.0144
	step [24/191], loss=68.2767
	step [25/191], loss=86.0849
	step [26/191], loss=72.1023
	step [27/191], loss=70.8419
	step [28/191], loss=68.4285
	step [29/191], loss=84.0837
	step [30/191], loss=80.5012
	step [31/191], loss=66.3315
	step [32/191], loss=73.9414
	step [33/191], loss=68.5107
	step [34/191], loss=63.5467
	step [35/191], loss=58.8088
	step [36/191], loss=85.6694
	step [37/191], loss=62.1146
	step [38/191], loss=62.1717
	step [39/191], loss=71.2746
	step [40/191], loss=77.1586
	step [41/191], loss=80.1068
	step [42/191], loss=69.7824
	step [43/191], loss=64.9624
	step [44/191], loss=81.5739
	step [45/191], loss=75.0846
	step [46/191], loss=62.7895
	step [47/191], loss=58.6565
	step [48/191], loss=79.6889
	step [49/191], loss=68.9511
	step [50/191], loss=73.4845
	step [51/191], loss=60.1923
	step [52/191], loss=56.9486
	step [53/191], loss=74.1595
	step [54/191], loss=63.7793
	step [55/191], loss=77.6320
	step [56/191], loss=69.6968
	step [57/191], loss=71.2280
	step [58/191], loss=74.7024
	step [59/191], loss=73.7395
	step [60/191], loss=71.9344
	step [61/191], loss=79.5340
	step [62/191], loss=68.0612
	step [63/191], loss=66.4679
	step [64/191], loss=78.8573
	step [65/191], loss=74.4385
	step [66/191], loss=73.3969
	step [67/191], loss=85.5087
	step [68/191], loss=73.2205
	step [69/191], loss=91.7157
	step [70/191], loss=73.2598
	step [71/191], loss=72.9290
	step [72/191], loss=77.3802
	step [73/191], loss=85.7632
	step [74/191], loss=72.5194
	step [75/191], loss=69.4963
	step [76/191], loss=62.7824
	step [77/191], loss=65.6738
	step [78/191], loss=57.5379
	step [79/191], loss=64.9252
	step [80/191], loss=57.9050
	step [81/191], loss=63.3725
	step [82/191], loss=86.7214
	step [83/191], loss=63.2703
	step [84/191], loss=79.1682
	step [85/191], loss=89.9556
	step [86/191], loss=81.3988
	step [87/191], loss=83.6458
	step [88/191], loss=65.6889
	step [89/191], loss=90.3047
	step [90/191], loss=92.4423
	step [91/191], loss=58.5962
	step [92/191], loss=65.3282
	step [93/191], loss=75.4449
	step [94/191], loss=85.8069
	step [95/191], loss=68.0883
	step [96/191], loss=78.9916
	step [97/191], loss=67.4069
	step [98/191], loss=71.5858
	step [99/191], loss=55.2175
	step [100/191], loss=51.6507
	step [101/191], loss=73.0465
	step [102/191], loss=63.7154
	step [103/191], loss=70.5095
	step [104/191], loss=74.0196
	step [105/191], loss=92.6029
	step [106/191], loss=70.1425
	step [107/191], loss=71.6910
	step [108/191], loss=72.6319
	step [109/191], loss=69.2311
	step [110/191], loss=67.3839
	step [111/191], loss=59.0518
	step [112/191], loss=64.2521
	step [113/191], loss=69.2017
	step [114/191], loss=69.6314
	step [115/191], loss=76.1604
	step [116/191], loss=67.9664
	step [117/191], loss=60.0302
	step [118/191], loss=78.0002
	step [119/191], loss=69.7506
	step [120/191], loss=68.2634
	step [121/191], loss=73.3084
	step [122/191], loss=65.9618
	step [123/191], loss=68.3904
	step [124/191], loss=63.1768
	step [125/191], loss=75.6380
	step [126/191], loss=78.3220
	step [127/191], loss=69.2758
	step [128/191], loss=80.4131
	step [129/191], loss=71.5305
	step [130/191], loss=67.9418
	step [131/191], loss=69.5637
	step [132/191], loss=81.7892
	step [133/191], loss=79.0112
	step [134/191], loss=65.9986
	step [135/191], loss=72.2413
	step [136/191], loss=82.4766
	step [137/191], loss=62.7215
	step [138/191], loss=60.1717
	step [139/191], loss=70.7450
	step [140/191], loss=71.6609
	step [141/191], loss=66.9449
	step [142/191], loss=79.9098
	step [143/191], loss=73.7587
	step [144/191], loss=57.3922
	step [145/191], loss=61.6138
	step [146/191], loss=94.2810
	step [147/191], loss=60.8272
	step [148/191], loss=80.6156
	step [149/191], loss=69.5528
	step [150/191], loss=71.0185
	step [151/191], loss=73.3233
	step [152/191], loss=69.8708
	step [153/191], loss=70.3054
	step [154/191], loss=66.6391
	step [155/191], loss=65.2239
	step [156/191], loss=69.9623
	step [157/191], loss=67.6417
	step [158/191], loss=68.4933
	step [159/191], loss=67.4348
	step [160/191], loss=71.0138
	step [161/191], loss=83.7061
	step [162/191], loss=70.6377
	step [163/191], loss=64.5471
	step [164/191], loss=71.6823
	step [165/191], loss=54.1836
	step [166/191], loss=68.6863
	step [167/191], loss=77.0555
	step [168/191], loss=73.1832
	step [169/191], loss=78.5542
	step [170/191], loss=78.3016
	step [171/191], loss=66.7900
	step [172/191], loss=80.6528
	step [173/191], loss=71.1993
	step [174/191], loss=68.3167
	step [175/191], loss=75.8117
	step [176/191], loss=83.2439
	step [177/191], loss=74.8420
	step [178/191], loss=79.3957
	step [179/191], loss=80.8404
	step [180/191], loss=93.3474
	step [181/191], loss=59.7508
	step [182/191], loss=56.4654
	step [183/191], loss=67.8451
	step [184/191], loss=59.8889
	step [185/191], loss=83.6105
	step [186/191], loss=69.2072
	step [187/191], loss=66.2704
	step [188/191], loss=64.9247
	step [189/191], loss=70.1767
	step [190/191], loss=74.7386
	step [191/191], loss=30.4384
	Evaluating
	loss=0.0138, precision=0.4358, recall=0.8932, f1=0.5858
Training epoch 22
	step [1/191], loss=75.4890
	step [2/191], loss=79.3893
	step [3/191], loss=73.0413
	step [4/191], loss=76.6867
	step [5/191], loss=67.4984
	step [6/191], loss=65.7897
	step [7/191], loss=66.7859
	step [8/191], loss=75.0551
	step [9/191], loss=64.0728
	step [10/191], loss=64.3895
	step [11/191], loss=80.4577
	step [12/191], loss=77.8944
	step [13/191], loss=70.8997
	step [14/191], loss=78.6469
	step [15/191], loss=76.4467
	step [16/191], loss=81.0495
	step [17/191], loss=66.9521
	step [18/191], loss=68.9730
	step [19/191], loss=78.2206
	step [20/191], loss=66.3992
	step [21/191], loss=61.6846
	step [22/191], loss=56.6359
	step [23/191], loss=69.5814
	step [24/191], loss=70.6052
	step [25/191], loss=68.0840
	step [26/191], loss=82.3870
	step [27/191], loss=79.9377
	step [28/191], loss=56.7125
	step [29/191], loss=72.9888
	step [30/191], loss=63.1843
	step [31/191], loss=54.2819
	step [32/191], loss=67.2916
	step [33/191], loss=60.2349
	step [34/191], loss=72.2173
	step [35/191], loss=73.1905
	step [36/191], loss=64.4944
	step [37/191], loss=72.4313
	step [38/191], loss=72.2880
	step [39/191], loss=82.0327
	step [40/191], loss=79.6624
	step [41/191], loss=82.4808
	step [42/191], loss=72.8098
	step [43/191], loss=81.5993
	step [44/191], loss=73.3663
	step [45/191], loss=71.8577
	step [46/191], loss=65.9017
	step [47/191], loss=82.8164
	step [48/191], loss=85.3997
	step [49/191], loss=72.9642
	step [50/191], loss=72.0102
	step [51/191], loss=77.5232
	step [52/191], loss=67.1567
	step [53/191], loss=81.8349
	step [54/191], loss=89.0479
	step [55/191], loss=76.3801
	step [56/191], loss=59.5494
	step [57/191], loss=66.0380
	step [58/191], loss=57.5604
	step [59/191], loss=75.7211
	step [60/191], loss=63.7507
	step [61/191], loss=68.6407
	step [62/191], loss=72.4533
	step [63/191], loss=75.3514
	step [64/191], loss=64.2125
	step [65/191], loss=64.4874
	step [66/191], loss=86.1004
	step [67/191], loss=57.0549
	step [68/191], loss=74.7983
	step [69/191], loss=78.2928
	step [70/191], loss=62.0373
	step [71/191], loss=77.1966
	step [72/191], loss=67.5219
	step [73/191], loss=63.6140
	step [74/191], loss=80.6604
	step [75/191], loss=65.4994
	step [76/191], loss=66.2100
	step [77/191], loss=66.7583
	step [78/191], loss=67.8292
	step [79/191], loss=69.5733
	step [80/191], loss=78.3203
	step [81/191], loss=56.8154
	step [82/191], loss=63.6688
	step [83/191], loss=79.0685
	step [84/191], loss=72.9541
	step [85/191], loss=82.2482
	step [86/191], loss=73.4474
	step [87/191], loss=88.5991
	step [88/191], loss=55.5242
	step [89/191], loss=80.9975
	step [90/191], loss=78.2363
	step [91/191], loss=54.7089
	step [92/191], loss=68.3026
	step [93/191], loss=60.7543
	step [94/191], loss=68.9686
	step [95/191], loss=61.2333
	step [96/191], loss=76.8488
	step [97/191], loss=76.9075
	step [98/191], loss=70.2983
	step [99/191], loss=76.8772
	step [100/191], loss=72.8718
	step [101/191], loss=81.2426
	step [102/191], loss=69.2236
	step [103/191], loss=75.5831
	step [104/191], loss=78.3873
	step [105/191], loss=78.4526
	step [106/191], loss=82.9877
	step [107/191], loss=69.7461
	step [108/191], loss=72.8712
	step [109/191], loss=85.1358
	step [110/191], loss=71.6894
	step [111/191], loss=63.0954
	step [112/191], loss=60.1523
	step [113/191], loss=72.7496
	step [114/191], loss=71.9067
	step [115/191], loss=71.5556
	step [116/191], loss=68.8956
	step [117/191], loss=65.9790
	step [118/191], loss=72.5274
	step [119/191], loss=67.3969
	step [120/191], loss=68.9261
	step [121/191], loss=63.4820
	step [122/191], loss=87.7332
	step [123/191], loss=74.0284
	step [124/191], loss=74.2323
	step [125/191], loss=62.7058
	step [126/191], loss=68.8233
	step [127/191], loss=70.5984
	step [128/191], loss=70.4505
	step [129/191], loss=74.7830
	step [130/191], loss=63.7315
	step [131/191], loss=64.4058
	step [132/191], loss=63.4691
	step [133/191], loss=80.0714
	step [134/191], loss=65.7087
	step [135/191], loss=52.8637
	step [136/191], loss=78.2944
	step [137/191], loss=71.2354
	step [138/191], loss=71.6246
	step [139/191], loss=67.1189
	step [140/191], loss=79.4898
	step [141/191], loss=61.6460
	step [142/191], loss=83.2363
	step [143/191], loss=65.1180
	step [144/191], loss=66.7210
	step [145/191], loss=83.4318
	step [146/191], loss=60.7881
	step [147/191], loss=58.9304
	step [148/191], loss=58.6334
	step [149/191], loss=73.7734
	step [150/191], loss=52.7611
	step [151/191], loss=72.0164
	step [152/191], loss=77.6185
	step [153/191], loss=63.7464
	step [154/191], loss=65.1085
	step [155/191], loss=82.3811
	step [156/191], loss=73.4118
	step [157/191], loss=67.1547
	step [158/191], loss=61.6211
	step [159/191], loss=81.4380
	step [160/191], loss=75.0728
	step [161/191], loss=77.6927
	step [162/191], loss=82.5308
	step [163/191], loss=76.9382
	step [164/191], loss=56.3116
	step [165/191], loss=55.9034
	step [166/191], loss=48.4479
	step [167/191], loss=68.3393
	step [168/191], loss=68.8870
	step [169/191], loss=85.8579
	step [170/191], loss=73.8063
	step [171/191], loss=63.2067
	step [172/191], loss=83.2237
	step [173/191], loss=57.4879
	step [174/191], loss=88.9762
	step [175/191], loss=74.0897
	step [176/191], loss=59.1557
	step [177/191], loss=97.7106
	step [178/191], loss=64.8248
	step [179/191], loss=67.2116
	step [180/191], loss=59.6649
	step [181/191], loss=88.1789
	step [182/191], loss=65.5156
	step [183/191], loss=61.3804
	step [184/191], loss=62.5810
	step [185/191], loss=60.9104
	step [186/191], loss=69.5871
	step [187/191], loss=82.6200
	step [188/191], loss=77.6070
	step [189/191], loss=65.3027
	step [190/191], loss=69.9446
	step [191/191], loss=33.6228
	Evaluating
	loss=0.0114, precision=0.5056, recall=0.8932, f1=0.6457
Training epoch 23
	step [1/191], loss=62.2686
	step [2/191], loss=54.8175
	step [3/191], loss=76.2769
	step [4/191], loss=83.3363
	step [5/191], loss=76.9960
	step [6/191], loss=65.4717
	step [7/191], loss=65.5495
	step [8/191], loss=68.8387
	step [9/191], loss=83.4089
	step [10/191], loss=86.8472
	step [11/191], loss=75.9623
	step [12/191], loss=72.4282
	step [13/191], loss=56.8191
	step [14/191], loss=71.4691
	step [15/191], loss=85.3163
	step [16/191], loss=64.9281
	step [17/191], loss=82.9712
	step [18/191], loss=63.4374
	step [19/191], loss=92.6727
	step [20/191], loss=55.0323
	step [21/191], loss=59.2891
	step [22/191], loss=70.1224
	step [23/191], loss=76.6605
	step [24/191], loss=72.9749
	step [25/191], loss=66.0302
	step [26/191], loss=68.4315
	step [27/191], loss=66.0270
	step [28/191], loss=60.9955
	step [29/191], loss=68.8753
	step [30/191], loss=67.5016
	step [31/191], loss=76.2783
	step [32/191], loss=72.5455
	step [33/191], loss=69.3929
	step [34/191], loss=61.9070
	step [35/191], loss=66.0102
	step [36/191], loss=77.3111
	step [37/191], loss=65.8828
	step [38/191], loss=62.1308
	step [39/191], loss=59.8127
	step [40/191], loss=69.6873
	step [41/191], loss=70.5413
	step [42/191], loss=69.2796
	step [43/191], loss=61.5211
	step [44/191], loss=68.1568
	step [45/191], loss=67.3965
	step [46/191], loss=67.8997
	step [47/191], loss=65.9366
	step [48/191], loss=77.8243
	step [49/191], loss=70.5151
	step [50/191], loss=60.8823
	step [51/191], loss=52.9847
	step [52/191], loss=85.0205
	step [53/191], loss=69.0614
	step [54/191], loss=73.7062
	step [55/191], loss=67.0438
	step [56/191], loss=62.7183
	step [57/191], loss=80.4721
	step [58/191], loss=60.2690
	step [59/191], loss=79.0251
	step [60/191], loss=70.0732
	step [61/191], loss=76.8658
	step [62/191], loss=83.3296
	step [63/191], loss=64.3174
	step [64/191], loss=67.5199
	step [65/191], loss=72.2688
	step [66/191], loss=74.8865
	step [67/191], loss=82.7349
	step [68/191], loss=81.8405
	step [69/191], loss=74.4528
	step [70/191], loss=55.5941
	step [71/191], loss=79.8216
	step [72/191], loss=60.1442
	step [73/191], loss=71.3898
	step [74/191], loss=67.1087
	step [75/191], loss=58.8094
	step [76/191], loss=80.7643
	step [77/191], loss=68.4139
	step [78/191], loss=66.4945
	step [79/191], loss=68.3294
	step [80/191], loss=59.0220
	step [81/191], loss=77.0252
	step [82/191], loss=64.6171
	step [83/191], loss=75.3665
	step [84/191], loss=74.7754
	step [85/191], loss=61.4222
	step [86/191], loss=72.5081
	step [87/191], loss=61.5787
	step [88/191], loss=77.0609
	step [89/191], loss=73.0015
	step [90/191], loss=58.9288
	step [91/191], loss=71.9457
	step [92/191], loss=70.7604
	step [93/191], loss=55.1816
	step [94/191], loss=67.9724
	step [95/191], loss=64.2888
	step [96/191], loss=66.5622
	step [97/191], loss=62.4065
	step [98/191], loss=85.8399
	step [99/191], loss=63.2611
	step [100/191], loss=75.0164
	step [101/191], loss=92.2652
	step [102/191], loss=68.4337
	step [103/191], loss=67.8549
	step [104/191], loss=76.3704
	step [105/191], loss=67.9763
	step [106/191], loss=72.9374
	step [107/191], loss=60.1384
	step [108/191], loss=64.6946
	step [109/191], loss=81.0603
	step [110/191], loss=72.9579
	step [111/191], loss=74.5160
	step [112/191], loss=80.4906
	step [113/191], loss=86.2029
	step [114/191], loss=84.7171
	step [115/191], loss=66.9956
	step [116/191], loss=66.4420
	step [117/191], loss=71.8342
	step [118/191], loss=71.0515
	step [119/191], loss=62.0525
	step [120/191], loss=59.2320
	step [121/191], loss=61.3007
	step [122/191], loss=65.3648
	step [123/191], loss=63.1852
	step [124/191], loss=75.6121
	step [125/191], loss=76.9556
	step [126/191], loss=84.3637
	step [127/191], loss=73.9363
	step [128/191], loss=60.0714
	step [129/191], loss=72.2063
	step [130/191], loss=65.5411
	step [131/191], loss=67.8451
	step [132/191], loss=60.4120
	step [133/191], loss=54.0039
	step [134/191], loss=82.5208
	step [135/191], loss=57.4112
	step [136/191], loss=61.5139
	step [137/191], loss=63.8479
	step [138/191], loss=74.7752
	step [139/191], loss=71.5427
	step [140/191], loss=72.8144
	step [141/191], loss=73.8359
	step [142/191], loss=80.2226
	step [143/191], loss=59.8311
	step [144/191], loss=63.6222
	step [145/191], loss=62.4841
	step [146/191], loss=67.2051
	step [147/191], loss=73.3800
	step [148/191], loss=67.4921
	step [149/191], loss=74.3435
	step [150/191], loss=68.4788
	step [151/191], loss=82.5198
	step [152/191], loss=59.7818
	step [153/191], loss=84.3049
	step [154/191], loss=55.4012
	step [155/191], loss=71.0925
	step [156/191], loss=56.3870
	step [157/191], loss=70.7344
	step [158/191], loss=87.0765
	step [159/191], loss=70.5716
	step [160/191], loss=72.1824
	step [161/191], loss=73.7685
	step [162/191], loss=84.0418
	step [163/191], loss=81.8259
	step [164/191], loss=64.3903
	step [165/191], loss=84.1411
	step [166/191], loss=65.0356
	step [167/191], loss=88.2311
	step [168/191], loss=83.5732
	step [169/191], loss=68.2544
	step [170/191], loss=61.3959
	step [171/191], loss=58.3591
	step [172/191], loss=87.8455
	step [173/191], loss=83.8880
	step [174/191], loss=61.8825
	step [175/191], loss=60.8329
	step [176/191], loss=67.0464
	step [177/191], loss=78.4144
	step [178/191], loss=74.2198
	step [179/191], loss=61.3737
	step [180/191], loss=72.7290
	step [181/191], loss=96.9279
	step [182/191], loss=65.7573
	step [183/191], loss=77.3168
	step [184/191], loss=76.1339
	step [185/191], loss=73.6709
	step [186/191], loss=64.4969
	step [187/191], loss=73.5618
	step [188/191], loss=66.9446
	step [189/191], loss=59.2557
	step [190/191], loss=67.1345
	step [191/191], loss=36.8286
	Evaluating
	loss=0.0112, precision=0.4825, recall=0.8633, f1=0.6190
Training epoch 24
	step [1/191], loss=67.0655
	step [2/191], loss=75.5754
	step [3/191], loss=65.2035
	step [4/191], loss=68.0253
	step [5/191], loss=69.4859
	step [6/191], loss=67.3224
	step [7/191], loss=70.6786
	step [8/191], loss=78.2188
	step [9/191], loss=70.2037
	step [10/191], loss=67.1168
	step [11/191], loss=66.6123
	step [12/191], loss=71.1653
	step [13/191], loss=67.4298
	step [14/191], loss=77.3777
	step [15/191], loss=63.0159
	step [16/191], loss=76.5829
	step [17/191], loss=83.7569
	step [18/191], loss=64.0541
	step [19/191], loss=72.1678
	step [20/191], loss=62.1711
	step [21/191], loss=74.6147
	step [22/191], loss=58.8880
	step [23/191], loss=59.8832
	step [24/191], loss=73.8740
	step [25/191], loss=72.5443
	step [26/191], loss=63.3302
	step [27/191], loss=72.7680
	step [28/191], loss=88.5546
	step [29/191], loss=92.0915
	step [30/191], loss=67.6280
	step [31/191], loss=69.3223
	step [32/191], loss=68.1706
	step [33/191], loss=72.1763
	step [34/191], loss=76.3153
	step [35/191], loss=86.3259
	step [36/191], loss=67.3519
	step [37/191], loss=79.9258
	step [38/191], loss=62.9428
	step [39/191], loss=56.7616
	step [40/191], loss=52.5887
	step [41/191], loss=64.9943
	step [42/191], loss=62.9371
	step [43/191], loss=76.3621
	step [44/191], loss=62.4111
	step [45/191], loss=59.2451
	step [46/191], loss=68.5984
	step [47/191], loss=69.1925
	step [48/191], loss=55.0840
	step [49/191], loss=80.5402
	step [50/191], loss=66.7888
	step [51/191], loss=60.7938
	step [52/191], loss=71.3690
	step [53/191], loss=82.7688
	step [54/191], loss=68.4002
	step [55/191], loss=71.2286
	step [56/191], loss=70.1034
	step [57/191], loss=70.6777
	step [58/191], loss=77.9594
	step [59/191], loss=64.4204
	step [60/191], loss=66.9112
	step [61/191], loss=77.0467
	step [62/191], loss=73.5532
	step [63/191], loss=72.4444
	step [64/191], loss=79.4684
	step [65/191], loss=64.2564
	step [66/191], loss=65.0476
	step [67/191], loss=65.9651
	step [68/191], loss=72.9712
	step [69/191], loss=61.9266
	step [70/191], loss=80.1162
	step [71/191], loss=71.4869
	step [72/191], loss=78.4998
	step [73/191], loss=69.7570
	step [74/191], loss=70.9247
	step [75/191], loss=63.5577
	step [76/191], loss=76.8864
	step [77/191], loss=61.7994
	step [78/191], loss=59.1715
	step [79/191], loss=74.3679
	step [80/191], loss=84.0929
	step [81/191], loss=59.8151
	step [82/191], loss=78.6726
	step [83/191], loss=75.1256
	step [84/191], loss=85.0942
	step [85/191], loss=76.2216
	step [86/191], loss=71.5710
	step [87/191], loss=79.5622
	step [88/191], loss=77.9296
	step [89/191], loss=81.3893
	step [90/191], loss=68.8576
	step [91/191], loss=73.4007
	step [92/191], loss=71.2847
	step [93/191], loss=69.2072
	step [94/191], loss=66.0813
	step [95/191], loss=73.3655
	step [96/191], loss=66.8648
	step [97/191], loss=78.4724
	step [98/191], loss=66.4435
	step [99/191], loss=70.3240
	step [100/191], loss=75.2919
	step [101/191], loss=65.4677
	step [102/191], loss=60.4631
	step [103/191], loss=75.0227
	step [104/191], loss=67.1907
	step [105/191], loss=66.2757
	step [106/191], loss=75.1843
	step [107/191], loss=69.1026
	step [108/191], loss=68.3914
	step [109/191], loss=74.4239
	step [110/191], loss=66.1109
	step [111/191], loss=68.8082
	step [112/191], loss=80.5118
	step [113/191], loss=75.1982
	step [114/191], loss=58.3192
	step [115/191], loss=72.3107
	step [116/191], loss=60.6131
	step [117/191], loss=61.5579
	step [118/191], loss=74.8266
	step [119/191], loss=74.5680
	step [120/191], loss=63.3100
	step [121/191], loss=84.3731
	step [122/191], loss=71.3622
	step [123/191], loss=78.0797
	step [124/191], loss=80.1246
	step [125/191], loss=74.5322
	step [126/191], loss=71.1502
	step [127/191], loss=61.8691
	step [128/191], loss=74.4972
	step [129/191], loss=76.1099
	step [130/191], loss=68.8557
	step [131/191], loss=71.1156
	step [132/191], loss=72.0368
	step [133/191], loss=70.1169
	step [134/191], loss=62.6368
	step [135/191], loss=58.1794
	step [136/191], loss=63.6459
	step [137/191], loss=69.1898
	step [138/191], loss=66.8481
	step [139/191], loss=73.5298
	step [140/191], loss=67.6324
	step [141/191], loss=66.2990
	step [142/191], loss=76.2938
	step [143/191], loss=61.1422
	step [144/191], loss=58.5834
	step [145/191], loss=60.1057
	step [146/191], loss=60.7048
	step [147/191], loss=77.8255
	step [148/191], loss=60.8350
	step [149/191], loss=68.8827
	step [150/191], loss=70.9590
	step [151/191], loss=73.4039
	step [152/191], loss=58.9764
	step [153/191], loss=67.2812
	step [154/191], loss=63.0945
	step [155/191], loss=68.9741
	step [156/191], loss=57.0274
	step [157/191], loss=66.6164
	step [158/191], loss=73.8092
	step [159/191], loss=62.8110
	step [160/191], loss=51.1515
	step [161/191], loss=59.8841
	step [162/191], loss=60.2440
	step [163/191], loss=75.7392
	step [164/191], loss=67.4724
	step [165/191], loss=62.0911
	step [166/191], loss=66.7200
	step [167/191], loss=63.6670
	step [168/191], loss=61.3935
	step [169/191], loss=67.5134
	step [170/191], loss=80.0785
	step [171/191], loss=75.2018
	step [172/191], loss=70.0510
	step [173/191], loss=72.3895
	step [174/191], loss=63.9729
	step [175/191], loss=66.1943
	step [176/191], loss=72.8508
	step [177/191], loss=53.3623
	step [178/191], loss=75.5707
	step [179/191], loss=60.0779
	step [180/191], loss=61.5283
	step [181/191], loss=76.2101
	step [182/191], loss=67.3458
	step [183/191], loss=76.6296
	step [184/191], loss=65.1755
	step [185/191], loss=67.5232
	step [186/191], loss=69.2046
	step [187/191], loss=71.4376
	step [188/191], loss=63.5560
	step [189/191], loss=74.2366
	step [190/191], loss=62.2928
	step [191/191], loss=26.6718
	Evaluating
	loss=0.0103, precision=0.4986, recall=0.8948, f1=0.6404
Training epoch 25
	step [1/191], loss=71.2607
	step [2/191], loss=85.2354
	step [3/191], loss=69.6916
	step [4/191], loss=65.9836
	step [5/191], loss=62.8186
	step [6/191], loss=50.7546
	step [7/191], loss=70.5411
	step [8/191], loss=74.8903
	step [9/191], loss=66.8613
	step [10/191], loss=54.9402
	step [11/191], loss=53.7980
	step [12/191], loss=61.7632
	step [13/191], loss=69.9272
	step [14/191], loss=62.7660
	step [15/191], loss=65.0125
	step [16/191], loss=78.0549
	step [17/191], loss=71.6590
	step [18/191], loss=71.8291
	step [19/191], loss=75.1882
	step [20/191], loss=80.6735
	step [21/191], loss=63.6877
	step [22/191], loss=64.2423
	step [23/191], loss=56.3557
	step [24/191], loss=68.3797
	step [25/191], loss=76.5073
	step [26/191], loss=74.4963
	step [27/191], loss=65.7958
	step [28/191], loss=63.7290
	step [29/191], loss=67.4452
	step [30/191], loss=60.2915
	step [31/191], loss=69.0852
	step [32/191], loss=71.5789
	step [33/191], loss=78.4882
	step [34/191], loss=64.1956
	step [35/191], loss=62.4252
	step [36/191], loss=59.5463
	step [37/191], loss=86.3999
	step [38/191], loss=61.4610
	step [39/191], loss=71.2873
	step [40/191], loss=68.8678
	step [41/191], loss=54.7658
	step [42/191], loss=68.8693
	step [43/191], loss=72.9561
	step [44/191], loss=72.2543
	step [45/191], loss=62.5309
	step [46/191], loss=72.4872
	step [47/191], loss=70.7326
	step [48/191], loss=65.8730
	step [49/191], loss=56.3092
	step [50/191], loss=68.7121
	step [51/191], loss=90.7757
	step [52/191], loss=66.5113
	step [53/191], loss=73.9023
	step [54/191], loss=70.7973
	step [55/191], loss=78.2054
	step [56/191], loss=74.5035
	step [57/191], loss=77.7373
	step [58/191], loss=77.7329
	step [59/191], loss=67.9444
	step [60/191], loss=66.3917
	step [61/191], loss=68.0526
	step [62/191], loss=62.5714
	step [63/191], loss=67.8929
	step [64/191], loss=90.3080
	step [65/191], loss=60.7443
	step [66/191], loss=61.5545
	step [67/191], loss=54.4751
	step [68/191], loss=78.1437
	step [69/191], loss=59.8703
	step [70/191], loss=79.4097
	step [71/191], loss=55.1608
	step [72/191], loss=65.1156
	step [73/191], loss=68.1981
	step [74/191], loss=77.1007
	step [75/191], loss=65.4390
	step [76/191], loss=65.5068
	step [77/191], loss=72.6400
	step [78/191], loss=68.3639
	step [79/191], loss=72.1639
	step [80/191], loss=72.5269
	step [81/191], loss=60.2693
	step [82/191], loss=82.5110
	step [83/191], loss=70.0610
	step [84/191], loss=75.4395
	step [85/191], loss=63.8315
	step [86/191], loss=68.5445
	step [87/191], loss=73.1132
	step [88/191], loss=76.1982
	step [89/191], loss=76.3953
	step [90/191], loss=64.0934
	step [91/191], loss=60.6627
	step [92/191], loss=63.9886
	step [93/191], loss=76.7258
	step [94/191], loss=60.3938
	step [95/191], loss=80.5151
	step [96/191], loss=75.3859
	step [97/191], loss=75.6334
	step [98/191], loss=73.7777
	step [99/191], loss=61.9066
	step [100/191], loss=72.5895
	step [101/191], loss=63.3393
	step [102/191], loss=57.5759
	step [103/191], loss=75.3596
	step [104/191], loss=56.0691
	step [105/191], loss=68.6972
	step [106/191], loss=68.0218
	step [107/191], loss=74.5651
	step [108/191], loss=67.3636
	step [109/191], loss=61.0853
	step [110/191], loss=52.9664
	step [111/191], loss=84.2603
	step [112/191], loss=80.3823
	step [113/191], loss=86.8706
	step [114/191], loss=75.0572
	step [115/191], loss=72.5230
	step [116/191], loss=66.0798
	step [117/191], loss=67.8131
	step [118/191], loss=60.1691
	step [119/191], loss=81.4131
	step [120/191], loss=62.1353
	step [121/191], loss=64.2278
	step [122/191], loss=64.3996
	step [123/191], loss=64.3737
	step [124/191], loss=67.4108
	step [125/191], loss=85.5238
	step [126/191], loss=65.7980
	step [127/191], loss=68.1227
	step [128/191], loss=64.1282
	step [129/191], loss=66.6476
	step [130/191], loss=54.4922
	step [131/191], loss=63.7875
	step [132/191], loss=75.4919
	step [133/191], loss=74.0529
	step [134/191], loss=77.9821
	step [135/191], loss=64.6020
	step [136/191], loss=73.7095
	step [137/191], loss=81.2297
	step [138/191], loss=76.2850
	step [139/191], loss=59.7310
	step [140/191], loss=61.8807
	step [141/191], loss=72.9978
	step [142/191], loss=58.0156
	step [143/191], loss=74.8215
	step [144/191], loss=71.5329
	step [145/191], loss=76.5726
	step [146/191], loss=57.0063
	step [147/191], loss=80.0294
	step [148/191], loss=78.3870
	step [149/191], loss=91.1754
	step [150/191], loss=63.7506
	step [151/191], loss=59.4166
	step [152/191], loss=67.2496
	step [153/191], loss=67.8116
	step [154/191], loss=62.1418
	step [155/191], loss=47.8880
	step [156/191], loss=76.3614
	step [157/191], loss=59.7987
	step [158/191], loss=68.5309
	step [159/191], loss=69.1019
	step [160/191], loss=76.8531
	step [161/191], loss=75.3557
	step [162/191], loss=61.7438
	step [163/191], loss=77.5934
	step [164/191], loss=75.7784
	step [165/191], loss=61.7329
	step [166/191], loss=66.2687
	step [167/191], loss=78.5223
	step [168/191], loss=72.1961
	step [169/191], loss=50.0069
	step [170/191], loss=59.3203
	step [171/191], loss=69.2595
	step [172/191], loss=77.5531
	step [173/191], loss=78.1151
	step [174/191], loss=73.0197
	step [175/191], loss=63.4913
	step [176/191], loss=68.4811
	step [177/191], loss=70.6693
	step [178/191], loss=69.0073
	step [179/191], loss=59.9255
	step [180/191], loss=58.6028
	step [181/191], loss=76.0625
	step [182/191], loss=62.4994
	step [183/191], loss=61.4693
	step [184/191], loss=64.0537
	step [185/191], loss=58.2768
	step [186/191], loss=53.7044
	step [187/191], loss=59.2091
	step [188/191], loss=69.3149
	step [189/191], loss=71.7426
	step [190/191], loss=64.9263
	step [191/191], loss=24.5286
	Evaluating
	loss=0.0110, precision=0.4756, recall=0.8967, f1=0.6216
Training epoch 26
	step [1/191], loss=61.5043
	step [2/191], loss=61.4034
	step [3/191], loss=72.6992
	step [4/191], loss=58.3023
	step [5/191], loss=69.6930
	step [6/191], loss=54.5656
	step [7/191], loss=64.2089
	step [8/191], loss=60.3229
	step [9/191], loss=79.4651
	step [10/191], loss=81.7643
	step [11/191], loss=69.2231
	step [12/191], loss=82.1116
	step [13/191], loss=61.1207
	step [14/191], loss=59.7160
	step [15/191], loss=52.5971
	step [16/191], loss=90.4091
	step [17/191], loss=76.7504
	step [18/191], loss=65.4689
	step [19/191], loss=72.9670
	step [20/191], loss=60.7743
	step [21/191], loss=69.8221
	step [22/191], loss=54.5155
	step [23/191], loss=63.1171
	step [24/191], loss=76.9768
	step [25/191], loss=66.7758
	step [26/191], loss=67.2205
	step [27/191], loss=60.4001
	step [28/191], loss=71.3191
	step [29/191], loss=87.4434
	step [30/191], loss=81.8110
	step [31/191], loss=75.4053
	step [32/191], loss=79.7137
	step [33/191], loss=71.7034
	step [34/191], loss=70.2081
	step [35/191], loss=80.0718
	step [36/191], loss=72.1855
	step [37/191], loss=71.4172
	step [38/191], loss=67.8793
	step [39/191], loss=70.2614
	step [40/191], loss=70.5592
	step [41/191], loss=73.9622
	step [42/191], loss=62.9576
	step [43/191], loss=65.6428
	step [44/191], loss=77.3431
	step [45/191], loss=68.0974
	step [46/191], loss=60.2875
	step [47/191], loss=53.0657
	step [48/191], loss=69.2891
	step [49/191], loss=73.1341
	step [50/191], loss=74.8784
	step [51/191], loss=68.5020
	step [52/191], loss=66.8392
	step [53/191], loss=54.0211
	step [54/191], loss=69.2533
	step [55/191], loss=66.2643
	step [56/191], loss=64.6962
	step [57/191], loss=52.6622
	step [58/191], loss=50.5544
	step [59/191], loss=59.7493
	step [60/191], loss=70.6799
	step [61/191], loss=68.3760
	step [62/191], loss=68.2974
	step [63/191], loss=84.5774
	step [64/191], loss=62.5004
	step [65/191], loss=49.8900
	step [66/191], loss=66.3677
	step [67/191], loss=66.3637
	step [68/191], loss=72.6163
	step [69/191], loss=67.4930
	step [70/191], loss=54.7476
	step [71/191], loss=73.1770
	step [72/191], loss=68.3804
	step [73/191], loss=73.6682
	step [74/191], loss=73.6097
	step [75/191], loss=62.3353
	step [76/191], loss=54.7062
	step [77/191], loss=57.5130
	step [78/191], loss=69.7023
	step [79/191], loss=74.7115
	step [80/191], loss=79.8710
	step [81/191], loss=72.3989
	step [82/191], loss=63.7761
	step [83/191], loss=63.7972
	step [84/191], loss=52.4897
	step [85/191], loss=59.2374
	step [86/191], loss=74.3780
	step [87/191], loss=68.0437
	step [88/191], loss=69.7037
	step [89/191], loss=72.1140
	step [90/191], loss=65.6483
	step [91/191], loss=70.9595
	step [92/191], loss=66.7018
	step [93/191], loss=69.8341
	step [94/191], loss=78.6752
	step [95/191], loss=72.9696
	step [96/191], loss=58.6483
	step [97/191], loss=64.9845
	step [98/191], loss=69.0018
	step [99/191], loss=78.2985
	step [100/191], loss=63.7780
	step [101/191], loss=72.4093
	step [102/191], loss=75.0931
	step [103/191], loss=73.6525
	step [104/191], loss=51.8573
	step [105/191], loss=67.3130
	step [106/191], loss=59.5836
	step [107/191], loss=58.5070
	step [108/191], loss=64.0107
	step [109/191], loss=71.5151
	step [110/191], loss=86.0230
	step [111/191], loss=73.2009
	step [112/191], loss=60.4411
	step [113/191], loss=75.4368
	step [114/191], loss=69.2858
	step [115/191], loss=60.3036
	step [116/191], loss=75.8652
	step [117/191], loss=64.8257
	step [118/191], loss=63.5926
	step [119/191], loss=72.9793
	step [120/191], loss=79.0185
	step [121/191], loss=81.4129
	step [122/191], loss=72.1230
	step [123/191], loss=54.9084
	step [124/191], loss=79.2773
	step [125/191], loss=83.2616
	step [126/191], loss=54.6551
	step [127/191], loss=62.2042
	step [128/191], loss=77.2548
	step [129/191], loss=56.2057
	step [130/191], loss=65.1475
	step [131/191], loss=50.7661
	step [132/191], loss=55.0486
	step [133/191], loss=71.9333
	step [134/191], loss=74.9741
	step [135/191], loss=64.9382
	step [136/191], loss=82.0616
	step [137/191], loss=67.5933
	step [138/191], loss=63.1981
	step [139/191], loss=69.7096
	step [140/191], loss=79.5740
	step [141/191], loss=72.8847
	step [142/191], loss=71.3539
	step [143/191], loss=73.4350
	step [144/191], loss=71.5738
	step [145/191], loss=74.4191
	step [146/191], loss=69.4673
	step [147/191], loss=60.6332
	step [148/191], loss=78.6094
	step [149/191], loss=74.1008
	step [150/191], loss=70.4496
	step [151/191], loss=65.2831
	step [152/191], loss=59.8453
	step [153/191], loss=59.6400
	step [154/191], loss=73.3875
	step [155/191], loss=68.5736
	step [156/191], loss=66.0378
	step [157/191], loss=72.1687
	step [158/191], loss=63.3846
	step [159/191], loss=68.4345
	step [160/191], loss=70.8047
	step [161/191], loss=63.4766
	step [162/191], loss=65.7427
	step [163/191], loss=81.9548
	step [164/191], loss=73.9109
	step [165/191], loss=67.3250
	step [166/191], loss=49.5328
	step [167/191], loss=65.5634
	step [168/191], loss=56.9500
	step [169/191], loss=61.2163
	step [170/191], loss=66.0754
	step [171/191], loss=61.7327
	step [172/191], loss=67.3465
	step [173/191], loss=62.6890
	step [174/191], loss=71.2206
	step [175/191], loss=56.6329
	step [176/191], loss=64.6194
	step [177/191], loss=59.0563
	step [178/191], loss=66.7414
	step [179/191], loss=75.9646
	step [180/191], loss=58.9303
	step [181/191], loss=72.3237
	step [182/191], loss=79.2354
	step [183/191], loss=69.6229
	step [184/191], loss=70.7649
	step [185/191], loss=69.0332
	step [186/191], loss=47.3107
	step [187/191], loss=55.4283
	step [188/191], loss=59.0300
	step [189/191], loss=75.5084
	step [190/191], loss=64.0261
	step [191/191], loss=32.5183
	Evaluating
	loss=0.0095, precision=0.5178, recall=0.8816, f1=0.6524
Training epoch 27
	step [1/191], loss=72.0195
	step [2/191], loss=59.7241
	step [3/191], loss=66.2401
	step [4/191], loss=67.3810
	step [5/191], loss=58.1675
	step [6/191], loss=69.2681
	step [7/191], loss=68.4460
	step [8/191], loss=47.6287
	step [9/191], loss=73.5219
	step [10/191], loss=53.5948
	step [11/191], loss=75.6564
	step [12/191], loss=83.1898
	step [13/191], loss=61.0854
	step [14/191], loss=73.4170
	step [15/191], loss=61.5322
	step [16/191], loss=64.1057
	step [17/191], loss=67.4823
	step [18/191], loss=64.1572
	step [19/191], loss=69.5235
	step [20/191], loss=65.8233
	step [21/191], loss=59.3589
	step [22/191], loss=58.6556
	step [23/191], loss=64.7673
	step [24/191], loss=62.4423
	step [25/191], loss=72.3347
	step [26/191], loss=87.0241
	step [27/191], loss=66.1443
	step [28/191], loss=70.5214
	step [29/191], loss=76.5234
	step [30/191], loss=67.0617
	step [31/191], loss=82.9171
	step [32/191], loss=64.8482
	step [33/191], loss=72.9915
	step [34/191], loss=64.7448
	step [35/191], loss=74.4267
	step [36/191], loss=53.5910
	step [37/191], loss=56.0788
	step [38/191], loss=69.4226
	step [39/191], loss=52.8646
	step [40/191], loss=67.5403
	step [41/191], loss=77.1478
	step [42/191], loss=53.1049
	step [43/191], loss=70.6722
	step [44/191], loss=75.8311
	step [45/191], loss=66.2512
	step [46/191], loss=72.2255
	step [47/191], loss=68.9897
	step [48/191], loss=61.9387
	step [49/191], loss=61.7973
	step [50/191], loss=65.5262
	step [51/191], loss=69.9004
	step [52/191], loss=63.1295
	step [53/191], loss=62.3621
	step [54/191], loss=72.7480
	step [55/191], loss=67.1747
	step [56/191], loss=57.9502
	step [57/191], loss=67.8199
	step [58/191], loss=55.7980
	step [59/191], loss=64.4542
	step [60/191], loss=50.1974
	step [61/191], loss=65.5871
	step [62/191], loss=61.8521
	step [63/191], loss=52.8072
	step [64/191], loss=68.1867
	step [65/191], loss=70.4896
	step [66/191], loss=64.6240
	step [67/191], loss=63.1917
	step [68/191], loss=66.4096
	step [69/191], loss=62.5604
	step [70/191], loss=61.5239
	step [71/191], loss=52.5905
	step [72/191], loss=66.7054
	step [73/191], loss=71.4206
	step [74/191], loss=63.3406
	step [75/191], loss=69.0594
	step [76/191], loss=56.8717
	step [77/191], loss=65.0201
	step [78/191], loss=59.2071
	step [79/191], loss=64.0980
	step [80/191], loss=81.6767
	step [81/191], loss=57.8150
	step [82/191], loss=74.0992
	step [83/191], loss=64.5435
	step [84/191], loss=64.8057
	step [85/191], loss=62.3032
	step [86/191], loss=63.3377
	step [87/191], loss=52.5894
	step [88/191], loss=54.1232
	step [89/191], loss=60.7956
	step [90/191], loss=82.5331
	step [91/191], loss=70.4102
	step [92/191], loss=68.6139
	step [93/191], loss=69.7801
	step [94/191], loss=88.1394
	step [95/191], loss=65.1453
	step [96/191], loss=63.0473
	step [97/191], loss=78.7215
	step [98/191], loss=62.3653
	step [99/191], loss=65.7288
	step [100/191], loss=74.4162
	step [101/191], loss=62.9224
	step [102/191], loss=74.6879
	step [103/191], loss=70.3762
	step [104/191], loss=77.9937
	step [105/191], loss=76.1491
	step [106/191], loss=83.7750
	step [107/191], loss=62.7202
	step [108/191], loss=58.5046
	step [109/191], loss=79.6179
	step [110/191], loss=68.1899
	step [111/191], loss=66.9989
	step [112/191], loss=75.0502
	step [113/191], loss=54.1362
	step [114/191], loss=92.8748
	step [115/191], loss=69.0338
	step [116/191], loss=48.6115
	step [117/191], loss=67.9312
	step [118/191], loss=75.4770
	step [119/191], loss=78.0120
	step [120/191], loss=59.4701
	step [121/191], loss=60.1141
	step [122/191], loss=69.4716
	step [123/191], loss=73.4926
	step [124/191], loss=64.6939
	step [125/191], loss=67.7754
	step [126/191], loss=52.0112
	step [127/191], loss=75.1543
	step [128/191], loss=58.9086
	step [129/191], loss=87.0626
	step [130/191], loss=65.0661
	step [131/191], loss=58.4809
	step [132/191], loss=76.2126
	step [133/191], loss=68.9277
	step [134/191], loss=73.1795
	step [135/191], loss=74.9025
	step [136/191], loss=72.1327
	step [137/191], loss=70.2767
	step [138/191], loss=64.7103
	step [139/191], loss=65.2090
	step [140/191], loss=65.6745
	step [141/191], loss=72.9867
	step [142/191], loss=68.6440
	step [143/191], loss=61.4343
	step [144/191], loss=71.1111
	step [145/191], loss=75.8069
	step [146/191], loss=62.6794
	step [147/191], loss=62.4747
	step [148/191], loss=76.1593
	step [149/191], loss=73.7417
	step [150/191], loss=66.3650
	step [151/191], loss=65.5754
	step [152/191], loss=68.3363
	step [153/191], loss=59.4251
	step [154/191], loss=61.9891
	step [155/191], loss=57.2595
	step [156/191], loss=62.2629
	step [157/191], loss=61.7451
	step [158/191], loss=63.9857
	step [159/191], loss=73.4391
	step [160/191], loss=51.2489
	step [161/191], loss=75.6570
	step [162/191], loss=81.3446
	step [163/191], loss=72.1420
	step [164/191], loss=76.5820
	step [165/191], loss=68.7677
	step [166/191], loss=71.3084
	step [167/191], loss=72.9675
	step [168/191], loss=69.6228
	step [169/191], loss=67.6712
	step [170/191], loss=71.5184
	step [171/191], loss=69.7747
	step [172/191], loss=59.2405
	step [173/191], loss=64.6679
	step [174/191], loss=71.0580
	step [175/191], loss=73.6784
	step [176/191], loss=52.7367
	step [177/191], loss=64.8283
	step [178/191], loss=57.0266
	step [179/191], loss=76.5127
	step [180/191], loss=64.0239
	step [181/191], loss=59.5995
	step [182/191], loss=71.8350
	step [183/191], loss=78.1548
	step [184/191], loss=57.9105
	step [185/191], loss=64.3012
	step [186/191], loss=63.3565
	step [187/191], loss=73.3266
	step [188/191], loss=79.0222
	step [189/191], loss=69.3732
	step [190/191], loss=63.3773
	step [191/191], loss=30.3093
	Evaluating
	loss=0.0107, precision=0.4557, recall=0.9060, f1=0.6064
Training epoch 28
	step [1/191], loss=82.8711
	step [2/191], loss=65.3903
	step [3/191], loss=72.7572
	step [4/191], loss=63.9307
	step [5/191], loss=75.0018
	step [6/191], loss=81.6912
	step [7/191], loss=62.6606
	step [8/191], loss=68.7950
	step [9/191], loss=63.5370
	step [10/191], loss=66.7314
	step [11/191], loss=56.6114
	step [12/191], loss=68.1050
	step [13/191], loss=58.6651
	step [14/191], loss=72.1162
	step [15/191], loss=67.8856
	step [16/191], loss=68.0426
	step [17/191], loss=70.5357
	step [18/191], loss=69.9805
	step [19/191], loss=46.7132
	step [20/191], loss=83.4499
	step [21/191], loss=78.7473
	step [22/191], loss=63.2288
	step [23/191], loss=63.5072
	step [24/191], loss=74.6711
	step [25/191], loss=60.7642
	step [26/191], loss=69.0847
	step [27/191], loss=64.4385
	step [28/191], loss=54.9343
	step [29/191], loss=57.3997
	step [30/191], loss=66.0910
	step [31/191], loss=65.6422
	step [32/191], loss=69.2229
	step [33/191], loss=63.7835
	step [34/191], loss=65.6945
	step [35/191], loss=61.9061
	step [36/191], loss=64.9790
	step [37/191], loss=65.2807
	step [38/191], loss=66.6710
	step [39/191], loss=62.4057
	step [40/191], loss=75.1363
	step [41/191], loss=81.0970
	step [42/191], loss=70.5766
	step [43/191], loss=63.7632
	step [44/191], loss=74.8652
	step [45/191], loss=57.3266
	step [46/191], loss=62.0225
	step [47/191], loss=65.6185
	step [48/191], loss=50.0302
	step [49/191], loss=64.9769
	step [50/191], loss=70.5791
	step [51/191], loss=57.0937
	step [52/191], loss=67.5673
	step [53/191], loss=73.9453
	step [54/191], loss=66.2100
	step [55/191], loss=77.1895
	step [56/191], loss=61.1216
	step [57/191], loss=70.9384
	step [58/191], loss=75.3782
	step [59/191], loss=84.1890
	step [60/191], loss=70.6605
	step [61/191], loss=64.7675
	step [62/191], loss=66.7633
	step [63/191], loss=81.6229
	step [64/191], loss=78.1594
	step [65/191], loss=60.7029
	step [66/191], loss=60.7940
	step [67/191], loss=58.5818
	step [68/191], loss=74.5046
	step [69/191], loss=65.7879
	step [70/191], loss=66.9542
	step [71/191], loss=61.6280
	step [72/191], loss=58.8717
	step [73/191], loss=54.1061
	step [74/191], loss=56.4240
	step [75/191], loss=49.5941
	step [76/191], loss=63.8546
	step [77/191], loss=61.4257
	step [78/191], loss=70.8386
	step [79/191], loss=73.8361
	step [80/191], loss=51.0705
	step [81/191], loss=78.3905
	step [82/191], loss=64.3976
	step [83/191], loss=50.2254
	step [84/191], loss=74.3076
	step [85/191], loss=74.5792
	step [86/191], loss=69.9458
	step [87/191], loss=78.2454
	step [88/191], loss=60.7712
	step [89/191], loss=70.7673
	step [90/191], loss=73.9563
	step [91/191], loss=62.0180
	step [92/191], loss=68.3074
	step [93/191], loss=67.4572
	step [94/191], loss=56.0600
	step [95/191], loss=58.5617
	step [96/191], loss=54.2654
	step [97/191], loss=71.1563
	step [98/191], loss=59.4168
	step [99/191], loss=67.3401
	step [100/191], loss=70.3052
	step [101/191], loss=62.4353
	step [102/191], loss=68.0119
	step [103/191], loss=70.0366
	step [104/191], loss=71.3637
	step [105/191], loss=65.1435
	step [106/191], loss=56.4557
	step [107/191], loss=62.2544
	step [108/191], loss=75.7204
	step [109/191], loss=76.8327
	step [110/191], loss=64.8660
	step [111/191], loss=68.4922
	step [112/191], loss=55.9706
	step [113/191], loss=74.1471
	step [114/191], loss=63.8268
	step [115/191], loss=57.2854
	step [116/191], loss=58.7341
	step [117/191], loss=69.2482
	step [118/191], loss=62.4981
	step [119/191], loss=54.9042
	step [120/191], loss=71.0872
	step [121/191], loss=75.0268
	step [122/191], loss=80.5033
	step [123/191], loss=63.6830
	step [124/191], loss=76.3649
	step [125/191], loss=69.1071
	step [126/191], loss=57.0851
	step [127/191], loss=63.1811
	step [128/191], loss=66.1569
	step [129/191], loss=62.2885
	step [130/191], loss=72.1531
	step [131/191], loss=65.9381
	step [132/191], loss=69.3000
	step [133/191], loss=68.5385
	step [134/191], loss=69.4442
	step [135/191], loss=66.9615
	step [136/191], loss=62.5155
	step [137/191], loss=70.7430
	step [138/191], loss=76.5752
	step [139/191], loss=53.9148
	step [140/191], loss=59.5072
	step [141/191], loss=67.5277
	step [142/191], loss=64.1621
	step [143/191], loss=61.8952
	step [144/191], loss=59.6856
	step [145/191], loss=67.6596
	step [146/191], loss=57.3790
	step [147/191], loss=54.9421
	step [148/191], loss=65.9626
	step [149/191], loss=59.4424
	step [150/191], loss=77.4023
	step [151/191], loss=56.3374
	step [152/191], loss=53.2047
	step [153/191], loss=67.4283
	step [154/191], loss=63.6274
	step [155/191], loss=70.5969
	step [156/191], loss=60.4290
	step [157/191], loss=83.1038
	step [158/191], loss=53.2982
	step [159/191], loss=73.7649
	step [160/191], loss=63.0022
	step [161/191], loss=73.6574
	step [162/191], loss=72.6721
	step [163/191], loss=75.5562
	step [164/191], loss=69.4396
	step [165/191], loss=79.6358
	step [166/191], loss=67.2207
	step [167/191], loss=54.3335
	step [168/191], loss=61.0235
	step [169/191], loss=68.9027
	step [170/191], loss=69.6943
	step [171/191], loss=60.6794
	step [172/191], loss=61.9691
	step [173/191], loss=54.7556
	step [174/191], loss=83.7586
	step [175/191], loss=76.2996
	step [176/191], loss=64.0418
	step [177/191], loss=72.3218
	step [178/191], loss=46.7670
	step [179/191], loss=61.9695
	step [180/191], loss=76.0512
	step [181/191], loss=64.8735
	step [182/191], loss=75.9528
	step [183/191], loss=55.1981
	step [184/191], loss=65.0244
	step [185/191], loss=50.6092
	step [186/191], loss=60.4371
	step [187/191], loss=69.0279
	step [188/191], loss=61.2011
	step [189/191], loss=65.1887
	step [190/191], loss=62.7465
	step [191/191], loss=31.0345
	Evaluating
	loss=0.0107, precision=0.4611, recall=0.8673, f1=0.6021
Training epoch 29
	step [1/191], loss=59.9663
	step [2/191], loss=58.7851
	step [3/191], loss=76.1579
	step [4/191], loss=79.7617
	step [5/191], loss=63.8134
	step [6/191], loss=74.1234
	step [7/191], loss=62.9518
	step [8/191], loss=62.6608
	step [9/191], loss=52.7725
	step [10/191], loss=73.9353
	step [11/191], loss=59.8422
	step [12/191], loss=66.8684
	step [13/191], loss=64.9400
	step [14/191], loss=70.4586
	step [15/191], loss=67.1162
	step [16/191], loss=60.2304
	step [17/191], loss=72.2308
	step [18/191], loss=67.9429
	step [19/191], loss=84.3922
	step [20/191], loss=69.5705
	step [21/191], loss=73.2837
	step [22/191], loss=71.1898
	step [23/191], loss=62.2625
	step [24/191], loss=61.8110
	step [25/191], loss=65.8386
	step [26/191], loss=53.8930
	step [27/191], loss=73.3213
	step [28/191], loss=59.1875
	step [29/191], loss=67.3370
	step [30/191], loss=76.9281
	step [31/191], loss=55.8451
	step [32/191], loss=67.4429
	step [33/191], loss=56.5088
	step [34/191], loss=65.8857
	step [35/191], loss=63.4186
	step [36/191], loss=74.8451
	step [37/191], loss=78.7770
	step [38/191], loss=59.1266
	step [39/191], loss=57.4003
	step [40/191], loss=65.8460
	step [41/191], loss=62.1364
	step [42/191], loss=79.4509
	step [43/191], loss=69.0413
	step [44/191], loss=66.4041
	step [45/191], loss=63.4258
	step [46/191], loss=58.0862
	step [47/191], loss=75.7214
	step [48/191], loss=68.5840
	step [49/191], loss=79.3239
	step [50/191], loss=54.9470
	step [51/191], loss=52.2194
	step [52/191], loss=63.3218
	step [53/191], loss=74.3445
	step [54/191], loss=61.4359
	step [55/191], loss=58.2048
	step [56/191], loss=57.1505
	step [57/191], loss=69.3356
	step [58/191], loss=66.7316
	step [59/191], loss=69.5008
	step [60/191], loss=72.3653
	step [61/191], loss=61.5353
	step [62/191], loss=56.6720
	step [63/191], loss=59.4300
	step [64/191], loss=62.7982
	step [65/191], loss=70.5181
	step [66/191], loss=63.0936
	step [67/191], loss=68.7069
	step [68/191], loss=64.6151
	step [69/191], loss=83.6830
	step [70/191], loss=68.2032
	step [71/191], loss=63.6171
	step [72/191], loss=66.6596
	step [73/191], loss=62.1698
	step [74/191], loss=70.2379
	step [75/191], loss=77.0724
	step [76/191], loss=66.4784
	step [77/191], loss=80.9090
	step [78/191], loss=59.0518
	step [79/191], loss=63.5903
	step [80/191], loss=66.7151
	step [81/191], loss=64.0952
	step [82/191], loss=68.1781
	step [83/191], loss=66.2464
	step [84/191], loss=55.9669
	step [85/191], loss=60.6517
	step [86/191], loss=60.8640
	step [87/191], loss=55.8373
	step [88/191], loss=69.1202
	step [89/191], loss=54.7168
	step [90/191], loss=65.6125
	step [91/191], loss=66.8865
	step [92/191], loss=82.2051
	step [93/191], loss=79.5955
	step [94/191], loss=62.1685
	step [95/191], loss=66.3344
	step [96/191], loss=84.3719
	step [97/191], loss=60.6012
	step [98/191], loss=58.5023
	step [99/191], loss=52.9559
	step [100/191], loss=63.8913
	step [101/191], loss=60.7826
	step [102/191], loss=71.0108
	step [103/191], loss=66.0993
	step [104/191], loss=71.9273
	step [105/191], loss=62.7976
	step [106/191], loss=45.0264
	step [107/191], loss=72.0226
	step [108/191], loss=62.1284
	step [109/191], loss=62.4549
	step [110/191], loss=63.6023
	step [111/191], loss=53.8616
	step [112/191], loss=71.4327
	step [113/191], loss=54.8740
	step [114/191], loss=77.5152
	step [115/191], loss=68.0273
	step [116/191], loss=60.7127
	step [117/191], loss=67.0029
	step [118/191], loss=67.8130
	step [119/191], loss=83.0066
	step [120/191], loss=53.4487
	step [121/191], loss=60.3158
	step [122/191], loss=50.5730
	step [123/191], loss=64.2572
	step [124/191], loss=62.7296
	step [125/191], loss=71.8263
	step [126/191], loss=61.2833
	step [127/191], loss=67.7709
	step [128/191], loss=76.1591
	step [129/191], loss=67.5911
	step [130/191], loss=71.4551
	step [131/191], loss=67.7084
	step [132/191], loss=58.7096
	step [133/191], loss=63.8355
	step [134/191], loss=65.4840
	step [135/191], loss=55.6377
	step [136/191], loss=68.8731
	step [137/191], loss=76.2687
	step [138/191], loss=65.0316
	step [139/191], loss=72.4870
	step [140/191], loss=63.4184
	step [141/191], loss=63.3201
	step [142/191], loss=82.5986
	step [143/191], loss=61.7274
	step [144/191], loss=78.1848
	step [145/191], loss=65.8674
	step [146/191], loss=56.0039
	step [147/191], loss=52.4913
	step [148/191], loss=54.3864
	step [149/191], loss=62.9013
	step [150/191], loss=64.1223
	step [151/191], loss=70.8725
	step [152/191], loss=61.6266
	step [153/191], loss=55.2458
	step [154/191], loss=69.3107
	step [155/191], loss=54.2473
	step [156/191], loss=58.2428
	step [157/191], loss=60.6892
	step [158/191], loss=59.4613
	step [159/191], loss=70.5201
	step [160/191], loss=60.4585
	step [161/191], loss=67.3196
	step [162/191], loss=63.6490
	step [163/191], loss=73.1440
	step [164/191], loss=64.2749
	step [165/191], loss=59.6943
	step [166/191], loss=69.4660
	step [167/191], loss=72.3942
	step [168/191], loss=62.3544
	step [169/191], loss=65.2046
	step [170/191], loss=59.7932
	step [171/191], loss=66.9330
	step [172/191], loss=57.5921
	step [173/191], loss=61.5368
	step [174/191], loss=67.9398
	step [175/191], loss=51.0434
	step [176/191], loss=65.3759
	step [177/191], loss=57.4251
	step [178/191], loss=68.3671
	step [179/191], loss=57.3633
	step [180/191], loss=59.5969
	step [181/191], loss=66.9290
	step [182/191], loss=66.2132
	step [183/191], loss=67.1009
	step [184/191], loss=79.0368
	step [185/191], loss=68.6802
	step [186/191], loss=77.9606
	step [187/191], loss=55.2295
	step [188/191], loss=62.5302
	step [189/191], loss=70.7376
	step [190/191], loss=58.5978
	step [191/191], loss=26.4763
	Evaluating
	loss=0.0103, precision=0.4577, recall=0.8627, f1=0.5981
Training epoch 30
	step [1/191], loss=61.5013
	step [2/191], loss=62.8711
	step [3/191], loss=53.3971
	step [4/191], loss=50.9009
	step [5/191], loss=56.2432
	step [6/191], loss=49.4817
	step [7/191], loss=76.5581
	step [8/191], loss=58.7636
	step [9/191], loss=64.1268
	step [10/191], loss=74.1342
	step [11/191], loss=75.8552
	step [12/191], loss=63.3088
	step [13/191], loss=64.5490
	step [14/191], loss=56.4849
	step [15/191], loss=66.5721
	step [16/191], loss=59.0122
	step [17/191], loss=69.9546
	step [18/191], loss=60.9302
	step [19/191], loss=67.2657
	step [20/191], loss=79.6412
	step [21/191], loss=66.1782
	step [22/191], loss=69.5088
	step [23/191], loss=48.2364
	step [24/191], loss=66.0306
	step [25/191], loss=62.8287
	step [26/191], loss=71.0712
	step [27/191], loss=57.9794
	step [28/191], loss=61.7890
	step [29/191], loss=73.8913
	step [30/191], loss=70.6001
	step [31/191], loss=66.3338
	step [32/191], loss=79.6454
	step [33/191], loss=63.2393
	step [34/191], loss=73.2647
	step [35/191], loss=53.2677
	step [36/191], loss=74.5543
	step [37/191], loss=61.6569
	step [38/191], loss=58.7657
	step [39/191], loss=64.6958
	step [40/191], loss=66.6110
	step [41/191], loss=71.3323
	step [42/191], loss=63.5668
	step [43/191], loss=67.6706
	step [44/191], loss=75.0845
	step [45/191], loss=65.0820
	step [46/191], loss=54.3781
	step [47/191], loss=69.5479
	step [48/191], loss=61.0899
	step [49/191], loss=56.5465
	step [50/191], loss=72.4727
	step [51/191], loss=66.9658
	step [52/191], loss=57.3574
	step [53/191], loss=48.6728
	step [54/191], loss=71.6173
	step [55/191], loss=66.4397
	step [56/191], loss=68.8992
	step [57/191], loss=79.4836
	step [58/191], loss=55.4558
	step [59/191], loss=61.9381
	step [60/191], loss=68.5439
	step [61/191], loss=51.5908
	step [62/191], loss=66.1814
	step [63/191], loss=74.9209
	step [64/191], loss=62.6442
	step [65/191], loss=60.3445
	step [66/191], loss=69.9040
	step [67/191], loss=48.2491
	step [68/191], loss=58.2041
	step [69/191], loss=60.7027
	step [70/191], loss=70.3464
	step [71/191], loss=61.6924
	step [72/191], loss=65.7352
	step [73/191], loss=78.6741
	step [74/191], loss=60.8098
	step [75/191], loss=72.6177
	step [76/191], loss=64.2304
	step [77/191], loss=76.5935
	step [78/191], loss=56.4337
	step [79/191], loss=56.4702
	step [80/191], loss=80.5286
	step [81/191], loss=55.8796
	step [82/191], loss=68.3563
	step [83/191], loss=69.7326
	step [84/191], loss=58.3241
	step [85/191], loss=64.7948
	step [86/191], loss=67.6556
	step [87/191], loss=61.7812
	step [88/191], loss=66.0246
	step [89/191], loss=55.1176
	step [90/191], loss=71.1934
	step [91/191], loss=57.6091
	step [92/191], loss=65.8757
	step [93/191], loss=56.4601
	step [94/191], loss=65.3241
	step [95/191], loss=65.2785
	step [96/191], loss=59.8367
	step [97/191], loss=73.1118
	step [98/191], loss=67.3814
	step [99/191], loss=59.5562
	step [100/191], loss=61.5942
	step [101/191], loss=62.6634
	step [102/191], loss=63.3342
	step [103/191], loss=65.0445
	step [104/191], loss=71.4384
	step [105/191], loss=70.2000
	step [106/191], loss=52.0387
	step [107/191], loss=75.7637
	step [108/191], loss=70.9509
	step [109/191], loss=76.5865
	step [110/191], loss=72.6388
	step [111/191], loss=78.0768
	step [112/191], loss=46.1940
	step [113/191], loss=60.1625
	step [114/191], loss=63.7036
	step [115/191], loss=55.2980
	step [116/191], loss=62.0547
	step [117/191], loss=63.5713
	step [118/191], loss=77.2825
	step [119/191], loss=79.1374
	step [120/191], loss=74.9835
	step [121/191], loss=52.4903
	step [122/191], loss=57.4598
	step [123/191], loss=60.8357
	step [124/191], loss=73.9964
	step [125/191], loss=73.7164
	step [126/191], loss=72.4485
	step [127/191], loss=58.3358
	step [128/191], loss=70.1391
	step [129/191], loss=56.0213
	step [130/191], loss=74.5310
	step [131/191], loss=58.3090
	step [132/191], loss=60.6589
	step [133/191], loss=66.0429
	step [134/191], loss=67.9219
	step [135/191], loss=64.0426
	step [136/191], loss=68.3400
	step [137/191], loss=60.7258
	step [138/191], loss=58.6151
	step [139/191], loss=60.8197
	step [140/191], loss=71.0658
	step [141/191], loss=65.7420
	step [142/191], loss=75.0005
	step [143/191], loss=63.8725
	step [144/191], loss=62.4183
	step [145/191], loss=51.6521
	step [146/191], loss=72.0528
	step [147/191], loss=65.3973
	step [148/191], loss=64.5255
	step [149/191], loss=73.3754
	step [150/191], loss=59.9988
	step [151/191], loss=64.7398
	step [152/191], loss=69.8415
	step [153/191], loss=64.1229
	step [154/191], loss=63.9882
	step [155/191], loss=52.8443
	step [156/191], loss=70.9049
	step [157/191], loss=53.7669
	step [158/191], loss=78.1342
	step [159/191], loss=61.5512
	step [160/191], loss=53.5224
	step [161/191], loss=59.8407
	step [162/191], loss=64.2089
	step [163/191], loss=72.0266
	step [164/191], loss=60.4171
	step [165/191], loss=75.2937
	step [166/191], loss=67.8493
	step [167/191], loss=63.5083
	step [168/191], loss=67.7785
	step [169/191], loss=54.7454
	step [170/191], loss=59.3388
	step [171/191], loss=64.9600
	step [172/191], loss=65.5092
	step [173/191], loss=65.1535
	step [174/191], loss=58.5616
	step [175/191], loss=51.3302
	step [176/191], loss=49.2494
	step [177/191], loss=70.7600
	step [178/191], loss=67.0811
	step [179/191], loss=69.8906
	step [180/191], loss=67.5401
	step [181/191], loss=70.8844
	step [182/191], loss=61.7195
	step [183/191], loss=66.9277
	step [184/191], loss=68.2969
	step [185/191], loss=72.4516
	step [186/191], loss=64.9808
	step [187/191], loss=53.7203
	step [188/191], loss=60.6828
	step [189/191], loss=60.2711
	step [190/191], loss=67.8425
	step [191/191], loss=34.7578
	Evaluating
	loss=0.0106, precision=0.4295, recall=0.8825, f1=0.5778
Training finished
best_f1: 0.6534639103795141
directing: Y rim_enhanced: True test_id 1
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15610 # image files with weight 15579
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4462 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/325], loss=342.6595
	step [2/325], loss=339.2558
	step [3/325], loss=298.5150
	step [4/325], loss=225.7064
	step [5/325], loss=241.3346
	step [6/325], loss=226.3542
	step [7/325], loss=228.3516
	step [8/325], loss=209.9923
	step [9/325], loss=203.4648
	step [10/325], loss=225.3934
	step [11/325], loss=230.6293
	step [12/325], loss=222.1891
	step [13/325], loss=212.0736
	step [14/325], loss=194.1753
	step [15/325], loss=205.2021
	step [16/325], loss=217.7076
	step [17/325], loss=194.9088
	step [18/325], loss=220.4887
	step [19/325], loss=191.8468
	step [20/325], loss=198.9340
	step [21/325], loss=203.4089
	step [22/325], loss=214.6204
	step [23/325], loss=198.5837
	step [24/325], loss=183.3513
	step [25/325], loss=219.4847
	step [26/325], loss=205.8617
	step [27/325], loss=211.9446
	step [28/325], loss=195.4451
	step [29/325], loss=183.8733
	step [30/325], loss=211.7240
	step [31/325], loss=165.7047
	step [32/325], loss=192.3194
	step [33/325], loss=173.0269
	step [34/325], loss=184.6290
	step [35/325], loss=176.5731
	step [36/325], loss=208.7125
	step [37/325], loss=184.8242
	step [38/325], loss=196.3518
	step [39/325], loss=191.8763
	step [40/325], loss=201.2629
	step [41/325], loss=174.3470
	step [42/325], loss=167.5682
	step [43/325], loss=166.4161
	step [44/325], loss=168.3378
	step [45/325], loss=171.5453
	step [46/325], loss=159.0084
	step [47/325], loss=173.3400
	step [48/325], loss=160.6656
	step [49/325], loss=174.2505
	step [50/325], loss=175.4949
	step [51/325], loss=170.0133
	step [52/325], loss=170.3975
	step [53/325], loss=171.8131
	step [54/325], loss=175.8128
	step [55/325], loss=182.6216
	step [56/325], loss=160.9787
	step [57/325], loss=169.1370
	step [58/325], loss=152.8539
	step [59/325], loss=182.1472
	step [60/325], loss=164.8106
	step [61/325], loss=161.4531
	step [62/325], loss=165.3862
	step [63/325], loss=147.6352
	step [64/325], loss=171.4909
	step [65/325], loss=150.0862
	step [66/325], loss=157.8978
	step [67/325], loss=175.8412
	step [68/325], loss=186.0693
	step [69/325], loss=169.9904
	step [70/325], loss=155.7069
	step [71/325], loss=158.0482
	step [72/325], loss=169.6685
	step [73/325], loss=143.2783
	step [74/325], loss=162.5098
	step [75/325], loss=167.0924
	step [76/325], loss=173.6895
	step [77/325], loss=153.4686
	step [78/325], loss=163.8918
	step [79/325], loss=165.8925
	step [80/325], loss=156.4919
	step [81/325], loss=152.0018
	step [82/325], loss=171.3475
	step [83/325], loss=154.6569
	step [84/325], loss=164.0861
	step [85/325], loss=145.6410
	step [86/325], loss=145.2630
	step [87/325], loss=159.7113
	step [88/325], loss=174.3079
	step [89/325], loss=166.4897
	step [90/325], loss=149.2151
	step [91/325], loss=176.7002
	step [92/325], loss=149.1176
	step [93/325], loss=140.5719
	step [94/325], loss=160.7281
	step [95/325], loss=154.1678
	step [96/325], loss=143.9584
	step [97/325], loss=150.3501
	step [98/325], loss=166.0148
	step [99/325], loss=174.1316
	step [100/325], loss=152.7237
	step [101/325], loss=143.9473
	step [102/325], loss=173.3201
	step [103/325], loss=174.0915
	step [104/325], loss=169.5620
	step [105/325], loss=136.9038
	step [106/325], loss=165.4510
	step [107/325], loss=157.8617
	step [108/325], loss=147.6996
	step [109/325], loss=124.8686
	step [110/325], loss=141.1081
	step [111/325], loss=164.5722
	step [112/325], loss=179.8597
	step [113/325], loss=151.4081
	step [114/325], loss=148.5728
	step [115/325], loss=151.5266
	step [116/325], loss=179.7275
	step [117/325], loss=143.1023
	step [118/325], loss=159.4069
	step [119/325], loss=151.1765
	step [120/325], loss=148.6451
	step [121/325], loss=159.6323
	step [122/325], loss=141.2150
	step [123/325], loss=139.8441
	step [124/325], loss=142.5958
	step [125/325], loss=135.1886
	step [126/325], loss=135.2866
	step [127/325], loss=153.0051
	step [128/325], loss=147.0011
	step [129/325], loss=160.0871
	step [130/325], loss=150.3218
	step [131/325], loss=144.9523
	step [132/325], loss=155.4849
	step [133/325], loss=147.8014
	step [134/325], loss=144.6762
	step [135/325], loss=145.8759
	step [136/325], loss=151.2367
	step [137/325], loss=139.0634
	step [138/325], loss=147.8500
	step [139/325], loss=175.3202
	step [140/325], loss=158.5198
	step [141/325], loss=146.7751
	step [142/325], loss=149.7711
	step [143/325], loss=147.4290
	step [144/325], loss=149.3925
	step [145/325], loss=150.2068
	step [146/325], loss=152.1719
	step [147/325], loss=138.2680
	step [148/325], loss=159.4553
	step [149/325], loss=139.4849
	step [150/325], loss=146.3126
	step [151/325], loss=154.7284
	step [152/325], loss=132.7047
	step [153/325], loss=132.7386
	step [154/325], loss=157.2426
	step [155/325], loss=154.4590
	step [156/325], loss=148.9264
	step [157/325], loss=122.6590
	step [158/325], loss=128.6609
	step [159/325], loss=141.8299
	step [160/325], loss=133.9536
	step [161/325], loss=142.6995
	step [162/325], loss=154.2845
	step [163/325], loss=146.9079
	step [164/325], loss=131.9362
	step [165/325], loss=143.9631
	step [166/325], loss=148.3337
	step [167/325], loss=142.7588
	step [168/325], loss=146.9722
	step [169/325], loss=143.9648
	step [170/325], loss=134.2155
	step [171/325], loss=153.5095
	step [172/325], loss=153.7111
	step [173/325], loss=126.6814
	step [174/325], loss=136.8222
	step [175/325], loss=126.9465
	step [176/325], loss=142.4640
	step [177/325], loss=153.4043
	step [178/325], loss=139.6723
	step [179/325], loss=122.6534
	step [180/325], loss=122.0636
	step [181/325], loss=131.0048
	step [182/325], loss=113.8004
	step [183/325], loss=141.8424
	step [184/325], loss=132.7124
	step [185/325], loss=127.6359
	step [186/325], loss=146.3097
	step [187/325], loss=130.7286
	step [188/325], loss=150.9619
	step [189/325], loss=135.4163
	step [190/325], loss=130.7896
	step [191/325], loss=136.8630
	step [192/325], loss=144.5046
	step [193/325], loss=148.7975
	step [194/325], loss=116.6674
	step [195/325], loss=144.9848
	step [196/325], loss=136.8910
	step [197/325], loss=144.1967
	step [198/325], loss=138.7401
	step [199/325], loss=132.0396
	step [200/325], loss=122.8228
	step [201/325], loss=146.2652
	step [202/325], loss=120.8189
	step [203/325], loss=150.3819
	step [204/325], loss=148.3350
	step [205/325], loss=144.6451
	step [206/325], loss=133.5491
	step [207/325], loss=125.7685
	step [208/325], loss=139.1852
	step [209/325], loss=137.2474
	step [210/325], loss=136.3497
	step [211/325], loss=127.9590
	step [212/325], loss=130.5188
	step [213/325], loss=130.2538
	step [214/325], loss=129.7977
	step [215/325], loss=142.2904
	step [216/325], loss=131.6963
	step [217/325], loss=127.5106
	step [218/325], loss=135.1089
	step [219/325], loss=134.7780
	step [220/325], loss=149.0995
	step [221/325], loss=149.8460
	step [222/325], loss=156.0450
	step [223/325], loss=148.8634
	step [224/325], loss=147.5753
	step [225/325], loss=126.7121
	step [226/325], loss=122.0278
	step [227/325], loss=119.4846
	step [228/325], loss=136.9221
	step [229/325], loss=151.7102
	step [230/325], loss=152.2732
	step [231/325], loss=136.0829
	step [232/325], loss=127.9297
	step [233/325], loss=140.3685
	step [234/325], loss=136.3859
	step [235/325], loss=129.5412
	step [236/325], loss=122.6272
	step [237/325], loss=122.3620
	step [238/325], loss=121.3011
	step [239/325], loss=127.9968
	step [240/325], loss=148.8948
	step [241/325], loss=121.2039
	step [242/325], loss=135.5957
	step [243/325], loss=135.2715
	step [244/325], loss=117.9229
	step [245/325], loss=128.3767
	step [246/325], loss=148.4944
	step [247/325], loss=145.8085
	step [248/325], loss=136.7722
	step [249/325], loss=139.2548
	step [250/325], loss=160.4243
	step [251/325], loss=156.1002
	step [252/325], loss=130.7538
	step [253/325], loss=150.3958
	step [254/325], loss=134.3821
	step [255/325], loss=131.0969
	step [256/325], loss=129.0749
	step [257/325], loss=133.7166
	step [258/325], loss=110.9878
	step [259/325], loss=131.7049
	step [260/325], loss=125.9260
	step [261/325], loss=127.2225
	step [262/325], loss=124.1628
	step [263/325], loss=124.0289
	step [264/325], loss=147.2948
	step [265/325], loss=130.4432
	step [266/325], loss=123.2601
	step [267/325], loss=137.4944
	step [268/325], loss=117.7333
	step [269/325], loss=174.0690
	step [270/325], loss=120.6485
	step [271/325], loss=137.3690
	step [272/325], loss=125.4185
	step [273/325], loss=150.6804
	step [274/325], loss=160.9086
	step [275/325], loss=111.9423
	step [276/325], loss=126.1914
	step [277/325], loss=142.2832
	step [278/325], loss=125.9844
	step [279/325], loss=131.4526
	step [280/325], loss=133.3615
	step [281/325], loss=124.3098
	step [282/325], loss=144.3895
	step [283/325], loss=130.8680
	step [284/325], loss=126.4703
	step [285/325], loss=121.8018
	step [286/325], loss=117.7759
	step [287/325], loss=150.2080
	step [288/325], loss=113.3213
	step [289/325], loss=125.0400
	step [290/325], loss=117.2927
	step [291/325], loss=127.8675
	step [292/325], loss=122.7988
	step [293/325], loss=134.4237
	step [294/325], loss=142.7878
	step [295/325], loss=132.4902
	step [296/325], loss=144.1162
	step [297/325], loss=126.5656
	step [298/325], loss=144.8429
	step [299/325], loss=130.2085
	step [300/325], loss=137.8044
	step [301/325], loss=139.9362
	step [302/325], loss=137.3431
	step [303/325], loss=118.4067
	step [304/325], loss=123.7516
	step [305/325], loss=126.8941
	step [306/325], loss=111.9855
	step [307/325], loss=126.5390
	step [308/325], loss=117.9120
	step [309/325], loss=146.6373
	step [310/325], loss=138.0775
	step [311/325], loss=119.3308
	step [312/325], loss=115.8970
	step [313/325], loss=112.3171
	step [314/325], loss=126.3671
	step [315/325], loss=127.0249
	step [316/325], loss=130.6573
	step [317/325], loss=116.0370
	step [318/325], loss=124.0668
	step [319/325], loss=128.9299
	step [320/325], loss=117.6910
	step [321/325], loss=133.1581
	step [322/325], loss=140.4649
	step [323/325], loss=133.1690
	step [324/325], loss=143.0058
	step [325/325], loss=72.0170
	Evaluating
	loss=0.3239, precision=0.4067, recall=0.9033, f1=0.5609
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/325], loss=102.0217
	step [2/325], loss=128.2383
	step [3/325], loss=120.1199
	step [4/325], loss=125.4442
	step [5/325], loss=122.4764
	step [6/325], loss=127.4923
	step [7/325], loss=123.7155
	step [8/325], loss=113.9647
	step [9/325], loss=121.3877
	step [10/325], loss=141.3454
	step [11/325], loss=139.9726
	step [12/325], loss=133.4659
	step [13/325], loss=123.5784
	step [14/325], loss=108.2655
	step [15/325], loss=140.6350
	step [16/325], loss=126.5698
	step [17/325], loss=130.5504
	step [18/325], loss=128.3925
	step [19/325], loss=140.6744
	step [20/325], loss=127.7107
	step [21/325], loss=119.5573
	step [22/325], loss=142.0607
	step [23/325], loss=117.6750
	step [24/325], loss=141.0201
	step [25/325], loss=129.0962
	step [26/325], loss=126.9584
	step [27/325], loss=117.9942
	step [28/325], loss=133.6858
	step [29/325], loss=115.9921
	step [30/325], loss=139.8985
	step [31/325], loss=125.5602
	step [32/325], loss=132.5927
	step [33/325], loss=120.6944
	step [34/325], loss=116.6145
	step [35/325], loss=142.2473
	step [36/325], loss=115.5132
	step [37/325], loss=120.8541
	step [38/325], loss=125.2449
	step [39/325], loss=110.7325
	step [40/325], loss=116.4803
	step [41/325], loss=126.7509
	step [42/325], loss=113.4190
	step [43/325], loss=125.0797
	step [44/325], loss=110.3866
	step [45/325], loss=136.2943
	step [46/325], loss=127.4528
	step [47/325], loss=120.6015
	step [48/325], loss=114.2125
	step [49/325], loss=124.8755
	step [50/325], loss=142.7532
	step [51/325], loss=132.9482
	step [52/325], loss=135.2205
	step [53/325], loss=114.4457
	step [54/325], loss=104.9524
	step [55/325], loss=140.0355
	step [56/325], loss=129.0545
	step [57/325], loss=118.0149
	step [58/325], loss=109.1827
	step [59/325], loss=122.4996
	step [60/325], loss=109.4511
	step [61/325], loss=122.0295
	step [62/325], loss=110.8779
	step [63/325], loss=122.5365
	step [64/325], loss=110.6538
	step [65/325], loss=114.2034
	step [66/325], loss=161.4015
	step [67/325], loss=135.4906
	step [68/325], loss=129.0098
	step [69/325], loss=133.6452
	step [70/325], loss=118.0779
	step [71/325], loss=113.9842
	step [72/325], loss=122.0070
	step [73/325], loss=125.4926
	step [74/325], loss=117.8594
	step [75/325], loss=136.1036
	step [76/325], loss=114.7024
	step [77/325], loss=147.0482
	step [78/325], loss=127.4430
	step [79/325], loss=124.9141
	step [80/325], loss=111.8738
	step [81/325], loss=126.7870
	step [82/325], loss=107.5312
	step [83/325], loss=128.7377
	step [84/325], loss=112.3733
	step [85/325], loss=120.2772
	step [86/325], loss=131.2468
	step [87/325], loss=126.3323
	step [88/325], loss=140.2240
	step [89/325], loss=118.5834
	step [90/325], loss=117.7659
	step [91/325], loss=142.7786
	step [92/325], loss=126.0195
	step [93/325], loss=95.6606
	step [94/325], loss=134.3641
	step [95/325], loss=128.6826
	step [96/325], loss=98.9057
	step [97/325], loss=131.6702
	step [98/325], loss=120.0434
	step [99/325], loss=126.4675
	step [100/325], loss=106.6794
	step [101/325], loss=119.3429
	step [102/325], loss=112.3484
	step [103/325], loss=116.6493
	step [104/325], loss=128.1936
	step [105/325], loss=114.4456
	step [106/325], loss=126.4290
	step [107/325], loss=120.9777
	step [108/325], loss=113.3445
	step [109/325], loss=119.8635
	step [110/325], loss=109.0575
	step [111/325], loss=125.2579
	step [112/325], loss=113.7397
	step [113/325], loss=154.2273
	step [114/325], loss=117.9553
	step [115/325], loss=131.5572
	step [116/325], loss=111.9879
	step [117/325], loss=119.3688
	step [118/325], loss=103.7249
	step [119/325], loss=119.9249
	step [120/325], loss=116.0339
	step [121/325], loss=122.7032
	step [122/325], loss=111.7140
	step [123/325], loss=116.4555
	step [124/325], loss=115.9058
	step [125/325], loss=118.5764
	step [126/325], loss=105.6979
	step [127/325], loss=121.9626
	step [128/325], loss=111.4141
	step [129/325], loss=140.6796
	step [130/325], loss=126.5530
	step [131/325], loss=115.5565
	step [132/325], loss=133.5708
	step [133/325], loss=116.6978
	step [134/325], loss=115.1708
	step [135/325], loss=124.4004
	step [136/325], loss=121.3453
	step [137/325], loss=115.4112
	step [138/325], loss=110.8564
	step [139/325], loss=105.2991
	step [140/325], loss=108.5903
	step [141/325], loss=126.8796
	step [142/325], loss=109.7198
	step [143/325], loss=107.3239
	step [144/325], loss=99.3926
	step [145/325], loss=122.4208
	step [146/325], loss=115.4782
	step [147/325], loss=117.0038
	step [148/325], loss=118.8000
	step [149/325], loss=110.1681
	step [150/325], loss=102.3979
	step [151/325], loss=135.5745
	step [152/325], loss=116.0854
	step [153/325], loss=123.7547
	step [154/325], loss=120.7460
	step [155/325], loss=121.0591
	step [156/325], loss=127.1113
	step [157/325], loss=119.8249
	step [158/325], loss=128.2357
	step [159/325], loss=101.3252
	step [160/325], loss=98.1463
	step [161/325], loss=106.0290
	step [162/325], loss=113.9291
	step [163/325], loss=115.0478
	step [164/325], loss=100.4795
	step [165/325], loss=112.2120
	step [166/325], loss=120.7012
	step [167/325], loss=131.8712
	step [168/325], loss=122.5823
	step [169/325], loss=120.2805
	step [170/325], loss=127.8786
	step [171/325], loss=104.9048
	step [172/325], loss=117.1389
	step [173/325], loss=137.1518
	step [174/325], loss=100.3845
	step [175/325], loss=110.5631
	step [176/325], loss=129.6580
	step [177/325], loss=130.5337
	step [178/325], loss=145.6509
	step [179/325], loss=110.1227
	step [180/325], loss=135.8840
	step [181/325], loss=117.0422
	step [182/325], loss=127.4942
	step [183/325], loss=104.3510
	step [184/325], loss=136.1995
	step [185/325], loss=112.7494
	step [186/325], loss=103.8303
	step [187/325], loss=110.4615
	step [188/325], loss=116.0442
	step [189/325], loss=113.0606
	step [190/325], loss=111.3000
	step [191/325], loss=121.5010
	step [192/325], loss=113.1724
	step [193/325], loss=113.7422
	step [194/325], loss=108.9055
	step [195/325], loss=103.0535
	step [196/325], loss=120.2933
	step [197/325], loss=116.7226
	step [198/325], loss=123.2763
	step [199/325], loss=110.5757
	step [200/325], loss=99.1230
	step [201/325], loss=118.7957
	step [202/325], loss=132.6825
	step [203/325], loss=101.9566
	step [204/325], loss=108.3432
	step [205/325], loss=113.0375
	step [206/325], loss=110.2569
	step [207/325], loss=103.4375
	step [208/325], loss=105.8493
	step [209/325], loss=151.4963
	step [210/325], loss=118.0891
	step [211/325], loss=99.2389
	step [212/325], loss=116.4135
	step [213/325], loss=120.1922
	step [214/325], loss=129.2590
	step [215/325], loss=107.0223
	step [216/325], loss=101.6548
	step [217/325], loss=126.6546
	step [218/325], loss=126.8448
	step [219/325], loss=108.4349
	step [220/325], loss=123.4854
	step [221/325], loss=113.3535
	step [222/325], loss=119.4871
	step [223/325], loss=106.0107
	step [224/325], loss=119.9569
	step [225/325], loss=118.5493
	step [226/325], loss=113.4662
	step [227/325], loss=101.9568
	step [228/325], loss=103.2579
	step [229/325], loss=129.6870
	step [230/325], loss=109.4931
	step [231/325], loss=127.6835
	step [232/325], loss=106.4055
	step [233/325], loss=98.4285
	step [234/325], loss=116.2393
	step [235/325], loss=126.3776
	step [236/325], loss=125.1468
	step [237/325], loss=117.9204
	step [238/325], loss=119.6656
	step [239/325], loss=115.5031
	step [240/325], loss=116.2052
	step [241/325], loss=91.3447
	step [242/325], loss=120.5038
	step [243/325], loss=103.6601
	step [244/325], loss=98.2365
	step [245/325], loss=96.6703
	step [246/325], loss=116.6791
	step [247/325], loss=109.2457
	step [248/325], loss=108.2254
	step [249/325], loss=114.0062
	step [250/325], loss=112.0806
	step [251/325], loss=122.8046
	step [252/325], loss=99.0695
	step [253/325], loss=86.6505
	step [254/325], loss=123.1087
	step [255/325], loss=119.0832
	step [256/325], loss=111.1330
	step [257/325], loss=100.7047
	step [258/325], loss=111.3068
	step [259/325], loss=114.1980
	step [260/325], loss=123.4973
	step [261/325], loss=109.6589
	step [262/325], loss=124.1449
	step [263/325], loss=134.3957
	step [264/325], loss=109.9562
	step [265/325], loss=102.4639
	step [266/325], loss=108.9759
	step [267/325], loss=103.1486
	step [268/325], loss=114.8767
	step [269/325], loss=93.9272
	step [270/325], loss=106.8260
	step [271/325], loss=123.2828
	step [272/325], loss=102.4394
	step [273/325], loss=116.8209
	step [274/325], loss=101.0807
	step [275/325], loss=133.1332
	step [276/325], loss=118.8671
	step [277/325], loss=115.7352
	step [278/325], loss=118.8006
	step [279/325], loss=99.3377
	step [280/325], loss=120.6322
	step [281/325], loss=103.5441
	step [282/325], loss=123.2410
	step [283/325], loss=134.0132
	step [284/325], loss=93.0966
	step [285/325], loss=103.7463
	step [286/325], loss=114.0157
	step [287/325], loss=108.3935
	step [288/325], loss=92.2180
	step [289/325], loss=91.5330
	step [290/325], loss=111.8606
	step [291/325], loss=113.7707
	step [292/325], loss=104.8752
	step [293/325], loss=106.8404
	step [294/325], loss=123.4169
	step [295/325], loss=116.2150
	step [296/325], loss=102.1489
	step [297/325], loss=120.3840
	step [298/325], loss=107.7726
	step [299/325], loss=95.7542
	step [300/325], loss=103.7974
	step [301/325], loss=109.5820
	step [302/325], loss=118.5208
	step [303/325], loss=89.5949
	step [304/325], loss=98.2212
	step [305/325], loss=131.4038
	step [306/325], loss=104.5684
	step [307/325], loss=118.7270
	step [308/325], loss=91.2893
	step [309/325], loss=117.4142
	step [310/325], loss=91.3874
	step [311/325], loss=105.9574
	step [312/325], loss=116.6950
	step [313/325], loss=119.7500
	step [314/325], loss=125.5858
	step [315/325], loss=96.5415
	step [316/325], loss=106.4590
	step [317/325], loss=103.1785
	step [318/325], loss=107.8851
	step [319/325], loss=110.9961
	step [320/325], loss=86.5731
	step [321/325], loss=111.4619
	step [322/325], loss=97.7024
	step [323/325], loss=108.9037
	step [324/325], loss=108.2923
	step [325/325], loss=65.0139
	Evaluating
	loss=0.2117, precision=0.4448, recall=0.9184, f1=0.5993
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/325], loss=93.3724
	step [2/325], loss=91.2371
	step [3/325], loss=105.6474
	step [4/325], loss=103.8146
	step [5/325], loss=102.5359
	step [6/325], loss=96.1735
	step [7/325], loss=98.9737
	step [8/325], loss=98.5283
	step [9/325], loss=116.6128
	step [10/325], loss=101.1126
	step [11/325], loss=121.1469
	step [12/325], loss=90.9222
	step [13/325], loss=118.3756
	step [14/325], loss=101.4472
	step [15/325], loss=110.2605
	step [16/325], loss=107.7490
	step [17/325], loss=115.2244
	step [18/325], loss=85.3004
	step [19/325], loss=126.3073
	step [20/325], loss=129.0065
	step [21/325], loss=116.0309
	step [22/325], loss=103.0163
	step [23/325], loss=111.4629
	step [24/325], loss=109.5299
	step [25/325], loss=119.1467
	step [26/325], loss=123.5809
	step [27/325], loss=97.1230
	step [28/325], loss=91.4869
	step [29/325], loss=109.9133
	step [30/325], loss=118.7078
	step [31/325], loss=109.6712
	step [32/325], loss=110.5635
	step [33/325], loss=112.2263
	step [34/325], loss=115.7461
	step [35/325], loss=99.4453
	step [36/325], loss=100.4033
	step [37/325], loss=116.0390
	step [38/325], loss=128.0739
	step [39/325], loss=102.6620
	step [40/325], loss=108.2069
	step [41/325], loss=104.0300
	step [42/325], loss=110.5985
	step [43/325], loss=100.7279
	step [44/325], loss=96.0829
	step [45/325], loss=102.6833
	step [46/325], loss=104.2915
	step [47/325], loss=98.1249
	step [48/325], loss=103.7325
	step [49/325], loss=94.8667
	step [50/325], loss=77.8418
	step [51/325], loss=114.2324
	step [52/325], loss=111.3983
	step [53/325], loss=111.2928
	step [54/325], loss=107.5128
	step [55/325], loss=98.3167
	step [56/325], loss=110.0097
	step [57/325], loss=109.3113
	step [58/325], loss=93.1872
	step [59/325], loss=110.6466
	step [60/325], loss=109.0693
	step [61/325], loss=100.9450
	step [62/325], loss=96.7713
	step [63/325], loss=89.0403
	step [64/325], loss=102.9424
	step [65/325], loss=120.7474
	step [66/325], loss=106.9962
	step [67/325], loss=122.2464
	step [68/325], loss=107.0584
	step [69/325], loss=91.4045
	step [70/325], loss=113.2822
	step [71/325], loss=111.4496
	step [72/325], loss=95.1084
	step [73/325], loss=123.5951
	step [74/325], loss=135.2021
	step [75/325], loss=113.5253
	step [76/325], loss=123.5496
	step [77/325], loss=118.9656
	step [78/325], loss=113.8763
	step [79/325], loss=93.0129
	step [80/325], loss=109.4114
	step [81/325], loss=102.8265
	step [82/325], loss=124.9486
	step [83/325], loss=113.9015
	step [84/325], loss=113.3566
	step [85/325], loss=115.9258
	step [86/325], loss=111.0059
	step [87/325], loss=112.7739
	step [88/325], loss=114.2396
	step [89/325], loss=111.9332
	step [90/325], loss=111.0950
	step [91/325], loss=132.0950
	step [92/325], loss=128.5888
	step [93/325], loss=94.0395
	step [94/325], loss=110.8424
	step [95/325], loss=91.2613
	step [96/325], loss=100.6895
	step [97/325], loss=100.6731
	step [98/325], loss=89.5781
	step [99/325], loss=100.4886
	step [100/325], loss=111.2371
	step [101/325], loss=105.0916
	step [102/325], loss=106.1855
	step [103/325], loss=105.6505
	step [104/325], loss=98.2599
	step [105/325], loss=128.1890
	step [106/325], loss=123.0496
	step [107/325], loss=112.4545
	step [108/325], loss=99.3593
	step [109/325], loss=89.2141
	step [110/325], loss=116.9056
	step [111/325], loss=106.5641
	step [112/325], loss=110.6601
	step [113/325], loss=105.4812
	step [114/325], loss=101.4399
	step [115/325], loss=108.4236
	step [116/325], loss=88.9870
	step [117/325], loss=119.9432
	step [118/325], loss=99.8853
	step [119/325], loss=117.9036
	step [120/325], loss=98.7218
	step [121/325], loss=101.7480
	step [122/325], loss=112.4113
	step [123/325], loss=104.2024
	step [124/325], loss=89.6250
	step [125/325], loss=106.5259
	step [126/325], loss=98.3224
	step [127/325], loss=98.5633
	step [128/325], loss=98.4089
	step [129/325], loss=111.0322
	step [130/325], loss=106.3023
	step [131/325], loss=105.0557
	step [132/325], loss=90.0863
	step [133/325], loss=107.1760
	step [134/325], loss=113.6262
	step [135/325], loss=104.7798
	step [136/325], loss=109.3244
	step [137/325], loss=104.1974
	step [138/325], loss=85.9401
	step [139/325], loss=93.5375
	step [140/325], loss=110.6067
	step [141/325], loss=116.1961
	step [142/325], loss=88.6402
	step [143/325], loss=110.1891
	step [144/325], loss=112.4412
	step [145/325], loss=90.1649
	step [146/325], loss=122.2671
	step [147/325], loss=103.4861
	step [148/325], loss=93.5146
	step [149/325], loss=110.2904
	step [150/325], loss=90.4371
	step [151/325], loss=106.7285
	step [152/325], loss=117.4808
	step [153/325], loss=99.4452
	step [154/325], loss=127.5147
	step [155/325], loss=106.0157
	step [156/325], loss=112.6974
	step [157/325], loss=98.0111
	step [158/325], loss=94.6124
	step [159/325], loss=107.4608
	step [160/325], loss=101.4902
	step [161/325], loss=95.9401
	step [162/325], loss=111.2187
	step [163/325], loss=96.0950
	step [164/325], loss=112.9697
	step [165/325], loss=115.9586
	step [166/325], loss=116.1902
	step [167/325], loss=104.0231
	step [168/325], loss=107.9258
	step [169/325], loss=128.9711
	step [170/325], loss=104.3566
	step [171/325], loss=107.6638
	step [172/325], loss=95.4952
	step [173/325], loss=115.0439
	step [174/325], loss=101.6837
	step [175/325], loss=100.8416
	step [176/325], loss=104.8998
	step [177/325], loss=114.0331
	step [178/325], loss=109.8194
	step [179/325], loss=116.5986
	step [180/325], loss=94.2921
	step [181/325], loss=84.7953
	step [182/325], loss=118.8245
	step [183/325], loss=88.4264
	step [184/325], loss=111.8109
	step [185/325], loss=96.3328
	step [186/325], loss=106.2713
	step [187/325], loss=114.6216
	step [188/325], loss=87.7809
	step [189/325], loss=105.7498
	step [190/325], loss=116.2628
	step [191/325], loss=95.4069
	step [192/325], loss=114.8320
	step [193/325], loss=122.2978
	step [194/325], loss=87.6834
	step [195/325], loss=109.8622
	step [196/325], loss=113.0039
	step [197/325], loss=109.8348
	step [198/325], loss=108.6874
	step [199/325], loss=113.4924
	step [200/325], loss=82.0155
	step [201/325], loss=109.8772
	step [202/325], loss=94.3770
	step [203/325], loss=101.2110
	step [204/325], loss=100.3304
	step [205/325], loss=109.9540
	step [206/325], loss=94.0568
	step [207/325], loss=86.4216
	step [208/325], loss=102.5915
	step [209/325], loss=95.4055
	step [210/325], loss=112.5248
	step [211/325], loss=96.2330
	step [212/325], loss=97.6813
	step [213/325], loss=93.8053
	step [214/325], loss=93.6425
	step [215/325], loss=89.5547
	step [216/325], loss=92.8786
	step [217/325], loss=101.9245
	step [218/325], loss=95.9458
	step [219/325], loss=95.5995
	step [220/325], loss=86.2344
	step [221/325], loss=89.4097
	step [222/325], loss=94.1271
	step [223/325], loss=91.6509
	step [224/325], loss=99.3750
	step [225/325], loss=109.3569
	step [226/325], loss=113.5740
	step [227/325], loss=120.9799
	step [228/325], loss=105.8773
	step [229/325], loss=106.9382
	step [230/325], loss=101.6983
	step [231/325], loss=113.0873
	step [232/325], loss=113.6130
	step [233/325], loss=98.0107
	step [234/325], loss=101.5501
	step [235/325], loss=104.0064
	step [236/325], loss=104.1483
	step [237/325], loss=91.1975
	step [238/325], loss=104.7235
	step [239/325], loss=89.0463
	step [240/325], loss=96.5009
	step [241/325], loss=105.5677
	step [242/325], loss=91.3932
	step [243/325], loss=117.5000
	step [244/325], loss=98.5907
	step [245/325], loss=106.6297
	step [246/325], loss=81.7407
	step [247/325], loss=97.1046
	step [248/325], loss=95.6621
	step [249/325], loss=116.8491
	step [250/325], loss=99.5066
	step [251/325], loss=96.3222
	step [252/325], loss=102.9644
	step [253/325], loss=87.1889
	step [254/325], loss=93.9398
	step [255/325], loss=117.2061
	step [256/325], loss=98.8367
	step [257/325], loss=100.6578
	step [258/325], loss=90.6899
	step [259/325], loss=102.1472
	step [260/325], loss=102.2647
	step [261/325], loss=93.3560
	step [262/325], loss=95.0194
	step [263/325], loss=107.0301
	step [264/325], loss=96.6573
	step [265/325], loss=118.2856
	step [266/325], loss=99.6143
	step [267/325], loss=98.8006
	step [268/325], loss=108.9923
	step [269/325], loss=86.5630
	step [270/325], loss=91.9970
	step [271/325], loss=97.0540
	step [272/325], loss=90.6431
	step [273/325], loss=108.4295
	step [274/325], loss=95.5030
	step [275/325], loss=85.3699
	step [276/325], loss=92.2761
	step [277/325], loss=102.1282
	step [278/325], loss=93.4073
	step [279/325], loss=103.6662
	step [280/325], loss=86.4595
	step [281/325], loss=104.4256
	step [282/325], loss=101.2613
	step [283/325], loss=102.8337
	step [284/325], loss=93.8151
	step [285/325], loss=103.7560
	step [286/325], loss=97.2161
	step [287/325], loss=98.0412
	step [288/325], loss=108.1667
	step [289/325], loss=84.2160
	step [290/325], loss=113.0913
	step [291/325], loss=85.8864
	step [292/325], loss=105.0815
	step [293/325], loss=99.5373
	step [294/325], loss=99.4191
	step [295/325], loss=116.9358
	step [296/325], loss=100.0194
	step [297/325], loss=117.7942
	step [298/325], loss=109.3354
	step [299/325], loss=87.9182
	step [300/325], loss=128.3273
	step [301/325], loss=113.7562
	step [302/325], loss=116.4844
	step [303/325], loss=96.0884
	step [304/325], loss=92.5377
	step [305/325], loss=90.0282
	step [306/325], loss=103.4055
	step [307/325], loss=78.3903
	step [308/325], loss=94.2738
	step [309/325], loss=93.6648
	step [310/325], loss=82.4021
	step [311/325], loss=94.4368
	step [312/325], loss=91.4437
	step [313/325], loss=98.3767
	step [314/325], loss=98.8306
	step [315/325], loss=96.3510
	step [316/325], loss=77.4264
	step [317/325], loss=113.1830
	step [318/325], loss=118.6828
	step [319/325], loss=115.8613
	step [320/325], loss=111.0215
	step [321/325], loss=94.0898
	step [322/325], loss=95.2953
	step [323/325], loss=86.7914
	step [324/325], loss=101.9452
	step [325/325], loss=58.1138
	Evaluating
	loss=0.1475, precision=0.4797, recall=0.9023, f1=0.6264
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/325], loss=113.9655
	step [2/325], loss=91.8561
	step [3/325], loss=117.3907
	step [4/325], loss=101.3513
	step [5/325], loss=91.8781
	step [6/325], loss=104.8563
	step [7/325], loss=114.6483
	step [8/325], loss=98.8615
	step [9/325], loss=98.0534
	step [10/325], loss=95.7314
	step [11/325], loss=85.5204
	step [12/325], loss=107.8576
	step [13/325], loss=101.3649
	step [14/325], loss=87.4901
	step [15/325], loss=93.0760
	step [16/325], loss=106.6417
	step [17/325], loss=117.2628
	step [18/325], loss=101.3995
	step [19/325], loss=89.4579
	step [20/325], loss=85.9586
	step [21/325], loss=91.0023
	step [22/325], loss=97.8842
	step [23/325], loss=97.7335
	step [24/325], loss=93.0848
	step [25/325], loss=111.7046
	step [26/325], loss=109.6244
	step [27/325], loss=95.7639
	step [28/325], loss=97.8524
	step [29/325], loss=92.7564
	step [30/325], loss=108.6431
	step [31/325], loss=101.2528
	step [32/325], loss=84.0925
	step [33/325], loss=98.8632
	step [34/325], loss=124.8933
	step [35/325], loss=102.4551
	step [36/325], loss=92.0377
	step [37/325], loss=99.9508
	step [38/325], loss=95.1172
	step [39/325], loss=100.7852
	step [40/325], loss=90.9662
	step [41/325], loss=85.6967
	step [42/325], loss=97.4458
	step [43/325], loss=85.6820
	step [44/325], loss=102.6323
	step [45/325], loss=95.1123
	step [46/325], loss=94.7377
	step [47/325], loss=87.8796
	step [48/325], loss=103.4566
	step [49/325], loss=82.8406
	step [50/325], loss=75.9335
	step [51/325], loss=96.9556
	step [52/325], loss=107.3255
	step [53/325], loss=95.3963
	step [54/325], loss=90.4949
	step [55/325], loss=98.4910
	step [56/325], loss=111.0596
	step [57/325], loss=94.1780
	step [58/325], loss=107.3472
	step [59/325], loss=98.7558
	step [60/325], loss=94.1182
	step [61/325], loss=88.7334
	step [62/325], loss=98.6003
	step [63/325], loss=85.4261
	step [64/325], loss=81.0245
	step [65/325], loss=102.7954
	step [66/325], loss=108.2546
	step [67/325], loss=102.8640
	step [68/325], loss=85.4299
	step [69/325], loss=85.7652
	step [70/325], loss=99.3224
	step [71/325], loss=92.8552
	step [72/325], loss=80.9841
	step [73/325], loss=98.6366
	step [74/325], loss=97.7374
	step [75/325], loss=87.1095
	step [76/325], loss=90.3396
	step [77/325], loss=96.6059
	step [78/325], loss=91.9475
	step [79/325], loss=106.9780
	step [80/325], loss=100.9942
	step [81/325], loss=85.4714
	step [82/325], loss=75.7405
	step [83/325], loss=94.4809
	step [84/325], loss=92.2938
	step [85/325], loss=105.1791
	step [86/325], loss=93.3483
	step [87/325], loss=99.1397
	step [88/325], loss=103.2422
	step [89/325], loss=88.0861
	step [90/325], loss=89.0046
	step [91/325], loss=83.3102
	step [92/325], loss=92.1057
	step [93/325], loss=88.5853
	step [94/325], loss=88.4656
	step [95/325], loss=72.8800
	step [96/325], loss=95.2772
	step [97/325], loss=95.1601
	step [98/325], loss=102.0312
	step [99/325], loss=103.3170
	step [100/325], loss=105.1273
	step [101/325], loss=103.5977
	step [102/325], loss=94.5845
	step [103/325], loss=84.0396
	step [104/325], loss=101.0704
	step [105/325], loss=82.0922
	step [106/325], loss=100.5380
	step [107/325], loss=80.0027
	step [108/325], loss=103.1062
	step [109/325], loss=84.6717
	step [110/325], loss=108.9499
	step [111/325], loss=99.9923
	step [112/325], loss=97.9430
	step [113/325], loss=97.4687
	step [114/325], loss=96.0589
	step [115/325], loss=104.4622
	step [116/325], loss=105.8172
	step [117/325], loss=81.4242
	step [118/325], loss=98.5642
	step [119/325], loss=106.7861
	step [120/325], loss=97.0810
	step [121/325], loss=100.6442
	step [122/325], loss=106.2530
	step [123/325], loss=109.1951
	step [124/325], loss=85.2256
	step [125/325], loss=82.6420
	step [126/325], loss=90.1301
	step [127/325], loss=101.5139
	step [128/325], loss=103.3376
	step [129/325], loss=96.4582
	step [130/325], loss=112.5831
	step [131/325], loss=91.4349
	step [132/325], loss=103.9736
	step [133/325], loss=100.4976
	step [134/325], loss=118.9668
	step [135/325], loss=101.9124
	step [136/325], loss=104.3434
	step [137/325], loss=100.4605
	step [138/325], loss=81.2902
	step [139/325], loss=83.3530
	step [140/325], loss=92.6946
	step [141/325], loss=90.3867
	step [142/325], loss=94.5188
	step [143/325], loss=86.5360
	step [144/325], loss=99.6673
	step [145/325], loss=83.0238
	step [146/325], loss=84.9091
	step [147/325], loss=95.2249
	step [148/325], loss=114.8134
	step [149/325], loss=96.6319
	step [150/325], loss=83.2358
	step [151/325], loss=95.1192
	step [152/325], loss=85.7684
	step [153/325], loss=88.2831
	step [154/325], loss=95.1008
	step [155/325], loss=108.6858
	step [156/325], loss=85.7253
	step [157/325], loss=93.9987
	step [158/325], loss=89.4578
	step [159/325], loss=88.0154
	step [160/325], loss=88.6048
	step [161/325], loss=87.2675
	step [162/325], loss=89.8904
	step [163/325], loss=93.9866
	step [164/325], loss=91.8920
	step [165/325], loss=94.2313
	step [166/325], loss=102.8041
	step [167/325], loss=92.8844
	step [168/325], loss=87.4798
	step [169/325], loss=90.1895
	step [170/325], loss=112.9485
	step [171/325], loss=83.6823
	step [172/325], loss=104.1394
	step [173/325], loss=80.6963
	step [174/325], loss=92.9871
	step [175/325], loss=99.5604
	step [176/325], loss=92.0570
	step [177/325], loss=76.8092
	step [178/325], loss=98.8943
	step [179/325], loss=101.4455
	step [180/325], loss=94.4148
	step [181/325], loss=81.3558
	step [182/325], loss=88.7328
	step [183/325], loss=108.4363
	step [184/325], loss=91.8035
	step [185/325], loss=84.8274
	step [186/325], loss=103.6300
	step [187/325], loss=77.8084
	step [188/325], loss=89.9664
	step [189/325], loss=106.0360
	step [190/325], loss=91.5556
	step [191/325], loss=96.2174
	step [192/325], loss=92.8465
	step [193/325], loss=93.7212
	step [194/325], loss=90.0938
	step [195/325], loss=96.4629
	step [196/325], loss=107.9531
	step [197/325], loss=105.2948
	step [198/325], loss=88.7892
	step [199/325], loss=97.2094
	step [200/325], loss=76.9883
	step [201/325], loss=103.1980
	step [202/325], loss=112.6630
	step [203/325], loss=86.9662
	step [204/325], loss=84.9068
	step [205/325], loss=92.9592
	step [206/325], loss=99.1219
	step [207/325], loss=88.3062
	step [208/325], loss=98.6767
	step [209/325], loss=79.2153
	step [210/325], loss=84.2734
	step [211/325], loss=89.9995
	step [212/325], loss=92.0554
	step [213/325], loss=96.7244
	step [214/325], loss=110.9265
	step [215/325], loss=91.7864
	step [216/325], loss=108.6469
	step [217/325], loss=101.2057
	step [218/325], loss=97.8140
	step [219/325], loss=91.6180
	step [220/325], loss=91.7141
	step [221/325], loss=100.4931
	step [222/325], loss=86.8487
	step [223/325], loss=104.5858
	step [224/325], loss=93.2862
	step [225/325], loss=105.5024
	step [226/325], loss=92.8429
	step [227/325], loss=86.5653
	step [228/325], loss=76.5480
	step [229/325], loss=103.1363
	step [230/325], loss=79.6555
	step [231/325], loss=81.8867
	step [232/325], loss=121.0492
	step [233/325], loss=101.2845
	step [234/325], loss=86.7158
	step [235/325], loss=101.4751
	step [236/325], loss=76.4258
	step [237/325], loss=96.8042
	step [238/325], loss=104.5616
	step [239/325], loss=86.0829
	step [240/325], loss=98.6857
	step [241/325], loss=98.5474
	step [242/325], loss=95.4349
	step [243/325], loss=90.9172
	step [244/325], loss=91.5352
	step [245/325], loss=96.8995
	step [246/325], loss=78.4592
	step [247/325], loss=90.9308
	step [248/325], loss=115.1001
	step [249/325], loss=96.5254
	step [250/325], loss=83.2543
	step [251/325], loss=94.0478
	step [252/325], loss=82.0049
	step [253/325], loss=95.9912
	step [254/325], loss=79.2353
	step [255/325], loss=94.6270
	step [256/325], loss=84.6941
	step [257/325], loss=78.0958
	step [258/325], loss=102.2752
	step [259/325], loss=110.1393
	step [260/325], loss=103.1994
	step [261/325], loss=79.3094
	step [262/325], loss=96.5181
	step [263/325], loss=66.4142
	step [264/325], loss=104.5521
	step [265/325], loss=98.1533
	step [266/325], loss=90.6625
	step [267/325], loss=89.9946
	step [268/325], loss=98.8350
	step [269/325], loss=94.8750
	step [270/325], loss=104.1831
	step [271/325], loss=112.3815
	step [272/325], loss=105.2678
	step [273/325], loss=91.9356
	step [274/325], loss=68.5572
	step [275/325], loss=105.9478
	step [276/325], loss=98.3711
	step [277/325], loss=90.8966
	step [278/325], loss=80.9181
	step [279/325], loss=105.0256
	step [280/325], loss=112.8257
	step [281/325], loss=86.8630
	step [282/325], loss=91.6195
	step [283/325], loss=87.6809
	step [284/325], loss=94.6476
	step [285/325], loss=90.9060
	step [286/325], loss=86.3105
	step [287/325], loss=101.4878
	step [288/325], loss=94.5031
	step [289/325], loss=90.2034
	step [290/325], loss=87.7682
	step [291/325], loss=101.5696
	step [292/325], loss=85.8265
	step [293/325], loss=92.8179
	step [294/325], loss=99.3355
	step [295/325], loss=85.4544
	step [296/325], loss=76.4948
	step [297/325], loss=76.3663
	step [298/325], loss=110.3194
	step [299/325], loss=95.1158
	step [300/325], loss=82.7011
	step [301/325], loss=74.3903
	step [302/325], loss=101.0096
	step [303/325], loss=76.7335
	step [304/325], loss=89.6686
	step [305/325], loss=85.8810
	step [306/325], loss=82.4260
	step [307/325], loss=86.5506
	step [308/325], loss=83.2492
	step [309/325], loss=85.1957
	step [310/325], loss=104.5770
	step [311/325], loss=103.3478
	step [312/325], loss=88.6391
	step [313/325], loss=91.6192
	step [314/325], loss=89.6821
	step [315/325], loss=114.4761
	step [316/325], loss=84.7463
	step [317/325], loss=85.7084
	step [318/325], loss=91.6758
	step [319/325], loss=90.3317
	step [320/325], loss=93.9590
	step [321/325], loss=99.4232
	step [322/325], loss=98.5279
	step [323/325], loss=100.8083
	step [324/325], loss=89.7117
	step [325/325], loss=43.7332
	Evaluating
	loss=0.1045, precision=0.4533, recall=0.9191, f1=0.6072
Training epoch 5
	step [1/325], loss=84.2850
	step [2/325], loss=82.1008
	step [3/325], loss=76.2991
	step [4/325], loss=95.2211
	step [5/325], loss=113.6831
	step [6/325], loss=82.2801
	step [7/325], loss=99.0854
	step [8/325], loss=90.9729
	step [9/325], loss=97.5424
	step [10/325], loss=104.7933
	step [11/325], loss=79.4442
	step [12/325], loss=119.2678
	step [13/325], loss=108.5556
	step [14/325], loss=67.2607
	step [15/325], loss=76.8482
	step [16/325], loss=97.8689
	step [17/325], loss=82.5463
	step [18/325], loss=77.9315
	step [19/325], loss=87.3930
	step [20/325], loss=109.9765
	step [21/325], loss=106.0825
	step [22/325], loss=92.8891
	step [23/325], loss=100.0146
	step [24/325], loss=95.4783
	step [25/325], loss=95.6469
	step [26/325], loss=99.7725
	step [27/325], loss=77.5065
	step [28/325], loss=91.1422
	step [29/325], loss=78.9085
	step [30/325], loss=64.6628
	step [31/325], loss=92.2496
	step [32/325], loss=73.1573
	step [33/325], loss=80.0923
	step [34/325], loss=88.2387
	step [35/325], loss=98.8039
	step [36/325], loss=83.7165
	step [37/325], loss=86.2901
	step [38/325], loss=92.8343
	step [39/325], loss=85.3660
	step [40/325], loss=87.5869
	step [41/325], loss=95.8858
	step [42/325], loss=73.5003
	step [43/325], loss=99.6512
	step [44/325], loss=102.8024
	step [45/325], loss=84.2736
	step [46/325], loss=85.5418
	step [47/325], loss=80.1411
	step [48/325], loss=101.9556
	step [49/325], loss=77.6571
	step [50/325], loss=91.9089
	step [51/325], loss=106.0251
	step [52/325], loss=87.6293
	step [53/325], loss=87.6626
	step [54/325], loss=81.6699
	step [55/325], loss=78.7599
	step [56/325], loss=96.8993
	step [57/325], loss=96.4370
	step [58/325], loss=84.1101
	step [59/325], loss=87.1613
	step [60/325], loss=89.1233
	step [61/325], loss=73.6366
	step [62/325], loss=95.8202
	step [63/325], loss=100.5218
	step [64/325], loss=86.7919
	step [65/325], loss=86.7841
	step [66/325], loss=88.8304
	step [67/325], loss=77.5315
	step [68/325], loss=87.5719
	step [69/325], loss=77.9368
	step [70/325], loss=105.8833
	step [71/325], loss=109.3530
	step [72/325], loss=104.0317
	step [73/325], loss=87.2747
	step [74/325], loss=75.6946
	step [75/325], loss=86.7328
	step [76/325], loss=106.8605
	step [77/325], loss=73.7145
	step [78/325], loss=97.8667
	step [79/325], loss=93.3821
	step [80/325], loss=87.8271
	step [81/325], loss=84.9306
	step [82/325], loss=90.9250
	step [83/325], loss=80.2327
	step [84/325], loss=81.3993
	step [85/325], loss=98.8697
	step [86/325], loss=98.2474
	step [87/325], loss=88.7413
	step [88/325], loss=78.2674
	step [89/325], loss=84.4112
	step [90/325], loss=83.7594
	step [91/325], loss=83.3322
	step [92/325], loss=78.9317
	step [93/325], loss=75.7125
	step [94/325], loss=104.1266
	step [95/325], loss=103.0214
	step [96/325], loss=111.8386
	step [97/325], loss=95.2483
	step [98/325], loss=100.7388
	step [99/325], loss=109.3147
	step [100/325], loss=96.5182
	step [101/325], loss=84.5533
	step [102/325], loss=86.1734
	step [103/325], loss=76.8903
	step [104/325], loss=108.1953
	step [105/325], loss=89.3077
	step [106/325], loss=109.9129
	step [107/325], loss=91.1609
	step [108/325], loss=101.5720
	step [109/325], loss=79.3689
	step [110/325], loss=79.8653
	step [111/325], loss=81.6831
	step [112/325], loss=84.6963
	step [113/325], loss=88.2588
	step [114/325], loss=78.1239
	step [115/325], loss=110.5680
	step [116/325], loss=89.8408
	step [117/325], loss=81.5630
	step [118/325], loss=96.6704
	step [119/325], loss=74.1710
	step [120/325], loss=90.6139
	step [121/325], loss=78.0922
	step [122/325], loss=82.8228
	step [123/325], loss=96.5728
	step [124/325], loss=91.9689
	step [125/325], loss=89.4027
	step [126/325], loss=74.6748
	step [127/325], loss=100.0873
	step [128/325], loss=78.7758
	step [129/325], loss=93.2906
	step [130/325], loss=85.8774
	step [131/325], loss=80.2226
	step [132/325], loss=69.4947
	step [133/325], loss=96.4520
	step [134/325], loss=79.2302
	step [135/325], loss=100.4886
	step [136/325], loss=92.7724
	step [137/325], loss=84.1133
	step [138/325], loss=87.0471
	step [139/325], loss=72.7459
	step [140/325], loss=80.1691
	step [141/325], loss=88.1519
	step [142/325], loss=97.7617
	step [143/325], loss=114.3188
	step [144/325], loss=86.1647
	step [145/325], loss=98.4287
	step [146/325], loss=88.0769
	step [147/325], loss=95.5430
	step [148/325], loss=79.8762
	step [149/325], loss=94.3893
	step [150/325], loss=84.7673
	step [151/325], loss=111.6835
	step [152/325], loss=92.8844
	step [153/325], loss=97.6937
	step [154/325], loss=76.5043
	step [155/325], loss=96.2794
	step [156/325], loss=95.5104
	step [157/325], loss=94.6143
	step [158/325], loss=88.0094
	step [159/325], loss=93.6985
	step [160/325], loss=92.2801
	step [161/325], loss=81.0389
	step [162/325], loss=86.5232
	step [163/325], loss=74.1495
	step [164/325], loss=91.5122
	step [165/325], loss=93.7724
	step [166/325], loss=101.8871
	step [167/325], loss=92.4798
	step [168/325], loss=86.0324
	step [169/325], loss=83.1702
	step [170/325], loss=95.9230
	step [171/325], loss=64.1596
	step [172/325], loss=86.4122
	step [173/325], loss=107.4275
	step [174/325], loss=90.5944
	step [175/325], loss=74.6232
	step [176/325], loss=92.2304
	step [177/325], loss=93.5326
	step [178/325], loss=101.7062
	step [179/325], loss=78.7861
	step [180/325], loss=106.3444
	step [181/325], loss=103.5675
	step [182/325], loss=96.7732
	step [183/325], loss=107.6921
	step [184/325], loss=82.4254
	step [185/325], loss=75.7857
	step [186/325], loss=91.9148
	step [187/325], loss=95.0270
	step [188/325], loss=77.9980
	step [189/325], loss=73.2609
	step [190/325], loss=100.9549
	step [191/325], loss=111.0836
	step [192/325], loss=91.5408
	step [193/325], loss=86.0821
	step [194/325], loss=92.7557
	step [195/325], loss=81.1797
	step [196/325], loss=79.9447
	step [197/325], loss=103.5239
	step [198/325], loss=92.7387
	step [199/325], loss=83.5488
	step [200/325], loss=82.9485
	step [201/325], loss=83.2998
	step [202/325], loss=71.4602
	step [203/325], loss=71.8319
	step [204/325], loss=89.2543
	step [205/325], loss=111.4161
	step [206/325], loss=96.0649
	step [207/325], loss=72.5085
	step [208/325], loss=87.6116
	step [209/325], loss=83.3830
	step [210/325], loss=83.3618
	step [211/325], loss=107.9657
	step [212/325], loss=75.4193
	step [213/325], loss=76.0878
	step [214/325], loss=81.3919
	step [215/325], loss=103.3072
	step [216/325], loss=90.6315
	step [217/325], loss=92.1497
	step [218/325], loss=87.4987
	step [219/325], loss=103.8597
	step [220/325], loss=75.4214
	step [221/325], loss=66.9624
	step [222/325], loss=80.7003
	step [223/325], loss=77.1604
	step [224/325], loss=104.3834
	step [225/325], loss=102.4417
	step [226/325], loss=84.8531
	step [227/325], loss=84.6789
	step [228/325], loss=78.5133
	step [229/325], loss=74.0875
	step [230/325], loss=70.6840
	step [231/325], loss=90.3102
	step [232/325], loss=82.9021
	step [233/325], loss=86.4485
	step [234/325], loss=82.9871
	step [235/325], loss=75.7429
	step [236/325], loss=93.8622
	step [237/325], loss=91.1368
	step [238/325], loss=73.1548
	step [239/325], loss=96.6004
	step [240/325], loss=89.0602
	step [241/325], loss=86.4999
	step [242/325], loss=81.7743
	step [243/325], loss=93.5758
	step [244/325], loss=86.1785
	step [245/325], loss=78.4663
	step [246/325], loss=102.5901
	step [247/325], loss=110.1280
	step [248/325], loss=77.7258
	step [249/325], loss=105.3913
	step [250/325], loss=87.1418
	step [251/325], loss=95.1982
	step [252/325], loss=78.1512
	step [253/325], loss=86.6284
	step [254/325], loss=76.2411
	step [255/325], loss=85.3229
	step [256/325], loss=88.3854
	step [257/325], loss=86.4805
	step [258/325], loss=91.2964
	step [259/325], loss=94.7592
	step [260/325], loss=95.0516
	step [261/325], loss=90.0603
	step [262/325], loss=85.1827
	step [263/325], loss=88.4168
	step [264/325], loss=87.8047
	step [265/325], loss=100.0167
	step [266/325], loss=89.1156
	step [267/325], loss=88.3612
	step [268/325], loss=89.7934
	step [269/325], loss=82.6443
	step [270/325], loss=70.3182
	step [271/325], loss=85.3327
	step [272/325], loss=77.5497
	step [273/325], loss=79.1303
	step [274/325], loss=68.9807
	step [275/325], loss=91.5196
	step [276/325], loss=74.8329
	step [277/325], loss=84.9338
	step [278/325], loss=75.5726
	step [279/325], loss=78.3943
	step [280/325], loss=100.1528
	step [281/325], loss=93.4567
	step [282/325], loss=85.1157
	step [283/325], loss=90.6936
	step [284/325], loss=97.1816
	step [285/325], loss=96.7644
	step [286/325], loss=84.8986
	step [287/325], loss=87.1516
	step [288/325], loss=102.5755
	step [289/325], loss=73.6722
	step [290/325], loss=63.9961
	step [291/325], loss=81.7955
	step [292/325], loss=83.5192
	step [293/325], loss=85.3726
	step [294/325], loss=88.2204
	step [295/325], loss=77.2827
	step [296/325], loss=92.9199
	step [297/325], loss=82.9851
	step [298/325], loss=78.0634
	step [299/325], loss=80.8643
	step [300/325], loss=107.9693
	step [301/325], loss=87.3262
	step [302/325], loss=100.0558
	step [303/325], loss=97.3649
	step [304/325], loss=72.6323
	step [305/325], loss=90.6749
	step [306/325], loss=94.2428
	step [307/325], loss=91.2275
	step [308/325], loss=89.5061
	step [309/325], loss=93.9677
	step [310/325], loss=88.8335
	step [311/325], loss=98.4790
	step [312/325], loss=86.7007
	step [313/325], loss=85.0490
	step [314/325], loss=90.5840
	step [315/325], loss=79.2386
	step [316/325], loss=98.7031
	step [317/325], loss=103.0157
	step [318/325], loss=95.0276
	step [319/325], loss=97.2263
	step [320/325], loss=88.1878
	step [321/325], loss=103.5323
	step [322/325], loss=85.0069
	step [323/325], loss=110.1565
	step [324/325], loss=80.7842
	step [325/325], loss=47.3072
	Evaluating
	loss=0.0781, precision=0.4564, recall=0.8751, f1=0.5999
Training epoch 6
	step [1/325], loss=82.6202
	step [2/325], loss=85.9138
	step [3/325], loss=82.0969
	step [4/325], loss=87.4523
	step [5/325], loss=75.9062
	step [6/325], loss=94.9755
	step [7/325], loss=83.4240
	step [8/325], loss=82.2685
	step [9/325], loss=92.5163
	step [10/325], loss=76.0598
	step [11/325], loss=92.3564
	step [12/325], loss=107.5982
	step [13/325], loss=85.5549
	step [14/325], loss=94.8450
	step [15/325], loss=91.4314
	step [16/325], loss=76.7547
	step [17/325], loss=84.4092
	step [18/325], loss=73.0146
	step [19/325], loss=85.1143
	step [20/325], loss=81.6641
	step [21/325], loss=100.2829
	step [22/325], loss=86.5348
	step [23/325], loss=78.4827
	step [24/325], loss=87.7223
	step [25/325], loss=80.2458
	step [26/325], loss=102.2066
	step [27/325], loss=86.6695
	step [28/325], loss=106.9851
	step [29/325], loss=77.6185
	step [30/325], loss=88.5519
	step [31/325], loss=77.6758
	step [32/325], loss=75.3835
	step [33/325], loss=88.7441
	step [34/325], loss=64.6520
	step [35/325], loss=92.3477
	step [36/325], loss=89.5923
	step [37/325], loss=94.6882
	step [38/325], loss=100.1149
	step [39/325], loss=74.8692
	step [40/325], loss=78.4820
	step [41/325], loss=93.8813
	step [42/325], loss=85.7973
	step [43/325], loss=85.7891
	step [44/325], loss=92.6820
	step [45/325], loss=100.8054
	step [46/325], loss=83.6096
	step [47/325], loss=84.0881
	step [48/325], loss=90.4568
	step [49/325], loss=84.3236
	step [50/325], loss=98.1161
	step [51/325], loss=99.1446
	step [52/325], loss=74.7596
	step [53/325], loss=84.6167
	step [54/325], loss=95.6418
	step [55/325], loss=103.5418
	step [56/325], loss=85.8196
	step [57/325], loss=85.9695
	step [58/325], loss=82.7579
	step [59/325], loss=87.9949
	step [60/325], loss=98.3516
	step [61/325], loss=86.2457
	step [62/325], loss=83.1635
	step [63/325], loss=92.1151
	step [64/325], loss=75.2280
	step [65/325], loss=97.7297
	step [66/325], loss=87.8196
	step [67/325], loss=87.4356
	step [68/325], loss=75.5441
	step [69/325], loss=83.7958
	step [70/325], loss=94.7988
	step [71/325], loss=69.1845
	step [72/325], loss=88.7303
	step [73/325], loss=83.8716
	step [74/325], loss=92.1273
	step [75/325], loss=81.9417
	step [76/325], loss=79.3585
	step [77/325], loss=77.7574
	step [78/325], loss=78.3951
	step [79/325], loss=91.5804
	step [80/325], loss=79.8779
	step [81/325], loss=84.6828
	step [82/325], loss=82.7585
	step [83/325], loss=68.0976
	step [84/325], loss=85.4088
	step [85/325], loss=75.2491
	step [86/325], loss=113.3561
	step [87/325], loss=88.7784
	step [88/325], loss=70.3535
	step [89/325], loss=82.9131
	step [90/325], loss=78.5202
	step [91/325], loss=91.4435
	step [92/325], loss=91.0733
	step [93/325], loss=59.4182
	step [94/325], loss=89.5483
	step [95/325], loss=77.9539
	step [96/325], loss=78.9426
	step [97/325], loss=64.3985
	step [98/325], loss=80.5923
	step [99/325], loss=74.4627
	step [100/325], loss=80.6908
	step [101/325], loss=78.3411
	step [102/325], loss=93.3575
	step [103/325], loss=87.2872
	step [104/325], loss=87.3129
	step [105/325], loss=105.9642
	step [106/325], loss=78.8197
	step [107/325], loss=70.2824
	step [108/325], loss=90.1846
	step [109/325], loss=103.8752
	step [110/325], loss=97.0333
	step [111/325], loss=84.7682
	step [112/325], loss=92.4867
	step [113/325], loss=80.9875
	step [114/325], loss=51.5001
	step [115/325], loss=86.9507
	step [116/325], loss=76.5865
	step [117/325], loss=81.3697
	step [118/325], loss=90.7607
	step [119/325], loss=90.1437
	step [120/325], loss=83.6205
	step [121/325], loss=80.7747
	step [122/325], loss=84.9121
	step [123/325], loss=88.1156
	step [124/325], loss=100.7338
	step [125/325], loss=87.7247
	step [126/325], loss=85.0289
	step [127/325], loss=84.4065
	step [128/325], loss=84.1259
	step [129/325], loss=71.5264
	step [130/325], loss=97.4448
	step [131/325], loss=99.6895
	step [132/325], loss=80.8339
	step [133/325], loss=93.6564
	step [134/325], loss=93.9279
	step [135/325], loss=74.5604
	step [136/325], loss=69.6479
	step [137/325], loss=78.9949
	step [138/325], loss=76.7342
	step [139/325], loss=112.2444
	step [140/325], loss=83.4514
	step [141/325], loss=81.1286
	step [142/325], loss=74.4305
	step [143/325], loss=64.4446
	step [144/325], loss=81.4374
	step [145/325], loss=96.9872
	step [146/325], loss=96.9711
	step [147/325], loss=89.9197
	step [148/325], loss=81.9159
	step [149/325], loss=100.9631
	step [150/325], loss=103.4436
	step [151/325], loss=83.1393
	step [152/325], loss=74.5020
	step [153/325], loss=91.7375
	step [154/325], loss=77.1876
	step [155/325], loss=82.1160
	step [156/325], loss=85.1341
	step [157/325], loss=78.6867
	step [158/325], loss=92.0241
	step [159/325], loss=80.3317
	step [160/325], loss=94.1469
	step [161/325], loss=88.7590
	step [162/325], loss=83.3534
	step [163/325], loss=69.0539
	step [164/325], loss=90.9276
	step [165/325], loss=90.0859
	step [166/325], loss=81.0184
	step [167/325], loss=82.0876
	step [168/325], loss=76.0112
	step [169/325], loss=84.0482
	step [170/325], loss=108.0141
	step [171/325], loss=73.0022
	step [172/325], loss=93.3023
	step [173/325], loss=84.6937
	step [174/325], loss=82.0178
	step [175/325], loss=79.0580
	step [176/325], loss=86.6849
	step [177/325], loss=86.1447
	step [178/325], loss=80.7748
	step [179/325], loss=86.0061
	step [180/325], loss=78.5499
	step [181/325], loss=77.4799
	step [182/325], loss=77.9843
	step [183/325], loss=79.2957
	step [184/325], loss=89.7774
	step [185/325], loss=90.2561
	step [186/325], loss=87.8496
	step [187/325], loss=87.7379
	step [188/325], loss=83.9223
	step [189/325], loss=85.8080
	step [190/325], loss=81.2156
	step [191/325], loss=76.9045
	step [192/325], loss=70.2461
	step [193/325], loss=83.7046
	step [194/325], loss=86.1250
	step [195/325], loss=75.6696
	step [196/325], loss=84.1086
	step [197/325], loss=87.5313
	step [198/325], loss=87.6941
	step [199/325], loss=90.1058
	step [200/325], loss=91.8711
	step [201/325], loss=91.6949
	step [202/325], loss=98.6810
	step [203/325], loss=91.7260
	step [204/325], loss=75.2944
	step [205/325], loss=80.9459
	step [206/325], loss=105.0309
	step [207/325], loss=95.7104
	step [208/325], loss=69.7770
	step [209/325], loss=91.3792
	step [210/325], loss=83.7355
	step [211/325], loss=78.9847
	step [212/325], loss=71.6472
	step [213/325], loss=66.4700
	step [214/325], loss=80.1870
	step [215/325], loss=76.9228
	step [216/325], loss=95.6751
	step [217/325], loss=79.4202
	step [218/325], loss=69.7339
	step [219/325], loss=86.9001
	step [220/325], loss=74.2958
	step [221/325], loss=85.5020
	step [222/325], loss=95.1902
	step [223/325], loss=84.3109
	step [224/325], loss=77.0745
	step [225/325], loss=82.8181
	step [226/325], loss=75.8217
	step [227/325], loss=74.1225
	step [228/325], loss=84.5672
	step [229/325], loss=82.0854
	step [230/325], loss=72.4937
	step [231/325], loss=84.7976
	step [232/325], loss=85.4385
	step [233/325], loss=80.2224
	step [234/325], loss=83.7078
	step [235/325], loss=62.4674
	step [236/325], loss=72.0883
	step [237/325], loss=88.6418
	step [238/325], loss=80.8664
	step [239/325], loss=92.0420
	step [240/325], loss=91.1565
	step [241/325], loss=73.5778
	step [242/325], loss=91.7495
	step [243/325], loss=83.4498
	step [244/325], loss=87.5647
	step [245/325], loss=75.2828
	step [246/325], loss=75.6794
	step [247/325], loss=87.2421
	step [248/325], loss=82.0788
	step [249/325], loss=91.5730
	step [250/325], loss=92.1166
	step [251/325], loss=84.6271
	step [252/325], loss=76.9750
	step [253/325], loss=71.4715
	step [254/325], loss=66.5750
	step [255/325], loss=79.7858
	step [256/325], loss=73.6145
	step [257/325], loss=74.4700
	step [258/325], loss=73.2346
	step [259/325], loss=68.3986
	step [260/325], loss=96.4164
	step [261/325], loss=101.4382
	step [262/325], loss=81.2935
	step [263/325], loss=81.4093
	step [264/325], loss=74.8726
	step [265/325], loss=75.0698
	step [266/325], loss=87.2464
	step [267/325], loss=94.9844
	step [268/325], loss=86.7014
	step [269/325], loss=84.6095
	step [270/325], loss=78.1855
	step [271/325], loss=87.1412
	step [272/325], loss=64.1070
	step [273/325], loss=85.1273
	step [274/325], loss=65.3249
	step [275/325], loss=84.0070
	step [276/325], loss=85.8816
	step [277/325], loss=82.6731
	step [278/325], loss=87.9723
	step [279/325], loss=74.7421
	step [280/325], loss=73.7392
	step [281/325], loss=90.0108
	step [282/325], loss=101.4551
	step [283/325], loss=86.1747
	step [284/325], loss=80.0000
	step [285/325], loss=90.4256
	step [286/325], loss=105.7124
	step [287/325], loss=80.8714
	step [288/325], loss=83.7754
	step [289/325], loss=80.2666
	step [290/325], loss=81.8095
	step [291/325], loss=68.0557
	step [292/325], loss=83.8746
	step [293/325], loss=88.9295
	step [294/325], loss=80.8849
	step [295/325], loss=87.3489
	step [296/325], loss=78.5847
	step [297/325], loss=91.6360
	step [298/325], loss=93.3512
	step [299/325], loss=73.4706
	step [300/325], loss=83.8296
	step [301/325], loss=100.1697
	step [302/325], loss=107.2759
	step [303/325], loss=79.9546
	step [304/325], loss=73.1933
	step [305/325], loss=65.5174
	step [306/325], loss=78.7927
	step [307/325], loss=88.6485
	step [308/325], loss=79.5065
	step [309/325], loss=91.1933
	step [310/325], loss=67.9648
	step [311/325], loss=80.1316
	step [312/325], loss=98.9224
	step [313/325], loss=84.7461
	step [314/325], loss=81.7024
	step [315/325], loss=77.0365
	step [316/325], loss=82.9130
	step [317/325], loss=77.4664
	step [318/325], loss=75.6641
	step [319/325], loss=79.4460
	step [320/325], loss=87.6207
	step [321/325], loss=89.7805
	step [322/325], loss=69.2743
	step [323/325], loss=70.9506
	step [324/325], loss=84.7827
	step [325/325], loss=40.9625
	Evaluating
	loss=0.0548, precision=0.4506, recall=0.8630, f1=0.5921
Training epoch 7
	step [1/325], loss=65.5537
	step [2/325], loss=93.7368
	step [3/325], loss=93.6929
	step [4/325], loss=68.5338
	step [5/325], loss=76.6279
	step [6/325], loss=104.6580
	step [7/325], loss=97.4865
	step [8/325], loss=89.8563
	step [9/325], loss=84.4305
	step [10/325], loss=81.9471
	step [11/325], loss=81.4638
	step [12/325], loss=74.3117
	step [13/325], loss=93.5636
	step [14/325], loss=74.8387
	step [15/325], loss=70.8877
	step [16/325], loss=67.4012
	step [17/325], loss=105.3846
	step [18/325], loss=94.1409
	step [19/325], loss=88.6274
	step [20/325], loss=68.2717
	step [21/325], loss=96.8874
	step [22/325], loss=60.4739
	step [23/325], loss=80.4557
	step [24/325], loss=81.9785
	step [25/325], loss=82.0758
	step [26/325], loss=88.4109
	step [27/325], loss=77.7302
	step [28/325], loss=77.5233
	step [29/325], loss=80.1032
	step [30/325], loss=86.3465
	step [31/325], loss=58.6682
	step [32/325], loss=83.6777
	step [33/325], loss=83.9592
	step [34/325], loss=72.8218
	step [35/325], loss=78.8681
	step [36/325], loss=87.5857
	step [37/325], loss=77.4783
	step [38/325], loss=81.7442
	step [39/325], loss=84.2585
	step [40/325], loss=94.0311
	step [41/325], loss=87.8316
	step [42/325], loss=70.6189
	step [43/325], loss=79.0028
	step [44/325], loss=74.1104
	step [45/325], loss=77.6875
	step [46/325], loss=76.3601
	step [47/325], loss=106.2278
	step [48/325], loss=61.9124
	step [49/325], loss=72.5656
	step [50/325], loss=78.5765
	step [51/325], loss=81.3428
	step [52/325], loss=91.6057
	step [53/325], loss=92.9137
	step [54/325], loss=97.0860
	step [55/325], loss=73.0498
	step [56/325], loss=80.4258
	step [57/325], loss=70.2888
	step [58/325], loss=88.2659
	step [59/325], loss=71.1588
	step [60/325], loss=67.7654
	step [61/325], loss=81.4447
	step [62/325], loss=84.7282
	step [63/325], loss=85.1042
	step [64/325], loss=77.2555
	step [65/325], loss=88.8933
	step [66/325], loss=78.2961
	step [67/325], loss=85.9543
	step [68/325], loss=77.6947
	step [69/325], loss=76.3796
	step [70/325], loss=83.2461
	step [71/325], loss=89.7724
	step [72/325], loss=81.6130
	step [73/325], loss=83.7991
	step [74/325], loss=91.5733
	step [75/325], loss=79.4070
	step [76/325], loss=69.1357
	step [77/325], loss=85.1099
	step [78/325], loss=66.4834
	step [79/325], loss=98.0127
	step [80/325], loss=76.8181
	step [81/325], loss=78.8799
	step [82/325], loss=80.4983
	step [83/325], loss=85.3193
	step [84/325], loss=74.0943
	step [85/325], loss=88.4349
	step [86/325], loss=93.6709
	step [87/325], loss=88.0691
	step [88/325], loss=65.1905
	step [89/325], loss=73.4486
	step [90/325], loss=76.0856
	step [91/325], loss=68.5706
	step [92/325], loss=78.1259
	step [93/325], loss=77.7776
	step [94/325], loss=82.0350
	step [95/325], loss=76.8711
	step [96/325], loss=90.0602
	step [97/325], loss=75.4579
	step [98/325], loss=76.0401
	step [99/325], loss=76.2539
	step [100/325], loss=88.6959
	step [101/325], loss=75.5775
	step [102/325], loss=78.9394
	step [103/325], loss=69.9019
	step [104/325], loss=93.5707
	step [105/325], loss=65.3601
	step [106/325], loss=90.6390
	step [107/325], loss=92.3931
	step [108/325], loss=77.5815
	step [109/325], loss=75.0133
	step [110/325], loss=79.1006
	step [111/325], loss=66.0127
	step [112/325], loss=68.2258
	step [113/325], loss=77.7099
	step [114/325], loss=91.4456
	step [115/325], loss=77.1423
	step [116/325], loss=78.0274
	step [117/325], loss=91.1580
	step [118/325], loss=106.8301
	step [119/325], loss=84.7773
	step [120/325], loss=66.8795
	step [121/325], loss=73.5781
	step [122/325], loss=71.1820
	step [123/325], loss=98.2953
	step [124/325], loss=81.6570
	step [125/325], loss=92.8179
	step [126/325], loss=66.3979
	step [127/325], loss=88.6898
	step [128/325], loss=91.3949
	step [129/325], loss=74.1376
	step [130/325], loss=87.5900
	step [131/325], loss=92.9330
	step [132/325], loss=60.6195
	step [133/325], loss=78.2779
	step [134/325], loss=77.1125
	step [135/325], loss=74.1505
	step [136/325], loss=98.8985
	step [137/325], loss=97.0798
	step [138/325], loss=105.5953
	step [139/325], loss=81.6718
	step [140/325], loss=78.9651
	step [141/325], loss=89.4147
	step [142/325], loss=85.4827
	step [143/325], loss=69.4213
	step [144/325], loss=88.8666
	step [145/325], loss=94.8351
	step [146/325], loss=77.6702
	step [147/325], loss=100.7923
	step [148/325], loss=86.0381
	step [149/325], loss=63.1788
	step [150/325], loss=80.2937
	step [151/325], loss=74.6899
	step [152/325], loss=83.6529
	step [153/325], loss=73.2747
	step [154/325], loss=66.6203
	step [155/325], loss=82.0630
	step [156/325], loss=76.7286
	step [157/325], loss=85.7818
	step [158/325], loss=85.5580
	step [159/325], loss=93.4650
	step [160/325], loss=80.3704
	step [161/325], loss=79.4853
	step [162/325], loss=84.9585
	step [163/325], loss=96.1062
	step [164/325], loss=78.4354
	step [165/325], loss=74.2861
	step [166/325], loss=81.8923
	step [167/325], loss=78.4283
	step [168/325], loss=74.7295
	step [169/325], loss=60.4166
	step [170/325], loss=92.4058
	step [171/325], loss=90.6027
	step [172/325], loss=77.9086
	step [173/325], loss=76.6376
	step [174/325], loss=73.6970
	step [175/325], loss=66.1992
	step [176/325], loss=80.9560
	step [177/325], loss=73.8318
	step [178/325], loss=75.5710
	step [179/325], loss=69.7161
	step [180/325], loss=78.2797
	step [181/325], loss=70.0956
	step [182/325], loss=79.6916
	step [183/325], loss=80.2881
	step [184/325], loss=83.8311
	step [185/325], loss=66.1961
	step [186/325], loss=79.8945
	step [187/325], loss=92.1414
	step [188/325], loss=86.0190
	step [189/325], loss=87.6886
	step [190/325], loss=64.8619
	step [191/325], loss=96.3856
	step [192/325], loss=70.6928
	step [193/325], loss=87.0304
	step [194/325], loss=106.8984
	step [195/325], loss=96.1666
	step [196/325], loss=80.6104
	step [197/325], loss=83.8202
	step [198/325], loss=69.6223
	step [199/325], loss=89.9201
	step [200/325], loss=85.8574
	step [201/325], loss=82.2258
	step [202/325], loss=89.0989
	step [203/325], loss=57.3095
	step [204/325], loss=87.8339
	step [205/325], loss=79.1094
	step [206/325], loss=91.4852
	step [207/325], loss=101.7276
	step [208/325], loss=78.8872
	step [209/325], loss=72.0563
	step [210/325], loss=73.3523
	step [211/325], loss=71.0274
	step [212/325], loss=90.2920
	step [213/325], loss=82.7811
	step [214/325], loss=73.5909
	step [215/325], loss=87.2188
	step [216/325], loss=103.2495
	step [217/325], loss=99.6153
	step [218/325], loss=86.8046
	step [219/325], loss=94.8873
	step [220/325], loss=71.7602
	step [221/325], loss=83.5397
	step [222/325], loss=86.7983
	step [223/325], loss=73.6688
	step [224/325], loss=86.8094
	step [225/325], loss=81.8448
	step [226/325], loss=87.3844
	step [227/325], loss=84.4372
	step [228/325], loss=72.8138
	step [229/325], loss=68.9831
	step [230/325], loss=58.8426
	step [231/325], loss=83.7060
	step [232/325], loss=90.3670
	step [233/325], loss=80.3358
	step [234/325], loss=76.7806
	step [235/325], loss=90.7509
	step [236/325], loss=81.5860
	step [237/325], loss=77.7683
	step [238/325], loss=80.5844
	step [239/325], loss=72.8638
	step [240/325], loss=95.6874
	step [241/325], loss=80.4454
	step [242/325], loss=107.4538
	step [243/325], loss=73.3513
	step [244/325], loss=85.3215
	step [245/325], loss=81.6512
	step [246/325], loss=86.6650
	step [247/325], loss=93.1324
	step [248/325], loss=77.1027
	step [249/325], loss=90.0282
	step [250/325], loss=78.6357
	step [251/325], loss=61.8486
	step [252/325], loss=62.4774
	step [253/325], loss=74.6319
	step [254/325], loss=78.1896
	step [255/325], loss=71.7863
	step [256/325], loss=77.0981
	step [257/325], loss=85.2621
	step [258/325], loss=75.4948
	step [259/325], loss=80.0235
	step [260/325], loss=94.9822
	step [261/325], loss=93.7062
	step [262/325], loss=63.7901
	step [263/325], loss=88.9430
	step [264/325], loss=98.8888
	step [265/325], loss=75.3257
	step [266/325], loss=75.3339
	step [267/325], loss=61.1501
	step [268/325], loss=89.6249
	step [269/325], loss=72.2388
	step [270/325], loss=82.6912
	step [271/325], loss=75.9091
	step [272/325], loss=80.2378
	step [273/325], loss=84.3041
	step [274/325], loss=60.0182
	step [275/325], loss=82.3738
	step [276/325], loss=83.4140
	step [277/325], loss=72.7473
	step [278/325], loss=60.7599
	step [279/325], loss=82.4647
	step [280/325], loss=94.9534
	step [281/325], loss=88.5895
	step [282/325], loss=80.9467
	step [283/325], loss=77.2525
	step [284/325], loss=75.0434
	step [285/325], loss=81.2812
	step [286/325], loss=81.3830
	step [287/325], loss=80.3446
	step [288/325], loss=80.9371
	step [289/325], loss=94.5337
	step [290/325], loss=84.9532
	step [291/325], loss=85.0535
	step [292/325], loss=87.3212
	step [293/325], loss=89.5200
	step [294/325], loss=72.1516
	step [295/325], loss=103.4437
	step [296/325], loss=88.5045
	step [297/325], loss=80.4204
	step [298/325], loss=73.3123
	step [299/325], loss=62.1433
	step [300/325], loss=90.4997
	step [301/325], loss=83.0007
	step [302/325], loss=60.5134
	step [303/325], loss=60.5185
	step [304/325], loss=82.6611
	step [305/325], loss=71.8138
	step [306/325], loss=71.6553
	step [307/325], loss=75.6460
	step [308/325], loss=83.4958
	step [309/325], loss=78.7138
	step [310/325], loss=70.5457
	step [311/325], loss=79.8996
	step [312/325], loss=90.5084
	step [313/325], loss=87.2484
	step [314/325], loss=80.0241
	step [315/325], loss=84.8452
	step [316/325], loss=68.5448
	step [317/325], loss=91.6486
	step [318/325], loss=93.2783
	step [319/325], loss=77.6751
	step [320/325], loss=81.3961
	step [321/325], loss=76.4671
	step [322/325], loss=88.3155
	step [323/325], loss=63.9296
	step [324/325], loss=89.7920
	step [325/325], loss=49.1021
	Evaluating
	loss=0.0395, precision=0.4376, recall=0.9116, f1=0.5913
Training epoch 8
	step [1/325], loss=88.7528
	step [2/325], loss=68.0391
	step [3/325], loss=84.8592
	step [4/325], loss=70.2678
	step [5/325], loss=78.4777
	step [6/325], loss=87.0782
	step [7/325], loss=82.5902
	step [8/325], loss=85.0445
	step [9/325], loss=91.4322
	step [10/325], loss=73.9786
	step [11/325], loss=68.3031
	step [12/325], loss=73.7068
	step [13/325], loss=90.5034
	step [14/325], loss=70.8683
	step [15/325], loss=85.7824
	step [16/325], loss=100.8496
	step [17/325], loss=91.5358
	step [18/325], loss=65.7623
	step [19/325], loss=83.0720
	step [20/325], loss=88.3961
	step [21/325], loss=64.5806
	step [22/325], loss=92.8775
	step [23/325], loss=81.3401
	step [24/325], loss=105.7050
	step [25/325], loss=99.0419
	step [26/325], loss=84.1666
	step [27/325], loss=84.5378
	step [28/325], loss=74.0715
	step [29/325], loss=72.4908
	step [30/325], loss=77.3996
	step [31/325], loss=82.5910
	step [32/325], loss=88.8041
	step [33/325], loss=71.6393
	step [34/325], loss=86.8030
	step [35/325], loss=81.0720
	step [36/325], loss=76.9271
	step [37/325], loss=80.0221
	step [38/325], loss=82.3627
	step [39/325], loss=79.8739
	step [40/325], loss=85.0488
	step [41/325], loss=76.3719
	step [42/325], loss=92.5549
	step [43/325], loss=76.9334
	step [44/325], loss=83.4918
	step [45/325], loss=81.0367
	step [46/325], loss=81.8410
	step [47/325], loss=69.7692
	step [48/325], loss=71.0374
	step [49/325], loss=75.7119
	step [50/325], loss=79.4900
	step [51/325], loss=71.7185
	step [52/325], loss=85.6291
	step [53/325], loss=82.2500
	step [54/325], loss=86.3988
	step [55/325], loss=91.6632
	step [56/325], loss=97.7225
	step [57/325], loss=80.8027
	step [58/325], loss=88.2930
	step [59/325], loss=104.0641
	step [60/325], loss=67.2250
	step [61/325], loss=74.0765
	step [62/325], loss=78.2924
	step [63/325], loss=68.1843
	step [64/325], loss=78.3172
	step [65/325], loss=91.9490
	step [66/325], loss=81.8634
	step [67/325], loss=88.4289
	step [68/325], loss=84.9240
	step [69/325], loss=88.4019
	step [70/325], loss=70.4868
	step [71/325], loss=75.6016
	step [72/325], loss=78.4402
	step [73/325], loss=70.3950
	step [74/325], loss=77.3182
	step [75/325], loss=72.8878
	step [76/325], loss=87.1831
	step [77/325], loss=80.4274
	step [78/325], loss=97.4178
	step [79/325], loss=73.8514
	step [80/325], loss=78.4584
	step [81/325], loss=84.8288
	step [82/325], loss=89.9498
	step [83/325], loss=99.1026
	step [84/325], loss=79.8713
	step [85/325], loss=83.1750
	step [86/325], loss=79.2586
	step [87/325], loss=87.2577
	step [88/325], loss=71.7015
	step [89/325], loss=84.3868
	step [90/325], loss=76.6015
	step [91/325], loss=79.9529
	step [92/325], loss=77.7830
	step [93/325], loss=74.7693
	step [94/325], loss=86.1188
	step [95/325], loss=86.9788
	step [96/325], loss=60.9418
	step [97/325], loss=72.1849
	step [98/325], loss=71.9289
	step [99/325], loss=85.6398
	step [100/325], loss=78.2301
	step [101/325], loss=58.6455
	step [102/325], loss=73.3980
	step [103/325], loss=82.1045
	step [104/325], loss=66.8009
	step [105/325], loss=65.9495
	step [106/325], loss=57.6319
	step [107/325], loss=88.1693
	step [108/325], loss=57.1604
	step [109/325], loss=99.7378
	step [110/325], loss=80.6431
	step [111/325], loss=90.8499
	step [112/325], loss=83.6298
	step [113/325], loss=61.6029
	step [114/325], loss=74.9629
	step [115/325], loss=76.2456
	step [116/325], loss=75.2370
	step [117/325], loss=99.6044
	step [118/325], loss=67.0659
	step [119/325], loss=66.1027
	step [120/325], loss=94.6485
	step [121/325], loss=76.6264
	step [122/325], loss=79.6595
	step [123/325], loss=90.9763
	step [124/325], loss=82.9792
	step [125/325], loss=88.2180
	step [126/325], loss=84.9717
	step [127/325], loss=65.9775
	step [128/325], loss=75.5579
	step [129/325], loss=82.5476
	step [130/325], loss=87.2002
	step [131/325], loss=86.5623
	step [132/325], loss=82.0816
	step [133/325], loss=73.2641
	step [134/325], loss=68.4876
	step [135/325], loss=72.0896
	step [136/325], loss=69.7113
	step [137/325], loss=83.8499
	step [138/325], loss=81.4170
	step [139/325], loss=73.2731
	step [140/325], loss=67.8783
	step [141/325], loss=89.3893
	step [142/325], loss=67.1679
	step [143/325], loss=78.4961
	step [144/325], loss=75.0041
	step [145/325], loss=71.5954
	step [146/325], loss=75.9349
	step [147/325], loss=78.4597
	step [148/325], loss=85.5252
	step [149/325], loss=76.1978
	step [150/325], loss=76.1789
	step [151/325], loss=96.9065
	step [152/325], loss=43.6783
	step [153/325], loss=85.0519
	step [154/325], loss=72.2728
	step [155/325], loss=73.2307
	step [156/325], loss=95.1775
	step [157/325], loss=88.0004
	step [158/325], loss=67.2072
	step [159/325], loss=61.1956
	step [160/325], loss=79.9547
	step [161/325], loss=66.7872
	step [162/325], loss=84.2183
	step [163/325], loss=93.7135
	step [164/325], loss=84.0799
	step [165/325], loss=72.1111
	step [166/325], loss=79.2651
	step [167/325], loss=74.9758
	step [168/325], loss=78.2715
	step [169/325], loss=88.0893
	step [170/325], loss=67.2347
	step [171/325], loss=86.7933
	step [172/325], loss=82.8112
	step [173/325], loss=75.6083
	step [174/325], loss=76.8623
	step [175/325], loss=87.5890
	step [176/325], loss=66.1032
	step [177/325], loss=75.5267
	step [178/325], loss=75.5935
	step [179/325], loss=72.6845
	step [180/325], loss=74.7796
	step [181/325], loss=83.4739
	step [182/325], loss=93.7715
	step [183/325], loss=74.1052
	step [184/325], loss=83.0090
	step [185/325], loss=84.7094
	step [186/325], loss=84.0275
	step [187/325], loss=93.6280
	step [188/325], loss=83.2743
	step [189/325], loss=69.7263
	step [190/325], loss=78.8928
	step [191/325], loss=69.1339
	step [192/325], loss=87.1158
	step [193/325], loss=79.5517
	step [194/325], loss=88.5423
	step [195/325], loss=91.5825
	step [196/325], loss=63.2310
	step [197/325], loss=65.7428
	step [198/325], loss=70.8858
	step [199/325], loss=71.0802
	step [200/325], loss=63.9578
	step [201/325], loss=70.4739
	step [202/325], loss=83.7788
	step [203/325], loss=59.0987
	step [204/325], loss=87.9495
	step [205/325], loss=90.4007
	step [206/325], loss=79.9450
	step [207/325], loss=76.4819
	step [208/325], loss=83.0560
	step [209/325], loss=91.0195
	step [210/325], loss=78.0021
	step [211/325], loss=78.2306
	step [212/325], loss=82.0703
	step [213/325], loss=68.1075
	step [214/325], loss=86.5173
	step [215/325], loss=75.8536
	step [216/325], loss=83.6235
	step [217/325], loss=83.2099
	step [218/325], loss=81.5103
	step [219/325], loss=83.9378
	step [220/325], loss=89.0409
	step [221/325], loss=72.2521
	step [222/325], loss=76.4868
	step [223/325], loss=72.2868
	step [224/325], loss=70.2230
	step [225/325], loss=81.3108
	step [226/325], loss=67.1382
	step [227/325], loss=69.2526
	step [228/325], loss=80.4046
	step [229/325], loss=56.1875
	step [230/325], loss=103.2315
	step [231/325], loss=89.6322
	step [232/325], loss=73.1265
	step [233/325], loss=82.6635
	step [234/325], loss=79.7063
	step [235/325], loss=70.0056
	step [236/325], loss=74.2941
	step [237/325], loss=92.8591
	step [238/325], loss=87.3487
	step [239/325], loss=73.9405
	step [240/325], loss=89.7987
	step [241/325], loss=78.3905
	step [242/325], loss=69.1658
	step [243/325], loss=74.2888
	step [244/325], loss=78.2938
	step [245/325], loss=69.2596
	step [246/325], loss=90.2770
	step [247/325], loss=74.5276
	step [248/325], loss=73.8364
	step [249/325], loss=58.7211
	step [250/325], loss=84.2825
	step [251/325], loss=77.3698
	step [252/325], loss=74.3912
	step [253/325], loss=82.3764
	step [254/325], loss=72.9271
	step [255/325], loss=69.8476
	step [256/325], loss=95.4154
	step [257/325], loss=70.3018
	step [258/325], loss=68.1287
	step [259/325], loss=71.3012
	step [260/325], loss=70.7228
	step [261/325], loss=80.9660
	step [262/325], loss=66.7175
	step [263/325], loss=80.3090
	step [264/325], loss=60.5565
	step [265/325], loss=91.5182
	step [266/325], loss=66.7266
	step [267/325], loss=80.4124
	step [268/325], loss=86.1626
	step [269/325], loss=72.9697
	step [270/325], loss=69.5162
	step [271/325], loss=72.6806
	step [272/325], loss=71.7552
	step [273/325], loss=69.1349
	step [274/325], loss=73.2715
	step [275/325], loss=88.3050
	step [276/325], loss=67.6827
	step [277/325], loss=89.7105
	step [278/325], loss=67.6080
	step [279/325], loss=89.6224
	step [280/325], loss=76.1445
	step [281/325], loss=71.7692
	step [282/325], loss=82.9402
	step [283/325], loss=76.5369
	step [284/325], loss=78.1986
	step [285/325], loss=59.3352
	step [286/325], loss=87.2744
	step [287/325], loss=99.6324
	step [288/325], loss=87.6711
	step [289/325], loss=98.3155
	step [290/325], loss=74.2836
	step [291/325], loss=78.2268
	step [292/325], loss=94.5430
	step [293/325], loss=63.8944
	step [294/325], loss=72.6655
	step [295/325], loss=86.5256
	step [296/325], loss=76.7898
	step [297/325], loss=70.1275
	step [298/325], loss=88.5914
	step [299/325], loss=63.5293
	step [300/325], loss=72.4875
	step [301/325], loss=77.3441
	step [302/325], loss=89.8272
	step [303/325], loss=63.1580
	step [304/325], loss=72.7591
	step [305/325], loss=77.8692
	step [306/325], loss=66.9416
	step [307/325], loss=79.0176
	step [308/325], loss=63.0303
	step [309/325], loss=85.5333
	step [310/325], loss=81.7997
	step [311/325], loss=97.3966
	step [312/325], loss=78.0064
	step [313/325], loss=70.7178
	step [314/325], loss=63.2390
	step [315/325], loss=79.7685
	step [316/325], loss=70.1286
	step [317/325], loss=81.7313
	step [318/325], loss=64.4351
	step [319/325], loss=75.6169
	step [320/325], loss=77.2163
	step [321/325], loss=94.2810
	step [322/325], loss=75.5581
	step [323/325], loss=64.2542
	step [324/325], loss=79.7309
	step [325/325], loss=44.7601
	Evaluating
	loss=0.0303, precision=0.4541, recall=0.9083, f1=0.6055
Training epoch 9
	step [1/325], loss=79.6563
	step [2/325], loss=63.7020
	step [3/325], loss=68.5458
	step [4/325], loss=88.4512
	step [5/325], loss=76.7499
	step [6/325], loss=66.3986
	step [7/325], loss=84.3684
	step [8/325], loss=80.2724
	step [9/325], loss=79.4893
	step [10/325], loss=59.5166
	step [11/325], loss=78.2263
	step [12/325], loss=81.8727
	step [13/325], loss=72.5109
	step [14/325], loss=80.2319
	step [15/325], loss=70.1200
	step [16/325], loss=79.9820
	step [17/325], loss=71.6099
	step [18/325], loss=103.5602
	step [19/325], loss=65.8410
	step [20/325], loss=83.4481
	step [21/325], loss=67.0057
	step [22/325], loss=94.2694
	step [23/325], loss=70.8456
	step [24/325], loss=71.1273
	step [25/325], loss=76.5774
	step [26/325], loss=91.1637
	step [27/325], loss=66.8794
	step [28/325], loss=73.7499
	step [29/325], loss=87.5848
	step [30/325], loss=101.9484
	step [31/325], loss=81.1812
	step [32/325], loss=80.8609
	step [33/325], loss=61.0636
	step [34/325], loss=54.7317
	step [35/325], loss=69.9428
	step [36/325], loss=98.6454
	step [37/325], loss=87.5226
	step [38/325], loss=64.9541
	step [39/325], loss=81.0423
	step [40/325], loss=74.8992
	step [41/325], loss=97.8438
	step [42/325], loss=69.8287
	step [43/325], loss=96.4790
	step [44/325], loss=72.6445
	step [45/325], loss=90.1365
	step [46/325], loss=78.1437
	step [47/325], loss=68.9028
	step [48/325], loss=82.7602
	step [49/325], loss=81.9098
	step [50/325], loss=89.9875
	step [51/325], loss=65.9885
	step [52/325], loss=89.8582
	step [53/325], loss=83.0383
	step [54/325], loss=80.0535
	step [55/325], loss=70.0940
	step [56/325], loss=70.3121
	step [57/325], loss=68.5313
	step [58/325], loss=73.9161
	step [59/325], loss=73.7760
	step [60/325], loss=75.3891
	step [61/325], loss=73.6966
	step [62/325], loss=85.3714
	step [63/325], loss=91.6436
	step [64/325], loss=81.6570
	step [65/325], loss=79.2069
	step [66/325], loss=84.9599
	step [67/325], loss=65.7798
	step [68/325], loss=77.8473
	step [69/325], loss=69.8129
	step [70/325], loss=69.9606
	step [71/325], loss=60.6588
	step [72/325], loss=74.1727
	step [73/325], loss=66.1985
	step [74/325], loss=82.8745
	step [75/325], loss=63.8591
	step [76/325], loss=84.5904
	step [77/325], loss=85.1159
	step [78/325], loss=79.2161
	step [79/325], loss=73.1197
	step [80/325], loss=76.8758
	step [81/325], loss=82.3077
	step [82/325], loss=74.1586
	step [83/325], loss=80.4447
	step [84/325], loss=64.4364
	step [85/325], loss=75.5802
	step [86/325], loss=56.3600
	step [87/325], loss=74.9233
	step [88/325], loss=90.7143
	step [89/325], loss=82.0779
	step [90/325], loss=78.4117
	step [91/325], loss=68.4647
	step [92/325], loss=75.1830
	step [93/325], loss=74.7697
	step [94/325], loss=77.1663
	step [95/325], loss=65.3378
	step [96/325], loss=77.4561
	step [97/325], loss=74.8681
	step [98/325], loss=72.9615
	step [99/325], loss=66.6004
	step [100/325], loss=57.7543
	step [101/325], loss=101.3327
	step [102/325], loss=75.9603
	step [103/325], loss=81.0957
	step [104/325], loss=83.3261
	step [105/325], loss=77.2665
	step [106/325], loss=84.8685
	step [107/325], loss=84.1901
	step [108/325], loss=85.2635
	step [109/325], loss=83.6128
	step [110/325], loss=82.8329
	step [111/325], loss=75.7169
	step [112/325], loss=86.3910
	step [113/325], loss=73.6695
	step [114/325], loss=73.5883
	step [115/325], loss=66.3436
	step [116/325], loss=77.0059
	step [117/325], loss=78.1280
	step [118/325], loss=78.5386
	step [119/325], loss=71.9760
	step [120/325], loss=86.3999
	step [121/325], loss=94.2560
	step [122/325], loss=69.0087
	step [123/325], loss=80.1255
	step [124/325], loss=57.4331
	step [125/325], loss=70.5313
	step [126/325], loss=71.3333
	step [127/325], loss=73.9129
	step [128/325], loss=75.8168
	step [129/325], loss=69.3254
	step [130/325], loss=80.2805
	step [131/325], loss=77.3536
	step [132/325], loss=80.5222
	step [133/325], loss=65.8603
	step [134/325], loss=85.8412
	step [135/325], loss=75.6272
	step [136/325], loss=78.8488
	step [137/325], loss=69.5656
	step [138/325], loss=77.2305
	step [139/325], loss=91.9260
	step [140/325], loss=69.3847
	step [141/325], loss=78.7641
	step [142/325], loss=73.7431
	step [143/325], loss=77.2365
	step [144/325], loss=81.1122
	step [145/325], loss=61.6721
	step [146/325], loss=91.3085
	step [147/325], loss=68.4507
	step [148/325], loss=84.5384
	step [149/325], loss=82.0169
	step [150/325], loss=75.9058
	step [151/325], loss=79.3944
	step [152/325], loss=58.4919
	step [153/325], loss=83.3386
	step [154/325], loss=97.3004
	step [155/325], loss=75.7777
	step [156/325], loss=60.8068
	step [157/325], loss=74.0303
	step [158/325], loss=79.4804
	step [159/325], loss=75.4028
	step [160/325], loss=56.9451
	step [161/325], loss=85.9111
	step [162/325], loss=83.2965
	step [163/325], loss=70.4779
	step [164/325], loss=69.4398
	step [165/325], loss=86.0105
	step [166/325], loss=83.6603
	step [167/325], loss=56.8871
	step [168/325], loss=81.9643
	step [169/325], loss=81.4195
	step [170/325], loss=92.8906
	step [171/325], loss=78.9355
	step [172/325], loss=79.1036
	step [173/325], loss=74.6631
	step [174/325], loss=68.7458
	step [175/325], loss=68.5453
	step [176/325], loss=68.7994
	step [177/325], loss=70.3847
	step [178/325], loss=84.6394
	step [179/325], loss=74.0045
	step [180/325], loss=83.0842
	step [181/325], loss=86.3983
	step [182/325], loss=87.2496
	step [183/325], loss=70.7267
	step [184/325], loss=72.1011
	step [185/325], loss=79.6823
	step [186/325], loss=78.3177
	step [187/325], loss=64.1676
	step [188/325], loss=82.2181
	step [189/325], loss=62.9479
	step [190/325], loss=83.3627
	step [191/325], loss=57.3554
	step [192/325], loss=86.1283
	step [193/325], loss=69.3777
	step [194/325], loss=85.0397
	step [195/325], loss=76.4403
	step [196/325], loss=73.6109
	step [197/325], loss=78.8532
	step [198/325], loss=88.6236
	step [199/325], loss=93.0610
	step [200/325], loss=74.0985
	step [201/325], loss=88.3130
	step [202/325], loss=93.5247
	step [203/325], loss=85.3596
	step [204/325], loss=77.1941
	step [205/325], loss=72.3520
	step [206/325], loss=61.6233
	step [207/325], loss=69.1845
	step [208/325], loss=77.2653
	step [209/325], loss=84.2198
	step [210/325], loss=92.3868
	step [211/325], loss=68.0795
	step [212/325], loss=74.4549
	step [213/325], loss=74.8377
	step [214/325], loss=81.6302
	step [215/325], loss=76.8450
	step [216/325], loss=94.7382
	step [217/325], loss=66.7871
	step [218/325], loss=93.6525
	step [219/325], loss=76.0994
	step [220/325], loss=78.5507
	step [221/325], loss=84.2067
	step [222/325], loss=84.0303
	step [223/325], loss=62.0179
	step [224/325], loss=67.3894
	step [225/325], loss=104.4273
	step [226/325], loss=72.3077
	step [227/325], loss=96.7636
	step [228/325], loss=90.5812
	step [229/325], loss=93.9971
	step [230/325], loss=70.9248
	step [231/325], loss=87.8537
	step [232/325], loss=71.3718
	step [233/325], loss=61.1692
	step [234/325], loss=81.2093
	step [235/325], loss=65.3686
	step [236/325], loss=104.0473
	step [237/325], loss=77.1046
	step [238/325], loss=73.5338
	step [239/325], loss=73.0523
	step [240/325], loss=85.5057
	step [241/325], loss=103.4894
	step [242/325], loss=65.4047
	step [243/325], loss=68.3798
	step [244/325], loss=86.7841
	step [245/325], loss=63.0137
	step [246/325], loss=81.7506
	step [247/325], loss=78.0808
	step [248/325], loss=90.9066
	step [249/325], loss=70.8755
	step [250/325], loss=63.5752
	step [251/325], loss=85.1892
	step [252/325], loss=77.2734
	step [253/325], loss=87.2149
	step [254/325], loss=72.0680
	step [255/325], loss=68.0452
	step [256/325], loss=66.3550
	step [257/325], loss=79.4938
	step [258/325], loss=78.9508
	step [259/325], loss=63.2504
	step [260/325], loss=78.2437
	step [261/325], loss=82.1674
	step [262/325], loss=80.1325
	step [263/325], loss=78.3145
	step [264/325], loss=68.5215
	step [265/325], loss=67.4961
	step [266/325], loss=83.7192
	step [267/325], loss=59.9526
	step [268/325], loss=91.8940
	step [269/325], loss=84.4719
	step [270/325], loss=53.9897
	step [271/325], loss=90.0646
	step [272/325], loss=79.1874
	step [273/325], loss=91.9668
	step [274/325], loss=82.3202
	step [275/325], loss=83.5893
	step [276/325], loss=67.8951
	step [277/325], loss=82.8211
	step [278/325], loss=71.8662
	step [279/325], loss=80.6436
	step [280/325], loss=64.9222
	step [281/325], loss=66.2570
	step [282/325], loss=76.2606
	step [283/325], loss=70.2278
	step [284/325], loss=71.9459
	step [285/325], loss=72.3866
	step [286/325], loss=78.2752
	step [287/325], loss=63.7145
	step [288/325], loss=63.3479
	step [289/325], loss=76.2094
	step [290/325], loss=80.6242
	step [291/325], loss=92.4453
	step [292/325], loss=79.1118
	step [293/325], loss=83.9046
	step [294/325], loss=81.6544
	step [295/325], loss=77.8102
	step [296/325], loss=72.3782
	step [297/325], loss=75.7767
	step [298/325], loss=75.3345
	step [299/325], loss=81.8280
	step [300/325], loss=74.6394
	step [301/325], loss=70.4362
	step [302/325], loss=79.3600
	step [303/325], loss=86.3426
	step [304/325], loss=88.6060
	step [305/325], loss=72.3703
	step [306/325], loss=72.6030
	step [307/325], loss=76.7759
	step [308/325], loss=87.0325
	step [309/325], loss=78.7118
	step [310/325], loss=66.0616
	step [311/325], loss=78.5219
	step [312/325], loss=58.8820
	step [313/325], loss=74.4663
	step [314/325], loss=82.4732
	step [315/325], loss=72.9350
	step [316/325], loss=67.5999
	step [317/325], loss=61.2569
	step [318/325], loss=76.1073
	step [319/325], loss=96.9608
	step [320/325], loss=80.4359
	step [321/325], loss=75.3331
	step [322/325], loss=86.5257
	step [323/325], loss=81.0074
	step [324/325], loss=78.6315
	step [325/325], loss=50.5082
	Evaluating
	loss=0.0262, precision=0.3936, recall=0.9126, f1=0.5500
Training epoch 10
	step [1/325], loss=69.9357
	step [2/325], loss=73.9172
	step [3/325], loss=71.3567
	step [4/325], loss=79.2466
	step [5/325], loss=74.4079
	step [6/325], loss=72.4589
	step [7/325], loss=79.1509
	step [8/325], loss=77.0711
	step [9/325], loss=87.9232
	step [10/325], loss=75.6361
	step [11/325], loss=82.6255
	step [12/325], loss=84.4367
	step [13/325], loss=95.7877
	step [14/325], loss=50.6613
	step [15/325], loss=68.7390
	step [16/325], loss=100.3785
	step [17/325], loss=77.8880
	step [18/325], loss=84.3396
	step [19/325], loss=68.4251
	step [20/325], loss=68.6252
	step [21/325], loss=84.4692
	step [22/325], loss=77.8236
	step [23/325], loss=74.3768
	step [24/325], loss=88.3231
	step [25/325], loss=80.9325
	step [26/325], loss=77.1778
	step [27/325], loss=79.2436
	step [28/325], loss=81.4798
	step [29/325], loss=81.4153
	step [30/325], loss=93.5689
	step [31/325], loss=67.0391
	step [32/325], loss=68.0329
	step [33/325], loss=71.2765
	step [34/325], loss=79.7018
	step [35/325], loss=74.4014
	step [36/325], loss=63.5429
	step [37/325], loss=71.9791
	step [38/325], loss=77.8879
	step [39/325], loss=85.1997
	step [40/325], loss=78.9886
	step [41/325], loss=79.8832
	step [42/325], loss=83.5298
	step [43/325], loss=75.1003
	step [44/325], loss=75.2771
	step [45/325], loss=75.6450
	step [46/325], loss=84.5494
	step [47/325], loss=71.7637
	step [48/325], loss=92.4905
	step [49/325], loss=75.4609
	step [50/325], loss=89.7909
	step [51/325], loss=85.1548
	step [52/325], loss=85.5384
	step [53/325], loss=70.7626
	step [54/325], loss=70.6873
	step [55/325], loss=78.9291
	step [56/325], loss=85.3763
	step [57/325], loss=74.9311
	step [58/325], loss=100.9557
	step [59/325], loss=83.8219
	step [60/325], loss=70.8898
	step [61/325], loss=74.0760
	step [62/325], loss=74.1608
	step [63/325], loss=68.2731
	step [64/325], loss=85.7842
	step [65/325], loss=80.4327
	step [66/325], loss=88.0877
	step [67/325], loss=74.0345
	step [68/325], loss=81.4484
	step [69/325], loss=74.2810
	step [70/325], loss=73.5816
	step [71/325], loss=81.9629
	step [72/325], loss=67.7979
	step [73/325], loss=73.3544
	step [74/325], loss=79.1351
	step [75/325], loss=62.9416
	step [76/325], loss=76.8749
	step [77/325], loss=82.0718
	step [78/325], loss=61.4455
	step [79/325], loss=75.5675
	step [80/325], loss=73.2321
	step [81/325], loss=69.2876
	step [82/325], loss=67.1143
	step [83/325], loss=70.8255
	step [84/325], loss=87.2194
	step [85/325], loss=60.0248
	step [86/325], loss=78.7307
	step [87/325], loss=55.6534
	step [88/325], loss=80.6531
	step [89/325], loss=75.9610
	step [90/325], loss=77.6093
	step [91/325], loss=61.6357
	step [92/325], loss=73.1392
	step [93/325], loss=71.1576
	step [94/325], loss=82.2925
	step [95/325], loss=68.7975
	step [96/325], loss=79.8096
	step [97/325], loss=89.7808
	step [98/325], loss=103.3825
	step [99/325], loss=76.5030
	step [100/325], loss=74.4658
	step [101/325], loss=63.4528
	step [102/325], loss=68.2912
	step [103/325], loss=73.0061
	step [104/325], loss=91.5281
	step [105/325], loss=74.6200
	step [106/325], loss=67.6829
	step [107/325], loss=66.0806
	step [108/325], loss=71.0768
	step [109/325], loss=88.0084
	step [110/325], loss=81.2068
	step [111/325], loss=78.5727
	step [112/325], loss=83.8242
	step [113/325], loss=81.4966
	step [114/325], loss=91.4639
	step [115/325], loss=64.3005
	step [116/325], loss=64.6475
	step [117/325], loss=67.8086
	step [118/325], loss=71.6735
	step [119/325], loss=73.1136
	step [120/325], loss=88.4083
	step [121/325], loss=72.8353
	step [122/325], loss=79.3691
	step [123/325], loss=76.0072
	step [124/325], loss=62.3309
	step [125/325], loss=57.9792
	step [126/325], loss=75.7879
	step [127/325], loss=91.8446
	step [128/325], loss=72.9419
	step [129/325], loss=80.3165
	step [130/325], loss=65.3900
	step [131/325], loss=78.8694
	step [132/325], loss=80.9745
	step [133/325], loss=57.0192
	step [134/325], loss=70.0406
	step [135/325], loss=86.3699
	step [136/325], loss=70.4438
	step [137/325], loss=78.2150
	step [138/325], loss=109.7503
	step [139/325], loss=72.9721
	step [140/325], loss=72.0835
	step [141/325], loss=87.4648
	step [142/325], loss=78.7157
	step [143/325], loss=87.1099
	step [144/325], loss=65.7973
	step [145/325], loss=95.0659
	step [146/325], loss=72.8414
	step [147/325], loss=73.6069
	step [148/325], loss=95.5379
	step [149/325], loss=73.3638
	step [150/325], loss=82.6800
	step [151/325], loss=63.3700
	step [152/325], loss=73.0328
	step [153/325], loss=74.8161
	step [154/325], loss=85.4185
	step [155/325], loss=78.3406
	step [156/325], loss=71.5844
	step [157/325], loss=59.3544
	step [158/325], loss=65.5980
	step [159/325], loss=70.5474
	step [160/325], loss=85.2434
	step [161/325], loss=98.3730
	step [162/325], loss=75.9802
	step [163/325], loss=69.6495
	step [164/325], loss=91.7701
	step [165/325], loss=74.9643
	step [166/325], loss=76.2207
	step [167/325], loss=77.3077
	step [168/325], loss=80.3912
	step [169/325], loss=72.0161
	step [170/325], loss=66.5496
	step [171/325], loss=65.4368
	step [172/325], loss=80.8896
	step [173/325], loss=78.8982
	step [174/325], loss=74.6933
	step [175/325], loss=82.0352
	step [176/325], loss=81.0926
	step [177/325], loss=74.3344
	step [178/325], loss=59.0303
	step [179/325], loss=81.7327
	step [180/325], loss=67.4722
	step [181/325], loss=92.4198
	step [182/325], loss=73.2777
	step [183/325], loss=93.6811
	step [184/325], loss=66.5544
	step [185/325], loss=68.1704
	step [186/325], loss=82.0658
	step [187/325], loss=66.4869
	step [188/325], loss=65.0387
	step [189/325], loss=78.6719
	step [190/325], loss=71.8706
	step [191/325], loss=82.2324
	step [192/325], loss=76.1872
	step [193/325], loss=73.3067
	step [194/325], loss=73.1936
	step [195/325], loss=69.5487
	step [196/325], loss=70.6964
	step [197/325], loss=81.3655
	step [198/325], loss=81.0488
	step [199/325], loss=63.5583
	step [200/325], loss=86.2038
	step [201/325], loss=77.0088
	step [202/325], loss=72.3339
	step [203/325], loss=66.9883
	step [204/325], loss=83.5036
	step [205/325], loss=98.1102
	step [206/325], loss=75.9555
	step [207/325], loss=75.2725
	step [208/325], loss=80.0140
	step [209/325], loss=62.6375
	step [210/325], loss=74.9074
	step [211/325], loss=85.1425
	step [212/325], loss=74.4651
	step [213/325], loss=85.3683
	step [214/325], loss=64.8706
	step [215/325], loss=80.3372
	step [216/325], loss=61.2662
	step [217/325], loss=79.7434
	step [218/325], loss=91.2819
	step [219/325], loss=73.4762
	step [220/325], loss=66.4242
	step [221/325], loss=70.3760
	step [222/325], loss=72.2712
	step [223/325], loss=75.6230
	step [224/325], loss=75.1710
	step [225/325], loss=95.1759
	step [226/325], loss=90.3253
	step [227/325], loss=84.3242
	step [228/325], loss=61.5886
	step [229/325], loss=74.6518
	step [230/325], loss=63.3384
	step [231/325], loss=65.3926
	step [232/325], loss=76.1900
	step [233/325], loss=75.7274
	step [234/325], loss=67.5669
	step [235/325], loss=60.8602
	step [236/325], loss=68.5599
	step [237/325], loss=75.9066
	step [238/325], loss=79.5892
	step [239/325], loss=72.4524
	step [240/325], loss=80.0568
	step [241/325], loss=81.0080
	step [242/325], loss=66.5667
	step [243/325], loss=75.5692
	step [244/325], loss=69.8232
	step [245/325], loss=80.5432
	step [246/325], loss=81.4184
	step [247/325], loss=72.6474
	step [248/325], loss=75.9840
	step [249/325], loss=80.2626
	step [250/325], loss=75.8777
	step [251/325], loss=70.1792
	step [252/325], loss=73.8065
	step [253/325], loss=66.9951
	step [254/325], loss=70.9387
	step [255/325], loss=73.5393
	step [256/325], loss=78.7880
	step [257/325], loss=63.7832
	step [258/325], loss=68.9368
	step [259/325], loss=88.3409
	step [260/325], loss=79.2954
	step [261/325], loss=83.4995
	step [262/325], loss=70.6910
	step [263/325], loss=62.5267
	step [264/325], loss=81.9792
	step [265/325], loss=64.8137
	step [266/325], loss=70.5235
	step [267/325], loss=89.2899
	step [268/325], loss=66.3225
	step [269/325], loss=72.4187
	step [270/325], loss=75.2001
	step [271/325], loss=65.9685
	step [272/325], loss=87.0654
	step [273/325], loss=96.6828
	step [274/325], loss=72.5905
	step [275/325], loss=81.8799
	step [276/325], loss=75.2733
	step [277/325], loss=64.2337
	step [278/325], loss=72.1949
	step [279/325], loss=71.7390
	step [280/325], loss=78.7664
	step [281/325], loss=88.0266
	step [282/325], loss=63.1119
	step [283/325], loss=57.3885
	step [284/325], loss=65.2972
	step [285/325], loss=73.4418
	step [286/325], loss=80.8914
	step [287/325], loss=79.1669
	step [288/325], loss=81.3384
	step [289/325], loss=65.9438
	step [290/325], loss=61.0226
	step [291/325], loss=83.0139
	step [292/325], loss=87.3603
	step [293/325], loss=77.7180
	step [294/325], loss=69.1069
	step [295/325], loss=78.8890
	step [296/325], loss=84.9754
	step [297/325], loss=74.2581
	step [298/325], loss=79.5180
	step [299/325], loss=68.3234
	step [300/325], loss=80.5393
	step [301/325], loss=82.5937
	step [302/325], loss=72.0364
	step [303/325], loss=91.3334
	step [304/325], loss=72.1266
	step [305/325], loss=76.4712
	step [306/325], loss=72.3546
	step [307/325], loss=71.2405
	step [308/325], loss=73.5086
	step [309/325], loss=69.4280
	step [310/325], loss=70.3447
	step [311/325], loss=79.2435
	step [312/325], loss=89.5167
	step [313/325], loss=77.3707
	step [314/325], loss=83.6237
	step [315/325], loss=78.9741
	step [316/325], loss=75.4659
	step [317/325], loss=65.8156
	step [318/325], loss=60.7427
	step [319/325], loss=92.0729
	step [320/325], loss=71.6698
	step [321/325], loss=84.7271
	step [322/325], loss=70.4438
	step [323/325], loss=77.8843
	step [324/325], loss=80.3807
	step [325/325], loss=39.7474
	Evaluating
	loss=0.0202, precision=0.4609, recall=0.8816, f1=0.6053
Training epoch 11
	step [1/325], loss=88.9887
	step [2/325], loss=87.4217
	step [3/325], loss=67.8335
	step [4/325], loss=82.4371
	step [5/325], loss=84.8943
	step [6/325], loss=91.3297
	step [7/325], loss=63.6039
	step [8/325], loss=57.3029
	step [9/325], loss=82.4207
	step [10/325], loss=71.9961
	step [11/325], loss=82.6385
	step [12/325], loss=76.2278
	step [13/325], loss=68.6866
	step [14/325], loss=79.5099
	step [15/325], loss=74.3970
	step [16/325], loss=82.5389
	step [17/325], loss=84.6337
	step [18/325], loss=54.5992
	step [19/325], loss=75.6132
	step [20/325], loss=74.9429
	step [21/325], loss=79.0077
	step [22/325], loss=73.8250
	step [23/325], loss=74.9034
	step [24/325], loss=74.0560
	step [25/325], loss=72.0684
	step [26/325], loss=64.8385
	step [27/325], loss=64.8032
	step [28/325], loss=86.8158
	step [29/325], loss=69.8539
	step [30/325], loss=66.2764
	step [31/325], loss=77.8020
	step [32/325], loss=71.0207
	step [33/325], loss=85.6857
	step [34/325], loss=67.2826
	step [35/325], loss=84.6162
	step [36/325], loss=90.0246
	step [37/325], loss=74.2431
	step [38/325], loss=65.6817
	step [39/325], loss=67.8174
	step [40/325], loss=68.7545
	step [41/325], loss=83.1240
	step [42/325], loss=75.5296
	step [43/325], loss=56.2135
	step [44/325], loss=74.4490
	step [45/325], loss=67.3616
	step [46/325], loss=60.9583
	step [47/325], loss=60.1152
	step [48/325], loss=84.3773
	step [49/325], loss=81.8433
	step [50/325], loss=68.9012
	step [51/325], loss=70.5306
	step [52/325], loss=81.8029
	step [53/325], loss=78.2229
	step [54/325], loss=74.5671
	step [55/325], loss=89.5572
	step [56/325], loss=52.4209
	step [57/325], loss=78.2407
	step [58/325], loss=64.0327
	step [59/325], loss=65.7377
	step [60/325], loss=53.5797
	step [61/325], loss=77.7096
	step [62/325], loss=66.7537
	step [63/325], loss=78.7617
	step [64/325], loss=89.0962
	step [65/325], loss=71.2245
	step [66/325], loss=72.9358
	step [67/325], loss=63.0413
	step [68/325], loss=82.0332
	step [69/325], loss=64.3876
	step [70/325], loss=73.2519
	step [71/325], loss=72.5711
	step [72/325], loss=77.5607
	step [73/325], loss=84.8441
	step [74/325], loss=74.4900
	step [75/325], loss=70.7329
	step [76/325], loss=73.7298
	step [77/325], loss=66.9802
	step [78/325], loss=77.2263
	step [79/325], loss=83.2202
	step [80/325], loss=82.4187
	step [81/325], loss=83.4739
	step [82/325], loss=69.2570
	step [83/325], loss=72.9196
	step [84/325], loss=81.2939
	step [85/325], loss=63.2921
	step [86/325], loss=78.6665
	step [87/325], loss=79.3388
	step [88/325], loss=67.6789
	step [89/325], loss=85.9513
	step [90/325], loss=66.8883
	step [91/325], loss=87.8360
	step [92/325], loss=75.6078
	step [93/325], loss=82.6734
	step [94/325], loss=71.7846
	step [95/325], loss=66.4081
	step [96/325], loss=57.3452
	step [97/325], loss=60.9408
	step [98/325], loss=79.2179
	step [99/325], loss=71.3141
	step [100/325], loss=77.5329
	step [101/325], loss=79.0292
	step [102/325], loss=82.0149
	step [103/325], loss=94.6422
	step [104/325], loss=61.6687
	step [105/325], loss=73.0972
	step [106/325], loss=59.6760
	step [107/325], loss=80.5051
	step [108/325], loss=82.2010
	step [109/325], loss=99.1504
	step [110/325], loss=73.4096
	step [111/325], loss=63.5918
	step [112/325], loss=93.9937
	step [113/325], loss=75.4189
	step [114/325], loss=80.3330
	step [115/325], loss=73.3292
	step [116/325], loss=80.5520
	step [117/325], loss=80.1830
	step [118/325], loss=80.2875
	step [119/325], loss=63.7008
	step [120/325], loss=73.5223
	step [121/325], loss=83.9613
	step [122/325], loss=70.6196
	step [123/325], loss=92.8894
	step [124/325], loss=88.0992
	step [125/325], loss=70.5107
	step [126/325], loss=80.4906
	step [127/325], loss=87.7136
	step [128/325], loss=59.5353
	step [129/325], loss=64.3307
	step [130/325], loss=92.9620
	step [131/325], loss=85.1361
	step [132/325], loss=66.2441
	step [133/325], loss=79.1799
	step [134/325], loss=77.5607
	step [135/325], loss=78.7444
	step [136/325], loss=76.4722
	step [137/325], loss=81.5478
	step [138/325], loss=55.7996
	step [139/325], loss=73.9656
	step [140/325], loss=85.6769
	step [141/325], loss=64.5580
	step [142/325], loss=69.3922
	step [143/325], loss=72.1148
	step [144/325], loss=88.4419
	step [145/325], loss=71.3928
	step [146/325], loss=79.5733
	step [147/325], loss=89.9202
	step [148/325], loss=77.0277
	step [149/325], loss=89.2291
	step [150/325], loss=62.8641
	step [151/325], loss=87.4449
	step [152/325], loss=95.0640
	step [153/325], loss=90.9069
	step [154/325], loss=90.3922
	step [155/325], loss=95.3634
	step [156/325], loss=69.6212
	step [157/325], loss=59.5744
	step [158/325], loss=71.0085
	step [159/325], loss=68.3658
	step [160/325], loss=62.8317
	step [161/325], loss=72.7715
	step [162/325], loss=82.2678
	step [163/325], loss=66.7246
	step [164/325], loss=73.0888
	step [165/325], loss=72.1974
	step [166/325], loss=64.8332
	step [167/325], loss=78.8076
	step [168/325], loss=59.1353
	step [169/325], loss=83.4707
	step [170/325], loss=82.7239
	step [171/325], loss=67.7239
	step [172/325], loss=67.5397
	step [173/325], loss=73.7568
	step [174/325], loss=81.9982
	step [175/325], loss=85.0024
	step [176/325], loss=65.6221
	step [177/325], loss=85.0376
	step [178/325], loss=62.2638
	step [179/325], loss=67.8464
	step [180/325], loss=69.4931
	step [181/325], loss=85.4708
	step [182/325], loss=69.1726
	step [183/325], loss=68.6134
	step [184/325], loss=78.4397
	step [185/325], loss=76.1862
	step [186/325], loss=83.4266
	step [187/325], loss=72.3394
	step [188/325], loss=70.7144
	step [189/325], loss=74.6921
	step [190/325], loss=82.4462
	step [191/325], loss=54.9710
	step [192/325], loss=79.1078
	step [193/325], loss=68.7872
	step [194/325], loss=85.1739
	step [195/325], loss=85.3533
	step [196/325], loss=90.0291
	step [197/325], loss=65.8501
	step [198/325], loss=65.0505
	step [199/325], loss=66.1235
	step [200/325], loss=79.8836
	step [201/325], loss=70.5979
	step [202/325], loss=78.9865
	step [203/325], loss=83.7558
	step [204/325], loss=71.0667
	step [205/325], loss=82.9654
	step [206/325], loss=56.5378
	step [207/325], loss=88.4954
	step [208/325], loss=76.0901
	step [209/325], loss=70.5420
	step [210/325], loss=60.0140
	step [211/325], loss=65.3596
	step [212/325], loss=74.2795
	step [213/325], loss=86.6974
	step [214/325], loss=72.0402
	step [215/325], loss=82.7194
	step [216/325], loss=67.1100
	step [217/325], loss=68.2743
	step [218/325], loss=83.0361
	step [219/325], loss=83.7633
	step [220/325], loss=81.2540
	step [221/325], loss=74.3891
	step [222/325], loss=66.9127
	step [223/325], loss=70.6563
	step [224/325], loss=91.8974
	step [225/325], loss=71.9615
	step [226/325], loss=77.8997
	step [227/325], loss=77.6678
	step [228/325], loss=75.6346
	step [229/325], loss=73.3269
	step [230/325], loss=71.7086
	step [231/325], loss=81.1410
	step [232/325], loss=68.9106
	step [233/325], loss=76.3255
	step [234/325], loss=79.8863
	step [235/325], loss=75.7221
	step [236/325], loss=76.6537
	step [237/325], loss=74.2460
	step [238/325], loss=72.1909
	step [239/325], loss=69.3706
	step [240/325], loss=84.3263
	step [241/325], loss=78.9693
	step [242/325], loss=82.7205
	step [243/325], loss=79.9299
	step [244/325], loss=61.6400
	step [245/325], loss=72.3965
	step [246/325], loss=96.1288
	step [247/325], loss=75.1005
	step [248/325], loss=57.6796
	step [249/325], loss=80.7003
	step [250/325], loss=80.3346
	step [251/325], loss=67.9520
	step [252/325], loss=70.9850
	step [253/325], loss=74.1210
	step [254/325], loss=75.7840
	step [255/325], loss=82.7941
	step [256/325], loss=70.3004
	step [257/325], loss=82.9075
	step [258/325], loss=71.4286
	step [259/325], loss=81.4413
	step [260/325], loss=92.2746
	step [261/325], loss=68.3648
	step [262/325], loss=66.1961
	step [263/325], loss=77.8032
	step [264/325], loss=84.6691
	step [265/325], loss=71.1955
	step [266/325], loss=70.0453
	step [267/325], loss=78.3343
	step [268/325], loss=89.2759
	step [269/325], loss=73.1131
	step [270/325], loss=74.4154
	step [271/325], loss=71.2974
	step [272/325], loss=61.5035
	step [273/325], loss=91.0275
	step [274/325], loss=72.8912
	step [275/325], loss=80.4140
	step [276/325], loss=58.9302
	step [277/325], loss=79.1306
	step [278/325], loss=60.3921
	step [279/325], loss=56.4579
	step [280/325], loss=59.4623
	step [281/325], loss=67.5352
	step [282/325], loss=68.2957
	step [283/325], loss=82.2482
	step [284/325], loss=81.2753
	step [285/325], loss=78.5082
	step [286/325], loss=71.1169
	step [287/325], loss=64.2222
	step [288/325], loss=71.4968
	step [289/325], loss=65.4898
	step [290/325], loss=67.6926
	step [291/325], loss=83.0105
	step [292/325], loss=67.8543
	step [293/325], loss=69.2380
	step [294/325], loss=71.0749
	step [295/325], loss=62.3422
	step [296/325], loss=68.0164
	step [297/325], loss=87.2352
	step [298/325], loss=81.5004
	step [299/325], loss=71.7871
	step [300/325], loss=75.1746
	step [301/325], loss=67.3033
	step [302/325], loss=72.2748
	step [303/325], loss=77.2930
	step [304/325], loss=65.2308
	step [305/325], loss=82.6208
	step [306/325], loss=73.6322
	step [307/325], loss=60.5412
	step [308/325], loss=66.0083
	step [309/325], loss=75.1833
	step [310/325], loss=82.2059
	step [311/325], loss=85.2404
	step [312/325], loss=70.1600
	step [313/325], loss=81.6877
	step [314/325], loss=74.5618
	step [315/325], loss=83.4941
	step [316/325], loss=68.6515
	step [317/325], loss=90.8454
	step [318/325], loss=78.1039
	step [319/325], loss=77.3035
	step [320/325], loss=93.3562
	step [321/325], loss=72.5890
	step [322/325], loss=76.6824
	step [323/325], loss=71.3326
	step [324/325], loss=73.6093
	step [325/325], loss=30.9202
	Evaluating
	loss=0.0177, precision=0.4807, recall=0.8727, f1=0.6199
Training epoch 12
	step [1/325], loss=68.4868
	step [2/325], loss=70.7617
	step [3/325], loss=61.6623
	step [4/325], loss=77.7002
	step [5/325], loss=65.9270
	step [6/325], loss=88.5551
	step [7/325], loss=93.8391
	step [8/325], loss=86.6071
	step [9/325], loss=75.2221
	step [10/325], loss=76.3579
	step [11/325], loss=82.2447
	step [12/325], loss=59.9891
	step [13/325], loss=69.2839
	step [14/325], loss=81.2864
	step [15/325], loss=73.2295
	step [16/325], loss=54.0107
	step [17/325], loss=73.1567
	step [18/325], loss=65.3187
	step [19/325], loss=71.3230
	step [20/325], loss=74.8736
	step [21/325], loss=70.7450
	step [22/325], loss=86.2179
	step [23/325], loss=62.6148
	step [24/325], loss=68.1280
	step [25/325], loss=75.9995
	step [26/325], loss=93.6064
	step [27/325], loss=61.7287
	step [28/325], loss=80.8980
	step [29/325], loss=75.1230
	step [30/325], loss=65.5274
	step [31/325], loss=83.5442
	step [32/325], loss=62.9582
	step [33/325], loss=74.9802
	step [34/325], loss=86.1914
	step [35/325], loss=63.8958
	step [36/325], loss=77.1798
	step [37/325], loss=67.3366
	step [38/325], loss=58.8350
	step [39/325], loss=71.3650
	step [40/325], loss=85.9176
	step [41/325], loss=65.4885
	step [42/325], loss=85.1995
	step [43/325], loss=78.8627
	step [44/325], loss=71.2543
	step [45/325], loss=73.3174
	step [46/325], loss=87.4204
	step [47/325], loss=69.6664
	step [48/325], loss=68.4965
	step [49/325], loss=85.2545
	step [50/325], loss=66.4265
	step [51/325], loss=76.0903
	step [52/325], loss=67.4925
	step [53/325], loss=67.5411
	step [54/325], loss=83.9738
	step [55/325], loss=80.6082
	step [56/325], loss=91.3844
	step [57/325], loss=70.1935
	step [58/325], loss=72.8907
	step [59/325], loss=80.4405
	step [60/325], loss=69.1749
	step [61/325], loss=63.3734
	step [62/325], loss=69.0833
	step [63/325], loss=84.9852
	step [64/325], loss=67.2131
	step [65/325], loss=67.6688
	step [66/325], loss=83.9277
	step [67/325], loss=69.2905
	step [68/325], loss=69.0008
	step [69/325], loss=62.7458
	step [70/325], loss=71.7268
	step [71/325], loss=74.0541
	step [72/325], loss=69.1789
	step [73/325], loss=91.9715
	step [74/325], loss=74.1711
	step [75/325], loss=70.8865
	step [76/325], loss=71.1858
	step [77/325], loss=64.3986
	step [78/325], loss=79.9213
	step [79/325], loss=78.3276
	step [80/325], loss=71.4969
	step [81/325], loss=86.1180
	step [82/325], loss=65.2275
	step [83/325], loss=73.1989
	step [84/325], loss=58.8949
	step [85/325], loss=91.1330
	step [86/325], loss=65.3814
	step [87/325], loss=65.0871
	step [88/325], loss=88.3074
	step [89/325], loss=93.9406
	step [90/325], loss=80.5821
	step [91/325], loss=78.6153
	step [92/325], loss=80.0753
	step [93/325], loss=90.0104
	step [94/325], loss=77.9113
	step [95/325], loss=70.0408
	step [96/325], loss=65.7806
	step [97/325], loss=87.8673
	step [98/325], loss=74.9131
	step [99/325], loss=71.5561
	step [100/325], loss=66.6932
	step [101/325], loss=76.0275
	step [102/325], loss=71.2314
	step [103/325], loss=65.9285
	step [104/325], loss=72.0248
	step [105/325], loss=68.9738
	step [106/325], loss=60.8807
	step [107/325], loss=74.5508
	step [108/325], loss=65.8502
	step [109/325], loss=67.6123
	step [110/325], loss=70.2092
	step [111/325], loss=72.1856
	step [112/325], loss=67.3277
	step [113/325], loss=83.5802
	step [114/325], loss=67.8871
	step [115/325], loss=60.1618
	step [116/325], loss=83.2872
	step [117/325], loss=66.3769
	step [118/325], loss=68.9193
	step [119/325], loss=79.8608
	step [120/325], loss=61.8442
	step [121/325], loss=70.4870
	step [122/325], loss=78.0768
	step [123/325], loss=73.1598
	step [124/325], loss=63.7414
	step [125/325], loss=66.4896
	step [126/325], loss=80.6630
	step [127/325], loss=70.9663
	step [128/325], loss=87.6264
	step [129/325], loss=58.4240
	step [130/325], loss=90.8802
	step [131/325], loss=71.6923
	step [132/325], loss=76.3560
	step [133/325], loss=78.8314
	step [134/325], loss=61.6540
	step [135/325], loss=58.6367
	step [136/325], loss=63.8219
	step [137/325], loss=57.1759
	step [138/325], loss=82.2315
	step [139/325], loss=91.8279
	step [140/325], loss=61.7995
	step [141/325], loss=71.8250
	step [142/325], loss=71.1764
	step [143/325], loss=93.9051
	step [144/325], loss=68.5266
	step [145/325], loss=79.8683
	step [146/325], loss=91.1194
	step [147/325], loss=79.8767
	step [148/325], loss=86.7817
	step [149/325], loss=73.4982
	step [150/325], loss=94.1366
	step [151/325], loss=66.1656
	step [152/325], loss=76.7820
	step [153/325], loss=73.5354
	step [154/325], loss=70.5244
	step [155/325], loss=81.1591
	step [156/325], loss=69.2121
	step [157/325], loss=71.3562
	step [158/325], loss=67.3023
	step [159/325], loss=71.5718
	step [160/325], loss=68.6007
	step [161/325], loss=77.7939
	step [162/325], loss=69.4057
	step [163/325], loss=83.6558
	step [164/325], loss=69.4193
	step [165/325], loss=75.4348
	step [166/325], loss=80.8920
	step [167/325], loss=83.3944
	step [168/325], loss=57.1208
	step [169/325], loss=107.3764
	step [170/325], loss=81.6767
	step [171/325], loss=79.7756
	step [172/325], loss=78.3484
	step [173/325], loss=86.4140
	step [174/325], loss=76.4023
	step [175/325], loss=91.8656
	step [176/325], loss=63.0614
	step [177/325], loss=76.0438
	step [178/325], loss=73.2692
	step [179/325], loss=72.1843
	step [180/325], loss=66.1221
	step [181/325], loss=65.8106
	step [182/325], loss=61.3708
	step [183/325], loss=67.2741
	step [184/325], loss=68.5066
	step [185/325], loss=80.0271
	step [186/325], loss=76.9067
	step [187/325], loss=84.0690
	step [188/325], loss=88.7409
	step [189/325], loss=63.2424
	step [190/325], loss=97.1579
	step [191/325], loss=64.3720
	step [192/325], loss=58.4068
	step [193/325], loss=59.0114
	step [194/325], loss=87.8810
	step [195/325], loss=64.2184
	step [196/325], loss=76.9428
	step [197/325], loss=75.3822
	step [198/325], loss=95.0174
	step [199/325], loss=78.4718
	step [200/325], loss=82.4029
	step [201/325], loss=71.4554
	step [202/325], loss=81.6243
	step [203/325], loss=56.1056
	step [204/325], loss=68.0447
	step [205/325], loss=82.5399
	step [206/325], loss=63.1376
	step [207/325], loss=71.5967
	step [208/325], loss=76.5216
	step [209/325], loss=61.7649
	step [210/325], loss=81.5425
	step [211/325], loss=68.4289
	step [212/325], loss=83.0751
	step [213/325], loss=76.2632
	step [214/325], loss=66.2081
	step [215/325], loss=74.7436
	step [216/325], loss=84.4000
	step [217/325], loss=84.4935
	step [218/325], loss=75.5827
	step [219/325], loss=66.5567
	step [220/325], loss=69.4405
	step [221/325], loss=85.9578
	step [222/325], loss=71.1880
	step [223/325], loss=74.6022
	step [224/325], loss=71.4354
	step [225/325], loss=85.2225
	step [226/325], loss=90.2370
	step [227/325], loss=83.9428
	step [228/325], loss=79.9743
	step [229/325], loss=77.8393
	step [230/325], loss=61.1322
	step [231/325], loss=65.8655
	step [232/325], loss=72.0245
	step [233/325], loss=88.6888
	step [234/325], loss=78.2412
	step [235/325], loss=85.2168
	step [236/325], loss=104.3293
	step [237/325], loss=76.3123
	step [238/325], loss=67.2797
	step [239/325], loss=69.6502
	step [240/325], loss=79.8538
	step [241/325], loss=78.0351
	step [242/325], loss=59.7478
	step [243/325], loss=79.2193
	step [244/325], loss=78.8777
	step [245/325], loss=93.9111
	step [246/325], loss=71.5699
	step [247/325], loss=73.2124
	step [248/325], loss=73.9010
	step [249/325], loss=74.5100
	step [250/325], loss=78.8851
	step [251/325], loss=71.6103
	step [252/325], loss=65.2822
	step [253/325], loss=76.7923
	step [254/325], loss=81.7354
	step [255/325], loss=78.9582
	step [256/325], loss=71.7989
	step [257/325], loss=70.7062
	step [258/325], loss=66.4883
	step [259/325], loss=87.5404
	step [260/325], loss=69.4137
	step [261/325], loss=74.6044
	step [262/325], loss=61.1280
	step [263/325], loss=61.2759
	step [264/325], loss=63.0553
	step [265/325], loss=86.5120
	step [266/325], loss=73.4680
	step [267/325], loss=71.7420
	step [268/325], loss=65.5186
	step [269/325], loss=65.4361
	step [270/325], loss=90.9946
	step [271/325], loss=70.6433
	step [272/325], loss=83.4924
	step [273/325], loss=63.4096
	step [274/325], loss=69.7643
	step [275/325], loss=74.4022
	step [276/325], loss=70.1875
	step [277/325], loss=67.2992
	step [278/325], loss=61.6607
	step [279/325], loss=72.2880
	step [280/325], loss=71.4562
	step [281/325], loss=68.1796
	step [282/325], loss=70.7666
	step [283/325], loss=58.1912
	step [284/325], loss=77.8949
	step [285/325], loss=77.1730
	step [286/325], loss=66.9932
	step [287/325], loss=96.7250
	step [288/325], loss=67.6354
	step [289/325], loss=78.7730
	step [290/325], loss=70.5469
	step [291/325], loss=79.1353
	step [292/325], loss=97.6439
	step [293/325], loss=62.3787
	step [294/325], loss=72.3616
	step [295/325], loss=74.4889
	step [296/325], loss=81.5358
	step [297/325], loss=54.5885
	step [298/325], loss=76.6838
	step [299/325], loss=84.5592
	step [300/325], loss=66.7949
	step [301/325], loss=63.4002
	step [302/325], loss=81.6608
	step [303/325], loss=77.5240
	step [304/325], loss=50.7376
	step [305/325], loss=82.3139
	step [306/325], loss=80.4936
	step [307/325], loss=73.7527
	step [308/325], loss=92.0004
	step [309/325], loss=62.2975
	step [310/325], loss=83.7652
	step [311/325], loss=53.5755
	step [312/325], loss=72.3004
	step [313/325], loss=62.1015
	step [314/325], loss=61.3159
	step [315/325], loss=66.4518
	step [316/325], loss=56.3074
	step [317/325], loss=73.5410
	step [318/325], loss=83.6987
	step [319/325], loss=74.7627
	step [320/325], loss=72.8062
	step [321/325], loss=81.4152
	step [322/325], loss=75.2053
	step [323/325], loss=92.0373
	step [324/325], loss=74.8989
	step [325/325], loss=59.6752
	Evaluating
	loss=0.0166, precision=0.3745, recall=0.9384, f1=0.5353
Training epoch 13
	step [1/325], loss=64.6391
	step [2/325], loss=73.6480
	step [3/325], loss=58.4009
	step [4/325], loss=64.5044
	step [5/325], loss=72.2647
	step [6/325], loss=84.0550
	step [7/325], loss=69.1179
	step [8/325], loss=78.1084
	step [9/325], loss=70.6559
	step [10/325], loss=89.4468
	step [11/325], loss=60.2130
	step [12/325], loss=62.9975
	step [13/325], loss=93.3370
	step [14/325], loss=77.8012
	step [15/325], loss=70.2982
	step [16/325], loss=83.2136
	step [17/325], loss=72.1038
	step [18/325], loss=71.6307
	step [19/325], loss=64.7645
	step [20/325], loss=73.4540
	step [21/325], loss=67.2882
	step [22/325], loss=73.9045
	step [23/325], loss=86.0925
	step [24/325], loss=77.0974
	step [25/325], loss=49.7552
	step [26/325], loss=72.6894
	step [27/325], loss=84.7461
	step [28/325], loss=67.3486
	step [29/325], loss=64.0567
	step [30/325], loss=79.1235
	step [31/325], loss=64.8315
	step [32/325], loss=98.8400
	step [33/325], loss=93.2255
	step [34/325], loss=66.5747
	step [35/325], loss=72.6162
	step [36/325], loss=67.6565
	step [37/325], loss=59.9255
	step [38/325], loss=70.8580
	step [39/325], loss=71.1108
	step [40/325], loss=61.1378
	step [41/325], loss=90.1461
	step [42/325], loss=70.0055
	step [43/325], loss=77.3345
	step [44/325], loss=71.9200
	step [45/325], loss=89.9129
	step [46/325], loss=65.2236
	step [47/325], loss=67.6560
	step [48/325], loss=60.2500
	step [49/325], loss=56.7094
	step [50/325], loss=74.3736
	step [51/325], loss=71.4654
	step [52/325], loss=74.4161
	step [53/325], loss=76.8970
	step [54/325], loss=73.1832
	step [55/325], loss=77.5737
	step [56/325], loss=62.7035
	step [57/325], loss=60.4249
	step [58/325], loss=60.3868
	step [59/325], loss=70.3839
	step [60/325], loss=77.7344
	step [61/325], loss=67.0699
	step [62/325], loss=81.3626
	step [63/325], loss=73.8670
	step [64/325], loss=66.2841
	step [65/325], loss=78.4190
	step [66/325], loss=62.5730
	step [67/325], loss=75.6488
	step [68/325], loss=80.2126
	step [69/325], loss=78.9421
	step [70/325], loss=61.7452
	step [71/325], loss=74.0868
	step [72/325], loss=57.4911
	step [73/325], loss=63.6269
	step [74/325], loss=79.2574
	step [75/325], loss=76.1900
	step [76/325], loss=73.2454
	step [77/325], loss=75.2214
	step [78/325], loss=87.0224
	step [79/325], loss=57.3608
	step [80/325], loss=65.7793
	step [81/325], loss=81.3205
	step [82/325], loss=79.8370
	step [83/325], loss=76.8502
	step [84/325], loss=59.4373
	step [85/325], loss=64.2307
	step [86/325], loss=90.0198
	step [87/325], loss=65.0891
	step [88/325], loss=91.7512
	step [89/325], loss=67.7028
	step [90/325], loss=80.4316
	step [91/325], loss=74.4173
	step [92/325], loss=68.9542
	step [93/325], loss=82.1498
	step [94/325], loss=67.6593
	step [95/325], loss=75.9870
	step [96/325], loss=65.0477
	step [97/325], loss=63.9066
	step [98/325], loss=77.6147
	step [99/325], loss=83.3433
	step [100/325], loss=70.9816
	step [101/325], loss=68.6608
	step [102/325], loss=82.2337
	step [103/325], loss=75.4830
	step [104/325], loss=94.9173
	step [105/325], loss=77.3424
	step [106/325], loss=70.9776
	step [107/325], loss=73.7770
	step [108/325], loss=67.6989
	step [109/325], loss=99.0857
	step [110/325], loss=77.3968
	step [111/325], loss=48.4141
	step [112/325], loss=80.6038
	step [113/325], loss=74.3897
	step [114/325], loss=56.1987
	step [115/325], loss=97.7440
	step [116/325], loss=78.0900
	step [117/325], loss=69.5692
	step [118/325], loss=75.7744
	step [119/325], loss=68.7069
	step [120/325], loss=63.3464
	step [121/325], loss=74.2533
	step [122/325], loss=74.4433
	step [123/325], loss=70.9908
	step [124/325], loss=69.9438
	step [125/325], loss=79.6084
	step [126/325], loss=66.6766
	step [127/325], loss=60.1241
	step [128/325], loss=58.4981
	step [129/325], loss=57.4813
	step [130/325], loss=64.4701
	step [131/325], loss=79.5305
	step [132/325], loss=64.0512
	step [133/325], loss=87.0363
	step [134/325], loss=68.8283
	step [135/325], loss=73.0003
	step [136/325], loss=88.5888
	step [137/325], loss=75.4012
	step [138/325], loss=59.9042
	step [139/325], loss=67.3493
	step [140/325], loss=94.0893
	step [141/325], loss=76.0300
	step [142/325], loss=77.0575
	step [143/325], loss=75.8510
	step [144/325], loss=61.2535
	step [145/325], loss=73.8733
	step [146/325], loss=64.2785
	step [147/325], loss=70.5605
	step [148/325], loss=80.9184
	step [149/325], loss=59.0378
	step [150/325], loss=58.2411
	step [151/325], loss=50.9830
	step [152/325], loss=72.6230
	step [153/325], loss=81.4891
	step [154/325], loss=86.0697
	step [155/325], loss=69.2428
	step [156/325], loss=59.3626
	step [157/325], loss=58.7627
	step [158/325], loss=73.1512
	step [159/325], loss=85.9174
	step [160/325], loss=69.2980
	step [161/325], loss=79.7013
	step [162/325], loss=76.7455
	step [163/325], loss=98.8793
	step [164/325], loss=90.0625
	step [165/325], loss=68.0093
	step [166/325], loss=83.2491
	step [167/325], loss=67.2384
	step [168/325], loss=85.1167
	step [169/325], loss=99.3912
	step [170/325], loss=75.0265
	step [171/325], loss=80.7910
	step [172/325], loss=61.9113
	step [173/325], loss=85.4794
	step [174/325], loss=73.1069
	step [175/325], loss=57.0511
	step [176/325], loss=67.0115
	step [177/325], loss=57.3187
	step [178/325], loss=76.5276
	step [179/325], loss=87.9172
	step [180/325], loss=64.1777
	step [181/325], loss=65.2286
	step [182/325], loss=75.8111
	step [183/325], loss=68.3364
	step [184/325], loss=78.5598
	step [185/325], loss=45.3231
	step [186/325], loss=76.1081
	step [187/325], loss=70.8630
	step [188/325], loss=87.0670
	step [189/325], loss=71.3042
	step [190/325], loss=85.7362
	step [191/325], loss=76.2894
	step [192/325], loss=76.1817
	step [193/325], loss=72.5862
	step [194/325], loss=85.3219
	step [195/325], loss=85.0141
	step [196/325], loss=78.8181
	step [197/325], loss=60.5240
	step [198/325], loss=76.2110
	step [199/325], loss=88.4750
	step [200/325], loss=70.9538
	step [201/325], loss=59.1811
	step [202/325], loss=61.7914
	step [203/325], loss=68.4738
	step [204/325], loss=75.1492
	step [205/325], loss=83.4465
	step [206/325], loss=70.2889
	step [207/325], loss=71.1889
	step [208/325], loss=71.6773
	step [209/325], loss=62.3484
	step [210/325], loss=77.2319
	step [211/325], loss=62.8501
	step [212/325], loss=80.4189
	step [213/325], loss=71.0929
	step [214/325], loss=54.7277
	step [215/325], loss=61.0977
	step [216/325], loss=72.9272
	step [217/325], loss=75.0438
	step [218/325], loss=62.5812
	step [219/325], loss=89.9548
	step [220/325], loss=56.6037
	step [221/325], loss=64.5360
	step [222/325], loss=78.4880
	step [223/325], loss=62.1395
	step [224/325], loss=79.4226
	step [225/325], loss=68.0990
	step [226/325], loss=87.4546
	step [227/325], loss=81.7236
	step [228/325], loss=89.0096
	step [229/325], loss=65.2272
	step [230/325], loss=71.5025
	step [231/325], loss=82.1807
	step [232/325], loss=81.2179
	step [233/325], loss=66.4843
	step [234/325], loss=68.1745
	step [235/325], loss=65.0734
	step [236/325], loss=78.0553
	step [237/325], loss=80.7910
	step [238/325], loss=72.4453
	step [239/325], loss=79.0812
	step [240/325], loss=66.6214
	step [241/325], loss=59.3487
	step [242/325], loss=68.9811
	step [243/325], loss=80.9515
	step [244/325], loss=64.9910
	step [245/325], loss=97.9613
	step [246/325], loss=75.4655
	step [247/325], loss=79.3803
	step [248/325], loss=80.1305
	step [249/325], loss=84.8780
	step [250/325], loss=78.0724
	step [251/325], loss=77.9371
	step [252/325], loss=56.5875
	step [253/325], loss=62.6901
	step [254/325], loss=93.0470
	step [255/325], loss=83.9330
	step [256/325], loss=74.2200
	step [257/325], loss=74.5022
	step [258/325], loss=87.5708
	step [259/325], loss=69.1758
	step [260/325], loss=81.7351
	step [261/325], loss=75.8596
	step [262/325], loss=77.1079
	step [263/325], loss=80.4222
	step [264/325], loss=68.3373
	step [265/325], loss=88.8864
	step [266/325], loss=68.5249
	step [267/325], loss=68.4522
	step [268/325], loss=66.3180
	step [269/325], loss=57.5692
	step [270/325], loss=89.8291
	step [271/325], loss=52.6790
	step [272/325], loss=77.7416
	step [273/325], loss=63.9337
	step [274/325], loss=63.9543
	step [275/325], loss=74.9050
	step [276/325], loss=77.7229
	step [277/325], loss=72.9364
	step [278/325], loss=97.1500
	step [279/325], loss=72.6553
	step [280/325], loss=80.8048
	step [281/325], loss=69.4324
	step [282/325], loss=61.4005
	step [283/325], loss=66.2619
	step [284/325], loss=61.7719
	step [285/325], loss=66.4823
	step [286/325], loss=94.0505
	step [287/325], loss=82.5276
	step [288/325], loss=80.4825
	step [289/325], loss=68.9954
	step [290/325], loss=63.4464
	step [291/325], loss=78.1617
	step [292/325], loss=100.0039
	step [293/325], loss=77.5456
	step [294/325], loss=63.9095
	step [295/325], loss=70.6654
	step [296/325], loss=72.9257
	step [297/325], loss=59.5010
	step [298/325], loss=85.7998
	step [299/325], loss=65.7514
	step [300/325], loss=69.7987
	step [301/325], loss=72.4094
	step [302/325], loss=83.4986
	step [303/325], loss=76.0376
	step [304/325], loss=70.3264
	step [305/325], loss=92.7445
	step [306/325], loss=64.8038
	step [307/325], loss=83.4622
	step [308/325], loss=69.1725
	step [309/325], loss=67.6862
	step [310/325], loss=86.1618
	step [311/325], loss=75.5553
	step [312/325], loss=77.6293
	step [313/325], loss=70.4146
	step [314/325], loss=58.6172
	step [315/325], loss=69.2617
	step [316/325], loss=71.2211
	step [317/325], loss=64.9709
	step [318/325], loss=81.4372
	step [319/325], loss=68.2835
	step [320/325], loss=71.0337
	step [321/325], loss=79.9698
	step [322/325], loss=73.7032
	step [323/325], loss=69.4556
	step [324/325], loss=76.2743
	step [325/325], loss=35.0940
	Evaluating
	loss=0.0141, precision=0.4176, recall=0.9109, f1=0.5727
Training epoch 14
	step [1/325], loss=91.1519
	step [2/325], loss=59.1694
	step [3/325], loss=86.1971
	step [4/325], loss=70.8119
	step [5/325], loss=68.8936
	step [6/325], loss=62.6026
	step [7/325], loss=82.6969
	step [8/325], loss=69.1235
	step [9/325], loss=69.7404
	step [10/325], loss=66.3293
	step [11/325], loss=62.1573
	step [12/325], loss=72.3288
	step [13/325], loss=83.1716
	step [14/325], loss=85.0839
	step [15/325], loss=72.2135
	step [16/325], loss=73.2340
	step [17/325], loss=64.5129
	step [18/325], loss=91.2642
	step [19/325], loss=72.5490
	step [20/325], loss=73.8781
	step [21/325], loss=85.3508
	step [22/325], loss=65.7195
	step [23/325], loss=72.8834
	step [24/325], loss=79.7561
	step [25/325], loss=70.4856
	step [26/325], loss=64.9904
	step [27/325], loss=76.3925
	step [28/325], loss=63.4883
	step [29/325], loss=61.0139
	step [30/325], loss=63.6335
	step [31/325], loss=63.4375
	step [32/325], loss=85.7242
	step [33/325], loss=65.7222
	step [34/325], loss=73.3829
	step [35/325], loss=66.9313
	step [36/325], loss=76.9772
	step [37/325], loss=72.2232
	step [38/325], loss=75.6338
	step [39/325], loss=66.6135
	step [40/325], loss=64.2466
	step [41/325], loss=80.8907
	step [42/325], loss=92.6212
	step [43/325], loss=66.0954
	step [44/325], loss=69.6780
	step [45/325], loss=68.4050
	step [46/325], loss=67.6089
	step [47/325], loss=76.1863
	step [48/325], loss=75.6536
	step [49/325], loss=74.6802
	step [50/325], loss=60.1680
	step [51/325], loss=69.7614
	step [52/325], loss=75.4307
	step [53/325], loss=49.3627
	step [54/325], loss=81.5566
	step [55/325], loss=83.0031
	step [56/325], loss=80.4176
	step [57/325], loss=72.5369
	step [58/325], loss=62.8326
	step [59/325], loss=67.1052
	step [60/325], loss=67.2870
	step [61/325], loss=60.6721
	step [62/325], loss=75.9808
	step [63/325], loss=73.7112
	step [64/325], loss=65.2839
	step [65/325], loss=83.5643
	step [66/325], loss=66.6521
	step [67/325], loss=76.3349
	step [68/325], loss=67.4131
	step [69/325], loss=76.4677
	step [70/325], loss=63.8477
	step [71/325], loss=99.7330
	step [72/325], loss=79.5257
	step [73/325], loss=63.4869
	step [74/325], loss=73.0148
	step [75/325], loss=58.0222
	step [76/325], loss=85.7039
	step [77/325], loss=59.3909
	step [78/325], loss=62.4949
	step [79/325], loss=62.3783
	step [80/325], loss=84.6491
	step [81/325], loss=64.8814
	step [82/325], loss=73.8835
	step [83/325], loss=68.4377
	step [84/325], loss=76.9884
	step [85/325], loss=73.0133
	step [86/325], loss=63.6613
	step [87/325], loss=77.8666
	step [88/325], loss=65.8210
	step [89/325], loss=80.1908
	step [90/325], loss=90.6383
	step [91/325], loss=65.8546
	step [92/325], loss=65.2708
	step [93/325], loss=66.3486
	step [94/325], loss=77.2819
	step [95/325], loss=82.2095
	step [96/325], loss=84.9941
	step [97/325], loss=68.0130
	step [98/325], loss=68.2489
	step [99/325], loss=77.8041
	step [100/325], loss=75.3800
	step [101/325], loss=71.0645
	step [102/325], loss=87.0123
	step [103/325], loss=67.8877
	step [104/325], loss=62.2198
	step [105/325], loss=76.5897
	step [106/325], loss=74.2933
	step [107/325], loss=81.5644
	step [108/325], loss=71.6557
	step [109/325], loss=76.5333
	step [110/325], loss=69.8089
	step [111/325], loss=80.2231
	step [112/325], loss=82.3856
	step [113/325], loss=60.7098
	step [114/325], loss=82.2415
	step [115/325], loss=81.7032
	step [116/325], loss=55.3823
	step [117/325], loss=67.7008
	step [118/325], loss=74.1715
	step [119/325], loss=61.7999
	step [120/325], loss=78.8673
	step [121/325], loss=63.5178
	step [122/325], loss=74.5610
	step [123/325], loss=80.2069
	step [124/325], loss=88.4950
	step [125/325], loss=63.8837
	step [126/325], loss=57.8083
	step [127/325], loss=75.7501
	step [128/325], loss=66.6622
	step [129/325], loss=80.2798
	step [130/325], loss=74.1437
	step [131/325], loss=73.4386
	step [132/325], loss=73.4594
	step [133/325], loss=68.7437
	step [134/325], loss=71.6048
	step [135/325], loss=67.9100
	step [136/325], loss=83.0409
	step [137/325], loss=61.7223
	step [138/325], loss=91.5795
	step [139/325], loss=70.9040
	step [140/325], loss=69.7640
	step [141/325], loss=76.4882
	step [142/325], loss=72.0615
	step [143/325], loss=63.1572
	step [144/325], loss=77.1918
	step [145/325], loss=77.4578
	step [146/325], loss=78.5235
	step [147/325], loss=70.7854
	step [148/325], loss=70.2723
	step [149/325], loss=56.1443
	step [150/325], loss=85.7016
	step [151/325], loss=79.3230
	step [152/325], loss=71.5273
	step [153/325], loss=73.1273
	step [154/325], loss=73.0904
	step [155/325], loss=76.0516
	step [156/325], loss=72.3162
	step [157/325], loss=82.8240
	step [158/325], loss=80.9905
	step [159/325], loss=62.3100
	step [160/325], loss=85.0614
	step [161/325], loss=81.4951
	step [162/325], loss=77.9741
	step [163/325], loss=54.2231
	step [164/325], loss=71.0092
	step [165/325], loss=79.6142
	step [166/325], loss=84.8391
	step [167/325], loss=72.1321
	step [168/325], loss=76.7784
	step [169/325], loss=81.5537
	step [170/325], loss=56.8277
	step [171/325], loss=66.9784
	step [172/325], loss=80.6358
	step [173/325], loss=75.5271
	step [174/325], loss=61.9519
	step [175/325], loss=68.9138
	step [176/325], loss=70.3115
	step [177/325], loss=69.9091
	step [178/325], loss=77.0071
	step [179/325], loss=70.2015
	step [180/325], loss=85.0834
	step [181/325], loss=57.8959
	step [182/325], loss=75.0063
	step [183/325], loss=79.1890
	step [184/325], loss=70.2235
	step [185/325], loss=67.1237
	step [186/325], loss=68.6329
	step [187/325], loss=78.5301
	step [188/325], loss=86.0924
	step [189/325], loss=75.8452
	step [190/325], loss=81.3711
	step [191/325], loss=94.6745
	step [192/325], loss=78.0221
	step [193/325], loss=60.1926
	step [194/325], loss=106.8931
	step [195/325], loss=64.3456
	step [196/325], loss=80.5913
	step [197/325], loss=80.4414
	step [198/325], loss=57.9010
	step [199/325], loss=69.5345
	step [200/325], loss=72.2543
	step [201/325], loss=87.9723
	step [202/325], loss=68.5785
	step [203/325], loss=77.4836
	step [204/325], loss=80.5237
	step [205/325], loss=74.2074
	step [206/325], loss=66.9909
	step [207/325], loss=79.8384
	step [208/325], loss=65.2814
	step [209/325], loss=76.8843
	step [210/325], loss=75.3211
	step [211/325], loss=57.9841
	step [212/325], loss=56.0377
	step [213/325], loss=72.4939
	step [214/325], loss=67.5145
	step [215/325], loss=75.5855
	step [216/325], loss=74.8124
	step [217/325], loss=70.9647
	step [218/325], loss=73.7806
	step [219/325], loss=70.3701
	step [220/325], loss=80.6037
	step [221/325], loss=72.8236
	step [222/325], loss=52.8084
	step [223/325], loss=62.0920
	step [224/325], loss=75.1641
	step [225/325], loss=61.3412
	step [226/325], loss=59.6094
	step [227/325], loss=71.2279
	step [228/325], loss=59.4624
	step [229/325], loss=65.4293
	step [230/325], loss=62.4924
	step [231/325], loss=63.1514
	step [232/325], loss=64.4989
	step [233/325], loss=66.9953
	step [234/325], loss=67.2422
	step [235/325], loss=72.1457
	step [236/325], loss=69.5717
	step [237/325], loss=72.5229
	step [238/325], loss=71.5774
	step [239/325], loss=75.2867
	step [240/325], loss=56.8817
	step [241/325], loss=70.7359
	step [242/325], loss=77.6799
	step [243/325], loss=81.3453
	step [244/325], loss=55.8479
	step [245/325], loss=76.7181
	step [246/325], loss=87.9699
	step [247/325], loss=67.0927
	step [248/325], loss=74.5192
	step [249/325], loss=85.9803
	step [250/325], loss=69.7019
	step [251/325], loss=71.8815
	step [252/325], loss=70.6944
	step [253/325], loss=67.7456
	step [254/325], loss=67.1167
	step [255/325], loss=71.2232
	step [256/325], loss=78.5636
	step [257/325], loss=83.1387
	step [258/325], loss=64.2840
	step [259/325], loss=76.5047
	step [260/325], loss=80.3770
	step [261/325], loss=68.3627
	step [262/325], loss=67.8070
	step [263/325], loss=62.1593
	step [264/325], loss=82.8388
	step [265/325], loss=66.4122
	step [266/325], loss=59.6959
	step [267/325], loss=80.5123
	step [268/325], loss=72.6736
	step [269/325], loss=71.7989
	step [270/325], loss=72.7200
	step [271/325], loss=79.9687
	step [272/325], loss=74.0725
	step [273/325], loss=79.7184
	step [274/325], loss=60.3395
	step [275/325], loss=75.4476
	step [276/325], loss=60.2037
	step [277/325], loss=77.5190
	step [278/325], loss=55.1659
	step [279/325], loss=67.9219
	step [280/325], loss=70.1617
	step [281/325], loss=68.6460
	step [282/325], loss=72.8626
	step [283/325], loss=63.4024
	step [284/325], loss=71.8303
	step [285/325], loss=63.9274
	step [286/325], loss=68.6445
	step [287/325], loss=74.6799
	step [288/325], loss=59.3577
	step [289/325], loss=87.9918
	step [290/325], loss=62.2789
	step [291/325], loss=82.4964
	step [292/325], loss=60.9315
	step [293/325], loss=70.3486
	step [294/325], loss=83.6157
	step [295/325], loss=70.2300
	step [296/325], loss=86.4059
	step [297/325], loss=61.6206
	step [298/325], loss=71.2993
	step [299/325], loss=66.7554
	step [300/325], loss=72.2017
	step [301/325], loss=75.6176
	step [302/325], loss=79.1892
	step [303/325], loss=84.4574
	step [304/325], loss=78.0207
	step [305/325], loss=74.3378
	step [306/325], loss=74.7430
	step [307/325], loss=70.1427
	step [308/325], loss=72.3709
	step [309/325], loss=71.9211
	step [310/325], loss=67.7159
	step [311/325], loss=94.8826
	step [312/325], loss=70.1514
	step [313/325], loss=76.1000
	step [314/325], loss=55.7948
	step [315/325], loss=68.0572
	step [316/325], loss=73.1339
	step [317/325], loss=79.1411
	step [318/325], loss=88.7735
	step [319/325], loss=78.3901
	step [320/325], loss=78.5522
	step [321/325], loss=67.1611
	step [322/325], loss=78.1768
	step [323/325], loss=63.3079
	step [324/325], loss=78.0511
	step [325/325], loss=36.5484
	Evaluating
	loss=0.0145, precision=0.3859, recall=0.9235, f1=0.5443
Training epoch 15
	step [1/325], loss=75.1935
	step [2/325], loss=54.4200
	step [3/325], loss=74.4400
	step [4/325], loss=58.3125
	step [5/325], loss=85.3895
	step [6/325], loss=74.6302
	step [7/325], loss=81.1834
	step [8/325], loss=84.9811
	step [9/325], loss=73.4065
	step [10/325], loss=61.0664
	step [11/325], loss=76.3682
	step [12/325], loss=68.9186
	step [13/325], loss=54.1050
	step [14/325], loss=71.8034
	step [15/325], loss=79.1154
	step [16/325], loss=71.1866
	step [17/325], loss=68.1216
	step [18/325], loss=78.0300
	step [19/325], loss=78.4762
	step [20/325], loss=71.3134
	step [21/325], loss=76.7716
	step [22/325], loss=70.8923
	step [23/325], loss=62.4697
	step [24/325], loss=80.6153
	step [25/325], loss=77.4191
	step [26/325], loss=70.7727
	step [27/325], loss=79.7383
	step [28/325], loss=69.7595
	step [29/325], loss=71.7575
	step [30/325], loss=71.2123
	step [31/325], loss=60.2114
	step [32/325], loss=50.1578
	step [33/325], loss=75.5123
	step [34/325], loss=78.0096
	step [35/325], loss=77.4082
	step [36/325], loss=56.3376
	step [37/325], loss=83.1264
	step [38/325], loss=61.7874
	step [39/325], loss=67.3459
	step [40/325], loss=72.1178
	step [41/325], loss=63.5170
	step [42/325], loss=72.8610
	step [43/325], loss=82.3831
	step [44/325], loss=63.5823
	step [45/325], loss=81.0350
	step [46/325], loss=73.4148
	step [47/325], loss=76.0012
	step [48/325], loss=55.5208
	step [49/325], loss=60.8661
	step [50/325], loss=65.1004
	step [51/325], loss=60.0868
	step [52/325], loss=66.2079
	step [53/325], loss=63.5321
	step [54/325], loss=78.4483
	step [55/325], loss=59.8548
	step [56/325], loss=71.3928
	step [57/325], loss=79.7836
	step [58/325], loss=86.7279
	step [59/325], loss=66.5903
	step [60/325], loss=63.1419
	step [61/325], loss=72.7158
	step [62/325], loss=79.9708
	step [63/325], loss=55.8589
	step [64/325], loss=61.1269
	step [65/325], loss=61.0500
	step [66/325], loss=74.5899
	step [67/325], loss=70.0868
	step [68/325], loss=62.6941
	step [69/325], loss=71.5887
	step [70/325], loss=69.4748
	step [71/325], loss=70.9740
	step [72/325], loss=55.3224
	step [73/325], loss=81.0017
	step [74/325], loss=72.5692
	step [75/325], loss=62.6722
	step [76/325], loss=66.2151
	step [77/325], loss=62.8888
	step [78/325], loss=73.7077
	step [79/325], loss=79.3996
	step [80/325], loss=72.6561
	step [81/325], loss=81.2558
	step [82/325], loss=79.8612
	step [83/325], loss=83.1498
	step [84/325], loss=67.4257
	step [85/325], loss=73.2269
	step [86/325], loss=78.7201
	step [87/325], loss=76.8106
	step [88/325], loss=68.9227
	step [89/325], loss=94.0211
	step [90/325], loss=77.3930
	step [91/325], loss=69.1001
	step [92/325], loss=68.9324
	step [93/325], loss=71.7502
	step [94/325], loss=64.4537
	step [95/325], loss=71.8780
	step [96/325], loss=68.7349
	step [97/325], loss=75.4025
	step [98/325], loss=79.0675
	step [99/325], loss=63.8867
	step [100/325], loss=83.4166
	step [101/325], loss=57.0662
	step [102/325], loss=65.8879
	step [103/325], loss=65.7841
	step [104/325], loss=89.6821
	step [105/325], loss=66.2213
	step [106/325], loss=78.0877
	step [107/325], loss=89.4237
	step [108/325], loss=72.4217
	step [109/325], loss=58.6503
	step [110/325], loss=79.7959
	step [111/325], loss=64.9796
	step [112/325], loss=62.7072
	step [113/325], loss=66.5819
	step [114/325], loss=73.6085
	step [115/325], loss=76.9044
	step [116/325], loss=83.5610
	step [117/325], loss=75.7735
	step [118/325], loss=58.5736
	step [119/325], loss=55.4156
	step [120/325], loss=78.2238
	step [121/325], loss=74.2986
	step [122/325], loss=74.3704
	step [123/325], loss=95.9920
	step [124/325], loss=60.2850
	step [125/325], loss=64.7418
	step [126/325], loss=78.1976
	step [127/325], loss=55.5200
	step [128/325], loss=87.0356
	step [129/325], loss=56.3893
	step [130/325], loss=67.6541
	step [131/325], loss=69.9248
	step [132/325], loss=72.3194
	step [133/325], loss=68.8182
	step [134/325], loss=68.8570
	step [135/325], loss=65.6481
	step [136/325], loss=74.8875
	step [137/325], loss=69.3095
	step [138/325], loss=58.0049
	step [139/325], loss=79.1201
	step [140/325], loss=78.1949
	step [141/325], loss=64.3750
	step [142/325], loss=60.6943
	step [143/325], loss=55.4984
	step [144/325], loss=63.0571
	step [145/325], loss=77.1268
	step [146/325], loss=86.2910
	step [147/325], loss=69.0523
	step [148/325], loss=81.3144
	step [149/325], loss=63.5371
	step [150/325], loss=61.6431
	step [151/325], loss=58.5711
	step [152/325], loss=63.1499
	step [153/325], loss=79.9623
	step [154/325], loss=67.5735
	step [155/325], loss=70.5970
	step [156/325], loss=77.1356
	step [157/325], loss=64.2148
	step [158/325], loss=85.5716
	step [159/325], loss=80.9765
	step [160/325], loss=79.2881
	step [161/325], loss=70.1096
	step [162/325], loss=80.6449
	step [163/325], loss=67.7683
	step [164/325], loss=74.9078
	step [165/325], loss=69.5847
	step [166/325], loss=80.3045
	step [167/325], loss=76.8128
	step [168/325], loss=63.4297
	step [169/325], loss=82.9164
	step [170/325], loss=86.9128
	step [171/325], loss=80.2462
	step [172/325], loss=69.8545
	step [173/325], loss=66.0809
	step [174/325], loss=61.5499
	step [175/325], loss=88.0816
	step [176/325], loss=74.1326
	step [177/325], loss=65.0140
	step [178/325], loss=71.1508
	step [179/325], loss=65.2300
	step [180/325], loss=65.2182
	step [181/325], loss=66.5908
	step [182/325], loss=72.4524
	step [183/325], loss=69.3024
	step [184/325], loss=73.7028
	step [185/325], loss=65.5028
	step [186/325], loss=80.3400
	step [187/325], loss=64.7830
	step [188/325], loss=66.5891
	step [189/325], loss=53.4942
	step [190/325], loss=65.0546
	step [191/325], loss=72.9696
	step [192/325], loss=72.1357
	step [193/325], loss=79.1018
	step [194/325], loss=69.9950
	step [195/325], loss=68.1346
	step [196/325], loss=70.3922
	step [197/325], loss=78.3259
	step [198/325], loss=70.1178
	step [199/325], loss=90.9440
	step [200/325], loss=66.1769
	step [201/325], loss=58.2764
	step [202/325], loss=74.7257
	step [203/325], loss=80.2581
	step [204/325], loss=61.3895
	step [205/325], loss=73.6578
	step [206/325], loss=68.5523
	step [207/325], loss=104.3664
	step [208/325], loss=69.4577
	step [209/325], loss=82.7290
	step [210/325], loss=69.9376
	step [211/325], loss=70.6464
	step [212/325], loss=76.0079
	step [213/325], loss=76.7248
	step [214/325], loss=66.1840
	step [215/325], loss=65.6764
	step [216/325], loss=68.8257
	step [217/325], loss=68.7473
	step [218/325], loss=69.6070
	step [219/325], loss=91.5615
	step [220/325], loss=83.3835
	step [221/325], loss=76.7760
	step [222/325], loss=52.2205
	step [223/325], loss=75.7473
	step [224/325], loss=79.4172
	step [225/325], loss=67.4475
	step [226/325], loss=78.9992
	step [227/325], loss=67.1770
	step [228/325], loss=74.2928
	step [229/325], loss=79.6538
	step [230/325], loss=71.2932
	step [231/325], loss=80.1882
	step [232/325], loss=84.1902
	step [233/325], loss=70.9576
	step [234/325], loss=96.1105
	step [235/325], loss=69.6434
	step [236/325], loss=60.6030
	step [237/325], loss=69.8121
	step [238/325], loss=63.6803
	step [239/325], loss=80.8167
	step [240/325], loss=68.9397
	step [241/325], loss=74.9669
	step [242/325], loss=70.7592
	step [243/325], loss=57.3665
	step [244/325], loss=67.9452
	step [245/325], loss=87.9525
	step [246/325], loss=95.6367
	step [247/325], loss=66.1092
	step [248/325], loss=71.2634
	step [249/325], loss=77.0451
	step [250/325], loss=69.4666
	step [251/325], loss=67.9324
	step [252/325], loss=77.7967
	step [253/325], loss=77.2600
	step [254/325], loss=85.9243
	step [255/325], loss=66.7494
	step [256/325], loss=84.5474
	step [257/325], loss=69.7336
	step [258/325], loss=78.8727
	step [259/325], loss=70.0299
	step [260/325], loss=57.0668
	step [261/325], loss=60.1360
	step [262/325], loss=72.9121
	step [263/325], loss=76.5748
	step [264/325], loss=85.0176
	step [265/325], loss=69.1838
	step [266/325], loss=60.9540
	step [267/325], loss=83.5925
	step [268/325], loss=77.0372
	step [269/325], loss=70.4739
	step [270/325], loss=80.6609
	step [271/325], loss=75.0909
	step [272/325], loss=82.1362
	step [273/325], loss=80.3463
	step [274/325], loss=66.0034
	step [275/325], loss=71.7047
	step [276/325], loss=84.6017
	step [277/325], loss=86.4539
	step [278/325], loss=58.6599
	step [279/325], loss=58.1944
	step [280/325], loss=81.4042
	step [281/325], loss=74.7714
	step [282/325], loss=69.6903
	step [283/325], loss=68.7127
	step [284/325], loss=65.5419
	step [285/325], loss=79.9054
	step [286/325], loss=57.8714
	step [287/325], loss=84.8453
	step [288/325], loss=74.1048
	step [289/325], loss=57.1470
	step [290/325], loss=66.1364
	step [291/325], loss=78.1782
	step [292/325], loss=74.5176
	step [293/325], loss=83.0961
	step [294/325], loss=74.3638
	step [295/325], loss=70.7836
	step [296/325], loss=75.8842
	step [297/325], loss=91.8210
	step [298/325], loss=81.1002
	step [299/325], loss=80.9459
	step [300/325], loss=81.1064
	step [301/325], loss=70.1620
	step [302/325], loss=87.9768
	step [303/325], loss=70.4937
	step [304/325], loss=59.2895
	step [305/325], loss=75.3088
	step [306/325], loss=80.4723
	step [307/325], loss=75.7859
	step [308/325], loss=68.3777
	step [309/325], loss=89.3540
	step [310/325], loss=77.4680
	step [311/325], loss=64.1956
	step [312/325], loss=69.4488
	step [313/325], loss=78.0578
	step [314/325], loss=68.1068
	step [315/325], loss=65.3159
	step [316/325], loss=67.0927
	step [317/325], loss=67.2528
	step [318/325], loss=58.6496
	step [319/325], loss=65.4805
	step [320/325], loss=78.1384
	step [321/325], loss=88.6691
	step [322/325], loss=80.5645
	step [323/325], loss=62.2392
	step [324/325], loss=57.5147
	step [325/325], loss=35.1946
	Evaluating
	loss=0.0109, precision=0.4302, recall=0.9100, f1=0.5842
Training epoch 16
	step [1/325], loss=49.9584
	step [2/325], loss=58.7299
	step [3/325], loss=71.0715
	step [4/325], loss=66.0972
	step [5/325], loss=49.0086
	step [6/325], loss=67.1603
	step [7/325], loss=97.4558
	step [8/325], loss=60.2356
	step [9/325], loss=70.2513
	step [10/325], loss=101.6567
	step [11/325], loss=71.3012
	step [12/325], loss=72.9538
	step [13/325], loss=76.1120
	step [14/325], loss=70.7125
	step [15/325], loss=82.7461
	step [16/325], loss=69.2956
	step [17/325], loss=71.3240
	step [18/325], loss=60.4534
	step [19/325], loss=73.1733
	step [20/325], loss=93.2935
	step [21/325], loss=66.0354
	step [22/325], loss=65.7039
	step [23/325], loss=61.1396
	step [24/325], loss=77.5151
	step [25/325], loss=70.9909
	step [26/325], loss=64.7139
	step [27/325], loss=72.7372
	step [28/325], loss=62.0005
	step [29/325], loss=79.6114
	step [30/325], loss=85.6650
	step [31/325], loss=63.6555
	step [32/325], loss=61.1772
	step [33/325], loss=77.0637
	step [34/325], loss=72.5297
	step [35/325], loss=65.0748
	step [36/325], loss=83.9755
	step [37/325], loss=95.9942
	step [38/325], loss=89.7016
	step [39/325], loss=73.1868
	step [40/325], loss=70.1365
	step [41/325], loss=71.4752
	step [42/325], loss=80.9141
	step [43/325], loss=76.5713
	step [44/325], loss=72.0253
	step [45/325], loss=58.7805
	step [46/325], loss=65.6466
	step [47/325], loss=73.1635
	step [48/325], loss=61.7885
	step [49/325], loss=75.0714
	step [50/325], loss=83.0427
	step [51/325], loss=83.3895
	step [52/325], loss=66.3978
	step [53/325], loss=84.7703
	step [54/325], loss=84.6995
	step [55/325], loss=73.0484
	step [56/325], loss=84.6471
	step [57/325], loss=72.6938
	step [58/325], loss=73.6387
	step [59/325], loss=67.8441
	step [60/325], loss=66.6851
	step [61/325], loss=64.9170
	step [62/325], loss=68.2119
	step [63/325], loss=63.2449
	step [64/325], loss=61.1834
	step [65/325], loss=67.0503
	step [66/325], loss=75.8233
	step [67/325], loss=64.9048
	step [68/325], loss=73.5993
	step [69/325], loss=97.7910
	step [70/325], loss=55.1374
	step [71/325], loss=67.4622
	step [72/325], loss=70.5749
	step [73/325], loss=60.2272
	step [74/325], loss=68.0392
	step [75/325], loss=82.2302
	step [76/325], loss=67.2036
	step [77/325], loss=65.9713
	step [78/325], loss=64.7701
	step [79/325], loss=59.8253
	step [80/325], loss=72.1272
	step [81/325], loss=65.8886
	step [82/325], loss=86.6000
	step [83/325], loss=89.1960
	step [84/325], loss=64.7159
	step [85/325], loss=63.6583
	step [86/325], loss=74.1407
	step [87/325], loss=61.4014
	step [88/325], loss=89.7363
	step [89/325], loss=77.3889
	step [90/325], loss=68.7687
	step [91/325], loss=67.0745
	step [92/325], loss=57.9723
	step [93/325], loss=65.8349
	step [94/325], loss=83.1918
	step [95/325], loss=72.2858
	step [96/325], loss=55.5272
	step [97/325], loss=70.9456
	step [98/325], loss=65.5488
	step [99/325], loss=85.5779
	step [100/325], loss=64.0304
	step [101/325], loss=67.3358
	step [102/325], loss=74.3128
	step [103/325], loss=70.2972
	step [104/325], loss=66.2063
	step [105/325], loss=77.5355
	step [106/325], loss=68.3550
	step [107/325], loss=72.2954
	step [108/325], loss=64.1530
	step [109/325], loss=80.9636
	step [110/325], loss=70.6140
	step [111/325], loss=54.1715
	step [112/325], loss=77.2019
	step [113/325], loss=60.3257
	step [114/325], loss=82.1251
	step [115/325], loss=71.2087
	step [116/325], loss=61.8423
	step [117/325], loss=73.1467
	step [118/325], loss=83.3216
	step [119/325], loss=64.5951
	step [120/325], loss=64.4539
	step [121/325], loss=62.9045
	step [122/325], loss=97.1067
	step [123/325], loss=80.8707
	step [124/325], loss=58.5209
	step [125/325], loss=73.6039
	step [126/325], loss=72.6204
	step [127/325], loss=50.8768
	step [128/325], loss=76.4189
	step [129/325], loss=61.2381
	step [130/325], loss=73.6650
	step [131/325], loss=85.8487
	step [132/325], loss=66.6085
	step [133/325], loss=83.0939
	step [134/325], loss=65.7207
	step [135/325], loss=68.1647
	step [136/325], loss=63.9650
	step [137/325], loss=75.5404
	step [138/325], loss=69.6976
	step [139/325], loss=58.7301
	step [140/325], loss=63.0568
	step [141/325], loss=76.4602
	step [142/325], loss=72.6492
	step [143/325], loss=74.5329
	step [144/325], loss=82.5584
	step [145/325], loss=73.4436
	step [146/325], loss=80.0960
	step [147/325], loss=84.9079
	step [148/325], loss=61.5659
	step [149/325], loss=80.6295
	step [150/325], loss=91.8312
	step [151/325], loss=76.2219
	step [152/325], loss=85.4455
	step [153/325], loss=71.2978
	step [154/325], loss=70.0074
	step [155/325], loss=73.8611
	step [156/325], loss=99.0951
	step [157/325], loss=74.8456
	step [158/325], loss=81.2013
	step [159/325], loss=66.8452
	step [160/325], loss=75.8965
	step [161/325], loss=74.3428
	step [162/325], loss=67.4338
	step [163/325], loss=73.2815
	step [164/325], loss=66.8399
	step [165/325], loss=69.6225
	step [166/325], loss=73.9699
	step [167/325], loss=78.1625
	step [168/325], loss=64.6843
	step [169/325], loss=72.6832
	step [170/325], loss=83.2011
	step [171/325], loss=72.3553
	step [172/325], loss=75.1040
	step [173/325], loss=65.8290
	step [174/325], loss=74.9561
	step [175/325], loss=56.0913
	step [176/325], loss=65.4939
	step [177/325], loss=72.3059
	step [178/325], loss=65.2485
	step [179/325], loss=71.7470
	step [180/325], loss=53.0047
	step [181/325], loss=81.0296
	step [182/325], loss=67.6304
	step [183/325], loss=66.0689
	step [184/325], loss=69.2027
	step [185/325], loss=77.9862
	step [186/325], loss=67.8034
	step [187/325], loss=68.7380
	step [188/325], loss=67.5306
	step [189/325], loss=54.4122
	step [190/325], loss=65.8549
	step [191/325], loss=72.9630
	step [192/325], loss=70.9306
	step [193/325], loss=60.8717
	step [194/325], loss=60.2041
	step [195/325], loss=56.6027
	step [196/325], loss=74.3476
	step [197/325], loss=62.5565
	step [198/325], loss=82.2612
	step [199/325], loss=65.4502
	step [200/325], loss=63.7839
	step [201/325], loss=67.7415
	step [202/325], loss=67.1313
	step [203/325], loss=90.6329
	step [204/325], loss=74.4198
	step [205/325], loss=81.9328
	step [206/325], loss=68.7843
	step [207/325], loss=59.1156
	step [208/325], loss=74.4894
	step [209/325], loss=67.5048
	step [210/325], loss=73.8077
	step [211/325], loss=78.1434
	step [212/325], loss=61.6655
	step [213/325], loss=62.3993
	step [214/325], loss=58.6881
	step [215/325], loss=56.1863
	step [216/325], loss=70.6812
	step [217/325], loss=84.0829
	step [218/325], loss=73.3853
	step [219/325], loss=101.9390
	step [220/325], loss=70.3254
	step [221/325], loss=73.8525
	step [222/325], loss=68.5774
	step [223/325], loss=61.4028
	step [224/325], loss=75.5812
	step [225/325], loss=74.2306
	step [226/325], loss=89.2807
	step [227/325], loss=69.6034
	step [228/325], loss=66.7670
	step [229/325], loss=61.0481
	step [230/325], loss=72.9659
	step [231/325], loss=75.4578
	step [232/325], loss=78.8842
	step [233/325], loss=74.6744
	step [234/325], loss=57.1477
	step [235/325], loss=58.6814
	step [236/325], loss=76.1629
	step [237/325], loss=78.8299
	step [238/325], loss=69.9709
	step [239/325], loss=71.2076
	step [240/325], loss=66.2700
	step [241/325], loss=71.8176
	step [242/325], loss=55.8485
	step [243/325], loss=69.7617
	step [244/325], loss=73.3217
	step [245/325], loss=82.0990
	step [246/325], loss=80.6932
	step [247/325], loss=71.0157
	step [248/325], loss=69.4464
	step [249/325], loss=68.3818
	step [250/325], loss=85.6096
	step [251/325], loss=66.8939
	step [252/325], loss=84.0315
	step [253/325], loss=79.4388
	step [254/325], loss=70.4836
	step [255/325], loss=63.5561
	step [256/325], loss=69.7049
	step [257/325], loss=74.3178
	step [258/325], loss=84.5189
	step [259/325], loss=56.8797
	step [260/325], loss=70.6349
	step [261/325], loss=68.7817
	step [262/325], loss=65.9931
	step [263/325], loss=74.2678
	step [264/325], loss=74.6987
	step [265/325], loss=62.3439
	step [266/325], loss=66.9821
	step [267/325], loss=82.4922
	step [268/325], loss=73.9821
	step [269/325], loss=59.7179
	step [270/325], loss=66.4626
	step [271/325], loss=74.3089
	step [272/325], loss=66.2445
	step [273/325], loss=79.8766
	step [274/325], loss=68.9477
	step [275/325], loss=69.4078
	step [276/325], loss=64.3979
	step [277/325], loss=60.3039
	step [278/325], loss=73.4311
	step [279/325], loss=58.3936
	step [280/325], loss=71.9295
	step [281/325], loss=76.3504
	step [282/325], loss=81.8694
	step [283/325], loss=56.5658
	step [284/325], loss=68.1106
	step [285/325], loss=66.9589
	step [286/325], loss=85.5814
	step [287/325], loss=72.4714
	step [288/325], loss=77.8034
	step [289/325], loss=81.0782
	step [290/325], loss=68.7000
	step [291/325], loss=70.1734
	step [292/325], loss=70.4281
	step [293/325], loss=61.4903
	step [294/325], loss=58.0602
	step [295/325], loss=58.9324
	step [296/325], loss=86.1571
	step [297/325], loss=78.3299
	step [298/325], loss=69.7445
	step [299/325], loss=74.8316
	step [300/325], loss=56.1084
	step [301/325], loss=72.6539
	step [302/325], loss=79.0559
	step [303/325], loss=78.2488
	step [304/325], loss=53.2494
	step [305/325], loss=79.8041
	step [306/325], loss=54.5341
	step [307/325], loss=72.7001
	step [308/325], loss=77.3093
	step [309/325], loss=67.6715
	step [310/325], loss=83.2734
	step [311/325], loss=82.6570
	step [312/325], loss=61.0398
	step [313/325], loss=58.1573
	step [314/325], loss=74.1696
	step [315/325], loss=80.5279
	step [316/325], loss=81.8588
	step [317/325], loss=73.3798
	step [318/325], loss=66.1447
	step [319/325], loss=66.0252
	step [320/325], loss=72.4593
	step [321/325], loss=64.9557
	step [322/325], loss=69.9059
	step [323/325], loss=75.7903
	step [324/325], loss=92.8929
	step [325/325], loss=40.4484
	Evaluating
	loss=0.0093, precision=0.4681, recall=0.9053, f1=0.6171
Training epoch 17
	step [1/325], loss=68.5433
	step [2/325], loss=74.4234
	step [3/325], loss=85.9541
	step [4/325], loss=70.4961
	step [5/325], loss=80.9919
	step [6/325], loss=68.2331
	step [7/325], loss=58.3615
	step [8/325], loss=68.8602
	step [9/325], loss=67.2822
	step [10/325], loss=80.3783
	step [11/325], loss=57.5888
	step [12/325], loss=80.7723
	step [13/325], loss=61.0315
	step [14/325], loss=77.8289
	step [15/325], loss=73.2613
	step [16/325], loss=65.9123
	step [17/325], loss=83.4262
	step [18/325], loss=69.6052
	step [19/325], loss=70.1226
	step [20/325], loss=73.1687
	step [21/325], loss=60.7505
	step [22/325], loss=70.8318
	step [23/325], loss=81.7181
	step [24/325], loss=60.1985
	step [25/325], loss=70.8344
	step [26/325], loss=81.0336
	step [27/325], loss=59.9604
	step [28/325], loss=93.1662
	step [29/325], loss=78.7294
	step [30/325], loss=71.1840
	step [31/325], loss=74.2982
	step [32/325], loss=61.2519
	step [33/325], loss=60.2265
	step [34/325], loss=76.2611
	step [35/325], loss=79.9752
	step [36/325], loss=64.9485
	step [37/325], loss=69.8827
	step [38/325], loss=63.3512
	step [39/325], loss=85.3691
	step [40/325], loss=79.7977
	step [41/325], loss=67.1970
	step [42/325], loss=65.4546
	step [43/325], loss=79.4719
	step [44/325], loss=74.1629
	step [45/325], loss=73.5111
	step [46/325], loss=82.0636
	step [47/325], loss=52.4434
	step [48/325], loss=75.4356
	step [49/325], loss=67.7995
	step [50/325], loss=66.1965
	step [51/325], loss=81.5363
	step [52/325], loss=74.6663
	step [53/325], loss=57.5087
	step [54/325], loss=77.0876
	step [55/325], loss=85.9270
	step [56/325], loss=76.8429
	step [57/325], loss=70.6295
	step [58/325], loss=68.5082
	step [59/325], loss=79.6068
	step [60/325], loss=61.7792
	step [61/325], loss=77.7544
	step [62/325], loss=71.4239
	step [63/325], loss=59.9131
	step [64/325], loss=80.7106
	step [65/325], loss=83.2017
	step [66/325], loss=68.7329
	step [67/325], loss=83.3151
	step [68/325], loss=71.3800
	step [69/325], loss=79.0042
	step [70/325], loss=64.6705
	step [71/325], loss=63.8220
	step [72/325], loss=74.1190
	step [73/325], loss=67.0845
	step [74/325], loss=74.5772
	step [75/325], loss=78.3537
	step [76/325], loss=88.4196
	step [77/325], loss=51.6905
	step [78/325], loss=65.9941
	step [79/325], loss=77.9715
	step [80/325], loss=58.3753
	step [81/325], loss=77.8911
	step [82/325], loss=64.2939
	step [83/325], loss=68.8704
	step [84/325], loss=45.1502
	step [85/325], loss=80.9724
	step [86/325], loss=80.9955
	step [87/325], loss=84.1900
	step [88/325], loss=80.3914
	step [89/325], loss=65.3831
	step [90/325], loss=80.6610
	step [91/325], loss=70.8792
	step [92/325], loss=83.1774
	step [93/325], loss=66.2065
	step [94/325], loss=76.5257
	step [95/325], loss=74.2951
	step [96/325], loss=58.2386
	step [97/325], loss=68.1004
	step [98/325], loss=86.8132
	step [99/325], loss=65.7107
	step [100/325], loss=57.5595
	step [101/325], loss=73.0276
	step [102/325], loss=76.8932
	step [103/325], loss=76.5628
	step [104/325], loss=88.2084
	step [105/325], loss=65.0621
	step [106/325], loss=70.2646
	step [107/325], loss=77.9269
	step [108/325], loss=82.9957
	step [109/325], loss=83.9099
	step [110/325], loss=73.0421
	step [111/325], loss=59.7621
	step [112/325], loss=59.7151
	step [113/325], loss=57.1746
	step [114/325], loss=81.5645
	step [115/325], loss=80.6265
	step [116/325], loss=68.6221
	step [117/325], loss=52.2074
	step [118/325], loss=94.8708
	step [119/325], loss=76.1448
	step [120/325], loss=55.2249
	step [121/325], loss=82.4353
	step [122/325], loss=65.8259
	step [123/325], loss=66.9118
	step [124/325], loss=72.6977
	step [125/325], loss=65.3232
	step [126/325], loss=84.5564
	step [127/325], loss=75.7871
	step [128/325], loss=65.8170
	step [129/325], loss=73.5833
	step [130/325], loss=61.2844
	step [131/325], loss=77.1530
	step [132/325], loss=74.7493
	step [133/325], loss=60.7667
	step [134/325], loss=74.0017
	step [135/325], loss=83.3533
	step [136/325], loss=81.9706
	step [137/325], loss=66.8533
	step [138/325], loss=70.6955
	step [139/325], loss=76.5079
	step [140/325], loss=68.1665
	step [141/325], loss=69.5624
	step [142/325], loss=60.4041
	step [143/325], loss=60.1041
	step [144/325], loss=67.6636
	step [145/325], loss=80.0647
	step [146/325], loss=72.9811
	step [147/325], loss=73.0119
	step [148/325], loss=84.1470
	step [149/325], loss=61.8711
	step [150/325], loss=73.4320
	step [151/325], loss=80.4637
	step [152/325], loss=74.2074
	step [153/325], loss=66.9166
	step [154/325], loss=77.1575
	step [155/325], loss=70.9308
	step [156/325], loss=75.2991
	step [157/325], loss=75.5984
	step [158/325], loss=71.9419
	step [159/325], loss=62.4899
	step [160/325], loss=62.8920
	step [161/325], loss=71.9648
	step [162/325], loss=69.6193
	step [163/325], loss=59.0487
	step [164/325], loss=72.1106
	step [165/325], loss=68.8893
	step [166/325], loss=58.3301
	step [167/325], loss=62.6377
	step [168/325], loss=69.3102
	step [169/325], loss=78.5268
	step [170/325], loss=53.2478
	step [171/325], loss=62.9725
	step [172/325], loss=65.4993
	step [173/325], loss=63.9251
	step [174/325], loss=63.7800
	step [175/325], loss=91.1753
	step [176/325], loss=69.9359
	step [177/325], loss=71.1702
	step [178/325], loss=70.4441
	step [179/325], loss=74.6786
	step [180/325], loss=66.0132
	step [181/325], loss=55.5617
	step [182/325], loss=72.5519
	step [183/325], loss=65.6953
	step [184/325], loss=68.6681
	step [185/325], loss=71.6332
	step [186/325], loss=51.9801
	step [187/325], loss=81.9427
	step [188/325], loss=54.8116
	step [189/325], loss=65.6977
	step [190/325], loss=84.2876
	step [191/325], loss=67.0753
	step [192/325], loss=66.5784
	step [193/325], loss=56.4154
	step [194/325], loss=56.9372
	step [195/325], loss=64.1929
	step [196/325], loss=83.7283
	step [197/325], loss=81.7489
	step [198/325], loss=69.3705
	step [199/325], loss=86.8153
	step [200/325], loss=63.7176
	step [201/325], loss=60.3221
	step [202/325], loss=74.2017
	step [203/325], loss=90.0224
	step [204/325], loss=63.3897
	step [205/325], loss=72.2596
	step [206/325], loss=66.8380
	step [207/325], loss=67.6860
	step [208/325], loss=87.3423
	step [209/325], loss=72.3490
	step [210/325], loss=60.7229
	step [211/325], loss=58.2026
	step [212/325], loss=62.7526
	step [213/325], loss=65.0167
	step [214/325], loss=72.4965
	step [215/325], loss=67.1196
	step [216/325], loss=57.5720
	step [217/325], loss=69.7305
	step [218/325], loss=72.8485
	step [219/325], loss=62.1026
	step [220/325], loss=73.2033
	step [221/325], loss=61.3461
	step [222/325], loss=80.6696
	step [223/325], loss=77.5332
	step [224/325], loss=73.7360
	step [225/325], loss=69.2327
	step [226/325], loss=81.8338
	step [227/325], loss=92.7518
	step [228/325], loss=64.9013
	step [229/325], loss=77.5736
	step [230/325], loss=58.4962
	step [231/325], loss=65.2254
	step [232/325], loss=76.8186
	step [233/325], loss=80.9219
	step [234/325], loss=61.2335
	step [235/325], loss=83.5807
	step [236/325], loss=62.5152
	step [237/325], loss=68.9523
	step [238/325], loss=66.3748
	step [239/325], loss=76.9629
	step [240/325], loss=78.2893
	step [241/325], loss=60.0075
	step [242/325], loss=63.4861
	step [243/325], loss=63.7458
	step [244/325], loss=61.7253
	step [245/325], loss=61.2345
	step [246/325], loss=64.9611
	step [247/325], loss=54.7794
	step [248/325], loss=59.1344
	step [249/325], loss=74.9077
	step [250/325], loss=50.6805
	step [251/325], loss=65.5204
	step [252/325], loss=82.0664
	step [253/325], loss=74.0482
	step [254/325], loss=71.1654
	step [255/325], loss=87.4642
	step [256/325], loss=71.5890
	step [257/325], loss=66.4478
	step [258/325], loss=79.1333
	step [259/325], loss=77.6398
	step [260/325], loss=63.2144
	step [261/325], loss=64.9670
	step [262/325], loss=70.7164
	step [263/325], loss=50.6541
	step [264/325], loss=67.9386
	step [265/325], loss=81.0910
	step [266/325], loss=73.6403
	step [267/325], loss=69.0105
	step [268/325], loss=66.2048
	step [269/325], loss=78.6562
	step [270/325], loss=72.6379
	step [271/325], loss=90.3826
	step [272/325], loss=65.8707
	step [273/325], loss=78.3140
	step [274/325], loss=69.8353
	step [275/325], loss=64.7420
	step [276/325], loss=77.2795
	step [277/325], loss=74.3366
	step [278/325], loss=60.1163
	step [279/325], loss=66.8068
	step [280/325], loss=81.2497
	step [281/325], loss=61.8669
	step [282/325], loss=76.3810
	step [283/325], loss=75.2785
	step [284/325], loss=81.4652
	step [285/325], loss=74.6158
	step [286/325], loss=59.3721
	step [287/325], loss=81.6907
	step [288/325], loss=78.3152
	step [289/325], loss=87.8876
	step [290/325], loss=69.4550
	step [291/325], loss=61.2089
	step [292/325], loss=58.3695
	step [293/325], loss=83.2396
	step [294/325], loss=70.4903
	step [295/325], loss=57.7746
	step [296/325], loss=71.5231
	step [297/325], loss=74.8113
	step [298/325], loss=68.9300
	step [299/325], loss=74.9534
	step [300/325], loss=91.7616
	step [301/325], loss=76.7697
	step [302/325], loss=56.1058
	step [303/325], loss=63.5955
	step [304/325], loss=54.6096
	step [305/325], loss=69.5159
	step [306/325], loss=58.9894
	step [307/325], loss=61.6487
	step [308/325], loss=51.0107
	step [309/325], loss=71.5091
	step [310/325], loss=75.2631
	step [311/325], loss=56.4861
	step [312/325], loss=64.4127
	step [313/325], loss=72.7525
	step [314/325], loss=62.9565
	step [315/325], loss=57.4094
	step [316/325], loss=67.0163
	step [317/325], loss=62.3086
	step [318/325], loss=60.6583
	step [319/325], loss=78.1782
	step [320/325], loss=82.2948
	step [321/325], loss=71.8111
	step [322/325], loss=72.9500
	step [323/325], loss=83.7702
	step [324/325], loss=79.8250
	step [325/325], loss=32.3173
	Evaluating
	loss=0.0093, precision=0.4402, recall=0.9027, f1=0.5918
Training epoch 18
	step [1/325], loss=55.4076
	step [2/325], loss=75.9124
	step [3/325], loss=76.5204
	step [4/325], loss=79.4288
	step [5/325], loss=66.1709
	step [6/325], loss=76.6561
	step [7/325], loss=65.8977
	step [8/325], loss=58.4812
	step [9/325], loss=69.8620
	step [10/325], loss=75.0483
	step [11/325], loss=79.6787
	step [12/325], loss=62.7715
	step [13/325], loss=83.5337
	step [14/325], loss=61.8230
	step [15/325], loss=56.3088
	step [16/325], loss=67.9907
	step [17/325], loss=77.6316
	step [18/325], loss=62.1462
	step [19/325], loss=71.3655
	step [20/325], loss=59.0068
	step [21/325], loss=74.0897
	step [22/325], loss=83.8355
	step [23/325], loss=94.3887
	step [24/325], loss=85.0945
	step [25/325], loss=80.5755
	step [26/325], loss=80.5951
	step [27/325], loss=88.3189
	step [28/325], loss=74.0271
	step [29/325], loss=67.2090
	step [30/325], loss=75.2943
	step [31/325], loss=73.4350
	step [32/325], loss=88.9473
	step [33/325], loss=68.7138
	step [34/325], loss=77.8559
	step [35/325], loss=59.9928
	step [36/325], loss=91.8560
	step [37/325], loss=77.3408
	step [38/325], loss=61.2186
	step [39/325], loss=55.5211
	step [40/325], loss=59.7041
	step [41/325], loss=72.6340
	step [42/325], loss=66.6701
	step [43/325], loss=65.3271
	step [44/325], loss=73.0157
	step [45/325], loss=55.1755
	step [46/325], loss=81.6948
	step [47/325], loss=63.1919
	step [48/325], loss=72.4110
	step [49/325], loss=77.6773
	step [50/325], loss=85.6315
	step [51/325], loss=67.8933
	step [52/325], loss=87.6390
	step [53/325], loss=68.1881
	step [54/325], loss=73.8641
	step [55/325], loss=79.6026
	step [56/325], loss=60.9861
	step [57/325], loss=74.3871
	step [58/325], loss=75.5121
	step [59/325], loss=55.4805
	step [60/325], loss=57.8652
	step [61/325], loss=92.0128
	step [62/325], loss=58.3859
	step [63/325], loss=65.6086
	step [64/325], loss=65.0823
	step [65/325], loss=71.0826
	step [66/325], loss=67.3734
	step [67/325], loss=66.9466
	step [68/325], loss=71.1162
	step [69/325], loss=61.2683
	step [70/325], loss=65.5236
	step [71/325], loss=66.4520
	step [72/325], loss=69.8726
	step [73/325], loss=77.0576
	step [74/325], loss=80.7952
	step [75/325], loss=76.1569
	step [76/325], loss=76.0472
	step [77/325], loss=60.7752
	step [78/325], loss=79.9180
	step [79/325], loss=58.7500
	step [80/325], loss=68.7220
	step [81/325], loss=73.1998
	step [82/325], loss=60.9656
	step [83/325], loss=65.3081
	step [84/325], loss=67.2999
	step [85/325], loss=52.4404
	step [86/325], loss=67.8770
	step [87/325], loss=50.0740
	step [88/325], loss=83.5873
	step [89/325], loss=72.3336
	step [90/325], loss=64.4305
	step [91/325], loss=75.2633
	step [92/325], loss=64.8707
	step [93/325], loss=63.7466
	step [94/325], loss=74.6431
	step [95/325], loss=90.5905
	step [96/325], loss=75.9257
	step [97/325], loss=73.5116
	step [98/325], loss=72.2988
	step [99/325], loss=63.4897
	step [100/325], loss=72.9974
	step [101/325], loss=56.9213
	step [102/325], loss=56.4817
	step [103/325], loss=72.4732
	step [104/325], loss=49.8460
	step [105/325], loss=63.6109
	step [106/325], loss=95.6709
	step [107/325], loss=63.8432
	step [108/325], loss=65.0947
	step [109/325], loss=68.6593
	step [110/325], loss=72.2051
	step [111/325], loss=71.3289
	step [112/325], loss=88.9714
	step [113/325], loss=51.9586
	step [114/325], loss=79.1038
	step [115/325], loss=81.6479
	step [116/325], loss=76.2822
	step [117/325], loss=70.4434
	step [118/325], loss=63.5912
	step [119/325], loss=85.7482
	step [120/325], loss=71.3803
	step [121/325], loss=74.7487
	step [122/325], loss=66.9265
	step [123/325], loss=52.8706
	step [124/325], loss=78.3083
	step [125/325], loss=82.3359
	step [126/325], loss=57.3585
	step [127/325], loss=68.0940
	step [128/325], loss=71.4729
	step [129/325], loss=60.2024
	step [130/325], loss=55.3650
	step [131/325], loss=65.1773
	step [132/325], loss=63.0985
	step [133/325], loss=69.8887
	step [134/325], loss=64.7574
	step [135/325], loss=73.1465
	step [136/325], loss=69.2733
	step [137/325], loss=53.7816
	step [138/325], loss=64.7440
	step [139/325], loss=65.2441
	step [140/325], loss=56.2382
	step [141/325], loss=71.4824
	step [142/325], loss=59.8001
	step [143/325], loss=77.4360
	step [144/325], loss=86.4718
	step [145/325], loss=65.3911
	step [146/325], loss=64.8164
	step [147/325], loss=75.3248
	step [148/325], loss=65.4478
	step [149/325], loss=69.4266
	step [150/325], loss=77.9325
	step [151/325], loss=71.0056
	step [152/325], loss=78.9177
	step [153/325], loss=68.7234
	step [154/325], loss=64.5961
	step [155/325], loss=65.1466
	step [156/325], loss=64.9473
	step [157/325], loss=77.9201
	step [158/325], loss=62.1644
	step [159/325], loss=63.7845
	step [160/325], loss=77.1878
	step [161/325], loss=56.1254
	step [162/325], loss=87.8326
	step [163/325], loss=66.7709
	step [164/325], loss=72.1212
	step [165/325], loss=80.9083
	step [166/325], loss=58.4351
	step [167/325], loss=82.7165
	step [168/325], loss=82.2849
	step [169/325], loss=67.5455
	step [170/325], loss=78.7698
	step [171/325], loss=64.7264
	step [172/325], loss=71.4795
	step [173/325], loss=66.5563
	step [174/325], loss=80.4485
	step [175/325], loss=62.9665
	step [176/325], loss=81.7700
	step [177/325], loss=77.3779
	step [178/325], loss=78.3363
	step [179/325], loss=71.8585
	step [180/325], loss=65.3025
	step [181/325], loss=77.5018
	step [182/325], loss=63.1962
	step [183/325], loss=77.2109
	step [184/325], loss=64.7300
	step [185/325], loss=62.8704
	step [186/325], loss=63.1575
	step [187/325], loss=89.6022
	step [188/325], loss=77.4476
	step [189/325], loss=49.0541
	step [190/325], loss=71.6937
	step [191/325], loss=71.3744
	step [192/325], loss=78.9151
	step [193/325], loss=84.1621
	step [194/325], loss=62.1896
	step [195/325], loss=69.3799
	step [196/325], loss=66.3639
	step [197/325], loss=55.6462
	step [198/325], loss=74.1808
	step [199/325], loss=59.1164
	step [200/325], loss=68.8485
	step [201/325], loss=65.5353
	step [202/325], loss=88.8707
	step [203/325], loss=73.2102
	step [204/325], loss=86.0926
	step [205/325], loss=57.6506
	step [206/325], loss=78.9319
	step [207/325], loss=70.8770
	step [208/325], loss=79.4273
	step [209/325], loss=61.4650
	step [210/325], loss=68.9250
	step [211/325], loss=68.4583
	step [212/325], loss=51.2824
	step [213/325], loss=90.6758
	step [214/325], loss=67.9909
	step [215/325], loss=65.0334
	step [216/325], loss=59.2467
	step [217/325], loss=85.3075
	step [218/325], loss=58.2002
	step [219/325], loss=65.3095
	step [220/325], loss=70.5878
	step [221/325], loss=74.6123
	step [222/325], loss=70.0700
	step [223/325], loss=67.9718
	step [224/325], loss=76.4332
	step [225/325], loss=55.2800
	step [226/325], loss=68.5439
	step [227/325], loss=62.9778
	step [228/325], loss=63.3916
	step [229/325], loss=60.7797
	step [230/325], loss=78.0235
	step [231/325], loss=67.7908
	step [232/325], loss=72.2292
	step [233/325], loss=59.4750
	step [234/325], loss=78.9389
	step [235/325], loss=70.1694
	step [236/325], loss=70.0373
	step [237/325], loss=69.0979
	step [238/325], loss=65.6068
	step [239/325], loss=61.6132
	step [240/325], loss=74.8596
	step [241/325], loss=78.6468
	step [242/325], loss=71.8962
	step [243/325], loss=85.0266
	step [244/325], loss=67.2349
	step [245/325], loss=77.2232
	step [246/325], loss=71.1103
	step [247/325], loss=66.2036
	step [248/325], loss=69.8634
	step [249/325], loss=64.6176
	step [250/325], loss=76.0325
	step [251/325], loss=69.0721
	step [252/325], loss=65.8756
	step [253/325], loss=79.9803
	step [254/325], loss=62.9968
	step [255/325], loss=62.1787
	step [256/325], loss=60.2741
	step [257/325], loss=88.7780
	step [258/325], loss=55.3187
	step [259/325], loss=65.1516
	step [260/325], loss=64.8309
	step [261/325], loss=58.0090
	step [262/325], loss=69.5770
	step [263/325], loss=64.4585
	step [264/325], loss=65.6674
	step [265/325], loss=65.0857
	step [266/325], loss=71.0664
	step [267/325], loss=65.7388
	step [268/325], loss=75.7844
	step [269/325], loss=72.6615
	step [270/325], loss=69.0492
	step [271/325], loss=96.3385
	step [272/325], loss=77.3690
	step [273/325], loss=76.8351
	step [274/325], loss=57.2659
	step [275/325], loss=76.9416
	step [276/325], loss=69.0450
	step [277/325], loss=60.6576
	step [278/325], loss=68.3373
	step [279/325], loss=84.7675
	step [280/325], loss=59.8099
	step [281/325], loss=65.4299
	step [282/325], loss=70.8323
	step [283/325], loss=74.1783
	step [284/325], loss=72.0096
	step [285/325], loss=63.7491
	step [286/325], loss=74.6502
	step [287/325], loss=58.1336
	step [288/325], loss=71.4070
	step [289/325], loss=55.7469
	step [290/325], loss=88.3589
	step [291/325], loss=69.2218
	step [292/325], loss=61.1848
	step [293/325], loss=85.6048
	step [294/325], loss=68.0626
	step [295/325], loss=67.9604
	step [296/325], loss=77.0103
	step [297/325], loss=64.8961
	step [298/325], loss=85.2287
	step [299/325], loss=68.9253
	step [300/325], loss=55.9831
	step [301/325], loss=61.5298
	step [302/325], loss=85.6665
	step [303/325], loss=71.5174
	step [304/325], loss=70.2163
	step [305/325], loss=85.6828
	step [306/325], loss=69.5582
	step [307/325], loss=75.4866
	step [308/325], loss=73.7339
	step [309/325], loss=59.4534
	step [310/325], loss=78.3642
	step [311/325], loss=66.3336
	step [312/325], loss=59.8642
	step [313/325], loss=59.7785
	step [314/325], loss=72.7080
	step [315/325], loss=59.7553
	step [316/325], loss=65.4512
	step [317/325], loss=73.4730
	step [318/325], loss=88.2822
	step [319/325], loss=68.0665
	step [320/325], loss=68.7671
	step [321/325], loss=69.5523
	step [322/325], loss=81.7434
	step [323/325], loss=70.2932
	step [324/325], loss=70.3279
	step [325/325], loss=38.8344
	Evaluating
	loss=0.0080, precision=0.4710, recall=0.9041, f1=0.6193
Training epoch 19
	step [1/325], loss=85.6971
	step [2/325], loss=82.1692
	step [3/325], loss=59.3582
	step [4/325], loss=81.4434
	step [5/325], loss=76.4113
	step [6/325], loss=83.9308
	step [7/325], loss=74.6587
	step [8/325], loss=75.1888
	step [9/325], loss=70.2835
	step [10/325], loss=68.8449
	step [11/325], loss=76.7343
	step [12/325], loss=61.0571
	step [13/325], loss=63.4442
	step [14/325], loss=89.8040
	step [15/325], loss=75.1280
	step [16/325], loss=77.8409
	step [17/325], loss=71.1205
	step [18/325], loss=78.3466
	step [19/325], loss=73.2888
	step [20/325], loss=87.2011
	step [21/325], loss=65.1226
	step [22/325], loss=65.1641
	step [23/325], loss=68.6240
	step [24/325], loss=79.3812
	step [25/325], loss=79.7677
	step [26/325], loss=63.6651
	step [27/325], loss=63.6720
	step [28/325], loss=76.4055
	step [29/325], loss=60.7590
	step [30/325], loss=59.7745
	step [31/325], loss=68.5688
	step [32/325], loss=66.4407
	step [33/325], loss=78.6912
	step [34/325], loss=58.0805
	step [35/325], loss=55.6452
	step [36/325], loss=76.0594
	step [37/325], loss=69.1471
	step [38/325], loss=61.2254
	step [39/325], loss=70.8445
	step [40/325], loss=81.0214
	step [41/325], loss=75.6376
	step [42/325], loss=82.2576
	step [43/325], loss=78.2013
	step [44/325], loss=75.0364
	step [45/325], loss=76.6117
	step [46/325], loss=84.3119
	step [47/325], loss=82.3529
	step [48/325], loss=77.5740
	step [49/325], loss=74.3018
	step [50/325], loss=62.2986
	step [51/325], loss=84.1682
	step [52/325], loss=75.7711
	step [53/325], loss=69.7640
	step [54/325], loss=73.4232
	step [55/325], loss=78.6867
	step [56/325], loss=75.6581
	step [57/325], loss=77.8107
	step [58/325], loss=65.8489
	step [59/325], loss=65.0467
	step [60/325], loss=79.3620
	step [61/325], loss=61.7761
	step [62/325], loss=76.8932
	step [63/325], loss=67.6282
	step [64/325], loss=82.9328
	step [65/325], loss=65.0535
	step [66/325], loss=63.6010
	step [67/325], loss=61.1678
	step [68/325], loss=70.2943
	step [69/325], loss=60.2316
	step [70/325], loss=71.7429
	step [71/325], loss=60.7019
	step [72/325], loss=75.4238
	step [73/325], loss=51.9896
	step [74/325], loss=75.8719
	step [75/325], loss=59.0939
	step [76/325], loss=81.3131
	step [77/325], loss=87.0296
	step [78/325], loss=66.3134
	step [79/325], loss=77.5506
	step [80/325], loss=63.9601
	step [81/325], loss=74.8893
	step [82/325], loss=70.0364
	step [83/325], loss=64.2638
	step [84/325], loss=70.1363
	step [85/325], loss=65.0865
	step [86/325], loss=77.5288
	step [87/325], loss=63.5374
	step [88/325], loss=65.4796
	step [89/325], loss=64.3455
	step [90/325], loss=64.5282
	step [91/325], loss=71.6541
	step [92/325], loss=77.8652
	step [93/325], loss=56.7585
	step [94/325], loss=60.4715
	step [95/325], loss=74.4508
	step [96/325], loss=76.2813
	step [97/325], loss=68.1304
	step [98/325], loss=63.2629
	step [99/325], loss=80.2833
	step [100/325], loss=60.2988
	step [101/325], loss=91.0852
	step [102/325], loss=55.5550
	step [103/325], loss=60.1141
	step [104/325], loss=65.7630
	step [105/325], loss=70.3063
	step [106/325], loss=64.7780
	step [107/325], loss=50.8307
	step [108/325], loss=57.1545
	step [109/325], loss=69.3948
	step [110/325], loss=68.1661
	step [111/325], loss=70.3507
	step [112/325], loss=71.5628
	step [113/325], loss=72.9106
	step [114/325], loss=58.3925
	step [115/325], loss=53.4292
	step [116/325], loss=58.8060
	step [117/325], loss=77.9830
	step [118/325], loss=75.9984
	step [119/325], loss=66.7656
	step [120/325], loss=58.6697
	step [121/325], loss=75.3338
	step [122/325], loss=56.4121
	step [123/325], loss=61.5994
	step [124/325], loss=78.8897
	step [125/325], loss=77.4122
	step [126/325], loss=77.7144
	step [127/325], loss=88.9746
	step [128/325], loss=76.8008
	step [129/325], loss=83.9439
	step [130/325], loss=76.5911
	step [131/325], loss=56.7115
	step [132/325], loss=58.7436
	step [133/325], loss=59.4866
	step [134/325], loss=72.8983
	step [135/325], loss=65.3210
	step [136/325], loss=54.3079
	step [137/325], loss=56.9568
	step [138/325], loss=71.4582
	step [139/325], loss=82.8665
	step [140/325], loss=76.2765
	step [141/325], loss=77.6851
	step [142/325], loss=75.2608
	step [143/325], loss=74.3746
	step [144/325], loss=78.7475
	step [145/325], loss=54.1528
	step [146/325], loss=54.9014
	step [147/325], loss=74.1699
	step [148/325], loss=70.4259
	step [149/325], loss=82.1077
	step [150/325], loss=72.8150
	step [151/325], loss=75.8659
	step [152/325], loss=67.9884
	step [153/325], loss=63.0690
	step [154/325], loss=76.0272
	step [155/325], loss=61.1356
	step [156/325], loss=65.3313
	step [157/325], loss=74.4683
	step [158/325], loss=70.4536
	step [159/325], loss=83.3484
	step [160/325], loss=67.1130
	step [161/325], loss=58.4212
	step [162/325], loss=74.4497
	step [163/325], loss=62.6922
	step [164/325], loss=72.1434
	step [165/325], loss=75.8377
	step [166/325], loss=64.4601
	step [167/325], loss=78.3135
	step [168/325], loss=62.4936
	step [169/325], loss=96.1968
	step [170/325], loss=59.9523
	step [171/325], loss=81.3005
	step [172/325], loss=64.9358
	step [173/325], loss=67.9856
	step [174/325], loss=68.4619
	step [175/325], loss=77.3922
	step [176/325], loss=71.7373
	step [177/325], loss=79.5435
	step [178/325], loss=74.2035
	step [179/325], loss=52.6207
	step [180/325], loss=59.5584
	step [181/325], loss=76.2817
	step [182/325], loss=60.8460
	step [183/325], loss=68.6372
	step [184/325], loss=63.9018
	step [185/325], loss=63.7125
	step [186/325], loss=70.6178
	step [187/325], loss=69.4035
	step [188/325], loss=91.7205
	step [189/325], loss=56.7660
	step [190/325], loss=59.0198
	step [191/325], loss=68.9349
	step [192/325], loss=63.2105
	step [193/325], loss=80.5675
	step [194/325], loss=65.2641
	step [195/325], loss=59.6242
	step [196/325], loss=64.0602
	step [197/325], loss=56.8510
	step [198/325], loss=68.5699
	step [199/325], loss=83.1230
	step [200/325], loss=66.1829
	step [201/325], loss=64.0164
	step [202/325], loss=79.7672
	step [203/325], loss=87.8395
	step [204/325], loss=68.2400
	step [205/325], loss=64.9831
	step [206/325], loss=76.7898
	step [207/325], loss=72.9810
	step [208/325], loss=83.1166
	step [209/325], loss=55.0335
	step [210/325], loss=71.8112
	step [211/325], loss=72.9113
	step [212/325], loss=71.3617
	step [213/325], loss=67.6819
	step [214/325], loss=70.2129
	step [215/325], loss=75.2348
	step [216/325], loss=53.1557
	step [217/325], loss=62.8423
	step [218/325], loss=60.5191
	step [219/325], loss=67.1684
	step [220/325], loss=67.6807
	step [221/325], loss=67.3456
	step [222/325], loss=59.0455
	step [223/325], loss=53.0194
	step [224/325], loss=73.9034
	step [225/325], loss=68.7300
	step [226/325], loss=66.1976
	step [227/325], loss=72.8050
	step [228/325], loss=69.3076
	step [229/325], loss=52.0770
	step [230/325], loss=59.4431
	step [231/325], loss=82.7963
	step [232/325], loss=65.4295
	step [233/325], loss=71.3046
	step [234/325], loss=85.3150
	step [235/325], loss=79.1135
	step [236/325], loss=67.3542
	step [237/325], loss=74.5239
	step [238/325], loss=72.4753
	step [239/325], loss=62.6553
	step [240/325], loss=76.6728
	step [241/325], loss=71.5849
	step [242/325], loss=76.4352
	step [243/325], loss=69.7726
	step [244/325], loss=64.7854
	step [245/325], loss=66.8401
	step [246/325], loss=70.8342
	step [247/325], loss=67.8012
	step [248/325], loss=70.8314
	step [249/325], loss=64.4209
	step [250/325], loss=54.4435
	step [251/325], loss=66.6057
	step [252/325], loss=76.5675
	step [253/325], loss=61.5034
	step [254/325], loss=59.7785
	step [255/325], loss=55.2314
	step [256/325], loss=64.1539
	step [257/325], loss=73.7759
	step [258/325], loss=78.0938
	step [259/325], loss=72.1635
	step [260/325], loss=52.5765
	step [261/325], loss=73.4158
	step [262/325], loss=92.2362
	step [263/325], loss=72.4000
	step [264/325], loss=75.1467
	step [265/325], loss=77.6393
	step [266/325], loss=77.3190
	step [267/325], loss=63.8537
	step [268/325], loss=49.4218
	step [269/325], loss=55.2637
	step [270/325], loss=60.9280
	step [271/325], loss=76.9042
	step [272/325], loss=60.3244
	step [273/325], loss=76.9352
	step [274/325], loss=55.7967
	step [275/325], loss=84.2220
	step [276/325], loss=75.4224
	step [277/325], loss=61.7109
	step [278/325], loss=60.7521
	step [279/325], loss=62.7052
	step [280/325], loss=73.2219
	step [281/325], loss=98.9470
	step [282/325], loss=63.4363
	step [283/325], loss=58.7146
	step [284/325], loss=74.1587
	step [285/325], loss=81.3190
	step [286/325], loss=62.1591
	step [287/325], loss=66.0318
	step [288/325], loss=69.7573
	step [289/325], loss=67.5074
	step [290/325], loss=67.1146
	step [291/325], loss=67.6476
	step [292/325], loss=86.8244
	step [293/325], loss=49.1097
	step [294/325], loss=53.1334
	step [295/325], loss=73.8875
	step [296/325], loss=67.7855
	step [297/325], loss=75.4263
	step [298/325], loss=77.3735
	step [299/325], loss=72.0912
	step [300/325], loss=62.9759
	step [301/325], loss=67.0740
	step [302/325], loss=66.5132
	step [303/325], loss=73.0576
	step [304/325], loss=85.9828
	step [305/325], loss=81.8794
	step [306/325], loss=71.2242
	step [307/325], loss=62.9255
	step [308/325], loss=75.4981
	step [309/325], loss=74.6473
	step [310/325], loss=66.1224
	step [311/325], loss=67.9515
	step [312/325], loss=79.7973
	step [313/325], loss=68.5172
	step [314/325], loss=71.1603
	step [315/325], loss=55.6773
	step [316/325], loss=64.7149
	step [317/325], loss=77.7596
	step [318/325], loss=74.0414
	step [319/325], loss=60.0478
	step [320/325], loss=51.3039
	step [321/325], loss=56.4254
	step [322/325], loss=69.1048
	step [323/325], loss=60.6290
	step [324/325], loss=57.1542
	step [325/325], loss=42.3230
	Evaluating
	loss=0.0094, precision=0.4391, recall=0.8823, f1=0.5864
Training epoch 20
	step [1/325], loss=67.4904
	step [2/325], loss=57.6702
	step [3/325], loss=83.6496
	step [4/325], loss=57.7248
	step [5/325], loss=62.7574
	step [6/325], loss=58.6027
	step [7/325], loss=71.3073
	step [8/325], loss=87.2741
	step [9/325], loss=73.5319
	step [10/325], loss=66.5681
	step [11/325], loss=69.9451
	step [12/325], loss=65.9202
	step [13/325], loss=72.0909
	step [14/325], loss=77.9631
	step [15/325], loss=81.2402
	step [16/325], loss=72.3366
	step [17/325], loss=68.6805
	step [18/325], loss=86.0920
	step [19/325], loss=82.4236
	step [20/325], loss=81.4382
	step [21/325], loss=61.7251
	step [22/325], loss=71.4318
	step [23/325], loss=70.5326
	step [24/325], loss=73.6917
	step [25/325], loss=58.7398
	step [26/325], loss=79.8750
	step [27/325], loss=67.7686
	step [28/325], loss=86.8421
	step [29/325], loss=65.4406
	step [30/325], loss=63.6771
	step [31/325], loss=72.3738
	step [32/325], loss=67.8895
	step [33/325], loss=64.7310
	step [34/325], loss=59.8443
	step [35/325], loss=74.7139
	step [36/325], loss=77.5544
	step [37/325], loss=65.3261
	step [38/325], loss=69.4601
	step [39/325], loss=69.2974
	step [40/325], loss=57.3324
	step [41/325], loss=89.0870
	step [42/325], loss=55.7088
	step [43/325], loss=76.6599
	step [44/325], loss=61.7276
	step [45/325], loss=62.0391
	step [46/325], loss=77.5213
	step [47/325], loss=69.6552
	step [48/325], loss=68.4418
	step [49/325], loss=54.7546
	step [50/325], loss=75.1080
	step [51/325], loss=65.9862
	step [52/325], loss=67.2183
	step [53/325], loss=59.7988
	step [54/325], loss=57.5480
	step [55/325], loss=77.7684
	step [56/325], loss=93.3776
	step [57/325], loss=67.6163
	step [58/325], loss=69.4569
	step [59/325], loss=71.3609
	step [60/325], loss=64.2229
	step [61/325], loss=56.1042
	step [62/325], loss=47.1331
	step [63/325], loss=78.5738
	step [64/325], loss=71.5164
	step [65/325], loss=73.2040
	step [66/325], loss=62.9004
	step [67/325], loss=58.7315
	step [68/325], loss=66.7000
	step [69/325], loss=58.8300
	step [70/325], loss=72.0334
	step [71/325], loss=60.6900
	step [72/325], loss=80.5728
	step [73/325], loss=71.4416
	step [74/325], loss=57.3512
	step [75/325], loss=58.7741
	step [76/325], loss=57.6958
	step [77/325], loss=88.1271
	step [78/325], loss=59.6702
	step [79/325], loss=69.3390
	step [80/325], loss=67.1782
	step [81/325], loss=65.8048
	step [82/325], loss=81.7455
	step [83/325], loss=80.6341
	step [84/325], loss=73.0560
	step [85/325], loss=64.6546
	step [86/325], loss=77.7009
	step [87/325], loss=69.6998
	step [88/325], loss=63.3142
	step [89/325], loss=71.1955
	step [90/325], loss=78.8055
	step [91/325], loss=79.0641
	step [92/325], loss=59.0798
	step [93/325], loss=71.6971
	step [94/325], loss=57.3246
	step [95/325], loss=57.2784
	step [96/325], loss=77.7664
	step [97/325], loss=65.2555
	step [98/325], loss=63.6908
	step [99/325], loss=61.4901
	step [100/325], loss=71.0371
	step [101/325], loss=74.9566
	step [102/325], loss=64.7222
	step [103/325], loss=63.6354
	step [104/325], loss=71.0827
	step [105/325], loss=83.1891
	step [106/325], loss=84.3357
	step [107/325], loss=61.8876
	step [108/325], loss=56.4545
	step [109/325], loss=57.3290
	step [110/325], loss=62.4485
	step [111/325], loss=74.2602
	step [112/325], loss=65.9406
	step [113/325], loss=66.7529
	step [114/325], loss=46.7199
	step [115/325], loss=82.1323
	step [116/325], loss=53.1668
	step [117/325], loss=73.8763
	step [118/325], loss=81.5901
	step [119/325], loss=62.1198
	step [120/325], loss=65.4394
	step [121/325], loss=57.6238
	step [122/325], loss=65.4635
	step [123/325], loss=60.3540
	step [124/325], loss=54.7249
	step [125/325], loss=50.4309
	step [126/325], loss=76.3017
	step [127/325], loss=74.2776
	step [128/325], loss=79.0017
	step [129/325], loss=62.7786
	step [130/325], loss=54.4061
	step [131/325], loss=67.9540
	step [132/325], loss=62.9698
	step [133/325], loss=69.6332
	step [134/325], loss=63.9655
	step [135/325], loss=65.4713
	step [136/325], loss=67.1486
	step [137/325], loss=71.9726
	step [138/325], loss=73.4827
	step [139/325], loss=68.3963
	step [140/325], loss=85.7205
	step [141/325], loss=74.8550
	step [142/325], loss=56.4432
	step [143/325], loss=69.7309
	step [144/325], loss=53.9932
	step [145/325], loss=55.8526
	step [146/325], loss=61.9134
	step [147/325], loss=69.4815
	step [148/325], loss=59.5056
	step [149/325], loss=75.8565
	step [150/325], loss=79.3563
	step [151/325], loss=69.8491
	step [152/325], loss=73.2000
	step [153/325], loss=83.4155
	step [154/325], loss=75.5837
	step [155/325], loss=63.8876
	step [156/325], loss=73.9980
	step [157/325], loss=64.5904
	step [158/325], loss=66.5599
	step [159/325], loss=54.7855
	step [160/325], loss=55.6255
	step [161/325], loss=55.0173
	step [162/325], loss=52.4947
	step [163/325], loss=66.7637
	step [164/325], loss=71.4679
	step [165/325], loss=76.5511
	step [166/325], loss=72.6423
	step [167/325], loss=77.5513
	step [168/325], loss=62.2644
	step [169/325], loss=82.2491
	step [170/325], loss=74.3503
	step [171/325], loss=68.9940
	step [172/325], loss=77.2579
	step [173/325], loss=74.8345
	step [174/325], loss=81.3535
	step [175/325], loss=62.0552
	step [176/325], loss=69.6555
	step [177/325], loss=63.8039
	step [178/325], loss=77.7333
	step [179/325], loss=55.5323
	step [180/325], loss=63.2647
	step [181/325], loss=65.0389
	step [182/325], loss=69.1156
	step [183/325], loss=70.5259
	step [184/325], loss=58.1427
	step [185/325], loss=62.0090
	step [186/325], loss=67.4883
	step [187/325], loss=85.5452
	step [188/325], loss=68.3427
	step [189/325], loss=77.3239
	step [190/325], loss=64.9818
	step [191/325], loss=73.0638
	step [192/325], loss=50.5403
	step [193/325], loss=81.6484
	step [194/325], loss=62.1980
	step [195/325], loss=82.7620
	step [196/325], loss=77.2240
	step [197/325], loss=70.4115
	step [198/325], loss=74.8597
	step [199/325], loss=66.3757
	step [200/325], loss=65.3570
	step [201/325], loss=84.4994
	step [202/325], loss=75.3398
	step [203/325], loss=72.7141
	step [204/325], loss=64.7826
	step [205/325], loss=56.6238
	step [206/325], loss=84.5419
	step [207/325], loss=68.8246
	step [208/325], loss=63.2267
	step [209/325], loss=55.2643
	step [210/325], loss=73.2927
	step [211/325], loss=74.9665
	step [212/325], loss=68.4289
	step [213/325], loss=70.4704
	step [214/325], loss=57.3053
	step [215/325], loss=63.0068
	step [216/325], loss=63.8454
	step [217/325], loss=84.3055
	step [218/325], loss=83.2866
	step [219/325], loss=73.5095
	step [220/325], loss=66.9879
	step [221/325], loss=69.5883
	step [222/325], loss=88.2970
	step [223/325], loss=78.6711
	step [224/325], loss=72.3470
	step [225/325], loss=77.4992
	step [226/325], loss=61.8615
	step [227/325], loss=65.3993
	step [228/325], loss=84.2478
	step [229/325], loss=74.4097
	step [230/325], loss=60.5356
	step [231/325], loss=72.1250
	step [232/325], loss=65.2688
	step [233/325], loss=61.1270
	step [234/325], loss=65.9752
	step [235/325], loss=67.2648
	step [236/325], loss=61.9956
	step [237/325], loss=65.7262
	step [238/325], loss=66.0488
	step [239/325], loss=77.1089
	step [240/325], loss=68.8340
	step [241/325], loss=70.9738
	step [242/325], loss=65.0676
	step [243/325], loss=65.8423
	step [244/325], loss=70.5350
	step [245/325], loss=75.6856
	step [246/325], loss=61.1891
	step [247/325], loss=77.0650
	step [248/325], loss=87.7788
	step [249/325], loss=70.2983
	step [250/325], loss=64.0372
	step [251/325], loss=56.9850
	step [252/325], loss=65.8965
	step [253/325], loss=73.5257
	step [254/325], loss=66.8692
	step [255/325], loss=77.7237
	step [256/325], loss=72.4358
	step [257/325], loss=56.3924
	step [258/325], loss=62.1485
	step [259/325], loss=69.6600
	step [260/325], loss=73.6031
	step [261/325], loss=73.2645
	step [262/325], loss=74.0373
	step [263/325], loss=82.3062
	step [264/325], loss=68.1736
	step [265/325], loss=73.5045
	step [266/325], loss=68.4803
	step [267/325], loss=69.1220
	step [268/325], loss=64.9963
	step [269/325], loss=64.6658
	step [270/325], loss=78.4766
	step [271/325], loss=64.2378
	step [272/325], loss=70.7762
	step [273/325], loss=71.9332
	step [274/325], loss=75.1703
	step [275/325], loss=56.6696
	step [276/325], loss=90.9849
	step [277/325], loss=65.3911
	step [278/325], loss=67.7314
	step [279/325], loss=76.8539
	step [280/325], loss=75.8484
	step [281/325], loss=83.0056
	step [282/325], loss=73.0459
	step [283/325], loss=88.8994
	step [284/325], loss=75.9816
	step [285/325], loss=58.9926
	step [286/325], loss=71.3671
	step [287/325], loss=74.5741
	step [288/325], loss=71.4891
	step [289/325], loss=66.8981
	step [290/325], loss=54.9029
	step [291/325], loss=79.0686
	step [292/325], loss=66.2126
	step [293/325], loss=73.2506
	step [294/325], loss=81.0648
	step [295/325], loss=66.1070
	step [296/325], loss=72.2528
	step [297/325], loss=60.0106
	step [298/325], loss=72.6226
	step [299/325], loss=46.9662
	step [300/325], loss=65.9080
	step [301/325], loss=66.8502
	step [302/325], loss=69.4686
	step [303/325], loss=72.6180
	step [304/325], loss=78.4775
	step [305/325], loss=69.2870
	step [306/325], loss=73.3638
	step [307/325], loss=65.8278
	step [308/325], loss=67.1972
	step [309/325], loss=70.7052
	step [310/325], loss=67.8539
	step [311/325], loss=60.7990
	step [312/325], loss=66.8523
	step [313/325], loss=71.2346
	step [314/325], loss=68.8847
	step [315/325], loss=67.0684
	step [316/325], loss=78.4484
	step [317/325], loss=72.8311
	step [318/325], loss=70.5465
	step [319/325], loss=72.7704
	step [320/325], loss=53.4372
	step [321/325], loss=78.1742
	step [322/325], loss=70.5368
	step [323/325], loss=63.0827
	step [324/325], loss=68.5565
	step [325/325], loss=42.5991
	Evaluating
	loss=0.0088, precision=0.4048, recall=0.8804, f1=0.5546
Training epoch 21
	step [1/325], loss=65.3812
	step [2/325], loss=64.9977
	step [3/325], loss=64.1934
	step [4/325], loss=59.7657
	step [5/325], loss=63.1923
	step [6/325], loss=57.4380
	step [7/325], loss=62.1718
	step [8/325], loss=57.5864
	step [9/325], loss=76.9381
	step [10/325], loss=71.0034
	step [11/325], loss=70.3173
	step [12/325], loss=79.1801
	step [13/325], loss=67.6718
	step [14/325], loss=77.8269
	step [15/325], loss=55.1711
	step [16/325], loss=57.7833
	step [17/325], loss=63.5392
	step [18/325], loss=69.5792
	step [19/325], loss=63.4450
	step [20/325], loss=54.7423
	step [21/325], loss=70.7957
	step [22/325], loss=59.5702
	step [23/325], loss=85.1018
	step [24/325], loss=73.3894
	step [25/325], loss=61.4349
	step [26/325], loss=56.5282
	step [27/325], loss=61.0918
	step [28/325], loss=53.2645
	step [29/325], loss=66.5295
	step [30/325], loss=79.8096
	step [31/325], loss=73.9530
	step [32/325], loss=66.0786
	step [33/325], loss=76.3036
	step [34/325], loss=70.6872
	step [35/325], loss=59.4617
	step [36/325], loss=65.6062
	step [37/325], loss=69.1347
	step [38/325], loss=63.7207
	step [39/325], loss=66.0312
	step [40/325], loss=62.3475
	step [41/325], loss=52.7454
	step [42/325], loss=58.8565
	step [43/325], loss=61.2083
	step [44/325], loss=78.0979
	step [45/325], loss=76.8074
	step [46/325], loss=57.8203
	step [47/325], loss=90.5733
	step [48/325], loss=75.9190
	step [49/325], loss=72.1640
	step [50/325], loss=66.2420
	step [51/325], loss=73.0194
	step [52/325], loss=74.3434
	step [53/325], loss=71.4514
	step [54/325], loss=53.3477
	step [55/325], loss=80.0654
	step [56/325], loss=63.9608
	step [57/325], loss=63.1681
	step [58/325], loss=56.7829
	step [59/325], loss=54.3092
	step [60/325], loss=72.1102
	step [61/325], loss=73.5569
	step [62/325], loss=60.7932
	step [63/325], loss=90.1969
	step [64/325], loss=69.5465
	step [65/325], loss=66.9589
	step [66/325], loss=49.0168
	step [67/325], loss=65.5161
	step [68/325], loss=75.2259
	step [69/325], loss=70.7949
	step [70/325], loss=64.6275
	step [71/325], loss=65.9249
	step [72/325], loss=81.1329
	step [73/325], loss=49.7038
	step [74/325], loss=61.5920
	step [75/325], loss=68.6358
	step [76/325], loss=81.1610
	step [77/325], loss=64.7784
	step [78/325], loss=69.7249
	step [79/325], loss=60.8943
	step [80/325], loss=72.1891
	step [81/325], loss=65.2503
	step [82/325], loss=70.6953
	step [83/325], loss=50.6028
	step [84/325], loss=59.6573
	step [85/325], loss=67.7288
	step [86/325], loss=68.2760
	step [87/325], loss=50.1291
	step [88/325], loss=76.5018
	step [89/325], loss=77.2787
	step [90/325], loss=60.2801
	step [91/325], loss=70.5611
	step [92/325], loss=76.4479
	step [93/325], loss=78.2690
	step [94/325], loss=62.4111
	step [95/325], loss=87.7480
	step [96/325], loss=60.4198
	step [97/325], loss=65.9098
	step [98/325], loss=70.1216
	step [99/325], loss=60.5056
	step [100/325], loss=67.2769
	step [101/325], loss=68.7516
	step [102/325], loss=61.5121
	step [103/325], loss=95.8195
	step [104/325], loss=70.3672
	step [105/325], loss=59.8317
	step [106/325], loss=67.3515
	step [107/325], loss=64.9167
	step [108/325], loss=58.9924
	step [109/325], loss=62.9514
	step [110/325], loss=57.7237
	step [111/325], loss=72.6282
	step [112/325], loss=70.7746
	step [113/325], loss=47.7314
	step [114/325], loss=80.1115
	step [115/325], loss=77.2002
	step [116/325], loss=60.6150
	step [117/325], loss=69.4005
	step [118/325], loss=85.1761
	step [119/325], loss=76.1401
	step [120/325], loss=61.0331
	step [121/325], loss=65.9358
	step [122/325], loss=70.5310
	step [123/325], loss=82.6277
	step [124/325], loss=67.8953
	step [125/325], loss=77.2974
	step [126/325], loss=69.0071
	step [127/325], loss=51.0438
	step [128/325], loss=60.5220
	step [129/325], loss=76.3114
	step [130/325], loss=66.6360
	step [131/325], loss=60.7794
	step [132/325], loss=69.9731
	step [133/325], loss=60.4943
	step [134/325], loss=76.4053
	step [135/325], loss=66.4193
	step [136/325], loss=65.8994
	step [137/325], loss=61.5539
	step [138/325], loss=77.7171
	step [139/325], loss=80.3260
	step [140/325], loss=55.6644
	step [141/325], loss=59.4967
	step [142/325], loss=75.1393
	step [143/325], loss=83.2928
	step [144/325], loss=81.6646
	step [145/325], loss=62.6833
	step [146/325], loss=75.1404
	step [147/325], loss=58.1720
	step [148/325], loss=57.2237
	step [149/325], loss=77.9972
	step [150/325], loss=63.5993
	step [151/325], loss=58.8785
	step [152/325], loss=77.0828
	step [153/325], loss=65.6236
	step [154/325], loss=84.2905
	step [155/325], loss=57.3997
	step [156/325], loss=79.9455
	step [157/325], loss=65.3114
	step [158/325], loss=78.2181
	step [159/325], loss=69.7699
	step [160/325], loss=70.2210
	step [161/325], loss=76.3668
	step [162/325], loss=75.1085
	step [163/325], loss=81.2687
	step [164/325], loss=62.9640
	step [165/325], loss=76.7245
	step [166/325], loss=75.5768
	step [167/325], loss=71.7150
	step [168/325], loss=64.5069
	step [169/325], loss=76.3805
	step [170/325], loss=74.2286
	step [171/325], loss=72.1544
	step [172/325], loss=76.4491
	step [173/325], loss=83.0932
	step [174/325], loss=79.6461
	step [175/325], loss=65.7161
	step [176/325], loss=55.1770
	step [177/325], loss=76.2229
	step [178/325], loss=62.4893
	step [179/325], loss=79.0227
	step [180/325], loss=79.3108
	step [181/325], loss=66.2662
	step [182/325], loss=63.6257
	step [183/325], loss=63.1502
	step [184/325], loss=57.4375
	step [185/325], loss=71.6780
	step [186/325], loss=65.8174
	step [187/325], loss=59.4993
	step [188/325], loss=76.0840
	step [189/325], loss=59.2452
	step [190/325], loss=54.3058
	step [191/325], loss=67.2787
	step [192/325], loss=79.4231
	step [193/325], loss=52.9268
	step [194/325], loss=74.4695
	step [195/325], loss=54.3008
	step [196/325], loss=75.0214
	step [197/325], loss=93.8727
	step [198/325], loss=73.0342
	step [199/325], loss=70.8048
	step [200/325], loss=62.8790
	step [201/325], loss=79.0031
	step [202/325], loss=84.8876
	step [203/325], loss=54.3579
	step [204/325], loss=61.0992
	step [205/325], loss=71.8256
	step [206/325], loss=58.6890
	step [207/325], loss=77.3778
	step [208/325], loss=67.9236
	step [209/325], loss=62.5746
	step [210/325], loss=64.8680
	step [211/325], loss=61.2467
	step [212/325], loss=72.7713
	step [213/325], loss=59.0176
	step [214/325], loss=71.8398
	step [215/325], loss=62.8264
	step [216/325], loss=62.1453
	step [217/325], loss=71.0824
	step [218/325], loss=69.6540
	step [219/325], loss=56.4009
	step [220/325], loss=58.7084
	step [221/325], loss=70.6936
	step [222/325], loss=68.5675
	step [223/325], loss=51.6240
	step [224/325], loss=67.0591
	step [225/325], loss=68.2506
	step [226/325], loss=59.2281
	step [227/325], loss=68.4933
	step [228/325], loss=65.2456
	step [229/325], loss=59.9235
	step [230/325], loss=71.3156
	step [231/325], loss=91.7601
	step [232/325], loss=67.0615
	step [233/325], loss=72.5509
	step [234/325], loss=62.8876
	step [235/325], loss=72.6837
	step [236/325], loss=71.9708
	step [237/325], loss=82.0760
	step [238/325], loss=73.2866
	step [239/325], loss=65.5454
	step [240/325], loss=79.7120
	step [241/325], loss=67.9766
	step [242/325], loss=71.5421
	step [243/325], loss=62.7198
	step [244/325], loss=68.1143
	step [245/325], loss=61.2253
	step [246/325], loss=60.2805
	step [247/325], loss=64.6374
	step [248/325], loss=52.1332
	step [249/325], loss=54.3401
	step [250/325], loss=75.9403
	step [251/325], loss=53.3103
	step [252/325], loss=75.9712
	step [253/325], loss=67.2386
	step [254/325], loss=77.2697
	step [255/325], loss=71.4068
	step [256/325], loss=60.9776
	step [257/325], loss=73.5792
	step [258/325], loss=53.9597
	step [259/325], loss=73.5879
	step [260/325], loss=84.2914
	step [261/325], loss=66.6670
	step [262/325], loss=81.5760
	step [263/325], loss=66.2372
	step [264/325], loss=63.2903
	step [265/325], loss=82.0918
	step [266/325], loss=64.4306
	step [267/325], loss=66.4451
	step [268/325], loss=76.9507
	step [269/325], loss=63.2320
	step [270/325], loss=57.1012
	step [271/325], loss=65.6476
	step [272/325], loss=59.5106
	step [273/325], loss=54.9879
	step [274/325], loss=82.3374
	step [275/325], loss=75.5596
	step [276/325], loss=64.6042
	step [277/325], loss=78.8456
	step [278/325], loss=80.7845
	step [279/325], loss=72.1347
	step [280/325], loss=55.7446
	step [281/325], loss=59.2753
	step [282/325], loss=72.9824
	step [283/325], loss=80.7433
	step [284/325], loss=70.4364
	step [285/325], loss=62.9060
	step [286/325], loss=58.5232
	step [287/325], loss=77.1059
	step [288/325], loss=80.4999
	step [289/325], loss=64.4239
	step [290/325], loss=82.4886
	step [291/325], loss=44.1176
	step [292/325], loss=81.1654
	step [293/325], loss=95.2465
	step [294/325], loss=62.4588
	step [295/325], loss=64.0557
	step [296/325], loss=66.2980
	step [297/325], loss=65.0328
	step [298/325], loss=77.4494
	step [299/325], loss=67.7818
	step [300/325], loss=78.1600
	step [301/325], loss=63.4837
	step [302/325], loss=73.4938
	step [303/325], loss=62.8868
	step [304/325], loss=91.0256
	step [305/325], loss=64.1839
	step [306/325], loss=68.7572
	step [307/325], loss=68.1985
	step [308/325], loss=60.1460
	step [309/325], loss=58.6920
	step [310/325], loss=73.8963
	step [311/325], loss=94.3826
	step [312/325], loss=61.9781
	step [313/325], loss=56.1920
	step [314/325], loss=81.0752
	step [315/325], loss=51.3930
	step [316/325], loss=87.7947
	step [317/325], loss=74.4688
	step [318/325], loss=60.2449
	step [319/325], loss=79.3628
	step [320/325], loss=72.7094
	step [321/325], loss=63.8845
	step [322/325], loss=73.4504
	step [323/325], loss=65.3977
	step [324/325], loss=76.2707
	step [325/325], loss=36.3446
	Evaluating
	loss=0.0070, precision=0.4874, recall=0.8699, f1=0.6248
Training epoch 22
	step [1/325], loss=67.3100
	step [2/325], loss=59.6857
	step [3/325], loss=67.5770
	step [4/325], loss=71.8443
	step [5/325], loss=70.1246
	step [6/325], loss=70.3126
	step [7/325], loss=88.6428
	step [8/325], loss=65.1922
	step [9/325], loss=97.6520
	step [10/325], loss=58.3245
	step [11/325], loss=65.4780
	step [12/325], loss=70.0781
	step [13/325], loss=50.4217
	step [14/325], loss=55.3123
	step [15/325], loss=72.1926
	step [16/325], loss=70.1532
	step [17/325], loss=62.5949
	step [18/325], loss=58.2179
	step [19/325], loss=75.9626
	step [20/325], loss=72.5761
	step [21/325], loss=63.2333
	step [22/325], loss=59.3890
	step [23/325], loss=69.9520
	step [24/325], loss=55.5394
	step [25/325], loss=67.1004
	step [26/325], loss=69.4762
	step [27/325], loss=71.5847
	step [28/325], loss=75.8605
	step [29/325], loss=72.3696
	step [30/325], loss=68.4029
	step [31/325], loss=59.7591
	step [32/325], loss=64.1894
	step [33/325], loss=57.0438
	step [34/325], loss=56.5236
	step [35/325], loss=70.8086
	step [36/325], loss=65.0481
	step [37/325], loss=75.1562
	step [38/325], loss=59.9265
	step [39/325], loss=57.4427
	step [40/325], loss=81.7287
	step [41/325], loss=53.7051
	step [42/325], loss=73.6253
	step [43/325], loss=61.2704
	step [44/325], loss=71.6866
	step [45/325], loss=68.5914
	step [46/325], loss=59.5295
	step [47/325], loss=63.5883
	step [48/325], loss=67.6040
	step [49/325], loss=75.9197
	step [50/325], loss=60.9054
	step [51/325], loss=52.8451
	step [52/325], loss=77.7869
	step [53/325], loss=75.7183
	step [54/325], loss=58.9276
	step [55/325], loss=79.8630
	step [56/325], loss=71.2719
	step [57/325], loss=75.3642
	step [58/325], loss=63.9572
	step [59/325], loss=57.6062
	step [60/325], loss=68.1843
	step [61/325], loss=70.8456
	step [62/325], loss=63.8050
	step [63/325], loss=64.0050
	step [64/325], loss=75.3300
	step [65/325], loss=76.3223
	step [66/325], loss=67.9726
	step [67/325], loss=75.9091
	step [68/325], loss=69.0935
	step [69/325], loss=59.4033
	step [70/325], loss=66.7894
	step [71/325], loss=51.4685
	step [72/325], loss=63.8123
	step [73/325], loss=70.4894
	step [74/325], loss=55.5752
	step [75/325], loss=64.8807
	step [76/325], loss=62.2907
	step [77/325], loss=66.7403
	step [78/325], loss=63.9535
	step [79/325], loss=89.0092
	step [80/325], loss=69.7795
	step [81/325], loss=57.5804
	step [82/325], loss=67.4617
	step [83/325], loss=74.6163
	step [84/325], loss=80.0058
	step [85/325], loss=71.0378
	step [86/325], loss=58.0553
	step [87/325], loss=61.2843
	step [88/325], loss=72.6925
	step [89/325], loss=75.7402
	step [90/325], loss=62.7953
	step [91/325], loss=59.8428
	step [92/325], loss=72.9674
	step [93/325], loss=64.2958
	step [94/325], loss=62.3145
	step [95/325], loss=81.0557
	step [96/325], loss=74.2673
	step [97/325], loss=60.0816
	step [98/325], loss=56.7713
	step [99/325], loss=68.4704
	step [100/325], loss=47.3828
	step [101/325], loss=53.9187
	step [102/325], loss=68.8436
	step [103/325], loss=72.4828
	step [104/325], loss=79.9311
	step [105/325], loss=57.2968
	step [106/325], loss=75.8572
	step [107/325], loss=65.7629
	step [108/325], loss=83.8996
	step [109/325], loss=61.9704
	step [110/325], loss=73.9142
	step [111/325], loss=61.8595
	step [112/325], loss=56.8815
	step [113/325], loss=60.8764
	step [114/325], loss=68.6298
	step [115/325], loss=61.4862
	step [116/325], loss=86.2169
	step [117/325], loss=70.1234
	step [118/325], loss=81.5516
	step [119/325], loss=84.0482
	step [120/325], loss=73.2435
	step [121/325], loss=61.2319
	step [122/325], loss=66.7620
	step [123/325], loss=71.7667
	step [124/325], loss=57.8519
	step [125/325], loss=71.2043
	step [126/325], loss=59.9824
	step [127/325], loss=76.0499
	step [128/325], loss=74.0006
	step [129/325], loss=67.5562
	step [130/325], loss=62.2853
	step [131/325], loss=73.0045
	step [132/325], loss=70.2461
	step [133/325], loss=79.6682
	step [134/325], loss=57.1667
	step [135/325], loss=61.5394
	step [136/325], loss=67.3456
	step [137/325], loss=62.1716
	step [138/325], loss=66.1608
	step [139/325], loss=57.4959
	step [140/325], loss=72.7955
	step [141/325], loss=53.0007
	step [142/325], loss=67.6670
	step [143/325], loss=73.1794
	step [144/325], loss=59.7610
	step [145/325], loss=88.4093
	step [146/325], loss=57.5968
	step [147/325], loss=48.9889
	step [148/325], loss=72.9000
	step [149/325], loss=78.6425
	step [150/325], loss=76.6069
	step [151/325], loss=75.4744
	step [152/325], loss=72.2418
	step [153/325], loss=73.5264
	step [154/325], loss=61.6195
	step [155/325], loss=75.2808
	step [156/325], loss=79.6828
	step [157/325], loss=79.7959
	step [158/325], loss=61.7524
	step [159/325], loss=52.9438
	step [160/325], loss=54.9722
	step [161/325], loss=63.0880
	step [162/325], loss=84.8734
	step [163/325], loss=71.8099
	step [164/325], loss=50.9598
	step [165/325], loss=61.7406
	step [166/325], loss=53.6826
	step [167/325], loss=84.9866
	step [168/325], loss=53.0837
	step [169/325], loss=75.4306
	step [170/325], loss=75.4074
	step [171/325], loss=71.2560
	step [172/325], loss=64.7303
	step [173/325], loss=87.7672
	step [174/325], loss=62.6628
	step [175/325], loss=53.7229
	step [176/325], loss=83.3219
	step [177/325], loss=62.9824
	step [178/325], loss=54.2819
	step [179/325], loss=63.8788
	step [180/325], loss=66.5819
	step [181/325], loss=83.7995
	step [182/325], loss=72.0141
	step [183/325], loss=65.0271
	step [184/325], loss=66.9843
	step [185/325], loss=70.0062
	step [186/325], loss=65.9170
	step [187/325], loss=63.0919
	step [188/325], loss=69.4491
	step [189/325], loss=74.0665
	step [190/325], loss=62.2486
	step [191/325], loss=61.1159
	step [192/325], loss=65.5993
	step [193/325], loss=58.7020
	step [194/325], loss=57.0477
	step [195/325], loss=61.4300
	step [196/325], loss=69.6889
	step [197/325], loss=73.6159
	step [198/325], loss=68.5381
	step [199/325], loss=51.7321
	step [200/325], loss=64.2187
	step [201/325], loss=62.6949
	step [202/325], loss=90.6734
	step [203/325], loss=55.9551
	step [204/325], loss=52.2020
	step [205/325], loss=63.2937
	step [206/325], loss=62.0430
	step [207/325], loss=73.7721
	step [208/325], loss=54.0433
	step [209/325], loss=79.7289
	step [210/325], loss=61.0098
	step [211/325], loss=71.0254
	step [212/325], loss=82.8888
	step [213/325], loss=68.3436
	step [214/325], loss=82.8018
	step [215/325], loss=63.8026
	step [216/325], loss=69.3574
	step [217/325], loss=48.7070
	step [218/325], loss=79.7211
	step [219/325], loss=67.7143
	step [220/325], loss=66.8325
	step [221/325], loss=55.6712
	step [222/325], loss=74.4171
	step [223/325], loss=79.0341
	step [224/325], loss=60.8158
	step [225/325], loss=70.1854
	step [226/325], loss=69.4201
	step [227/325], loss=72.8396
	step [228/325], loss=79.2391
	step [229/325], loss=62.9382
	step [230/325], loss=63.0896
	step [231/325], loss=68.4562
	step [232/325], loss=59.8015
	step [233/325], loss=55.4716
	step [234/325], loss=70.2939
	step [235/325], loss=68.0301
	step [236/325], loss=66.7948
	step [237/325], loss=54.5980
	step [238/325], loss=64.0911
	step [239/325], loss=71.3476
	step [240/325], loss=83.6441
	step [241/325], loss=73.5090
	step [242/325], loss=76.0934
	step [243/325], loss=82.9095
	step [244/325], loss=77.1481
	step [245/325], loss=80.2447
	step [246/325], loss=64.0173
	step [247/325], loss=79.4166
	step [248/325], loss=65.6259
	step [249/325], loss=73.1902
	step [250/325], loss=71.6320
	step [251/325], loss=55.2623
	step [252/325], loss=52.7104
	step [253/325], loss=67.6773
	step [254/325], loss=78.3106
	step [255/325], loss=74.5988
	step [256/325], loss=74.0234
	step [257/325], loss=44.2342
	step [258/325], loss=61.1654
	step [259/325], loss=78.1103
	step [260/325], loss=75.2617
	step [261/325], loss=81.7566
	step [262/325], loss=57.4498
	step [263/325], loss=42.0734
	step [264/325], loss=73.7542
	step [265/325], loss=65.0961
	step [266/325], loss=56.5276
	step [267/325], loss=81.2620
	step [268/325], loss=64.1956
	step [269/325], loss=82.3773
	step [270/325], loss=70.5660
	step [271/325], loss=55.7363
	step [272/325], loss=81.7540
	step [273/325], loss=68.7318
	step [274/325], loss=76.2172
	step [275/325], loss=68.0790
	step [276/325], loss=63.5738
	step [277/325], loss=66.3016
	step [278/325], loss=82.4345
	step [279/325], loss=69.7057
	step [280/325], loss=47.5474
	step [281/325], loss=67.7067
	step [282/325], loss=66.8393
	step [283/325], loss=71.9232
	step [284/325], loss=70.9131
	step [285/325], loss=62.0592
	step [286/325], loss=59.3364
	step [287/325], loss=66.5358
	step [288/325], loss=62.9934
	step [289/325], loss=66.1258
	step [290/325], loss=70.7900
	step [291/325], loss=85.4654
	step [292/325], loss=64.7940
	step [293/325], loss=88.7646
	step [294/325], loss=76.5350
	step [295/325], loss=82.4136
	step [296/325], loss=68.4977
	step [297/325], loss=70.1832
	step [298/325], loss=80.1084
	step [299/325], loss=66.3351
	step [300/325], loss=64.7849
	step [301/325], loss=65.4530
	step [302/325], loss=48.9690
	step [303/325], loss=78.6813
	step [304/325], loss=62.7084
	step [305/325], loss=68.7889
	step [306/325], loss=90.1438
	step [307/325], loss=69.6805
	step [308/325], loss=70.8993
	step [309/325], loss=66.8412
	step [310/325], loss=64.1976
	step [311/325], loss=60.6769
	step [312/325], loss=75.9712
	step [313/325], loss=68.3678
	step [314/325], loss=73.7689
	step [315/325], loss=73.6110
	step [316/325], loss=60.4343
	step [317/325], loss=78.1719
	step [318/325], loss=71.9293
	step [319/325], loss=56.6144
	step [320/325], loss=71.4091
	step [321/325], loss=66.6681
	step [322/325], loss=66.0513
	step [323/325], loss=65.4651
	step [324/325], loss=70.9658
	step [325/325], loss=44.4268
	Evaluating
