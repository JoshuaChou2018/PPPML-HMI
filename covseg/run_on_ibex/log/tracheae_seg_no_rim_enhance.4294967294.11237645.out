Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: True test_id 0
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9412 # image files with weight 9372
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2471 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9372
Using 4 GPUs
best_f1 is: 0.5159224197816442
Going to train epochs [4-33]
Training epoch 4
	step [1/147], loss=156.2520
	step [2/147], loss=152.6969
	step [3/147], loss=158.7731
	step [4/147], loss=172.6720
	step [5/147], loss=177.2776
	step [6/147], loss=194.3344
	step [7/147], loss=188.9558
	step [8/147], loss=149.8385
	step [9/147], loss=161.0582
	step [10/147], loss=176.9655
	step [11/147], loss=166.5337
	step [12/147], loss=163.5732
	step [13/147], loss=168.1865
	step [14/147], loss=164.4731
	step [15/147], loss=149.3387
	step [16/147], loss=169.8928
	step [17/147], loss=179.6290
	step [18/147], loss=163.4658
	step [19/147], loss=178.8447
	step [20/147], loss=168.6530
	step [21/147], loss=175.6348
	step [22/147], loss=146.0943
	step [23/147], loss=157.0879
	step [24/147], loss=173.3917
	step [25/147], loss=164.0182
	step [26/147], loss=148.8891
	step [27/147], loss=164.7038
	step [28/147], loss=188.1017
	step [29/147], loss=136.0329
	step [30/147], loss=157.8813
	step [31/147], loss=178.1639
	step [32/147], loss=189.0577
	step [33/147], loss=148.7079
	step [34/147], loss=167.9529
	step [35/147], loss=138.7840
	step [36/147], loss=168.1489
	step [37/147], loss=161.6277
	step [38/147], loss=157.2446
	step [39/147], loss=154.3590
	step [40/147], loss=158.2616
	step [41/147], loss=164.9755
	step [42/147], loss=152.9829
	step [43/147], loss=163.3876
	step [44/147], loss=169.7137
	step [45/147], loss=176.2075
	step [46/147], loss=174.5206
	step [47/147], loss=172.3652
	step [48/147], loss=154.2605
	step [49/147], loss=163.5200
	step [50/147], loss=153.7146
	step [51/147], loss=177.9941
	step [52/147], loss=172.6390
	step [53/147], loss=165.5564
	step [54/147], loss=163.6115
	step [55/147], loss=160.3184
	step [56/147], loss=151.9980
	step [57/147], loss=146.8284
	step [58/147], loss=159.0152
	step [59/147], loss=157.0833
	step [60/147], loss=157.0429
	step [61/147], loss=141.8304
	step [62/147], loss=153.9430
	step [63/147], loss=162.2935
	step [64/147], loss=165.5713
	step [65/147], loss=148.0462
	step [66/147], loss=177.7725
	step [67/147], loss=178.5463
	step [68/147], loss=157.2303
	step [69/147], loss=148.8668
	step [70/147], loss=150.7720
	step [71/147], loss=163.6689
	step [72/147], loss=164.2606
	step [73/147], loss=139.6647
	step [74/147], loss=144.2984
	step [75/147], loss=151.0421
	step [76/147], loss=172.0370
	step [77/147], loss=149.9425
	step [78/147], loss=163.9305
	step [79/147], loss=153.2079
	step [80/147], loss=157.8997
	step [81/147], loss=176.6592
	step [82/147], loss=151.8127
	step [83/147], loss=148.0239
	step [84/147], loss=166.2189
	step [85/147], loss=155.8541
	step [86/147], loss=158.8986
	step [87/147], loss=154.7170
	step [88/147], loss=150.3899
	step [89/147], loss=146.5219
	step [90/147], loss=156.7373
	step [91/147], loss=155.1237
	step [92/147], loss=183.0699
	step [93/147], loss=152.1557
	step [94/147], loss=168.7188
	step [95/147], loss=174.0623
	step [96/147], loss=158.9414
	step [97/147], loss=135.7545
	step [98/147], loss=161.0606
	step [99/147], loss=158.4250
	step [100/147], loss=156.3790
	step [101/147], loss=138.7147
	step [102/147], loss=180.8157
	step [103/147], loss=153.6435
	step [104/147], loss=156.3680
	step [105/147], loss=144.8917
	step [106/147], loss=162.1898
	step [107/147], loss=158.1472
	step [108/147], loss=151.4282
	step [109/147], loss=143.5052
	step [110/147], loss=154.2253
	step [111/147], loss=169.1339
	step [112/147], loss=162.2308
	step [113/147], loss=165.8032
	step [114/147], loss=175.5705
	step [115/147], loss=133.0535
	step [116/147], loss=160.7325
	step [117/147], loss=147.6954
	step [118/147], loss=137.3885
	step [119/147], loss=156.4416
	step [120/147], loss=158.3335
	step [121/147], loss=165.2804
	step [122/147], loss=162.2296
	step [123/147], loss=147.3679
	step [124/147], loss=156.5265
	step [125/147], loss=149.8458
	step [126/147], loss=169.8453
	step [127/147], loss=156.3372
	step [128/147], loss=154.5665
	step [129/147], loss=152.2588
	step [130/147], loss=150.6256
	step [131/147], loss=161.8740
	step [132/147], loss=130.3392
	step [133/147], loss=164.6960
	step [134/147], loss=144.9572
	step [135/147], loss=149.5322
	step [136/147], loss=151.4262
	step [137/147], loss=128.3908
	step [138/147], loss=162.0766
	step [139/147], loss=167.8198
	step [140/147], loss=174.9022
	step [141/147], loss=154.6609
	step [142/147], loss=151.5917
	step [143/147], loss=161.5011
	step [144/147], loss=156.9460
	step [145/147], loss=153.7994
	step [146/147], loss=146.8332
	step [147/147], loss=68.9923
	Evaluating
	loss=0.2085, precision=0.3267, recall=0.9341, f1=0.4841
Training epoch 5
	step [1/147], loss=153.8045
	step [2/147], loss=136.2143
	step [3/147], loss=143.3891
	step [4/147], loss=165.4064
	step [5/147], loss=145.5865
	step [6/147], loss=151.2068
	step [7/147], loss=133.9821
	step [8/147], loss=155.0138
	step [9/147], loss=150.1155
	step [10/147], loss=144.6811
	step [11/147], loss=149.2403
	step [12/147], loss=160.9783
	step [13/147], loss=156.7895
	step [14/147], loss=149.7413
	step [15/147], loss=166.3813
	step [16/147], loss=199.1870
	step [17/147], loss=126.2577
	step [18/147], loss=157.6443
	step [19/147], loss=134.4606
	step [20/147], loss=167.0565
	step [21/147], loss=144.1612
	step [22/147], loss=169.9556
	step [23/147], loss=165.9927
	step [24/147], loss=174.2793
	step [25/147], loss=161.5795
	step [26/147], loss=171.6535
	step [27/147], loss=125.8075
	step [28/147], loss=132.9497
	step [29/147], loss=171.6684
	step [30/147], loss=171.4269
	step [31/147], loss=165.2052
	step [32/147], loss=165.6627
	step [33/147], loss=140.1035
	step [34/147], loss=126.4464
	step [35/147], loss=150.1331
	step [36/147], loss=166.0411
	step [37/147], loss=168.6454
	step [38/147], loss=169.2809
	step [39/147], loss=135.7445
	step [40/147], loss=147.7163
	step [41/147], loss=170.6387
	step [42/147], loss=152.2627
	step [43/147], loss=142.8289
	step [44/147], loss=145.5532
	step [45/147], loss=145.7703
	step [46/147], loss=146.9922
	step [47/147], loss=149.6724
	step [48/147], loss=169.7077
	step [49/147], loss=150.7150
	step [50/147], loss=147.9779
	step [51/147], loss=137.0439
	step [52/147], loss=148.4976
	step [53/147], loss=145.1976
	step [54/147], loss=148.4111
	step [55/147], loss=148.4720
	step [56/147], loss=137.6938
	step [57/147], loss=135.5259
	step [58/147], loss=142.9221
	step [59/147], loss=153.1037
	step [60/147], loss=130.4294
	step [61/147], loss=178.3376
	step [62/147], loss=151.4432
	step [63/147], loss=123.5573
	step [64/147], loss=132.4034
	step [65/147], loss=139.3235
	step [66/147], loss=155.7982
	step [67/147], loss=126.7245
	step [68/147], loss=152.2114
	step [69/147], loss=148.5401
	step [70/147], loss=142.6050
	step [71/147], loss=152.5797
	step [72/147], loss=126.5582
	step [73/147], loss=150.2438
	step [74/147], loss=149.3869
	step [75/147], loss=135.8060
	step [76/147], loss=156.2430
	step [77/147], loss=147.9728
	step [78/147], loss=128.6319
	step [79/147], loss=129.1820
	step [80/147], loss=149.2366
	step [81/147], loss=155.9836
	step [82/147], loss=134.4166
	step [83/147], loss=183.0850
	step [84/147], loss=141.9972
	step [85/147], loss=147.8308
	step [86/147], loss=150.0603
	step [87/147], loss=145.7850
	step [88/147], loss=167.2731
	step [89/147], loss=138.4754
	step [90/147], loss=159.2699
	step [91/147], loss=151.0018
	step [92/147], loss=131.8731
	step [93/147], loss=155.9541
	step [94/147], loss=145.5047
	step [95/147], loss=166.1329
	step [96/147], loss=150.3839
	step [97/147], loss=161.7348
	step [98/147], loss=133.0071
	step [99/147], loss=160.2029
	step [100/147], loss=155.2811
	step [101/147], loss=160.1324
	step [102/147], loss=129.7671
	step [103/147], loss=123.5668
	step [104/147], loss=140.7077
	step [105/147], loss=166.2432
	step [106/147], loss=137.2487
	step [107/147], loss=169.4373
	step [108/147], loss=158.7506
	step [109/147], loss=168.6737
	step [110/147], loss=126.5133
	step [111/147], loss=152.9205
	step [112/147], loss=160.3930
	step [113/147], loss=158.3032
	step [114/147], loss=142.4309
	step [115/147], loss=141.3576
	step [116/147], loss=174.2707
	step [117/147], loss=159.7354
	step [118/147], loss=132.2927
	step [119/147], loss=157.9614
	step [120/147], loss=136.5354
	step [121/147], loss=144.3964
	step [122/147], loss=146.3513
	step [123/147], loss=153.3236
	step [124/147], loss=145.4021
	step [125/147], loss=147.5103
	step [126/147], loss=147.8141
	step [127/147], loss=134.1088
	step [128/147], loss=133.7050
	step [129/147], loss=140.6324
	step [130/147], loss=138.8841
	step [131/147], loss=137.0799
	step [132/147], loss=124.5992
	step [133/147], loss=125.3514
	step [134/147], loss=151.4984
	step [135/147], loss=157.5470
	step [136/147], loss=148.6839
	step [137/147], loss=132.0474
	step [138/147], loss=135.2385
	step [139/147], loss=139.7629
	step [140/147], loss=147.4717
	step [141/147], loss=151.9321
	step [142/147], loss=115.9846
	step [143/147], loss=138.2280
	step [144/147], loss=141.2725
	step [145/147], loss=134.5302
	step [146/147], loss=141.9529
	step [147/147], loss=50.9725
	Evaluating
	loss=0.1614, precision=0.3435, recall=0.9441, f1=0.5038
Training epoch 6
	step [1/147], loss=124.3208
	step [2/147], loss=131.9078
	step [3/147], loss=136.4457
	step [4/147], loss=144.9194
	step [5/147], loss=127.1188
	step [6/147], loss=165.6458
	step [7/147], loss=120.1231
	step [8/147], loss=144.1709
	step [9/147], loss=134.6538
	step [10/147], loss=144.0270
	step [11/147], loss=135.3860
	step [12/147], loss=132.6025
	step [13/147], loss=153.2517
	step [14/147], loss=132.4983
	step [15/147], loss=140.6649
	step [16/147], loss=153.2566
	step [17/147], loss=151.0963
	step [18/147], loss=143.4817
	step [19/147], loss=137.7269
	step [20/147], loss=139.0249
	step [21/147], loss=152.5029
	step [22/147], loss=145.4895
	step [23/147], loss=131.6028
	step [24/147], loss=129.4080
	step [25/147], loss=150.5137
	step [26/147], loss=142.6502
	step [27/147], loss=142.8299
	step [28/147], loss=134.3397
	step [29/147], loss=162.1326
	step [30/147], loss=138.4495
	step [31/147], loss=137.2515
	step [32/147], loss=134.9474
	step [33/147], loss=110.5983
	step [34/147], loss=143.2001
	step [35/147], loss=120.1072
	step [36/147], loss=143.5711
	step [37/147], loss=132.0099
	step [38/147], loss=128.0132
	step [39/147], loss=170.9487
	step [40/147], loss=134.8212
	step [41/147], loss=143.1912
	step [42/147], loss=138.5749
	step [43/147], loss=151.6789
	step [44/147], loss=154.6086
	step [45/147], loss=119.6222
	step [46/147], loss=125.0841
	step [47/147], loss=138.4031
	step [48/147], loss=146.2921
	step [49/147], loss=163.3360
	step [50/147], loss=146.4781
	step [51/147], loss=142.0829
	step [52/147], loss=122.4032
	step [53/147], loss=158.3803
	step [54/147], loss=156.3264
	step [55/147], loss=145.6560
	step [56/147], loss=137.6992
	step [57/147], loss=136.8980
	step [58/147], loss=151.9000
	step [59/147], loss=136.5661
	step [60/147], loss=120.2236
	step [61/147], loss=135.8771
	step [62/147], loss=151.3809
	step [63/147], loss=129.2709
	step [64/147], loss=136.6530
	step [65/147], loss=140.8823
	step [66/147], loss=136.9777
	step [67/147], loss=145.1959
	step [68/147], loss=150.5494
	step [69/147], loss=157.4122
	step [70/147], loss=145.4431
	step [71/147], loss=138.7838
	step [72/147], loss=141.2197
	step [73/147], loss=146.3949
	step [74/147], loss=150.9414
	step [75/147], loss=128.9913
	step [76/147], loss=128.7644
	step [77/147], loss=136.1743
	step [78/147], loss=133.5914
	step [79/147], loss=140.0262
	step [80/147], loss=126.3736
	step [81/147], loss=125.6767
	step [82/147], loss=143.8919
	step [83/147], loss=157.1429
	step [84/147], loss=145.0397
	step [85/147], loss=155.6320
	step [86/147], loss=145.9736
	step [87/147], loss=130.1141
	step [88/147], loss=157.7364
	step [89/147], loss=163.0817
	step [90/147], loss=139.7868
	step [91/147], loss=118.5737
	step [92/147], loss=152.8137
	step [93/147], loss=141.0304
	step [94/147], loss=137.6017
	step [95/147], loss=138.1288
	step [96/147], loss=160.2434
	step [97/147], loss=138.5292
	step [98/147], loss=148.7477
	step [99/147], loss=134.4272
	step [100/147], loss=121.1983
	step [101/147], loss=135.2865
	step [102/147], loss=140.9752
	step [103/147], loss=135.2742
	step [104/147], loss=128.1744
	step [105/147], loss=149.5780
	step [106/147], loss=142.3605
	step [107/147], loss=119.7807
	step [108/147], loss=139.6011
	step [109/147], loss=145.1923
	step [110/147], loss=136.2648
	step [111/147], loss=147.9248
	step [112/147], loss=148.6562
	step [113/147], loss=154.1234
	step [114/147], loss=129.6535
	step [115/147], loss=136.8485
	step [116/147], loss=136.8255
	step [117/147], loss=148.0444
	step [118/147], loss=150.5837
	step [119/147], loss=147.8912
	step [120/147], loss=129.2194
	step [121/147], loss=147.3328
	step [122/147], loss=154.1548
	step [123/147], loss=126.6656
	step [124/147], loss=128.4049
	step [125/147], loss=142.0974
	step [126/147], loss=122.8058
	step [127/147], loss=132.4930
	step [128/147], loss=158.9885
	step [129/147], loss=145.5762
	step [130/147], loss=134.4997
	step [131/147], loss=123.0122
	step [132/147], loss=145.3914
	step [133/147], loss=134.1531
	step [134/147], loss=156.1918
	step [135/147], loss=130.1438
	step [136/147], loss=148.5580
	step [137/147], loss=144.8720
	step [138/147], loss=124.0906
	step [139/147], loss=121.0241
	step [140/147], loss=142.2632
	step [141/147], loss=135.4024
	step [142/147], loss=145.6147
	step [143/147], loss=146.1062
	step [144/147], loss=131.0863
	step [145/147], loss=133.4901
	step [146/147], loss=139.1029
	step [147/147], loss=56.1652
	Evaluating
	loss=0.1308, precision=0.3862, recall=0.9260, f1=0.5451
saving model as: 0_saved_model.pth
Training epoch 7
	step [1/147], loss=140.3134
	step [2/147], loss=129.6857
	step [3/147], loss=162.3198
	step [4/147], loss=147.5039
	step [5/147], loss=121.9720
	step [6/147], loss=136.7374
	step [7/147], loss=140.5552
	step [8/147], loss=138.0963
	step [9/147], loss=141.7219
	step [10/147], loss=130.6180
	step [11/147], loss=133.8307
	step [12/147], loss=131.9351
	step [13/147], loss=128.5903
	step [14/147], loss=139.4865
	step [15/147], loss=127.2049
	step [16/147], loss=164.7826
	step [17/147], loss=142.5318
	step [18/147], loss=131.4600
	step [19/147], loss=141.8550
	step [20/147], loss=123.5537
	step [21/147], loss=132.4844
	step [22/147], loss=125.6652
	step [23/147], loss=129.0138
	step [24/147], loss=139.7571
	step [25/147], loss=120.6370
	step [26/147], loss=147.6685
	step [27/147], loss=116.6743
	step [28/147], loss=122.3278
	step [29/147], loss=127.2760
	step [30/147], loss=160.0067
	step [31/147], loss=134.3036
	step [32/147], loss=138.7984
	step [33/147], loss=140.0720
	step [34/147], loss=135.1630
	step [35/147], loss=158.4447
	step [36/147], loss=128.6292
	step [37/147], loss=132.2769
	step [38/147], loss=138.1449
	step [39/147], loss=156.9553
	step [40/147], loss=129.3197
	step [41/147], loss=120.5184
	step [42/147], loss=125.4118
	step [43/147], loss=137.5131
	step [44/147], loss=150.3123
	step [45/147], loss=136.0199
	step [46/147], loss=128.4670
	step [47/147], loss=151.6904
	step [48/147], loss=134.2165
	step [49/147], loss=133.3834
	step [50/147], loss=135.1512
	step [51/147], loss=132.8751
	step [52/147], loss=142.6475
	step [53/147], loss=129.0892
	step [54/147], loss=145.5007
	step [55/147], loss=118.9709
	step [56/147], loss=125.5594
	step [57/147], loss=137.9649
	step [58/147], loss=134.0290
	step [59/147], loss=128.1993
	step [60/147], loss=122.2324
	step [61/147], loss=131.6018
	step [62/147], loss=133.1744
	step [63/147], loss=148.0957
	step [64/147], loss=121.2415
	step [65/147], loss=145.0976
	step [66/147], loss=129.8326
	step [67/147], loss=153.1488
	step [68/147], loss=125.9712
	step [69/147], loss=145.3860
	step [70/147], loss=124.9944
	step [71/147], loss=141.1626
	step [72/147], loss=128.6577
	step [73/147], loss=133.1728
	step [74/147], loss=138.0972
	step [75/147], loss=152.7124
	step [76/147], loss=139.6503
	step [77/147], loss=118.3911
	step [78/147], loss=124.3586
	step [79/147], loss=104.0508
	step [80/147], loss=124.0467
	step [81/147], loss=113.0400
	step [82/147], loss=154.9557
	step [83/147], loss=114.6450
	step [84/147], loss=119.0961
	step [85/147], loss=129.6702
	step [86/147], loss=143.0319
	step [87/147], loss=122.7283
	step [88/147], loss=127.9023
	step [89/147], loss=152.6973
	step [90/147], loss=121.4914
	step [91/147], loss=132.2531
	step [92/147], loss=135.3107
	step [93/147], loss=113.5927
	step [94/147], loss=129.4774
	step [95/147], loss=120.0459
	step [96/147], loss=134.5019
	step [97/147], loss=137.3426
	step [98/147], loss=127.1988
	step [99/147], loss=148.4761
	step [100/147], loss=147.5327
	step [101/147], loss=138.0343
	step [102/147], loss=134.9388
	step [103/147], loss=144.8873
	step [104/147], loss=127.9167
	step [105/147], loss=119.3201
	step [106/147], loss=118.4588
	step [107/147], loss=127.0587
	step [108/147], loss=127.9987
	step [109/147], loss=160.8608
	step [110/147], loss=168.0495
	step [111/147], loss=163.6475
	step [112/147], loss=129.8183
	step [113/147], loss=137.7972
	step [114/147], loss=123.9075
	step [115/147], loss=130.8251
	step [116/147], loss=147.0852
	step [117/147], loss=122.6354
	step [118/147], loss=138.8378
	step [119/147], loss=125.7787
	step [120/147], loss=141.2536
	step [121/147], loss=135.9982
	step [122/147], loss=128.0996
	step [123/147], loss=144.5643
	step [124/147], loss=171.5330
	step [125/147], loss=115.0095
	step [126/147], loss=123.2405
	step [127/147], loss=106.6430
	step [128/147], loss=123.9292
	step [129/147], loss=110.7981
	step [130/147], loss=122.8445
	step [131/147], loss=129.4385
	step [132/147], loss=120.3451
	step [133/147], loss=134.5706
	step [134/147], loss=121.2132
	step [135/147], loss=135.9647
	step [136/147], loss=129.5764
	step [137/147], loss=133.4326
	step [138/147], loss=174.0971
	step [139/147], loss=137.7554
	step [140/147], loss=116.0285
	step [141/147], loss=135.6789
	step [142/147], loss=141.6947
	step [143/147], loss=114.3698
	step [144/147], loss=155.9179
	step [145/147], loss=118.8681
	step [146/147], loss=134.8430
	step [147/147], loss=53.9095
	Evaluating
	loss=0.1089, precision=0.3367, recall=0.9359, f1=0.4952
Training epoch 8
	step [1/147], loss=121.2958
	step [2/147], loss=123.1060
	step [3/147], loss=144.7167
	step [4/147], loss=130.4924
	step [5/147], loss=156.7083
	step [6/147], loss=131.3824
	step [7/147], loss=123.0937
	step [8/147], loss=144.7477
	step [9/147], loss=116.0217
	step [10/147], loss=108.3531
	step [11/147], loss=110.7383
	step [12/147], loss=139.1555
	step [13/147], loss=126.1398
	step [14/147], loss=133.6371
	step [15/147], loss=125.3888
	step [16/147], loss=127.6253
	step [17/147], loss=122.7845
	step [18/147], loss=136.7779
	step [19/147], loss=146.1631
	step [20/147], loss=116.3854
	step [21/147], loss=158.4220
	step [22/147], loss=125.5695
	step [23/147], loss=125.9492
	step [24/147], loss=137.4192
	step [25/147], loss=120.8185
	step [26/147], loss=126.9693
	step [27/147], loss=154.9922
	step [28/147], loss=152.6833
	step [29/147], loss=132.2446
	step [30/147], loss=125.3133
	step [31/147], loss=120.4223
	step [32/147], loss=164.4086
	step [33/147], loss=115.1495
	step [34/147], loss=131.6546
	step [35/147], loss=130.9542
	step [36/147], loss=135.7097
	step [37/147], loss=145.0707
	step [38/147], loss=120.9397
	step [39/147], loss=112.7867
	step [40/147], loss=120.5527
	step [41/147], loss=137.1461
	step [42/147], loss=133.4837
	step [43/147], loss=125.6068
	step [44/147], loss=125.2933
	step [45/147], loss=130.9752
	step [46/147], loss=131.6284
	step [47/147], loss=125.3916
	step [48/147], loss=139.1571
	step [49/147], loss=125.0160
	step [50/147], loss=122.2927
	step [51/147], loss=132.2128
	step [52/147], loss=127.3941
	step [53/147], loss=123.9831
	step [54/147], loss=139.1695
	step [55/147], loss=114.6536
	step [56/147], loss=134.2818
	step [57/147], loss=121.3230
	step [58/147], loss=111.3033
	step [59/147], loss=131.4604
	step [60/147], loss=113.7398
	step [61/147], loss=105.6088
	step [62/147], loss=141.4822
	step [63/147], loss=129.6326
	step [64/147], loss=131.0601
	step [65/147], loss=120.0376
	step [66/147], loss=120.4145
	step [67/147], loss=139.4776
	step [68/147], loss=106.1511
	step [69/147], loss=117.6936
	step [70/147], loss=137.9355
	step [71/147], loss=141.7709
	step [72/147], loss=125.3481
	step [73/147], loss=145.6087
	step [74/147], loss=131.9774
	step [75/147], loss=141.9318
	step [76/147], loss=157.5293
	step [77/147], loss=142.1261
	step [78/147], loss=128.9978
	step [79/147], loss=137.3367
	step [80/147], loss=119.8295
	step [81/147], loss=130.7878
	step [82/147], loss=130.0574
	step [83/147], loss=125.3224
	step [84/147], loss=126.4493
	step [85/147], loss=142.8155
	step [86/147], loss=91.9299
	step [87/147], loss=137.8344
	step [88/147], loss=145.0835
	step [89/147], loss=121.3687
	step [90/147], loss=156.2264
	step [91/147], loss=121.3664
	step [92/147], loss=135.9332
	step [93/147], loss=117.1401
	step [94/147], loss=112.7535
	step [95/147], loss=119.2474
	step [96/147], loss=135.2211
	step [97/147], loss=137.6301
	step [98/147], loss=99.1997
	step [99/147], loss=128.4082
	step [100/147], loss=130.9178
	step [101/147], loss=145.9911
	step [102/147], loss=136.1886
	step [103/147], loss=127.5629
	step [104/147], loss=149.2214
	step [105/147], loss=150.4265
	step [106/147], loss=146.2471
	step [107/147], loss=126.3014
	step [108/147], loss=129.2533
	step [109/147], loss=125.6172
	step [110/147], loss=127.1507
	step [111/147], loss=130.1435
	step [112/147], loss=115.1270
	step [113/147], loss=131.1317
	step [114/147], loss=134.1126
	step [115/147], loss=128.4988
	step [116/147], loss=103.6826
	step [117/147], loss=135.3418
	step [118/147], loss=136.1636
	step [119/147], loss=129.5622
	step [120/147], loss=133.7528
	step [121/147], loss=127.4525
	step [122/147], loss=113.0091
	step [123/147], loss=111.3080
	step [124/147], loss=124.5939
	step [125/147], loss=142.0529
	step [126/147], loss=117.1432
	step [127/147], loss=125.0327
	step [128/147], loss=139.6278
	step [129/147], loss=131.3124
	step [130/147], loss=137.1681
	step [131/147], loss=121.5710
	step [132/147], loss=134.0639
	step [133/147], loss=126.8751
	step [134/147], loss=125.3322
	step [135/147], loss=140.0727
	step [136/147], loss=113.4633
	step [137/147], loss=135.3105
	step [138/147], loss=123.3939
	step [139/147], loss=116.4766
	step [140/147], loss=124.3304
	step [141/147], loss=93.5041
	step [142/147], loss=112.8244
	step [143/147], loss=140.3549
	step [144/147], loss=135.7176
	step [145/147], loss=143.0627
	step [146/147], loss=116.7130
	step [147/147], loss=65.1314
	Evaluating
	loss=0.0888, precision=0.3722, recall=0.9297, f1=0.5316
Training epoch 9
	step [1/147], loss=130.9125
	step [2/147], loss=113.7193
	step [3/147], loss=156.6946
	step [4/147], loss=126.1995
	step [5/147], loss=97.9637
	step [6/147], loss=127.9682
	step [7/147], loss=116.5428
	step [8/147], loss=132.8812
	step [9/147], loss=114.2554
	step [10/147], loss=106.7807
	step [11/147], loss=138.8897
	step [12/147], loss=123.3493
	step [13/147], loss=114.4323
	step [14/147], loss=127.9120
	step [15/147], loss=132.6794
	step [16/147], loss=127.3171
	step [17/147], loss=128.4080
	step [18/147], loss=124.0215
	step [19/147], loss=129.0750
	step [20/147], loss=131.3948
	step [21/147], loss=129.9755
	step [22/147], loss=147.9201
	step [23/147], loss=134.0681
	step [24/147], loss=127.6677
	step [25/147], loss=100.9394
	step [26/147], loss=124.1264
	step [27/147], loss=105.3474
	step [28/147], loss=128.5082
	step [29/147], loss=123.8413
	step [30/147], loss=138.8503
	step [31/147], loss=123.6337
	step [32/147], loss=102.5127
	step [33/147], loss=121.6225
	step [34/147], loss=127.0946
	step [35/147], loss=120.6537
	step [36/147], loss=122.4509
	step [37/147], loss=118.9855
	step [38/147], loss=114.3116
	step [39/147], loss=117.0882
	step [40/147], loss=119.0664
	step [41/147], loss=115.0287
	step [42/147], loss=129.5628
	step [43/147], loss=118.7159
	step [44/147], loss=142.9601
	step [45/147], loss=139.7912
	step [46/147], loss=128.8707
	step [47/147], loss=107.4196
	step [48/147], loss=130.6782
	step [49/147], loss=125.3464
	step [50/147], loss=138.0532
	step [51/147], loss=134.3730
	step [52/147], loss=133.1979
	step [53/147], loss=123.6023
	step [54/147], loss=118.9250
	step [55/147], loss=123.0883
	step [56/147], loss=100.9828
	step [57/147], loss=111.6815
	step [58/147], loss=136.8071
	step [59/147], loss=108.3549
	step [60/147], loss=124.1652
	step [61/147], loss=111.8657
	step [62/147], loss=135.1800
	step [63/147], loss=134.3031
	step [64/147], loss=114.0184
	step [65/147], loss=118.6390
	step [66/147], loss=134.2358
	step [67/147], loss=142.9789
	step [68/147], loss=118.7800
	step [69/147], loss=120.1289
	step [70/147], loss=127.5121
	step [71/147], loss=129.1474
	step [72/147], loss=111.3664
	step [73/147], loss=111.6734
	step [74/147], loss=125.1378
	step [75/147], loss=131.5841
	step [76/147], loss=132.2859
	step [77/147], loss=134.6095
	step [78/147], loss=122.2827
	step [79/147], loss=123.9294
	step [80/147], loss=145.1559
	step [81/147], loss=119.9193
	step [82/147], loss=133.5746
	step [83/147], loss=112.4335
	step [84/147], loss=127.5268
	step [85/147], loss=120.6161
	step [86/147], loss=142.2056
	step [87/147], loss=120.3725
	step [88/147], loss=140.4939
	step [89/147], loss=130.8480
	step [90/147], loss=118.4498
	step [91/147], loss=119.9526
	step [92/147], loss=128.7101
	step [93/147], loss=129.8933
	step [94/147], loss=121.8253
	step [95/147], loss=113.9119
	step [96/147], loss=134.2878
	step [97/147], loss=116.2170
	step [98/147], loss=106.4322
	step [99/147], loss=121.0696
	step [100/147], loss=147.9554
	step [101/147], loss=127.1416
	step [102/147], loss=150.9052
	step [103/147], loss=126.5055
	step [104/147], loss=141.0494
	step [105/147], loss=126.8689
	step [106/147], loss=148.6614
	step [107/147], loss=120.5723
	step [108/147], loss=142.3940
	step [109/147], loss=118.0148
	step [110/147], loss=120.1489
	step [111/147], loss=112.3905
	step [112/147], loss=144.1373
	step [113/147], loss=108.9895
	step [114/147], loss=136.9825
	step [115/147], loss=122.1837
	step [116/147], loss=109.3454
	step [117/147], loss=113.8174
	step [118/147], loss=140.3706
	step [119/147], loss=107.4528
	step [120/147], loss=119.0501
	step [121/147], loss=112.7240
	step [122/147], loss=119.9058
	step [123/147], loss=137.2457
	step [124/147], loss=108.4782
	step [125/147], loss=126.5822
	step [126/147], loss=126.1461
	step [127/147], loss=104.8145
	step [128/147], loss=116.2927
	step [129/147], loss=132.2832
	step [130/147], loss=121.8778
	step [131/147], loss=118.4463
	step [132/147], loss=134.7328
	step [133/147], loss=96.0780
	step [134/147], loss=138.2043
	step [135/147], loss=144.5107
	step [136/147], loss=130.7880
	step [137/147], loss=127.6835
	step [138/147], loss=121.3712
	step [139/147], loss=150.3448
	step [140/147], loss=109.0231
	step [141/147], loss=124.3494
	step [142/147], loss=126.7680
	step [143/147], loss=142.4373
	step [144/147], loss=135.6018
	step [145/147], loss=124.0470
	step [146/147], loss=138.2290
	step [147/147], loss=55.5995
	Evaluating
	loss=0.0776, precision=0.3841, recall=0.9141, f1=0.5410
Training epoch 10
	step [1/147], loss=139.7328
	step [2/147], loss=123.8818
	step [3/147], loss=113.6908
	step [4/147], loss=125.7633
	step [5/147], loss=119.6233
	step [6/147], loss=136.4541
	step [7/147], loss=137.2055
	step [8/147], loss=113.1162
	step [9/147], loss=121.5049
	step [10/147], loss=118.9906
	step [11/147], loss=104.3102
	step [12/147], loss=114.3993
	step [13/147], loss=139.0634
	step [14/147], loss=98.9391
	step [15/147], loss=125.7219
	step [16/147], loss=124.5807
	step [17/147], loss=119.6255
	step [18/147], loss=135.2585
	step [19/147], loss=124.3219
	step [20/147], loss=148.8718
	step [21/147], loss=136.9315
	step [22/147], loss=134.2973
	step [23/147], loss=116.0160
	step [24/147], loss=131.8394
	step [25/147], loss=122.8569
	step [26/147], loss=121.9219
	step [27/147], loss=120.0260
	step [28/147], loss=117.1514
	step [29/147], loss=133.1949
	step [30/147], loss=137.2820
	step [31/147], loss=105.2192
	step [32/147], loss=115.2204
	step [33/147], loss=142.4556
	step [34/147], loss=122.7122
	step [35/147], loss=123.1434
	step [36/147], loss=119.4418
	step [37/147], loss=120.1615
	step [38/147], loss=102.0432
	step [39/147], loss=121.9835
	step [40/147], loss=119.5616
	step [41/147], loss=119.7106
	step [42/147], loss=144.3947
	step [43/147], loss=121.9824
	step [44/147], loss=120.7993
	step [45/147], loss=127.6590
	step [46/147], loss=111.7146
	step [47/147], loss=127.3173
	step [48/147], loss=125.7939
	step [49/147], loss=123.7718
	step [50/147], loss=120.2488
	step [51/147], loss=118.3454
	step [52/147], loss=129.5235
	step [53/147], loss=125.7305
	step [54/147], loss=122.3619
	step [55/147], loss=124.2366
	step [56/147], loss=116.4708
	step [57/147], loss=112.1807
	step [58/147], loss=110.3314
	step [59/147], loss=132.7747
	step [60/147], loss=129.4899
	step [61/147], loss=117.4031
	step [62/147], loss=104.6055
	step [63/147], loss=118.7671
	step [64/147], loss=115.2655
	step [65/147], loss=133.9882
	step [66/147], loss=130.2277
	step [67/147], loss=97.5378
	step [68/147], loss=149.2448
	step [69/147], loss=123.9498
	step [70/147], loss=105.0105
	step [71/147], loss=114.7512
	step [72/147], loss=130.8717
	step [73/147], loss=114.0268
	step [74/147], loss=104.9761
	step [75/147], loss=119.7280
	step [76/147], loss=124.2880
	step [77/147], loss=121.2935
	step [78/147], loss=153.4611
	step [79/147], loss=126.2816
	step [80/147], loss=108.4461
	step [81/147], loss=136.6966
	step [82/147], loss=143.6595
	step [83/147], loss=119.2412
	step [84/147], loss=128.4668
	step [85/147], loss=120.2886
	step [86/147], loss=129.8145
	step [87/147], loss=124.0690
	step [88/147], loss=116.8861
	step [89/147], loss=127.8452
	step [90/147], loss=126.7825
	step [91/147], loss=129.1270
	step [92/147], loss=127.9693
	step [93/147], loss=114.8132
	step [94/147], loss=135.4767
	step [95/147], loss=123.2785
	step [96/147], loss=121.2550
	step [97/147], loss=124.5266
	step [98/147], loss=116.6803
	step [99/147], loss=96.8896
	step [100/147], loss=107.8821
	step [101/147], loss=125.0034
	step [102/147], loss=128.2247
	step [103/147], loss=144.3741
	step [104/147], loss=99.4144
	step [105/147], loss=125.1076
	step [106/147], loss=127.4763
	step [107/147], loss=110.3093
	step [108/147], loss=116.6019
	step [109/147], loss=109.8179
	step [110/147], loss=120.5017
	step [111/147], loss=117.8579
	step [112/147], loss=116.9436
	step [113/147], loss=118.3320
	step [114/147], loss=119.6801
	step [115/147], loss=123.6225
	step [116/147], loss=132.2053
	step [117/147], loss=133.3669
	step [118/147], loss=122.9324
	step [119/147], loss=107.1150
	step [120/147], loss=100.5212
	step [121/147], loss=136.5695
	step [122/147], loss=128.0683
	step [123/147], loss=132.2814
	step [124/147], loss=116.0700
	step [125/147], loss=136.9530
	step [126/147], loss=122.5160
	step [127/147], loss=113.8034
	step [128/147], loss=114.3116
	step [129/147], loss=123.3926
	step [130/147], loss=152.9601
	step [131/147], loss=136.0675
	step [132/147], loss=118.7873
	step [133/147], loss=128.1485
	step [134/147], loss=115.1315
	step [135/147], loss=124.1644
	step [136/147], loss=107.7392
	step [137/147], loss=129.5744
	step [138/147], loss=125.5359
	step [139/147], loss=110.1844
	step [140/147], loss=127.6902
	step [141/147], loss=119.2024
	step [142/147], loss=120.0402
	step [143/147], loss=122.7927
	step [144/147], loss=103.2268
	step [145/147], loss=112.4831
	step [146/147], loss=112.9230
	step [147/147], loss=45.7414
	Evaluating
	loss=0.0667, precision=0.3437, recall=0.9332, f1=0.5023
Training epoch 11
	step [1/147], loss=130.7463
	step [2/147], loss=121.9402
	step [3/147], loss=109.7490
	step [4/147], loss=133.7788
	step [5/147], loss=132.9972
	step [6/147], loss=128.0566
	step [7/147], loss=133.6336
	step [8/147], loss=107.6968
	step [9/147], loss=124.3574
	step [10/147], loss=97.4760
	step [11/147], loss=134.1400
	step [12/147], loss=103.0123
	step [13/147], loss=121.1102
	step [14/147], loss=130.5483
	step [15/147], loss=113.8163
	step [16/147], loss=112.2705
	step [17/147], loss=117.0016
	step [18/147], loss=122.1815
	step [19/147], loss=111.7810
	step [20/147], loss=131.5228
	step [21/147], loss=137.1510
	step [22/147], loss=139.6005
	step [23/147], loss=104.5405
	step [24/147], loss=117.4408
	step [25/147], loss=109.1940
	step [26/147], loss=122.6749
	step [27/147], loss=131.7967
	step [28/147], loss=160.7365
	step [29/147], loss=110.0559
	step [30/147], loss=115.8049
	step [31/147], loss=120.9090
	step [32/147], loss=136.7129
	step [33/147], loss=119.7066
	step [34/147], loss=111.1347
	step [35/147], loss=107.0419
	step [36/147], loss=129.9115
	step [37/147], loss=121.5369
	step [38/147], loss=128.9722
	step [39/147], loss=104.3016
	step [40/147], loss=117.4839
	step [41/147], loss=129.5527
	step [42/147], loss=114.4926
	step [43/147], loss=125.3255
	step [44/147], loss=114.6223
	step [45/147], loss=127.2337
	step [46/147], loss=129.8777
	step [47/147], loss=107.1685
	step [48/147], loss=100.5431
	step [49/147], loss=124.7932
	step [50/147], loss=111.9588
	step [51/147], loss=113.0991
	step [52/147], loss=113.9275
	step [53/147], loss=122.7368
	step [54/147], loss=130.7419
	step [55/147], loss=114.4296
	step [56/147], loss=132.5789
	step [57/147], loss=140.7259
	step [58/147], loss=118.0417
	step [59/147], loss=122.2506
	step [60/147], loss=100.8260
	step [61/147], loss=139.4405
	step [62/147], loss=120.8227
	step [63/147], loss=132.3887
	step [64/147], loss=118.8918
	step [65/147], loss=116.2507
	step [66/147], loss=121.1485
	step [67/147], loss=112.5852
	step [68/147], loss=130.0816
	step [69/147], loss=124.5216
	step [70/147], loss=125.7385
	step [71/147], loss=115.3699
	step [72/147], loss=111.4275
	step [73/147], loss=125.2557
	step [74/147], loss=118.9293
	step [75/147], loss=116.6177
	step [76/147], loss=109.1826
	step [77/147], loss=122.7905
	step [78/147], loss=125.3811
	step [79/147], loss=134.3059
	step [80/147], loss=143.5316
	step [81/147], loss=121.8712
	step [82/147], loss=105.5960
	step [83/147], loss=101.4079
	step [84/147], loss=120.5875
	step [85/147], loss=106.5035
	step [86/147], loss=96.4333
	step [87/147], loss=121.1189
	step [88/147], loss=99.0212
	step [89/147], loss=99.3315
	step [90/147], loss=113.4140
	step [91/147], loss=132.7617
	step [92/147], loss=102.8884
	step [93/147], loss=113.3104
	step [94/147], loss=132.1533
	step [95/147], loss=151.4317
	step [96/147], loss=134.8181
	step [97/147], loss=124.9572
	step [98/147], loss=114.1163
	step [99/147], loss=112.4589
	step [100/147], loss=129.4455
	step [101/147], loss=127.3165
	step [102/147], loss=139.4664
	step [103/147], loss=118.9976
	step [104/147], loss=132.3422
	step [105/147], loss=119.9541
	step [106/147], loss=104.9891
	step [107/147], loss=112.7178
	step [108/147], loss=112.2303
	step [109/147], loss=96.1160
	step [110/147], loss=114.6850
	step [111/147], loss=99.3275
	step [112/147], loss=119.9377
	step [113/147], loss=110.8206
	step [114/147], loss=111.2826
	step [115/147], loss=119.7053
	step [116/147], loss=98.5510
	step [117/147], loss=106.2305
	step [118/147], loss=127.5513
	step [119/147], loss=119.7562
	step [120/147], loss=113.8410
	step [121/147], loss=132.9563
	step [122/147], loss=139.6165
	step [123/147], loss=126.8357
	step [124/147], loss=115.8742
	step [125/147], loss=120.2872
	step [126/147], loss=132.9385
	step [127/147], loss=120.2739
	step [128/147], loss=117.1447
	step [129/147], loss=103.3264
	step [130/147], loss=112.9904
	step [131/147], loss=86.1343
	step [132/147], loss=109.8420
	step [133/147], loss=129.7299
	step [134/147], loss=126.6162
	step [135/147], loss=124.8435
	step [136/147], loss=124.6877
	step [137/147], loss=125.3762
	step [138/147], loss=123.5894
	step [139/147], loss=115.7499
	step [140/147], loss=117.2655
	step [141/147], loss=109.3925
	step [142/147], loss=129.6399
	step [143/147], loss=133.0606
	step [144/147], loss=99.3889
	step [145/147], loss=140.0236
	step [146/147], loss=118.2122
	step [147/147], loss=51.0831
	Evaluating
	loss=0.0575, precision=0.3177, recall=0.9384, f1=0.4747
Training epoch 12
	step [1/147], loss=129.6249
	step [2/147], loss=125.8247
	step [3/147], loss=120.4282
	step [4/147], loss=125.5159
	step [5/147], loss=120.4456
	step [6/147], loss=124.8318
	step [7/147], loss=109.8474
	step [8/147], loss=107.5372
	step [9/147], loss=127.6783
	step [10/147], loss=113.2421
	step [11/147], loss=103.7354
	step [12/147], loss=111.3212
	step [13/147], loss=109.1359
	step [14/147], loss=114.2247
	step [15/147], loss=107.4246
	step [16/147], loss=137.5563
	step [17/147], loss=96.1284
	step [18/147], loss=89.4345
	step [19/147], loss=126.7965
	step [20/147], loss=133.0266
	step [21/147], loss=131.8793
	step [22/147], loss=123.6625
	step [23/147], loss=125.1700
	step [24/147], loss=126.5218
	step [25/147], loss=125.7497
	step [26/147], loss=115.8383
	step [27/147], loss=110.2217
	step [28/147], loss=119.0418
	step [29/147], loss=141.7733
	step [30/147], loss=97.4006
	step [31/147], loss=132.6169
	step [32/147], loss=103.5922
	step [33/147], loss=109.7999
	step [34/147], loss=116.0563
	step [35/147], loss=123.6758
	step [36/147], loss=117.0306
	step [37/147], loss=126.2788
	step [38/147], loss=132.0952
	step [39/147], loss=132.0199
	step [40/147], loss=114.1593
	step [41/147], loss=116.3232
	step [42/147], loss=118.1999
	step [43/147], loss=122.3020
	step [44/147], loss=112.1943
	step [45/147], loss=127.7820
	step [46/147], loss=118.5702
	step [47/147], loss=145.7216
	step [48/147], loss=116.0507
	step [49/147], loss=120.8111
	step [50/147], loss=114.1180
	step [51/147], loss=107.1524
	step [52/147], loss=117.5240
	step [53/147], loss=115.0636
	step [54/147], loss=118.3022
	step [55/147], loss=110.3304
	step [56/147], loss=115.9821
	step [57/147], loss=125.9900
	step [58/147], loss=142.6851
	step [59/147], loss=99.2847
	step [60/147], loss=125.7119
	step [61/147], loss=122.0268
	step [62/147], loss=122.8094
	step [63/147], loss=116.0367
	step [64/147], loss=109.8653
	step [65/147], loss=145.1343
	step [66/147], loss=109.1779
	step [67/147], loss=122.4806
	step [68/147], loss=123.6553
	step [69/147], loss=118.8126
	step [70/147], loss=119.9176
	step [71/147], loss=106.3720
	step [72/147], loss=109.0306
	step [73/147], loss=105.7822
	step [74/147], loss=109.1592
	step [75/147], loss=96.6902
	step [76/147], loss=123.4592
	step [77/147], loss=137.2397
	step [78/147], loss=111.4542
	step [79/147], loss=141.0671
	step [80/147], loss=116.4525
	step [81/147], loss=122.3655
	step [82/147], loss=119.4968
	step [83/147], loss=107.0121
	step [84/147], loss=126.9372
	step [85/147], loss=121.1173
	step [86/147], loss=115.5878
	step [87/147], loss=103.3815
	step [88/147], loss=98.0215
	step [89/147], loss=102.6213
	step [90/147], loss=119.0721
	step [91/147], loss=114.0116
	step [92/147], loss=127.0195
	step [93/147], loss=133.7340
	step [94/147], loss=130.1352
	step [95/147], loss=128.4395
	step [96/147], loss=133.0668
	step [97/147], loss=127.7043
	step [98/147], loss=126.9381
	step [99/147], loss=119.9397
	step [100/147], loss=113.4632
	step [101/147], loss=105.3596
	step [102/147], loss=102.8322
	step [103/147], loss=117.8599
	step [104/147], loss=117.1540
	step [105/147], loss=104.7065
	step [106/147], loss=124.6031
	step [107/147], loss=120.2555
	step [108/147], loss=105.3800
	step [109/147], loss=132.1314
	step [110/147], loss=148.4754
	step [111/147], loss=125.1309
	step [112/147], loss=102.6884
	step [113/147], loss=109.8115
	step [114/147], loss=118.1009
	step [115/147], loss=115.9310
	step [116/147], loss=119.6259
	step [117/147], loss=123.8818
	step [118/147], loss=93.9130
	step [119/147], loss=113.9813
	step [120/147], loss=113.6453
	step [121/147], loss=113.4520
	step [122/147], loss=122.9622
	step [123/147], loss=119.8835
	step [124/147], loss=113.9492
	step [125/147], loss=112.8780
	step [126/147], loss=119.1799
	step [127/147], loss=105.6602
	step [128/147], loss=133.6382
	step [129/147], loss=111.4800
	step [130/147], loss=133.1775
	step [131/147], loss=121.8600
	step [132/147], loss=121.7815
	step [133/147], loss=107.0245
	step [134/147], loss=110.0800
	step [135/147], loss=131.9057
	step [136/147], loss=125.7603
	step [137/147], loss=115.9436
	step [138/147], loss=91.4174
	step [139/147], loss=124.5340
	step [140/147], loss=83.6727
	step [141/147], loss=112.0659
	step [142/147], loss=139.0371
	step [143/147], loss=121.0520
	step [144/147], loss=112.2674
	step [145/147], loss=105.1030
	step [146/147], loss=120.9966
	step [147/147], loss=44.8086
	Evaluating
	loss=0.0485, precision=0.3534, recall=0.9401, f1=0.5137
Training epoch 13
	step [1/147], loss=106.4329
	step [2/147], loss=132.7010
	step [3/147], loss=129.1070
	step [4/147], loss=93.7953
	step [5/147], loss=125.3624
	step [6/147], loss=136.4164
	step [7/147], loss=114.4478
	step [8/147], loss=125.7135
	step [9/147], loss=121.7414
	step [10/147], loss=127.9424
	step [11/147], loss=101.2457
	step [12/147], loss=102.2128
	step [13/147], loss=110.8671
	step [14/147], loss=127.2088
	step [15/147], loss=109.6158
	step [16/147], loss=112.3190
	step [17/147], loss=111.3164
	step [18/147], loss=105.5591
	step [19/147], loss=119.8730
	step [20/147], loss=107.1412
	step [21/147], loss=115.3741
	step [22/147], loss=102.5883
	step [23/147], loss=118.9635
	step [24/147], loss=118.3158
	step [25/147], loss=128.9768
	step [26/147], loss=109.3281
	step [27/147], loss=106.7517
	step [28/147], loss=118.8525
	step [29/147], loss=113.0223
	step [30/147], loss=116.0473
	step [31/147], loss=104.0607
	step [32/147], loss=99.7695
	step [33/147], loss=113.5258
	step [34/147], loss=108.8499
	step [35/147], loss=106.0770
	step [36/147], loss=113.0329
	step [37/147], loss=110.5323
	step [38/147], loss=114.6593
	step [39/147], loss=122.6369
	step [40/147], loss=133.0408
	step [41/147], loss=118.1159
	step [42/147], loss=129.2902
	step [43/147], loss=93.3972
	step [44/147], loss=117.7287
	step [45/147], loss=105.7187
	step [46/147], loss=131.5787
	step [47/147], loss=120.6362
	step [48/147], loss=129.0620
	step [49/147], loss=122.2211
	step [50/147], loss=99.8193
	step [51/147], loss=123.6676
	step [52/147], loss=123.0708
	step [53/147], loss=102.4826
	step [54/147], loss=120.1375
	step [55/147], loss=106.7435
	step [56/147], loss=132.3304
	step [57/147], loss=126.6483
	step [58/147], loss=108.6676
	step [59/147], loss=129.9986
	step [60/147], loss=113.2689
	step [61/147], loss=121.6565
	step [62/147], loss=119.7820
	step [63/147], loss=131.9568
	step [64/147], loss=115.5761
	step [65/147], loss=103.7186
	step [66/147], loss=127.9948
	step [67/147], loss=127.2782
	step [68/147], loss=122.6834
	step [69/147], loss=103.0609
	step [70/147], loss=128.9279
	step [71/147], loss=144.6990
	step [72/147], loss=121.5999
	step [73/147], loss=115.3260
	step [74/147], loss=113.8568
	step [75/147], loss=105.2782
	step [76/147], loss=125.7589
	step [77/147], loss=111.6550
	step [78/147], loss=102.4183
	step [79/147], loss=107.2533
	step [80/147], loss=117.5309
	step [81/147], loss=121.5141
	step [82/147], loss=96.8084
	step [83/147], loss=126.5896
	step [84/147], loss=116.5619
	step [85/147], loss=100.3053
	step [86/147], loss=117.0691
	step [87/147], loss=115.4349
	step [88/147], loss=106.5732
	step [89/147], loss=111.5160
	step [90/147], loss=144.8771
	step [91/147], loss=119.9368
	step [92/147], loss=110.3416
	step [93/147], loss=92.4896
	step [94/147], loss=108.5835
	step [95/147], loss=101.9368
	step [96/147], loss=102.6177
	step [97/147], loss=129.5176
	step [98/147], loss=99.2665
	step [99/147], loss=98.7033
	step [100/147], loss=103.0477
	step [101/147], loss=121.5523
	step [102/147], loss=130.6002
	step [103/147], loss=124.8224
	step [104/147], loss=112.3999
	step [105/147], loss=118.6674
	step [106/147], loss=110.9075
	step [107/147], loss=124.7666
	step [108/147], loss=130.6591
	step [109/147], loss=125.9979
	step [110/147], loss=103.5692
	step [111/147], loss=110.1734
	step [112/147], loss=112.4646
	step [113/147], loss=125.2376
	step [114/147], loss=108.5581
	step [115/147], loss=103.5526
	step [116/147], loss=108.6742
	step [117/147], loss=132.1700
	step [118/147], loss=134.0791
	step [119/147], loss=118.3431
	step [120/147], loss=105.8175
	step [121/147], loss=112.9986
	step [122/147], loss=127.6021
	step [123/147], loss=116.8551
	step [124/147], loss=127.6566
	step [125/147], loss=111.7808
	step [126/147], loss=118.2086
	step [127/147], loss=111.7885
	step [128/147], loss=125.3193
	step [129/147], loss=127.0063
	step [130/147], loss=117.9711
	step [131/147], loss=114.8161
	step [132/147], loss=137.5487
	step [133/147], loss=127.8911
	step [134/147], loss=107.1945
	step [135/147], loss=124.9963
	step [136/147], loss=135.0153
	step [137/147], loss=111.7712
	step [138/147], loss=99.3609
	step [139/147], loss=142.1062
	step [140/147], loss=120.3402
	step [141/147], loss=106.6662
	step [142/147], loss=115.8121
	step [143/147], loss=92.1762
	step [144/147], loss=117.6119
	step [145/147], loss=131.7017
	step [146/147], loss=96.8476
	step [147/147], loss=55.1530
	Evaluating
	loss=0.0455, precision=0.3245, recall=0.9447, f1=0.4831
Training epoch 14
	step [1/147], loss=116.1533
	step [2/147], loss=135.7356
	step [3/147], loss=98.9021
	step [4/147], loss=105.1701
	step [5/147], loss=119.4385
	step [6/147], loss=91.6298
	step [7/147], loss=105.1183
	step [8/147], loss=112.3457
	step [9/147], loss=103.9921
	step [10/147], loss=114.8247
	step [11/147], loss=106.6914
	step [12/147], loss=118.2908
	step [13/147], loss=109.3484
	step [14/147], loss=116.1530
	step [15/147], loss=112.6417
	step [16/147], loss=116.0799
	step [17/147], loss=99.9410
	step [18/147], loss=115.1759
	step [19/147], loss=118.0438
	step [20/147], loss=140.0242
	step [21/147], loss=105.5257
	step [22/147], loss=106.0481
	step [23/147], loss=145.7496
	step [24/147], loss=105.7388
	step [25/147], loss=118.0328
	step [26/147], loss=137.1511
	step [27/147], loss=140.3988
	step [28/147], loss=122.2445
	step [29/147], loss=100.4248
	step [30/147], loss=100.3162
	step [31/147], loss=107.5092
	step [32/147], loss=103.3034
	step [33/147], loss=126.6684
	step [34/147], loss=138.4871
	step [35/147], loss=136.6740
	step [36/147], loss=116.4556
	step [37/147], loss=100.6338
	step [38/147], loss=127.0150
	step [39/147], loss=132.4613
	step [40/147], loss=121.0760
	step [41/147], loss=131.6936
	step [42/147], loss=125.6997
	step [43/147], loss=114.6658
	step [44/147], loss=112.8379
	step [45/147], loss=120.0514
	step [46/147], loss=112.3205
	step [47/147], loss=117.1190
	step [48/147], loss=97.3990
	step [49/147], loss=108.5064
	step [50/147], loss=108.6563
	step [51/147], loss=112.5430
	step [52/147], loss=107.6039
	step [53/147], loss=130.6405
	step [54/147], loss=119.2816
	step [55/147], loss=115.3831
	step [56/147], loss=123.4630
	step [57/147], loss=108.1644
	step [58/147], loss=121.4743
	step [59/147], loss=109.6274
	step [60/147], loss=121.9742
	step [61/147], loss=114.9032
	step [62/147], loss=119.7457
	step [63/147], loss=115.9586
	step [64/147], loss=123.8887
	step [65/147], loss=106.0908
	step [66/147], loss=108.7992
	step [67/147], loss=111.0880
	step [68/147], loss=124.0466
	step [69/147], loss=108.0948
	step [70/147], loss=132.1920
	step [71/147], loss=113.3023
	step [72/147], loss=101.3778
	step [73/147], loss=111.6848
	step [74/147], loss=126.8884
	step [75/147], loss=109.5974
	step [76/147], loss=115.1761
	step [77/147], loss=118.6022
	step [78/147], loss=122.1792
	step [79/147], loss=108.2657
	step [80/147], loss=114.3824
	step [81/147], loss=124.8417
	step [82/147], loss=116.3272
	step [83/147], loss=89.0651
	step [84/147], loss=128.4490
	step [85/147], loss=106.7636
	step [86/147], loss=124.8755
	step [87/147], loss=108.9384
	step [88/147], loss=106.3702
	step [89/147], loss=104.6603
	step [90/147], loss=115.1548
	step [91/147], loss=103.7094
	step [92/147], loss=109.4031
	step [93/147], loss=101.4895
	step [94/147], loss=123.6369
	step [95/147], loss=117.8198
	step [96/147], loss=115.8094
	step [97/147], loss=104.0460
	step [98/147], loss=118.9483
	step [99/147], loss=90.4343
	step [100/147], loss=114.5316
	step [101/147], loss=119.2567
	step [102/147], loss=107.6249
	step [103/147], loss=126.5079
	step [104/147], loss=113.2135
	step [105/147], loss=115.4610
	step [106/147], loss=104.3549
	step [107/147], loss=114.7608
	step [108/147], loss=109.2818
	step [109/147], loss=121.1461
	step [110/147], loss=100.5895
	step [111/147], loss=94.2033
	step [112/147], loss=103.5963
	step [113/147], loss=126.7206
	step [114/147], loss=119.1803
	step [115/147], loss=109.0753
	step [116/147], loss=118.0171
	step [117/147], loss=116.9605
	step [118/147], loss=114.1258
	step [119/147], loss=118.9282
	step [120/147], loss=94.3817
	step [121/147], loss=124.0487
	step [122/147], loss=90.5902
	step [123/147], loss=113.4459
	step [124/147], loss=110.4406
	step [125/147], loss=119.0979
	step [126/147], loss=123.3129
	step [127/147], loss=110.2581
	step [128/147], loss=133.7261
	step [129/147], loss=113.3734
	step [130/147], loss=117.3880
	step [131/147], loss=129.4572
	step [132/147], loss=106.4250
	step [133/147], loss=105.0081
	step [134/147], loss=111.4880
	step [135/147], loss=121.9170
	step [136/147], loss=109.0600
	step [137/147], loss=108.7389
	step [138/147], loss=113.6637
	step [139/147], loss=133.0993
	step [140/147], loss=119.3988
	step [141/147], loss=103.9580
	step [142/147], loss=111.9664
	step [143/147], loss=119.2744
	step [144/147], loss=104.3755
	step [145/147], loss=113.5633
	step [146/147], loss=127.3791
	step [147/147], loss=60.6765
	Evaluating
	loss=0.0393, precision=0.3432, recall=0.9348, f1=0.5021
Training epoch 15
	step [1/147], loss=130.0826
	step [2/147], loss=116.0005
	step [3/147], loss=120.3055
	step [4/147], loss=126.3803
	step [5/147], loss=119.9509
	step [6/147], loss=128.2495
	step [7/147], loss=99.0495
	step [8/147], loss=116.0017
	step [9/147], loss=113.7263
	step [10/147], loss=91.1422
	step [11/147], loss=139.8792
	step [12/147], loss=123.6401
	step [13/147], loss=118.6538
	step [14/147], loss=113.0367
	step [15/147], loss=127.4728
	step [16/147], loss=106.4109
	step [17/147], loss=106.5874
	step [18/147], loss=97.7684
	step [19/147], loss=109.0786
	step [20/147], loss=116.2243
	step [21/147], loss=104.0650
	step [22/147], loss=95.7158
	step [23/147], loss=102.7757
	step [24/147], loss=110.0013
	step [25/147], loss=99.5044
	step [26/147], loss=116.3928
	step [27/147], loss=104.5537
	step [28/147], loss=137.2526
	step [29/147], loss=116.8669
	step [30/147], loss=117.6566
	step [31/147], loss=108.4918
	step [32/147], loss=125.1415
	step [33/147], loss=93.1430
	step [34/147], loss=119.6800
	step [35/147], loss=109.9256
	step [36/147], loss=122.1777
	step [37/147], loss=105.9343
	step [38/147], loss=103.4840
	step [39/147], loss=108.0245
	step [40/147], loss=99.9120
	step [41/147], loss=121.8899
	step [42/147], loss=121.4265
	step [43/147], loss=113.4700
	step [44/147], loss=114.4594
	step [45/147], loss=104.6821
	step [46/147], loss=117.4032
	step [47/147], loss=118.1267
	step [48/147], loss=109.0187
	step [49/147], loss=133.3322
	step [50/147], loss=108.9539
	step [51/147], loss=101.6575
	step [52/147], loss=120.6100
	step [53/147], loss=124.9562
	step [54/147], loss=94.6715
	step [55/147], loss=107.7097
	step [56/147], loss=112.3603
	step [57/147], loss=116.7173
	step [58/147], loss=103.1455
	step [59/147], loss=113.3947
	step [60/147], loss=112.0921
	step [61/147], loss=111.5497
	step [62/147], loss=107.2969
	step [63/147], loss=111.4678
	step [64/147], loss=124.8745
	step [65/147], loss=136.7166
	step [66/147], loss=105.8380
	step [67/147], loss=109.6186
	step [68/147], loss=135.9942
	step [69/147], loss=103.0197
	step [70/147], loss=131.5861
	step [71/147], loss=112.7842
	step [72/147], loss=121.0425
	step [73/147], loss=109.5041
	step [74/147], loss=108.2636
	step [75/147], loss=121.5816
	step [76/147], loss=120.4492
	step [77/147], loss=121.1671
	step [78/147], loss=119.4702
	step [79/147], loss=103.4381
	step [80/147], loss=101.8096
	step [81/147], loss=105.0970
	step [82/147], loss=125.7352
	step [83/147], loss=129.1150
	step [84/147], loss=108.7717
	step [85/147], loss=126.3129
	step [86/147], loss=99.9641
	step [87/147], loss=123.9795
	step [88/147], loss=101.7976
	step [89/147], loss=111.4002
	step [90/147], loss=114.8865
	step [91/147], loss=111.0835
	step [92/147], loss=121.5200
	step [93/147], loss=114.6074
	step [94/147], loss=107.7717
	step [95/147], loss=111.7612
	step [96/147], loss=116.5509
	step [97/147], loss=110.3288
	step [98/147], loss=109.2205
	step [99/147], loss=115.2518
	step [100/147], loss=117.9802
	step [101/147], loss=101.1524
	step [102/147], loss=133.3369
	step [103/147], loss=134.8711
	step [104/147], loss=107.1735
	step [105/147], loss=108.4609
	step [106/147], loss=100.4176
	step [107/147], loss=118.9327
	step [108/147], loss=109.8686
	step [109/147], loss=117.7314
	step [110/147], loss=97.9847
	step [111/147], loss=108.0719
	step [112/147], loss=130.9497
	step [113/147], loss=129.2897
	step [114/147], loss=110.0319
	step [115/147], loss=103.5185
	step [116/147], loss=129.6159
	step [117/147], loss=91.6472
	step [118/147], loss=122.3493
	step [119/147], loss=118.8022
	step [120/147], loss=113.8088
	step [121/147], loss=98.2345
	step [122/147], loss=109.5317
	step [123/147], loss=101.0145
	step [124/147], loss=117.4148
	step [125/147], loss=102.5881
	step [126/147], loss=126.5766
	step [127/147], loss=98.9505
	step [128/147], loss=102.2881
	step [129/147], loss=110.7987
	step [130/147], loss=123.6810
	step [131/147], loss=112.1157
	step [132/147], loss=116.4050
	step [133/147], loss=99.8597
	step [134/147], loss=129.4402
	step [135/147], loss=113.7458
	step [136/147], loss=119.2083
	step [137/147], loss=103.5743
	step [138/147], loss=110.1607
	step [139/147], loss=113.1302
	step [140/147], loss=90.8293
	step [141/147], loss=96.8611
	step [142/147], loss=110.0279
	step [143/147], loss=120.7160
	step [144/147], loss=96.0510
	step [145/147], loss=106.0131
	step [146/147], loss=109.9662
	step [147/147], loss=59.8051
	Evaluating
	loss=0.0349, precision=0.3404, recall=0.9421, f1=0.5001
Training epoch 16
	step [1/147], loss=121.4633
	step [2/147], loss=110.5912
	step [3/147], loss=142.0554
	step [4/147], loss=110.9859
	step [5/147], loss=112.0988
	step [6/147], loss=98.6688
	step [7/147], loss=134.5033
	step [8/147], loss=102.3476
	step [9/147], loss=117.9733
	step [10/147], loss=99.9342
	step [11/147], loss=109.6744
	step [12/147], loss=115.8695
	step [13/147], loss=127.6165
	step [14/147], loss=97.9488
	step [15/147], loss=127.6282
	step [16/147], loss=109.8460
	step [17/147], loss=123.4115
	step [18/147], loss=123.1779
	step [19/147], loss=118.8988
	step [20/147], loss=105.7903
	step [21/147], loss=116.4651
	step [22/147], loss=112.7117
	step [23/147], loss=103.1616
	step [24/147], loss=122.9851
	step [25/147], loss=94.5692
	step [26/147], loss=124.3007
	step [27/147], loss=93.7897
	step [28/147], loss=104.8962
	step [29/147], loss=102.3533
	step [30/147], loss=126.6452
	step [31/147], loss=116.4966
	step [32/147], loss=92.1877
	step [33/147], loss=130.0962
	step [34/147], loss=112.9799
	step [35/147], loss=122.5382
	step [36/147], loss=92.4742
	step [37/147], loss=101.9891
	step [38/147], loss=98.3185
	step [39/147], loss=106.6383
	step [40/147], loss=98.7907
	step [41/147], loss=113.8683
	step [42/147], loss=103.7173
	step [43/147], loss=113.0265
	step [44/147], loss=117.0166
	step [45/147], loss=118.2302
	step [46/147], loss=110.6155
	step [47/147], loss=118.4357
	step [48/147], loss=115.9026
	step [49/147], loss=95.5441
	step [50/147], loss=125.5022
	step [51/147], loss=120.8286
	step [52/147], loss=90.5568
	step [53/147], loss=107.6635
	step [54/147], loss=121.6229
	step [55/147], loss=106.5381
	step [56/147], loss=99.8227
	step [57/147], loss=122.5268
	step [58/147], loss=124.9475
	step [59/147], loss=94.0911
	step [60/147], loss=110.3356
	step [61/147], loss=120.6763
	step [62/147], loss=94.5521
	step [63/147], loss=100.5488
	step [64/147], loss=104.3037
	step [65/147], loss=90.2745
	step [66/147], loss=110.9129
	step [67/147], loss=129.8355
	step [68/147], loss=107.1169
	step [69/147], loss=109.5648
	step [70/147], loss=129.9388
	step [71/147], loss=112.4020
	step [72/147], loss=90.1778
	step [73/147], loss=117.3771
	step [74/147], loss=108.9874
	step [75/147], loss=114.0261
	step [76/147], loss=90.1152
	step [77/147], loss=112.0972
	step [78/147], loss=116.8928
	step [79/147], loss=104.6395
	step [80/147], loss=119.0380
	step [81/147], loss=111.6606
	step [82/147], loss=93.0880
	step [83/147], loss=127.0152
	step [84/147], loss=132.7336
	step [85/147], loss=108.3158
	step [86/147], loss=111.4883
	step [87/147], loss=102.1541
	step [88/147], loss=110.4214
	step [89/147], loss=107.8205
	step [90/147], loss=132.7227
	step [91/147], loss=119.1349
	step [92/147], loss=126.5225
	step [93/147], loss=103.8477
	step [94/147], loss=127.3973
	step [95/147], loss=116.6405
	step [96/147], loss=115.3432
	step [97/147], loss=120.2296
	step [98/147], loss=133.7179
	step [99/147], loss=93.7231
	step [100/147], loss=131.4456
	step [101/147], loss=99.2894
	step [102/147], loss=114.3671
	step [103/147], loss=116.9792
	step [104/147], loss=113.5109
	step [105/147], loss=90.5818
	step [106/147], loss=116.6094
	step [107/147], loss=99.6637
	step [108/147], loss=112.5148
	step [109/147], loss=120.5600
	step [110/147], loss=108.9980
	step [111/147], loss=135.2822
	step [112/147], loss=106.4699
	step [113/147], loss=106.3722
	step [114/147], loss=106.9662
	step [115/147], loss=107.1595
	step [116/147], loss=92.4559
	step [117/147], loss=126.5263
	step [118/147], loss=109.2610
	step [119/147], loss=105.8195
	step [120/147], loss=123.2569
	step [121/147], loss=124.6511
	step [122/147], loss=94.8223
	step [123/147], loss=110.1025
	step [124/147], loss=113.4095
	step [125/147], loss=111.4562
	step [126/147], loss=110.9417
	step [127/147], loss=126.5972
	step [128/147], loss=113.0910
	step [129/147], loss=129.0938
	step [130/147], loss=118.4502
	step [131/147], loss=89.1765
	step [132/147], loss=108.6714
	step [133/147], loss=118.8482
	step [134/147], loss=104.4628
	step [135/147], loss=102.1472
	step [136/147], loss=118.3916
	step [137/147], loss=108.6286
	step [138/147], loss=104.9473
	step [139/147], loss=121.0294
	step [140/147], loss=106.5147
	step [141/147], loss=99.2957
	step [142/147], loss=96.2020
	step [143/147], loss=85.5836
	step [144/147], loss=122.5217
	step [145/147], loss=106.2064
	step [146/147], loss=110.8564
	step [147/147], loss=59.2862
	Evaluating
	loss=0.0327, precision=0.3407, recall=0.9431, f1=0.5006
Training epoch 17
	step [1/147], loss=101.4433
	step [2/147], loss=103.8360
	step [3/147], loss=114.6167
	step [4/147], loss=110.1369
	step [5/147], loss=111.6092
	step [6/147], loss=113.0275
	step [7/147], loss=124.8867
	step [8/147], loss=121.5042
	step [9/147], loss=113.9304
	step [10/147], loss=100.7390
	step [11/147], loss=112.3396
	step [12/147], loss=104.2087
	step [13/147], loss=136.7684
	step [14/147], loss=117.4003
	step [15/147], loss=96.7584
	step [16/147], loss=115.7292
	step [17/147], loss=121.0327
	step [18/147], loss=127.4106
	step [19/147], loss=112.4278
	step [20/147], loss=115.7497
	step [21/147], loss=103.6663
	step [22/147], loss=114.3895
	step [23/147], loss=98.0730
	step [24/147], loss=124.9615
	step [25/147], loss=122.2958
	step [26/147], loss=132.7480
	step [27/147], loss=100.1782
	step [28/147], loss=109.5216
	step [29/147], loss=100.9201
	step [30/147], loss=113.0052
	step [31/147], loss=111.1275
	step [32/147], loss=115.0149
	step [33/147], loss=124.9868
	step [34/147], loss=104.7482
	step [35/147], loss=105.9348
	step [36/147], loss=97.1705
	step [37/147], loss=118.7720
	step [38/147], loss=114.1306
	step [39/147], loss=106.3609
	step [40/147], loss=125.1339
	step [41/147], loss=112.5430
	step [42/147], loss=113.2100
	step [43/147], loss=108.5010
	step [44/147], loss=129.6058
	step [45/147], loss=106.0252
	step [46/147], loss=122.0462
	step [47/147], loss=121.8103
	step [48/147], loss=121.2868
	step [49/147], loss=141.1064
	step [50/147], loss=91.1188
	step [51/147], loss=133.7729
	step [52/147], loss=131.0732
	step [53/147], loss=110.4531
	step [54/147], loss=112.8620
	step [55/147], loss=97.6549
	step [56/147], loss=111.9391
	step [57/147], loss=116.9092
	step [58/147], loss=127.5482
	step [59/147], loss=116.2946
	step [60/147], loss=84.2879
	step [61/147], loss=113.8999
	step [62/147], loss=106.5357
	step [63/147], loss=107.3002
	step [64/147], loss=121.9312
	step [65/147], loss=117.8003
	step [66/147], loss=114.5804
	step [67/147], loss=127.2676
	step [68/147], loss=84.1041
	step [69/147], loss=107.8644
	step [70/147], loss=106.6888
	step [71/147], loss=115.3118
	step [72/147], loss=102.2613
	step [73/147], loss=86.5159
	step [74/147], loss=98.6242
	step [75/147], loss=98.3574
	step [76/147], loss=122.6691
	step [77/147], loss=114.6320
	step [78/147], loss=119.3548
	step [79/147], loss=134.1102
	step [80/147], loss=98.1628
	step [81/147], loss=105.0532
	step [82/147], loss=124.4699
	step [83/147], loss=110.0927
	step [84/147], loss=118.3916
	step [85/147], loss=129.6678
	step [86/147], loss=100.0884
	step [87/147], loss=107.1859
	step [88/147], loss=100.5991
	step [89/147], loss=90.2019
	step [90/147], loss=128.6902
	step [91/147], loss=94.2124
	step [92/147], loss=101.9211
	step [93/147], loss=94.3403
	step [94/147], loss=87.4536
	step [95/147], loss=114.0571
	step [96/147], loss=99.7414
	step [97/147], loss=116.4752
	step [98/147], loss=105.9300
	step [99/147], loss=116.9686
	step [100/147], loss=122.3871
	step [101/147], loss=101.8543
	step [102/147], loss=109.7525
	step [103/147], loss=110.4436
	step [104/147], loss=98.9299
	step [105/147], loss=109.3183
	step [106/147], loss=119.3971
	step [107/147], loss=111.5307
	step [108/147], loss=114.6313
	step [109/147], loss=122.4270
	step [110/147], loss=101.1739
	step [111/147], loss=98.2774
	step [112/147], loss=123.3355
	step [113/147], loss=113.1020
	step [114/147], loss=110.8147
	step [115/147], loss=88.7774
	step [116/147], loss=101.0658
	step [117/147], loss=112.1498
	step [118/147], loss=102.1014
	step [119/147], loss=101.5993
	step [120/147], loss=100.6946
	step [121/147], loss=119.8437
	step [122/147], loss=85.4729
	step [123/147], loss=95.0739
	step [124/147], loss=116.5805
	step [125/147], loss=107.5463
	step [126/147], loss=114.6048
	step [127/147], loss=109.6813
	step [128/147], loss=101.1784
	step [129/147], loss=120.3334
	step [130/147], loss=110.5046
	step [131/147], loss=108.7101
	step [132/147], loss=123.0136
	step [133/147], loss=118.4120
	step [134/147], loss=106.9475
	step [135/147], loss=106.8365
	step [136/147], loss=113.5301
	step [137/147], loss=98.2718
	step [138/147], loss=128.1301
	step [139/147], loss=107.3051
	step [140/147], loss=108.5513
	step [141/147], loss=110.0463
	step [142/147], loss=116.3371
	step [143/147], loss=114.8672
	step [144/147], loss=103.8567
	step [145/147], loss=98.5398
	step [146/147], loss=129.5947
	step [147/147], loss=47.9896
	Evaluating
	loss=0.0293, precision=0.3554, recall=0.9434, f1=0.5163
Training epoch 18
	step [1/147], loss=129.8512
	step [2/147], loss=101.1094
	step [3/147], loss=101.3261
	step [4/147], loss=125.1898
	step [5/147], loss=115.7002
	step [6/147], loss=82.7753
	step [7/147], loss=115.9773
	step [8/147], loss=112.4966
	step [9/147], loss=108.5132
	step [10/147], loss=116.6693
	step [11/147], loss=111.8855
	step [12/147], loss=111.3280
	step [13/147], loss=91.3191
	step [14/147], loss=109.3189
	step [15/147], loss=129.1545
	step [16/147], loss=114.1604
	step [17/147], loss=133.4709
	step [18/147], loss=111.6658
	step [19/147], loss=103.6969
	step [20/147], loss=119.6497
	step [21/147], loss=126.5958
	step [22/147], loss=97.1365
	step [23/147], loss=107.9722
	step [24/147], loss=133.3214
	step [25/147], loss=107.8607
	step [26/147], loss=101.2758
	step [27/147], loss=106.8407
	step [28/147], loss=89.1477
	step [29/147], loss=115.6469
	step [30/147], loss=123.8361
	step [31/147], loss=116.7898
	step [32/147], loss=125.6594
	step [33/147], loss=115.8258
	step [34/147], loss=117.4148
	step [35/147], loss=100.1839
	step [36/147], loss=105.5799
	step [37/147], loss=119.2936
	step [38/147], loss=93.8954
	step [39/147], loss=120.4246
	step [40/147], loss=123.2950
	step [41/147], loss=94.6219
	step [42/147], loss=107.8917
	step [43/147], loss=104.2808
	step [44/147], loss=122.6057
	step [45/147], loss=132.1990
	step [46/147], loss=104.9477
	step [47/147], loss=90.6159
	step [48/147], loss=117.8284
	step [49/147], loss=107.1846
	step [50/147], loss=94.1226
	step [51/147], loss=117.4576
	step [52/147], loss=106.0134
	step [53/147], loss=121.4795
	step [54/147], loss=99.3411
	step [55/147], loss=120.7456
	step [56/147], loss=113.7201
	step [57/147], loss=122.6379
	step [58/147], loss=111.0351
	step [59/147], loss=116.1330
	step [60/147], loss=115.6077
	step [61/147], loss=107.8252
	step [62/147], loss=89.7559
	step [63/147], loss=99.1108
	step [64/147], loss=107.7220
	step [65/147], loss=109.9930
	step [66/147], loss=112.4088
	step [67/147], loss=106.3141
	step [68/147], loss=118.2943
	step [69/147], loss=102.9992
	step [70/147], loss=110.4257
	step [71/147], loss=141.5099
	step [72/147], loss=92.8991
	step [73/147], loss=110.4187
	step [74/147], loss=128.3241
	step [75/147], loss=115.1304
	step [76/147], loss=97.6973
	step [77/147], loss=119.2732
	step [78/147], loss=104.4723
	step [79/147], loss=103.1203
	step [80/147], loss=102.2422
	step [81/147], loss=103.0823
	step [82/147], loss=94.9230
	step [83/147], loss=121.7734
	step [84/147], loss=104.8436
	step [85/147], loss=109.0749
	step [86/147], loss=114.9193
	step [87/147], loss=101.7589
	step [88/147], loss=100.2305
	step [89/147], loss=95.7619
	step [90/147], loss=100.2863
	step [91/147], loss=96.5693
	step [92/147], loss=109.6217
	step [93/147], loss=110.8026
	step [94/147], loss=124.5410
	step [95/147], loss=110.3113
	step [96/147], loss=114.2724
	step [97/147], loss=104.8991
	step [98/147], loss=121.5058
	step [99/147], loss=103.6723
	step [100/147], loss=122.7102
	step [101/147], loss=111.1018
	step [102/147], loss=114.6285
	step [103/147], loss=101.0749
	step [104/147], loss=107.1069
	step [105/147], loss=98.6716
	step [106/147], loss=84.6868
	step [107/147], loss=127.8673
	step [108/147], loss=111.6085
	step [109/147], loss=104.2803
	step [110/147], loss=110.0704
	step [111/147], loss=114.8203
	step [112/147], loss=113.1782
	step [113/147], loss=114.0525
	step [114/147], loss=108.6628
	step [115/147], loss=107.1106
	step [116/147], loss=121.3683
	step [117/147], loss=134.4920
	step [118/147], loss=115.5979
	step [119/147], loss=110.1864
	step [120/147], loss=106.9319
	step [121/147], loss=93.8650
	step [122/147], loss=117.7311
	step [123/147], loss=111.5927
	step [124/147], loss=92.0096
	step [125/147], loss=110.6424
	step [126/147], loss=112.0971
	step [127/147], loss=107.9034
	step [128/147], loss=104.5506
	step [129/147], loss=114.3811
	step [130/147], loss=114.8900
	step [131/147], loss=106.7127
	step [132/147], loss=111.2059
	step [133/147], loss=108.8450
	step [134/147], loss=102.0066
	step [135/147], loss=115.3446
	step [136/147], loss=87.6866
	step [137/147], loss=108.3345
	step [138/147], loss=113.0312
	step [139/147], loss=89.7412
	step [140/147], loss=109.0672
	step [141/147], loss=91.4061
	step [142/147], loss=105.4072
	step [143/147], loss=116.2786
	step [144/147], loss=94.9699
	step [145/147], loss=77.2130
	step [146/147], loss=112.3206
	step [147/147], loss=44.3315
	Evaluating
	loss=0.0287, precision=0.3141, recall=0.9410, f1=0.4709
Training epoch 19
	step [1/147], loss=88.4577
	step [2/147], loss=104.0881
	step [3/147], loss=116.5340
	step [4/147], loss=124.5399
	step [5/147], loss=109.7474
	step [6/147], loss=127.9521
	step [7/147], loss=99.0605
	step [8/147], loss=113.5207
	step [9/147], loss=116.7812
	step [10/147], loss=118.8485
	step [11/147], loss=107.1746
	step [12/147], loss=99.4473
	step [13/147], loss=108.4550
	step [14/147], loss=112.8292
	step [15/147], loss=102.3508
	step [16/147], loss=106.5453
	step [17/147], loss=101.2000
	step [18/147], loss=113.3818
	step [19/147], loss=129.0313
	step [20/147], loss=103.3387
	step [21/147], loss=121.9648
	step [22/147], loss=105.3779
	step [23/147], loss=115.2945
	step [24/147], loss=120.7044
	step [25/147], loss=114.7640
	step [26/147], loss=107.1644
	step [27/147], loss=110.1430
	step [28/147], loss=104.7748
	step [29/147], loss=95.9140
	step [30/147], loss=117.5834
	step [31/147], loss=112.3617
	step [32/147], loss=116.8853
	step [33/147], loss=107.6810
	step [34/147], loss=123.4647
	step [35/147], loss=120.0273
	step [36/147], loss=107.1153
	step [37/147], loss=94.8829
	step [38/147], loss=100.1165
	step [39/147], loss=113.2094
	step [40/147], loss=98.9682
	step [41/147], loss=109.6570
	step [42/147], loss=101.8148
	step [43/147], loss=100.3430
	step [44/147], loss=85.0461
	step [45/147], loss=120.5551
	step [46/147], loss=124.2670
	step [47/147], loss=94.6163
	step [48/147], loss=104.2639
	step [49/147], loss=105.3248
	step [50/147], loss=108.6762
	step [51/147], loss=98.7382
	step [52/147], loss=111.5386
	step [53/147], loss=113.5427
	step [54/147], loss=114.3411
	step [55/147], loss=106.2572
	step [56/147], loss=109.5294
	step [57/147], loss=113.4588
	step [58/147], loss=118.9135
	step [59/147], loss=86.6479
	step [60/147], loss=90.4427
	step [61/147], loss=92.6648
	step [62/147], loss=105.1643
	step [63/147], loss=94.3869
	step [64/147], loss=89.5480
	step [65/147], loss=109.8062
	step [66/147], loss=113.9597
	step [67/147], loss=110.0969
	step [68/147], loss=114.3375
	step [69/147], loss=136.0500
	step [70/147], loss=90.9841
	step [71/147], loss=112.5264
	step [72/147], loss=117.9682
	step [73/147], loss=95.2126
	step [74/147], loss=114.7119
	step [75/147], loss=114.3975
	step [76/147], loss=146.8274
	step [77/147], loss=110.5324
	step [78/147], loss=114.1843
	step [79/147], loss=118.9180
	step [80/147], loss=90.4112
	step [81/147], loss=117.9194
	step [82/147], loss=102.7998
	step [83/147], loss=108.0639
	step [84/147], loss=102.2734
	step [85/147], loss=100.6693
	step [86/147], loss=88.3792
	step [87/147], loss=104.5502
	step [88/147], loss=92.2752
	step [89/147], loss=97.6552
	step [90/147], loss=108.3520
	step [91/147], loss=102.7743
	step [92/147], loss=105.9416
	step [93/147], loss=112.0854
	step [94/147], loss=122.7568
	step [95/147], loss=115.2170
	step [96/147], loss=106.3932
	step [97/147], loss=112.3531
	step [98/147], loss=111.4613
	step [99/147], loss=102.5742
	step [100/147], loss=117.7882
	step [101/147], loss=119.6493
	step [102/147], loss=127.4262
	step [103/147], loss=113.3004
	step [104/147], loss=106.1622
	step [105/147], loss=129.3460
	step [106/147], loss=113.4039
	step [107/147], loss=126.1448
	step [108/147], loss=124.4530
	step [109/147], loss=99.0529
	step [110/147], loss=112.1481
	step [111/147], loss=120.4923
	step [112/147], loss=113.4950
	step [113/147], loss=124.5526
	step [114/147], loss=94.7800
	step [115/147], loss=112.6006
	step [116/147], loss=120.8908
	step [117/147], loss=89.8966
	step [118/147], loss=111.5263
	step [119/147], loss=98.9189
	step [120/147], loss=107.9320
	step [121/147], loss=112.9635
	step [122/147], loss=134.9245
	step [123/147], loss=84.8225
	step [124/147], loss=97.2543
	step [125/147], loss=104.8105
	step [126/147], loss=116.4613
	step [127/147], loss=110.8777
	step [128/147], loss=108.7725
	step [129/147], loss=121.6316
	step [130/147], loss=92.7178
	step [131/147], loss=96.8908
	step [132/147], loss=103.9676
	step [133/147], loss=104.5303
	step [134/147], loss=98.6038
	step [135/147], loss=113.9431
	step [136/147], loss=118.6415
	step [137/147], loss=111.6310
	step [138/147], loss=102.0365
	step [139/147], loss=76.6605
	step [140/147], loss=99.8829
	step [141/147], loss=115.4865
	step [142/147], loss=92.8508
	step [143/147], loss=95.3359
	step [144/147], loss=114.2110
	step [145/147], loss=107.9880
	step [146/147], loss=102.4819
	step [147/147], loss=41.0945
	Evaluating
	loss=0.0251, precision=0.3797, recall=0.9212, f1=0.5378
Training epoch 20
	step [1/147], loss=143.0907
	step [2/147], loss=93.5481
	step [3/147], loss=94.3846
	step [4/147], loss=107.9249
	step [5/147], loss=113.1981
	step [6/147], loss=109.8926
	step [7/147], loss=134.5354
	step [8/147], loss=87.8845
	step [9/147], loss=104.6495
	step [10/147], loss=90.4434
	step [11/147], loss=100.0268
	step [12/147], loss=90.5868
	step [13/147], loss=95.1773
	step [14/147], loss=97.7860
	step [15/147], loss=114.2587
	step [16/147], loss=102.0333
	step [17/147], loss=110.4472
	step [18/147], loss=124.5257
	step [19/147], loss=107.1688
	step [20/147], loss=90.7882
	step [21/147], loss=113.8576
	step [22/147], loss=116.6354
	step [23/147], loss=124.0009
	step [24/147], loss=122.8027
	step [25/147], loss=109.7000
	step [26/147], loss=112.1810
	step [27/147], loss=97.0352
	step [28/147], loss=83.2964
	step [29/147], loss=101.3521
	step [30/147], loss=121.6098
	step [31/147], loss=103.9874
	step [32/147], loss=108.1856
	step [33/147], loss=113.3268
	step [34/147], loss=105.6502
	step [35/147], loss=105.9065
	step [36/147], loss=101.1851
	step [37/147], loss=98.7185
	step [38/147], loss=105.1384
	step [39/147], loss=90.9140
	step [40/147], loss=101.4256
	step [41/147], loss=121.1158
	step [42/147], loss=106.6762
	step [43/147], loss=95.5567
	step [44/147], loss=123.7731
	step [45/147], loss=99.6243
	step [46/147], loss=104.7106
	step [47/147], loss=109.7998
	step [48/147], loss=98.8380
	step [49/147], loss=120.1066
	step [50/147], loss=110.9595
	step [51/147], loss=91.6074
	step [52/147], loss=109.8775
	step [53/147], loss=92.8179
	step [54/147], loss=116.5137
	step [55/147], loss=98.3687
	step [56/147], loss=94.6619
	step [57/147], loss=94.4319
	step [58/147], loss=105.6749
	step [59/147], loss=106.7471
	step [60/147], loss=104.7263
	step [61/147], loss=113.1531
	step [62/147], loss=98.9426
	step [63/147], loss=101.1503
	step [64/147], loss=89.4860
	step [65/147], loss=121.0781
	step [66/147], loss=139.6719
	step [67/147], loss=103.0905
	step [68/147], loss=110.0879
	step [69/147], loss=103.6752
	step [70/147], loss=111.6316
	step [71/147], loss=104.1022
	step [72/147], loss=102.6869
	step [73/147], loss=131.3823
	step [74/147], loss=90.9670
	step [75/147], loss=134.0679
	step [76/147], loss=111.8424
	step [77/147], loss=129.5538
	step [78/147], loss=113.7294
	step [79/147], loss=100.6271
	step [80/147], loss=120.4883
	step [81/147], loss=105.1335
	step [82/147], loss=106.0457
	step [83/147], loss=96.6699
	step [84/147], loss=103.1328
	step [85/147], loss=113.1882
	step [86/147], loss=107.6270
	step [87/147], loss=109.2676
	step [88/147], loss=127.0221
	step [89/147], loss=130.8264
	step [90/147], loss=106.1380
	step [91/147], loss=104.8904
	step [92/147], loss=108.0588
	step [93/147], loss=100.5103
	step [94/147], loss=104.6885
	step [95/147], loss=102.5486
	step [96/147], loss=103.8975
	step [97/147], loss=120.1009
	step [98/147], loss=102.2978
	step [99/147], loss=110.4195
	step [100/147], loss=90.4411
	step [101/147], loss=108.1536
	step [102/147], loss=107.5187
	step [103/147], loss=112.2595
	step [104/147], loss=103.3406
	step [105/147], loss=127.0665
	step [106/147], loss=94.9339
	step [107/147], loss=101.4429
	step [108/147], loss=122.7890
	step [109/147], loss=120.7029
	step [110/147], loss=108.6810
	step [111/147], loss=100.2875
	step [112/147], loss=105.1725
	step [113/147], loss=94.6128
	step [114/147], loss=112.0203
	step [115/147], loss=132.1574
	step [116/147], loss=115.8864
	step [117/147], loss=107.4539
	step [118/147], loss=105.2332
	step [119/147], loss=110.1874
	step [120/147], loss=96.5511
	step [121/147], loss=119.6590
	step [122/147], loss=116.7868
	step [123/147], loss=130.7820
	step [124/147], loss=120.1966
	step [125/147], loss=114.2894
	step [126/147], loss=91.0308
	step [127/147], loss=109.3035
	step [128/147], loss=114.0510
	step [129/147], loss=111.5971
	step [130/147], loss=99.3973
	step [131/147], loss=110.9856
	step [132/147], loss=101.1207
	step [133/147], loss=111.9690
	step [134/147], loss=84.6676
	step [135/147], loss=90.0556
	step [136/147], loss=128.0473
	step [137/147], loss=94.8096
	step [138/147], loss=107.2656
	step [139/147], loss=117.3180
	step [140/147], loss=86.0911
	step [141/147], loss=108.1985
	step [142/147], loss=113.9148
	step [143/147], loss=113.5578
	step [144/147], loss=119.9281
	step [145/147], loss=103.2438
	step [146/147], loss=91.2863
	step [147/147], loss=46.5316
	Evaluating
	loss=0.0253, precision=0.3258, recall=0.9266, f1=0.4821
Training epoch 21
	step [1/147], loss=127.9645
	step [2/147], loss=120.3890
	step [3/147], loss=122.9504
	step [4/147], loss=93.6888
	step [5/147], loss=115.9105
	step [6/147], loss=118.9996
	step [7/147], loss=87.0538
	step [8/147], loss=105.4845
	step [9/147], loss=118.4046
	step [10/147], loss=93.4554
	step [11/147], loss=123.9246
	step [12/147], loss=131.5154
	step [13/147], loss=137.8824
	step [14/147], loss=120.7534
	step [15/147], loss=98.5825
	step [16/147], loss=104.1730
	step [17/147], loss=92.2962
	step [18/147], loss=93.8949
	step [19/147], loss=102.6516
	step [20/147], loss=92.5101
	step [21/147], loss=106.6141
	step [22/147], loss=133.1865
	step [23/147], loss=99.8308
	step [24/147], loss=103.5718
	step [25/147], loss=101.5333
	step [26/147], loss=108.8231
	step [27/147], loss=103.0412
	step [28/147], loss=113.6172
	step [29/147], loss=95.0066
	step [30/147], loss=123.0468
	step [31/147], loss=103.2896
	step [32/147], loss=101.6767
	step [33/147], loss=106.5624
	step [34/147], loss=95.5451
	step [35/147], loss=107.2972
	step [36/147], loss=107.2608
	step [37/147], loss=123.3881
	step [38/147], loss=107.9008
	step [39/147], loss=121.4710
	step [40/147], loss=124.2196
	step [41/147], loss=92.2132
	step [42/147], loss=105.3150
	step [43/147], loss=92.4235
	step [44/147], loss=107.1213
	step [45/147], loss=110.7694
	step [46/147], loss=115.6735
	step [47/147], loss=106.7770
	step [48/147], loss=128.3461
	step [49/147], loss=120.9276
	step [50/147], loss=95.9990
	step [51/147], loss=114.6523
	step [52/147], loss=97.5001
	step [53/147], loss=113.2893
	step [54/147], loss=91.7478
	step [55/147], loss=96.0647
	step [56/147], loss=97.0096
	step [57/147], loss=86.8814
	step [58/147], loss=119.1105
	step [59/147], loss=92.0570
	step [60/147], loss=115.6781
	step [61/147], loss=100.6181
	step [62/147], loss=99.6420
	step [63/147], loss=105.1635
	step [64/147], loss=112.4407
	step [65/147], loss=108.9236
	step [66/147], loss=106.9776
	step [67/147], loss=109.8202
	step [68/147], loss=98.8409
	step [69/147], loss=125.4025
	step [70/147], loss=99.4140
	step [71/147], loss=99.8356
	step [72/147], loss=114.4368
	step [73/147], loss=106.7597
	step [74/147], loss=104.7027
	step [75/147], loss=114.4838
	step [76/147], loss=124.3051
	step [77/147], loss=110.6657
	step [78/147], loss=107.0672
	step [79/147], loss=106.1591
	step [80/147], loss=90.5647
	step [81/147], loss=105.5558
	step [82/147], loss=102.7174
	step [83/147], loss=98.1544
	step [84/147], loss=118.6630
	step [85/147], loss=108.3942
	step [86/147], loss=103.2843
	step [87/147], loss=108.9522
	step [88/147], loss=112.8121
	step [89/147], loss=110.6095
	step [90/147], loss=123.4419
	step [91/147], loss=101.0858
	step [92/147], loss=105.3250
	step [93/147], loss=104.1193
	step [94/147], loss=132.2333
	step [95/147], loss=103.9714
	step [96/147], loss=89.1756
	step [97/147], loss=101.5451
	step [98/147], loss=93.6632
	step [99/147], loss=104.9461
	step [100/147], loss=115.5242
	step [101/147], loss=97.4189
	step [102/147], loss=110.0444
	step [103/147], loss=85.6749
	step [104/147], loss=102.1483
	step [105/147], loss=99.3658
	step [106/147], loss=118.2555
	step [107/147], loss=92.1781
	step [108/147], loss=115.3688
	step [109/147], loss=115.6530
	step [110/147], loss=118.5759
	step [111/147], loss=83.2324
	step [112/147], loss=90.7650
	step [113/147], loss=121.6935
	step [114/147], loss=93.8503
	step [115/147], loss=97.8631
	step [116/147], loss=110.5429
	step [117/147], loss=93.4511
	step [118/147], loss=92.5669
	step [119/147], loss=112.1472
	step [120/147], loss=103.1226
	step [121/147], loss=97.2472
	step [122/147], loss=118.9580
	step [123/147], loss=101.0945
	step [124/147], loss=105.6911
	step [125/147], loss=89.3506
	step [126/147], loss=115.2658
	step [127/147], loss=125.2719
	step [128/147], loss=114.6774
	step [129/147], loss=97.1940
	step [130/147], loss=103.5473
	step [131/147], loss=106.5725
	step [132/147], loss=103.9278
	step [133/147], loss=105.6828
	step [134/147], loss=96.5772
	step [135/147], loss=95.9845
	step [136/147], loss=114.7738
	step [137/147], loss=118.2150
	step [138/147], loss=112.2438
	step [139/147], loss=107.4103
	step [140/147], loss=99.0303
	step [141/147], loss=123.8751
	step [142/147], loss=113.2695
	step [143/147], loss=92.8606
	step [144/147], loss=113.0894
	step [145/147], loss=115.7992
	step [146/147], loss=98.9503
	step [147/147], loss=39.6452
	Evaluating
	loss=0.0222, precision=0.3684, recall=0.9334, f1=0.5283
Training epoch 22
	step [1/147], loss=108.9137
	step [2/147], loss=91.9635
	step [3/147], loss=121.1104
	step [4/147], loss=101.4174
	step [5/147], loss=105.0383
	step [6/147], loss=111.8952
	step [7/147], loss=102.8824
	step [8/147], loss=88.2109
	step [9/147], loss=109.1952
	step [10/147], loss=92.7185
	step [11/147], loss=96.3503
	step [12/147], loss=105.0319
	step [13/147], loss=91.4615
	step [14/147], loss=92.5291
	step [15/147], loss=125.7845
	step [16/147], loss=103.5444
	step [17/147], loss=98.1262
	step [18/147], loss=99.1181
	step [19/147], loss=113.8207
	step [20/147], loss=119.0894
	step [21/147], loss=101.4526
	step [22/147], loss=102.5652
	step [23/147], loss=109.1244
	step [24/147], loss=112.8737
	step [25/147], loss=97.9823
	step [26/147], loss=97.2968
	step [27/147], loss=123.3893
	step [28/147], loss=116.5159
	step [29/147], loss=112.5876
	step [30/147], loss=97.5291
	step [31/147], loss=102.3225
	step [32/147], loss=102.8941
	step [33/147], loss=101.8763
	step [34/147], loss=95.2131
	step [35/147], loss=91.8886
	step [36/147], loss=95.5554
	step [37/147], loss=117.5514
	step [38/147], loss=96.9235
	step [39/147], loss=99.4444
	step [40/147], loss=124.7166
	step [41/147], loss=110.7110
	step [42/147], loss=92.4673
	step [43/147], loss=108.7443
	step [44/147], loss=109.3833
	step [45/147], loss=116.9070
	step [46/147], loss=91.0797
	step [47/147], loss=116.3963
	step [48/147], loss=101.4597
	step [49/147], loss=108.2699
	step [50/147], loss=126.6279
	step [51/147], loss=93.5542
	step [52/147], loss=132.8429
	step [53/147], loss=109.2258
	step [54/147], loss=89.7321
	step [55/147], loss=110.1931
	step [56/147], loss=103.1945
	step [57/147], loss=127.3693
	step [58/147], loss=106.5757
	step [59/147], loss=107.6444
	step [60/147], loss=98.1588
	step [61/147], loss=111.2383
	step [62/147], loss=100.6912
	step [63/147], loss=96.2225
	step [64/147], loss=110.4818
	step [65/147], loss=89.0730
	step [66/147], loss=111.0666
	step [67/147], loss=107.9472
	step [68/147], loss=122.9983
	step [69/147], loss=114.2618
	step [70/147], loss=99.2247
	step [71/147], loss=98.8118
	step [72/147], loss=105.3323
	step [73/147], loss=85.2977
	step [74/147], loss=102.3916
	step [75/147], loss=106.8317
	step [76/147], loss=106.1764
	step [77/147], loss=95.0692
	step [78/147], loss=94.6096
	step [79/147], loss=112.6009
	step [80/147], loss=112.0526
	step [81/147], loss=110.3619
	step [82/147], loss=101.5051
	step [83/147], loss=102.0361
	step [84/147], loss=113.4123
	step [85/147], loss=98.5852
	step [86/147], loss=99.6353
	step [87/147], loss=97.0090
	step [88/147], loss=126.0887
	step [89/147], loss=101.7303
	step [90/147], loss=116.5617
	step [91/147], loss=117.0008
	step [92/147], loss=101.9085
	step [93/147], loss=101.2659
	step [94/147], loss=101.9887
	step [95/147], loss=98.7892
	step [96/147], loss=103.9413
	step [97/147], loss=99.4793
	step [98/147], loss=105.6575
	step [99/147], loss=112.3767
	step [100/147], loss=116.8030
	step [101/147], loss=122.9762
	step [102/147], loss=121.8282
	step [103/147], loss=91.3322
	step [104/147], loss=100.1732
	step [105/147], loss=116.5970
	step [106/147], loss=110.6870
	step [107/147], loss=117.7665
	step [108/147], loss=110.2473
	step [109/147], loss=107.9080
	step [110/147], loss=100.3474
	step [111/147], loss=123.1789
	step [112/147], loss=113.3514
	step [113/147], loss=104.3849
	step [114/147], loss=136.7682
	step [115/147], loss=102.1217
	step [116/147], loss=105.5856
	step [117/147], loss=103.3444
	step [118/147], loss=112.0493
	step [119/147], loss=114.3244
	step [120/147], loss=96.4269
	step [121/147], loss=100.2892
	step [122/147], loss=97.9032
	step [123/147], loss=94.5780
	step [124/147], loss=109.0348
	step [125/147], loss=89.2205
	step [126/147], loss=112.0264
	step [127/147], loss=94.9631
	step [128/147], loss=101.7733
	step [129/147], loss=112.4036
	step [130/147], loss=108.4503
	step [131/147], loss=118.0228
	step [132/147], loss=91.4793
	step [133/147], loss=84.0187
	step [134/147], loss=115.6411
	step [135/147], loss=117.0010
	step [136/147], loss=100.9884
	step [137/147], loss=92.4263
	step [138/147], loss=96.4419
	step [139/147], loss=91.4911
	step [140/147], loss=102.7442
	step [141/147], loss=89.5198
	step [142/147], loss=99.3875
	step [143/147], loss=98.3093
	step [144/147], loss=108.0642
	step [145/147], loss=102.5140
	step [146/147], loss=105.4756
	step [147/147], loss=52.4211
	Evaluating
	loss=0.0191, precision=0.3948, recall=0.9314, f1=0.5545
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/147], loss=97.8707
	step [2/147], loss=103.9176
	step [3/147], loss=97.4353
	step [4/147], loss=110.4727
	step [5/147], loss=103.9195
	step [6/147], loss=98.5191
	step [7/147], loss=105.2775
	step [8/147], loss=114.3223
	step [9/147], loss=108.0287
	step [10/147], loss=114.3545
	step [11/147], loss=102.3982
	step [12/147], loss=126.2057
	step [13/147], loss=94.7077
	step [14/147], loss=89.8686
	step [15/147], loss=110.5874
	step [16/147], loss=86.0620
	step [17/147], loss=109.4466
	step [18/147], loss=104.7450
	step [19/147], loss=81.9493
	step [20/147], loss=110.6754
	step [21/147], loss=91.9514
	step [22/147], loss=96.4097
	step [23/147], loss=120.3159
	step [24/147], loss=111.8925
	step [25/147], loss=112.8317
	step [26/147], loss=110.2805
	step [27/147], loss=92.6734
	step [28/147], loss=104.1433
	step [29/147], loss=125.6581
	step [30/147], loss=99.6322
	step [31/147], loss=100.7994
	step [32/147], loss=112.4553
	step [33/147], loss=109.9610
	step [34/147], loss=102.1142
	step [35/147], loss=122.9349
	step [36/147], loss=120.0368
	step [37/147], loss=107.2430
	step [38/147], loss=103.9517
	step [39/147], loss=122.3584
	step [40/147], loss=105.4602
	step [41/147], loss=131.6615
	step [42/147], loss=109.4014
	step [43/147], loss=135.2149
	step [44/147], loss=103.0014
	step [45/147], loss=101.8895
	step [46/147], loss=104.3276
	step [47/147], loss=96.5955
	step [48/147], loss=104.8045
	step [49/147], loss=107.6303
	step [50/147], loss=117.9123
	step [51/147], loss=98.3408
	step [52/147], loss=101.4348
	step [53/147], loss=98.9094
	step [54/147], loss=105.3567
	step [55/147], loss=125.5168
	step [56/147], loss=116.3096
	step [57/147], loss=98.1343
	step [58/147], loss=98.2913
	step [59/147], loss=116.9697
	step [60/147], loss=95.5289
	step [61/147], loss=121.3998
	step [62/147], loss=90.6456
	step [63/147], loss=108.7306
	step [64/147], loss=99.6203
	step [65/147], loss=102.4445
	step [66/147], loss=108.7355
	step [67/147], loss=129.3642
	step [68/147], loss=103.3745
	step [69/147], loss=111.5761
	step [70/147], loss=100.4333
	step [71/147], loss=114.3951
	step [72/147], loss=105.0115
	step [73/147], loss=109.7887
	step [74/147], loss=91.7417
	step [75/147], loss=107.2443
	step [76/147], loss=90.6542
	step [77/147], loss=111.8825
	step [78/147], loss=86.4617
	step [79/147], loss=94.4132
	step [80/147], loss=127.2291
	step [81/147], loss=127.0876
	step [82/147], loss=126.3360
	step [83/147], loss=105.8628
	step [84/147], loss=94.7245
	step [85/147], loss=93.3158
	step [86/147], loss=93.1712
	step [87/147], loss=96.3041
	step [88/147], loss=117.8464
	step [89/147], loss=88.5464
	step [90/147], loss=102.4705
	step [91/147], loss=105.9630
	step [92/147], loss=114.7411
	step [93/147], loss=120.2101
	step [94/147], loss=93.8034
	step [95/147], loss=102.8914
	step [96/147], loss=92.3502
	step [97/147], loss=113.8432
	step [98/147], loss=116.9989
	step [99/147], loss=94.4362
	step [100/147], loss=115.4256
	step [101/147], loss=111.3100
	step [102/147], loss=111.8129
	step [103/147], loss=92.7569
	step [104/147], loss=98.4390
	step [105/147], loss=104.8057
	step [106/147], loss=108.5451
	step [107/147], loss=99.2077
	step [108/147], loss=93.5828
	step [109/147], loss=102.3791
	step [110/147], loss=109.7475
	step [111/147], loss=101.8251
	step [112/147], loss=118.6818
	step [113/147], loss=100.4768
	step [114/147], loss=96.2190
	step [115/147], loss=101.2447
	step [116/147], loss=109.8179
	step [117/147], loss=105.9179
	step [118/147], loss=103.1649
	step [119/147], loss=121.9235
	step [120/147], loss=110.2455
	step [121/147], loss=101.1863
	step [122/147], loss=108.8186
	step [123/147], loss=86.0887
	step [124/147], loss=90.8934
	step [125/147], loss=118.1333
	step [126/147], loss=92.5594
	step [127/147], loss=87.9711
	step [128/147], loss=96.3885
	step [129/147], loss=104.0758
	step [130/147], loss=83.9041
	step [131/147], loss=114.3932
	step [132/147], loss=107.0595
	step [133/147], loss=102.4890
	step [134/147], loss=87.8049
	step [135/147], loss=101.4449
	step [136/147], loss=83.6199
	step [137/147], loss=89.0715
	step [138/147], loss=91.2206
	step [139/147], loss=98.0435
	step [140/147], loss=102.0412
	step [141/147], loss=102.8862
	step [142/147], loss=96.4755
	step [143/147], loss=101.5064
	step [144/147], loss=95.2874
	step [145/147], loss=100.7935
	step [146/147], loss=92.1171
	step [147/147], loss=39.9135
	Evaluating
	loss=0.0177, precision=0.4028, recall=0.9214, f1=0.5605
saving model as: 0_saved_model.pth
Training epoch 24
	step [1/147], loss=107.6852
	step [2/147], loss=91.7202
	step [3/147], loss=102.4337
	step [4/147], loss=101.3475
	step [5/147], loss=105.9604
	step [6/147], loss=108.6396
	step [7/147], loss=107.0233
	step [8/147], loss=108.0846
	step [9/147], loss=117.1423
	step [10/147], loss=111.4765
	step [11/147], loss=110.3322
	step [12/147], loss=97.5714
	step [13/147], loss=109.5254
	step [14/147], loss=103.6399
	step [15/147], loss=103.6176
	step [16/147], loss=96.8723
	step [17/147], loss=87.2231
	step [18/147], loss=105.7919
	step [19/147], loss=111.4955
	step [20/147], loss=87.8497
	step [21/147], loss=89.0762
	step [22/147], loss=115.2664
	step [23/147], loss=82.9504
	step [24/147], loss=115.0080
	step [25/147], loss=89.4323
	step [26/147], loss=102.0065
	step [27/147], loss=88.4031
	step [28/147], loss=96.0404
	step [29/147], loss=108.8541
	step [30/147], loss=122.8502
	step [31/147], loss=101.9669
	step [32/147], loss=96.9955
	step [33/147], loss=97.7173
	step [34/147], loss=110.1103
	step [35/147], loss=117.3421
	step [36/147], loss=113.7031
	step [37/147], loss=77.5822
	step [38/147], loss=110.2111
	step [39/147], loss=102.6680
	step [40/147], loss=109.0296
	step [41/147], loss=97.4027
	step [42/147], loss=115.8718
	step [43/147], loss=98.5935
	step [44/147], loss=86.0978
	step [45/147], loss=103.5593
	step [46/147], loss=92.6496
	step [47/147], loss=108.7404
	step [48/147], loss=122.3766
	step [49/147], loss=105.8837
	step [50/147], loss=120.6932
	step [51/147], loss=108.3253
	step [52/147], loss=105.9838
	step [53/147], loss=98.7002
	step [54/147], loss=108.7190
	step [55/147], loss=109.8492
	step [56/147], loss=91.9151
	step [57/147], loss=100.7876
	step [58/147], loss=89.2186
	step [59/147], loss=96.5843
	step [60/147], loss=108.6856
	step [61/147], loss=104.2113
	step [62/147], loss=114.1821
	step [63/147], loss=88.6003
	step [64/147], loss=116.4712
	step [65/147], loss=94.9024
	step [66/147], loss=100.3455
	step [67/147], loss=88.0689
	step [68/147], loss=111.1825
	step [69/147], loss=106.8117
	step [70/147], loss=91.6695
	step [71/147], loss=105.9747
	step [72/147], loss=110.3382
	step [73/147], loss=106.6043
	step [74/147], loss=99.2440
	step [75/147], loss=120.0112
	step [76/147], loss=100.5306
	step [77/147], loss=89.4933
	step [78/147], loss=102.4021
	step [79/147], loss=106.7430
	step [80/147], loss=97.1218
	step [81/147], loss=90.6738
	step [82/147], loss=89.7869
	step [83/147], loss=98.2285
	step [84/147], loss=99.5929
	step [85/147], loss=112.5172
	step [86/147], loss=97.4256
	step [87/147], loss=103.0070
	step [88/147], loss=105.2501
	step [89/147], loss=110.3658
	step [90/147], loss=99.7734
	step [91/147], loss=106.3315
	step [92/147], loss=121.6606
	step [93/147], loss=109.1144
	step [94/147], loss=89.3838
	step [95/147], loss=89.5336
	step [96/147], loss=87.4386
	step [97/147], loss=102.1814
	step [98/147], loss=121.3221
	step [99/147], loss=92.0611
	step [100/147], loss=95.2095
	step [101/147], loss=109.0532
	step [102/147], loss=127.5521
	step [103/147], loss=107.1697
	step [104/147], loss=105.4014
	step [105/147], loss=107.8712
	step [106/147], loss=110.9451
	step [107/147], loss=106.6868
	step [108/147], loss=93.4483
	step [109/147], loss=114.1025
	step [110/147], loss=99.3300
	step [111/147], loss=90.4538
	step [112/147], loss=105.8514
	step [113/147], loss=101.9812
	step [114/147], loss=108.5851
	step [115/147], loss=117.5410
	step [116/147], loss=113.0855
	step [117/147], loss=104.3903
	step [118/147], loss=107.4174
	step [119/147], loss=90.2659
	step [120/147], loss=114.8987
	step [121/147], loss=92.5072
	step [122/147], loss=89.5665
	step [123/147], loss=110.0883
	step [124/147], loss=102.5052
	step [125/147], loss=99.0010
	step [126/147], loss=94.5429
	step [127/147], loss=118.9500
	step [128/147], loss=108.1403
	step [129/147], loss=116.2721
	step [130/147], loss=93.7992
	step [131/147], loss=99.0007
	step [132/147], loss=104.7486
	step [133/147], loss=117.2090
	step [134/147], loss=114.6736
	step [135/147], loss=98.6964
	step [136/147], loss=111.6806
	step [137/147], loss=114.1140
	step [138/147], loss=104.0317
	step [139/147], loss=96.9422
	step [140/147], loss=109.8954
	step [141/147], loss=115.4315
	step [142/147], loss=106.2357
	step [143/147], loss=94.3563
	step [144/147], loss=104.8865
	step [145/147], loss=95.1721
	step [146/147], loss=109.5031
	step [147/147], loss=40.5604
	Evaluating
	loss=0.0192, precision=0.4070, recall=0.9045, f1=0.5614
saving model as: 0_saved_model.pth
Training epoch 25
	step [1/147], loss=94.6172
	step [2/147], loss=102.8941
	step [3/147], loss=108.0857
	step [4/147], loss=118.0509
	step [5/147], loss=105.4885
	step [6/147], loss=100.6187
	step [7/147], loss=108.0989
	step [8/147], loss=102.5606
	step [9/147], loss=105.4972
	step [10/147], loss=89.9251
	step [11/147], loss=110.3024
	step [12/147], loss=95.6872
	step [13/147], loss=97.2032
	step [14/147], loss=95.1020
	step [15/147], loss=108.3518
	step [16/147], loss=107.5577
	step [17/147], loss=98.5353
	step [18/147], loss=95.5075
	step [19/147], loss=97.8357
	step [20/147], loss=86.9632
	step [21/147], loss=95.2101
	step [22/147], loss=106.4342
	step [23/147], loss=88.0120
	step [24/147], loss=93.5976
	step [25/147], loss=90.0272
	step [26/147], loss=84.0374
	step [27/147], loss=116.4739
	step [28/147], loss=93.7676
	step [29/147], loss=81.0573
	step [30/147], loss=108.9550
	step [31/147], loss=103.0891
	step [32/147], loss=95.2441
	step [33/147], loss=92.9431
	step [34/147], loss=114.0991
	step [35/147], loss=113.2599
	step [36/147], loss=102.0789
	step [37/147], loss=108.7574
	step [38/147], loss=119.5760
	step [39/147], loss=96.0387
	step [40/147], loss=94.3323
	step [41/147], loss=82.3996
	step [42/147], loss=121.8347
	step [43/147], loss=87.2669
	step [44/147], loss=100.2937
	step [45/147], loss=100.3837
	step [46/147], loss=98.8079
	step [47/147], loss=93.0498
	step [48/147], loss=88.8099
	step [49/147], loss=82.6138
	step [50/147], loss=104.7499
	step [51/147], loss=97.7890
	step [52/147], loss=117.6531
	step [53/147], loss=90.6807
	step [54/147], loss=99.4272
	step [55/147], loss=82.5743
	step [56/147], loss=110.5460
	step [57/147], loss=112.1359
	step [58/147], loss=104.8538
	step [59/147], loss=106.3154
	step [60/147], loss=126.2677
	step [61/147], loss=84.5465
	step [62/147], loss=85.9731
	step [63/147], loss=104.9877
	step [64/147], loss=92.0655
	step [65/147], loss=95.5491
	step [66/147], loss=114.8323
	step [67/147], loss=125.2395
	step [68/147], loss=108.6176
	step [69/147], loss=97.8424
	step [70/147], loss=93.2377
	step [71/147], loss=108.4315
	step [72/147], loss=88.7627
	step [73/147], loss=122.8343
	step [74/147], loss=105.6939
	step [75/147], loss=112.2086
	step [76/147], loss=111.1557
	step [77/147], loss=96.0448
	step [78/147], loss=122.1503
	step [79/147], loss=120.4278
	step [80/147], loss=105.5466
	step [81/147], loss=102.2261
	step [82/147], loss=111.3896
	step [83/147], loss=98.6428
	step [84/147], loss=101.1120
	step [85/147], loss=121.2200
	step [86/147], loss=112.8022
	step [87/147], loss=98.7364
	step [88/147], loss=105.4435
	step [89/147], loss=104.7799
	step [90/147], loss=112.5846
	step [91/147], loss=92.7189
	step [92/147], loss=110.1519
	step [93/147], loss=112.3140
	step [94/147], loss=108.8050
	step [95/147], loss=111.6733
	step [96/147], loss=81.8993
	step [97/147], loss=82.4381
	step [98/147], loss=88.7245
	step [99/147], loss=99.5810
	step [100/147], loss=91.7521
	step [101/147], loss=109.9639
	step [102/147], loss=117.0178
	step [103/147], loss=131.7038
	step [104/147], loss=96.0979
	step [105/147], loss=93.4511
	step [106/147], loss=105.4289
	step [107/147], loss=101.5414
	step [108/147], loss=99.5903
	step [109/147], loss=102.0211
	step [110/147], loss=124.8985
	step [111/147], loss=116.9672
	step [112/147], loss=104.7381
	step [113/147], loss=128.2197
	step [114/147], loss=108.7033
	step [115/147], loss=81.1045
	step [116/147], loss=101.2891
	step [117/147], loss=105.6438
	step [118/147], loss=122.9229
	step [119/147], loss=105.8857
	step [120/147], loss=92.0337
	step [121/147], loss=109.4061
	step [122/147], loss=100.7968
	step [123/147], loss=113.0080
	step [124/147], loss=83.7334
	step [125/147], loss=107.7689
	step [126/147], loss=88.1115
	step [127/147], loss=102.2148
	step [128/147], loss=95.6418
	step [129/147], loss=101.1599
	step [130/147], loss=123.5497
	step [131/147], loss=96.7012
	step [132/147], loss=103.7354
	step [133/147], loss=97.9643
	step [134/147], loss=119.6053
	step [135/147], loss=111.7887
	step [136/147], loss=104.7924
	step [137/147], loss=106.7256
	step [138/147], loss=94.8700
	step [139/147], loss=85.6806
	step [140/147], loss=119.2599
	step [141/147], loss=108.5993
	step [142/147], loss=97.2691
	step [143/147], loss=97.8226
	step [144/147], loss=108.9462
	step [145/147], loss=109.7375
	step [146/147], loss=96.9884
	step [147/147], loss=48.9168
	Evaluating
	loss=0.0170, precision=0.4011, recall=0.9122, f1=0.5572
Training epoch 26
	step [1/147], loss=101.2633
	step [2/147], loss=121.5492
	step [3/147], loss=99.5190
	step [4/147], loss=87.3808
	step [5/147], loss=107.4680
	step [6/147], loss=85.1926
	step [7/147], loss=83.5404
	step [8/147], loss=113.0525
	step [9/147], loss=138.1586
	step [10/147], loss=89.6246
	step [11/147], loss=120.6663
	step [12/147], loss=81.9517
	step [13/147], loss=100.2876
	step [14/147], loss=112.9945
	step [15/147], loss=111.9183
	step [16/147], loss=87.1364
	step [17/147], loss=103.0214
	step [18/147], loss=114.5153
	step [19/147], loss=89.7622
	step [20/147], loss=117.3616
	step [21/147], loss=101.1613
	step [22/147], loss=103.2567
	step [23/147], loss=101.5668
	step [24/147], loss=102.2633
	step [25/147], loss=103.6187
	step [26/147], loss=113.3892
	step [27/147], loss=100.6983
	step [28/147], loss=89.6864
	step [29/147], loss=87.4398
	step [30/147], loss=117.7656
	step [31/147], loss=91.2613
	step [32/147], loss=87.3051
	step [33/147], loss=98.0801
	step [34/147], loss=92.2486
	step [35/147], loss=120.6963
	step [36/147], loss=96.3091
	step [37/147], loss=83.5512
	step [38/147], loss=96.5747
	step [39/147], loss=107.9300
	step [40/147], loss=101.4633
	step [41/147], loss=110.5609
	step [42/147], loss=116.3485
	step [43/147], loss=112.1965
	step [44/147], loss=109.3999
	step [45/147], loss=103.4000
	step [46/147], loss=75.6202
	step [47/147], loss=109.4396
	step [48/147], loss=95.6227
	step [49/147], loss=104.8228
	step [50/147], loss=99.5172
	step [51/147], loss=79.6367
	step [52/147], loss=96.9869
	step [53/147], loss=89.8745
	step [54/147], loss=113.9200
	step [55/147], loss=101.1949
	step [56/147], loss=102.0733
	step [57/147], loss=105.0442
	step [58/147], loss=108.9143
	step [59/147], loss=87.2198
	step [60/147], loss=101.5689
	step [61/147], loss=84.2593
	step [62/147], loss=84.4747
	step [63/147], loss=90.8207
	step [64/147], loss=102.3344
	step [65/147], loss=111.8146
	step [66/147], loss=119.3074
	step [67/147], loss=108.6617
	step [68/147], loss=102.6224
	step [69/147], loss=100.3163
	step [70/147], loss=91.0765
	step [71/147], loss=82.0361
	step [72/147], loss=92.0576
	step [73/147], loss=100.1669
	step [74/147], loss=83.6212
	step [75/147], loss=116.0542
	step [76/147], loss=105.4653
	step [77/147], loss=107.4981
	step [78/147], loss=87.2430
	step [79/147], loss=107.4659
	step [80/147], loss=91.2022
	step [81/147], loss=96.9803
	step [82/147], loss=111.2731
	step [83/147], loss=95.4137
	step [84/147], loss=103.2562
	step [85/147], loss=101.9242
	step [86/147], loss=127.4820
	step [87/147], loss=102.6628
	step [88/147], loss=111.3171
	step [89/147], loss=89.3699
	step [90/147], loss=98.0930
	step [91/147], loss=99.0652
	step [92/147], loss=103.2934
	step [93/147], loss=113.1792
	step [94/147], loss=93.4057
	step [95/147], loss=96.8283
	step [96/147], loss=100.9454
	step [97/147], loss=105.4720
	step [98/147], loss=102.1875
	step [99/147], loss=94.3619
	step [100/147], loss=105.9103
	step [101/147], loss=102.6451
	step [102/147], loss=96.6306
	step [103/147], loss=107.8102
	step [104/147], loss=100.8283
	step [105/147], loss=105.9701
	step [106/147], loss=104.9207
	step [107/147], loss=118.9545
	step [108/147], loss=115.7362
	step [109/147], loss=80.9929
	step [110/147], loss=99.5189
	step [111/147], loss=92.9145
	step [112/147], loss=91.9570
	step [113/147], loss=118.4780
	step [114/147], loss=92.9554
	step [115/147], loss=123.2070
	step [116/147], loss=103.4273
	step [117/147], loss=106.5155
	step [118/147], loss=95.3190
	step [119/147], loss=97.4523
	step [120/147], loss=117.2009
	step [121/147], loss=110.0580
	step [122/147], loss=102.1015
	step [123/147], loss=97.5780
	step [124/147], loss=121.3372
	step [125/147], loss=100.2380
	step [126/147], loss=90.7058
	step [127/147], loss=125.0971
	step [128/147], loss=113.9923
	step [129/147], loss=98.6214
	step [130/147], loss=95.9657
	step [131/147], loss=89.4821
	step [132/147], loss=99.3874
	step [133/147], loss=103.6589
	step [134/147], loss=100.8949
	step [135/147], loss=98.7877
	step [136/147], loss=100.6603
	step [137/147], loss=90.1648
	step [138/147], loss=117.0673
	step [139/147], loss=109.1983
	step [140/147], loss=107.7309
	step [141/147], loss=90.6022
	step [142/147], loss=104.2871
	step [143/147], loss=104.7784
	step [144/147], loss=102.7384
	step [145/147], loss=114.8621
	step [146/147], loss=106.1783
	step [147/147], loss=49.9024
	Evaluating
	loss=0.0196, precision=0.3223, recall=0.9422, f1=0.4803
Training epoch 27
	step [1/147], loss=85.6600
	step [2/147], loss=107.7886
	step [3/147], loss=119.2406
	step [4/147], loss=96.0538
	step [5/147], loss=111.9206
	step [6/147], loss=111.6080
	step [7/147], loss=112.3610
	step [8/147], loss=116.0063
	step [9/147], loss=119.7434
	step [10/147], loss=83.7472
	step [11/147], loss=102.5112
	step [12/147], loss=111.3070
	step [13/147], loss=89.2329
	step [14/147], loss=90.8154
	step [15/147], loss=88.5720
	step [16/147], loss=88.0081
	step [17/147], loss=101.9456
	step [18/147], loss=114.9492
	step [19/147], loss=85.0640
	step [20/147], loss=99.7866
	step [21/147], loss=102.9653
	step [22/147], loss=97.5304
	step [23/147], loss=94.0191
	step [24/147], loss=90.9928
	step [25/147], loss=112.2290
	step [26/147], loss=101.1838
	step [27/147], loss=103.1819
	step [28/147], loss=106.6454
	step [29/147], loss=97.5862
	step [30/147], loss=101.2215
	step [31/147], loss=89.4265
	step [32/147], loss=105.7558
	step [33/147], loss=88.2713
	step [34/147], loss=96.7078
	step [35/147], loss=100.4137
	step [36/147], loss=113.4648
	step [37/147], loss=101.1057
	step [38/147], loss=91.5659
	step [39/147], loss=96.2786
	step [40/147], loss=103.0465
	step [41/147], loss=114.7855
	step [42/147], loss=112.0134
	step [43/147], loss=107.0984
	step [44/147], loss=94.2237
	step [45/147], loss=91.8534
	step [46/147], loss=103.7675
	step [47/147], loss=109.9167
	step [48/147], loss=110.5983
	step [49/147], loss=95.1420
	step [50/147], loss=97.4294
	step [51/147], loss=110.7947
	step [52/147], loss=98.2686
	step [53/147], loss=104.2829
	step [54/147], loss=99.5408
	step [55/147], loss=93.9607
	step [56/147], loss=116.0596
	step [57/147], loss=86.4896
	step [58/147], loss=115.6314
	step [59/147], loss=102.3606
	step [60/147], loss=106.8853
	step [61/147], loss=126.0655
	step [62/147], loss=96.2847
	step [63/147], loss=104.9003
	step [64/147], loss=115.0572
	step [65/147], loss=98.2442
	step [66/147], loss=80.3960
	step [67/147], loss=102.6802
	step [68/147], loss=81.7374
	step [69/147], loss=106.4403
	step [70/147], loss=104.0076
	step [71/147], loss=94.4098
	step [72/147], loss=101.6983
	step [73/147], loss=98.0859
	step [74/147], loss=93.1659
	step [75/147], loss=90.7396
	step [76/147], loss=90.7054
	step [77/147], loss=105.8222
	step [78/147], loss=107.7634
	step [79/147], loss=125.1004
	step [80/147], loss=111.6050
	step [81/147], loss=92.9796
	step [82/147], loss=103.3785
	step [83/147], loss=95.6832
	step [84/147], loss=101.2142
	step [85/147], loss=94.3790
	step [86/147], loss=105.6515
	step [87/147], loss=94.2381
	step [88/147], loss=108.0611
	step [89/147], loss=98.4880
	step [90/147], loss=114.9083
	step [91/147], loss=100.3669
	step [92/147], loss=103.9425
	step [93/147], loss=83.2643
	step [94/147], loss=88.7624
	step [95/147], loss=122.6324
	step [96/147], loss=93.8557
	step [97/147], loss=99.0130
	step [98/147], loss=108.9808
	step [99/147], loss=104.1682
	step [100/147], loss=100.2363
	step [101/147], loss=83.0023
	step [102/147], loss=92.2185
	step [103/147], loss=88.1466
	step [104/147], loss=112.2917
	step [105/147], loss=91.3942
	step [106/147], loss=91.5455
	step [107/147], loss=92.0514
	step [108/147], loss=97.4734
	step [109/147], loss=94.4946
	step [110/147], loss=97.9945
	step [111/147], loss=98.1056
	step [112/147], loss=110.2995
	step [113/147], loss=106.1529
	step [114/147], loss=113.2990
	step [115/147], loss=91.7654
	step [116/147], loss=89.9769
	step [117/147], loss=114.2373
	step [118/147], loss=93.7631
	step [119/147], loss=106.7725
	step [120/147], loss=101.4177
	step [121/147], loss=122.5635
	step [122/147], loss=103.2634
	step [123/147], loss=95.9827
	step [124/147], loss=103.0766
	step [125/147], loss=90.5436
	step [126/147], loss=121.5978
	step [127/147], loss=118.6780
	step [128/147], loss=106.7311
	step [129/147], loss=109.9159
	step [130/147], loss=116.2114
	step [131/147], loss=100.5573
	step [132/147], loss=83.1308
	step [133/147], loss=94.3664
	step [134/147], loss=105.1486
	step [135/147], loss=95.0352
	step [136/147], loss=106.4640
	step [137/147], loss=88.5154
	step [138/147], loss=98.2724
	step [139/147], loss=110.9528
	step [140/147], loss=95.2149
	step [141/147], loss=94.7589
	step [142/147], loss=97.8674
	step [143/147], loss=115.9494
	step [144/147], loss=106.6666
	step [145/147], loss=90.7503
	step [146/147], loss=99.7519
	step [147/147], loss=42.2719
	Evaluating
	loss=0.0171, precision=0.3627, recall=0.9246, f1=0.5210
Training epoch 28
	step [1/147], loss=110.5816
	step [2/147], loss=103.8436
	step [3/147], loss=95.7455
	step [4/147], loss=103.2114
	step [5/147], loss=89.4850
	step [6/147], loss=104.9030
	step [7/147], loss=96.6541
	step [8/147], loss=100.7151
	step [9/147], loss=95.6237
	step [10/147], loss=121.5318
	step [11/147], loss=105.4337
	step [12/147], loss=82.5662
	step [13/147], loss=106.7625
	step [14/147], loss=88.7918
	step [15/147], loss=95.5100
	step [16/147], loss=100.9135
	step [17/147], loss=100.7537
	step [18/147], loss=102.9288
	step [19/147], loss=94.4828
	step [20/147], loss=105.1370
	step [21/147], loss=103.8290
	step [22/147], loss=97.2301
	step [23/147], loss=87.1101
	step [24/147], loss=99.7941
	step [25/147], loss=101.4968
	step [26/147], loss=101.0016
	step [27/147], loss=93.4565
	step [28/147], loss=96.5197
	step [29/147], loss=106.4160
	step [30/147], loss=112.3576
	step [31/147], loss=89.5860
	step [32/147], loss=86.1080
	step [33/147], loss=101.9443
	step [34/147], loss=88.5171
	step [35/147], loss=91.1412
	step [36/147], loss=100.3150
	step [37/147], loss=109.2532
	step [38/147], loss=96.9428
	step [39/147], loss=94.2599
	step [40/147], loss=101.6889
	step [41/147], loss=106.6952
	step [42/147], loss=106.6537
	step [43/147], loss=121.3423
	step [44/147], loss=96.9202
	step [45/147], loss=109.5312
	step [46/147], loss=115.2709
	step [47/147], loss=94.7279
	step [48/147], loss=103.6505
	step [49/147], loss=89.1831
	step [50/147], loss=99.2918
	step [51/147], loss=80.3432
	step [52/147], loss=103.1273
	step [53/147], loss=91.9854
	step [54/147], loss=109.6393
	step [55/147], loss=93.1066
	step [56/147], loss=82.9319
	step [57/147], loss=86.9420
	step [58/147], loss=113.7043
	step [59/147], loss=87.0262
	step [60/147], loss=119.8740
	step [61/147], loss=102.3573
	step [62/147], loss=97.2254
	step [63/147], loss=96.9025
	step [64/147], loss=93.2966
	step [65/147], loss=131.0752
	step [66/147], loss=93.4387
	step [67/147], loss=101.3742
	step [68/147], loss=100.1931
	step [69/147], loss=90.3056
	step [70/147], loss=85.8533
	step [71/147], loss=122.6142
	step [72/147], loss=74.7028
	step [73/147], loss=74.3088
	step [74/147], loss=102.8885
	step [75/147], loss=105.1308
	step [76/147], loss=91.7200
	step [77/147], loss=105.8979
	step [78/147], loss=102.2268
	step [79/147], loss=106.4215
	step [80/147], loss=112.0842
	step [81/147], loss=118.1315
	step [82/147], loss=102.9898
	step [83/147], loss=102.3457
	step [84/147], loss=90.2211
	step [85/147], loss=97.0707
	step [86/147], loss=106.1483
	step [87/147], loss=108.1636
	step [88/147], loss=97.3362
	step [89/147], loss=101.4638
	step [90/147], loss=98.1125
	step [91/147], loss=98.7112
	step [92/147], loss=111.2716
	step [93/147], loss=106.8045
	step [94/147], loss=84.2267
	step [95/147], loss=98.9152
	step [96/147], loss=100.9185
	step [97/147], loss=110.6160
	step [98/147], loss=108.4228
	step [99/147], loss=96.6583
	step [100/147], loss=88.6866
	step [101/147], loss=113.9043
	step [102/147], loss=115.3233
	step [103/147], loss=98.0780
	step [104/147], loss=104.0517
	step [105/147], loss=99.3030
	step [106/147], loss=105.6266
	step [107/147], loss=109.5748
	step [108/147], loss=98.2998
	step [109/147], loss=99.6362
	step [110/147], loss=95.5840
	step [111/147], loss=106.1764
	step [112/147], loss=98.4277
	step [113/147], loss=96.3312
	step [114/147], loss=92.4363
	step [115/147], loss=92.4151
	step [116/147], loss=99.8258
	step [117/147], loss=101.5433
	step [118/147], loss=110.8513
	step [119/147], loss=97.4040
	step [120/147], loss=103.3305
	step [121/147], loss=91.9948
	step [122/147], loss=87.4253
	step [123/147], loss=102.9399
	step [124/147], loss=101.2685
	step [125/147], loss=113.9204
	step [126/147], loss=122.4375
	step [127/147], loss=94.8836
	step [128/147], loss=112.7833
	step [129/147], loss=99.2819
	step [130/147], loss=82.6892
	step [131/147], loss=93.7777
	step [132/147], loss=86.7395
	step [133/147], loss=110.2064
	step [134/147], loss=84.7374
	step [135/147], loss=105.5524
	step [136/147], loss=118.6132
	step [137/147], loss=116.3886
	step [138/147], loss=116.1152
	step [139/147], loss=89.7940
	step [140/147], loss=95.2061
	step [141/147], loss=94.6250
	step [142/147], loss=90.3073
	step [143/147], loss=87.9817
	step [144/147], loss=95.4240
	step [145/147], loss=101.9476
	step [146/147], loss=94.6850
	step [147/147], loss=29.6827
	Evaluating
	loss=0.0133, precision=0.4378, recall=0.9134, f1=0.5919
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/147], loss=94.2787
	step [2/147], loss=106.3998
	step [3/147], loss=108.1352
	step [4/147], loss=100.1193
	step [5/147], loss=101.7796
	step [6/147], loss=119.7932
	step [7/147], loss=89.1010
	step [8/147], loss=87.8302
	step [9/147], loss=96.0877
	step [10/147], loss=100.8293
	step [11/147], loss=86.4689
	step [12/147], loss=91.3289
	step [13/147], loss=98.7621
	step [14/147], loss=103.5417
	step [15/147], loss=119.2053
	step [16/147], loss=80.0198
	step [17/147], loss=108.9448
	step [18/147], loss=99.0649
	step [19/147], loss=112.8658
	step [20/147], loss=117.5977
	step [21/147], loss=90.8399
	step [22/147], loss=102.3011
	step [23/147], loss=100.7952
	step [24/147], loss=86.8998
	step [25/147], loss=103.2307
	step [26/147], loss=97.6879
	step [27/147], loss=94.7856
	step [28/147], loss=89.2556
	step [29/147], loss=99.1067
	step [30/147], loss=99.1826
	step [31/147], loss=92.0554
	step [32/147], loss=103.7241
	step [33/147], loss=119.2988
	step [34/147], loss=105.4564
	step [35/147], loss=103.1259
	step [36/147], loss=90.9697
	step [37/147], loss=102.4958
	step [38/147], loss=105.4905
	step [39/147], loss=109.1808
	step [40/147], loss=103.1703
	step [41/147], loss=92.9097
	step [42/147], loss=104.5890
	step [43/147], loss=110.0705
	step [44/147], loss=94.1616
	step [45/147], loss=94.5770
	step [46/147], loss=106.0969
	step [47/147], loss=95.8086
	step [48/147], loss=83.0999
	step [49/147], loss=107.2637
	step [50/147], loss=92.5807
	step [51/147], loss=110.7361
	step [52/147], loss=105.1254
	step [53/147], loss=97.5611
	step [54/147], loss=120.7229
	step [55/147], loss=100.7181
	step [56/147], loss=99.1170
	step [57/147], loss=90.3001
	step [58/147], loss=72.6467
	step [59/147], loss=89.9061
	step [60/147], loss=97.6652
	step [61/147], loss=117.4848
	step [62/147], loss=86.0372
	step [63/147], loss=94.7406
	step [64/147], loss=105.4834
	step [65/147], loss=114.8174
	step [66/147], loss=124.2408
	step [67/147], loss=99.2232
	step [68/147], loss=95.0405
	step [69/147], loss=95.1017
	step [70/147], loss=93.0749
	step [71/147], loss=88.6024
	step [72/147], loss=78.3795
	step [73/147], loss=118.3105
	step [74/147], loss=110.4365
	step [75/147], loss=94.1650
	step [76/147], loss=114.8873
	step [77/147], loss=92.2576
	step [78/147], loss=95.9895
	step [79/147], loss=127.6957
	step [80/147], loss=89.7884
	step [81/147], loss=91.4322
	step [82/147], loss=89.5931
	step [83/147], loss=96.4223
	step [84/147], loss=84.3430
	step [85/147], loss=116.6777
	step [86/147], loss=102.2626
	step [87/147], loss=113.6125
	step [88/147], loss=82.7162
	step [89/147], loss=109.3052
	step [90/147], loss=105.3642
	step [91/147], loss=73.5777
	step [92/147], loss=94.9891
	step [93/147], loss=81.1067
	step [94/147], loss=104.5078
	step [95/147], loss=85.7781
	step [96/147], loss=86.5875
	step [97/147], loss=103.7009
	step [98/147], loss=106.3662
	step [99/147], loss=101.0702
	step [100/147], loss=103.2411
	step [101/147], loss=105.9757
	step [102/147], loss=111.9011
	step [103/147], loss=71.1216
	step [104/147], loss=107.9431
	step [105/147], loss=89.1678
	step [106/147], loss=93.8892
	step [107/147], loss=105.5100
	step [108/147], loss=102.6562
	step [109/147], loss=96.1583
	step [110/147], loss=93.2089
	step [111/147], loss=101.1607
	step [112/147], loss=91.9269
	step [113/147], loss=102.0259
	step [114/147], loss=110.4411
	step [115/147], loss=93.6338
	step [116/147], loss=91.6071
	step [117/147], loss=115.5940
	step [118/147], loss=76.2472
	step [119/147], loss=109.7775
	step [120/147], loss=89.4325
	step [121/147], loss=91.3068
	step [122/147], loss=89.9306
	step [123/147], loss=102.2838
	step [124/147], loss=85.9946
	step [125/147], loss=98.4005
	step [126/147], loss=109.2773
	step [127/147], loss=89.1357
	step [128/147], loss=113.2397
	step [129/147], loss=105.7023
	step [130/147], loss=82.7398
	step [131/147], loss=115.3082
	step [132/147], loss=118.7504
	step [133/147], loss=109.2140
	step [134/147], loss=81.2102
	step [135/147], loss=120.9062
	step [136/147], loss=92.3565
	step [137/147], loss=113.8344
	step [138/147], loss=86.7419
	step [139/147], loss=93.8824
	step [140/147], loss=98.5300
	step [141/147], loss=118.6809
	step [142/147], loss=86.3469
	step [143/147], loss=108.7352
	step [144/147], loss=85.5549
	step [145/147], loss=108.3659
	step [146/147], loss=114.9854
	step [147/147], loss=41.4874
	Evaluating
	loss=0.0146, precision=0.3975, recall=0.8883, f1=0.5493
Training epoch 30
	step [1/147], loss=88.7288
	step [2/147], loss=104.0192
	step [3/147], loss=102.6959
	step [4/147], loss=108.8846
	step [5/147], loss=99.3985
	step [6/147], loss=84.5258
	step [7/147], loss=94.8211
	step [8/147], loss=95.2770
	step [9/147], loss=96.6525
	step [10/147], loss=94.3470
	step [11/147], loss=93.2322
	step [12/147], loss=87.4876
	step [13/147], loss=117.0514
	step [14/147], loss=102.3926
	step [15/147], loss=128.2454
	step [16/147], loss=109.5654
	step [17/147], loss=101.1120
	step [18/147], loss=84.6100
	step [19/147], loss=102.5647
	step [20/147], loss=94.8575
	step [21/147], loss=93.4552
	step [22/147], loss=90.1179
	step [23/147], loss=79.2518
	step [24/147], loss=94.2613
	step [25/147], loss=89.6518
	step [26/147], loss=99.1717
	step [27/147], loss=88.6303
	step [28/147], loss=96.7411
	step [29/147], loss=103.3141
	step [30/147], loss=106.1127
	step [31/147], loss=107.6620
	step [32/147], loss=97.8840
	step [33/147], loss=102.7944
	step [34/147], loss=117.5804
	step [35/147], loss=96.0092
	step [36/147], loss=99.7540
	step [37/147], loss=81.3255
	step [38/147], loss=109.5822
	step [39/147], loss=109.5716
	step [40/147], loss=96.5976
	step [41/147], loss=103.2868
	step [42/147], loss=109.1226
	step [43/147], loss=110.9850
	step [44/147], loss=106.1973
	step [45/147], loss=75.4515
	step [46/147], loss=111.7357
	step [47/147], loss=102.2461
	step [48/147], loss=111.7904
	step [49/147], loss=106.5448
	step [50/147], loss=75.8980
	step [51/147], loss=105.7050
	step [52/147], loss=103.8360
	step [53/147], loss=118.4548
	step [54/147], loss=86.5040
	step [55/147], loss=91.1230
	step [56/147], loss=105.7137
	step [57/147], loss=89.9635
	step [58/147], loss=99.4707
	step [59/147], loss=99.9542
	step [60/147], loss=84.3246
	step [61/147], loss=100.7547
	step [62/147], loss=99.7935
	step [63/147], loss=100.3434
	step [64/147], loss=92.7224
	step [65/147], loss=129.2502
	step [66/147], loss=96.5297
	step [67/147], loss=87.1875
	step [68/147], loss=89.0565
	step [69/147], loss=100.6307
	step [70/147], loss=97.6436
	step [71/147], loss=105.5587
	step [72/147], loss=95.6451
	step [73/147], loss=107.7049
	step [74/147], loss=101.4260
	step [75/147], loss=94.2759
	step [76/147], loss=96.3623
	step [77/147], loss=87.0855
	step [78/147], loss=106.9671
	step [79/147], loss=110.2543
	step [80/147], loss=91.4642
	step [81/147], loss=105.3579
	step [82/147], loss=100.1363
	step [83/147], loss=96.3763
	step [84/147], loss=77.2554
	step [85/147], loss=107.0737
	step [86/147], loss=100.0109
	step [87/147], loss=106.5840
	step [88/147], loss=104.7230
	step [89/147], loss=102.2007
	step [90/147], loss=110.3257
	step [91/147], loss=109.8516
	step [92/147], loss=82.5899
	step [93/147], loss=86.2003
	step [94/147], loss=92.1310
	step [95/147], loss=99.1545
	step [96/147], loss=95.8369
	step [97/147], loss=105.6433
	step [98/147], loss=89.8929
	step [99/147], loss=90.4605
	step [100/147], loss=116.3222
	step [101/147], loss=115.1512
	step [102/147], loss=86.3974
	step [103/147], loss=88.6124
	step [104/147], loss=109.8974
	step [105/147], loss=101.2834
	step [106/147], loss=105.3101
	step [107/147], loss=92.0943
	step [108/147], loss=87.7191
	step [109/147], loss=71.4146
	step [110/147], loss=108.6989
	step [111/147], loss=105.1034
	step [112/147], loss=96.1158
	step [113/147], loss=95.9643
	step [114/147], loss=90.3324
	step [115/147], loss=91.2681
	step [116/147], loss=107.2867
	step [117/147], loss=94.9000
	step [118/147], loss=91.6351
	step [119/147], loss=101.1361
	step [120/147], loss=93.0963
	step [121/147], loss=116.0891
	step [122/147], loss=100.0911
	step [123/147], loss=111.2226
	step [124/147], loss=108.6338
	step [125/147], loss=91.4830
	step [126/147], loss=92.5629
	step [127/147], loss=92.1360
	step [128/147], loss=104.9780
	step [129/147], loss=102.8078
	step [130/147], loss=90.1438
	step [131/147], loss=102.8023
	step [132/147], loss=100.0410
	step [133/147], loss=100.6441
	step [134/147], loss=86.9028
	step [135/147], loss=103.6630
	step [136/147], loss=93.9612
	step [137/147], loss=89.8842
	step [138/147], loss=90.1165
	step [139/147], loss=81.2461
	step [140/147], loss=96.4304
	step [141/147], loss=90.8470
	step [142/147], loss=96.4651
	step [143/147], loss=107.5182
	step [144/147], loss=96.9210
	step [145/147], loss=105.4331
	step [146/147], loss=84.0504
	step [147/147], loss=39.9287
	Evaluating
	loss=0.0181, precision=0.2921, recall=0.9110, f1=0.4424
Training epoch 31
	step [1/147], loss=95.1877
	step [2/147], loss=109.1972
	step [3/147], loss=95.3023
	step [4/147], loss=101.4215
	step [5/147], loss=87.3365
	step [6/147], loss=85.8084
	step [7/147], loss=89.2004
	step [8/147], loss=103.3183
	step [9/147], loss=106.3501
	step [10/147], loss=103.2612
	step [11/147], loss=113.8966
	step [12/147], loss=112.3518
	step [13/147], loss=93.7060
	step [14/147], loss=88.8562
	step [15/147], loss=99.1876
	step [16/147], loss=90.3706
	step [17/147], loss=102.0492
	step [18/147], loss=98.2796
	step [19/147], loss=95.9437
	step [20/147], loss=87.1671
	step [21/147], loss=85.1820
	step [22/147], loss=93.1038
	step [23/147], loss=113.0748
	step [24/147], loss=106.9691
	step [25/147], loss=96.4231
	step [26/147], loss=108.4456
	step [27/147], loss=90.5388
	step [28/147], loss=116.5306
	step [29/147], loss=108.4203
	step [30/147], loss=96.2180
	step [31/147], loss=90.6873
	step [32/147], loss=97.2981
	step [33/147], loss=96.1045
	step [34/147], loss=101.1324
	step [35/147], loss=93.4862
	step [36/147], loss=99.1943
	step [37/147], loss=110.9541
	step [38/147], loss=100.2296
	step [39/147], loss=104.6148
	step [40/147], loss=96.3554
	step [41/147], loss=87.4553
	step [42/147], loss=94.3384
	step [43/147], loss=109.8115
	step [44/147], loss=91.2946
	step [45/147], loss=89.1572
	step [46/147], loss=89.5307
	step [47/147], loss=95.8092
	step [48/147], loss=110.0569
	step [49/147], loss=106.0684
	step [50/147], loss=94.6594
	step [51/147], loss=99.5022
	step [52/147], loss=98.6212
	step [53/147], loss=94.4275
	step [54/147], loss=84.9013
	step [55/147], loss=99.6318
	step [56/147], loss=103.2884
	step [57/147], loss=81.2931
	step [58/147], loss=94.3860
	step [59/147], loss=110.6597
	step [60/147], loss=86.8875
	step [61/147], loss=70.9893
	step [62/147], loss=86.5611
	step [63/147], loss=75.7785
	step [64/147], loss=96.6657
	step [65/147], loss=93.2008
	step [66/147], loss=95.8196
	step [67/147], loss=103.2868
	step [68/147], loss=93.8486
	step [69/147], loss=97.1903
	step [70/147], loss=96.5994
	step [71/147], loss=102.2053
	step [72/147], loss=103.7850
	step [73/147], loss=80.6477
	step [74/147], loss=97.9065
	step [75/147], loss=86.2613
	step [76/147], loss=81.0943
	step [77/147], loss=113.1119
	step [78/147], loss=111.6024
	step [79/147], loss=91.7499
	step [80/147], loss=95.5361
	step [81/147], loss=92.5647
	step [82/147], loss=100.6791
	step [83/147], loss=89.4553
	step [84/147], loss=108.8102
	step [85/147], loss=76.9491
	step [86/147], loss=112.7169
	step [87/147], loss=87.6328
	step [88/147], loss=118.9003
	step [89/147], loss=106.3757
	step [90/147], loss=104.2752
	step [91/147], loss=93.1867
	step [92/147], loss=99.2484
	step [93/147], loss=110.1407
	step [94/147], loss=102.0160
	step [95/147], loss=97.2988
	step [96/147], loss=76.8014
	step [97/147], loss=94.8799
	step [98/147], loss=103.3568
	step [99/147], loss=87.6466
	step [100/147], loss=121.6513
	step [101/147], loss=90.6481
	step [102/147], loss=105.7558
	step [103/147], loss=91.9127
	step [104/147], loss=97.4099
	step [105/147], loss=86.7025
	step [106/147], loss=97.7014
	step [107/147], loss=101.1755
	step [108/147], loss=104.0699
	step [109/147], loss=84.2734
	step [110/147], loss=101.2911
	step [111/147], loss=89.8961
	step [112/147], loss=96.7592
	step [113/147], loss=87.9616
	step [114/147], loss=92.7738
	step [115/147], loss=101.6812
	step [116/147], loss=102.2381
	step [117/147], loss=94.8252
	step [118/147], loss=85.0130
	step [119/147], loss=101.9837
	step [120/147], loss=94.8215
	step [121/147], loss=102.9537
	step [122/147], loss=106.7671
	step [123/147], loss=110.6735
	step [124/147], loss=107.9873
	step [125/147], loss=93.8796
	step [126/147], loss=113.3205
	step [127/147], loss=87.8320
	step [128/147], loss=87.6148
	step [129/147], loss=106.7620
	step [130/147], loss=113.5410
	step [131/147], loss=107.9901
	step [132/147], loss=104.2747
	step [133/147], loss=93.5355
	step [134/147], loss=89.4432
	step [135/147], loss=93.1377
	step [136/147], loss=101.4486
	step [137/147], loss=93.8442
	step [138/147], loss=111.9659
	step [139/147], loss=104.5468
	step [140/147], loss=106.7798
	step [141/147], loss=91.7863
	step [142/147], loss=102.9278
	step [143/147], loss=95.8793
	step [144/147], loss=104.7615
	step [145/147], loss=93.6211
	step [146/147], loss=106.8083
	step [147/147], loss=40.1620
	Evaluating
	loss=0.0143, precision=0.3546, recall=0.9127, f1=0.5107
Training epoch 32
	step [1/147], loss=103.8892
	step [2/147], loss=91.7332
	step [3/147], loss=77.2534
	step [4/147], loss=105.8518
	step [5/147], loss=107.2684
	step [6/147], loss=63.8936
	step [7/147], loss=95.0500
	step [8/147], loss=114.5814
	step [9/147], loss=101.6778
	step [10/147], loss=90.9518
	step [11/147], loss=85.1915
	step [12/147], loss=101.2040
	step [13/147], loss=94.6088
	step [14/147], loss=103.4086
	step [15/147], loss=117.1862
	step [16/147], loss=84.0667
	step [17/147], loss=114.8311
	step [18/147], loss=85.6805
	step [19/147], loss=97.2054
	step [20/147], loss=86.0404
	step [21/147], loss=90.8717
	step [22/147], loss=111.8786
	step [23/147], loss=95.1653
	step [24/147], loss=91.0954
	step [25/147], loss=88.1233
	step [26/147], loss=88.9456
	step [27/147], loss=93.7561
	step [28/147], loss=87.8964
	step [29/147], loss=103.5889
	step [30/147], loss=107.6991
	step [31/147], loss=96.9615
	step [32/147], loss=94.3842
	step [33/147], loss=92.0740
	step [34/147], loss=98.5052
	step [35/147], loss=79.2085
	step [36/147], loss=105.0648
	step [37/147], loss=87.4995
	step [38/147], loss=115.3233
	step [39/147], loss=94.3298
	step [40/147], loss=102.5748
	step [41/147], loss=104.0531
	step [42/147], loss=94.6717
	step [43/147], loss=104.0243
	step [44/147], loss=70.0953
	step [45/147], loss=103.5117
	step [46/147], loss=104.1562
	step [47/147], loss=98.1681
	step [48/147], loss=117.4457
	step [49/147], loss=87.5724
	step [50/147], loss=114.6426
	step [51/147], loss=122.0858
	step [52/147], loss=104.1719
	step [53/147], loss=107.7717
	step [54/147], loss=95.5899
	step [55/147], loss=93.8330
	step [56/147], loss=92.4522
	step [57/147], loss=89.5910
	step [58/147], loss=104.7256
	step [59/147], loss=88.0155
	step [60/147], loss=78.4495
	step [61/147], loss=93.7307
	step [62/147], loss=89.4528
	step [63/147], loss=85.7839
	step [64/147], loss=91.9469
	step [65/147], loss=103.3348
	step [66/147], loss=112.5613
	step [67/147], loss=104.4789
	step [68/147], loss=111.5749
	step [69/147], loss=94.9210
	step [70/147], loss=100.1514
	step [71/147], loss=109.0967
	step [72/147], loss=95.1559
	step [73/147], loss=93.5518
	step [74/147], loss=94.4612
	step [75/147], loss=108.8615
	step [76/147], loss=93.6415
	step [77/147], loss=100.5899
	step [78/147], loss=97.7517
	step [79/147], loss=96.3122
	step [80/147], loss=85.6512
	step [81/147], loss=88.5062
	step [82/147], loss=97.3196
	step [83/147], loss=93.2842
	step [84/147], loss=95.9967
	step [85/147], loss=87.6166
	step [86/147], loss=93.9543
	step [87/147], loss=93.5395
	step [88/147], loss=100.4252
	step [89/147], loss=91.8033
	step [90/147], loss=121.8706
	step [91/147], loss=100.1902
	step [92/147], loss=112.0992
	step [93/147], loss=100.7754
	step [94/147], loss=94.1575
	step [95/147], loss=97.3703
	step [96/147], loss=100.0292
	step [97/147], loss=97.0329
	step [98/147], loss=89.0779
	step [99/147], loss=100.6039
	step [100/147], loss=93.0345
	step [101/147], loss=96.9715
	step [102/147], loss=118.8883
	step [103/147], loss=89.1800
	step [104/147], loss=94.2640
	step [105/147], loss=109.1126
	step [106/147], loss=103.4472
	step [107/147], loss=93.1261
	step [108/147], loss=84.5419
	step [109/147], loss=86.9377
	step [110/147], loss=78.0140
	step [111/147], loss=96.1149
	step [112/147], loss=76.5600
	step [113/147], loss=92.9102
	step [114/147], loss=106.3058
	step [115/147], loss=111.0842
	step [116/147], loss=84.2921
	step [117/147], loss=106.6613
	step [118/147], loss=88.2046
	step [119/147], loss=98.1712
	step [120/147], loss=112.1562
	step [121/147], loss=104.5294
	step [122/147], loss=88.8169
	step [123/147], loss=101.1883
	step [124/147], loss=127.0798
	step [125/147], loss=109.0694
	step [126/147], loss=91.1797
	step [127/147], loss=98.2945
	step [128/147], loss=111.0396
	step [129/147], loss=101.7353
	step [130/147], loss=86.3866
	step [131/147], loss=76.2280
	step [132/147], loss=103.2396
	step [133/147], loss=78.0183
	step [134/147], loss=106.2236
	step [135/147], loss=115.0575
	step [136/147], loss=93.6993
	step [137/147], loss=104.0317
	step [138/147], loss=85.4241
	step [139/147], loss=94.9109
	step [140/147], loss=106.7626
	step [141/147], loss=92.3356
	step [142/147], loss=79.5107
	step [143/147], loss=120.1715
	step [144/147], loss=97.4532
	step [145/147], loss=106.3182
	step [146/147], loss=87.9232
	step [147/147], loss=35.0843
	Evaluating
	loss=0.0139, precision=0.3710, recall=0.8908, f1=0.5238
Training epoch 33
	step [1/147], loss=90.0871
	step [2/147], loss=117.2738
	step [3/147], loss=92.5453
	step [4/147], loss=96.9455
	step [5/147], loss=86.6752
	step [6/147], loss=86.8864
	step [7/147], loss=76.0987
	step [8/147], loss=96.2509
	step [9/147], loss=89.3429
	step [10/147], loss=81.7954
	step [11/147], loss=90.5856
	step [12/147], loss=102.6256
	step [13/147], loss=93.9722
	step [14/147], loss=85.1097
	step [15/147], loss=92.5391
	step [16/147], loss=87.0200
	step [17/147], loss=109.7990
	step [18/147], loss=91.7665
	step [19/147], loss=96.2357
	step [20/147], loss=92.0774
	step [21/147], loss=91.4013
	step [22/147], loss=89.7114
	step [23/147], loss=98.3972
	step [24/147], loss=100.1347
	step [25/147], loss=98.6795
	step [26/147], loss=89.4666
	step [27/147], loss=105.9776
	step [28/147], loss=93.8695
	step [29/147], loss=127.8804
	step [30/147], loss=99.1439
	step [31/147], loss=107.8073
	step [32/147], loss=89.9055
	step [33/147], loss=109.7047
	step [34/147], loss=119.2067
	step [35/147], loss=103.6804
	step [36/147], loss=101.8423
	step [37/147], loss=100.2630
	step [38/147], loss=84.3236
	step [39/147], loss=88.0350
	step [40/147], loss=78.8059
	step [41/147], loss=101.7324
	step [42/147], loss=97.5251
	step [43/147], loss=100.2360
	step [44/147], loss=90.9187
	step [45/147], loss=101.4425
	step [46/147], loss=79.4694
	step [47/147], loss=89.4609
	step [48/147], loss=98.7278
	step [49/147], loss=97.8886
	step [50/147], loss=98.3224
	step [51/147], loss=93.9756
	step [52/147], loss=88.4443
	step [53/147], loss=98.2449
	step [54/147], loss=105.3695
	step [55/147], loss=113.1774
	step [56/147], loss=100.5269
	step [57/147], loss=86.1401
	step [58/147], loss=95.1261
	step [59/147], loss=86.2110
	step [60/147], loss=90.7233
	step [61/147], loss=94.5239
	step [62/147], loss=88.5385
	step [63/147], loss=70.1098
	step [64/147], loss=99.7852
	step [65/147], loss=85.2686
	step [66/147], loss=78.0987
	step [67/147], loss=90.6999
	step [68/147], loss=96.5518
	step [69/147], loss=107.1190
	step [70/147], loss=96.6763
	step [71/147], loss=108.7350
	step [72/147], loss=91.6984
	step [73/147], loss=104.1635
	step [74/147], loss=107.8348
	step [75/147], loss=99.7453
	step [76/147], loss=97.4447
	step [77/147], loss=110.8477
	step [78/147], loss=114.1277
	step [79/147], loss=103.4542
	step [80/147], loss=85.2530
	step [81/147], loss=95.8289
	step [82/147], loss=102.8559
	step [83/147], loss=93.9977
	step [84/147], loss=88.1037
	step [85/147], loss=89.6349
	step [86/147], loss=85.8610
	step [87/147], loss=88.6485
	step [88/147], loss=98.9351
	step [89/147], loss=92.7618
	step [90/147], loss=104.3599
	step [91/147], loss=96.2844
	step [92/147], loss=103.6416
	step [93/147], loss=87.8122
	step [94/147], loss=113.3868
	step [95/147], loss=86.3253
	step [96/147], loss=93.2244
	step [97/147], loss=106.0911
	step [98/147], loss=113.8695
	step [99/147], loss=94.9984
	step [100/147], loss=78.1897
	step [101/147], loss=111.1783
	step [102/147], loss=107.6431
	step [103/147], loss=101.5653
	step [104/147], loss=98.2523
	step [105/147], loss=99.2844
	step [106/147], loss=103.6741
	step [107/147], loss=90.0361
	step [108/147], loss=90.9015
	step [109/147], loss=105.3674
	step [110/147], loss=91.9362
	step [111/147], loss=95.0026
	step [112/147], loss=94.0454
	step [113/147], loss=94.1289
	step [114/147], loss=104.6423
	step [115/147], loss=102.2340
	step [116/147], loss=79.6168
	step [117/147], loss=98.2826
	step [118/147], loss=81.5716
	step [119/147], loss=91.7385
	step [120/147], loss=79.6829
	step [121/147], loss=95.9228
	step [122/147], loss=87.4116
	step [123/147], loss=121.9128
	step [124/147], loss=99.1986
	step [125/147], loss=96.8846
	step [126/147], loss=80.2508
	step [127/147], loss=119.4466
	step [128/147], loss=97.5420
	step [129/147], loss=92.8087
	step [130/147], loss=86.1866
	step [131/147], loss=98.5143
	step [132/147], loss=101.6232
	step [133/147], loss=87.9266
	step [134/147], loss=95.4087
	step [135/147], loss=109.6520
	step [136/147], loss=100.9993
	step [137/147], loss=75.9936
	step [138/147], loss=112.3152
	step [139/147], loss=89.6394
	step [140/147], loss=91.8241
	step [141/147], loss=76.8242
	step [142/147], loss=97.9547
	step [143/147], loss=101.9017
	step [144/147], loss=80.8658
	step [145/147], loss=94.0962
	step [146/147], loss=104.0867
	step [147/147], loss=39.0579
	Evaluating
	loss=0.0119, precision=0.4075, recall=0.9051, f1=0.5620
Training finished
best_f1: 0.5918913271244265
directing: Y rim_enhanced: True test_id 0
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15948 # image files with weight 15917
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4124 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/249], loss=854.3621
	step [2/249], loss=503.2946
	step [3/249], loss=399.2301
	step [4/249], loss=423.0761
	step [5/249], loss=379.2073
	step [6/249], loss=375.9200
	step [7/249], loss=363.9064
	step [8/249], loss=372.5143
	step [9/249], loss=296.1759
	step [10/249], loss=278.5640
	step [11/249], loss=278.4233
	step [12/249], loss=307.2792
	step [13/249], loss=316.5938
	step [14/249], loss=302.2250
	step [15/249], loss=240.7721
	step [16/249], loss=257.3066
	step [17/249], loss=258.2317
	step [18/249], loss=252.8996
	step [19/249], loss=267.8466
	step [20/249], loss=259.2449
	step [21/249], loss=284.4506
	step [22/249], loss=248.5240
	step [23/249], loss=240.5647
	step [24/249], loss=271.4406
	step [25/249], loss=248.4068
	step [26/249], loss=245.7147
	step [27/249], loss=271.8065
	step [28/249], loss=278.1098
	step [29/249], loss=277.3984
	step [30/249], loss=246.9860
	step [31/249], loss=247.1248
	step [32/249], loss=229.3014
	step [33/249], loss=236.1540
	step [34/249], loss=247.1885
	step [35/249], loss=259.2255
	step [36/249], loss=244.7730
	step [37/249], loss=249.1834
	step [38/249], loss=207.8118
	step [39/249], loss=214.8207
	step [40/249], loss=274.9290
	step [41/249], loss=267.3253
	step [42/249], loss=258.7278
	step [43/249], loss=229.4291
	step [44/249], loss=255.0094
	step [45/249], loss=237.6223
	step [46/249], loss=231.3036
	step [47/249], loss=229.8424
	step [48/249], loss=259.8188
	step [49/249], loss=270.1997
	step [50/249], loss=234.5476
	step [51/249], loss=240.0810
	step [52/249], loss=229.9040
	step [53/249], loss=225.8949
	step [54/249], loss=243.8501
	step [55/249], loss=230.2621
	step [56/249], loss=237.8029
	step [57/249], loss=214.9151
	step [58/249], loss=219.8376
	step [59/249], loss=246.1558
	step [60/249], loss=236.4541
	step [61/249], loss=244.3102
	step [62/249], loss=241.6530
	step [63/249], loss=221.6032
	step [64/249], loss=219.0759
	step [65/249], loss=245.0238
	step [66/249], loss=229.6224
	step [67/249], loss=208.7720
	step [68/249], loss=244.0866
	step [69/249], loss=238.5117
	step [70/249], loss=234.6364
	step [71/249], loss=228.2628
	step [72/249], loss=234.8839
	step [73/249], loss=208.8595
	step [74/249], loss=237.8503
	step [75/249], loss=213.8889
	step [76/249], loss=221.8524
	step [77/249], loss=203.1217
	step [78/249], loss=238.8013
	step [79/249], loss=210.4350
	step [80/249], loss=211.2729
	step [81/249], loss=230.9193
	step [82/249], loss=224.6925
	step [83/249], loss=234.0721
	step [84/249], loss=210.2074
	step [85/249], loss=205.8713
	step [86/249], loss=219.7324
	step [87/249], loss=216.7153
	step [88/249], loss=238.1062
	step [89/249], loss=227.2173
	step [90/249], loss=245.0697
	step [91/249], loss=213.0858
	step [92/249], loss=219.0359
	step [93/249], loss=220.9511
	step [94/249], loss=210.6110
	step [95/249], loss=218.7979
	step [96/249], loss=198.3785
	step [97/249], loss=216.7816
	step [98/249], loss=233.9527
	step [99/249], loss=199.5056
	step [100/249], loss=201.3628
	step [101/249], loss=227.6398
	step [102/249], loss=215.4694
	step [103/249], loss=221.5755
	step [104/249], loss=207.5529
	step [105/249], loss=207.2378
	step [106/249], loss=202.5363
	step [107/249], loss=196.4345
	step [108/249], loss=195.4864
	step [109/249], loss=181.8661
	step [110/249], loss=181.2976
	step [111/249], loss=206.5291
	step [112/249], loss=209.1872
	step [113/249], loss=215.3543
	step [114/249], loss=182.5482
	step [115/249], loss=218.5738
	step [116/249], loss=224.1250
	step [117/249], loss=191.8541
	step [118/249], loss=196.7672
	step [119/249], loss=194.5369
	step [120/249], loss=191.7101
	step [121/249], loss=188.1890
	step [122/249], loss=206.2629
	step [123/249], loss=199.0925
	step [124/249], loss=244.4304
	step [125/249], loss=227.7380
	step [126/249], loss=193.3826
	step [127/249], loss=197.3331
	step [128/249], loss=211.0996
	step [129/249], loss=189.4207
	step [130/249], loss=214.8428
	step [131/249], loss=212.8269
	step [132/249], loss=223.7823
	step [133/249], loss=213.4536
	step [134/249], loss=221.7229
	step [135/249], loss=171.2733
	step [136/249], loss=210.8626
	step [137/249], loss=222.7122
	step [138/249], loss=195.3330
	step [139/249], loss=221.8864
	step [140/249], loss=225.6931
	step [141/249], loss=223.0296
	step [142/249], loss=193.7106
	step [143/249], loss=189.2267
	step [144/249], loss=203.8366
	step [145/249], loss=179.6257
	step [146/249], loss=180.5155
	step [147/249], loss=224.6895
	step [148/249], loss=203.5582
	step [149/249], loss=184.9478
	step [150/249], loss=210.1496
	step [151/249], loss=210.6400
	step [152/249], loss=198.8330
	step [153/249], loss=199.3334
	step [154/249], loss=186.4528
	step [155/249], loss=201.5471
	step [156/249], loss=189.4702
	step [157/249], loss=206.0214
	step [158/249], loss=178.8885
	step [159/249], loss=228.8481
	step [160/249], loss=195.8665
	step [161/249], loss=201.5812
	step [162/249], loss=209.8284
	step [163/249], loss=204.6289
	step [164/249], loss=197.6405
	step [165/249], loss=220.6695
	step [166/249], loss=215.7875
	step [167/249], loss=214.0344
	step [168/249], loss=207.1728
	step [169/249], loss=197.1479
	step [170/249], loss=204.2328
	step [171/249], loss=188.0334
	step [172/249], loss=180.8921
	step [173/249], loss=217.8669
	step [174/249], loss=201.9936
	step [175/249], loss=189.0301
	step [176/249], loss=203.7759
	step [177/249], loss=180.8098
	step [178/249], loss=189.8682
	step [179/249], loss=201.9346
	step [180/249], loss=225.3415
	step [181/249], loss=196.4968
	step [182/249], loss=183.5245
	step [183/249], loss=207.2349
	step [184/249], loss=186.5597
	step [185/249], loss=207.3964
	step [186/249], loss=186.3406
	step [187/249], loss=172.8026
	step [188/249], loss=185.2884
	step [189/249], loss=213.2832
	step [190/249], loss=192.9162
	step [191/249], loss=191.8299
	step [192/249], loss=192.3491
	step [193/249], loss=205.0717
	step [194/249], loss=192.7361
	step [195/249], loss=177.9966
	step [196/249], loss=172.7796
	step [197/249], loss=191.8453
	step [198/249], loss=184.6634
	step [199/249], loss=209.1589
	step [200/249], loss=188.2505
	step [201/249], loss=212.8376
	step [202/249], loss=207.9275
	step [203/249], loss=199.0617
	step [204/249], loss=182.0289
	step [205/249], loss=196.4176
	step [206/249], loss=184.5118
	step [207/249], loss=177.1823
	step [208/249], loss=183.3153
	step [209/249], loss=193.1133
	step [210/249], loss=190.7118
	step [211/249], loss=189.7696
	step [212/249], loss=212.8942
	step [213/249], loss=189.9450
	step [214/249], loss=190.1593
	step [215/249], loss=169.4864
	step [216/249], loss=194.5479
	step [217/249], loss=198.5731
	step [218/249], loss=183.1741
	step [219/249], loss=192.4809
	step [220/249], loss=201.5919
	step [221/249], loss=186.4755
	step [222/249], loss=177.4561
	step [223/249], loss=182.4729
	step [224/249], loss=193.0384
	step [225/249], loss=198.7899
	step [226/249], loss=166.2657
	step [227/249], loss=181.9185
	step [228/249], loss=188.9212
	step [229/249], loss=196.7114
	step [230/249], loss=185.3837
	step [231/249], loss=175.1150
	step [232/249], loss=178.2819
	step [233/249], loss=186.1976
	step [234/249], loss=173.5627
	step [235/249], loss=194.5094
	step [236/249], loss=173.8860
	step [237/249], loss=184.2195
	step [238/249], loss=196.5474
	step [239/249], loss=185.7360
	step [240/249], loss=189.8944
	step [241/249], loss=193.9416
	step [242/249], loss=183.0704
	step [243/249], loss=200.8340
	step [244/249], loss=148.3163
	step [245/249], loss=179.2938
	step [246/249], loss=184.0858
	step [247/249], loss=182.1490
	step [248/249], loss=197.2757
	step [249/249], loss=118.8398
	Evaluating
	loss=0.3381, precision=0.2198, recall=0.9438, f1=0.3566
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/249], loss=190.0443
	step [2/249], loss=184.4304
	step [3/249], loss=193.5057
	step [4/249], loss=165.0327
	step [5/249], loss=171.4124
	step [6/249], loss=181.1746
	step [7/249], loss=165.4048
	step [8/249], loss=186.7540
	step [9/249], loss=173.8353
	step [10/249], loss=177.2155
	step [11/249], loss=170.9067
	step [12/249], loss=194.5485
	step [13/249], loss=173.2384
	step [14/249], loss=171.1802
	step [15/249], loss=187.4583
	step [16/249], loss=169.2258
	step [17/249], loss=177.0018
	step [18/249], loss=170.8625
	step [19/249], loss=189.4877
	step [20/249], loss=157.8173
	step [21/249], loss=178.4914
	step [22/249], loss=206.1363
	step [23/249], loss=156.7776
	step [24/249], loss=168.7348
	step [25/249], loss=163.9949
	step [26/249], loss=190.3595
	step [27/249], loss=169.1953
	step [28/249], loss=175.9532
	step [29/249], loss=181.1801
	step [30/249], loss=183.1135
	step [31/249], loss=163.7477
	step [32/249], loss=191.3312
	step [33/249], loss=183.1417
	step [34/249], loss=177.5113
	step [35/249], loss=160.6882
	step [36/249], loss=163.2205
	step [37/249], loss=177.8303
	step [38/249], loss=184.6637
	step [39/249], loss=166.8356
	step [40/249], loss=216.8042
	step [41/249], loss=150.0740
	step [42/249], loss=170.8354
	step [43/249], loss=212.4762
	step [44/249], loss=172.2115
	step [45/249], loss=174.7420
	step [46/249], loss=181.8578
	step [47/249], loss=171.9979
	step [48/249], loss=198.6155
	step [49/249], loss=186.4384
	step [50/249], loss=166.9932
	step [51/249], loss=194.4919
	step [52/249], loss=168.6429
	step [53/249], loss=191.8445
	step [54/249], loss=166.2911
	step [55/249], loss=193.8458
	step [56/249], loss=187.1581
	step [57/249], loss=163.5928
	step [58/249], loss=177.9906
	step [59/249], loss=168.7491
	step [60/249], loss=176.2205
	step [61/249], loss=184.6041
	step [62/249], loss=171.2468
	step [63/249], loss=187.1506
	step [64/249], loss=174.0463
	step [65/249], loss=159.7395
	step [66/249], loss=168.1918
	step [67/249], loss=176.8061
	step [68/249], loss=178.4172
	step [69/249], loss=175.6617
	step [70/249], loss=179.3565
	step [71/249], loss=171.4555
	step [72/249], loss=179.7142
	step [73/249], loss=195.0232
	step [74/249], loss=180.9310
	step [75/249], loss=170.2937
	step [76/249], loss=185.7607
	step [77/249], loss=158.8561
	step [78/249], loss=167.0895
	step [79/249], loss=171.8424
	step [80/249], loss=176.2868
	step [81/249], loss=156.3697
	step [82/249], loss=172.8320
	step [83/249], loss=156.8097
	step [84/249], loss=162.3814
	step [85/249], loss=152.0244
	step [86/249], loss=166.4342
	step [87/249], loss=184.2583
	step [88/249], loss=159.0001
	step [89/249], loss=162.5375
	step [90/249], loss=147.9659
	step [91/249], loss=175.7668
	step [92/249], loss=152.7155
	step [93/249], loss=157.9244
	step [94/249], loss=168.3481
	step [95/249], loss=186.4764
	step [96/249], loss=153.8598
	step [97/249], loss=164.9523
	step [98/249], loss=173.8512
	step [99/249], loss=167.1059
	step [100/249], loss=152.6978
	step [101/249], loss=177.5579
	step [102/249], loss=177.0423
	step [103/249], loss=170.2525
	step [104/249], loss=162.4461
	step [105/249], loss=183.2438
	step [106/249], loss=171.8438
	step [107/249], loss=177.9572
	step [108/249], loss=159.0154
	step [109/249], loss=173.8767
	step [110/249], loss=190.8450
	step [111/249], loss=175.5829
	step [112/249], loss=160.0069
	step [113/249], loss=167.8682
	step [114/249], loss=172.6773
	step [115/249], loss=156.6240
	step [116/249], loss=189.0327
	step [117/249], loss=166.5471
	step [118/249], loss=175.0769
	step [119/249], loss=186.9811
	step [120/249], loss=175.4819
	step [121/249], loss=149.7033
	step [122/249], loss=161.3223
	step [123/249], loss=149.3493
	step [124/249], loss=179.2189
	step [125/249], loss=154.9303
	step [126/249], loss=147.3565
	step [127/249], loss=164.2104
	step [128/249], loss=157.2184
	step [129/249], loss=167.2114
	step [130/249], loss=156.6490
	step [131/249], loss=155.9699
	step [132/249], loss=179.0363
	step [133/249], loss=168.8646
	step [134/249], loss=142.7813
	step [135/249], loss=169.1820
	step [136/249], loss=195.3284
	step [137/249], loss=184.6467
	step [138/249], loss=160.8537
	step [139/249], loss=194.1186
	step [140/249], loss=176.5791
	step [141/249], loss=166.4104
	step [142/249], loss=178.0388
	step [143/249], loss=144.3680
	step [144/249], loss=170.8747
	step [145/249], loss=161.8769
	step [146/249], loss=172.1344
	step [147/249], loss=185.5475
	step [148/249], loss=164.3029
	step [149/249], loss=158.8599
	step [150/249], loss=152.8987
	step [151/249], loss=186.3671
	step [152/249], loss=173.9622
	step [153/249], loss=192.3083
	step [154/249], loss=192.8480
	step [155/249], loss=162.4401
	step [156/249], loss=170.1115
	step [157/249], loss=170.2312
	step [158/249], loss=192.2184
	step [159/249], loss=157.7838
	step [160/249], loss=155.4043
	step [161/249], loss=163.9732
	step [162/249], loss=161.4637
	step [163/249], loss=141.6795
	step [164/249], loss=152.8483
	step [165/249], loss=162.8811
	step [166/249], loss=194.1294
	step [167/249], loss=167.0287
	step [168/249], loss=161.4118
	step [169/249], loss=169.5735
	step [170/249], loss=154.5001
	step [171/249], loss=143.6716
	step [172/249], loss=174.2426
	step [173/249], loss=184.6882
	step [174/249], loss=154.7879
	step [175/249], loss=168.4020
	step [176/249], loss=149.4300
	step [177/249], loss=168.3957
	step [178/249], loss=195.4744
	step [179/249], loss=189.2242
	step [180/249], loss=143.9760
	step [181/249], loss=149.2517
	step [182/249], loss=179.6156
	step [183/249], loss=161.8457
	step [184/249], loss=155.1588
	step [185/249], loss=164.5254
	step [186/249], loss=176.9249
	step [187/249], loss=172.0088
	step [188/249], loss=165.6390
	step [189/249], loss=164.1016
	step [190/249], loss=171.2790
	step [191/249], loss=168.9472
	step [192/249], loss=154.9856
	step [193/249], loss=158.1542
	step [194/249], loss=169.4691
	step [195/249], loss=150.8565
	step [196/249], loss=154.3730
	step [197/249], loss=141.7946
	step [198/249], loss=146.1553
	step [199/249], loss=155.1247
	step [200/249], loss=165.3495
	step [201/249], loss=176.3859
	step [202/249], loss=142.9162
	step [203/249], loss=162.2323
	step [204/249], loss=161.0032
	step [205/249], loss=155.2599
	step [206/249], loss=159.5385
	step [207/249], loss=173.1830
	step [208/249], loss=184.1970
	step [209/249], loss=140.2218
	step [210/249], loss=164.1390
	step [211/249], loss=157.0610
	step [212/249], loss=170.0386
	step [213/249], loss=161.3267
	step [214/249], loss=161.1423
	step [215/249], loss=172.5339
	step [216/249], loss=160.0388
	step [217/249], loss=158.8024
	step [218/249], loss=149.8171
	step [219/249], loss=138.0352
	step [220/249], loss=168.6342
	step [221/249], loss=159.0106
	step [222/249], loss=144.2862
	step [223/249], loss=166.8170
	step [224/249], loss=167.8125
	step [225/249], loss=154.2011
	step [226/249], loss=162.1174
	step [227/249], loss=141.7988
	step [228/249], loss=177.5314
	step [229/249], loss=152.7323
	step [230/249], loss=149.2953
	step [231/249], loss=152.6110
	step [232/249], loss=175.3227
	step [233/249], loss=142.0962
	step [234/249], loss=169.2640
	step [235/249], loss=155.7173
	step [236/249], loss=148.8800
	step [237/249], loss=168.0288
	step [238/249], loss=186.9668
	step [239/249], loss=168.7215
	step [240/249], loss=131.5706
	step [241/249], loss=163.4612
	step [242/249], loss=155.1606
	step [243/249], loss=170.5633
	step [244/249], loss=167.5592
	step [245/249], loss=133.5629
	step [246/249], loss=173.5844
	step [247/249], loss=169.3439
	step [248/249], loss=165.7473
	step [249/249], loss=101.7563
	Evaluating
	loss=0.2423, precision=0.3135, recall=0.9390, f1=0.4701
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/249], loss=167.5723
	step [2/249], loss=144.4643
	step [3/249], loss=144.7275
	step [4/249], loss=167.1888
	step [5/249], loss=152.7443
	step [6/249], loss=163.0172
	step [7/249], loss=170.7693
	step [8/249], loss=165.5452
	step [9/249], loss=166.7510
	step [10/249], loss=151.6230
	step [11/249], loss=149.2666
	step [12/249], loss=164.8872
	step [13/249], loss=160.0435
	step [14/249], loss=172.2189
	step [15/249], loss=151.4738
	step [16/249], loss=154.0980
	step [17/249], loss=151.4070
	step [18/249], loss=146.1790
	step [19/249], loss=144.7077
	step [20/249], loss=138.8690
	step [21/249], loss=159.6373
	step [22/249], loss=155.4384
	step [23/249], loss=155.4393
	step [24/249], loss=157.7136
	step [25/249], loss=142.2596
	step [26/249], loss=140.6597
	step [27/249], loss=161.6447
	step [28/249], loss=182.5769
	step [29/249], loss=129.3250
	step [30/249], loss=161.2039
	step [31/249], loss=150.0329
	step [32/249], loss=171.8357
	step [33/249], loss=148.8063
	step [34/249], loss=153.1098
	step [35/249], loss=172.4821
	step [36/249], loss=149.3746
	step [37/249], loss=169.3962
	step [38/249], loss=144.4631
	step [39/249], loss=160.0393
	step [40/249], loss=133.9800
	step [41/249], loss=163.6335
	step [42/249], loss=156.4083
	step [43/249], loss=161.5114
	step [44/249], loss=176.6886
	step [45/249], loss=142.8555
	step [46/249], loss=150.1683
	step [47/249], loss=140.2188
	step [48/249], loss=142.1285
	step [49/249], loss=178.7821
	step [50/249], loss=159.1669
	step [51/249], loss=165.7865
	step [52/249], loss=153.5222
	step [53/249], loss=170.5364
	step [54/249], loss=149.7501
	step [55/249], loss=154.8949
	step [56/249], loss=142.5544
	step [57/249], loss=150.3194
	step [58/249], loss=151.1847
	step [59/249], loss=149.0582
	step [60/249], loss=173.0544
	step [61/249], loss=167.9912
	step [62/249], loss=143.1067
	step [63/249], loss=140.2416
	step [64/249], loss=152.4061
	step [65/249], loss=146.9191
	step [66/249], loss=164.7520
	step [67/249], loss=146.7394
	step [68/249], loss=149.2638
	step [69/249], loss=137.7618
	step [70/249], loss=133.0490
	step [71/249], loss=133.8328
	step [72/249], loss=133.6905
	step [73/249], loss=158.9952
	step [74/249], loss=153.7720
	step [75/249], loss=135.9710
	step [76/249], loss=152.2777
	step [77/249], loss=155.7096
	step [78/249], loss=154.6033
	step [79/249], loss=153.3006
	step [80/249], loss=159.1953
	step [81/249], loss=147.5866
	step [82/249], loss=144.4936
	step [83/249], loss=140.5774
	step [84/249], loss=153.7226
	step [85/249], loss=166.9355
	step [86/249], loss=164.4122
	step [87/249], loss=175.9510
	step [88/249], loss=154.3744
	step [89/249], loss=127.8430
	step [90/249], loss=155.2006
	step [91/249], loss=131.3795
	step [92/249], loss=143.5882
	step [93/249], loss=148.1377
	step [94/249], loss=137.0710
	step [95/249], loss=137.6954
	step [96/249], loss=157.3799
	step [97/249], loss=157.5317
	step [98/249], loss=171.2972
	step [99/249], loss=157.3604
	step [100/249], loss=162.3628
	step [101/249], loss=157.9973
	step [102/249], loss=130.6847
	step [103/249], loss=157.5065
	step [104/249], loss=142.1782
	step [105/249], loss=155.2028
	step [106/249], loss=147.2026
	step [107/249], loss=142.7466
	step [108/249], loss=129.0169
	step [109/249], loss=150.0550
	step [110/249], loss=157.5392
	step [111/249], loss=176.6409
	step [112/249], loss=151.2399
	step [113/249], loss=138.3538
	step [114/249], loss=151.4030
	step [115/249], loss=141.3056
	step [116/249], loss=140.6585
	step [117/249], loss=160.2158
	step [118/249], loss=159.4974
	step [119/249], loss=134.4853
	step [120/249], loss=140.6755
	step [121/249], loss=156.9337
	step [122/249], loss=143.5412
	step [123/249], loss=163.9923
	step [124/249], loss=150.0167
	step [125/249], loss=155.0609
	step [126/249], loss=153.0352
	step [127/249], loss=137.9550
	step [128/249], loss=156.3062
	step [129/249], loss=148.4432
	step [130/249], loss=153.5263
	step [131/249], loss=136.7048
	step [132/249], loss=149.9476
	step [133/249], loss=142.8177
	step [134/249], loss=154.7624
	step [135/249], loss=164.4140
	step [136/249], loss=147.6400
	step [137/249], loss=146.7865
	step [138/249], loss=148.2092
	step [139/249], loss=147.4435
	step [140/249], loss=143.1410
	step [141/249], loss=170.7757
	step [142/249], loss=166.7123
	step [143/249], loss=149.6686
	step [144/249], loss=121.2574
	step [145/249], loss=163.3967
	step [146/249], loss=156.4665
	step [147/249], loss=142.0909
	step [148/249], loss=133.6679
	step [149/249], loss=156.9355
	step [150/249], loss=164.0312
	step [151/249], loss=137.4845
	step [152/249], loss=132.7196
	step [153/249], loss=139.4393
	step [154/249], loss=138.5175
	step [155/249], loss=159.2846
	step [156/249], loss=147.6829
	step [157/249], loss=160.3097
	step [158/249], loss=153.5400
	step [159/249], loss=139.1290
	step [160/249], loss=169.5166
	step [161/249], loss=144.0737
	step [162/249], loss=146.5154
	step [163/249], loss=163.1082
	step [164/249], loss=146.5068
	step [165/249], loss=138.8175
	step [166/249], loss=146.1447
	step [167/249], loss=147.8481
	step [168/249], loss=142.5535
	step [169/249], loss=137.0606
	step [170/249], loss=158.6228
	step [171/249], loss=146.0624
	step [172/249], loss=163.9596
	step [173/249], loss=129.0814
	step [174/249], loss=146.4653
	step [175/249], loss=142.0095
	step [176/249], loss=168.1701
	step [177/249], loss=164.1288
	step [178/249], loss=137.3577
	step [179/249], loss=129.7707
	step [180/249], loss=158.5348
	step [181/249], loss=153.1546
	step [182/249], loss=140.0433
	step [183/249], loss=141.8450
	step [184/249], loss=128.2778
	step [185/249], loss=139.0089
	step [186/249], loss=143.0322
	step [187/249], loss=138.1564
	step [188/249], loss=118.5259
	step [189/249], loss=139.5795
	step [190/249], loss=132.0294
	step [191/249], loss=146.0994
	step [192/249], loss=161.9946
	step [193/249], loss=140.4403
	step [194/249], loss=143.5514
	step [195/249], loss=151.6332
	step [196/249], loss=150.4910
	step [197/249], loss=131.8577
	step [198/249], loss=160.5391
	step [199/249], loss=146.6575
	step [200/249], loss=153.9578
	step [201/249], loss=140.9670
	step [202/249], loss=138.7555
	step [203/249], loss=159.5642
	step [204/249], loss=145.1525
	step [205/249], loss=146.4436
	step [206/249], loss=141.6228
	step [207/249], loss=131.5511
	step [208/249], loss=137.3207
	step [209/249], loss=150.0072
	step [210/249], loss=138.7108
	step [211/249], loss=142.3795
	step [212/249], loss=138.7576
	step [213/249], loss=138.1310
	step [214/249], loss=158.8456
	step [215/249], loss=136.6553
	step [216/249], loss=145.8229
	step [217/249], loss=144.3140
	step [218/249], loss=140.7743
	step [219/249], loss=133.6192
	step [220/249], loss=154.4183
	step [221/249], loss=165.0540
	step [222/249], loss=154.1034
	step [223/249], loss=141.0795
	step [224/249], loss=143.8711
	step [225/249], loss=143.4730
	step [226/249], loss=131.7997
	step [227/249], loss=131.6100
	step [228/249], loss=138.6337
	step [229/249], loss=144.2019
	step [230/249], loss=135.1380
	step [231/249], loss=145.4364
	step [232/249], loss=125.3343
	step [233/249], loss=146.3308
	step [234/249], loss=143.6737
	step [235/249], loss=159.4739
	step [236/249], loss=151.8108
	step [237/249], loss=151.3969
	step [238/249], loss=144.7307
	step [239/249], loss=119.8052
	step [240/249], loss=136.3056
	step [241/249], loss=139.5005
	step [242/249], loss=137.3823
	step [243/249], loss=130.4913
	step [244/249], loss=144.7823
	step [245/249], loss=133.3961
	step [246/249], loss=148.2493
	step [247/249], loss=149.1586
	step [248/249], loss=144.3555
	step [249/249], loss=117.2207
	Evaluating
	loss=0.1697, precision=0.3242, recall=0.9141, f1=0.4787
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/249], loss=162.3288
	step [2/249], loss=137.0891
	step [3/249], loss=140.7655
	step [4/249], loss=136.8380
	step [5/249], loss=145.7327
	step [6/249], loss=139.4326
	step [7/249], loss=138.6185
	step [8/249], loss=128.8891
	step [9/249], loss=148.6280
	step [10/249], loss=157.0157
	step [11/249], loss=139.5256
	step [12/249], loss=169.6389
	step [13/249], loss=132.0300
	step [14/249], loss=124.2713
	step [15/249], loss=144.9717
	step [16/249], loss=133.8572
	step [17/249], loss=152.7881
	step [18/249], loss=138.3547
	step [19/249], loss=127.4015
	step [20/249], loss=118.9083
	step [21/249], loss=146.9725
	step [22/249], loss=132.1976
	step [23/249], loss=148.5722
	step [24/249], loss=143.2776
	step [25/249], loss=121.9364
	step [26/249], loss=163.8674
	step [27/249], loss=140.8502
	step [28/249], loss=142.2681
	step [29/249], loss=152.8849
	step [30/249], loss=142.3953
	step [31/249], loss=140.1956
	step [32/249], loss=136.6209
	step [33/249], loss=134.8371
	step [34/249], loss=142.2135
	step [35/249], loss=158.7629
	step [36/249], loss=137.7330
	step [37/249], loss=137.0097
	step [38/249], loss=146.0985
	step [39/249], loss=148.8354
	step [40/249], loss=128.6289
	step [41/249], loss=149.9640
	step [42/249], loss=144.3854
	step [43/249], loss=154.9623
	step [44/249], loss=146.6581
	step [45/249], loss=145.9037
	step [46/249], loss=147.1678
	step [47/249], loss=143.1475
	step [48/249], loss=124.2282
	step [49/249], loss=152.2478
	step [50/249], loss=140.0203
	step [51/249], loss=143.3077
	step [52/249], loss=142.7365
	step [53/249], loss=149.0192
	step [54/249], loss=136.0320
	step [55/249], loss=140.9017
	step [56/249], loss=140.7334
	step [57/249], loss=143.5750
	step [58/249], loss=125.3376
	step [59/249], loss=136.6035
	step [60/249], loss=145.8772
	step [61/249], loss=156.8636
	step [62/249], loss=123.5719
	step [63/249], loss=151.2275
	step [64/249], loss=132.3880
	step [65/249], loss=126.3381
	step [66/249], loss=127.2649
	step [67/249], loss=141.8073
	step [68/249], loss=140.3658
	step [69/249], loss=158.8110
	step [70/249], loss=152.7059
	step [71/249], loss=150.0617
	step [72/249], loss=116.2427
	step [73/249], loss=165.0996
	step [74/249], loss=136.9980
	step [75/249], loss=145.2151
	step [76/249], loss=146.9846
	step [77/249], loss=112.6011
	step [78/249], loss=144.6901
	step [79/249], loss=142.1932
	step [80/249], loss=108.6315
	step [81/249], loss=152.5905
	step [82/249], loss=128.7001
	step [83/249], loss=140.3776
	step [84/249], loss=156.7250
	step [85/249], loss=121.8268
	step [86/249], loss=154.6130
	step [87/249], loss=107.7708
	step [88/249], loss=143.3510
	step [89/249], loss=131.5641
	step [90/249], loss=158.1525
	step [91/249], loss=133.5894
	step [92/249], loss=125.7893
	step [93/249], loss=117.4368
	step [94/249], loss=136.5354
	step [95/249], loss=150.5998
	step [96/249], loss=144.9389
	step [97/249], loss=134.1665
	step [98/249], loss=136.2301
	step [99/249], loss=144.1049
	step [100/249], loss=139.0107
	step [101/249], loss=159.5880
	step [102/249], loss=119.2572
	step [103/249], loss=129.5007
	step [104/249], loss=144.4029
	step [105/249], loss=127.0339
	step [106/249], loss=134.9333
	step [107/249], loss=141.6887
	step [108/249], loss=137.6332
	step [109/249], loss=153.3050
	step [110/249], loss=144.5697
	step [111/249], loss=151.8500
	step [112/249], loss=141.7025
	step [113/249], loss=107.9879
	step [114/249], loss=131.4343
	step [115/249], loss=133.6608
	step [116/249], loss=129.4351
	step [117/249], loss=139.0568
	step [118/249], loss=133.7762
	step [119/249], loss=145.7194
	step [120/249], loss=137.5052
	step [121/249], loss=135.0096
	step [122/249], loss=124.1387
	step [123/249], loss=145.3410
	step [124/249], loss=130.0515
	step [125/249], loss=123.9279
	step [126/249], loss=120.2087
	step [127/249], loss=131.6802
	step [128/249], loss=143.5594
	step [129/249], loss=142.8083
	step [130/249], loss=153.3770
	step [131/249], loss=129.6440
	step [132/249], loss=167.4836
	step [133/249], loss=130.9926
	step [134/249], loss=128.9452
	step [135/249], loss=124.0557
	step [136/249], loss=131.4578
	step [137/249], loss=164.4043
	step [138/249], loss=120.4483
	step [139/249], loss=133.7495
	step [140/249], loss=139.8775
	step [141/249], loss=122.8600
	step [142/249], loss=164.2983
	step [143/249], loss=149.5841
	step [144/249], loss=153.0509
	step [145/249], loss=139.6082
	step [146/249], loss=135.4426
	step [147/249], loss=124.2718
	step [148/249], loss=126.2279
	step [149/249], loss=135.2551
	step [150/249], loss=153.5585
	step [151/249], loss=147.8955
	step [152/249], loss=125.2249
	step [153/249], loss=157.4659
	step [154/249], loss=111.6073
	step [155/249], loss=126.9124
	step [156/249], loss=139.4530
	step [157/249], loss=133.2134
	step [158/249], loss=142.8906
	step [159/249], loss=139.9131
	step [160/249], loss=140.4933
	step [161/249], loss=122.7500
	step [162/249], loss=138.4519
	step [163/249], loss=132.1664
	step [164/249], loss=132.3285
	step [165/249], loss=121.1126
	step [166/249], loss=122.0804
	step [167/249], loss=122.6307
	step [168/249], loss=159.8630
	step [169/249], loss=131.4076
	step [170/249], loss=131.3983
	step [171/249], loss=121.7331
	step [172/249], loss=131.8536
	step [173/249], loss=146.4088
	step [174/249], loss=122.4554
	step [175/249], loss=148.4395
	step [176/249], loss=137.2612
	step [177/249], loss=127.0377
	step [178/249], loss=168.6045
	step [179/249], loss=161.5970
	step [180/249], loss=133.5992
	step [181/249], loss=129.6715
	step [182/249], loss=126.2229
	step [183/249], loss=139.4691
	step [184/249], loss=145.5182
	step [185/249], loss=131.7438
	step [186/249], loss=144.3411
	step [187/249], loss=143.9476
	step [188/249], loss=112.6641
	step [189/249], loss=130.3483
	step [190/249], loss=142.0401
	step [191/249], loss=124.1758
	step [192/249], loss=123.4666
	step [193/249], loss=136.2683
	step [194/249], loss=143.7782
	step [195/249], loss=126.6107
	step [196/249], loss=140.7365
	step [197/249], loss=147.9059
	step [198/249], loss=126.5964
	step [199/249], loss=141.5762
	step [200/249], loss=112.0950
	step [201/249], loss=134.5516
	step [202/249], loss=114.1454
	step [203/249], loss=128.2086
	step [204/249], loss=137.0353
	step [205/249], loss=119.4229
	step [206/249], loss=128.5500
	step [207/249], loss=138.5754
	step [208/249], loss=144.7387
	step [209/249], loss=138.0786
	step [210/249], loss=142.3954
	step [211/249], loss=126.9104
	step [212/249], loss=120.5252
	step [213/249], loss=129.5026
	step [214/249], loss=126.8102
	step [215/249], loss=137.2693
	step [216/249], loss=134.7825
	step [217/249], loss=137.5932
	step [218/249], loss=141.9302
	step [219/249], loss=133.3584
	step [220/249], loss=118.4951
	step [221/249], loss=147.2437
	step [222/249], loss=134.1779
	step [223/249], loss=135.6218
	step [224/249], loss=132.8293
	step [225/249], loss=120.7949
	step [226/249], loss=131.3799
	step [227/249], loss=108.3167
	step [228/249], loss=106.1773
	step [229/249], loss=121.4446
	step [230/249], loss=123.1404
	step [231/249], loss=117.2733
	step [232/249], loss=136.9264
	step [233/249], loss=124.5196
	step [234/249], loss=130.3880
	step [235/249], loss=124.4688
	step [236/249], loss=125.1654
	step [237/249], loss=131.5537
	step [238/249], loss=130.2077
	step [239/249], loss=120.8235
	step [240/249], loss=108.8253
	step [241/249], loss=123.8917
	step [242/249], loss=144.7001
	step [243/249], loss=126.1992
	step [244/249], loss=135.3319
	step [245/249], loss=127.1095
	step [246/249], loss=130.5515
	step [247/249], loss=126.0862
	step [248/249], loss=123.6114
	step [249/249], loss=92.8415
	Evaluating
	loss=0.1248, precision=0.3406, recall=0.8941, f1=0.4933
saving model as: 0_saved_model.pth
Training epoch 5
	step [1/249], loss=128.5777
	step [2/249], loss=133.4047
	step [3/249], loss=132.3421
	step [4/249], loss=142.9564
	step [5/249], loss=119.6780
	step [6/249], loss=125.1346
	step [7/249], loss=130.0915
	step [8/249], loss=139.1905
	step [9/249], loss=137.4056
	step [10/249], loss=143.9649
	step [11/249], loss=140.9939
	step [12/249], loss=143.6097
	step [13/249], loss=142.7331
	step [14/249], loss=125.8869
	step [15/249], loss=121.8459
	step [16/249], loss=127.3478
	step [17/249], loss=130.1597
	step [18/249], loss=151.7424
	step [19/249], loss=130.9849
	step [20/249], loss=137.6520
	step [21/249], loss=148.2416
	step [22/249], loss=128.1949
	step [23/249], loss=127.5796
	step [24/249], loss=123.0136
	step [25/249], loss=135.4488
	step [26/249], loss=124.2336
	step [27/249], loss=141.6446
	step [28/249], loss=126.8901
	step [29/249], loss=109.6067
	step [30/249], loss=121.8833
	step [31/249], loss=121.5842
	step [32/249], loss=147.1759
	step [33/249], loss=137.3365
	step [34/249], loss=136.1941
	step [35/249], loss=129.1851
	step [36/249], loss=138.9913
	step [37/249], loss=132.3664
	step [38/249], loss=140.2147
	step [39/249], loss=140.4670
	step [40/249], loss=136.2467
	step [41/249], loss=129.4290
	step [42/249], loss=131.6096
	step [43/249], loss=116.1144
	step [44/249], loss=124.2925
	step [45/249], loss=139.4557
	step [46/249], loss=119.6107
	step [47/249], loss=122.0317
	step [48/249], loss=138.3454
	step [49/249], loss=137.8816
	step [50/249], loss=138.7991
	step [51/249], loss=111.6190
	step [52/249], loss=151.4510
	step [53/249], loss=131.8997
	step [54/249], loss=125.2828
	step [55/249], loss=156.7048
	step [56/249], loss=126.9184
	step [57/249], loss=118.5860
	step [58/249], loss=117.8387
	step [59/249], loss=135.4061
	step [60/249], loss=128.3284
	step [61/249], loss=131.6442
	step [62/249], loss=140.4789
	step [63/249], loss=107.3612
	step [64/249], loss=121.6166
	step [65/249], loss=126.4280
	step [66/249], loss=150.3549
	step [67/249], loss=142.3443
	step [68/249], loss=118.3059
	step [69/249], loss=134.2003
	step [70/249], loss=130.9446
	step [71/249], loss=131.2959
	step [72/249], loss=134.7708
	step [73/249], loss=149.0947
	step [74/249], loss=119.1143
	step [75/249], loss=136.7362
	step [76/249], loss=118.9487
	step [77/249], loss=130.4144
	step [78/249], loss=124.7183
	step [79/249], loss=118.3686
	step [80/249], loss=148.3794
	step [81/249], loss=110.1570
	step [82/249], loss=121.2286
	step [83/249], loss=127.8581
	step [84/249], loss=117.1806
	step [85/249], loss=114.4916
	step [86/249], loss=129.0669
	step [87/249], loss=123.7313
	step [88/249], loss=117.3690
	step [89/249], loss=113.8051
	step [90/249], loss=133.8419
	step [91/249], loss=120.5688
	step [92/249], loss=127.5765
	step [93/249], loss=114.1663
	step [94/249], loss=114.4226
	step [95/249], loss=147.8299
	step [96/249], loss=121.3070
	step [97/249], loss=139.8842
	step [98/249], loss=123.4904
	step [99/249], loss=119.7014
	step [100/249], loss=121.3994
	step [101/249], loss=143.7364
	step [102/249], loss=129.9272
	step [103/249], loss=115.2672
	step [104/249], loss=134.2509
	step [105/249], loss=122.1777
	step [106/249], loss=123.0971
	step [107/249], loss=122.7598
	step [108/249], loss=128.1083
	step [109/249], loss=121.1985
	step [110/249], loss=112.9927
	step [111/249], loss=120.0933
	step [112/249], loss=115.5814
	step [113/249], loss=134.1228
	step [114/249], loss=126.7952
	step [115/249], loss=126.7855
	step [116/249], loss=148.0094
	step [117/249], loss=135.1578
	step [118/249], loss=122.2594
	step [119/249], loss=139.7599
	step [120/249], loss=125.7830
	step [121/249], loss=122.9459
	step [122/249], loss=114.9416
	step [123/249], loss=145.6127
	step [124/249], loss=127.0247
	step [125/249], loss=131.5289
	step [126/249], loss=129.4101
	step [127/249], loss=124.3083
	step [128/249], loss=118.2642
	step [129/249], loss=117.1666
	step [130/249], loss=117.1868
	step [131/249], loss=119.3480
	step [132/249], loss=122.2791
	step [133/249], loss=138.7031
	step [134/249], loss=124.7421
	step [135/249], loss=134.3770
	step [136/249], loss=135.6124
	step [137/249], loss=139.8871
	step [138/249], loss=122.1311
	step [139/249], loss=139.5321
	step [140/249], loss=123.2088
	step [141/249], loss=116.8255
	step [142/249], loss=122.6647
	step [143/249], loss=136.9576
	step [144/249], loss=131.2574
	step [145/249], loss=134.0808
	step [146/249], loss=131.8982
	step [147/249], loss=127.7157
	step [148/249], loss=115.7767
	step [149/249], loss=131.5712
	step [150/249], loss=134.7713
	step [151/249], loss=103.3244
	step [152/249], loss=134.3446
	step [153/249], loss=137.6433
	step [154/249], loss=137.9436
	step [155/249], loss=129.6535
	step [156/249], loss=116.4392
	step [157/249], loss=117.1255
	step [158/249], loss=130.4240
	step [159/249], loss=118.0635
	step [160/249], loss=111.6494
	step [161/249], loss=123.6393
	step [162/249], loss=131.5082
	step [163/249], loss=129.3515
	step [164/249], loss=135.6062
	step [165/249], loss=152.6652
	step [166/249], loss=121.1179
	step [167/249], loss=143.5716
	step [168/249], loss=125.7892
	step [169/249], loss=122.4493
	step [170/249], loss=113.1147
	step [171/249], loss=115.3816
	step [172/249], loss=127.2572
	step [173/249], loss=136.1643
	step [174/249], loss=109.7600
	step [175/249], loss=128.2546
	step [176/249], loss=122.6835
	step [177/249], loss=128.9290
	step [178/249], loss=135.9500
	step [179/249], loss=121.5367
	step [180/249], loss=138.0273
	step [181/249], loss=132.2950
	step [182/249], loss=132.2879
	step [183/249], loss=133.1317
	step [184/249], loss=119.8421
	step [185/249], loss=129.5378
	step [186/249], loss=121.6976
	step [187/249], loss=136.2676
	step [188/249], loss=113.9296
	step [189/249], loss=130.7156
	step [190/249], loss=137.6088
	step [191/249], loss=124.7106
	step [192/249], loss=111.8620
	step [193/249], loss=120.2622
	step [194/249], loss=101.9774
	step [195/249], loss=159.8277
	step [196/249], loss=119.1776
	step [197/249], loss=144.3253
	step [198/249], loss=142.3170
	step [199/249], loss=129.4518
	step [200/249], loss=124.6742
	step [201/249], loss=119.0456
	step [202/249], loss=109.2374
	step [203/249], loss=126.8531
	step [204/249], loss=111.4997
	step [205/249], loss=126.5050
	step [206/249], loss=116.6249
	step [207/249], loss=124.9712
	step [208/249], loss=129.8605
	step [209/249], loss=114.9447
	step [210/249], loss=119.1117
	step [211/249], loss=136.9853
	step [212/249], loss=113.2630
	step [213/249], loss=113.1935
	step [214/249], loss=135.2340
	step [215/249], loss=109.9395
	step [216/249], loss=134.0812
	step [217/249], loss=115.9008
	step [218/249], loss=120.8967
	step [219/249], loss=126.8278
	step [220/249], loss=127.8506
	step [221/249], loss=116.9707
	step [222/249], loss=132.1367
	step [223/249], loss=112.6753
	step [224/249], loss=106.2363
	step [225/249], loss=134.0654
	step [226/249], loss=134.4272
	step [227/249], loss=141.6612
	step [228/249], loss=110.0620
	step [229/249], loss=119.3546
	step [230/249], loss=140.3084
	step [231/249], loss=121.9785
	step [232/249], loss=127.5246
	step [233/249], loss=153.2966
	step [234/249], loss=115.0096
	step [235/249], loss=120.8412
	step [236/249], loss=127.1398
	step [237/249], loss=125.1008
	step [238/249], loss=118.5921
	step [239/249], loss=121.2734
	step [240/249], loss=111.7043
	step [241/249], loss=129.3295
	step [242/249], loss=141.9368
	step [243/249], loss=126.8202
	step [244/249], loss=148.4234
	step [245/249], loss=119.9292
	step [246/249], loss=132.8747
	step [247/249], loss=103.1826
	step [248/249], loss=132.2091
	step [249/249], loss=84.5946
	Evaluating
	loss=0.0923, precision=0.3010, recall=0.9261, f1=0.4544
Training epoch 6
	step [1/249], loss=120.4903
	step [2/249], loss=131.9005
	step [3/249], loss=120.6000
	step [4/249], loss=131.6880
	step [5/249], loss=130.6712
	step [6/249], loss=144.4834
	step [7/249], loss=121.5674
	step [8/249], loss=134.7716
	step [9/249], loss=113.0077
	step [10/249], loss=140.6516
	step [11/249], loss=129.6944
	step [12/249], loss=149.3909
	step [13/249], loss=136.4270
	step [14/249], loss=117.7641
	step [15/249], loss=112.9829
	step [16/249], loss=129.8689
	step [17/249], loss=130.1126
	step [18/249], loss=117.0303
	step [19/249], loss=109.5100
	step [20/249], loss=123.7130
	step [21/249], loss=121.3790
	step [22/249], loss=108.8915
	step [23/249], loss=114.0873
	step [24/249], loss=126.7629
	step [25/249], loss=127.5671
	step [26/249], loss=120.6353
	step [27/249], loss=114.5926
	step [28/249], loss=126.4809
	step [29/249], loss=134.2070
	step [30/249], loss=131.1543
	step [31/249], loss=116.6260
	step [32/249], loss=109.4377
	step [33/249], loss=127.4560
	step [34/249], loss=95.0193
	step [35/249], loss=109.2809
	step [36/249], loss=128.7641
	step [37/249], loss=114.3205
	step [38/249], loss=118.5134
	step [39/249], loss=127.5559
	step [40/249], loss=116.1491
	step [41/249], loss=127.2350
	step [42/249], loss=124.9157
	step [43/249], loss=108.6139
	step [44/249], loss=153.8890
	step [45/249], loss=132.5222
	step [46/249], loss=129.2145
	step [47/249], loss=110.8326
	step [48/249], loss=142.4763
	step [49/249], loss=122.2783
	step [50/249], loss=132.2348
	step [51/249], loss=114.9734
	step [52/249], loss=116.0795
	step [53/249], loss=131.3866
	step [54/249], loss=124.7860
	step [55/249], loss=133.2722
	step [56/249], loss=114.2490
	step [57/249], loss=140.0106
	step [58/249], loss=141.1252
	step [59/249], loss=112.1806
	step [60/249], loss=123.7045
	step [61/249], loss=113.2635
	step [62/249], loss=114.9336
	step [63/249], loss=124.7572
	step [64/249], loss=115.4126
	step [65/249], loss=123.4947
	step [66/249], loss=140.1537
	step [67/249], loss=127.1586
	step [68/249], loss=137.8085
	step [69/249], loss=134.2708
	step [70/249], loss=157.8027
	step [71/249], loss=114.4956
	step [72/249], loss=115.1894
	step [73/249], loss=113.5580
	step [74/249], loss=116.4221
	step [75/249], loss=97.2370
	step [76/249], loss=118.2146
	step [77/249], loss=119.1583
	step [78/249], loss=118.5750
	step [79/249], loss=118.0315
	step [80/249], loss=129.7222
	step [81/249], loss=123.5481
	step [82/249], loss=110.0970
	step [83/249], loss=116.0570
	step [84/249], loss=125.1101
	step [85/249], loss=126.7710
	step [86/249], loss=100.7113
	step [87/249], loss=123.4312
	step [88/249], loss=113.4551
	step [89/249], loss=133.8939
	step [90/249], loss=121.8684
	step [91/249], loss=118.4593
	step [92/249], loss=113.6206
	step [93/249], loss=128.8974
	step [94/249], loss=119.6709
	step [95/249], loss=136.5110
	step [96/249], loss=154.8164
	step [97/249], loss=119.6938
	step [98/249], loss=100.6221
	step [99/249], loss=118.3338
	step [100/249], loss=121.5062
	step [101/249], loss=130.1940
	step [102/249], loss=125.3016
	step [103/249], loss=118.0364
	step [104/249], loss=144.0427
	step [105/249], loss=124.5541
	step [106/249], loss=141.4804
	step [107/249], loss=122.2151
	step [108/249], loss=128.2826
	step [109/249], loss=112.5984
	step [110/249], loss=107.8980
	step [111/249], loss=123.4422
	step [112/249], loss=115.4098
	step [113/249], loss=106.6925
	step [114/249], loss=112.5026
	step [115/249], loss=117.1620
	step [116/249], loss=98.2911
	step [117/249], loss=117.4570
	step [118/249], loss=124.7291
	step [119/249], loss=117.2143
	step [120/249], loss=110.3570
	step [121/249], loss=135.3376
	step [122/249], loss=109.6680
	step [123/249], loss=109.0980
	step [124/249], loss=124.7495
	step [125/249], loss=121.6835
	step [126/249], loss=119.6957
	step [127/249], loss=128.8756
	step [128/249], loss=135.0959
	step [129/249], loss=141.9366
	step [130/249], loss=126.9669
	step [131/249], loss=113.3826
	step [132/249], loss=116.9262
	step [133/249], loss=133.5343
	step [134/249], loss=121.6224
	step [135/249], loss=141.4997
	step [136/249], loss=114.8988
	step [137/249], loss=132.5607
	step [138/249], loss=118.8420
	step [139/249], loss=120.8543
	step [140/249], loss=140.1655
	step [141/249], loss=112.8003
	step [142/249], loss=104.9730
	step [143/249], loss=118.8769
	step [144/249], loss=143.3862
	step [145/249], loss=118.7097
	step [146/249], loss=102.3465
	step [147/249], loss=112.5019
	step [148/249], loss=128.5125
	step [149/249], loss=118.0744
	step [150/249], loss=120.2404
	step [151/249], loss=117.8233
	step [152/249], loss=117.6847
	step [153/249], loss=118.9209
	step [154/249], loss=132.5443
	step [155/249], loss=109.4040
	step [156/249], loss=101.3379
	step [157/249], loss=118.7432
	step [158/249], loss=137.3800
	step [159/249], loss=103.1933
	step [160/249], loss=107.5048
	step [161/249], loss=139.3806
	step [162/249], loss=128.4637
	step [163/249], loss=132.5420
	step [164/249], loss=107.4043
	step [165/249], loss=108.4135
	step [166/249], loss=121.2071
	step [167/249], loss=124.3938
	step [168/249], loss=120.8118
	step [169/249], loss=126.1680
	step [170/249], loss=115.6112
	step [171/249], loss=128.2229
	step [172/249], loss=120.9878
	step [173/249], loss=127.5770
	step [174/249], loss=106.2386
	step [175/249], loss=110.3260
	step [176/249], loss=111.1056
	step [177/249], loss=122.2160
	step [178/249], loss=108.5266
	step [179/249], loss=128.0089
	step [180/249], loss=110.6709
	step [181/249], loss=120.4424
	step [182/249], loss=141.3540
	step [183/249], loss=109.7730
	step [184/249], loss=117.3788
	step [185/249], loss=137.1071
	step [186/249], loss=113.4144
	step [187/249], loss=106.0410
	step [188/249], loss=105.6302
	step [189/249], loss=115.7300
	step [190/249], loss=146.8388
	step [191/249], loss=112.5526
	step [192/249], loss=124.4371
	step [193/249], loss=123.8924
	step [194/249], loss=108.1065
	step [195/249], loss=128.0860
	step [196/249], loss=103.2823
	step [197/249], loss=124.2169
	step [198/249], loss=106.7761
	step [199/249], loss=116.2013
	step [200/249], loss=126.2177
	step [201/249], loss=128.3742
	step [202/249], loss=118.7157
	step [203/249], loss=118.8137
	step [204/249], loss=124.1847
	step [205/249], loss=136.2537
	step [206/249], loss=106.3069
	step [207/249], loss=125.6890
	step [208/249], loss=126.0479
	step [209/249], loss=119.1384
	step [210/249], loss=131.0068
	step [211/249], loss=119.0847
	step [212/249], loss=117.9502
	step [213/249], loss=128.3259
	step [214/249], loss=101.0362
	step [215/249], loss=120.6336
	step [216/249], loss=89.1623
	step [217/249], loss=128.2769
	step [218/249], loss=117.5031
	step [219/249], loss=96.2258
	step [220/249], loss=127.5472
	step [221/249], loss=123.8451
	step [222/249], loss=104.8369
	step [223/249], loss=112.5948
	step [224/249], loss=123.2082
	step [225/249], loss=134.8307
	step [226/249], loss=116.1641
	step [227/249], loss=114.1965
	step [228/249], loss=112.6107
	step [229/249], loss=109.6501
	step [230/249], loss=110.4001
	step [231/249], loss=110.6535
	step [232/249], loss=137.7245
	step [233/249], loss=151.0914
	step [234/249], loss=138.0233
	step [235/249], loss=113.5883
	step [236/249], loss=123.2423
	step [237/249], loss=109.3833
	step [238/249], loss=141.6737
	step [239/249], loss=110.7262
	step [240/249], loss=121.3356
	step [241/249], loss=123.8800
	step [242/249], loss=115.4740
	step [243/249], loss=125.5026
	step [244/249], loss=139.8648
	step [245/249], loss=117.8143
	step [246/249], loss=112.0538
	step [247/249], loss=124.5719
	step [248/249], loss=130.0490
	step [249/249], loss=93.0521
	Evaluating
	loss=0.0708, precision=0.3469, recall=0.8897, f1=0.4992
saving model as: 0_saved_model.pth
Training epoch 7
	step [1/249], loss=103.2828
	step [2/249], loss=112.8886
	step [3/249], loss=114.4546
	step [4/249], loss=113.4466
	step [5/249], loss=135.3991
	step [6/249], loss=127.9054
	step [7/249], loss=136.4408
	step [8/249], loss=107.4365
	step [9/249], loss=109.1166
	step [10/249], loss=129.7677
	step [11/249], loss=115.3242
	step [12/249], loss=114.9297
	step [13/249], loss=107.2116
	step [14/249], loss=121.6410
	step [15/249], loss=121.0385
	step [16/249], loss=106.5309
	step [17/249], loss=121.5386
	step [18/249], loss=132.5072
	step [19/249], loss=128.3318
	step [20/249], loss=116.9853
	step [21/249], loss=115.7329
	step [22/249], loss=119.9645
	step [23/249], loss=110.7702
	step [24/249], loss=104.2505
	step [25/249], loss=107.6817
	step [26/249], loss=126.4651
	step [27/249], loss=133.6400
	step [28/249], loss=121.0171
	step [29/249], loss=119.5041
	step [30/249], loss=124.4207
	step [31/249], loss=120.1361
	step [32/249], loss=117.8792
	step [33/249], loss=109.0732
	step [34/249], loss=120.4138
	step [35/249], loss=109.8918
	step [36/249], loss=118.0723
	step [37/249], loss=99.8366
	step [38/249], loss=106.7805
	step [39/249], loss=120.8392
	step [40/249], loss=98.6434
	step [41/249], loss=124.9055
	step [42/249], loss=117.6439
	step [43/249], loss=118.0741
	step [44/249], loss=114.9080
	step [45/249], loss=108.9839
	step [46/249], loss=101.2856
	step [47/249], loss=112.0514
	step [48/249], loss=122.8120
	step [49/249], loss=110.0228
	step [50/249], loss=104.6468
	step [51/249], loss=102.9235
	step [52/249], loss=109.2702
	step [53/249], loss=112.2607
	step [54/249], loss=101.7195
	step [55/249], loss=111.9368
	step [56/249], loss=117.7631
	step [57/249], loss=107.1124
	step [58/249], loss=91.6769
	step [59/249], loss=128.5426
	step [60/249], loss=107.3508
	step [61/249], loss=116.4661
	step [62/249], loss=116.3612
	step [63/249], loss=100.3396
	step [64/249], loss=115.9527
	step [65/249], loss=127.5531
	step [66/249], loss=114.8716
	step [67/249], loss=123.7520
	step [68/249], loss=128.2963
	step [69/249], loss=110.2956
	step [70/249], loss=112.1986
	step [71/249], loss=131.0720
	step [72/249], loss=133.9804
	step [73/249], loss=115.3960
	step [74/249], loss=125.6611
	step [75/249], loss=110.0710
	step [76/249], loss=124.1013
	step [77/249], loss=116.6746
	step [78/249], loss=138.1933
	step [79/249], loss=132.3452
	step [80/249], loss=113.7301
	step [81/249], loss=125.6447
	step [82/249], loss=110.2445
	step [83/249], loss=118.5313
	step [84/249], loss=118.1120
	step [85/249], loss=106.6560
	step [86/249], loss=123.5399
	step [87/249], loss=144.8447
	step [88/249], loss=146.6717
	step [89/249], loss=145.4779
	step [90/249], loss=134.7567
	step [91/249], loss=115.3743
	step [92/249], loss=127.4054
	step [93/249], loss=102.5406
	step [94/249], loss=110.4432
	step [95/249], loss=116.2216
	step [96/249], loss=115.7197
	step [97/249], loss=114.2098
	step [98/249], loss=127.1358
	step [99/249], loss=124.9549
	step [100/249], loss=101.3234
	step [101/249], loss=132.3018
	step [102/249], loss=91.4695
	step [103/249], loss=123.4728
	step [104/249], loss=113.5268
	step [105/249], loss=122.0792
	step [106/249], loss=109.4510
	step [107/249], loss=119.7043
	step [108/249], loss=124.9176
	step [109/249], loss=120.3596
	step [110/249], loss=135.7350
	step [111/249], loss=112.6264
	step [112/249], loss=117.6275
	step [113/249], loss=116.2311
	step [114/249], loss=102.9887
	step [115/249], loss=125.1884
	step [116/249], loss=140.0370
	step [117/249], loss=107.4041
	step [118/249], loss=141.6678
	step [119/249], loss=130.6736
	step [120/249], loss=115.3386
	step [121/249], loss=125.5380
	step [122/249], loss=132.5850
	step [123/249], loss=114.1511
	step [124/249], loss=139.1827
	step [125/249], loss=101.2163
	step [126/249], loss=103.9093
	step [127/249], loss=117.9518
	step [128/249], loss=107.6742
	step [129/249], loss=127.9877
	step [130/249], loss=111.2984
	step [131/249], loss=135.8744
	step [132/249], loss=126.0956
	step [133/249], loss=131.9059
	step [134/249], loss=111.3703
	step [135/249], loss=108.4704
	step [136/249], loss=114.8724
	step [137/249], loss=117.5990
	step [138/249], loss=112.2575
	step [139/249], loss=119.2694
	step [140/249], loss=123.4686
	step [141/249], loss=122.1466
	step [142/249], loss=115.4346
	step [143/249], loss=121.9681
	step [144/249], loss=118.3903
	step [145/249], loss=110.6592
	step [146/249], loss=106.6378
	step [147/249], loss=106.3466
	step [148/249], loss=116.2157
	step [149/249], loss=128.3929
	step [150/249], loss=95.0292
	step [151/249], loss=117.2325
	step [152/249], loss=110.4847
	step [153/249], loss=127.4874
	step [154/249], loss=122.6677
	step [155/249], loss=106.6841
	step [156/249], loss=127.2310
	step [157/249], loss=124.2324
	step [158/249], loss=126.1945
	step [159/249], loss=91.5490
	step [160/249], loss=111.3260
	step [161/249], loss=102.1344
	step [162/249], loss=131.7943
	step [163/249], loss=131.5153
	step [164/249], loss=106.4179
	step [165/249], loss=127.3734
	step [166/249], loss=115.4499
	step [167/249], loss=124.8487
	step [168/249], loss=120.9832
	step [169/249], loss=127.1752
	step [170/249], loss=120.9087
	step [171/249], loss=130.4234
	step [172/249], loss=110.2518
	step [173/249], loss=115.1968
	step [174/249], loss=108.2701
	step [175/249], loss=116.8239
	step [176/249], loss=126.1757
	step [177/249], loss=132.4448
	step [178/249], loss=124.4725
	step [179/249], loss=112.3901
	step [180/249], loss=122.1624
	step [181/249], loss=99.4965
	step [182/249], loss=102.2353
	step [183/249], loss=106.8313
	step [184/249], loss=134.2281
	step [185/249], loss=120.9603
	step [186/249], loss=110.4498
	step [187/249], loss=129.1353
	step [188/249], loss=141.1342
	step [189/249], loss=123.8607
	step [190/249], loss=104.0183
	step [191/249], loss=105.6590
	step [192/249], loss=114.0860
	step [193/249], loss=111.7536
	step [194/249], loss=109.1702
	step [195/249], loss=108.7472
	step [196/249], loss=113.0077
	step [197/249], loss=105.3076
	step [198/249], loss=121.3904
	step [199/249], loss=95.1350
	step [200/249], loss=104.9340
	step [201/249], loss=113.0770
	step [202/249], loss=129.5875
	step [203/249], loss=97.9633
	step [204/249], loss=108.1017
	step [205/249], loss=93.9795
	step [206/249], loss=93.2462
	step [207/249], loss=119.5483
	step [208/249], loss=107.1585
	step [209/249], loss=119.4121
	step [210/249], loss=108.8678
	step [211/249], loss=129.4498
	step [212/249], loss=142.5015
	step [213/249], loss=117.6938
	step [214/249], loss=99.4499
	step [215/249], loss=115.3155
	step [216/249], loss=116.4527
	step [217/249], loss=135.4915
	step [218/249], loss=100.5477
	step [219/249], loss=113.7278
	step [220/249], loss=110.2557
	step [221/249], loss=108.1785
	step [222/249], loss=136.3729
	step [223/249], loss=114.9542
	step [224/249], loss=117.9646
	step [225/249], loss=121.5947
	step [226/249], loss=103.4562
	step [227/249], loss=91.7308
	step [228/249], loss=97.9752
	step [229/249], loss=123.4798
	step [230/249], loss=117.0555
	step [231/249], loss=136.0784
	step [232/249], loss=104.6177
	step [233/249], loss=119.7070
	step [234/249], loss=108.9882
	step [235/249], loss=122.7503
	step [236/249], loss=118.6567
	step [237/249], loss=134.8347
	step [238/249], loss=121.4987
	step [239/249], loss=157.1631
	step [240/249], loss=114.9822
	step [241/249], loss=111.7550
	step [242/249], loss=121.7339
	step [243/249], loss=118.2294
	step [244/249], loss=109.0753
	step [245/249], loss=138.1408
	step [246/249], loss=109.1387
	step [247/249], loss=115.6697
	step [248/249], loss=95.7916
	step [249/249], loss=83.7425
	Evaluating
	loss=0.0556, precision=0.3044, recall=0.9309, f1=0.4588
Training epoch 8
	step [1/249], loss=103.8462
	step [2/249], loss=129.2469
	step [3/249], loss=130.5006
	step [4/249], loss=112.9319
	step [5/249], loss=115.4171
	step [6/249], loss=110.3383
	step [7/249], loss=125.0732
	step [8/249], loss=104.0543
	step [9/249], loss=112.9499
	step [10/249], loss=132.7728
	step [11/249], loss=110.3961
	step [12/249], loss=122.6614
	step [13/249], loss=133.0781
	step [14/249], loss=149.3160
	step [15/249], loss=118.8287
	step [16/249], loss=122.2299
	step [17/249], loss=137.7433
	step [18/249], loss=96.6349
	step [19/249], loss=112.1206
	step [20/249], loss=107.1789
	step [21/249], loss=115.4796
	step [22/249], loss=100.6584
	step [23/249], loss=107.0673
	step [24/249], loss=103.2364
	step [25/249], loss=126.1252
	step [26/249], loss=118.8405
	step [27/249], loss=134.0044
	step [28/249], loss=111.5087
	step [29/249], loss=109.0480
	step [30/249], loss=107.3472
	step [31/249], loss=131.2251
	step [32/249], loss=122.0583
	step [33/249], loss=97.7409
	step [34/249], loss=105.8125
	step [35/249], loss=99.2800
	step [36/249], loss=119.4351
	step [37/249], loss=115.0163
	step [38/249], loss=111.0558
	step [39/249], loss=97.8764
	step [40/249], loss=101.9150
	step [41/249], loss=120.3040
	step [42/249], loss=117.5319
	step [43/249], loss=111.3650
	step [44/249], loss=96.4964
	step [45/249], loss=133.6816
	step [46/249], loss=94.2977
	step [47/249], loss=99.8887
	step [48/249], loss=99.4739
	step [49/249], loss=99.5178
	step [50/249], loss=117.2040
	step [51/249], loss=111.0253
	step [52/249], loss=115.1307
	step [53/249], loss=120.3946
	step [54/249], loss=119.5230
	step [55/249], loss=132.5069
	step [56/249], loss=122.9316
	step [57/249], loss=111.3787
	step [58/249], loss=134.2666
	step [59/249], loss=118.9727
	step [60/249], loss=118.4787
	step [61/249], loss=102.2531
	step [62/249], loss=105.8190
	step [63/249], loss=105.9004
	step [64/249], loss=111.8754
	step [65/249], loss=131.8151
	step [66/249], loss=120.9595
	step [67/249], loss=106.8130
	step [68/249], loss=133.1872
	step [69/249], loss=120.0859
	step [70/249], loss=131.2130
	step [71/249], loss=112.4818
	step [72/249], loss=113.5513
	step [73/249], loss=137.3155
	step [74/249], loss=119.1820
	step [75/249], loss=112.8027
	step [76/249], loss=127.1066
	step [77/249], loss=131.6627
	step [78/249], loss=107.8848
	step [79/249], loss=129.7870
	step [80/249], loss=100.8710
	step [81/249], loss=127.1608
	step [82/249], loss=114.6750
	step [83/249], loss=117.0098
	step [84/249], loss=119.5162
	step [85/249], loss=108.3794
	step [86/249], loss=101.2963
	step [87/249], loss=127.1165
	step [88/249], loss=131.8645
	step [89/249], loss=100.5094
	step [90/249], loss=121.8215
	step [91/249], loss=105.0693
	step [92/249], loss=111.9236
	step [93/249], loss=123.2071
	step [94/249], loss=118.5779
	step [95/249], loss=108.0237
	step [96/249], loss=104.0431
	step [97/249], loss=123.9675
	step [98/249], loss=106.1803
	step [99/249], loss=102.0792
	step [100/249], loss=126.6548
	step [101/249], loss=110.8024
	step [102/249], loss=133.8449
	step [103/249], loss=127.1528
	step [104/249], loss=122.0409
	step [105/249], loss=114.8526
	step [106/249], loss=93.5048
	step [107/249], loss=108.9050
	step [108/249], loss=123.4348
	step [109/249], loss=107.0967
	step [110/249], loss=102.8726
	step [111/249], loss=105.0484
	step [112/249], loss=99.3203
	step [113/249], loss=101.1375
	step [114/249], loss=120.2366
	step [115/249], loss=103.9442
	step [116/249], loss=101.5466
	step [117/249], loss=102.2298
	step [118/249], loss=101.0879
	step [119/249], loss=107.0526
	step [120/249], loss=115.8292
	step [121/249], loss=124.2144
	step [122/249], loss=121.1796
	step [123/249], loss=108.3192
	step [124/249], loss=113.3547
	step [125/249], loss=122.5567
	step [126/249], loss=124.2950
	step [127/249], loss=110.2251
	step [128/249], loss=101.0977
	step [129/249], loss=126.6601
	step [130/249], loss=113.2817
	step [131/249], loss=122.4906
	step [132/249], loss=107.8098
	step [133/249], loss=97.0920
	step [134/249], loss=117.7209
	step [135/249], loss=118.1059
	step [136/249], loss=109.4770
	step [137/249], loss=113.4549
	step [138/249], loss=121.8739
	step [139/249], loss=116.3237
	step [140/249], loss=123.3469
	step [141/249], loss=121.7466
	step [142/249], loss=129.7108
	step [143/249], loss=106.7864
	step [144/249], loss=136.5580
	step [145/249], loss=110.4400
	step [146/249], loss=117.9354
	step [147/249], loss=113.8197
	step [148/249], loss=101.8940
	step [149/249], loss=86.0670
	step [150/249], loss=120.0458
	step [151/249], loss=135.7446
	step [152/249], loss=124.9105
	step [153/249], loss=125.2763
	step [154/249], loss=138.7965
	step [155/249], loss=124.6573
	step [156/249], loss=110.4662
	step [157/249], loss=108.7836
	step [158/249], loss=102.2383
	step [159/249], loss=118.5961
	step [160/249], loss=117.4743
	step [161/249], loss=128.9074
	step [162/249], loss=113.9599
	step [163/249], loss=101.5375
	step [164/249], loss=100.9122
	step [165/249], loss=107.2963
	step [166/249], loss=116.4053
	step [167/249], loss=111.6171
	step [168/249], loss=95.7162
	step [169/249], loss=127.9716
	step [170/249], loss=111.5969
	step [171/249], loss=112.2639
	step [172/249], loss=114.4159
	step [173/249], loss=123.1138
	step [174/249], loss=103.8841
	step [175/249], loss=111.7697
	step [176/249], loss=121.7592
	step [177/249], loss=114.5902
	step [178/249], loss=99.6448
	step [179/249], loss=105.3480
	step [180/249], loss=135.4609
	step [181/249], loss=130.9735
	step [182/249], loss=114.5369
	step [183/249], loss=110.5049
	step [184/249], loss=103.4266
	step [185/249], loss=119.2961
	step [186/249], loss=109.1413
	step [187/249], loss=101.7528
	step [188/249], loss=100.2464
	step [189/249], loss=113.0904
	step [190/249], loss=104.8464
	step [191/249], loss=126.4083
	step [192/249], loss=109.0354
	step [193/249], loss=115.7287
	step [194/249], loss=114.2103
	step [195/249], loss=114.1620
	step [196/249], loss=110.8870
	step [197/249], loss=114.5191
	step [198/249], loss=122.4981
	step [199/249], loss=128.1604
	step [200/249], loss=93.3202
	step [201/249], loss=100.9753
	step [202/249], loss=110.5776
	step [203/249], loss=126.7099
	step [204/249], loss=103.3160
	step [205/249], loss=104.4180
	step [206/249], loss=122.4645
	step [207/249], loss=128.4980
	step [208/249], loss=121.1842
	step [209/249], loss=98.8016
	step [210/249], loss=100.6948
	step [211/249], loss=116.4908
	step [212/249], loss=118.2542
	step [213/249], loss=121.5836
	step [214/249], loss=87.2139
	step [215/249], loss=107.8599
	step [216/249], loss=120.2356
	step [217/249], loss=121.1086
	step [218/249], loss=109.9243
	step [219/249], loss=107.8657
	step [220/249], loss=107.4306
	step [221/249], loss=103.8666
	step [222/249], loss=113.2388
	step [223/249], loss=108.5043
	step [224/249], loss=108.1772
	step [225/249], loss=114.2788
	step [226/249], loss=110.3981
	step [227/249], loss=100.7067
	step [228/249], loss=118.3901
	step [229/249], loss=129.8679
	step [230/249], loss=97.9850
	step [231/249], loss=124.3134
	step [232/249], loss=99.3878
	step [233/249], loss=115.8678
	step [234/249], loss=97.6621
	step [235/249], loss=114.6197
	step [236/249], loss=114.3070
	step [237/249], loss=119.2380
	step [238/249], loss=120.8671
	step [239/249], loss=95.7360
	step [240/249], loss=98.3002
	step [241/249], loss=105.6927
	step [242/249], loss=109.7796
	step [243/249], loss=126.0927
	step [244/249], loss=105.8964
	step [245/249], loss=132.3892
	step [246/249], loss=118.2222
	step [247/249], loss=110.6281
	step [248/249], loss=87.7029
	step [249/249], loss=91.5144
	Evaluating
	loss=0.0462, precision=0.3253, recall=0.8940, f1=0.4770
Training epoch 9
	step [1/249], loss=124.2321
	step [2/249], loss=148.6618
	step [3/249], loss=99.5857
	step [4/249], loss=102.5121
	step [5/249], loss=106.8756
	step [6/249], loss=103.4648
	step [7/249], loss=84.4482
	step [8/249], loss=112.1126
	step [9/249], loss=119.9549
	step [10/249], loss=111.7279
	step [11/249], loss=103.6197
	step [12/249], loss=95.9195
	step [13/249], loss=116.7828
	step [14/249], loss=121.3215
	step [15/249], loss=100.1118
	step [16/249], loss=105.7478
	step [17/249], loss=107.9487
	step [18/249], loss=100.7023
	step [19/249], loss=122.3752
	step [20/249], loss=122.8195
	step [21/249], loss=110.1758
	step [22/249], loss=125.2306
	step [23/249], loss=103.7477
	step [24/249], loss=96.2363
	step [25/249], loss=142.2075
	step [26/249], loss=119.3523
	step [27/249], loss=111.9149
	step [28/249], loss=130.8186
	step [29/249], loss=107.8530
	step [30/249], loss=109.8775
	step [31/249], loss=116.3270
	step [32/249], loss=99.4256
	step [33/249], loss=102.2740
	step [34/249], loss=96.9030
	step [35/249], loss=96.0391
	step [36/249], loss=116.1110
	step [37/249], loss=120.1322
	step [38/249], loss=122.8299
	step [39/249], loss=100.5683
	step [40/249], loss=116.0408
	step [41/249], loss=111.1811
	step [42/249], loss=127.8479
	step [43/249], loss=102.2915
	step [44/249], loss=108.3004
	step [45/249], loss=115.7469
	step [46/249], loss=102.2599
	step [47/249], loss=117.1254
	step [48/249], loss=114.9756
	step [49/249], loss=121.4431
	step [50/249], loss=124.6386
	step [51/249], loss=118.5991
	step [52/249], loss=101.5856
	step [53/249], loss=107.6615
	step [54/249], loss=108.6423
	step [55/249], loss=110.4111
	step [56/249], loss=114.7651
	step [57/249], loss=112.1068
	step [58/249], loss=126.2062
	step [59/249], loss=143.6786
	step [60/249], loss=104.3368
	step [61/249], loss=104.6285
	step [62/249], loss=111.3357
	step [63/249], loss=138.2113
	step [64/249], loss=96.7705
	step [65/249], loss=121.1734
	step [66/249], loss=109.6739
	step [67/249], loss=120.9524
	step [68/249], loss=110.7071
	step [69/249], loss=109.3947
	step [70/249], loss=113.3130
	step [71/249], loss=78.6299
	step [72/249], loss=120.3503
	step [73/249], loss=126.6960
	step [74/249], loss=111.9960
	step [75/249], loss=125.5290
	step [76/249], loss=100.2321
	step [77/249], loss=121.1639
	step [78/249], loss=109.3615
	step [79/249], loss=102.0611
	step [80/249], loss=113.4762
	step [81/249], loss=99.0422
	step [82/249], loss=111.2856
	step [83/249], loss=110.2368
	step [84/249], loss=101.7999
	step [85/249], loss=109.9365
	step [86/249], loss=111.2570
	step [87/249], loss=115.0404
	step [88/249], loss=117.4445
	step [89/249], loss=125.5309
	step [90/249], loss=110.7958
	step [91/249], loss=113.9158
	step [92/249], loss=127.0024
	step [93/249], loss=122.4022
	step [94/249], loss=122.4794
	step [95/249], loss=108.5760
	step [96/249], loss=119.2744
	step [97/249], loss=109.5558
	step [98/249], loss=140.6856
	step [99/249], loss=113.7639
	step [100/249], loss=113.1764
	step [101/249], loss=119.5950
	step [102/249], loss=125.0516
	step [103/249], loss=109.7507
	step [104/249], loss=112.3316
	step [105/249], loss=124.4297
	step [106/249], loss=113.8276
	step [107/249], loss=123.9119
	step [108/249], loss=121.4221
	step [109/249], loss=108.8881
	step [110/249], loss=115.6764
	step [111/249], loss=126.8804
	step [112/249], loss=121.2153
	step [113/249], loss=97.8824
	step [114/249], loss=104.5328
	step [115/249], loss=129.5127
	step [116/249], loss=116.1161
	step [117/249], loss=106.1464
	step [118/249], loss=135.6180
	step [119/249], loss=122.8863
	step [120/249], loss=111.2750
	step [121/249], loss=106.1965
	step [122/249], loss=108.7889
	step [123/249], loss=105.7243
	step [124/249], loss=113.3687
	step [125/249], loss=127.0090
	step [126/249], loss=103.7459
	step [127/249], loss=94.0993
	step [128/249], loss=105.6446
	step [129/249], loss=90.9820
	step [130/249], loss=97.3194
	step [131/249], loss=110.7393
	step [132/249], loss=110.0295
	step [133/249], loss=129.2953
	step [134/249], loss=91.2154
	step [135/249], loss=106.8515
	step [136/249], loss=88.3506
	step [137/249], loss=104.7102
	step [138/249], loss=112.9587
	step [139/249], loss=118.9614
	step [140/249], loss=113.0083
	step [141/249], loss=101.4771
	step [142/249], loss=97.6513
	step [143/249], loss=124.6642
	step [144/249], loss=99.7035
	step [145/249], loss=109.0433
	step [146/249], loss=107.2849
	step [147/249], loss=103.4766
	step [148/249], loss=106.5188
	step [149/249], loss=113.3670
	step [150/249], loss=105.8215
	step [151/249], loss=105.3341
	step [152/249], loss=105.1071
	step [153/249], loss=88.8018
	step [154/249], loss=104.2427
	step [155/249], loss=111.3529
	step [156/249], loss=101.6752
	step [157/249], loss=104.6453
	step [158/249], loss=113.3199
	step [159/249], loss=97.9713
	step [160/249], loss=117.2372
	step [161/249], loss=113.7490
	step [162/249], loss=90.2098
	step [163/249], loss=121.1647
	step [164/249], loss=108.6207
	step [165/249], loss=100.1110
	step [166/249], loss=118.6886
	step [167/249], loss=105.7102
	step [168/249], loss=101.5131
	step [169/249], loss=120.4965
	step [170/249], loss=110.8026
	step [171/249], loss=116.0113
	step [172/249], loss=100.4031
	step [173/249], loss=121.1317
	step [174/249], loss=114.6760
	step [175/249], loss=115.1353
	step [176/249], loss=107.8436
	step [177/249], loss=125.7402
	step [178/249], loss=112.9481
	step [179/249], loss=119.6493
	step [180/249], loss=111.3880
	step [181/249], loss=111.2740
	step [182/249], loss=103.8345
	step [183/249], loss=119.8212
	step [184/249], loss=142.0466
	step [185/249], loss=120.0006
	step [186/249], loss=113.1422
	step [187/249], loss=96.1219
	step [188/249], loss=109.1022
	step [189/249], loss=113.4279
	step [190/249], loss=102.4136
	step [191/249], loss=119.9514
	step [192/249], loss=97.0996
	step [193/249], loss=91.0182
	step [194/249], loss=143.3190
	step [195/249], loss=113.9119
	step [196/249], loss=108.6147
	step [197/249], loss=100.2352
	step [198/249], loss=92.7754
	step [199/249], loss=138.1437
	step [200/249], loss=109.7950
	step [201/249], loss=113.6464
	step [202/249], loss=112.7965
	step [203/249], loss=122.0135
	step [204/249], loss=109.1434
	step [205/249], loss=98.6368
	step [206/249], loss=135.3108
	step [207/249], loss=101.5187
	step [208/249], loss=126.5583
	step [209/249], loss=92.8646
	step [210/249], loss=102.6581
	step [211/249], loss=114.9405
	step [212/249], loss=109.9429
	step [213/249], loss=112.7666
	step [214/249], loss=124.6283
	step [215/249], loss=99.6197
	step [216/249], loss=126.9766
	step [217/249], loss=132.2498
	step [218/249], loss=112.6449
	step [219/249], loss=92.3159
	step [220/249], loss=115.1372
	step [221/249], loss=119.4252
	step [222/249], loss=106.2489
	step [223/249], loss=122.3034
	step [224/249], loss=122.0181
	step [225/249], loss=114.6815
	step [226/249], loss=91.6435
	step [227/249], loss=108.0136
	step [228/249], loss=110.3564
	step [229/249], loss=91.4951
	step [230/249], loss=119.1993
	step [231/249], loss=101.2930
	step [232/249], loss=114.9963
	step [233/249], loss=112.7043
	step [234/249], loss=121.0422
	step [235/249], loss=122.3623
	step [236/249], loss=105.7029
	step [237/249], loss=101.9764
	step [238/249], loss=93.9995
	step [239/249], loss=124.0265
	step [240/249], loss=108.1132
	step [241/249], loss=108.3826
	step [242/249], loss=110.7326
	step [243/249], loss=89.7048
	step [244/249], loss=107.8043
	step [245/249], loss=107.0894
	step [246/249], loss=100.1293
	step [247/249], loss=105.6005
	step [248/249], loss=112.2634
	step [249/249], loss=90.4484
	Evaluating
	loss=0.0362, precision=0.3360, recall=0.9390, f1=0.4949
Training epoch 10
	step [1/249], loss=111.0076
	step [2/249], loss=113.5972
	step [3/249], loss=92.5563
	step [4/249], loss=112.5102
	step [5/249], loss=107.5217
	step [6/249], loss=88.4993
	step [7/249], loss=114.4170
	step [8/249], loss=103.7871
	step [9/249], loss=94.0643
	step [10/249], loss=101.8680
	step [11/249], loss=122.1040
	step [12/249], loss=121.7237
	step [13/249], loss=110.7576
	step [14/249], loss=101.9144
	step [15/249], loss=97.9850
	step [16/249], loss=125.0779
	step [17/249], loss=100.7851
	step [18/249], loss=114.9707
	step [19/249], loss=101.7858
	step [20/249], loss=117.0149
	step [21/249], loss=117.3726
	step [22/249], loss=118.4660
	step [23/249], loss=120.8415
	step [24/249], loss=107.6055
	step [25/249], loss=124.3516
	step [26/249], loss=124.4633
	step [27/249], loss=102.2098
	step [28/249], loss=111.8182
	step [29/249], loss=122.6347
	step [30/249], loss=119.9476
	step [31/249], loss=134.6351
	step [32/249], loss=109.8353
	step [33/249], loss=114.8163
	step [34/249], loss=100.3219
	step [35/249], loss=93.9824
	step [36/249], loss=103.1679
	step [37/249], loss=105.6010
	step [38/249], loss=93.5854
	step [39/249], loss=100.1882
	step [40/249], loss=139.5665
	step [41/249], loss=105.7934
	step [42/249], loss=108.4609
	step [43/249], loss=97.6182
	step [44/249], loss=119.9489
	step [45/249], loss=122.2780
	step [46/249], loss=118.9632
	step [47/249], loss=121.0082
	step [48/249], loss=97.6151
	step [49/249], loss=98.4969
	step [50/249], loss=122.2737
	step [51/249], loss=86.0924
	step [52/249], loss=111.0342
	step [53/249], loss=132.9524
	step [54/249], loss=113.3288
	step [55/249], loss=102.4636
	step [56/249], loss=132.8704
	step [57/249], loss=115.1821
	step [58/249], loss=121.0004
	step [59/249], loss=98.0538
	step [60/249], loss=119.1752
	step [61/249], loss=126.4630
	step [62/249], loss=96.3576
	step [63/249], loss=114.6517
	step [64/249], loss=100.9388
	step [65/249], loss=109.6955
	step [66/249], loss=115.8389
	step [67/249], loss=104.5463
	step [68/249], loss=117.3621
	step [69/249], loss=106.9415
	step [70/249], loss=96.4208
	step [71/249], loss=113.6068
	step [72/249], loss=108.3100
	step [73/249], loss=114.8165
	step [74/249], loss=96.6013
	step [75/249], loss=112.2656
	step [76/249], loss=111.4601
	step [77/249], loss=111.5134
	step [78/249], loss=110.2510
	step [79/249], loss=116.0437
	step [80/249], loss=107.7151
	step [81/249], loss=106.1816
	step [82/249], loss=100.8911
	step [83/249], loss=83.8407
	step [84/249], loss=135.1797
	step [85/249], loss=86.3736
	step [86/249], loss=127.5593
	step [87/249], loss=107.4094
	step [88/249], loss=100.4698
	step [89/249], loss=109.5702
	step [90/249], loss=104.1621
	step [91/249], loss=109.3114
	step [92/249], loss=116.0603
	step [93/249], loss=104.5682
	step [94/249], loss=120.7127
	step [95/249], loss=116.9710
	step [96/249], loss=105.3062
	step [97/249], loss=98.0933
	step [98/249], loss=120.2542
	step [99/249], loss=95.8982
	step [100/249], loss=88.6451
	step [101/249], loss=134.3565
	step [102/249], loss=87.8246
	step [103/249], loss=107.5154
	step [104/249], loss=119.6247
	step [105/249], loss=121.6277
	step [106/249], loss=98.3009
	step [107/249], loss=96.2939
	step [108/249], loss=88.5357
	step [109/249], loss=110.0024
	step [110/249], loss=109.3476
	step [111/249], loss=139.4522
	step [112/249], loss=102.6964
	step [113/249], loss=125.5200
	step [114/249], loss=118.5777
	step [115/249], loss=125.4599
	step [116/249], loss=97.9280
	step [117/249], loss=102.5267
	step [118/249], loss=122.5240
	step [119/249], loss=119.4519
	step [120/249], loss=118.0049
	step [121/249], loss=105.8477
	step [122/249], loss=97.3292
	step [123/249], loss=100.4583
	step [124/249], loss=118.9961
	step [125/249], loss=104.2482
	step [126/249], loss=125.3356
	step [127/249], loss=97.7544
	step [128/249], loss=110.7299
	step [129/249], loss=119.7590
	step [130/249], loss=105.5403
	step [131/249], loss=116.9473
	step [132/249], loss=116.3696
	step [133/249], loss=106.3551
	step [134/249], loss=95.3392
	step [135/249], loss=121.7100
	step [136/249], loss=127.7731
	step [137/249], loss=117.3900
	step [138/249], loss=116.0950
	step [139/249], loss=99.9230
	step [140/249], loss=104.4118
	step [141/249], loss=93.5701
	step [142/249], loss=106.6340
	step [143/249], loss=102.9574
	step [144/249], loss=119.9060
	step [145/249], loss=112.7466
	step [146/249], loss=108.1397
	step [147/249], loss=104.8458
	step [148/249], loss=109.9133
	step [149/249], loss=103.6290
	step [150/249], loss=127.4154
	step [151/249], loss=102.4391
	step [152/249], loss=119.8123
	step [153/249], loss=94.9488
	step [154/249], loss=97.1352
	step [155/249], loss=107.3222
	step [156/249], loss=124.3243
	step [157/249], loss=114.6310
	step [158/249], loss=111.2654
	step [159/249], loss=128.6730
	step [160/249], loss=123.3622
	step [161/249], loss=117.0408
	step [162/249], loss=75.9218
	step [163/249], loss=97.9219
	step [164/249], loss=100.1786
	step [165/249], loss=107.4794
	step [166/249], loss=123.8047
	step [167/249], loss=116.0651
	step [168/249], loss=94.3435
	step [169/249], loss=107.5165
	step [170/249], loss=99.1345
	step [171/249], loss=107.3704
	step [172/249], loss=122.8787
	step [173/249], loss=92.3169
	step [174/249], loss=116.5715
	step [175/249], loss=114.1816
	step [176/249], loss=116.9300
	step [177/249], loss=89.5892
	step [178/249], loss=114.8504
	step [179/249], loss=121.0648
	step [180/249], loss=98.9991
	step [181/249], loss=142.8824
	step [182/249], loss=108.4465
	step [183/249], loss=106.4991
	step [184/249], loss=87.6690
	step [185/249], loss=114.1626
	step [186/249], loss=110.0235
	step [187/249], loss=107.6422
	step [188/249], loss=102.3398
	step [189/249], loss=109.0480
	step [190/249], loss=90.8316
	step [191/249], loss=98.9833
	step [192/249], loss=108.3538
	step [193/249], loss=99.4764
	step [194/249], loss=119.1876
	step [195/249], loss=112.3172
	step [196/249], loss=94.6525
	step [197/249], loss=115.7659
	step [198/249], loss=85.3741
	step [199/249], loss=103.9207
	step [200/249], loss=136.4265
	step [201/249], loss=110.7581
	step [202/249], loss=131.1766
	step [203/249], loss=95.3559
	step [204/249], loss=118.0479
	step [205/249], loss=75.7976
	step [206/249], loss=117.5011
	step [207/249], loss=115.6141
	step [208/249], loss=134.5217
	step [209/249], loss=109.0381
	step [210/249], loss=117.1391
	step [211/249], loss=119.2874
	step [212/249], loss=105.1363
	step [213/249], loss=105.0999
	step [214/249], loss=100.5525
	step [215/249], loss=113.3172
	step [216/249], loss=111.6167
	step [217/249], loss=97.7717
	step [218/249], loss=121.5673
	step [219/249], loss=88.7093
	step [220/249], loss=114.8669
	step [221/249], loss=105.7355
	step [222/249], loss=117.6968
	step [223/249], loss=122.6343
	step [224/249], loss=95.0267
	step [225/249], loss=116.6996
	step [226/249], loss=97.1469
	step [227/249], loss=90.3063
	step [228/249], loss=112.6962
	step [229/249], loss=123.5403
	step [230/249], loss=107.2323
	step [231/249], loss=100.6599
	step [232/249], loss=97.1906
	step [233/249], loss=99.7222
	step [234/249], loss=91.0550
	step [235/249], loss=117.5114
	step [236/249], loss=104.5046
	step [237/249], loss=100.9310
	step [238/249], loss=84.7951
	step [239/249], loss=105.5823
	step [240/249], loss=111.2089
	step [241/249], loss=98.6818
	step [242/249], loss=122.4172
	step [243/249], loss=98.6302
	step [244/249], loss=124.3452
	step [245/249], loss=109.0406
	step [246/249], loss=107.9677
	step [247/249], loss=115.0606
	step [248/249], loss=123.1639
	step [249/249], loss=63.0857
	Evaluating
	loss=0.0283, precision=0.3628, recall=0.9357, f1=0.5229
saving model as: 0_saved_model.pth
Training epoch 11
	step [1/249], loss=105.7739
	step [2/249], loss=114.0378
	step [3/249], loss=128.6408
	step [4/249], loss=100.8237
	step [5/249], loss=107.8376
	step [6/249], loss=116.8697
	step [7/249], loss=91.0573
	step [8/249], loss=86.8835
	step [9/249], loss=106.3879
	step [10/249], loss=119.1074
	step [11/249], loss=127.5185
	step [12/249], loss=102.3931
	step [13/249], loss=85.3597
	step [14/249], loss=111.1449
	step [15/249], loss=108.0571
	step [16/249], loss=96.0498
	step [17/249], loss=102.8501
	step [18/249], loss=111.4979
	step [19/249], loss=110.7919
	step [20/249], loss=99.1599
	step [21/249], loss=106.5165
	step [22/249], loss=110.2816
	step [23/249], loss=110.7888
	step [24/249], loss=119.6500
	step [25/249], loss=96.5044
	step [26/249], loss=85.5730
	step [27/249], loss=113.5960
	step [28/249], loss=104.6306
	step [29/249], loss=106.7156
	step [30/249], loss=111.6723
	step [31/249], loss=107.0034
	step [32/249], loss=98.8811
	step [33/249], loss=122.7349
	step [34/249], loss=97.6835
	step [35/249], loss=102.9736
	step [36/249], loss=97.8177
	step [37/249], loss=128.8131
	step [38/249], loss=109.3542
	step [39/249], loss=109.4438
	step [40/249], loss=100.5322
	step [41/249], loss=118.6504
	step [42/249], loss=105.1291
	step [43/249], loss=81.1129
	step [44/249], loss=98.8845
	step [45/249], loss=100.6374
	step [46/249], loss=107.3151
	step [47/249], loss=116.7301
	step [48/249], loss=96.1317
	step [49/249], loss=109.9905
	step [50/249], loss=108.0509
	step [51/249], loss=119.5307
	step [52/249], loss=117.7743
	step [53/249], loss=102.7126
	step [54/249], loss=107.2251
	step [55/249], loss=102.7722
	step [56/249], loss=105.7991
	step [57/249], loss=118.5320
	step [58/249], loss=105.8711
	step [59/249], loss=106.1241
	step [60/249], loss=109.5259
	step [61/249], loss=100.0323
	step [62/249], loss=93.3609
	step [63/249], loss=96.1887
	step [64/249], loss=92.0714
	step [65/249], loss=104.9251
	step [66/249], loss=108.9434
	step [67/249], loss=125.0741
	step [68/249], loss=110.0536
	step [69/249], loss=93.7674
	step [70/249], loss=123.0098
	step [71/249], loss=116.2025
	step [72/249], loss=89.7444
	step [73/249], loss=119.4068
	step [74/249], loss=98.4371
	step [75/249], loss=105.8564
	step [76/249], loss=98.7638
	step [77/249], loss=127.4686
	step [78/249], loss=128.5641
	step [79/249], loss=100.8888
	step [80/249], loss=116.4261
	step [81/249], loss=90.8012
	step [82/249], loss=123.8884
	step [83/249], loss=113.3154
	step [84/249], loss=110.6815
	step [85/249], loss=97.3718
	step [86/249], loss=113.0333
	step [87/249], loss=100.4779
	step [88/249], loss=118.9003
	step [89/249], loss=123.1422
	step [90/249], loss=127.8507
	step [91/249], loss=95.3299
	step [92/249], loss=103.4902
	step [93/249], loss=110.4569
	step [94/249], loss=124.9602
	step [95/249], loss=107.3136
	step [96/249], loss=106.5378
	step [97/249], loss=83.4841
	step [98/249], loss=100.0270
	step [99/249], loss=104.3848
	step [100/249], loss=114.3703
	step [101/249], loss=102.4755
	step [102/249], loss=124.8745
	step [103/249], loss=113.2665
	step [104/249], loss=86.5671
	step [105/249], loss=105.5987
	step [106/249], loss=112.6930
	step [107/249], loss=103.5744
	step [108/249], loss=107.3627
	step [109/249], loss=108.2866
	step [110/249], loss=123.8873
	step [111/249], loss=115.8616
	step [112/249], loss=100.2610
	step [113/249], loss=119.6814
	step [114/249], loss=110.2227
	step [115/249], loss=120.4438
	step [116/249], loss=98.7386
	step [117/249], loss=108.2872
	step [118/249], loss=114.8713
	step [119/249], loss=112.4456
	step [120/249], loss=102.1578
	step [121/249], loss=102.1373
	step [122/249], loss=137.2863
	step [123/249], loss=104.5309
	step [124/249], loss=105.8707
	step [125/249], loss=114.0330
	step [126/249], loss=105.4448
	step [127/249], loss=102.9599
	step [128/249], loss=122.7426
	step [129/249], loss=105.8805
	step [130/249], loss=117.9818
	step [131/249], loss=98.1342
	step [132/249], loss=115.0630
	step [133/249], loss=111.9689
	step [134/249], loss=111.2278
	step [135/249], loss=119.7355
	step [136/249], loss=105.4056
	step [137/249], loss=120.7810
	step [138/249], loss=109.7973
	step [139/249], loss=110.6429
	step [140/249], loss=120.6709
	step [141/249], loss=86.7833
	step [142/249], loss=106.0191
	step [143/249], loss=85.8082
	step [144/249], loss=130.4380
	step [145/249], loss=112.9607
	step [146/249], loss=104.9184
	step [147/249], loss=101.9032
	step [148/249], loss=123.8037
	step [149/249], loss=100.4442
	step [150/249], loss=123.8029
	step [151/249], loss=103.6783
	step [152/249], loss=80.0707
	step [153/249], loss=100.7158
	step [154/249], loss=106.2039
	step [155/249], loss=104.8532
	step [156/249], loss=114.5143
	step [157/249], loss=106.9529
	step [158/249], loss=118.8038
	step [159/249], loss=109.7919
	step [160/249], loss=116.8574
	step [161/249], loss=124.9146
	step [162/249], loss=129.1911
	step [163/249], loss=117.1012
	step [164/249], loss=126.8152
	step [165/249], loss=111.5905
	step [166/249], loss=112.4501
	step [167/249], loss=96.2904
	step [168/249], loss=106.5637
	step [169/249], loss=105.1960
	step [170/249], loss=108.1479
	step [171/249], loss=108.6236
	step [172/249], loss=94.3140
	step [173/249], loss=124.8072
	step [174/249], loss=130.5013
	step [175/249], loss=115.9165
	step [176/249], loss=118.9440
	step [177/249], loss=112.1976
	step [178/249], loss=105.1221
	step [179/249], loss=107.3814
	step [180/249], loss=105.5643
	step [181/249], loss=100.9176
	step [182/249], loss=93.9620
	step [183/249], loss=85.9571
	step [184/249], loss=129.8385
	step [185/249], loss=112.7556
	step [186/249], loss=91.6612
	step [187/249], loss=114.4550
	step [188/249], loss=111.1401
	step [189/249], loss=95.2893
	step [190/249], loss=101.0626
	step [191/249], loss=104.3013
	step [192/249], loss=116.0265
	step [193/249], loss=98.1218
	step [194/249], loss=100.9299
	step [195/249], loss=108.7332
	step [196/249], loss=99.3416
	step [197/249], loss=93.5183
	step [198/249], loss=108.9075
	step [199/249], loss=115.5363
	step [200/249], loss=98.8543
	step [201/249], loss=118.4491
	step [202/249], loss=94.7914
	step [203/249], loss=108.4506
	step [204/249], loss=124.4340
	step [205/249], loss=100.2521
	step [206/249], loss=89.0335
	step [207/249], loss=97.2474
	step [208/249], loss=105.0215
	step [209/249], loss=132.5471
	step [210/249], loss=110.6451
	step [211/249], loss=101.8294
	step [212/249], loss=124.8746
	step [213/249], loss=104.6381
	step [214/249], loss=114.3596
	step [215/249], loss=115.3549
	step [216/249], loss=106.4419
	step [217/249], loss=107.5224
	step [218/249], loss=111.8739
	step [219/249], loss=107.1871
	step [220/249], loss=113.9463
	step [221/249], loss=107.9227
	step [222/249], loss=89.8780
	step [223/249], loss=108.4660
	step [224/249], loss=97.9347
	step [225/249], loss=104.5032
	step [226/249], loss=108.8028
	step [227/249], loss=102.3056
	step [228/249], loss=112.3443
	step [229/249], loss=100.6653
	step [230/249], loss=103.5700
	step [231/249], loss=119.0771
	step [232/249], loss=88.1868
	step [233/249], loss=129.1645
	step [234/249], loss=103.5406
	step [235/249], loss=103.8199
	step [236/249], loss=98.4375
	step [237/249], loss=111.6180
	step [238/249], loss=105.6829
	step [239/249], loss=123.9786
	step [240/249], loss=91.5961
	step [241/249], loss=113.1107
	step [242/249], loss=101.6365
	step [243/249], loss=103.8795
	step [244/249], loss=111.4603
	step [245/249], loss=99.4714
	step [246/249], loss=105.1627
	step [247/249], loss=101.9334
	step [248/249], loss=102.1675
	step [249/249], loss=72.9741
	Evaluating
	loss=0.0265, precision=0.3503, recall=0.9227, f1=0.5078
Training epoch 12
	step [1/249], loss=99.6553
	step [2/249], loss=114.2909
	step [3/249], loss=95.1221
	step [4/249], loss=107.4193
	step [5/249], loss=108.0787
	step [6/249], loss=116.7628
	step [7/249], loss=109.6672
	step [8/249], loss=117.8373
	step [9/249], loss=116.6954
	step [10/249], loss=98.7121
	step [11/249], loss=108.2417
	step [12/249], loss=99.6468
	step [13/249], loss=120.6630
	step [14/249], loss=101.9379
	step [15/249], loss=103.3496
	step [16/249], loss=84.4699
	step [17/249], loss=110.9396
	step [18/249], loss=109.0815
	step [19/249], loss=115.7420
	step [20/249], loss=114.4324
	step [21/249], loss=102.1124
	step [22/249], loss=95.9125
	step [23/249], loss=99.3456
	step [24/249], loss=98.7138
	step [25/249], loss=97.4161
	step [26/249], loss=119.0984
	step [27/249], loss=113.5010
	step [28/249], loss=97.5185
	step [29/249], loss=88.4292
	step [30/249], loss=114.3896
	step [31/249], loss=99.5626
	step [32/249], loss=93.9837
	step [33/249], loss=101.1513
	step [34/249], loss=85.6976
	step [35/249], loss=125.8174
	step [36/249], loss=113.6361
	step [37/249], loss=101.6462
	step [38/249], loss=109.8810
	step [39/249], loss=97.1339
	step [40/249], loss=106.7345
	step [41/249], loss=125.4992
	step [42/249], loss=113.0855
	step [43/249], loss=106.9897
	step [44/249], loss=112.3674
	step [45/249], loss=128.7933
	step [46/249], loss=111.3564
	step [47/249], loss=96.8346
	step [48/249], loss=83.1348
	step [49/249], loss=106.7621
	step [50/249], loss=131.0140
	step [51/249], loss=92.1310
	step [52/249], loss=95.1010
	step [53/249], loss=129.5506
	step [54/249], loss=105.9269
	step [55/249], loss=111.5536
	step [56/249], loss=91.6818
	step [57/249], loss=115.9780
	step [58/249], loss=125.5389
	step [59/249], loss=113.1287
	step [60/249], loss=120.0592
	step [61/249], loss=107.6439
	step [62/249], loss=115.4336
	step [63/249], loss=104.1462
	step [64/249], loss=100.0584
	step [65/249], loss=124.0876
	step [66/249], loss=108.7542
	step [67/249], loss=99.5823
	step [68/249], loss=111.8354
	step [69/249], loss=103.1374
	step [70/249], loss=100.2572
	step [71/249], loss=111.5539
	step [72/249], loss=101.6355
	step [73/249], loss=112.6138
	step [74/249], loss=116.5038
	step [75/249], loss=82.3746
	step [76/249], loss=110.1844
	step [77/249], loss=107.1463
	step [78/249], loss=104.3303
	step [79/249], loss=97.1073
	step [80/249], loss=111.0370
	step [81/249], loss=112.4425
	step [82/249], loss=113.6531
	step [83/249], loss=121.5055
	step [84/249], loss=98.4435
	step [85/249], loss=105.7185
	step [86/249], loss=97.3579
	step [87/249], loss=98.3292
	step [88/249], loss=99.5510
	step [89/249], loss=115.8326
	step [90/249], loss=105.9804
	step [91/249], loss=114.8369
	step [92/249], loss=104.1662
	step [93/249], loss=92.1559
	step [94/249], loss=100.7725
	step [95/249], loss=98.0793
	step [96/249], loss=122.6214
	step [97/249], loss=98.4720
	step [98/249], loss=116.0311
	step [99/249], loss=102.8189
	step [100/249], loss=126.0232
	step [101/249], loss=106.6987
	step [102/249], loss=120.3841
	step [103/249], loss=92.6965
	step [104/249], loss=95.8565
	step [105/249], loss=93.9469
	step [106/249], loss=135.9868
	step [107/249], loss=91.3912
	step [108/249], loss=113.3017
	step [109/249], loss=99.0479
	step [110/249], loss=99.2932
	step [111/249], loss=119.3733
	step [112/249], loss=108.4615
	step [113/249], loss=89.5121
	step [114/249], loss=114.4061
	step [115/249], loss=107.0378
	step [116/249], loss=112.8111
	step [117/249], loss=94.6647
	step [118/249], loss=93.2607
	step [119/249], loss=110.0649
	step [120/249], loss=120.4139
	step [121/249], loss=125.4737
	step [122/249], loss=112.5571
	step [123/249], loss=128.7101
	step [124/249], loss=104.8860
	step [125/249], loss=100.5529
	step [126/249], loss=96.1583
	step [127/249], loss=106.1786
	step [128/249], loss=109.6055
	step [129/249], loss=104.4289
	step [130/249], loss=95.1316
	step [131/249], loss=93.2846
	step [132/249], loss=118.4869
	step [133/249], loss=106.6104
	step [134/249], loss=104.5449
	step [135/249], loss=107.4479
	step [136/249], loss=119.5640
	step [137/249], loss=99.2897
	step [138/249], loss=99.7206
	step [139/249], loss=101.2079
	step [140/249], loss=105.3246
	step [141/249], loss=85.3903
	step [142/249], loss=118.1268
	step [143/249], loss=102.7202
	step [144/249], loss=122.9266
	step [145/249], loss=108.4363
	step [146/249], loss=102.2459
	step [147/249], loss=119.2628
	step [148/249], loss=105.4258
	step [149/249], loss=100.7051
	step [150/249], loss=114.5668
	step [151/249], loss=114.9179
	step [152/249], loss=94.1920
	step [153/249], loss=105.9561
	step [154/249], loss=107.0123
	step [155/249], loss=90.7028
	step [156/249], loss=126.4296
	step [157/249], loss=94.9675
	step [158/249], loss=104.6003
	step [159/249], loss=113.7209
	step [160/249], loss=109.8379
	step [161/249], loss=133.3285
	step [162/249], loss=111.7814
	step [163/249], loss=112.6909
	step [164/249], loss=114.9925
	step [165/249], loss=113.7698
	step [166/249], loss=92.9367
	step [167/249], loss=100.5539
	step [168/249], loss=108.6978
	step [169/249], loss=124.4095
	step [170/249], loss=104.6117
	step [171/249], loss=101.1272
	step [172/249], loss=105.4015
	step [173/249], loss=112.4409
	step [174/249], loss=108.0001
	step [175/249], loss=83.5779
	step [176/249], loss=93.3084
	step [177/249], loss=80.1220
	step [178/249], loss=95.7202
	step [179/249], loss=127.0250
	step [180/249], loss=101.3238
	step [181/249], loss=86.0927
	step [182/249], loss=116.5953
	step [183/249], loss=103.8129
	step [184/249], loss=95.5844
	step [185/249], loss=114.1273
	step [186/249], loss=109.2071
	step [187/249], loss=107.6217
	step [188/249], loss=111.5637
	step [189/249], loss=97.3638
	step [190/249], loss=139.1065
	step [191/249], loss=108.5559
	step [192/249], loss=102.7192
	step [193/249], loss=108.1991
	step [194/249], loss=99.3169
	step [195/249], loss=105.8316
	step [196/249], loss=81.6641
	step [197/249], loss=99.7079
	step [198/249], loss=120.5817
	step [199/249], loss=105.2604
	step [200/249], loss=115.0840
	step [201/249], loss=126.6767
	step [202/249], loss=90.5165
	step [203/249], loss=96.4583
	step [204/249], loss=117.4076
	step [205/249], loss=94.3923
	step [206/249], loss=87.7308
	step [207/249], loss=111.8644
	step [208/249], loss=108.8033
	step [209/249], loss=103.8717
	step [210/249], loss=102.2018
	step [211/249], loss=85.8870
	step [212/249], loss=115.4198
	step [213/249], loss=93.7535
	step [214/249], loss=123.8801
	step [215/249], loss=104.5271
	step [216/249], loss=112.7804
	step [217/249], loss=104.3563
	step [218/249], loss=121.1284
	step [219/249], loss=97.1796
	step [220/249], loss=104.1692
	step [221/249], loss=111.4229
	step [222/249], loss=108.8300
	step [223/249], loss=116.5084
	step [224/249], loss=84.9457
	step [225/249], loss=87.6637
	step [226/249], loss=100.3830
	step [227/249], loss=95.3371
	step [228/249], loss=105.1159
	step [229/249], loss=122.6940
	step [230/249], loss=111.3884
	step [231/249], loss=84.1320
	step [232/249], loss=101.9762
	step [233/249], loss=106.1669
	step [234/249], loss=109.3643
	step [235/249], loss=103.7629
	step [236/249], loss=84.0262
	step [237/249], loss=101.9807
	step [238/249], loss=111.1456
	step [239/249], loss=116.5941
	step [240/249], loss=107.8257
	step [241/249], loss=104.9210
	step [242/249], loss=111.7955
	step [243/249], loss=124.3879
	step [244/249], loss=86.8899
	step [245/249], loss=106.1408
	step [246/249], loss=80.4583
	step [247/249], loss=117.2609
	step [248/249], loss=101.8465
	step [249/249], loss=62.0925
	Evaluating
	loss=0.0220, precision=0.3946, recall=0.9185, f1=0.5521
saving model as: 0_saved_model.pth
Training epoch 13
	step [1/249], loss=114.8517
	step [2/249], loss=87.3730
	step [3/249], loss=111.1610
	step [4/249], loss=93.2717
	step [5/249], loss=100.1228
	step [6/249], loss=92.5721
	step [7/249], loss=97.1485
	step [8/249], loss=134.0193
	step [9/249], loss=93.1712
	step [10/249], loss=82.7898
	step [11/249], loss=124.9158
	step [12/249], loss=111.6864
	step [13/249], loss=103.2139
	step [14/249], loss=98.0794
	step [15/249], loss=101.3130
	step [16/249], loss=102.2811
	step [17/249], loss=97.7800
	step [18/249], loss=111.2514
	step [19/249], loss=103.2552
	step [20/249], loss=97.8614
	step [21/249], loss=109.8539
	step [22/249], loss=98.4586
	step [23/249], loss=99.9014
	step [24/249], loss=89.5661
	step [25/249], loss=107.3009
	step [26/249], loss=98.9206
	step [27/249], loss=97.9758
	step [28/249], loss=99.8032
	step [29/249], loss=89.9243
	step [30/249], loss=81.2565
	step [31/249], loss=97.9975
	step [32/249], loss=122.3416
	step [33/249], loss=100.7036
	step [34/249], loss=83.6100
	step [35/249], loss=129.0979
	step [36/249], loss=100.7155
	step [37/249], loss=106.6900
	step [38/249], loss=113.0092
	step [39/249], loss=96.2338
	step [40/249], loss=92.8552
	step [41/249], loss=119.2278
	step [42/249], loss=119.9352
	step [43/249], loss=109.5284
	step [44/249], loss=104.0175
	step [45/249], loss=120.0410
	step [46/249], loss=74.3576
	step [47/249], loss=115.2030
	step [48/249], loss=116.9286
	step [49/249], loss=104.8314
	step [50/249], loss=97.3251
	step [51/249], loss=101.8711
	step [52/249], loss=103.1326
	step [53/249], loss=105.3602
	step [54/249], loss=111.2507
	step [55/249], loss=101.4174
	step [56/249], loss=113.9517
	step [57/249], loss=113.0891
	step [58/249], loss=107.7551
	step [59/249], loss=104.5898
	step [60/249], loss=86.4895
	step [61/249], loss=89.5210
	step [62/249], loss=105.1878
	step [63/249], loss=112.1937
	step [64/249], loss=86.6892
	step [65/249], loss=98.7627
	step [66/249], loss=108.2400
	step [67/249], loss=104.0843
	step [68/249], loss=129.4452
	step [69/249], loss=102.2544
	step [70/249], loss=115.8492
	step [71/249], loss=104.5195
	step [72/249], loss=100.2682
	step [73/249], loss=126.9129
	step [74/249], loss=101.6110
	step [75/249], loss=98.2546
	step [76/249], loss=84.5272
	step [77/249], loss=102.8150
	step [78/249], loss=115.6788
	step [79/249], loss=109.0592
	step [80/249], loss=119.9399
	step [81/249], loss=121.4719
	step [82/249], loss=99.1695
	step [83/249], loss=118.6630
	step [84/249], loss=102.7666
	step [85/249], loss=123.5989
	step [86/249], loss=121.3829
	step [87/249], loss=132.3038
	step [88/249], loss=96.9561
	step [89/249], loss=90.4634
	step [90/249], loss=126.8093
	step [91/249], loss=96.7454
	step [92/249], loss=106.6812
	step [93/249], loss=88.2599
	step [94/249], loss=108.6115
	step [95/249], loss=111.4496
	step [96/249], loss=88.4045
	step [97/249], loss=103.2899
	step [98/249], loss=114.1251
	step [99/249], loss=117.0480
	step [100/249], loss=109.9795
	step [101/249], loss=100.7699
	step [102/249], loss=111.9048
	step [103/249], loss=128.1529
	step [104/249], loss=100.0573
	step [105/249], loss=118.2233
	step [106/249], loss=122.7305
	step [107/249], loss=88.4875
	step [108/249], loss=81.6199
	step [109/249], loss=97.9886
	step [110/249], loss=119.0208
	step [111/249], loss=138.8103
	step [112/249], loss=101.3265
	step [113/249], loss=135.2749
	step [114/249], loss=112.4353
	step [115/249], loss=103.6926
	step [116/249], loss=108.6394
	step [117/249], loss=91.1838
	step [118/249], loss=106.0147
	step [119/249], loss=105.2260
	step [120/249], loss=113.3431
	step [121/249], loss=134.9363
	step [122/249], loss=102.2811
	step [123/249], loss=98.2922
	step [124/249], loss=115.2188
	step [125/249], loss=108.3495
	step [126/249], loss=103.7449
	step [127/249], loss=100.8682
	step [128/249], loss=111.3619
	step [129/249], loss=115.5264
	step [130/249], loss=112.0799
	step [131/249], loss=85.8477
	step [132/249], loss=114.6645
	step [133/249], loss=108.9025
	step [134/249], loss=93.0224
	step [135/249], loss=85.7395
	step [136/249], loss=95.9023
	step [137/249], loss=104.0105
	step [138/249], loss=101.1364
	step [139/249], loss=99.4032
	step [140/249], loss=112.7503
	step [141/249], loss=96.9060
	step [142/249], loss=100.3444
	step [143/249], loss=109.3910
	step [144/249], loss=141.3798
	step [145/249], loss=113.3976
	step [146/249], loss=116.6104
	step [147/249], loss=100.0973
	step [148/249], loss=100.3460
	step [149/249], loss=89.3965
	step [150/249], loss=96.5212
	step [151/249], loss=105.9004
	step [152/249], loss=102.7310
	step [153/249], loss=120.9725
	step [154/249], loss=115.2509
	step [155/249], loss=98.4875
	step [156/249], loss=101.2417
	step [157/249], loss=98.5021
	step [158/249], loss=109.4088
	step [159/249], loss=95.6559
	step [160/249], loss=117.3897
	step [161/249], loss=103.9979
	step [162/249], loss=101.6979
	step [163/249], loss=90.6882
	step [164/249], loss=109.3123
	step [165/249], loss=98.5955
	step [166/249], loss=116.2229
	step [167/249], loss=102.9970
	step [168/249], loss=102.7772
	step [169/249], loss=124.4008
	step [170/249], loss=99.5282
	step [171/249], loss=98.5194
	step [172/249], loss=108.4574
	step [173/249], loss=95.6306
	step [174/249], loss=93.6765
	step [175/249], loss=109.0894
	step [176/249], loss=100.0205
	step [177/249], loss=81.6343
	step [178/249], loss=109.8198
	step [179/249], loss=95.3875
	step [180/249], loss=100.9811
	step [181/249], loss=109.3282
	step [182/249], loss=89.8072
	step [183/249], loss=104.0487
	step [184/249], loss=90.8261
	step [185/249], loss=126.2285
	step [186/249], loss=99.1521
	step [187/249], loss=92.1626
	step [188/249], loss=95.0972
	step [189/249], loss=106.9476
	step [190/249], loss=114.5552
	step [191/249], loss=101.4770
	step [192/249], loss=97.9829
	step [193/249], loss=114.7821
	step [194/249], loss=83.1705
	step [195/249], loss=95.4822
	step [196/249], loss=105.8051
	step [197/249], loss=94.1121
	step [198/249], loss=95.8129
	step [199/249], loss=101.0608
	step [200/249], loss=93.9550
	step [201/249], loss=102.4954
	step [202/249], loss=99.9035
	step [203/249], loss=100.2609
	step [204/249], loss=114.8371
	step [205/249], loss=120.3750
	step [206/249], loss=99.8080
	step [207/249], loss=118.6793
	step [208/249], loss=97.6086
	step [209/249], loss=119.9379
	step [210/249], loss=102.1502
	step [211/249], loss=94.9828
	step [212/249], loss=111.1243
	step [213/249], loss=105.5630
	step [214/249], loss=99.4059
	step [215/249], loss=105.7514
	step [216/249], loss=91.1271
	step [217/249], loss=116.9865
	step [218/249], loss=127.2048
	step [219/249], loss=121.5659
	step [220/249], loss=104.9526
	step [221/249], loss=96.8015
	step [222/249], loss=94.9658
	step [223/249], loss=95.8802
	step [224/249], loss=131.3107
	step [225/249], loss=110.4739
	step [226/249], loss=112.6891
	step [227/249], loss=103.5177
	step [228/249], loss=101.8838
	step [229/249], loss=98.6170
	step [230/249], loss=93.0838
	step [231/249], loss=121.0915
	step [232/249], loss=98.7639
	step [233/249], loss=95.6047
	step [234/249], loss=98.8340
	step [235/249], loss=117.1506
	step [236/249], loss=107.6509
	step [237/249], loss=96.2978
	step [238/249], loss=122.1852
	step [239/249], loss=96.3942
	step [240/249], loss=102.0832
	step [241/249], loss=118.3048
	step [242/249], loss=102.3803
	step [243/249], loss=100.0862
	step [244/249], loss=118.3844
	step [245/249], loss=119.9573
	step [246/249], loss=95.0410
	step [247/249], loss=81.0644
	step [248/249], loss=95.1295
	step [249/249], loss=57.8165
	Evaluating
	loss=0.0199, precision=0.3572, recall=0.8998, f1=0.5114
Training epoch 14
	step [1/249], loss=110.8984
	step [2/249], loss=111.0944
	step [3/249], loss=102.8041
	step [4/249], loss=98.9333
	step [5/249], loss=126.8158
	step [6/249], loss=86.8040
	step [7/249], loss=96.3326
	step [8/249], loss=108.6664
	step [9/249], loss=100.7590
	step [10/249], loss=110.9796
	step [11/249], loss=106.7391
	step [12/249], loss=123.4377
	step [13/249], loss=101.1115
	step [14/249], loss=103.4253
	step [15/249], loss=117.9751
	step [16/249], loss=99.1012
	step [17/249], loss=95.1977
	step [18/249], loss=92.0336
	step [19/249], loss=91.2944
	step [20/249], loss=109.1785
	step [21/249], loss=111.9972
	step [22/249], loss=80.2762
	step [23/249], loss=112.6977
	step [24/249], loss=102.5197
	step [25/249], loss=106.2386
	step [26/249], loss=122.9944
	step [27/249], loss=115.7149
	step [28/249], loss=108.4629
	step [29/249], loss=112.1539
	step [30/249], loss=118.7875
	step [31/249], loss=99.7397
	step [32/249], loss=117.7256
	step [33/249], loss=105.4443
	step [34/249], loss=99.4576
	step [35/249], loss=106.7632
	step [36/249], loss=97.5446
	step [37/249], loss=101.6421
	step [38/249], loss=116.7390
	step [39/249], loss=90.1889
	step [40/249], loss=107.3994
	step [41/249], loss=128.3374
	step [42/249], loss=107.0000
	step [43/249], loss=99.7586
	step [44/249], loss=89.5841
	step [45/249], loss=98.5906
	step [46/249], loss=100.8338
	step [47/249], loss=94.7591
	step [48/249], loss=129.9840
	step [49/249], loss=106.5471
	step [50/249], loss=85.8753
	step [51/249], loss=100.9220
	step [52/249], loss=127.3282
	step [53/249], loss=100.0765
	step [54/249], loss=106.5893
	step [55/249], loss=104.5145
	step [56/249], loss=94.0235
	step [57/249], loss=112.3490
	step [58/249], loss=101.2017
	step [59/249], loss=97.8540
	step [60/249], loss=121.5389
	step [61/249], loss=95.9862
	step [62/249], loss=109.8230
	step [63/249], loss=104.7979
	step [64/249], loss=99.7861
	step [65/249], loss=122.6699
	step [66/249], loss=90.2164
	step [67/249], loss=120.4544
	step [68/249], loss=111.4973
	step [69/249], loss=94.7299
	step [70/249], loss=106.7496
	step [71/249], loss=119.0528
	step [72/249], loss=100.8940
	step [73/249], loss=92.2488
	step [74/249], loss=100.9960
	step [75/249], loss=112.5029
	step [76/249], loss=102.9464
	step [77/249], loss=108.8739
	step [78/249], loss=105.6409
	step [79/249], loss=108.3867
	step [80/249], loss=105.3692
	step [81/249], loss=83.3046
	step [82/249], loss=124.5877
	step [83/249], loss=97.5758
	step [84/249], loss=106.6916
	step [85/249], loss=94.3293
	step [86/249], loss=86.2716
	step [87/249], loss=90.4131
	step [88/249], loss=82.2614
	step [89/249], loss=80.8506
	step [90/249], loss=132.7982
	step [91/249], loss=116.9632
	step [92/249], loss=97.2575
	step [93/249], loss=114.2957
	step [94/249], loss=110.4969
	step [95/249], loss=98.4320
	step [96/249], loss=94.3532
	step [97/249], loss=110.2533
	step [98/249], loss=108.5015
	step [99/249], loss=103.5583
	step [100/249], loss=101.7254
	step [101/249], loss=110.2762
	step [102/249], loss=107.2838
	step [103/249], loss=98.9470
	step [104/249], loss=87.7604
	step [105/249], loss=112.5615
	step [106/249], loss=121.2940
	step [107/249], loss=101.1614
	step [108/249], loss=93.4313
	step [109/249], loss=100.4752
	step [110/249], loss=99.1608
	step [111/249], loss=93.0998
	step [112/249], loss=97.0971
	step [113/249], loss=101.2458
	step [114/249], loss=91.4903
	step [115/249], loss=94.9981
	step [116/249], loss=105.1664
	step [117/249], loss=98.5776
	step [118/249], loss=87.8857
	step [119/249], loss=89.3529
	step [120/249], loss=94.9382
	step [121/249], loss=102.5023
	step [122/249], loss=115.9027
	step [123/249], loss=114.1840
	step [124/249], loss=116.5943
	step [125/249], loss=89.1745
	step [126/249], loss=99.4602
	step [127/249], loss=109.9477
	step [128/249], loss=97.5478
	step [129/249], loss=83.3725
	step [130/249], loss=102.3550
	step [131/249], loss=78.7953
	step [132/249], loss=96.5779
	step [133/249], loss=91.8184
	step [134/249], loss=125.1508
	step [135/249], loss=99.5833
	step [136/249], loss=120.8517
	step [137/249], loss=110.4497
	step [138/249], loss=84.1684
	step [139/249], loss=111.8567
	step [140/249], loss=129.5093
	step [141/249], loss=119.0994
	step [142/249], loss=107.2250
	step [143/249], loss=71.7165
	step [144/249], loss=120.3173
	step [145/249], loss=93.5306
	step [146/249], loss=108.6018
	step [147/249], loss=89.7976
	step [148/249], loss=132.4725
	step [149/249], loss=94.9441
	step [150/249], loss=102.0302
	step [151/249], loss=84.9723
	step [152/249], loss=111.5338
	step [153/249], loss=102.5626
	step [154/249], loss=108.7590
	step [155/249], loss=126.2182
	step [156/249], loss=96.8932
	step [157/249], loss=109.3857
	step [158/249], loss=93.5005
	step [159/249], loss=108.7929
	step [160/249], loss=84.5763
	step [161/249], loss=94.5890
	step [162/249], loss=115.2883
	step [163/249], loss=106.6269
	step [164/249], loss=111.9091
	step [165/249], loss=114.3557
	step [166/249], loss=100.0045
	step [167/249], loss=110.4794
	step [168/249], loss=102.0463
	step [169/249], loss=93.5634
	step [170/249], loss=99.4539
	step [171/249], loss=81.6176
	step [172/249], loss=110.8974
	step [173/249], loss=92.3532
	step [174/249], loss=114.4136
	step [175/249], loss=95.4816
	step [176/249], loss=110.5172
	step [177/249], loss=138.6726
	step [178/249], loss=104.5397
	step [179/249], loss=106.4115
	step [180/249], loss=131.5663
	step [181/249], loss=118.2891
	step [182/249], loss=127.7831
	step [183/249], loss=94.7874
	step [184/249], loss=116.7249
	step [185/249], loss=118.5986
	step [186/249], loss=102.5514
	step [187/249], loss=104.7001
	step [188/249], loss=87.2052
	step [189/249], loss=112.0948
	step [190/249], loss=87.6389
	step [191/249], loss=95.6894
	step [192/249], loss=99.2721
	step [193/249], loss=93.8021
	step [194/249], loss=96.1978
	step [195/249], loss=111.0839
	step [196/249], loss=97.8627
	step [197/249], loss=104.3146
	step [198/249], loss=96.7022
	step [199/249], loss=96.3521
	step [200/249], loss=104.6245
	step [201/249], loss=94.7209
	step [202/249], loss=113.2683
	step [203/249], loss=92.1379
	step [204/249], loss=93.5519
	step [205/249], loss=111.6118
	step [206/249], loss=102.7131
	step [207/249], loss=96.0431
	step [208/249], loss=99.2939
	step [209/249], loss=97.4320
	step [210/249], loss=121.1189
	step [211/249], loss=113.8710
	step [212/249], loss=106.5612
	step [213/249], loss=138.1320
	step [214/249], loss=95.7092
	step [215/249], loss=95.9393
	step [216/249], loss=99.7212
	step [217/249], loss=101.7140
	step [218/249], loss=90.6914
	step [219/249], loss=116.4883
	step [220/249], loss=83.8022
	step [221/249], loss=105.6076
	step [222/249], loss=98.8004
	step [223/249], loss=113.5445
	step [224/249], loss=103.3982
	step [225/249], loss=97.5288
	step [226/249], loss=83.2762
	step [227/249], loss=110.3578
	step [228/249], loss=101.9899
	step [229/249], loss=107.2083
	step [230/249], loss=98.7919
	step [231/249], loss=95.6738
	step [232/249], loss=110.8720
	step [233/249], loss=74.8492
	step [234/249], loss=120.3823
	step [235/249], loss=102.7905
	step [236/249], loss=114.5145
	step [237/249], loss=99.5552
	step [238/249], loss=106.6446
	step [239/249], loss=94.3677
	step [240/249], loss=96.5317
	step [241/249], loss=107.0278
	step [242/249], loss=96.2867
	step [243/249], loss=96.7538
	step [244/249], loss=107.1178
	step [245/249], loss=87.9093
	step [246/249], loss=116.6176
	step [247/249], loss=113.9462
	step [248/249], loss=89.2444
	step [249/249], loss=56.3447
	Evaluating
	loss=0.0171, precision=0.3860, recall=0.9062, f1=0.5414
Training epoch 15
	step [1/249], loss=110.6672
	step [2/249], loss=114.2540
	step [3/249], loss=124.9806
	step [4/249], loss=101.6867
	step [5/249], loss=113.5430
	step [6/249], loss=111.4343
	step [7/249], loss=101.1122
	step [8/249], loss=96.2268
	step [9/249], loss=96.8430
	step [10/249], loss=113.5189
	step [11/249], loss=102.2071
	step [12/249], loss=101.3104
	step [13/249], loss=120.3908
	step [14/249], loss=91.9152
	step [15/249], loss=97.3281
	step [16/249], loss=117.5012
	step [17/249], loss=105.1985
	step [18/249], loss=95.1402
	step [19/249], loss=107.5258
	step [20/249], loss=95.5104
	step [21/249], loss=104.4952
	step [22/249], loss=91.2694
	step [23/249], loss=97.7690
	step [24/249], loss=95.5457
	step [25/249], loss=100.1080
	step [26/249], loss=109.6302
	step [27/249], loss=96.9089
	step [28/249], loss=113.6901
	step [29/249], loss=96.0311
	step [30/249], loss=103.6686
	step [31/249], loss=91.5138
	step [32/249], loss=104.5525
	step [33/249], loss=96.5819
	step [34/249], loss=114.2510
	step [35/249], loss=121.0785
	step [36/249], loss=92.7723
	step [37/249], loss=90.6261
	step [38/249], loss=92.4909
	step [39/249], loss=119.6045
	step [40/249], loss=84.5144
	step [41/249], loss=90.5929
	step [42/249], loss=105.1071
	step [43/249], loss=122.1379
	step [44/249], loss=106.1117
	step [45/249], loss=106.8468
	step [46/249], loss=107.5939
	step [47/249], loss=109.7461
	step [48/249], loss=102.6918
	step [49/249], loss=123.0487
	step [50/249], loss=103.2646
	step [51/249], loss=87.6469
	step [52/249], loss=95.7287
	step [53/249], loss=93.9267
	step [54/249], loss=127.8428
	step [55/249], loss=102.2585
	step [56/249], loss=113.8868
	step [57/249], loss=111.5797
	step [58/249], loss=117.1640
	step [59/249], loss=92.2251
	step [60/249], loss=100.5601
	step [61/249], loss=99.4942
	step [62/249], loss=108.0146
	step [63/249], loss=95.6976
	step [64/249], loss=112.9421
	step [65/249], loss=99.4784
	step [66/249], loss=112.3541
	step [67/249], loss=117.7977
	step [68/249], loss=93.5126
	step [69/249], loss=121.4891
	step [70/249], loss=94.9537
	step [71/249], loss=94.0583
	step [72/249], loss=105.3480
	step [73/249], loss=100.0444
	step [74/249], loss=87.5695
	step [75/249], loss=100.1907
	step [76/249], loss=103.7608
	step [77/249], loss=96.5791
	step [78/249], loss=114.4253
	step [79/249], loss=97.0647
	step [80/249], loss=103.4673
	step [81/249], loss=97.1965
	step [82/249], loss=100.8069
	step [83/249], loss=93.7831
	step [84/249], loss=100.6193
	step [85/249], loss=87.7741
	step [86/249], loss=89.5623
	step [87/249], loss=89.5812
	step [88/249], loss=90.1270
	step [89/249], loss=114.8822
	step [90/249], loss=99.1837
	step [91/249], loss=92.3844
	step [92/249], loss=100.0487
	step [93/249], loss=101.2029
	step [94/249], loss=87.8376
	step [95/249], loss=97.5725
	step [96/249], loss=86.1912
	step [97/249], loss=107.0691
	step [98/249], loss=114.3417
	step [99/249], loss=91.4526
	step [100/249], loss=99.8525
	step [101/249], loss=114.2381
	step [102/249], loss=70.9810
	step [103/249], loss=110.7222
	step [104/249], loss=104.2600
	step [105/249], loss=107.7201
	step [106/249], loss=119.2719
	step [107/249], loss=101.6589
	step [108/249], loss=90.9195
	step [109/249], loss=105.3386
	step [110/249], loss=118.9572
	step [111/249], loss=130.6248
	step [112/249], loss=115.8993
	step [113/249], loss=93.0968
	step [114/249], loss=109.8620
	step [115/249], loss=92.9859
	step [116/249], loss=97.5718
	step [117/249], loss=98.0182
	step [118/249], loss=131.3397
	step [119/249], loss=107.8680
	step [120/249], loss=101.7948
	step [121/249], loss=109.7113
	step [122/249], loss=102.2897
	step [123/249], loss=102.7943
	step [124/249], loss=116.7594
	step [125/249], loss=113.2918
	step [126/249], loss=92.6088
	step [127/249], loss=97.3277
	step [128/249], loss=115.2401
	step [129/249], loss=117.3347
	step [130/249], loss=82.0138
	step [131/249], loss=88.7291
	step [132/249], loss=106.5476
	step [133/249], loss=118.2561
	step [134/249], loss=93.7337
	step [135/249], loss=104.5498
	step [136/249], loss=99.3917
	step [137/249], loss=102.1044
	step [138/249], loss=98.4008
	step [139/249], loss=112.1721
	step [140/249], loss=86.6494
	step [141/249], loss=107.5192
	step [142/249], loss=103.7831
	step [143/249], loss=91.4772
	step [144/249], loss=119.0504
	step [145/249], loss=110.4849
	step [146/249], loss=101.9910
	step [147/249], loss=102.8793
	step [148/249], loss=92.2005
	step [149/249], loss=114.8978
	step [150/249], loss=102.5448
	step [151/249], loss=103.6786
	step [152/249], loss=108.9136
	step [153/249], loss=114.9574
	step [154/249], loss=106.9810
	step [155/249], loss=112.6804
	step [156/249], loss=105.4043
	step [157/249], loss=93.0597
	step [158/249], loss=106.0355
	step [159/249], loss=86.8243
	step [160/249], loss=109.9087
	step [161/249], loss=89.7606
	step [162/249], loss=92.2109
	step [163/249], loss=88.7595
	step [164/249], loss=85.1443
	step [165/249], loss=141.7491
	step [166/249], loss=112.5263
	step [167/249], loss=101.6916
	step [168/249], loss=107.1837
	step [169/249], loss=113.7865
	step [170/249], loss=112.8466
	step [171/249], loss=112.7208
	step [172/249], loss=106.9531
	step [173/249], loss=113.3311
	step [174/249], loss=105.0899
	step [175/249], loss=93.1257
	step [176/249], loss=97.2277
	step [177/249], loss=105.9582
	step [178/249], loss=93.3927
	step [179/249], loss=112.9783
	step [180/249], loss=91.4506
	step [181/249], loss=115.4154
	step [182/249], loss=99.2324
	step [183/249], loss=87.1022
	step [184/249], loss=108.7600
	step [185/249], loss=100.1004
	step [186/249], loss=102.5393
	step [187/249], loss=102.4580
	step [188/249], loss=93.6076
	step [189/249], loss=109.4012
	step [190/249], loss=99.0756
	step [191/249], loss=78.3879
	step [192/249], loss=107.0121
	step [193/249], loss=108.2263
	step [194/249], loss=109.0027
	step [195/249], loss=107.2210
	step [196/249], loss=100.6358
	step [197/249], loss=112.8419
	step [198/249], loss=100.8009
	step [199/249], loss=110.1094
	step [200/249], loss=120.5629
	step [201/249], loss=119.3450
	step [202/249], loss=103.3066
	step [203/249], loss=94.0337
	step [204/249], loss=102.7964
	step [205/249], loss=105.9115
	step [206/249], loss=101.9001
	step [207/249], loss=98.9965
	step [208/249], loss=101.9425
	step [209/249], loss=95.8414
	step [210/249], loss=98.5681
	step [211/249], loss=91.9164
	step [212/249], loss=88.3154
	step [213/249], loss=109.2730
	step [214/249], loss=105.2011
	step [215/249], loss=112.6309
	step [216/249], loss=88.7719
	step [217/249], loss=114.0489
	step [218/249], loss=128.2442
	step [219/249], loss=101.2924
	step [220/249], loss=83.9279
	step [221/249], loss=101.6617
	step [222/249], loss=111.4240
	step [223/249], loss=98.4155
	step [224/249], loss=93.5138
	step [225/249], loss=93.9628
	step [226/249], loss=97.0964
	step [227/249], loss=116.1715
	step [228/249], loss=105.3332
	step [229/249], loss=106.8318
	step [230/249], loss=84.8456
	step [231/249], loss=105.0325
	step [232/249], loss=110.9447
	step [233/249], loss=106.6578
	step [234/249], loss=101.7207
	step [235/249], loss=106.7302
	step [236/249], loss=101.0663
	step [237/249], loss=104.3547
	step [238/249], loss=117.8554
	step [239/249], loss=94.1444
	step [240/249], loss=84.7260
	step [241/249], loss=107.9476
	step [242/249], loss=105.2067
	step [243/249], loss=89.9324
	step [244/249], loss=92.5891
	step [245/249], loss=96.0116
	step [246/249], loss=87.4621
	step [247/249], loss=95.1391
	step [248/249], loss=97.2186
	step [249/249], loss=64.3013
	Evaluating
	loss=0.0155, precision=0.3803, recall=0.9050, f1=0.5356
Training epoch 16
	step [1/249], loss=99.3517
	step [2/249], loss=117.0443
	step [3/249], loss=103.1198
	step [4/249], loss=116.5461
	step [5/249], loss=101.6003
	step [6/249], loss=113.7275
	step [7/249], loss=100.9840
	step [8/249], loss=120.8821
	step [9/249], loss=97.2678
	step [10/249], loss=95.7320
	step [11/249], loss=90.2342
	step [12/249], loss=101.5206
	step [13/249], loss=107.3024
	step [14/249], loss=95.2860
	step [15/249], loss=98.7625
	step [16/249], loss=95.8202
	step [17/249], loss=104.2218
	step [18/249], loss=101.8252
	step [19/249], loss=92.1124
	step [20/249], loss=111.0272
	step [21/249], loss=109.8120
	step [22/249], loss=106.9305
	step [23/249], loss=104.2628
	step [24/249], loss=108.3206
	step [25/249], loss=109.9578
	step [26/249], loss=114.7150
	step [27/249], loss=100.6152
	step [28/249], loss=86.6258
	step [29/249], loss=123.3709
	step [30/249], loss=98.9670
	step [31/249], loss=113.8824
	step [32/249], loss=116.3401
	step [33/249], loss=102.8358
	step [34/249], loss=93.3853
	step [35/249], loss=103.4476
	step [36/249], loss=97.9379
	step [37/249], loss=110.5161
	step [38/249], loss=94.2190
	step [39/249], loss=96.8833
	step [40/249], loss=90.9919
	step [41/249], loss=115.8005
	step [42/249], loss=81.8940
	step [43/249], loss=90.9221
	step [44/249], loss=105.7220
	step [45/249], loss=111.7477
	step [46/249], loss=96.9053
	step [47/249], loss=96.0387
	step [48/249], loss=85.9816
	step [49/249], loss=91.4822
	step [50/249], loss=97.8666
	step [51/249], loss=110.1692
	step [52/249], loss=89.1697
	step [53/249], loss=99.4092
	step [54/249], loss=85.1462
	step [55/249], loss=102.0332
	step [56/249], loss=88.4875
	step [57/249], loss=97.4639
	step [58/249], loss=88.2818
	step [59/249], loss=112.6233
	step [60/249], loss=82.8600
	step [61/249], loss=102.8826
	step [62/249], loss=99.6864
	step [63/249], loss=101.8454
	step [64/249], loss=96.4329
	step [65/249], loss=93.9588
	step [66/249], loss=95.7419
	step [67/249], loss=107.3594
	step [68/249], loss=100.7754
	step [69/249], loss=110.4478
	step [70/249], loss=97.4317
	step [71/249], loss=126.0345
	step [72/249], loss=102.6534
	step [73/249], loss=92.9531
	step [74/249], loss=111.1254
	step [75/249], loss=84.2799
	step [76/249], loss=127.6928
	step [77/249], loss=98.9779
	step [78/249], loss=115.6049
	step [79/249], loss=112.0537
	step [80/249], loss=99.8808
	step [81/249], loss=83.5986
	step [82/249], loss=112.2689
	step [83/249], loss=95.4174
	step [84/249], loss=96.9204
	step [85/249], loss=97.3510
	step [86/249], loss=118.2888
	step [87/249], loss=94.0868
	step [88/249], loss=104.9636
	step [89/249], loss=103.7170
	step [90/249], loss=101.6418
	step [91/249], loss=104.8497
	step [92/249], loss=114.4807
	step [93/249], loss=101.9410
	step [94/249], loss=102.7207
	step [95/249], loss=124.6092
	step [96/249], loss=83.9678
	step [97/249], loss=86.3400
	step [98/249], loss=92.9322
	step [99/249], loss=114.1110
	step [100/249], loss=101.0422
	step [101/249], loss=106.0189
	step [102/249], loss=104.6813
	step [103/249], loss=98.8300
	step [104/249], loss=92.8244
	step [105/249], loss=90.1091
	step [106/249], loss=99.1436
	step [107/249], loss=95.1868
	step [108/249], loss=108.3579
	step [109/249], loss=99.6524
	step [110/249], loss=106.3517
	step [111/249], loss=88.3480
	step [112/249], loss=101.8253
	step [113/249], loss=105.3774
	step [114/249], loss=122.3208
	step [115/249], loss=114.3040
	step [116/249], loss=102.7698
	step [117/249], loss=81.7810
	step [118/249], loss=95.6956
	step [119/249], loss=71.6917
	step [120/249], loss=109.9074
	step [121/249], loss=123.0371
	step [122/249], loss=99.2167
	step [123/249], loss=87.2958
	step [124/249], loss=113.8251
	step [125/249], loss=94.4980
	step [126/249], loss=89.8980
	step [127/249], loss=94.8434
	step [128/249], loss=95.4938
	step [129/249], loss=93.2514
	step [130/249], loss=119.9503
	step [131/249], loss=101.9467
	step [132/249], loss=98.4855
	step [133/249], loss=94.3036
	step [134/249], loss=91.5537
	step [135/249], loss=100.3531
	step [136/249], loss=91.0212
	step [137/249], loss=100.8762
	step [138/249], loss=100.0946
	step [139/249], loss=105.3934
	step [140/249], loss=112.9759
	step [141/249], loss=96.4678
	step [142/249], loss=95.2209
	step [143/249], loss=92.6216
	step [144/249], loss=99.7419
	step [145/249], loss=98.0202
	step [146/249], loss=102.4467
	step [147/249], loss=103.4240
	step [148/249], loss=111.7736
	step [149/249], loss=83.7867
	step [150/249], loss=98.0967
	step [151/249], loss=110.8978
	step [152/249], loss=108.2665
	step [153/249], loss=89.5398
	step [154/249], loss=95.2368
	step [155/249], loss=102.8845
	step [156/249], loss=113.3834
	step [157/249], loss=77.7504
	step [158/249], loss=104.5497
	step [159/249], loss=104.1718
	step [160/249], loss=103.3133
	step [161/249], loss=106.6342
	step [162/249], loss=105.8387
	step [163/249], loss=99.1666
	step [164/249], loss=117.4833
	step [165/249], loss=116.7532
	step [166/249], loss=99.4825
	step [167/249], loss=107.5795
	step [168/249], loss=100.6159
	step [169/249], loss=88.0069
	step [170/249], loss=96.0140
	step [171/249], loss=100.9989
	step [172/249], loss=131.3473
	step [173/249], loss=97.0076
	step [174/249], loss=113.4337
	step [175/249], loss=101.5670
	step [176/249], loss=95.9329
	step [177/249], loss=75.6863
	step [178/249], loss=100.2239
	step [179/249], loss=82.8860
	step [180/249], loss=100.9213
	step [181/249], loss=99.3415
	step [182/249], loss=109.7831
	step [183/249], loss=95.6414
	step [184/249], loss=78.8010
	step [185/249], loss=91.3344
	step [186/249], loss=87.6810
	step [187/249], loss=109.6111
	step [188/249], loss=103.3033
	step [189/249], loss=104.4186
	step [190/249], loss=125.6115
	step [191/249], loss=88.4885
	step [192/249], loss=95.4729
	step [193/249], loss=106.9674
	step [194/249], loss=112.6000
	step [195/249], loss=107.1824
	step [196/249], loss=96.3141
	step [197/249], loss=84.5837
	step [198/249], loss=90.3157
	step [199/249], loss=113.5266
	step [200/249], loss=84.3813
	step [201/249], loss=110.9897
	step [202/249], loss=104.8092
	step [203/249], loss=101.0285
	step [204/249], loss=117.9455
	step [205/249], loss=118.3538
	step [206/249], loss=108.4400
	step [207/249], loss=111.1517
	step [208/249], loss=97.3876
	step [209/249], loss=123.7247
	step [210/249], loss=110.4327
	step [211/249], loss=80.3909
	step [212/249], loss=105.6131
	step [213/249], loss=120.4291
	step [214/249], loss=101.7963
	step [215/249], loss=107.1665
	step [216/249], loss=93.2123
	step [217/249], loss=106.0590
	step [218/249], loss=97.0907
	step [219/249], loss=99.4877
	step [220/249], loss=128.9293
	step [221/249], loss=119.3847
	step [222/249], loss=89.6342
	step [223/249], loss=103.3424
	step [224/249], loss=107.1670
	step [225/249], loss=107.5611
	step [226/249], loss=112.3629
	step [227/249], loss=117.9363
	step [228/249], loss=120.6712
	step [229/249], loss=115.4599
	step [230/249], loss=106.8666
	step [231/249], loss=94.2068
	step [232/249], loss=103.8901
	step [233/249], loss=95.3614
	step [234/249], loss=119.2000
	step [235/249], loss=98.2078
	step [236/249], loss=107.4364
	step [237/249], loss=114.7119
	step [238/249], loss=81.9353
	step [239/249], loss=104.9293
	step [240/249], loss=96.6787
	step [241/249], loss=110.7416
	step [242/249], loss=82.1111
	step [243/249], loss=89.6226
	step [244/249], loss=108.3789
	step [245/249], loss=95.6082
	step [246/249], loss=110.4138
	step [247/249], loss=118.7223
	step [248/249], loss=114.7638
	step [249/249], loss=80.0078
	Evaluating
	loss=0.0128, precision=0.4694, recall=0.8885, f1=0.6143
saving model as: 0_saved_model.pth
Training epoch 17
	step [1/249], loss=111.3326
	step [2/249], loss=103.1827
	step [3/249], loss=103.6292
	step [4/249], loss=118.1757
	step [5/249], loss=82.7955
	step [6/249], loss=76.8371
	step [7/249], loss=91.3876
	step [8/249], loss=112.0174
	step [9/249], loss=86.9790
	step [10/249], loss=108.4597
	step [11/249], loss=94.3036
	step [12/249], loss=91.5958
	step [13/249], loss=107.7559
	step [14/249], loss=113.5205
	step [15/249], loss=108.7855
	step [16/249], loss=96.6694
	step [17/249], loss=91.8495
	step [18/249], loss=99.6468
	step [19/249], loss=99.5756
	step [20/249], loss=111.1858
	step [21/249], loss=94.9695
	step [22/249], loss=108.0978
	step [23/249], loss=93.7190
	step [24/249], loss=116.2082
	step [25/249], loss=98.7541
	step [26/249], loss=86.5116
	step [27/249], loss=107.7501
	step [28/249], loss=95.7943
	step [29/249], loss=108.9362
	step [30/249], loss=105.7276
	step [31/249], loss=89.4232
	step [32/249], loss=100.0265
	step [33/249], loss=102.7106
	step [34/249], loss=90.3278
	step [35/249], loss=100.0123
	step [36/249], loss=90.7527
	step [37/249], loss=90.7485
	step [38/249], loss=98.2990
	step [39/249], loss=108.3849
	step [40/249], loss=110.1952
	step [41/249], loss=114.9954
	step [42/249], loss=95.1418
	step [43/249], loss=100.0287
	step [44/249], loss=97.7998
	step [45/249], loss=104.1557
	step [46/249], loss=114.2897
	step [47/249], loss=93.7209
	step [48/249], loss=99.5677
	step [49/249], loss=100.4951
	step [50/249], loss=116.4877
	step [51/249], loss=106.6347
	step [52/249], loss=109.2827
	step [53/249], loss=91.9582
	step [54/249], loss=136.5461
	step [55/249], loss=107.5624
	step [56/249], loss=87.8689
	step [57/249], loss=124.8407
	step [58/249], loss=99.1766
	step [59/249], loss=104.7583
	step [60/249], loss=104.9938
	step [61/249], loss=81.8768
	step [62/249], loss=96.6809
	step [63/249], loss=102.4589
	step [64/249], loss=107.4382
	step [65/249], loss=105.0407
	step [66/249], loss=90.4160
	step [67/249], loss=100.7424
	step [68/249], loss=101.4665
	step [69/249], loss=102.4047
	step [70/249], loss=103.7992
	step [71/249], loss=125.3397
	step [72/249], loss=103.1501
	step [73/249], loss=96.8702
	step [74/249], loss=104.3430
	step [75/249], loss=104.9996
	step [76/249], loss=93.2034
	step [77/249], loss=84.8171
	step [78/249], loss=108.8268
	step [79/249], loss=99.8969
	step [80/249], loss=88.8808
	step [81/249], loss=81.3538
	step [82/249], loss=92.2180
	step [83/249], loss=97.7894
	step [84/249], loss=93.7574
	step [85/249], loss=98.8554
	step [86/249], loss=100.8107
	step [87/249], loss=100.5030
	step [88/249], loss=82.5208
	step [89/249], loss=95.4371
	step [90/249], loss=93.5056
	step [91/249], loss=96.1248
	step [92/249], loss=98.6651
	step [93/249], loss=88.8755
	step [94/249], loss=105.7437
	step [95/249], loss=106.2988
	step [96/249], loss=108.5263
	step [97/249], loss=103.0854
	step [98/249], loss=104.3686
	step [99/249], loss=108.0327
	step [100/249], loss=118.6545
	step [101/249], loss=97.7615
	step [102/249], loss=110.8561
	step [103/249], loss=92.3527
	step [104/249], loss=91.0378
	step [105/249], loss=101.6692
	step [106/249], loss=95.5521
	step [107/249], loss=79.5130
	step [108/249], loss=111.5539
	step [109/249], loss=105.6265
	step [110/249], loss=101.6620
	step [111/249], loss=88.5779
	step [112/249], loss=89.0059
	step [113/249], loss=96.7558
	step [114/249], loss=107.6309
	step [115/249], loss=92.1113
	step [116/249], loss=99.2019
	step [117/249], loss=103.2442
	step [118/249], loss=88.3979
	step [119/249], loss=109.0289
	step [120/249], loss=96.9092
	step [121/249], loss=111.2182
	step [122/249], loss=88.0699
	step [123/249], loss=98.5400
	step [124/249], loss=90.8698
	step [125/249], loss=111.8008
	step [126/249], loss=97.9215
	step [127/249], loss=105.6790
	step [128/249], loss=98.4964
	step [129/249], loss=106.2211
	step [130/249], loss=90.6660
	step [131/249], loss=107.4652
	step [132/249], loss=98.3602
	step [133/249], loss=110.0876
	step [134/249], loss=107.2700
	step [135/249], loss=91.2872
	step [136/249], loss=84.4729
	step [137/249], loss=92.3782
	step [138/249], loss=89.6224
	step [139/249], loss=109.2582
	step [140/249], loss=117.1014
	step [141/249], loss=110.8230
	step [142/249], loss=92.0822
	step [143/249], loss=116.5729
	step [144/249], loss=107.9964
	step [145/249], loss=96.2820
	step [146/249], loss=92.9329
	step [147/249], loss=123.1223
	step [148/249], loss=102.3682
	step [149/249], loss=85.8093
	step [150/249], loss=100.9906
	step [151/249], loss=88.2866
	step [152/249], loss=87.4865
	step [153/249], loss=103.0466
	step [154/249], loss=96.3832
	step [155/249], loss=102.4010
	step [156/249], loss=111.6799
	step [157/249], loss=108.4150
	step [158/249], loss=107.9547
	step [159/249], loss=118.3862
	step [160/249], loss=117.3477
	step [161/249], loss=108.7363
	step [162/249], loss=102.4909
	step [163/249], loss=94.4178
	step [164/249], loss=120.2728
	step [165/249], loss=94.5013
	step [166/249], loss=84.2011
	step [167/249], loss=112.9348
	step [168/249], loss=105.1714
	step [169/249], loss=96.5887
	step [170/249], loss=91.0067
	step [171/249], loss=88.1713
	step [172/249], loss=104.4870
	step [173/249], loss=105.6419
	step [174/249], loss=88.4956
	step [175/249], loss=113.8678
	step [176/249], loss=73.7271
	step [177/249], loss=98.6206
	step [178/249], loss=97.1683
	step [179/249], loss=73.5127
	step [180/249], loss=106.6115
	step [181/249], loss=86.8985
	step [182/249], loss=115.5574
	step [183/249], loss=107.0609
	step [184/249], loss=83.3058
	step [185/249], loss=102.0222
	step [186/249], loss=106.7740
	step [187/249], loss=91.1602
	step [188/249], loss=97.5502
	step [189/249], loss=90.6194
	step [190/249], loss=94.5572
	step [191/249], loss=89.2633
	step [192/249], loss=116.4676
	step [193/249], loss=110.5882
	step [194/249], loss=100.6242
	step [195/249], loss=101.0452
	step [196/249], loss=111.0317
	step [197/249], loss=103.7394
	step [198/249], loss=118.9298
	step [199/249], loss=106.0228
	step [200/249], loss=101.7587
	step [201/249], loss=103.0776
	step [202/249], loss=106.2590
	step [203/249], loss=102.4511
	step [204/249], loss=102.4286
	step [205/249], loss=104.0601
	step [206/249], loss=99.2982
	step [207/249], loss=127.4607
	step [208/249], loss=106.8449
	step [209/249], loss=91.8280
	step [210/249], loss=105.9570
	step [211/249], loss=108.3445
	step [212/249], loss=105.5742
	step [213/249], loss=99.5982
	step [214/249], loss=105.6103
	step [215/249], loss=93.7709
	step [216/249], loss=97.4052
	step [217/249], loss=108.7620
	step [218/249], loss=96.0663
	step [219/249], loss=84.8322
	step [220/249], loss=93.6285
	step [221/249], loss=111.0827
	step [222/249], loss=97.0333
	step [223/249], loss=109.3048
	step [224/249], loss=105.4231
	step [225/249], loss=91.7219
	step [226/249], loss=102.4724
	step [227/249], loss=96.4563
	step [228/249], loss=88.4570
	step [229/249], loss=100.8419
	step [230/249], loss=116.2404
	step [231/249], loss=110.1386
	step [232/249], loss=108.4282
	step [233/249], loss=106.6724
	step [234/249], loss=94.5894
	step [235/249], loss=115.1838
	step [236/249], loss=101.2238
	step [237/249], loss=112.7944
	step [238/249], loss=99.8945
	step [239/249], loss=92.3995
	step [240/249], loss=78.9979
	step [241/249], loss=101.4872
	step [242/249], loss=85.9883
	step [243/249], loss=105.9051
	step [244/249], loss=88.5386
	step [245/249], loss=95.1596
	step [246/249], loss=83.8730
	step [247/249], loss=97.9386
	step [248/249], loss=116.8392
	step [249/249], loss=63.1071
	Evaluating
	loss=0.0115, precision=0.4197, recall=0.8920, f1=0.5708
Training epoch 18
	step [1/249], loss=106.0076
	step [2/249], loss=106.1087
	step [3/249], loss=105.1646
	step [4/249], loss=98.4403
	step [5/249], loss=114.9756
	step [6/249], loss=88.3924
	step [7/249], loss=117.8859
	step [8/249], loss=106.1276
	step [9/249], loss=93.5151
	step [10/249], loss=96.5615
	step [11/249], loss=90.2188
	step [12/249], loss=112.1644
	step [13/249], loss=94.9076
	step [14/249], loss=98.9355
	step [15/249], loss=103.5085
	step [16/249], loss=102.3332
	step [17/249], loss=100.3799
	step [18/249], loss=107.2292
	step [19/249], loss=104.0367
	step [20/249], loss=100.0435
	step [21/249], loss=94.5615
	step [22/249], loss=94.0412
	step [23/249], loss=102.5846
	step [24/249], loss=102.6087
	step [25/249], loss=112.5254
	step [26/249], loss=116.6805
	step [27/249], loss=76.3071
	step [28/249], loss=91.4369
	step [29/249], loss=95.4680
	step [30/249], loss=99.4162
	step [31/249], loss=103.0594
	step [32/249], loss=100.4707
	step [33/249], loss=90.0534
	step [34/249], loss=123.7249
	step [35/249], loss=84.9448
	step [36/249], loss=106.1456
	step [37/249], loss=113.3950
	step [38/249], loss=107.4088
	step [39/249], loss=85.3790
	step [40/249], loss=94.3452
	step [41/249], loss=94.8700
	step [42/249], loss=98.3041
	step [43/249], loss=106.9083
	step [44/249], loss=92.4012
	step [45/249], loss=120.2619
	step [46/249], loss=85.7598
	step [47/249], loss=115.0595
	step [48/249], loss=111.9297
	step [49/249], loss=118.4243
	step [50/249], loss=102.7608
	step [51/249], loss=97.1144
	step [52/249], loss=106.3915
	step [53/249], loss=95.0978
	step [54/249], loss=130.3680
	step [55/249], loss=115.4346
	step [56/249], loss=91.9714
	step [57/249], loss=113.2896
	step [58/249], loss=105.0577
	step [59/249], loss=87.0602
	step [60/249], loss=123.1739
	step [61/249], loss=124.3136
	step [62/249], loss=113.5849
	step [63/249], loss=92.6294
	step [64/249], loss=75.9341
	step [65/249], loss=85.2157
	step [66/249], loss=90.7814
	step [67/249], loss=103.5454
	step [68/249], loss=106.0064
	step [69/249], loss=97.5667
	step [70/249], loss=88.2824
	step [71/249], loss=117.4525
	step [72/249], loss=92.5865
	step [73/249], loss=85.4952
	step [74/249], loss=106.8746
	step [75/249], loss=79.1220
	step [76/249], loss=106.8987
	step [77/249], loss=89.7437
	step [78/249], loss=80.6192
	step [79/249], loss=107.9838
	step [80/249], loss=109.5512
	step [81/249], loss=97.9757
	step [82/249], loss=98.2328
	step [83/249], loss=112.1921
	step [84/249], loss=98.4696
	step [85/249], loss=118.8851
	step [86/249], loss=87.1593
	step [87/249], loss=117.4663
	step [88/249], loss=86.4828
	step [89/249], loss=104.1939
	step [90/249], loss=95.9859
	step [91/249], loss=86.6949
	step [92/249], loss=103.9601
	step [93/249], loss=98.8008
	step [94/249], loss=94.6962
	step [95/249], loss=101.1227
	step [96/249], loss=102.5466
	step [97/249], loss=104.0790
	step [98/249], loss=92.9788
	step [99/249], loss=97.2547
	step [100/249], loss=108.1837
	step [101/249], loss=106.7952
	step [102/249], loss=96.0966
	step [103/249], loss=101.3763
	step [104/249], loss=107.3014
	step [105/249], loss=102.0463
	step [106/249], loss=95.6711
	step [107/249], loss=82.2598
	step [108/249], loss=93.7222
	step [109/249], loss=98.2809
	step [110/249], loss=107.2007
	step [111/249], loss=111.5729
	step [112/249], loss=104.1273
	step [113/249], loss=98.6627
	step [114/249], loss=99.8732
	step [115/249], loss=82.0118
	step [116/249], loss=105.8271
	step [117/249], loss=98.3025
	step [118/249], loss=121.4402
	step [119/249], loss=89.1611
	step [120/249], loss=109.4898
	step [121/249], loss=113.4149
	step [122/249], loss=101.2992
	step [123/249], loss=107.5545
	step [124/249], loss=82.4983
	step [125/249], loss=101.7080
	step [126/249], loss=112.0075
	step [127/249], loss=99.0535
	step [128/249], loss=114.1712
	step [129/249], loss=96.4368
	step [130/249], loss=77.9383
	step [131/249], loss=109.8292
	step [132/249], loss=113.9469
	step [133/249], loss=101.9640
	step [134/249], loss=102.0010
	step [135/249], loss=110.5276
	step [136/249], loss=98.7665
	step [137/249], loss=107.9583
	step [138/249], loss=100.3225
	step [139/249], loss=91.6223
	step [140/249], loss=101.0878
	step [141/249], loss=86.7904
	step [142/249], loss=84.5511
	step [143/249], loss=103.5834
	step [144/249], loss=94.2499
	step [145/249], loss=104.4387
	step [146/249], loss=107.0896
	step [147/249], loss=87.8297
	step [148/249], loss=111.0123
	step [149/249], loss=87.0724
	step [150/249], loss=107.3981
	step [151/249], loss=113.4910
	step [152/249], loss=89.2681
	step [153/249], loss=125.3264
	step [154/249], loss=110.7807
	step [155/249], loss=85.8331
	step [156/249], loss=90.8714
	step [157/249], loss=100.0123
	step [158/249], loss=94.7136
	step [159/249], loss=84.2009
	step [160/249], loss=103.3261
	step [161/249], loss=94.8781
	step [162/249], loss=102.2791
	step [163/249], loss=98.8678
	step [164/249], loss=112.7975
	step [165/249], loss=93.9064
	step [166/249], loss=110.0338
	step [167/249], loss=107.9019
	step [168/249], loss=96.1109
	step [169/249], loss=95.6847
	step [170/249], loss=78.8077
	step [171/249], loss=90.1081
	step [172/249], loss=103.5979
	step [173/249], loss=79.6241
	step [174/249], loss=105.7891
	step [175/249], loss=96.0178
	step [176/249], loss=95.2043
	step [177/249], loss=94.2019
	step [178/249], loss=107.1891
	step [179/249], loss=91.8394
	step [180/249], loss=94.4089
	step [181/249], loss=97.1451
	step [182/249], loss=103.6324
	step [183/249], loss=96.2685
	step [184/249], loss=101.6818
	step [185/249], loss=102.1091
	step [186/249], loss=86.3935
	step [187/249], loss=103.5202
	step [188/249], loss=77.2209
	step [189/249], loss=90.2776
	step [190/249], loss=106.6415
	step [191/249], loss=115.9328
	step [192/249], loss=121.1579
	step [193/249], loss=85.9792
	step [194/249], loss=81.7536
	step [195/249], loss=101.5481
	step [196/249], loss=91.2123
	step [197/249], loss=103.0428
	step [198/249], loss=90.8162
	step [199/249], loss=98.8750
	step [200/249], loss=112.7083
	step [201/249], loss=87.2049
	step [202/249], loss=96.8558
	step [203/249], loss=98.2734
	step [204/249], loss=116.1484
	step [205/249], loss=115.6439
	step [206/249], loss=95.5907
	step [207/249], loss=99.0603
	step [208/249], loss=81.7104
	step [209/249], loss=105.6031
	step [210/249], loss=102.1059
	step [211/249], loss=91.7128
	step [212/249], loss=113.5888
	step [213/249], loss=104.7413
	step [214/249], loss=87.6632
	step [215/249], loss=86.3087
	step [216/249], loss=96.7910
	step [217/249], loss=88.7634
	step [218/249], loss=102.1414
	step [219/249], loss=106.0317
	step [220/249], loss=96.0369
	step [221/249], loss=104.1842
	step [222/249], loss=94.0836
	step [223/249], loss=106.8485
	step [224/249], loss=87.2488
	step [225/249], loss=92.9826
	step [226/249], loss=85.5996
	step [227/249], loss=102.8369
	step [228/249], loss=114.0081
	step [229/249], loss=100.6121
	step [230/249], loss=94.8961
	step [231/249], loss=80.3731
	step [232/249], loss=83.4695
	step [233/249], loss=124.3917
	step [234/249], loss=103.2139
	step [235/249], loss=80.2211
	step [236/249], loss=113.8218
	step [237/249], loss=105.0591
	step [238/249], loss=74.4014
	step [239/249], loss=95.0630
	step [240/249], loss=106.5319
	step [241/249], loss=81.3092
	step [242/249], loss=95.3898
	step [243/249], loss=101.3175
	step [244/249], loss=102.3787
	step [245/249], loss=77.7886
	step [246/249], loss=87.8304
	step [247/249], loss=99.9680
	step [248/249], loss=84.5754
	step [249/249], loss=74.8616
	Evaluating
	loss=0.0138, precision=0.3317, recall=0.9173, f1=0.4872
Training epoch 19
	step [1/249], loss=96.2109
	step [2/249], loss=109.0192
	step [3/249], loss=91.7383
	step [4/249], loss=108.6251
	step [5/249], loss=89.1118
	step [6/249], loss=97.4389
	step [7/249], loss=104.7399
	step [8/249], loss=96.9575
	step [9/249], loss=88.3197
	step [10/249], loss=98.5349
	step [11/249], loss=98.0125
	step [12/249], loss=118.0112
	step [13/249], loss=82.7456
	step [14/249], loss=102.3380
	step [15/249], loss=95.3436
	step [16/249], loss=98.1820
	step [17/249], loss=108.9297
	step [18/249], loss=96.2736
	step [19/249], loss=97.1200
	step [20/249], loss=96.3188
	step [21/249], loss=96.2850
	step [22/249], loss=109.7342
	step [23/249], loss=97.5208
	step [24/249], loss=98.8228
	step [25/249], loss=99.5585
	step [26/249], loss=108.9178
	step [27/249], loss=117.6393
	step [28/249], loss=112.0651
	step [29/249], loss=95.0750
	step [30/249], loss=110.0738
	step [31/249], loss=93.3985
	step [32/249], loss=93.4009
	step [33/249], loss=99.3843
	step [34/249], loss=106.9605
	step [35/249], loss=92.1379
	step [36/249], loss=96.1754
	step [37/249], loss=104.1490
	step [38/249], loss=81.2969
	step [39/249], loss=81.6882
	step [40/249], loss=103.5120
	step [41/249], loss=87.1273
	step [42/249], loss=105.4930
	step [43/249], loss=90.6208
	step [44/249], loss=96.6793
	step [45/249], loss=89.8522
	step [46/249], loss=109.7450
	step [47/249], loss=84.5747
	step [48/249], loss=96.3856
	step [49/249], loss=102.3108
	step [50/249], loss=94.4800
	step [51/249], loss=93.6205
	step [52/249], loss=83.8836
	step [53/249], loss=106.5620
	step [54/249], loss=86.8867
	step [55/249], loss=109.5946
	step [56/249], loss=109.1576
	step [57/249], loss=115.4099
	step [58/249], loss=108.2595
	step [59/249], loss=77.4581
	step [60/249], loss=91.2718
	step [61/249], loss=131.7391
	step [62/249], loss=101.3747
	step [63/249], loss=87.3893
	step [64/249], loss=101.4805
	step [65/249], loss=84.2910
	step [66/249], loss=105.6533
	step [67/249], loss=116.1610
	step [68/249], loss=104.3983
	step [69/249], loss=101.7274
	step [70/249], loss=102.1807
	step [71/249], loss=99.4475
	step [72/249], loss=81.1750
	step [73/249], loss=93.7270
	step [74/249], loss=95.3399
	step [75/249], loss=116.8024
	step [76/249], loss=117.0165
	step [77/249], loss=98.2810
	step [78/249], loss=97.3597
	step [79/249], loss=96.6074
	step [80/249], loss=126.4432
	step [81/249], loss=103.1400
	step [82/249], loss=112.0228
	step [83/249], loss=129.1667
	step [84/249], loss=100.2875
	step [85/249], loss=105.1834
	step [86/249], loss=101.5428
	step [87/249], loss=82.1405
	step [88/249], loss=110.7638
	step [89/249], loss=105.3980
	step [90/249], loss=90.2991
	step [91/249], loss=103.5344
	step [92/249], loss=107.8162
	step [93/249], loss=86.4183
	step [94/249], loss=85.0459
	step [95/249], loss=121.7386
	step [96/249], loss=103.6996
	step [97/249], loss=98.5582
	step [98/249], loss=87.9390
	step [99/249], loss=103.0144
	step [100/249], loss=101.3406
	step [101/249], loss=96.0534
	step [102/249], loss=102.0043
	step [103/249], loss=100.7064
	step [104/249], loss=80.3164
	step [105/249], loss=98.6722
	step [106/249], loss=95.2600
	step [107/249], loss=99.6887
	step [108/249], loss=109.1640
	step [109/249], loss=80.4465
	step [110/249], loss=109.6704
	step [111/249], loss=103.9296
	step [112/249], loss=93.6078
	step [113/249], loss=101.9379
	step [114/249], loss=77.4362
	step [115/249], loss=95.8492
	step [116/249], loss=96.3689
	step [117/249], loss=81.4666
	step [118/249], loss=106.9707
	step [119/249], loss=93.1381
	step [120/249], loss=105.4274
	step [121/249], loss=126.4232
	step [122/249], loss=97.7572
	step [123/249], loss=86.3153
	step [124/249], loss=119.1830
	step [125/249], loss=97.1118
	step [126/249], loss=87.5177
	step [127/249], loss=83.3070
	step [128/249], loss=98.2985
	step [129/249], loss=85.9723
	step [130/249], loss=90.5210
	step [131/249], loss=95.2985
	step [132/249], loss=116.1999
	step [133/249], loss=96.3983
	step [134/249], loss=99.3239
	step [135/249], loss=103.1441
	step [136/249], loss=94.5687
	step [137/249], loss=109.3127
	step [138/249], loss=108.4668
	step [139/249], loss=121.2638
	step [140/249], loss=89.8412
	step [141/249], loss=103.9576
	step [142/249], loss=107.4714
	step [143/249], loss=124.1759
	step [144/249], loss=91.9536
	step [145/249], loss=97.5256
	step [146/249], loss=102.0547
	step [147/249], loss=95.2838
	step [148/249], loss=95.9023
	step [149/249], loss=98.1541
	step [150/249], loss=91.4455
	step [151/249], loss=98.6568
	step [152/249], loss=100.5493
	step [153/249], loss=105.2203
	step [154/249], loss=91.7123
	step [155/249], loss=90.8090
	step [156/249], loss=101.9702
	step [157/249], loss=81.3019
	step [158/249], loss=81.3330
	step [159/249], loss=110.9059
	step [160/249], loss=84.4839
	step [161/249], loss=105.9418
	step [162/249], loss=117.2002
	step [163/249], loss=90.7784
	step [164/249], loss=83.9274
	step [165/249], loss=108.0766
	step [166/249], loss=90.5521
	step [167/249], loss=80.8921
	step [168/249], loss=93.4258
	step [169/249], loss=87.7881
	step [170/249], loss=93.2283
	step [171/249], loss=102.1204
	step [172/249], loss=115.2370
	step [173/249], loss=98.7918
	step [174/249], loss=100.0945
	step [175/249], loss=96.5660
	step [176/249], loss=94.6855
	step [177/249], loss=127.9389
	step [178/249], loss=101.2412
	step [179/249], loss=95.1853
	step [180/249], loss=91.9656
	step [181/249], loss=110.3023
	step [182/249], loss=123.1870
	step [183/249], loss=93.6474
	step [184/249], loss=102.9058
	step [185/249], loss=78.2068
	step [186/249], loss=108.3307
	step [187/249], loss=97.0437
	step [188/249], loss=92.7125
	step [189/249], loss=116.1258
	step [190/249], loss=110.0401
	step [191/249], loss=98.5147
	step [192/249], loss=114.7280
	step [193/249], loss=89.2421
	step [194/249], loss=72.5583
	step [195/249], loss=88.1877
	step [196/249], loss=92.6392
	step [197/249], loss=93.4458
	step [198/249], loss=75.0972
	step [199/249], loss=101.8502
	step [200/249], loss=112.3338
	step [201/249], loss=91.9660
	step [202/249], loss=102.4108
	step [203/249], loss=98.2517
	step [204/249], loss=113.9214
	step [205/249], loss=95.6328
	step [206/249], loss=103.4311
	step [207/249], loss=104.7599
	step [208/249], loss=94.3234
	step [209/249], loss=90.4891
	step [210/249], loss=80.9565
	step [211/249], loss=93.4089
	step [212/249], loss=109.3989
	step [213/249], loss=91.5104
	step [214/249], loss=98.1848
	step [215/249], loss=87.7500
	step [216/249], loss=86.0412
	step [217/249], loss=80.8592
	step [218/249], loss=101.5572
	step [219/249], loss=92.1473
	step [220/249], loss=91.6891
	step [221/249], loss=98.8149
	step [222/249], loss=109.8268
	step [223/249], loss=105.2741
	step [224/249], loss=126.7481
	step [225/249], loss=79.3397
	step [226/249], loss=88.5789
	step [227/249], loss=97.2928
	step [228/249], loss=108.1571
	step [229/249], loss=87.6171
	step [230/249], loss=94.7514
	step [231/249], loss=84.0478
	step [232/249], loss=105.7796
	step [233/249], loss=98.9710
	step [234/249], loss=81.2957
	step [235/249], loss=94.0242
	step [236/249], loss=102.3946
	step [237/249], loss=103.2467
	step [238/249], loss=93.0808
	step [239/249], loss=104.2966
	step [240/249], loss=91.3145
	step [241/249], loss=113.1850
	step [242/249], loss=109.9332
	step [243/249], loss=92.5123
	step [244/249], loss=87.6797
	step [245/249], loss=113.1317
	step [246/249], loss=87.9251
	step [247/249], loss=121.4082
	step [248/249], loss=99.9995
	step [249/249], loss=75.3797
	Evaluating
	loss=0.0115, precision=0.3439, recall=0.9268, f1=0.5017
Training epoch 20
	step [1/249], loss=104.4389
	step [2/249], loss=83.2045
	step [3/249], loss=108.1689
	step [4/249], loss=99.0219
	step [5/249], loss=104.7158
	step [6/249], loss=99.8645
	step [7/249], loss=101.7246
	step [8/249], loss=94.4076
	step [9/249], loss=83.8806
	step [10/249], loss=109.8648
	step [11/249], loss=111.3798
	step [12/249], loss=117.6558
	step [13/249], loss=109.6695
	step [14/249], loss=88.3631
	step [15/249], loss=87.0501
	step [16/249], loss=84.6803
	step [17/249], loss=92.6938
	step [18/249], loss=82.4855
	step [19/249], loss=85.2073
	step [20/249], loss=100.1051
	step [21/249], loss=106.9444
	step [22/249], loss=102.6723
	step [23/249], loss=107.9169
	step [24/249], loss=89.1966
	step [25/249], loss=89.4851
	step [26/249], loss=96.8339
	step [27/249], loss=78.4387
	step [28/249], loss=92.7306
	step [29/249], loss=81.9294
	step [30/249], loss=106.3130
	step [31/249], loss=77.4857
	step [32/249], loss=109.7932
	step [33/249], loss=104.0682
	step [34/249], loss=94.9622
	step [35/249], loss=89.3407
	step [36/249], loss=93.8200
	step [37/249], loss=100.9318
	step [38/249], loss=98.2710
	step [39/249], loss=108.9744
	step [40/249], loss=114.2753
	step [41/249], loss=113.0075
	step [42/249], loss=97.8005
	step [43/249], loss=98.6895
	step [44/249], loss=87.8735
	step [45/249], loss=92.9132
	step [46/249], loss=104.4009
	step [47/249], loss=100.9806
	step [48/249], loss=87.6657
	step [49/249], loss=100.0574
	step [50/249], loss=101.6365
	step [51/249], loss=106.5093
	step [52/249], loss=123.9594
	step [53/249], loss=98.4931
	step [54/249], loss=92.4510
	step [55/249], loss=99.0503
	step [56/249], loss=94.9646
	step [57/249], loss=102.4887
	step [58/249], loss=101.7726
	step [59/249], loss=108.5001
	step [60/249], loss=94.8677
	step [61/249], loss=89.1755
	step [62/249], loss=104.4217
	step [63/249], loss=102.1979
	step [64/249], loss=82.7933
	step [65/249], loss=100.5615
	step [66/249], loss=98.6646
	step [67/249], loss=108.4455
	step [68/249], loss=100.8518
	step [69/249], loss=108.2777
	step [70/249], loss=89.5183
	step [71/249], loss=91.0860
	step [72/249], loss=104.3090
	step [73/249], loss=85.0042
	step [74/249], loss=99.4909
	step [75/249], loss=98.5802
	step [76/249], loss=95.5054
	step [77/249], loss=100.9728
	step [78/249], loss=93.1973
	step [79/249], loss=103.3138
	step [80/249], loss=96.1136
	step [81/249], loss=108.4135
	step [82/249], loss=104.8507
	step [83/249], loss=112.1448
	step [84/249], loss=98.0541
	step [85/249], loss=93.9342
	step [86/249], loss=92.9125
	step [87/249], loss=91.9949
	step [88/249], loss=106.0565
	step [89/249], loss=87.5480
	step [90/249], loss=110.9898
	step [91/249], loss=95.4894
	step [92/249], loss=87.9407
	step [93/249], loss=102.5074
	step [94/249], loss=76.2513
	step [95/249], loss=104.2610
	step [96/249], loss=104.8167
	step [97/249], loss=107.9756
	step [98/249], loss=81.7462
	step [99/249], loss=102.1404
	step [100/249], loss=97.9570
	step [101/249], loss=95.0374
	step [102/249], loss=98.6189
	step [103/249], loss=84.6074
	step [104/249], loss=67.8962
	step [105/249], loss=93.9696
	step [106/249], loss=89.4348
	step [107/249], loss=91.9274
	step [108/249], loss=98.4282
	step [109/249], loss=95.9386
	step [110/249], loss=96.9988
	step [111/249], loss=90.2792
	step [112/249], loss=109.9497
	step [113/249], loss=121.7145
	step [114/249], loss=97.5364
	step [115/249], loss=101.4611
	step [116/249], loss=86.9784
	step [117/249], loss=111.5354
	step [118/249], loss=101.0664
	step [119/249], loss=103.5750
	step [120/249], loss=85.3205
	step [121/249], loss=96.6277
	step [122/249], loss=101.9421
	step [123/249], loss=89.6603
	step [124/249], loss=74.8211
	step [125/249], loss=98.8962
	step [126/249], loss=99.8422
	step [127/249], loss=92.3277
	step [128/249], loss=102.8737
	step [129/249], loss=115.1051
	step [130/249], loss=79.9369
	step [131/249], loss=104.8688
	step [132/249], loss=102.5490
	step [133/249], loss=103.3472
	step [134/249], loss=109.2226
	step [135/249], loss=89.2322
	step [136/249], loss=92.9168
	step [137/249], loss=114.5611
	step [138/249], loss=87.6364
	step [139/249], loss=110.9358
	step [140/249], loss=92.3711
	step [141/249], loss=95.2273
	step [142/249], loss=95.5974
	step [143/249], loss=99.1653
	step [144/249], loss=85.8504
	step [145/249], loss=88.0036
	step [146/249], loss=106.1420
	step [147/249], loss=89.8056
	step [148/249], loss=88.5872
	step [149/249], loss=98.2886
	step [150/249], loss=96.6821
	step [151/249], loss=91.7838
	step [152/249], loss=86.5210
	step [153/249], loss=102.6162
	step [154/249], loss=93.9342
	step [155/249], loss=103.8944
	step [156/249], loss=105.6639
	step [157/249], loss=98.0381
	step [158/249], loss=105.5536
	step [159/249], loss=105.9524
	step [160/249], loss=99.1131
	step [161/249], loss=104.6372
	step [162/249], loss=103.5274
	step [163/249], loss=114.7044
	step [164/249], loss=105.5126
	step [165/249], loss=87.8604
	step [166/249], loss=99.0566
	step [167/249], loss=107.2589
	step [168/249], loss=88.2506
	step [169/249], loss=105.7312
	step [170/249], loss=92.7514
	step [171/249], loss=89.6600
	step [172/249], loss=106.2056
	step [173/249], loss=124.7396
	step [174/249], loss=122.8001
	step [175/249], loss=89.4615
	step [176/249], loss=86.7301
	step [177/249], loss=70.5324
	step [178/249], loss=104.9166
	step [179/249], loss=98.4296
	step [180/249], loss=103.6676
	step [181/249], loss=97.2007
	step [182/249], loss=88.0583
	step [183/249], loss=108.3248
	step [184/249], loss=80.8111
	step [185/249], loss=122.3896
	step [186/249], loss=94.3038
	step [187/249], loss=101.1313
	step [188/249], loss=101.3130
	step [189/249], loss=89.6659
	step [190/249], loss=93.1958
	step [191/249], loss=105.8775
	step [192/249], loss=97.6334
	step [193/249], loss=91.5389
	step [194/249], loss=109.7300
	step [195/249], loss=94.9202
	step [196/249], loss=84.9932
	step [197/249], loss=97.8052
	step [198/249], loss=101.4226
	step [199/249], loss=70.0595
	step [200/249], loss=114.2732
	step [201/249], loss=115.6937
	step [202/249], loss=72.5242
	step [203/249], loss=94.3599
	step [204/249], loss=91.6870
	step [205/249], loss=92.6058
	step [206/249], loss=79.3437
	step [207/249], loss=100.9620
	step [208/249], loss=92.0761
	step [209/249], loss=98.8175
	step [210/249], loss=74.2453
	step [211/249], loss=112.4055
	step [212/249], loss=90.8924
	step [213/249], loss=98.9301
	step [214/249], loss=103.9127
	step [215/249], loss=107.0590
	step [216/249], loss=119.2995
	step [217/249], loss=94.1043
	step [218/249], loss=94.3361
	step [219/249], loss=104.8484
	step [220/249], loss=110.0070
	step [221/249], loss=95.6003
	step [222/249], loss=119.9990
	step [223/249], loss=95.2912
	step [224/249], loss=92.7397
	step [225/249], loss=99.0390
	step [226/249], loss=105.4412
	step [227/249], loss=89.2526
	step [228/249], loss=91.2247
	step [229/249], loss=81.8484
	step [230/249], loss=93.9234
	step [231/249], loss=98.3634
	step [232/249], loss=92.4007
	step [233/249], loss=102.9903
	step [234/249], loss=108.9388
	step [235/249], loss=93.1431
	step [236/249], loss=93.4912
	step [237/249], loss=94.3002
	step [238/249], loss=92.1031
	step [239/249], loss=73.3394
	step [240/249], loss=92.6946
	step [241/249], loss=91.3258
	step [242/249], loss=102.3226
	step [243/249], loss=105.7887
	step [244/249], loss=88.0451
	step [245/249], loss=106.4499
	step [246/249], loss=87.7855
	step [247/249], loss=97.9990
	step [248/249], loss=95.2338
	step [249/249], loss=75.7064
	Evaluating
	loss=0.0105, precision=0.3733, recall=0.9106, f1=0.5295
Training epoch 21
	step [1/249], loss=103.3194
	step [2/249], loss=86.7448
	step [3/249], loss=92.9271
	step [4/249], loss=90.3042
	step [5/249], loss=102.9306
	step [6/249], loss=96.0000
	step [7/249], loss=106.7043
	step [8/249], loss=78.2119
	step [9/249], loss=85.2655
	step [10/249], loss=91.7009
	step [11/249], loss=103.2117
	step [12/249], loss=95.3360
	step [13/249], loss=88.4065
	step [14/249], loss=93.3877
	step [15/249], loss=101.5984
	step [16/249], loss=94.3924
	step [17/249], loss=107.9003
	step [18/249], loss=106.2112
	step [19/249], loss=86.8577
	step [20/249], loss=105.2400
	step [21/249], loss=88.6060
	step [22/249], loss=84.9113
	step [23/249], loss=104.5622
	step [24/249], loss=79.1225
	step [25/249], loss=106.3946
	step [26/249], loss=89.5790
	step [27/249], loss=95.8284
	step [28/249], loss=100.4959
	step [29/249], loss=81.3861
	step [30/249], loss=89.5792
	step [31/249], loss=98.9219
	step [32/249], loss=100.2375
	step [33/249], loss=80.5625
	step [34/249], loss=95.3600
	step [35/249], loss=85.6033
	step [36/249], loss=110.2745
	step [37/249], loss=104.1534
	step [38/249], loss=98.8345
	step [39/249], loss=97.1351
	step [40/249], loss=86.7063
	step [41/249], loss=69.9957
	step [42/249], loss=96.6532
	step [43/249], loss=95.1785
	step [44/249], loss=89.0888
	step [45/249], loss=96.9791
	step [46/249], loss=104.4020
	step [47/249], loss=98.5937
	step [48/249], loss=95.3798
	step [49/249], loss=98.7762
	step [50/249], loss=83.7419
	step [51/249], loss=94.4852
	step [52/249], loss=112.1065
	step [53/249], loss=110.2446
	step [54/249], loss=105.8069
	step [55/249], loss=96.6881
	step [56/249], loss=94.6039
	step [57/249], loss=100.6712
	step [58/249], loss=105.3737
	step [59/249], loss=98.4827
	step [60/249], loss=105.5921
	step [61/249], loss=91.1187
	step [62/249], loss=101.0515
	step [63/249], loss=123.0720
	step [64/249], loss=75.3120
	step [65/249], loss=93.2697
	step [66/249], loss=102.0820
	step [67/249], loss=96.3227
	step [68/249], loss=93.8980
	step [69/249], loss=97.6307
	step [70/249], loss=113.4988
	step [71/249], loss=109.4376
	step [72/249], loss=122.9785
	step [73/249], loss=97.3899
	step [74/249], loss=76.2452
	step [75/249], loss=101.5441
	step [76/249], loss=105.5412
	step [77/249], loss=79.4533
	step [78/249], loss=107.9827
	step [79/249], loss=89.1595
	step [80/249], loss=94.2148
	step [81/249], loss=107.2758
	step [82/249], loss=85.0413
	step [83/249], loss=110.8889
	step [84/249], loss=102.0169
	step [85/249], loss=99.5285
	step [86/249], loss=102.6671
	step [87/249], loss=114.5726
	step [88/249], loss=104.7247
	step [89/249], loss=89.9420
	step [90/249], loss=93.7040
	step [91/249], loss=97.7031
	step [92/249], loss=82.1922
	step [93/249], loss=80.1075
	step [94/249], loss=89.3851
	step [95/249], loss=100.0086
	step [96/249], loss=91.2745
	step [97/249], loss=95.7979
	step [98/249], loss=89.7506
	step [99/249], loss=90.9639
	step [100/249], loss=101.5168
	step [101/249], loss=94.7661
	step [102/249], loss=105.9811
	step [103/249], loss=91.1430
	step [104/249], loss=90.5006
	step [105/249], loss=92.4356
	step [106/249], loss=80.3036
	step [107/249], loss=81.3077
	step [108/249], loss=87.5488
	step [109/249], loss=78.2835
	step [110/249], loss=90.4971
	step [111/249], loss=104.3402
	step [112/249], loss=85.7791
	step [113/249], loss=86.2815
	step [114/249], loss=112.9805
	step [115/249], loss=85.0850
	step [116/249], loss=103.8981
	step [117/249], loss=88.3953
	step [118/249], loss=115.0813
	step [119/249], loss=90.5638
	step [120/249], loss=93.8683
	step [121/249], loss=116.7686
	step [122/249], loss=115.3799
	step [123/249], loss=78.2174
	step [124/249], loss=85.9714
	step [125/249], loss=102.7935
	step [126/249], loss=118.6014
	step [127/249], loss=106.0296
	step [128/249], loss=99.4801
	step [129/249], loss=105.2827
	step [130/249], loss=86.7631
	step [131/249], loss=106.9523
	step [132/249], loss=92.6078
	step [133/249], loss=95.6186
	step [134/249], loss=104.9451
	step [135/249], loss=109.4880
	step [136/249], loss=102.7550
	step [137/249], loss=84.1494
	step [138/249], loss=79.6478
	step [139/249], loss=108.4291
	step [140/249], loss=110.4909
	step [141/249], loss=101.3753
	step [142/249], loss=82.6233
	step [143/249], loss=86.5045
	step [144/249], loss=103.4247
	step [145/249], loss=100.7509
	step [146/249], loss=74.6605
	step [147/249], loss=111.3497
	step [148/249], loss=82.3226
	step [149/249], loss=123.1625
	step [150/249], loss=107.8642
	step [151/249], loss=113.9409
	step [152/249], loss=92.7183
	step [153/249], loss=111.8120
	step [154/249], loss=94.3201
	step [155/249], loss=96.7897
	step [156/249], loss=92.0726
	step [157/249], loss=105.0307
	step [158/249], loss=83.7621
	step [159/249], loss=103.6806
	step [160/249], loss=101.0914
	step [161/249], loss=94.1353
	step [162/249], loss=91.8372
	step [163/249], loss=91.6477
	step [164/249], loss=93.3118
	step [165/249], loss=102.8892
	step [166/249], loss=92.2442
	step [167/249], loss=75.6732
	step [168/249], loss=123.0130
	step [169/249], loss=89.2491
	step [170/249], loss=89.0885
	step [171/249], loss=94.2136
	step [172/249], loss=106.3193
	step [173/249], loss=90.9963
	step [174/249], loss=105.2262
	step [175/249], loss=102.5285
	step [176/249], loss=91.4114
	step [177/249], loss=101.1907
	step [178/249], loss=111.0239
	step [179/249], loss=93.5544
	step [180/249], loss=95.7620
	step [181/249], loss=86.9742
	step [182/249], loss=103.4002
	step [183/249], loss=89.5209
	step [184/249], loss=79.6918
	step [185/249], loss=94.3362
	step [186/249], loss=96.5772
	step [187/249], loss=101.4732
	step [188/249], loss=105.9220
	step [189/249], loss=113.6880
	step [190/249], loss=102.4829
	step [191/249], loss=98.0628
	step [192/249], loss=97.4697
	step [193/249], loss=126.0644
	step [194/249], loss=102.8501
	step [195/249], loss=112.1430
	step [196/249], loss=90.1784
	step [197/249], loss=109.3267
	step [198/249], loss=102.3020
	step [199/249], loss=100.5872
	step [200/249], loss=96.9621
	step [201/249], loss=100.7629
	step [202/249], loss=107.4327
	step [203/249], loss=93.0201
	step [204/249], loss=88.3589
	step [205/249], loss=82.7293
	step [206/249], loss=88.4414
	step [207/249], loss=88.2022
	step [208/249], loss=89.5274
	step [209/249], loss=96.4948
	step [210/249], loss=113.7526
	step [211/249], loss=110.1669
	step [212/249], loss=108.4139
	step [213/249], loss=78.0102
	step [214/249], loss=85.6512
	step [215/249], loss=70.5057
	step [216/249], loss=94.9902
	step [217/249], loss=95.3734
	step [218/249], loss=116.4082
	step [219/249], loss=97.5303
	step [220/249], loss=88.0443
	step [221/249], loss=78.8767
	step [222/249], loss=111.2511
	step [223/249], loss=73.0605
	step [224/249], loss=92.5873
	step [225/249], loss=110.9210
	step [226/249], loss=92.7447
	step [227/249], loss=99.7509
	step [228/249], loss=86.2201
	step [229/249], loss=106.3081
	step [230/249], loss=85.0983
	step [231/249], loss=104.2326
	step [232/249], loss=105.5108
	step [233/249], loss=80.5652
	step [234/249], loss=78.6807
	step [235/249], loss=109.4376
	step [236/249], loss=92.4843
	step [237/249], loss=94.4137
	step [238/249], loss=99.0906
	step [239/249], loss=112.8333
	step [240/249], loss=108.4422
	step [241/249], loss=110.8650
	step [242/249], loss=84.0033
	step [243/249], loss=97.6237
	step [244/249], loss=97.2995
	step [245/249], loss=91.8859
	step [246/249], loss=102.5240
	step [247/249], loss=85.1784
	step [248/249], loss=82.8982
	step [249/249], loss=65.2204
	Evaluating
	loss=0.0104, precision=0.3306, recall=0.9271, f1=0.4874
Training epoch 22
	step [1/249], loss=82.7131
	step [2/249], loss=91.6436
	step [3/249], loss=96.8956
	step [4/249], loss=89.3394
	step [5/249], loss=102.4122
	step [6/249], loss=97.3925
	step [7/249], loss=82.2239
	step [8/249], loss=107.3440
	step [9/249], loss=88.6487
	step [10/249], loss=99.7572
	step [11/249], loss=78.2938
	step [12/249], loss=102.1398
	step [13/249], loss=95.8142
	step [14/249], loss=98.6501
	step [15/249], loss=91.2053
	step [16/249], loss=88.5753
	step [17/249], loss=95.6608
	step [18/249], loss=112.8993
	step [19/249], loss=96.3903
	step [20/249], loss=93.5696
	step [21/249], loss=84.0997
	step [22/249], loss=92.0524
	step [23/249], loss=97.6188
	step [24/249], loss=107.8650
	step [25/249], loss=93.2640
	step [26/249], loss=93.4884
	step [27/249], loss=96.9462
	step [28/249], loss=111.3481
	step [29/249], loss=75.7082
	step [30/249], loss=98.8768
	step [31/249], loss=83.5763
	step [32/249], loss=98.4395
	step [33/249], loss=105.4694
	step [34/249], loss=92.2418
	step [35/249], loss=94.3648
	step [36/249], loss=87.9002
	step [37/249], loss=112.9251
	step [38/249], loss=92.8319
	step [39/249], loss=123.4044
	step [40/249], loss=96.6965
	step [41/249], loss=92.1767
	step [42/249], loss=102.6526
	step [43/249], loss=73.2059
	step [44/249], loss=99.1343
	step [45/249], loss=90.6954
	step [46/249], loss=99.5323
	step [47/249], loss=91.3590
	step [48/249], loss=99.7319
	step [49/249], loss=102.1466
	step [50/249], loss=80.3759
	step [51/249], loss=98.8336
	step [52/249], loss=107.1971
	step [53/249], loss=89.1392
	step [54/249], loss=92.0660
	step [55/249], loss=99.4912
	step [56/249], loss=99.4406
	step [57/249], loss=93.2281
	step [58/249], loss=105.2582
	step [59/249], loss=89.6684
	step [60/249], loss=103.9724
	step [61/249], loss=74.2676
	step [62/249], loss=101.6866
	step [63/249], loss=89.4795
	step [64/249], loss=89.0669
	step [65/249], loss=94.7241
	step [66/249], loss=106.7179
	step [67/249], loss=115.8180
	step [68/249], loss=93.2029
	step [69/249], loss=104.7504
	step [70/249], loss=115.4479
	step [71/249], loss=102.4454
	step [72/249], loss=112.4503
	step [73/249], loss=91.5323
	step [74/249], loss=86.7609
	step [75/249], loss=102.9384
	step [76/249], loss=98.9306
	step [77/249], loss=90.0044
	step [78/249], loss=95.2441
	step [79/249], loss=107.4925
	step [80/249], loss=106.1142
	step [81/249], loss=87.0725
	step [82/249], loss=111.4340
	step [83/249], loss=91.3340
	step [84/249], loss=96.5379
	step [85/249], loss=101.4574
	step [86/249], loss=90.0396
	step [87/249], loss=94.7569
	step [88/249], loss=93.1998
	step [89/249], loss=103.7186
	step [90/249], loss=113.0415
	step [91/249], loss=93.2239
	step [92/249], loss=73.2753
	step [93/249], loss=88.5274
	step [94/249], loss=89.1458
	step [95/249], loss=107.8389
	step [96/249], loss=113.0174
	step [97/249], loss=99.1232
	step [98/249], loss=80.8876
	step [99/249], loss=98.6415
	step [100/249], loss=83.4569
	step [101/249], loss=94.9184
	step [102/249], loss=84.0146
	step [103/249], loss=82.3144
	step [104/249], loss=98.0815
	step [105/249], loss=83.6991
	step [106/249], loss=98.5003
	step [107/249], loss=93.3911
	step [108/249], loss=106.7328
	step [109/249], loss=113.9197
	step [110/249], loss=97.4478
	step [111/249], loss=87.1096
	step [112/249], loss=100.1349
	step [113/249], loss=101.1573
	step [114/249], loss=82.3418
	step [115/249], loss=84.4290
	step [116/249], loss=83.4134
	step [117/249], loss=97.5600
	step [118/249], loss=100.7500
	step [119/249], loss=89.9653
	step [120/249], loss=107.4318
	step [121/249], loss=92.5024
	step [122/249], loss=77.1349
	step [123/249], loss=89.0930
	step [124/249], loss=110.0470
	step [125/249], loss=107.9686
	step [126/249], loss=108.6848
	step [127/249], loss=95.9097
	step [128/249], loss=99.3310
	step [129/249], loss=101.8537
	step [130/249], loss=83.9989
	step [131/249], loss=96.8711
	step [132/249], loss=95.8263
	step [133/249], loss=77.3914
	step [134/249], loss=106.1684
	step [135/249], loss=99.3004
	step [136/249], loss=106.6541
	step [137/249], loss=100.4184
	step [138/249], loss=92.8486
	step [139/249], loss=97.4968
	step [140/249], loss=86.8590
	step [141/249], loss=78.0618
	step [142/249], loss=104.2736
	step [143/249], loss=83.3469
	step [144/249], loss=86.6543
	step [145/249], loss=98.2930
	step [146/249], loss=106.5745
	step [147/249], loss=91.8326
	step [148/249], loss=107.9339
	step [149/249], loss=94.9559
	step [150/249], loss=96.3823
	step [151/249], loss=85.3735
	step [152/249], loss=80.8620
	step [153/249], loss=110.8090
	step [154/249], loss=104.0087
	step [155/249], loss=96.3254
	step [156/249], loss=88.4273
	step [157/249], loss=95.3919
	step [158/249], loss=93.8801
	step [159/249], loss=97.1418
	step [160/249], loss=96.7653
	step [161/249], loss=100.6838
	step [162/249], loss=90.9656
	step [163/249], loss=93.9259
	step [164/249], loss=96.1563
	step [165/249], loss=121.8043
	step [166/249], loss=108.0322
	step [167/249], loss=102.9475
	step [168/249], loss=102.5614
	step [169/249], loss=86.2419
	step [170/249], loss=74.6554
	step [171/249], loss=92.2973
	step [172/249], loss=91.4858
	step [173/249], loss=85.5047
	step [174/249], loss=88.0360
	step [175/249], loss=102.9715
	step [176/249], loss=117.0326
	step [177/249], loss=84.5785
	step [178/249], loss=107.0645
	step [179/249], loss=100.3205
	step [180/249], loss=99.5971
	step [181/249], loss=81.5139
	step [182/249], loss=76.3859
	step [183/249], loss=103.3823
	step [184/249], loss=73.7996
	step [185/249], loss=86.6503
	step [186/249], loss=94.2179
	step [187/249], loss=94.5724
	step [188/249], loss=103.6266
	step [189/249], loss=86.9137
	step [190/249], loss=125.9516
	step [191/249], loss=81.3970
	step [192/249], loss=72.0270
	step [193/249], loss=94.0345
	step [194/249], loss=87.5283
	step [195/249], loss=97.0079
	step [196/249], loss=99.3300
	step [197/249], loss=90.9459
	step [198/249], loss=82.4697
	step [199/249], loss=101.4419
	step [200/249], loss=109.8736
	step [201/249], loss=85.0516
	step [202/249], loss=96.9164
	step [203/249], loss=92.2566
	step [204/249], loss=94.5296
	step [205/249], loss=91.8984
	step [206/249], loss=96.1875
	step [207/249], loss=100.9878
	step [208/249], loss=111.0224
	step [209/249], loss=98.0789
	step [210/249], loss=80.4392
	step [211/249], loss=108.9935
	step [212/249], loss=105.6912
	step [213/249], loss=93.8183
	step [214/249], loss=85.7833
	step [215/249], loss=81.1454
	step [216/249], loss=85.4364
	step [217/249], loss=124.0203
	step [218/249], loss=102.3062
	step [219/249], loss=89.1212
	step [220/249], loss=106.9598
	step [221/249], loss=105.1850
	step [222/249], loss=99.9189
	step [223/249], loss=76.3872
	step [224/249], loss=99.6783
	step [225/249], loss=65.4873
	step [226/249], loss=97.1608
	step [227/249], loss=101.6455
	step [228/249], loss=86.2867
	step [229/249], loss=76.3310
	step [230/249], loss=85.9050
	step [231/249], loss=94.9167
	step [232/249], loss=91.3055
	step [233/249], loss=87.7356
	step [234/249], loss=101.3514
	step [235/249], loss=107.3332
	step [236/249], loss=111.9686
	step [237/249], loss=76.5343
	step [238/249], loss=111.9288
	step [239/249], loss=117.0717
	step [240/249], loss=97.2270
	step [241/249], loss=110.2720
	step [242/249], loss=115.7568
	step [243/249], loss=85.5178
	step [244/249], loss=98.1960
	step [245/249], loss=93.9834
	step [246/249], loss=117.2590
	step [247/249], loss=72.5011
	step [248/249], loss=108.3681
	step [249/249], loss=64.7194
	Evaluating
	loss=0.0098, precision=0.3607, recall=0.8978, f1=0.5146
Training epoch 23
	step [1/249], loss=110.6680
	step [2/249], loss=104.3343
	step [3/249], loss=92.0907
	step [4/249], loss=93.0097
	step [5/249], loss=102.4926
	step [6/249], loss=79.1687
	step [7/249], loss=86.2029
	step [8/249], loss=79.6415
	step [9/249], loss=91.7201
	step [10/249], loss=94.1758
	step [11/249], loss=96.5608
	step [12/249], loss=95.3199
	step [13/249], loss=105.6388
	step [14/249], loss=92.8987
	step [15/249], loss=87.5312
	step [16/249], loss=108.4719
	step [17/249], loss=86.2290
	step [18/249], loss=92.3955
	step [19/249], loss=109.2317
	step [20/249], loss=77.1879
	step [21/249], loss=98.0108
	step [22/249], loss=91.4688
	step [23/249], loss=70.8186
	step [24/249], loss=103.2378
	step [25/249], loss=96.9723
	step [26/249], loss=99.1881
	step [27/249], loss=91.8615
	step [28/249], loss=103.5473
	step [29/249], loss=94.4570
	step [30/249], loss=96.3812
	step [31/249], loss=115.0645
	step [32/249], loss=107.3588
	step [33/249], loss=95.0544
	step [34/249], loss=102.4331
	step [35/249], loss=106.1621
	step [36/249], loss=92.8156
	step [37/249], loss=88.2082
	step [38/249], loss=98.0730
	step [39/249], loss=85.8430
	step [40/249], loss=89.8672
	step [41/249], loss=85.1705
	step [42/249], loss=93.0903
	step [43/249], loss=81.9703
	step [44/249], loss=110.3913
	step [45/249], loss=91.4455
	step [46/249], loss=95.3291
	step [47/249], loss=88.4954
	step [48/249], loss=113.0015
	step [49/249], loss=99.7946
	step [50/249], loss=103.4209
	step [51/249], loss=86.9295
	step [52/249], loss=104.5520
	step [53/249], loss=94.0216
	step [54/249], loss=91.6428
	step [55/249], loss=82.7272
	step [56/249], loss=108.3080
	step [57/249], loss=105.9319
	step [58/249], loss=91.0454
	step [59/249], loss=85.9746
	step [60/249], loss=84.1839
	step [61/249], loss=87.6863
	step [62/249], loss=101.1576
	step [63/249], loss=96.3587
	step [64/249], loss=86.8668
	step [65/249], loss=96.3368
	step [66/249], loss=96.8730
	step [67/249], loss=83.3357
	step [68/249], loss=104.9404
	step [69/249], loss=107.5584
	step [70/249], loss=95.2838
	step [71/249], loss=97.9501
	step [72/249], loss=86.3593
	step [73/249], loss=89.3435
	step [74/249], loss=91.6402
	step [75/249], loss=87.1525
	step [76/249], loss=82.0295
	step [77/249], loss=91.0502
	step [78/249], loss=80.5212
	step [79/249], loss=94.8025
	step [80/249], loss=88.5908
	step [81/249], loss=115.1143
	step [82/249], loss=90.4638
	step [83/249], loss=80.6994
	step [84/249], loss=97.9865
	step [85/249], loss=101.2025
	step [86/249], loss=101.8001
	step [87/249], loss=87.0241
	step [88/249], loss=83.9858
	step [89/249], loss=91.9102
	step [90/249], loss=103.6597
	step [91/249], loss=82.7145
	step [92/249], loss=87.6850
	step [93/249], loss=93.9468
	step [94/249], loss=89.6754
	step [95/249], loss=104.8331
	step [96/249], loss=115.1474
	step [97/249], loss=99.4820
	step [98/249], loss=101.6010
	step [99/249], loss=88.4261
	step [100/249], loss=110.9970
	step [101/249], loss=106.5265
	step [102/249], loss=98.2063
	step [103/249], loss=85.9169
	step [104/249], loss=94.0167
	step [105/249], loss=94.2490
	step [106/249], loss=85.5200
	step [107/249], loss=102.9051
	step [108/249], loss=102.7862
	step [109/249], loss=77.5044
	step [110/249], loss=86.6930
	step [111/249], loss=111.5927
	step [112/249], loss=105.0102
	step [113/249], loss=114.2182
	step [114/249], loss=84.3490
	step [115/249], loss=86.6459
	step [116/249], loss=104.1391
	step [117/249], loss=93.2412
	step [118/249], loss=93.4338
	step [119/249], loss=101.9146
	step [120/249], loss=101.1794
	step [121/249], loss=108.0975
	step [122/249], loss=82.9328
	step [123/249], loss=84.2062
	step [124/249], loss=87.1854
	step [125/249], loss=96.5703
	step [126/249], loss=131.4868
	step [127/249], loss=95.1562
	step [128/249], loss=89.0362
	step [129/249], loss=111.4311
	step [130/249], loss=97.9099
	step [131/249], loss=101.9889
	step [132/249], loss=87.6731
	step [133/249], loss=113.3047
	step [134/249], loss=85.0496
	step [135/249], loss=102.8977
	step [136/249], loss=106.7230
	step [137/249], loss=112.6977
	step [138/249], loss=96.9872
	step [139/249], loss=92.5563
	step [140/249], loss=102.2909
	step [141/249], loss=99.2517
	step [142/249], loss=86.9910
	step [143/249], loss=87.6465
	step [144/249], loss=91.2824
	step [145/249], loss=96.3965
	step [146/249], loss=83.5536
	step [147/249], loss=85.9236
	step [148/249], loss=100.6694
	step [149/249], loss=100.4076
	step [150/249], loss=106.7736
	step [151/249], loss=80.7939
	step [152/249], loss=68.1083
	step [153/249], loss=100.9441
	step [154/249], loss=89.6608
	step [155/249], loss=87.7758
	step [156/249], loss=98.5500
	step [157/249], loss=94.3584
	step [158/249], loss=95.6557
	step [159/249], loss=87.8135
	step [160/249], loss=99.5149
	step [161/249], loss=87.1372
	step [162/249], loss=103.9565
	step [163/249], loss=86.7133
	step [164/249], loss=91.9107
	step [165/249], loss=86.5692
	step [166/249], loss=82.9663
	step [167/249], loss=87.3821
	step [168/249], loss=87.3716
	step [169/249], loss=94.2717
	step [170/249], loss=85.0422
	step [171/249], loss=91.4213
	step [172/249], loss=111.1253
	step [173/249], loss=122.9924
	step [174/249], loss=97.2099
	step [175/249], loss=87.4420
	step [176/249], loss=98.4735
	step [177/249], loss=86.0706
	step [178/249], loss=84.9338
	step [179/249], loss=93.6589
	step [180/249], loss=110.7273
	step [181/249], loss=107.6939
	step [182/249], loss=91.5447
	step [183/249], loss=82.4916
	step [184/249], loss=101.8884
	step [185/249], loss=92.6633
	step [186/249], loss=90.3449
	step [187/249], loss=114.0796
	step [188/249], loss=88.7813
	step [189/249], loss=93.5083
	step [190/249], loss=96.7944
	step [191/249], loss=91.7133
	step [192/249], loss=102.2564
	step [193/249], loss=91.2380
	step [194/249], loss=84.2157
	step [195/249], loss=109.7182
	step [196/249], loss=89.7094
	step [197/249], loss=99.6780
	step [198/249], loss=97.4291
	step [199/249], loss=95.2149
	step [200/249], loss=102.9505
	step [201/249], loss=106.7126
	step [202/249], loss=101.3516
	step [203/249], loss=94.3008
	step [204/249], loss=94.7221
	step [205/249], loss=95.6082
	step [206/249], loss=89.6962
	step [207/249], loss=83.9128
	step [208/249], loss=89.6794
	step [209/249], loss=105.8088
	step [210/249], loss=99.4324
	step [211/249], loss=111.2119
	step [212/249], loss=80.7665
	step [213/249], loss=99.7568
	step [214/249], loss=84.0257
	step [215/249], loss=81.2499
	step [216/249], loss=81.8738
	step [217/249], loss=86.3543
	step [218/249], loss=89.7662
	step [219/249], loss=89.8260
	step [220/249], loss=94.3375
	step [221/249], loss=81.9490
	step [222/249], loss=124.4671
	step [223/249], loss=67.9414
	step [224/249], loss=96.6535
	step [225/249], loss=103.3611
	step [226/249], loss=95.8450
	step [227/249], loss=76.0251
	step [228/249], loss=125.3838
	step [229/249], loss=87.4879
	step [230/249], loss=90.1792
	step [231/249], loss=88.1242
	step [232/249], loss=100.6317
	step [233/249], loss=98.3895
	step [234/249], loss=90.6244
	step [235/249], loss=90.0379
	step [236/249], loss=79.7847
	step [237/249], loss=101.6341
	step [238/249], loss=96.6331
	step [239/249], loss=98.8183
	step [240/249], loss=117.3911
	step [241/249], loss=89.7029
	step [242/249], loss=94.5258
	step [243/249], loss=92.5451
	step [244/249], loss=86.6088
	step [245/249], loss=98.0938
	step [246/249], loss=86.5544
	step [247/249], loss=106.1360
	step [248/249], loss=88.9619
	step [249/249], loss=69.7647
	Evaluating
	loss=0.0080, precision=0.3777, recall=0.8996, f1=0.5320
Training epoch 24
	step [1/249], loss=109.1560
	step [2/249], loss=95.3497
	step [3/249], loss=101.4671
	step [4/249], loss=82.2068
	step [5/249], loss=102.9650
	step [6/249], loss=89.1127
	step [7/249], loss=86.4789
	step [8/249], loss=100.5594
	step [9/249], loss=100.7426
	step [10/249], loss=93.9113
	step [11/249], loss=99.9274
	step [12/249], loss=91.8593
	step [13/249], loss=80.8003
	step [14/249], loss=92.1478
	step [15/249], loss=96.5632
	step [16/249], loss=106.5522
	step [17/249], loss=100.4533
	step [18/249], loss=77.5298
	step [19/249], loss=94.8608
	step [20/249], loss=124.7410
	step [21/249], loss=85.4589
	step [22/249], loss=101.0830
	step [23/249], loss=88.3472
	step [24/249], loss=97.4420
	step [25/249], loss=93.2698
	step [26/249], loss=88.2078
	step [27/249], loss=86.2166
	step [28/249], loss=87.5769
	step [29/249], loss=91.8078
	step [30/249], loss=77.3285
	step [31/249], loss=81.4629
	step [32/249], loss=90.0593
	step [33/249], loss=74.1395
	step [34/249], loss=98.0862
	step [35/249], loss=95.7691
	step [36/249], loss=91.1304
	step [37/249], loss=94.2339
	step [38/249], loss=90.2661
	step [39/249], loss=96.3900
	step [40/249], loss=109.1567
	step [41/249], loss=102.7327
	step [42/249], loss=101.6395
	step [43/249], loss=85.7828
	step [44/249], loss=84.3072
	step [45/249], loss=86.9879
	step [46/249], loss=96.0690
	step [47/249], loss=113.3164
	step [48/249], loss=93.2174
	step [49/249], loss=99.9603
	step [50/249], loss=72.4855
	step [51/249], loss=88.2013
	step [52/249], loss=97.1323
	step [53/249], loss=111.3192
	step [54/249], loss=92.1241
	step [55/249], loss=93.0548
	step [56/249], loss=97.4229
	step [57/249], loss=87.8620
	step [58/249], loss=87.1683
	step [59/249], loss=108.9057
	step [60/249], loss=107.4954
	step [61/249], loss=77.5907
	step [62/249], loss=92.9748
	step [63/249], loss=97.8904
	step [64/249], loss=94.3464
	step [65/249], loss=93.5194
	step [66/249], loss=101.1369
	step [67/249], loss=90.8977
	step [68/249], loss=81.0381
	step [69/249], loss=79.6352
	step [70/249], loss=93.2023
	step [71/249], loss=107.1864
	step [72/249], loss=104.2430
	step [73/249], loss=107.5569
	step [74/249], loss=85.2865
	step [75/249], loss=86.4355
	step [76/249], loss=89.4403
	step [77/249], loss=112.2010
	step [78/249], loss=83.4380
	step [79/249], loss=95.6070
	step [80/249], loss=86.1969
	step [81/249], loss=77.5097
	step [82/249], loss=87.2718
	step [83/249], loss=100.9110
	step [84/249], loss=99.5740
	step [85/249], loss=96.4941
	step [86/249], loss=81.3662
	step [87/249], loss=106.6589
	step [88/249], loss=95.0308
	step [89/249], loss=111.3154
	step [90/249], loss=112.5712
	step [91/249], loss=91.2855
	step [92/249], loss=102.3340
	step [93/249], loss=87.3178
	step [94/249], loss=94.1109
	step [95/249], loss=110.5333
	step [96/249], loss=96.2817
	step [97/249], loss=95.2128
	step [98/249], loss=98.1739
	step [99/249], loss=104.2435
	step [100/249], loss=96.0821
	step [101/249], loss=99.4599
	step [102/249], loss=102.8768
	step [103/249], loss=68.7408
	step [104/249], loss=85.6875
	step [105/249], loss=91.3151
	step [106/249], loss=102.4091
	step [107/249], loss=97.2687
	step [108/249], loss=88.4584
	step [109/249], loss=74.4516
	step [110/249], loss=73.0392
	step [111/249], loss=95.6262
	step [112/249], loss=103.5019
	step [113/249], loss=86.0634
	step [114/249], loss=84.3696
	step [115/249], loss=106.3921
	step [116/249], loss=92.6650
	step [117/249], loss=71.1502
	step [118/249], loss=95.6261
	step [119/249], loss=85.9426
	step [120/249], loss=103.2813
	step [121/249], loss=85.1216
	step [122/249], loss=83.0426
	step [123/249], loss=89.3266
	step [124/249], loss=93.1687
	step [125/249], loss=100.3909
	step [126/249], loss=101.5136
	step [127/249], loss=84.0365
	step [128/249], loss=96.3841
	step [129/249], loss=97.2575
	step [130/249], loss=114.4640
	step [131/249], loss=98.4429
	step [132/249], loss=94.5487
	step [133/249], loss=124.0893
	step [134/249], loss=98.9054
	step [135/249], loss=95.0000
	step [136/249], loss=95.2189
	step [137/249], loss=105.3279
	step [138/249], loss=88.0530
	step [139/249], loss=93.6649
	step [140/249], loss=92.6752
	step [141/249], loss=92.4346
	step [142/249], loss=80.3461
	step [143/249], loss=94.3083
	step [144/249], loss=100.7263
	step [145/249], loss=86.4528
	step [146/249], loss=92.5114
	step [147/249], loss=90.0321
	step [148/249], loss=108.5824
	step [149/249], loss=84.6331
	step [150/249], loss=90.4939
	step [151/249], loss=81.4563
	step [152/249], loss=102.8631
	step [153/249], loss=96.6814
	step [154/249], loss=103.9666
	step [155/249], loss=86.4679
	step [156/249], loss=98.6669
	step [157/249], loss=82.8112
	step [158/249], loss=111.1692
	step [159/249], loss=85.9290
	step [160/249], loss=101.6742
	step [161/249], loss=107.1026
	step [162/249], loss=87.7241
	step [163/249], loss=80.8290
	step [164/249], loss=104.9269
	step [165/249], loss=114.5526
	step [166/249], loss=90.3492
	step [167/249], loss=80.4385
	step [168/249], loss=93.4389
	step [169/249], loss=84.9282
	step [170/249], loss=105.6181
	step [171/249], loss=98.8860
	step [172/249], loss=92.1986
	step [173/249], loss=88.9004
	step [174/249], loss=84.5017
	step [175/249], loss=106.8231
	step [176/249], loss=94.1154
	step [177/249], loss=107.4138
	step [178/249], loss=82.7923
	step [179/249], loss=106.9234
	step [180/249], loss=92.1968
	step [181/249], loss=81.1926
	step [182/249], loss=88.1575
	step [183/249], loss=106.1628
	step [184/249], loss=81.7330
	step [185/249], loss=76.2638
	step [186/249], loss=97.3619
	step [187/249], loss=103.1943
	step [188/249], loss=84.3588
	step [189/249], loss=82.4037
	step [190/249], loss=103.7561
	step [191/249], loss=111.7775
	step [192/249], loss=88.5886
	step [193/249], loss=95.5939
	step [194/249], loss=78.7589
	step [195/249], loss=88.6871
	step [196/249], loss=98.8271
	step [197/249], loss=102.2431
	step [198/249], loss=97.1391
	step [199/249], loss=96.9984
	step [200/249], loss=91.9434
	step [201/249], loss=80.8400
	step [202/249], loss=95.5886
	step [203/249], loss=94.1951
	step [204/249], loss=91.0465
	step [205/249], loss=84.8027
	step [206/249], loss=74.4812
	step [207/249], loss=83.8276
	step [208/249], loss=87.8740
	step [209/249], loss=79.9519
	step [210/249], loss=85.0563
	step [211/249], loss=91.1001
	step [212/249], loss=101.0464
	step [213/249], loss=90.7872
	step [214/249], loss=109.0189
	step [215/249], loss=83.4303
	step [216/249], loss=88.6090
	step [217/249], loss=104.0183
	step [218/249], loss=99.4252
	step [219/249], loss=94.4037
	step [220/249], loss=97.5201
	step [221/249], loss=82.1922
	step [222/249], loss=92.9898
	step [223/249], loss=82.1244
	step [224/249], loss=89.2012
	step [225/249], loss=127.8493
	step [226/249], loss=87.2189
	step [227/249], loss=93.6964
	step [228/249], loss=88.0171
	step [229/249], loss=89.5406
	step [230/249], loss=100.9416
	step [231/249], loss=94.7932
	step [232/249], loss=110.4405
	step [233/249], loss=96.3899
	step [234/249], loss=108.8879
	step [235/249], loss=96.4326
	step [236/249], loss=86.7412
	step [237/249], loss=97.1396
	step [238/249], loss=92.1722
	step [239/249], loss=95.6064
	step [240/249], loss=95.6291
	step [241/249], loss=105.3920
	step [242/249], loss=95.1777
	step [243/249], loss=85.8696
	step [244/249], loss=111.9070
	step [245/249], loss=84.9631
	step [246/249], loss=99.7767
	step [247/249], loss=95.5722
	step [248/249], loss=98.9892
	step [249/249], loss=64.0226
	Evaluating
	loss=0.0095, precision=0.3362, recall=0.9292, f1=0.4938
Training epoch 25
	step [1/249], loss=78.3491
	step [2/249], loss=102.5829
	step [3/249], loss=112.4911
	step [4/249], loss=99.8773
	step [5/249], loss=94.8241
	step [6/249], loss=74.0112
	step [7/249], loss=84.7649
	step [8/249], loss=103.8793
	step [9/249], loss=83.2626
	step [10/249], loss=89.4806
	step [11/249], loss=71.4069
	step [12/249], loss=88.5681
	step [13/249], loss=105.4022
	step [14/249], loss=95.4349
	step [15/249], loss=89.7073
	step [16/249], loss=92.6078
	step [17/249], loss=95.6447
	step [18/249], loss=84.5197
	step [19/249], loss=83.2580
	step [20/249], loss=99.1220
	step [21/249], loss=107.0259
	step [22/249], loss=80.9156
	step [23/249], loss=112.1637
	step [24/249], loss=88.6158
	step [25/249], loss=89.5940
	step [26/249], loss=98.0087
	step [27/249], loss=98.1307
	step [28/249], loss=105.9481
	step [29/249], loss=97.8976
	step [30/249], loss=104.3718
	step [31/249], loss=101.2782
	step [32/249], loss=82.0967
	step [33/249], loss=83.9674
	step [34/249], loss=96.5863
	step [35/249], loss=91.7250
	step [36/249], loss=89.7111
	step [37/249], loss=74.4374
	step [38/249], loss=88.2309
	step [39/249], loss=87.2231
	step [40/249], loss=96.6537
	step [41/249], loss=116.5319
	step [42/249], loss=84.2912
	step [43/249], loss=94.1899
	step [44/249], loss=120.7446
	step [45/249], loss=94.9781
	step [46/249], loss=93.7487
	step [47/249], loss=94.4648
	step [48/249], loss=99.0123
	step [49/249], loss=89.2684
	step [50/249], loss=84.3585
	step [51/249], loss=89.7915
	step [52/249], loss=96.4236
	step [53/249], loss=76.2575
	step [54/249], loss=83.7291
	step [55/249], loss=90.5554
	step [56/249], loss=97.8853
	step [57/249], loss=83.8618
	step [58/249], loss=101.1588
	step [59/249], loss=96.1342
	step [60/249], loss=91.7336
	step [61/249], loss=95.0709
	step [62/249], loss=90.6601
	step [63/249], loss=92.0481
	step [64/249], loss=76.9131
	step [65/249], loss=108.4332
	step [66/249], loss=84.7133
	step [67/249], loss=98.1227
	step [68/249], loss=74.9496
	step [69/249], loss=99.7912
	step [70/249], loss=88.9894
	step [71/249], loss=105.9279
	step [72/249], loss=68.8448
	step [73/249], loss=98.2759
	step [74/249], loss=98.8925
	step [75/249], loss=95.3758
	step [76/249], loss=96.4246
	step [77/249], loss=92.5225
	step [78/249], loss=85.8263
	step [79/249], loss=102.9355
	step [80/249], loss=89.2278
	step [81/249], loss=111.6613
	step [82/249], loss=91.4231
	step [83/249], loss=97.4330
	step [84/249], loss=77.2439
	step [85/249], loss=91.8750
	step [86/249], loss=78.9253
	step [87/249], loss=103.8341
	step [88/249], loss=100.6111
	step [89/249], loss=83.0957
	step [90/249], loss=86.9605
	step [91/249], loss=111.2037
	step [92/249], loss=107.2350
	step [93/249], loss=81.2322
	step [94/249], loss=94.2512
	step [95/249], loss=82.3976
	step [96/249], loss=85.6688
	step [97/249], loss=108.4581
	step [98/249], loss=98.5325
	step [99/249], loss=90.5626
	step [100/249], loss=99.8525
	step [101/249], loss=93.1142
	step [102/249], loss=101.6028
	step [103/249], loss=89.2942
	step [104/249], loss=78.4001
	step [105/249], loss=115.8802
	step [106/249], loss=111.5548
	step [107/249], loss=97.5515
	step [108/249], loss=87.3279
	step [109/249], loss=78.8420
	step [110/249], loss=93.6828
	step [111/249], loss=89.5026
	step [112/249], loss=78.9570
	step [113/249], loss=102.5014
	step [114/249], loss=91.9361
	step [115/249], loss=100.4049
	step [116/249], loss=91.1167
	step [117/249], loss=107.2941
	step [118/249], loss=91.2773
	step [119/249], loss=80.9333
	step [120/249], loss=105.9398
	step [121/249], loss=90.9243
	step [122/249], loss=82.1796
	step [123/249], loss=92.9625
	step [124/249], loss=90.5937
	step [125/249], loss=89.5827
	step [126/249], loss=82.1981
	step [127/249], loss=83.3457
	step [128/249], loss=91.5300
	step [129/249], loss=87.6747
	step [130/249], loss=96.2088
	step [131/249], loss=91.8652
	step [132/249], loss=107.5989
	step [133/249], loss=109.4007
	step [134/249], loss=107.4263
	step [135/249], loss=92.0516
	step [136/249], loss=73.2081
	step [137/249], loss=85.4861
	step [138/249], loss=91.3122
	step [139/249], loss=91.1676
	step [140/249], loss=80.5335
	step [141/249], loss=82.1936
	step [142/249], loss=93.3389
	step [143/249], loss=112.3593
	step [144/249], loss=76.3112
	step [145/249], loss=82.1787
	step [146/249], loss=86.0460
	step [147/249], loss=95.4963
	step [148/249], loss=110.0006
	step [149/249], loss=98.0578
	step [150/249], loss=89.7903
	step [151/249], loss=96.5684
	step [152/249], loss=97.9930
	step [153/249], loss=80.3150
	step [154/249], loss=112.1951
	step [155/249], loss=84.4019
	step [156/249], loss=75.9651
	step [157/249], loss=89.4655
	step [158/249], loss=87.9966
	step [159/249], loss=76.2996
	step [160/249], loss=98.3717
	step [161/249], loss=83.8905
	step [162/249], loss=109.4162
	step [163/249], loss=103.0881
	step [164/249], loss=108.1572
	step [165/249], loss=88.3078
	step [166/249], loss=75.7488
	step [167/249], loss=81.0618
	step [168/249], loss=109.0881
	step [169/249], loss=96.1851
	step [170/249], loss=101.7110
	step [171/249], loss=96.5508
	step [172/249], loss=91.1432
	step [173/249], loss=97.4507
	step [174/249], loss=85.7707
	step [175/249], loss=93.9058
	step [176/249], loss=96.9829
	step [177/249], loss=114.1894
	step [178/249], loss=79.0441
	step [179/249], loss=107.9776
	step [180/249], loss=99.2288
	step [181/249], loss=84.7548
	step [182/249], loss=85.8438
	step [183/249], loss=71.3469
	step [184/249], loss=93.5088
	step [185/249], loss=104.2731
	step [186/249], loss=102.6060
	step [187/249], loss=83.8853
	step [188/249], loss=86.8756
	step [189/249], loss=98.2600
	step [190/249], loss=98.2138
	step [191/249], loss=96.4999
	step [192/249], loss=118.8474
	step [193/249], loss=87.8435
	step [194/249], loss=96.8727
	step [195/249], loss=92.6349
	step [196/249], loss=93.6798
	step [197/249], loss=100.2861
	step [198/249], loss=75.2865
	step [199/249], loss=105.3614
	step [200/249], loss=91.9827
	step [201/249], loss=91.4109
	step [202/249], loss=84.2447
	step [203/249], loss=88.2181
	step [204/249], loss=98.4541
	step [205/249], loss=95.0378
	step [206/249], loss=86.5576
	step [207/249], loss=83.2206
	step [208/249], loss=93.9883
	step [209/249], loss=103.7178
	step [210/249], loss=101.6999
	step [211/249], loss=102.2426
	step [212/249], loss=100.1217
	step [213/249], loss=97.5473
	step [214/249], loss=83.3136
	step [215/249], loss=97.8503
	step [216/249], loss=86.7636
	step [217/249], loss=95.3199
	step [218/249], loss=83.6570
	step [219/249], loss=91.0793
	step [220/249], loss=94.7014
	step [221/249], loss=101.6013
	step [222/249], loss=106.8748
	step [223/249], loss=86.8095
	step [224/249], loss=87.3854
	step [225/249], loss=83.5158
	step [226/249], loss=72.3861
	step [227/249], loss=96.5441
	step [228/249], loss=93.9133
	step [229/249], loss=91.7663
	step [230/249], loss=93.2596
	step [231/249], loss=94.2669
	step [232/249], loss=104.6404
	step [233/249], loss=87.0408
	step [234/249], loss=99.2796
	step [235/249], loss=96.4748
	step [236/249], loss=102.7010
	step [237/249], loss=104.8672
	step [238/249], loss=114.2415
	step [239/249], loss=90.7621
	step [240/249], loss=117.9072
	step [241/249], loss=102.5225
	step [242/249], loss=93.1684
	step [243/249], loss=89.7132
	step [244/249], loss=100.8520
	step [245/249], loss=91.1112
	step [246/249], loss=91.3656
	step [247/249], loss=96.0959
	step [248/249], loss=98.2902
	step [249/249], loss=57.1268
	Evaluating
	loss=0.0083, precision=0.3615, recall=0.9138, f1=0.5181
Training epoch 26
	step [1/249], loss=101.2614
	step [2/249], loss=99.7224
	step [3/249], loss=94.7775
	step [4/249], loss=84.1305
	step [5/249], loss=99.9957
	step [6/249], loss=73.5246
	step [7/249], loss=97.3670
	step [8/249], loss=99.9955
	step [9/249], loss=102.0741
	step [10/249], loss=83.0760
	step [11/249], loss=90.7984
	step [12/249], loss=93.4129
	step [13/249], loss=93.9024
	step [14/249], loss=88.8832
	step [15/249], loss=111.0191
	step [16/249], loss=85.9592
	step [17/249], loss=103.2881
	step [18/249], loss=79.4018
	step [19/249], loss=103.4023
	step [20/249], loss=94.2228
	step [21/249], loss=82.0587
	step [22/249], loss=96.9926
	step [23/249], loss=94.2793
	step [24/249], loss=78.5810
	step [25/249], loss=99.2036
	step [26/249], loss=90.2801
	step [27/249], loss=105.2725
	step [28/249], loss=98.7883
	step [29/249], loss=87.6301
	step [30/249], loss=81.6098
	step [31/249], loss=89.5867
	step [32/249], loss=83.6355
	step [33/249], loss=106.5924
	step [34/249], loss=101.9895
	step [35/249], loss=107.7490
	step [36/249], loss=96.6051
	step [37/249], loss=86.9620
	step [38/249], loss=88.8383
	step [39/249], loss=100.8615
	step [40/249], loss=105.8047
	step [41/249], loss=89.8416
	step [42/249], loss=83.9626
	step [43/249], loss=86.6135
	step [44/249], loss=96.0691
	step [45/249], loss=84.7750
	step [46/249], loss=94.4452
	step [47/249], loss=86.2654
	step [48/249], loss=82.1131
	step [49/249], loss=93.7993
	step [50/249], loss=98.6439
	step [51/249], loss=107.9583
	step [52/249], loss=85.7171
	step [53/249], loss=93.2116
	step [54/249], loss=93.0122
	step [55/249], loss=91.9912
	step [56/249], loss=103.7545
	step [57/249], loss=92.4784
	step [58/249], loss=106.3318
	step [59/249], loss=95.0183
	step [60/249], loss=86.2865
	step [61/249], loss=83.0773
	step [62/249], loss=87.6782
	step [63/249], loss=97.3060
	step [64/249], loss=94.1894
	step [65/249], loss=100.9652
	step [66/249], loss=98.4523
	step [67/249], loss=87.8143
	step [68/249], loss=98.2146
	step [69/249], loss=87.5035
	step [70/249], loss=103.7571
	step [71/249], loss=92.7555
	step [72/249], loss=92.2464
	step [73/249], loss=90.8238
	step [74/249], loss=81.1398
	step [75/249], loss=97.7334
	step [76/249], loss=85.6288
	step [77/249], loss=117.4477
	step [78/249], loss=71.3701
	step [79/249], loss=123.0959
	step [80/249], loss=91.4561
	step [81/249], loss=108.4225
	step [82/249], loss=81.5506
	step [83/249], loss=93.0467
	step [84/249], loss=90.0364
	step [85/249], loss=76.4445
	step [86/249], loss=118.5975
	step [87/249], loss=103.3040
	step [88/249], loss=94.3543
	step [89/249], loss=98.5663
	step [90/249], loss=86.4553
	step [91/249], loss=92.7199
	step [92/249], loss=91.7713
	step [93/249], loss=96.4040
	step [94/249], loss=90.8648
	step [95/249], loss=95.9942
	step [96/249], loss=99.5168
	step [97/249], loss=107.1728
	step [98/249], loss=78.4541
	step [99/249], loss=84.9199
	step [100/249], loss=92.9175
	step [101/249], loss=78.9218
	step [102/249], loss=70.7757
	step [103/249], loss=86.5769
	step [104/249], loss=85.5671
	step [105/249], loss=97.9311
	step [106/249], loss=88.3361
	step [107/249], loss=78.0983
	step [108/249], loss=82.2100
	step [109/249], loss=90.9393
	step [110/249], loss=109.3549
	step [111/249], loss=100.1193
	step [112/249], loss=80.7332
	step [113/249], loss=82.3405
	step [114/249], loss=94.8576
	step [115/249], loss=95.3371
	step [116/249], loss=80.5930
	step [117/249], loss=84.1965
	step [118/249], loss=108.9353
	step [119/249], loss=80.4748
	step [120/249], loss=97.3043
	step [121/249], loss=77.6782
	step [122/249], loss=130.5910
	step [123/249], loss=91.6116
	step [124/249], loss=75.4547
	step [125/249], loss=85.0981
	step [126/249], loss=74.7999
	step [127/249], loss=99.7094
	step [128/249], loss=91.8298
	step [129/249], loss=85.6912
	step [130/249], loss=108.3013
	step [131/249], loss=116.0630
	step [132/249], loss=118.0354
	step [133/249], loss=88.2506
	step [134/249], loss=109.4372
	step [135/249], loss=104.9701
	step [136/249], loss=109.6977
	step [137/249], loss=92.5927
	step [138/249], loss=105.2493
	step [139/249], loss=93.7948
	step [140/249], loss=100.8090
	step [141/249], loss=76.0202
	step [142/249], loss=99.0966
	step [143/249], loss=90.4079
	step [144/249], loss=89.6572
	step [145/249], loss=79.6478
	step [146/249], loss=71.8893
	step [147/249], loss=110.5590
	step [148/249], loss=84.5792
	step [149/249], loss=70.1683
	step [150/249], loss=74.3074
	step [151/249], loss=104.5107
	step [152/249], loss=91.2014
	step [153/249], loss=88.9133
	step [154/249], loss=86.0524
	step [155/249], loss=76.9224
	step [156/249], loss=88.1060
	step [157/249], loss=97.6320
	step [158/249], loss=84.8693
	step [159/249], loss=94.6677
	step [160/249], loss=79.2969
	step [161/249], loss=92.3921
	step [162/249], loss=99.7720
	step [163/249], loss=74.5486
	step [164/249], loss=87.3594
	step [165/249], loss=97.9975
	step [166/249], loss=105.5548
	step [167/249], loss=75.9076
	step [168/249], loss=90.7085
	step [169/249], loss=79.1173
	step [170/249], loss=86.0704
	step [171/249], loss=82.5752
	step [172/249], loss=95.4316
	step [173/249], loss=103.5193
	step [174/249], loss=81.2426
	step [175/249], loss=94.7475
	step [176/249], loss=83.4667
	step [177/249], loss=89.4675
	step [178/249], loss=116.3466
	step [179/249], loss=78.6927
	step [180/249], loss=87.5805
	step [181/249], loss=103.8781
	step [182/249], loss=90.2868
	step [183/249], loss=83.2985
	step [184/249], loss=83.4576
	step [185/249], loss=96.4113
	step [186/249], loss=103.1354
	step [187/249], loss=83.3376
	step [188/249], loss=87.8681
	step [189/249], loss=116.5671
	step [190/249], loss=75.7846
	step [191/249], loss=88.5241
	step [192/249], loss=94.0155
	step [193/249], loss=81.2854
	step [194/249], loss=90.9041
	step [195/249], loss=96.2309
	step [196/249], loss=83.3050
	step [197/249], loss=82.6997
	step [198/249], loss=90.8120
	step [199/249], loss=92.4586
	step [200/249], loss=119.7760
	step [201/249], loss=101.1679
	step [202/249], loss=87.6300
	step [203/249], loss=87.2823
	step [204/249], loss=90.3109
	step [205/249], loss=88.6697
	step [206/249], loss=101.9746
	step [207/249], loss=94.7307
	step [208/249], loss=103.2970
	step [209/249], loss=84.4284
	step [210/249], loss=88.2718
	step [211/249], loss=90.4625
	step [212/249], loss=100.3626
	step [213/249], loss=101.8471
	step [214/249], loss=72.5227
	step [215/249], loss=82.9305
	step [216/249], loss=95.1008
	step [217/249], loss=108.6828
	step [218/249], loss=97.2091
	step [219/249], loss=87.4769
	step [220/249], loss=75.5388
	step [221/249], loss=85.6995
	step [222/249], loss=100.0012
	step [223/249], loss=102.0856
	step [224/249], loss=107.2229
	step [225/249], loss=90.0865
	step [226/249], loss=79.7370
	step [227/249], loss=93.8818
	step [228/249], loss=99.2090
	step [229/249], loss=91.0998
	step [230/249], loss=102.0331
	step [231/249], loss=92.9772
	step [232/249], loss=84.7378
	step [233/249], loss=88.5085
	step [234/249], loss=90.8684
	step [235/249], loss=95.5433
	step [236/249], loss=108.0608
	step [237/249], loss=100.3354
	step [238/249], loss=81.1716
	step [239/249], loss=98.6766
	step [240/249], loss=79.0845
	step [241/249], loss=96.8694
	step [242/249], loss=73.7068
	step [243/249], loss=84.4277
	step [244/249], loss=84.3865
	step [245/249], loss=98.4753
	step [246/249], loss=105.1383
	step [247/249], loss=109.6289
	step [248/249], loss=84.4812
	step [249/249], loss=69.4286
	Evaluating
	loss=0.0085, precision=0.3402, recall=0.9063, f1=0.4947
Training epoch 27
	step [1/249], loss=74.4569
	step [2/249], loss=98.2402
	step [3/249], loss=85.1376
	step [4/249], loss=83.2961
	step [5/249], loss=90.8475
	step [6/249], loss=93.1728
	step [7/249], loss=77.2625
	step [8/249], loss=68.8313
	step [9/249], loss=101.0843
	step [10/249], loss=98.5170
	step [11/249], loss=81.0702
	step [12/249], loss=89.9304
	step [13/249], loss=91.6328
	step [14/249], loss=93.0065
	step [15/249], loss=86.1499
	step [16/249], loss=87.7840
	step [17/249], loss=90.0843
	step [18/249], loss=96.5111
	step [19/249], loss=87.7241
	step [20/249], loss=83.2390
	step [21/249], loss=95.7708
	step [22/249], loss=86.5238
	step [23/249], loss=98.4981
	step [24/249], loss=81.5504
	step [25/249], loss=97.6383
	step [26/249], loss=83.1061
	step [27/249], loss=67.7588
	step [28/249], loss=71.4946
	step [29/249], loss=88.3090
	step [30/249], loss=99.2558
	step [31/249], loss=111.4403
	step [32/249], loss=105.0874
	step [33/249], loss=86.5658
	step [34/249], loss=104.5998
	step [35/249], loss=91.2160
	step [36/249], loss=89.0686
	step [37/249], loss=88.9734
	step [38/249], loss=98.2240
	step [39/249], loss=77.7403
	step [40/249], loss=107.5997
	step [41/249], loss=81.6816
	step [42/249], loss=96.4133
	step [43/249], loss=91.0677
	step [44/249], loss=87.0072
	step [45/249], loss=80.5158
	step [46/249], loss=107.1096
	step [47/249], loss=91.2940
	step [48/249], loss=95.8263
	step [49/249], loss=91.4798
	step [50/249], loss=95.0791
	step [51/249], loss=79.3841
	step [52/249], loss=110.0468
	step [53/249], loss=83.0639
	step [54/249], loss=75.8544
	step [55/249], loss=110.9716
	step [56/249], loss=75.7916
	step [57/249], loss=99.0949
	step [58/249], loss=89.1504
	step [59/249], loss=89.4839
	step [60/249], loss=89.3569
	step [61/249], loss=107.1197
	step [62/249], loss=79.2068
	step [63/249], loss=102.6628
	step [64/249], loss=98.7288
	step [65/249], loss=93.4465
	step [66/249], loss=90.5764
	step [67/249], loss=96.4630
	step [68/249], loss=113.2915
	step [69/249], loss=88.1452
	step [70/249], loss=95.6099
	step [71/249], loss=88.3690
	step [72/249], loss=68.3215
	step [73/249], loss=100.7662
	step [74/249], loss=82.3021
	step [75/249], loss=115.4412
	step [76/249], loss=90.0889
	step [77/249], loss=101.4885
	step [78/249], loss=66.1244
	step [79/249], loss=89.5514
	step [80/249], loss=91.2039
	step [81/249], loss=95.3330
	step [82/249], loss=81.6052
	step [83/249], loss=89.0659
	step [84/249], loss=107.6273
	step [85/249], loss=96.3798
	step [86/249], loss=98.4495
	step [87/249], loss=97.6267
	step [88/249], loss=86.5244
	step [89/249], loss=78.3209
	step [90/249], loss=78.4089
	step [91/249], loss=81.3147
	step [92/249], loss=80.4133
	step [93/249], loss=85.6318
	step [94/249], loss=101.6861
	step [95/249], loss=88.3395
	step [96/249], loss=102.8545
	step [97/249], loss=96.6613
	step [98/249], loss=98.0453
	step [99/249], loss=85.0501
	step [100/249], loss=96.9707
	step [101/249], loss=104.6767
	step [102/249], loss=84.2260
	step [103/249], loss=94.4005
	step [104/249], loss=101.5605
	step [105/249], loss=91.4733
	step [106/249], loss=99.9712
	step [107/249], loss=98.9315
	step [108/249], loss=98.0700
	step [109/249], loss=75.9056
	step [110/249], loss=68.3820
	step [111/249], loss=85.2152
	step [112/249], loss=96.4944
	step [113/249], loss=92.8130
	step [114/249], loss=82.3724
	step [115/249], loss=106.3992
	step [116/249], loss=108.3361
	step [117/249], loss=103.4259
	step [118/249], loss=115.0813
	step [119/249], loss=112.4902
	step [120/249], loss=101.4311
	step [121/249], loss=92.1405
	step [122/249], loss=93.3900
	step [123/249], loss=81.9004
	step [124/249], loss=76.0148
	step [125/249], loss=98.2490
	step [126/249], loss=93.7042
	step [127/249], loss=92.7419
	step [128/249], loss=81.2323
	step [129/249], loss=88.3526
	step [130/249], loss=80.3029
	step [131/249], loss=90.1261
	step [132/249], loss=86.1965
	step [133/249], loss=85.8627
	step [134/249], loss=92.2603
	step [135/249], loss=99.5329
	step [136/249], loss=87.9991
	step [137/249], loss=97.4919
	step [138/249], loss=104.9913
	step [139/249], loss=94.0298
	step [140/249], loss=90.3204
	step [141/249], loss=97.4943
	step [142/249], loss=80.3092
	step [143/249], loss=89.6053
	step [144/249], loss=73.7139
	step [145/249], loss=77.1244
	step [146/249], loss=77.3279
	step [147/249], loss=89.3708
	step [148/249], loss=97.7297
	step [149/249], loss=87.8667
	step [150/249], loss=121.0018
	step [151/249], loss=80.5940
	step [152/249], loss=82.9627
	step [153/249], loss=80.4721
	step [154/249], loss=94.6707
	step [155/249], loss=92.6935
	step [156/249], loss=86.4444
	step [157/249], loss=87.0043
	step [158/249], loss=76.1061
	step [159/249], loss=103.8639
	step [160/249], loss=94.8443
	step [161/249], loss=81.3549
	step [162/249], loss=92.5958
	step [163/249], loss=92.9600
	step [164/249], loss=76.3676
	step [165/249], loss=85.1105
	step [166/249], loss=83.6544
	step [167/249], loss=96.4623
	step [168/249], loss=87.5038
	step [169/249], loss=97.6266
	step [170/249], loss=86.1996
	step [171/249], loss=83.0571
	step [172/249], loss=109.1743
	step [173/249], loss=96.5119
	step [174/249], loss=91.6933
	step [175/249], loss=96.3051
	step [176/249], loss=83.4230
	step [177/249], loss=107.6135
	step [178/249], loss=87.2672
	step [179/249], loss=92.8653
	step [180/249], loss=96.6952
	step [181/249], loss=84.5828
	step [182/249], loss=95.4797
	step [183/249], loss=94.0660
	step [184/249], loss=118.6978
	step [185/249], loss=94.1550
	step [186/249], loss=97.1872
	step [187/249], loss=99.0926
	step [188/249], loss=100.3185
	step [189/249], loss=113.3845
	step [190/249], loss=91.5617
	step [191/249], loss=96.1152
	step [192/249], loss=93.0722
	step [193/249], loss=93.6422
	step [194/249], loss=104.1926
	step [195/249], loss=108.2266
	step [196/249], loss=91.7013
	step [197/249], loss=72.5574
	step [198/249], loss=93.2621
	step [199/249], loss=104.1031
	step [200/249], loss=94.1307
	step [201/249], loss=87.7180
	step [202/249], loss=100.2750
	step [203/249], loss=94.2829
	step [204/249], loss=90.1640
	step [205/249], loss=105.3844
	step [206/249], loss=82.2697
	step [207/249], loss=97.5638
	step [208/249], loss=92.8623
	step [209/249], loss=76.7594
	step [210/249], loss=86.6923
	step [211/249], loss=86.6464
	step [212/249], loss=106.7901
	step [213/249], loss=104.0332
	step [214/249], loss=70.3918
	step [215/249], loss=88.7633
	step [216/249], loss=93.2356
	step [217/249], loss=82.3816
	step [218/249], loss=74.4047
	step [219/249], loss=98.4082
	step [220/249], loss=82.2883
	step [221/249], loss=79.2594
	step [222/249], loss=76.2236
	step [223/249], loss=94.8739
	step [224/249], loss=79.6658
	step [225/249], loss=88.2149
	step [226/249], loss=84.8796
	step [227/249], loss=86.3470
	step [228/249], loss=92.9902
	step [229/249], loss=103.6620
	step [230/249], loss=105.0926
	step [231/249], loss=97.4064
	step [232/249], loss=80.2329
	step [233/249], loss=101.4062
	step [234/249], loss=104.7432
	step [235/249], loss=97.9517
	step [236/249], loss=77.2112
	step [237/249], loss=80.6993
	step [238/249], loss=78.9876
	step [239/249], loss=90.3633
	step [240/249], loss=94.1843
	step [241/249], loss=83.9104
	step [242/249], loss=117.1459
	step [243/249], loss=88.3980
	step [244/249], loss=107.8524
	step [245/249], loss=114.5892
	step [246/249], loss=94.3641
	step [247/249], loss=110.0112
	step [248/249], loss=105.2281
	step [249/249], loss=55.1858
	Evaluating
	loss=0.0079, precision=0.3698, recall=0.9165, f1=0.5270
Training epoch 28
	step [1/249], loss=94.3508
	step [2/249], loss=82.8412
	step [3/249], loss=94.3193
	step [4/249], loss=79.1745
	step [5/249], loss=82.7786
	step [6/249], loss=112.2822
	step [7/249], loss=75.2212
	step [8/249], loss=89.8315
	step [9/249], loss=92.3405
	step [10/249], loss=98.4457
	step [11/249], loss=86.4573
	step [12/249], loss=79.2461
	step [13/249], loss=94.3052
	step [14/249], loss=107.1964
	step [15/249], loss=92.8783
	step [16/249], loss=75.0364
	step [17/249], loss=78.5837
	step [18/249], loss=83.5156
	step [19/249], loss=77.1828
	step [20/249], loss=77.1033
	step [21/249], loss=95.0435
	step [22/249], loss=102.9706
	step [23/249], loss=91.9842
	step [24/249], loss=84.8634
	step [25/249], loss=107.4434
	step [26/249], loss=91.4001
	step [27/249], loss=96.3401
	step [28/249], loss=93.3851
	step [29/249], loss=77.7206
	step [30/249], loss=92.4573
	step [31/249], loss=89.5018
	step [32/249], loss=95.2756
	step [33/249], loss=84.5764
	step [34/249], loss=98.6163
	step [35/249], loss=98.7269
	step [36/249], loss=82.4990
	step [37/249], loss=94.2076
	step [38/249], loss=82.5006
	step [39/249], loss=70.5702
	step [40/249], loss=75.0100
	step [41/249], loss=76.4025
	step [42/249], loss=92.7764
	step [43/249], loss=115.6496
	step [44/249], loss=102.7122
	step [45/249], loss=86.8114
	step [46/249], loss=98.9785
	step [47/249], loss=90.8594
	step [48/249], loss=90.0247
	step [49/249], loss=84.0107
	step [50/249], loss=88.5802
	step [51/249], loss=86.3734
	step [52/249], loss=103.9230
	step [53/249], loss=104.8956
	step [54/249], loss=79.8000
	step [55/249], loss=93.2567
	step [56/249], loss=83.1725
	step [57/249], loss=84.2873
	step [58/249], loss=94.6221
	step [59/249], loss=71.8080
	step [60/249], loss=81.7226
	step [61/249], loss=95.3020
	step [62/249], loss=83.0412
	step [63/249], loss=67.5756
	step [64/249], loss=93.0635
	step [65/249], loss=92.9743
	step [66/249], loss=74.4653
	step [67/249], loss=88.6472
	step [68/249], loss=83.7628
	step [69/249], loss=93.9979
	step [70/249], loss=90.3390
	step [71/249], loss=87.9097
	step [72/249], loss=89.4420
	step [73/249], loss=83.5843
	step [74/249], loss=80.8107
	step [75/249], loss=89.8977
	step [76/249], loss=77.8296
	step [77/249], loss=81.3559
	step [78/249], loss=98.9611
	step [79/249], loss=77.5316
	step [80/249], loss=100.3123
	step [81/249], loss=83.1343
	step [82/249], loss=87.1175
	step [83/249], loss=95.8959
	step [84/249], loss=91.3221
	step [85/249], loss=102.7854
	step [86/249], loss=102.3697
	step [87/249], loss=89.4490
	step [88/249], loss=97.9588
	step [89/249], loss=86.0285
	step [90/249], loss=80.1997
	step [91/249], loss=81.7858
	step [92/249], loss=104.1644
	step [93/249], loss=94.7262
	step [94/249], loss=85.0508
	step [95/249], loss=87.9225
	step [96/249], loss=78.6167
	step [97/249], loss=95.7225
	step [98/249], loss=94.4626
	step [99/249], loss=77.5627
	step [100/249], loss=86.1618
	step [101/249], loss=75.2821
	step [102/249], loss=97.0627
	step [103/249], loss=107.4190
	step [104/249], loss=99.3317
	step [105/249], loss=80.8187
	step [106/249], loss=84.4214
	step [107/249], loss=89.8550
	step [108/249], loss=100.6917
	step [109/249], loss=109.9912
	step [110/249], loss=76.3745
	step [111/249], loss=86.4135
	step [112/249], loss=95.8566
	step [113/249], loss=97.5827
	step [114/249], loss=93.4000
	step [115/249], loss=96.2210
	step [116/249], loss=96.2046
	step [117/249], loss=76.5484
	step [118/249], loss=97.6463
	step [119/249], loss=106.2664
	step [120/249], loss=77.9131
	step [121/249], loss=88.3042
	step [122/249], loss=91.1573
	step [123/249], loss=80.9342
	step [124/249], loss=78.8282
	step [125/249], loss=91.9438
	step [126/249], loss=101.2506
	step [127/249], loss=102.9537
	step [128/249], loss=103.3192
	step [129/249], loss=85.2970
	step [130/249], loss=112.7229
	step [131/249], loss=95.0189
	step [132/249], loss=87.0674
	step [133/249], loss=91.5928
	step [134/249], loss=91.5531
	step [135/249], loss=75.0515
	step [136/249], loss=91.7847
	step [137/249], loss=97.7208
	step [138/249], loss=92.0662
	step [139/249], loss=85.6725
	step [140/249], loss=80.9986
	step [141/249], loss=90.9818
	step [142/249], loss=83.4266
	step [143/249], loss=102.9885
	step [144/249], loss=90.5116
	step [145/249], loss=89.2984
	step [146/249], loss=79.9099
	step [147/249], loss=105.0374
	step [148/249], loss=106.1980
	step [149/249], loss=74.1453
	step [150/249], loss=99.7126
	step [151/249], loss=96.3400
	step [152/249], loss=92.5097
	step [153/249], loss=77.6176
	step [154/249], loss=95.1362
	step [155/249], loss=93.1567
	step [156/249], loss=90.6292
	step [157/249], loss=100.0829
	step [158/249], loss=67.4631
	step [159/249], loss=83.7610
	step [160/249], loss=72.4926
	step [161/249], loss=96.1775
	step [162/249], loss=96.0283
	step [163/249], loss=88.8600
	step [164/249], loss=89.7432
	step [165/249], loss=103.5950
	step [166/249], loss=96.3761
	step [167/249], loss=108.7406
	step [168/249], loss=101.3450
	step [169/249], loss=102.0360
	step [170/249], loss=82.3052
	step [171/249], loss=80.8430
	step [172/249], loss=103.4801
	step [173/249], loss=86.0028
	step [174/249], loss=95.4728
	step [175/249], loss=80.2719
	step [176/249], loss=91.7682
	step [177/249], loss=105.6258
	step [178/249], loss=77.3385
	step [179/249], loss=115.3343
	step [180/249], loss=99.9611
	step [181/249], loss=93.3724
	step [182/249], loss=84.4383
	step [183/249], loss=82.0413
	step [184/249], loss=97.4386
	step [185/249], loss=112.8217
	step [186/249], loss=89.6689
	step [187/249], loss=97.7395
	step [188/249], loss=86.0580
	step [189/249], loss=80.0813
	step [190/249], loss=98.4556
	step [191/249], loss=89.0218
	step [192/249], loss=92.5550
	step [193/249], loss=100.1750
	step [194/249], loss=93.9903
	step [195/249], loss=75.2606
	step [196/249], loss=79.2908
	step [197/249], loss=104.3074
	step [198/249], loss=99.1833
	step [199/249], loss=83.1329
	step [200/249], loss=92.9854
	step [201/249], loss=91.2112
	step [202/249], loss=94.3417
	step [203/249], loss=84.0948
	step [204/249], loss=104.3732
	step [205/249], loss=97.6944
	step [206/249], loss=104.4677
	step [207/249], loss=92.7926
	step [208/249], loss=90.0436
	step [209/249], loss=103.0914
	step [210/249], loss=90.6881
	step [211/249], loss=90.0236
	step [212/249], loss=99.2833
	step [213/249], loss=80.7170
	step [214/249], loss=86.9226
	step [215/249], loss=93.5196
	step [216/249], loss=95.5793
	step [217/249], loss=84.8665
	step [218/249], loss=72.7083
	step [219/249], loss=88.5224
	step [220/249], loss=118.6255
	step [221/249], loss=83.0424
	step [222/249], loss=101.4099
	step [223/249], loss=84.8609
	step [224/249], loss=83.6592
	step [225/249], loss=81.3462
	step [226/249], loss=100.2662
	step [227/249], loss=89.2030
	step [228/249], loss=103.6157
	step [229/249], loss=94.6677
	step [230/249], loss=92.4024
	step [231/249], loss=91.7838
	step [232/249], loss=86.0436
	step [233/249], loss=73.4353
	step [234/249], loss=92.4702
	step [235/249], loss=90.4413
	step [236/249], loss=95.1096
	step [237/249], loss=96.1581
	step [238/249], loss=104.6459
	step [239/249], loss=90.6232
	step [240/249], loss=89.4335
	step [241/249], loss=79.0528
	step [242/249], loss=96.0648
	step [243/249], loss=83.1571
	step [244/249], loss=75.7289
	step [245/249], loss=96.0991
	step [246/249], loss=102.2841
	step [247/249], loss=96.1794
	step [248/249], loss=111.7481
	step [249/249], loss=63.9809
	Evaluating
	loss=0.0084, precision=0.3395, recall=0.9269, f1=0.4970
Training epoch 29
	step [1/249], loss=77.9666
	step [2/249], loss=83.3012
	step [3/249], loss=89.5848
	step [4/249], loss=73.2833
	step [5/249], loss=103.3594
	step [6/249], loss=100.0693
	step [7/249], loss=103.5687
	step [8/249], loss=78.3876
	step [9/249], loss=87.4983
	step [10/249], loss=96.0303
	step [11/249], loss=94.1064
	step [12/249], loss=85.3789
	step [13/249], loss=87.4291
	step [14/249], loss=87.7288
	step [15/249], loss=87.1979
	step [16/249], loss=89.1052
	step [17/249], loss=83.8462
	step [18/249], loss=102.8321
	step [19/249], loss=94.0590
	step [20/249], loss=98.6023
	step [21/249], loss=95.3460
	step [22/249], loss=78.3778
	step [23/249], loss=81.1461
	step [24/249], loss=74.0822
	step [25/249], loss=86.1448
	step [26/249], loss=93.8599
	step [27/249], loss=86.2814
	step [28/249], loss=94.8092
	step [29/249], loss=89.1286
	step [30/249], loss=78.1317
	step [31/249], loss=90.7425
	step [32/249], loss=91.4151
	step [33/249], loss=86.4591
	step [34/249], loss=96.9751
	step [35/249], loss=99.5961
	step [36/249], loss=83.5089
	step [37/249], loss=98.7345
	step [38/249], loss=94.8752
	step [39/249], loss=69.2092
	step [40/249], loss=102.3747
	step [41/249], loss=90.1415
	step [42/249], loss=89.3249
	step [43/249], loss=98.1015
	step [44/249], loss=84.6840
	step [45/249], loss=82.0361
	step [46/249], loss=100.6535
	step [47/249], loss=89.2212
	step [48/249], loss=90.3259
	step [49/249], loss=79.9763
	step [50/249], loss=104.6554
	step [51/249], loss=111.2555
	step [52/249], loss=85.0910
	step [53/249], loss=106.5756
	step [54/249], loss=83.1467
	step [55/249], loss=91.5494
	step [56/249], loss=89.7433
	step [57/249], loss=81.6854
	step [58/249], loss=92.9607
	step [59/249], loss=95.3376
	step [60/249], loss=77.6704
	step [61/249], loss=91.1070
	step [62/249], loss=79.2519
	step [63/249], loss=103.4155
	step [64/249], loss=86.2817
	step [65/249], loss=89.3097
	step [66/249], loss=79.4872
	step [67/249], loss=95.6588
	step [68/249], loss=84.4336
	step [69/249], loss=96.0210
	step [70/249], loss=88.3951
	step [71/249], loss=100.5151
	step [72/249], loss=82.3346
	step [73/249], loss=74.4669
	step [74/249], loss=83.5670
	step [75/249], loss=94.0568
	step [76/249], loss=100.5688
	step [77/249], loss=112.9928
	step [78/249], loss=84.5838
	step [79/249], loss=103.4479
	step [80/249], loss=78.6872
	step [81/249], loss=90.6669
	step [82/249], loss=76.5075
	step [83/249], loss=107.8396
	step [84/249], loss=116.2596
	step [85/249], loss=80.1386
	step [86/249], loss=83.4522
	step [87/249], loss=84.8848
	step [88/249], loss=75.4235
	step [89/249], loss=113.9941
	step [90/249], loss=98.6280
	step [91/249], loss=72.6796
	step [92/249], loss=91.3206
	step [93/249], loss=77.5435
	step [94/249], loss=109.0044
	step [95/249], loss=84.7744
	step [96/249], loss=96.7093
	step [97/249], loss=85.1705
	step [98/249], loss=83.9937
	step [99/249], loss=72.8000
	step [100/249], loss=82.9477
	step [101/249], loss=78.9943
	step [102/249], loss=86.4580
	step [103/249], loss=80.4786
	step [104/249], loss=67.4471
	step [105/249], loss=77.0027
	step [106/249], loss=85.0331
	step [107/249], loss=88.0004
	step [108/249], loss=98.3857
	step [109/249], loss=100.5340
	step [110/249], loss=100.5793
	step [111/249], loss=79.2414
	step [112/249], loss=92.3638
	step [113/249], loss=93.2088
	step [114/249], loss=89.6424
	step [115/249], loss=77.6117
	step [116/249], loss=91.2121
	step [117/249], loss=76.3800
	step [118/249], loss=85.1926
	step [119/249], loss=102.8740
	step [120/249], loss=84.6272
	step [121/249], loss=77.7421
	step [122/249], loss=87.2937
	step [123/249], loss=96.7895
	step [124/249], loss=89.7265
	step [125/249], loss=92.5505
	step [126/249], loss=83.9763
	step [127/249], loss=91.2115
	step [128/249], loss=66.8832
	step [129/249], loss=100.0923
	step [130/249], loss=70.4818
	step [131/249], loss=92.4479
	step [132/249], loss=92.1368
	step [133/249], loss=83.1019
	step [134/249], loss=83.6349
	step [135/249], loss=93.0811
	step [136/249], loss=95.1449
	step [137/249], loss=71.1079
	step [138/249], loss=91.6899
	step [139/249], loss=86.1957
	step [140/249], loss=101.7752
	step [141/249], loss=76.9307
	step [142/249], loss=92.3298
	step [143/249], loss=115.6446
	step [144/249], loss=84.0732
	step [145/249], loss=108.4995
	step [146/249], loss=72.3257
	step [147/249], loss=85.6244
	step [148/249], loss=103.6641
	step [149/249], loss=66.0044
	step [150/249], loss=72.8732
	step [151/249], loss=81.6789
	step [152/249], loss=100.8118
	step [153/249], loss=99.2019
	step [154/249], loss=76.2247
	step [155/249], loss=88.3007
	step [156/249], loss=104.2743
	step [157/249], loss=80.8269
	step [158/249], loss=85.8550
	step [159/249], loss=88.4694
	step [160/249], loss=75.6992
	step [161/249], loss=79.6022
	step [162/249], loss=92.5877
	step [163/249], loss=108.2881
	step [164/249], loss=94.5035
	step [165/249], loss=88.3486
	step [166/249], loss=80.9761
	step [167/249], loss=92.5518
	step [168/249], loss=98.2923
	step [169/249], loss=96.2975
	step [170/249], loss=96.2589
	step [171/249], loss=74.8628
	step [172/249], loss=99.0467
	step [173/249], loss=101.7082
	step [174/249], loss=107.9943
	step [175/249], loss=77.4045
	step [176/249], loss=81.4042
	step [177/249], loss=98.6034
	step [178/249], loss=78.7862
	step [179/249], loss=101.8145
	step [180/249], loss=96.0217
	step [181/249], loss=96.8295
	step [182/249], loss=91.7000
	step [183/249], loss=85.8562
	step [184/249], loss=89.0262
	step [185/249], loss=92.6771
	step [186/249], loss=120.1732
	step [187/249], loss=94.7166
	step [188/249], loss=76.4419
	step [189/249], loss=103.3135
	step [190/249], loss=121.2164
	step [191/249], loss=77.4092
	step [192/249], loss=95.3169
	step [193/249], loss=91.8108
	step [194/249], loss=102.5054
	step [195/249], loss=90.8572
	step [196/249], loss=92.2039
	step [197/249], loss=104.9310
	step [198/249], loss=95.4768
	step [199/249], loss=79.0095
	step [200/249], loss=92.6948
	step [201/249], loss=76.4644
	step [202/249], loss=93.2440
	step [203/249], loss=84.4188
	step [204/249], loss=73.5239
	step [205/249], loss=96.6645
	step [206/249], loss=79.1945
	step [207/249], loss=80.8280
	step [208/249], loss=82.6858
	step [209/249], loss=95.9609
	step [210/249], loss=91.7428
	step [211/249], loss=96.7585
	step [212/249], loss=107.2406
	step [213/249], loss=108.6850
	step [214/249], loss=89.6850
	step [215/249], loss=85.6137
	step [216/249], loss=84.9364
	step [217/249], loss=86.4710
	step [218/249], loss=91.8923
	step [219/249], loss=101.9012
	step [220/249], loss=94.1237
	step [221/249], loss=91.4893
	step [222/249], loss=112.4207
	step [223/249], loss=80.8159
	step [224/249], loss=106.5821
	step [225/249], loss=80.0783
	step [226/249], loss=105.3382
	step [227/249], loss=92.2773
	step [228/249], loss=88.6651
	step [229/249], loss=82.8489
	step [230/249], loss=89.2190
	step [231/249], loss=85.9690
	step [232/249], loss=91.9348
	step [233/249], loss=111.9697
	step [234/249], loss=95.9290
	step [235/249], loss=126.4831
	step [236/249], loss=104.2193
	step [237/249], loss=85.7361
	step [238/249], loss=107.6090
	step [239/249], loss=97.3978
	step [240/249], loss=97.1431
	step [241/249], loss=78.6192
	step [242/249], loss=90.1615
	step [243/249], loss=94.3022
	step [244/249], loss=72.4014
	step [245/249], loss=67.3661
	step [246/249], loss=83.4054
	step [247/249], loss=100.7942
	step [248/249], loss=85.8280
	step [249/249], loss=66.8854
	Evaluating
	loss=0.0076, precision=0.3538, recall=0.9251, f1=0.5119
Training epoch 30
	step [1/249], loss=84.1426
	step [2/249], loss=104.1825
	step [3/249], loss=104.9702
	step [4/249], loss=89.0614
	step [5/249], loss=101.3520
	step [6/249], loss=93.5288
	step [7/249], loss=111.1181
	step [8/249], loss=93.7658
	step [9/249], loss=83.1935
	step [10/249], loss=90.2250
	step [11/249], loss=95.1913
	step [12/249], loss=85.9341
	step [13/249], loss=88.2751
	step [14/249], loss=84.9826
	step [15/249], loss=83.2381
	step [16/249], loss=77.5945
	step [17/249], loss=92.5812
	step [18/249], loss=87.7812
	step [19/249], loss=86.2692
	step [20/249], loss=85.7490
	step [21/249], loss=91.1026
	step [22/249], loss=97.7434
	step [23/249], loss=79.7690
	step [24/249], loss=99.0123
	step [25/249], loss=76.9682
	step [26/249], loss=95.7124
	step [27/249], loss=101.1543
	step [28/249], loss=106.9092
	step [29/249], loss=94.4301
	step [30/249], loss=83.3928
	step [31/249], loss=91.6271
	step [32/249], loss=97.7720
	step [33/249], loss=89.6749
	step [34/249], loss=86.6779
	step [35/249], loss=110.1159
	step [36/249], loss=89.0260
	step [37/249], loss=101.2097
	step [38/249], loss=83.8787
	step [39/249], loss=84.6390
	step [40/249], loss=98.4985
	step [41/249], loss=70.5906
	step [42/249], loss=72.3909
	step [43/249], loss=111.2596
	step [44/249], loss=94.4592
	step [45/249], loss=95.1933
	step [46/249], loss=85.8410
	step [47/249], loss=71.6348
	step [48/249], loss=68.8783
	step [49/249], loss=97.2132
	step [50/249], loss=76.1604
	step [51/249], loss=83.0782
	step [52/249], loss=91.4946
	step [53/249], loss=90.1134
	step [54/249], loss=87.5039
	step [55/249], loss=95.3754
	step [56/249], loss=90.7600
	step [57/249], loss=85.2985
	step [58/249], loss=95.1693
	step [59/249], loss=66.7084
	step [60/249], loss=82.7154
	step [61/249], loss=99.8047
	step [62/249], loss=99.8805
	step [63/249], loss=94.8630
	step [64/249], loss=88.9896
	step [65/249], loss=82.9167
	step [66/249], loss=93.6597
	step [67/249], loss=87.8397
	step [68/249], loss=99.0004
	step [69/249], loss=89.7455
	step [70/249], loss=73.7486
	step [71/249], loss=76.1829
	step [72/249], loss=97.5083
	step [73/249], loss=103.7057
	step [74/249], loss=67.3832
	step [75/249], loss=81.8835
	step [76/249], loss=92.1883
	step [77/249], loss=76.8308
	step [78/249], loss=86.9626
	step [79/249], loss=69.8288
	step [80/249], loss=87.1025
	step [81/249], loss=79.4666
	step [82/249], loss=93.4209
	step [83/249], loss=87.9700
	step [84/249], loss=93.8999
	step [85/249], loss=92.9511
	step [86/249], loss=81.7961
	step [87/249], loss=86.6673
	step [88/249], loss=88.4241
	step [89/249], loss=94.4527
	step [90/249], loss=92.0875
	step [91/249], loss=97.3221
	step [92/249], loss=89.7600
	step [93/249], loss=78.6600
	step [94/249], loss=82.6797
	step [95/249], loss=70.9541
	step [96/249], loss=98.1362
	step [97/249], loss=82.8789
	step [98/249], loss=93.1624
	step [99/249], loss=94.5293
	step [100/249], loss=101.9023
	step [101/249], loss=74.5241
	step [102/249], loss=77.7714
	step [103/249], loss=86.8747
	step [104/249], loss=71.8306
	step [105/249], loss=94.1799
	step [106/249], loss=108.7561
	step [107/249], loss=81.0653
	step [108/249], loss=63.9988
	step [109/249], loss=85.5061
	step [110/249], loss=90.3911
	step [111/249], loss=90.8804
	step [112/249], loss=69.6264
	step [113/249], loss=92.6570
	step [114/249], loss=95.1529
	step [115/249], loss=88.7979
	step [116/249], loss=85.3800
	step [117/249], loss=85.1269
	step [118/249], loss=79.3702
	step [119/249], loss=83.7191
	step [120/249], loss=104.5974
	step [121/249], loss=84.3006
	step [122/249], loss=101.1488
	step [123/249], loss=75.0973
	step [124/249], loss=107.5838
	step [125/249], loss=92.3102
	step [126/249], loss=90.8813
	step [127/249], loss=85.8763
	step [128/249], loss=94.4816
	step [129/249], loss=89.4339
	step [130/249], loss=82.3577
	step [131/249], loss=80.5555
	step [132/249], loss=79.4542
	step [133/249], loss=64.1374
	step [134/249], loss=87.4514
	step [135/249], loss=105.1151
	step [136/249], loss=72.9332
	step [137/249], loss=92.2849
	step [138/249], loss=85.7346
	step [139/249], loss=87.4128
	step [140/249], loss=95.3937
	step [141/249], loss=78.6025
	step [142/249], loss=93.2129
	step [143/249], loss=102.5087
	step [144/249], loss=83.7295
	step [145/249], loss=111.7761
	step [146/249], loss=108.3374
	step [147/249], loss=91.2475
	step [148/249], loss=100.9954
	step [149/249], loss=93.4755
	step [150/249], loss=88.6817
	step [151/249], loss=85.1723
	step [152/249], loss=83.6064
	step [153/249], loss=94.0322
	step [154/249], loss=86.2047
	step [155/249], loss=80.2007
	step [156/249], loss=75.8163
	step [157/249], loss=95.0074
	step [158/249], loss=75.5596
	step [159/249], loss=80.2882
	step [160/249], loss=99.8159
	step [161/249], loss=74.3683
	step [162/249], loss=102.7944
	step [163/249], loss=75.8510
	step [164/249], loss=84.0206
	step [165/249], loss=102.1902
	step [166/249], loss=79.5651
	step [167/249], loss=95.6275
	step [168/249], loss=90.8294
	step [169/249], loss=97.0051
	step [170/249], loss=90.7363
	step [171/249], loss=101.1576
	step [172/249], loss=88.8187
	step [173/249], loss=93.8310
	step [174/249], loss=85.1169
	step [175/249], loss=94.4805
	step [176/249], loss=99.3810
	step [177/249], loss=122.3143
	step [178/249], loss=86.6295
	step [179/249], loss=99.0162
	step [180/249], loss=88.2113
	step [181/249], loss=100.1918
	step [182/249], loss=95.6679
	step [183/249], loss=93.1501
	step [184/249], loss=88.4564
	step [185/249], loss=82.9333
	step [186/249], loss=82.0890
	step [187/249], loss=74.6386
	step [188/249], loss=94.1782
	step [189/249], loss=91.0764
	step [190/249], loss=91.4574
	step [191/249], loss=77.4934
	step [192/249], loss=87.0091
	step [193/249], loss=91.6424
	step [194/249], loss=86.8985
	step [195/249], loss=94.3979
	step [196/249], loss=82.0705
	step [197/249], loss=104.1360
	step [198/249], loss=95.4146
	step [199/249], loss=81.2594
	step [200/249], loss=104.0715
	step [201/249], loss=111.3030
	step [202/249], loss=80.7167
	step [203/249], loss=101.6485
	step [204/249], loss=93.0934
	step [205/249], loss=90.0007
	step [206/249], loss=97.9341
	step [207/249], loss=100.3506
	step [208/249], loss=97.3875
	step [209/249], loss=83.9610
	step [210/249], loss=98.9842
	step [211/249], loss=91.9119
	step [212/249], loss=97.6672
	step [213/249], loss=88.3156
	step [214/249], loss=84.9706
	step [215/249], loss=94.6390
	step [216/249], loss=84.0849
	step [217/249], loss=82.7953
	step [218/249], loss=76.3813
	step [219/249], loss=99.9573
	step [220/249], loss=85.8803
	step [221/249], loss=89.8174
	step [222/249], loss=86.7071
	step [223/249], loss=95.5842
	step [224/249], loss=86.9038
	step [225/249], loss=80.1583
	step [226/249], loss=76.2810
	step [227/249], loss=102.8478
	step [228/249], loss=75.6096
	step [229/249], loss=82.3942
	step [230/249], loss=81.8833
	step [231/249], loss=100.5858
	step [232/249], loss=95.8001
	step [233/249], loss=83.0744
	step [234/249], loss=99.6226
	step [235/249], loss=91.3148
	step [236/249], loss=88.7714
	step [237/249], loss=98.7211
	step [238/249], loss=91.6841
	step [239/249], loss=74.8425
	step [240/249], loss=78.2563
	step [241/249], loss=94.1679
	step [242/249], loss=59.6039
	step [243/249], loss=88.6687
	step [244/249], loss=92.1067
	step [245/249], loss=75.2310
	step [246/249], loss=86.0345
	step [247/249], loss=105.9331
	step [248/249], loss=87.7859
	step [249/249], loss=53.2298
	Evaluating
	loss=0.0058, precision=0.4373, recall=0.8984, f1=0.5883
Training finished
best_f1: 0.6142502233261438
directing: Z rim_enhanced: True test_id 0
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 12281 # image files with weight 12232
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 15579 # all weight files in weight_dir: 3263 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/192], loss=444.4403
	step [2/192], loss=412.3165
	step [3/192], loss=363.0834
	step [4/192], loss=338.8309
	step [5/192], loss=328.2099
	step [6/192], loss=301.0130
	step [7/192], loss=286.6579
	step [8/192], loss=291.7943
	step [9/192], loss=281.3994
	step [10/192], loss=283.6380
	step [11/192], loss=298.4958
	step [12/192], loss=267.0507
	step [13/192], loss=289.7050
	step [14/192], loss=252.3307
	step [15/192], loss=272.7155
	step [16/192], loss=268.4731
	step [17/192], loss=312.7243
	step [18/192], loss=244.7112
	step [19/192], loss=259.5981
	step [20/192], loss=244.4003
	step [21/192], loss=280.8344
	step [22/192], loss=249.5604
	step [23/192], loss=258.7037
	step [24/192], loss=285.0587
	step [25/192], loss=234.9544
	step [26/192], loss=260.1171
	step [27/192], loss=253.8997
	step [28/192], loss=248.5442
	step [29/192], loss=241.9585
	step [30/192], loss=246.5498
	step [31/192], loss=231.0811
	step [32/192], loss=227.8296
	step [33/192], loss=234.9265
	step [34/192], loss=235.4581
	step [35/192], loss=230.0611
	step [36/192], loss=208.3791
	step [37/192], loss=232.9664
	step [38/192], loss=219.0691
	step [39/192], loss=226.1197
	step [40/192], loss=225.4820
	step [41/192], loss=235.6086
	step [42/192], loss=213.0035
	step [43/192], loss=227.2490
	step [44/192], loss=215.5623
	step [45/192], loss=229.6784
	step [46/192], loss=229.1682
	step [47/192], loss=219.6226
	step [48/192], loss=220.7149
	step [49/192], loss=222.7497
	step [50/192], loss=195.3997
	step [51/192], loss=212.8786
	step [52/192], loss=211.8795
	step [53/192], loss=212.5173
	step [54/192], loss=197.6200
	step [55/192], loss=205.8998
	step [56/192], loss=219.2674
	step [57/192], loss=200.7923
	step [58/192], loss=215.2561
	step [59/192], loss=215.7395
	step [60/192], loss=184.8464
	step [61/192], loss=193.5971
	step [62/192], loss=190.0652
	step [63/192], loss=189.0826
	step [64/192], loss=192.3265
	step [65/192], loss=219.5596
	step [66/192], loss=188.7615
	step [67/192], loss=202.6682
	step [68/192], loss=177.8619
	step [69/192], loss=195.1637
	step [70/192], loss=188.9506
	step [71/192], loss=202.8485
	step [72/192], loss=185.7834
	step [73/192], loss=188.5368
	step [74/192], loss=192.8780
	step [75/192], loss=201.5191
	step [76/192], loss=202.3872
	step [77/192], loss=204.9868
	step [78/192], loss=198.9280
	step [79/192], loss=187.5305
	step [80/192], loss=202.4565
	step [81/192], loss=198.6447
	step [82/192], loss=199.8020
	step [83/192], loss=180.1946
	step [84/192], loss=177.7611
	step [85/192], loss=218.0161
	step [86/192], loss=184.8780
	step [87/192], loss=177.6109
	step [88/192], loss=181.7560
	step [89/192], loss=179.9914
	step [90/192], loss=192.5125
	step [91/192], loss=185.4308
	step [92/192], loss=206.1026
	step [93/192], loss=190.6065
	step [94/192], loss=192.6239
	step [95/192], loss=183.1200
	step [96/192], loss=183.3049
	step [97/192], loss=174.1305
	step [98/192], loss=188.9579
	step [99/192], loss=187.0398
	step [100/192], loss=169.3927
	step [101/192], loss=174.2513
	step [102/192], loss=172.9213
	step [103/192], loss=176.8713
	step [104/192], loss=193.9864
	step [105/192], loss=183.9084
	step [106/192], loss=192.3648
	step [107/192], loss=183.3060
	step [108/192], loss=171.5953
	step [109/192], loss=177.2793
	step [110/192], loss=194.5959
	step [111/192], loss=193.8443
	step [112/192], loss=168.2160
	step [113/192], loss=163.4738
	step [114/192], loss=173.7063
	step [115/192], loss=179.8313
	step [116/192], loss=169.8267
	step [117/192], loss=193.6097
	step [118/192], loss=185.2238
	step [119/192], loss=183.4059
	step [120/192], loss=188.3150
	step [121/192], loss=186.2665
	step [122/192], loss=186.7343
	step [123/192], loss=169.5485
	step [124/192], loss=178.0778
	step [125/192], loss=191.3739
	step [126/192], loss=169.2192
	step [127/192], loss=165.5189
	step [128/192], loss=182.1544
	step [129/192], loss=160.2002
	step [130/192], loss=168.4945
	step [131/192], loss=177.5282
	step [132/192], loss=181.2886
	step [133/192], loss=169.3264
	step [134/192], loss=175.7940
	step [135/192], loss=182.4632
	step [136/192], loss=168.6379
	step [137/192], loss=155.9619
	step [138/192], loss=167.0672
	step [139/192], loss=176.4975
	step [140/192], loss=168.5100
	step [141/192], loss=174.1381
	step [142/192], loss=151.7059
	step [143/192], loss=171.7077
	step [144/192], loss=168.3694
	step [145/192], loss=175.0380
	step [146/192], loss=162.9839
	step [147/192], loss=179.8197
	step [148/192], loss=166.4490
	step [149/192], loss=172.1515
	step [150/192], loss=170.5334
	step [151/192], loss=160.7125
	step [152/192], loss=163.3525
	step [153/192], loss=181.3360
	step [154/192], loss=173.4474
	step [155/192], loss=179.3084
	step [156/192], loss=168.0262
	step [157/192], loss=179.9043
	step [158/192], loss=183.0208
	step [159/192], loss=179.4589
	step [160/192], loss=165.5639
	step [161/192], loss=170.3252
	step [162/192], loss=149.1870
	step [163/192], loss=155.5186
	step [164/192], loss=170.4504
	step [165/192], loss=182.6857
	step [166/192], loss=169.6285
	step [167/192], loss=175.3300
	step [168/192], loss=170.7134
	step [169/192], loss=159.8674
	step [170/192], loss=171.9436
	step [171/192], loss=162.3038
	step [172/192], loss=164.8403
	step [173/192], loss=182.7332
	step [174/192], loss=158.6121
	step [175/192], loss=164.9613
	step [176/192], loss=158.4537
	step [177/192], loss=163.3552
	step [178/192], loss=167.5748
	step [179/192], loss=174.2536
	step [180/192], loss=149.9288
	step [181/192], loss=159.1848
	step [182/192], loss=171.8950
	step [183/192], loss=145.1416
	step [184/192], loss=161.6224
	step [185/192], loss=154.5230
	step [186/192], loss=180.9585
	step [187/192], loss=181.8333
	step [188/192], loss=157.1120
	step [189/192], loss=189.2085
	step [190/192], loss=157.4885
	step [191/192], loss=166.2566
	step [192/192], loss=21.7262
	Evaluating
	loss=0.2792, precision=0.2857, recall=0.9031, f1=0.4341
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/192], loss=161.4735
	step [2/192], loss=174.7845
	step [3/192], loss=158.5819
	step [4/192], loss=156.1301
	step [5/192], loss=147.6585
	step [6/192], loss=191.1536
	step [7/192], loss=169.2571
	step [8/192], loss=172.3181
	step [9/192], loss=164.1170
	step [10/192], loss=161.6969
	step [11/192], loss=179.0869
	step [12/192], loss=165.7084
	step [13/192], loss=162.4538
	step [14/192], loss=173.8761
	step [15/192], loss=165.4269
	step [16/192], loss=156.4949
	step [17/192], loss=157.6870
	step [18/192], loss=162.7950
	step [19/192], loss=154.1187
	step [20/192], loss=151.5285
	step [21/192], loss=157.8294
	step [22/192], loss=159.8554
	step [23/192], loss=170.6186
	step [24/192], loss=156.4748
	step [25/192], loss=158.7408
	step [26/192], loss=169.8889
	step [27/192], loss=162.2065
	step [28/192], loss=140.1721
	step [29/192], loss=147.8103
	step [30/192], loss=159.3943
	step [31/192], loss=149.0420
	step [32/192], loss=170.6097
	step [33/192], loss=161.5453
	step [34/192], loss=148.6884
	step [35/192], loss=147.8940
	step [36/192], loss=154.0403
	step [37/192], loss=147.9551
	step [38/192], loss=139.2243
	step [39/192], loss=165.0686
	step [40/192], loss=144.1489
	step [41/192], loss=165.5058
	step [42/192], loss=149.9679
	step [43/192], loss=160.0841
	step [44/192], loss=162.0391
	step [45/192], loss=162.9714
	step [46/192], loss=140.3046
	step [47/192], loss=166.7962
	step [48/192], loss=167.4155
	step [49/192], loss=159.3545
	step [50/192], loss=138.1307
	step [51/192], loss=150.3581
	step [52/192], loss=155.9966
	step [53/192], loss=163.7937
	step [54/192], loss=151.8706
	step [55/192], loss=160.6310
	step [56/192], loss=158.3873
	step [57/192], loss=167.9395
	step [58/192], loss=150.0951
	step [59/192], loss=159.2280
	step [60/192], loss=153.2048
	step [61/192], loss=157.1480
	step [62/192], loss=165.8252
	step [63/192], loss=146.5856
	step [64/192], loss=152.9771
	step [65/192], loss=164.4318
	step [66/192], loss=162.1086
	step [67/192], loss=156.6587
	step [68/192], loss=140.3413
	step [69/192], loss=158.2404
	step [70/192], loss=165.0901
	step [71/192], loss=150.9363
	step [72/192], loss=154.6384
	step [73/192], loss=175.4242
	step [74/192], loss=150.3360
	step [75/192], loss=165.5466
	step [76/192], loss=179.7198
	step [77/192], loss=165.5086
	step [78/192], loss=162.9866
	step [79/192], loss=146.6178
	step [80/192], loss=156.6617
	step [81/192], loss=144.3288
	step [82/192], loss=160.9612
	step [83/192], loss=138.0434
	step [84/192], loss=149.0989
	step [85/192], loss=149.4906
	step [86/192], loss=166.7286
	step [87/192], loss=163.2386
	step [88/192], loss=175.6824
	step [89/192], loss=147.0881
	step [90/192], loss=160.1688
	step [91/192], loss=144.1115
	step [92/192], loss=131.0020
	step [93/192], loss=168.8463
	step [94/192], loss=161.4559
	step [95/192], loss=140.0168
	step [96/192], loss=147.1322
	step [97/192], loss=152.6696
	step [98/192], loss=156.2252
	step [99/192], loss=142.8050
	step [100/192], loss=156.2976
	step [101/192], loss=154.5740
	step [102/192], loss=141.7711
	step [103/192], loss=158.1036
	step [104/192], loss=142.6101
	step [105/192], loss=153.0436
	step [106/192], loss=162.0373
	step [107/192], loss=160.7668
	step [108/192], loss=147.0527
	step [109/192], loss=147.2615
	step [110/192], loss=159.9807
	step [111/192], loss=146.3873
	step [112/192], loss=165.5164
	step [113/192], loss=158.1759
	step [114/192], loss=174.7642
	step [115/192], loss=153.5035
	step [116/192], loss=144.0095
	step [117/192], loss=154.0023
	step [118/192], loss=139.5368
	step [119/192], loss=141.4021
	step [120/192], loss=138.6735
	step [121/192], loss=151.5826
	step [122/192], loss=157.4667
	step [123/192], loss=155.3285
	step [124/192], loss=157.9446
	step [125/192], loss=152.7311
	step [126/192], loss=175.3513
	step [127/192], loss=167.5600
	step [128/192], loss=147.2543
	step [129/192], loss=149.3885
	step [130/192], loss=148.1194
	step [131/192], loss=161.3593
	step [132/192], loss=145.6173
	step [133/192], loss=142.5519
	step [134/192], loss=171.2847
	step [135/192], loss=138.7401
	step [136/192], loss=172.9336
	step [137/192], loss=161.6832
	step [138/192], loss=140.8572
	step [139/192], loss=160.7748
	step [140/192], loss=144.2697
	step [141/192], loss=153.9545
	step [142/192], loss=142.6892
	step [143/192], loss=139.0228
	step [144/192], loss=137.4655
	step [145/192], loss=140.3281
	step [146/192], loss=160.7237
	step [147/192], loss=154.1011
	step [148/192], loss=142.9353
	step [149/192], loss=160.4388
	step [150/192], loss=131.6075
	step [151/192], loss=146.6692
	step [152/192], loss=151.4863
	step [153/192], loss=143.2912
	step [154/192], loss=152.0261
	step [155/192], loss=137.9242
	step [156/192], loss=166.9111
	step [157/192], loss=149.7719
	step [158/192], loss=155.9577
	step [159/192], loss=150.7944
	step [160/192], loss=143.8968
	step [161/192], loss=152.6620
	step [162/192], loss=136.7820
	step [163/192], loss=158.8634
	step [164/192], loss=151.9419
	step [165/192], loss=152.4748
	step [166/192], loss=135.0571
	step [167/192], loss=145.2511
	step [168/192], loss=127.4887
	step [169/192], loss=160.3345
	step [170/192], loss=140.6232
	step [171/192], loss=131.9010
	step [172/192], loss=151.2070
	step [173/192], loss=158.3355
	step [174/192], loss=150.6739
	step [175/192], loss=143.2112
	step [176/192], loss=142.6975
	step [177/192], loss=150.5103
	step [178/192], loss=159.6334
	step [179/192], loss=148.7383
	step [180/192], loss=144.0676
	step [181/192], loss=156.0689
	step [182/192], loss=153.1768
	step [183/192], loss=146.7573
	step [184/192], loss=169.5282
	step [185/192], loss=153.0038
	step [186/192], loss=152.2488
	step [187/192], loss=142.9973
	step [188/192], loss=151.2225
	step [189/192], loss=134.5010
	step [190/192], loss=139.2480
	step [191/192], loss=125.7019
	step [192/192], loss=18.1151
	Evaluating
	loss=0.2196, precision=0.2432, recall=0.9063, f1=0.3834
Training epoch 3
	step [1/192], loss=137.0108
	step [2/192], loss=161.4543
	step [3/192], loss=141.3123
	step [4/192], loss=146.9912
	step [5/192], loss=148.4358
	step [6/192], loss=143.8688
	step [7/192], loss=171.9281
	step [8/192], loss=137.0069
	step [9/192], loss=148.1829
	step [10/192], loss=153.2556
	step [11/192], loss=149.7426
	step [12/192], loss=146.5374
	step [13/192], loss=139.8397
	step [14/192], loss=161.7589
	step [15/192], loss=152.5009
	step [16/192], loss=137.1454
	step [17/192], loss=133.8349
	step [18/192], loss=156.3230
	step [19/192], loss=144.5232
	step [20/192], loss=143.3636
	step [21/192], loss=140.9295
	step [22/192], loss=148.2275
	step [23/192], loss=138.0027
	step [24/192], loss=153.3833
	step [25/192], loss=126.6339
	step [26/192], loss=141.8414
	step [27/192], loss=143.8271
	step [28/192], loss=148.3348
	step [29/192], loss=153.7334
	step [30/192], loss=162.6047
	step [31/192], loss=150.8869
	step [32/192], loss=159.3320
	step [33/192], loss=128.1477
	step [34/192], loss=125.5988
	step [35/192], loss=135.8133
	step [36/192], loss=141.7493
	step [37/192], loss=160.4433
	step [38/192], loss=140.8907
	step [39/192], loss=152.6269
	step [40/192], loss=123.2360
	step [41/192], loss=163.0312
	step [42/192], loss=150.8321
	step [43/192], loss=145.7992
	step [44/192], loss=148.7691
	step [45/192], loss=136.4085
	step [46/192], loss=152.9004
	step [47/192], loss=148.5914
	step [48/192], loss=146.7613
	step [49/192], loss=146.7868
	step [50/192], loss=139.0330
	step [51/192], loss=135.1330
	step [52/192], loss=152.2267
	step [53/192], loss=138.8722
	step [54/192], loss=146.2716
	step [55/192], loss=136.1384
	step [56/192], loss=158.1285
	step [57/192], loss=157.5152
	step [58/192], loss=150.4074
	step [59/192], loss=128.7633
	step [60/192], loss=121.3520
	step [61/192], loss=138.6894
	step [62/192], loss=132.0987
	step [63/192], loss=142.1887
	step [64/192], loss=143.3914
	step [65/192], loss=153.6636
	step [66/192], loss=134.3339
	step [67/192], loss=129.6241
	step [68/192], loss=147.1140
	step [69/192], loss=142.1955
	step [70/192], loss=154.4388
	step [71/192], loss=159.1150
	step [72/192], loss=141.8972
	step [73/192], loss=147.9438
	step [74/192], loss=146.3546
	step [75/192], loss=150.0477
	step [76/192], loss=147.8527
	step [77/192], loss=142.3051
	step [78/192], loss=156.2047
	step [79/192], loss=141.0530
	step [80/192], loss=157.2761
	step [81/192], loss=145.1070
	step [82/192], loss=138.8454
	step [83/192], loss=129.7668
	step [84/192], loss=159.5080
	step [85/192], loss=137.9749
	step [86/192], loss=137.2928
	step [87/192], loss=137.2407
	step [88/192], loss=148.1530
	step [89/192], loss=135.0704
	step [90/192], loss=138.0384
	step [91/192], loss=133.8161
	step [92/192], loss=139.7689
	step [93/192], loss=138.9801
	step [94/192], loss=132.9741
	step [95/192], loss=147.1568
	step [96/192], loss=143.9794
	step [97/192], loss=140.5196
	step [98/192], loss=127.9088
	step [99/192], loss=134.3281
	step [100/192], loss=164.0282
	step [101/192], loss=131.8081
	step [102/192], loss=125.3394
	step [103/192], loss=129.5052
	step [104/192], loss=153.8675
	step [105/192], loss=137.9231
	step [106/192], loss=160.9402
	step [107/192], loss=141.0434
	step [108/192], loss=138.2851
	step [109/192], loss=138.3115
	step [110/192], loss=136.7863
	step [111/192], loss=151.3542
	step [112/192], loss=152.9706
	step [113/192], loss=122.2497
	step [114/192], loss=154.1513
	step [115/192], loss=123.1520
	step [116/192], loss=153.2770
	step [117/192], loss=147.7339
	step [118/192], loss=142.4359
	step [119/192], loss=131.7936
	step [120/192], loss=151.3460
	step [121/192], loss=149.0144
	step [122/192], loss=140.3974
	step [123/192], loss=124.4941
	step [124/192], loss=151.0868
	step [125/192], loss=136.8019
	step [126/192], loss=144.3097
	step [127/192], loss=137.3736
	step [128/192], loss=136.5184
	step [129/192], loss=139.5005
	step [130/192], loss=134.7209
	step [131/192], loss=136.3180
	step [132/192], loss=137.3857
	step [133/192], loss=139.0510
	step [134/192], loss=139.0468
	step [135/192], loss=137.7865
	step [136/192], loss=151.9328
	step [137/192], loss=121.0798
	step [138/192], loss=150.9601
	step [139/192], loss=138.4692
	step [140/192], loss=127.8066
	step [141/192], loss=142.8409
	step [142/192], loss=131.9372
	step [143/192], loss=133.0784
	step [144/192], loss=132.4268
	step [145/192], loss=148.9437
	step [146/192], loss=129.3439
	step [147/192], loss=131.9983
	step [148/192], loss=137.9644
	step [149/192], loss=133.0735
	step [150/192], loss=136.1608
	step [151/192], loss=143.2569
	step [152/192], loss=142.3484
	step [153/192], loss=148.4638
	step [154/192], loss=126.7284
	step [155/192], loss=140.7725
	step [156/192], loss=133.1104
	step [157/192], loss=119.7237
	step [158/192], loss=136.5139
	step [159/192], loss=159.3010
	step [160/192], loss=149.8983
	step [161/192], loss=140.9998
	step [162/192], loss=139.6579
	step [163/192], loss=135.1339
	step [164/192], loss=131.0692
	step [165/192], loss=139.3868
	step [166/192], loss=138.1730
	step [167/192], loss=135.0762
	step [168/192], loss=145.1648
	step [169/192], loss=129.4066
	step [170/192], loss=139.2136
	step [171/192], loss=134.2145
	step [172/192], loss=148.9513
	step [173/192], loss=115.7325
	step [174/192], loss=126.1071
	step [175/192], loss=143.4679
	step [176/192], loss=126.9023
	step [177/192], loss=125.2111
	step [178/192], loss=134.3221
	step [179/192], loss=143.9596
	step [180/192], loss=134.3497
	step [181/192], loss=132.0686
	step [182/192], loss=140.2941
	step [183/192], loss=161.4429
	step [184/192], loss=137.8772
	step [185/192], loss=137.9186
	step [186/192], loss=150.4981
	step [187/192], loss=147.3848
	step [188/192], loss=146.5956
	step [189/192], loss=127.2597
	step [190/192], loss=137.5997
	step [191/192], loss=144.0687
	step [192/192], loss=18.9418
	Evaluating
	loss=0.1736, precision=0.2252, recall=0.9468, f1=0.3639
Training epoch 4
	step [1/192], loss=155.2932
	step [2/192], loss=136.4717
	step [3/192], loss=147.0610
	step [4/192], loss=131.0314
	step [5/192], loss=132.9166
	step [6/192], loss=157.5710
	step [7/192], loss=151.4249
	step [8/192], loss=135.6846
	step [9/192], loss=145.3534
	step [10/192], loss=134.8202
	step [11/192], loss=136.7280
	step [12/192], loss=142.8145
	step [13/192], loss=120.7836
	step [14/192], loss=119.7161
	step [15/192], loss=137.1928
	step [16/192], loss=137.2409
	step [17/192], loss=163.3867
	step [18/192], loss=146.6725
	step [19/192], loss=157.6375
	step [20/192], loss=141.2031
	step [21/192], loss=135.9992
	step [22/192], loss=125.5848
	step [23/192], loss=131.3565
	step [24/192], loss=144.3027
	step [25/192], loss=125.3784
	step [26/192], loss=134.3678
	step [27/192], loss=140.5677
	step [28/192], loss=159.4887
	step [29/192], loss=143.9702
	step [30/192], loss=157.0893
	step [31/192], loss=133.0464
	step [32/192], loss=136.7571
	step [33/192], loss=133.8915
	step [34/192], loss=124.6931
	step [35/192], loss=124.6582
	step [36/192], loss=120.2406
	step [37/192], loss=129.9425
	step [38/192], loss=143.6026
	step [39/192], loss=114.4959
	step [40/192], loss=133.5547
	step [41/192], loss=143.5646
	step [42/192], loss=153.8515
	step [43/192], loss=144.9920
	step [44/192], loss=131.2270
	step [45/192], loss=140.5760
	step [46/192], loss=145.2898
	step [47/192], loss=128.9426
	step [48/192], loss=139.7685
	step [49/192], loss=135.2952
	step [50/192], loss=139.1139
	step [51/192], loss=133.4224
	step [52/192], loss=140.5955
	step [53/192], loss=147.4783
	step [54/192], loss=125.8458
	step [55/192], loss=141.9444
	step [56/192], loss=126.5082
	step [57/192], loss=133.3648
	step [58/192], loss=131.0811
	step [59/192], loss=151.4404
	step [60/192], loss=118.0774
	step [61/192], loss=127.7705
	step [62/192], loss=143.2707
	step [63/192], loss=138.4130
	step [64/192], loss=131.7433
	step [65/192], loss=139.4225
	step [66/192], loss=132.6858
	step [67/192], loss=140.4518
	step [68/192], loss=130.9002
	step [69/192], loss=123.3241
	step [70/192], loss=127.6531
	step [71/192], loss=132.9945
	step [72/192], loss=151.3947
	step [73/192], loss=124.4263
	step [74/192], loss=129.9994
	step [75/192], loss=130.6628
	step [76/192], loss=130.5538
	step [77/192], loss=121.4298
	step [78/192], loss=135.6646
	step [79/192], loss=138.7258
	step [80/192], loss=121.4439
	step [81/192], loss=117.5745
	step [82/192], loss=134.5532
	step [83/192], loss=136.4410
	step [84/192], loss=129.5439
	step [85/192], loss=134.5288
	step [86/192], loss=151.5632
	step [87/192], loss=133.9983
	step [88/192], loss=144.1911
	step [89/192], loss=126.9535
	step [90/192], loss=148.0996
	step [91/192], loss=133.8546
	step [92/192], loss=139.8745
	step [93/192], loss=120.2134
	step [94/192], loss=153.1262
	step [95/192], loss=136.0571
	step [96/192], loss=134.1597
	step [97/192], loss=152.4272
	step [98/192], loss=133.5942
	step [99/192], loss=124.6103
	step [100/192], loss=127.0372
	step [101/192], loss=117.3840
	step [102/192], loss=156.1531
	step [103/192], loss=130.0595
	step [104/192], loss=129.4191
	step [105/192], loss=117.5874
	step [106/192], loss=126.1499
	step [107/192], loss=135.3484
	step [108/192], loss=135.1377
	step [109/192], loss=142.2052
	step [110/192], loss=120.1706
	step [111/192], loss=132.1273
	step [112/192], loss=135.9724
	step [113/192], loss=123.8809
	step [114/192], loss=133.6332
	step [115/192], loss=126.8115
	step [116/192], loss=134.2079
	step [117/192], loss=129.3889
	step [118/192], loss=119.2356
	step [119/192], loss=129.2053
	step [120/192], loss=134.8983
	step [121/192], loss=129.9891
	step [122/192], loss=141.2350
	step [123/192], loss=123.0983
	step [124/192], loss=114.3516
	step [125/192], loss=132.7373
	step [126/192], loss=122.9659
	step [127/192], loss=140.6749
	step [128/192], loss=137.1966
	step [129/192], loss=124.6947
	step [130/192], loss=135.5272
	step [131/192], loss=148.5726
	step [132/192], loss=132.3342
	step [133/192], loss=117.7923
	step [134/192], loss=136.8948
	step [135/192], loss=129.8139
	step [136/192], loss=135.4770
	step [137/192], loss=122.6116
	step [138/192], loss=125.7472
	step [139/192], loss=127.7073
	step [140/192], loss=151.1722
	step [141/192], loss=128.0002
	step [142/192], loss=122.9286
	step [143/192], loss=122.3370
	step [144/192], loss=131.9210
	step [145/192], loss=113.6035
	step [146/192], loss=130.0390
	step [147/192], loss=133.8822
	step [148/192], loss=117.3175
	step [149/192], loss=125.2749
	step [150/192], loss=132.4805
	step [151/192], loss=126.0110
	step [152/192], loss=121.0463
	step [153/192], loss=119.2861
	step [154/192], loss=135.6888
	step [155/192], loss=136.0917
	step [156/192], loss=152.0790
	step [157/192], loss=126.6617
	step [158/192], loss=116.9919
	step [159/192], loss=134.7906
	step [160/192], loss=121.4648
	step [161/192], loss=120.6078
	step [162/192], loss=132.4015
	step [163/192], loss=123.4109
	step [164/192], loss=129.4494
	step [165/192], loss=135.6237
	step [166/192], loss=131.7731
	step [167/192], loss=126.4010
	step [168/192], loss=116.3800
	step [169/192], loss=132.4304
	step [170/192], loss=109.9945
	step [171/192], loss=122.9519
	step [172/192], loss=137.0431
	step [173/192], loss=114.3167
	step [174/192], loss=122.9816
	step [175/192], loss=137.0218
	step [176/192], loss=134.5812
	step [177/192], loss=133.0900
	step [178/192], loss=126.7750
	step [179/192], loss=132.7508
	step [180/192], loss=113.1555
	step [181/192], loss=129.4735
	step [182/192], loss=134.6439
	step [183/192], loss=139.1400
	step [184/192], loss=127.6440
	step [185/192], loss=119.5881
	step [186/192], loss=128.6661
	step [187/192], loss=122.1095
	step [188/192], loss=132.5064
	step [189/192], loss=131.6118
	step [190/192], loss=131.0571
	step [191/192], loss=126.4924
	step [192/192], loss=18.5810
	Evaluating
	loss=0.1345, precision=0.2637, recall=0.9222, f1=0.4101
Training epoch 5
	step [1/192], loss=140.0544
	step [2/192], loss=140.4032
	step [3/192], loss=139.1920
	step [4/192], loss=113.2391
	step [5/192], loss=121.0881
	step [6/192], loss=134.1807
	step [7/192], loss=148.0484
	step [8/192], loss=137.1875
	step [9/192], loss=133.3387
	step [10/192], loss=125.0282
	step [11/192], loss=106.0419
	step [12/192], loss=136.3605
	step [13/192], loss=133.4633
	step [14/192], loss=131.0281
	step [15/192], loss=134.0980
	step [16/192], loss=118.1299
	step [17/192], loss=118.2967
	step [18/192], loss=130.6429
	step [19/192], loss=135.8813
	step [20/192], loss=122.0657
	step [21/192], loss=115.6063
	step [22/192], loss=130.1214
	step [23/192], loss=135.4138
	step [24/192], loss=131.3931
	step [25/192], loss=140.4854
	step [26/192], loss=129.3576
	step [27/192], loss=124.1862
	step [28/192], loss=115.9803
	step [29/192], loss=123.9068
	step [30/192], loss=129.0613
	step [31/192], loss=142.5033
	step [32/192], loss=149.8266
	step [33/192], loss=138.3046
	step [34/192], loss=124.4246
	step [35/192], loss=111.1808
	step [36/192], loss=130.4576
	step [37/192], loss=137.5951
	step [38/192], loss=128.3041
	step [39/192], loss=126.5926
	step [40/192], loss=138.2036
	step [41/192], loss=110.9008
	step [42/192], loss=115.7180
	step [43/192], loss=122.6952
	step [44/192], loss=140.8471
	step [45/192], loss=116.3530
	step [46/192], loss=123.4670
	step [47/192], loss=136.5395
	step [48/192], loss=122.7907
	step [49/192], loss=116.4059
	step [50/192], loss=122.8513
	step [51/192], loss=122.8724
	step [52/192], loss=137.1436
	step [53/192], loss=117.7041
	step [54/192], loss=135.0119
	step [55/192], loss=121.6654
	step [56/192], loss=127.3510
	step [57/192], loss=142.3419
	step [58/192], loss=130.0601
	step [59/192], loss=118.3139
	step [60/192], loss=122.4554
	step [61/192], loss=134.0863
	step [62/192], loss=122.0426
	step [63/192], loss=130.3147
	step [64/192], loss=117.7354
	step [65/192], loss=136.7678
	step [66/192], loss=136.7402
	step [67/192], loss=138.0817
	step [68/192], loss=129.3160
	step [69/192], loss=140.8181
	step [70/192], loss=117.7472
	step [71/192], loss=139.1384
	step [72/192], loss=120.5798
	step [73/192], loss=114.9207
	step [74/192], loss=135.5962
	step [75/192], loss=121.6321
	step [76/192], loss=117.6729
	step [77/192], loss=132.9941
	step [78/192], loss=140.3634
	step [79/192], loss=131.5854
	step [80/192], loss=123.9637
	step [81/192], loss=136.7500
	step [82/192], loss=124.7045
	step [83/192], loss=121.9327
	step [84/192], loss=123.5789
	step [85/192], loss=125.4826
	step [86/192], loss=126.6885
	step [87/192], loss=135.5701
	step [88/192], loss=120.7324
	step [89/192], loss=127.8399
	step [90/192], loss=118.1908
	step [91/192], loss=137.0283
	step [92/192], loss=129.5752
	step [93/192], loss=121.3905
	step [94/192], loss=123.8363
	step [95/192], loss=121.0537
	step [96/192], loss=127.4143
	step [97/192], loss=120.9031
	step [98/192], loss=124.9305
	step [99/192], loss=131.6992
	step [100/192], loss=114.6910
	step [101/192], loss=131.9689
	step [102/192], loss=127.6617
	step [103/192], loss=111.2006
	step [104/192], loss=124.6317
	step [105/192], loss=128.1206
	step [106/192], loss=144.5750
	step [107/192], loss=135.2474
	step [108/192], loss=115.9059
	step [109/192], loss=120.4741
	step [110/192], loss=114.2646
	step [111/192], loss=111.6296
	step [112/192], loss=134.0756
	step [113/192], loss=125.2360
	step [114/192], loss=122.8623
	step [115/192], loss=119.2941
	step [116/192], loss=101.5716
	step [117/192], loss=134.9140
	step [118/192], loss=127.4684
	step [119/192], loss=125.2593
	step [120/192], loss=127.8775
	step [121/192], loss=124.6068
	step [122/192], loss=121.7876
	step [123/192], loss=112.1699
	step [124/192], loss=119.4462
	step [125/192], loss=118.3351
	step [126/192], loss=122.3184
	step [127/192], loss=114.8878
	step [128/192], loss=139.7173
	step [129/192], loss=122.8960
	step [130/192], loss=119.8306
	step [131/192], loss=128.4219
	step [132/192], loss=122.5261
	step [133/192], loss=117.0376
	step [134/192], loss=127.2758
	step [135/192], loss=133.4061
	step [136/192], loss=136.8270
	step [137/192], loss=138.0342
	step [138/192], loss=117.7242
	step [139/192], loss=111.1379
	step [140/192], loss=135.0451
	step [141/192], loss=143.9926
	step [142/192], loss=137.8706
	step [143/192], loss=116.9383
	step [144/192], loss=119.3814
	step [145/192], loss=128.1021
	step [146/192], loss=124.9798
	step [147/192], loss=127.4614
	step [148/192], loss=123.7028
	step [149/192], loss=121.3754
	step [150/192], loss=120.7283
	step [151/192], loss=123.4170
	step [152/192], loss=116.4294
	step [153/192], loss=120.6682
	step [154/192], loss=112.8594
	step [155/192], loss=135.0235
	step [156/192], loss=137.0624
	step [157/192], loss=125.8223
	step [158/192], loss=134.8816
	step [159/192], loss=116.8439
	step [160/192], loss=119.3707
	step [161/192], loss=121.4873
	step [162/192], loss=127.6561
	step [163/192], loss=113.9977
	step [164/192], loss=123.0619
	step [165/192], loss=116.4545
	step [166/192], loss=131.8522
	step [167/192], loss=113.5365
	step [168/192], loss=132.2905
	step [169/192], loss=119.6843
	step [170/192], loss=115.0258
	step [171/192], loss=113.0072
	step [172/192], loss=129.4804
	step [173/192], loss=127.1466
	step [174/192], loss=120.5820
	step [175/192], loss=115.6316
	step [176/192], loss=137.7668
	step [177/192], loss=136.1762
	step [178/192], loss=126.5030
	step [179/192], loss=121.1285
	step [180/192], loss=137.3638
	step [181/192], loss=126.8060
	step [182/192], loss=124.6581
	step [183/192], loss=129.9995
	step [184/192], loss=107.6735
	step [185/192], loss=131.6703
	step [186/192], loss=126.6517
	step [187/192], loss=126.3641
	step [188/192], loss=143.1110
	step [189/192], loss=124.9708
	step [190/192], loss=119.3460
	step [191/192], loss=109.2036
	step [192/192], loss=19.0072
	Evaluating
	loss=0.1043, precision=0.3631, recall=0.9096, f1=0.5191
saving model as: 0_saved_model.pth
Training epoch 6
	step [1/192], loss=122.5174
	step [2/192], loss=119.5061
	step [3/192], loss=133.6731
	step [4/192], loss=116.3646
	step [5/192], loss=143.4782
	step [6/192], loss=129.1086
	step [7/192], loss=118.3301
	step [8/192], loss=125.4499
	step [9/192], loss=113.6178
	step [10/192], loss=129.2515
	step [11/192], loss=130.8537
	step [12/192], loss=131.7349
	step [13/192], loss=141.7241
	step [14/192], loss=106.1035
	step [15/192], loss=132.1147
	step [16/192], loss=130.3599
	step [17/192], loss=122.6691
	step [18/192], loss=125.8496
	step [19/192], loss=126.9075
	step [20/192], loss=130.0618
	step [21/192], loss=133.7834
	step [22/192], loss=114.5499
	step [23/192], loss=127.0357
	step [24/192], loss=114.3445
	step [25/192], loss=125.3721
	step [26/192], loss=132.1364
	step [27/192], loss=129.2490
	step [28/192], loss=112.5103
	step [29/192], loss=113.0946
	step [30/192], loss=115.6866
	step [31/192], loss=126.8233
	step [32/192], loss=133.4096
	step [33/192], loss=112.7543
	step [34/192], loss=102.3510
	step [35/192], loss=121.1680
	step [36/192], loss=116.7152
	step [37/192], loss=111.4586
	step [38/192], loss=125.8655
	step [39/192], loss=140.0253
	step [40/192], loss=98.4790
	step [41/192], loss=132.9404
	step [42/192], loss=137.2404
	step [43/192], loss=117.3378
	step [44/192], loss=131.3771
	step [45/192], loss=119.7934
	step [46/192], loss=108.4761
	step [47/192], loss=116.9980
	step [48/192], loss=119.1454
	step [49/192], loss=123.2704
	step [50/192], loss=113.9783
	step [51/192], loss=123.2156
	step [52/192], loss=115.4292
	step [53/192], loss=124.8540
	step [54/192], loss=130.2299
	step [55/192], loss=131.7543
	step [56/192], loss=118.6934
	step [57/192], loss=127.1154
	step [58/192], loss=129.0382
	step [59/192], loss=148.5598
	step [60/192], loss=123.0713
	step [61/192], loss=137.8591
	step [62/192], loss=119.3836
	step [63/192], loss=135.3855
	step [64/192], loss=124.6094
	step [65/192], loss=128.9057
	step [66/192], loss=121.7764
	step [67/192], loss=119.6561
	step [68/192], loss=126.0788
	step [69/192], loss=115.6576
	step [70/192], loss=127.2416
	step [71/192], loss=117.2837
	step [72/192], loss=121.3148
	step [73/192], loss=133.2018
	step [74/192], loss=126.5386
	step [75/192], loss=121.7499
	step [76/192], loss=131.0880
	step [77/192], loss=132.1979
	step [78/192], loss=113.3343
	step [79/192], loss=110.5649
	step [80/192], loss=115.7566
	step [81/192], loss=99.8641
	step [82/192], loss=122.4401
	step [83/192], loss=133.5817
	step [84/192], loss=125.6928
	step [85/192], loss=123.6246
	step [86/192], loss=114.1723
	step [87/192], loss=133.7916
	step [88/192], loss=126.1009
	step [89/192], loss=135.9018
	step [90/192], loss=116.8188
	step [91/192], loss=123.6028
	step [92/192], loss=133.2987
	step [93/192], loss=121.6230
	step [94/192], loss=118.2922
	step [95/192], loss=112.1995
	step [96/192], loss=141.0709
	step [97/192], loss=121.5960
	step [98/192], loss=129.7735
	step [99/192], loss=132.8086
	step [100/192], loss=124.4122
	step [101/192], loss=106.5958
	step [102/192], loss=118.9536
	step [103/192], loss=119.2755
	step [104/192], loss=110.2161
	step [105/192], loss=94.0613
	step [106/192], loss=109.2786
	step [107/192], loss=109.6258
	step [108/192], loss=120.5394
	step [109/192], loss=123.7032
	step [110/192], loss=118.2275
	step [111/192], loss=140.7881
	step [112/192], loss=121.2650
	step [113/192], loss=124.9787
	step [114/192], loss=121.3811
	step [115/192], loss=133.8673
	step [116/192], loss=107.6476
	step [117/192], loss=122.8258
	step [118/192], loss=107.2256
	step [119/192], loss=118.4068
	step [120/192], loss=120.5012
	step [121/192], loss=108.9744
	step [122/192], loss=120.2398
	step [123/192], loss=116.4383
	step [124/192], loss=136.1583
	step [125/192], loss=133.6800
	step [126/192], loss=115.9266
	step [127/192], loss=137.1931
	step [128/192], loss=139.6860
	step [129/192], loss=107.5324
	step [130/192], loss=118.1280
	step [131/192], loss=127.4498
	step [132/192], loss=115.0770
	step [133/192], loss=101.7319
	step [134/192], loss=115.6563
	step [135/192], loss=118.5051
	step [136/192], loss=118.9478
	step [137/192], loss=105.3017
	step [138/192], loss=120.9545
	step [139/192], loss=103.2199
	step [140/192], loss=133.6683
	step [141/192], loss=115.7524
	step [142/192], loss=117.1720
	step [143/192], loss=127.0754
	step [144/192], loss=112.0043
	step [145/192], loss=118.2252
	step [146/192], loss=123.1335
	step [147/192], loss=104.8455
	step [148/192], loss=113.1772
	step [149/192], loss=128.8795
	step [150/192], loss=121.3365
	step [151/192], loss=129.3816
	step [152/192], loss=117.4682
	step [153/192], loss=131.5875
	step [154/192], loss=133.3039
	step [155/192], loss=117.8636
	step [156/192], loss=117.2005
	step [157/192], loss=113.5926
	step [158/192], loss=115.4905
	step [159/192], loss=126.9522
	step [160/192], loss=113.3256
	step [161/192], loss=122.2738
	step [162/192], loss=104.3694
	step [163/192], loss=115.0533
	step [164/192], loss=118.0320
	step [165/192], loss=117.1260
	step [166/192], loss=107.1790
	step [167/192], loss=121.1198
	step [168/192], loss=124.8138
	step [169/192], loss=120.3680
	step [170/192], loss=103.9378
	step [171/192], loss=135.4721
	step [172/192], loss=127.4363
	step [173/192], loss=125.2717
	step [174/192], loss=98.7525
	step [175/192], loss=134.8851
	step [176/192], loss=113.2686
	step [177/192], loss=109.2234
	step [178/192], loss=117.6325
	step [179/192], loss=123.2954
	step [180/192], loss=100.3132
	step [181/192], loss=113.9888
	step [182/192], loss=121.1854
	step [183/192], loss=105.3209
	step [184/192], loss=118.9261
	step [185/192], loss=108.0914
	step [186/192], loss=144.5813
	step [187/192], loss=120.4776
	step [188/192], loss=120.0725
	step [189/192], loss=140.8360
	step [190/192], loss=102.4565
	step [191/192], loss=105.0170
	step [192/192], loss=12.8307
	Evaluating
	loss=0.0880, precision=0.3419, recall=0.8956, f1=0.4949
Training epoch 7
	step [1/192], loss=125.8881
	step [2/192], loss=113.7237
	step [3/192], loss=117.7140
	step [4/192], loss=116.2438
	step [5/192], loss=128.3249
	step [6/192], loss=148.4521
	step [7/192], loss=122.2784
	step [8/192], loss=111.1432
	step [9/192], loss=113.6581
	step [10/192], loss=116.1824
	step [11/192], loss=140.1966
	step [12/192], loss=123.1679
	step [13/192], loss=118.2847
	step [14/192], loss=113.1225
	step [15/192], loss=107.4969
	step [16/192], loss=110.2709
	step [17/192], loss=128.2232
	step [18/192], loss=115.4830
	step [19/192], loss=127.5608
	step [20/192], loss=120.6155
	step [21/192], loss=117.7145
	step [22/192], loss=117.5978
	step [23/192], loss=108.7707
	step [24/192], loss=118.8154
	step [25/192], loss=111.7908
	step [26/192], loss=122.2905
	step [27/192], loss=123.4068
	step [28/192], loss=121.3226
	step [29/192], loss=122.9321
	step [30/192], loss=97.5320
	step [31/192], loss=123.0476
	step [32/192], loss=144.2737
	step [33/192], loss=126.3663
	step [34/192], loss=111.6610
	step [35/192], loss=128.2914
	step [36/192], loss=119.2953
	step [37/192], loss=121.4409
	step [38/192], loss=111.3263
	step [39/192], loss=110.8335
	step [40/192], loss=124.7921
	step [41/192], loss=108.9377
	step [42/192], loss=121.1572
	step [43/192], loss=115.0979
	step [44/192], loss=116.8074
	step [45/192], loss=128.5533
	step [46/192], loss=111.5139
	step [47/192], loss=118.2237
	step [48/192], loss=117.4410
	step [49/192], loss=109.3605
	step [50/192], loss=126.7145
	step [51/192], loss=135.5790
	step [52/192], loss=114.9525
	step [53/192], loss=126.6517
	step [54/192], loss=111.8988
	step [55/192], loss=108.6126
	step [56/192], loss=132.0094
	step [57/192], loss=109.3314
	step [58/192], loss=130.6761
	step [59/192], loss=135.3525
	step [60/192], loss=110.9875
	step [61/192], loss=112.3855
	step [62/192], loss=129.6686
	step [63/192], loss=126.7232
	step [64/192], loss=115.3801
	step [65/192], loss=119.1304
	step [66/192], loss=119.2081
	step [67/192], loss=114.0181
	step [68/192], loss=130.4344
	step [69/192], loss=101.6057
	step [70/192], loss=119.7809
	step [71/192], loss=121.7413
	step [72/192], loss=104.6157
	step [73/192], loss=136.6102
	step [74/192], loss=111.2323
	step [75/192], loss=123.8634
	step [76/192], loss=105.2883
	step [77/192], loss=137.6483
	step [78/192], loss=123.1551
	step [79/192], loss=123.6755
	step [80/192], loss=101.2496
	step [81/192], loss=113.9340
	step [82/192], loss=117.1647
	step [83/192], loss=104.7170
	step [84/192], loss=116.7918
	step [85/192], loss=117.9040
	step [86/192], loss=118.8585
	step [87/192], loss=118.2832
	step [88/192], loss=109.5446
	step [89/192], loss=118.8680
	step [90/192], loss=114.9891
	step [91/192], loss=128.8015
	step [92/192], loss=113.5356
	step [93/192], loss=122.8533
	step [94/192], loss=113.6705
	step [95/192], loss=114.3666
	step [96/192], loss=116.3890
	step [97/192], loss=112.0028
	step [98/192], loss=126.3447
	step [99/192], loss=118.8153
	step [100/192], loss=119.8292
	step [101/192], loss=115.1514
	step [102/192], loss=108.9081
	step [103/192], loss=126.3110
	step [104/192], loss=121.4260
	step [105/192], loss=126.8297
	step [106/192], loss=117.9412
	step [107/192], loss=118.1400
	step [108/192], loss=104.5973
	step [109/192], loss=119.5528
	step [110/192], loss=122.6887
	step [111/192], loss=114.6596
	step [112/192], loss=115.8045
	step [113/192], loss=103.4024
	step [114/192], loss=109.3105
	step [115/192], loss=119.7809
	step [116/192], loss=125.8033
	step [117/192], loss=113.5781
	step [118/192], loss=106.8111
	step [119/192], loss=104.8790
	step [120/192], loss=109.0503
	step [121/192], loss=106.9033
	step [122/192], loss=118.2051
	step [123/192], loss=108.9503
	step [124/192], loss=111.0776
	step [125/192], loss=104.3543
	step [126/192], loss=112.2197
	step [127/192], loss=117.6046
	step [128/192], loss=116.3556
	step [129/192], loss=129.4325
	step [130/192], loss=114.2069
	step [131/192], loss=121.1710
	step [132/192], loss=123.7951
	step [133/192], loss=133.1524
	step [134/192], loss=117.1306
	step [135/192], loss=122.7677
	step [136/192], loss=116.0536
	step [137/192], loss=107.8064
	step [138/192], loss=114.6336
	step [139/192], loss=113.8541
	step [140/192], loss=130.0710
	step [141/192], loss=109.7801
	step [142/192], loss=109.0418
	step [143/192], loss=126.5992
	step [144/192], loss=120.3893
	step [145/192], loss=105.5777
	step [146/192], loss=119.5532
	step [147/192], loss=110.6590
	step [148/192], loss=121.1150
	step [149/192], loss=110.3606
	step [150/192], loss=113.6749
	step [151/192], loss=102.7520
	step [152/192], loss=108.0663
	step [153/192], loss=104.8586
	step [154/192], loss=121.7890
	step [155/192], loss=114.5570
	step [156/192], loss=103.3672
	step [157/192], loss=119.0129
	step [158/192], loss=96.6918
	step [159/192], loss=125.8402
	step [160/192], loss=125.3253
	step [161/192], loss=113.6815
	step [162/192], loss=104.0586
	step [163/192], loss=107.9767
	step [164/192], loss=116.9939
	step [165/192], loss=107.0271
	step [166/192], loss=120.8718
	step [167/192], loss=117.6498
	step [168/192], loss=105.3975
	step [169/192], loss=126.2728
	step [170/192], loss=124.3958
	step [171/192], loss=108.2897
	step [172/192], loss=109.8482
	step [173/192], loss=108.2686
	step [174/192], loss=130.9496
	step [175/192], loss=126.0206
	step [176/192], loss=92.6897
	step [177/192], loss=111.5107
	step [178/192], loss=112.9101
	step [179/192], loss=116.8940
	step [180/192], loss=108.9889
	step [181/192], loss=130.6549
	step [182/192], loss=116.1479
	step [183/192], loss=113.1564
	step [184/192], loss=120.7281
	step [185/192], loss=109.5803
	step [186/192], loss=119.8906
	step [187/192], loss=116.4359
	step [188/192], loss=117.9587
	step [189/192], loss=113.8620
	step [190/192], loss=124.7869
	step [191/192], loss=124.6392
	step [192/192], loss=16.7525
	Evaluating
	loss=0.0718, precision=0.3585, recall=0.9120, f1=0.5146
Training epoch 8
	step [1/192], loss=112.1688
	step [2/192], loss=96.4691
	step [3/192], loss=120.8003
	step [4/192], loss=112.1727
	step [5/192], loss=113.6961
	step [6/192], loss=128.6785
	step [7/192], loss=123.6945
	step [8/192], loss=115.4326
	step [9/192], loss=129.5027
	step [10/192], loss=96.4577
	step [11/192], loss=122.4804
	step [12/192], loss=124.3992
	step [13/192], loss=107.8210
	step [14/192], loss=114.5498
	step [15/192], loss=104.7857
	step [16/192], loss=119.4660
	step [17/192], loss=112.0814
	step [18/192], loss=109.8371
	step [19/192], loss=86.8751
	step [20/192], loss=113.0127
	step [21/192], loss=118.5370
	step [22/192], loss=118.3589
	step [23/192], loss=125.7341
	step [24/192], loss=127.5760
	step [25/192], loss=105.2463
	step [26/192], loss=137.8427
	step [27/192], loss=103.2824
	step [28/192], loss=93.4817
	step [29/192], loss=96.3412
	step [30/192], loss=110.9896
	step [31/192], loss=123.8472
	step [32/192], loss=110.0670
	step [33/192], loss=126.8902
	step [34/192], loss=108.8976
	step [35/192], loss=122.0240
	step [36/192], loss=144.4665
	step [37/192], loss=111.8744
	step [38/192], loss=113.7104
	step [39/192], loss=95.1299
	step [40/192], loss=127.0505
	step [41/192], loss=104.2847
	step [42/192], loss=120.8490
	step [43/192], loss=130.5631
	step [44/192], loss=104.9500
	step [45/192], loss=109.3349
	step [46/192], loss=118.6521
	step [47/192], loss=115.5763
	step [48/192], loss=128.8258
	step [49/192], loss=117.2066
	step [50/192], loss=120.6755
	step [51/192], loss=102.6499
	step [52/192], loss=123.2510
	step [53/192], loss=127.7947
	step [54/192], loss=114.3854
	step [55/192], loss=117.2408
	step [56/192], loss=114.2978
	step [57/192], loss=113.5761
	step [58/192], loss=104.3890
	step [59/192], loss=98.9410
	step [60/192], loss=106.7160
	step [61/192], loss=113.6363
	step [62/192], loss=102.6958
	step [63/192], loss=120.2048
	step [64/192], loss=107.1192
	step [65/192], loss=104.7468
	step [66/192], loss=115.0255
	step [67/192], loss=109.3944
	step [68/192], loss=104.7875
	step [69/192], loss=114.3065
	step [70/192], loss=112.9993
	step [71/192], loss=109.7290
	step [72/192], loss=131.9960
	step [73/192], loss=108.7843
	step [74/192], loss=113.7912
	step [75/192], loss=111.4089
	step [76/192], loss=107.7067
	step [77/192], loss=102.9330
	step [78/192], loss=119.5137
	step [79/192], loss=111.0505
	step [80/192], loss=128.9895
	step [81/192], loss=101.3072
	step [82/192], loss=127.3425
	step [83/192], loss=105.7183
	step [84/192], loss=124.9372
	step [85/192], loss=125.4450
	step [86/192], loss=107.6801
	step [87/192], loss=112.6098
	step [88/192], loss=122.3879
	step [89/192], loss=118.3642
	step [90/192], loss=118.5229
	step [91/192], loss=95.9179
	step [92/192], loss=122.1666
	step [93/192], loss=102.9433
	step [94/192], loss=95.4158
	step [95/192], loss=124.4664
	step [96/192], loss=134.5044
	step [97/192], loss=100.5434
	step [98/192], loss=101.9275
	step [99/192], loss=127.0454
	step [100/192], loss=113.2377
	step [101/192], loss=138.2915
	step [102/192], loss=125.2023
	step [103/192], loss=128.5461
	step [104/192], loss=106.8118
	step [105/192], loss=109.2557
	step [106/192], loss=114.1558
	step [107/192], loss=111.5075
	step [108/192], loss=111.6477
	step [109/192], loss=111.1347
	step [110/192], loss=109.0810
	step [111/192], loss=107.2825
	step [112/192], loss=105.5413
	step [113/192], loss=122.3395
	step [114/192], loss=123.9142
	step [115/192], loss=117.1631
	step [116/192], loss=101.7122
	step [117/192], loss=113.6185
	step [118/192], loss=103.0679
	step [119/192], loss=110.8634
	step [120/192], loss=120.7406
	step [121/192], loss=150.6411
	step [122/192], loss=122.6952
	step [123/192], loss=115.6121
	step [124/192], loss=125.0520
	step [125/192], loss=129.2973
	step [126/192], loss=123.1944
	step [127/192], loss=107.6970
	step [128/192], loss=91.2110
	step [129/192], loss=109.2995
	step [130/192], loss=103.7534
	step [131/192], loss=103.0618
	step [132/192], loss=115.5672
	step [133/192], loss=113.5209
	step [134/192], loss=130.6746
	step [135/192], loss=103.3558
	step [136/192], loss=109.0723
	step [137/192], loss=106.4021
	step [138/192], loss=124.3225
	step [139/192], loss=123.6818
	step [140/192], loss=108.2074
	step [141/192], loss=103.1215
	step [142/192], loss=112.2345
	step [143/192], loss=106.2798
	step [144/192], loss=115.8772
	step [145/192], loss=115.6964
	step [146/192], loss=125.1780
	step [147/192], loss=98.9369
	step [148/192], loss=106.2251
	step [149/192], loss=111.0998
	step [150/192], loss=123.6129
	step [151/192], loss=118.0121
	step [152/192], loss=125.8922
	step [153/192], loss=98.7249
	step [154/192], loss=119.8995
	step [155/192], loss=121.2019
	step [156/192], loss=119.9927
	step [157/192], loss=114.9250
	step [158/192], loss=119.5182
	step [159/192], loss=112.5284
	step [160/192], loss=109.3216
	step [161/192], loss=103.1377
	step [162/192], loss=110.4800
	step [163/192], loss=111.0325
	step [164/192], loss=108.8667
	step [165/192], loss=101.1667
	step [166/192], loss=118.2756
	step [167/192], loss=101.9566
	step [168/192], loss=111.6788
	step [169/192], loss=107.6669
	step [170/192], loss=104.4008
	step [171/192], loss=100.8854
	step [172/192], loss=98.2939
	step [173/192], loss=126.7243
	step [174/192], loss=103.6621
	step [175/192], loss=119.3878
	step [176/192], loss=108.6705
	step [177/192], loss=101.5893
	step [178/192], loss=104.9634
	step [179/192], loss=111.4927
	step [180/192], loss=129.2519
	step [181/192], loss=112.4346
	step [182/192], loss=113.8177
	step [183/192], loss=121.6836
	step [184/192], loss=113.6947
	step [185/192], loss=105.4258
	step [186/192], loss=120.2710
	step [187/192], loss=113.1568
	step [188/192], loss=120.3148
	step [189/192], loss=127.1129
	step [190/192], loss=132.9302
	step [191/192], loss=127.2263
	step [192/192], loss=17.1829
	Evaluating
	loss=0.0608, precision=0.3361, recall=0.9125, f1=0.4912
Training epoch 9
	step [1/192], loss=113.1045
	step [2/192], loss=110.1574
	step [3/192], loss=110.8257
	step [4/192], loss=112.0296
	step [5/192], loss=111.7414
	step [6/192], loss=95.0417
	step [7/192], loss=125.1417
	step [8/192], loss=110.8299
	step [9/192], loss=120.6741
	step [10/192], loss=117.7487
	step [11/192], loss=118.7130
	step [12/192], loss=122.4244
	step [13/192], loss=112.5876
	step [14/192], loss=127.4367
	step [15/192], loss=102.8640
	step [16/192], loss=123.4275
	step [17/192], loss=102.9371
	step [18/192], loss=112.3244
	step [19/192], loss=96.3877
	step [20/192], loss=100.4453
	step [21/192], loss=98.5113
	step [22/192], loss=101.9683
	step [23/192], loss=105.2733
	step [24/192], loss=120.7489
	step [25/192], loss=107.4522
	step [26/192], loss=126.8227
	step [27/192], loss=110.9250
	step [28/192], loss=118.3269
	step [29/192], loss=123.9306
	step [30/192], loss=125.1058
	step [31/192], loss=121.2434
	step [32/192], loss=113.0914
	step [33/192], loss=120.8066
	step [34/192], loss=105.8289
	step [35/192], loss=102.6956
	step [36/192], loss=128.5531
	step [37/192], loss=99.0160
	step [38/192], loss=95.6419
	step [39/192], loss=104.7645
	step [40/192], loss=102.7629
	step [41/192], loss=134.7015
	step [42/192], loss=96.8246
	step [43/192], loss=114.3741
	step [44/192], loss=121.1354
	step [45/192], loss=108.8481
	step [46/192], loss=106.5105
	step [47/192], loss=112.0080
	step [48/192], loss=107.1368
	step [49/192], loss=123.3488
	step [50/192], loss=112.2846
	step [51/192], loss=114.1186
	step [52/192], loss=107.5608
	step [53/192], loss=100.1557
	step [54/192], loss=113.3546
	step [55/192], loss=108.2882
	step [56/192], loss=108.2695
	step [57/192], loss=111.3975
	step [58/192], loss=115.0157
	step [59/192], loss=115.3199
	step [60/192], loss=106.7806
	step [61/192], loss=107.8755
	step [62/192], loss=116.4637
	step [63/192], loss=110.2481
	step [64/192], loss=124.2189
	step [65/192], loss=102.2281
	step [66/192], loss=91.0949
	step [67/192], loss=107.6803
	step [68/192], loss=105.2449
	step [69/192], loss=118.1583
	step [70/192], loss=103.8421
	step [71/192], loss=104.0626
	step [72/192], loss=103.4729
	step [73/192], loss=118.3641
	step [74/192], loss=119.6179
	step [75/192], loss=112.5387
	step [76/192], loss=119.9090
	step [77/192], loss=114.3121
	step [78/192], loss=118.6413
	step [79/192], loss=119.4244
	step [80/192], loss=110.5649
	step [81/192], loss=109.1461
	step [82/192], loss=105.9286
	step [83/192], loss=105.8523
	step [84/192], loss=100.4448
	step [85/192], loss=101.0675
	step [86/192], loss=116.6104
	step [87/192], loss=101.6992
	step [88/192], loss=122.9947
	step [89/192], loss=138.9892
	step [90/192], loss=104.9093
	step [91/192], loss=119.6857
	step [92/192], loss=117.7676
	step [93/192], loss=125.3917
	step [94/192], loss=93.8097
	step [95/192], loss=120.1115
	step [96/192], loss=110.3825
	step [97/192], loss=107.1186
	step [98/192], loss=110.7354
	step [99/192], loss=107.5740
	step [100/192], loss=124.9801
	step [101/192], loss=134.5213
	step [102/192], loss=122.1562
	step [103/192], loss=103.9901
	step [104/192], loss=119.6533
	step [105/192], loss=96.3650
	step [106/192], loss=117.8523
	step [107/192], loss=113.5004
	step [108/192], loss=107.3835
	step [109/192], loss=109.1351
	step [110/192], loss=100.6051
	step [111/192], loss=99.1043
	step [112/192], loss=107.1316
	step [113/192], loss=123.2439
	step [114/192], loss=118.1562
	step [115/192], loss=116.3558
	step [116/192], loss=120.4988
	step [117/192], loss=112.3759
	step [118/192], loss=112.0480
	step [119/192], loss=111.3726
	step [120/192], loss=97.1370
	step [121/192], loss=107.5234
	step [122/192], loss=106.5800
	step [123/192], loss=107.1105
	step [124/192], loss=118.1539
	step [125/192], loss=106.3136
	step [126/192], loss=117.4054
	step [127/192], loss=99.2131
	step [128/192], loss=118.6573
	step [129/192], loss=109.2225
	step [130/192], loss=138.3219
	step [131/192], loss=117.2088
	step [132/192], loss=106.4327
	step [133/192], loss=107.2087
	step [134/192], loss=103.3998
	step [135/192], loss=119.0275
	step [136/192], loss=114.4942
	step [137/192], loss=117.2018
	step [138/192], loss=94.0258
	step [139/192], loss=112.9375
	step [140/192], loss=110.6015
	step [141/192], loss=113.3269
	step [142/192], loss=114.7582
	step [143/192], loss=117.1049
	step [144/192], loss=107.9424
	step [145/192], loss=111.4522
	step [146/192], loss=114.5953
	step [147/192], loss=103.0215
	step [148/192], loss=103.5698
	step [149/192], loss=94.3183
	step [150/192], loss=97.4017
	step [151/192], loss=107.8795
	step [152/192], loss=108.9614
	step [153/192], loss=105.2406
	step [154/192], loss=110.6860
	step [155/192], loss=109.8978
	step [156/192], loss=117.0744
	step [157/192], loss=89.7679
	step [158/192], loss=107.7635
	step [159/192], loss=103.1946
	step [160/192], loss=95.8373
	step [161/192], loss=98.6428
	step [162/192], loss=113.1885
	step [163/192], loss=119.6221
	step [164/192], loss=114.2796
	step [165/192], loss=105.5447
	step [166/192], loss=125.9620
	step [167/192], loss=105.9967
	step [168/192], loss=107.8516
	step [169/192], loss=101.3007
	step [170/192], loss=115.7767
	step [171/192], loss=96.2622
	step [172/192], loss=114.5133
	step [173/192], loss=105.8649
	step [174/192], loss=102.4002
	step [175/192], loss=120.0987
	step [176/192], loss=118.7253
	step [177/192], loss=103.0927
	step [178/192], loss=98.7064
	step [179/192], loss=123.0232
	step [180/192], loss=113.2216
	step [181/192], loss=123.8739
	step [182/192], loss=99.0583
	step [183/192], loss=105.5237
	step [184/192], loss=103.0418
	step [185/192], loss=107.0696
	step [186/192], loss=101.8926
	step [187/192], loss=115.2450
	step [188/192], loss=112.2698
	step [189/192], loss=110.9388
	step [190/192], loss=101.0373
	step [191/192], loss=124.3355
	step [192/192], loss=14.9584
	Evaluating
	loss=0.0528, precision=0.2981, recall=0.9307, f1=0.4516
Training epoch 10
	step [1/192], loss=98.3809
	step [2/192], loss=111.7780
	step [3/192], loss=107.2454
	step [4/192], loss=107.5263
	step [5/192], loss=102.6302
	step [6/192], loss=113.3219
	step [7/192], loss=110.2376
	step [8/192], loss=106.3706
	step [9/192], loss=120.2650
	step [10/192], loss=109.9653
	step [11/192], loss=96.3276
	step [12/192], loss=113.1181
	step [13/192], loss=111.2590
	step [14/192], loss=122.7113
	step [15/192], loss=97.1700
	step [16/192], loss=105.5716
	step [17/192], loss=116.7061
	step [18/192], loss=96.9420
	step [19/192], loss=103.2487
	step [20/192], loss=111.5633
	step [21/192], loss=105.9354
	step [22/192], loss=100.3951
	step [23/192], loss=114.0313
	step [24/192], loss=103.8811
	step [25/192], loss=96.9501
	step [26/192], loss=113.7850
	step [27/192], loss=112.7634
	step [28/192], loss=103.9361
	step [29/192], loss=119.4136
	step [30/192], loss=104.5822
	step [31/192], loss=100.6548
	step [32/192], loss=105.1477
	step [33/192], loss=114.0953
	step [34/192], loss=109.5257
	step [35/192], loss=117.2974
	step [36/192], loss=106.0264
	step [37/192], loss=113.3704
	step [38/192], loss=114.5238
	step [39/192], loss=104.2246
	step [40/192], loss=92.7500
	step [41/192], loss=112.1143
	step [42/192], loss=116.7713
	step [43/192], loss=123.7302
	step [44/192], loss=94.8160
	step [45/192], loss=106.1266
	step [46/192], loss=112.5830
	step [47/192], loss=111.5985
	step [48/192], loss=103.2553
	step [49/192], loss=107.9563
	step [50/192], loss=110.4997
	step [51/192], loss=112.5976
	step [52/192], loss=95.0508
	step [53/192], loss=99.5817
	step [54/192], loss=114.1881
	step [55/192], loss=114.4109
	step [56/192], loss=97.4001
	step [57/192], loss=122.9854
	step [58/192], loss=110.3599
	step [59/192], loss=112.4465
	step [60/192], loss=105.1530
	step [61/192], loss=97.7768
	step [62/192], loss=109.1383
	step [63/192], loss=109.0488
	step [64/192], loss=113.9149
	step [65/192], loss=109.9263
	step [66/192], loss=108.1629
	step [67/192], loss=120.3140
	step [68/192], loss=109.5513
	step [69/192], loss=121.2383
	step [70/192], loss=95.7455
	step [71/192], loss=129.9371
	step [72/192], loss=112.2536
	step [73/192], loss=110.9941
	step [74/192], loss=103.7966
	step [75/192], loss=113.0974
	step [76/192], loss=107.2070
	step [77/192], loss=113.7192
	step [78/192], loss=107.2353
	step [79/192], loss=118.5286
	step [80/192], loss=121.3337
	step [81/192], loss=112.4199
	step [82/192], loss=100.1175
	step [83/192], loss=106.4492
	step [84/192], loss=107.9788
	step [85/192], loss=104.2645
	step [86/192], loss=114.2047
	step [87/192], loss=105.8314
	step [88/192], loss=98.0654
	step [89/192], loss=101.5302
	step [90/192], loss=108.5229
	step [91/192], loss=123.4515
	step [92/192], loss=111.8337
	step [93/192], loss=97.8716
	step [94/192], loss=115.4046
	step [95/192], loss=118.0466
	step [96/192], loss=116.0645
	step [97/192], loss=108.2105
	step [98/192], loss=110.6102
	step [99/192], loss=113.0533
	step [100/192], loss=98.7598
	step [101/192], loss=107.2875
	step [102/192], loss=107.1874
	step [103/192], loss=104.1518
	step [104/192], loss=92.5587
	step [105/192], loss=104.9017
	step [106/192], loss=116.9041
	step [107/192], loss=103.0900
	step [108/192], loss=109.3856
	step [109/192], loss=106.4562
	step [110/192], loss=112.5597
	step [111/192], loss=108.9197
	step [112/192], loss=104.4313
	step [113/192], loss=103.3625
	step [114/192], loss=112.1388
	step [115/192], loss=106.8016
	step [116/192], loss=113.3576
	step [117/192], loss=110.0917
	step [118/192], loss=116.4281
	step [119/192], loss=105.7834
	step [120/192], loss=91.0336
	step [121/192], loss=98.9129
	step [122/192], loss=100.7419
	step [123/192], loss=97.3477
	step [124/192], loss=99.1100
	step [125/192], loss=113.6305
	step [126/192], loss=100.3224
	step [127/192], loss=107.3994
	step [128/192], loss=95.9253
	step [129/192], loss=111.1180
	step [130/192], loss=113.8461
	step [131/192], loss=111.8339
	step [132/192], loss=115.6347
	step [133/192], loss=97.8371
	step [134/192], loss=115.9647
	step [135/192], loss=112.0964
	step [136/192], loss=118.1837
	step [137/192], loss=114.0587
	step [138/192], loss=96.6320
	step [139/192], loss=113.9044
	step [140/192], loss=108.6616
	step [141/192], loss=114.1358
	step [142/192], loss=113.4163
	step [143/192], loss=96.8610
	step [144/192], loss=109.8636
	step [145/192], loss=109.8219
	step [146/192], loss=94.8802
	step [147/192], loss=106.8668
	step [148/192], loss=108.6157
	step [149/192], loss=95.7200
	step [150/192], loss=105.7292
	step [151/192], loss=109.4181
	step [152/192], loss=102.8803
	step [153/192], loss=115.5156
	step [154/192], loss=98.9900
	step [155/192], loss=122.0585
	step [156/192], loss=98.5129
	step [157/192], loss=108.9946
	step [158/192], loss=111.3692
	step [159/192], loss=112.1915
	step [160/192], loss=111.4180
	step [161/192], loss=115.9713
	step [162/192], loss=117.9531
	step [163/192], loss=98.9512
	step [164/192], loss=116.1050
	step [165/192], loss=113.0423
	step [166/192], loss=97.3981
	step [167/192], loss=97.4044
	step [168/192], loss=100.0162
	step [169/192], loss=126.8835
	step [170/192], loss=125.5475
	step [171/192], loss=111.4659
	step [172/192], loss=96.6013
	step [173/192], loss=110.7192
	step [174/192], loss=109.4635
	step [175/192], loss=94.4604
	step [176/192], loss=100.9541
	step [177/192], loss=115.2720
	step [178/192], loss=111.2711
	step [179/192], loss=107.0423
	step [180/192], loss=103.1858
	step [181/192], loss=121.5900
	step [182/192], loss=122.3382
	step [183/192], loss=95.5042
	step [184/192], loss=111.3946
	step [185/192], loss=103.4099
	step [186/192], loss=124.1484
	step [187/192], loss=108.0738
	step [188/192], loss=111.6457
	step [189/192], loss=116.4994
	step [190/192], loss=107.9956
	step [191/192], loss=108.5698
	step [192/192], loss=10.8456
	Evaluating
	loss=0.0461, precision=0.3126, recall=0.9111, f1=0.4655
Training epoch 11
	step [1/192], loss=103.8083
	step [2/192], loss=107.2913
	step [3/192], loss=95.4283
	step [4/192], loss=114.4518
	step [5/192], loss=108.8598
	step [6/192], loss=115.0084
	step [7/192], loss=109.1047
	step [8/192], loss=113.1068
	step [9/192], loss=119.2514
	step [10/192], loss=104.6153
	step [11/192], loss=111.8337
	step [12/192], loss=101.8858
	step [13/192], loss=110.0876
	step [14/192], loss=112.2445
	step [15/192], loss=116.1028
	step [16/192], loss=107.5189
	step [17/192], loss=106.2019
	step [18/192], loss=105.1268
	step [19/192], loss=103.4022
	step [20/192], loss=127.1886
	step [21/192], loss=110.6562
	step [22/192], loss=108.2513
	step [23/192], loss=106.2793
	step [24/192], loss=105.2400
	step [25/192], loss=100.8491
	step [26/192], loss=108.4220
	step [27/192], loss=108.3678
	step [28/192], loss=115.0407
	step [29/192], loss=101.6380
	step [30/192], loss=97.3045
	step [31/192], loss=124.1150
	step [32/192], loss=103.0108
	step [33/192], loss=105.0432
	step [34/192], loss=116.3815
	step [35/192], loss=105.9117
	step [36/192], loss=109.9115
	step [37/192], loss=104.3379
	step [38/192], loss=105.3573
	step [39/192], loss=101.3000
	step [40/192], loss=92.1004
	step [41/192], loss=111.3361
	step [42/192], loss=91.9848
	step [43/192], loss=97.5951
	step [44/192], loss=105.2888
	step [45/192], loss=104.8625
	step [46/192], loss=80.6417
	step [47/192], loss=96.6661
	step [48/192], loss=99.5274
	step [49/192], loss=112.1996
	step [50/192], loss=109.6628
	step [51/192], loss=110.3218
	step [52/192], loss=119.1638
	step [53/192], loss=114.9219
	step [54/192], loss=106.9515
	step [55/192], loss=100.5619
	step [56/192], loss=106.1654
	step [57/192], loss=100.7899
	step [58/192], loss=96.3726
	step [59/192], loss=117.6557
	step [60/192], loss=96.4631
	step [61/192], loss=95.2864
	step [62/192], loss=108.2960
	step [63/192], loss=107.1340
	step [64/192], loss=91.4016
	step [65/192], loss=92.6077
	step [66/192], loss=100.4245
	step [67/192], loss=100.3537
	step [68/192], loss=112.8369
	step [69/192], loss=111.4118
	step [70/192], loss=102.9630
	step [71/192], loss=104.2988
	step [72/192], loss=110.5971
	step [73/192], loss=101.7332
	step [74/192], loss=119.6600
	step [75/192], loss=100.2109
	step [76/192], loss=108.6971
	step [77/192], loss=117.2501
	step [78/192], loss=112.3437
	step [79/192], loss=108.5488
	step [80/192], loss=113.1678
	step [81/192], loss=102.8765
	step [82/192], loss=102.3874
	step [83/192], loss=90.0338
	step [84/192], loss=105.0228
	step [85/192], loss=110.4629
	step [86/192], loss=96.5257
	step [87/192], loss=100.3402
	step [88/192], loss=113.5199
	step [89/192], loss=106.7147
	step [90/192], loss=115.2135
	step [91/192], loss=111.9821
	step [92/192], loss=109.8425
	step [93/192], loss=110.9199
	step [94/192], loss=100.6367
	step [95/192], loss=101.5507
	step [96/192], loss=123.0738
	step [97/192], loss=108.5693
	step [98/192], loss=111.7901
	step [99/192], loss=118.2837
	step [100/192], loss=105.4455
	step [101/192], loss=92.8934
	step [102/192], loss=101.9275
	step [103/192], loss=117.9197
	step [104/192], loss=93.0643
	step [105/192], loss=123.3355
	step [106/192], loss=104.0408
	step [107/192], loss=108.2702
	step [108/192], loss=93.6077
	step [109/192], loss=109.5666
	step [110/192], loss=93.8182
	step [111/192], loss=106.1059
	step [112/192], loss=98.9316
	step [113/192], loss=103.9933
	step [114/192], loss=100.0556
	step [115/192], loss=102.8215
	step [116/192], loss=123.0383
	step [117/192], loss=100.1050
	step [118/192], loss=100.7510
	step [119/192], loss=98.9118
	step [120/192], loss=101.6328
	step [121/192], loss=107.0245
	step [122/192], loss=108.9680
	step [123/192], loss=107.3810
	step [124/192], loss=112.6157
	step [125/192], loss=105.2563
	step [126/192], loss=116.2761
	step [127/192], loss=109.5589
	step [128/192], loss=98.1016
	step [129/192], loss=111.7625
	step [130/192], loss=114.9773
	step [131/192], loss=97.2646
	step [132/192], loss=107.2087
	step [133/192], loss=127.7620
	step [134/192], loss=111.8181
	step [135/192], loss=105.6386
	step [136/192], loss=115.7552
	step [137/192], loss=96.2695
	step [138/192], loss=104.2874
	step [139/192], loss=116.9255
	step [140/192], loss=107.4592
	step [141/192], loss=106.5773
	step [142/192], loss=95.5171
	step [143/192], loss=106.9577
	step [144/192], loss=109.5303
	step [145/192], loss=96.8542
	step [146/192], loss=107.6507
	step [147/192], loss=110.3741
	step [148/192], loss=101.6736
	step [149/192], loss=110.3760
	step [150/192], loss=96.9855
	step [151/192], loss=129.2023
	step [152/192], loss=104.5523
	step [153/192], loss=103.5614
	step [154/192], loss=109.5221
	step [155/192], loss=95.4768
	step [156/192], loss=109.5494
	step [157/192], loss=100.3116
	step [158/192], loss=125.6963
	step [159/192], loss=111.7327
	step [160/192], loss=103.8584
	step [161/192], loss=121.6555
	step [162/192], loss=110.6479
	step [163/192], loss=96.4752
	step [164/192], loss=106.2540
	step [165/192], loss=98.6625
	step [166/192], loss=108.0930
	step [167/192], loss=103.8884
	step [168/192], loss=108.1638
	step [169/192], loss=106.2277
	step [170/192], loss=114.3677
	step [171/192], loss=112.3124
	step [172/192], loss=110.4314
	step [173/192], loss=95.3542
	step [174/192], loss=104.1426
	step [175/192], loss=90.5221
	step [176/192], loss=109.2311
	step [177/192], loss=119.3370
	step [178/192], loss=102.0456
	step [179/192], loss=102.9248
	step [180/192], loss=98.5606
	step [181/192], loss=100.8083
	step [182/192], loss=126.1243
	step [183/192], loss=102.1943
	step [184/192], loss=112.5306
	step [185/192], loss=91.9932
	step [186/192], loss=91.7743
	step [187/192], loss=111.0247
	step [188/192], loss=100.3559
	step [189/192], loss=97.4809
	step [190/192], loss=100.9986
	step [191/192], loss=134.6840
	step [192/192], loss=13.6217
	Evaluating
	loss=0.0407, precision=0.3560, recall=0.9077, f1=0.5114
Training epoch 12
	step [1/192], loss=112.5138
	step [2/192], loss=113.7788
	step [3/192], loss=102.9949
	step [4/192], loss=103.4659
	step [5/192], loss=110.3388
	step [6/192], loss=114.2497
	step [7/192], loss=110.5926
	step [8/192], loss=94.0630
	step [9/192], loss=104.4305
	step [10/192], loss=96.3553
	step [11/192], loss=108.4703
	step [12/192], loss=100.7650
	step [13/192], loss=104.4407
	step [14/192], loss=97.9037
	step [15/192], loss=90.6684
	step [16/192], loss=100.3977
	step [17/192], loss=102.2627
	step [18/192], loss=113.3176
	step [19/192], loss=101.0739
	step [20/192], loss=95.4464
	step [21/192], loss=99.1661
	step [22/192], loss=105.4472
	step [23/192], loss=112.7118
	step [24/192], loss=111.7378
	step [25/192], loss=90.6533
	step [26/192], loss=95.3901
	step [27/192], loss=100.0917
	step [28/192], loss=91.2483
	step [29/192], loss=92.2929
	step [30/192], loss=105.7190
	step [31/192], loss=106.5224
	step [32/192], loss=104.8040
	step [33/192], loss=103.9333
	step [34/192], loss=103.7069
	step [35/192], loss=113.4971
	step [36/192], loss=105.4377
	step [37/192], loss=93.0564
	step [38/192], loss=117.8614
	step [39/192], loss=112.5423
	step [40/192], loss=118.1722
	step [41/192], loss=120.9625
	step [42/192], loss=113.5013
	step [43/192], loss=109.8751
	step [44/192], loss=108.5859
	step [45/192], loss=101.0730
	step [46/192], loss=102.1069
	step [47/192], loss=96.3394
	step [48/192], loss=103.3193
	step [49/192], loss=115.7761
	step [50/192], loss=123.4620
	step [51/192], loss=108.0606
	step [52/192], loss=106.5635
	step [53/192], loss=109.1555
	step [54/192], loss=105.2353
	step [55/192], loss=93.5830
	step [56/192], loss=99.6930
	step [57/192], loss=108.0021
	step [58/192], loss=93.6118
	step [59/192], loss=115.5257
	step [60/192], loss=90.6900
	step [61/192], loss=102.9874
	step [62/192], loss=121.2730
	step [63/192], loss=90.0007
	step [64/192], loss=95.2321
	step [65/192], loss=108.2033
	step [66/192], loss=95.1876
	step [67/192], loss=117.8437
	step [68/192], loss=107.8722
	step [69/192], loss=99.5749
	step [70/192], loss=94.8206
	step [71/192], loss=103.0195
	step [72/192], loss=112.3408
	step [73/192], loss=104.2165
	step [74/192], loss=102.0146
	step [75/192], loss=111.3219
	step [76/192], loss=100.7167
	step [77/192], loss=121.3490
	step [78/192], loss=90.5865
	step [79/192], loss=95.0628
	step [80/192], loss=102.7159
	step [81/192], loss=99.5315
	step [82/192], loss=107.0428
	step [83/192], loss=106.6191
	step [84/192], loss=99.3415
	step [85/192], loss=97.3785
	step [86/192], loss=98.6225
	step [87/192], loss=90.8646
	step [88/192], loss=114.1792
	step [89/192], loss=92.6603
	step [90/192], loss=101.5069
	step [91/192], loss=100.9580
	step [92/192], loss=95.4778
	step [93/192], loss=95.0273
	step [94/192], loss=109.7110
	step [95/192], loss=109.4165
	step [96/192], loss=100.2655
	step [97/192], loss=107.8017
	step [98/192], loss=122.2691
	step [99/192], loss=100.1394
	step [100/192], loss=108.8485
	step [101/192], loss=116.0010
	step [102/192], loss=97.6479
	step [103/192], loss=134.7873
	step [104/192], loss=94.8719
	step [105/192], loss=102.5022
	step [106/192], loss=94.7243
	step [107/192], loss=91.1718
	step [108/192], loss=105.9303
	step [109/192], loss=100.4790
	step [110/192], loss=102.6593
	step [111/192], loss=102.8495
	step [112/192], loss=100.6118
	step [113/192], loss=98.0133
	step [114/192], loss=108.3620
	step [115/192], loss=117.8481
	step [116/192], loss=104.5590
	step [117/192], loss=113.6846
	step [118/192], loss=98.1703
	step [119/192], loss=97.6432
	step [120/192], loss=110.1958
	step [121/192], loss=94.0807
	step [122/192], loss=106.5122
	step [123/192], loss=115.4332
	step [124/192], loss=111.9092
	step [125/192], loss=106.0184
	step [126/192], loss=103.7129
	step [127/192], loss=104.2529
	step [128/192], loss=104.1326
	step [129/192], loss=104.4808
	step [130/192], loss=107.9186
	step [131/192], loss=99.6677
	step [132/192], loss=98.7763
	step [133/192], loss=102.0296
	step [134/192], loss=101.3100
	step [135/192], loss=111.7526
	step [136/192], loss=103.0454
	step [137/192], loss=109.7224
	step [138/192], loss=93.7575
	step [139/192], loss=112.1083
	step [140/192], loss=114.5852
	step [141/192], loss=112.5536
	step [142/192], loss=110.6699
	step [143/192], loss=94.4657
	step [144/192], loss=80.4530
	step [145/192], loss=97.6573
	step [146/192], loss=106.2857
	step [147/192], loss=109.2916
	step [148/192], loss=115.8182
	step [149/192], loss=116.8328
	step [150/192], loss=99.1774
	step [151/192], loss=101.4020
	step [152/192], loss=127.1807
	step [153/192], loss=108.2199
	step [154/192], loss=107.1414
	step [155/192], loss=107.7332
	step [156/192], loss=104.9245
	step [157/192], loss=107.0201
	step [158/192], loss=104.8790
	step [159/192], loss=102.2339
	step [160/192], loss=108.1732
	step [161/192], loss=102.5548
	step [162/192], loss=106.6403
	step [163/192], loss=109.1242
	step [164/192], loss=106.3066
	step [165/192], loss=107.5231
	step [166/192], loss=115.8509
	step [167/192], loss=107.1272
	step [168/192], loss=103.4615
	step [169/192], loss=89.6455
	step [170/192], loss=107.7580
	step [171/192], loss=95.3683
	step [172/192], loss=108.9399
	step [173/192], loss=103.5434
	step [174/192], loss=103.1566
	step [175/192], loss=114.7799
	step [176/192], loss=96.6758
	step [177/192], loss=117.9232
	step [178/192], loss=96.6091
	step [179/192], loss=93.9295
	step [180/192], loss=113.9547
	step [181/192], loss=85.2122
	step [182/192], loss=89.0339
	step [183/192], loss=126.4450
	step [184/192], loss=110.2104
	step [185/192], loss=95.2296
	step [186/192], loss=97.9745
	step [187/192], loss=110.5317
	step [188/192], loss=99.0219
	step [189/192], loss=87.8124
	step [190/192], loss=91.4298
	step [191/192], loss=120.7409
	step [192/192], loss=13.6477
	Evaluating
	loss=0.0343, precision=0.3689, recall=0.9210, f1=0.5268
saving model as: 0_saved_model.pth
Training epoch 13
	step [1/192], loss=99.9791
	step [2/192], loss=108.2888
	step [3/192], loss=100.8778
	step [4/192], loss=106.5560
	step [5/192], loss=106.4014
	step [6/192], loss=95.1679
	step [7/192], loss=106.3292
	step [8/192], loss=117.4682
	step [9/192], loss=94.0786
	step [10/192], loss=106.9751
	step [11/192], loss=110.9377
	step [12/192], loss=99.5585
	step [13/192], loss=100.7771
	step [14/192], loss=88.8898
	step [15/192], loss=111.5985
	step [16/192], loss=96.0068
	step [17/192], loss=102.3317
	step [18/192], loss=103.1935
	step [19/192], loss=114.6667
	step [20/192], loss=116.0295
	step [21/192], loss=99.3527
	step [22/192], loss=103.3846
	step [23/192], loss=108.5167
	step [24/192], loss=103.8098
	step [25/192], loss=102.0359
	step [26/192], loss=99.6334
	step [27/192], loss=108.3043
	step [28/192], loss=96.7628
	step [29/192], loss=89.3399
	step [30/192], loss=109.8815
	step [31/192], loss=101.3899
	step [32/192], loss=87.3366
	step [33/192], loss=105.0808
	step [34/192], loss=94.1499
	step [35/192], loss=95.1442
	step [36/192], loss=105.6633
	step [37/192], loss=117.8290
	step [38/192], loss=104.1819
	step [39/192], loss=105.2941
	step [40/192], loss=106.0133
	step [41/192], loss=101.3491
	step [42/192], loss=94.7027
	step [43/192], loss=100.2005
	step [44/192], loss=90.7658
	step [45/192], loss=97.0563
	step [46/192], loss=126.3098
	step [47/192], loss=100.4428
	step [48/192], loss=120.0686
	step [49/192], loss=109.4686
	step [50/192], loss=114.4773
	step [51/192], loss=111.6945
	step [52/192], loss=107.1739
	step [53/192], loss=101.4122
	step [54/192], loss=97.0162
	step [55/192], loss=87.3277
	step [56/192], loss=105.2027
	step [57/192], loss=100.3146
	step [58/192], loss=97.4838
	step [59/192], loss=101.2581
	step [60/192], loss=94.0624
	step [61/192], loss=91.5292
	step [62/192], loss=94.9132
	step [63/192], loss=110.8226
	step [64/192], loss=114.0846
	step [65/192], loss=90.6958
	step [66/192], loss=103.6877
	step [67/192], loss=110.3402
	step [68/192], loss=87.4807
	step [69/192], loss=87.0300
	step [70/192], loss=114.3368
	step [71/192], loss=95.3184
	step [72/192], loss=101.9396
	step [73/192], loss=121.6736
	step [74/192], loss=105.6456
	step [75/192], loss=89.5671
	step [76/192], loss=99.0664
	step [77/192], loss=99.3118
	step [78/192], loss=118.7688
	step [79/192], loss=108.5727
	step [80/192], loss=100.5077
	step [81/192], loss=106.7901
	step [82/192], loss=97.4259
	step [83/192], loss=106.4533
	step [84/192], loss=99.8142
	step [85/192], loss=101.6407
	step [86/192], loss=111.5031
	step [87/192], loss=121.0752
	step [88/192], loss=107.8753
	step [89/192], loss=95.7217
	step [90/192], loss=93.2753
	step [91/192], loss=117.7398
	step [92/192], loss=110.7017
	step [93/192], loss=102.3023
	step [94/192], loss=106.8434
	step [95/192], loss=100.0407
	step [96/192], loss=92.7489
	step [97/192], loss=107.0999
	step [98/192], loss=97.5126
	step [99/192], loss=106.5922
	step [100/192], loss=93.9249
	step [101/192], loss=106.9577
	step [102/192], loss=93.4013
	step [103/192], loss=110.8257
	step [104/192], loss=118.1035
	step [105/192], loss=115.1484
	step [106/192], loss=95.2394
	step [107/192], loss=100.0974
	step [108/192], loss=105.7621
	step [109/192], loss=98.4767
	step [110/192], loss=115.7818
	step [111/192], loss=105.7485
	step [112/192], loss=122.8246
	step [113/192], loss=116.9183
	step [114/192], loss=101.8865
	step [115/192], loss=113.4948
	step [116/192], loss=100.0042
	step [117/192], loss=98.7070
	step [118/192], loss=100.7886
	step [119/192], loss=96.7734
	step [120/192], loss=106.2531
	step [121/192], loss=103.0152
	step [122/192], loss=98.2977
	step [123/192], loss=105.2485
	step [124/192], loss=101.9924
	step [125/192], loss=98.8924
	step [126/192], loss=110.9550
	step [127/192], loss=95.2051
	step [128/192], loss=96.5117
	step [129/192], loss=102.3463
	step [130/192], loss=96.1827
	step [131/192], loss=101.5953
	step [132/192], loss=96.9827
	step [133/192], loss=100.7565
	step [134/192], loss=91.6655
	step [135/192], loss=100.5280
	step [136/192], loss=90.8953
	step [137/192], loss=104.0448
	step [138/192], loss=102.5794
	step [139/192], loss=98.4709
	step [140/192], loss=107.0446
	step [141/192], loss=96.2928
	step [142/192], loss=102.7176
	step [143/192], loss=85.6714
	step [144/192], loss=80.4550
	step [145/192], loss=114.0739
	step [146/192], loss=101.3201
	step [147/192], loss=97.8472
	step [148/192], loss=102.2181
	step [149/192], loss=109.4954
	step [150/192], loss=94.7135
	step [151/192], loss=113.5013
	step [152/192], loss=112.6961
	step [153/192], loss=104.2515
	step [154/192], loss=97.1525
	step [155/192], loss=102.7894
	step [156/192], loss=91.4963
	step [157/192], loss=103.5355
	step [158/192], loss=98.2417
	step [159/192], loss=110.6258
	step [160/192], loss=109.1935
	step [161/192], loss=102.8230
	step [162/192], loss=121.0511
	step [163/192], loss=112.1186
	step [164/192], loss=112.7201
	step [165/192], loss=92.9775
	step [166/192], loss=85.4294
	step [167/192], loss=105.8113
	step [168/192], loss=82.2275
	step [169/192], loss=104.9911
	step [170/192], loss=104.7948
	step [171/192], loss=107.9538
	step [172/192], loss=94.8819
	step [173/192], loss=96.8579
	step [174/192], loss=130.6327
	step [175/192], loss=87.8776
	step [176/192], loss=99.3253
	step [177/192], loss=95.2363
	step [178/192], loss=103.6008
	step [179/192], loss=119.0381
	step [180/192], loss=105.4094
	step [181/192], loss=96.6111
	step [182/192], loss=104.9269
	step [183/192], loss=103.0596
	step [184/192], loss=102.0615
	step [185/192], loss=97.3263
	step [186/192], loss=108.0514
	step [187/192], loss=103.8398
	step [188/192], loss=95.1437
	step [189/192], loss=102.2644
	step [190/192], loss=106.3299
	step [191/192], loss=95.5713
	step [192/192], loss=10.4349
	Evaluating
	loss=0.0295, precision=0.3823, recall=0.9340, f1=0.5425
saving model as: 0_saved_model.pth
Training epoch 14
	step [1/192], loss=113.5007
	step [2/192], loss=101.0028
	step [3/192], loss=112.0837
	step [4/192], loss=99.6834
	step [5/192], loss=95.4766
	step [6/192], loss=103.8017
	step [7/192], loss=94.8530
	step [8/192], loss=101.7434
	step [9/192], loss=85.8266
	step [10/192], loss=87.4452
	step [11/192], loss=87.8841
	step [12/192], loss=104.4833
	step [13/192], loss=100.9018
	step [14/192], loss=115.3508
	step [15/192], loss=99.6577
	step [16/192], loss=107.2290
	step [17/192], loss=97.3008
	step [18/192], loss=105.0409
	step [19/192], loss=112.2889
	step [20/192], loss=99.1177
	step [21/192], loss=91.5286
	step [22/192], loss=93.4656
	step [23/192], loss=109.8945
	step [24/192], loss=94.8817
	step [25/192], loss=93.6315
	step [26/192], loss=103.4604
	step [27/192], loss=93.6850
	step [28/192], loss=103.6693
	step [29/192], loss=92.1837
	step [30/192], loss=87.1275
	step [31/192], loss=104.6616
	step [32/192], loss=99.9873
	step [33/192], loss=94.2072
	step [34/192], loss=98.2850
	step [35/192], loss=102.6464
	step [36/192], loss=100.3663
	step [37/192], loss=100.7145
	step [38/192], loss=112.3677
	step [39/192], loss=106.1964
	step [40/192], loss=106.2490
	step [41/192], loss=97.9563
	step [42/192], loss=106.8244
	step [43/192], loss=101.6090
	step [44/192], loss=95.9206
	step [45/192], loss=97.7701
	step [46/192], loss=85.4787
	step [47/192], loss=105.0041
	step [48/192], loss=109.5680
	step [49/192], loss=110.5787
	step [50/192], loss=102.5716
	step [51/192], loss=95.0184
	step [52/192], loss=116.3210
	step [53/192], loss=101.0667
	step [54/192], loss=101.1056
	step [55/192], loss=93.4167
	step [56/192], loss=102.0574
	step [57/192], loss=88.8409
	step [58/192], loss=90.4233
	step [59/192], loss=103.6417
	step [60/192], loss=102.9594
	step [61/192], loss=100.3699
	step [62/192], loss=106.1741
	step [63/192], loss=80.2326
	step [64/192], loss=103.3042
	step [65/192], loss=85.4668
	step [66/192], loss=87.9694
	step [67/192], loss=94.7670
	step [68/192], loss=95.2038
	step [69/192], loss=90.2038
	step [70/192], loss=105.3204
	step [71/192], loss=99.2727
	step [72/192], loss=106.6352
	step [73/192], loss=98.8437
	step [74/192], loss=110.8322
	step [75/192], loss=97.9138
	step [76/192], loss=114.5998
	step [77/192], loss=98.7460
	step [78/192], loss=82.9142
	step [79/192], loss=97.4593
	step [80/192], loss=97.6375
	step [81/192], loss=83.1342
	step [82/192], loss=95.2981
	step [83/192], loss=100.8997
	step [84/192], loss=96.7145
	step [85/192], loss=99.7867
	step [86/192], loss=113.2935
	step [87/192], loss=97.6606
	step [88/192], loss=93.5201
	step [89/192], loss=87.8061
	step [90/192], loss=94.9694
	step [91/192], loss=104.2372
	step [92/192], loss=94.8107
	step [93/192], loss=109.1876
	step [94/192], loss=106.5650
	step [95/192], loss=101.5170
	step [96/192], loss=106.9214
	step [97/192], loss=96.6647
	step [98/192], loss=84.6083
	step [99/192], loss=92.7226
	step [100/192], loss=103.9505
	step [101/192], loss=92.4085
	step [102/192], loss=117.8581
	step [103/192], loss=95.2914
	step [104/192], loss=112.7654
	step [105/192], loss=95.3494
	step [106/192], loss=107.7169
	step [107/192], loss=94.8609
	step [108/192], loss=97.4802
	step [109/192], loss=116.7498
	step [110/192], loss=91.8018
	step [111/192], loss=116.4432
	step [112/192], loss=107.5893
	step [113/192], loss=89.9086
	step [114/192], loss=107.1992
	step [115/192], loss=97.6501
	step [116/192], loss=90.4805
	step [117/192], loss=104.0784
	step [118/192], loss=99.7034
	step [119/192], loss=102.8544
	step [120/192], loss=91.9173
	step [121/192], loss=84.4997
	step [122/192], loss=100.3381
	step [123/192], loss=93.7824
	step [124/192], loss=96.1941
	step [125/192], loss=100.6084
	step [126/192], loss=112.6248
	step [127/192], loss=92.9413
	step [128/192], loss=106.6037
	step [129/192], loss=114.2236
	step [130/192], loss=101.3827
	step [131/192], loss=94.8272
	step [132/192], loss=104.1572
	step [133/192], loss=110.7726
	step [134/192], loss=95.6850
	step [135/192], loss=115.2196
	step [136/192], loss=76.3371
	step [137/192], loss=103.1832
	step [138/192], loss=108.3390
	step [139/192], loss=106.1000
	step [140/192], loss=90.1924
	step [141/192], loss=90.8842
	step [142/192], loss=102.0847
	step [143/192], loss=90.5108
	step [144/192], loss=92.8094
	step [145/192], loss=114.3480
	step [146/192], loss=100.5337
	step [147/192], loss=100.8765
	step [148/192], loss=101.8906
	step [149/192], loss=106.3358
	step [150/192], loss=114.5337
	step [151/192], loss=111.4014
	step [152/192], loss=114.8945
	step [153/192], loss=117.2482
	step [154/192], loss=97.9605
	step [155/192], loss=106.6124
	step [156/192], loss=105.4350
	step [157/192], loss=106.4204
	step [158/192], loss=103.1819
	step [159/192], loss=108.8203
	step [160/192], loss=94.6726
	step [161/192], loss=104.4559
	step [162/192], loss=100.1060
	step [163/192], loss=114.6702
	step [164/192], loss=99.8621
	step [165/192], loss=109.5493
	step [166/192], loss=106.3506
	step [167/192], loss=97.7913
	step [168/192], loss=102.8790
	step [169/192], loss=102.5161
	step [170/192], loss=113.4243
	step [171/192], loss=108.7475
	step [172/192], loss=116.8445
	step [173/192], loss=94.0223
	step [174/192], loss=92.7416
	step [175/192], loss=107.8638
	step [176/192], loss=120.5578
	step [177/192], loss=97.7687
	step [178/192], loss=94.7334
	step [179/192], loss=102.6203
	step [180/192], loss=110.3613
	step [181/192], loss=103.2299
	step [182/192], loss=124.6379
	step [183/192], loss=102.6491
	step [184/192], loss=102.7828
	step [185/192], loss=84.0891
	step [186/192], loss=108.0999
	step [187/192], loss=122.4575
	step [188/192], loss=95.8972
	step [189/192], loss=94.6735
	step [190/192], loss=107.4685
	step [191/192], loss=99.1162
	step [192/192], loss=13.3625
	Evaluating
	loss=0.0267, precision=0.3720, recall=0.9227, f1=0.5303
Training epoch 15
	step [1/192], loss=83.1190
	step [2/192], loss=121.5360
	step [3/192], loss=117.0375
	step [4/192], loss=93.1588
	step [5/192], loss=88.6259
	step [6/192], loss=116.0212
	step [7/192], loss=95.0044
	step [8/192], loss=111.1716
	step [9/192], loss=99.3812
	step [10/192], loss=113.0998
	step [11/192], loss=81.7150
	step [12/192], loss=104.9271
	step [13/192], loss=103.9012
	step [14/192], loss=100.7693
	step [15/192], loss=110.1872
	step [16/192], loss=84.8580
	step [17/192], loss=117.6144
	step [18/192], loss=90.6010
	step [19/192], loss=97.2534
	step [20/192], loss=93.9079
	step [21/192], loss=89.6182
	step [22/192], loss=105.9931
	step [23/192], loss=100.2399
	step [24/192], loss=95.9071
	step [25/192], loss=90.9231
	step [26/192], loss=103.1096
	step [27/192], loss=101.6287
	step [28/192], loss=89.9880
	step [29/192], loss=104.9164
	step [30/192], loss=118.5794
	step [31/192], loss=96.9853
	step [32/192], loss=105.9682
	step [33/192], loss=103.2490
	step [34/192], loss=90.4346
	step [35/192], loss=99.7849
	step [36/192], loss=99.8721
	step [37/192], loss=96.2643
	step [38/192], loss=110.8134
	step [39/192], loss=99.3572
	step [40/192], loss=100.8576
	step [41/192], loss=94.4140
	step [42/192], loss=101.2677
	step [43/192], loss=112.9913
	step [44/192], loss=105.4282
	step [45/192], loss=102.7354
	step [46/192], loss=83.8274
	step [47/192], loss=99.5321
	step [48/192], loss=97.9048
	step [49/192], loss=98.3729
	step [50/192], loss=99.4506
	step [51/192], loss=91.6315
	step [52/192], loss=102.0013
	step [53/192], loss=103.2737
	step [54/192], loss=91.5814
	step [55/192], loss=100.2056
	step [56/192], loss=97.8083
	step [57/192], loss=111.3728
	step [58/192], loss=104.9975
	step [59/192], loss=105.0832
	step [60/192], loss=98.8765
	step [61/192], loss=89.2904
	step [62/192], loss=93.5008
	step [63/192], loss=106.2046
	step [64/192], loss=96.1521
	step [65/192], loss=107.0540
	step [66/192], loss=101.7624
	step [67/192], loss=92.7638
	step [68/192], loss=109.2081
	step [69/192], loss=89.6940
	step [70/192], loss=97.5092
	step [71/192], loss=119.0907
	step [72/192], loss=106.9792
	step [73/192], loss=101.2526
	step [74/192], loss=125.2203
	step [75/192], loss=108.4540
	step [76/192], loss=117.5084
	step [77/192], loss=107.3332
	step [78/192], loss=103.5418
	step [79/192], loss=78.0562
	step [80/192], loss=115.0962
	step [81/192], loss=99.8713
	step [82/192], loss=110.4872
	step [83/192], loss=99.6202
	step [84/192], loss=96.9167
	step [85/192], loss=96.1592
	step [86/192], loss=104.4417
	step [87/192], loss=92.7240
	step [88/192], loss=91.9433
	step [89/192], loss=85.6500
	step [90/192], loss=103.3923
	step [91/192], loss=110.4392
	step [92/192], loss=102.1804
	step [93/192], loss=96.2330
	step [94/192], loss=94.4532
	step [95/192], loss=99.4861
	step [96/192], loss=104.3627
	step [97/192], loss=94.2039
	step [98/192], loss=102.9820
	step [99/192], loss=89.2883
	step [100/192], loss=101.9596
	step [101/192], loss=107.2492
	step [102/192], loss=85.8929
	step [103/192], loss=79.5954
	step [104/192], loss=103.6064
	step [105/192], loss=96.7851
	step [106/192], loss=93.2822
	step [107/192], loss=99.7601
	step [108/192], loss=88.7562
	step [109/192], loss=103.3285
	step [110/192], loss=97.1742
	step [111/192], loss=104.5020
	step [112/192], loss=105.1080
	step [113/192], loss=84.3402
	step [114/192], loss=112.0871
	step [115/192], loss=101.9444
	step [116/192], loss=87.5501
	step [117/192], loss=103.6642
	step [118/192], loss=87.6797
	step [119/192], loss=109.0275
	step [120/192], loss=102.5977
	step [121/192], loss=94.2614
	step [122/192], loss=102.5360
	step [123/192], loss=104.0188
	step [124/192], loss=100.0081
	step [125/192], loss=100.4672
	step [126/192], loss=108.3271
	step [127/192], loss=91.7861
	step [128/192], loss=95.4282
	step [129/192], loss=105.9741
	step [130/192], loss=107.6333
	step [131/192], loss=99.7377
	step [132/192], loss=107.0564
	step [133/192], loss=96.3218
	step [134/192], loss=98.8338
	step [135/192], loss=106.6793
	step [136/192], loss=109.2919
	step [137/192], loss=108.8101
	step [138/192], loss=86.6830
	step [139/192], loss=90.8300
	step [140/192], loss=90.8273
	step [141/192], loss=93.2646
	step [142/192], loss=96.1489
	step [143/192], loss=104.2098
	step [144/192], loss=87.1203
	step [145/192], loss=104.7463
	step [146/192], loss=98.6787
	step [147/192], loss=107.9189
	step [148/192], loss=102.8879
	step [149/192], loss=101.8170
	step [150/192], loss=91.5523
	step [151/192], loss=93.6848
	step [152/192], loss=113.1973
	step [153/192], loss=103.7956
	step [154/192], loss=112.1838
	step [155/192], loss=103.4699
	step [156/192], loss=91.4975
	step [157/192], loss=102.6362
	step [158/192], loss=97.9147
	step [159/192], loss=84.4356
	step [160/192], loss=96.8905
	step [161/192], loss=100.1950
	step [162/192], loss=88.6495
	step [163/192], loss=92.8295
	step [164/192], loss=97.4946
	step [165/192], loss=100.2157
	step [166/192], loss=97.6495
	step [167/192], loss=90.5991
	step [168/192], loss=101.6134
	step [169/192], loss=110.5344
	step [170/192], loss=87.8120
	step [171/192], loss=94.2404
	step [172/192], loss=92.3633
	step [173/192], loss=97.6556
	step [174/192], loss=90.3665
	step [175/192], loss=83.7071
	step [176/192], loss=95.2419
	step [177/192], loss=104.3894
	step [178/192], loss=96.2911
	step [179/192], loss=94.2848
	step [180/192], loss=95.9325
	step [181/192], loss=99.4834
	step [182/192], loss=107.9844
	step [183/192], loss=97.6244
	step [184/192], loss=94.4767
	step [185/192], loss=84.1800
	step [186/192], loss=97.4511
	step [187/192], loss=101.1580
	step [188/192], loss=100.7966
	step [189/192], loss=107.1850
	step [190/192], loss=101.9356
	step [191/192], loss=99.1317
	step [192/192], loss=11.1066
	Evaluating
	loss=0.0254, precision=0.3784, recall=0.9071, f1=0.5340
Training epoch 16
	step [1/192], loss=108.4717
	step [2/192], loss=85.7238
	step [3/192], loss=109.7481
	step [4/192], loss=90.7420
	step [5/192], loss=96.5480
	step [6/192], loss=95.7498
	step [7/192], loss=99.2597
	step [8/192], loss=96.3951
	step [9/192], loss=94.3173
	step [10/192], loss=102.5885
	step [11/192], loss=91.2121
	step [12/192], loss=101.9523
	step [13/192], loss=92.3553
	step [14/192], loss=93.5275
	step [15/192], loss=103.7597
	step [16/192], loss=90.6348
	step [17/192], loss=104.2486
	step [18/192], loss=84.7649
	step [19/192], loss=103.9026
	step [20/192], loss=87.7467
	step [21/192], loss=91.8188
	step [22/192], loss=100.1986
	step [23/192], loss=98.2572
	step [24/192], loss=98.6586
	step [25/192], loss=106.6138
	step [26/192], loss=87.2677
	step [27/192], loss=107.6815
	step [28/192], loss=95.1923
	step [29/192], loss=87.0950
	step [30/192], loss=105.7016
	step [31/192], loss=96.8660
	step [32/192], loss=99.0003
	step [33/192], loss=95.6094
	step [34/192], loss=102.8893
	step [35/192], loss=87.6737
	step [36/192], loss=109.9087
	step [37/192], loss=111.7436
	step [38/192], loss=91.5993
	step [39/192], loss=104.5268
	step [40/192], loss=95.7619
	step [41/192], loss=87.5076
	step [42/192], loss=111.7274
	step [43/192], loss=102.7425
	step [44/192], loss=99.3535
	step [45/192], loss=94.9746
	step [46/192], loss=101.9840
	step [47/192], loss=97.0337
	step [48/192], loss=105.0621
	step [49/192], loss=97.1978
	step [50/192], loss=96.7645
	step [51/192], loss=83.4475
	step [52/192], loss=102.9105
	step [53/192], loss=95.1436
	step [54/192], loss=92.9131
	step [55/192], loss=91.3778
	step [56/192], loss=93.4904
	step [57/192], loss=89.4313
	step [58/192], loss=114.5111
	step [59/192], loss=90.1164
	step [60/192], loss=92.4704
	step [61/192], loss=96.1786
	step [62/192], loss=89.5867
	step [63/192], loss=92.9807
	step [64/192], loss=84.4951
	step [65/192], loss=114.2198
	step [66/192], loss=115.5730
	step [67/192], loss=113.3379
	step [68/192], loss=107.6387
	step [69/192], loss=97.2218
	step [70/192], loss=126.0856
	step [71/192], loss=112.0762
	step [72/192], loss=89.4784
	step [73/192], loss=91.9547
	step [74/192], loss=91.9215
	step [75/192], loss=107.7856
	step [76/192], loss=111.8508
	step [77/192], loss=90.3160
	step [78/192], loss=106.8995
	step [79/192], loss=104.7713
	step [80/192], loss=101.1916
	step [81/192], loss=106.3017
	step [82/192], loss=85.5186
	step [83/192], loss=100.9967
	step [84/192], loss=85.8311
	step [85/192], loss=90.3789
	step [86/192], loss=92.7475
	step [87/192], loss=88.9432
	step [88/192], loss=98.9853
	step [89/192], loss=100.0864
	step [90/192], loss=108.5382
	step [91/192], loss=87.2181
	step [92/192], loss=108.4790
	step [93/192], loss=95.1479
	step [94/192], loss=101.4363
	step [95/192], loss=91.5891
	step [96/192], loss=103.1924
	step [97/192], loss=100.5585
	step [98/192], loss=96.3196
	step [99/192], loss=85.0125
	step [100/192], loss=104.5228
	step [101/192], loss=100.6574
	step [102/192], loss=106.3509
	step [103/192], loss=91.2372
	step [104/192], loss=102.2052
	step [105/192], loss=92.7220
	step [106/192], loss=97.3157
	step [107/192], loss=102.2356
	step [108/192], loss=90.8247
	step [109/192], loss=96.6543
	step [110/192], loss=91.5477
	step [111/192], loss=99.8297
	step [112/192], loss=107.6253
	step [113/192], loss=103.8391
	step [114/192], loss=84.1979
	step [115/192], loss=119.6221
	step [116/192], loss=101.7460
	step [117/192], loss=98.0440
	step [118/192], loss=94.5188
	step [119/192], loss=103.4003
	step [120/192], loss=94.8931
	step [121/192], loss=98.0514
	step [122/192], loss=76.3513
	step [123/192], loss=92.5374
	step [124/192], loss=113.7544
	step [125/192], loss=98.7705
	step [126/192], loss=110.0493
	step [127/192], loss=80.0754
	step [128/192], loss=103.3030
	step [129/192], loss=99.3991
	step [130/192], loss=99.5858
	step [131/192], loss=85.7299
	step [132/192], loss=113.3216
	step [133/192], loss=111.9975
	step [134/192], loss=92.5101
	step [135/192], loss=108.3704
	step [136/192], loss=103.8293
	step [137/192], loss=97.8466
	step [138/192], loss=95.7902
	step [139/192], loss=95.6986
	step [140/192], loss=97.7535
	step [141/192], loss=88.5419
	step [142/192], loss=96.9104
	step [143/192], loss=87.4848
	step [144/192], loss=93.9294
	step [145/192], loss=104.2078
	step [146/192], loss=96.7747
	step [147/192], loss=89.0979
	step [148/192], loss=94.7723
	step [149/192], loss=86.4545
	step [150/192], loss=83.3518
	step [151/192], loss=106.7561
	step [152/192], loss=86.9404
	step [153/192], loss=96.3461
	step [154/192], loss=88.3457
	step [155/192], loss=96.6211
	step [156/192], loss=108.1103
	step [157/192], loss=99.5893
	step [158/192], loss=111.3923
	step [159/192], loss=98.6712
	step [160/192], loss=106.8604
	step [161/192], loss=92.2345
	step [162/192], loss=99.7623
	step [163/192], loss=96.5875
	step [164/192], loss=97.2281
	step [165/192], loss=113.9695
	step [166/192], loss=89.7584
	step [167/192], loss=87.1154
	step [168/192], loss=93.5437
	step [169/192], loss=95.8021
	step [170/192], loss=97.7449
	step [171/192], loss=91.4688
	step [172/192], loss=88.2437
	step [173/192], loss=105.5737
	step [174/192], loss=94.9992
	step [175/192], loss=86.0162
	step [176/192], loss=89.3163
	step [177/192], loss=100.8839
	step [178/192], loss=93.8314
	step [179/192], loss=94.9163
	step [180/192], loss=100.1135
	step [181/192], loss=91.0779
	step [182/192], loss=94.1301
	step [183/192], loss=92.1738
	step [184/192], loss=111.7575
	step [185/192], loss=106.8997
	step [186/192], loss=98.3660
	step [187/192], loss=90.1937
	step [188/192], loss=84.6385
	step [189/192], loss=105.1833
	step [190/192], loss=86.5147
	step [191/192], loss=100.7173
	step [192/192], loss=16.7615
	Evaluating
	loss=0.0236, precision=0.3549, recall=0.9169, f1=0.5117
Training epoch 17
	step [1/192], loss=87.5312
	step [2/192], loss=102.9266
	step [3/192], loss=94.9429
	step [4/192], loss=80.2150
	step [5/192], loss=89.8820
	step [6/192], loss=112.3249
	step [7/192], loss=106.7019
	step [8/192], loss=88.6543
	step [9/192], loss=101.0597
	step [10/192], loss=90.4198
	step [11/192], loss=88.9498
	step [12/192], loss=116.0417
	step [13/192], loss=94.5230
	step [14/192], loss=85.9861
	step [15/192], loss=115.0851
	step [16/192], loss=102.3086
	step [17/192], loss=95.1051
	step [18/192], loss=97.0016
	step [19/192], loss=95.0790
	step [20/192], loss=98.5908
	step [21/192], loss=112.5739
	step [22/192], loss=102.0019
	step [23/192], loss=89.0890
	step [24/192], loss=118.8192
	step [25/192], loss=90.6675
	step [26/192], loss=85.4535
	step [27/192], loss=101.1640
	step [28/192], loss=95.7460
	step [29/192], loss=103.5007
	step [30/192], loss=94.3985
	step [31/192], loss=94.2532
	step [32/192], loss=88.9854
	step [33/192], loss=98.0510
	step [34/192], loss=98.2665
	step [35/192], loss=99.6270
	step [36/192], loss=99.1042
	step [37/192], loss=91.6870
	step [38/192], loss=98.5331
	step [39/192], loss=108.8926
	step [40/192], loss=96.1250
	step [41/192], loss=98.6111
	step [42/192], loss=96.0734
	step [43/192], loss=92.2430
	step [44/192], loss=86.3151
	step [45/192], loss=105.0908
	step [46/192], loss=81.4833
	step [47/192], loss=102.4908
	step [48/192], loss=108.7141
	step [49/192], loss=89.3158
	step [50/192], loss=77.9815
	step [51/192], loss=94.6567
	step [52/192], loss=102.8748
	step [53/192], loss=95.5436
	step [54/192], loss=102.2839
	step [55/192], loss=100.3925
	step [56/192], loss=95.3788
	step [57/192], loss=93.5843
	step [58/192], loss=78.8704
	step [59/192], loss=91.1219
	step [60/192], loss=117.2846
	step [61/192], loss=106.6641
	step [62/192], loss=104.6211
	step [63/192], loss=93.7374
	step [64/192], loss=88.5505
	step [65/192], loss=101.7531
	step [66/192], loss=95.7853
	step [67/192], loss=90.9641
	step [68/192], loss=91.0310
	step [69/192], loss=93.6758
	step [70/192], loss=94.9878
	step [71/192], loss=103.4459
	step [72/192], loss=92.5096
	step [73/192], loss=100.7521
	step [74/192], loss=98.8226
	step [75/192], loss=103.5172
	step [76/192], loss=88.7495
	step [77/192], loss=107.8681
	step [78/192], loss=109.2160
	step [79/192], loss=91.5895
	step [80/192], loss=94.0942
	step [81/192], loss=106.2117
	step [82/192], loss=87.3884
	step [83/192], loss=89.6784
	step [84/192], loss=100.2363
	step [85/192], loss=106.8955
	step [86/192], loss=106.7522
	step [87/192], loss=101.5384
	step [88/192], loss=98.4938
	step [89/192], loss=102.5185
	step [90/192], loss=103.6000
	step [91/192], loss=100.9565
	step [92/192], loss=107.2235
	step [93/192], loss=103.2251
	step [94/192], loss=97.1813
	step [95/192], loss=95.9822
	step [96/192], loss=84.4424
	step [97/192], loss=101.0845
	step [98/192], loss=92.6838
	step [99/192], loss=83.7430
	step [100/192], loss=98.3001
	step [101/192], loss=94.8051
	step [102/192], loss=87.6188
	step [103/192], loss=87.4827
	step [104/192], loss=91.7276
	step [105/192], loss=93.9014
	step [106/192], loss=102.6586
	step [107/192], loss=94.4908
	step [108/192], loss=88.1366
	step [109/192], loss=88.0778
	step [110/192], loss=106.3239
	step [111/192], loss=98.3049
	step [112/192], loss=86.6270
	step [113/192], loss=106.2920
	step [114/192], loss=109.5450
	step [115/192], loss=94.8975
	step [116/192], loss=101.6315
	step [117/192], loss=97.5272
	step [118/192], loss=91.5034
	step [119/192], loss=94.7189
	step [120/192], loss=98.2271
	step [121/192], loss=86.0423
	step [122/192], loss=91.6171
	step [123/192], loss=83.6743
	step [124/192], loss=84.7495
	step [125/192], loss=103.7425
	step [126/192], loss=86.5367
	step [127/192], loss=89.0456
	step [128/192], loss=87.7934
	step [129/192], loss=106.1118
	step [130/192], loss=98.8443
	step [131/192], loss=104.5768
	step [132/192], loss=101.8425
	step [133/192], loss=88.3575
	step [134/192], loss=90.5311
	step [135/192], loss=90.2806
	step [136/192], loss=94.4665
	step [137/192], loss=112.9940
	step [138/192], loss=101.6498
	step [139/192], loss=107.3418
	step [140/192], loss=102.5652
	step [141/192], loss=109.1228
	step [142/192], loss=85.5556
	step [143/192], loss=85.8707
	step [144/192], loss=91.0356
	step [145/192], loss=110.6904
	step [146/192], loss=88.5334
	step [147/192], loss=100.0790
	step [148/192], loss=95.3604
	step [149/192], loss=94.3879
	step [150/192], loss=94.0994
	step [151/192], loss=100.5963
	step [152/192], loss=91.7468
	step [153/192], loss=87.6893
	step [154/192], loss=101.5060
	step [155/192], loss=91.3687
	step [156/192], loss=97.5791
	step [157/192], loss=93.9366
	step [158/192], loss=91.7065
	step [159/192], loss=91.7932
	step [160/192], loss=101.0565
	step [161/192], loss=79.9079
	step [162/192], loss=113.5087
	step [163/192], loss=108.7965
	step [164/192], loss=97.2209
	step [165/192], loss=92.4441
	step [166/192], loss=88.5954
	step [167/192], loss=110.2558
	step [168/192], loss=89.4742
	step [169/192], loss=96.3186
	step [170/192], loss=83.5139
	step [171/192], loss=109.2164
	step [172/192], loss=93.5651
	step [173/192], loss=83.5301
	step [174/192], loss=91.3975
	step [175/192], loss=97.0710
	step [176/192], loss=96.6813
	step [177/192], loss=87.9073
	step [178/192], loss=85.0337
	step [179/192], loss=87.9261
	step [180/192], loss=91.1474
	step [181/192], loss=89.0993
	step [182/192], loss=111.3348
	step [183/192], loss=111.9988
	step [184/192], loss=83.6289
	step [185/192], loss=97.8684
	step [186/192], loss=90.7111
	step [187/192], loss=109.4281
	step [188/192], loss=90.3011
	step [189/192], loss=97.0026
	step [190/192], loss=96.6681
	step [191/192], loss=93.0070
	step [192/192], loss=12.6715
	Evaluating
	loss=0.0237, precision=0.3239, recall=0.9108, f1=0.4778
Training epoch 18
	step [1/192], loss=95.7995
	step [2/192], loss=90.3391
	step [3/192], loss=94.9309
	step [4/192], loss=98.1048
	step [5/192], loss=103.6935
	step [6/192], loss=98.3436
	step [7/192], loss=87.1384
	step [8/192], loss=96.9619
	step [9/192], loss=101.9238
	step [10/192], loss=98.1060
	step [11/192], loss=106.7908
	step [12/192], loss=100.0108
	step [13/192], loss=83.5242
	step [14/192], loss=97.2556
	step [15/192], loss=92.8233
	step [16/192], loss=103.6219
	step [17/192], loss=93.2099
	step [18/192], loss=88.3451
	step [19/192], loss=76.8716
	step [20/192], loss=107.9554
	step [21/192], loss=91.7822
	step [22/192], loss=101.3903
	step [23/192], loss=101.1207
	step [24/192], loss=99.2936
	step [25/192], loss=92.7880
	step [26/192], loss=102.0734
	step [27/192], loss=88.4055
	step [28/192], loss=85.3497
	step [29/192], loss=87.4799
	step [30/192], loss=85.2247
	step [31/192], loss=85.3620
	step [32/192], loss=102.7539
	step [33/192], loss=100.3608
	step [34/192], loss=84.8169
	step [35/192], loss=96.2963
	step [36/192], loss=103.7671
	step [37/192], loss=107.6329
	step [38/192], loss=101.7188
	step [39/192], loss=92.0980
	step [40/192], loss=91.2071
	step [41/192], loss=94.3064
	step [42/192], loss=91.9239
	step [43/192], loss=93.3337
	step [44/192], loss=87.2844
	step [45/192], loss=87.7740
	step [46/192], loss=87.4246
	step [47/192], loss=108.5461
	step [48/192], loss=94.8343
	step [49/192], loss=85.7800
	step [50/192], loss=92.4032
	step [51/192], loss=83.9008
	step [52/192], loss=93.7787
	step [53/192], loss=94.7231
	step [54/192], loss=110.0764
	step [55/192], loss=114.7804
	step [56/192], loss=76.4226
	step [57/192], loss=91.8185
	step [58/192], loss=79.9427
	step [59/192], loss=100.8620
	step [60/192], loss=94.5101
	step [61/192], loss=110.3214
	step [62/192], loss=102.2665
	step [63/192], loss=97.7878
	step [64/192], loss=102.7949
	step [65/192], loss=102.6311
	step [66/192], loss=86.7232
	step [67/192], loss=81.3390
	step [68/192], loss=103.8185
	step [69/192], loss=97.9300
	step [70/192], loss=78.5698
	step [71/192], loss=97.8650
	step [72/192], loss=88.3705
	step [73/192], loss=104.3580
	step [74/192], loss=85.0739
	step [75/192], loss=88.0264
	step [76/192], loss=103.5594
	step [77/192], loss=103.0908
	step [78/192], loss=89.2757
	step [79/192], loss=87.7869
	step [80/192], loss=106.9652
	step [81/192], loss=89.3091
	step [82/192], loss=84.1090
	step [83/192], loss=91.8657
	step [84/192], loss=101.6854
	step [85/192], loss=90.5233
	step [86/192], loss=97.8883
	step [87/192], loss=92.6556
	step [88/192], loss=89.3559
	step [89/192], loss=86.8761
	step [90/192], loss=104.0184
	step [91/192], loss=101.6038
	step [92/192], loss=93.0768
	step [93/192], loss=96.0753
	step [94/192], loss=93.0695
	step [95/192], loss=90.1636
	step [96/192], loss=100.0801
	step [97/192], loss=93.1454
	step [98/192], loss=99.3743
	step [99/192], loss=92.1761
	step [100/192], loss=96.4519
	step [101/192], loss=101.9445
	step [102/192], loss=90.0185
	step [103/192], loss=101.0698
	step [104/192], loss=85.7447
	step [105/192], loss=106.1030
	step [106/192], loss=79.6091
	step [107/192], loss=106.2817
	step [108/192], loss=82.3739
	step [109/192], loss=97.7564
	step [110/192], loss=102.0887
	step [111/192], loss=101.7188
	step [112/192], loss=82.2534
	step [113/192], loss=99.4807
	step [114/192], loss=106.2945
	step [115/192], loss=105.4390
	step [116/192], loss=102.6529
	step [117/192], loss=94.8384
	step [118/192], loss=89.0523
	step [119/192], loss=84.9896
	step [120/192], loss=89.5411
	step [121/192], loss=106.4663
	step [122/192], loss=95.7247
	step [123/192], loss=105.5086
	step [124/192], loss=75.9870
	step [125/192], loss=93.3756
	step [126/192], loss=70.7925
	step [127/192], loss=93.6799
	step [128/192], loss=91.5580
	step [129/192], loss=86.7276
	step [130/192], loss=104.6114
	step [131/192], loss=96.9857
	step [132/192], loss=99.8320
	step [133/192], loss=86.5306
	step [134/192], loss=84.8092
	step [135/192], loss=85.3535
	step [136/192], loss=84.8315
	step [137/192], loss=93.5461
	step [138/192], loss=98.1254
	step [139/192], loss=87.0547
	step [140/192], loss=80.8161
	step [141/192], loss=92.8512
	step [142/192], loss=80.5653
	step [143/192], loss=88.6537
	step [144/192], loss=95.1849
	step [145/192], loss=111.0817
	step [146/192], loss=99.2876
	step [147/192], loss=84.7690
	step [148/192], loss=92.5247
	step [149/192], loss=95.6865
	step [150/192], loss=81.0908
	step [151/192], loss=83.4414
	step [152/192], loss=93.3318
	step [153/192], loss=92.6520
	step [154/192], loss=92.6965
	step [155/192], loss=88.8025
	step [156/192], loss=104.1376
	step [157/192], loss=99.9354
	step [158/192], loss=85.9420
	step [159/192], loss=100.8379
	step [160/192], loss=93.2743
	step [161/192], loss=103.7955
	step [162/192], loss=105.8064
	step [163/192], loss=78.2923
	step [164/192], loss=99.3981
	step [165/192], loss=101.2092
	step [166/192], loss=92.5371
	step [167/192], loss=90.3030
	step [168/192], loss=90.6540
	step [169/192], loss=106.3264
	step [170/192], loss=89.7006
	step [171/192], loss=80.2010
	step [172/192], loss=98.2397
	step [173/192], loss=97.2563
	step [174/192], loss=103.9450
	step [175/192], loss=103.1873
	step [176/192], loss=86.4194
	step [177/192], loss=95.2163
	step [178/192], loss=104.9144
	step [179/192], loss=81.1762
	step [180/192], loss=104.8395
	step [181/192], loss=96.1283
	step [182/192], loss=99.3913
	step [183/192], loss=96.6903
	step [184/192], loss=97.3627
	step [185/192], loss=100.3417
	step [186/192], loss=114.6270
	step [187/192], loss=92.8173
	step [188/192], loss=99.4866
	step [189/192], loss=95.4866
	step [190/192], loss=110.4685
	step [191/192], loss=85.3827
	step [192/192], loss=10.5714
	Evaluating
	loss=0.0217, precision=0.3419, recall=0.9078, f1=0.4967
Training epoch 19
	step [1/192], loss=106.9702
	step [2/192], loss=106.9893
	step [3/192], loss=95.9953
	step [4/192], loss=92.6338
	step [5/192], loss=100.0626
	step [6/192], loss=92.3644
	step [7/192], loss=105.8732
	step [8/192], loss=96.9260
	step [9/192], loss=98.6261
	step [10/192], loss=82.8088
	step [11/192], loss=101.1469
	step [12/192], loss=99.1697
	step [13/192], loss=80.5628
	step [14/192], loss=95.0410
	step [15/192], loss=78.4036
	step [16/192], loss=103.0315
	step [17/192], loss=90.2794
	step [18/192], loss=86.1024
	step [19/192], loss=94.1617
	step [20/192], loss=98.3713
	step [21/192], loss=100.2359
	step [22/192], loss=101.8769
	step [23/192], loss=94.3714
	step [24/192], loss=87.8045
	step [25/192], loss=95.1328
	step [26/192], loss=96.6974
	step [27/192], loss=94.1700
	step [28/192], loss=91.7775
	step [29/192], loss=102.6987
	step [30/192], loss=105.5865
	step [31/192], loss=95.6745
	step [32/192], loss=104.8875
	step [33/192], loss=83.1151
	step [34/192], loss=86.6268
	step [35/192], loss=91.8290
	step [36/192], loss=79.0371
	step [37/192], loss=92.6940
	step [38/192], loss=86.8528
	step [39/192], loss=90.1586
	step [40/192], loss=84.3003
	step [41/192], loss=97.0873
	step [42/192], loss=93.5014
	step [43/192], loss=91.4008
	step [44/192], loss=90.9844
	step [45/192], loss=105.2223
	step [46/192], loss=94.3531
	step [47/192], loss=72.3211
	step [48/192], loss=92.2972
	step [49/192], loss=101.1445
	step [50/192], loss=86.5325
	step [51/192], loss=101.2503
	step [52/192], loss=96.9468
	step [53/192], loss=86.1492
	step [54/192], loss=96.3669
	step [55/192], loss=67.8779
	step [56/192], loss=78.4697
	step [57/192], loss=99.4678
	step [58/192], loss=96.5828
	step [59/192], loss=107.6831
	step [60/192], loss=102.6062
	step [61/192], loss=98.1501
	step [62/192], loss=101.0734
	step [63/192], loss=82.7757
	step [64/192], loss=69.8856
	step [65/192], loss=75.4132
	step [66/192], loss=81.8382
	step [67/192], loss=103.1263
	step [68/192], loss=95.2925
	step [69/192], loss=83.3007
	step [70/192], loss=98.2637
	step [71/192], loss=100.3574
	step [72/192], loss=97.2759
	step [73/192], loss=84.4972
	step [74/192], loss=83.6036
	step [75/192], loss=108.9422
	step [76/192], loss=93.0817
	step [77/192], loss=91.0507
	step [78/192], loss=87.5992
	step [79/192], loss=94.2603
	step [80/192], loss=93.4913
	step [81/192], loss=99.2047
	step [82/192], loss=91.2801
	step [83/192], loss=81.6713
	step [84/192], loss=103.3880
	step [85/192], loss=104.8022
	step [86/192], loss=78.9128
	step [87/192], loss=83.4828
	step [88/192], loss=95.8822
	step [89/192], loss=104.1929
	step [90/192], loss=98.4338
	step [91/192], loss=90.2243
	step [92/192], loss=95.0524
	step [93/192], loss=97.4956
	step [94/192], loss=107.1465
	step [95/192], loss=96.7485
	step [96/192], loss=107.1163
	step [97/192], loss=102.1473
	step [98/192], loss=97.7649
	step [99/192], loss=94.5828
	step [100/192], loss=88.4297
	step [101/192], loss=97.2033
	step [102/192], loss=90.7762
	step [103/192], loss=91.0649
	step [104/192], loss=87.6014
	step [105/192], loss=101.0488
	step [106/192], loss=78.7685
	step [107/192], loss=86.6837
	step [108/192], loss=83.9543
	step [109/192], loss=102.6155
	step [110/192], loss=82.1863
	step [111/192], loss=99.0253
	step [112/192], loss=93.6441
	step [113/192], loss=90.1126
	step [114/192], loss=85.5673
	step [115/192], loss=107.2502
	step [116/192], loss=91.0909
	step [117/192], loss=98.3180
	step [118/192], loss=102.1558
	step [119/192], loss=77.1804
	step [120/192], loss=87.0403
	step [121/192], loss=89.5505
	step [122/192], loss=89.6685
	step [123/192], loss=87.3380
	step [124/192], loss=87.8148
	step [125/192], loss=90.0132
	step [126/192], loss=103.9814
	step [127/192], loss=90.7639
	step [128/192], loss=93.7099
	step [129/192], loss=85.5266
	step [130/192], loss=102.4727
	step [131/192], loss=80.2462
	step [132/192], loss=91.7203
	step [133/192], loss=105.4131
	step [134/192], loss=98.6236
	step [135/192], loss=90.5601
	step [136/192], loss=97.8621
	step [137/192], loss=77.9484
	step [138/192], loss=104.5384
	step [139/192], loss=111.3477
	step [140/192], loss=87.6391
	step [141/192], loss=89.8906
	step [142/192], loss=91.6086
	step [143/192], loss=102.9109
	step [144/192], loss=85.1385
	step [145/192], loss=106.0205
	step [146/192], loss=97.2088
	step [147/192], loss=97.2253
	step [148/192], loss=96.3817
	step [149/192], loss=105.6057
	step [150/192], loss=105.5322
	step [151/192], loss=100.1666
	step [152/192], loss=99.3480
	step [153/192], loss=89.1614
	step [154/192], loss=84.2264
	step [155/192], loss=89.6520
	step [156/192], loss=100.8746
	step [157/192], loss=100.7474
	step [158/192], loss=87.2858
	step [159/192], loss=86.1102
	step [160/192], loss=87.0184
	step [161/192], loss=99.9240
	step [162/192], loss=81.8873
	step [163/192], loss=89.6374
	step [164/192], loss=87.9300
	step [165/192], loss=79.6615
	step [166/192], loss=91.9264
	step [167/192], loss=86.5694
	step [168/192], loss=101.1782
	step [169/192], loss=95.6790
	step [170/192], loss=88.8207
	step [171/192], loss=90.6495
	step [172/192], loss=83.1730
	step [173/192], loss=94.7404
	step [174/192], loss=101.8571
	step [175/192], loss=93.5258
	step [176/192], loss=87.2123
	step [177/192], loss=89.2471
	step [178/192], loss=88.2352
	step [179/192], loss=101.9688
	step [180/192], loss=98.8251
	step [181/192], loss=81.7789
	step [182/192], loss=91.5349
	step [183/192], loss=96.1830
	step [184/192], loss=101.4898
	step [185/192], loss=89.4970
	step [186/192], loss=102.3149
	step [187/192], loss=88.7734
	step [188/192], loss=76.5275
	step [189/192], loss=86.7870
	step [190/192], loss=99.9644
	step [191/192], loss=97.8028
	step [192/192], loss=11.2054
	Evaluating
	loss=0.0188, precision=0.3460, recall=0.9153, f1=0.5022
Training epoch 20
	step [1/192], loss=88.1280
	step [2/192], loss=86.0735
	step [3/192], loss=97.5300
	step [4/192], loss=81.3913
	step [5/192], loss=81.4062
	step [6/192], loss=111.2360
	step [7/192], loss=90.5450
	step [8/192], loss=91.5666
	step [9/192], loss=89.3833
	step [10/192], loss=97.4590
	step [11/192], loss=99.8203
	step [12/192], loss=94.7914
	step [13/192], loss=82.9915
	step [14/192], loss=93.5510
	step [15/192], loss=83.3741
	step [16/192], loss=109.8123
	step [17/192], loss=80.4742
	step [18/192], loss=88.1992
	step [19/192], loss=101.3517
	step [20/192], loss=93.3158
	step [21/192], loss=94.9025
	step [22/192], loss=90.2770
	step [23/192], loss=99.5846
	step [24/192], loss=85.5273
	step [25/192], loss=96.8399
	step [26/192], loss=88.5174
	step [27/192], loss=92.4646
	step [28/192], loss=102.4329
	step [29/192], loss=86.7285
	step [30/192], loss=95.4969
	step [31/192], loss=84.9883
	step [32/192], loss=75.3888
	step [33/192], loss=83.9048
	step [34/192], loss=90.2747
	step [35/192], loss=97.0145
	step [36/192], loss=77.1759
	step [37/192], loss=80.4928
	step [38/192], loss=88.7729
	step [39/192], loss=90.2286
	step [40/192], loss=90.3145
	step [41/192], loss=93.0961
	step [42/192], loss=105.7120
	step [43/192], loss=97.1842
	step [44/192], loss=102.5715
	step [45/192], loss=95.2577
	step [46/192], loss=104.8612
	step [47/192], loss=106.2895
	step [48/192], loss=100.4331
	step [49/192], loss=81.2516
	step [50/192], loss=74.4976
	step [51/192], loss=85.7840
	step [52/192], loss=85.7783
	step [53/192], loss=113.9894
	step [54/192], loss=81.7097
	step [55/192], loss=92.4978
	step [56/192], loss=83.9016
	step [57/192], loss=93.5995
	step [58/192], loss=104.0587
	step [59/192], loss=85.6310
	step [60/192], loss=95.0660
	step [61/192], loss=91.8831
	step [62/192], loss=83.2116
	step [63/192], loss=100.1283
	step [64/192], loss=82.4367
	step [65/192], loss=85.6878
	step [66/192], loss=92.1084
	step [67/192], loss=86.7010
	step [68/192], loss=79.1027
	step [69/192], loss=93.8662
	step [70/192], loss=99.4525
	step [71/192], loss=73.2895
	step [72/192], loss=100.5567
	step [73/192], loss=88.8831
	step [74/192], loss=89.7323
	step [75/192], loss=96.4084
	step [76/192], loss=89.2471
	step [77/192], loss=95.6856
	step [78/192], loss=101.0901
	step [79/192], loss=102.8041
	step [80/192], loss=103.3566
	step [81/192], loss=91.4243
	step [82/192], loss=103.4982
	step [83/192], loss=108.4593
	step [84/192], loss=80.5195
	step [85/192], loss=100.9663
	step [86/192], loss=89.7455
	step [87/192], loss=99.3605
	step [88/192], loss=94.2874
	step [89/192], loss=75.1827
	step [90/192], loss=102.7347
	step [91/192], loss=77.1031
	step [92/192], loss=87.6576
	step [93/192], loss=108.3269
	step [94/192], loss=89.7222
	step [95/192], loss=103.0195
	step [96/192], loss=100.2758
	step [97/192], loss=92.2610
	step [98/192], loss=100.7460
	step [99/192], loss=87.7256
	step [100/192], loss=97.9339
	step [101/192], loss=93.4836
	step [102/192], loss=74.6575
	step [103/192], loss=97.4671
	step [104/192], loss=94.1972
	step [105/192], loss=94.6068
	step [106/192], loss=86.5649
	step [107/192], loss=86.9240
	step [108/192], loss=95.5488
	step [109/192], loss=97.7359
	step [110/192], loss=94.3992
	step [111/192], loss=97.2993
	step [112/192], loss=91.7777
	step [113/192], loss=84.6188
	step [114/192], loss=105.7930
	step [115/192], loss=93.4263
	step [116/192], loss=83.7346
	step [117/192], loss=97.1368
	step [118/192], loss=98.4109
	step [119/192], loss=106.4247
	step [120/192], loss=80.3068
	step [121/192], loss=86.4425
	step [122/192], loss=96.9260
	step [123/192], loss=88.9729
	step [124/192], loss=85.8012
	step [125/192], loss=86.1330
	step [126/192], loss=111.5569
	step [127/192], loss=92.4788
	step [128/192], loss=75.7148
	step [129/192], loss=93.1044
	step [130/192], loss=100.4179
	step [131/192], loss=106.8526
	step [132/192], loss=91.1888
	step [133/192], loss=96.0264
	step [134/192], loss=85.5360
	step [135/192], loss=93.0311
	step [136/192], loss=97.4239
	step [137/192], loss=90.8369
	step [138/192], loss=93.1159
	step [139/192], loss=95.5028
	step [140/192], loss=93.9963
	step [141/192], loss=95.7721
	step [142/192], loss=101.7680
	step [143/192], loss=100.1928
	step [144/192], loss=87.1299
	step [145/192], loss=79.9957
	step [146/192], loss=95.9878
	step [147/192], loss=102.5710
	step [148/192], loss=90.7128
	step [149/192], loss=98.8968
	step [150/192], loss=88.4338
	step [151/192], loss=93.0592
	step [152/192], loss=77.3726
	step [153/192], loss=78.6854
	step [154/192], loss=81.8943
	step [155/192], loss=82.7679
	step [156/192], loss=94.1717
	step [157/192], loss=82.7217
	step [158/192], loss=90.6268
	step [159/192], loss=81.4918
	step [160/192], loss=86.3424
	step [161/192], loss=81.7945
	step [162/192], loss=96.1420
	step [163/192], loss=78.1386
	step [164/192], loss=84.7000
	step [165/192], loss=104.1897
	step [166/192], loss=100.5686
	step [167/192], loss=86.5353
	step [168/192], loss=77.5683
	step [169/192], loss=75.2224
	step [170/192], loss=80.1173
	step [171/192], loss=104.8677
	step [172/192], loss=95.0895
	step [173/192], loss=92.1978
	step [174/192], loss=95.1394
	step [175/192], loss=90.8213
	step [176/192], loss=85.0557
	step [177/192], loss=106.7485
	step [178/192], loss=100.3436
	step [179/192], loss=107.3519
	step [180/192], loss=85.8053
	step [181/192], loss=97.5284
	step [182/192], loss=70.1501
	step [183/192], loss=82.1578
	step [184/192], loss=96.3344
	step [185/192], loss=94.2390
	step [186/192], loss=93.7872
	step [187/192], loss=84.1415
	step [188/192], loss=110.6210
	step [189/192], loss=94.0243
	step [190/192], loss=89.0775
	step [191/192], loss=90.0113
	step [192/192], loss=11.9540
	Evaluating
	loss=0.0169, precision=0.3725, recall=0.8874, f1=0.5247
Training epoch 21
	step [1/192], loss=95.5072
	step [2/192], loss=89.5768
	step [3/192], loss=78.5395
	step [4/192], loss=96.2060
	step [5/192], loss=83.7016
	step [6/192], loss=71.4514
	step [7/192], loss=88.4035
	step [8/192], loss=78.6611
	step [9/192], loss=76.4977
	step [10/192], loss=82.3135
	step [11/192], loss=85.5792
	step [12/192], loss=88.9433
	step [13/192], loss=102.2233
	step [14/192], loss=87.2884
	step [15/192], loss=87.1861
	step [16/192], loss=93.9105
	step [17/192], loss=111.0223
	step [18/192], loss=78.0811
	step [19/192], loss=94.8668
	step [20/192], loss=97.6242
	step [21/192], loss=98.5994
	step [22/192], loss=91.6985
	step [23/192], loss=103.9668
	step [24/192], loss=92.7053
	step [25/192], loss=85.6977
	step [26/192], loss=78.2609
	step [27/192], loss=92.5746
	step [28/192], loss=102.8116
	step [29/192], loss=94.4772
	step [30/192], loss=96.0569
	step [31/192], loss=90.7444
	step [32/192], loss=78.5853
	step [33/192], loss=106.2907
	step [34/192], loss=99.9375
	step [35/192], loss=106.5730
	step [36/192], loss=91.3229
	step [37/192], loss=86.2206
	step [38/192], loss=84.6818
	step [39/192], loss=87.2265
	step [40/192], loss=85.8831
	step [41/192], loss=90.6090
	step [42/192], loss=92.5298
	step [43/192], loss=83.1571
	step [44/192], loss=94.9002
	step [45/192], loss=88.3649
	step [46/192], loss=94.5876
	step [47/192], loss=89.8531
	step [48/192], loss=82.1940
	step [49/192], loss=92.6863
	step [50/192], loss=84.7730
	step [51/192], loss=81.0111
	step [52/192], loss=79.0931
	step [53/192], loss=94.4905
	step [54/192], loss=86.1849
	step [55/192], loss=96.4376
	step [56/192], loss=89.8014
	step [57/192], loss=88.7870
	step [58/192], loss=94.1752
	step [59/192], loss=81.4769
	step [60/192], loss=70.0259
	step [61/192], loss=104.1866
	step [62/192], loss=71.8967
	step [63/192], loss=94.9167
	step [64/192], loss=98.3145
	step [65/192], loss=97.2199
	step [66/192], loss=77.8766
	step [67/192], loss=95.3317
	step [68/192], loss=87.8993
	step [69/192], loss=90.0811
	step [70/192], loss=94.9442
	step [71/192], loss=92.6540
	step [72/192], loss=92.4230
	step [73/192], loss=95.8338
	step [74/192], loss=82.5840
	step [75/192], loss=110.1506
	step [76/192], loss=90.8036
	step [77/192], loss=87.2416
	step [78/192], loss=102.4485
	step [79/192], loss=77.9718
	step [80/192], loss=85.4325
	step [81/192], loss=100.0390
	step [82/192], loss=93.0929
	step [83/192], loss=97.3788
	step [84/192], loss=100.7353
	step [85/192], loss=81.6681
	step [86/192], loss=99.4686
	step [87/192], loss=102.5339
	step [88/192], loss=90.6992
	step [89/192], loss=81.0750
	step [90/192], loss=101.4846
	step [91/192], loss=85.4809
	step [92/192], loss=85.5706
	step [93/192], loss=104.8026
	step [94/192], loss=92.2504
	step [95/192], loss=87.1662
	step [96/192], loss=87.3296
	step [97/192], loss=76.1566
	step [98/192], loss=98.7863
	step [99/192], loss=91.2489
	step [100/192], loss=97.0234
	step [101/192], loss=88.2233
	step [102/192], loss=106.4141
	step [103/192], loss=88.8895
	step [104/192], loss=97.7500
	step [105/192], loss=88.9364
	step [106/192], loss=86.8462
	step [107/192], loss=90.8193
	step [108/192], loss=81.0411
	step [109/192], loss=86.2780
	step [110/192], loss=96.1772
	step [111/192], loss=83.4353
	step [112/192], loss=97.2379
	step [113/192], loss=90.5197
	step [114/192], loss=109.5138
	step [115/192], loss=96.5338
	step [116/192], loss=85.5191
	step [117/192], loss=93.5827
	step [118/192], loss=91.9852
	step [119/192], loss=101.4885
	step [120/192], loss=87.2123
	step [121/192], loss=92.7263
	step [122/192], loss=85.0358
	step [123/192], loss=86.8839
	step [124/192], loss=97.5294
	step [125/192], loss=83.9990
	step [126/192], loss=89.3696
	step [127/192], loss=107.7760
	step [128/192], loss=84.6695
	step [129/192], loss=93.0933
	step [130/192], loss=72.0549
	step [131/192], loss=83.7258
	step [132/192], loss=78.3955
	step [133/192], loss=97.8634
	step [134/192], loss=86.0313
	step [135/192], loss=93.8835
	step [136/192], loss=100.1431
	step [137/192], loss=90.4729
	step [138/192], loss=97.1329
	step [139/192], loss=95.6606
	step [140/192], loss=93.7009
	step [141/192], loss=78.2884
	step [142/192], loss=100.2919
	step [143/192], loss=89.7588
	step [144/192], loss=78.5645
	step [145/192], loss=85.0498
	step [146/192], loss=90.5978
	step [147/192], loss=81.0323
	step [148/192], loss=97.5655
	step [149/192], loss=94.6469
	step [150/192], loss=90.7678
	step [151/192], loss=90.6962
	step [152/192], loss=92.9950
	step [153/192], loss=96.9336
	step [154/192], loss=93.5309
	step [155/192], loss=98.4320
	step [156/192], loss=95.1799
	step [157/192], loss=70.5901
	step [158/192], loss=92.1550
	step [159/192], loss=77.5031
	step [160/192], loss=90.1340
	step [161/192], loss=87.1364
	step [162/192], loss=91.9534
	step [163/192], loss=109.0294
	step [164/192], loss=88.1476
	step [165/192], loss=90.2126
	step [166/192], loss=80.6831
	step [167/192], loss=87.8395
	step [168/192], loss=102.4278
	step [169/192], loss=91.9009
	step [170/192], loss=90.4469
	step [171/192], loss=85.2088
	step [172/192], loss=87.0150
	step [173/192], loss=90.9606
	step [174/192], loss=97.7766
	step [175/192], loss=89.4591
	step [176/192], loss=91.4732
	step [177/192], loss=90.4867
	step [178/192], loss=89.5843
	step [179/192], loss=85.4632
	step [180/192], loss=97.1137
	step [181/192], loss=106.0035
	step [182/192], loss=97.0553
	step [183/192], loss=86.4503
	step [184/192], loss=95.0247
	step [185/192], loss=88.1019
	step [186/192], loss=99.7119
	step [187/192], loss=88.2036
	step [188/192], loss=92.7908
	step [189/192], loss=95.1589
	step [190/192], loss=95.0540
	step [191/192], loss=95.7059
	step [192/192], loss=13.8440
	Evaluating
	loss=0.0164, precision=0.3832, recall=0.9023, f1=0.5380
Training epoch 22
	step [1/192], loss=88.2648
	step [2/192], loss=102.3876
	step [3/192], loss=91.4044
	step [4/192], loss=93.1394
	step [5/192], loss=99.0452
	step [6/192], loss=92.1757
	step [7/192], loss=84.1383
	step [8/192], loss=88.6343
	step [9/192], loss=66.5272
	step [10/192], loss=92.3702
	step [11/192], loss=78.7378
	step [12/192], loss=89.7385
	step [13/192], loss=87.2763
	step [14/192], loss=97.2260
	step [15/192], loss=99.1185
	step [16/192], loss=86.5970
	step [17/192], loss=97.9450
	step [18/192], loss=91.3035
	step [19/192], loss=82.8219
	step [20/192], loss=95.2227
	step [21/192], loss=90.0249
	step [22/192], loss=91.7184
	step [23/192], loss=78.8940
	step [24/192], loss=76.9291
	step [25/192], loss=68.0047
	step [26/192], loss=85.8297
	step [27/192], loss=85.9029
	step [28/192], loss=97.2705
	step [29/192], loss=92.7066
	step [30/192], loss=85.9593
	step [31/192], loss=89.0118
	step [32/192], loss=84.0741
	step [33/192], loss=116.1863
	step [34/192], loss=98.5819
	step [35/192], loss=99.3745
	step [36/192], loss=84.3747
	step [37/192], loss=88.9509
	step [38/192], loss=94.8360
	step [39/192], loss=96.6397
	step [40/192], loss=91.7791
	step [41/192], loss=74.1231
	step [42/192], loss=91.5068
	step [43/192], loss=106.3124
	step [44/192], loss=84.8617
	step [45/192], loss=92.5022
	step [46/192], loss=80.9969
	step [47/192], loss=101.9068
	step [48/192], loss=85.6943
	step [49/192], loss=90.8989
	step [50/192], loss=91.2996
	step [51/192], loss=95.5737
	step [52/192], loss=103.9844
	step [53/192], loss=95.9152
	step [54/192], loss=111.9871
	step [55/192], loss=104.4180
	step [56/192], loss=87.2584
	step [57/192], loss=87.4408
	step [58/192], loss=84.1578
	step [59/192], loss=92.7217
	step [60/192], loss=89.6336
	step [61/192], loss=92.3468
	step [62/192], loss=87.8683
	step [63/192], loss=87.2749
	step [64/192], loss=89.7224
	step [65/192], loss=99.9088
	step [66/192], loss=73.9224
	step [67/192], loss=85.5405
	step [68/192], loss=97.5294
	step [69/192], loss=86.2708
	step [70/192], loss=106.3603
	step [71/192], loss=78.1644
	step [72/192], loss=86.6416
	step [73/192], loss=95.1440
	step [74/192], loss=77.9158
	step [75/192], loss=99.8502
	step [76/192], loss=87.4027
	step [77/192], loss=72.4547
	step [78/192], loss=93.8520
	step [79/192], loss=96.7270
	step [80/192], loss=86.8920
	step [81/192], loss=93.3244
	step [82/192], loss=95.1715
	step [83/192], loss=88.1993
	step [84/192], loss=90.0518
	step [85/192], loss=81.0028
	step [86/192], loss=85.9892
	step [87/192], loss=96.7049
	step [88/192], loss=97.1125
	step [89/192], loss=101.8826
	step [90/192], loss=87.8847
	step [91/192], loss=85.5487
	step [92/192], loss=79.9920
	step [93/192], loss=93.5570
	step [94/192], loss=79.4321
	step [95/192], loss=90.0247
	step [96/192], loss=99.4127
	step [97/192], loss=89.4021
	step [98/192], loss=84.6972
	step [99/192], loss=80.8788
	step [100/192], loss=94.4353
	step [101/192], loss=83.8703
	step [102/192], loss=84.4307
	step [103/192], loss=91.2448
	step [104/192], loss=82.6898
	step [105/192], loss=101.8015
	step [106/192], loss=74.9057
	step [107/192], loss=88.2889
	step [108/192], loss=98.3414
	step [109/192], loss=77.9638
	step [110/192], loss=100.0766
	step [111/192], loss=97.5357
	step [112/192], loss=87.1516
	step [113/192], loss=92.4730
	step [114/192], loss=105.5317
	step [115/192], loss=86.6094
	step [116/192], loss=88.5066
	step [117/192], loss=76.5517
	step [118/192], loss=91.0727
	step [119/192], loss=91.2480
	step [120/192], loss=87.3491
	step [121/192], loss=84.5602
	step [122/192], loss=99.0381
	step [123/192], loss=84.1794
	step [124/192], loss=93.0873
	step [125/192], loss=82.2118
	step [126/192], loss=82.6916
	step [127/192], loss=88.1783
	step [128/192], loss=79.0887
	step [129/192], loss=97.7511
	step [130/192], loss=89.9515
	step [131/192], loss=91.8075
	step [132/192], loss=91.6073
	step [133/192], loss=90.3095
	step [134/192], loss=92.4279
	step [135/192], loss=100.7104
	step [136/192], loss=95.6646
	step [137/192], loss=99.3783
	step [138/192], loss=78.1347
	step [139/192], loss=77.4281
	step [140/192], loss=115.0211
	step [141/192], loss=80.7232
	step [142/192], loss=80.8483
	step [143/192], loss=88.0757
	step [144/192], loss=104.4925
	step [145/192], loss=97.9520
	step [146/192], loss=78.9424
	step [147/192], loss=87.1521
	step [148/192], loss=82.0858
	step [149/192], loss=94.3617
	step [150/192], loss=98.9762
	step [151/192], loss=85.9564
	step [152/192], loss=84.6047
	step [153/192], loss=98.4597
	step [154/192], loss=86.3498
	step [155/192], loss=82.0733
	step [156/192], loss=85.8686
	step [157/192], loss=86.1538
	step [158/192], loss=85.4321
	step [159/192], loss=95.7266
	step [160/192], loss=82.4990
	step [161/192], loss=89.6060
	step [162/192], loss=88.1224
	step [163/192], loss=97.4018
	step [164/192], loss=106.3782
	step [165/192], loss=84.2225
	step [166/192], loss=79.2616
	step [167/192], loss=93.6336
	step [168/192], loss=96.7294
	step [169/192], loss=74.1802
	step [170/192], loss=85.5913
	step [171/192], loss=79.5373
	step [172/192], loss=108.6387
	step [173/192], loss=100.6934
	step [174/192], loss=99.6055
	step [175/192], loss=91.8551
	step [176/192], loss=105.6187
	step [177/192], loss=86.5019
	step [178/192], loss=78.8154
	step [179/192], loss=80.3867
	step [180/192], loss=87.3301
	step [181/192], loss=93.1711
	step [182/192], loss=78.9423
	step [183/192], loss=85.5854
	step [184/192], loss=71.2706
	step [185/192], loss=80.2855
	step [186/192], loss=102.9856
	step [187/192], loss=81.1945
	step [188/192], loss=98.4606
	step [189/192], loss=88.5072
	step [190/192], loss=96.9180
	step [191/192], loss=89.9885
	step [192/192], loss=14.8865
	Evaluating
	loss=0.0145, precision=0.3863, recall=0.8959, f1=0.5398
Training epoch 23
	step [1/192], loss=80.1019
	step [2/192], loss=95.1592
	step [3/192], loss=89.8131
	step [4/192], loss=75.4194
	step [5/192], loss=90.6195
	step [6/192], loss=86.8158
	step [7/192], loss=106.2443
	step [8/192], loss=103.8010
	step [9/192], loss=91.6867
	step [10/192], loss=98.0446
	step [11/192], loss=100.2466
	step [12/192], loss=76.9224
	step [13/192], loss=79.7422
	step [14/192], loss=87.7817
	step [15/192], loss=86.0043
	step [16/192], loss=89.1021
	step [17/192], loss=94.4003
	step [18/192], loss=97.1439
	step [19/192], loss=78.7878
	step [20/192], loss=87.7234
	step [21/192], loss=86.2367
	step [22/192], loss=82.2714
	step [23/192], loss=91.1294
	step [24/192], loss=90.1784
	step [25/192], loss=97.2611
	step [26/192], loss=83.5264
	step [27/192], loss=90.5777
	step [28/192], loss=88.3102
	step [29/192], loss=81.5378
	step [30/192], loss=82.3793
	step [31/192], loss=84.4261
	step [32/192], loss=99.1400
	step [33/192], loss=93.3043
	step [34/192], loss=93.3190
	step [35/192], loss=86.0993
	step [36/192], loss=73.0822
	step [37/192], loss=87.8915
	step [38/192], loss=85.7405
	step [39/192], loss=91.4649
	step [40/192], loss=72.1141
	step [41/192], loss=98.1234
	step [42/192], loss=92.1646
	step [43/192], loss=89.3344
	step [44/192], loss=80.2371
	step [45/192], loss=84.7178
	step [46/192], loss=85.6582
	step [47/192], loss=83.9948
	step [48/192], loss=84.6032
	step [49/192], loss=101.6492
	step [50/192], loss=101.0088
	step [51/192], loss=110.1884
	step [52/192], loss=107.1539
	step [53/192], loss=82.9233
	step [54/192], loss=86.0696
	step [55/192], loss=94.7300
	step [56/192], loss=91.2354
	step [57/192], loss=81.1504
	step [58/192], loss=85.2709
	step [59/192], loss=88.1912
	step [60/192], loss=84.4945
	step [61/192], loss=102.0542
	step [62/192], loss=88.6453
	step [63/192], loss=90.5897
	step [64/192], loss=82.7061
	step [65/192], loss=80.2075
	step [66/192], loss=93.9167
	step [67/192], loss=103.0350
	step [68/192], loss=112.9237
	step [69/192], loss=111.9452
	step [70/192], loss=99.2721
	step [71/192], loss=93.7123
	step [72/192], loss=92.6055
	step [73/192], loss=84.0129
	step [74/192], loss=93.4137
	step [75/192], loss=71.6302
	step [76/192], loss=92.9367
	step [77/192], loss=85.2469
	step [78/192], loss=94.1919
	step [79/192], loss=80.9151
	step [80/192], loss=84.7453
	step [81/192], loss=90.1362
	step [82/192], loss=88.9203
	step [83/192], loss=85.2169
	step [84/192], loss=84.2216
	step [85/192], loss=89.4490
	step [86/192], loss=87.2923
	step [87/192], loss=93.9902
	step [88/192], loss=88.3109
	step [89/192], loss=70.3417
	step [90/192], loss=66.9455
	step [91/192], loss=74.0780
	step [92/192], loss=87.6738
	step [93/192], loss=86.6264
	step [94/192], loss=72.4877
	step [95/192], loss=92.3481
	step [96/192], loss=68.3107
	step [97/192], loss=85.9666
	step [98/192], loss=82.3650
	step [99/192], loss=86.1601
	step [100/192], loss=91.6182
	step [101/192], loss=81.9593
	step [102/192], loss=91.1943
	step [103/192], loss=99.2285
	step [104/192], loss=85.5597
	step [105/192], loss=66.4420
	step [106/192], loss=87.6706
	step [107/192], loss=114.8984
	step [108/192], loss=84.4986
	step [109/192], loss=83.5037
	step [110/192], loss=82.7400
	step [111/192], loss=83.1394
	step [112/192], loss=75.7462
	step [113/192], loss=83.6638
	step [114/192], loss=83.4882
	step [115/192], loss=66.0509
	step [116/192], loss=76.1195
	step [117/192], loss=81.0510
	step [118/192], loss=98.4516
	step [119/192], loss=81.5864
	step [120/192], loss=94.0931
	step [121/192], loss=89.4069
	step [122/192], loss=84.6716
	step [123/192], loss=93.5049
	step [124/192], loss=88.0455
	step [125/192], loss=92.0528
	step [126/192], loss=98.0893
	step [127/192], loss=95.3272
	step [128/192], loss=89.9381
	step [129/192], loss=95.8668
	step [130/192], loss=100.9594
	step [131/192], loss=86.4041
	step [132/192], loss=96.6419
	step [133/192], loss=105.2375
	step [134/192], loss=80.8751
	step [135/192], loss=93.1513
	step [136/192], loss=94.7381
	step [137/192], loss=96.6188
	step [138/192], loss=88.7438
	step [139/192], loss=78.9667
	step [140/192], loss=100.8823
	step [141/192], loss=87.0797
	step [142/192], loss=80.0596
	step [143/192], loss=70.0111
	step [144/192], loss=84.0197
	step [145/192], loss=113.6881
	step [146/192], loss=80.9368
	step [147/192], loss=83.6390
	step [148/192], loss=84.1405
	step [149/192], loss=85.9020
	step [150/192], loss=88.2994
	step [151/192], loss=95.6319
	step [152/192], loss=77.4390
	step [153/192], loss=86.2584
	step [154/192], loss=92.1050
	step [155/192], loss=100.5159
	step [156/192], loss=94.1917
	step [157/192], loss=82.6197
	step [158/192], loss=80.4220
	step [159/192], loss=87.5256
	step [160/192], loss=81.2936
	step [161/192], loss=77.1959
	step [162/192], loss=109.9575
	step [163/192], loss=86.3548
	step [164/192], loss=83.3213
	step [165/192], loss=91.4826
	step [166/192], loss=96.7931
	step [167/192], loss=88.5636
	step [168/192], loss=84.0579
	step [169/192], loss=67.1429
	step [170/192], loss=80.4122
	step [171/192], loss=91.8122
	step [172/192], loss=83.3802
	step [173/192], loss=95.7664
	step [174/192], loss=94.4132
	step [175/192], loss=110.1654
	step [176/192], loss=93.1709
	step [177/192], loss=86.4802
	step [178/192], loss=87.4346
	step [179/192], loss=88.9851
	step [180/192], loss=91.8414
	step [181/192], loss=82.3742
	step [182/192], loss=98.4400
	step [183/192], loss=88.0845
	step [184/192], loss=80.8401
	step [185/192], loss=99.6082
	step [186/192], loss=80.1384
	step [187/192], loss=92.7965
	step [188/192], loss=81.4199
	step [189/192], loss=94.9339
	step [190/192], loss=96.6064
	step [191/192], loss=84.7243
	step [192/192], loss=14.1498
	Evaluating
	loss=0.0147, precision=0.3625, recall=0.9098, f1=0.5184
Training epoch 24
	step [1/192], loss=78.5182
	step [2/192], loss=87.0617
	step [3/192], loss=90.4143
	step [4/192], loss=81.2920
	step [5/192], loss=87.2758
	step [6/192], loss=91.9199
	step [7/192], loss=78.8213
	step [8/192], loss=92.7783
	step [9/192], loss=76.1939
	step [10/192], loss=82.0914
	step [11/192], loss=93.7476
	step [12/192], loss=87.5737
	step [13/192], loss=85.1804
	step [14/192], loss=84.5832
	step [15/192], loss=94.2576
	step [16/192], loss=84.8254
	step [17/192], loss=78.7357
	step [18/192], loss=93.1428
	step [19/192], loss=79.9543
	step [20/192], loss=83.6306
	step [21/192], loss=90.8286
	step [22/192], loss=85.8122
	step [23/192], loss=89.1613
	step [24/192], loss=82.4022
	step [25/192], loss=101.8750
	step [26/192], loss=104.6721
	step [27/192], loss=94.1064
	step [28/192], loss=90.1178
	step [29/192], loss=89.8607
	step [30/192], loss=80.0907
	step [31/192], loss=93.0816
	step [32/192], loss=97.2440
	step [33/192], loss=89.5175
	step [34/192], loss=82.5507
	step [35/192], loss=94.9973
	step [36/192], loss=86.8795
	step [37/192], loss=107.7530
	step [38/192], loss=95.2908
	step [39/192], loss=89.0274
	step [40/192], loss=74.2112
	step [41/192], loss=88.9956
	step [42/192], loss=82.0770
	step [43/192], loss=94.2119
	step [44/192], loss=91.8877
	step [45/192], loss=97.9700
	step [46/192], loss=89.1796
	step [47/192], loss=90.2034
	step [48/192], loss=85.5094
	step [49/192], loss=102.4264
	step [50/192], loss=104.2756
	step [51/192], loss=89.7002
	step [52/192], loss=77.1688
	step [53/192], loss=96.0266
	step [54/192], loss=97.6637
	step [55/192], loss=68.2375
	step [56/192], loss=88.0211
	step [57/192], loss=91.1758
	step [58/192], loss=76.0630
	step [59/192], loss=81.6175
	step [60/192], loss=82.5460
	step [61/192], loss=104.3360
	step [62/192], loss=93.1486
	step [63/192], loss=98.9106
	step [64/192], loss=90.4810
	step [65/192], loss=77.8139
	step [66/192], loss=100.8450
	step [67/192], loss=77.8968
	step [68/192], loss=77.7482
	step [69/192], loss=96.7491
	step [70/192], loss=87.5199
	step [71/192], loss=84.3497
	step [72/192], loss=84.1935
	step [73/192], loss=72.2374
	step [74/192], loss=84.4731
	step [75/192], loss=80.4948
	step [76/192], loss=85.0532
	step [77/192], loss=83.6377
	step [78/192], loss=80.4846
	step [79/192], loss=80.1185
	step [80/192], loss=95.6873
	step [81/192], loss=91.5909
	step [82/192], loss=81.4974
	step [83/192], loss=77.3225
	step [84/192], loss=92.3191
	step [85/192], loss=90.3340
	step [86/192], loss=91.1914
	step [87/192], loss=100.2111
	step [88/192], loss=85.8314
	step [89/192], loss=87.8642
	step [90/192], loss=85.2072
	step [91/192], loss=81.2128
	step [92/192], loss=86.8220
	step [93/192], loss=87.6990
	step [94/192], loss=78.1462
	step [95/192], loss=98.8127
	step [96/192], loss=88.4655
	step [97/192], loss=78.1844
	step [98/192], loss=96.6259
	step [99/192], loss=91.7103
	step [100/192], loss=83.8989
	step [101/192], loss=99.5205
	step [102/192], loss=90.2224
	step [103/192], loss=97.7348
	step [104/192], loss=78.8989
	step [105/192], loss=91.4794
	step [106/192], loss=77.7883
	step [107/192], loss=77.1201
	step [108/192], loss=83.4037
	step [109/192], loss=80.1813
	step [110/192], loss=91.3391
	step [111/192], loss=82.0182
	step [112/192], loss=77.0103
	step [113/192], loss=100.6187
	step [114/192], loss=80.3776
	step [115/192], loss=79.5785
	step [116/192], loss=85.4101
	step [117/192], loss=79.8302
	step [118/192], loss=106.1477
	step [119/192], loss=76.7175
	step [120/192], loss=88.5683
	step [121/192], loss=98.1804
	step [122/192], loss=94.1043
	step [123/192], loss=87.0563
	step [124/192], loss=90.5522
	step [125/192], loss=105.7688
	step [126/192], loss=87.3719
	step [127/192], loss=80.1090
	step [128/192], loss=82.8362
	step [129/192], loss=76.1523
	step [130/192], loss=78.5146
	step [131/192], loss=101.7988
	step [132/192], loss=79.4943
	step [133/192], loss=91.5765
	step [134/192], loss=88.9256
	step [135/192], loss=91.1959
	step [136/192], loss=86.3584
	step [137/192], loss=97.6146
	step [138/192], loss=83.1134
	step [139/192], loss=90.8692
	step [140/192], loss=84.8501
	step [141/192], loss=92.4886
	step [142/192], loss=84.2208
	step [143/192], loss=88.1483
	step [144/192], loss=79.6897
	step [145/192], loss=83.7934
	step [146/192], loss=84.9715
	step [147/192], loss=91.1420
	step [148/192], loss=88.7503
	step [149/192], loss=84.7843
	step [150/192], loss=73.6459
	step [151/192], loss=87.8877
	step [152/192], loss=80.0164
	step [153/192], loss=88.0924
	step [154/192], loss=89.7355
	step [155/192], loss=94.7820
	step [156/192], loss=76.8544
	step [157/192], loss=84.3781
	step [158/192], loss=94.3988
	step [159/192], loss=82.7299
	step [160/192], loss=99.1202
	step [161/192], loss=86.0839
	step [162/192], loss=86.9071
	step [163/192], loss=87.0085
	step [164/192], loss=71.7778
	step [165/192], loss=98.6667
	step [166/192], loss=82.7467
	step [167/192], loss=95.7048
	step [168/192], loss=86.3720
	step [169/192], loss=71.6118
	step [170/192], loss=89.3837
	step [171/192], loss=98.2314
	step [172/192], loss=71.0099
	step [173/192], loss=85.4768
	step [174/192], loss=88.7062
	step [175/192], loss=84.2099
	step [176/192], loss=80.8111
	step [177/192], loss=84.1633
	step [178/192], loss=90.5935
	step [179/192], loss=85.0744
	step [180/192], loss=89.3268
	step [181/192], loss=78.6038
	step [182/192], loss=90.2242
	step [183/192], loss=86.0494
	step [184/192], loss=87.6115
	step [185/192], loss=91.2133
	step [186/192], loss=69.6962
	step [187/192], loss=91.0041
	step [188/192], loss=88.8517
	step [189/192], loss=89.6981
	step [190/192], loss=90.2751
	step [191/192], loss=75.6154
	step [192/192], loss=12.4475
	Evaluating
	loss=0.0121, precision=0.4053, recall=0.8905, f1=0.5571
saving model as: 0_saved_model.pth
Training epoch 25
	step [1/192], loss=80.4741
	step [2/192], loss=86.7176
	step [3/192], loss=106.4611
	step [4/192], loss=81.7872
	step [5/192], loss=86.9814
	step [6/192], loss=87.8623
	step [7/192], loss=73.2487
	step [8/192], loss=81.0805
	step [9/192], loss=70.6241
	step [10/192], loss=93.1710
	step [11/192], loss=84.3166
	step [12/192], loss=95.1103
	step [13/192], loss=78.5594
	step [14/192], loss=99.8965
	step [15/192], loss=76.3152
	step [16/192], loss=75.0579
	step [17/192], loss=98.1723
	step [18/192], loss=88.0886
	step [19/192], loss=89.7943
	step [20/192], loss=91.0155
	step [21/192], loss=77.6461
	step [22/192], loss=75.0779
	step [23/192], loss=93.3826
	step [24/192], loss=90.2148
	step [25/192], loss=93.6434
	step [26/192], loss=86.9803
	step [27/192], loss=93.1559
	step [28/192], loss=90.8258
	step [29/192], loss=89.2030
	step [30/192], loss=79.7961
	step [31/192], loss=85.1398
	step [32/192], loss=87.9507
	step [33/192], loss=92.7607
	step [34/192], loss=88.1553
	step [35/192], loss=83.8424
	step [36/192], loss=81.6755
	step [37/192], loss=89.9785
	step [38/192], loss=70.6447
	step [39/192], loss=85.5774
	step [40/192], loss=85.1422
	step [41/192], loss=74.9088
	step [42/192], loss=81.5453
	step [43/192], loss=90.7487
	step [44/192], loss=95.0754
	step [45/192], loss=91.9662
	step [46/192], loss=83.6033
	step [47/192], loss=85.6147
	step [48/192], loss=84.7042
	step [49/192], loss=100.8976
	step [50/192], loss=82.6609
	step [51/192], loss=77.7805
	step [52/192], loss=81.9020
	step [53/192], loss=84.3795
	step [54/192], loss=90.3776
	step [55/192], loss=82.8708
	step [56/192], loss=74.0084
	step [57/192], loss=78.1032
	step [58/192], loss=77.0944
	step [59/192], loss=87.7966
	step [60/192], loss=91.6952
	step [61/192], loss=79.3165
	step [62/192], loss=79.4912
	step [63/192], loss=89.8456
	step [64/192], loss=93.1980
	step [65/192], loss=84.2271
	step [66/192], loss=67.2934
	step [67/192], loss=95.7221
	step [68/192], loss=93.6037
	step [69/192], loss=74.9047
	step [70/192], loss=94.8041
	step [71/192], loss=83.9133
	step [72/192], loss=86.4284
	step [73/192], loss=84.5855
	step [74/192], loss=90.7703
	step [75/192], loss=76.5588
	step [76/192], loss=82.7311
	step [77/192], loss=77.5376
	step [78/192], loss=95.2407
	step [79/192], loss=76.4575
	step [80/192], loss=83.3415
	step [81/192], loss=84.4837
	step [82/192], loss=107.2895
	step [83/192], loss=91.5475
	step [84/192], loss=88.4963
	step [85/192], loss=107.2186
	step [86/192], loss=96.8076
	step [87/192], loss=98.4822
	step [88/192], loss=79.0387
	step [89/192], loss=92.9008
	step [90/192], loss=104.0476
	step [91/192], loss=93.5879
	step [92/192], loss=81.0725
	step [93/192], loss=82.5516
	step [94/192], loss=82.8627
	step [95/192], loss=110.4249
	step [96/192], loss=78.6966
	step [97/192], loss=89.7746
	step [98/192], loss=85.0791
	step [99/192], loss=93.5538
	step [100/192], loss=75.0977
	step [101/192], loss=76.1843
	step [102/192], loss=101.3333
	step [103/192], loss=80.4820
	step [104/192], loss=81.4843
	step [105/192], loss=88.0649
	step [106/192], loss=90.9145
	step [107/192], loss=74.0106
	step [108/192], loss=95.9862
	step [109/192], loss=95.5035
	step [110/192], loss=93.3400
	step [111/192], loss=85.0651
	step [112/192], loss=87.1906
	step [113/192], loss=92.9841
	step [114/192], loss=98.9198
	step [115/192], loss=87.8064
	step [116/192], loss=77.0959
	step [117/192], loss=85.8663
	step [118/192], loss=78.2342
	step [119/192], loss=95.7057
	step [120/192], loss=91.8583
	step [121/192], loss=76.6809
	step [122/192], loss=74.4801
	step [123/192], loss=66.9018
	step [124/192], loss=68.4938
	step [125/192], loss=86.2558
	step [126/192], loss=77.2016
	step [127/192], loss=101.9362
	step [128/192], loss=90.2986
	step [129/192], loss=77.8654
	step [130/192], loss=82.5118
	step [131/192], loss=82.6219
	step [132/192], loss=86.5841
	step [133/192], loss=78.3800
	step [134/192], loss=91.8684
	step [135/192], loss=95.9278
	step [136/192], loss=91.2578
	step [137/192], loss=79.4138
	step [138/192], loss=86.5952
	step [139/192], loss=87.3366
	step [140/192], loss=93.8489
	step [141/192], loss=79.8170
	step [142/192], loss=74.0620
	step [143/192], loss=92.8491
	step [144/192], loss=94.6886
	step [145/192], loss=83.2197
	step [146/192], loss=72.0009
	step [147/192], loss=85.7520
	step [148/192], loss=86.4601
	step [149/192], loss=97.0581
	step [150/192], loss=78.0879
	step [151/192], loss=98.2434
	step [152/192], loss=84.1311
	step [153/192], loss=74.8724
	step [154/192], loss=88.9423
	step [155/192], loss=88.3218
	step [156/192], loss=79.1412
	step [157/192], loss=73.6213
	step [158/192], loss=72.7773
	step [159/192], loss=80.3752
	step [160/192], loss=98.1565
	step [161/192], loss=91.4331
	step [162/192], loss=93.8014
	step [163/192], loss=85.4848
	step [164/192], loss=88.6500
	step [165/192], loss=82.8503
	step [166/192], loss=90.1922
	step [167/192], loss=85.8013
	step [168/192], loss=93.5781
	step [169/192], loss=84.9906
	step [170/192], loss=85.0804
	step [171/192], loss=94.2285
	step [172/192], loss=92.3133
	step [173/192], loss=81.4691
	step [174/192], loss=93.1306
	step [175/192], loss=92.4251
	step [176/192], loss=64.8694
	step [177/192], loss=89.6236
	step [178/192], loss=88.6223
	step [179/192], loss=87.4872
	step [180/192], loss=83.4387
	step [181/192], loss=77.1634
	step [182/192], loss=96.1541
	step [183/192], loss=98.0013
	step [184/192], loss=79.4719
	step [185/192], loss=74.2683
	step [186/192], loss=89.7991
	step [187/192], loss=98.8466
	step [188/192], loss=76.7688
	step [189/192], loss=84.9890
	step [190/192], loss=92.8146
	step [191/192], loss=81.3252
	step [192/192], loss=13.8688
	Evaluating
	loss=0.0125, precision=0.3955, recall=0.9131, f1=0.5519
Training epoch 26
	step [1/192], loss=82.5956
	step [2/192], loss=82.5009
	step [3/192], loss=95.8885
	step [4/192], loss=80.5789
	step [5/192], loss=83.4338
	step [6/192], loss=90.0042
	step [7/192], loss=80.0001
	step [8/192], loss=80.9725
	step [9/192], loss=90.4604
	step [10/192], loss=79.3621
	step [11/192], loss=87.5331
	step [12/192], loss=83.8324
	step [13/192], loss=77.7814
	step [14/192], loss=76.5128
	step [15/192], loss=76.7591
	step [16/192], loss=89.6329
	step [17/192], loss=103.0564
	step [18/192], loss=72.5178
	step [19/192], loss=83.0751
	step [20/192], loss=83.8703
	step [21/192], loss=93.8243
	step [22/192], loss=83.9231
	step [23/192], loss=94.4559
	step [24/192], loss=67.5328
	step [25/192], loss=90.0880
	step [26/192], loss=83.9619
	step [27/192], loss=86.7968
	step [28/192], loss=104.9859
	step [29/192], loss=84.4354
	step [30/192], loss=96.0413
	step [31/192], loss=71.8162
	step [32/192], loss=75.8656
	step [33/192], loss=94.5309
	step [34/192], loss=101.3102
	step [35/192], loss=92.7921
	step [36/192], loss=84.4612
	step [37/192], loss=86.0627
	step [38/192], loss=80.3355
	step [39/192], loss=77.9036
	step [40/192], loss=101.0923
	step [41/192], loss=84.0049
	step [42/192], loss=84.7974
	step [43/192], loss=102.1614
	step [44/192], loss=94.0368
	step [45/192], loss=89.0404
	step [46/192], loss=78.4639
	step [47/192], loss=98.0318
	step [48/192], loss=81.5702
	step [49/192], loss=82.7743
	step [50/192], loss=83.6021
	step [51/192], loss=80.1867
	step [52/192], loss=93.6772
	step [53/192], loss=79.7522
	step [54/192], loss=88.3332
	step [55/192], loss=88.6683
	step [56/192], loss=80.2450
	step [57/192], loss=88.5736
	step [58/192], loss=87.8958
	step [59/192], loss=78.1240
	step [60/192], loss=94.8984
	step [61/192], loss=90.0594
	step [62/192], loss=78.5306
	step [63/192], loss=84.4921
	step [64/192], loss=86.0462
	step [65/192], loss=80.0055
	step [66/192], loss=86.7954
	step [67/192], loss=82.9696
	step [68/192], loss=81.8693
	step [69/192], loss=86.7153
	step [70/192], loss=85.9631
	step [71/192], loss=80.0153
	step [72/192], loss=80.3325
	step [73/192], loss=88.9056
	step [74/192], loss=91.1424
	step [75/192], loss=83.1040
	step [76/192], loss=92.3963
	step [77/192], loss=84.9480
	step [78/192], loss=86.6425
	step [79/192], loss=91.8732
	step [80/192], loss=80.6017
	step [81/192], loss=78.7088
	step [82/192], loss=80.2155
	step [83/192], loss=88.1469
	step [84/192], loss=83.1900
	step [85/192], loss=72.6055
	step [86/192], loss=82.4965
	step [87/192], loss=74.1472
	step [88/192], loss=84.6453
	step [89/192], loss=82.9470
	step [90/192], loss=84.5707
	step [91/192], loss=94.5882
	step [92/192], loss=81.5571
	step [93/192], loss=99.2129
	step [94/192], loss=78.4079
	step [95/192], loss=84.5376
	step [96/192], loss=76.9858
	step [97/192], loss=81.5724
	step [98/192], loss=94.8402
	step [99/192], loss=88.9503
	step [100/192], loss=101.6881
	step [101/192], loss=83.7561
	step [102/192], loss=92.8021
	step [103/192], loss=80.3649
	step [104/192], loss=81.1188
	step [105/192], loss=98.7779
	step [106/192], loss=87.8515
	step [107/192], loss=96.0476
	step [108/192], loss=81.8716
	step [109/192], loss=88.4417
	step [110/192], loss=73.0298
	step [111/192], loss=77.2115
	step [112/192], loss=83.0683
	step [113/192], loss=84.0072
	step [114/192], loss=83.6542
	step [115/192], loss=91.4275
	step [116/192], loss=79.2079
	step [117/192], loss=83.1818
	step [118/192], loss=98.4128
	step [119/192], loss=88.9126
	step [120/192], loss=77.8509
	step [121/192], loss=113.1178
	step [122/192], loss=74.2314
	step [123/192], loss=72.8990
	step [124/192], loss=87.6889
	step [125/192], loss=81.7976
	step [126/192], loss=82.4506
	step [127/192], loss=85.3330
	step [128/192], loss=84.8621
	step [129/192], loss=94.2674
	step [130/192], loss=71.6068
	step [131/192], loss=76.0699
	step [132/192], loss=95.2103
	step [133/192], loss=104.8300
	step [134/192], loss=94.8954
	step [135/192], loss=89.7876
	step [136/192], loss=84.4862
	step [137/192], loss=95.4627
	step [138/192], loss=84.5171
	step [139/192], loss=98.8900
	step [140/192], loss=84.1234
	step [141/192], loss=72.6229
	step [142/192], loss=82.6445
	step [143/192], loss=78.1764
	step [144/192], loss=105.2778
	step [145/192], loss=86.3794
	step [146/192], loss=88.6666
	step [147/192], loss=92.5396
	step [148/192], loss=96.0788
	step [149/192], loss=93.6166
	step [150/192], loss=85.7930
	step [151/192], loss=80.6015
	step [152/192], loss=83.4529
	step [153/192], loss=92.1915
	step [154/192], loss=89.6308
	step [155/192], loss=80.7005
	step [156/192], loss=81.5328
	step [157/192], loss=81.8654
	step [158/192], loss=75.0241
	step [159/192], loss=92.7340
	step [160/192], loss=77.1129
	step [161/192], loss=79.4724
	step [162/192], loss=84.9456
	step [163/192], loss=87.4180
	step [164/192], loss=88.6063
	step [165/192], loss=100.0241
	step [166/192], loss=87.3595
	step [167/192], loss=79.7184
	step [168/192], loss=95.4990
	step [169/192], loss=78.1358
	step [170/192], loss=91.2487
	step [171/192], loss=95.2557
	step [172/192], loss=61.2674
	step [173/192], loss=84.8821
	step [174/192], loss=80.6038
	step [175/192], loss=86.1410
	step [176/192], loss=71.5966
	step [177/192], loss=78.2788
	step [178/192], loss=66.9847
	step [179/192], loss=97.3792
	step [180/192], loss=79.5643
	step [181/192], loss=95.0108
	step [182/192], loss=81.9334
	step [183/192], loss=89.2839
	step [184/192], loss=82.8547
	step [185/192], loss=74.3383
	step [186/192], loss=68.4163
	step [187/192], loss=75.4056
	step [188/192], loss=81.1713
	step [189/192], loss=78.4095
	step [190/192], loss=79.7153
	step [191/192], loss=101.0633
	step [192/192], loss=8.8122
	Evaluating
	loss=0.0122, precision=0.3732, recall=0.9041, f1=0.5283
Training epoch 27
	step [1/192], loss=87.7702
	step [2/192], loss=68.1621
	step [3/192], loss=81.6252
	step [4/192], loss=86.3338
	step [5/192], loss=80.4527
	step [6/192], loss=72.9451
	step [7/192], loss=77.2529
	step [8/192], loss=92.3692
	step [9/192], loss=86.0730
	step [10/192], loss=95.6726
	step [11/192], loss=89.3959
	step [12/192], loss=79.3225
	step [13/192], loss=96.0141
	step [14/192], loss=85.4734
	step [15/192], loss=83.2102
	step [16/192], loss=75.6028
	step [17/192], loss=82.8556
	step [18/192], loss=83.4396
	step [19/192], loss=71.8419
	step [20/192], loss=77.4328
	step [21/192], loss=87.4218
	step [22/192], loss=85.9281
	step [23/192], loss=91.4491
	step [24/192], loss=79.1147
	step [25/192], loss=88.6099
	step [26/192], loss=101.4289
	step [27/192], loss=88.8838
	step [28/192], loss=85.7398
	step [29/192], loss=70.1877
	step [30/192], loss=83.4737
	step [31/192], loss=93.0920
	step [32/192], loss=74.9608
	step [33/192], loss=102.7995
	step [34/192], loss=85.8946
	step [35/192], loss=77.0280
	step [36/192], loss=82.7933
	step [37/192], loss=91.4298
	step [38/192], loss=75.4024
	step [39/192], loss=88.5128
	step [40/192], loss=91.6988
	step [41/192], loss=78.3455
	step [42/192], loss=98.5738
	step [43/192], loss=96.9904
	step [44/192], loss=75.6805
	step [45/192], loss=78.2013
	step [46/192], loss=91.1186
	step [47/192], loss=92.7448
	step [48/192], loss=82.3521
	step [49/192], loss=87.1727
	step [50/192], loss=95.9194
	step [51/192], loss=84.3721
	step [52/192], loss=88.9931
	step [53/192], loss=75.0341
	step [54/192], loss=95.9255
	step [55/192], loss=78.6786
	step [56/192], loss=88.5311
	step [57/192], loss=80.4042
	step [58/192], loss=87.4249
	step [59/192], loss=70.8775
	step [60/192], loss=79.1304
	step [61/192], loss=70.8002
	step [62/192], loss=93.9922
	step [63/192], loss=77.2181
	step [64/192], loss=73.9683
	step [65/192], loss=76.8291
	step [66/192], loss=78.4900
	step [67/192], loss=93.3953
	step [68/192], loss=96.0941
	step [69/192], loss=83.1292
	step [70/192], loss=91.6471
	step [71/192], loss=93.5472
	step [72/192], loss=87.3422
	step [73/192], loss=75.6310
	step [74/192], loss=90.4742
	step [75/192], loss=86.6863
	step [76/192], loss=82.5104
	step [77/192], loss=85.6900
	step [78/192], loss=96.9689
	step [79/192], loss=72.8929
	step [80/192], loss=81.9674
	step [81/192], loss=81.5983
	step [82/192], loss=78.5794
	step [83/192], loss=70.0747
	step [84/192], loss=90.5417
	step [85/192], loss=77.3269
	step [86/192], loss=92.2372
	step [87/192], loss=75.1241
	step [88/192], loss=82.4982
	step [89/192], loss=85.7704
	step [90/192], loss=86.0265
	step [91/192], loss=83.1333
	step [92/192], loss=92.7027
	step [93/192], loss=99.9713
	step [94/192], loss=89.6857
	step [95/192], loss=79.6781
	step [96/192], loss=93.1746
	step [97/192], loss=74.9007
	step [98/192], loss=94.6892
	step [99/192], loss=69.7065
	step [100/192], loss=80.0081
	step [101/192], loss=93.2160
	step [102/192], loss=87.2393
	step [103/192], loss=71.2404
	step [104/192], loss=85.9850
	step [105/192], loss=72.5256
	step [106/192], loss=87.7448
	step [107/192], loss=84.7525
	step [108/192], loss=88.5775
	step [109/192], loss=93.2790
	step [110/192], loss=86.5504
	step [111/192], loss=96.8470
	step [112/192], loss=95.0126
	step [113/192], loss=83.1936
	step [114/192], loss=93.1510
	step [115/192], loss=81.6066
	step [116/192], loss=95.1383
	step [117/192], loss=87.3576
	step [118/192], loss=82.9461
	step [119/192], loss=93.2692
	step [120/192], loss=88.4764
	step [121/192], loss=77.7231
	step [122/192], loss=78.3157
	step [123/192], loss=76.4654
	step [124/192], loss=84.0284
	step [125/192], loss=92.8346
	step [126/192], loss=91.0258
	step [127/192], loss=91.0548
	step [128/192], loss=85.1864
	step [129/192], loss=78.9100
	step [130/192], loss=81.1486
	step [131/192], loss=76.8809
	step [132/192], loss=93.8233
	step [133/192], loss=77.3207
	step [134/192], loss=87.7602
	step [135/192], loss=76.8672
	step [136/192], loss=80.8805
	step [137/192], loss=75.6783
	step [138/192], loss=81.1420
	step [139/192], loss=79.3682
	step [140/192], loss=89.1712
	step [141/192], loss=87.5105
	step [142/192], loss=97.0251
	step [143/192], loss=88.8788
	step [144/192], loss=87.5735
	step [145/192], loss=86.1637
	step [146/192], loss=88.4230
	step [147/192], loss=86.8908
	step [148/192], loss=83.4411
	step [149/192], loss=98.1283
	step [150/192], loss=88.7119
	step [151/192], loss=94.1109
	step [152/192], loss=93.3414
	step [153/192], loss=82.0896
	step [154/192], loss=83.0413
	step [155/192], loss=90.8459
	step [156/192], loss=91.7891
	step [157/192], loss=68.5300
	step [158/192], loss=74.2981
	step [159/192], loss=70.8041
	step [160/192], loss=76.6210
	step [161/192], loss=93.9112
	step [162/192], loss=91.5384
	step [163/192], loss=103.5962
	step [164/192], loss=83.9328
	step [165/192], loss=83.0763
	step [166/192], loss=75.8125
	step [167/192], loss=83.2138
	step [168/192], loss=79.8061
	step [169/192], loss=89.7682
	step [170/192], loss=86.4106
	step [171/192], loss=95.9785
	step [172/192], loss=77.8061
	step [173/192], loss=92.3957
	step [174/192], loss=86.0220
	step [175/192], loss=86.1013
	step [176/192], loss=78.6278
	step [177/192], loss=63.3912
	step [178/192], loss=88.1144
	step [179/192], loss=90.9814
	step [180/192], loss=77.6371
	step [181/192], loss=82.4193
	step [182/192], loss=86.3729
	step [183/192], loss=65.5337
	step [184/192], loss=74.9667
	step [185/192], loss=87.2987
	step [186/192], loss=89.9612
	step [187/192], loss=85.5517
	step [188/192], loss=70.5355
	step [189/192], loss=83.2246
	step [190/192], loss=96.2779
	step [191/192], loss=76.3975
	step [192/192], loss=9.9907
	Evaluating
	loss=0.0124, precision=0.3436, recall=0.9141, f1=0.4995
Training epoch 28
	step [1/192], loss=91.2938
	step [2/192], loss=77.3621
	step [3/192], loss=86.6380
	step [4/192], loss=94.0273
	step [5/192], loss=73.9467
	step [6/192], loss=83.1209
	step [7/192], loss=89.6129
	step [8/192], loss=92.7815
	step [9/192], loss=99.4810
	step [10/192], loss=91.2517
	step [11/192], loss=85.7672
	step [12/192], loss=93.7973
	step [13/192], loss=101.0892
	step [14/192], loss=89.4641
	step [15/192], loss=77.8161
	step [16/192], loss=86.8531
	step [17/192], loss=73.2365
	step [18/192], loss=87.9286
	step [19/192], loss=83.0145
	step [20/192], loss=71.5629
	step [21/192], loss=101.9251
	step [22/192], loss=84.9240
	step [23/192], loss=77.5776
	step [24/192], loss=75.9309
	step [25/192], loss=86.3561
	step [26/192], loss=70.3876
	step [27/192], loss=86.7729
	step [28/192], loss=73.1280
	step [29/192], loss=81.1020
	step [30/192], loss=72.7163
	step [31/192], loss=89.9728
	step [32/192], loss=89.8904
	step [33/192], loss=89.8375
	step [34/192], loss=88.2981
	step [35/192], loss=80.7325
	step [36/192], loss=84.1068
	step [37/192], loss=95.0352
	step [38/192], loss=77.7727
	step [39/192], loss=83.2991
	step [40/192], loss=99.2643
	step [41/192], loss=76.6556
	step [42/192], loss=85.4278
	step [43/192], loss=79.8782
	step [44/192], loss=79.1352
	step [45/192], loss=92.1348
	step [46/192], loss=68.7386
	step [47/192], loss=76.4210
	step [48/192], loss=106.4918
	step [49/192], loss=76.3167
	step [50/192], loss=97.4648
	step [51/192], loss=90.9334
	step [52/192], loss=84.0558
	step [53/192], loss=81.7746
	step [54/192], loss=104.6436
	step [55/192], loss=91.5115
	step [56/192], loss=87.7837
	step [57/192], loss=89.1302
	step [58/192], loss=95.6955
	step [59/192], loss=74.1731
	step [60/192], loss=88.9099
	step [61/192], loss=83.5167
	step [62/192], loss=75.6124
	step [63/192], loss=82.1023
	step [64/192], loss=83.6269
	step [65/192], loss=90.2297
	step [66/192], loss=94.5093
	step [67/192], loss=76.3389
	step [68/192], loss=86.9851
	step [69/192], loss=87.4193
	step [70/192], loss=91.1842
	step [71/192], loss=88.8729
	step [72/192], loss=88.2432
	step [73/192], loss=84.6021
	step [74/192], loss=74.7809
	step [75/192], loss=74.2487
	step [76/192], loss=77.9822
	step [77/192], loss=76.3522
	step [78/192], loss=77.6591
	step [79/192], loss=99.8785
	step [80/192], loss=82.2249
	step [81/192], loss=81.7372
	step [82/192], loss=91.6180
	step [83/192], loss=91.5638
	step [84/192], loss=83.0326
	step [85/192], loss=77.9146
	step [86/192], loss=90.7110
	step [87/192], loss=76.6463
	step [88/192], loss=93.6585
	step [89/192], loss=86.4786
	step [90/192], loss=82.2092
	step [91/192], loss=86.7229
	step [92/192], loss=75.7384
	step [93/192], loss=97.2708
	step [94/192], loss=75.6218
	step [95/192], loss=84.9038
	step [96/192], loss=86.5234
	step [97/192], loss=79.1207
	step [98/192], loss=77.9916
	step [99/192], loss=79.8231
	step [100/192], loss=86.1074
	step [101/192], loss=74.1716
	step [102/192], loss=91.3311
	step [103/192], loss=83.3742
	step [104/192], loss=93.9369
	step [105/192], loss=81.3899
	step [106/192], loss=83.8108
	step [107/192], loss=86.0932
	step [108/192], loss=90.1978
	step [109/192], loss=78.4357
	step [110/192], loss=72.9931
	step [111/192], loss=85.8530
	step [112/192], loss=80.3239
	step [113/192], loss=85.8596
	step [114/192], loss=80.3556
	step [115/192], loss=76.2439
	step [116/192], loss=87.9833
	step [117/192], loss=85.6140
	step [118/192], loss=94.5355
	step [119/192], loss=86.5119
	step [120/192], loss=87.2912
	step [121/192], loss=81.8787
	step [122/192], loss=88.7008
	step [123/192], loss=84.0973
	step [124/192], loss=73.1223
	step [125/192], loss=98.0988
	step [126/192], loss=83.8364
	step [127/192], loss=78.2878
	step [128/192], loss=74.4149
	step [129/192], loss=95.4290
	step [130/192], loss=67.9376
	step [131/192], loss=73.2121
	step [132/192], loss=84.0950
	step [133/192], loss=80.8284
	step [134/192], loss=78.2216
	step [135/192], loss=80.2906
	step [136/192], loss=85.8089
	step [137/192], loss=89.9150
	step [138/192], loss=100.1820
	step [139/192], loss=97.1618
	step [140/192], loss=86.5755
	step [141/192], loss=86.9880
	step [142/192], loss=79.4839
	step [143/192], loss=90.4149
	step [144/192], loss=67.9836
	step [145/192], loss=87.1512
	step [146/192], loss=65.5500
	step [147/192], loss=84.0654
	step [148/192], loss=79.2678
	step [149/192], loss=80.7391
	step [150/192], loss=88.1717
	step [151/192], loss=67.8704
	step [152/192], loss=78.8419
	step [153/192], loss=84.9395
	step [154/192], loss=75.1987
	step [155/192], loss=81.9729
	step [156/192], loss=81.3036
	step [157/192], loss=75.9395
	step [158/192], loss=80.0482
	step [159/192], loss=86.1648
	step [160/192], loss=77.1928
	step [161/192], loss=75.5770
	step [162/192], loss=68.6328
	step [163/192], loss=82.3538
	step [164/192], loss=77.9353
	step [165/192], loss=75.1667
	step [166/192], loss=79.4365
	step [167/192], loss=76.1825
	step [168/192], loss=82.7095
	step [169/192], loss=88.5672
	step [170/192], loss=89.9329
	step [171/192], loss=96.7945
	step [172/192], loss=74.1748
	step [173/192], loss=73.2872
	step [174/192], loss=85.5192
	step [175/192], loss=85.8299
	step [176/192], loss=94.9610
	step [177/192], loss=91.0856
	step [178/192], loss=79.5640
	step [179/192], loss=80.5218
	step [180/192], loss=83.2525
	step [181/192], loss=79.5549
	step [182/192], loss=85.1194
	step [183/192], loss=82.3171
	step [184/192], loss=76.7921
	step [185/192], loss=75.2330
	step [186/192], loss=97.1296
	step [187/192], loss=87.7751
	step [188/192], loss=77.8962
	step [189/192], loss=96.1335
	step [190/192], loss=82.4468
	step [191/192], loss=75.4846
	step [192/192], loss=9.2140
	Evaluating
	loss=0.0135, precision=0.3233, recall=0.9087, f1=0.4769
Training epoch 29
	step [1/192], loss=96.4165
	step [2/192], loss=88.9682
	step [3/192], loss=79.7607
	step [4/192], loss=78.1775
	step [5/192], loss=80.1372
	step [6/192], loss=78.8401
	step [7/192], loss=84.2914
	step [8/192], loss=88.3404
	step [9/192], loss=90.2898
	step [10/192], loss=86.0186
	step [11/192], loss=70.5602
	step [12/192], loss=89.2624
	step [13/192], loss=90.7749
	step [14/192], loss=82.6811
	step [15/192], loss=89.5970
	step [16/192], loss=72.2996
	step [17/192], loss=90.9920
	step [18/192], loss=97.1732
	step [19/192], loss=81.3643
	step [20/192], loss=84.5760
	step [21/192], loss=81.2265
	step [22/192], loss=88.4345
	step [23/192], loss=82.3606
	step [24/192], loss=103.0245
	step [25/192], loss=87.1618
	step [26/192], loss=77.0188
	step [27/192], loss=84.7281
	step [28/192], loss=91.5846
	step [29/192], loss=67.0325
	step [30/192], loss=88.4816
	step [31/192], loss=85.8101
	step [32/192], loss=90.5999
	step [33/192], loss=74.8875
	step [34/192], loss=100.7437
	step [35/192], loss=85.6925
	step [36/192], loss=78.4791
	step [37/192], loss=81.4255
	step [38/192], loss=85.5940
	step [39/192], loss=93.0978
	step [40/192], loss=85.7898
	step [41/192], loss=64.2112
	step [42/192], loss=97.1451
	step [43/192], loss=88.7511
	step [44/192], loss=83.5073
	step [45/192], loss=75.8788
	step [46/192], loss=80.5019
	step [47/192], loss=83.8200
	step [48/192], loss=78.9204
	step [49/192], loss=77.5765
	step [50/192], loss=89.4730
	step [51/192], loss=84.0628
	step [52/192], loss=95.4596
	step [53/192], loss=75.2768
	step [54/192], loss=93.3392
	step [55/192], loss=81.1478
	step [56/192], loss=85.1308
	step [57/192], loss=88.7322
	step [58/192], loss=95.4028
	step [59/192], loss=67.2165
	step [60/192], loss=96.6098
	step [61/192], loss=74.0742
	step [62/192], loss=73.9758
	step [63/192], loss=79.8282
	step [64/192], loss=78.7863
	step [65/192], loss=73.3491
	step [66/192], loss=83.5193
	step [67/192], loss=85.7538
	step [68/192], loss=81.4855
	step [69/192], loss=72.7693
	step [70/192], loss=83.5683
	step [71/192], loss=83.2223
	step [72/192], loss=100.1304
	step [73/192], loss=68.4444
	step [74/192], loss=74.7119
	step [75/192], loss=82.5667
	step [76/192], loss=80.0178
	step [77/192], loss=77.5469
	step [78/192], loss=95.5031
	step [79/192], loss=78.8030
	step [80/192], loss=95.6131
	step [81/192], loss=91.9090
	step [82/192], loss=86.8637
	step [83/192], loss=90.2763
	step [84/192], loss=87.6447
	step [85/192], loss=89.5817
	step [86/192], loss=75.8318
	step [87/192], loss=73.8867
	step [88/192], loss=90.2506
	step [89/192], loss=73.9507
	step [90/192], loss=78.6750
	step [91/192], loss=76.3719
	step [92/192], loss=86.6532
	step [93/192], loss=99.5475
	step [94/192], loss=76.8961
	step [95/192], loss=69.8671
	step [96/192], loss=93.8080
	step [97/192], loss=92.9882
	step [98/192], loss=69.4752
	step [99/192], loss=78.1412
	step [100/192], loss=91.5860
	step [101/192], loss=73.8436
	step [102/192], loss=86.5129
	step [103/192], loss=76.3981
	step [104/192], loss=81.0570
	step [105/192], loss=69.3501
	step [106/192], loss=86.7708
	step [107/192], loss=86.3410
	step [108/192], loss=77.9240
	step [109/192], loss=62.2577
	step [110/192], loss=79.9466
	step [111/192], loss=91.0869
	step [112/192], loss=83.2998
	step [113/192], loss=72.8653
	step [114/192], loss=87.2147
	step [115/192], loss=79.1496
	step [116/192], loss=82.4577
	step [117/192], loss=80.9297
	step [118/192], loss=61.0900
	step [119/192], loss=95.3245
	step [120/192], loss=77.6313
	step [121/192], loss=69.4191
	step [122/192], loss=82.6205
	step [123/192], loss=90.5149
	step [124/192], loss=88.1012
	step [125/192], loss=78.0645
	step [126/192], loss=93.3119
	step [127/192], loss=85.3972
	step [128/192], loss=91.4270
	step [129/192], loss=74.9552
	step [130/192], loss=76.1683
	step [131/192], loss=84.9197
	step [132/192], loss=62.7266
	step [133/192], loss=80.5630
	step [134/192], loss=81.7050
	step [135/192], loss=82.3622
	step [136/192], loss=90.0938
	step [137/192], loss=90.5234
	step [138/192], loss=80.0340
	step [139/192], loss=59.1865
	step [140/192], loss=111.3424
	step [141/192], loss=86.1109
	step [142/192], loss=78.9667
	step [143/192], loss=83.8649
	step [144/192], loss=92.5985
	step [145/192], loss=84.5382
	step [146/192], loss=77.8263
	step [147/192], loss=84.6134
	step [148/192], loss=92.4126
	step [149/192], loss=83.7719
	step [150/192], loss=91.9366
	step [151/192], loss=89.1292
	step [152/192], loss=84.7391
	step [153/192], loss=85.7796
	step [154/192], loss=88.1971
	step [155/192], loss=90.9468
	step [156/192], loss=86.8628
	step [157/192], loss=79.1920
	step [158/192], loss=83.0409
	step [159/192], loss=77.7447
	step [160/192], loss=86.2365
	step [161/192], loss=79.6400
	step [162/192], loss=93.3907
	step [163/192], loss=81.1536
	step [164/192], loss=91.6921
	step [165/192], loss=84.7214
	step [166/192], loss=71.1705
	step [167/192], loss=82.5184
	step [168/192], loss=78.8977
	step [169/192], loss=79.4001
	step [170/192], loss=92.7895
	step [171/192], loss=77.7853
	step [172/192], loss=79.0113
	step [173/192], loss=85.9940
	step [174/192], loss=82.5357
	step [175/192], loss=83.3182
	step [176/192], loss=70.0531
	step [177/192], loss=77.9818
	step [178/192], loss=85.0364
	step [179/192], loss=95.2269
	step [180/192], loss=79.7856
	step [181/192], loss=88.2020
	step [182/192], loss=60.0263
	step [183/192], loss=91.6648
	step [184/192], loss=78.1924
	step [185/192], loss=63.3212
	step [186/192], loss=75.5666
	step [187/192], loss=81.5514
	step [188/192], loss=74.4027
	step [189/192], loss=77.6989
	step [190/192], loss=73.1691
	step [191/192], loss=87.5694
	step [192/192], loss=14.6482
	Evaluating
	loss=0.0086, precision=0.4576, recall=0.8926, f1=0.6050
saving model as: 0_saved_model.pth
Training epoch 30
	step [1/192], loss=83.2659
	step [2/192], loss=83.1870
	step [3/192], loss=73.4324
	step [4/192], loss=83.0318
	step [5/192], loss=81.6900
	step [6/192], loss=75.4156
	step [7/192], loss=83.4759
	step [8/192], loss=89.4144
	step [9/192], loss=81.5794
	step [10/192], loss=70.1401
	step [11/192], loss=87.2390
	step [12/192], loss=87.3208
	step [13/192], loss=81.9733
	step [14/192], loss=85.1398
	step [15/192], loss=81.5320
	step [16/192], loss=83.7005
	step [17/192], loss=85.8382
	step [18/192], loss=85.6022
	step [19/192], loss=96.7568
	step [20/192], loss=79.3672
	step [21/192], loss=96.7031
	step [22/192], loss=76.2294
	step [23/192], loss=82.8797
	step [24/192], loss=70.3314
	step [25/192], loss=76.7850
	step [26/192], loss=79.0493
	step [27/192], loss=77.3001
	step [28/192], loss=80.0077
	step [29/192], loss=87.1577
	step [30/192], loss=89.4314
	step [31/192], loss=84.8593
	step [32/192], loss=81.0821
	step [33/192], loss=100.2604
	step [34/192], loss=84.7031
	step [35/192], loss=91.7045
	step [36/192], loss=85.3802
	step [37/192], loss=68.5782
	step [38/192], loss=86.4828
	step [39/192], loss=82.5316
	step [40/192], loss=91.3639
	step [41/192], loss=79.7199
	step [42/192], loss=77.5743
	step [43/192], loss=81.4009
	step [44/192], loss=99.7328
	step [45/192], loss=78.7610
	step [46/192], loss=83.6386
	step [47/192], loss=81.7934
	step [48/192], loss=74.8721
	step [49/192], loss=73.1831
	step [50/192], loss=89.1732
	step [51/192], loss=75.2764
	step [52/192], loss=75.5297
	step [53/192], loss=88.6293
	step [54/192], loss=85.4926
	step [55/192], loss=76.4476
	step [56/192], loss=62.8376
	step [57/192], loss=82.4974
	step [58/192], loss=77.1893
	step [59/192], loss=74.4885
	step [60/192], loss=79.7358
	step [61/192], loss=94.6001
	step [62/192], loss=87.1623
	step [63/192], loss=83.3433
	step [64/192], loss=80.4762
	step [65/192], loss=79.6708
	step [66/192], loss=82.1485
	step [67/192], loss=77.8416
	step [68/192], loss=63.0263
	step [69/192], loss=81.0945
	step [70/192], loss=78.3721
	step [71/192], loss=88.1235
	step [72/192], loss=81.9018
	step [73/192], loss=85.3261
	step [74/192], loss=75.6587
	step [75/192], loss=74.3630
	step [76/192], loss=83.2087
	step [77/192], loss=92.4477
	step [78/192], loss=79.8046
	step [79/192], loss=89.2584
	step [80/192], loss=84.4516
	step [81/192], loss=84.9370
	step [82/192], loss=80.7199
	step [83/192], loss=88.8940
	step [84/192], loss=81.9267
	step [85/192], loss=75.1864
	step [86/192], loss=76.2866
	step [87/192], loss=89.8156
	step [88/192], loss=80.1946
	step [89/192], loss=72.1916
	step [90/192], loss=85.5032
	step [91/192], loss=77.5995
	step [92/192], loss=93.4266
	step [93/192], loss=71.7052
	step [94/192], loss=102.0604
	step [95/192], loss=90.0246
	step [96/192], loss=77.7415
	step [97/192], loss=70.2468
	step [98/192], loss=78.5996
	step [99/192], loss=72.4375
	step [100/192], loss=69.2181
	step [101/192], loss=79.4012
	step [102/192], loss=86.3368
	step [103/192], loss=67.0903
	step [104/192], loss=68.2581
	step [105/192], loss=85.9932
	step [106/192], loss=81.2222
	step [107/192], loss=67.5826
	step [108/192], loss=73.4767
	step [109/192], loss=92.7598
	step [110/192], loss=88.0714
	step [111/192], loss=73.7933
	step [112/192], loss=82.8231
	step [113/192], loss=84.6026
	step [114/192], loss=95.5772
	step [115/192], loss=86.4706
	step [116/192], loss=88.1561
	step [117/192], loss=86.8147
	step [118/192], loss=78.2123
	step [119/192], loss=71.9652
	step [120/192], loss=77.1384
	step [121/192], loss=97.6842
	step [122/192], loss=69.5056
	step [123/192], loss=78.3693
	step [124/192], loss=78.2749
	step [125/192], loss=73.3509
	step [126/192], loss=95.5865
	step [127/192], loss=82.2685
	step [128/192], loss=90.2459
	step [129/192], loss=82.5737
	step [130/192], loss=80.4952
	step [131/192], loss=69.9881
	step [132/192], loss=83.8251
	step [133/192], loss=84.2673
	step [134/192], loss=84.2814
	step [135/192], loss=82.2628
	step [136/192], loss=88.1325
	step [137/192], loss=79.0125
	step [138/192], loss=92.5859
	step [139/192], loss=74.2835
	step [140/192], loss=80.9097
	step [141/192], loss=81.0204
	step [142/192], loss=67.9832
	step [143/192], loss=81.1462
	step [144/192], loss=88.3330
	step [145/192], loss=80.9658
	step [146/192], loss=103.9800
	step [147/192], loss=94.2358
	step [148/192], loss=85.0997
	step [149/192], loss=86.4990
	step [150/192], loss=80.3144
	step [151/192], loss=77.6041
	step [152/192], loss=77.4386
	step [153/192], loss=89.6213
	step [154/192], loss=78.8608
	step [155/192], loss=95.8932
	step [156/192], loss=95.5739
	step [157/192], loss=91.2163
	step [158/192], loss=83.2751
	step [159/192], loss=89.6265
	step [160/192], loss=79.5547
	step [161/192], loss=80.2444
	step [162/192], loss=97.0970
	step [163/192], loss=85.5264
	step [164/192], loss=95.2255
	step [165/192], loss=68.3672
	step [166/192], loss=70.0229
	step [167/192], loss=72.7855
	step [168/192], loss=90.0987
	step [169/192], loss=79.9312
	step [170/192], loss=91.1549
	step [171/192], loss=79.0133
	step [172/192], loss=95.3040
	step [173/192], loss=89.0761
	step [174/192], loss=89.3351
	step [175/192], loss=94.2138
	step [176/192], loss=91.1198
	step [177/192], loss=82.4410
	step [178/192], loss=85.7731
	step [179/192], loss=76.9428
	step [180/192], loss=68.1647
	step [181/192], loss=75.2002
	step [182/192], loss=94.7151
	step [183/192], loss=73.9401
	step [184/192], loss=81.7554
	step [185/192], loss=101.0919
	step [186/192], loss=71.7736
	step [187/192], loss=85.0870
	step [188/192], loss=99.6841
	step [189/192], loss=63.6366
	step [190/192], loss=72.1689
	step [191/192], loss=78.3633
	step [192/192], loss=11.5539
	Evaluating
	loss=0.0098, precision=0.3983, recall=0.9130, f1=0.5546
Training finished
best_f1: 0.6050422305477697
directing: X rim_enhanced: True test_id 1
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12135 # all weight files in weight_dir: 2708 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/143], loss=545.0430
	step [2/143], loss=469.0046
	step [3/143], loss=413.3242
	step [4/143], loss=354.8214
	step [5/143], loss=408.2067
	step [6/143], loss=306.5111
	step [7/143], loss=331.2467
	step [8/143], loss=318.9848
	step [9/143], loss=305.5521
	step [10/143], loss=335.4554
	step [11/143], loss=283.2834
	step [12/143], loss=318.2364
	step [13/143], loss=320.8425
	step [14/143], loss=311.2822
	step [15/143], loss=306.6021
	step [16/143], loss=287.3414
	step [17/143], loss=281.6766
	step [18/143], loss=283.1154
	step [19/143], loss=271.5193
	step [20/143], loss=306.0783
	step [21/143], loss=259.1828
	step [22/143], loss=264.5763
	step [23/143], loss=259.5211
	step [24/143], loss=286.7489
	step [25/143], loss=261.5391
	step [26/143], loss=278.7022
	step [27/143], loss=260.7254
	step [28/143], loss=267.8787
	step [29/143], loss=290.4206
	step [30/143], loss=279.6291
	step [31/143], loss=249.5869
	step [32/143], loss=270.6272
	step [33/143], loss=246.0425
	step [34/143], loss=272.8164
	step [35/143], loss=270.0041
	step [36/143], loss=272.1855
	step [37/143], loss=245.6973
	step [38/143], loss=235.8368
	step [39/143], loss=245.4596
	step [40/143], loss=262.0211
	step [41/143], loss=251.5596
	step [42/143], loss=231.9398
	step [43/143], loss=270.7114
	step [44/143], loss=254.4557
	step [45/143], loss=246.9905
	step [46/143], loss=245.2346
	step [47/143], loss=226.7539
	step [48/143], loss=236.7876
	step [49/143], loss=213.9355
	step [50/143], loss=258.0894
	step [51/143], loss=250.9502
	step [52/143], loss=243.2560
	step [53/143], loss=231.3848
	step [54/143], loss=227.6711
	step [55/143], loss=216.4970
	step [56/143], loss=236.7020
	step [57/143], loss=222.9335
	step [58/143], loss=238.6058
	step [59/143], loss=224.3491
	step [60/143], loss=219.1795
	step [61/143], loss=237.8248
	step [62/143], loss=240.8474
	step [63/143], loss=248.7720
	step [64/143], loss=248.4127
	step [65/143], loss=253.4514
	step [66/143], loss=234.7208
	step [67/143], loss=239.3045
	step [68/143], loss=228.9775
	step [69/143], loss=241.3093
	step [70/143], loss=232.6951
	step [71/143], loss=228.1122
	step [72/143], loss=221.7124
	step [73/143], loss=220.0799
	step [74/143], loss=222.3625
	step [75/143], loss=243.3098
	step [76/143], loss=230.9012
	step [77/143], loss=221.3561
	step [78/143], loss=227.1276
	step [79/143], loss=220.4920
	step [80/143], loss=210.3360
	step [81/143], loss=201.0526
	step [82/143], loss=233.4793
	step [83/143], loss=220.8897
	step [84/143], loss=216.8110
	step [85/143], loss=221.2631
	step [86/143], loss=209.1975
	step [87/143], loss=230.1626
	step [88/143], loss=213.8761
	step [89/143], loss=218.1965
	step [90/143], loss=246.5219
	step [91/143], loss=237.1004
	step [92/143], loss=224.6118
	step [93/143], loss=213.0961
	step [94/143], loss=218.8228
	step [95/143], loss=226.8137
	step [96/143], loss=221.0985
	step [97/143], loss=213.9244
	step [98/143], loss=198.6570
	step [99/143], loss=211.4428
	step [100/143], loss=214.5819
	step [101/143], loss=207.3207
	step [102/143], loss=217.3544
	step [103/143], loss=210.3983
	step [104/143], loss=202.2337
	step [105/143], loss=202.7959
	step [106/143], loss=220.1281
	step [107/143], loss=217.6687
	step [108/143], loss=191.9720
	step [109/143], loss=207.1088
	step [110/143], loss=203.5063
	step [111/143], loss=217.9758
	step [112/143], loss=223.4353
	step [113/143], loss=211.7356
	step [114/143], loss=216.7405
	step [115/143], loss=206.4770
	step [116/143], loss=214.7553
	step [117/143], loss=202.2498
	step [118/143], loss=226.9007
	step [119/143], loss=212.0547
	step [120/143], loss=196.8128
	step [121/143], loss=203.9846
	step [122/143], loss=192.0071
	step [123/143], loss=218.0519
	step [124/143], loss=222.9445
	step [125/143], loss=194.6677
	step [126/143], loss=215.0979
	step [127/143], loss=232.3896
	step [128/143], loss=211.4206
	step [129/143], loss=181.9967
	step [130/143], loss=229.8806
	step [131/143], loss=216.7270
	step [132/143], loss=210.6605
	step [133/143], loss=223.1812
	step [134/143], loss=212.2976
	step [135/143], loss=214.2713
	step [136/143], loss=203.0137
	step [137/143], loss=220.4877
	step [138/143], loss=198.2816
	step [139/143], loss=204.8083
	step [140/143], loss=206.5277
	step [141/143], loss=179.6630
	step [142/143], loss=226.9299
	step [143/143], loss=169.6841
	Evaluating
	loss=0.4464, precision=0.0806, recall=0.9037, f1=0.1481
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/143], loss=190.4459
	step [2/143], loss=195.7261
	step [3/143], loss=207.8493
	step [4/143], loss=202.0142
	step [5/143], loss=220.7732
	step [6/143], loss=188.5790
	step [7/143], loss=197.5800
	step [8/143], loss=207.3992
	step [9/143], loss=203.4745
	step [10/143], loss=223.8390
	step [11/143], loss=202.5676
	step [12/143], loss=224.6309
	step [13/143], loss=221.2791
	step [14/143], loss=191.8207
	step [15/143], loss=179.0149
	step [16/143], loss=194.7378
	step [17/143], loss=175.9902
	step [18/143], loss=223.8359
	step [19/143], loss=200.9562
	step [20/143], loss=200.7938
	step [21/143], loss=201.9616
	step [22/143], loss=214.6731
	step [23/143], loss=193.9311
	step [24/143], loss=196.1694
	step [25/143], loss=203.5963
	step [26/143], loss=182.7318
	step [27/143], loss=193.7837
	step [28/143], loss=206.8022
	step [29/143], loss=208.2796
	step [30/143], loss=193.8388
	step [31/143], loss=204.2086
	step [32/143], loss=194.2610
	step [33/143], loss=194.2744
	step [34/143], loss=185.0358
	step [35/143], loss=188.4066
	step [36/143], loss=176.1010
	step [37/143], loss=182.5242
	step [38/143], loss=195.3787
	step [39/143], loss=196.5337
	step [40/143], loss=192.7486
	step [41/143], loss=192.3694
	step [42/143], loss=183.4447
	step [43/143], loss=209.8137
	step [44/143], loss=212.4176
	step [45/143], loss=177.5521
	step [46/143], loss=166.8671
	step [47/143], loss=185.1397
	step [48/143], loss=179.6467
	step [49/143], loss=181.4001
	step [50/143], loss=185.8630
	step [51/143], loss=197.7911
	step [52/143], loss=174.4802
	step [53/143], loss=188.1577
	step [54/143], loss=196.3321
	step [55/143], loss=180.9591
	step [56/143], loss=208.3185
	step [57/143], loss=216.6059
	step [58/143], loss=190.4503
	step [59/143], loss=204.2242
	step [60/143], loss=172.8805
	step [61/143], loss=192.5373
	step [62/143], loss=209.0910
	step [63/143], loss=200.5880
	step [64/143], loss=195.8130
	step [65/143], loss=193.7567
	step [66/143], loss=182.0610
	step [67/143], loss=203.4227
	step [68/143], loss=212.5002
	step [69/143], loss=181.4281
	step [70/143], loss=181.0524
	step [71/143], loss=176.7680
	step [72/143], loss=199.4290
	step [73/143], loss=176.0194
	step [74/143], loss=179.4287
	step [75/143], loss=177.4550
	step [76/143], loss=174.7398
	step [77/143], loss=185.8347
	step [78/143], loss=193.7064
	step [79/143], loss=187.5406
	step [80/143], loss=184.2397
	step [81/143], loss=199.1260
	step [82/143], loss=188.7305
	step [83/143], loss=177.1505
	step [84/143], loss=208.2588
	step [85/143], loss=196.0997
	step [86/143], loss=181.0562
	step [87/143], loss=203.9785
	step [88/143], loss=167.1621
	step [89/143], loss=201.1665
	step [90/143], loss=179.9845
	step [91/143], loss=188.7302
	step [92/143], loss=162.7793
	step [93/143], loss=177.3285
	step [94/143], loss=206.4860
	step [95/143], loss=197.2347
	step [96/143], loss=174.8671
	step [97/143], loss=174.4636
	step [98/143], loss=183.1774
	step [99/143], loss=159.4432
	step [100/143], loss=190.5484
	step [101/143], loss=188.8062
	step [102/143], loss=186.4542
	step [103/143], loss=197.4465
	step [104/143], loss=179.3861
	step [105/143], loss=176.9197
	step [106/143], loss=190.5997
	step [107/143], loss=178.1216
	step [108/143], loss=210.2198
	step [109/143], loss=189.4127
	step [110/143], loss=162.6721
	step [111/143], loss=187.3873
	step [112/143], loss=191.3999
	step [113/143], loss=188.2549
	step [114/143], loss=190.3185
	step [115/143], loss=184.8235
	step [116/143], loss=186.9779
	step [117/143], loss=169.2072
	step [118/143], loss=179.8938
	step [119/143], loss=151.7371
	step [120/143], loss=186.8374
	step [121/143], loss=174.3702
	step [122/143], loss=182.3224
	step [123/143], loss=178.4843
	step [124/143], loss=185.1576
	step [125/143], loss=177.2941
	step [126/143], loss=171.9626
	step [127/143], loss=185.0464
	step [128/143], loss=184.0813
	step [129/143], loss=170.2368
	step [130/143], loss=191.5252
	step [131/143], loss=161.2502
	step [132/143], loss=188.1275
	step [133/143], loss=173.7074
	step [134/143], loss=165.1061
	step [135/143], loss=183.9084
	step [136/143], loss=182.5207
	step [137/143], loss=163.8095
	step [138/143], loss=163.6613
	step [139/143], loss=179.3068
	step [140/143], loss=165.8588
	step [141/143], loss=171.0544
	step [142/143], loss=183.0657
	step [143/143], loss=158.3233
	Evaluating
	loss=0.3354, precision=0.3629, recall=0.8853, f1=0.5148
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/143], loss=193.4793
	step [2/143], loss=175.5343
	step [3/143], loss=190.5128
	step [4/143], loss=174.5849
	step [5/143], loss=176.9749
	step [6/143], loss=195.2873
	step [7/143], loss=175.9048
	step [8/143], loss=155.8244
	step [9/143], loss=170.1642
	step [10/143], loss=183.4383
	step [11/143], loss=203.8325
	step [12/143], loss=177.9902
	step [13/143], loss=171.9598
	step [14/143], loss=164.7129
	step [15/143], loss=197.8035
	step [16/143], loss=159.6281
	step [17/143], loss=181.1278
	step [18/143], loss=174.8396
	step [19/143], loss=178.2876
	step [20/143], loss=176.9451
	step [21/143], loss=191.8232
	step [22/143], loss=182.5596
	step [23/143], loss=134.7489
	step [24/143], loss=176.0434
	step [25/143], loss=169.7723
	step [26/143], loss=167.4367
	step [27/143], loss=174.4284
	step [28/143], loss=177.8059
	step [29/143], loss=164.8330
	step [30/143], loss=171.5887
	step [31/143], loss=175.7262
	step [32/143], loss=170.0146
	step [33/143], loss=171.2973
	step [34/143], loss=162.5448
	step [35/143], loss=185.5079
	step [36/143], loss=182.7189
	step [37/143], loss=185.3547
	step [38/143], loss=182.1014
	step [39/143], loss=175.7470
	step [40/143], loss=169.8975
	step [41/143], loss=178.1061
	step [42/143], loss=176.9476
	step [43/143], loss=186.7045
	step [44/143], loss=171.8610
	step [45/143], loss=172.9543
	step [46/143], loss=198.4898
	step [47/143], loss=161.2677
	step [48/143], loss=164.4868
	step [49/143], loss=173.1389
	step [50/143], loss=177.8127
	step [51/143], loss=167.6007
	step [52/143], loss=158.9713
	step [53/143], loss=176.6480
	step [54/143], loss=175.1576
	step [55/143], loss=149.3679
	step [56/143], loss=170.9929
	step [57/143], loss=174.4698
	step [58/143], loss=181.5781
	step [59/143], loss=166.6050
	step [60/143], loss=172.7505
	step [61/143], loss=154.6035
	step [62/143], loss=179.4411
	step [63/143], loss=168.8861
	step [64/143], loss=152.4216
	step [65/143], loss=154.7530
	step [66/143], loss=154.4775
	step [67/143], loss=161.5031
	step [68/143], loss=160.0206
	step [69/143], loss=138.5448
	step [70/143], loss=162.9530
	step [71/143], loss=165.2505
	step [72/143], loss=174.4253
	step [73/143], loss=181.3300
	step [74/143], loss=168.3213
	step [75/143], loss=162.2804
	step [76/143], loss=154.8714
	step [77/143], loss=159.8389
	step [78/143], loss=185.5404
	step [79/143], loss=169.2920
	step [80/143], loss=175.6814
	step [81/143], loss=188.4889
	step [82/143], loss=154.1766
	step [83/143], loss=148.1042
	step [84/143], loss=154.8381
	step [85/143], loss=165.5918
	step [86/143], loss=160.5219
	step [87/143], loss=166.0540
	step [88/143], loss=161.3427
	step [89/143], loss=156.4308
	step [90/143], loss=166.2939
	step [91/143], loss=171.7071
	step [92/143], loss=176.7910
	step [93/143], loss=154.0678
	step [94/143], loss=168.9492
	step [95/143], loss=159.1425
	step [96/143], loss=182.0185
	step [97/143], loss=158.6472
	step [98/143], loss=175.0386
	step [99/143], loss=152.8018
	step [100/143], loss=160.4987
	step [101/143], loss=159.4938
	step [102/143], loss=165.4919
	step [103/143], loss=152.2961
	step [104/143], loss=200.4229
	step [105/143], loss=184.6376
	step [106/143], loss=175.9083
	step [107/143], loss=152.2494
	step [108/143], loss=170.2310
	step [109/143], loss=173.0503
	step [110/143], loss=154.4310
	step [111/143], loss=159.7016
	step [112/143], loss=160.8358
	step [113/143], loss=148.6071
	step [114/143], loss=162.7890
	step [115/143], loss=155.7537
	step [116/143], loss=160.1910
	step [117/143], loss=144.4680
	step [118/143], loss=179.5941
	step [119/143], loss=154.0710
	step [120/143], loss=163.3544
	step [121/143], loss=148.3248
	step [122/143], loss=177.5656
	step [123/143], loss=149.9767
	step [124/143], loss=141.9245
	step [125/143], loss=145.5431
	step [126/143], loss=162.4058
	step [127/143], loss=163.7396
	step [128/143], loss=152.4705
	step [129/143], loss=179.2411
	step [130/143], loss=173.9450
	step [131/143], loss=140.7718
	step [132/143], loss=159.0135
	step [133/143], loss=146.7318
	step [134/143], loss=148.8933
	step [135/143], loss=165.3624
	step [136/143], loss=167.8369
	step [137/143], loss=149.8334
	step [138/143], loss=176.2446
	step [139/143], loss=148.3722
	step [140/143], loss=139.8203
	step [141/143], loss=168.9853
	step [142/143], loss=149.5413
	step [143/143], loss=130.0401
	Evaluating
	loss=0.2558, precision=0.4525, recall=0.8899, f1=0.5999
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/143], loss=160.3175
	step [2/143], loss=150.5422
	step [3/143], loss=149.9370
	step [4/143], loss=150.4767
	step [5/143], loss=152.1437
	step [6/143], loss=169.7327
	step [7/143], loss=152.0164
	step [8/143], loss=149.8093
	step [9/143], loss=165.6499
	step [10/143], loss=165.4964
	step [11/143], loss=145.0147
	step [12/143], loss=163.3058
	step [13/143], loss=155.9282
	step [14/143], loss=156.0407
	step [15/143], loss=153.5132
	step [16/143], loss=151.3293
	step [17/143], loss=162.3269
	step [18/143], loss=151.4970
	step [19/143], loss=166.6908
	step [20/143], loss=160.2424
	step [21/143], loss=187.2659
	step [22/143], loss=167.5993
	step [23/143], loss=157.8441
	step [24/143], loss=148.2357
	step [25/143], loss=152.7071
	step [26/143], loss=153.1520
	step [27/143], loss=152.9100
	step [28/143], loss=170.9954
	step [29/143], loss=160.3383
	step [30/143], loss=144.4864
	step [31/143], loss=142.3338
	step [32/143], loss=148.7261
	step [33/143], loss=171.8051
	step [34/143], loss=164.0489
	step [35/143], loss=141.8398
	step [36/143], loss=158.7693
	step [37/143], loss=142.7366
	step [38/143], loss=153.7221
	step [39/143], loss=170.8448
	step [40/143], loss=162.2917
	step [41/143], loss=146.6072
	step [42/143], loss=151.0661
	step [43/143], loss=160.1529
	step [44/143], loss=156.6429
	step [45/143], loss=154.0414
	step [46/143], loss=170.7746
	step [47/143], loss=184.2514
	step [48/143], loss=140.9688
	step [49/143], loss=179.7294
	step [50/143], loss=143.1573
	step [51/143], loss=150.5384
	step [52/143], loss=149.0029
	step [53/143], loss=162.8161
	step [54/143], loss=154.7241
	step [55/143], loss=139.7682
	step [56/143], loss=167.1981
	step [57/143], loss=142.9308
	step [58/143], loss=147.9055
	step [59/143], loss=159.3991
	step [60/143], loss=175.9161
	step [61/143], loss=133.9322
	step [62/143], loss=142.6640
	step [63/143], loss=141.4512
	step [64/143], loss=145.1221
	step [65/143], loss=165.7589
	step [66/143], loss=150.2025
	step [67/143], loss=160.0532
	step [68/143], loss=146.5824
	step [69/143], loss=147.6745
	step [70/143], loss=164.3596
	step [71/143], loss=158.0103
	step [72/143], loss=138.5392
	step [73/143], loss=168.0994
	step [74/143], loss=150.3971
	step [75/143], loss=150.4458
	step [76/143], loss=146.5674
	step [77/143], loss=147.7773
	step [78/143], loss=151.8692
	step [79/143], loss=159.9062
	step [80/143], loss=127.5568
	step [81/143], loss=140.7539
	step [82/143], loss=140.0986
	step [83/143], loss=166.6539
	step [84/143], loss=144.5112
	step [85/143], loss=133.9223
	step [86/143], loss=156.9744
	step [87/143], loss=151.5906
	step [88/143], loss=152.3875
	step [89/143], loss=165.1926
	step [90/143], loss=163.9058
	step [91/143], loss=165.3175
	step [92/143], loss=166.2942
	step [93/143], loss=150.3353
	step [94/143], loss=173.1866
	step [95/143], loss=152.5436
	step [96/143], loss=156.6617
	step [97/143], loss=153.1744
	step [98/143], loss=153.6134
	step [99/143], loss=157.8293
	step [100/143], loss=148.9077
	step [101/143], loss=156.5084
	step [102/143], loss=145.7098
	step [103/143], loss=153.8080
	step [104/143], loss=146.5538
	step [105/143], loss=170.7360
	step [106/143], loss=131.1309
	step [107/143], loss=134.5368
	step [108/143], loss=169.1290
	step [109/143], loss=135.1296
	step [110/143], loss=177.6665
	step [111/143], loss=154.2174
	step [112/143], loss=146.3019
	step [113/143], loss=167.8800
	step [114/143], loss=149.0088
	step [115/143], loss=152.4738
	step [116/143], loss=132.2560
	step [117/143], loss=160.8049
	step [118/143], loss=141.4783
	step [119/143], loss=138.4952
	step [120/143], loss=135.5110
	step [121/143], loss=167.5204
	step [122/143], loss=122.9580
	step [123/143], loss=135.2368
	step [124/143], loss=142.7879
	step [125/143], loss=162.3439
	step [126/143], loss=156.6647
	step [127/143], loss=134.8715
	step [128/143], loss=138.0798
	step [129/143], loss=164.5214
	step [130/143], loss=154.2134
	step [131/143], loss=136.4301
	step [132/143], loss=137.1794
	step [133/143], loss=151.4385
	step [134/143], loss=158.7103
	step [135/143], loss=139.5678
	step [136/143], loss=151.9261
	step [137/143], loss=143.6400
	step [138/143], loss=135.8704
	step [139/143], loss=170.3900
	step [140/143], loss=159.7082
	step [141/143], loss=155.9234
	step [142/143], loss=131.8828
	step [143/143], loss=120.8529
	Evaluating
	loss=0.2015, precision=0.4809, recall=0.8812, f1=0.6223
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/143], loss=148.7753
	step [2/143], loss=126.6895
	step [3/143], loss=148.7573
	step [4/143], loss=147.6126
	step [5/143], loss=144.4791
	step [6/143], loss=141.7195
	step [7/143], loss=135.0027
	step [8/143], loss=157.5915
	step [9/143], loss=151.2051
	step [10/143], loss=142.8704
	step [11/143], loss=147.5859
	step [12/143], loss=137.9383
	step [13/143], loss=150.1648
	step [14/143], loss=137.9489
	step [15/143], loss=163.5118
	step [16/143], loss=165.3939
	step [17/143], loss=143.3004
	step [18/143], loss=136.1219
	step [19/143], loss=150.1971
	step [20/143], loss=183.1733
	step [21/143], loss=139.3483
	step [22/143], loss=155.2476
	step [23/143], loss=150.3362
	step [24/143], loss=127.5770
	step [25/143], loss=158.4156
	step [26/143], loss=144.7323
	step [27/143], loss=151.4987
	step [28/143], loss=121.4643
	step [29/143], loss=138.1491
	step [30/143], loss=143.6241
	step [31/143], loss=133.6983
	step [32/143], loss=155.3797
	step [33/143], loss=139.0958
	step [34/143], loss=141.6042
	step [35/143], loss=129.2357
	step [36/143], loss=135.1296
	step [37/143], loss=141.8006
	step [38/143], loss=139.4850
	step [39/143], loss=150.5591
	step [40/143], loss=160.6953
	step [41/143], loss=147.3282
	step [42/143], loss=132.7203
	step [43/143], loss=152.8650
	step [44/143], loss=142.9296
	step [45/143], loss=154.7271
	step [46/143], loss=151.3250
	step [47/143], loss=157.4418
	step [48/143], loss=147.2440
	step [49/143], loss=161.5050
	step [50/143], loss=128.7148
	step [51/143], loss=127.1660
	step [52/143], loss=144.6564
	step [53/143], loss=141.0618
	step [54/143], loss=158.5053
	step [55/143], loss=148.8350
	step [56/143], loss=143.6079
	step [57/143], loss=153.3007
	step [58/143], loss=134.3287
	step [59/143], loss=137.7490
	step [60/143], loss=140.3398
	step [61/143], loss=139.6723
	step [62/143], loss=128.4945
	step [63/143], loss=145.3976
	step [64/143], loss=122.9473
	step [65/143], loss=144.6120
	step [66/143], loss=152.3024
	step [67/143], loss=131.4856
	step [68/143], loss=139.5392
	step [69/143], loss=158.8690
	step [70/143], loss=141.3405
	step [71/143], loss=138.8497
	step [72/143], loss=147.9028
	step [73/143], loss=139.7825
	step [74/143], loss=143.5844
	step [75/143], loss=147.7613
	step [76/143], loss=146.1241
	step [77/143], loss=165.8753
	step [78/143], loss=141.7993
	step [79/143], loss=160.6085
	step [80/143], loss=134.3172
	step [81/143], loss=147.3051
	step [82/143], loss=138.0042
	step [83/143], loss=127.3180
	step [84/143], loss=154.0860
	step [85/143], loss=143.1190
	step [86/143], loss=136.1253
	step [87/143], loss=134.9688
	step [88/143], loss=156.6797
	step [89/143], loss=165.7232
	step [90/143], loss=148.3845
	step [91/143], loss=124.2572
	step [92/143], loss=148.1283
	step [93/143], loss=132.9311
	step [94/143], loss=128.9644
	step [95/143], loss=143.6358
	step [96/143], loss=132.0104
	step [97/143], loss=120.5912
	step [98/143], loss=129.4789
	step [99/143], loss=137.0368
	step [100/143], loss=124.8516
	step [101/143], loss=146.0967
	step [102/143], loss=142.0385
	step [103/143], loss=130.6189
	step [104/143], loss=142.1337
	step [105/143], loss=138.3195
	step [106/143], loss=141.1962
	step [107/143], loss=127.3033
	step [108/143], loss=149.8914
	step [109/143], loss=140.5115
	step [110/143], loss=163.7022
	step [111/143], loss=132.3247
	step [112/143], loss=131.6469
	step [113/143], loss=132.9182
	step [114/143], loss=141.6818
	step [115/143], loss=153.2687
	step [116/143], loss=146.2396
	step [117/143], loss=115.6720
	step [118/143], loss=114.8119
	step [119/143], loss=146.4491
	step [120/143], loss=138.7769
	step [121/143], loss=147.3432
	step [122/143], loss=140.5562
	step [123/143], loss=157.2066
	step [124/143], loss=124.7816
	step [125/143], loss=157.8617
	step [126/143], loss=130.3185
	step [127/143], loss=134.7577
	step [128/143], loss=141.5206
	step [129/143], loss=147.3716
	step [130/143], loss=141.2441
	step [131/143], loss=140.8020
	step [132/143], loss=118.7737
	step [133/143], loss=138.2953
	step [134/143], loss=138.3917
	step [135/143], loss=133.1216
	step [136/143], loss=142.7554
	step [137/143], loss=136.3264
	step [138/143], loss=132.7162
	step [139/143], loss=157.0228
	step [140/143], loss=152.2342
	step [141/143], loss=135.1414
	step [142/143], loss=139.4104
	step [143/143], loss=122.4867
	Evaluating
	loss=0.1626, precision=0.4939, recall=0.8763, f1=0.6317
saving model as: 1_saved_model.pth
Training epoch 6
	step [1/143], loss=147.6700
	step [2/143], loss=146.1968
	step [3/143], loss=146.5563
	step [4/143], loss=132.8039
	step [5/143], loss=129.1665
	step [6/143], loss=132.0503
	step [7/143], loss=132.7394
	step [8/143], loss=128.6111
	step [9/143], loss=152.2558
	step [10/143], loss=139.6424
	step [11/143], loss=123.0278
	step [12/143], loss=142.4885
	step [13/143], loss=146.7979
	step [14/143], loss=136.4501
	step [15/143], loss=124.1187
	step [16/143], loss=138.3719
	step [17/143], loss=144.1101
	step [18/143], loss=151.0658
	step [19/143], loss=143.4705
	step [20/143], loss=144.6094
	step [21/143], loss=110.6156
	step [22/143], loss=148.2711
	step [23/143], loss=145.4794
	step [24/143], loss=152.0866
	step [25/143], loss=129.5006
	step [26/143], loss=125.0485
	step [27/143], loss=141.0394
	step [28/143], loss=125.2353
	step [29/143], loss=155.2003
	step [30/143], loss=136.7014
	step [31/143], loss=131.0431
	step [32/143], loss=123.9967
	step [33/143], loss=156.6671
	step [34/143], loss=133.4755
	step [35/143], loss=126.3779
	step [36/143], loss=128.6323
	step [37/143], loss=129.0848
	step [38/143], loss=117.2839
	step [39/143], loss=129.6662
	step [40/143], loss=140.5222
	step [41/143], loss=129.1288
	step [42/143], loss=131.3735
	step [43/143], loss=128.5997
	step [44/143], loss=160.9713
	step [45/143], loss=145.4795
	step [46/143], loss=137.5977
	step [47/143], loss=143.0567
	step [48/143], loss=126.8486
	step [49/143], loss=132.2221
	step [50/143], loss=142.2195
	step [51/143], loss=142.4918
	step [52/143], loss=129.3836
	step [53/143], loss=126.8047
	step [54/143], loss=158.7040
	step [55/143], loss=136.9714
	step [56/143], loss=142.9607
	step [57/143], loss=137.4069
	step [58/143], loss=129.1479
	step [59/143], loss=136.5247
	step [60/143], loss=114.9732
	step [61/143], loss=132.9602
	step [62/143], loss=127.1873
	step [63/143], loss=124.6784
	step [64/143], loss=136.4220
	step [65/143], loss=136.5559
	step [66/143], loss=109.1051
	step [67/143], loss=137.3351
	step [68/143], loss=131.4795
	step [69/143], loss=134.0858
	step [70/143], loss=148.6104
	step [71/143], loss=128.4684
	step [72/143], loss=112.2196
	step [73/143], loss=127.2575
	step [74/143], loss=127.1667
	step [75/143], loss=114.4720
	step [76/143], loss=148.3194
	step [77/143], loss=136.6797
	step [78/143], loss=146.3042
	step [79/143], loss=150.4666
	step [80/143], loss=149.1616
	step [81/143], loss=141.7422
	step [82/143], loss=144.1146
	step [83/143], loss=120.8056
	step [84/143], loss=156.7757
	step [85/143], loss=150.7711
	step [86/143], loss=122.2202
	step [87/143], loss=124.9953
	step [88/143], loss=125.0256
	step [89/143], loss=145.7832
	step [90/143], loss=157.6691
	step [91/143], loss=148.6388
	step [92/143], loss=141.8541
	step [93/143], loss=145.8648
	step [94/143], loss=124.0080
	step [95/143], loss=134.1209
	step [96/143], loss=150.8366
	step [97/143], loss=146.5815
	step [98/143], loss=115.9995
	step [99/143], loss=121.3173
	step [100/143], loss=125.9117
	step [101/143], loss=138.9051
	step [102/143], loss=134.8914
	step [103/143], loss=147.1551
	step [104/143], loss=131.2772
	step [105/143], loss=127.2649
	step [106/143], loss=118.0536
	step [107/143], loss=135.5544
	step [108/143], loss=155.2141
	step [109/143], loss=129.8946
	step [110/143], loss=137.2717
	step [111/143], loss=112.6264
	step [112/143], loss=136.0324
	step [113/143], loss=137.2155
	step [114/143], loss=113.1348
	step [115/143], loss=117.1055
	step [116/143], loss=137.7673
	step [117/143], loss=117.8625
	step [118/143], loss=163.2721
	step [119/143], loss=129.2087
	step [120/143], loss=159.3390
	step [121/143], loss=139.7768
	step [122/143], loss=145.9130
	step [123/143], loss=126.5892
	step [124/143], loss=149.8419
	step [125/143], loss=123.7815
	step [126/143], loss=121.0408
	step [127/143], loss=124.7992
	step [128/143], loss=133.9825
	step [129/143], loss=128.8355
	step [130/143], loss=149.3732
	step [131/143], loss=112.7720
	step [132/143], loss=131.9873
	step [133/143], loss=123.7313
	step [134/143], loss=129.7568
	step [135/143], loss=127.8343
	step [136/143], loss=147.6717
	step [137/143], loss=122.6278
	step [138/143], loss=133.5522
	step [139/143], loss=132.3401
	step [140/143], loss=123.8480
	step [141/143], loss=134.7961
	step [142/143], loss=136.3445
	step [143/143], loss=103.9680
	Evaluating
	loss=0.1303, precision=0.4926, recall=0.8967, f1=0.6359
saving model as: 1_saved_model.pth
Training epoch 7
	step [1/143], loss=125.1585
	step [2/143], loss=134.3098
	step [3/143], loss=140.2168
	step [4/143], loss=117.5847
	step [5/143], loss=136.5038
	step [6/143], loss=137.1267
	step [7/143], loss=127.1240
	step [8/143], loss=129.7252
	step [9/143], loss=139.0609
	step [10/143], loss=150.6823
	step [11/143], loss=148.3058
	step [12/143], loss=153.9457
	step [13/143], loss=129.2084
	step [14/143], loss=137.5774
	step [15/143], loss=120.4943
	step [16/143], loss=131.9188
	step [17/143], loss=153.9225
	step [18/143], loss=125.7436
	step [19/143], loss=109.3058
	step [20/143], loss=123.7847
	step [21/143], loss=142.3881
	step [22/143], loss=142.0484
	step [23/143], loss=133.0271
	step [24/143], loss=115.9790
	step [25/143], loss=104.8814
	step [26/143], loss=140.2380
	step [27/143], loss=126.7535
	step [28/143], loss=130.4670
	step [29/143], loss=137.2831
	step [30/143], loss=125.9659
	step [31/143], loss=135.7280
	step [32/143], loss=141.0262
	step [33/143], loss=117.0549
	step [34/143], loss=145.5726
	step [35/143], loss=131.2168
	step [36/143], loss=113.2795
	step [37/143], loss=123.8476
	step [38/143], loss=121.4059
	step [39/143], loss=145.3896
	step [40/143], loss=127.8315
	step [41/143], loss=145.1313
	step [42/143], loss=134.5543
	step [43/143], loss=149.0192
	step [44/143], loss=114.0809
	step [45/143], loss=140.7822
	step [46/143], loss=145.9090
	step [47/143], loss=133.5463
	step [48/143], loss=135.1824
	step [49/143], loss=119.6954
	step [50/143], loss=137.3909
	step [51/143], loss=126.9621
	step [52/143], loss=134.4376
	step [53/143], loss=152.1029
	step [54/143], loss=125.2112
	step [55/143], loss=106.0731
	step [56/143], loss=116.7264
	step [57/143], loss=116.5557
	step [58/143], loss=134.6559
	step [59/143], loss=138.8767
	step [60/143], loss=125.6976
	step [61/143], loss=132.8745
	step [62/143], loss=145.3793
	step [63/143], loss=115.4124
	step [64/143], loss=126.9578
	step [65/143], loss=129.4036
	step [66/143], loss=153.8489
	step [67/143], loss=139.7143
	step [68/143], loss=126.9920
	step [69/143], loss=101.6814
	step [70/143], loss=134.3723
	step [71/143], loss=115.8486
	step [72/143], loss=134.8218
	step [73/143], loss=148.8490
	step [74/143], loss=126.9970
	step [75/143], loss=112.3095
	step [76/143], loss=106.7805
	step [77/143], loss=108.7320
	step [78/143], loss=139.9136
	step [79/143], loss=97.0935
	step [80/143], loss=132.5962
	step [81/143], loss=125.9771
	step [82/143], loss=137.0065
	step [83/143], loss=122.9656
	step [84/143], loss=147.9492
	step [85/143], loss=128.8629
	step [86/143], loss=119.0227
	step [87/143], loss=142.1956
	step [88/143], loss=140.1200
	step [89/143], loss=109.3212
	step [90/143], loss=132.4021
	step [91/143], loss=120.4479
	step [92/143], loss=127.1143
	step [93/143], loss=106.8162
	step [94/143], loss=127.4698
	step [95/143], loss=109.9686
	step [96/143], loss=123.3332
	step [97/143], loss=145.4017
	step [98/143], loss=143.5933
	step [99/143], loss=165.8420
	step [100/143], loss=131.1655
	step [101/143], loss=130.2814
	step [102/143], loss=122.0747
	step [103/143], loss=126.7222
	step [104/143], loss=131.5273
	step [105/143], loss=120.6348
	step [106/143], loss=132.6442
	step [107/143], loss=128.1945
	step [108/143], loss=118.3653
	step [109/143], loss=122.1919
	step [110/143], loss=116.7590
	step [111/143], loss=112.6308
	step [112/143], loss=126.1239
	step [113/143], loss=119.7536
	step [114/143], loss=135.5862
	step [115/143], loss=136.0925
	step [116/143], loss=132.8104
	step [117/143], loss=129.7337
	step [118/143], loss=123.5244
	step [119/143], loss=121.1052
	step [120/143], loss=129.2022
	step [121/143], loss=138.9905
	step [122/143], loss=126.9213
	step [123/143], loss=133.9316
	step [124/143], loss=110.9060
	step [125/143], loss=137.7081
	step [126/143], loss=132.8839
	step [127/143], loss=106.8566
	step [128/143], loss=142.0911
	step [129/143], loss=154.1931
	step [130/143], loss=121.5690
	step [131/143], loss=125.0775
	step [132/143], loss=130.1488
	step [133/143], loss=133.4944
	step [134/143], loss=110.2395
	step [135/143], loss=109.2688
	step [136/143], loss=112.7847
	step [137/143], loss=118.8730
	step [138/143], loss=133.2361
	step [139/143], loss=113.5238
	step [140/143], loss=123.8992
	step [141/143], loss=126.1861
	step [142/143], loss=126.4018
	step [143/143], loss=102.0963
	Evaluating
	loss=0.1080, precision=0.4771, recall=0.8885, f1=0.6208
Training epoch 8
	step [1/143], loss=127.8411
	step [2/143], loss=142.7195
	step [3/143], loss=131.2113
	step [4/143], loss=119.4497
	step [5/143], loss=138.0622
	step [6/143], loss=137.8210
	step [7/143], loss=126.8156
	step [8/143], loss=123.3531
	step [9/143], loss=135.8206
	step [10/143], loss=125.9827
	step [11/143], loss=152.1036
	step [12/143], loss=133.2875
	step [13/143], loss=131.5416
	step [14/143], loss=107.3932
	step [15/143], loss=126.3006
	step [16/143], loss=127.0408
	step [17/143], loss=122.1333
	step [18/143], loss=122.3260
	step [19/143], loss=115.8309
	step [20/143], loss=132.3233
	step [21/143], loss=125.4482
	step [22/143], loss=118.4151
	step [23/143], loss=124.7471
	step [24/143], loss=105.7643
	step [25/143], loss=128.7744
	step [26/143], loss=119.9791
	step [27/143], loss=115.9743
	step [28/143], loss=129.0132
	step [29/143], loss=114.4046
	step [30/143], loss=119.4808
	step [31/143], loss=128.5938
	step [32/143], loss=128.1797
	step [33/143], loss=136.1201
	step [34/143], loss=139.2946
	step [35/143], loss=144.2327
	step [36/143], loss=130.9296
	step [37/143], loss=130.0776
	step [38/143], loss=115.0784
	step [39/143], loss=109.9492
	step [40/143], loss=117.0444
	step [41/143], loss=138.0611
	step [42/143], loss=109.0327
	step [43/143], loss=123.7009
	step [44/143], loss=131.5915
	step [45/143], loss=126.7556
	step [46/143], loss=159.4619
	step [47/143], loss=127.7581
	step [48/143], loss=128.8569
	step [49/143], loss=126.8810
	step [50/143], loss=133.7502
	step [51/143], loss=145.4975
	step [52/143], loss=120.9861
	step [53/143], loss=127.4185
	step [54/143], loss=117.5311
	step [55/143], loss=132.7771
	step [56/143], loss=111.8204
	step [57/143], loss=110.6614
	step [58/143], loss=124.4204
	step [59/143], loss=107.1123
	step [60/143], loss=118.0550
	step [61/143], loss=106.5830
	step [62/143], loss=116.3212
	step [63/143], loss=123.9606
	step [64/143], loss=153.0807
	step [65/143], loss=121.6513
	step [66/143], loss=119.3875
	step [67/143], loss=107.7986
	step [68/143], loss=125.3453
	step [69/143], loss=147.9693
	step [70/143], loss=124.7103
	step [71/143], loss=113.0657
	step [72/143], loss=115.4771
	step [73/143], loss=147.8857
	step [74/143], loss=106.9145
	step [75/143], loss=113.9982
	step [76/143], loss=124.6628
	step [77/143], loss=114.7626
	step [78/143], loss=131.6408
	step [79/143], loss=117.6231
	step [80/143], loss=125.1295
	step [81/143], loss=112.9824
	step [82/143], loss=144.6706
	step [83/143], loss=137.1669
	step [84/143], loss=135.1607
	step [85/143], loss=124.4645
	step [86/143], loss=124.3558
	step [87/143], loss=134.8483
	step [88/143], loss=119.4754
	step [89/143], loss=118.7533
	step [90/143], loss=139.0979
	step [91/143], loss=138.9379
	step [92/143], loss=116.6787
	step [93/143], loss=124.6263
	step [94/143], loss=121.7382
	step [95/143], loss=139.6752
	step [96/143], loss=101.8217
	step [97/143], loss=101.2586
	step [98/143], loss=124.7822
	step [99/143], loss=107.4943
	step [100/143], loss=107.3154
	step [101/143], loss=124.5146
	step [102/143], loss=124.3886
	step [103/143], loss=123.1727
	step [104/143], loss=107.8793
	step [105/143], loss=122.1376
	step [106/143], loss=126.0060
	step [107/143], loss=127.9711
	step [108/143], loss=109.4892
	step [109/143], loss=148.5799
	step [110/143], loss=121.2504
	step [111/143], loss=122.9274
	step [112/143], loss=113.9256
	step [113/143], loss=117.0544
	step [114/143], loss=132.8682
	step [115/143], loss=111.0585
	step [116/143], loss=108.4073
	step [117/143], loss=103.4040
	step [118/143], loss=119.4085
	step [119/143], loss=110.2407
	step [120/143], loss=146.4973
	step [121/143], loss=122.1527
	step [122/143], loss=107.3223
	step [123/143], loss=140.9077
	step [124/143], loss=141.4411
	step [125/143], loss=122.4626
	step [126/143], loss=122.6765
	step [127/143], loss=124.2630
	step [128/143], loss=117.4634
	step [129/143], loss=132.6829
	step [130/143], loss=102.5604
	step [131/143], loss=119.5804
	step [132/143], loss=119.1258
	step [133/143], loss=124.6108
	step [134/143], loss=121.6979
	step [135/143], loss=135.6442
	step [136/143], loss=139.4218
	step [137/143], loss=126.9636
	step [138/143], loss=125.1478
	step [139/143], loss=121.6974
	step [140/143], loss=140.8533
	step [141/143], loss=108.5545
	step [142/143], loss=118.4198
	step [143/143], loss=80.6210
	Evaluating
	loss=0.0905, precision=0.5059, recall=0.8885, f1=0.6447
saving model as: 1_saved_model.pth
Training epoch 9
	step [1/143], loss=130.4831
	step [2/143], loss=132.6247
	step [3/143], loss=118.2076
	step [4/143], loss=107.2243
	step [5/143], loss=129.7558
	step [6/143], loss=122.4886
	step [7/143], loss=131.8230
	step [8/143], loss=121.5860
	step [9/143], loss=111.9804
	step [10/143], loss=101.5610
	step [11/143], loss=118.5171
	step [12/143], loss=101.5085
	step [13/143], loss=112.8863
	step [14/143], loss=143.3560
	step [15/143], loss=115.9787
	step [16/143], loss=119.9108
	step [17/143], loss=137.2937
	step [18/143], loss=143.4040
	step [19/143], loss=114.2890
	step [20/143], loss=120.5251
	step [21/143], loss=125.4166
	step [22/143], loss=120.3555
	step [23/143], loss=110.8215
	step [24/143], loss=120.6810
	step [25/143], loss=123.8703
	step [26/143], loss=125.3684
	step [27/143], loss=114.5789
	step [28/143], loss=130.3666
	step [29/143], loss=138.7873
	step [30/143], loss=119.3545
	step [31/143], loss=106.7299
	step [32/143], loss=119.6851
	step [33/143], loss=121.8531
	step [34/143], loss=110.5925
	step [35/143], loss=99.5706
	step [36/143], loss=130.9706
	step [37/143], loss=116.9678
	step [38/143], loss=140.9684
	step [39/143], loss=124.3384
	step [40/143], loss=108.2357
	step [41/143], loss=124.1659
	step [42/143], loss=110.8232
	step [43/143], loss=101.8293
	step [44/143], loss=133.5811
	step [45/143], loss=122.7820
	step [46/143], loss=133.8505
	step [47/143], loss=148.4195
	step [48/143], loss=106.8457
	step [49/143], loss=123.2074
	step [50/143], loss=115.4869
	step [51/143], loss=127.6667
	step [52/143], loss=128.1962
	step [53/143], loss=116.1739
	step [54/143], loss=96.8301
	step [55/143], loss=143.0403
	step [56/143], loss=125.2142
	step [57/143], loss=115.3342
	step [58/143], loss=135.5167
	step [59/143], loss=99.7039
	step [60/143], loss=130.4185
	step [61/143], loss=133.5852
	step [62/143], loss=120.7776
	step [63/143], loss=105.9292
	step [64/143], loss=117.1480
	step [65/143], loss=114.7168
	step [66/143], loss=116.2121
	step [67/143], loss=138.2091
	step [68/143], loss=122.7183
	step [69/143], loss=127.8172
	step [70/143], loss=109.6332
	step [71/143], loss=120.9882
	step [72/143], loss=113.8748
	step [73/143], loss=104.6700
	step [74/143], loss=104.7678
	step [75/143], loss=119.0389
	step [76/143], loss=121.5906
	step [77/143], loss=130.1713
	step [78/143], loss=125.9204
	step [79/143], loss=109.6222
	step [80/143], loss=119.0176
	step [81/143], loss=109.5603
	step [82/143], loss=117.5940
	step [83/143], loss=109.5704
	step [84/143], loss=120.7739
	step [85/143], loss=131.7325
	step [86/143], loss=103.0127
	step [87/143], loss=112.0608
	step [88/143], loss=108.3428
	step [89/143], loss=116.6071
	step [90/143], loss=131.4232
	step [91/143], loss=124.1186
	step [92/143], loss=103.6596
	step [93/143], loss=111.1741
	step [94/143], loss=125.4495
	step [95/143], loss=126.9716
	step [96/143], loss=124.2846
	step [97/143], loss=121.7474
	step [98/143], loss=109.9975
	step [99/143], loss=129.9378
	step [100/143], loss=117.9377
	step [101/143], loss=124.3731
	step [102/143], loss=116.6637
	step [103/143], loss=126.9152
	step [104/143], loss=110.9972
	step [105/143], loss=136.7619
	step [106/143], loss=128.2376
	step [107/143], loss=135.1005
	step [108/143], loss=107.8625
	step [109/143], loss=116.9188
	step [110/143], loss=104.7029
	step [111/143], loss=129.3788
	step [112/143], loss=124.2639
	step [113/143], loss=121.1113
	step [114/143], loss=121.3651
	step [115/143], loss=112.7555
	step [116/143], loss=100.6673
	step [117/143], loss=122.8149
	step [118/143], loss=135.7377
	step [119/143], loss=112.4935
	step [120/143], loss=127.5789
	step [121/143], loss=113.7661
	step [122/143], loss=125.5600
	step [123/143], loss=128.3347
	step [124/143], loss=119.3789
	step [125/143], loss=124.5126
	step [126/143], loss=121.6929
	step [127/143], loss=100.7481
	step [128/143], loss=122.0370
	step [129/143], loss=125.7034
	step [130/143], loss=126.9292
	step [131/143], loss=127.4635
	step [132/143], loss=140.6401
	step [133/143], loss=106.2911
	step [134/143], loss=120.7499
	step [135/143], loss=115.4857
	step [136/143], loss=129.0599
	step [137/143], loss=126.3587
	step [138/143], loss=119.4371
	step [139/143], loss=106.9721
	step [140/143], loss=126.8136
	step [141/143], loss=111.1501
	step [142/143], loss=128.7645
	step [143/143], loss=103.8212
	Evaluating
	loss=0.0782, precision=0.4329, recall=0.9123, f1=0.5872
Training epoch 10
	step [1/143], loss=120.1666
	step [2/143], loss=102.8070
	step [3/143], loss=126.3582
	step [4/143], loss=124.1215
	step [5/143], loss=106.3701
	step [6/143], loss=127.7243
	step [7/143], loss=121.8603
	step [8/143], loss=120.0776
	step [9/143], loss=119.2961
	step [10/143], loss=130.8363
	step [11/143], loss=115.2910
	step [12/143], loss=129.8993
	step [13/143], loss=113.3090
	step [14/143], loss=131.3726
	step [15/143], loss=120.0731
	step [16/143], loss=103.4263
	step [17/143], loss=106.6950
	step [18/143], loss=113.2726
	step [19/143], loss=125.6693
	step [20/143], loss=140.9906
	step [21/143], loss=111.6159
	step [22/143], loss=109.4860
	step [23/143], loss=116.3401
	step [24/143], loss=136.1160
	step [25/143], loss=112.4690
	step [26/143], loss=100.3879
	step [27/143], loss=118.6588
	step [28/143], loss=126.7597
	step [29/143], loss=119.6176
	step [30/143], loss=131.7100
	step [31/143], loss=123.9034
	step [32/143], loss=120.7856
	step [33/143], loss=108.0183
	step [34/143], loss=145.8152
	step [35/143], loss=107.0917
	step [36/143], loss=116.0546
	step [37/143], loss=100.7359
	step [38/143], loss=119.6114
	step [39/143], loss=114.9144
	step [40/143], loss=131.4917
	step [41/143], loss=96.6272
	step [42/143], loss=131.6206
	step [43/143], loss=110.6897
	step [44/143], loss=135.0909
	step [45/143], loss=123.7869
	step [46/143], loss=125.7475
	step [47/143], loss=104.1250
	step [48/143], loss=128.5765
	step [49/143], loss=114.2727
	step [50/143], loss=101.5765
	step [51/143], loss=110.5009
	step [52/143], loss=109.5939
	step [53/143], loss=126.3401
	step [54/143], loss=115.0340
	step [55/143], loss=140.6705
	step [56/143], loss=130.9468
	step [57/143], loss=105.8104
	step [58/143], loss=108.5513
	step [59/143], loss=128.6422
	step [60/143], loss=99.2111
	step [61/143], loss=117.5459
	step [62/143], loss=126.5077
	step [63/143], loss=120.6191
	step [64/143], loss=99.9009
	step [65/143], loss=122.9126
	step [66/143], loss=118.5920
	step [67/143], loss=116.2542
	step [68/143], loss=106.5555
	step [69/143], loss=115.9064
	step [70/143], loss=104.8781
	step [71/143], loss=114.9477
	step [72/143], loss=124.6147
	step [73/143], loss=106.0757
	step [74/143], loss=116.4616
	step [75/143], loss=106.5556
	step [76/143], loss=111.2170
	step [77/143], loss=117.1868
	step [78/143], loss=131.2876
	step [79/143], loss=114.7869
	step [80/143], loss=122.6373
	step [81/143], loss=148.2433
	step [82/143], loss=117.0580
	step [83/143], loss=105.9243
	step [84/143], loss=127.8167
	step [85/143], loss=112.1665
	step [86/143], loss=107.5095
	step [87/143], loss=108.2610
	step [88/143], loss=106.3341
	step [89/143], loss=118.7068
	step [90/143], loss=115.1067
	step [91/143], loss=117.0721
	step [92/143], loss=106.2633
	step [93/143], loss=131.6178
	step [94/143], loss=117.1633
	step [95/143], loss=118.9344
	step [96/143], loss=98.7579
	step [97/143], loss=122.2618
	step [98/143], loss=125.7950
	step [99/143], loss=102.6805
	step [100/143], loss=110.7303
	step [101/143], loss=124.8207
	step [102/143], loss=121.2345
	step [103/143], loss=126.6357
	step [104/143], loss=129.2676
	step [105/143], loss=134.2529
	step [106/143], loss=99.3498
	step [107/143], loss=112.5511
	step [108/143], loss=121.3558
	step [109/143], loss=112.7205
	step [110/143], loss=107.2676
	step [111/143], loss=119.4162
	step [112/143], loss=119.9455
	step [113/143], loss=110.9153
	step [114/143], loss=105.2293
	step [115/143], loss=122.2071
	step [116/143], loss=110.2724
	step [117/143], loss=115.1172
	step [118/143], loss=127.6943
	step [119/143], loss=110.3823
	step [120/143], loss=89.6342
	step [121/143], loss=92.8177
	step [122/143], loss=102.3461
	step [123/143], loss=135.6206
	step [124/143], loss=134.3649
	step [125/143], loss=114.6415
	step [126/143], loss=135.5125
	step [127/143], loss=102.7175
	step [128/143], loss=110.6814
	step [129/143], loss=115.8058
	step [130/143], loss=98.4918
	step [131/143], loss=107.2387
	step [132/143], loss=131.3147
	step [133/143], loss=119.3594
	step [134/143], loss=120.9967
	step [135/143], loss=118.1608
	step [136/143], loss=115.1328
	step [137/143], loss=132.0320
	step [138/143], loss=114.6084
	step [139/143], loss=111.7856
	step [140/143], loss=136.5750
	step [141/143], loss=119.6722
	step [142/143], loss=121.3279
	step [143/143], loss=89.3762
	Evaluating
	loss=0.0660, precision=0.4700, recall=0.8905, f1=0.6152
Training epoch 11
	step [1/143], loss=116.9674
	step [2/143], loss=123.2114
	step [3/143], loss=115.8606
	step [4/143], loss=111.2059
	step [5/143], loss=124.7074
	step [6/143], loss=115.8814
	step [7/143], loss=112.0229
	step [8/143], loss=108.2528
	step [9/143], loss=115.6379
	step [10/143], loss=96.2161
	step [11/143], loss=112.7320
	step [12/143], loss=108.8780
	step [13/143], loss=116.5551
	step [14/143], loss=96.5377
	step [15/143], loss=124.7389
	step [16/143], loss=128.1722
	step [17/143], loss=102.6897
	step [18/143], loss=110.8306
	step [19/143], loss=131.1405
	step [20/143], loss=102.1205
	step [21/143], loss=117.8102
	step [22/143], loss=109.0065
	step [23/143], loss=126.1804
	step [24/143], loss=120.1034
	step [25/143], loss=98.8548
	step [26/143], loss=114.4081
	step [27/143], loss=118.0680
	step [28/143], loss=109.5717
	step [29/143], loss=107.6242
	step [30/143], loss=103.1171
	step [31/143], loss=124.8768
	step [32/143], loss=119.7168
	step [33/143], loss=118.6824
	step [34/143], loss=127.2094
	step [35/143], loss=105.2937
	step [36/143], loss=123.9247
	step [37/143], loss=122.3037
	step [38/143], loss=122.7870
	step [39/143], loss=133.9866
	step [40/143], loss=113.3753
	step [41/143], loss=125.0001
	step [42/143], loss=127.6868
	step [43/143], loss=126.2547
	step [44/143], loss=125.8425
	step [45/143], loss=105.4234
	step [46/143], loss=112.6224
	step [47/143], loss=102.3811
	step [48/143], loss=116.5062
	step [49/143], loss=147.0070
	step [50/143], loss=99.7762
	step [51/143], loss=123.6561
	step [52/143], loss=124.3446
	step [53/143], loss=100.8692
	step [54/143], loss=124.1536
	step [55/143], loss=105.0094
	step [56/143], loss=129.0776
	step [57/143], loss=123.7672
	step [58/143], loss=114.5992
	step [59/143], loss=97.8704
	step [60/143], loss=126.0875
	step [61/143], loss=129.1815
	step [62/143], loss=105.1249
	step [63/143], loss=117.8418
	step [64/143], loss=98.9372
	step [65/143], loss=116.8479
	step [66/143], loss=106.0144
	step [67/143], loss=108.7328
	step [68/143], loss=91.9943
	step [69/143], loss=123.3189
	step [70/143], loss=121.8410
	step [71/143], loss=112.3016
	step [72/143], loss=117.5741
	step [73/143], loss=113.0938
	step [74/143], loss=118.6509
	step [75/143], loss=105.0676
	step [76/143], loss=104.5393
	step [77/143], loss=107.2096
	step [78/143], loss=107.4299
	step [79/143], loss=106.8537
	step [80/143], loss=93.6943
	step [81/143], loss=107.6153
	step [82/143], loss=115.3239
	step [83/143], loss=113.8100
	step [84/143], loss=100.1010
	step [85/143], loss=79.6286
	step [86/143], loss=99.2737
	step [87/143], loss=101.0405
	step [88/143], loss=121.1066
	step [89/143], loss=121.3235
	step [90/143], loss=99.2567
	step [91/143], loss=106.5677
	step [92/143], loss=110.4668
	step [93/143], loss=118.2232
	step [94/143], loss=113.5314
	step [95/143], loss=101.4712
	step [96/143], loss=109.5471
	step [97/143], loss=107.6579
	step [98/143], loss=130.6418
	step [99/143], loss=110.4974
	step [100/143], loss=111.1248
	step [101/143], loss=116.6311
	step [102/143], loss=119.0188
	step [103/143], loss=132.3199
	step [104/143], loss=123.7997
	step [105/143], loss=107.4362
	step [106/143], loss=97.6334
	step [107/143], loss=115.6254
	step [108/143], loss=109.1076
	step [109/143], loss=117.9650
	step [110/143], loss=133.2109
	step [111/143], loss=113.7217
	step [112/143], loss=106.2720
	step [113/143], loss=100.6856
	step [114/143], loss=114.5028
	step [115/143], loss=116.0173
	step [116/143], loss=112.5705
	step [117/143], loss=125.6513
	step [118/143], loss=129.6341
	step [119/143], loss=109.8199
	step [120/143], loss=111.4295
	step [121/143], loss=116.5854
	step [122/143], loss=109.3778
	step [123/143], loss=127.5610
	step [124/143], loss=111.0229
	step [125/143], loss=100.0599
	step [126/143], loss=96.0946
	step [127/143], loss=123.9828
	step [128/143], loss=112.8334
	step [129/143], loss=130.6106
	step [130/143], loss=98.9538
	step [131/143], loss=121.4148
	step [132/143], loss=123.3397
	step [133/143], loss=120.0756
	step [134/143], loss=106.2083
	step [135/143], loss=123.8972
	step [136/143], loss=120.6489
	step [137/143], loss=119.8777
	step [138/143], loss=105.0600
	step [139/143], loss=121.7456
	step [140/143], loss=127.1923
	step [141/143], loss=100.2187
	step [142/143], loss=118.1632
	step [143/143], loss=103.5626
	Evaluating
	loss=0.0568, precision=0.4929, recall=0.8708, f1=0.6295
Training epoch 12
	step [1/143], loss=117.0330
	step [2/143], loss=125.2914
	step [3/143], loss=120.7064
	step [4/143], loss=102.2759
	step [5/143], loss=113.5991
	step [6/143], loss=116.7944
	step [7/143], loss=118.5446
	step [8/143], loss=101.6503
	step [9/143], loss=96.0780
	step [10/143], loss=115.2765
	step [11/143], loss=105.6801
	step [12/143], loss=112.2665
	step [13/143], loss=109.2387
	step [14/143], loss=105.1559
	step [15/143], loss=123.1076
	step [16/143], loss=124.9405
	step [17/143], loss=114.9128
	step [18/143], loss=102.6864
	step [19/143], loss=131.2179
	step [20/143], loss=116.6488
	step [21/143], loss=100.0410
	step [22/143], loss=112.3934
	step [23/143], loss=100.5854
	step [24/143], loss=107.8024
	step [25/143], loss=122.5632
	step [26/143], loss=116.7650
	step [27/143], loss=85.6196
	step [28/143], loss=101.1290
	step [29/143], loss=114.6695
	step [30/143], loss=128.1960
	step [31/143], loss=110.5023
	step [32/143], loss=107.2309
	step [33/143], loss=117.6143
	step [34/143], loss=108.5399
	step [35/143], loss=132.0602
	step [36/143], loss=112.0159
	step [37/143], loss=105.3690
	step [38/143], loss=109.3579
	step [39/143], loss=117.8276
	step [40/143], loss=123.8173
	step [41/143], loss=115.0476
	step [42/143], loss=119.4393
	step [43/143], loss=122.8983
	step [44/143], loss=117.0644
	step [45/143], loss=108.7820
	step [46/143], loss=92.1511
	step [47/143], loss=114.2301
	step [48/143], loss=124.6800
	step [49/143], loss=92.2775
	step [50/143], loss=131.6051
	step [51/143], loss=111.1863
	step [52/143], loss=100.3066
	step [53/143], loss=100.8515
	step [54/143], loss=126.0421
	step [55/143], loss=83.7938
	step [56/143], loss=93.0726
	step [57/143], loss=120.6871
	step [58/143], loss=102.9981
	step [59/143], loss=122.1399
	step [60/143], loss=109.6695
	step [61/143], loss=108.8900
	step [62/143], loss=123.7372
	step [63/143], loss=112.3875
	step [64/143], loss=105.3192
	step [65/143], loss=115.0086
	step [66/143], loss=114.2384
	step [67/143], loss=113.7680
	step [68/143], loss=104.0288
	step [69/143], loss=121.1721
	step [70/143], loss=113.9063
	step [71/143], loss=110.6101
	step [72/143], loss=122.6332
	step [73/143], loss=123.0262
	step [74/143], loss=124.5716
	step [75/143], loss=106.2504
	step [76/143], loss=108.4890
	step [77/143], loss=125.3453
	step [78/143], loss=119.7242
	step [79/143], loss=104.1801
	step [80/143], loss=115.5873
	step [81/143], loss=96.8470
	step [82/143], loss=96.3655
	step [83/143], loss=120.1621
	step [84/143], loss=110.1236
	step [85/143], loss=112.8405
	step [86/143], loss=113.1487
	step [87/143], loss=100.6827
	step [88/143], loss=131.7130
	step [89/143], loss=104.2418
	step [90/143], loss=110.0287
	step [91/143], loss=110.6806
	step [92/143], loss=120.4548
	step [93/143], loss=109.7461
	step [94/143], loss=106.1876
	step [95/143], loss=122.2492
	step [96/143], loss=99.6440
	step [97/143], loss=128.2246
	step [98/143], loss=110.5406
	step [99/143], loss=93.4478
	step [100/143], loss=108.0537
	step [101/143], loss=111.2428
	step [102/143], loss=119.4003
	step [103/143], loss=113.6152
	step [104/143], loss=104.4967
	step [105/143], loss=133.6933
	step [106/143], loss=107.1574
	step [107/143], loss=116.6105
	step [108/143], loss=111.8746
	step [109/143], loss=113.7164
	step [110/143], loss=109.9275
	step [111/143], loss=105.7697
	step [112/143], loss=118.7072
	step [113/143], loss=147.9343
	step [114/143], loss=114.6285
	step [115/143], loss=127.1837
	step [116/143], loss=87.5141
	step [117/143], loss=115.9772
	step [118/143], loss=121.6981
	step [119/143], loss=127.9906
	step [120/143], loss=122.2091
	step [121/143], loss=106.1868
	step [122/143], loss=98.5922
	step [123/143], loss=102.0339
	step [124/143], loss=112.2098
	step [125/143], loss=104.5000
	step [126/143], loss=98.8631
	step [127/143], loss=122.0410
	step [128/143], loss=116.2196
	step [129/143], loss=104.5719
	step [130/143], loss=106.0153
	step [131/143], loss=112.7548
	step [132/143], loss=115.7748
	step [133/143], loss=102.2962
	step [134/143], loss=112.9053
	step [135/143], loss=115.8238
	step [136/143], loss=110.9119
	step [137/143], loss=105.7007
	step [138/143], loss=118.1323
	step [139/143], loss=98.8277
	step [140/143], loss=124.5941
	step [141/143], loss=122.8896
	step [142/143], loss=92.8555
	step [143/143], loss=89.6319
	Evaluating
	loss=0.0485, precision=0.5287, recall=0.8689, f1=0.6574
saving model as: 1_saved_model.pth
Training epoch 13
	step [1/143], loss=123.6383
	step [2/143], loss=108.9706
	step [3/143], loss=110.3168
	step [4/143], loss=114.9529
	step [5/143], loss=107.5589
	step [6/143], loss=96.6767
	step [7/143], loss=98.5378
	step [8/143], loss=124.8323
	step [9/143], loss=106.5562
	step [10/143], loss=109.9306
	step [11/143], loss=98.6972
	step [12/143], loss=102.4706
	step [13/143], loss=111.4135
	step [14/143], loss=101.0489
	step [15/143], loss=92.2611
	step [16/143], loss=113.0681
	step [17/143], loss=92.9192
	step [18/143], loss=106.5422
	step [19/143], loss=106.1145
	step [20/143], loss=125.7007
	step [21/143], loss=98.3664
	step [22/143], loss=96.8897
	step [23/143], loss=109.2315
	step [24/143], loss=113.6873
	step [25/143], loss=99.4926
	step [26/143], loss=130.3262
	step [27/143], loss=117.8786
	step [28/143], loss=103.4916
	step [29/143], loss=127.7004
	step [30/143], loss=107.4193
	step [31/143], loss=106.4531
	step [32/143], loss=124.5285
	step [33/143], loss=92.0837
	step [34/143], loss=104.7538
	step [35/143], loss=105.3279
	step [36/143], loss=132.8559
	step [37/143], loss=107.7485
	step [38/143], loss=115.8664
	step [39/143], loss=109.5113
	step [40/143], loss=125.3717
	step [41/143], loss=107.0435
	step [42/143], loss=120.7622
	step [43/143], loss=111.8811
	step [44/143], loss=97.3448
	step [45/143], loss=106.5208
	step [46/143], loss=120.1614
	step [47/143], loss=103.3045
	step [48/143], loss=127.6927
	step [49/143], loss=109.2340
	step [50/143], loss=98.4789
	step [51/143], loss=121.8327
	step [52/143], loss=119.7839
	step [53/143], loss=117.1092
	step [54/143], loss=119.3173
	step [55/143], loss=134.4314
	step [56/143], loss=108.0751
	step [57/143], loss=107.9113
	step [58/143], loss=113.2639
	step [59/143], loss=94.0223
	step [60/143], loss=126.5730
	step [61/143], loss=109.5351
	step [62/143], loss=93.7825
	step [63/143], loss=94.7918
	step [64/143], loss=110.9006
	step [65/143], loss=117.6151
	step [66/143], loss=113.3938
	step [67/143], loss=87.8847
	step [68/143], loss=105.7634
	step [69/143], loss=111.8811
	step [70/143], loss=114.8726
	step [71/143], loss=106.8439
	step [72/143], loss=107.4852
	step [73/143], loss=99.4647
	step [74/143], loss=116.1805
	step [75/143], loss=98.0361
	step [76/143], loss=115.3197
	step [77/143], loss=105.3935
	step [78/143], loss=108.5110
	step [79/143], loss=108.4157
	step [80/143], loss=114.4600
	step [81/143], loss=85.8972
	step [82/143], loss=103.3791
	step [83/143], loss=130.2704
	step [84/143], loss=113.2730
	step [85/143], loss=93.8216
	step [86/143], loss=121.6176
	step [87/143], loss=122.3304
	step [88/143], loss=97.3663
	step [89/143], loss=100.7082
	step [90/143], loss=120.7514
	step [91/143], loss=124.0378
	step [92/143], loss=106.4752
	step [93/143], loss=111.8502
	step [94/143], loss=85.8154
	step [95/143], loss=118.2357
	step [96/143], loss=107.7118
	step [97/143], loss=109.7102
	step [98/143], loss=127.3960
	step [99/143], loss=111.5957
	step [100/143], loss=127.7403
	step [101/143], loss=116.5058
	step [102/143], loss=108.2644
	step [103/143], loss=108.3903
	step [104/143], loss=123.3982
	step [105/143], loss=116.1925
	step [106/143], loss=96.9720
	step [107/143], loss=114.6308
	step [108/143], loss=112.1615
	step [109/143], loss=105.5700
	step [110/143], loss=101.0459
	step [111/143], loss=120.0246
	step [112/143], loss=110.6510
	step [113/143], loss=104.5451
	step [114/143], loss=122.3867
	step [115/143], loss=109.9686
	step [116/143], loss=123.7796
	step [117/143], loss=90.4348
	step [118/143], loss=100.6685
	step [119/143], loss=114.9685
	step [120/143], loss=103.4207
	step [121/143], loss=105.2900
	step [122/143], loss=112.0898
	step [123/143], loss=100.4737
	step [124/143], loss=110.8072
	step [125/143], loss=90.8941
	step [126/143], loss=108.2840
	step [127/143], loss=123.8142
	step [128/143], loss=94.8448
	step [129/143], loss=122.9677
	step [130/143], loss=118.1741
	step [131/143], loss=117.5639
	step [132/143], loss=114.2053
	step [133/143], loss=123.0413
	step [134/143], loss=103.7737
	step [135/143], loss=95.3086
	step [136/143], loss=104.6105
	step [137/143], loss=104.4886
	step [138/143], loss=99.1895
	step [139/143], loss=109.7142
	step [140/143], loss=114.1857
	step [141/143], loss=119.5132
	step [142/143], loss=111.7383
	step [143/143], loss=91.7680
	Evaluating
	loss=0.0442, precision=0.4506, recall=0.9153, f1=0.6039
Training epoch 14
	step [1/143], loss=108.5924
	step [2/143], loss=98.7783
	step [3/143], loss=104.4305
	step [4/143], loss=89.2044
	step [5/143], loss=107.3756
	step [6/143], loss=105.9461
	step [7/143], loss=106.8977
	step [8/143], loss=124.6354
	step [9/143], loss=84.2973
	step [10/143], loss=135.5742
	step [11/143], loss=115.0117
	step [12/143], loss=110.4268
	step [13/143], loss=124.0246
	step [14/143], loss=117.2599
	step [15/143], loss=114.8918
	step [16/143], loss=116.4305
	step [17/143], loss=107.0085
	step [18/143], loss=107.5209
	step [19/143], loss=118.6950
	step [20/143], loss=101.6692
	step [21/143], loss=94.3683
	step [22/143], loss=107.0924
	step [23/143], loss=105.3712
	step [24/143], loss=105.9529
	step [25/143], loss=108.8802
	step [26/143], loss=114.0757
	step [27/143], loss=122.2644
	step [28/143], loss=124.4121
	step [29/143], loss=125.1662
	step [30/143], loss=119.4466
	step [31/143], loss=117.4359
	step [32/143], loss=99.8118
	step [33/143], loss=104.5695
	step [34/143], loss=104.8863
	step [35/143], loss=98.2663
	step [36/143], loss=114.1424
	step [37/143], loss=109.5641
	step [38/143], loss=109.2828
	step [39/143], loss=93.4743
	step [40/143], loss=115.4956
	step [41/143], loss=112.0767
	step [42/143], loss=112.6991
	step [43/143], loss=108.2230
	step [44/143], loss=124.7234
	step [45/143], loss=98.9094
	step [46/143], loss=129.5644
	step [47/143], loss=100.9627
	step [48/143], loss=87.7944
	step [49/143], loss=89.5139
	step [50/143], loss=96.4417
	step [51/143], loss=116.4976
	step [52/143], loss=104.3214
	step [53/143], loss=113.6774
	step [54/143], loss=115.1072
	step [55/143], loss=107.0016
	step [56/143], loss=106.3902
	step [57/143], loss=89.0924
	step [58/143], loss=96.0294
	step [59/143], loss=119.2993
	step [60/143], loss=104.0398
	step [61/143], loss=115.3032
	step [62/143], loss=106.6623
	step [63/143], loss=124.4324
	step [64/143], loss=107.6292
	step [65/143], loss=103.8853
	step [66/143], loss=113.8612
	step [67/143], loss=110.1832
	step [68/143], loss=115.0773
	step [69/143], loss=120.8100
	step [70/143], loss=106.1156
	step [71/143], loss=110.1410
	step [72/143], loss=114.3487
	step [73/143], loss=96.9875
	step [74/143], loss=114.2058
	step [75/143], loss=110.0199
	step [76/143], loss=116.6739
	step [77/143], loss=88.6784
	step [78/143], loss=106.7002
	step [79/143], loss=107.9784
	step [80/143], loss=110.1183
	step [81/143], loss=108.2748
	step [82/143], loss=104.8436
	step [83/143], loss=103.3496
	step [84/143], loss=94.5285
	step [85/143], loss=98.9529
	step [86/143], loss=115.5461
	step [87/143], loss=104.5773
	step [88/143], loss=98.6838
	step [89/143], loss=87.1095
	step [90/143], loss=102.1135
	step [91/143], loss=91.4339
	step [92/143], loss=128.7996
	step [93/143], loss=131.3350
	step [94/143], loss=108.7855
	step [95/143], loss=104.9627
	step [96/143], loss=101.6021
	step [97/143], loss=106.3566
	step [98/143], loss=100.8413
	step [99/143], loss=116.5707
	step [100/143], loss=104.4957
	step [101/143], loss=92.4933
	step [102/143], loss=98.0758
	step [103/143], loss=107.8119
	step [104/143], loss=104.8692
	step [105/143], loss=121.8940
	step [106/143], loss=111.5775
	step [107/143], loss=96.7878
	step [108/143], loss=129.1670
	step [109/143], loss=102.7740
	step [110/143], loss=94.2040
	step [111/143], loss=105.9556
	step [112/143], loss=125.9906
	step [113/143], loss=120.3101
	step [114/143], loss=104.3817
	step [115/143], loss=112.8702
	step [116/143], loss=118.4402
	step [117/143], loss=115.1721
	step [118/143], loss=120.4875
	step [119/143], loss=106.4286
	step [120/143], loss=111.8251
	step [121/143], loss=98.1760
	step [122/143], loss=122.5204
	step [123/143], loss=98.0902
	step [124/143], loss=101.1950
	step [125/143], loss=117.9996
	step [126/143], loss=117.3992
	step [127/143], loss=106.5130
	step [128/143], loss=105.5451
	step [129/143], loss=104.2808
	step [130/143], loss=103.7637
	step [131/143], loss=111.0802
	step [132/143], loss=102.9523
	step [133/143], loss=89.3186
	step [134/143], loss=109.2262
	step [135/143], loss=87.2122
	step [136/143], loss=109.0350
	step [137/143], loss=78.2941
	step [138/143], loss=113.7872
	step [139/143], loss=103.5345
	step [140/143], loss=118.6581
	step [141/143], loss=103.6401
	step [142/143], loss=105.9472
	step [143/143], loss=82.6549
	Evaluating
	loss=0.0371, precision=0.5141, recall=0.9021, f1=0.6549
Training epoch 15
	step [1/143], loss=133.9448
	step [2/143], loss=97.2462
	step [3/143], loss=85.7903
	step [4/143], loss=96.6904
	step [5/143], loss=125.2403
	step [6/143], loss=101.9330
	step [7/143], loss=99.6456
	step [8/143], loss=96.3981
	step [9/143], loss=99.6845
	step [10/143], loss=128.6225
	step [11/143], loss=95.8906
	step [12/143], loss=103.3465
	step [13/143], loss=110.2233
	step [14/143], loss=93.9403
	step [15/143], loss=106.2427
	step [16/143], loss=120.2635
	step [17/143], loss=95.2253
	step [18/143], loss=105.5916
	step [19/143], loss=116.5794
	step [20/143], loss=109.1572
	step [21/143], loss=119.3867
	step [22/143], loss=103.5629
	step [23/143], loss=117.1023
	step [24/143], loss=103.6163
	step [25/143], loss=95.5292
	step [26/143], loss=102.0224
	step [27/143], loss=116.5761
	step [28/143], loss=109.7486
	step [29/143], loss=127.5173
	step [30/143], loss=118.8522
	step [31/143], loss=104.1075
	step [32/143], loss=94.6825
	step [33/143], loss=93.6240
	step [34/143], loss=100.7702
	step [35/143], loss=106.9068
	step [36/143], loss=100.6864
	step [37/143], loss=109.1809
	step [38/143], loss=97.1172
	step [39/143], loss=117.9348
	step [40/143], loss=113.1000
	step [41/143], loss=94.8026
	step [42/143], loss=104.7840
	step [43/143], loss=94.5538
	step [44/143], loss=102.5876
	step [45/143], loss=95.6938
	step [46/143], loss=115.1503
	step [47/143], loss=115.6960
	step [48/143], loss=96.3577
	step [49/143], loss=95.3772
	step [50/143], loss=97.4897
	step [51/143], loss=132.6157
	step [52/143], loss=92.6847
	step [53/143], loss=106.8825
	step [54/143], loss=98.2321
	step [55/143], loss=97.1181
	step [56/143], loss=106.4851
	step [57/143], loss=73.3400
	step [58/143], loss=107.2029
	step [59/143], loss=113.7784
	step [60/143], loss=120.4009
	step [61/143], loss=101.8230
	step [62/143], loss=122.1676
	step [63/143], loss=99.4777
	step [64/143], loss=127.6233
	step [65/143], loss=113.0088
	step [66/143], loss=100.8039
	step [67/143], loss=87.9301
	step [68/143], loss=115.0414
	step [69/143], loss=105.5653
	step [70/143], loss=110.7105
	step [71/143], loss=110.3202
	step [72/143], loss=121.2560
	step [73/143], loss=111.7922
	step [74/143], loss=105.2949
	step [75/143], loss=93.0418
	step [76/143], loss=115.4342
	step [77/143], loss=93.8787
	step [78/143], loss=115.9237
	step [79/143], loss=124.8866
	step [80/143], loss=106.7059
	step [81/143], loss=122.5766
	step [82/143], loss=103.2073
	step [83/143], loss=111.5423
	step [84/143], loss=112.6218
	step [85/143], loss=94.4300
	step [86/143], loss=92.0389
	step [87/143], loss=106.9057
	step [88/143], loss=110.1699
	step [89/143], loss=107.6473
	step [90/143], loss=105.9296
	step [91/143], loss=112.8137
	step [92/143], loss=108.7906
	step [93/143], loss=103.6711
	step [94/143], loss=110.4106
	step [95/143], loss=90.0057
	step [96/143], loss=105.4592
	step [97/143], loss=115.0288
	step [98/143], loss=112.1760
	step [99/143], loss=107.5040
	step [100/143], loss=107.4140
	step [101/143], loss=95.2469
	step [102/143], loss=117.4000
	step [103/143], loss=121.9722
	step [104/143], loss=124.9286
	step [105/143], loss=107.5777
	step [106/143], loss=94.0525
	step [107/143], loss=120.9274
	step [108/143], loss=108.0138
	step [109/143], loss=126.7205
	step [110/143], loss=109.4510
	step [111/143], loss=102.6566
	step [112/143], loss=97.4151
	step [113/143], loss=90.5330
	step [114/143], loss=105.0641
	step [115/143], loss=111.7614
	step [116/143], loss=106.8005
	step [117/143], loss=118.4457
	step [118/143], loss=93.5325
	step [119/143], loss=108.8159
	step [120/143], loss=98.4654
	step [121/143], loss=106.2543
	step [122/143], loss=94.4214
	step [123/143], loss=117.8860
	step [124/143], loss=108.9300
	step [125/143], loss=88.0570
	step [126/143], loss=101.6306
	step [127/143], loss=110.6306
	step [128/143], loss=96.4750
	step [129/143], loss=94.4798
	step [130/143], loss=99.5663
	step [131/143], loss=126.3935
	step [132/143], loss=124.9400
	step [133/143], loss=94.4268
	step [134/143], loss=93.2053
	step [135/143], loss=117.6072
	step [136/143], loss=104.5179
	step [137/143], loss=104.7958
	step [138/143], loss=92.4591
	step [139/143], loss=93.8287
	step [140/143], loss=108.0037
	step [141/143], loss=89.7354
	step [142/143], loss=105.4208
	step [143/143], loss=82.4517
	Evaluating
	loss=0.0335, precision=0.5220, recall=0.8875, f1=0.6574
saving model as: 1_saved_model.pth
Training epoch 16
	step [1/143], loss=124.5925
	step [2/143], loss=99.8561
	step [3/143], loss=103.4239
	step [4/143], loss=120.5681
	step [5/143], loss=106.6022
	step [6/143], loss=103.5994
	step [7/143], loss=122.7271
	step [8/143], loss=108.5595
	step [9/143], loss=86.4837
	step [10/143], loss=109.1693
	step [11/143], loss=113.0216
	step [12/143], loss=109.3673
	step [13/143], loss=109.0250
	step [14/143], loss=107.4917
	step [15/143], loss=102.7021
	step [16/143], loss=120.1884
	step [17/143], loss=116.7839
	step [18/143], loss=107.9416
	step [19/143], loss=132.9929
	step [20/143], loss=97.8602
	step [21/143], loss=107.6423
	step [22/143], loss=126.1421
	step [23/143], loss=106.5096
	step [24/143], loss=110.1687
	step [25/143], loss=113.1947
	step [26/143], loss=104.2267
	step [27/143], loss=96.4419
	step [28/143], loss=99.5831
	step [29/143], loss=110.8510
	step [30/143], loss=107.0475
	step [31/143], loss=110.8897
	step [32/143], loss=109.0762
	step [33/143], loss=102.2968
	step [34/143], loss=90.3834
	step [35/143], loss=83.6624
	step [36/143], loss=107.3312
	step [37/143], loss=113.7273
	step [38/143], loss=105.9469
	step [39/143], loss=97.1314
	step [40/143], loss=91.0135
	step [41/143], loss=112.8897
	step [42/143], loss=105.2342
	step [43/143], loss=99.8328
	step [44/143], loss=108.4548
	step [45/143], loss=113.4807
	step [46/143], loss=92.6973
	step [47/143], loss=119.7135
	step [48/143], loss=92.5573
	step [49/143], loss=109.5462
	step [50/143], loss=105.9225
	step [51/143], loss=107.0318
	step [52/143], loss=103.8371
	step [53/143], loss=96.6638
	step [54/143], loss=109.0572
	step [55/143], loss=111.3241
	step [56/143], loss=99.1897
	step [57/143], loss=107.2944
	step [58/143], loss=105.1714
	step [59/143], loss=99.8115
	step [60/143], loss=123.2561
	step [61/143], loss=95.9396
	step [62/143], loss=87.8031
	step [63/143], loss=110.9431
	step [64/143], loss=81.6661
	step [65/143], loss=110.2900
	step [66/143], loss=103.0634
	step [67/143], loss=106.7055
	step [68/143], loss=103.4073
	step [69/143], loss=93.7904
	step [70/143], loss=105.8087
	step [71/143], loss=104.2006
	step [72/143], loss=128.1275
	step [73/143], loss=95.2966
	step [74/143], loss=118.0777
	step [75/143], loss=109.1851
	step [76/143], loss=110.8466
	step [77/143], loss=102.7832
	step [78/143], loss=102.6054
	step [79/143], loss=101.5944
	step [80/143], loss=95.6950
	step [81/143], loss=108.0957
	step [82/143], loss=112.7247
	step [83/143], loss=94.8817
	step [84/143], loss=102.3275
	step [85/143], loss=102.7876
	step [86/143], loss=105.4840
	step [87/143], loss=96.8842
	step [88/143], loss=100.5564
	step [89/143], loss=109.9186
	step [90/143], loss=120.1662
	step [91/143], loss=99.0140
	step [92/143], loss=102.2481
	step [93/143], loss=101.3609
	step [94/143], loss=120.0805
	step [95/143], loss=93.6371
	step [96/143], loss=108.5097
	step [97/143], loss=99.3362
	step [98/143], loss=113.3113
	step [99/143], loss=105.4973
	step [100/143], loss=105.1553
	step [101/143], loss=98.5946
	step [102/143], loss=111.0546
	step [103/143], loss=105.2400
	step [104/143], loss=120.2174
	step [105/143], loss=105.9719
	step [106/143], loss=87.5539
	step [107/143], loss=86.8525
	step [108/143], loss=108.5932
	step [109/143], loss=99.9975
	step [110/143], loss=96.3872
	step [111/143], loss=93.8944
	step [112/143], loss=80.2430
	step [113/143], loss=92.9876
	step [114/143], loss=101.9745
	step [115/143], loss=113.0027
	step [116/143], loss=103.7200
	step [117/143], loss=81.5316
	step [118/143], loss=108.3540
	step [119/143], loss=84.1222
	step [120/143], loss=98.9587
	step [121/143], loss=124.9409
	step [122/143], loss=95.6634
	step [123/143], loss=116.7875
	step [124/143], loss=94.1690
	step [125/143], loss=115.2213
	step [126/143], loss=101.3785
	step [127/143], loss=99.8003
	step [128/143], loss=113.0916
	step [129/143], loss=102.3216
	step [130/143], loss=112.3169
	step [131/143], loss=103.6614
	step [132/143], loss=117.1318
	step [133/143], loss=116.2444
	step [134/143], loss=113.7574
	step [135/143], loss=88.6302
	step [136/143], loss=106.6952
	step [137/143], loss=103.3524
	step [138/143], loss=95.7992
	step [139/143], loss=97.9750
	step [140/143], loss=91.4272
	step [141/143], loss=115.7438
	step [142/143], loss=96.3339
	step [143/143], loss=79.8322
	Evaluating
	loss=0.0321, precision=0.4823, recall=0.8880, f1=0.6251
Training epoch 17
	step [1/143], loss=94.7712
	step [2/143], loss=112.8091
	step [3/143], loss=104.9281
	step [4/143], loss=97.5725
	step [5/143], loss=105.1717
	step [6/143], loss=108.4255
	step [7/143], loss=106.1525
	step [8/143], loss=93.2938
	step [9/143], loss=98.6784
	step [10/143], loss=123.8824
	step [11/143], loss=99.2014
	step [12/143], loss=98.8744
	step [13/143], loss=100.0250
	step [14/143], loss=89.7409
	step [15/143], loss=98.5967
	step [16/143], loss=95.8820
	step [17/143], loss=100.1041
	step [18/143], loss=107.9442
	step [19/143], loss=110.0927
	step [20/143], loss=102.0085
	step [21/143], loss=89.6089
	step [22/143], loss=115.3771
	step [23/143], loss=107.9251
	step [24/143], loss=112.0735
	step [25/143], loss=95.1841
	step [26/143], loss=91.5468
	step [27/143], loss=109.3593
	step [28/143], loss=112.9220
	step [29/143], loss=87.5677
	step [30/143], loss=114.9685
	step [31/143], loss=114.7893
	step [32/143], loss=100.5564
	step [33/143], loss=93.0488
	step [34/143], loss=98.6906
	step [35/143], loss=114.7141
	step [36/143], loss=116.8951
	step [37/143], loss=97.3024
	step [38/143], loss=106.0320
	step [39/143], loss=119.1782
	step [40/143], loss=95.8699
	step [41/143], loss=114.3884
	step [42/143], loss=106.5592
	step [43/143], loss=100.8760
	step [44/143], loss=101.8432
	step [45/143], loss=129.8874
	step [46/143], loss=103.0474
	step [47/143], loss=102.3378
	step [48/143], loss=102.5150
	step [49/143], loss=128.1881
	step [50/143], loss=114.5861
	step [51/143], loss=106.2517
	step [52/143], loss=93.5219
	step [53/143], loss=95.5243
	step [54/143], loss=99.6852
	step [55/143], loss=89.5840
	step [56/143], loss=119.6474
	step [57/143], loss=109.8614
	step [58/143], loss=94.7433
	step [59/143], loss=110.6232
	step [60/143], loss=86.2264
	step [61/143], loss=85.8596
	step [62/143], loss=105.2177
	step [63/143], loss=116.5429
	step [64/143], loss=90.1444
	step [65/143], loss=105.9915
	step [66/143], loss=96.5772
	step [67/143], loss=92.1296
	step [68/143], loss=93.1781
	step [69/143], loss=121.8368
	step [70/143], loss=96.9463
	step [71/143], loss=87.8770
	step [72/143], loss=105.1797
	step [73/143], loss=110.7657
	step [74/143], loss=113.8825
	step [75/143], loss=100.6078
	step [76/143], loss=112.2536
	step [77/143], loss=115.3770
	step [78/143], loss=94.0265
	step [79/143], loss=108.0060
	step [80/143], loss=91.9362
	step [81/143], loss=99.3961
	step [82/143], loss=98.7482
	step [83/143], loss=117.1942
	step [84/143], loss=97.8550
	step [85/143], loss=109.6558
	step [86/143], loss=89.6370
	step [87/143], loss=83.2006
	step [88/143], loss=99.6652
	step [89/143], loss=106.3536
	step [90/143], loss=106.8971
	step [91/143], loss=106.1119
	step [92/143], loss=99.6965
	step [93/143], loss=118.4628
	step [94/143], loss=110.0106
	step [95/143], loss=119.0495
	step [96/143], loss=88.8417
	step [97/143], loss=112.0689
	step [98/143], loss=100.4473
	step [99/143], loss=101.3162
	step [100/143], loss=91.1360
	step [101/143], loss=103.9420
	step [102/143], loss=95.9269
	step [103/143], loss=102.4074
	step [104/143], loss=103.3742
	step [105/143], loss=82.8331
	step [106/143], loss=98.7011
	step [107/143], loss=94.8042
	step [108/143], loss=92.5076
	step [109/143], loss=88.9176
	step [110/143], loss=107.1934
	step [111/143], loss=116.9970
	step [112/143], loss=104.8297
	step [113/143], loss=106.3962
	step [114/143], loss=112.1700
	step [115/143], loss=99.3812
	step [116/143], loss=107.8455
	step [117/143], loss=120.2259
	step [118/143], loss=131.3718
	step [119/143], loss=106.5703
	step [120/143], loss=97.5148
	step [121/143], loss=98.5258
	step [122/143], loss=97.2161
	step [123/143], loss=109.1329
	step [124/143], loss=95.2948
	step [125/143], loss=92.5404
	step [126/143], loss=110.2466
	step [127/143], loss=103.9423
	step [128/143], loss=102.1059
	step [129/143], loss=113.4901
	step [130/143], loss=95.3677
	step [131/143], loss=83.9169
	step [132/143], loss=93.0096
	step [133/143], loss=138.0547
	step [134/143], loss=108.4600
	step [135/143], loss=109.4273
	step [136/143], loss=91.7894
	step [137/143], loss=101.3010
	step [138/143], loss=120.9883
	step [139/143], loss=98.9656
	step [140/143], loss=80.6208
	step [141/143], loss=104.5080
	step [142/143], loss=81.3865
	step [143/143], loss=86.0823
	Evaluating
	loss=0.0303, precision=0.4153, recall=0.9233, f1=0.5729
Training epoch 18
	step [1/143], loss=113.5538
	step [2/143], loss=105.9655
	step [3/143], loss=98.3030
	step [4/143], loss=97.2393
	step [5/143], loss=95.9915
	step [6/143], loss=116.2347
	step [7/143], loss=104.4881
	step [8/143], loss=110.1203
	step [9/143], loss=97.5085
	step [10/143], loss=117.3738
	step [11/143], loss=121.0916
	step [12/143], loss=93.6284
	step [13/143], loss=111.6415
	step [14/143], loss=111.9585
	step [15/143], loss=106.1575
	step [16/143], loss=103.4457
	step [17/143], loss=125.2245
	step [18/143], loss=119.0087
	step [19/143], loss=106.1528
	step [20/143], loss=88.0306
	step [21/143], loss=115.1833
	step [22/143], loss=97.8320
	step [23/143], loss=109.5102
	step [24/143], loss=118.7207
	step [25/143], loss=97.3659
	step [26/143], loss=90.0054
	step [27/143], loss=122.4063
	step [28/143], loss=115.8978
	step [29/143], loss=109.5335
	step [30/143], loss=91.7847
	step [31/143], loss=84.7331
	step [32/143], loss=102.5591
	step [33/143], loss=91.6037
	step [34/143], loss=80.8669
	step [35/143], loss=97.0876
	step [36/143], loss=100.0004
	step [37/143], loss=86.9077
	step [38/143], loss=97.7117
	step [39/143], loss=129.4018
	step [40/143], loss=102.5827
	step [41/143], loss=124.1760
	step [42/143], loss=83.8594
	step [43/143], loss=106.7814
	step [44/143], loss=108.2168
	step [45/143], loss=104.9444
	step [46/143], loss=93.1481
	step [47/143], loss=94.8591
	step [48/143], loss=109.2493
	step [49/143], loss=104.9770
	step [50/143], loss=100.2999
	step [51/143], loss=104.1151
	step [52/143], loss=98.1463
	step [53/143], loss=115.9411
	step [54/143], loss=104.5292
	step [55/143], loss=92.4531
	step [56/143], loss=87.2920
	step [57/143], loss=92.5031
	step [58/143], loss=107.2893
	step [59/143], loss=86.4538
	step [60/143], loss=86.6693
	step [61/143], loss=118.1339
	step [62/143], loss=102.1749
	step [63/143], loss=87.5179
	step [64/143], loss=108.1337
	step [65/143], loss=85.7926
	step [66/143], loss=99.7227
	step [67/143], loss=93.2229
	step [68/143], loss=105.2245
	step [69/143], loss=110.9159
	step [70/143], loss=106.1195
	step [71/143], loss=107.6464
	step [72/143], loss=111.2325
	step [73/143], loss=102.0121
	step [74/143], loss=84.6525
	step [75/143], loss=127.6101
	step [76/143], loss=93.5889
	step [77/143], loss=106.1176
	step [78/143], loss=101.5270
	step [79/143], loss=99.1993
	step [80/143], loss=91.0908
	step [81/143], loss=93.9870
	step [82/143], loss=105.4163
	step [83/143], loss=94.3063
	step [84/143], loss=97.0695
	step [85/143], loss=108.6660
	step [86/143], loss=103.5717
	step [87/143], loss=93.6056
	step [88/143], loss=89.4892
	step [89/143], loss=102.2194
	step [90/143], loss=110.6809
	step [91/143], loss=104.2743
	step [92/143], loss=96.3504
	step [93/143], loss=99.4857
	step [94/143], loss=95.4841
	step [95/143], loss=92.0235
	step [96/143], loss=122.7927
	step [97/143], loss=94.2798
	step [98/143], loss=91.1493
	step [99/143], loss=104.9831
	step [100/143], loss=95.9158
	step [101/143], loss=109.8972
	step [102/143], loss=88.4080
	step [103/143], loss=104.1908
	step [104/143], loss=94.2931
	step [105/143], loss=101.1741
	step [106/143], loss=94.8877
	step [107/143], loss=97.8783
	step [108/143], loss=98.3602
	step [109/143], loss=86.2952
	step [110/143], loss=94.9373
	step [111/143], loss=116.5207
	step [112/143], loss=93.6392
	step [113/143], loss=98.8131
	step [114/143], loss=97.6111
	step [115/143], loss=94.3173
	step [116/143], loss=87.7741
	step [117/143], loss=102.2424
	step [118/143], loss=108.5310
	step [119/143], loss=103.8970
	step [120/143], loss=100.2968
	step [121/143], loss=113.9819
	step [122/143], loss=102.8419
	step [123/143], loss=102.2690
	step [124/143], loss=109.5395
	step [125/143], loss=94.7914
	step [126/143], loss=112.1970
	step [127/143], loss=104.1288
	step [128/143], loss=100.7607
	step [129/143], loss=107.4546
	step [130/143], loss=103.2724
	step [131/143], loss=91.5958
	step [132/143], loss=86.9777
	step [133/143], loss=114.9448
	step [134/143], loss=83.5643
	step [135/143], loss=96.6738
	step [136/143], loss=120.3875
	step [137/143], loss=100.9754
	step [138/143], loss=101.4918
	step [139/143], loss=119.4187
	step [140/143], loss=85.5008
	step [141/143], loss=90.2490
	step [142/143], loss=105.8627
	step [143/143], loss=81.1867
	Evaluating
	loss=0.0281, precision=0.4166, recall=0.8970, f1=0.5690
Training epoch 19
	step [1/143], loss=100.2041
	step [2/143], loss=88.0533
	step [3/143], loss=117.8722
	step [4/143], loss=81.8228
	step [5/143], loss=97.4198
	step [6/143], loss=117.0981
	step [7/143], loss=113.1813
	step [8/143], loss=97.0104
	step [9/143], loss=118.0528
	step [10/143], loss=97.1347
	step [11/143], loss=99.7371
	step [12/143], loss=102.3698
	step [13/143], loss=93.8566
	step [14/143], loss=95.2230
	step [15/143], loss=95.6650
	step [16/143], loss=95.3776
	step [17/143], loss=95.3385
	step [18/143], loss=98.1843
	step [19/143], loss=91.8179
	step [20/143], loss=102.9351
	step [21/143], loss=107.8121
	step [22/143], loss=110.1252
	step [23/143], loss=101.1813
	step [24/143], loss=95.5028
	step [25/143], loss=91.3182
	step [26/143], loss=98.9798
	step [27/143], loss=97.2359
	step [28/143], loss=93.9381
	step [29/143], loss=114.9600
	step [30/143], loss=75.7433
	step [31/143], loss=122.2242
	step [32/143], loss=100.3448
	step [33/143], loss=98.5396
	step [34/143], loss=99.5136
	step [35/143], loss=101.8092
	step [36/143], loss=104.8147
	step [37/143], loss=95.1494
	step [38/143], loss=93.5749
	step [39/143], loss=104.8540
	step [40/143], loss=85.9934
	step [41/143], loss=99.0142
	step [42/143], loss=116.0617
	step [43/143], loss=97.4433
	step [44/143], loss=108.1726
	step [45/143], loss=106.8787
	step [46/143], loss=95.2806
	step [47/143], loss=115.3780
	step [48/143], loss=88.0599
	step [49/143], loss=95.6447
	step [50/143], loss=100.9145
	step [51/143], loss=84.3556
	step [52/143], loss=101.8415
	step [53/143], loss=88.0481
	step [54/143], loss=112.4138
	step [55/143], loss=92.3678
	step [56/143], loss=93.6365
	step [57/143], loss=92.8187
	step [58/143], loss=102.1776
	step [59/143], loss=98.5105
	step [60/143], loss=119.6718
	step [61/143], loss=104.5768
	step [62/143], loss=99.3075
	step [63/143], loss=94.7596
	step [64/143], loss=105.1831
	step [65/143], loss=104.3214
	step [66/143], loss=121.0334
	step [67/143], loss=114.2039
	step [68/143], loss=97.4828
	step [69/143], loss=111.8478
	step [70/143], loss=110.5454
	step [71/143], loss=94.9300
	step [72/143], loss=95.9845
	step [73/143], loss=118.3900
	step [74/143], loss=87.9779
	step [75/143], loss=96.1319
	step [76/143], loss=105.6806
	step [77/143], loss=117.3897
	step [78/143], loss=96.2532
	step [79/143], loss=85.0077
	step [80/143], loss=111.5706
	step [81/143], loss=92.3057
	step [82/143], loss=100.4655
	step [83/143], loss=123.0817
	step [84/143], loss=82.5032
	step [85/143], loss=124.3126
	step [86/143], loss=100.2836
	step [87/143], loss=89.5508
	step [88/143], loss=99.8929
	step [89/143], loss=101.2049
	step [90/143], loss=90.6128
	step [91/143], loss=112.9778
	step [92/143], loss=100.6270
	step [93/143], loss=90.1133
	step [94/143], loss=118.1047
	step [95/143], loss=86.7701
	step [96/143], loss=97.7208
	step [97/143], loss=85.2856
	step [98/143], loss=93.7767
	step [99/143], loss=93.7372
	step [100/143], loss=97.5345
	step [101/143], loss=107.6417
	step [102/143], loss=89.2655
	step [103/143], loss=106.7551
	step [104/143], loss=99.3116
	step [105/143], loss=90.1779
	step [106/143], loss=100.3883
	step [107/143], loss=81.9400
	step [108/143], loss=98.2788
	step [109/143], loss=75.9296
	step [110/143], loss=99.3908
	step [111/143], loss=98.1149
	step [112/143], loss=106.7642
	step [113/143], loss=113.7017
	step [114/143], loss=101.4392
	step [115/143], loss=88.4678
	step [116/143], loss=109.3360
	step [117/143], loss=87.2725
	step [118/143], loss=106.4660
	step [119/143], loss=77.3116
	step [120/143], loss=109.4456
	step [121/143], loss=100.9035
	step [122/143], loss=105.2281
	step [123/143], loss=112.1975
	step [124/143], loss=108.9374
	step [125/143], loss=91.5177
	step [126/143], loss=94.5599
	step [127/143], loss=84.0694
	step [128/143], loss=118.7566
	step [129/143], loss=95.3505
	step [130/143], loss=84.3926
	step [131/143], loss=92.6371
	step [132/143], loss=96.0466
	step [133/143], loss=95.1561
	step [134/143], loss=86.0065
	step [135/143], loss=104.4890
	step [136/143], loss=94.4837
	step [137/143], loss=106.6296
	step [138/143], loss=121.1925
	step [139/143], loss=83.3551
	step [140/143], loss=102.3713
	step [141/143], loss=107.4223
	step [142/143], loss=102.5157
	step [143/143], loss=76.4445
	Evaluating
	loss=0.0243, precision=0.4999, recall=0.8805, f1=0.6377
Training epoch 20
	step [1/143], loss=100.5228
	step [2/143], loss=96.4793
	step [3/143], loss=90.0508
	step [4/143], loss=94.0944
	step [5/143], loss=94.8977
	step [6/143], loss=98.3569
	step [7/143], loss=99.5191
	step [8/143], loss=103.6237
	step [9/143], loss=96.0761
	step [10/143], loss=105.3600
	step [11/143], loss=111.6656
	step [12/143], loss=115.0161
	step [13/143], loss=92.9879
	step [14/143], loss=94.6580
	step [15/143], loss=109.9247
	step [16/143], loss=92.5938
	step [17/143], loss=104.7070
	step [18/143], loss=89.8456
	step [19/143], loss=113.1449
	step [20/143], loss=109.5768
	step [21/143], loss=83.8311
	step [22/143], loss=102.2013
	step [23/143], loss=109.9822
	step [24/143], loss=97.8813
	step [25/143], loss=89.0851
	step [26/143], loss=104.7791
	step [27/143], loss=82.7536
	step [28/143], loss=124.6974
	step [29/143], loss=96.8697
	step [30/143], loss=98.1927
	step [31/143], loss=108.1273
	step [32/143], loss=92.6313
	step [33/143], loss=78.1702
	step [34/143], loss=80.9282
	step [35/143], loss=95.8315
	step [36/143], loss=111.9995
	step [37/143], loss=93.3732
	step [38/143], loss=92.9945
	step [39/143], loss=106.9370
	step [40/143], loss=99.4209
	step [41/143], loss=81.9646
	step [42/143], loss=104.4426
	step [43/143], loss=94.6589
	step [44/143], loss=100.5328
	step [45/143], loss=89.8010
	step [46/143], loss=94.2458
	step [47/143], loss=95.5239
	step [48/143], loss=103.1720
	step [49/143], loss=91.9637
	step [50/143], loss=122.1363
	step [51/143], loss=105.4540
	step [52/143], loss=100.4250
	step [53/143], loss=91.7114
	step [54/143], loss=107.1369
	step [55/143], loss=98.1221
	step [56/143], loss=87.8246
	step [57/143], loss=88.6122
	step [58/143], loss=124.5675
	step [59/143], loss=99.8736
	step [60/143], loss=105.7751
	step [61/143], loss=103.9972
	step [62/143], loss=99.4940
	step [63/143], loss=101.6812
	step [64/143], loss=88.7539
	step [65/143], loss=110.4048
	step [66/143], loss=74.0593
	step [67/143], loss=86.7169
	step [68/143], loss=72.9854
	step [69/143], loss=101.4963
	step [70/143], loss=115.1623
	step [71/143], loss=105.2501
	step [72/143], loss=99.2109
	step [73/143], loss=74.2743
	step [74/143], loss=112.5508
	step [75/143], loss=92.7708
	step [76/143], loss=106.5021
	step [77/143], loss=99.3951
	step [78/143], loss=116.0301
	step [79/143], loss=101.8997
	step [80/143], loss=95.3706
	step [81/143], loss=99.2856
	step [82/143], loss=112.4696
	step [83/143], loss=91.3370
	step [84/143], loss=112.0924
	step [85/143], loss=105.0123
	step [86/143], loss=87.2535
	step [87/143], loss=99.6878
	step [88/143], loss=93.7727
	step [89/143], loss=101.3777
	step [90/143], loss=96.7665
	step [91/143], loss=88.1410
	step [92/143], loss=109.8949
	step [93/143], loss=99.1403
	step [94/143], loss=97.8448
	step [95/143], loss=70.0083
	step [96/143], loss=119.0554
	step [97/143], loss=83.3549
	step [98/143], loss=110.6957
	step [99/143], loss=95.4657
	step [100/143], loss=99.5863
	step [101/143], loss=91.9012
	step [102/143], loss=101.2828
	step [103/143], loss=103.2109
	step [104/143], loss=92.2745
	step [105/143], loss=89.0066
	step [106/143], loss=103.9236
	step [107/143], loss=100.2129
	step [108/143], loss=98.2579
	step [109/143], loss=93.7713
	step [110/143], loss=106.2057
	step [111/143], loss=85.6922
	step [112/143], loss=91.2634
	step [113/143], loss=98.1451
	step [114/143], loss=106.7807
	step [115/143], loss=97.7741
	step [116/143], loss=92.8363
	step [117/143], loss=99.9206
	step [118/143], loss=93.3799
	step [119/143], loss=93.4634
	step [120/143], loss=103.7277
	step [121/143], loss=108.5193
	step [122/143], loss=102.5011
	step [123/143], loss=91.3129
	step [124/143], loss=98.6044
	step [125/143], loss=98.9561
	step [126/143], loss=104.4456
	step [127/143], loss=93.4474
	step [128/143], loss=82.5289
	step [129/143], loss=104.8719
	step [130/143], loss=88.0018
	step [131/143], loss=92.9524
	step [132/143], loss=104.5692
	step [133/143], loss=98.5103
	step [134/143], loss=90.0731
	step [135/143], loss=86.1759
	step [136/143], loss=111.9209
	step [137/143], loss=107.6353
	step [138/143], loss=89.4039
	step [139/143], loss=95.9528
	step [140/143], loss=99.6015
	step [141/143], loss=89.6309
	step [142/143], loss=74.6636
	step [143/143], loss=102.8974
	Evaluating
	loss=0.0243, precision=0.4348, recall=0.8975, f1=0.5858
Training epoch 21
	step [1/143], loss=110.5878
	step [2/143], loss=111.1171
	step [3/143], loss=101.7598
	step [4/143], loss=80.5507
	step [5/143], loss=83.0680
	step [6/143], loss=110.2989
	step [7/143], loss=84.0871
	step [8/143], loss=79.2303
	step [9/143], loss=113.1922
	step [10/143], loss=96.6060
	step [11/143], loss=97.1780
	step [12/143], loss=82.1806
	step [13/143], loss=109.2743
	step [14/143], loss=101.5813
	step [15/143], loss=120.7780
	step [16/143], loss=94.8020
	step [17/143], loss=94.8912
	step [18/143], loss=98.1889
	step [19/143], loss=109.7772
	step [20/143], loss=103.9594
	step [21/143], loss=95.9541
	step [22/143], loss=89.8884
	step [23/143], loss=101.5001
	step [24/143], loss=91.8436
	step [25/143], loss=92.3175
	step [26/143], loss=73.2575
	step [27/143], loss=68.7221
	step [28/143], loss=89.5545
	step [29/143], loss=109.9196
	step [30/143], loss=101.5703
	step [31/143], loss=102.7079
	step [32/143], loss=88.3120
	step [33/143], loss=81.1462
	step [34/143], loss=82.2547
	step [35/143], loss=99.4049
	step [36/143], loss=86.2901
	step [37/143], loss=108.9744
	step [38/143], loss=118.4881
	step [39/143], loss=81.3741
	step [40/143], loss=90.7622
	step [41/143], loss=102.9461
	step [42/143], loss=97.7674
	step [43/143], loss=96.7141
	step [44/143], loss=111.2522
	step [45/143], loss=101.2567
	step [46/143], loss=97.1952
	step [47/143], loss=90.9704
	step [48/143], loss=109.7823
	step [49/143], loss=93.6114
	step [50/143], loss=109.8920
	step [51/143], loss=111.9682
	step [52/143], loss=83.2926
	step [53/143], loss=96.4959
	step [54/143], loss=103.6283
	step [55/143], loss=101.4880
	step [56/143], loss=121.7695
	step [57/143], loss=87.5368
	step [58/143], loss=126.7844
	step [59/143], loss=96.5218
	step [60/143], loss=95.1210
	step [61/143], loss=94.7395
	step [62/143], loss=89.2494
	step [63/143], loss=103.1707
	step [64/143], loss=99.4687
	step [65/143], loss=113.2153
	step [66/143], loss=97.2768
	step [67/143], loss=88.5920
	step [68/143], loss=93.1020
	step [69/143], loss=91.3995
	step [70/143], loss=93.5257
	step [71/143], loss=109.1789
	step [72/143], loss=102.2955
	step [73/143], loss=103.9675
	step [74/143], loss=98.3013
	step [75/143], loss=93.9422
	step [76/143], loss=79.1804
	step [77/143], loss=94.1453
	step [78/143], loss=95.7939
	step [79/143], loss=88.6265
	step [80/143], loss=104.3195
	step [81/143], loss=97.0077
	step [82/143], loss=88.2240
	step [83/143], loss=86.3754
	step [84/143], loss=67.9412
	step [85/143], loss=98.8433
	step [86/143], loss=108.6500
	step [87/143], loss=94.6839
	step [88/143], loss=102.0111
	step [89/143], loss=101.2885
	step [90/143], loss=113.5335
	step [91/143], loss=81.3914
	step [92/143], loss=96.5912
	step [93/143], loss=94.9191
	step [94/143], loss=111.2434
	step [95/143], loss=93.5658
	step [96/143], loss=90.3082
	step [97/143], loss=121.6865
	step [98/143], loss=96.4883
	step [99/143], loss=93.4597
	step [100/143], loss=101.0066
	step [101/143], loss=77.3998
	step [102/143], loss=85.6328
	step [103/143], loss=109.5536
	step [104/143], loss=91.0673
	step [105/143], loss=92.6047
	step [106/143], loss=94.1286
	step [107/143], loss=98.3057
	step [108/143], loss=110.6241
	step [109/143], loss=92.1880
	step [110/143], loss=99.7212
	step [111/143], loss=89.6376
	step [112/143], loss=107.6206
	step [113/143], loss=76.6041
	step [114/143], loss=97.0502
	step [115/143], loss=101.6205
	step [116/143], loss=87.8768
	step [117/143], loss=90.3269
	step [118/143], loss=94.5867
	step [119/143], loss=86.4261
	step [120/143], loss=78.8052
	step [121/143], loss=92.8292
	step [122/143], loss=115.7327
	step [123/143], loss=89.9978
	step [124/143], loss=92.4956
	step [125/143], loss=106.0028
	step [126/143], loss=101.8441
	step [127/143], loss=104.4412
	step [128/143], loss=115.8921
	step [129/143], loss=91.6906
	step [130/143], loss=87.2019
	step [131/143], loss=112.9534
	step [132/143], loss=101.9188
	step [133/143], loss=96.1240
	step [134/143], loss=89.5783
	step [135/143], loss=88.3231
	step [136/143], loss=100.3704
	step [137/143], loss=88.4257
	step [138/143], loss=108.8867
	step [139/143], loss=89.9160
	step [140/143], loss=90.1635
	step [141/143], loss=101.4145
	step [142/143], loss=97.2763
	step [143/143], loss=73.7281
	Evaluating
	loss=0.0192, precision=0.5384, recall=0.8699, f1=0.6651
saving model as: 1_saved_model.pth
Training epoch 22
	step [1/143], loss=95.5424
	step [2/143], loss=92.3530
	step [3/143], loss=87.4527
	step [4/143], loss=85.3955
	step [5/143], loss=87.4649
	step [6/143], loss=97.5934
	step [7/143], loss=104.4250
	step [8/143], loss=97.4025
	step [9/143], loss=78.9053
	step [10/143], loss=90.7548
	step [11/143], loss=92.2220
	step [12/143], loss=77.7202
	step [13/143], loss=88.6844
	step [14/143], loss=92.4500
	step [15/143], loss=108.6103
	step [16/143], loss=82.5222
	step [17/143], loss=77.2791
	step [18/143], loss=91.2094
	step [19/143], loss=92.5026
	step [20/143], loss=96.2508
	step [21/143], loss=101.4850
	step [22/143], loss=83.6063
	step [23/143], loss=101.6301
	step [24/143], loss=110.2191
	step [25/143], loss=103.6974
	step [26/143], loss=107.8077
	step [27/143], loss=87.9820
	step [28/143], loss=98.7928
	step [29/143], loss=93.1041
	step [30/143], loss=106.2010
	step [31/143], loss=94.4032
	step [32/143], loss=95.0646
	step [33/143], loss=102.2281
	step [34/143], loss=91.9533
	step [35/143], loss=92.8794
	step [36/143], loss=95.5854
	step [37/143], loss=102.3182
	step [38/143], loss=103.5645
	step [39/143], loss=86.3468
	step [40/143], loss=85.8078
	step [41/143], loss=99.8964
	step [42/143], loss=91.2618
	step [43/143], loss=80.7234
	step [44/143], loss=87.9008
	step [45/143], loss=102.5933
	step [46/143], loss=86.8556
	step [47/143], loss=92.8485
	step [48/143], loss=100.5238
	step [49/143], loss=107.5476
	step [50/143], loss=98.0549
	step [51/143], loss=91.5838
	step [52/143], loss=86.7259
	step [53/143], loss=90.0560
	step [54/143], loss=88.7273
	step [55/143], loss=96.7469
	step [56/143], loss=108.5514
	step [57/143], loss=100.6108
	step [58/143], loss=94.3791
	step [59/143], loss=94.7768
	step [60/143], loss=106.4555
	step [61/143], loss=92.3428
	step [62/143], loss=99.8160
	step [63/143], loss=101.1094
	step [64/143], loss=109.9640
	step [65/143], loss=85.2685
	step [66/143], loss=82.7608
	step [67/143], loss=102.0273
	step [68/143], loss=105.1451
	step [69/143], loss=108.9658
	step [70/143], loss=92.2605
	step [71/143], loss=116.0766
	step [72/143], loss=88.3341
	step [73/143], loss=95.1389
	step [74/143], loss=91.7506
	step [75/143], loss=83.3764
	step [76/143], loss=121.1229
	step [77/143], loss=95.5337
	step [78/143], loss=99.2870
	step [79/143], loss=79.9732
	step [80/143], loss=92.7124
	step [81/143], loss=80.8971
	step [82/143], loss=98.6013
	step [83/143], loss=98.9717
	step [84/143], loss=105.6143
	step [85/143], loss=116.2384
	step [86/143], loss=101.5455
	step [87/143], loss=106.8143
	step [88/143], loss=106.6191
	step [89/143], loss=97.3437
	step [90/143], loss=102.0551
	step [91/143], loss=108.9082
	step [92/143], loss=94.6500
	step [93/143], loss=87.2743
	step [94/143], loss=88.6720
	step [95/143], loss=85.8802
	step [96/143], loss=83.1651
	step [97/143], loss=103.8273
	step [98/143], loss=99.4151
	step [99/143], loss=122.2078
	step [100/143], loss=92.1540
	step [101/143], loss=82.5564
	step [102/143], loss=99.2690
	step [103/143], loss=75.6892
	step [104/143], loss=102.6388
	step [105/143], loss=101.8421
	step [106/143], loss=110.1400
	step [107/143], loss=89.8042
	step [108/143], loss=84.8393
	step [109/143], loss=109.6951
	step [110/143], loss=96.9442
	step [111/143], loss=110.7816
	step [112/143], loss=111.6882
	step [113/143], loss=77.0645
	step [114/143], loss=96.2815
	step [115/143], loss=90.9608
	step [116/143], loss=112.6097
	step [117/143], loss=97.4997
	step [118/143], loss=95.9500
	step [119/143], loss=75.7376
	step [120/143], loss=94.1188
	step [121/143], loss=93.6100
	step [122/143], loss=102.9543
	step [123/143], loss=93.2297
	step [124/143], loss=109.6382
	step [125/143], loss=97.6022
	step [126/143], loss=97.2701
	step [127/143], loss=89.7948
	step [128/143], loss=80.1624
	step [129/143], loss=92.9413
	step [130/143], loss=93.6994
	step [131/143], loss=101.5837
	step [132/143], loss=97.8591
	step [133/143], loss=93.9247
	step [134/143], loss=84.7944
	step [135/143], loss=94.6920
	step [136/143], loss=101.8556
	step [137/143], loss=96.3235
	step [138/143], loss=88.8576
	step [139/143], loss=89.5222
	step [140/143], loss=99.5882
	step [141/143], loss=97.8011
	step [142/143], loss=88.2825
	step [143/143], loss=74.1204
	Evaluating
	loss=0.0186, precision=0.4774, recall=0.9056, f1=0.6252
Training epoch 23
	step [1/143], loss=110.3378
	step [2/143], loss=103.1553
	step [3/143], loss=83.2164
	step [4/143], loss=109.7436
	step [5/143], loss=100.3231
	step [6/143], loss=94.6520
	step [7/143], loss=92.6579
	step [8/143], loss=117.4782
	step [9/143], loss=95.7472
	step [10/143], loss=101.6991
	step [11/143], loss=102.4279
	step [12/143], loss=101.2881
	step [13/143], loss=94.3788
	step [14/143], loss=113.2757
	step [15/143], loss=92.5814
	step [16/143], loss=84.5992
	step [17/143], loss=84.1921
	step [18/143], loss=96.6063
	step [19/143], loss=93.7254
	step [20/143], loss=95.8311
	step [21/143], loss=104.7759
	step [22/143], loss=84.3390
	step [23/143], loss=85.2957
	step [24/143], loss=90.4461
	step [25/143], loss=92.9187
	step [26/143], loss=73.7887
	step [27/143], loss=102.6343
	step [28/143], loss=86.9878
	step [29/143], loss=98.0860
	step [30/143], loss=88.2728
	step [31/143], loss=99.3411
	step [32/143], loss=98.7132
	step [33/143], loss=104.6702
	step [34/143], loss=84.8062
	step [35/143], loss=95.5580
	step [36/143], loss=98.8829
	step [37/143], loss=86.8276
	step [38/143], loss=87.4519
	step [39/143], loss=103.8141
	step [40/143], loss=103.6088
	step [41/143], loss=111.7084
	step [42/143], loss=89.2757
	step [43/143], loss=85.2553
	step [44/143], loss=81.4923
	step [45/143], loss=94.9170
	step [46/143], loss=92.1256
	step [47/143], loss=90.3163
	step [48/143], loss=82.5533
	step [49/143], loss=79.2953
	step [50/143], loss=92.2452
	step [51/143], loss=102.7168
	step [52/143], loss=103.7678
	step [53/143], loss=102.1474
	step [54/143], loss=99.5340
	step [55/143], loss=85.0891
	step [56/143], loss=81.4416
	step [57/143], loss=105.3432
	step [58/143], loss=88.7190
	step [59/143], loss=83.6684
	step [60/143], loss=91.0301
	step [61/143], loss=99.3616
	step [62/143], loss=109.0774
	step [63/143], loss=100.8953
	step [64/143], loss=86.6161
	step [65/143], loss=88.7982
	step [66/143], loss=82.1942
	step [67/143], loss=91.7053
	step [68/143], loss=97.3915
	step [69/143], loss=87.8990
	step [70/143], loss=102.5542
	step [71/143], loss=95.6686
	step [72/143], loss=91.7988
	step [73/143], loss=97.4764
	step [74/143], loss=99.8692
	step [75/143], loss=81.9216
	step [76/143], loss=88.4079
	step [77/143], loss=106.1827
	step [78/143], loss=87.0849
	step [79/143], loss=82.7665
	step [80/143], loss=96.8121
	step [81/143], loss=93.2712
	step [82/143], loss=100.6574
	step [83/143], loss=101.0436
	step [84/143], loss=111.2740
	step [85/143], loss=80.0264
	step [86/143], loss=88.7952
	step [87/143], loss=104.6502
	step [88/143], loss=89.6903
	step [89/143], loss=103.8857
	step [90/143], loss=91.6708
	step [91/143], loss=92.8231
	step [92/143], loss=95.4283
	step [93/143], loss=94.6043
	step [94/143], loss=95.9358
	step [95/143], loss=105.7015
	step [96/143], loss=89.1359
	step [97/143], loss=94.8431
	step [98/143], loss=96.8433
	step [99/143], loss=100.5189
	step [100/143], loss=82.4796
	step [101/143], loss=115.9965
	step [102/143], loss=96.5775
	step [103/143], loss=82.9975
	step [104/143], loss=94.0575
	step [105/143], loss=89.5593
	step [106/143], loss=107.8271
	step [107/143], loss=100.6845
	step [108/143], loss=81.3398
	step [109/143], loss=96.9062
	step [110/143], loss=81.6688
	step [111/143], loss=99.3694
	step [112/143], loss=102.6513
	step [113/143], loss=86.4958
	step [114/143], loss=97.7402
	step [115/143], loss=92.3717
	step [116/143], loss=110.7169
	step [117/143], loss=88.7160
	step [118/143], loss=95.3686
	step [119/143], loss=84.1392
	step [120/143], loss=109.4205
	step [121/143], loss=87.3190
	step [122/143], loss=88.9281
	step [123/143], loss=72.8659
	step [124/143], loss=113.2228
	step [125/143], loss=87.8073
	step [126/143], loss=97.3867
	step [127/143], loss=79.0385
	step [128/143], loss=100.1710
	step [129/143], loss=105.3504
	step [130/143], loss=80.8010
	step [131/143], loss=88.1461
	step [132/143], loss=104.3867
	step [133/143], loss=85.9783
	step [134/143], loss=94.1604
	step [135/143], loss=88.1236
	step [136/143], loss=94.2235
	step [137/143], loss=79.8583
	step [138/143], loss=77.9536
	step [139/143], loss=106.2061
	step [140/143], loss=74.6828
	step [141/143], loss=85.4081
	step [142/143], loss=99.4703
	step [143/143], loss=63.6170
	Evaluating
	loss=0.0176, precision=0.4704, recall=0.9058, f1=0.6192
Training epoch 24
	step [1/143], loss=92.6529
	step [2/143], loss=95.3927
	step [3/143], loss=101.8340
	step [4/143], loss=106.1194
	step [5/143], loss=92.3120
	step [6/143], loss=92.3590
	step [7/143], loss=95.1649
	step [8/143], loss=81.6785
	step [9/143], loss=83.5544
	step [10/143], loss=84.3851
	step [11/143], loss=74.7643
	step [12/143], loss=73.9749
	step [13/143], loss=80.3358
	step [14/143], loss=103.6438
	step [15/143], loss=83.0663
	step [16/143], loss=94.1107
	step [17/143], loss=109.7635
	step [18/143], loss=77.5287
	step [19/143], loss=98.3918
	step [20/143], loss=92.4875
	step [21/143], loss=82.1815
	step [22/143], loss=91.2461
	step [23/143], loss=89.7705
	step [24/143], loss=111.7215
	step [25/143], loss=107.0269
	step [26/143], loss=78.5291
	step [27/143], loss=91.2641
	step [28/143], loss=103.4526
	step [29/143], loss=81.1076
	step [30/143], loss=91.4757
	step [31/143], loss=96.7377
	step [32/143], loss=109.1474
	step [33/143], loss=103.1934
	step [34/143], loss=103.7031
	step [35/143], loss=104.1239
	step [36/143], loss=86.0319
	step [37/143], loss=94.3508
	step [38/143], loss=88.2449
	step [39/143], loss=87.9701
	step [40/143], loss=89.1657
	step [41/143], loss=85.7658
	step [42/143], loss=92.1936
	step [43/143], loss=95.1650
	step [44/143], loss=102.0028
	step [45/143], loss=85.3849
	step [46/143], loss=97.8754
	step [47/143], loss=96.0964
	step [48/143], loss=104.2057
	step [49/143], loss=107.1465
	step [50/143], loss=105.9846
	step [51/143], loss=90.8251
	step [52/143], loss=99.7176
	step [53/143], loss=99.0073
	step [54/143], loss=88.4268
	step [55/143], loss=81.6630
	step [56/143], loss=97.7991
	step [57/143], loss=88.8983
	step [58/143], loss=107.9832
	step [59/143], loss=91.3456
	step [60/143], loss=93.0753
	step [61/143], loss=88.7496
	step [62/143], loss=78.8401
	step [63/143], loss=95.8270
	step [64/143], loss=81.3074
	step [65/143], loss=96.3641
	step [66/143], loss=104.5811
	step [67/143], loss=109.0613
	step [68/143], loss=99.3350
	step [69/143], loss=92.4949
	step [70/143], loss=108.7585
	step [71/143], loss=98.8749
	step [72/143], loss=92.4812
	step [73/143], loss=101.9516
	step [74/143], loss=111.2265
	step [75/143], loss=100.3816
	step [76/143], loss=91.9599
	step [77/143], loss=97.2617
	step [78/143], loss=99.0135
	step [79/143], loss=75.0846
	step [80/143], loss=82.2862
	step [81/143], loss=90.4067
	step [82/143], loss=88.8234
	step [83/143], loss=77.0040
	step [84/143], loss=97.2610
	step [85/143], loss=90.7759
	step [86/143], loss=93.3991
	step [87/143], loss=86.5174
	step [88/143], loss=99.2675
	step [89/143], loss=95.4459
	step [90/143], loss=81.8282
	step [91/143], loss=76.8371
	step [92/143], loss=86.3400
	step [93/143], loss=84.9960
	step [94/143], loss=96.2074
	step [95/143], loss=85.9977
	step [96/143], loss=108.1943
	step [97/143], loss=95.9628
	step [98/143], loss=100.8177
	step [99/143], loss=89.3790
	step [100/143], loss=92.9037
	step [101/143], loss=84.4959
	step [102/143], loss=90.0872
	step [103/143], loss=81.9040
	step [104/143], loss=90.5522
	step [105/143], loss=105.5735
	step [106/143], loss=91.6721
	step [107/143], loss=89.0741
	step [108/143], loss=91.1923
	step [109/143], loss=91.5829
	step [110/143], loss=92.6695
	step [111/143], loss=115.9330
	step [112/143], loss=82.3004
	step [113/143], loss=85.8781
	step [114/143], loss=101.6786
	step [115/143], loss=84.2275
	step [116/143], loss=93.1684
	step [117/143], loss=94.3789
	step [118/143], loss=91.9531
	step [119/143], loss=99.5091
	step [120/143], loss=92.0754
	step [121/143], loss=93.2177
	step [122/143], loss=98.6599
	step [123/143], loss=93.7764
	step [124/143], loss=88.9268
	step [125/143], loss=77.6438
	step [126/143], loss=103.3691
	step [127/143], loss=84.6662
	step [128/143], loss=91.2667
	step [129/143], loss=98.0305
	step [130/143], loss=101.5215
	step [131/143], loss=104.5723
	step [132/143], loss=82.4367
	step [133/143], loss=74.7970
	step [134/143], loss=94.6772
	step [135/143], loss=99.6564
	step [136/143], loss=83.3353
	step [137/143], loss=80.3086
	step [138/143], loss=88.0903
	step [139/143], loss=86.3009
	step [140/143], loss=92.3209
	step [141/143], loss=79.6191
	step [142/143], loss=95.1177
	step [143/143], loss=92.7776
	Evaluating
	loss=0.0159, precision=0.5102, recall=0.8823, f1=0.6465
Training epoch 25
	step [1/143], loss=93.9809
	step [2/143], loss=101.9557
	step [3/143], loss=94.8136
	step [4/143], loss=83.7387
	step [5/143], loss=97.5995
	step [6/143], loss=109.0752
	step [7/143], loss=99.4311
	step [8/143], loss=88.3335
	step [9/143], loss=94.9983
	step [10/143], loss=92.6870
	step [11/143], loss=91.5108
	step [12/143], loss=92.6228
	step [13/143], loss=103.5540
	step [14/143], loss=96.2300
	step [15/143], loss=87.5069
	step [16/143], loss=91.5645
	step [17/143], loss=93.4954
	step [18/143], loss=85.2411
	step [19/143], loss=101.3989
	step [20/143], loss=70.5974
	step [21/143], loss=95.5460
	step [22/143], loss=90.2147
	step [23/143], loss=85.5138
	step [24/143], loss=91.6358
	step [25/143], loss=89.3411
	step [26/143], loss=102.7913
	step [27/143], loss=82.7720
	step [28/143], loss=69.7634
	step [29/143], loss=89.4011
	step [30/143], loss=93.4877
	step [31/143], loss=86.5445
	step [32/143], loss=90.2920
	step [33/143], loss=99.6813
	step [34/143], loss=90.0462
	step [35/143], loss=97.4381
	step [36/143], loss=87.7890
	step [37/143], loss=79.4742
	step [38/143], loss=80.9064
	step [39/143], loss=82.2453
	step [40/143], loss=98.8551
	step [41/143], loss=92.1923
	step [42/143], loss=87.7057
	step [43/143], loss=107.0432
	step [44/143], loss=82.1940
	step [45/143], loss=84.3104
	step [46/143], loss=82.2472
	step [47/143], loss=92.5558
	step [48/143], loss=100.6636
	step [49/143], loss=107.0343
	step [50/143], loss=82.7208
	step [51/143], loss=77.6093
	step [52/143], loss=78.4920
	step [53/143], loss=84.7301
	step [54/143], loss=94.3654
	step [55/143], loss=100.7980
	step [56/143], loss=91.0623
	step [57/143], loss=94.1494
	step [58/143], loss=96.2287
	step [59/143], loss=95.7678
	step [60/143], loss=87.7944
	step [61/143], loss=88.4764
	step [62/143], loss=76.2896
	step [63/143], loss=87.2540
	step [64/143], loss=84.6240
	step [65/143], loss=91.5974
	step [66/143], loss=106.4358
	step [67/143], loss=98.9172
	step [68/143], loss=89.1398
	step [69/143], loss=90.6414
	step [70/143], loss=88.5368
	step [71/143], loss=77.9245
	step [72/143], loss=89.6644
	step [73/143], loss=82.8955
	step [74/143], loss=99.8341
	step [75/143], loss=89.6997
	step [76/143], loss=77.3881
	step [77/143], loss=103.7487
	step [78/143], loss=86.3436
	step [79/143], loss=85.5250
	step [80/143], loss=106.5752
	step [81/143], loss=112.8122
	step [82/143], loss=90.0024
	step [83/143], loss=78.7047
	step [84/143], loss=97.2268
	step [85/143], loss=104.8409
	step [86/143], loss=102.3283
	step [87/143], loss=88.0179
	step [88/143], loss=94.6961
	step [89/143], loss=87.8800
	step [90/143], loss=114.4759
	step [91/143], loss=81.4039
	step [92/143], loss=78.7622
	step [93/143], loss=84.8724
	step [94/143], loss=78.3736
	step [95/143], loss=99.9169
	step [96/143], loss=91.6319
	step [97/143], loss=88.5701
	step [98/143], loss=78.6745
	step [99/143], loss=96.2288
	step [100/143], loss=104.9733
	step [101/143], loss=82.6647
	step [102/143], loss=85.3155
	step [103/143], loss=103.4442
	step [104/143], loss=84.3697
	step [105/143], loss=91.0694
	step [106/143], loss=69.7092
	step [107/143], loss=90.5819
	step [108/143], loss=98.2769
	step [109/143], loss=82.2879
	step [110/143], loss=98.9787
	step [111/143], loss=90.4446
	step [112/143], loss=89.4707
	step [113/143], loss=85.1627
	step [114/143], loss=99.7688
	step [115/143], loss=86.8238
	step [116/143], loss=80.5217
	step [117/143], loss=81.9495
	step [118/143], loss=81.1035
	step [119/143], loss=107.1407
	step [120/143], loss=91.6201
	step [121/143], loss=83.7412
	step [122/143], loss=86.2685
	step [123/143], loss=106.9629
	step [124/143], loss=78.6339
	step [125/143], loss=82.1305
	step [126/143], loss=88.7365
	step [127/143], loss=102.9513
	step [128/143], loss=104.7683
	step [129/143], loss=92.5095
	step [130/143], loss=71.6448
	step [131/143], loss=103.5970
	step [132/143], loss=102.7669
	step [133/143], loss=98.4510
	step [134/143], loss=100.7201
	step [135/143], loss=88.1957
	step [136/143], loss=88.6845
	step [137/143], loss=98.5112
	step [138/143], loss=85.9046
	step [139/143], loss=88.7420
	step [140/143], loss=106.9004
	step [141/143], loss=96.0818
	step [142/143], loss=88.2159
	step [143/143], loss=74.5304
	Evaluating
	loss=0.0149, precision=0.5255, recall=0.8647, f1=0.6537
Training epoch 26
	step [1/143], loss=75.6307
	step [2/143], loss=85.9517
	step [3/143], loss=88.9413
	step [4/143], loss=88.8786
	step [5/143], loss=101.0326
	step [6/143], loss=90.9658
	step [7/143], loss=83.4574
	step [8/143], loss=72.3799
	step [9/143], loss=86.4184
	step [10/143], loss=98.0615
	step [11/143], loss=72.8653
	step [12/143], loss=92.7189
	step [13/143], loss=87.7487
	step [14/143], loss=82.4395
	step [15/143], loss=84.8401
	step [16/143], loss=88.2712
	step [17/143], loss=102.1337
	step [18/143], loss=99.8833
	step [19/143], loss=100.5915
	step [20/143], loss=71.5511
	step [21/143], loss=103.7844
	step [22/143], loss=88.6130
	step [23/143], loss=104.5913
	step [24/143], loss=103.4615
	step [25/143], loss=90.9776
	step [26/143], loss=77.2857
	step [27/143], loss=85.5760
	step [28/143], loss=86.0215
	step [29/143], loss=82.7320
	step [30/143], loss=106.5272
	step [31/143], loss=90.5241
	step [32/143], loss=79.7485
	step [33/143], loss=84.3391
	step [34/143], loss=80.6552
	step [35/143], loss=87.7840
	step [36/143], loss=104.0315
	step [37/143], loss=109.1750
	step [38/143], loss=101.9133
	step [39/143], loss=86.9481
	step [40/143], loss=81.5614
	step [41/143], loss=88.2575
	step [42/143], loss=103.7138
	step [43/143], loss=94.7682
	step [44/143], loss=79.1525
	step [45/143], loss=82.9722
	step [46/143], loss=82.4054
	step [47/143], loss=90.5738
	step [48/143], loss=90.1711
	step [49/143], loss=68.2588
	step [50/143], loss=102.0890
	step [51/143], loss=98.1059
	step [52/143], loss=103.1983
	step [53/143], loss=83.6062
	step [54/143], loss=98.3907
	step [55/143], loss=115.4076
	step [56/143], loss=87.3145
	step [57/143], loss=87.2588
	step [58/143], loss=76.9796
	step [59/143], loss=93.8248
	step [60/143], loss=74.0704
	step [61/143], loss=97.5174
	step [62/143], loss=83.3758
	step [63/143], loss=112.3192
	step [64/143], loss=78.7004
	step [65/143], loss=100.8920
	step [66/143], loss=88.3186
	step [67/143], loss=70.9674
	step [68/143], loss=104.1671
	step [69/143], loss=82.2875
	step [70/143], loss=88.2261
	step [71/143], loss=97.8893
	step [72/143], loss=92.7138
	step [73/143], loss=95.6665
	step [74/143], loss=77.7379
	step [75/143], loss=91.3329
	step [76/143], loss=84.3517
	step [77/143], loss=79.5266
	step [78/143], loss=71.9927
	step [79/143], loss=95.0697
	step [80/143], loss=108.7322
	step [81/143], loss=89.6469
	step [82/143], loss=85.7340
	step [83/143], loss=79.5186
	step [84/143], loss=89.8927
	step [85/143], loss=92.1827
	step [86/143], loss=102.5237
	step [87/143], loss=91.7473
	step [88/143], loss=89.5516
	step [89/143], loss=89.8532
	step [90/143], loss=98.1965
	step [91/143], loss=79.8280
	step [92/143], loss=89.0754
	step [93/143], loss=104.1539
	step [94/143], loss=69.2473
	step [95/143], loss=79.8037
	step [96/143], loss=89.1971
	step [97/143], loss=99.5453
	step [98/143], loss=80.5532
	step [99/143], loss=87.6622
	step [100/143], loss=78.3465
	step [101/143], loss=104.0376
	step [102/143], loss=82.0487
	step [103/143], loss=112.4122
	step [104/143], loss=81.0791
	step [105/143], loss=90.4147
	step [106/143], loss=95.0477
	step [107/143], loss=72.2988
	step [108/143], loss=88.4774
	step [109/143], loss=92.1842
	step [110/143], loss=98.8726
	step [111/143], loss=77.8208
	step [112/143], loss=82.1245
	step [113/143], loss=88.6407
	step [114/143], loss=98.8964
	step [115/143], loss=88.5639
	step [116/143], loss=101.4524
	step [117/143], loss=75.1410
	step [118/143], loss=90.8067
	step [119/143], loss=106.4935
	step [120/143], loss=96.2084
	step [121/143], loss=107.7519
	step [122/143], loss=108.2046
	step [123/143], loss=99.1939
	step [124/143], loss=100.3043
	step [125/143], loss=97.8304
	step [126/143], loss=98.4669
	step [127/143], loss=74.4023
	step [128/143], loss=91.4135
	step [129/143], loss=81.5592
	step [130/143], loss=102.8204
	step [131/143], loss=83.7772
	step [132/143], loss=81.8087
	step [133/143], loss=94.4776
	step [134/143], loss=78.5722
	step [135/143], loss=90.8470
	step [136/143], loss=82.9767
	step [137/143], loss=112.0441
	step [138/143], loss=85.1884
	step [139/143], loss=98.9144
	step [140/143], loss=77.0273
	step [141/143], loss=81.7732
	step [142/143], loss=93.9361
	step [143/143], loss=73.5781
	Evaluating
	loss=0.0143, precision=0.4855, recall=0.8756, f1=0.6247
Training epoch 27
	step [1/143], loss=75.2859
	step [2/143], loss=82.3175
	step [3/143], loss=99.5967
	step [4/143], loss=82.7098
	step [5/143], loss=102.2553
	step [6/143], loss=91.9138
	step [7/143], loss=81.6931
	step [8/143], loss=92.6979
	step [9/143], loss=97.4146
	step [10/143], loss=85.6360
	step [11/143], loss=76.3071
	step [12/143], loss=88.6219
	step [13/143], loss=87.6064
	step [14/143], loss=97.1788
	step [15/143], loss=83.4411
	step [16/143], loss=98.0114
	step [17/143], loss=79.9314
	step [18/143], loss=79.1171
	step [19/143], loss=76.8663
	step [20/143], loss=96.3268
	step [21/143], loss=96.0375
	step [22/143], loss=87.9322
	step [23/143], loss=83.5556
	step [24/143], loss=83.5618
	step [25/143], loss=104.8130
	step [26/143], loss=100.9700
	step [27/143], loss=85.9635
	step [28/143], loss=93.9429
	step [29/143], loss=95.1361
	step [30/143], loss=82.2960
	step [31/143], loss=95.9921
	step [32/143], loss=93.7529
	step [33/143], loss=88.0732
	step [34/143], loss=95.4481
	step [35/143], loss=86.5406
	step [36/143], loss=82.1540
	step [37/143], loss=93.1980
	step [38/143], loss=91.4658
	step [39/143], loss=83.8120
	step [40/143], loss=95.5948
	step [41/143], loss=80.4375
	step [42/143], loss=75.8457
	step [43/143], loss=69.3475
	step [44/143], loss=91.1091
	step [45/143], loss=96.3875
	step [46/143], loss=97.8432
	step [47/143], loss=91.7569
	step [48/143], loss=81.9876
	step [49/143], loss=108.5803
	step [50/143], loss=110.0384
	step [51/143], loss=88.6826
	step [52/143], loss=75.6898
	step [53/143], loss=82.4594
	step [54/143], loss=92.6606
	step [55/143], loss=88.5615
	step [56/143], loss=89.8408
	step [57/143], loss=92.4339
	step [58/143], loss=77.7486
	step [59/143], loss=93.1199
	step [60/143], loss=79.2455
	step [61/143], loss=85.2298
	step [62/143], loss=85.9076
	step [63/143], loss=81.9705
	step [64/143], loss=75.1491
	step [65/143], loss=103.0633
	step [66/143], loss=96.4007
	step [67/143], loss=99.6283
	step [68/143], loss=89.6540
	step [69/143], loss=86.4752
	step [70/143], loss=95.3249
	step [71/143], loss=106.3289
	step [72/143], loss=80.6714
	step [73/143], loss=100.0754
	step [74/143], loss=80.8528
	step [75/143], loss=93.3929
	step [76/143], loss=75.7135
	step [77/143], loss=88.6032
	step [78/143], loss=94.2520
	step [79/143], loss=101.0502
	step [80/143], loss=69.8952
	step [81/143], loss=86.2825
	step [82/143], loss=75.0098
	step [83/143], loss=82.3570
	step [84/143], loss=93.1740
	step [85/143], loss=90.6462
	step [86/143], loss=73.4048
	step [87/143], loss=94.0155
	step [88/143], loss=87.1932
	step [89/143], loss=106.0730
	step [90/143], loss=85.5871
	step [91/143], loss=80.8683
	step [92/143], loss=99.0826
	step [93/143], loss=103.5723
	step [94/143], loss=101.7043
	step [95/143], loss=93.7554
	step [96/143], loss=76.0791
	step [97/143], loss=87.0019
	step [98/143], loss=72.5622
	step [99/143], loss=87.2763
	step [100/143], loss=89.9581
	step [101/143], loss=103.0823
	step [102/143], loss=93.1395
	step [103/143], loss=82.8452
	step [104/143], loss=104.9182
	step [105/143], loss=98.2213
	step [106/143], loss=81.6004
	step [107/143], loss=89.8376
	step [108/143], loss=96.6769
	step [109/143], loss=91.3488
	step [110/143], loss=97.2328
	step [111/143], loss=89.0427
	step [112/143], loss=92.8224
	step [113/143], loss=84.8766
	step [114/143], loss=95.6299
	step [115/143], loss=92.1964
	step [116/143], loss=81.2002
	step [117/143], loss=81.2848
	step [118/143], loss=98.0984
	step [119/143], loss=77.4418
	step [120/143], loss=80.3458
	step [121/143], loss=94.4771
	step [122/143], loss=92.7547
	step [123/143], loss=94.4890
	step [124/143], loss=75.2611
	step [125/143], loss=97.5104
	step [126/143], loss=92.2982
	step [127/143], loss=84.1957
	step [128/143], loss=101.6164
	step [129/143], loss=80.3198
	step [130/143], loss=86.7700
	step [131/143], loss=71.5792
	step [132/143], loss=71.7184
	step [133/143], loss=100.0787
	step [134/143], loss=90.7643
	step [135/143], loss=105.6770
	step [136/143], loss=100.8655
	step [137/143], loss=83.7130
	step [138/143], loss=89.4932
	step [139/143], loss=94.2658
	step [140/143], loss=73.3731
	step [141/143], loss=82.5685
	step [142/143], loss=85.5139
	step [143/143], loss=69.8726
	Evaluating
	loss=0.0134, precision=0.5085, recall=0.8519, f1=0.6369
Training epoch 28
	step [1/143], loss=80.8375
	step [2/143], loss=98.8543
	step [3/143], loss=88.3931
	step [4/143], loss=100.1329
	step [5/143], loss=97.1692
	step [6/143], loss=85.2111
	step [7/143], loss=92.5495
	step [8/143], loss=81.5478
	step [9/143], loss=89.7524
	step [10/143], loss=93.2478
	step [11/143], loss=103.2535
	step [12/143], loss=82.1637
	step [13/143], loss=99.8914
	step [14/143], loss=100.2943
	step [15/143], loss=87.8512
	step [16/143], loss=86.4955
	step [17/143], loss=79.4875
	step [18/143], loss=97.8207
	step [19/143], loss=67.0590
	step [20/143], loss=88.5448
	step [21/143], loss=80.3906
	step [22/143], loss=84.8774
	step [23/143], loss=98.4043
	step [24/143], loss=97.3241
	step [25/143], loss=102.6048
	step [26/143], loss=87.3791
	step [27/143], loss=93.1795
	step [28/143], loss=107.9412
	step [29/143], loss=90.1463
	step [30/143], loss=97.7960
	step [31/143], loss=102.2020
	step [32/143], loss=78.2905
	step [33/143], loss=85.7338
	step [34/143], loss=98.1640
	step [35/143], loss=76.2638
	step [36/143], loss=91.6242
	step [37/143], loss=80.6222
	step [38/143], loss=84.4372
	step [39/143], loss=91.3358
	step [40/143], loss=88.3612
	step [41/143], loss=85.1669
	step [42/143], loss=82.3559
	step [43/143], loss=78.6432
	step [44/143], loss=67.7026
	step [45/143], loss=77.1571
	step [46/143], loss=91.7823
	step [47/143], loss=78.9859
	step [48/143], loss=101.4968
	step [49/143], loss=84.7590
	step [50/143], loss=88.0107
	step [51/143], loss=82.1156
	step [52/143], loss=78.7053
	step [53/143], loss=101.5431
	step [54/143], loss=81.5560
	step [55/143], loss=86.5632
	step [56/143], loss=86.6751
	step [57/143], loss=95.9198
	step [58/143], loss=80.3787
	step [59/143], loss=84.1589
	step [60/143], loss=91.9102
	step [61/143], loss=97.2269
	step [62/143], loss=84.0093
	step [63/143], loss=75.1389
	step [64/143], loss=97.3336
	step [65/143], loss=88.9739
	step [66/143], loss=80.9766
	step [67/143], loss=95.6771
	step [68/143], loss=83.2639
	step [69/143], loss=87.3561
	step [70/143], loss=93.6788
	step [71/143], loss=78.7625
	step [72/143], loss=83.2141
	step [73/143], loss=88.8235
	step [74/143], loss=86.6468
	step [75/143], loss=88.1244
	step [76/143], loss=78.3847
	step [77/143], loss=104.1184
	step [78/143], loss=89.2044
	step [79/143], loss=73.0608
	step [80/143], loss=90.1842
	step [81/143], loss=84.1454
	step [82/143], loss=75.0312
	step [83/143], loss=80.2804
	step [84/143], loss=82.3777
	step [85/143], loss=86.3502
	step [86/143], loss=90.8308
	step [87/143], loss=94.4462
	step [88/143], loss=85.4536
	step [89/143], loss=83.7709
	step [90/143], loss=76.2764
	step [91/143], loss=75.9349
	step [92/143], loss=89.8653
	step [93/143], loss=80.2104
	step [94/143], loss=88.7992
	step [95/143], loss=86.6758
	step [96/143], loss=96.1783
	step [97/143], loss=85.4930
	step [98/143], loss=72.1943
	step [99/143], loss=71.7898
	step [100/143], loss=87.5585
	step [101/143], loss=86.7243
	step [102/143], loss=82.6140
	step [103/143], loss=94.7097
	step [104/143], loss=97.0128
	step [105/143], loss=81.1649
	step [106/143], loss=89.3939
	step [107/143], loss=90.7992
	step [108/143], loss=92.1423
	step [109/143], loss=91.9328
	step [110/143], loss=87.5254
	step [111/143], loss=84.2864
	step [112/143], loss=99.2088
	step [113/143], loss=88.6444
	step [114/143], loss=96.2561
	step [115/143], loss=87.0769
	step [116/143], loss=105.9016
	step [117/143], loss=91.3345
	step [118/143], loss=76.0037
	step [119/143], loss=96.3013
	step [120/143], loss=94.2750
	step [121/143], loss=79.9994
	step [122/143], loss=76.4250
	step [123/143], loss=91.5365
	step [124/143], loss=97.7884
	step [125/143], loss=91.6809
	step [126/143], loss=79.9984
	step [127/143], loss=84.1790
	step [128/143], loss=91.1315
	step [129/143], loss=78.1846
	step [130/143], loss=85.8268
	step [131/143], loss=79.6488
	step [132/143], loss=93.8523
	step [133/143], loss=91.5508
	step [134/143], loss=99.5612
	step [135/143], loss=93.2367
	step [136/143], loss=82.2646
	step [137/143], loss=103.7804
	step [138/143], loss=84.2047
	step [139/143], loss=71.0433
	step [140/143], loss=82.8875
	step [141/143], loss=83.1322
	step [142/143], loss=97.4607
	step [143/143], loss=73.7349
	Evaluating
	loss=0.0128, precision=0.5014, recall=0.8587, f1=0.6331
Training epoch 29
	step [1/143], loss=101.6001
	step [2/143], loss=74.6964
	step [3/143], loss=85.3503
	step [4/143], loss=73.8611
	step [5/143], loss=87.7456
	step [6/143], loss=88.2418
	step [7/143], loss=93.4492
	step [8/143], loss=68.5869
	step [9/143], loss=97.0596
	step [10/143], loss=79.1765
	step [11/143], loss=88.1164
	step [12/143], loss=92.1955
	step [13/143], loss=73.4433
	step [14/143], loss=97.4440
	step [15/143], loss=89.4281
	step [16/143], loss=72.4581
	step [17/143], loss=102.5586
	step [18/143], loss=94.0771
	step [19/143], loss=99.6521
	step [20/143], loss=87.3665
	step [21/143], loss=89.1063
	step [22/143], loss=77.6701
	step [23/143], loss=94.3474
	step [24/143], loss=94.2878
	step [25/143], loss=77.0499
	step [26/143], loss=74.7168
	step [27/143], loss=78.4416
	step [28/143], loss=90.4144
	step [29/143], loss=82.7682
	step [30/143], loss=88.1102
	step [31/143], loss=98.0698
	step [32/143], loss=76.4931
	step [33/143], loss=65.3832
	step [34/143], loss=87.2065
	step [35/143], loss=74.8985
	step [36/143], loss=71.0010
	step [37/143], loss=81.1777
	step [38/143], loss=91.0348
	step [39/143], loss=95.4189
	step [40/143], loss=74.4782
	step [41/143], loss=76.3534
	step [42/143], loss=78.7385
	step [43/143], loss=65.2779
	step [44/143], loss=77.0436
	step [45/143], loss=90.2226
	step [46/143], loss=92.0579
	step [47/143], loss=81.1122
	step [48/143], loss=81.8902
	step [49/143], loss=85.2762
	step [50/143], loss=95.6057
	step [51/143], loss=88.0565
	step [52/143], loss=87.3831
	step [53/143], loss=90.2488
	step [54/143], loss=71.6951
	step [55/143], loss=91.4759
	step [56/143], loss=85.4732
	step [57/143], loss=86.9314
	step [58/143], loss=91.1546
	step [59/143], loss=75.3914
	step [60/143], loss=78.1246
	step [61/143], loss=80.5206
	step [62/143], loss=86.0193
	step [63/143], loss=86.7870
	step [64/143], loss=93.4671
	step [65/143], loss=92.0157
	step [66/143], loss=88.6620
	step [67/143], loss=88.3774
	step [68/143], loss=100.3238
	step [69/143], loss=85.1870
	step [70/143], loss=83.5402
	step [71/143], loss=99.2292
	step [72/143], loss=88.3431
	step [73/143], loss=80.3531
	step [74/143], loss=99.4947
	step [75/143], loss=81.7709
	step [76/143], loss=71.5343
	step [77/143], loss=84.4324
	step [78/143], loss=81.7973
	step [79/143], loss=103.8368
	step [80/143], loss=80.7308
	step [81/143], loss=85.0322
	step [82/143], loss=72.1950
	step [83/143], loss=111.4292
	step [84/143], loss=93.6567
	step [85/143], loss=79.1192
	step [86/143], loss=87.1498
	step [87/143], loss=98.7972
	step [88/143], loss=85.0254
	step [89/143], loss=91.5446
	step [90/143], loss=97.0121
	step [91/143], loss=81.5667
	step [92/143], loss=85.1841
	step [93/143], loss=93.8380
	step [94/143], loss=94.6389
	step [95/143], loss=69.1016
	step [96/143], loss=98.0905
	step [97/143], loss=77.8126
	step [98/143], loss=86.4449
	step [99/143], loss=90.9465
	step [100/143], loss=95.2245
	step [101/143], loss=76.0301
	step [102/143], loss=90.0270
	step [103/143], loss=86.5931
	step [104/143], loss=97.8109
	step [105/143], loss=87.6632
	step [106/143], loss=74.3825
	step [107/143], loss=86.3985
	step [108/143], loss=100.3532
	step [109/143], loss=86.6030
	step [110/143], loss=92.7220
	step [111/143], loss=85.2584
	step [112/143], loss=87.2620
	step [113/143], loss=85.4695
	step [114/143], loss=85.5589
	step [115/143], loss=85.6247
	step [116/143], loss=92.7721
	step [117/143], loss=103.2381
	step [118/143], loss=92.7281
	step [119/143], loss=87.5402
	step [120/143], loss=98.7805
	step [121/143], loss=85.3602
	step [122/143], loss=87.8313
	step [123/143], loss=84.0126
	step [124/143], loss=84.2313
	step [125/143], loss=109.3130
	step [126/143], loss=87.0871
	step [127/143], loss=86.4007
	step [128/143], loss=97.9665
	step [129/143], loss=82.8145
	step [130/143], loss=91.3122
	step [131/143], loss=83.8624
	step [132/143], loss=80.8858
	step [133/143], loss=83.3213
	step [134/143], loss=88.8019
	step [135/143], loss=84.5078
	step [136/143], loss=81.8528
	step [137/143], loss=79.0820
	step [138/143], loss=94.6651
	step [139/143], loss=84.9599
	step [140/143], loss=105.8058
	step [141/143], loss=94.4877
	step [142/143], loss=79.9333
	step [143/143], loss=64.4780
	Evaluating
	loss=0.0134, precision=0.4644, recall=0.8788, f1=0.6076
Training epoch 30
	step [1/143], loss=90.6518
	step [2/143], loss=80.3923
	step [3/143], loss=91.4133
	step [4/143], loss=74.8466
	step [5/143], loss=72.4765
	step [6/143], loss=86.9250
	step [7/143], loss=81.8654
	step [8/143], loss=98.6007
	step [9/143], loss=93.4436
	step [10/143], loss=80.2455
	step [11/143], loss=80.6441
	step [12/143], loss=94.2267
	step [13/143], loss=100.8372
	step [14/143], loss=67.8895
	step [15/143], loss=82.7019
	step [16/143], loss=86.7959
	step [17/143], loss=96.5022
	step [18/143], loss=83.7072
	step [19/143], loss=90.8654
	step [20/143], loss=82.7635
	step [21/143], loss=84.6774
	step [22/143], loss=89.6872
	step [23/143], loss=67.2236
	step [24/143], loss=74.6964
	step [25/143], loss=90.2571
	step [26/143], loss=77.8184
	step [27/143], loss=74.8277
	step [28/143], loss=81.8813
	step [29/143], loss=80.7125
	step [30/143], loss=89.1169
	step [31/143], loss=95.4493
	step [32/143], loss=99.4325
	step [33/143], loss=81.4223
	step [34/143], loss=87.6848
	step [35/143], loss=96.0166
	step [36/143], loss=75.8395
	step [37/143], loss=87.2018
	step [38/143], loss=89.8615
	step [39/143], loss=104.3785
	step [40/143], loss=99.6599
	step [41/143], loss=72.0130
	step [42/143], loss=85.6769
	step [43/143], loss=83.3653
	step [44/143], loss=77.2366
	step [45/143], loss=78.7413
	step [46/143], loss=68.8851
	step [47/143], loss=84.5851
	step [48/143], loss=85.4424
	step [49/143], loss=90.6761
	step [50/143], loss=81.4329
	step [51/143], loss=92.6337
	step [52/143], loss=79.5501
	step [53/143], loss=75.5968
	step [54/143], loss=77.1091
	step [55/143], loss=86.1981
	step [56/143], loss=69.1963
	step [57/143], loss=84.3347
	step [58/143], loss=101.9705
	step [59/143], loss=84.4812
	step [60/143], loss=104.2204
	step [61/143], loss=87.3480
	step [62/143], loss=85.4052
	step [63/143], loss=71.8836
	step [64/143], loss=74.2926
	step [65/143], loss=92.1334
	step [66/143], loss=87.8963
	step [67/143], loss=107.4178
	step [68/143], loss=88.6729
	step [69/143], loss=94.1015
	step [70/143], loss=92.1158
	step [71/143], loss=65.7481
	step [72/143], loss=93.6532
	step [73/143], loss=79.1746
	step [74/143], loss=78.4919
	step [75/143], loss=76.3517
	step [76/143], loss=81.1284
	step [77/143], loss=82.7435
	step [78/143], loss=85.1445
	step [79/143], loss=90.0971
	step [80/143], loss=84.3176
	step [81/143], loss=78.9893
	step [82/143], loss=84.2018
	step [83/143], loss=85.0228
	step [84/143], loss=91.8069
	step [85/143], loss=100.2597
	step [86/143], loss=76.6365
	step [87/143], loss=102.2778
	step [88/143], loss=85.6336
	step [89/143], loss=67.8945
	step [90/143], loss=101.4127
	step [91/143], loss=89.1273
	step [92/143], loss=87.7763
	step [93/143], loss=86.4081
	step [94/143], loss=102.1543
	step [95/143], loss=80.7203
	step [96/143], loss=89.9044
	step [97/143], loss=82.3046
	step [98/143], loss=79.6637
	step [99/143], loss=74.1176
	step [100/143], loss=83.3165
	step [101/143], loss=87.3625
	step [102/143], loss=93.2591
	step [103/143], loss=86.0888
	step [104/143], loss=86.6280
	step [105/143], loss=76.3008
	step [106/143], loss=101.1299
	step [107/143], loss=89.6470
	step [108/143], loss=97.5230
	step [109/143], loss=89.3665
	step [110/143], loss=78.9177
	step [111/143], loss=89.5612
	step [112/143], loss=96.4286
	step [113/143], loss=95.5111
	step [114/143], loss=88.4795
	step [115/143], loss=79.8588
	step [116/143], loss=92.3101
	step [117/143], loss=77.4307
	step [118/143], loss=83.8945
	step [119/143], loss=108.9170
	step [120/143], loss=92.3078
	step [121/143], loss=85.3732
	step [122/143], loss=81.3991
	step [123/143], loss=80.1292
	step [124/143], loss=91.9457
	step [125/143], loss=78.8402
	step [126/143], loss=89.6739
	step [127/143], loss=81.0377
	step [128/143], loss=86.3363
	step [129/143], loss=96.5144
	step [130/143], loss=84.0840
	step [131/143], loss=100.5276
	step [132/143], loss=62.9435
	step [133/143], loss=63.2790
	step [134/143], loss=86.0302
	step [135/143], loss=74.7643
	step [136/143], loss=83.8420
	step [137/143], loss=79.5381
	step [138/143], loss=87.8649
	step [139/143], loss=70.9725
	step [140/143], loss=73.4158
	step [141/143], loss=90.7303
	step [142/143], loss=74.1214
	step [143/143], loss=77.9304
	Evaluating
	loss=0.0116, precision=0.5216, recall=0.8323, f1=0.6413
Training finished
best_f1: 0.6651241081591234
directing: Y rim_enhanced: True test_id 1
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15610 # image files with weight 15579
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_361_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_357_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_234_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_374_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_173_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_242_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_364_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_234_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_382_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4462 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=705.8049
	step [2/244], loss=334.2784
	step [3/244], loss=310.1613
	step [4/244], loss=320.3579
	step [5/244], loss=334.6386
	step [6/244], loss=369.3127
	step [7/244], loss=331.9867
	step [8/244], loss=281.6473
	step [9/244], loss=306.6971
	step [10/244], loss=333.8353
	step [11/244], loss=289.5878
	step [12/244], loss=298.2141
	step [13/244], loss=274.2940
	step [14/244], loss=300.0772
	step [15/244], loss=278.2296
	step [16/244], loss=274.7737
	step [17/244], loss=291.0134
	step [18/244], loss=285.8144
	step [19/244], loss=273.8563
	step [20/244], loss=258.3149
	step [21/244], loss=244.9962
	step [22/244], loss=247.0630
	step [23/244], loss=256.6059
	step [24/244], loss=276.0434
	step [25/244], loss=285.9168
	step [26/244], loss=263.7159
	step [27/244], loss=246.5262
	step [28/244], loss=231.6792
	step [29/244], loss=255.2710
	step [30/244], loss=265.5835
	step [31/244], loss=246.1644
	step [32/244], loss=253.1221
	step [33/244], loss=260.4626
	step [34/244], loss=251.8449
	step [35/244], loss=255.9042
	step [36/244], loss=228.4805
	step [37/244], loss=236.3527
	step [38/244], loss=247.2362
	step [39/244], loss=249.9612
	step [40/244], loss=233.5852
	step [41/244], loss=254.6871
	step [42/244], loss=226.0544
	step [43/244], loss=221.2654
	step [44/244], loss=232.5489
	step [45/244], loss=204.8129
	step [46/244], loss=221.3785
	step [47/244], loss=222.2122
	step [48/244], loss=216.2765
	step [49/244], loss=245.5235
	step [50/244], loss=208.0511
	step [51/244], loss=208.0450
	step [52/244], loss=209.4233
	step [53/244], loss=215.0372
	step [54/244], loss=254.0299
	step [55/244], loss=247.0561
	step [56/244], loss=214.6128
	step [57/244], loss=241.7853
	step [58/244], loss=224.2534
	step [59/244], loss=202.9771
	step [60/244], loss=218.8591
	step [61/244], loss=229.6240
	step [62/244], loss=225.1305
	step [63/244], loss=235.1239
	step [64/244], loss=229.0722
	step [65/244], loss=216.8779
	step [66/244], loss=222.0088
	step [67/244], loss=202.3686
	step [68/244], loss=214.4370
	step [69/244], loss=241.3841
	step [70/244], loss=198.0788
	step [71/244], loss=208.9667
	step [72/244], loss=216.7899
	step [73/244], loss=223.3429
	step [74/244], loss=212.8036
	step [75/244], loss=227.3135
	step [76/244], loss=203.4163
	step [77/244], loss=240.0917
	step [78/244], loss=203.1433
	step [79/244], loss=215.3835
	step [80/244], loss=218.6314
	step [81/244], loss=201.6937
	step [82/244], loss=208.1456
	step [83/244], loss=208.7967
	step [84/244], loss=212.7316
	step [85/244], loss=215.9524
	step [86/244], loss=225.3587
	step [87/244], loss=206.8754
	step [88/244], loss=208.9395
	step [89/244], loss=226.3383
	step [90/244], loss=204.8337
	step [91/244], loss=202.8409
	step [92/244], loss=219.0246
	step [93/244], loss=208.9016
	step [94/244], loss=218.5843
	step [95/244], loss=204.9062
	step [96/244], loss=207.9233
	step [97/244], loss=202.4876
	step [98/244], loss=208.9216
	step [99/244], loss=237.5917
	step [100/244], loss=197.4266
	step [101/244], loss=208.0109
	step [102/244], loss=211.7011
	step [103/244], loss=202.8794
	step [104/244], loss=194.3017
	step [105/244], loss=209.1841
	step [106/244], loss=188.5387
	step [107/244], loss=192.4725
	step [108/244], loss=182.5644
	step [109/244], loss=189.6394
	step [110/244], loss=198.1648
	step [111/244], loss=200.3412
	step [112/244], loss=203.4549
	step [113/244], loss=197.6264
	step [114/244], loss=184.8734
	step [115/244], loss=211.5598
	step [116/244], loss=191.2014
	step [117/244], loss=192.7582
	step [118/244], loss=194.9113
	step [119/244], loss=182.2651
	step [120/244], loss=189.2906
	step [121/244], loss=202.0578
	step [122/244], loss=194.0692
	step [123/244], loss=206.1728
	step [124/244], loss=184.0310
	step [125/244], loss=209.3445
	step [126/244], loss=207.4226
	step [127/244], loss=219.2508
	step [128/244], loss=192.8193
	step [129/244], loss=214.0901
	step [130/244], loss=202.8264
	step [131/244], loss=209.5779
	step [132/244], loss=195.3664
	step [133/244], loss=201.9320
	step [134/244], loss=222.7238
	step [135/244], loss=210.1173
	step [136/244], loss=189.3651
	step [137/244], loss=192.3981
	step [138/244], loss=183.8069
	step [139/244], loss=199.0953
	step [140/244], loss=183.0147
	step [141/244], loss=201.1135
	step [142/244], loss=187.6981
	step [143/244], loss=192.1541
	step [144/244], loss=174.9390
	step [145/244], loss=216.7491
	step [146/244], loss=181.1353
	step [147/244], loss=197.3046
	step [148/244], loss=198.5569
	step [149/244], loss=177.0364
	step [150/244], loss=215.4922
	step [151/244], loss=195.8012
	step [152/244], loss=191.7992
	step [153/244], loss=185.3948
	step [154/244], loss=186.4641
	step [155/244], loss=181.3951
	step [156/244], loss=161.2777
	step [157/244], loss=191.9276
	step [158/244], loss=210.1819
	step [159/244], loss=189.0041
	step [160/244], loss=181.4602
	step [161/244], loss=176.5920
	step [162/244], loss=187.6019
	step [163/244], loss=182.2737
	step [164/244], loss=188.9416
	step [165/244], loss=186.5880
	step [166/244], loss=177.3343
	step [167/244], loss=170.8343
	step [168/244], loss=192.3782
	step [169/244], loss=187.6395
	step [170/244], loss=173.0865
	step [171/244], loss=173.2703
	step [172/244], loss=183.9710
	step [173/244], loss=179.2528
	step [174/244], loss=165.6520
	step [175/244], loss=181.3313
	step [176/244], loss=169.1845
	step [177/244], loss=195.4298
	step [178/244], loss=186.6914
	step [179/244], loss=199.1372
	step [180/244], loss=183.8872
	step [181/244], loss=157.6388
	step [182/244], loss=194.9265
	step [183/244], loss=179.2794
	step [184/244], loss=174.5538
	step [185/244], loss=168.0563
	step [186/244], loss=188.3441
	step [187/244], loss=183.6567
	step [188/244], loss=196.1103
	step [189/244], loss=173.6150
	step [190/244], loss=194.8372
	step [191/244], loss=190.6552
	step [192/244], loss=173.9911
	step [193/244], loss=183.9706
	step [194/244], loss=199.8211
	step [195/244], loss=167.1973
	step [196/244], loss=176.3305
	step [197/244], loss=179.7327
	step [198/244], loss=198.7362
	step [199/244], loss=174.4488
	step [200/244], loss=162.8432
	step [201/244], loss=173.1526
	step [202/244], loss=164.2105
	step [203/244], loss=165.3425
	step [204/244], loss=161.9982
	step [205/244], loss=184.1336
	step [206/244], loss=155.9817
	step [207/244], loss=173.5011
	step [208/244], loss=162.4807
	step [209/244], loss=160.7372
	step [210/244], loss=201.0033
	step [211/244], loss=177.4571
	step [212/244], loss=170.0045
	step [213/244], loss=188.2066
	step [214/244], loss=181.3157
	step [215/244], loss=177.0277
	step [216/244], loss=163.3210
	step [217/244], loss=179.9678
	step [218/244], loss=178.7357
	step [219/244], loss=168.7556
	step [220/244], loss=183.9323
	step [221/244], loss=181.1075
	step [222/244], loss=166.2523
	step [223/244], loss=194.4619
	step [224/244], loss=179.3488
	step [225/244], loss=182.3699
	step [226/244], loss=182.4634
	step [227/244], loss=169.5198
	step [228/244], loss=180.7627
	step [229/244], loss=175.3843
	step [230/244], loss=170.3632
	step [231/244], loss=163.5044
	step [232/244], loss=182.5586
	step [233/244], loss=178.0942
	step [234/244], loss=189.2706
	step [235/244], loss=177.7376
	step [236/244], loss=176.6081
	step [237/244], loss=183.9458
	step [238/244], loss=172.4004
	step [239/244], loss=198.1497
	step [240/244], loss=183.6341
	step [241/244], loss=189.2164
	step [242/244], loss=173.7309
	step [243/244], loss=177.5698
	step [244/244], loss=69.4908
	Evaluating
	loss=0.3396, precision=0.2972, recall=0.8540, f1=0.4409
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/244], loss=155.9354
	step [2/244], loss=174.8481
	step [3/244], loss=166.0029
	step [4/244], loss=178.7101
	step [5/244], loss=184.9786
	step [6/244], loss=159.5439
	step [7/244], loss=204.2291
	step [8/244], loss=158.2861
	step [9/244], loss=181.1559
	step [10/244], loss=194.6753
	step [11/244], loss=159.0659
	step [12/244], loss=155.2036
	step [13/244], loss=175.1772
	step [14/244], loss=174.3392
	step [15/244], loss=197.8162
	step [16/244], loss=164.9500
	step [17/244], loss=187.2049
	step [18/244], loss=196.7044
	step [19/244], loss=176.9283
	step [20/244], loss=165.7191
	step [21/244], loss=155.2822
	step [22/244], loss=164.0562
	step [23/244], loss=166.9696
	step [24/244], loss=179.9094
	step [25/244], loss=167.8293
	step [26/244], loss=169.2248
	step [27/244], loss=165.6832
	step [28/244], loss=180.2481
	step [29/244], loss=174.6533
	step [30/244], loss=181.5643
	step [31/244], loss=169.3643
	step [32/244], loss=169.1124
	step [33/244], loss=159.0394
	step [34/244], loss=186.0410
	step [35/244], loss=169.5583
	step [36/244], loss=168.2765
	step [37/244], loss=172.2284
	step [38/244], loss=151.3141
	step [39/244], loss=181.0102
	step [40/244], loss=174.4352
	step [41/244], loss=162.8495
	step [42/244], loss=151.8477
	step [43/244], loss=157.2692
	step [44/244], loss=204.0150
	step [45/244], loss=160.0163
	step [46/244], loss=149.6112
	step [47/244], loss=172.2336
	step [48/244], loss=166.5595
	step [49/244], loss=207.1342
	step [50/244], loss=173.9308
	step [51/244], loss=168.0835
	step [52/244], loss=180.8978
	step [53/244], loss=163.8683
	step [54/244], loss=160.1738
	step [55/244], loss=175.5172
	step [56/244], loss=175.9409
	step [57/244], loss=173.2850
	step [58/244], loss=179.5366
	step [59/244], loss=153.2506
	step [60/244], loss=156.0344
	step [61/244], loss=176.8718
	step [62/244], loss=176.7323
	step [63/244], loss=154.4729
	step [64/244], loss=172.4691
	step [65/244], loss=191.5211
	step [66/244], loss=147.6221
	step [67/244], loss=174.1737
	step [68/244], loss=161.8849
	step [69/244], loss=174.8993
	step [70/244], loss=177.4965
	step [71/244], loss=178.2628
	step [72/244], loss=158.4968
	step [73/244], loss=163.0300
	step [74/244], loss=172.0610
	step [75/244], loss=162.7358
	step [76/244], loss=151.8782
	step [77/244], loss=169.1111
	step [78/244], loss=158.6334
	step [79/244], loss=146.6204
	step [80/244], loss=133.4304
	step [81/244], loss=170.9848
	step [82/244], loss=146.6009
	step [83/244], loss=173.9274
	step [84/244], loss=144.6051
	step [85/244], loss=149.3085
	step [86/244], loss=180.5297
	step [87/244], loss=156.8023
	step [88/244], loss=161.0820
	step [89/244], loss=161.4992
	step [90/244], loss=165.2823
	step [91/244], loss=156.8757
	step [92/244], loss=157.6288
	step [93/244], loss=173.9765
	step [94/244], loss=163.5250
	step [95/244], loss=162.7336
	step [96/244], loss=134.7897
	step [97/244], loss=192.1094
	step [98/244], loss=160.8641
	step [99/244], loss=145.8927
	step [100/244], loss=132.4737
	step [101/244], loss=162.5844
	step [102/244], loss=157.1526
	step [103/244], loss=147.4500
	step [104/244], loss=159.6405
	step [105/244], loss=168.5367
	step [106/244], loss=163.0373
	step [107/244], loss=141.7293
	step [108/244], loss=159.6636
	step [109/244], loss=174.7081
	step [110/244], loss=187.7894
	step [111/244], loss=147.8458
	step [112/244], loss=164.6780
	step [113/244], loss=146.9071
	step [114/244], loss=150.0280
	step [115/244], loss=139.6705
	step [116/244], loss=158.7565
	step [117/244], loss=146.6955
	step [118/244], loss=160.6118
	step [119/244], loss=173.7224
	step [120/244], loss=161.3444
	step [121/244], loss=165.4154
	step [122/244], loss=169.1597
	step [123/244], loss=145.4548
	step [124/244], loss=129.8466
	step [125/244], loss=159.7735
	step [126/244], loss=139.5237
	step [127/244], loss=150.0434
	step [128/244], loss=154.9741
	step [129/244], loss=146.9172
	step [130/244], loss=166.2356
	step [131/244], loss=181.6199
	step [132/244], loss=177.6324
	step [133/244], loss=155.8787
	step [134/244], loss=161.6097
	step [135/244], loss=167.0746
	step [136/244], loss=168.3226
	step [137/244], loss=160.8357
	step [138/244], loss=155.6812
	step [139/244], loss=162.5527
	step [140/244], loss=159.8933
	step [141/244], loss=139.7992
	step [142/244], loss=170.3110
	step [143/244], loss=163.6874
	step [144/244], loss=155.6231
	step [145/244], loss=148.6821
	step [146/244], loss=167.4577
	step [147/244], loss=153.1863
	step [148/244], loss=145.1621
	step [149/244], loss=156.1227
	step [150/244], loss=160.9422
	step [151/244], loss=140.7840
	step [152/244], loss=127.4074
	step [153/244], loss=157.4292
	step [154/244], loss=181.0373
	step [155/244], loss=149.1733
	step [156/244], loss=170.7849
	step [157/244], loss=156.6573
	step [158/244], loss=137.4115
	step [159/244], loss=158.8368
	step [160/244], loss=158.0768
	step [161/244], loss=171.5493
	step [162/244], loss=144.4917
	step [163/244], loss=133.3993
	step [164/244], loss=143.1143
	step [165/244], loss=163.7753
	step [166/244], loss=138.3177
	step [167/244], loss=150.4987
	step [168/244], loss=170.5346
	step [169/244], loss=149.8354
	step [170/244], loss=182.8929
	step [171/244], loss=146.1831
	step [172/244], loss=137.7607
	step [173/244], loss=139.8608
	step [174/244], loss=148.8477
	step [175/244], loss=147.7861
	step [176/244], loss=140.2141
	step [177/244], loss=147.1854
	step [178/244], loss=155.2058
	step [179/244], loss=152.1689
	step [180/244], loss=147.9117
	step [181/244], loss=150.8093
	step [182/244], loss=142.2347
	step [183/244], loss=148.5945
	step [184/244], loss=127.1213
	step [185/244], loss=141.4726
	step [186/244], loss=138.1977
	step [187/244], loss=135.3520
	step [188/244], loss=162.3552
	step [189/244], loss=149.9940
	step [190/244], loss=164.9229
	step [191/244], loss=142.6374
	step [192/244], loss=149.1948
	step [193/244], loss=155.4479
	step [194/244], loss=169.8074
	step [195/244], loss=156.3270
	step [196/244], loss=139.2662
	step [197/244], loss=155.0798
	step [198/244], loss=187.4292
	step [199/244], loss=134.3864
	step [200/244], loss=141.7965
	step [201/244], loss=156.5090
	step [202/244], loss=153.7748
	step [203/244], loss=159.8426
	step [204/244], loss=133.0614
	step [205/244], loss=170.9843
	step [206/244], loss=132.5509
	step [207/244], loss=165.9675
	step [208/244], loss=149.6461
	step [209/244], loss=169.5111
	step [210/244], loss=163.4541
	step [211/244], loss=162.1025
	step [212/244], loss=159.9910
	step [213/244], loss=147.7393
	step [214/244], loss=161.7093
	step [215/244], loss=136.5437
	step [216/244], loss=148.9337
	step [217/244], loss=137.9546
	step [218/244], loss=159.7221
	step [219/244], loss=142.6115
	step [220/244], loss=149.4306
	step [221/244], loss=148.3017
	step [222/244], loss=148.0863
	step [223/244], loss=149.8820
	step [224/244], loss=140.0109
	step [225/244], loss=138.2515
	step [226/244], loss=140.5316
	step [227/244], loss=129.8948
	step [228/244], loss=165.7719
	step [229/244], loss=143.0160
	step [230/244], loss=148.1031
	step [231/244], loss=150.3918
	step [232/244], loss=153.3575
	step [233/244], loss=151.9011
	step [234/244], loss=141.8785
	step [235/244], loss=155.9630
	step [236/244], loss=142.7881
	step [237/244], loss=165.1974
	step [238/244], loss=129.4982
	step [239/244], loss=170.1952
	step [240/244], loss=134.7531
	step [241/244], loss=150.8940
	step [242/244], loss=137.7044
	step [243/244], loss=172.7070
	step [244/244], loss=67.4812
	Evaluating
	loss=0.2164, precision=0.4242, recall=0.9125, f1=0.5792
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/244], loss=131.0195
	step [2/244], loss=160.9505
	step [3/244], loss=128.5600
	step [4/244], loss=147.0847
	step [5/244], loss=135.8469
	step [6/244], loss=136.9491
	step [7/244], loss=123.8116
	step [8/244], loss=134.6675
	step [9/244], loss=154.2447
	step [10/244], loss=135.1557
	step [11/244], loss=151.1845
	step [12/244], loss=155.5256
	step [13/244], loss=147.8912
	step [14/244], loss=145.9866
	step [15/244], loss=157.3712
	step [16/244], loss=144.9634
	step [17/244], loss=146.7419
	step [18/244], loss=165.0238
	step [19/244], loss=155.4990
	step [20/244], loss=152.9310
	step [21/244], loss=176.8631
	step [22/244], loss=139.4433
	step [23/244], loss=142.5711
	step [24/244], loss=161.5364
	step [25/244], loss=138.9547
	step [26/244], loss=130.0447
	step [27/244], loss=150.7902
	step [28/244], loss=139.9447
	step [29/244], loss=128.0465
	step [30/244], loss=162.3060
	step [31/244], loss=142.3943
	step [32/244], loss=169.7556
	step [33/244], loss=121.8624
	step [34/244], loss=128.0919
	step [35/244], loss=152.5498
	step [36/244], loss=129.5579
	step [37/244], loss=155.3699
	step [38/244], loss=128.4583
	step [39/244], loss=156.0816
	step [40/244], loss=130.9879
	step [41/244], loss=135.0692
	step [42/244], loss=139.4843
	step [43/244], loss=152.5177
	step [44/244], loss=129.2251
	step [45/244], loss=142.2120
	step [46/244], loss=168.3453
	step [47/244], loss=129.9833
	step [48/244], loss=156.7563
	step [49/244], loss=138.0454
	step [50/244], loss=151.5379
	step [51/244], loss=150.1588
	step [52/244], loss=131.8631
	step [53/244], loss=142.4138
	step [54/244], loss=145.8295
	step [55/244], loss=132.1678
	step [56/244], loss=145.5016
	step [57/244], loss=135.3794
	step [58/244], loss=133.8599
	step [59/244], loss=119.8547
	step [60/244], loss=140.6540
	step [61/244], loss=128.7545
	step [62/244], loss=136.6732
	step [63/244], loss=155.6646
	step [64/244], loss=130.9331
	step [65/244], loss=145.7960
	step [66/244], loss=154.3682
	step [67/244], loss=144.3735
	step [68/244], loss=143.8392
	step [69/244], loss=144.2449
	step [70/244], loss=158.1776
	step [71/244], loss=153.4105
	step [72/244], loss=149.4327
	step [73/244], loss=128.0904
	step [74/244], loss=168.3684
	step [75/244], loss=145.2453
	step [76/244], loss=152.5714
	step [77/244], loss=150.5647
	step [78/244], loss=127.7733
	step [79/244], loss=150.7588
	step [80/244], loss=151.3842
	step [81/244], loss=126.9044
	step [82/244], loss=140.1365
	step [83/244], loss=155.5008
	step [84/244], loss=133.2996
	step [85/244], loss=145.2952
	step [86/244], loss=132.9977
	step [87/244], loss=130.2635
	step [88/244], loss=126.5388
	step [89/244], loss=155.7780
	step [90/244], loss=146.8692
	step [91/244], loss=141.5290
	step [92/244], loss=135.7697
	step [93/244], loss=134.8004
	step [94/244], loss=137.2318
	step [95/244], loss=120.8869
	step [96/244], loss=160.1974
	step [97/244], loss=162.0310
	step [98/244], loss=156.7653
	step [99/244], loss=150.1834
	step [100/244], loss=147.6360
	step [101/244], loss=132.1301
	step [102/244], loss=143.6587
	step [103/244], loss=127.5921
	step [104/244], loss=150.5421
	step [105/244], loss=129.2393
	step [106/244], loss=129.1441
	step [107/244], loss=139.8385
	step [108/244], loss=153.6785
	step [109/244], loss=141.0005
	step [110/244], loss=150.7891
	step [111/244], loss=155.9214
	step [112/244], loss=137.8459
	step [113/244], loss=142.7832
	step [114/244], loss=141.7372
	step [115/244], loss=140.2874
	step [116/244], loss=158.7620
	step [117/244], loss=134.9978
	step [118/244], loss=140.4832
	step [119/244], loss=141.0415
	step [120/244], loss=144.9459
	step [121/244], loss=131.5244
	step [122/244], loss=136.9359
	step [123/244], loss=135.7186
	step [124/244], loss=141.8959
	step [125/244], loss=138.1641
	step [126/244], loss=136.6143
	step [127/244], loss=126.1495
	step [128/244], loss=144.3544
	step [129/244], loss=136.8601
	step [130/244], loss=130.2835
	step [131/244], loss=139.5993
	step [132/244], loss=129.1740
	step [133/244], loss=150.7198
	step [134/244], loss=141.7196
	step [135/244], loss=119.6891
	step [136/244], loss=146.6417
	step [137/244], loss=121.6494
	step [138/244], loss=140.1373
	step [139/244], loss=134.3918
	step [140/244], loss=135.7038
	step [141/244], loss=115.1097
	step [142/244], loss=147.6899
	step [143/244], loss=136.7324
	step [144/244], loss=155.6110
	step [145/244], loss=143.7813
	step [146/244], loss=121.8792
	step [147/244], loss=136.8388
	step [148/244], loss=136.9271
	step [149/244], loss=141.6113
	step [150/244], loss=128.3510
	step [151/244], loss=150.3618
	step [152/244], loss=134.7200
	step [153/244], loss=140.8453
	step [154/244], loss=151.1772
	step [155/244], loss=141.5374
	step [156/244], loss=161.2923
	step [157/244], loss=131.7625
	step [158/244], loss=134.3858
	step [159/244], loss=137.4019
	step [160/244], loss=129.6422
	step [161/244], loss=148.7693
	step [162/244], loss=126.6195
	step [163/244], loss=122.0690
	step [164/244], loss=139.1837
	step [165/244], loss=128.7253
	step [166/244], loss=140.7377
	step [167/244], loss=134.7825
	step [168/244], loss=145.3686
	step [169/244], loss=157.8640
	step [170/244], loss=127.4632
	step [171/244], loss=144.1615
	step [172/244], loss=139.4644
	step [173/244], loss=140.3437
	step [174/244], loss=124.7298
	step [175/244], loss=139.1735
	step [176/244], loss=140.0269
	step [177/244], loss=114.0751
	step [178/244], loss=131.0838
	step [179/244], loss=122.3154
	step [180/244], loss=138.1515
	step [181/244], loss=132.3405
	step [182/244], loss=143.9413
	step [183/244], loss=146.8344
	step [184/244], loss=149.7017
	step [185/244], loss=122.3682
	step [186/244], loss=152.1612
	step [187/244], loss=124.5540
	step [188/244], loss=137.9847
	step [189/244], loss=157.5214
	step [190/244], loss=118.5799
	step [191/244], loss=130.9061
	step [192/244], loss=139.6226
	step [193/244], loss=137.8881
	step [194/244], loss=129.0146
	step [195/244], loss=138.2236
	step [196/244], loss=136.6477
	step [197/244], loss=124.6259
	step [198/244], loss=131.1075
	step [199/244], loss=143.2986
	step [200/244], loss=126.5702
	step [201/244], loss=126.5652
	step [202/244], loss=113.7737
	step [203/244], loss=164.5947
	step [204/244], loss=133.2309
	step [205/244], loss=148.1300
	step [206/244], loss=132.1024
	step [207/244], loss=127.9260
	step [208/244], loss=142.6289
	step [209/244], loss=127.1191
	step [210/244], loss=140.7402
	step [211/244], loss=122.6653
	step [212/244], loss=133.3234
	step [213/244], loss=138.4633
	step [214/244], loss=119.4864
	step [215/244], loss=151.0772
	step [216/244], loss=131.9834
	step [217/244], loss=110.5778
	step [218/244], loss=143.2114
	step [219/244], loss=132.3921
	step [220/244], loss=125.8305
	step [221/244], loss=145.0438
	step [222/244], loss=113.7001
	step [223/244], loss=141.3867
	step [224/244], loss=145.4778
	step [225/244], loss=132.5785
	step [226/244], loss=145.4500
	step [227/244], loss=139.3403
	step [228/244], loss=126.6841
	step [229/244], loss=117.2030
	step [230/244], loss=138.2017
	step [231/244], loss=129.1172
	step [232/244], loss=123.4242
	step [233/244], loss=141.4430
	step [234/244], loss=130.0730
	step [235/244], loss=133.9289
	step [236/244], loss=130.2863
	step [237/244], loss=133.4425
	step [238/244], loss=127.4907
	step [239/244], loss=145.8916
	step [240/244], loss=124.0592
	step [241/244], loss=125.2012
	step [242/244], loss=135.1599
	step [243/244], loss=129.4468
	step [244/244], loss=62.2767
	Evaluating
	loss=0.1512, precision=0.4612, recall=0.9014, f1=0.6102
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/244], loss=141.6000
	step [2/244], loss=124.1419
	step [3/244], loss=134.4919
	step [4/244], loss=124.9326
	step [5/244], loss=120.6158
	step [6/244], loss=133.5044
	step [7/244], loss=143.7740
	step [8/244], loss=140.3931
	step [9/244], loss=131.6000
	step [10/244], loss=118.7531
	step [11/244], loss=131.7800
	step [12/244], loss=133.2202
	step [13/244], loss=131.3982
	step [14/244], loss=128.9253
	step [15/244], loss=151.9974
	step [16/244], loss=124.2982
	step [17/244], loss=126.7324
	step [18/244], loss=140.2810
	step [19/244], loss=138.2499
	step [20/244], loss=157.8353
	step [21/244], loss=135.6066
	step [22/244], loss=130.5052
	step [23/244], loss=124.9086
	step [24/244], loss=155.0162
	step [25/244], loss=124.1789
	step [26/244], loss=123.9396
	step [27/244], loss=129.1800
	step [28/244], loss=132.8293
	step [29/244], loss=148.3057
	step [30/244], loss=126.1103
	step [31/244], loss=132.0367
	step [32/244], loss=135.9109
	step [33/244], loss=137.8777
	step [34/244], loss=129.5717
	step [35/244], loss=139.6949
	step [36/244], loss=122.6389
	step [37/244], loss=132.9328
	step [38/244], loss=118.3151
	step [39/244], loss=109.1823
	step [40/244], loss=124.4805
	step [41/244], loss=123.8173
	step [42/244], loss=107.2428
	step [43/244], loss=137.8303
	step [44/244], loss=101.1944
	step [45/244], loss=138.4963
	step [46/244], loss=133.1282
	step [47/244], loss=133.7807
	step [48/244], loss=121.0780
	step [49/244], loss=137.6242
	step [50/244], loss=125.7424
	step [51/244], loss=111.8672
	step [52/244], loss=129.4900
	step [53/244], loss=120.2502
	step [54/244], loss=133.8840
	step [55/244], loss=142.3769
	step [56/244], loss=122.6686
	step [57/244], loss=118.9380
	step [58/244], loss=147.7990
	step [59/244], loss=124.3992
	step [60/244], loss=126.8456
	step [61/244], loss=126.3516
	step [62/244], loss=132.9865
	step [63/244], loss=144.6774
	step [64/244], loss=124.0280
	step [65/244], loss=118.5321
	step [66/244], loss=128.8740
	step [67/244], loss=125.8785
	step [68/244], loss=108.0449
	step [69/244], loss=114.8831
	step [70/244], loss=146.0679
	step [71/244], loss=146.3489
	step [72/244], loss=151.2297
	step [73/244], loss=144.4940
	step [74/244], loss=148.9761
	step [75/244], loss=122.7815
	step [76/244], loss=112.8277
	step [77/244], loss=125.5196
	step [78/244], loss=105.9795
	step [79/244], loss=133.5587
	step [80/244], loss=136.5092
	step [81/244], loss=122.5771
	step [82/244], loss=128.8926
	step [83/244], loss=128.1490
	step [84/244], loss=115.2502
	step [85/244], loss=132.4598
	step [86/244], loss=112.9838
	step [87/244], loss=125.0702
	step [88/244], loss=122.5531
	step [89/244], loss=134.1122
	step [90/244], loss=119.7988
	step [91/244], loss=122.3979
	step [92/244], loss=119.6477
	step [93/244], loss=142.1569
	step [94/244], loss=134.0658
	step [95/244], loss=120.1615
	step [96/244], loss=147.1100
	step [97/244], loss=140.1827
	step [98/244], loss=128.1999
	step [99/244], loss=126.7271
	step [100/244], loss=128.3270
	step [101/244], loss=129.7438
	step [102/244], loss=143.9012
	step [103/244], loss=131.8847
	step [104/244], loss=135.1978
	step [105/244], loss=127.7936
	step [106/244], loss=123.2288
	step [107/244], loss=107.8317
	step [108/244], loss=144.8091
	step [109/244], loss=140.3998
	step [110/244], loss=130.6930
	step [111/244], loss=133.2168
	step [112/244], loss=146.9714
	step [113/244], loss=122.9669
	step [114/244], loss=108.0937
	step [115/244], loss=109.0552
	step [116/244], loss=130.4530
	step [117/244], loss=118.7312
	step [118/244], loss=134.2235
	step [119/244], loss=115.2252
	step [120/244], loss=122.1849
	step [121/244], loss=116.3861
	step [122/244], loss=132.0903
	step [123/244], loss=108.3188
	step [124/244], loss=135.9689
	step [125/244], loss=133.9726
	step [126/244], loss=130.9918
	step [127/244], loss=128.1033
	step [128/244], loss=134.6433
	step [129/244], loss=147.8329
	step [130/244], loss=122.9617
	step [131/244], loss=154.0833
	step [132/244], loss=117.7003
	step [133/244], loss=119.3770
	step [134/244], loss=112.9948
	step [135/244], loss=117.1087
	step [136/244], loss=119.5441
	step [137/244], loss=131.0885
	step [138/244], loss=149.9226
	step [139/244], loss=136.5795
	step [140/244], loss=119.8215
	step [141/244], loss=140.6455
	step [142/244], loss=120.0431
	step [143/244], loss=128.7492
	step [144/244], loss=116.2909
	step [145/244], loss=114.4699
	step [146/244], loss=146.1113
	step [147/244], loss=138.9840
	step [148/244], loss=129.3270
	step [149/244], loss=119.9629
	step [150/244], loss=131.0428
	step [151/244], loss=130.4065
	step [152/244], loss=123.5601
	step [153/244], loss=113.5721
	step [154/244], loss=95.3187
	step [155/244], loss=134.5531
	step [156/244], loss=153.8868
	step [157/244], loss=128.6751
	step [158/244], loss=143.7617
	step [159/244], loss=126.2681
	step [160/244], loss=134.1552
	step [161/244], loss=113.9863
	step [162/244], loss=108.6316
	step [163/244], loss=138.2695
	step [164/244], loss=136.2282
	step [165/244], loss=129.1936
	step [166/244], loss=119.5402
	step [167/244], loss=127.1737
	step [168/244], loss=113.8250
	step [169/244], loss=115.8867
	step [170/244], loss=117.5252
	step [171/244], loss=114.1879
	step [172/244], loss=127.8707
	step [173/244], loss=115.9227
	step [174/244], loss=127.7503
	step [175/244], loss=111.7894
	step [176/244], loss=124.7256
	step [177/244], loss=115.0574
	step [178/244], loss=114.9094
	step [179/244], loss=131.7186
	step [180/244], loss=128.9287
	step [181/244], loss=130.4314
	step [182/244], loss=126.8177
	step [183/244], loss=140.5112
	step [184/244], loss=116.6385
	step [185/244], loss=132.9356
	step [186/244], loss=122.2222
	step [187/244], loss=139.2901
	step [188/244], loss=108.8219
	step [189/244], loss=130.9927
	step [190/244], loss=129.4804
	step [191/244], loss=131.9061
	step [192/244], loss=146.7023
	step [193/244], loss=106.9486
	step [194/244], loss=126.2539
	step [195/244], loss=139.5885
	step [196/244], loss=109.9552
	step [197/244], loss=101.0777
	step [198/244], loss=137.3153
	step [199/244], loss=106.5516
	step [200/244], loss=132.8283
	step [201/244], loss=120.1918
	step [202/244], loss=133.1514
	step [203/244], loss=109.3882
	step [204/244], loss=118.7263
	step [205/244], loss=115.3870
	step [206/244], loss=112.6992
	step [207/244], loss=152.8254
	step [208/244], loss=104.9127
	step [209/244], loss=118.1955
	step [210/244], loss=123.3345
	step [211/244], loss=112.1578
	step [212/244], loss=121.2610
	step [213/244], loss=110.4043
	step [214/244], loss=126.2776
	step [215/244], loss=143.3535
	step [216/244], loss=118.9999
	step [217/244], loss=128.2098
	step [218/244], loss=111.4888
	step [219/244], loss=138.8210
	step [220/244], loss=127.3227
	step [221/244], loss=123.5886
	step [222/244], loss=121.5090
	step [223/244], loss=123.4379
	step [224/244], loss=128.7332
	step [225/244], loss=111.2211
	step [226/244], loss=129.8608
	step [227/244], loss=126.0821
	step [228/244], loss=114.3240
	step [229/244], loss=115.8231
	step [230/244], loss=123.9239
	step [231/244], loss=124.3015
	step [232/244], loss=107.2316
	step [233/244], loss=120.8905
	step [234/244], loss=119.5545
	step [235/244], loss=125.9463
	step [236/244], loss=142.2632
	step [237/244], loss=111.6492
	step [238/244], loss=113.8075
	step [239/244], loss=116.0244
	step [240/244], loss=121.8348
	step [241/244], loss=133.9111
	step [242/244], loss=105.0398
	step [243/244], loss=130.3402
	step [244/244], loss=45.4170
	Evaluating
	loss=0.1043, precision=0.4165, recall=0.9107, f1=0.5716
Training epoch 5
	step [1/244], loss=135.2065
	step [2/244], loss=135.6076
	step [3/244], loss=124.8035
	step [4/244], loss=112.1907
	step [5/244], loss=122.7919
	step [6/244], loss=140.4078
	step [7/244], loss=111.1495
	step [8/244], loss=107.8607
	step [9/244], loss=140.6211
	step [10/244], loss=136.9669
	step [11/244], loss=110.0921
	step [12/244], loss=133.1216
	step [13/244], loss=109.4249
	step [14/244], loss=101.7574
	step [15/244], loss=125.7336
	step [16/244], loss=142.1794
	step [17/244], loss=124.1431
	step [18/244], loss=144.0669
	step [19/244], loss=123.6071
	step [20/244], loss=106.8890
	step [21/244], loss=127.9693
	step [22/244], loss=128.3170
	step [23/244], loss=113.1645
	step [24/244], loss=111.7438
	step [25/244], loss=129.6295
	step [26/244], loss=113.6779
	step [27/244], loss=136.8627
	step [28/244], loss=103.5636
	step [29/244], loss=124.4023
	step [30/244], loss=122.7326
	step [31/244], loss=128.2592
	step [32/244], loss=135.9452
	step [33/244], loss=127.7084
	step [34/244], loss=128.2762
	step [35/244], loss=117.8651
	step [36/244], loss=108.3959
	step [37/244], loss=125.9736
	step [38/244], loss=116.3712
	step [39/244], loss=121.7347
	step [40/244], loss=129.9198
	step [41/244], loss=106.3330
	step [42/244], loss=106.9680
	step [43/244], loss=118.7276
	step [44/244], loss=131.3510
	step [45/244], loss=115.7176
	step [46/244], loss=117.4827
	step [47/244], loss=112.3818
	step [48/244], loss=110.9386
	step [49/244], loss=106.0602
	step [50/244], loss=123.0007
	step [51/244], loss=130.8828
	step [52/244], loss=123.9796
	step [53/244], loss=141.2928
	step [54/244], loss=115.1618
	step [55/244], loss=110.9642
	step [56/244], loss=99.0964
	step [57/244], loss=123.8259
	step [58/244], loss=114.8817
	step [59/244], loss=117.2075
	step [60/244], loss=146.7158
	step [61/244], loss=133.7483
	step [62/244], loss=123.1226
	step [63/244], loss=137.5025
	step [64/244], loss=126.2762
	step [65/244], loss=127.0051
	step [66/244], loss=123.9597
	step [67/244], loss=132.5975
	step [68/244], loss=116.5401
	step [69/244], loss=120.5014
	step [70/244], loss=116.7105
	step [71/244], loss=118.9401
	step [72/244], loss=130.0529
	step [73/244], loss=123.2492
	step [74/244], loss=125.7735
	step [75/244], loss=149.7288
	step [76/244], loss=141.1785
	step [77/244], loss=112.3548
	step [78/244], loss=130.4484
	step [79/244], loss=118.5240
	step [80/244], loss=112.3633
	step [81/244], loss=124.2113
	step [82/244], loss=126.7565
	step [83/244], loss=143.5779
	step [84/244], loss=121.8532
	step [85/244], loss=146.3545
	step [86/244], loss=127.2922
	step [87/244], loss=109.7712
	step [88/244], loss=106.9598
	step [89/244], loss=125.8736
	step [90/244], loss=119.9459
	step [91/244], loss=113.5889
	step [92/244], loss=103.6966
	step [93/244], loss=123.1124
	step [94/244], loss=126.2704
	step [95/244], loss=112.1887
	step [96/244], loss=115.0002
	step [97/244], loss=115.9883
	step [98/244], loss=109.8770
	step [99/244], loss=124.8081
	step [100/244], loss=126.0878
	step [101/244], loss=115.3942
	step [102/244], loss=124.3191
	step [103/244], loss=106.7318
	step [104/244], loss=122.3837
	step [105/244], loss=112.2064
	step [106/244], loss=97.2180
	step [107/244], loss=120.9295
	step [108/244], loss=124.2274
	step [109/244], loss=126.5998
	step [110/244], loss=105.3773
	step [111/244], loss=99.4578
	step [112/244], loss=115.3867
	step [113/244], loss=108.8741
	step [114/244], loss=140.0922
	step [115/244], loss=128.2402
	step [116/244], loss=116.2948
	step [117/244], loss=118.0410
	step [118/244], loss=123.6293
	step [119/244], loss=105.7679
	step [120/244], loss=113.5023
	step [121/244], loss=121.3560
	step [122/244], loss=138.5491
	step [123/244], loss=97.9327
	step [124/244], loss=95.9728
	step [125/244], loss=111.0921
	step [126/244], loss=116.2925
	step [127/244], loss=114.9883
	step [128/244], loss=105.9003
	step [129/244], loss=116.3952
	step [130/244], loss=130.9534
	step [131/244], loss=107.1945
	step [132/244], loss=104.1514
	step [133/244], loss=122.3033
	step [134/244], loss=108.5506
	step [135/244], loss=127.9101
	step [136/244], loss=116.1124
	step [137/244], loss=112.7654
	step [138/244], loss=103.7233
	step [139/244], loss=118.0159
	step [140/244], loss=110.0069
	step [141/244], loss=119.0363
	step [142/244], loss=131.9007
	step [143/244], loss=110.2531
	step [144/244], loss=134.1778
	step [145/244], loss=123.3857
	step [146/244], loss=125.7661
	step [147/244], loss=126.4514
	step [148/244], loss=119.0828
	step [149/244], loss=96.1387
	step [150/244], loss=110.0403
	step [151/244], loss=105.2798
	step [152/244], loss=100.5758
	step [153/244], loss=109.9909
	step [154/244], loss=101.4862
	step [155/244], loss=99.7513
	step [156/244], loss=141.7866
	step [157/244], loss=122.5367
	step [158/244], loss=107.1149
	step [159/244], loss=109.6146
	step [160/244], loss=131.8573
	step [161/244], loss=123.1582
	step [162/244], loss=103.7572
	step [163/244], loss=100.7978
	step [164/244], loss=125.1176
	step [165/244], loss=132.2358
	step [166/244], loss=110.4099
	step [167/244], loss=120.4751
	step [168/244], loss=129.0883
	step [169/244], loss=116.3597
	step [170/244], loss=114.7465
	step [171/244], loss=98.6181
	step [172/244], loss=105.4391
	step [173/244], loss=134.8031
	step [174/244], loss=125.1518
	step [175/244], loss=126.9773
	step [176/244], loss=104.5737
	step [177/244], loss=112.6193
	step [178/244], loss=123.5098
	step [179/244], loss=106.0139
	step [180/244], loss=118.4077
	step [181/244], loss=119.4318
	step [182/244], loss=118.8585
	step [183/244], loss=116.9244
	step [184/244], loss=110.3403
	step [185/244], loss=100.4331
	step [186/244], loss=108.4319
	step [187/244], loss=117.4110
	step [188/244], loss=112.8488
	step [189/244], loss=130.4231
	step [190/244], loss=97.5234
	step [191/244], loss=123.2525
	step [192/244], loss=103.1488
	step [193/244], loss=132.7139
	step [194/244], loss=131.2487
	step [195/244], loss=110.2508
	step [196/244], loss=132.8426
	step [197/244], loss=133.2888
	step [198/244], loss=118.8945
	step [199/244], loss=118.1856
	step [200/244], loss=110.1410
	step [201/244], loss=106.4412
	step [202/244], loss=145.3572
	step [203/244], loss=89.6122
	step [204/244], loss=95.4283
	step [205/244], loss=108.1705
	step [206/244], loss=123.5573
	step [207/244], loss=143.2019
	step [208/244], loss=109.0410
	step [209/244], loss=132.4288
	step [210/244], loss=128.5151
	step [211/244], loss=103.1063
	step [212/244], loss=127.4315
	step [213/244], loss=114.1829
	step [214/244], loss=116.0449
	step [215/244], loss=107.1715
	step [216/244], loss=106.5938
	step [217/244], loss=125.0177
	step [218/244], loss=125.8513
	step [219/244], loss=110.4588
	step [220/244], loss=109.0100
	step [221/244], loss=117.9302
	step [222/244], loss=101.1140
	step [223/244], loss=120.6863
	step [224/244], loss=105.0069
	step [225/244], loss=114.4503
	step [226/244], loss=110.7192
	step [227/244], loss=129.4592
	step [228/244], loss=119.8167
	step [229/244], loss=105.6461
	step [230/244], loss=118.8710
	step [231/244], loss=128.2338
	step [232/244], loss=128.4042
	step [233/244], loss=111.9800
	step [234/244], loss=108.2027
	step [235/244], loss=105.6783
	step [236/244], loss=144.1755
	step [237/244], loss=112.5676
	step [238/244], loss=111.3710
	step [239/244], loss=128.1574
	step [240/244], loss=117.0064
	step [241/244], loss=84.2597
	step [242/244], loss=116.5275
	step [243/244], loss=109.2367
	step [244/244], loss=39.8634
	Evaluating
	loss=0.0772, precision=0.4078, recall=0.9068, f1=0.5626
Training epoch 6
	step [1/244], loss=89.0182
	step [2/244], loss=121.4921
	step [3/244], loss=119.4585
	step [4/244], loss=105.7046
	step [5/244], loss=128.2927
	step [6/244], loss=134.7484
	step [7/244], loss=112.9266
	step [8/244], loss=97.7944
	step [9/244], loss=91.6266
	step [10/244], loss=122.8392
	step [11/244], loss=130.7172
	step [12/244], loss=108.1402
	step [13/244], loss=121.6270
	step [14/244], loss=120.1274
	step [15/244], loss=128.6980
	step [16/244], loss=116.0773
	step [17/244], loss=132.4121
	step [18/244], loss=126.5571
	step [19/244], loss=133.0995
	step [20/244], loss=98.7655
	step [21/244], loss=110.3840
	step [22/244], loss=99.8925
	step [23/244], loss=117.7510
	step [24/244], loss=90.2459
	step [25/244], loss=111.7290
	step [26/244], loss=103.3897
	step [27/244], loss=115.8253
	step [28/244], loss=110.9031
	step [29/244], loss=135.1078
	step [30/244], loss=112.8255
	step [31/244], loss=112.7757
	step [32/244], loss=122.2209
	step [33/244], loss=123.1739
	step [34/244], loss=134.7828
	step [35/244], loss=87.0541
	step [36/244], loss=136.7614
	step [37/244], loss=120.7334
	step [38/244], loss=102.9562
	step [39/244], loss=109.6255
	step [40/244], loss=130.5058
	step [41/244], loss=112.8965
	step [42/244], loss=133.0504
	step [43/244], loss=101.3226
	step [44/244], loss=129.8672
	step [45/244], loss=93.4443
	step [46/244], loss=102.8871
	step [47/244], loss=111.7867
	step [48/244], loss=115.0041
	step [49/244], loss=101.6357
	step [50/244], loss=99.4665
	step [51/244], loss=137.0615
	step [52/244], loss=93.7984
	step [53/244], loss=104.5508
	step [54/244], loss=111.4724
	step [55/244], loss=112.1705
	step [56/244], loss=108.1927
	step [57/244], loss=112.1123
	step [58/244], loss=103.8508
	step [59/244], loss=100.3517
	step [60/244], loss=137.3635
	step [61/244], loss=105.3252
	step [62/244], loss=106.5427
	step [63/244], loss=103.4254
	step [64/244], loss=97.5562
	step [65/244], loss=111.8672
	step [66/244], loss=130.3521
	step [67/244], loss=104.3092
	step [68/244], loss=129.8596
	step [69/244], loss=124.1719
	step [70/244], loss=107.0495
	step [71/244], loss=134.0318
	step [72/244], loss=118.0397
	step [73/244], loss=109.6532
	step [74/244], loss=114.9132
	step [75/244], loss=109.8201
	step [76/244], loss=102.1490
	step [77/244], loss=98.5453
	step [78/244], loss=118.3183
	step [79/244], loss=130.8256
	step [80/244], loss=123.8032
	step [81/244], loss=105.2962
	step [82/244], loss=105.0788
	step [83/244], loss=108.7622
	step [84/244], loss=104.4644
	step [85/244], loss=98.0787
	step [86/244], loss=97.0774
	step [87/244], loss=124.3907
	step [88/244], loss=127.9667
	step [89/244], loss=115.4937
	step [90/244], loss=96.8758
	step [91/244], loss=107.9718
	step [92/244], loss=119.1500
	step [93/244], loss=113.9857
	step [94/244], loss=115.3335
	step [95/244], loss=109.5648
	step [96/244], loss=107.0046
	step [97/244], loss=112.2580
	step [98/244], loss=121.0378
	step [99/244], loss=119.9740
	step [100/244], loss=112.3669
	step [101/244], loss=106.4448
	step [102/244], loss=108.8576
	step [103/244], loss=105.4072
	step [104/244], loss=110.1232
	step [105/244], loss=104.3611
	step [106/244], loss=128.3763
	step [107/244], loss=116.2910
	step [108/244], loss=102.6743
	step [109/244], loss=101.5760
	step [110/244], loss=127.0612
	step [111/244], loss=109.8067
	step [112/244], loss=120.7520
	step [113/244], loss=121.8125
	step [114/244], loss=132.1284
	step [115/244], loss=104.9609
	step [116/244], loss=126.7566
	step [117/244], loss=117.1023
	step [118/244], loss=104.8404
	step [119/244], loss=127.3547
	step [120/244], loss=138.9928
	step [121/244], loss=113.0993
	step [122/244], loss=108.6933
	step [123/244], loss=104.8456
	step [124/244], loss=115.6181
	step [125/244], loss=129.7542
	step [126/244], loss=112.9448
	step [127/244], loss=113.6036
	step [128/244], loss=104.4949
	step [129/244], loss=118.6430
	step [130/244], loss=139.2754
	step [131/244], loss=118.9826
	step [132/244], loss=130.6251
	step [133/244], loss=128.3444
	step [134/244], loss=117.3020
	step [135/244], loss=124.8087
	step [136/244], loss=118.6055
	step [137/244], loss=138.1267
	step [138/244], loss=117.7921
	step [139/244], loss=100.2305
	step [140/244], loss=122.1871
	step [141/244], loss=117.2775
	step [142/244], loss=112.2447
	step [143/244], loss=117.9438
	step [144/244], loss=109.8716
	step [145/244], loss=118.2258
	step [146/244], loss=118.9420
	step [147/244], loss=102.0481
	step [148/244], loss=126.1921
	step [149/244], loss=110.4815
	step [150/244], loss=124.4924
	step [151/244], loss=123.8377
	step [152/244], loss=93.7509
	step [153/244], loss=115.8066
	step [154/244], loss=107.1304
	step [155/244], loss=134.5786
	step [156/244], loss=135.9803
	step [157/244], loss=112.7191
	step [158/244], loss=117.0902
	step [159/244], loss=98.4040
	step [160/244], loss=115.8875
	step [161/244], loss=132.8956
	step [162/244], loss=92.3774
	step [163/244], loss=97.2700
	step [164/244], loss=102.2573
	step [165/244], loss=107.4767
	step [166/244], loss=118.2498
	step [167/244], loss=128.8982
	step [168/244], loss=133.2443
	step [169/244], loss=87.7556
	step [170/244], loss=113.0676
	step [171/244], loss=109.1959
	step [172/244], loss=99.8670
	step [173/244], loss=110.7991
	step [174/244], loss=134.8598
	step [175/244], loss=120.0327
	step [176/244], loss=128.2754
	step [177/244], loss=113.8504
	step [178/244], loss=111.9775
	step [179/244], loss=109.0020
	step [180/244], loss=96.5375
	step [181/244], loss=119.5356
	step [182/244], loss=95.3299
	step [183/244], loss=107.3678
	step [184/244], loss=109.3235
	step [185/244], loss=116.0881
	step [186/244], loss=126.2218
	step [187/244], loss=114.2255
	step [188/244], loss=113.8028
	step [189/244], loss=125.8887
	step [190/244], loss=121.1264
	step [191/244], loss=102.5607
	step [192/244], loss=101.1508
	step [193/244], loss=91.0505
	step [194/244], loss=109.0109
	step [195/244], loss=111.3667
	step [196/244], loss=103.1665
	step [197/244], loss=109.1822
	step [198/244], loss=89.7938
	step [199/244], loss=125.2010
	step [200/244], loss=108.0979
	step [201/244], loss=124.7514
	step [202/244], loss=113.5982
	step [203/244], loss=128.2036
	step [204/244], loss=123.9641
	step [205/244], loss=104.7744
	step [206/244], loss=110.9250
	step [207/244], loss=117.7032
	step [208/244], loss=131.7558
	step [209/244], loss=125.4829
	step [210/244], loss=117.5732
	step [211/244], loss=106.9187
	step [212/244], loss=120.0184
	step [213/244], loss=93.8017
	step [214/244], loss=100.5253
	step [215/244], loss=98.1964
	step [216/244], loss=120.5191
	step [217/244], loss=118.8155
	step [218/244], loss=91.2411
	step [219/244], loss=113.2478
	step [220/244], loss=90.6856
	step [221/244], loss=124.5125
	step [222/244], loss=107.4149
	step [223/244], loss=126.3575
	step [224/244], loss=111.5003
	step [225/244], loss=84.8231
	step [226/244], loss=116.7066
	step [227/244], loss=101.4953
	step [228/244], loss=130.4374
	step [229/244], loss=99.1266
	step [230/244], loss=106.5709
	step [231/244], loss=103.3513
	step [232/244], loss=87.6185
	step [233/244], loss=132.9288
	step [234/244], loss=109.1853
	step [235/244], loss=110.7989
	step [236/244], loss=116.1908
	step [237/244], loss=106.0676
	step [238/244], loss=125.0047
	step [239/244], loss=121.4850
	step [240/244], loss=104.3448
	step [241/244], loss=109.6890
	step [242/244], loss=104.5302
	step [243/244], loss=104.6656
	step [244/244], loss=38.1491
	Evaluating
	loss=0.0625, precision=0.4049, recall=0.8660, f1=0.5518
Training epoch 7
	step [1/244], loss=123.0781
	step [2/244], loss=94.2661
	step [3/244], loss=135.4765
	step [4/244], loss=109.3793
	step [5/244], loss=114.7463
	step [6/244], loss=110.7864
	step [7/244], loss=114.0054
	step [8/244], loss=88.5111
	step [9/244], loss=94.1819
	step [10/244], loss=130.8209
	step [11/244], loss=109.4739
	step [12/244], loss=101.6888
	step [13/244], loss=93.1944
	step [14/244], loss=115.5243
	step [15/244], loss=105.2035
	step [16/244], loss=109.0118
	step [17/244], loss=138.4289
	step [18/244], loss=97.3259
	step [19/244], loss=105.9521
	step [20/244], loss=116.9844
	step [21/244], loss=107.1309
	step [22/244], loss=113.2438
	step [23/244], loss=92.5446
	step [24/244], loss=109.5781
	step [25/244], loss=129.6425
	step [26/244], loss=126.2791
	step [27/244], loss=104.3181
	step [28/244], loss=100.2731
	step [29/244], loss=104.7956
	step [30/244], loss=98.7774
	step [31/244], loss=103.0751
	step [32/244], loss=100.1443
	step [33/244], loss=117.7026
	step [34/244], loss=97.4570
	step [35/244], loss=111.8882
	step [36/244], loss=138.0943
	step [37/244], loss=116.5482
	step [38/244], loss=88.8125
	step [39/244], loss=128.8284
	step [40/244], loss=135.2523
	step [41/244], loss=110.0911
	step [42/244], loss=89.2116
	step [43/244], loss=106.0747
	step [44/244], loss=121.0261
	step [45/244], loss=123.0303
	step [46/244], loss=103.7329
	step [47/244], loss=107.8417
	step [48/244], loss=102.6433
	step [49/244], loss=98.1098
	step [50/244], loss=110.9956
	step [51/244], loss=98.3989
	step [52/244], loss=125.7803
	step [53/244], loss=104.1809
	step [54/244], loss=124.1509
	step [55/244], loss=117.8382
	step [56/244], loss=101.1222
	step [57/244], loss=94.7579
	step [58/244], loss=120.6264
	step [59/244], loss=117.9398
	step [60/244], loss=97.1169
	step [61/244], loss=110.3392
	step [62/244], loss=102.9296
	step [63/244], loss=116.5951
	step [64/244], loss=108.5048
	step [65/244], loss=105.5992
	step [66/244], loss=130.0951
	step [67/244], loss=107.6281
	step [68/244], loss=100.1975
	step [69/244], loss=106.1541
	step [70/244], loss=115.7565
	step [71/244], loss=123.0988
	step [72/244], loss=120.2746
	step [73/244], loss=117.2915
	step [74/244], loss=99.6615
	step [75/244], loss=105.2459
	step [76/244], loss=107.9212
	step [77/244], loss=110.8308
	step [78/244], loss=103.7586
	step [79/244], loss=95.6925
	step [80/244], loss=104.6324
	step [81/244], loss=105.8850
	step [82/244], loss=99.1585
	step [83/244], loss=112.3768
	step [84/244], loss=108.5592
	step [85/244], loss=120.6175
	step [86/244], loss=98.9034
	step [87/244], loss=111.5506
	step [88/244], loss=102.0817
	step [89/244], loss=96.5514
	step [90/244], loss=115.0130
	step [91/244], loss=93.0076
	step [92/244], loss=103.1779
	step [93/244], loss=102.7606
	step [94/244], loss=120.2017
	step [95/244], loss=124.4448
	step [96/244], loss=121.4786
	step [97/244], loss=97.3046
	step [98/244], loss=119.5155
	step [99/244], loss=101.7679
	step [100/244], loss=106.6271
	step [101/244], loss=99.2021
	step [102/244], loss=120.6514
	step [103/244], loss=126.0377
	step [104/244], loss=111.2730
	step [105/244], loss=104.6728
	step [106/244], loss=121.8021
	step [107/244], loss=87.4954
	step [108/244], loss=102.8116
	step [109/244], loss=107.2540
	step [110/244], loss=135.4762
	step [111/244], loss=107.8834
	step [112/244], loss=123.9646
	step [113/244], loss=119.5611
	step [114/244], loss=99.5164
	step [115/244], loss=118.4717
	step [116/244], loss=108.0075
	step [117/244], loss=114.0877
	step [118/244], loss=116.3749
	step [119/244], loss=98.7874
	step [120/244], loss=99.3879
	step [121/244], loss=91.2473
	step [122/244], loss=118.4710
	step [123/244], loss=109.8120
	step [124/244], loss=134.5564
	step [125/244], loss=119.3074
	step [126/244], loss=103.0673
	step [127/244], loss=94.2549
	step [128/244], loss=113.9263
	step [129/244], loss=105.7686
	step [130/244], loss=98.1815
	step [131/244], loss=103.6472
	step [132/244], loss=105.8562
	step [133/244], loss=136.5261
	step [134/244], loss=107.3431
	step [135/244], loss=100.3694
	step [136/244], loss=98.5426
	step [137/244], loss=124.1889
	step [138/244], loss=111.6363
	step [139/244], loss=123.1114
	step [140/244], loss=103.3884
	step [141/244], loss=122.2357
	step [142/244], loss=93.2261
	step [143/244], loss=91.3897
	step [144/244], loss=92.2464
	step [145/244], loss=102.8088
	step [146/244], loss=97.6496
	step [147/244], loss=118.5594
	step [148/244], loss=91.1087
	step [149/244], loss=109.5081
	step [150/244], loss=103.3288
	step [151/244], loss=96.4424
	step [152/244], loss=112.5913
	step [153/244], loss=102.1694
	step [154/244], loss=107.0599
	step [155/244], loss=116.6642
	step [156/244], loss=93.0305
	step [157/244], loss=96.0466
	step [158/244], loss=92.9701
	step [159/244], loss=92.3213
	step [160/244], loss=92.7745
	step [161/244], loss=99.6389
	step [162/244], loss=110.8236
	step [163/244], loss=107.6179
	step [164/244], loss=98.5894
	step [165/244], loss=105.9211
	step [166/244], loss=97.5766
	step [167/244], loss=98.8268
	step [168/244], loss=117.6106
	step [169/244], loss=106.1109
	step [170/244], loss=115.0766
	step [171/244], loss=86.0595
	step [172/244], loss=104.8244
	step [173/244], loss=107.9907
	step [174/244], loss=123.0285
	step [175/244], loss=105.6109
	step [176/244], loss=90.8785
	step [177/244], loss=113.5386
	step [178/244], loss=107.8858
	step [179/244], loss=98.8164
	step [180/244], loss=104.1854
	step [181/244], loss=103.7425
	step [182/244], loss=105.9941
	step [183/244], loss=114.3638
	step [184/244], loss=118.9697
	step [185/244], loss=106.6128
	step [186/244], loss=114.6003
	step [187/244], loss=116.2972
	step [188/244], loss=124.7747
	step [189/244], loss=107.8571
	step [190/244], loss=100.4721
	step [191/244], loss=102.3364
	step [192/244], loss=104.6997
	step [193/244], loss=105.8958
	step [194/244], loss=105.1404
	step [195/244], loss=116.7699
	step [196/244], loss=91.7580
	step [197/244], loss=110.4842
	step [198/244], loss=117.3651
	step [199/244], loss=117.7634
	step [200/244], loss=105.8657
	step [201/244], loss=124.4203
	step [202/244], loss=132.8577
	step [203/244], loss=106.9621
	step [204/244], loss=117.4049
	step [205/244], loss=113.7265
	step [206/244], loss=80.6413
	step [207/244], loss=114.2495
	step [208/244], loss=103.7159
	step [209/244], loss=121.1539
	step [210/244], loss=110.5963
	step [211/244], loss=109.4858
	step [212/244], loss=113.6030
	step [213/244], loss=113.6710
	step [214/244], loss=108.4737
	step [215/244], loss=112.0210
	step [216/244], loss=122.3408
	step [217/244], loss=115.3934
	step [218/244], loss=110.2067
	step [219/244], loss=107.6185
	step [220/244], loss=106.5952
	step [221/244], loss=110.8141
	step [222/244], loss=114.1911
	step [223/244], loss=117.2263
	step [224/244], loss=100.7917
	step [225/244], loss=124.7912
	step [226/244], loss=109.2786
	step [227/244], loss=116.8252
	step [228/244], loss=102.1281
	step [229/244], loss=79.9979
	step [230/244], loss=104.9854
	step [231/244], loss=116.5736
	step [232/244], loss=98.1878
	step [233/244], loss=122.4409
	step [234/244], loss=119.5801
	step [235/244], loss=117.3448
	step [236/244], loss=146.3338
	step [237/244], loss=122.2594
	step [238/244], loss=111.6127
	step [239/244], loss=115.9759
	step [240/244], loss=116.5643
	step [241/244], loss=125.0093
	step [242/244], loss=100.9645
	step [243/244], loss=99.5291
	step [244/244], loss=58.4926
	Evaluating
	loss=0.0472, precision=0.4444, recall=0.9041, f1=0.5959
Training epoch 8
	step [1/244], loss=118.9098
	step [2/244], loss=103.5779
	step [3/244], loss=114.5580
	step [4/244], loss=106.2070
	step [5/244], loss=108.6930
	step [6/244], loss=123.8803
	step [7/244], loss=90.4759
	step [8/244], loss=124.6167
	step [9/244], loss=94.1970
	step [10/244], loss=119.2749
	step [11/244], loss=107.0732
	step [12/244], loss=109.9986
	step [13/244], loss=97.2952
	step [14/244], loss=95.5025
	step [15/244], loss=101.7733
	step [16/244], loss=129.1334
	step [17/244], loss=94.8741
	step [18/244], loss=100.8786
	step [19/244], loss=106.5657
	step [20/244], loss=99.2933
	step [21/244], loss=104.7329
	step [22/244], loss=138.9033
	step [23/244], loss=116.9271
	step [24/244], loss=140.0390
	step [25/244], loss=116.0080
	step [26/244], loss=113.8688
	step [27/244], loss=119.9876
	step [28/244], loss=130.7844
	step [29/244], loss=93.9250
	step [30/244], loss=99.3137
	step [31/244], loss=110.6530
	step [32/244], loss=96.6315
	step [33/244], loss=111.0549
	step [34/244], loss=121.7855
	step [35/244], loss=101.7002
	step [36/244], loss=98.4576
	step [37/244], loss=124.4778
	step [38/244], loss=94.3429
	step [39/244], loss=124.5414
	step [40/244], loss=94.6050
	step [41/244], loss=89.6318
	step [42/244], loss=102.0751
	step [43/244], loss=110.8524
	step [44/244], loss=112.7021
	step [45/244], loss=106.4496
	step [46/244], loss=115.3160
	step [47/244], loss=108.1031
	step [48/244], loss=93.4088
	step [49/244], loss=90.6131
	step [50/244], loss=102.0627
	step [51/244], loss=93.5249
	step [52/244], loss=105.8335
	step [53/244], loss=106.1224
	step [54/244], loss=90.0395
	step [55/244], loss=90.7513
	step [56/244], loss=118.2868
	step [57/244], loss=100.1122
	step [58/244], loss=110.3471
	step [59/244], loss=101.2109
	step [60/244], loss=133.0341
	step [61/244], loss=110.9737
	step [62/244], loss=100.2182
	step [63/244], loss=121.1886
	step [64/244], loss=110.2940
	step [65/244], loss=99.4571
	step [66/244], loss=104.8955
	step [67/244], loss=111.6094
	step [68/244], loss=103.9682
	step [69/244], loss=99.2022
	step [70/244], loss=91.2921
	step [71/244], loss=103.6016
	step [72/244], loss=107.3309
	step [73/244], loss=109.2896
	step [74/244], loss=98.3748
	step [75/244], loss=112.0722
	step [76/244], loss=118.5645
	step [77/244], loss=100.8609
	step [78/244], loss=110.2553
	step [79/244], loss=107.9322
	step [80/244], loss=120.9092
	step [81/244], loss=93.3783
	step [82/244], loss=96.4269
	step [83/244], loss=121.6321
	step [84/244], loss=94.1002
	step [85/244], loss=96.8086
	step [86/244], loss=99.0035
	step [87/244], loss=120.9178
	step [88/244], loss=101.9483
	step [89/244], loss=88.2085
	step [90/244], loss=105.9913
	step [91/244], loss=113.7038
	step [92/244], loss=107.6772
	step [93/244], loss=108.1305
	step [94/244], loss=99.0726
	step [95/244], loss=103.4113
	step [96/244], loss=102.2429
	step [97/244], loss=115.0605
	step [98/244], loss=129.0213
	step [99/244], loss=100.1081
	step [100/244], loss=121.0027
	step [101/244], loss=108.8284
	step [102/244], loss=111.6002
	step [103/244], loss=130.1341
	step [104/244], loss=97.2152
	step [105/244], loss=113.0843
	step [106/244], loss=111.3907
	step [107/244], loss=99.8449
	step [108/244], loss=95.4878
	step [109/244], loss=129.2302
	step [110/244], loss=114.1057
	step [111/244], loss=117.2996
	step [112/244], loss=103.4669
	step [113/244], loss=123.1111
	step [114/244], loss=108.9624
	step [115/244], loss=93.0773
	step [116/244], loss=109.6945
	step [117/244], loss=95.9880
	step [118/244], loss=100.3240
	step [119/244], loss=121.1300
	step [120/244], loss=104.3339
	step [121/244], loss=89.6252
	step [122/244], loss=107.3926
	step [123/244], loss=116.3603
	step [124/244], loss=104.3643
	step [125/244], loss=112.5442
	step [126/244], loss=96.0649
	step [127/244], loss=93.1173
	step [128/244], loss=113.7275
	step [129/244], loss=122.4082
	step [130/244], loss=131.8133
	step [131/244], loss=109.6319
	step [132/244], loss=130.6571
	step [133/244], loss=113.4731
	step [134/244], loss=109.9469
	step [135/244], loss=110.0703
	step [136/244], loss=96.7232
	step [137/244], loss=107.5002
	step [138/244], loss=107.4768
	step [139/244], loss=93.5672
	step [140/244], loss=114.2364
	step [141/244], loss=98.7424
	step [142/244], loss=102.6149
	step [143/244], loss=92.6880
	step [144/244], loss=102.2147
	step [145/244], loss=98.9984
	step [146/244], loss=96.1400
	step [147/244], loss=111.3644
	step [148/244], loss=110.1929
	step [149/244], loss=109.0133
	step [150/244], loss=133.4051
	step [151/244], loss=100.5359
	step [152/244], loss=97.1155
	step [153/244], loss=102.3167
	step [154/244], loss=97.6328
	step [155/244], loss=99.3418
	step [156/244], loss=105.4051
	step [157/244], loss=83.8471
	step [158/244], loss=95.9258
	step [159/244], loss=98.8753
	step [160/244], loss=86.3183
	step [161/244], loss=118.5442
	step [162/244], loss=99.1679
	step [163/244], loss=123.4182
	step [164/244], loss=95.5183
	step [165/244], loss=125.2765
	step [166/244], loss=109.8836
	step [167/244], loss=103.8234
	step [168/244], loss=103.8109
	step [169/244], loss=111.6457
	step [170/244], loss=114.2546
	step [171/244], loss=95.7624
	step [172/244], loss=124.0388
	step [173/244], loss=117.9746
	step [174/244], loss=133.2046
	step [175/244], loss=111.4814
	step [176/244], loss=81.3697
	step [177/244], loss=103.5512
	step [178/244], loss=80.2352
	step [179/244], loss=105.2888
	step [180/244], loss=83.6039
	step [181/244], loss=99.6983
	step [182/244], loss=117.5621
	step [183/244], loss=104.2134
	step [184/244], loss=130.1053
	step [185/244], loss=113.5268
	step [186/244], loss=115.5529
	step [187/244], loss=102.1782
	step [188/244], loss=103.0792
	step [189/244], loss=99.5815
	step [190/244], loss=104.5758
	step [191/244], loss=82.3844
	step [192/244], loss=115.2587
	step [193/244], loss=114.8947
	step [194/244], loss=106.2398
	step [195/244], loss=109.4029
	step [196/244], loss=112.2754
	step [197/244], loss=106.5012
	step [198/244], loss=105.8503
	step [199/244], loss=118.1945
	step [200/244], loss=97.7093
	step [201/244], loss=107.7346
	step [202/244], loss=106.5257
	step [203/244], loss=97.7292
	step [204/244], loss=100.8474
	step [205/244], loss=97.1112
	step [206/244], loss=98.1805
	step [207/244], loss=98.5518
	step [208/244], loss=107.6286
	step [209/244], loss=115.3586
	step [210/244], loss=85.6495
	step [211/244], loss=104.3756
	step [212/244], loss=95.1563
	step [213/244], loss=111.1169
	step [214/244], loss=86.9875
	step [215/244], loss=104.7553
	step [216/244], loss=96.0610
	step [217/244], loss=130.4850
	step [218/244], loss=93.3307
	step [219/244], loss=95.6369
	step [220/244], loss=109.5025
	step [221/244], loss=134.4831
	step [222/244], loss=90.3084
	step [223/244], loss=95.3528
	step [224/244], loss=112.6413
	step [225/244], loss=101.0981
	step [226/244], loss=109.4055
	step [227/244], loss=97.5871
	step [228/244], loss=110.9030
	step [229/244], loss=98.8560
	step [230/244], loss=106.3565
	step [231/244], loss=116.2706
	step [232/244], loss=90.7291
	step [233/244], loss=105.9481
	step [234/244], loss=92.6517
	step [235/244], loss=118.0550
	step [236/244], loss=83.9258
	step [237/244], loss=105.4080
	step [238/244], loss=109.9243
	step [239/244], loss=119.5058
	step [240/244], loss=110.9497
	step [241/244], loss=110.3649
	step [242/244], loss=110.0305
	step [243/244], loss=104.2636
	step [244/244], loss=58.4436
	Evaluating
	loss=0.0376, precision=0.4137, recall=0.8977, f1=0.5664
Training epoch 9
	step [1/244], loss=87.1200
	step [2/244], loss=95.0757
	step [3/244], loss=110.4590
	step [4/244], loss=135.8054
	step [5/244], loss=98.4268
	step [6/244], loss=88.8609
	step [7/244], loss=99.5739
	step [8/244], loss=103.6484
	step [9/244], loss=98.3549
	step [10/244], loss=98.5395
	step [11/244], loss=115.9371
	step [12/244], loss=117.5725
	step [13/244], loss=118.2526
	step [14/244], loss=100.3218
	step [15/244], loss=89.7617
	step [16/244], loss=93.4222
	step [17/244], loss=103.7719
	step [18/244], loss=96.4908
	step [19/244], loss=115.3925
	step [20/244], loss=116.8244
	step [21/244], loss=101.6960
	step [22/244], loss=111.8542
	step [23/244], loss=121.0022
	step [24/244], loss=97.2574
	step [25/244], loss=101.9378
	step [26/244], loss=96.8190
	step [27/244], loss=124.3742
	step [28/244], loss=110.1708
	step [29/244], loss=117.2504
	step [30/244], loss=108.0698
	step [31/244], loss=99.2208
	step [32/244], loss=107.9563
	step [33/244], loss=109.6632
	step [34/244], loss=104.4546
	step [35/244], loss=99.1921
	step [36/244], loss=95.3411
	step [37/244], loss=91.5535
	step [38/244], loss=109.9234
	step [39/244], loss=100.8506
	step [40/244], loss=104.2272
	step [41/244], loss=92.7368
	step [42/244], loss=103.6155
	step [43/244], loss=134.3771
	step [44/244], loss=94.8353
	step [45/244], loss=104.5299
	step [46/244], loss=104.3218
	step [47/244], loss=100.9946
	step [48/244], loss=118.6709
	step [49/244], loss=116.1381
	step [50/244], loss=106.6411
	step [51/244], loss=105.2318
	step [52/244], loss=115.5645
	step [53/244], loss=108.5280
	step [54/244], loss=84.6859
	step [55/244], loss=136.7976
	step [56/244], loss=95.7369
	step [57/244], loss=102.9982
	step [58/244], loss=110.4831
	step [59/244], loss=108.2276
	step [60/244], loss=83.9146
	step [61/244], loss=94.9445
	step [62/244], loss=116.4618
	step [63/244], loss=106.9951
	step [64/244], loss=96.2414
	step [65/244], loss=136.3277
	step [66/244], loss=106.4108
	step [67/244], loss=120.6535
	step [68/244], loss=115.0685
	step [69/244], loss=90.6863
	step [70/244], loss=97.7505
	step [71/244], loss=104.0826
	step [72/244], loss=113.7117
	step [73/244], loss=91.4742
	step [74/244], loss=102.6552
	step [75/244], loss=106.4012
	step [76/244], loss=115.1702
	step [77/244], loss=102.6609
	step [78/244], loss=97.0671
	step [79/244], loss=94.2998
	step [80/244], loss=99.9476
	step [81/244], loss=95.6573
	step [82/244], loss=97.4585
	step [83/244], loss=109.2469
	step [84/244], loss=107.0163
	step [85/244], loss=100.4048
	step [86/244], loss=87.6419
	step [87/244], loss=107.3304
	step [88/244], loss=102.7928
	step [89/244], loss=92.0365
	step [90/244], loss=125.3496
	step [91/244], loss=99.7071
	step [92/244], loss=115.7646
	step [93/244], loss=98.9507
	step [94/244], loss=120.4101
	step [95/244], loss=89.8805
	step [96/244], loss=100.4378
	step [97/244], loss=104.9570
	step [98/244], loss=98.7653
	step [99/244], loss=90.4888
	step [100/244], loss=98.0440
	step [101/244], loss=89.7027
	step [102/244], loss=119.7389
	step [103/244], loss=109.6264
	step [104/244], loss=98.9429
	step [105/244], loss=103.4323
	step [106/244], loss=112.7891
	step [107/244], loss=77.6564
	step [108/244], loss=109.8406
	step [109/244], loss=95.2783
	step [110/244], loss=111.0875
	step [111/244], loss=98.8100
	step [112/244], loss=120.3085
	step [113/244], loss=103.9793
	step [114/244], loss=132.1134
	step [115/244], loss=97.8580
	step [116/244], loss=105.4127
	step [117/244], loss=104.3146
	step [118/244], loss=105.8240
	step [119/244], loss=103.1692
	step [120/244], loss=105.6529
	step [121/244], loss=92.0556
	step [122/244], loss=102.3786
	step [123/244], loss=106.0016
	step [124/244], loss=107.7588
	step [125/244], loss=91.6646
	step [126/244], loss=86.4437
	step [127/244], loss=120.6706
	step [128/244], loss=104.1736
	step [129/244], loss=105.2319
	step [130/244], loss=105.3303
	step [131/244], loss=117.9132
	step [132/244], loss=114.3888
	step [133/244], loss=119.0650
	step [134/244], loss=136.7474
	step [135/244], loss=102.0488
	step [136/244], loss=108.1439
	step [137/244], loss=94.0758
	step [138/244], loss=104.2937
	step [139/244], loss=125.0818
	step [140/244], loss=84.1389
	step [141/244], loss=106.6322
	step [142/244], loss=96.8723
	step [143/244], loss=83.2697
	step [144/244], loss=89.4960
	step [145/244], loss=108.1639
	step [146/244], loss=122.3257
	step [147/244], loss=125.8514
	step [148/244], loss=103.8200
	step [149/244], loss=109.9242
	step [150/244], loss=105.7496
	step [151/244], loss=96.6213
	step [152/244], loss=98.0442
	step [153/244], loss=128.4595
	step [154/244], loss=95.8894
	step [155/244], loss=104.3883
	step [156/244], loss=96.4670
	step [157/244], loss=105.5410
	step [158/244], loss=91.3410
	step [159/244], loss=97.4631
	step [160/244], loss=72.6335
	step [161/244], loss=109.7814
	step [162/244], loss=129.0887
	step [163/244], loss=86.0927
	step [164/244], loss=110.9911
	step [165/244], loss=101.3389
	step [166/244], loss=103.5958
	step [167/244], loss=108.6835
	step [168/244], loss=113.7511
	step [169/244], loss=104.9894
	step [170/244], loss=92.2551
	step [171/244], loss=90.1021
	step [172/244], loss=96.1985
	step [173/244], loss=106.1039
	step [174/244], loss=85.4343
	step [175/244], loss=113.1421
	step [176/244], loss=117.8855
	step [177/244], loss=107.2865
	step [178/244], loss=98.3950
	step [179/244], loss=110.8327
	step [180/244], loss=100.6255
	step [181/244], loss=108.0764
	step [182/244], loss=81.2691
	step [183/244], loss=120.7645
	step [184/244], loss=101.2733
	step [185/244], loss=82.4742
	step [186/244], loss=106.9984
	step [187/244], loss=114.1579
	step [188/244], loss=86.8287
	step [189/244], loss=105.3472
	step [190/244], loss=102.7265
	step [191/244], loss=106.5139
	step [192/244], loss=114.8767
	step [193/244], loss=97.0594
	step [194/244], loss=97.8241
	step [195/244], loss=107.5641
	step [196/244], loss=95.2200
	step [197/244], loss=94.2415
	step [198/244], loss=87.5566
	step [199/244], loss=113.6102
	step [200/244], loss=98.6308
	step [201/244], loss=101.8794
	step [202/244], loss=108.5809
	step [203/244], loss=106.3732
	step [204/244], loss=126.9016
	step [205/244], loss=133.5910
	step [206/244], loss=109.1151
	step [207/244], loss=98.0021
	step [208/244], loss=113.6956
	step [209/244], loss=108.9827
	step [210/244], loss=112.0827
	step [211/244], loss=118.9478
	step [212/244], loss=99.4810
	step [213/244], loss=109.3782
	step [214/244], loss=110.2247
	step [215/244], loss=102.9621
	step [216/244], loss=85.6936
	step [217/244], loss=104.4554
	step [218/244], loss=91.9961
	step [219/244], loss=102.9211
	step [220/244], loss=113.8560
	step [221/244], loss=94.7624
	step [222/244], loss=110.9614
	step [223/244], loss=104.5314
	step [224/244], loss=106.5512
	step [225/244], loss=103.4075
	step [226/244], loss=92.5113
	step [227/244], loss=103.8855
	step [228/244], loss=97.8134
	step [229/244], loss=118.2741
	step [230/244], loss=83.5423
	step [231/244], loss=102.0230
	step [232/244], loss=98.8965
	step [233/244], loss=107.8114
	step [234/244], loss=120.6295
	step [235/244], loss=75.9807
	step [236/244], loss=91.5564
	step [237/244], loss=108.0307
	step [238/244], loss=113.5164
	step [239/244], loss=101.4838
	step [240/244], loss=86.3160
	step [241/244], loss=125.0815
	step [242/244], loss=109.4767
	step [243/244], loss=102.4847
	step [244/244], loss=48.5804
	Evaluating
	loss=0.0325, precision=0.3984, recall=0.9060, f1=0.5534
Training epoch 10
	step [1/244], loss=119.4927
	step [2/244], loss=117.6164
	step [3/244], loss=108.7500
	step [4/244], loss=105.7630
	step [5/244], loss=97.7422
	step [6/244], loss=93.0920
	step [7/244], loss=116.3414
	step [8/244], loss=87.1744
	step [9/244], loss=99.9574
	step [10/244], loss=106.0406
	step [11/244], loss=112.5474
	step [12/244], loss=108.3440
	step [13/244], loss=95.8321
	step [14/244], loss=110.4052
	step [15/244], loss=129.1944
	step [16/244], loss=112.8684
	step [17/244], loss=96.5213
	step [18/244], loss=111.7960
	step [19/244], loss=110.7203
	step [20/244], loss=85.6213
	step [21/244], loss=102.3819
	step [22/244], loss=97.9478
	step [23/244], loss=98.0775
	step [24/244], loss=76.8248
	step [25/244], loss=78.8999
	step [26/244], loss=95.7946
	step [27/244], loss=86.3501
	step [28/244], loss=110.5541
	step [29/244], loss=92.2455
	step [30/244], loss=97.4627
	step [31/244], loss=89.5909
	step [32/244], loss=109.9412
	step [33/244], loss=112.4188
	step [34/244], loss=83.4056
	step [35/244], loss=107.4945
	step [36/244], loss=98.8870
	step [37/244], loss=119.9039
	step [38/244], loss=89.6552
	step [39/244], loss=118.4498
	step [40/244], loss=117.4062
	step [41/244], loss=107.3712
	step [42/244], loss=122.6105
	step [43/244], loss=92.2841
	step [44/244], loss=103.3004
	step [45/244], loss=105.2824
	step [46/244], loss=103.3037
	step [47/244], loss=80.3728
	step [48/244], loss=97.1891
	step [49/244], loss=99.9449
	step [50/244], loss=103.0105
	step [51/244], loss=99.9646
	step [52/244], loss=93.9052
	step [53/244], loss=103.5707
	step [54/244], loss=97.7854
	step [55/244], loss=91.1022
	step [56/244], loss=82.3329
	step [57/244], loss=120.1180
	step [58/244], loss=113.3767
	step [59/244], loss=97.4713
	step [60/244], loss=87.7794
	step [61/244], loss=114.2270
	step [62/244], loss=108.1331
	step [63/244], loss=97.3604
	step [64/244], loss=116.8243
	step [65/244], loss=86.7837
	step [66/244], loss=105.4178
	step [67/244], loss=105.1908
	step [68/244], loss=98.5563
	step [69/244], loss=106.1002
	step [70/244], loss=105.5403
	step [71/244], loss=103.4869
	step [72/244], loss=111.1356
	step [73/244], loss=96.0956
	step [74/244], loss=104.7011
	step [75/244], loss=107.2055
	step [76/244], loss=118.8640
	step [77/244], loss=94.5031
	step [78/244], loss=95.4785
	step [79/244], loss=111.2785
	step [80/244], loss=125.8040
	step [81/244], loss=87.0997
	step [82/244], loss=125.3539
	step [83/244], loss=87.2276
	step [84/244], loss=97.8976
	step [85/244], loss=95.6956
	step [86/244], loss=98.9332
	step [87/244], loss=85.7273
	step [88/244], loss=96.2188
	step [89/244], loss=116.7444
	step [90/244], loss=99.7293
	step [91/244], loss=101.3892
	step [92/244], loss=99.2970
	step [93/244], loss=125.3942
	step [94/244], loss=115.5266
	step [95/244], loss=102.4681
	step [96/244], loss=100.8001
	step [97/244], loss=113.8605
	step [98/244], loss=94.6247
	step [99/244], loss=93.4999
	step [100/244], loss=115.7056
	step [101/244], loss=121.7099
	step [102/244], loss=105.2995
	step [103/244], loss=87.0798
	step [104/244], loss=98.5992
	step [105/244], loss=90.4066
	step [106/244], loss=101.1708
	step [107/244], loss=106.4437
	step [108/244], loss=96.6702
	step [109/244], loss=103.4719
	step [110/244], loss=114.5819
	step [111/244], loss=101.9656
	step [112/244], loss=100.9870
	step [113/244], loss=112.3086
	step [114/244], loss=117.3776
	step [115/244], loss=117.1479
	step [116/244], loss=107.4502
	step [117/244], loss=88.2198
	step [118/244], loss=103.3225
	step [119/244], loss=104.4425
	step [120/244], loss=91.1552
	step [121/244], loss=95.8117
	step [122/244], loss=107.2819
	step [123/244], loss=88.0929
	step [124/244], loss=105.7127
	step [125/244], loss=85.5175
	step [126/244], loss=84.8745
	step [127/244], loss=111.1696
	step [128/244], loss=114.8744
	step [129/244], loss=108.8128
	step [130/244], loss=103.1831
	step [131/244], loss=112.9392
	step [132/244], loss=116.6940
	step [133/244], loss=111.3946
	step [134/244], loss=95.6793
	step [135/244], loss=89.6375
	step [136/244], loss=102.1335
	step [137/244], loss=105.9202
	step [138/244], loss=101.9428
	step [139/244], loss=105.1761
	step [140/244], loss=93.3212
	step [141/244], loss=114.3491
	step [142/244], loss=117.4895
	step [143/244], loss=88.0848
	step [144/244], loss=95.3471
	step [145/244], loss=101.7774
	step [146/244], loss=107.9439
	step [147/244], loss=103.2823
	step [148/244], loss=81.8582
	step [149/244], loss=89.3096
	step [150/244], loss=106.2721
	step [151/244], loss=99.2909
	step [152/244], loss=104.5585
	step [153/244], loss=97.8136
	step [154/244], loss=100.6996
	step [155/244], loss=114.4661
	step [156/244], loss=112.1308
	step [157/244], loss=96.6535
	step [158/244], loss=94.4522
	step [159/244], loss=90.1426
	step [160/244], loss=114.1074
	step [161/244], loss=110.6383
	step [162/244], loss=110.6623
	step [163/244], loss=106.8941
	step [164/244], loss=87.1394
	step [165/244], loss=109.5812
	step [166/244], loss=103.6450
	step [167/244], loss=97.6318
	step [168/244], loss=98.8263
	step [169/244], loss=114.1772
	step [170/244], loss=86.3975
	step [171/244], loss=119.9828
	step [172/244], loss=87.4229
	step [173/244], loss=101.8204
	step [174/244], loss=93.3363
	step [175/244], loss=98.3783
	step [176/244], loss=105.7100
	step [177/244], loss=95.9845
	step [178/244], loss=105.1105
	step [179/244], loss=118.8453
	step [180/244], loss=84.9293
	step [181/244], loss=90.9184
	step [182/244], loss=94.5311
	step [183/244], loss=117.7668
	step [184/244], loss=90.0939
	step [185/244], loss=108.7218
	step [186/244], loss=101.1280
	step [187/244], loss=108.4778
	step [188/244], loss=102.1431
	step [189/244], loss=102.7614
	step [190/244], loss=97.8022
	step [191/244], loss=97.6284
	step [192/244], loss=125.7972
	step [193/244], loss=101.5176
	step [194/244], loss=98.2763
	step [195/244], loss=92.5795
	step [196/244], loss=97.9604
	step [197/244], loss=121.9178
	step [198/244], loss=103.2703
	step [199/244], loss=93.9597
	step [200/244], loss=90.4256
	step [201/244], loss=110.1018
	step [202/244], loss=90.7234
	step [203/244], loss=77.1513
	step [204/244], loss=100.8716
	step [205/244], loss=103.2184
	step [206/244], loss=119.0393
	step [207/244], loss=99.8357
	step [208/244], loss=122.9499
	step [209/244], loss=107.6684
	step [210/244], loss=96.1408
	step [211/244], loss=94.9184
	step [212/244], loss=104.8879
	step [213/244], loss=95.0913
	step [214/244], loss=99.8458
	step [215/244], loss=92.5506
	step [216/244], loss=109.9249
	step [217/244], loss=100.9330
	step [218/244], loss=118.2659
	step [219/244], loss=105.1553
	step [220/244], loss=106.9447
	step [221/244], loss=83.5923
	step [222/244], loss=136.6872
	step [223/244], loss=95.4114
	step [224/244], loss=114.4083
	step [225/244], loss=101.9392
	step [226/244], loss=108.4415
	step [227/244], loss=94.9760
	step [228/244], loss=92.6540
	step [229/244], loss=107.1269
	step [230/244], loss=104.4494
	step [231/244], loss=107.5384
	step [232/244], loss=103.1609
	step [233/244], loss=83.5790
	step [234/244], loss=117.7581
	step [235/244], loss=121.2625
	step [236/244], loss=133.3764
	step [237/244], loss=92.3613
	step [238/244], loss=97.2806
	step [239/244], loss=93.1436
	step [240/244], loss=89.8250
	step [241/244], loss=108.2650
	step [242/244], loss=100.5514
	step [243/244], loss=95.0804
	step [244/244], loss=50.8481
	Evaluating
	loss=0.0253, precision=0.4713, recall=0.8907, f1=0.6165
saving model as: 1_saved_model.pth
Training epoch 11
	step [1/244], loss=89.8647
	step [2/244], loss=103.7721
	step [3/244], loss=120.1702
	step [4/244], loss=71.5488
	step [5/244], loss=100.4790
	step [6/244], loss=100.9942
	step [7/244], loss=95.0897
	step [8/244], loss=97.5348
	step [9/244], loss=107.6792
	step [10/244], loss=92.3277
	step [11/244], loss=111.5858
	step [12/244], loss=114.9414
	step [13/244], loss=79.4367
	step [14/244], loss=103.3079
	step [15/244], loss=92.5499
	step [16/244], loss=98.8374
	step [17/244], loss=97.8428
	step [18/244], loss=112.6722
	step [19/244], loss=120.7393
	step [20/244], loss=81.4890
	step [21/244], loss=89.7683
	step [22/244], loss=95.6168
	step [23/244], loss=101.4471
	step [24/244], loss=102.2481
	step [25/244], loss=104.0578
	step [26/244], loss=86.8808
	step [27/244], loss=88.7764
	step [28/244], loss=112.6804
	step [29/244], loss=92.5791
	step [30/244], loss=101.0430
	step [31/244], loss=103.2128
	step [32/244], loss=112.7577
	step [33/244], loss=112.0256
	step [34/244], loss=99.7161
	step [35/244], loss=100.2824
	step [36/244], loss=105.2458
	step [37/244], loss=90.4494
	step [38/244], loss=119.2833
	step [39/244], loss=98.2499
	step [40/244], loss=129.4975
	step [41/244], loss=87.7127
	step [42/244], loss=98.2697
	step [43/244], loss=124.6794
	step [44/244], loss=111.9548
	step [45/244], loss=87.3510
	step [46/244], loss=105.7521
	step [47/244], loss=94.8669
	step [48/244], loss=113.0642
	step [49/244], loss=116.7487
	step [50/244], loss=94.5480
	step [51/244], loss=98.9325
	step [52/244], loss=93.9622
	step [53/244], loss=101.4035
	step [54/244], loss=105.5509
	step [55/244], loss=106.6583
	step [56/244], loss=104.1182
	step [57/244], loss=96.5235
	step [58/244], loss=91.5621
	step [59/244], loss=97.4206
	step [60/244], loss=106.0340
	step [61/244], loss=100.3943
	step [62/244], loss=108.9746
	step [63/244], loss=103.4186
	step [64/244], loss=79.1794
	step [65/244], loss=95.4943
	step [66/244], loss=119.8584
	step [67/244], loss=108.7860
	step [68/244], loss=107.9680
	step [69/244], loss=124.0525
	step [70/244], loss=114.4879
	step [71/244], loss=74.5649
	step [72/244], loss=111.7068
	step [73/244], loss=97.5856
	step [74/244], loss=128.2852
	step [75/244], loss=109.6780
	step [76/244], loss=111.8196
	step [77/244], loss=104.2499
	step [78/244], loss=96.6285
	step [79/244], loss=104.0596
	step [80/244], loss=95.2756
	step [81/244], loss=86.4662
	step [82/244], loss=108.3352
	step [83/244], loss=87.9693
	step [84/244], loss=79.3810
	step [85/244], loss=98.3752
	step [86/244], loss=106.7707
	step [87/244], loss=90.3966
	step [88/244], loss=109.5615
	step [89/244], loss=96.1719
	step [90/244], loss=116.6957
	step [91/244], loss=111.7482
	step [92/244], loss=112.6645
	step [93/244], loss=97.6289
	step [94/244], loss=94.8373
	step [95/244], loss=103.5712
	step [96/244], loss=98.7245
	step [97/244], loss=111.5978
	step [98/244], loss=116.7973
	step [99/244], loss=103.8417
	step [100/244], loss=113.5569
	step [101/244], loss=85.4023
	step [102/244], loss=72.0519
	step [103/244], loss=96.6858
	step [104/244], loss=110.6663
	step [105/244], loss=108.5067
	step [106/244], loss=103.3106
	step [107/244], loss=99.0592
	step [108/244], loss=114.5923
	step [109/244], loss=103.6908
	step [110/244], loss=118.7051
	step [111/244], loss=106.0767
	step [112/244], loss=106.5981
	step [113/244], loss=96.5252
	step [114/244], loss=107.4213
	step [115/244], loss=101.8047
	step [116/244], loss=117.0291
	step [117/244], loss=105.1976
	step [118/244], loss=101.5201
	step [119/244], loss=121.4816
	step [120/244], loss=126.8694
	step [121/244], loss=98.8005
	step [122/244], loss=101.8653
	step [123/244], loss=100.4360
	step [124/244], loss=117.9903
	step [125/244], loss=101.4611
	step [126/244], loss=112.1230
	step [127/244], loss=100.8340
	step [128/244], loss=103.4811
	step [129/244], loss=107.2072
	step [130/244], loss=101.7228
	step [131/244], loss=96.4078
	step [132/244], loss=98.8479
	step [133/244], loss=104.8777
	step [134/244], loss=106.2155
	step [135/244], loss=81.0991
	step [136/244], loss=109.4843
	step [137/244], loss=98.5664
	step [138/244], loss=103.9000
	step [139/244], loss=104.6750
	step [140/244], loss=108.4199
	step [141/244], loss=99.1246
	step [142/244], loss=112.3909
	step [143/244], loss=97.4657
	step [144/244], loss=90.8232
	step [145/244], loss=102.6268
	step [146/244], loss=96.4537
	step [147/244], loss=103.7259
	step [148/244], loss=100.8253
	step [149/244], loss=92.3224
	step [150/244], loss=101.3642
	step [151/244], loss=92.1719
	step [152/244], loss=114.2189
	step [153/244], loss=85.4799
	step [154/244], loss=97.4681
	step [155/244], loss=96.9298
	step [156/244], loss=96.0651
	step [157/244], loss=96.9036
	step [158/244], loss=101.5348
	step [159/244], loss=118.6137
	step [160/244], loss=103.9997
	step [161/244], loss=96.3955
	step [162/244], loss=93.1658
	step [163/244], loss=116.8290
	step [164/244], loss=74.6093
	step [165/244], loss=95.2132
	step [166/244], loss=108.2380
	step [167/244], loss=89.6441
	step [168/244], loss=95.0734
	step [169/244], loss=97.0538
	step [170/244], loss=97.6562
	step [171/244], loss=92.4726
	step [172/244], loss=104.6008
	step [173/244], loss=123.9901
	step [174/244], loss=110.8913
	step [175/244], loss=85.8350
	step [176/244], loss=90.4452
	step [177/244], loss=96.6272
	step [178/244], loss=84.7995
	step [179/244], loss=91.6151
	step [180/244], loss=100.9288
	step [181/244], loss=109.0507
	step [182/244], loss=89.5994
	step [183/244], loss=97.8523
	step [184/244], loss=112.6018
	step [185/244], loss=95.8109
	step [186/244], loss=98.6769
	step [187/244], loss=99.8494
	step [188/244], loss=99.5432
	step [189/244], loss=86.2259
	step [190/244], loss=98.2891
	step [191/244], loss=95.6630
	step [192/244], loss=84.1740
	step [193/244], loss=103.1324
	step [194/244], loss=110.0984
	step [195/244], loss=103.1599
	step [196/244], loss=100.3371
	step [197/244], loss=101.0601
	step [198/244], loss=119.0029
	step [199/244], loss=100.0844
	step [200/244], loss=88.4711
	step [201/244], loss=93.5899
	step [202/244], loss=107.0294
	step [203/244], loss=96.4223
	step [204/244], loss=116.3305
	step [205/244], loss=87.8069
	step [206/244], loss=101.7922
	step [207/244], loss=84.3551
	step [208/244], loss=81.4678
	step [209/244], loss=112.4462
	step [210/244], loss=104.1759
	step [211/244], loss=87.0744
	step [212/244], loss=101.6853
	step [213/244], loss=98.6983
	step [214/244], loss=112.2057
	step [215/244], loss=118.6060
	step [216/244], loss=96.5613
	step [217/244], loss=110.3873
	step [218/244], loss=86.3181
	step [219/244], loss=100.9620
	step [220/244], loss=87.7450
	step [221/244], loss=73.8635
	step [222/244], loss=102.7503
	step [223/244], loss=84.9476
	step [224/244], loss=80.3828
	step [225/244], loss=100.3179
	step [226/244], loss=97.6337
	step [227/244], loss=107.3307
	step [228/244], loss=113.0742
	step [229/244], loss=101.0964
	step [230/244], loss=93.3041
	step [231/244], loss=71.8545
	step [232/244], loss=85.4451
	step [233/244], loss=113.9616
	step [234/244], loss=99.7074
	step [235/244], loss=87.0485
	step [236/244], loss=106.5159
	step [237/244], loss=107.7187
	step [238/244], loss=93.4492
	step [239/244], loss=93.8998
	step [240/244], loss=114.6016
	step [241/244], loss=100.7957
	step [242/244], loss=77.8891
	step [243/244], loss=105.4579
	step [244/244], loss=44.0846
	Evaluating
	loss=0.0229, precision=0.4300, recall=0.8947, f1=0.5808
Training epoch 12
	step [1/244], loss=90.3829
	step [2/244], loss=96.9349
	step [3/244], loss=111.3941
	step [4/244], loss=108.3044
	step [5/244], loss=102.6844
	step [6/244], loss=91.8613
	step [7/244], loss=117.1702
	step [8/244], loss=100.2574
	step [9/244], loss=93.6924
	step [10/244], loss=102.6419
	step [11/244], loss=103.8978
	step [12/244], loss=92.8218
	step [13/244], loss=95.0244
	step [14/244], loss=102.4170
	step [15/244], loss=97.3885
	step [16/244], loss=103.5294
	step [17/244], loss=101.9937
	step [18/244], loss=107.1999
	step [19/244], loss=85.0103
	step [20/244], loss=123.6661
	step [21/244], loss=90.6361
	step [22/244], loss=103.5317
	step [23/244], loss=91.2612
	step [24/244], loss=127.0432
	step [25/244], loss=98.1986
	step [26/244], loss=102.9020
	step [27/244], loss=94.1144
	step [28/244], loss=110.8937
	step [29/244], loss=98.7791
	step [30/244], loss=100.4559
	step [31/244], loss=106.3832
	step [32/244], loss=115.0958
	step [33/244], loss=94.8559
	step [34/244], loss=113.9037
	step [35/244], loss=116.9302
	step [36/244], loss=95.7786
	step [37/244], loss=103.4246
	step [38/244], loss=105.1505
	step [39/244], loss=84.9208
	step [40/244], loss=93.0786
	step [41/244], loss=102.8219
	step [42/244], loss=94.1085
	step [43/244], loss=92.3248
	step [44/244], loss=118.7075
	step [45/244], loss=91.0624
	step [46/244], loss=75.7780
	step [47/244], loss=107.4732
	step [48/244], loss=93.7032
	step [49/244], loss=99.0457
	step [50/244], loss=94.2557
	step [51/244], loss=120.5810
	step [52/244], loss=108.3185
	step [53/244], loss=84.6021
	step [54/244], loss=92.5435
	step [55/244], loss=95.4028
	step [56/244], loss=111.8018
	step [57/244], loss=113.8925
	step [58/244], loss=93.3110
	step [59/244], loss=119.1379
	step [60/244], loss=109.7537
	step [61/244], loss=78.8879
	step [62/244], loss=88.5427
	step [63/244], loss=105.6377
	step [64/244], loss=85.6182
	step [65/244], loss=96.6207
	step [66/244], loss=107.7734
	step [67/244], loss=116.5049
	step [68/244], loss=102.5426
	step [69/244], loss=73.0158
	step [70/244], loss=102.1605
	step [71/244], loss=87.9508
	step [72/244], loss=111.9259
	step [73/244], loss=106.8433
	step [74/244], loss=98.5747
	step [75/244], loss=78.2588
	step [76/244], loss=115.3281
	step [77/244], loss=93.9857
	step [78/244], loss=101.5911
	step [79/244], loss=109.3504
	step [80/244], loss=86.6185
	step [81/244], loss=88.4139
	step [82/244], loss=102.0264
	step [83/244], loss=85.2677
	step [84/244], loss=109.9277
	step [85/244], loss=104.0177
	step [86/244], loss=93.6466
	step [87/244], loss=100.6337
	step [88/244], loss=107.3771
	step [89/244], loss=102.2400
	step [90/244], loss=93.8501
	step [91/244], loss=95.2320
	step [92/244], loss=106.2259
	step [93/244], loss=92.7616
	step [94/244], loss=117.2187
	step [95/244], loss=115.1674
	step [96/244], loss=88.9431
	step [97/244], loss=100.3811
	step [98/244], loss=80.3658
	step [99/244], loss=109.5992
	step [100/244], loss=94.3644
	step [101/244], loss=106.5591
	step [102/244], loss=96.9481
	step [103/244], loss=119.5857
	step [104/244], loss=111.8638
	step [105/244], loss=101.2908
	step [106/244], loss=88.5201
	step [107/244], loss=105.7688
	step [108/244], loss=100.0063
	step [109/244], loss=96.2394
	step [110/244], loss=99.8862
	step [111/244], loss=110.2137
	step [112/244], loss=85.2146
	step [113/244], loss=98.4540
	step [114/244], loss=102.0289
	step [115/244], loss=100.5570
	step [116/244], loss=79.0307
	step [117/244], loss=104.8905
	step [118/244], loss=103.8736
	step [119/244], loss=98.2191
	step [120/244], loss=106.5510
	step [121/244], loss=109.2105
	step [122/244], loss=98.3377
	step [123/244], loss=87.1532
	step [124/244], loss=88.1812
	step [125/244], loss=96.6663
	step [126/244], loss=87.7151
	step [127/244], loss=93.8040
	step [128/244], loss=113.6977
	step [129/244], loss=86.3055
	step [130/244], loss=94.0215
	step [131/244], loss=94.3100
	step [132/244], loss=109.6363
	step [133/244], loss=97.0329
	step [134/244], loss=99.4610
	step [135/244], loss=109.0090
	step [136/244], loss=100.0145
	step [137/244], loss=111.8792
	step [138/244], loss=109.5480
	step [139/244], loss=98.3214
	step [140/244], loss=121.9336
	step [141/244], loss=102.4145
	step [142/244], loss=101.9131
	step [143/244], loss=97.7133
	step [144/244], loss=98.7389
	step [145/244], loss=85.7148
	step [146/244], loss=122.0502
	step [147/244], loss=83.5359
	step [148/244], loss=79.8659
	step [149/244], loss=113.9733
	step [150/244], loss=97.3575
	step [151/244], loss=122.6460
	step [152/244], loss=118.6012
	step [153/244], loss=92.3006
	step [154/244], loss=92.1579
	step [155/244], loss=89.9681
	step [156/244], loss=103.9784
	step [157/244], loss=106.2200
	step [158/244], loss=92.5824
	step [159/244], loss=89.4464
	step [160/244], loss=97.7333
	step [161/244], loss=90.6347
	step [162/244], loss=101.6789
	step [163/244], loss=108.2774
	step [164/244], loss=87.9746
	step [165/244], loss=84.7138
	step [166/244], loss=98.7928
	step [167/244], loss=110.9273
	step [168/244], loss=109.6204
	step [169/244], loss=81.4877
	step [170/244], loss=107.4200
	step [171/244], loss=82.1465
	step [172/244], loss=95.6587
	step [173/244], loss=90.9295
	step [174/244], loss=96.6078
	step [175/244], loss=105.3971
	step [176/244], loss=102.6498
	step [177/244], loss=102.5476
	step [178/244], loss=101.7553
	step [179/244], loss=109.8913
	step [180/244], loss=104.4488
	step [181/244], loss=97.0450
	step [182/244], loss=91.6971
	step [183/244], loss=110.5412
	step [184/244], loss=106.2030
	step [185/244], loss=94.0612
	step [186/244], loss=90.5437
	step [187/244], loss=95.0528
	step [188/244], loss=97.0770
	step [189/244], loss=90.0609
	step [190/244], loss=116.6059
	step [191/244], loss=101.4463
	step [192/244], loss=95.4675
	step [193/244], loss=90.0152
	step [194/244], loss=104.4134
	step [195/244], loss=104.2969
	step [196/244], loss=100.2203
	step [197/244], loss=100.3787
	step [198/244], loss=120.8584
	step [199/244], loss=95.9004
	step [200/244], loss=97.4079
	step [201/244], loss=94.3833
	step [202/244], loss=110.9457
	step [203/244], loss=122.5375
	step [204/244], loss=69.6699
	step [205/244], loss=105.0430
	step [206/244], loss=107.0325
	step [207/244], loss=91.1986
	step [208/244], loss=103.7990
	step [209/244], loss=88.6824
	step [210/244], loss=89.0522
	step [211/244], loss=94.0476
	step [212/244], loss=98.2500
	step [213/244], loss=88.6964
	step [214/244], loss=109.4580
	step [215/244], loss=100.9974
	step [216/244], loss=92.9735
	step [217/244], loss=99.0778
	step [218/244], loss=98.1993
	step [219/244], loss=101.5684
	step [220/244], loss=75.9349
	step [221/244], loss=112.0117
	step [222/244], loss=114.8294
	step [223/244], loss=114.6758
	step [224/244], loss=81.1006
	step [225/244], loss=104.4522
	step [226/244], loss=85.0018
	step [227/244], loss=89.5689
	step [228/244], loss=87.2988
	step [229/244], loss=98.9820
	step [230/244], loss=93.4078
	step [231/244], loss=114.9372
	step [232/244], loss=110.2591
	step [233/244], loss=73.9014
	step [234/244], loss=110.7724
	step [235/244], loss=92.9651
	step [236/244], loss=96.7938
	step [237/244], loss=96.5957
	step [238/244], loss=93.6099
	step [239/244], loss=105.0409
	step [240/244], loss=101.8264
	step [241/244], loss=109.8956
	step [242/244], loss=90.0654
	step [243/244], loss=112.0740
	step [244/244], loss=44.7406
	Evaluating
	loss=0.0193, precision=0.4518, recall=0.8904, f1=0.5994
Training epoch 13
	step [1/244], loss=87.4596
	step [2/244], loss=104.0224
	step [3/244], loss=104.1193
	step [4/244], loss=94.6365
	step [5/244], loss=105.9576
	step [6/244], loss=96.2653
	step [7/244], loss=70.7361
	step [8/244], loss=100.5936
	step [9/244], loss=83.2995
	step [10/244], loss=92.3725
	step [11/244], loss=93.4311
	step [12/244], loss=120.9500
	step [13/244], loss=95.6930
	step [14/244], loss=96.9092
	step [15/244], loss=109.8290
	step [16/244], loss=118.7388
	step [17/244], loss=106.6015
	step [18/244], loss=108.7364
	step [19/244], loss=130.0501
	step [20/244], loss=99.3551
	step [21/244], loss=88.2136
	step [22/244], loss=85.5719
	step [23/244], loss=116.8303
	step [24/244], loss=79.1179
	step [25/244], loss=96.2301
	step [26/244], loss=97.3195
	step [27/244], loss=109.0135
	step [28/244], loss=73.2146
	step [29/244], loss=99.0852
	step [30/244], loss=80.7931
	step [31/244], loss=110.4150
	step [32/244], loss=91.0304
	step [33/244], loss=92.7260
	step [34/244], loss=93.4284
	step [35/244], loss=100.4972
	step [36/244], loss=71.5473
	step [37/244], loss=117.6167
	step [38/244], loss=103.3344
	step [39/244], loss=100.7481
	step [40/244], loss=72.9267
	step [41/244], loss=93.1577
	step [42/244], loss=107.6332
	step [43/244], loss=93.5213
	step [44/244], loss=94.0748
	step [45/244], loss=84.1009
	step [46/244], loss=96.0025
	step [47/244], loss=94.7678
	step [48/244], loss=104.7991
	step [49/244], loss=101.5217
	step [50/244], loss=80.5565
	step [51/244], loss=113.7329
	step [52/244], loss=86.0568
	step [53/244], loss=114.4999
	step [54/244], loss=105.8794
	step [55/244], loss=89.9663
	step [56/244], loss=89.3150
	step [57/244], loss=92.9985
	step [58/244], loss=100.5026
	step [59/244], loss=86.4978
	step [60/244], loss=103.5883
	step [61/244], loss=87.5142
	step [62/244], loss=93.5216
	step [63/244], loss=107.9748
	step [64/244], loss=90.2128
	step [65/244], loss=83.8993
	step [66/244], loss=79.0771
	step [67/244], loss=82.5445
	step [68/244], loss=103.7665
	step [69/244], loss=96.6439
	step [70/244], loss=92.6036
	step [71/244], loss=107.7131
	step [72/244], loss=103.5401
	step [73/244], loss=93.6559
	step [74/244], loss=94.7741
	step [75/244], loss=101.1165
	step [76/244], loss=105.0184
	step [77/244], loss=96.7757
	step [78/244], loss=107.0873
	step [79/244], loss=88.9793
	step [80/244], loss=89.8486
	step [81/244], loss=104.4404
	step [82/244], loss=87.8320
	step [83/244], loss=107.9396
	step [84/244], loss=105.2057
	step [85/244], loss=115.8182
	step [86/244], loss=103.6967
	step [87/244], loss=100.8019
	step [88/244], loss=94.6272
	step [89/244], loss=94.3523
	step [90/244], loss=91.7853
	step [91/244], loss=107.1883
	step [92/244], loss=95.5437
	step [93/244], loss=100.9495
	step [94/244], loss=105.4765
	step [95/244], loss=86.2721
	step [96/244], loss=96.7588
	step [97/244], loss=133.3039
	step [98/244], loss=117.9617
	step [99/244], loss=88.2019
	step [100/244], loss=104.7788
	step [101/244], loss=81.0663
	step [102/244], loss=124.9829
	step [103/244], loss=87.9752
	step [104/244], loss=104.4784
	step [105/244], loss=125.3885
	step [106/244], loss=124.6684
	step [107/244], loss=112.8929
	step [108/244], loss=111.7223
	step [109/244], loss=86.9084
	step [110/244], loss=101.6814
	step [111/244], loss=88.5701
	step [112/244], loss=97.7390
	step [113/244], loss=113.9160
	step [114/244], loss=95.9887
	step [115/244], loss=86.9895
	step [116/244], loss=112.1653
	step [117/244], loss=87.2787
	step [118/244], loss=99.9877
	step [119/244], loss=81.8497
	step [120/244], loss=96.4539
	step [121/244], loss=99.7215
	step [122/244], loss=111.2053
	step [123/244], loss=97.9244
	step [124/244], loss=87.0394
	step [125/244], loss=103.1573
	step [126/244], loss=116.6283
	step [127/244], loss=98.2687
	step [128/244], loss=103.8532
	step [129/244], loss=111.8642
	step [130/244], loss=108.0935
	step [131/244], loss=98.2674
	step [132/244], loss=91.6511
	step [133/244], loss=97.0905
	step [134/244], loss=107.3125
	step [135/244], loss=100.7336
	step [136/244], loss=91.1921
	step [137/244], loss=88.3299
	step [138/244], loss=100.5005
	step [139/244], loss=110.1591
	step [140/244], loss=97.1497
	step [141/244], loss=99.0000
	step [142/244], loss=111.3753
	step [143/244], loss=99.5027
	step [144/244], loss=97.3374
	step [145/244], loss=101.1397
	step [146/244], loss=91.8034
	step [147/244], loss=100.2121
	step [148/244], loss=101.4932
	step [149/244], loss=83.7325
	step [150/244], loss=97.3232
	step [151/244], loss=91.5250
	step [152/244], loss=110.6535
	step [153/244], loss=117.6686
	step [154/244], loss=101.4532
	step [155/244], loss=108.0636
	step [156/244], loss=99.1323
	step [157/244], loss=93.5408
	step [158/244], loss=105.1645
	step [159/244], loss=110.4462
	step [160/244], loss=97.5355
	step [161/244], loss=87.3548
	step [162/244], loss=97.4989
	step [163/244], loss=105.8859
	step [164/244], loss=88.8147
	step [165/244], loss=85.6128
	step [166/244], loss=91.7567
	step [167/244], loss=85.6094
	step [168/244], loss=97.5107
	step [169/244], loss=91.9355
	step [170/244], loss=80.5384
	step [171/244], loss=95.7249
	step [172/244], loss=105.0096
	step [173/244], loss=114.3152
	step [174/244], loss=89.6945
	step [175/244], loss=107.1025
	step [176/244], loss=98.6918
	step [177/244], loss=85.6418
	step [178/244], loss=86.6567
	step [179/244], loss=102.7763
	step [180/244], loss=91.4412
	step [181/244], loss=120.8658
	step [182/244], loss=106.1340
	step [183/244], loss=112.5557
	step [184/244], loss=92.7588
	step [185/244], loss=96.7981
	step [186/244], loss=87.7551
	step [187/244], loss=83.3534
	step [188/244], loss=102.6040
	step [189/244], loss=84.8795
	step [190/244], loss=88.8977
	step [191/244], loss=110.0475
	step [192/244], loss=105.0060
	step [193/244], loss=105.9205
	step [194/244], loss=94.1362
	step [195/244], loss=92.7164
	step [196/244], loss=91.9727
	step [197/244], loss=105.9676
	step [198/244], loss=101.1597
	step [199/244], loss=93.6429
	step [200/244], loss=89.9361
	step [201/244], loss=89.7707
	step [202/244], loss=108.5238
	step [203/244], loss=96.2375
	step [204/244], loss=124.7436
	step [205/244], loss=91.4934
	step [206/244], loss=108.9609
	step [207/244], loss=99.4916
	step [208/244], loss=104.5309
	step [209/244], loss=96.8365
	step [210/244], loss=94.9687
	step [211/244], loss=95.3291
	step [212/244], loss=105.9204
	step [213/244], loss=103.8691
	step [214/244], loss=86.2554
	step [215/244], loss=104.9494
	step [216/244], loss=85.4573
	step [217/244], loss=89.6847
	step [218/244], loss=112.1395
	step [219/244], loss=98.9903
	step [220/244], loss=86.6886
	step [221/244], loss=81.0039
	step [222/244], loss=87.8288
	step [223/244], loss=95.9536
	step [224/244], loss=113.4546
	step [225/244], loss=101.0246
	step [226/244], loss=103.5057
	step [227/244], loss=75.5982
	step [228/244], loss=106.0051
	step [229/244], loss=78.8480
	step [230/244], loss=107.1291
	step [231/244], loss=77.0175
	step [232/244], loss=104.2539
	step [233/244], loss=114.1734
	step [234/244], loss=90.1315
	step [235/244], loss=83.9664
	step [236/244], loss=103.7790
	step [237/244], loss=99.9989
	step [238/244], loss=97.0562
	step [239/244], loss=90.5727
	step [240/244], loss=107.0340
	step [241/244], loss=92.6453
	step [242/244], loss=100.5054
	step [243/244], loss=80.5144
	step [244/244], loss=49.2333
	Evaluating
	loss=0.0180, precision=0.3739, recall=0.9193, f1=0.5316
Training epoch 14
	step [1/244], loss=96.5193
	step [2/244], loss=98.7161
	step [3/244], loss=105.1511
	step [4/244], loss=108.9392
	step [5/244], loss=97.9521
	step [6/244], loss=92.8872
	step [7/244], loss=106.9332
	step [8/244], loss=95.0492
	step [9/244], loss=98.4405
	step [10/244], loss=100.9777
	step [11/244], loss=79.6924
	step [12/244], loss=108.1183
	step [13/244], loss=112.7255
	step [14/244], loss=101.9274
	step [15/244], loss=109.8788
	step [16/244], loss=75.2225
	step [17/244], loss=86.3702
	step [18/244], loss=78.9643
	step [19/244], loss=91.2602
	step [20/244], loss=85.3388
	step [21/244], loss=129.3741
	step [22/244], loss=104.2555
	step [23/244], loss=123.7569
	step [24/244], loss=100.9509
	step [25/244], loss=97.8879
	step [26/244], loss=93.8889
	step [27/244], loss=89.6561
	step [28/244], loss=91.1232
	step [29/244], loss=103.9042
	step [30/244], loss=88.9509
	step [31/244], loss=97.3290
	step [32/244], loss=92.5491
	step [33/244], loss=94.9556
	step [34/244], loss=94.9112
	step [35/244], loss=94.7005
	step [36/244], loss=95.5221
	step [37/244], loss=104.2911
	step [38/244], loss=95.1432
	step [39/244], loss=96.6808
	step [40/244], loss=108.6707
	step [41/244], loss=88.3235
	step [42/244], loss=89.3933
	step [43/244], loss=86.2415
	step [44/244], loss=99.6830
	step [45/244], loss=105.4293
	step [46/244], loss=71.1269
	step [47/244], loss=100.9178
	step [48/244], loss=95.8768
	step [49/244], loss=90.9676
	step [50/244], loss=85.6317
	step [51/244], loss=99.5205
	step [52/244], loss=104.6781
	step [53/244], loss=107.7190
	step [54/244], loss=109.5613
	step [55/244], loss=104.9519
	step [56/244], loss=100.1736
	step [57/244], loss=87.3606
	step [58/244], loss=97.2283
	step [59/244], loss=111.6972
	step [60/244], loss=81.9535
	step [61/244], loss=91.9576
	step [62/244], loss=95.3540
	step [63/244], loss=89.1735
	step [64/244], loss=121.1199
	step [65/244], loss=111.6126
	step [66/244], loss=80.8312
	step [67/244], loss=95.2838
	step [68/244], loss=131.9551
	step [69/244], loss=106.9037
	step [70/244], loss=98.1334
	step [71/244], loss=89.6405
	step [72/244], loss=98.9251
	step [73/244], loss=101.7116
	step [74/244], loss=108.9535
	step [75/244], loss=108.2654
	step [76/244], loss=107.1501
	step [77/244], loss=78.0481
	step [78/244], loss=85.9905
	step [79/244], loss=85.0572
	step [80/244], loss=104.5550
	step [81/244], loss=111.8385
	step [82/244], loss=92.3528
	step [83/244], loss=91.8175
	step [84/244], loss=97.3006
	step [85/244], loss=113.2181
	step [86/244], loss=114.1506
	step [87/244], loss=101.4258
	step [88/244], loss=97.0360
	step [89/244], loss=97.2074
	step [90/244], loss=83.9987
	step [91/244], loss=81.8934
	step [92/244], loss=99.8171
	step [93/244], loss=109.4823
	step [94/244], loss=102.1015
	step [95/244], loss=97.7359
	step [96/244], loss=103.7942
	step [97/244], loss=90.3307
	step [98/244], loss=106.0844
	step [99/244], loss=95.8286
	step [100/244], loss=104.6788
	step [101/244], loss=102.2899
	step [102/244], loss=72.7375
	step [103/244], loss=109.0882
	step [104/244], loss=117.7385
	step [105/244], loss=93.2630
	step [106/244], loss=92.7175
	step [107/244], loss=87.2781
	step [108/244], loss=110.2921
	step [109/244], loss=98.5970
	step [110/244], loss=101.2156
	step [111/244], loss=88.1303
	step [112/244], loss=91.2099
	step [113/244], loss=104.3108
	step [114/244], loss=96.2718
	step [115/244], loss=113.7092
	step [116/244], loss=95.9898
	step [117/244], loss=109.6754
	step [118/244], loss=91.6153
	step [119/244], loss=96.8058
	step [120/244], loss=101.2304
	step [121/244], loss=77.3210
	step [122/244], loss=92.0394
	step [123/244], loss=91.9266
	step [124/244], loss=93.0927
	step [125/244], loss=96.9295
	step [126/244], loss=113.6501
	step [127/244], loss=93.9616
	step [128/244], loss=114.8842
	step [129/244], loss=89.4570
	step [130/244], loss=101.8241
	step [131/244], loss=91.0116
	step [132/244], loss=74.1382
	step [133/244], loss=90.6736
	step [134/244], loss=91.5750
	step [135/244], loss=96.0432
	step [136/244], loss=107.8706
	step [137/244], loss=91.6134
	step [138/244], loss=97.3098
	step [139/244], loss=109.9543
	step [140/244], loss=92.7413
	step [141/244], loss=88.1197
	step [142/244], loss=118.4438
	step [143/244], loss=90.3412
	step [144/244], loss=84.4521
	step [145/244], loss=94.1782
	step [146/244], loss=94.2362
	step [147/244], loss=109.2683
	step [148/244], loss=101.9855
	step [149/244], loss=104.0917
	step [150/244], loss=91.3767
	step [151/244], loss=94.1000
	step [152/244], loss=97.6559
	step [153/244], loss=117.0967
	step [154/244], loss=92.2429
	step [155/244], loss=94.7219
	step [156/244], loss=89.7035
	step [157/244], loss=105.1862
	step [158/244], loss=94.4535
	step [159/244], loss=91.1970
	step [160/244], loss=101.1909
	step [161/244], loss=100.1547
	step [162/244], loss=88.7568
	step [163/244], loss=105.8751
	step [164/244], loss=118.3518
	step [165/244], loss=105.9006
	step [166/244], loss=98.7056
	step [167/244], loss=82.7129
	step [168/244], loss=84.9622
	step [169/244], loss=95.5586
	step [170/244], loss=91.9729
	step [171/244], loss=108.3914
	step [172/244], loss=81.0978
	step [173/244], loss=97.2811
	step [174/244], loss=94.8723
	step [175/244], loss=102.5887
	step [176/244], loss=111.3084
	step [177/244], loss=92.9290
	step [178/244], loss=87.4840
	step [179/244], loss=104.3513
	step [180/244], loss=81.4252
	step [181/244], loss=109.6775
	step [182/244], loss=87.3123
	step [183/244], loss=103.3578
	step [184/244], loss=82.2340
	step [185/244], loss=91.7362
	step [186/244], loss=80.0854
	step [187/244], loss=102.9265
	step [188/244], loss=119.1229
	step [189/244], loss=117.1677
	step [190/244], loss=92.2768
	step [191/244], loss=96.7879
	step [192/244], loss=104.9520
	step [193/244], loss=93.1122
	step [194/244], loss=94.1874
	step [195/244], loss=98.7810
	step [196/244], loss=83.2245
	step [197/244], loss=91.9611
	step [198/244], loss=85.7265
	step [199/244], loss=92.2453
	step [200/244], loss=91.1113
	step [201/244], loss=109.5199
	step [202/244], loss=90.2259
	step [203/244], loss=95.9643
	step [204/244], loss=110.6101
	step [205/244], loss=95.8803
	step [206/244], loss=76.6394
	step [207/244], loss=104.4913
	step [208/244], loss=82.1967
	step [209/244], loss=90.4685
	step [210/244], loss=100.9221
	step [211/244], loss=102.8915
	step [212/244], loss=99.8572
	step [213/244], loss=110.4904
	step [214/244], loss=93.1661
	step [215/244], loss=85.1232
	step [216/244], loss=131.3834
	step [217/244], loss=89.6606
	step [218/244], loss=91.5438
	step [219/244], loss=72.7500
	step [220/244], loss=104.0763
	step [221/244], loss=104.4876
	step [222/244], loss=95.4013
	step [223/244], loss=72.4748
	step [224/244], loss=108.9793
	step [225/244], loss=74.3300
	step [226/244], loss=96.9955
	step [227/244], loss=96.4456
	step [228/244], loss=110.1454
	step [229/244], loss=109.1241
	step [230/244], loss=92.1601
	step [231/244], loss=107.3404
	step [232/244], loss=97.8859
	step [233/244], loss=95.7298
	step [234/244], loss=93.4101
	step [235/244], loss=88.8047
	step [236/244], loss=100.3492
	step [237/244], loss=90.6747
	step [238/244], loss=108.1344
	step [239/244], loss=87.9315
	step [240/244], loss=104.5063
	step [241/244], loss=93.8522
	step [242/244], loss=88.4216
	step [243/244], loss=86.4516
	step [244/244], loss=40.8576
	Evaluating
	loss=0.0165, precision=0.3924, recall=0.9008, f1=0.5467
Training epoch 15
	step [1/244], loss=108.3672
	step [2/244], loss=84.9911
	step [3/244], loss=116.4128
	step [4/244], loss=86.3907
	step [5/244], loss=108.3014
	step [6/244], loss=84.8016
	step [7/244], loss=104.3380
	step [8/244], loss=100.1186
	step [9/244], loss=109.3438
	step [10/244], loss=109.0017
	step [11/244], loss=87.2072
	step [12/244], loss=100.4153
	step [13/244], loss=88.3036
	step [14/244], loss=101.6189
	step [15/244], loss=96.9913
	step [16/244], loss=89.1127
	step [17/244], loss=104.8658
	step [18/244], loss=85.7395
	step [19/244], loss=94.9970
	step [20/244], loss=101.7495
	step [21/244], loss=107.0449
	step [22/244], loss=85.3694
	step [23/244], loss=80.8813
	step [24/244], loss=93.1280
	step [25/244], loss=94.4501
	step [26/244], loss=92.6673
	step [27/244], loss=85.8860
	step [28/244], loss=83.1025
	step [29/244], loss=93.4364
	step [30/244], loss=104.2226
	step [31/244], loss=90.8799
	step [32/244], loss=93.7212
	step [33/244], loss=91.7428
	step [34/244], loss=101.2848
	step [35/244], loss=97.7273
	step [36/244], loss=72.1953
	step [37/244], loss=90.8741
	step [38/244], loss=109.9497
	step [39/244], loss=88.7598
	step [40/244], loss=119.1145
	step [41/244], loss=105.9708
	step [42/244], loss=98.9340
	step [43/244], loss=94.2242
	step [44/244], loss=93.4852
	step [45/244], loss=89.0921
	step [46/244], loss=103.9515
	step [47/244], loss=101.8238
	step [48/244], loss=84.9422
	step [49/244], loss=92.6617
	step [50/244], loss=100.0492
	step [51/244], loss=95.3240
	step [52/244], loss=89.7823
	step [53/244], loss=88.4646
	step [54/244], loss=82.2141
	step [55/244], loss=92.9612
	step [56/244], loss=88.4999
	step [57/244], loss=87.2390
	step [58/244], loss=107.1104
	step [59/244], loss=88.8493
	step [60/244], loss=87.6858
	step [61/244], loss=113.9309
	step [62/244], loss=100.1950
	step [63/244], loss=105.8881
	step [64/244], loss=96.3899
	step [65/244], loss=103.3551
	step [66/244], loss=93.2900
	step [67/244], loss=98.4754
	step [68/244], loss=108.9514
	step [69/244], loss=87.3980
	step [70/244], loss=91.7033
	step [71/244], loss=114.6009
	step [72/244], loss=85.6019
	step [73/244], loss=79.8517
	step [74/244], loss=95.3975
	step [75/244], loss=92.5350
	step [76/244], loss=95.1514
	step [77/244], loss=96.6065
	step [78/244], loss=99.4401
	step [79/244], loss=102.8020
	step [80/244], loss=97.3876
	step [81/244], loss=104.0427
	step [82/244], loss=108.3166
	step [83/244], loss=93.7304
	step [84/244], loss=94.7041
	step [85/244], loss=111.5412
	step [86/244], loss=76.7550
	step [87/244], loss=86.5279
	step [88/244], loss=93.9844
	step [89/244], loss=78.9922
	step [90/244], loss=96.8670
	step [91/244], loss=78.9494
	step [92/244], loss=101.7252
	step [93/244], loss=87.7954
	step [94/244], loss=89.3874
	step [95/244], loss=92.2213
	step [96/244], loss=111.3678
	step [97/244], loss=113.4644
	step [98/244], loss=86.7805
	step [99/244], loss=110.4479
	step [100/244], loss=93.6181
	step [101/244], loss=104.8613
	step [102/244], loss=100.2179
	step [103/244], loss=94.4861
	step [104/244], loss=107.2800
	step [105/244], loss=83.3186
	step [106/244], loss=92.6977
	step [107/244], loss=97.1427
	step [108/244], loss=85.9283
	step [109/244], loss=96.7464
	step [110/244], loss=113.9640
	step [111/244], loss=100.8301
	step [112/244], loss=108.9412
	step [113/244], loss=103.5199
	step [114/244], loss=90.9158
	step [115/244], loss=82.7935
	step [116/244], loss=109.2859
	step [117/244], loss=89.2011
	step [118/244], loss=114.7154
	step [119/244], loss=91.9037
	step [120/244], loss=100.4933
	step [121/244], loss=79.9882
	step [122/244], loss=93.6032
	step [123/244], loss=94.2025
	step [124/244], loss=97.0028
	step [125/244], loss=85.9100
	step [126/244], loss=100.0094
	step [127/244], loss=74.0703
	step [128/244], loss=100.9618
	step [129/244], loss=107.1532
	step [130/244], loss=107.8250
	step [131/244], loss=89.1033
	step [132/244], loss=96.4480
	step [133/244], loss=93.2465
	step [134/244], loss=91.1400
	step [135/244], loss=84.3414
	step [136/244], loss=107.9903
	step [137/244], loss=80.5970
	step [138/244], loss=108.8067
	step [139/244], loss=93.8338
	step [140/244], loss=96.5163
	step [141/244], loss=91.0352
	step [142/244], loss=82.5741
	step [143/244], loss=64.1889
	step [144/244], loss=101.5031
	step [145/244], loss=115.6308
	step [146/244], loss=96.2819
	step [147/244], loss=77.3242
	step [148/244], loss=87.2627
	step [149/244], loss=112.4762
	step [150/244], loss=107.4683
	step [151/244], loss=98.8818
	step [152/244], loss=94.3931
	step [153/244], loss=97.8479
	step [154/244], loss=112.9384
	step [155/244], loss=94.4621
	step [156/244], loss=102.0745
	step [157/244], loss=102.5922
	step [158/244], loss=82.0583
	step [159/244], loss=76.3039
	step [160/244], loss=107.7905
	step [161/244], loss=94.4648
	step [162/244], loss=102.3641
	step [163/244], loss=116.9873
	step [164/244], loss=108.2076
	step [165/244], loss=103.7498
	step [166/244], loss=98.0184
	step [167/244], loss=95.7368
	step [168/244], loss=107.3229
	step [169/244], loss=77.8773
	step [170/244], loss=99.7966
	step [171/244], loss=90.3460
	step [172/244], loss=102.5915
	step [173/244], loss=86.1831
	step [174/244], loss=115.3363
	step [175/244], loss=97.5748
	step [176/244], loss=87.3731
	step [177/244], loss=109.1211
	step [178/244], loss=83.8224
	step [179/244], loss=99.6588
	step [180/244], loss=93.0089
	step [181/244], loss=106.8845
	step [182/244], loss=97.7257
	step [183/244], loss=104.7436
	step [184/244], loss=90.7855
	step [185/244], loss=98.3267
	step [186/244], loss=99.0627
	step [187/244], loss=99.6600
	step [188/244], loss=93.4616
	step [189/244], loss=86.9254
	step [190/244], loss=121.2658
	step [191/244], loss=103.5911
	step [192/244], loss=77.5911
	step [193/244], loss=87.3877
	step [194/244], loss=83.7187
	step [195/244], loss=92.9350
	step [196/244], loss=104.9486
	step [197/244], loss=98.9922
	step [198/244], loss=122.5469
	step [199/244], loss=95.5224
	step [200/244], loss=97.5594
	step [201/244], loss=92.5401
	step [202/244], loss=96.7589
	step [203/244], loss=87.0421
	step [204/244], loss=112.2654
	step [205/244], loss=101.1288
	step [206/244], loss=85.0822
	step [207/244], loss=101.9645
	step [208/244], loss=96.7560
	step [209/244], loss=94.2211
	step [210/244], loss=101.5892
	step [211/244], loss=113.1562
	step [212/244], loss=85.0276
	step [213/244], loss=96.7066
	step [214/244], loss=74.0366
	step [215/244], loss=98.4576
	step [216/244], loss=98.9954
	step [217/244], loss=94.8105
	step [218/244], loss=115.3751
	step [219/244], loss=96.3695
	step [220/244], loss=87.4927
	step [221/244], loss=103.4340
	step [222/244], loss=99.6582
	step [223/244], loss=92.5015
	step [224/244], loss=90.8371
	step [225/244], loss=96.0349
	step [226/244], loss=99.3797
	step [227/244], loss=96.2802
	step [228/244], loss=78.0397
	step [229/244], loss=116.2389
	step [230/244], loss=98.4598
	step [231/244], loss=99.6083
	step [232/244], loss=83.8978
	step [233/244], loss=112.0494
	step [234/244], loss=101.4579
	step [235/244], loss=98.2752
	step [236/244], loss=75.9692
	step [237/244], loss=86.1248
	step [238/244], loss=91.6475
	step [239/244], loss=110.7490
	step [240/244], loss=78.3555
	step [241/244], loss=83.8766
	step [242/244], loss=78.0173
	step [243/244], loss=90.8418
	step [244/244], loss=41.3661
	Evaluating
	loss=0.0154, precision=0.3858, recall=0.9151, f1=0.5427
Training epoch 16
	step [1/244], loss=87.4459
	step [2/244], loss=102.5371
	step [3/244], loss=90.3768
	step [4/244], loss=92.9772
	step [5/244], loss=108.0506
	step [6/244], loss=84.2581
	step [7/244], loss=90.2034
	step [8/244], loss=88.2063
	step [9/244], loss=98.5161
	step [10/244], loss=95.4738
	step [11/244], loss=97.9766
	step [12/244], loss=94.2450
	step [13/244], loss=95.0952
	step [14/244], loss=120.0026
	step [15/244], loss=97.7823
	step [16/244], loss=106.6737
	step [17/244], loss=96.2737
	step [18/244], loss=91.5486
	step [19/244], loss=92.9097
	step [20/244], loss=86.6385
	step [21/244], loss=97.5869
	step [22/244], loss=106.4489
	step [23/244], loss=89.8712
	step [24/244], loss=115.9415
	step [25/244], loss=98.2891
	step [26/244], loss=78.8833
	step [27/244], loss=110.4170
	step [28/244], loss=101.8443
	step [29/244], loss=76.4589
	step [30/244], loss=80.2840
	step [31/244], loss=109.9715
	step [32/244], loss=115.6437
	step [33/244], loss=99.0562
	step [34/244], loss=89.1389
	step [35/244], loss=94.6348
	step [36/244], loss=101.6115
	step [37/244], loss=97.6982
	step [38/244], loss=91.0789
	step [39/244], loss=95.2550
	step [40/244], loss=94.1567
	step [41/244], loss=99.3574
	step [42/244], loss=98.4464
	step [43/244], loss=101.2116
	step [44/244], loss=88.9788
	step [45/244], loss=84.0545
	step [46/244], loss=81.2767
	step [47/244], loss=94.7437
	step [48/244], loss=79.5319
	step [49/244], loss=87.0976
	step [50/244], loss=93.0595
	step [51/244], loss=97.1010
	step [52/244], loss=90.6601
	step [53/244], loss=103.4117
	step [54/244], loss=80.7850
	step [55/244], loss=86.0584
	step [56/244], loss=96.3565
	step [57/244], loss=105.3399
	step [58/244], loss=94.4225
	step [59/244], loss=97.7565
	step [60/244], loss=108.0353
	step [61/244], loss=86.6185
	step [62/244], loss=92.3440
	step [63/244], loss=70.1955
	step [64/244], loss=83.4500
	step [65/244], loss=97.2157
	step [66/244], loss=105.4845
	step [67/244], loss=93.6597
	step [68/244], loss=114.7824
	step [69/244], loss=116.4330
	step [70/244], loss=87.9043
	step [71/244], loss=94.8870
	step [72/244], loss=79.8425
	step [73/244], loss=95.3155
	step [74/244], loss=95.0645
	step [75/244], loss=97.5820
	step [76/244], loss=110.8279
	step [77/244], loss=95.7360
	step [78/244], loss=82.6868
	step [79/244], loss=103.9661
	step [80/244], loss=106.4614
	step [81/244], loss=91.5555
	step [82/244], loss=105.7777
	step [83/244], loss=102.3339
	step [84/244], loss=103.8832
	step [85/244], loss=95.0986
	step [86/244], loss=90.9714
	step [87/244], loss=101.7849
	step [88/244], loss=83.8455
	step [89/244], loss=94.5215
	step [90/244], loss=117.2160
	step [91/244], loss=100.8998
	step [92/244], loss=97.0254
	step [93/244], loss=92.8671
	step [94/244], loss=89.0078
	step [95/244], loss=88.6195
	step [96/244], loss=89.4975
	step [97/244], loss=84.0928
	step [98/244], loss=102.0380
	step [99/244], loss=114.6705
	step [100/244], loss=106.4650
	step [101/244], loss=107.7427
	step [102/244], loss=92.1360
	step [103/244], loss=88.8841
	step [104/244], loss=87.2414
	step [105/244], loss=93.9365
	step [106/244], loss=114.9709
	step [107/244], loss=80.3664
	step [108/244], loss=121.1280
	step [109/244], loss=93.4335
	step [110/244], loss=92.2319
	step [111/244], loss=117.9820
	step [112/244], loss=95.0426
	step [113/244], loss=108.3235
	step [114/244], loss=95.2560
	step [115/244], loss=95.5104
	step [116/244], loss=96.5292
	step [117/244], loss=102.1042
	step [118/244], loss=84.2615
	step [119/244], loss=94.9406
	step [120/244], loss=108.0594
	step [121/244], loss=100.1539
	step [122/244], loss=84.7607
	step [123/244], loss=100.3175
	step [124/244], loss=92.6210
	step [125/244], loss=84.7663
	step [126/244], loss=94.3966
	step [127/244], loss=89.2720
	step [128/244], loss=102.0870
	step [129/244], loss=101.9870
	step [130/244], loss=101.7415
	step [131/244], loss=98.6075
	step [132/244], loss=98.6130
	step [133/244], loss=80.9452
	step [134/244], loss=93.8214
	step [135/244], loss=114.9945
	step [136/244], loss=90.3133
	step [137/244], loss=97.3369
	step [138/244], loss=79.3524
	step [139/244], loss=96.6996
	step [140/244], loss=110.5704
	step [141/244], loss=89.5326
	step [142/244], loss=102.5822
	step [143/244], loss=72.0238
	step [144/244], loss=91.6709
	step [145/244], loss=94.5912
	step [146/244], loss=95.9813
	step [147/244], loss=103.9961
	step [148/244], loss=104.3219
	step [149/244], loss=99.2306
	step [150/244], loss=100.2996
	step [151/244], loss=88.2104
	step [152/244], loss=112.4343
	step [153/244], loss=83.3753
	step [154/244], loss=99.3792
	step [155/244], loss=85.2552
	step [156/244], loss=91.8304
	step [157/244], loss=100.1047
	step [158/244], loss=98.2850
	step [159/244], loss=93.9567
	step [160/244], loss=83.7192
	step [161/244], loss=88.2307
	step [162/244], loss=84.9319
	step [163/244], loss=105.7482
	step [164/244], loss=85.9908
	step [165/244], loss=96.2025
	step [166/244], loss=97.1035
	step [167/244], loss=100.9596
	step [168/244], loss=104.8761
	step [169/244], loss=113.5063
	step [170/244], loss=96.1568
	step [171/244], loss=87.9206
	step [172/244], loss=93.7673
	step [173/244], loss=98.7934
	step [174/244], loss=102.0115
	step [175/244], loss=76.4386
	step [176/244], loss=88.9744
	step [177/244], loss=77.3707
	step [178/244], loss=101.7112
	step [179/244], loss=95.2206
	step [180/244], loss=106.5018
	step [181/244], loss=88.9248
	step [182/244], loss=86.9201
	step [183/244], loss=103.0996
	step [184/244], loss=82.8447
	step [185/244], loss=92.8297
	step [186/244], loss=92.9635
	step [187/244], loss=111.5333
	step [188/244], loss=97.7125
	step [189/244], loss=80.1679
	step [190/244], loss=83.8375
	step [191/244], loss=88.0356
	step [192/244], loss=128.3920
	step [193/244], loss=98.6313
	step [194/244], loss=97.4790
	step [195/244], loss=98.3017
	step [196/244], loss=104.6937
	step [197/244], loss=103.7023
	step [198/244], loss=103.1016
	step [199/244], loss=100.1632
	step [200/244], loss=82.3545
	step [201/244], loss=78.1335
	step [202/244], loss=78.7391
	step [203/244], loss=90.3838
	step [204/244], loss=118.4686
	step [205/244], loss=86.9912
	step [206/244], loss=82.3979
	step [207/244], loss=94.9519
	step [208/244], loss=108.9761
	step [209/244], loss=98.8791
	step [210/244], loss=88.1291
	step [211/244], loss=84.9512
	step [212/244], loss=87.8703
	step [213/244], loss=93.5516
	step [214/244], loss=105.4026
	step [215/244], loss=83.7389
	step [216/244], loss=82.3554
	step [217/244], loss=92.3895
	step [218/244], loss=106.1386
	step [219/244], loss=103.3806
	step [220/244], loss=93.7247
	step [221/244], loss=74.6917
	step [222/244], loss=103.4320
	step [223/244], loss=96.4908
	step [224/244], loss=80.8323
	step [225/244], loss=105.8007
	step [226/244], loss=66.7976
	step [227/244], loss=98.7271
	step [228/244], loss=103.2069
	step [229/244], loss=94.0853
	step [230/244], loss=116.0499
	step [231/244], loss=82.6089
	step [232/244], loss=95.0215
	step [233/244], loss=97.0069
	step [234/244], loss=91.7491
	step [235/244], loss=90.1907
	step [236/244], loss=97.7021
	step [237/244], loss=83.2251
	step [238/244], loss=78.8436
	step [239/244], loss=111.7153
	step [240/244], loss=76.9810
	step [241/244], loss=103.9579
	step [242/244], loss=75.5427
	step [243/244], loss=100.3323
	step [244/244], loss=35.7063
	Evaluating
	loss=0.0136, precision=0.3838, recall=0.9140, f1=0.5406
Training epoch 17
	step [1/244], loss=94.5433
	step [2/244], loss=88.4525
	step [3/244], loss=94.6749
	step [4/244], loss=91.8743
	step [5/244], loss=82.0476
	step [6/244], loss=104.3778
	step [7/244], loss=86.9106
	step [8/244], loss=103.4516
	step [9/244], loss=101.4660
	step [10/244], loss=110.3771
	step [11/244], loss=95.9794
	step [12/244], loss=100.5570
	step [13/244], loss=83.1806
	step [14/244], loss=83.6961
	step [15/244], loss=94.5953
	step [16/244], loss=84.4967
	step [17/244], loss=95.4374
	step [18/244], loss=112.6516
	step [19/244], loss=92.1887
	step [20/244], loss=100.0208
	step [21/244], loss=99.0950
	step [22/244], loss=103.8115
	step [23/244], loss=84.6686
	step [24/244], loss=114.3030
	step [25/244], loss=80.9288
	step [26/244], loss=109.8692
	step [27/244], loss=90.0281
	step [28/244], loss=79.8613
	step [29/244], loss=87.0963
	step [30/244], loss=81.6049
	step [31/244], loss=95.7334
	step [32/244], loss=89.3973
	step [33/244], loss=88.4918
	step [34/244], loss=79.0509
	step [35/244], loss=89.0005
	step [36/244], loss=83.0428
	step [37/244], loss=81.3581
	step [38/244], loss=94.0363
	step [39/244], loss=84.7055
	step [40/244], loss=102.9713
	step [41/244], loss=94.8864
	step [42/244], loss=103.1793
	step [43/244], loss=81.2244
	step [44/244], loss=82.7262
	step [45/244], loss=87.5230
	step [46/244], loss=84.5832
	step [47/244], loss=82.7464
	step [48/244], loss=92.8360
	step [49/244], loss=79.4013
	step [50/244], loss=110.4339
	step [51/244], loss=84.0456
	step [52/244], loss=103.1702
	step [53/244], loss=86.7893
	step [54/244], loss=105.1194
	step [55/244], loss=90.3261
	step [56/244], loss=81.7876
	step [57/244], loss=93.4694
	step [58/244], loss=86.7193
	step [59/244], loss=76.5492
	step [60/244], loss=129.6931
	step [61/244], loss=92.1442
	step [62/244], loss=98.7499
	step [63/244], loss=100.5939
	step [64/244], loss=86.9736
	step [65/244], loss=98.8405
	step [66/244], loss=93.3613
	step [67/244], loss=89.3967
	step [68/244], loss=90.3397
	step [69/244], loss=96.3843
	step [70/244], loss=82.7309
	step [71/244], loss=96.7857
	step [72/244], loss=111.7444
	step [73/244], loss=81.6709
	step [74/244], loss=81.0357
	step [75/244], loss=97.3867
	step [76/244], loss=77.2474
	step [77/244], loss=97.6009
	step [78/244], loss=99.1659
	step [79/244], loss=90.7548
	step [80/244], loss=87.4609
	step [81/244], loss=79.0189
	step [82/244], loss=100.4754
	step [83/244], loss=111.3390
	step [84/244], loss=112.4170
	step [85/244], loss=104.5801
	step [86/244], loss=86.3108
	step [87/244], loss=104.5023
	step [88/244], loss=86.4572
	step [89/244], loss=81.2715
	step [90/244], loss=85.8486
	step [91/244], loss=106.5004
	step [92/244], loss=96.8326
	step [93/244], loss=97.5333
	step [94/244], loss=76.0817
	step [95/244], loss=91.1914
	step [96/244], loss=92.2445
	step [97/244], loss=91.7329
	step [98/244], loss=87.4868
	step [99/244], loss=100.9703
	step [100/244], loss=106.0256
	step [101/244], loss=86.5486
	step [102/244], loss=96.4239
	step [103/244], loss=94.2632
	step [104/244], loss=72.5489
	step [105/244], loss=118.7433
	step [106/244], loss=106.2390
	step [107/244], loss=83.8574
	step [108/244], loss=91.1117
	step [109/244], loss=92.9728
	step [110/244], loss=93.3409
	step [111/244], loss=105.6826
	step [112/244], loss=87.0129
	step [113/244], loss=107.9241
	step [114/244], loss=82.2847
	step [115/244], loss=108.9945
	step [116/244], loss=73.4402
	step [117/244], loss=102.7391
	step [118/244], loss=103.4181
	step [119/244], loss=80.8377
	step [120/244], loss=107.3698
	step [121/244], loss=84.6078
	step [122/244], loss=94.7408
	step [123/244], loss=83.7789
	step [124/244], loss=104.7217
	step [125/244], loss=80.1832
	step [126/244], loss=81.3014
	step [127/244], loss=94.7337
	step [128/244], loss=87.6982
	step [129/244], loss=93.0518
	step [130/244], loss=98.2722
	step [131/244], loss=82.5069
	step [132/244], loss=80.1275
	step [133/244], loss=86.4296
	step [134/244], loss=92.6676
	step [135/244], loss=78.2952
	step [136/244], loss=97.2064
	step [137/244], loss=116.3431
	step [138/244], loss=77.3186
	step [139/244], loss=83.0414
	step [140/244], loss=93.2749
	step [141/244], loss=92.8381
	step [142/244], loss=102.5204
	step [143/244], loss=85.1932
	step [144/244], loss=88.1289
	step [145/244], loss=99.0404
	step [146/244], loss=97.8421
	step [147/244], loss=76.3462
	step [148/244], loss=110.1633
	step [149/244], loss=94.1682
	step [150/244], loss=96.3869
	step [151/244], loss=91.3075
	step [152/244], loss=89.8913
	step [153/244], loss=84.6483
	step [154/244], loss=108.6108
	step [155/244], loss=87.9783
	step [156/244], loss=87.4941
	step [157/244], loss=90.7095
	step [158/244], loss=100.1749
	step [159/244], loss=101.5932
	step [160/244], loss=108.5280
	step [161/244], loss=89.5371
	step [162/244], loss=119.3262
	step [163/244], loss=99.3991
	step [164/244], loss=106.9308
	step [165/244], loss=112.0074
	step [166/244], loss=98.3325
	step [167/244], loss=107.9470
	step [168/244], loss=108.1775
	step [169/244], loss=92.7087
	step [170/244], loss=104.6851
	step [171/244], loss=100.6852
	step [172/244], loss=85.0163
	step [173/244], loss=92.9572
	step [174/244], loss=101.4578
	step [175/244], loss=75.7813
	step [176/244], loss=99.3274
	step [177/244], loss=92.6281
	step [178/244], loss=103.3897
	step [179/244], loss=84.6214
	step [180/244], loss=99.2374
	step [181/244], loss=96.7534
	step [182/244], loss=76.1334
	step [183/244], loss=103.6742
	step [184/244], loss=90.9267
	step [185/244], loss=96.6266
	step [186/244], loss=102.2146
	step [187/244], loss=100.4657
	step [188/244], loss=97.8079
	step [189/244], loss=78.5335
	step [190/244], loss=99.5139
	step [191/244], loss=84.1983
	step [192/244], loss=112.2485
	step [193/244], loss=102.2378
	step [194/244], loss=108.7551
	step [195/244], loss=116.7618
	step [196/244], loss=93.5209
	step [197/244], loss=99.1438
	step [198/244], loss=92.4086
	step [199/244], loss=85.4121
	step [200/244], loss=78.5844
	step [201/244], loss=113.6592
	step [202/244], loss=95.8012
	step [203/244], loss=96.4991
	step [204/244], loss=86.2440
	step [205/244], loss=100.5918
	step [206/244], loss=90.4658
	step [207/244], loss=96.4296
	step [208/244], loss=80.5670
	step [209/244], loss=77.7453
	step [210/244], loss=75.5039
	step [211/244], loss=83.7194
	step [212/244], loss=102.0617
	step [213/244], loss=109.5536
	step [214/244], loss=105.1934
	step [215/244], loss=105.1000
	step [216/244], loss=98.0424
	step [217/244], loss=96.5813
	step [218/244], loss=96.9118
	step [219/244], loss=99.3682
	step [220/244], loss=95.9417
	step [221/244], loss=78.3467
	step [222/244], loss=95.0509
	step [223/244], loss=92.2487
	step [224/244], loss=87.3185
	step [225/244], loss=106.7668
	step [226/244], loss=110.1206
	step [227/244], loss=120.4010
	step [228/244], loss=118.8204
	step [229/244], loss=82.4952
	step [230/244], loss=98.6449
	step [231/244], loss=92.4472
	step [232/244], loss=91.2498
	step [233/244], loss=81.4492
	step [234/244], loss=92.7577
	step [235/244], loss=78.4249
	step [236/244], loss=90.5366
	step [237/244], loss=86.7736
	step [238/244], loss=114.0294
	step [239/244], loss=91.4336
	step [240/244], loss=76.7848
	step [241/244], loss=86.0022
	step [242/244], loss=109.5148
	step [243/244], loss=92.2959
	step [244/244], loss=45.7848
	Evaluating
	loss=0.0131, precision=0.4033, recall=0.9042, f1=0.5578
Training epoch 18
	step [1/244], loss=90.3207
	step [2/244], loss=79.0144
	step [3/244], loss=102.9477
	step [4/244], loss=81.1404
	step [5/244], loss=82.0815
	step [6/244], loss=92.1108
	step [7/244], loss=109.9526
	step [8/244], loss=117.7089
	step [9/244], loss=117.9056
	step [10/244], loss=71.4645
	step [11/244], loss=77.7201
	step [12/244], loss=93.3168
	step [13/244], loss=92.7846
	step [14/244], loss=108.5993
	step [15/244], loss=86.2639
	step [16/244], loss=98.6439
	step [17/244], loss=97.8727
	step [18/244], loss=92.7756
	step [19/244], loss=81.6143
	step [20/244], loss=103.6836
	step [21/244], loss=102.4149
	step [22/244], loss=99.6279
	step [23/244], loss=85.9177
	step [24/244], loss=99.0879
	step [25/244], loss=88.3278
	step [26/244], loss=107.8855
	step [27/244], loss=81.6087
	step [28/244], loss=70.6489
	step [29/244], loss=107.5920
	step [30/244], loss=88.7783
	step [31/244], loss=83.3807
	step [32/244], loss=94.4877
	step [33/244], loss=111.5178
	step [34/244], loss=102.7170
	step [35/244], loss=102.9886
	step [36/244], loss=93.7169
	step [37/244], loss=92.4296
	step [38/244], loss=93.8853
	step [39/244], loss=103.5432
	step [40/244], loss=97.0736
	step [41/244], loss=90.3970
	step [42/244], loss=92.1227
	step [43/244], loss=88.8637
	step [44/244], loss=98.8952
	step [45/244], loss=99.5435
	step [46/244], loss=94.0543
	step [47/244], loss=100.5084
	step [48/244], loss=86.4038
	step [49/244], loss=106.3241
	step [50/244], loss=96.6010
	step [51/244], loss=77.7135
	step [52/244], loss=89.2960
	step [53/244], loss=71.7561
	step [54/244], loss=91.7130
	step [55/244], loss=79.5040
	step [56/244], loss=99.4170
	step [57/244], loss=104.0776
	step [58/244], loss=88.0823
	step [59/244], loss=82.9806
	step [60/244], loss=86.3506
	step [61/244], loss=92.3199
	step [62/244], loss=94.4509
	step [63/244], loss=84.0235
	step [64/244], loss=93.6058
	step [65/244], loss=98.9692
	step [66/244], loss=96.4368
	step [67/244], loss=79.0792
	step [68/244], loss=86.4874
	step [69/244], loss=94.8394
	step [70/244], loss=99.1254
	step [71/244], loss=90.3230
	step [72/244], loss=94.8376
	step [73/244], loss=104.0986
	step [74/244], loss=101.8828
	step [75/244], loss=83.9818
	step [76/244], loss=105.7646
	step [77/244], loss=86.0439
	step [78/244], loss=80.1277
	step [79/244], loss=73.3760
	step [80/244], loss=75.7373
	step [81/244], loss=107.7120
	step [82/244], loss=80.5789
	step [83/244], loss=87.9524
	step [84/244], loss=100.1881
	step [85/244], loss=74.2927
	step [86/244], loss=93.6778
	step [87/244], loss=82.0958
	step [88/244], loss=101.9308
	step [89/244], loss=96.1623
	step [90/244], loss=87.3203
	step [91/244], loss=92.3864
	step [92/244], loss=85.3778
	step [93/244], loss=91.2173
	step [94/244], loss=89.1273
	step [95/244], loss=83.4995
	step [96/244], loss=111.8128
	step [97/244], loss=94.3072
	step [98/244], loss=76.2863
	step [99/244], loss=107.2888
	step [100/244], loss=92.2646
	step [101/244], loss=93.7468
	step [102/244], loss=97.9219
	step [103/244], loss=86.4619
	step [104/244], loss=90.5964
	step [105/244], loss=104.4955
	step [106/244], loss=91.6243
	step [107/244], loss=95.7258
	step [108/244], loss=100.9618
	step [109/244], loss=99.5651
	step [110/244], loss=81.0439
	step [111/244], loss=86.3304
	step [112/244], loss=80.4289
	step [113/244], loss=88.7603
	step [114/244], loss=120.7016
	step [115/244], loss=103.6711
	step [116/244], loss=69.9500
	step [117/244], loss=77.0625
	step [118/244], loss=108.2318
	step [119/244], loss=86.6285
	step [120/244], loss=82.9286
	step [121/244], loss=99.3387
	step [122/244], loss=117.4995
	step [123/244], loss=91.1237
	step [124/244], loss=111.6303
	step [125/244], loss=77.3237
	step [126/244], loss=100.4606
	step [127/244], loss=87.2536
	step [128/244], loss=82.8515
	step [129/244], loss=87.9950
	step [130/244], loss=109.7665
	step [131/244], loss=75.9909
	step [132/244], loss=74.7693
	step [133/244], loss=93.0356
	step [134/244], loss=92.2145
	step [135/244], loss=93.0131
	step [136/244], loss=81.1573
	step [137/244], loss=85.6204
	step [138/244], loss=109.8857
	step [139/244], loss=92.8949
	step [140/244], loss=100.3346
	step [141/244], loss=106.2365
	step [142/244], loss=86.4102
	step [143/244], loss=114.5462
	step [144/244], loss=112.8091
	step [145/244], loss=119.6119
	step [146/244], loss=99.5339
	step [147/244], loss=109.7830
	step [148/244], loss=101.2970
	step [149/244], loss=99.2855
	step [150/244], loss=82.1355
	step [151/244], loss=83.4293
	step [152/244], loss=90.6827
	step [153/244], loss=99.1394
	step [154/244], loss=79.2528
	step [155/244], loss=99.4500
	step [156/244], loss=110.7101
	step [157/244], loss=117.0968
	step [158/244], loss=101.0604
	step [159/244], loss=116.1673
	step [160/244], loss=98.1030
	step [161/244], loss=92.8044
	step [162/244], loss=106.1770
	step [163/244], loss=77.9073
	step [164/244], loss=86.1585
	step [165/244], loss=99.6077
	step [166/244], loss=118.4608
	step [167/244], loss=99.3896
	step [168/244], loss=110.8524
	step [169/244], loss=97.0344
	step [170/244], loss=76.4677
	step [171/244], loss=76.3341
	step [172/244], loss=103.9091
	step [173/244], loss=111.8494
	step [174/244], loss=102.1781
	step [175/244], loss=101.7811
	step [176/244], loss=107.3004
	step [177/244], loss=95.3414
	step [178/244], loss=88.4466
	step [179/244], loss=96.3081
	step [180/244], loss=95.5513
	step [181/244], loss=85.0815
	step [182/244], loss=83.3477
	step [183/244], loss=91.3131
	step [184/244], loss=86.2988
	step [185/244], loss=96.7553
	step [186/244], loss=95.8256
	step [187/244], loss=76.8745
	step [188/244], loss=87.8591
	step [189/244], loss=68.3602
	step [190/244], loss=82.3447
	step [191/244], loss=114.8076
	step [192/244], loss=92.2023
	step [193/244], loss=85.2865
	step [194/244], loss=94.2226
	step [195/244], loss=96.9832
	step [196/244], loss=80.3762
	step [197/244], loss=79.4680
	step [198/244], loss=86.2283
	step [199/244], loss=93.0309
	step [200/244], loss=81.8945
	step [201/244], loss=85.7854
	step [202/244], loss=86.0921
	step [203/244], loss=84.8892
	step [204/244], loss=91.9215
	step [205/244], loss=109.4474
	step [206/244], loss=100.5224
	step [207/244], loss=79.9024
	step [208/244], loss=102.2115
	step [209/244], loss=127.8736
	step [210/244], loss=111.4517
	step [211/244], loss=87.5898
	step [212/244], loss=97.0501
	step [213/244], loss=80.6352
	step [214/244], loss=93.7221
	step [215/244], loss=88.8900
	step [216/244], loss=61.5980
	step [217/244], loss=87.6339
	step [218/244], loss=81.0050
	step [219/244], loss=86.3658
	step [220/244], loss=85.5201
	step [221/244], loss=105.1771
	step [222/244], loss=103.7226
	step [223/244], loss=76.1220
	step [224/244], loss=101.6681
	step [225/244], loss=81.2000
	step [226/244], loss=93.3188
	step [227/244], loss=101.4423
	step [228/244], loss=88.3779
	step [229/244], loss=90.7929
	step [230/244], loss=79.4479
	step [231/244], loss=82.3291
	step [232/244], loss=95.5721
	step [233/244], loss=88.2161
	step [234/244], loss=95.6544
	step [235/244], loss=71.9642
	step [236/244], loss=108.7704
	step [237/244], loss=91.1676
	step [238/244], loss=97.3604
	step [239/244], loss=100.1709
	step [240/244], loss=101.9090
	step [241/244], loss=77.8813
	step [242/244], loss=87.0355
	step [243/244], loss=80.4839
	step [244/244], loss=44.5060
	Evaluating
	loss=0.0109, precision=0.4562, recall=0.8792, f1=0.6007
Training epoch 19
	step [1/244], loss=106.8358
	step [2/244], loss=92.1336
	step [3/244], loss=91.2455
	step [4/244], loss=87.7263
	step [5/244], loss=85.0589
	step [6/244], loss=93.5491
	step [7/244], loss=108.5667
	step [8/244], loss=75.0832
	step [9/244], loss=86.4014
	step [10/244], loss=91.4893
	step [11/244], loss=110.2281
	step [12/244], loss=92.9342
	step [13/244], loss=92.0081
	step [14/244], loss=89.9121
	step [15/244], loss=125.9015
	step [16/244], loss=96.8873
	step [17/244], loss=75.1463
	step [18/244], loss=92.2841
	step [19/244], loss=91.4447
	step [20/244], loss=90.4974
	step [21/244], loss=96.7296
	step [22/244], loss=108.1041
	step [23/244], loss=98.5902
	step [24/244], loss=83.1906
	step [25/244], loss=99.2012
	step [26/244], loss=80.3659
	step [27/244], loss=113.9727
	step [28/244], loss=101.1779
	step [29/244], loss=73.8813
	step [30/244], loss=91.4351
	step [31/244], loss=88.8587
	step [32/244], loss=97.2188
	step [33/244], loss=89.2812
	step [34/244], loss=104.1738
	step [35/244], loss=90.4470
	step [36/244], loss=83.8475
	step [37/244], loss=77.4332
	step [38/244], loss=91.5690
	step [39/244], loss=111.3597
	step [40/244], loss=96.4134
	step [41/244], loss=105.3057
	step [42/244], loss=82.8913
	step [43/244], loss=104.4135
	step [44/244], loss=97.7661
	step [45/244], loss=95.2943
	step [46/244], loss=81.2088
	step [47/244], loss=110.9128
	step [48/244], loss=101.3818
	step [49/244], loss=89.0747
	step [50/244], loss=90.7084
	step [51/244], loss=99.6652
	step [52/244], loss=103.7015
	step [53/244], loss=84.3189
	step [54/244], loss=93.4316
	step [55/244], loss=106.2853
	step [56/244], loss=94.6216
	step [57/244], loss=101.2364
	step [58/244], loss=93.0751
	step [59/244], loss=87.1476
	step [60/244], loss=83.4307
	step [61/244], loss=82.8749
	step [62/244], loss=108.1376
	step [63/244], loss=105.8497
	step [64/244], loss=104.0290
	step [65/244], loss=92.4269
	step [66/244], loss=95.8783
	step [67/244], loss=84.3337
	step [68/244], loss=95.0455
	step [69/244], loss=75.1952
	step [70/244], loss=106.0043
	step [71/244], loss=90.1035
	step [72/244], loss=89.6456
	step [73/244], loss=96.3010
	step [74/244], loss=109.5761
	step [75/244], loss=92.8423
	step [76/244], loss=97.9928
	step [77/244], loss=106.9862
	step [78/244], loss=94.5774
	step [79/244], loss=92.5820
	step [80/244], loss=90.4167
	step [81/244], loss=90.4500
	step [82/244], loss=82.8865
	step [83/244], loss=77.2224
	step [84/244], loss=98.4573
	step [85/244], loss=103.3889
	step [86/244], loss=84.7114
	step [87/244], loss=98.9262
	step [88/244], loss=95.3878
	step [89/244], loss=88.5893
	step [90/244], loss=86.2585
	step [91/244], loss=77.6195
	step [92/244], loss=86.8691
	step [93/244], loss=99.4379
	step [94/244], loss=85.6075
	step [95/244], loss=91.8439
	step [96/244], loss=75.4379
	step [97/244], loss=89.6193
	step [98/244], loss=95.8781
	step [99/244], loss=89.5975
	step [100/244], loss=107.5900
	step [101/244], loss=92.2403
	step [102/244], loss=97.1766
	step [103/244], loss=91.7320
	step [104/244], loss=74.2212
	step [105/244], loss=96.0924
	step [106/244], loss=92.3544
	step [107/244], loss=84.1657
	step [108/244], loss=94.6855
	step [109/244], loss=90.1792
	step [110/244], loss=87.6049
	step [111/244], loss=88.5107
	step [112/244], loss=86.5610
	step [113/244], loss=101.2465
	step [114/244], loss=101.3182
	step [115/244], loss=83.9539
	step [116/244], loss=107.5556
	step [117/244], loss=110.1456
	step [118/244], loss=88.9154
	step [119/244], loss=88.4448
	step [120/244], loss=88.6676
	step [121/244], loss=96.4618
	step [122/244], loss=96.3866
	step [123/244], loss=91.7209
	step [124/244], loss=89.4172
	step [125/244], loss=118.9922
	step [126/244], loss=110.5609
	step [127/244], loss=79.3953
	step [128/244], loss=76.6896
	step [129/244], loss=87.1122
	step [130/244], loss=109.8982
	step [131/244], loss=82.0961
	step [132/244], loss=80.8631
	step [133/244], loss=84.5300
	step [134/244], loss=89.9048
	step [135/244], loss=101.2357
	step [136/244], loss=94.8438
	step [137/244], loss=77.7816
	step [138/244], loss=90.4334
	step [139/244], loss=77.7633
	step [140/244], loss=81.7278
	step [141/244], loss=88.8825
	step [142/244], loss=79.9593
	step [143/244], loss=93.3233
	step [144/244], loss=91.6693
	step [145/244], loss=100.7810
	step [146/244], loss=75.9963
	step [147/244], loss=79.3992
	step [148/244], loss=85.7236
	step [149/244], loss=99.8939
	step [150/244], loss=84.8087
	step [151/244], loss=85.5210
	step [152/244], loss=96.1836
	step [153/244], loss=80.0505
	step [154/244], loss=87.6512
	step [155/244], loss=78.2472
	step [156/244], loss=92.8127
	step [157/244], loss=90.8327
	step [158/244], loss=96.8750
	step [159/244], loss=83.0771
	step [160/244], loss=96.9981
	step [161/244], loss=92.2604
	step [162/244], loss=72.5667
	step [163/244], loss=78.5529
	step [164/244], loss=88.7936
	step [165/244], loss=85.6743
	step [166/244], loss=97.4887
	step [167/244], loss=108.5569
	step [168/244], loss=114.1426
	step [169/244], loss=91.1965
	step [170/244], loss=84.8067
	step [171/244], loss=94.6267
	step [172/244], loss=89.9920
	step [173/244], loss=96.1208
	step [174/244], loss=100.4300
	step [175/244], loss=80.5022
	step [176/244], loss=109.8441
	step [177/244], loss=73.5685
	step [178/244], loss=107.1488
	step [179/244], loss=78.5129
	step [180/244], loss=94.5833
	step [181/244], loss=79.6428
	step [182/244], loss=93.8509
	step [183/244], loss=116.8764
	step [184/244], loss=87.9205
	step [185/244], loss=86.1518
	step [186/244], loss=96.3453
	step [187/244], loss=95.3852
	step [188/244], loss=83.7956
	step [189/244], loss=91.4271
	step [190/244], loss=90.9431
	step [191/244], loss=90.9507
	step [192/244], loss=93.7154
	step [193/244], loss=98.7989
	step [194/244], loss=77.0351
	step [195/244], loss=93.9313
	step [196/244], loss=105.3793
	step [197/244], loss=76.1744
	step [198/244], loss=93.9266
	step [199/244], loss=92.7262
	step [200/244], loss=102.8770
	step [201/244], loss=90.1037
	step [202/244], loss=88.4408
	step [203/244], loss=110.6658
	step [204/244], loss=90.2777
	step [205/244], loss=80.4581
	step [206/244], loss=97.7324
	step [207/244], loss=92.2853
	step [208/244], loss=91.1310
	step [209/244], loss=101.4962
	step [210/244], loss=86.9789
	step [211/244], loss=80.7366
	step [212/244], loss=100.2052
	step [213/244], loss=76.6667
	step [214/244], loss=83.0541
	step [215/244], loss=78.4420
	step [216/244], loss=86.8850
	step [217/244], loss=93.4845
	step [218/244], loss=83.1225
	step [219/244], loss=84.4482
	step [220/244], loss=81.5523
	step [221/244], loss=82.7397
	step [222/244], loss=106.2932
	step [223/244], loss=97.8397
	step [224/244], loss=80.2890
	step [225/244], loss=91.3568
	step [226/244], loss=89.4427
	step [227/244], loss=93.9826
	step [228/244], loss=90.1169
	step [229/244], loss=87.3140
	step [230/244], loss=82.8516
	step [231/244], loss=106.2358
	step [232/244], loss=96.5902
	step [233/244], loss=104.8692
	step [234/244], loss=74.1017
	step [235/244], loss=120.5966
	step [236/244], loss=106.0574
	step [237/244], loss=97.7214
	step [238/244], loss=83.9560
	step [239/244], loss=103.5977
	step [240/244], loss=90.5358
	step [241/244], loss=107.5866
	step [242/244], loss=93.7104
	step [243/244], loss=103.3899
	step [244/244], loss=31.0190
	Evaluating
	loss=0.0112, precision=0.3920, recall=0.8778, f1=0.5420
Training epoch 20
