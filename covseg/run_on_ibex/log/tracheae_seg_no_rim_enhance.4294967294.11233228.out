Loading anaconda...
...Anaconda env loaded
directing: Z rim_enhanced: True test_id 0
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12281 # image files with weight 12281
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3263 # image files with weight 3263
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12281
Using 4 GPUs
Going to train epochs [101-150]
Training epoch 101
	step [1/192], loss=62.3950
	step [2/192], loss=68.1407
	step [3/192], loss=69.2477
	step [4/192], loss=75.8436
	step [5/192], loss=67.1675
	step [6/192], loss=68.7792
	step [7/192], loss=66.9130
	step [8/192], loss=64.2013
	step [9/192], loss=68.4159
	step [10/192], loss=63.7596
	step [11/192], loss=85.6262
	step [12/192], loss=70.9810
	step [13/192], loss=71.0603
	step [14/192], loss=75.3956
	step [15/192], loss=55.7365
	step [16/192], loss=74.8778
	step [17/192], loss=58.8479
	step [18/192], loss=70.9872
	step [19/192], loss=61.4841
	step [20/192], loss=65.9790
	step [21/192], loss=65.7641
	step [22/192], loss=58.1426
	step [23/192], loss=63.3680
	step [24/192], loss=63.5313
	step [25/192], loss=57.4120
	step [26/192], loss=70.9426
	step [27/192], loss=62.1751
	step [28/192], loss=55.7193
	step [29/192], loss=70.0833
	step [30/192], loss=73.5072
	step [31/192], loss=72.3271
	step [32/192], loss=63.7113
	step [33/192], loss=79.2921
	step [34/192], loss=72.4786
	step [35/192], loss=65.6685
	step [36/192], loss=81.1885
	step [37/192], loss=78.4247
	step [38/192], loss=61.7747
	step [39/192], loss=70.9891
	step [40/192], loss=64.8160
	step [41/192], loss=60.3102
	step [42/192], loss=51.7124
	step [43/192], loss=71.1954
	step [44/192], loss=61.1264
	step [45/192], loss=59.5857
	step [46/192], loss=75.3740
	step [47/192], loss=57.4202
	step [48/192], loss=55.8095
	step [49/192], loss=68.5257
	step [50/192], loss=59.1409
	step [51/192], loss=90.9960
	step [52/192], loss=72.6142
	step [53/192], loss=78.4853
	step [54/192], loss=57.6164
	step [55/192], loss=63.3246
	step [56/192], loss=85.4888
	step [57/192], loss=70.2688
	step [58/192], loss=65.0247
	step [59/192], loss=76.0679
	step [60/192], loss=73.3809
	step [61/192], loss=57.9171
	step [62/192], loss=55.9224
	step [63/192], loss=67.2667
	step [64/192], loss=67.3392
	step [65/192], loss=76.8136
	step [66/192], loss=57.7693
	step [67/192], loss=75.3706
	step [68/192], loss=77.4616
	step [69/192], loss=58.2344
	step [70/192], loss=67.2028
	step [71/192], loss=57.0199
	step [72/192], loss=63.7485
	step [73/192], loss=51.6354
	step [74/192], loss=72.2468
	step [75/192], loss=63.8083
	step [76/192], loss=70.1811
	step [77/192], loss=68.1084
	step [78/192], loss=73.0336
	step [79/192], loss=64.8036
	step [80/192], loss=71.1168
	step [81/192], loss=69.8607
	step [82/192], loss=54.0163
	step [83/192], loss=70.4276
	step [84/192], loss=77.9679
	step [85/192], loss=61.9581
	step [86/192], loss=73.4315
	step [87/192], loss=73.9172
	step [88/192], loss=68.5858
	step [89/192], loss=58.8646
	step [90/192], loss=78.7205
	step [91/192], loss=66.1073
	step [92/192], loss=71.6916
	step [93/192], loss=73.0500
	step [94/192], loss=78.7517
	step [95/192], loss=65.8935
	step [96/192], loss=69.1809
	step [97/192], loss=78.5891
	step [98/192], loss=68.9553
	step [99/192], loss=62.0442
	step [100/192], loss=84.9750
	step [101/192], loss=66.9298
	step [102/192], loss=78.1041
	step [103/192], loss=75.7956
	step [104/192], loss=65.4344
	step [105/192], loss=69.5000
	step [106/192], loss=69.7925
	step [107/192], loss=82.6959
	step [108/192], loss=71.4562
	step [109/192], loss=69.7439
	step [110/192], loss=59.1865
	step [111/192], loss=68.9181
	step [112/192], loss=57.5935
	step [113/192], loss=65.8842
	step [114/192], loss=72.4044
	step [115/192], loss=77.4764
	step [116/192], loss=58.7013
	step [117/192], loss=57.1993
	step [118/192], loss=70.5697
	step [119/192], loss=68.0210
	step [120/192], loss=67.9449
	step [121/192], loss=67.0204
	step [122/192], loss=65.0188
	step [123/192], loss=76.0999
	step [124/192], loss=80.5337
	step [125/192], loss=69.9133
	step [126/192], loss=76.2399
	step [127/192], loss=76.8025
	step [128/192], loss=68.3173
	step [129/192], loss=72.2953
	step [130/192], loss=70.4576
	step [131/192], loss=57.0759
	step [132/192], loss=66.2037
	step [133/192], loss=66.2834
	step [134/192], loss=63.4391
	step [135/192], loss=72.1992
	step [136/192], loss=72.1657
	step [137/192], loss=76.8813
	step [138/192], loss=79.2856
	step [139/192], loss=70.2324
	step [140/192], loss=77.8616
	step [141/192], loss=66.6344
	step [142/192], loss=73.0508
	step [143/192], loss=72.7224
	step [144/192], loss=71.2783
	step [145/192], loss=59.5716
	step [146/192], loss=59.2004
	step [147/192], loss=70.4763
	step [148/192], loss=75.6256
	step [149/192], loss=63.7360
	step [150/192], loss=59.8596
	step [151/192], loss=75.0395
	step [152/192], loss=58.2246
	step [153/192], loss=64.9431
	step [154/192], loss=61.6299
	step [155/192], loss=60.6052
	step [156/192], loss=69.0945
	step [157/192], loss=67.9629
	step [158/192], loss=64.2605
	step [159/192], loss=64.0867
	step [160/192], loss=66.0278
	step [161/192], loss=69.8622
	step [162/192], loss=77.1671
	step [163/192], loss=68.9691
	step [164/192], loss=66.4363
	step [165/192], loss=58.6079
	step [166/192], loss=75.8191
	step [167/192], loss=67.3352
	step [168/192], loss=60.7897
	step [169/192], loss=69.1627
	step [170/192], loss=67.4837
	step [171/192], loss=78.1166
	step [172/192], loss=65.6777
	step [173/192], loss=60.7254
	step [174/192], loss=80.8509
	step [175/192], loss=70.4077
	step [176/192], loss=72.6012
	step [177/192], loss=54.4386
	step [178/192], loss=83.6224
	step [179/192], loss=77.6264
	step [180/192], loss=74.1484
	step [181/192], loss=77.7496
	step [182/192], loss=65.7128
	step [183/192], loss=65.6969
	step [184/192], loss=68.9315
	step [185/192], loss=60.8626
	step [186/192], loss=62.1504
	step [187/192], loss=79.3957
	step [188/192], loss=67.8878
	step [189/192], loss=80.3805
	step [190/192], loss=63.8405
	step [191/192], loss=51.7145
	step [192/192], loss=60.3222
	Evaluating
	loss=0.0081, precision=0.3313, recall=0.8664, f1=0.4793
saving model as: 0_saved_model.pth
Training epoch 102
	step [1/192], loss=75.9706
	step [2/192], loss=68.1447
	step [3/192], loss=60.6367
	step [4/192], loss=59.6165
	step [5/192], loss=66.0436
	step [6/192], loss=53.4219
	step [7/192], loss=49.6547
	step [8/192], loss=67.5999
	step [9/192], loss=63.9964
	step [10/192], loss=55.4005
	step [11/192], loss=61.0707
	step [12/192], loss=72.4822
	step [13/192], loss=67.6899
	step [14/192], loss=82.4690
	step [15/192], loss=60.4980
	step [16/192], loss=67.7051
	step [17/192], loss=60.1723
	step [18/192], loss=73.5628
	step [19/192], loss=75.8283
	step [20/192], loss=71.2866
	step [21/192], loss=66.5641
	step [22/192], loss=62.5779
	step [23/192], loss=57.7561
	step [24/192], loss=78.8342
	step [25/192], loss=66.2964
	step [26/192], loss=63.9880
	step [27/192], loss=63.2139
	step [28/192], loss=67.6874
	step [29/192], loss=61.2134
	step [30/192], loss=76.6212
	step [31/192], loss=67.7895
	step [32/192], loss=69.9357
	step [33/192], loss=77.1659
	step [34/192], loss=72.3465
	step [35/192], loss=68.3287
	step [36/192], loss=67.6977
	step [37/192], loss=63.0320
	step [38/192], loss=72.3047
	step [39/192], loss=62.2447
	step [40/192], loss=55.7326
	step [41/192], loss=64.9472
	step [42/192], loss=80.4092
	step [43/192], loss=67.8973
	step [44/192], loss=80.7192
	step [45/192], loss=71.5045
	step [46/192], loss=72.3514
	step [47/192], loss=67.1631
	step [48/192], loss=69.6941
	step [49/192], loss=64.2996
	step [50/192], loss=69.8236
	step [51/192], loss=72.2735
	step [52/192], loss=61.0138
	step [53/192], loss=68.4451
	step [54/192], loss=68.9051
	step [55/192], loss=60.8429
	step [56/192], loss=75.3284
	step [57/192], loss=60.3136
	step [58/192], loss=65.8392
	step [59/192], loss=65.4947
	step [60/192], loss=72.1360
	step [61/192], loss=76.7677
	step [62/192], loss=78.9611
	step [63/192], loss=62.5643
	step [64/192], loss=70.5803
	step [65/192], loss=62.4282
	step [66/192], loss=66.4026
	step [67/192], loss=63.2930
	step [68/192], loss=69.4587
	step [69/192], loss=74.0230
	step [70/192], loss=77.7127
	step [71/192], loss=91.3253
	step [72/192], loss=79.3321
	step [73/192], loss=63.4180
	step [74/192], loss=73.2882
	step [75/192], loss=67.3772
	step [76/192], loss=70.3241
	step [77/192], loss=67.9750
	step [78/192], loss=56.9119
	step [79/192], loss=73.7099
	step [80/192], loss=62.9302
	step [81/192], loss=74.1473
	step [82/192], loss=66.6439
	step [83/192], loss=76.0378
	step [84/192], loss=68.4285
	step [85/192], loss=69.2301
	step [86/192], loss=58.5507
	step [87/192], loss=64.6814
	step [88/192], loss=81.6716
	step [89/192], loss=78.0756
	step [90/192], loss=62.9146
	step [91/192], loss=59.4454
	step [92/192], loss=71.3380
	step [93/192], loss=67.7886
	step [94/192], loss=74.3781
	step [95/192], loss=69.8637
	step [96/192], loss=70.8191
	step [97/192], loss=66.8324
	step [98/192], loss=70.2208
	step [99/192], loss=67.3857
	step [100/192], loss=70.4195
	step [101/192], loss=45.1622
	step [102/192], loss=63.6368
	step [103/192], loss=69.5021
	step [104/192], loss=71.5361
	step [105/192], loss=56.7616
	step [106/192], loss=64.8738
	step [107/192], loss=70.0079
	step [108/192], loss=57.6233
	step [109/192], loss=68.3890
	step [110/192], loss=75.7899
	step [111/192], loss=59.5675
	step [112/192], loss=52.9615
	step [113/192], loss=91.2228
	step [114/192], loss=60.1057
	step [115/192], loss=77.7717
	step [116/192], loss=73.6720
	step [117/192], loss=59.2935
	step [118/192], loss=71.3799
	step [119/192], loss=65.7425
	step [120/192], loss=76.8782
	step [121/192], loss=61.5828
	step [122/192], loss=55.2148
	step [123/192], loss=57.6904
	step [124/192], loss=69.7412
	step [125/192], loss=69.1883
	step [126/192], loss=71.0318
	step [127/192], loss=61.3314
	step [128/192], loss=58.1727
	step [129/192], loss=74.5147
	step [130/192], loss=57.1372
	step [131/192], loss=59.9619
	step [132/192], loss=73.6049
	step [133/192], loss=62.0829
	step [134/192], loss=73.9799
	step [135/192], loss=70.9138
	step [136/192], loss=75.3164
	step [137/192], loss=78.5896
	step [138/192], loss=68.8887
	step [139/192], loss=62.3354
	step [140/192], loss=77.1797
	step [141/192], loss=77.5516
	step [142/192], loss=61.4798
	step [143/192], loss=63.0878
	step [144/192], loss=75.7001
	step [145/192], loss=77.3601
	step [146/192], loss=69.0745
	step [147/192], loss=64.8123
	step [148/192], loss=78.0236
	step [149/192], loss=73.3357
	step [150/192], loss=81.7240
	step [151/192], loss=70.8248
	step [152/192], loss=70.8057
	step [153/192], loss=66.2048
	step [154/192], loss=81.9597
	step [155/192], loss=67.4586
	step [156/192], loss=61.6254
	step [157/192], loss=53.4099
	step [158/192], loss=76.9202
	step [159/192], loss=71.6313
	step [160/192], loss=65.1333
	step [161/192], loss=69.2198
	step [162/192], loss=73.3990
	step [163/192], loss=89.2710
	step [164/192], loss=79.1587
	step [165/192], loss=66.0014
	step [166/192], loss=63.3293
	step [167/192], loss=67.6565
	step [168/192], loss=75.6070
	step [169/192], loss=76.9075
	step [170/192], loss=67.5633
	step [171/192], loss=63.2039
	step [172/192], loss=81.4830
	step [173/192], loss=68.8319
	step [174/192], loss=57.7054
	step [175/192], loss=60.0449
	step [176/192], loss=66.7636
	step [177/192], loss=63.7506
	step [178/192], loss=62.9609
	step [179/192], loss=60.4491
	step [180/192], loss=68.2384
	step [181/192], loss=65.3547
	step [182/192], loss=66.9545
	step [183/192], loss=63.6203
	step [184/192], loss=56.5708
	step [185/192], loss=84.2834
	step [186/192], loss=68.6747
	step [187/192], loss=62.1179
	step [188/192], loss=67.1744
	step [189/192], loss=55.3533
	step [190/192], loss=56.9906
	step [191/192], loss=68.6822
	step [192/192], loss=54.0886
	Evaluating
	loss=0.0065, precision=0.4053, recall=0.8752, f1=0.5540
saving model as: 0_saved_model.pth
Training epoch 103
	step [1/192], loss=67.9714
	step [2/192], loss=57.0328
	step [3/192], loss=58.9301
	step [4/192], loss=76.1430
	step [5/192], loss=70.9430
	step [6/192], loss=69.0394
	step [7/192], loss=75.8495
	step [8/192], loss=75.3311
	step [9/192], loss=62.0669
	step [10/192], loss=67.9349
	step [11/192], loss=56.4078
	step [12/192], loss=72.4889
	step [13/192], loss=69.0225
	step [14/192], loss=66.7688
	step [15/192], loss=68.1097
	step [16/192], loss=61.5989
	step [17/192], loss=65.6097
	step [18/192], loss=49.6990
	step [19/192], loss=61.2809
	step [20/192], loss=64.8451
	step [21/192], loss=63.9046
	step [22/192], loss=64.3036
	step [23/192], loss=63.1716
	step [24/192], loss=64.6064
	step [25/192], loss=69.4453
	step [26/192], loss=80.6424
	step [27/192], loss=85.8506
	step [28/192], loss=71.3985
	step [29/192], loss=72.9868
	step [30/192], loss=69.9509
	step [31/192], loss=54.3443
	step [32/192], loss=69.3459
	step [33/192], loss=66.8192
	step [34/192], loss=72.9880
	step [35/192], loss=61.0850
	step [36/192], loss=64.7958
	step [37/192], loss=67.3144
	step [38/192], loss=62.2027
	step [39/192], loss=81.1258
	step [40/192], loss=59.9545
	step [41/192], loss=59.0569
	step [42/192], loss=64.5580
	step [43/192], loss=50.6580
	step [44/192], loss=74.4882
	step [45/192], loss=66.3034
	step [46/192], loss=66.4513
	step [47/192], loss=60.8490
	step [48/192], loss=65.4956
	step [49/192], loss=63.6182
	step [50/192], loss=73.8786
	step [51/192], loss=73.2225
	step [52/192], loss=66.0447
	step [53/192], loss=56.4868
	step [54/192], loss=69.5121
	step [55/192], loss=61.3301
	step [56/192], loss=73.7478
	step [57/192], loss=68.9262
	step [58/192], loss=56.2537
	step [59/192], loss=65.3702
	step [60/192], loss=74.8421
	step [61/192], loss=63.0329
	step [62/192], loss=74.8251
	step [63/192], loss=58.4657
	step [64/192], loss=55.2452
	step [65/192], loss=84.7917
	step [66/192], loss=70.2014
	step [67/192], loss=59.5004
	step [68/192], loss=66.0394
	step [69/192], loss=74.7758
	step [70/192], loss=74.5419
	step [71/192], loss=67.1489
	step [72/192], loss=71.6418
	step [73/192], loss=62.1173
	step [74/192], loss=66.6853
	step [75/192], loss=66.0409
	step [76/192], loss=76.9787
	step [77/192], loss=68.8704
	step [78/192], loss=71.7290
	step [79/192], loss=56.5224
	step [80/192], loss=63.6386
	step [81/192], loss=71.0253
	step [82/192], loss=71.0246
	step [83/192], loss=71.9396
	step [84/192], loss=66.5696
	step [85/192], loss=58.8349
	step [86/192], loss=63.5956
	step [87/192], loss=71.2659
	step [88/192], loss=71.8956
	step [89/192], loss=70.3220
	step [90/192], loss=63.9142
	step [91/192], loss=59.5574
	step [92/192], loss=56.0633
	step [93/192], loss=53.2127
	step [94/192], loss=74.3863
	step [95/192], loss=67.8424
	step [96/192], loss=67.1441
	step [97/192], loss=63.5191
	step [98/192], loss=87.7188
	step [99/192], loss=76.7846
	step [100/192], loss=80.3190
	step [101/192], loss=69.4198
	step [102/192], loss=72.8943
	step [103/192], loss=64.4657
	step [104/192], loss=73.2294
	step [105/192], loss=67.4040
	step [106/192], loss=58.1006
	step [107/192], loss=76.1862
	step [108/192], loss=77.6314
	step [109/192], loss=64.5777
	step [110/192], loss=61.1880
	step [111/192], loss=56.7459
	step [112/192], loss=75.9072
	step [113/192], loss=66.5470
	step [114/192], loss=62.3585
	step [115/192], loss=72.8808
	step [116/192], loss=77.7995
	step [117/192], loss=71.2315
	step [118/192], loss=69.5117
	step [119/192], loss=69.1872
	step [120/192], loss=70.1288
	step [121/192], loss=69.5753
	step [122/192], loss=64.7275
	step [123/192], loss=72.9329
	step [124/192], loss=69.8842
	step [125/192], loss=60.5241
	step [126/192], loss=66.1767
	step [127/192], loss=58.6412
	step [128/192], loss=73.4919
	step [129/192], loss=72.7210
	step [130/192], loss=54.7472
	step [131/192], loss=88.7312
	step [132/192], loss=62.9018
	step [133/192], loss=74.9997
	step [134/192], loss=72.1436
	step [135/192], loss=61.2754
	step [136/192], loss=63.1555
	step [137/192], loss=66.9286
	step [138/192], loss=76.8615
	step [139/192], loss=75.3113
	step [140/192], loss=71.5343
	step [141/192], loss=65.6297
	step [142/192], loss=70.7291
	step [143/192], loss=57.3877
	step [144/192], loss=75.2590
	step [145/192], loss=67.2105
	step [146/192], loss=69.7461
	step [147/192], loss=64.7823
	step [148/192], loss=70.1343
	step [149/192], loss=68.4366
	step [150/192], loss=79.3294
	step [151/192], loss=82.7237
	step [152/192], loss=91.6737
	step [153/192], loss=69.9942
	step [154/192], loss=70.3710
	step [155/192], loss=76.3959
	step [156/192], loss=57.1512
	step [157/192], loss=58.9718
	step [158/192], loss=68.7691
	step [159/192], loss=69.0641
	step [160/192], loss=69.2298
	step [161/192], loss=62.7166
	step [162/192], loss=69.8920
	step [163/192], loss=65.6097
	step [164/192], loss=63.3131
	step [165/192], loss=57.8944
	step [166/192], loss=61.5105
	step [167/192], loss=79.0112
	step [168/192], loss=61.5641
	step [169/192], loss=78.6754
	step [170/192], loss=69.3718
	step [171/192], loss=68.7465
	step [172/192], loss=59.1400
	step [173/192], loss=76.9733
	step [174/192], loss=73.4423
	step [175/192], loss=69.1583
	step [176/192], loss=70.0547
	step [177/192], loss=60.8466
	step [178/192], loss=68.4767
	step [179/192], loss=74.2597
	step [180/192], loss=70.3307
	step [181/192], loss=70.7292
	step [182/192], loss=70.2024
	step [183/192], loss=63.9490
	step [184/192], loss=71.4365
	step [185/192], loss=56.1989
	step [186/192], loss=66.5430
	step [187/192], loss=68.9989
	step [188/192], loss=70.4505
	step [189/192], loss=69.8030
	step [190/192], loss=68.0938
	step [191/192], loss=67.2997
	step [192/192], loss=58.1304
	Evaluating
	loss=0.0078, precision=0.3403, recall=0.8792, f1=0.4907
Training epoch 104
	step [1/192], loss=68.0042
	step [2/192], loss=65.4547
	step [3/192], loss=63.3733
	step [4/192], loss=63.2497
	step [5/192], loss=67.8249
	step [6/192], loss=74.1900
	step [7/192], loss=72.7464
	step [8/192], loss=71.0414
	step [9/192], loss=71.5379
	step [10/192], loss=75.9215
	step [11/192], loss=61.5272
	step [12/192], loss=80.2014
	step [13/192], loss=72.2521
	step [14/192], loss=74.7390
	step [15/192], loss=69.6668
	step [16/192], loss=62.1956
	step [17/192], loss=74.2193
	step [18/192], loss=55.4427
	step [19/192], loss=79.9966
	step [20/192], loss=70.5346
	step [21/192], loss=49.7192
	step [22/192], loss=88.9506
	step [23/192], loss=52.2199
	step [24/192], loss=52.0820
	step [25/192], loss=71.2871
	step [26/192], loss=63.0247
	step [27/192], loss=61.3703
	step [28/192], loss=79.1425
	step [29/192], loss=54.3822
	step [30/192], loss=52.7547
	step [31/192], loss=63.8296
	step [32/192], loss=80.9593
	step [33/192], loss=58.5635
	step [34/192], loss=66.7119
	step [35/192], loss=57.3736
	step [36/192], loss=55.9217
	step [37/192], loss=69.0779
	step [38/192], loss=62.7619
	step [39/192], loss=72.5715
	step [40/192], loss=77.9367
	step [41/192], loss=78.7900
	step [42/192], loss=61.9918
	step [43/192], loss=76.8578
	step [44/192], loss=79.1981
	step [45/192], loss=76.8714
	step [46/192], loss=66.6888
	step [47/192], loss=85.1423
	step [48/192], loss=69.8955
	step [49/192], loss=63.6381
	step [50/192], loss=66.0917
	step [51/192], loss=57.5579
	step [52/192], loss=57.5004
	step [53/192], loss=73.1948
	step [54/192], loss=72.0803
	step [55/192], loss=79.5911
	step [56/192], loss=59.5383
	step [57/192], loss=67.8121
	step [58/192], loss=73.8321
	step [59/192], loss=67.8635
	step [60/192], loss=56.0048
	step [61/192], loss=63.3742
	step [62/192], loss=72.7766
	step [63/192], loss=72.6975
	step [64/192], loss=66.2030
	step [65/192], loss=57.7027
	step [66/192], loss=60.7821
	step [67/192], loss=66.2869
	step [68/192], loss=69.4877
	step [69/192], loss=64.8326
	step [70/192], loss=64.6756
	step [71/192], loss=59.6382
	step [72/192], loss=72.7039
	step [73/192], loss=80.2057
	step [74/192], loss=74.4936
	step [75/192], loss=74.5392
	step [76/192], loss=64.7101
	step [77/192], loss=67.6128
	step [78/192], loss=63.0971
	step [79/192], loss=61.7755
	step [80/192], loss=68.3527
	step [81/192], loss=64.5207
	step [82/192], loss=58.3146
	step [83/192], loss=68.9452
	step [84/192], loss=70.2353
	step [85/192], loss=70.9675
	step [86/192], loss=64.0660
	step [87/192], loss=75.1648
	step [88/192], loss=80.5033
	step [89/192], loss=73.7247
	step [90/192], loss=53.8913
	step [91/192], loss=59.3300
	step [92/192], loss=62.8139
	step [93/192], loss=71.5933
	step [94/192], loss=74.9564
	step [95/192], loss=57.0468
	step [96/192], loss=64.2788
	step [97/192], loss=49.0014
	step [98/192], loss=59.6135
	step [99/192], loss=71.6955
	step [100/192], loss=65.5806
	step [101/192], loss=68.5660
	step [102/192], loss=61.2612
	step [103/192], loss=68.8232
	step [104/192], loss=58.1519
	step [105/192], loss=74.7369
	step [106/192], loss=64.0346
	step [107/192], loss=68.9026
	step [108/192], loss=68.3037
	step [109/192], loss=75.7634
	step [110/192], loss=57.0986
	step [111/192], loss=68.9146
	step [112/192], loss=60.8280
	step [113/192], loss=75.2902
	step [114/192], loss=67.4272
	step [115/192], loss=72.9527
	step [116/192], loss=57.0848
	step [117/192], loss=76.1408
	step [118/192], loss=70.0070
	step [119/192], loss=87.4714
	step [120/192], loss=78.2128
	step [121/192], loss=77.1723
	step [122/192], loss=67.3512
	step [123/192], loss=78.3029
	step [124/192], loss=57.0832
	step [125/192], loss=71.4012
	step [126/192], loss=61.8988
	step [127/192], loss=73.0737
	step [128/192], loss=72.6401
	step [129/192], loss=67.6314
	step [130/192], loss=63.5309
	step [131/192], loss=63.8366
	step [132/192], loss=76.8344
	step [133/192], loss=63.6455
	step [134/192], loss=62.0865
	step [135/192], loss=76.2869
	step [136/192], loss=67.8609
	step [137/192], loss=74.3062
	step [138/192], loss=83.5748
	step [139/192], loss=80.8515
	step [140/192], loss=67.4412
	step [141/192], loss=62.9305
	step [142/192], loss=71.1323
	step [143/192], loss=66.5984
	step [144/192], loss=67.7249
	step [145/192], loss=66.3092
	step [146/192], loss=67.6100
	step [147/192], loss=71.7493
	step [148/192], loss=65.8131
	step [149/192], loss=63.7939
	step [150/192], loss=58.4538
	step [151/192], loss=63.5995
	step [152/192], loss=69.1633
	step [153/192], loss=57.2372
	step [154/192], loss=70.9987
	step [155/192], loss=62.4197
	step [156/192], loss=65.8869
	step [157/192], loss=66.0062
	step [158/192], loss=70.9559
	step [159/192], loss=74.4914
	step [160/192], loss=74.9292
	step [161/192], loss=74.1119
	step [162/192], loss=74.1370
	step [163/192], loss=66.4177
	step [164/192], loss=68.2056
	step [165/192], loss=63.5921
	step [166/192], loss=61.4710
	step [167/192], loss=59.9640
	step [168/192], loss=71.6779
	step [169/192], loss=65.5380
	step [170/192], loss=76.8118
	step [171/192], loss=57.8941
	step [172/192], loss=79.2386
	step [173/192], loss=78.4854
	step [174/192], loss=72.4125
	step [175/192], loss=58.8282
	step [176/192], loss=65.7688
	step [177/192], loss=56.4320
	step [178/192], loss=74.3220
	step [179/192], loss=73.8298
	step [180/192], loss=54.7533
	step [181/192], loss=63.9729
	step [182/192], loss=82.0450
	step [183/192], loss=63.1669
	step [184/192], loss=55.2740
	step [185/192], loss=68.9492
	step [186/192], loss=57.8899
	step [187/192], loss=71.6632
	step [188/192], loss=61.5149
	step [189/192], loss=70.6126
	step [190/192], loss=69.6815
	step [191/192], loss=68.1940
	step [192/192], loss=61.9304
	Evaluating
	loss=0.0061, precision=0.4292, recall=0.8680, f1=0.5744
saving model as: 0_saved_model.pth
Training epoch 105
	step [1/192], loss=70.9890
	step [2/192], loss=65.2008
	step [3/192], loss=58.7977
	step [4/192], loss=61.6493
	step [5/192], loss=69.6517
	step [6/192], loss=72.8932
	step [7/192], loss=60.1117
	step [8/192], loss=61.5251
	step [9/192], loss=66.1212
	step [10/192], loss=66.9404
	step [11/192], loss=75.4175
	step [12/192], loss=72.8401
	step [13/192], loss=68.8674
	step [14/192], loss=70.1333
	step [15/192], loss=63.0950
	step [16/192], loss=59.4081
	step [17/192], loss=73.0516
	step [18/192], loss=63.5062
	step [19/192], loss=65.1433
	step [20/192], loss=63.0661
	step [21/192], loss=67.0891
	step [22/192], loss=62.9923
	step [23/192], loss=55.3369
	step [24/192], loss=64.5638
	step [25/192], loss=68.4053
	step [26/192], loss=71.3303
	step [27/192], loss=67.9521
	step [28/192], loss=72.1467
	step [29/192], loss=64.7655
	step [30/192], loss=56.0173
	step [31/192], loss=61.4766
	step [32/192], loss=72.5660
	step [33/192], loss=55.5057
	step [34/192], loss=65.3042
	step [35/192], loss=59.2223
	step [36/192], loss=68.7858
	step [37/192], loss=69.2284
	step [38/192], loss=62.7419
	step [39/192], loss=63.3543
	step [40/192], loss=81.7853
	step [41/192], loss=55.8964
	step [42/192], loss=74.0822
	step [43/192], loss=64.9597
	step [44/192], loss=83.6898
	step [45/192], loss=65.0347
	step [46/192], loss=62.4752
	step [47/192], loss=63.0643
	step [48/192], loss=76.3888
	step [49/192], loss=66.0348
	step [50/192], loss=66.5217
	step [51/192], loss=68.9349
	step [52/192], loss=58.0666
	step [53/192], loss=72.4482
	step [54/192], loss=68.7116
	step [55/192], loss=65.9847
	step [56/192], loss=74.5244
	step [57/192], loss=74.7355
	step [58/192], loss=62.3715
	step [59/192], loss=64.8762
	step [60/192], loss=55.4169
	step [61/192], loss=60.8337
	step [62/192], loss=55.0298
	step [63/192], loss=68.0434
	step [64/192], loss=76.2117
	step [65/192], loss=65.0346
	step [66/192], loss=59.4119
	step [67/192], loss=80.4236
	step [68/192], loss=60.1934
	step [69/192], loss=71.0334
	step [70/192], loss=72.7536
	step [71/192], loss=57.4402
	step [72/192], loss=72.3053
	step [73/192], loss=56.2018
	step [74/192], loss=73.8882
	step [75/192], loss=65.8045
	step [76/192], loss=74.9321
	step [77/192], loss=68.4545
	step [78/192], loss=82.4598
	step [79/192], loss=72.1838
	step [80/192], loss=59.0720
	step [81/192], loss=69.8089
	step [82/192], loss=59.1565
	step [83/192], loss=70.4013
	step [84/192], loss=63.7480
	step [85/192], loss=68.1642
	step [86/192], loss=63.6756
	step [87/192], loss=64.2397
	step [88/192], loss=54.8450
	step [89/192], loss=69.8681
	step [90/192], loss=78.8044
	step [91/192], loss=63.2270
	step [92/192], loss=74.3473
	step [93/192], loss=77.2622
	step [94/192], loss=68.7901
	step [95/192], loss=72.6703
	step [96/192], loss=63.8928
	step [97/192], loss=72.9751
	step [98/192], loss=66.1024
	step [99/192], loss=59.1554
	step [100/192], loss=69.1249
	step [101/192], loss=58.7526
	step [102/192], loss=68.5528
	step [103/192], loss=62.4607
	step [104/192], loss=67.1986
	step [105/192], loss=68.2873
	step [106/192], loss=68.9955
	step [107/192], loss=72.5207
	step [108/192], loss=66.0880
	step [109/192], loss=54.9292
	step [110/192], loss=71.4380
	step [111/192], loss=77.4892
	step [112/192], loss=89.7627
	step [113/192], loss=66.7047
	step [114/192], loss=70.9383
	step [115/192], loss=62.4078
	step [116/192], loss=73.8392
	step [117/192], loss=57.7596
	step [118/192], loss=66.1059
	step [119/192], loss=54.1544
	step [120/192], loss=77.8265
	step [121/192], loss=70.8502
	step [122/192], loss=70.2795
	step [123/192], loss=63.8933
	step [124/192], loss=61.5660
	step [125/192], loss=73.9399
	step [126/192], loss=63.0316
	step [127/192], loss=77.9419
	step [128/192], loss=73.9023
	step [129/192], loss=70.5122
	step [130/192], loss=78.4508
	step [131/192], loss=72.7061
	step [132/192], loss=68.3466
	step [133/192], loss=92.0253
	step [134/192], loss=70.6492
	step [135/192], loss=58.4965
	step [136/192], loss=70.1427
	step [137/192], loss=71.0188
	step [138/192], loss=73.5918
	step [139/192], loss=75.6334
	step [140/192], loss=68.6822
	step [141/192], loss=64.7130
	step [142/192], loss=68.0730
	step [143/192], loss=79.7907
	step [144/192], loss=75.8719
	step [145/192], loss=63.4990
	step [146/192], loss=61.6775
	step [147/192], loss=65.0917
	step [148/192], loss=62.0801
	step [149/192], loss=71.1363
	step [150/192], loss=59.8749
	step [151/192], loss=56.8253
	step [152/192], loss=66.4975
	step [153/192], loss=62.2010
	step [154/192], loss=87.9668
	step [155/192], loss=67.9395
	step [156/192], loss=71.8853
	step [157/192], loss=70.6113
	step [158/192], loss=65.0817
	step [159/192], loss=76.8724
	step [160/192], loss=69.3130
	step [161/192], loss=69.6579
	step [162/192], loss=70.1835
	step [163/192], loss=59.2978
	step [164/192], loss=63.6629
	step [165/192], loss=63.3449
	step [166/192], loss=84.5230
	step [167/192], loss=82.6579
	step [168/192], loss=68.9192
	step [169/192], loss=72.7269
	step [170/192], loss=75.1512
	step [171/192], loss=68.7340
	step [172/192], loss=63.1212
	step [173/192], loss=66.9896
	step [174/192], loss=67.7082
	step [175/192], loss=67.1618
	step [176/192], loss=77.5004
	step [177/192], loss=73.6337
	step [178/192], loss=70.5894
	step [179/192], loss=75.0441
	step [180/192], loss=64.4431
	step [181/192], loss=61.4123
	step [182/192], loss=65.9727
	step [183/192], loss=63.7375
	step [184/192], loss=67.9714
	step [185/192], loss=70.1887
	step [186/192], loss=70.1608
	step [187/192], loss=63.4199
	step [188/192], loss=65.9842
	step [189/192], loss=67.3863
	step [190/192], loss=60.0540
	step [191/192], loss=65.1029
	step [192/192], loss=51.4405
	Evaluating
	loss=0.0061, precision=0.4148, recall=0.8599, f1=0.5596
Training epoch 106
	step [1/192], loss=62.9471
	step [2/192], loss=68.9718
	step [3/192], loss=59.8319
	step [4/192], loss=63.7267
	step [5/192], loss=66.0400
	step [6/192], loss=73.3794
	step [7/192], loss=72.3200
	step [8/192], loss=69.9577
	step [9/192], loss=76.2157
	step [10/192], loss=65.4559
	step [11/192], loss=68.3246
	step [12/192], loss=70.1193
	step [13/192], loss=66.6451
	step [14/192], loss=59.2208
	step [15/192], loss=68.2147
	step [16/192], loss=63.4044
	step [17/192], loss=66.2820
	step [18/192], loss=62.0322
	step [19/192], loss=73.7851
	step [20/192], loss=74.8589
	step [21/192], loss=67.8294
	step [22/192], loss=60.0634
	step [23/192], loss=69.4712
	step [24/192], loss=62.5943
	step [25/192], loss=70.6583
	step [26/192], loss=73.2561
	step [27/192], loss=71.5405
	step [28/192], loss=68.9582
	step [29/192], loss=69.5287
	step [30/192], loss=59.3519
	step [31/192], loss=66.1679
	step [32/192], loss=65.4904
	step [33/192], loss=76.7361
	step [34/192], loss=64.9141
	step [35/192], loss=64.2353
	step [36/192], loss=68.8436
	step [37/192], loss=73.8014
	step [38/192], loss=57.8488
	step [39/192], loss=57.9407
	step [40/192], loss=64.2971
	step [41/192], loss=67.9027
	step [42/192], loss=64.9623
	step [43/192], loss=68.8293
	step [44/192], loss=67.8754
	step [45/192], loss=62.1713
	step [46/192], loss=71.0182
	step [47/192], loss=58.3569
	step [48/192], loss=68.1274
	step [49/192], loss=61.1281
	step [50/192], loss=77.9997
	step [51/192], loss=60.1045
	step [52/192], loss=60.8550
	step [53/192], loss=69.6371
	step [54/192], loss=77.2173
	step [55/192], loss=63.2550
	step [56/192], loss=60.0908
	step [57/192], loss=69.7535
	step [58/192], loss=67.1357
	step [59/192], loss=75.6876
	step [60/192], loss=67.3203
	step [61/192], loss=71.2120
	step [62/192], loss=67.1078
	step [63/192], loss=64.9969
	step [64/192], loss=66.5678
	step [65/192], loss=72.8954
	step [66/192], loss=66.1227
	step [67/192], loss=76.9294
	step [68/192], loss=77.1303
	step [69/192], loss=64.1178
	step [70/192], loss=65.1179
	step [71/192], loss=57.9454
	step [72/192], loss=71.3517
	step [73/192], loss=71.3855
	step [74/192], loss=89.7105
	step [75/192], loss=71.6368
	step [76/192], loss=72.9775
	step [77/192], loss=65.5355
	step [78/192], loss=71.4977
	step [79/192], loss=82.4952
	step [80/192], loss=64.8266
	step [81/192], loss=54.9746
	step [82/192], loss=69.0587
	step [83/192], loss=63.8369
	step [84/192], loss=61.6969
	step [85/192], loss=57.8919
	step [86/192], loss=67.2147
	step [87/192], loss=61.7333
	step [88/192], loss=60.4867
	step [89/192], loss=65.1692
	step [90/192], loss=63.2219
	step [91/192], loss=59.4310
	step [92/192], loss=64.7590
	step [93/192], loss=70.4670
	step [94/192], loss=55.7150
	step [95/192], loss=56.2953
	step [96/192], loss=64.5682
	step [97/192], loss=70.7318
	step [98/192], loss=51.2579
	step [99/192], loss=74.5957
	step [100/192], loss=79.6485
	step [101/192], loss=65.6494
	step [102/192], loss=74.9353
	step [103/192], loss=71.3435
	step [104/192], loss=66.8043
	step [105/192], loss=64.5752
	step [106/192], loss=72.9960
	step [107/192], loss=74.1392
	step [108/192], loss=71.7311
	step [109/192], loss=61.9408
	step [110/192], loss=71.7073
	step [111/192], loss=65.3156
	step [112/192], loss=73.3910
	step [113/192], loss=64.7317
	step [114/192], loss=73.5679
	step [115/192], loss=54.7198
	step [116/192], loss=70.7254
	step [117/192], loss=65.7971
	step [118/192], loss=62.7730
	step [119/192], loss=69.2957
	step [120/192], loss=62.7245
	step [121/192], loss=70.4279
	step [122/192], loss=61.6696
	step [123/192], loss=79.5147
	step [124/192], loss=67.1524
	step [125/192], loss=63.5547
	step [126/192], loss=62.0570
	step [127/192], loss=64.4317
	step [128/192], loss=67.9580
	step [129/192], loss=73.5231
	step [130/192], loss=68.9950
	step [131/192], loss=80.4831
	step [132/192], loss=69.0603
	step [133/192], loss=64.8096
	step [134/192], loss=68.5455
	step [135/192], loss=60.6536
	step [136/192], loss=62.1471
	step [137/192], loss=72.2828
	step [138/192], loss=51.0935
	step [139/192], loss=74.6723
	step [140/192], loss=60.9605
	step [141/192], loss=68.3715
	step [142/192], loss=63.5932
	step [143/192], loss=64.7188
	step [144/192], loss=62.7836
	step [145/192], loss=67.1031
	step [146/192], loss=80.5428
	step [147/192], loss=76.0864
	step [148/192], loss=67.4528
	step [149/192], loss=67.9632
	step [150/192], loss=69.9839
	step [151/192], loss=58.7744
	step [152/192], loss=59.3952
	step [153/192], loss=52.3118
	step [154/192], loss=62.2784
	step [155/192], loss=67.1504
	step [156/192], loss=68.4118
	step [157/192], loss=65.4059
	step [158/192], loss=77.2155
	step [159/192], loss=74.6320
	step [160/192], loss=85.6447
	step [161/192], loss=65.5235
	step [162/192], loss=75.2510
	step [163/192], loss=76.3519
	step [164/192], loss=69.6142
	step [165/192], loss=58.2404
	step [166/192], loss=70.3782
	step [167/192], loss=57.4684
	step [168/192], loss=71.5388
	step [169/192], loss=70.2649
	step [170/192], loss=81.9069
	step [171/192], loss=72.4778
	step [172/192], loss=64.8816
	step [173/192], loss=64.3271
	step [174/192], loss=71.8740
	step [175/192], loss=66.5458
	step [176/192], loss=72.2012
	step [177/192], loss=74.1805
	step [178/192], loss=69.4311
	step [179/192], loss=76.4444
	step [180/192], loss=60.2710
	step [181/192], loss=64.2230
	step [182/192], loss=71.3097
	step [183/192], loss=64.2420
	step [184/192], loss=75.2151
	step [185/192], loss=69.1008
	step [186/192], loss=64.8413
	step [187/192], loss=66.1001
	step [188/192], loss=64.6755
	step [189/192], loss=68.6137
	step [190/192], loss=70.3223
	step [191/192], loss=67.6422
	step [192/192], loss=66.4322
	Evaluating
	loss=0.0077, precision=0.3415, recall=0.8805, f1=0.4922
Training epoch 107
	step [1/192], loss=57.8892
	step [2/192], loss=72.3287
	step [3/192], loss=61.4286
	step [4/192], loss=63.6185
	step [5/192], loss=64.2878
	step [6/192], loss=79.4280
	step [7/192], loss=78.0470
	step [8/192], loss=71.5102
	step [9/192], loss=64.1904
	step [10/192], loss=62.8326
	step [11/192], loss=71.1250
	step [12/192], loss=68.8615
	step [13/192], loss=79.4748
	step [14/192], loss=73.4429
	step [15/192], loss=69.8091
	step [16/192], loss=73.6482
	step [17/192], loss=67.7518
	step [18/192], loss=69.7432
	step [19/192], loss=62.4418
	step [20/192], loss=68.2346
	step [21/192], loss=77.7584
	step [22/192], loss=64.1080
	step [23/192], loss=72.5636
	step [24/192], loss=69.3551
	step [25/192], loss=79.9569
	step [26/192], loss=66.0199
	step [27/192], loss=71.8011
	step [28/192], loss=61.0557
	step [29/192], loss=63.7819
	step [30/192], loss=60.2215
	step [31/192], loss=69.9893
	step [32/192], loss=71.2142
	step [33/192], loss=63.6525
	step [34/192], loss=72.0268
	step [35/192], loss=66.7573
	step [36/192], loss=59.4378
	step [37/192], loss=67.9544
	step [38/192], loss=69.9211
	step [39/192], loss=76.0028
	step [40/192], loss=69.4827
	step [41/192], loss=72.0872
	step [42/192], loss=82.1336
	step [43/192], loss=67.0838
	step [44/192], loss=63.4338
	step [45/192], loss=60.6035
	step [46/192], loss=77.9639
	step [47/192], loss=62.0472
	step [48/192], loss=63.2527
	step [49/192], loss=71.0275
	step [50/192], loss=67.7781
	step [51/192], loss=70.5323
	step [52/192], loss=70.7681
	step [53/192], loss=60.1057
	step [54/192], loss=69.0821
	step [55/192], loss=67.6503
	step [56/192], loss=61.6729
	step [57/192], loss=70.1083
	step [58/192], loss=67.6408
	step [59/192], loss=66.0844
	step [60/192], loss=66.7477
	step [61/192], loss=74.8599
	step [62/192], loss=61.0333
	step [63/192], loss=63.5573
	step [64/192], loss=68.3616
	step [65/192], loss=61.9637
	step [66/192], loss=64.3990
	step [67/192], loss=71.7794
	step [68/192], loss=68.0026
	step [69/192], loss=64.3436
	step [70/192], loss=60.6326
	step [71/192], loss=61.5893
	step [72/192], loss=65.8102
	step [73/192], loss=57.9888
	step [74/192], loss=64.1367
	step [75/192], loss=69.4239
	step [76/192], loss=77.4482
	step [77/192], loss=69.8462
	step [78/192], loss=64.4793
	step [79/192], loss=63.1241
	step [80/192], loss=64.8913
	step [81/192], loss=64.5073
	step [82/192], loss=61.1704
	step [83/192], loss=66.9607
	step [84/192], loss=61.4253
	step [85/192], loss=66.4154
	step [86/192], loss=59.1816
	step [87/192], loss=67.7294
	step [88/192], loss=76.9756
	step [89/192], loss=67.5553
	step [90/192], loss=83.6064
	step [91/192], loss=76.3359
	step [92/192], loss=64.3557
	step [93/192], loss=75.8276
	step [94/192], loss=71.9715
	step [95/192], loss=60.9401
	step [96/192], loss=69.6083
	step [97/192], loss=62.3741
	step [98/192], loss=62.8310
	step [99/192], loss=69.5235
	step [100/192], loss=76.6667
	step [101/192], loss=75.9793
	step [102/192], loss=64.7339
	step [103/192], loss=69.2786
	step [104/192], loss=72.0155
	step [105/192], loss=68.3103
	step [106/192], loss=56.8578
	step [107/192], loss=68.6831
	step [108/192], loss=75.5001
	step [109/192], loss=61.0899
	step [110/192], loss=74.0636
	step [111/192], loss=68.8052
	step [112/192], loss=66.9120
	step [113/192], loss=57.4867
	step [114/192], loss=69.8904
	step [115/192], loss=80.1440
	step [116/192], loss=76.6678
	step [117/192], loss=87.0996
	step [118/192], loss=61.4950
	step [119/192], loss=61.3459
	step [120/192], loss=80.3115
	step [121/192], loss=76.5836
	step [122/192], loss=71.3052
	step [123/192], loss=66.0302
	step [124/192], loss=56.7961
	step [125/192], loss=74.7000
	step [126/192], loss=53.4831
	step [127/192], loss=73.2266
	step [128/192], loss=69.6272
	step [129/192], loss=61.8220
	step [130/192], loss=75.7396
	step [131/192], loss=69.8738
	step [132/192], loss=58.4402
	step [133/192], loss=68.9942
	step [134/192], loss=63.6860
	step [135/192], loss=70.9406
	step [136/192], loss=53.8779
	step [137/192], loss=68.3342
	step [138/192], loss=71.1441
	step [139/192], loss=62.4098
	step [140/192], loss=62.5434
	step [141/192], loss=60.7192
	step [142/192], loss=74.5989
	step [143/192], loss=70.0949
	step [144/192], loss=62.2850
	step [145/192], loss=70.1748
	step [146/192], loss=67.4770
	step [147/192], loss=68.3437
	step [148/192], loss=71.8432
	step [149/192], loss=56.8606
	step [150/192], loss=70.4748
	step [151/192], loss=68.6301
	step [152/192], loss=61.7763
	step [153/192], loss=73.5428
	step [154/192], loss=73.5592
	step [155/192], loss=64.7925
	step [156/192], loss=65.3503
	step [157/192], loss=77.9704
	step [158/192], loss=73.2454
	step [159/192], loss=63.3597
	step [160/192], loss=58.0061
	step [161/192], loss=72.9612
	step [162/192], loss=67.5925
	step [163/192], loss=71.0352
	step [164/192], loss=61.2306
	step [165/192], loss=61.5807
	step [166/192], loss=70.1610
	step [167/192], loss=58.1732
	step [168/192], loss=62.9297
	step [169/192], loss=59.0998
	step [170/192], loss=60.4244
	step [171/192], loss=66.2223
	step [172/192], loss=86.1199
	step [173/192], loss=71.3253
	step [174/192], loss=69.8826
	step [175/192], loss=75.1461
	step [176/192], loss=64.4946
	step [177/192], loss=69.6141
	step [178/192], loss=65.3569
	step [179/192], loss=65.2442
	step [180/192], loss=58.7693
	step [181/192], loss=75.3940
	step [182/192], loss=57.4310
	step [183/192], loss=60.8120
	step [184/192], loss=69.8427
	step [185/192], loss=64.4681
	step [186/192], loss=65.7031
	step [187/192], loss=66.3003
	step [188/192], loss=69.9117
	step [189/192], loss=64.4076
	step [190/192], loss=61.5152
	step [191/192], loss=73.7697
	step [192/192], loss=62.6733
	Evaluating
	loss=0.0068, precision=0.3752, recall=0.8676, f1=0.5238
Training epoch 108
	step [1/192], loss=62.8643
	step [2/192], loss=65.4503
	step [3/192], loss=54.6218
	step [4/192], loss=74.7172
	step [5/192], loss=55.1040
	step [6/192], loss=69.3689
	step [7/192], loss=79.4867
	step [8/192], loss=53.9997
	step [9/192], loss=71.2834
	step [10/192], loss=70.7495
	step [11/192], loss=73.9132
	step [12/192], loss=73.5615
	step [13/192], loss=73.7525
	step [14/192], loss=62.9845
	step [15/192], loss=64.3938
	step [16/192], loss=73.1849
	step [17/192], loss=75.0232
	step [18/192], loss=68.7782
	step [19/192], loss=74.4779
	step [20/192], loss=58.0578
	step [21/192], loss=63.8396
	step [22/192], loss=53.9955
	step [23/192], loss=81.1539
	step [24/192], loss=71.7962
	step [25/192], loss=79.1917
	step [26/192], loss=71.3876
	step [27/192], loss=55.6854
	step [28/192], loss=64.4370
	step [29/192], loss=59.7490
	step [30/192], loss=64.0791
	step [31/192], loss=58.3980
	step [32/192], loss=67.0224
	step [33/192], loss=65.0851
	step [34/192], loss=69.6486
	step [35/192], loss=65.7534
	step [36/192], loss=87.8671
	step [37/192], loss=64.0114
	step [38/192], loss=66.7767
	step [39/192], loss=79.8264
	step [40/192], loss=74.0793
	step [41/192], loss=62.0615
	step [42/192], loss=67.0782
	step [43/192], loss=66.1203
	step [44/192], loss=64.5955
	step [45/192], loss=69.6828
	step [46/192], loss=56.6984
	step [47/192], loss=59.6068
	step [48/192], loss=77.6671
	step [49/192], loss=61.4219
	step [50/192], loss=64.3627
	step [51/192], loss=76.5934
	step [52/192], loss=80.5638
	step [53/192], loss=61.7860
	step [54/192], loss=78.2478
	step [55/192], loss=75.8029
	step [56/192], loss=76.3991
	step [57/192], loss=64.9758
	step [58/192], loss=61.8236
	step [59/192], loss=58.9344
	step [60/192], loss=82.1346
	step [61/192], loss=55.7098
	step [62/192], loss=64.0768
	step [63/192], loss=70.2443
	step [64/192], loss=67.5233
	step [65/192], loss=65.1746
	step [66/192], loss=76.2452
	step [67/192], loss=59.5862
	step [68/192], loss=63.4005
	step [69/192], loss=81.5662
	step [70/192], loss=55.6053
	step [71/192], loss=68.2928
	step [72/192], loss=66.8245
	step [73/192], loss=65.0998
	step [74/192], loss=69.3860
	step [75/192], loss=66.0585
	step [76/192], loss=83.5013
	step [77/192], loss=81.8346
	step [78/192], loss=75.6918
	step [79/192], loss=77.7896
	step [80/192], loss=64.6522
	step [81/192], loss=65.4269
	step [82/192], loss=72.0134
	step [83/192], loss=61.8759
	step [84/192], loss=62.9050
	step [85/192], loss=72.0110
	step [86/192], loss=61.0615
	step [87/192], loss=61.2028
	step [88/192], loss=77.0791
	step [89/192], loss=61.4405
	step [90/192], loss=70.1206
	step [91/192], loss=67.2734
	step [92/192], loss=70.6532
	step [93/192], loss=65.1972
	step [94/192], loss=50.9756
	step [95/192], loss=68.1607
	step [96/192], loss=67.9049
	step [97/192], loss=67.8330
	step [98/192], loss=65.9071
	step [99/192], loss=66.9412
	step [100/192], loss=70.5706
	step [101/192], loss=73.1522
	step [102/192], loss=65.1395
	step [103/192], loss=69.3144
	step [104/192], loss=69.0309
	step [105/192], loss=63.2674
	step [106/192], loss=65.2438
	step [107/192], loss=57.1712
	step [108/192], loss=60.2150
	step [109/192], loss=75.6566
	step [110/192], loss=76.0411
	step [111/192], loss=72.2049
	step [112/192], loss=65.6664
	step [113/192], loss=70.4090
	step [114/192], loss=63.5327
	step [115/192], loss=74.5616
	step [116/192], loss=63.7983
	step [117/192], loss=79.8543
	step [118/192], loss=81.0939
	step [119/192], loss=58.9885
	step [120/192], loss=68.2297
	step [121/192], loss=59.9437
	step [122/192], loss=66.2202
	step [123/192], loss=71.7123
	step [124/192], loss=65.9487
	step [125/192], loss=62.3478
	step [126/192], loss=61.8856
	step [127/192], loss=66.8288
	step [128/192], loss=60.7352
	step [129/192], loss=64.8610
	step [130/192], loss=52.5040
	step [131/192], loss=74.6302
	step [132/192], loss=57.3359
	step [133/192], loss=67.4232
	step [134/192], loss=78.4459
	step [135/192], loss=73.0402
	step [136/192], loss=68.0180
	step [137/192], loss=88.0522
	step [138/192], loss=53.3662
	step [139/192], loss=59.5052
	step [140/192], loss=77.0199
	step [141/192], loss=67.9480
	step [142/192], loss=72.7246
	step [143/192], loss=66.7917
	step [144/192], loss=68.0756
	step [145/192], loss=64.2399
	step [146/192], loss=64.6222
	step [147/192], loss=61.5180
	step [148/192], loss=68.6946
	step [149/192], loss=57.9502
	step [150/192], loss=62.3581
	step [151/192], loss=69.6155
	step [152/192], loss=67.4158
	step [153/192], loss=65.8505
	step [154/192], loss=53.7486
	step [155/192], loss=60.7787
	step [156/192], loss=68.4712
	step [157/192], loss=58.4670
	step [158/192], loss=67.1236
	step [159/192], loss=62.8387
	step [160/192], loss=60.2646
	step [161/192], loss=62.8195
	step [162/192], loss=67.3295
	step [163/192], loss=75.4461
	step [164/192], loss=67.9928
	step [165/192], loss=64.2770
	step [166/192], loss=68.4620
	step [167/192], loss=59.1057
	step [168/192], loss=64.9633
	step [169/192], loss=74.6875
	step [170/192], loss=79.9981
	step [171/192], loss=63.8306
	step [172/192], loss=63.3013
	step [173/192], loss=54.3379
	step [174/192], loss=68.7228
	step [175/192], loss=67.2409
	step [176/192], loss=66.2143
	step [177/192], loss=75.3084
	step [178/192], loss=68.2025
	step [179/192], loss=69.3676
	step [180/192], loss=67.8636
	step [181/192], loss=54.1015
	step [182/192], loss=56.0484
	step [183/192], loss=67.3323
	step [184/192], loss=69.3732
	step [185/192], loss=62.5390
	step [186/192], loss=65.6018
	step [187/192], loss=66.1628
	step [188/192], loss=71.4881
	step [189/192], loss=69.6311
	step [190/192], loss=64.4737
	step [191/192], loss=72.7877
	step [192/192], loss=64.4597
	Evaluating
	loss=0.0071, precision=0.3798, recall=0.8808, f1=0.5307
Training epoch 109
	step [1/192], loss=68.4493
	step [2/192], loss=67.3402
	step [3/192], loss=65.9923
	step [4/192], loss=69.5401
	step [5/192], loss=65.9448
	step [6/192], loss=60.2852
	step [7/192], loss=59.0460
	step [8/192], loss=74.4521
	step [9/192], loss=64.5001
	step [10/192], loss=76.6800
	step [11/192], loss=72.6873
	step [12/192], loss=66.1477
	step [13/192], loss=61.1901
	step [14/192], loss=63.4571
	step [15/192], loss=72.4608
	step [16/192], loss=64.9296
	step [17/192], loss=71.5967
	step [18/192], loss=67.7552
	step [19/192], loss=65.8258
	step [20/192], loss=78.1783
	step [21/192], loss=64.2957
	step [22/192], loss=57.9898
	step [23/192], loss=60.7045
	step [24/192], loss=68.2167
	step [25/192], loss=64.9670
	step [26/192], loss=71.7757
	step [27/192], loss=55.0766
	step [28/192], loss=58.3031
	step [29/192], loss=73.5928
	step [30/192], loss=57.7052
	step [31/192], loss=58.8107
	step [32/192], loss=71.2266
	step [33/192], loss=69.9924
	step [34/192], loss=71.0397
	step [35/192], loss=80.9145
	step [36/192], loss=89.0398
	step [37/192], loss=65.2761
	step [38/192], loss=57.3049
	step [39/192], loss=71.1958
	step [40/192], loss=55.8673
	step [41/192], loss=70.2489
	step [42/192], loss=77.2506
	step [43/192], loss=65.4051
	step [44/192], loss=78.1125
	step [45/192], loss=81.8345
	step [46/192], loss=67.1253
	step [47/192], loss=69.7333
	step [48/192], loss=68.2880
	step [49/192], loss=68.5135
	step [50/192], loss=63.4012
	step [51/192], loss=59.1975
	step [52/192], loss=56.2165
	step [53/192], loss=56.1545
	step [54/192], loss=67.1107
	step [55/192], loss=60.4357
	step [56/192], loss=63.6388
	step [57/192], loss=74.9783
	step [58/192], loss=69.7385
	step [59/192], loss=73.9911
	step [60/192], loss=73.4538
	step [61/192], loss=63.8994
	step [62/192], loss=60.9701
	step [63/192], loss=75.5264
	step [64/192], loss=66.4297
	step [65/192], loss=67.8369
	step [66/192], loss=59.9749
	step [67/192], loss=64.9499
	step [68/192], loss=70.9015
	step [69/192], loss=60.2042
	step [70/192], loss=50.0686
	step [71/192], loss=64.4570
	step [72/192], loss=65.2255
	step [73/192], loss=59.9943
	step [74/192], loss=66.0136
	step [75/192], loss=64.7132
	step [76/192], loss=59.4898
	step [77/192], loss=61.5411
	step [78/192], loss=67.3975
	step [79/192], loss=71.4973
	step [80/192], loss=68.5254
	step [81/192], loss=68.4104
	step [82/192], loss=72.5125
	step [83/192], loss=60.7489
	step [84/192], loss=63.4654
	step [85/192], loss=62.3524
	step [86/192], loss=66.1827
	step [87/192], loss=61.6808
	step [88/192], loss=60.4821
	step [89/192], loss=58.2064
	step [90/192], loss=69.5315
	step [91/192], loss=62.6185
	step [92/192], loss=58.6081
	step [93/192], loss=67.3131
	step [94/192], loss=73.9460
	step [95/192], loss=70.0887
	step [96/192], loss=75.7175
	step [97/192], loss=68.9243
	step [98/192], loss=68.5508
	step [99/192], loss=74.2808
	step [100/192], loss=72.1644
	step [101/192], loss=63.4969
	step [102/192], loss=60.9012
	step [103/192], loss=72.4806
	step [104/192], loss=84.1412
	step [105/192], loss=71.2209
	step [106/192], loss=82.9938
	step [107/192], loss=49.1448
	step [108/192], loss=72.3176
	step [109/192], loss=76.7892
	step [110/192], loss=70.1943
	step [111/192], loss=62.4678
	step [112/192], loss=65.2332
	step [113/192], loss=67.5175
	step [114/192], loss=72.0027
	step [115/192], loss=63.8632
	step [116/192], loss=72.4956
	step [117/192], loss=68.8914
	step [118/192], loss=63.2524
	step [119/192], loss=66.6936
	step [120/192], loss=60.6720
	step [121/192], loss=70.8827
	step [122/192], loss=72.3794
	step [123/192], loss=59.0875
	step [124/192], loss=64.6081
	step [125/192], loss=61.4314
	step [126/192], loss=69.1444
	step [127/192], loss=71.8573
	step [128/192], loss=69.7914
	step [129/192], loss=62.2035
	step [130/192], loss=64.2454
	step [131/192], loss=64.3157
	step [132/192], loss=70.7814
	step [133/192], loss=65.1552
	step [134/192], loss=69.4039
	step [135/192], loss=77.1741
	step [136/192], loss=71.0608
	step [137/192], loss=66.3857
	step [138/192], loss=63.8061
	step [139/192], loss=62.3647
	step [140/192], loss=71.8798
	step [141/192], loss=58.2988
	step [142/192], loss=81.0385
	step [143/192], loss=67.4757
	step [144/192], loss=56.0075
	step [145/192], loss=68.4969
	step [146/192], loss=76.4965
	step [147/192], loss=67.0214
	step [148/192], loss=52.7813
	step [149/192], loss=72.3283
	step [150/192], loss=57.5565
	step [151/192], loss=67.7219
	step [152/192], loss=80.2909
	step [153/192], loss=73.9554
	step [154/192], loss=65.6064
	step [155/192], loss=70.3608
	step [156/192], loss=62.5237
	step [157/192], loss=67.5902
	step [158/192], loss=81.8475
	step [159/192], loss=74.4460
	step [160/192], loss=77.9370
	step [161/192], loss=67.7498
	step [162/192], loss=71.1971
	step [163/192], loss=71.4011
	step [164/192], loss=67.1501
	step [165/192], loss=63.9571
	step [166/192], loss=68.6201
	step [167/192], loss=71.1886
	step [168/192], loss=65.5638
	step [169/192], loss=64.3168
	step [170/192], loss=56.9357
	step [171/192], loss=60.5989
	step [172/192], loss=71.3905
	step [173/192], loss=75.5798
	step [174/192], loss=62.5810
	step [175/192], loss=54.1899
	step [176/192], loss=61.6364
	step [177/192], loss=70.3428
	step [178/192], loss=61.7253
	step [179/192], loss=64.8041
	step [180/192], loss=77.7477
	step [181/192], loss=73.2452
	step [182/192], loss=57.7729
	step [183/192], loss=59.0749
	step [184/192], loss=63.7682
	step [185/192], loss=65.6425
	step [186/192], loss=65.3418
	step [187/192], loss=75.4282
	step [188/192], loss=64.5391
	step [189/192], loss=58.9894
	step [190/192], loss=65.0994
	step [191/192], loss=75.0594
	step [192/192], loss=53.6001
	Evaluating
	loss=0.0074, precision=0.3307, recall=0.8675, f1=0.4789
Training epoch 110
	step [1/192], loss=71.8883
	step [2/192], loss=65.5238
	step [3/192], loss=60.8218
	step [4/192], loss=76.0228
	step [5/192], loss=74.0241
	step [6/192], loss=59.4363
	step [7/192], loss=80.4044
	step [8/192], loss=56.6548
	step [9/192], loss=63.9270
	step [10/192], loss=58.9806
	step [11/192], loss=65.3537
	step [12/192], loss=61.9695
	step [13/192], loss=78.6986
	step [14/192], loss=71.3729
	step [15/192], loss=61.0175
	step [16/192], loss=64.1821
	step [17/192], loss=70.0593
	step [18/192], loss=64.6870
	step [19/192], loss=56.4863
	step [20/192], loss=64.4370
	step [21/192], loss=60.5662
	step [22/192], loss=66.2539
	step [23/192], loss=61.4466
	step [24/192], loss=61.7948
	step [25/192], loss=70.9840
	step [26/192], loss=73.9487
	step [27/192], loss=70.3154
	step [28/192], loss=59.4711
	step [29/192], loss=63.7747
	step [30/192], loss=66.9916
	step [31/192], loss=61.5499
	step [32/192], loss=55.3252
	step [33/192], loss=63.7351
	step [34/192], loss=60.4604
	step [35/192], loss=53.5844
	step [36/192], loss=57.1874
	step [37/192], loss=62.1819
	step [38/192], loss=66.8360
	step [39/192], loss=72.0697
	step [40/192], loss=79.0460
	step [41/192], loss=68.1126
	step [42/192], loss=59.9984
	step [43/192], loss=61.5029
	step [44/192], loss=68.0528
	step [45/192], loss=57.2252
	step [46/192], loss=63.5780
	step [47/192], loss=64.8202
	step [48/192], loss=69.2359
	step [49/192], loss=71.8421
	step [50/192], loss=70.3422
	step [51/192], loss=55.0297
	step [52/192], loss=68.5465
	step [53/192], loss=71.4541
	step [54/192], loss=73.3202
	step [55/192], loss=68.6705
	step [56/192], loss=66.4120
	step [57/192], loss=67.4381
	step [58/192], loss=73.2413
	step [59/192], loss=64.5545
	step [60/192], loss=62.8648
	step [61/192], loss=61.1178
	step [62/192], loss=76.5613
	step [63/192], loss=69.7793
	step [64/192], loss=64.3580
	step [65/192], loss=72.6231
	step [66/192], loss=59.3659
	step [67/192], loss=60.5417
	step [68/192], loss=68.3180
	step [69/192], loss=51.9747
	step [70/192], loss=78.0356
	step [71/192], loss=66.2599
	step [72/192], loss=86.3378
	step [73/192], loss=61.7277
	step [74/192], loss=71.2792
	step [75/192], loss=69.5114
	step [76/192], loss=61.7145
	step [77/192], loss=70.8450
	step [78/192], loss=59.0915
	step [79/192], loss=73.2289
	step [80/192], loss=56.8830
	step [81/192], loss=63.5976
	step [82/192], loss=65.8752
	step [83/192], loss=77.2678
	step [84/192], loss=66.8321
	step [85/192], loss=73.2026
	step [86/192], loss=70.8877
	step [87/192], loss=59.3080
	step [88/192], loss=64.9546
	step [89/192], loss=69.4846
	step [90/192], loss=69.5003
	step [91/192], loss=69.0465
	step [92/192], loss=72.2848
	step [93/192], loss=68.1287
	step [94/192], loss=64.9714
	step [95/192], loss=68.9665
	step [96/192], loss=75.8300
	step [97/192], loss=67.3690
	step [98/192], loss=62.0563
	step [99/192], loss=61.3543
	step [100/192], loss=68.7612
	step [101/192], loss=68.1146
	step [102/192], loss=77.5718
	step [103/192], loss=63.2996
	step [104/192], loss=67.2033
	step [105/192], loss=63.9601
	step [106/192], loss=63.8909
	step [107/192], loss=70.1697
	step [108/192], loss=64.1368
	step [109/192], loss=63.5277
	step [110/192], loss=62.6963
	step [111/192], loss=60.4461
	step [112/192], loss=80.7389
	step [113/192], loss=77.9927
	step [114/192], loss=65.0938
	step [115/192], loss=66.0149
	step [116/192], loss=68.1187
	step [117/192], loss=64.7331
	step [118/192], loss=67.3335
	step [119/192], loss=76.9379
	step [120/192], loss=83.0454
	step [121/192], loss=82.6586
	step [122/192], loss=59.7862
	step [123/192], loss=74.8122
	step [124/192], loss=67.4136
	step [125/192], loss=74.5649
	step [126/192], loss=60.3373
	step [127/192], loss=61.8050
	step [128/192], loss=75.4229
	step [129/192], loss=68.4122
	step [130/192], loss=71.8688
	step [131/192], loss=63.0965
	step [132/192], loss=69.8751
	step [133/192], loss=51.6791
	step [134/192], loss=62.9079
	step [135/192], loss=80.9978
	step [136/192], loss=66.2999
	step [137/192], loss=58.8194
	step [138/192], loss=68.9704
	step [139/192], loss=64.6830
	step [140/192], loss=64.3993
	step [141/192], loss=67.3682
	step [142/192], loss=63.3013
	step [143/192], loss=62.4123
	step [144/192], loss=63.5826
	step [145/192], loss=72.7155
	step [146/192], loss=68.8838
	step [147/192], loss=68.2449
	step [148/192], loss=62.9536
	step [149/192], loss=54.7855
	step [150/192], loss=57.9320
	step [151/192], loss=63.4875
	step [152/192], loss=70.2028
	step [153/192], loss=68.0115
	step [154/192], loss=73.8746
	step [155/192], loss=80.5590
	step [156/192], loss=78.6592
	step [157/192], loss=78.7453
	step [158/192], loss=62.0530
	step [159/192], loss=64.8527
	step [160/192], loss=80.4693
	step [161/192], loss=54.4371
	step [162/192], loss=59.1062
	step [163/192], loss=67.5446
	step [164/192], loss=70.8120
	step [165/192], loss=54.8277
	step [166/192], loss=80.2350
	step [167/192], loss=66.5950
	step [168/192], loss=66.0382
	step [169/192], loss=66.6482
	step [170/192], loss=64.3892
	step [171/192], loss=67.6455
	step [172/192], loss=69.7731
	step [173/192], loss=51.9791
	step [174/192], loss=55.2894
	step [175/192], loss=58.6782
	step [176/192], loss=58.1823
	step [177/192], loss=68.7703
	step [178/192], loss=58.0299
	step [179/192], loss=66.1391
	step [180/192], loss=87.9664
	step [181/192], loss=66.4633
	step [182/192], loss=65.3281
	step [183/192], loss=81.7787
	step [184/192], loss=70.7447
	step [185/192], loss=66.9994
	step [186/192], loss=72.1829
	step [187/192], loss=61.2968
	step [188/192], loss=71.9835
	step [189/192], loss=73.7910
	step [190/192], loss=55.9426
	step [191/192], loss=68.1604
	step [192/192], loss=69.7535
	Evaluating
	loss=0.0086, precision=0.2553, recall=0.8629, f1=0.3940
Training epoch 111
	step [1/192], loss=72.1152
	step [2/192], loss=63.7397
	step [3/192], loss=64.0519
	step [4/192], loss=58.4429
	step [5/192], loss=79.9583
	step [6/192], loss=63.4960
	step [7/192], loss=66.5339
	step [8/192], loss=71.3057
	step [9/192], loss=64.2236
	step [10/192], loss=54.3167
	step [11/192], loss=79.8728
	step [12/192], loss=57.4938
	step [13/192], loss=66.3927
	step [14/192], loss=65.1787
	step [15/192], loss=61.5693
	step [16/192], loss=68.6339
	step [17/192], loss=64.1148
	step [18/192], loss=76.4862
	step [19/192], loss=66.5418
	step [20/192], loss=70.5941
	step [21/192], loss=60.1631
	step [22/192], loss=68.8482
	step [23/192], loss=73.6790
	step [24/192], loss=67.2695
	step [25/192], loss=66.1742
	step [26/192], loss=65.5010
	step [27/192], loss=62.3451
	step [28/192], loss=61.9348
	step [29/192], loss=53.4847
	step [30/192], loss=62.1948
	step [31/192], loss=73.7603
	step [32/192], loss=66.0780
	step [33/192], loss=66.0601
	step [34/192], loss=67.7813
	step [35/192], loss=73.7664
	step [36/192], loss=57.1881
	step [37/192], loss=63.4178
	step [38/192], loss=71.9265
	step [39/192], loss=63.1018
	step [40/192], loss=82.1207
	step [41/192], loss=56.6470
	step [42/192], loss=67.8952
	step [43/192], loss=67.8163
	step [44/192], loss=68.4431
	step [45/192], loss=79.2955
	step [46/192], loss=67.9373
	step [47/192], loss=55.3037
	step [48/192], loss=63.7971
	step [49/192], loss=70.1937
	step [50/192], loss=76.4489
	step [51/192], loss=73.8815
	step [52/192], loss=72.7637
	step [53/192], loss=78.6820
	step [54/192], loss=69.8631
	step [55/192], loss=67.3287
	step [56/192], loss=69.2575
	step [57/192], loss=64.5818
	step [58/192], loss=63.4817
	step [59/192], loss=62.0102
	step [60/192], loss=61.9163
	step [61/192], loss=66.1235
	step [62/192], loss=66.1046
	step [63/192], loss=64.6508
	step [64/192], loss=57.7273
	step [65/192], loss=76.6112
	step [66/192], loss=56.8048
	step [67/192], loss=57.4354
	step [68/192], loss=69.0005
	step [69/192], loss=80.3815
	step [70/192], loss=67.2444
	step [71/192], loss=72.4353
	step [72/192], loss=66.2534
	step [73/192], loss=62.6748
	step [74/192], loss=64.9231
	step [75/192], loss=68.5415
	step [76/192], loss=67.7291
	step [77/192], loss=70.1252
	step [78/192], loss=58.5838
	step [79/192], loss=70.3595
	step [80/192], loss=64.7644
	step [81/192], loss=68.3687
	step [82/192], loss=72.6909
	step [83/192], loss=66.8866
	step [84/192], loss=68.8587
	step [85/192], loss=63.0145
	step [86/192], loss=65.5477
	step [87/192], loss=56.4152
	step [88/192], loss=73.0332
	step [89/192], loss=72.6665
	step [90/192], loss=55.7721
	step [91/192], loss=71.8232
	step [92/192], loss=67.3679
	step [93/192], loss=64.7284
	step [94/192], loss=65.7225
	step [95/192], loss=78.6650
	step [96/192], loss=69.0802
	step [97/192], loss=60.4832
	step [98/192], loss=62.2767
	step [99/192], loss=64.0641
	step [100/192], loss=64.7156
	step [101/192], loss=64.2613
	step [102/192], loss=65.1621
	step [103/192], loss=80.8358
	step [104/192], loss=76.0208
	step [105/192], loss=76.5566
	step [106/192], loss=53.3034
	step [107/192], loss=68.8401
	step [108/192], loss=66.7496
	step [109/192], loss=66.5847
	step [110/192], loss=65.0688
	step [111/192], loss=65.7561
	step [112/192], loss=65.5596
	step [113/192], loss=50.9087
	step [114/192], loss=60.8813
	step [115/192], loss=58.0043
	step [116/192], loss=74.1844
	step [117/192], loss=62.7834
	step [118/192], loss=70.7894
	step [119/192], loss=70.5627
	step [120/192], loss=65.3705
	step [121/192], loss=69.7107
	step [122/192], loss=62.3771
	step [123/192], loss=61.9818
	step [124/192], loss=69.5058
	step [125/192], loss=62.5602
	step [126/192], loss=71.5736
	step [127/192], loss=65.8242
	step [128/192], loss=77.9630
	step [129/192], loss=68.0445
	step [130/192], loss=68.2942
	step [131/192], loss=73.9785
	step [132/192], loss=71.7066
	step [133/192], loss=61.5176
	step [134/192], loss=63.1011
	step [135/192], loss=75.6795
	step [136/192], loss=61.3770
	step [137/192], loss=59.6317
	step [138/192], loss=72.9285
	step [139/192], loss=60.7247
	step [140/192], loss=64.7497
	step [141/192], loss=71.0125
	step [142/192], loss=73.5804
	step [143/192], loss=67.7900
	step [144/192], loss=61.8783
	step [145/192], loss=62.3257
	step [146/192], loss=71.6872
	step [147/192], loss=65.2996
	step [148/192], loss=67.3210
	step [149/192], loss=72.6802
	step [150/192], loss=64.0007
	step [151/192], loss=65.8109
	step [152/192], loss=77.9234
	step [153/192], loss=71.3231
	step [154/192], loss=63.7362
	step [155/192], loss=68.8456
	step [156/192], loss=70.3566
	step [157/192], loss=71.8638
	step [158/192], loss=61.5703
	step [159/192], loss=72.0695
	step [160/192], loss=54.9357
	step [161/192], loss=67.8638
	step [162/192], loss=71.2793
	step [163/192], loss=59.6218
	step [164/192], loss=62.9376
	step [165/192], loss=66.9828
	step [166/192], loss=59.9068
	step [167/192], loss=78.6356
	step [168/192], loss=62.0165
	step [169/192], loss=63.1749
	step [170/192], loss=80.7364
	step [171/192], loss=60.5866
	step [172/192], loss=68.9849
	step [173/192], loss=77.9709
	step [174/192], loss=77.4535
	step [175/192], loss=77.6269
	step [176/192], loss=60.9158
	step [177/192], loss=63.7884
	step [178/192], loss=62.7165
	step [179/192], loss=72.6907
	step [180/192], loss=68.3341
	step [181/192], loss=75.8360
	step [182/192], loss=68.2761
	step [183/192], loss=52.6402
	step [184/192], loss=71.5357
	step [185/192], loss=65.4595
	step [186/192], loss=64.4769
	step [187/192], loss=60.6996
	step [188/192], loss=72.9560
	step [189/192], loss=66.9837
	step [190/192], loss=73.5602
	step [191/192], loss=74.6388
	step [192/192], loss=50.2223
	Evaluating
	loss=0.0073, precision=0.3656, recall=0.8722, f1=0.5153
Training epoch 112
	step [1/192], loss=68.1721
	step [2/192], loss=63.9863
	step [3/192], loss=77.1002
	step [4/192], loss=71.3907
	step [5/192], loss=65.5295
	step [6/192], loss=60.4020
	step [7/192], loss=67.4945
	step [8/192], loss=68.2643
	step [9/192], loss=65.0960
	step [10/192], loss=61.6755
	step [11/192], loss=50.6656
	step [12/192], loss=72.7039
	step [13/192], loss=56.2229
	step [14/192], loss=70.6442
	step [15/192], loss=65.8840
	step [16/192], loss=61.0901
	step [17/192], loss=62.8685
	step [18/192], loss=60.2682
	step [19/192], loss=83.5364
	step [20/192], loss=73.2879
	step [21/192], loss=73.9791
	step [22/192], loss=58.1457
	step [23/192], loss=57.7835
	step [24/192], loss=72.1596
	step [25/192], loss=65.7872
	step [26/192], loss=65.6865
	step [27/192], loss=66.6344
	step [28/192], loss=58.9125
	step [29/192], loss=72.4871
	step [30/192], loss=68.2201
	step [31/192], loss=61.3310
	step [32/192], loss=67.1901
	step [33/192], loss=67.3245
	step [34/192], loss=66.2088
	step [35/192], loss=72.6028
	step [36/192], loss=63.1385
	step [37/192], loss=65.7241
	step [38/192], loss=58.5240
	step [39/192], loss=62.2341
	step [40/192], loss=70.7044
	step [41/192], loss=66.8796
	step [42/192], loss=76.5002
	step [43/192], loss=68.3170
	step [44/192], loss=62.8635
	step [45/192], loss=64.9548
	step [46/192], loss=61.1346
	step [47/192], loss=78.1819
	step [48/192], loss=75.5187
	step [49/192], loss=67.6636
	step [50/192], loss=77.2311
	step [51/192], loss=66.1198
	step [52/192], loss=64.3783
	step [53/192], loss=73.5584
	step [54/192], loss=60.3161
	step [55/192], loss=60.2342
	step [56/192], loss=75.7944
	step [57/192], loss=73.9654
	step [58/192], loss=59.3226
	step [59/192], loss=75.9551
	step [60/192], loss=82.3128
	step [61/192], loss=65.6376
	step [62/192], loss=73.4651
	step [63/192], loss=64.1609
	step [64/192], loss=62.2829
	step [65/192], loss=68.5179
	step [66/192], loss=69.0846
	step [67/192], loss=73.4194
	step [68/192], loss=60.1872
	step [69/192], loss=67.5308
	step [70/192], loss=62.5523
	step [71/192], loss=61.8222
	step [72/192], loss=59.9767
	step [73/192], loss=71.5128
	step [74/192], loss=64.5671
	step [75/192], loss=69.7548
	step [76/192], loss=73.3421
	step [77/192], loss=79.7231
	step [78/192], loss=62.6778
	step [79/192], loss=70.4670
	step [80/192], loss=63.1952
	step [81/192], loss=65.0635
	step [82/192], loss=74.1358
	step [83/192], loss=55.8604
	step [84/192], loss=49.2598
	step [85/192], loss=68.5400
	step [86/192], loss=79.6865
	step [87/192], loss=65.6002
	step [88/192], loss=65.2952
	step [89/192], loss=68.5291
	step [90/192], loss=67.3462
	step [91/192], loss=54.6353
	step [92/192], loss=78.6695
	step [93/192], loss=63.6911
	step [94/192], loss=65.8970
	step [95/192], loss=65.8335
	step [96/192], loss=63.9550
	step [97/192], loss=65.9360
	step [98/192], loss=70.2225
	step [99/192], loss=55.7842
	step [100/192], loss=70.8664
	step [101/192], loss=63.7795
	step [102/192], loss=76.8985
	step [103/192], loss=63.1105
	step [104/192], loss=58.0604
	step [105/192], loss=87.4186
	step [106/192], loss=74.0022
	step [107/192], loss=64.6125
	step [108/192], loss=68.3966
	step [109/192], loss=76.4679
	step [110/192], loss=64.8793
	step [111/192], loss=61.7745
	step [112/192], loss=60.9022
	step [113/192], loss=78.5768
	step [114/192], loss=63.7845
	step [115/192], loss=61.8294
	step [116/192], loss=69.8332
	step [117/192], loss=72.2481
	step [118/192], loss=74.4527
	step [119/192], loss=75.1437
	step [120/192], loss=65.5993
	step [121/192], loss=56.2835
	step [122/192], loss=71.2005
	step [123/192], loss=71.9538
	step [124/192], loss=66.5100
	step [125/192], loss=68.4805
	step [126/192], loss=73.4313
	step [127/192], loss=80.2468
	step [128/192], loss=63.2737
	step [129/192], loss=62.8455
	step [130/192], loss=66.8410
	step [131/192], loss=67.5237
	step [132/192], loss=67.3928
	step [133/192], loss=74.5701
	step [134/192], loss=65.8212
	step [135/192], loss=62.9153
	step [136/192], loss=61.4572
	step [137/192], loss=56.5475
	step [138/192], loss=82.8752
	step [139/192], loss=65.2212
	step [140/192], loss=58.2965
	step [141/192], loss=72.8285
	step [142/192], loss=70.7393
	step [143/192], loss=74.7933
	step [144/192], loss=70.7224
	step [145/192], loss=62.7996
	step [146/192], loss=71.6146
	step [147/192], loss=74.9996
	step [148/192], loss=77.2278
	step [149/192], loss=56.9987
	step [150/192], loss=67.9295
	step [151/192], loss=59.4266
	step [152/192], loss=67.0303
	step [153/192], loss=60.0600
	step [154/192], loss=64.5619
	step [155/192], loss=64.6321
	step [156/192], loss=60.2492
	step [157/192], loss=59.9258
	step [158/192], loss=59.2660
	step [159/192], loss=57.8253
	step [160/192], loss=72.4158
	step [161/192], loss=66.5436
	step [162/192], loss=71.3942
	step [163/192], loss=65.9194
	step [164/192], loss=61.6586
	step [165/192], loss=58.9470
	step [166/192], loss=57.4419
	step [167/192], loss=65.6371
	step [168/192], loss=74.6363
	step [169/192], loss=64.3148
	step [170/192], loss=65.3706
	step [171/192], loss=65.8377
	step [172/192], loss=67.5665
	step [173/192], loss=67.4841
	step [174/192], loss=59.5575
	step [175/192], loss=60.9506
	step [176/192], loss=77.6814
	step [177/192], loss=67.3545
	step [178/192], loss=66.7953
	step [179/192], loss=69.4619
	step [180/192], loss=65.1956
	step [181/192], loss=54.8741
	step [182/192], loss=62.9957
	step [183/192], loss=66.6132
	step [184/192], loss=68.2912
	step [185/192], loss=63.2539
	step [186/192], loss=58.0950
	step [187/192], loss=72.4603
	step [188/192], loss=65.4060
	step [189/192], loss=54.9037
	step [190/192], loss=67.2253
	step [191/192], loss=54.0843
	step [192/192], loss=56.0154
	Evaluating
	loss=0.0060, precision=0.4173, recall=0.8661, f1=0.5633
Training epoch 113
	step [1/192], loss=62.8595
	step [2/192], loss=60.5563
	step [3/192], loss=69.0310
	step [4/192], loss=57.2397
	step [5/192], loss=60.0410
	step [6/192], loss=70.4583
	step [7/192], loss=66.5608
	step [8/192], loss=57.6679
	step [9/192], loss=66.4393
	step [10/192], loss=71.1205
	step [11/192], loss=63.7207
	step [12/192], loss=58.1678
	step [13/192], loss=73.6636
	step [14/192], loss=60.6688
	step [15/192], loss=71.8808
	step [16/192], loss=68.7270
	step [17/192], loss=66.7964
	step [18/192], loss=68.5588
	step [19/192], loss=68.0483
	step [20/192], loss=53.3184
	step [21/192], loss=57.7538
	step [22/192], loss=76.8372
	step [23/192], loss=74.2368
	step [24/192], loss=57.1693
	step [25/192], loss=65.8272
	step [26/192], loss=64.9845
	step [27/192], loss=71.8013
	step [28/192], loss=66.7149
	step [29/192], loss=65.8164
	step [30/192], loss=55.0947
	step [31/192], loss=62.0309
	step [32/192], loss=82.0889
	step [33/192], loss=78.7902
	step [34/192], loss=67.5447
	step [35/192], loss=61.1807
	step [36/192], loss=53.3268
	step [37/192], loss=70.8464
	step [38/192], loss=63.3372
	step [39/192], loss=68.8914
	step [40/192], loss=70.3963
	step [41/192], loss=63.5687
	step [42/192], loss=57.2661
	step [43/192], loss=64.7692
	step [44/192], loss=61.5429
	step [45/192], loss=71.2879
	step [46/192], loss=66.3990
	step [47/192], loss=70.9360
	step [48/192], loss=73.9231
	step [49/192], loss=63.6424
	step [50/192], loss=64.5372
	step [51/192], loss=59.6448
	step [52/192], loss=70.0086
	step [53/192], loss=52.5282
	step [54/192], loss=73.9793
	step [55/192], loss=54.9850
	step [56/192], loss=55.7227
	step [57/192], loss=52.7816
	step [58/192], loss=61.2947
	step [59/192], loss=76.2787
	step [60/192], loss=66.8640
	step [61/192], loss=62.5923
	step [62/192], loss=74.4320
	step [63/192], loss=62.5904
	step [64/192], loss=80.2650
	step [65/192], loss=67.6063
	step [66/192], loss=68.3758
	step [67/192], loss=63.2400
	step [68/192], loss=61.9353
	step [69/192], loss=65.4339
	step [70/192], loss=64.3796
	step [71/192], loss=72.3785
	step [72/192], loss=71.7759
	step [73/192], loss=62.7898
	step [74/192], loss=62.7408
	step [75/192], loss=67.8792
	step [76/192], loss=55.1551
	step [77/192], loss=69.8940
	step [78/192], loss=57.8702
	step [79/192], loss=64.6945
	step [80/192], loss=74.0399
	step [81/192], loss=60.6237
	step [82/192], loss=69.8017
	step [83/192], loss=56.2828
	step [84/192], loss=67.6964
	step [85/192], loss=62.8415
	step [86/192], loss=66.7027
	step [87/192], loss=69.2264
	step [88/192], loss=73.5572
	step [89/192], loss=61.3137
	step [90/192], loss=66.4807
	step [91/192], loss=84.0147
	step [92/192], loss=75.3592
	step [93/192], loss=67.6210
	step [94/192], loss=66.3974
	step [95/192], loss=68.0472
	step [96/192], loss=53.5598
	step [97/192], loss=64.9711
	step [98/192], loss=63.7354
	step [99/192], loss=70.0066
	step [100/192], loss=66.6927
	step [101/192], loss=62.1930
	step [102/192], loss=65.4527
	step [103/192], loss=59.2138
	step [104/192], loss=60.8314
	step [105/192], loss=62.1133
	step [106/192], loss=64.7102
	step [107/192], loss=66.0471
	step [108/192], loss=70.8671
	step [109/192], loss=70.3419
	step [110/192], loss=76.9597
	step [111/192], loss=67.6343
	step [112/192], loss=75.6785
	step [113/192], loss=65.9499
	step [114/192], loss=77.6651
	step [115/192], loss=68.0129
	step [116/192], loss=69.6666
	step [117/192], loss=66.6943
	step [118/192], loss=48.5339
	step [119/192], loss=69.7933
	step [120/192], loss=64.3711
	step [121/192], loss=72.4288
	step [122/192], loss=76.9740
	step [123/192], loss=62.9785
	step [124/192], loss=65.4072
	step [125/192], loss=68.2771
	step [126/192], loss=72.4840
	step [127/192], loss=72.6517
	step [128/192], loss=60.8144
	step [129/192], loss=62.6335
	step [130/192], loss=70.3163
	step [131/192], loss=62.3945
	step [132/192], loss=58.7760
	step [133/192], loss=57.6527
	step [134/192], loss=68.0331
	step [135/192], loss=79.2148
	step [136/192], loss=66.7376
	step [137/192], loss=57.6202
	step [138/192], loss=73.5795
	step [139/192], loss=83.1404
	step [140/192], loss=75.9303
	step [141/192], loss=53.2454
	step [142/192], loss=60.7854
	step [143/192], loss=71.7818
	step [144/192], loss=75.2999
	step [145/192], loss=63.6793
	step [146/192], loss=61.8383
	step [147/192], loss=80.7745
	step [148/192], loss=57.4249
	step [149/192], loss=64.2571
	step [150/192], loss=72.6973
	step [151/192], loss=73.4442
	step [152/192], loss=74.7970
	step [153/192], loss=63.6701
	step [154/192], loss=64.5766
	step [155/192], loss=59.7797
	step [156/192], loss=71.7043
	step [157/192], loss=52.9141
	step [158/192], loss=59.8082
	step [159/192], loss=79.0858
	step [160/192], loss=68.0284
	step [161/192], loss=72.7258
	step [162/192], loss=64.5587
	step [163/192], loss=62.3848
	step [164/192], loss=74.8010
	step [165/192], loss=70.3371
	step [166/192], loss=59.5088
	step [167/192], loss=72.7096
	step [168/192], loss=69.3157
	step [169/192], loss=62.8274
	step [170/192], loss=70.7513
	step [171/192], loss=59.4374
	step [172/192], loss=72.0702
	step [173/192], loss=61.8850
	step [174/192], loss=68.7354
	step [175/192], loss=71.9969
	step [176/192], loss=85.4996
	step [177/192], loss=77.6790
	step [178/192], loss=60.0443
	step [179/192], loss=70.7558
	step [180/192], loss=66.7396
	step [181/192], loss=74.5183
	step [182/192], loss=65.6634
	step [183/192], loss=62.4184
	step [184/192], loss=68.8420
	step [185/192], loss=67.8826
	step [186/192], loss=66.5958
	step [187/192], loss=70.9703
	step [188/192], loss=54.9875
	step [189/192], loss=69.5314
	step [190/192], loss=66.0243
	step [191/192], loss=61.3586
	step [192/192], loss=65.4197
	Evaluating
	loss=0.0069, precision=0.3670, recall=0.8809, f1=0.5181
Training epoch 114
	step [1/192], loss=72.5123
	step [2/192], loss=60.7668
	step [3/192], loss=68.0044
	step [4/192], loss=65.3977
	step [5/192], loss=59.3884
	step [6/192], loss=82.2718
	step [7/192], loss=64.2042
	step [8/192], loss=56.5618
	step [9/192], loss=66.9097
	step [10/192], loss=61.4100
	step [11/192], loss=72.1338
	step [12/192], loss=74.0557
	step [13/192], loss=78.6079
	step [14/192], loss=55.0439
	step [15/192], loss=75.8283
	step [16/192], loss=61.1143
	step [17/192], loss=67.4307
	step [18/192], loss=83.2408
	step [19/192], loss=59.7581
	step [20/192], loss=54.7263
	step [21/192], loss=59.2044
	step [22/192], loss=59.4458
	step [23/192], loss=78.1977
	step [24/192], loss=74.0998
	step [25/192], loss=64.6173
	step [26/192], loss=69.4751
	step [27/192], loss=70.0908
	step [28/192], loss=70.8384
	step [29/192], loss=75.3158
	step [30/192], loss=78.7253
	step [31/192], loss=68.7027
	step [32/192], loss=66.9985
	step [33/192], loss=62.4340
	step [34/192], loss=59.3961
	step [35/192], loss=63.8877
	step [36/192], loss=68.6934
	step [37/192], loss=80.7477
	step [38/192], loss=67.2208
	step [39/192], loss=55.3021
	step [40/192], loss=66.7167
	step [41/192], loss=71.2518
	step [42/192], loss=56.2219
	step [43/192], loss=59.0205
	step [44/192], loss=68.0336
	step [45/192], loss=69.9930
	step [46/192], loss=61.6221
	step [47/192], loss=65.6629
	step [48/192], loss=65.2326
	step [49/192], loss=68.9502
	step [50/192], loss=61.9339
	step [51/192], loss=62.9161
	step [52/192], loss=65.8201
	step [53/192], loss=56.2480
	step [54/192], loss=68.0029
	step [55/192], loss=71.8431
	step [56/192], loss=72.9038
	step [57/192], loss=70.5080
	step [58/192], loss=58.7537
	step [59/192], loss=72.5397
	step [60/192], loss=68.2271
	step [61/192], loss=68.7336
	step [62/192], loss=65.4252
	step [63/192], loss=69.9777
	step [64/192], loss=59.2552
	step [65/192], loss=66.2225
	step [66/192], loss=63.6915
	step [67/192], loss=65.2906
	step [68/192], loss=60.7583
	step [69/192], loss=55.4649
	step [70/192], loss=60.5092
	step [71/192], loss=58.6302
	step [72/192], loss=65.4906
	step [73/192], loss=76.4070
	step [74/192], loss=55.4766
	step [75/192], loss=59.9995
	step [76/192], loss=72.9318
	step [77/192], loss=69.5561
	step [78/192], loss=63.8499
	step [79/192], loss=72.8938
	step [80/192], loss=63.1558
	step [81/192], loss=75.0758
	step [82/192], loss=58.0758
	step [83/192], loss=63.9707
	step [84/192], loss=64.8014
	step [85/192], loss=67.8374
	step [86/192], loss=72.7325
	step [87/192], loss=69.2110
	step [88/192], loss=63.7652
	step [89/192], loss=66.0573
	step [90/192], loss=66.1485
	step [91/192], loss=62.6561
	step [92/192], loss=68.0035
	step [93/192], loss=73.1978
	step [94/192], loss=59.8247
	step [95/192], loss=81.7519
	step [96/192], loss=66.4898
	step [97/192], loss=60.3886
	step [98/192], loss=68.5529
	step [99/192], loss=68.5174
	step [100/192], loss=70.8594
	step [101/192], loss=64.5721
	step [102/192], loss=65.0588
	step [103/192], loss=61.2782
	step [104/192], loss=65.9827
	step [105/192], loss=70.8435
	step [106/192], loss=63.3738
	step [107/192], loss=74.0578
	step [108/192], loss=66.1380
	step [109/192], loss=60.5826
	step [110/192], loss=69.4989
	step [111/192], loss=58.8102
	step [112/192], loss=51.5059
	step [113/192], loss=64.3803
	step [114/192], loss=55.8293
	step [115/192], loss=62.0858
	step [116/192], loss=66.0899
	step [117/192], loss=59.3324
	step [118/192], loss=73.4658
	step [119/192], loss=72.8667
	step [120/192], loss=69.7080
	step [121/192], loss=70.0757
	step [122/192], loss=65.1753
	step [123/192], loss=68.7756
	step [124/192], loss=66.0173
	step [125/192], loss=75.5243
	step [126/192], loss=61.1410
	step [127/192], loss=56.3581
	step [128/192], loss=56.7805
	step [129/192], loss=62.8424
	step [130/192], loss=70.0979
	step [131/192], loss=76.1237
	step [132/192], loss=64.9454
	step [133/192], loss=60.0880
	step [134/192], loss=69.9142
	step [135/192], loss=79.5221
	step [136/192], loss=70.3528
	step [137/192], loss=59.8997
	step [138/192], loss=64.9172
	step [139/192], loss=70.1506
	step [140/192], loss=74.8395
	step [141/192], loss=67.6179
	step [142/192], loss=65.0510
	step [143/192], loss=69.0980
	step [144/192], loss=55.0445
	step [145/192], loss=65.3204
	step [146/192], loss=71.3038
	step [147/192], loss=63.0980
	step [148/192], loss=74.5467
	step [149/192], loss=63.5246
	step [150/192], loss=70.1447
	step [151/192], loss=70.9331
	step [152/192], loss=60.9211
	step [153/192], loss=84.4937
	step [154/192], loss=71.4532
	step [155/192], loss=71.4979
	step [156/192], loss=56.2849
	step [157/192], loss=71.2936
	step [158/192], loss=63.8188
	step [159/192], loss=50.7788
	step [160/192], loss=73.4113
	step [161/192], loss=66.1298
	step [162/192], loss=59.0636
	step [163/192], loss=62.0617
	step [164/192], loss=63.9264
	step [165/192], loss=62.6706
	step [166/192], loss=65.9180
	step [167/192], loss=60.3519
	step [168/192], loss=71.0365
	step [169/192], loss=66.6916
	step [170/192], loss=57.8406
	step [171/192], loss=71.5034
	step [172/192], loss=72.5118
	step [173/192], loss=69.8930
	step [174/192], loss=66.7258
	step [175/192], loss=67.8194
	step [176/192], loss=64.8223
	step [177/192], loss=70.4791
	step [178/192], loss=65.4965
	step [179/192], loss=66.6070
	step [180/192], loss=63.8318
	step [181/192], loss=59.3271
	step [182/192], loss=67.6142
	step [183/192], loss=62.0722
	step [184/192], loss=52.5894
	step [185/192], loss=61.6267
	step [186/192], loss=59.5512
	step [187/192], loss=63.6051
	step [188/192], loss=60.6358
	step [189/192], loss=76.7385
	step [190/192], loss=71.7055
	step [191/192], loss=73.3431
	step [192/192], loss=63.3375
	Evaluating
	loss=0.0066, precision=0.3632, recall=0.8714, f1=0.5127
Training epoch 115
	step [1/192], loss=55.2790
	step [2/192], loss=60.4172
	step [3/192], loss=68.8232
	step [4/192], loss=63.1679
	step [5/192], loss=62.4536
	step [6/192], loss=66.1456
	step [7/192], loss=57.2654
	step [8/192], loss=75.7933
	step [9/192], loss=63.7794
	step [10/192], loss=73.2726
	step [11/192], loss=63.7685
	step [12/192], loss=73.7047
	step [13/192], loss=67.5488
	step [14/192], loss=60.2905
	step [15/192], loss=63.1202
	step [16/192], loss=79.6672
	step [17/192], loss=82.3878
	step [18/192], loss=68.8360
	step [19/192], loss=59.4119
	step [20/192], loss=66.8671
	step [21/192], loss=62.3069
	step [22/192], loss=64.4317
	step [23/192], loss=76.2374
	step [24/192], loss=69.2236
	step [25/192], loss=56.3417
	step [26/192], loss=70.8751
	step [27/192], loss=66.8344
	step [28/192], loss=76.7579
	step [29/192], loss=64.7728
	step [30/192], loss=62.9202
	step [31/192], loss=60.9347
	step [32/192], loss=55.4246
	step [33/192], loss=63.9415
	step [34/192], loss=65.8596
	step [35/192], loss=67.3113
	step [36/192], loss=63.0391
	step [37/192], loss=64.7628
	step [38/192], loss=63.5541
	step [39/192], loss=69.7026
	step [40/192], loss=78.0861
	step [41/192], loss=81.9838
	step [42/192], loss=63.6697
	step [43/192], loss=68.8774
	step [44/192], loss=57.1018
	step [45/192], loss=63.6624
	step [46/192], loss=58.8807
	step [47/192], loss=76.5820
	step [48/192], loss=71.2837
	step [49/192], loss=63.7162
	step [50/192], loss=57.5891
	step [51/192], loss=71.3547
	step [52/192], loss=62.4598
	step [53/192], loss=77.9626
	step [54/192], loss=63.3559
	step [55/192], loss=80.2678
	step [56/192], loss=53.0949
	step [57/192], loss=60.1748
	step [58/192], loss=66.9120
	step [59/192], loss=65.4284
	step [60/192], loss=61.7022
	step [61/192], loss=67.7055
	step [62/192], loss=72.9158
	step [63/192], loss=56.2245
	step [64/192], loss=51.9291
	step [65/192], loss=57.0117
	step [66/192], loss=68.8148
	step [67/192], loss=62.5338
	step [68/192], loss=69.1007
	step [69/192], loss=72.3432
	step [70/192], loss=62.2759
	step [71/192], loss=70.2348
	step [72/192], loss=60.7521
	step [73/192], loss=63.4573
	step [74/192], loss=57.2170
	step [75/192], loss=66.1086
	step [76/192], loss=72.0980
	step [77/192], loss=60.1027
	step [78/192], loss=85.7776
	step [79/192], loss=63.8052
	step [80/192], loss=65.9947
	step [81/192], loss=54.5658
	step [82/192], loss=70.4141
	step [83/192], loss=58.4076
	step [84/192], loss=72.3647
	step [85/192], loss=68.8626
	step [86/192], loss=59.4977
	step [87/192], loss=72.6974
	step [88/192], loss=65.6639
	step [89/192], loss=77.0603
	step [90/192], loss=59.4140
	step [91/192], loss=74.6617
	step [92/192], loss=75.3008
	step [93/192], loss=67.3937
	step [94/192], loss=63.7600
	step [95/192], loss=59.4567
	step [96/192], loss=63.1996
	step [97/192], loss=55.0952
	step [98/192], loss=61.3365
	step [99/192], loss=71.6183
	step [100/192], loss=63.3244
	step [101/192], loss=60.7502
	step [102/192], loss=57.8681
	step [103/192], loss=71.6382
	step [104/192], loss=68.3238
	step [105/192], loss=63.8652
	step [106/192], loss=60.7275
	step [107/192], loss=83.2110
	step [108/192], loss=64.4060
	step [109/192], loss=64.3333
	step [110/192], loss=64.8184
	step [111/192], loss=58.2063
	step [112/192], loss=70.9556
	step [113/192], loss=60.0427
	step [114/192], loss=71.0237
	step [115/192], loss=69.0836
	step [116/192], loss=69.2992
	step [117/192], loss=71.2202
	step [118/192], loss=74.2054
	step [119/192], loss=65.8631
	step [120/192], loss=81.6592
	step [121/192], loss=77.2192
	step [122/192], loss=59.0090
	step [123/192], loss=71.1997
	step [124/192], loss=70.2374
	step [125/192], loss=67.6741
	step [126/192], loss=57.1673
	step [127/192], loss=79.0733
	step [128/192], loss=76.5103
	step [129/192], loss=69.3213
	step [130/192], loss=72.2553
	step [131/192], loss=61.3230
	step [132/192], loss=51.7598
	step [133/192], loss=78.0487
	step [134/192], loss=64.2949
	step [135/192], loss=70.8764
	step [136/192], loss=66.7766
	step [137/192], loss=65.9325
	step [138/192], loss=70.0880
	step [139/192], loss=69.5005
	step [140/192], loss=71.7881
	step [141/192], loss=75.7196
	step [142/192], loss=69.1854
	step [143/192], loss=70.5217
	step [144/192], loss=64.7146
	step [145/192], loss=61.2900
	step [146/192], loss=69.7470
	step [147/192], loss=70.8946
	step [148/192], loss=63.8327
	step [149/192], loss=65.5206
	step [150/192], loss=65.3548
	step [151/192], loss=59.7397
	step [152/192], loss=65.1190
	step [153/192], loss=61.9912
	step [154/192], loss=62.3410
	step [155/192], loss=69.4847
	step [156/192], loss=69.7003
	step [157/192], loss=59.5245
	step [158/192], loss=73.6369
	step [159/192], loss=66.9736
	step [160/192], loss=63.0552
	step [161/192], loss=66.0333
	step [162/192], loss=63.4242
	step [163/192], loss=66.6335
	step [164/192], loss=67.9666
	step [165/192], loss=65.6905
	step [166/192], loss=63.6344
	step [167/192], loss=64.9324
	step [168/192], loss=66.1124
	step [169/192], loss=64.4335
	step [170/192], loss=60.0965
	step [171/192], loss=57.3098
	step [172/192], loss=72.7851
	step [173/192], loss=64.6157
	step [174/192], loss=68.3640
	step [175/192], loss=69.0300
	step [176/192], loss=59.8669
	step [177/192], loss=58.8322
	step [178/192], loss=69.2178
	step [179/192], loss=63.4733
	step [180/192], loss=68.5039
	step [181/192], loss=63.8629
	step [182/192], loss=69.7713
	step [183/192], loss=61.1893
	step [184/192], loss=73.1423
	step [185/192], loss=66.3611
	step [186/192], loss=72.0701
	step [187/192], loss=68.4612
	step [188/192], loss=69.7701
	step [189/192], loss=69.4569
	step [190/192], loss=56.2429
	step [191/192], loss=63.7419
	step [192/192], loss=69.8275
	Evaluating
	loss=0.0069, precision=0.3816, recall=0.8658, f1=0.5297
Training epoch 116
	step [1/192], loss=79.1493
	step [2/192], loss=88.0818
	step [3/192], loss=64.8094
	step [4/192], loss=67.2315
	step [5/192], loss=61.5012
	step [6/192], loss=64.7857
	step [7/192], loss=69.7091
	step [8/192], loss=60.7290
	step [9/192], loss=70.1975
	step [10/192], loss=64.8993
	step [11/192], loss=58.3482
	step [12/192], loss=66.1203
	step [13/192], loss=59.9790
	step [14/192], loss=62.8524
	step [15/192], loss=52.0453
	step [16/192], loss=53.3496
	step [17/192], loss=72.5818
	step [18/192], loss=68.7418
	step [19/192], loss=69.1223
	step [20/192], loss=57.3969
	step [21/192], loss=69.9716
	step [22/192], loss=57.0274
	step [23/192], loss=60.4923
	step [24/192], loss=62.0353
	step [25/192], loss=71.3788
	step [26/192], loss=58.3263
	step [27/192], loss=69.0181
	step [28/192], loss=58.7228
	step [29/192], loss=65.5121
	step [30/192], loss=78.7822
	step [31/192], loss=61.3613
	step [32/192], loss=61.6518
	step [33/192], loss=70.7346
	step [34/192], loss=58.6384
	step [35/192], loss=78.9057
	step [36/192], loss=60.3195
	step [37/192], loss=65.7850
	step [38/192], loss=57.5160
	step [39/192], loss=69.9523
	step [40/192], loss=71.4692
	step [41/192], loss=53.1164
	step [42/192], loss=65.6030
	step [43/192], loss=57.6761
	step [44/192], loss=71.8608
	step [45/192], loss=64.1916
	step [46/192], loss=71.0319
	step [47/192], loss=57.1950
	step [48/192], loss=69.4286
	step [49/192], loss=69.7545
	step [50/192], loss=62.6642
	step [51/192], loss=60.8804
	step [52/192], loss=68.1853
	step [53/192], loss=67.4752
	step [54/192], loss=69.1476
	step [55/192], loss=63.0103
	step [56/192], loss=63.2523
	step [57/192], loss=56.2762
	step [58/192], loss=81.4625
	step [59/192], loss=71.6040
	step [60/192], loss=65.0637
	step [61/192], loss=74.8260
	step [62/192], loss=56.0626
	step [63/192], loss=64.2771
	step [64/192], loss=64.3533
	step [65/192], loss=58.0802
	step [66/192], loss=65.8097
	step [67/192], loss=79.7943
	step [68/192], loss=65.4001
	step [69/192], loss=57.8983
	step [70/192], loss=77.2863
	step [71/192], loss=78.7095
	step [72/192], loss=67.6648
	step [73/192], loss=57.6365
	step [74/192], loss=62.3879
	step [75/192], loss=79.7561
	step [76/192], loss=73.0663
	step [77/192], loss=54.8871
	step [78/192], loss=59.2766
	step [79/192], loss=75.8616
	step [80/192], loss=67.4103
	step [81/192], loss=60.1556
	step [82/192], loss=60.6074
	step [83/192], loss=70.0249
	step [84/192], loss=61.2329
	step [85/192], loss=70.3398
	step [86/192], loss=72.3855
	step [87/192], loss=66.3447
	step [88/192], loss=58.9893
	step [89/192], loss=63.3505
	step [90/192], loss=72.6866
	step [91/192], loss=69.7588
	step [92/192], loss=60.9547
	step [93/192], loss=72.3024
	step [94/192], loss=67.8896
	step [95/192], loss=66.1374
	step [96/192], loss=74.1319
	step [97/192], loss=60.1162
	step [98/192], loss=63.5636
	step [99/192], loss=74.6861
	step [100/192], loss=76.4425
	step [101/192], loss=66.1099
	step [102/192], loss=75.6073
	step [103/192], loss=69.5103
	step [104/192], loss=67.2442
	step [105/192], loss=59.8277
	step [106/192], loss=70.3198
	step [107/192], loss=53.7630
	step [108/192], loss=68.0607
	step [109/192], loss=57.5133
	step [110/192], loss=68.2103
	step [111/192], loss=67.3784
	step [112/192], loss=68.7542
	step [113/192], loss=63.3826
	step [114/192], loss=75.8016
	step [115/192], loss=65.7744
	step [116/192], loss=69.7773
	step [117/192], loss=60.6088
	step [118/192], loss=62.5018
	step [119/192], loss=63.2597
	step [120/192], loss=77.7334
	step [121/192], loss=73.6980
	step [122/192], loss=65.5437
	step [123/192], loss=55.9229
	step [124/192], loss=73.4293
	step [125/192], loss=66.6503
	step [126/192], loss=71.7147
	step [127/192], loss=62.5988
	step [128/192], loss=62.8355
	step [129/192], loss=57.6004
	step [130/192], loss=73.1533
	step [131/192], loss=76.5173
	step [132/192], loss=60.1876
	step [133/192], loss=69.9770
	step [134/192], loss=71.2472
	step [135/192], loss=69.4264
	step [136/192], loss=74.3213
	step [137/192], loss=69.9868
	step [138/192], loss=63.7976
	step [139/192], loss=76.3899
	step [140/192], loss=62.6050
	step [141/192], loss=52.7671
	step [142/192], loss=59.8807
	step [143/192], loss=67.9121
	step [144/192], loss=61.1486
	step [145/192], loss=58.2704
	step [146/192], loss=68.1976
	step [147/192], loss=69.2820
	step [148/192], loss=72.8252
	step [149/192], loss=65.6695
	step [150/192], loss=64.7592
	step [151/192], loss=54.0622
	step [152/192], loss=62.5489
	step [153/192], loss=71.4335
	step [154/192], loss=75.8861
	step [155/192], loss=53.7510
	step [156/192], loss=68.5455
	step [157/192], loss=66.3661
	step [158/192], loss=67.5274
	step [159/192], loss=53.5228
	step [160/192], loss=64.6788
	step [161/192], loss=65.5850
	step [162/192], loss=74.3690
	step [163/192], loss=73.0689
	step [164/192], loss=70.0800
	step [165/192], loss=56.2032
	step [166/192], loss=79.6165
	step [167/192], loss=60.1994
	step [168/192], loss=73.0456
	step [169/192], loss=65.6957
	step [170/192], loss=72.0571
	step [171/192], loss=70.9868
	step [172/192], loss=71.3310
	step [173/192], loss=59.7807
	step [174/192], loss=64.8951
	step [175/192], loss=59.8041
	step [176/192], loss=67.2483
	step [177/192], loss=77.9303
	step [178/192], loss=68.3835
	step [179/192], loss=46.8782
	step [180/192], loss=70.3665
	step [181/192], loss=66.0811
	step [182/192], loss=55.4324
	step [183/192], loss=65.1086
	step [184/192], loss=61.4722
	step [185/192], loss=68.5869
	step [186/192], loss=71.7122
	step [187/192], loss=61.1077
	step [188/192], loss=72.8265
	step [189/192], loss=62.7247
	step [190/192], loss=63.1856
	step [191/192], loss=69.2409
	step [192/192], loss=44.4704
	Evaluating
	loss=0.0063, precision=0.3904, recall=0.8716, f1=0.5393
Training epoch 117
	step [1/192], loss=60.9806
	step [2/192], loss=69.2363
	step [3/192], loss=71.3331
	step [4/192], loss=66.5595
	step [5/192], loss=64.1295
	step [6/192], loss=64.9875
	step [7/192], loss=62.7795
	step [8/192], loss=61.1706
	step [9/192], loss=63.9042
	step [10/192], loss=60.9921
	step [11/192], loss=62.4457
	step [12/192], loss=61.7779
	step [13/192], loss=68.2273
	step [14/192], loss=66.2812
	step [15/192], loss=72.8042
	step [16/192], loss=59.4330
	step [17/192], loss=77.4031
	step [18/192], loss=61.8360
	step [19/192], loss=70.3473
	step [20/192], loss=65.0447
	step [21/192], loss=68.2957
	step [22/192], loss=62.6475
	step [23/192], loss=57.3535
	step [24/192], loss=61.0348
	step [25/192], loss=77.6838
	step [26/192], loss=65.2620
	step [27/192], loss=61.0618
	step [28/192], loss=67.4784
	step [29/192], loss=67.7442
	step [30/192], loss=64.9833
	step [31/192], loss=63.6580
	step [32/192], loss=59.1273
	step [33/192], loss=65.8576
	step [34/192], loss=51.4626
	step [35/192], loss=73.0926
	step [36/192], loss=65.4331
	step [37/192], loss=59.2870
	step [38/192], loss=76.1204
	step [39/192], loss=60.5072
	step [40/192], loss=70.1870
	step [41/192], loss=63.2848
	step [42/192], loss=74.0353
	step [43/192], loss=63.5038
	step [44/192], loss=70.0112
	step [45/192], loss=68.3258
	step [46/192], loss=77.7149
	step [47/192], loss=50.2717
	step [48/192], loss=69.9707
	step [49/192], loss=73.7001
	step [50/192], loss=69.5246
	step [51/192], loss=68.6454
	step [52/192], loss=71.9655
	step [53/192], loss=74.8521
	step [54/192], loss=67.3674
	step [55/192], loss=68.1940
	step [56/192], loss=69.2939
	step [57/192], loss=53.9409
	step [58/192], loss=60.6057
	step [59/192], loss=72.5459
	step [60/192], loss=68.0610
	step [61/192], loss=55.3096
	step [62/192], loss=66.1455
	step [63/192], loss=73.3816
	step [64/192], loss=62.6230
	step [65/192], loss=68.8713
	step [66/192], loss=61.3074
	step [67/192], loss=75.9060
	step [68/192], loss=50.4319
	step [69/192], loss=76.0593
	step [70/192], loss=67.8947
	step [71/192], loss=64.1958
	step [72/192], loss=57.0755
	step [73/192], loss=54.7704
	step [74/192], loss=66.1240
	step [75/192], loss=75.2499
	step [76/192], loss=70.3785
	step [77/192], loss=67.9890
	step [78/192], loss=71.0443
	step [79/192], loss=73.0678
	step [80/192], loss=69.2296
	step [81/192], loss=64.2671
	step [82/192], loss=58.3533
	step [83/192], loss=63.4014
	step [84/192], loss=58.2249
	step [85/192], loss=62.8435
	step [86/192], loss=73.1116
	step [87/192], loss=61.3051
	step [88/192], loss=59.9708
	step [89/192], loss=55.2834
	step [90/192], loss=65.4246
	step [91/192], loss=63.2619
	step [92/192], loss=67.3775
	step [93/192], loss=68.0479
	step [94/192], loss=65.3993
	step [95/192], loss=61.0819
	step [96/192], loss=63.4715
	step [97/192], loss=80.6974
	step [98/192], loss=65.2518
	step [99/192], loss=59.0132
	step [100/192], loss=68.3185
	step [101/192], loss=65.7020
	step [102/192], loss=75.2856
	step [103/192], loss=74.9355
	step [104/192], loss=49.2847
	step [105/192], loss=59.4927
	step [106/192], loss=58.1229
	step [107/192], loss=61.5706
	step [108/192], loss=58.1031
	step [109/192], loss=73.9384
	step [110/192], loss=53.8542
	step [111/192], loss=64.0685
	step [112/192], loss=70.0934
	step [113/192], loss=72.2467
	step [114/192], loss=55.5781
	step [115/192], loss=67.7541
	step [116/192], loss=65.6865
	step [117/192], loss=65.0648
	step [118/192], loss=66.8845
	step [119/192], loss=79.2182
	step [120/192], loss=71.7413
	step [121/192], loss=64.2968
	step [122/192], loss=71.9531
	step [123/192], loss=68.3616
	step [124/192], loss=70.6826
	step [125/192], loss=63.0460
	step [126/192], loss=78.9619
	step [127/192], loss=59.9788
	step [128/192], loss=57.7488
	step [129/192], loss=70.7906
	step [130/192], loss=67.7627
	step [131/192], loss=57.1239
	step [132/192], loss=80.2976
	step [133/192], loss=63.5058
	step [134/192], loss=74.7587
	step [135/192], loss=65.2054
	step [136/192], loss=65.6520
	step [137/192], loss=64.4082
	step [138/192], loss=67.1324
	step [139/192], loss=55.6805
	step [140/192], loss=67.6695
	step [141/192], loss=62.5177
	step [142/192], loss=66.6484
	step [143/192], loss=57.9253
	step [144/192], loss=67.7461
	step [145/192], loss=62.8909
	step [146/192], loss=68.5878
	step [147/192], loss=57.5713
	step [148/192], loss=68.2932
	step [149/192], loss=68.3736
	step [150/192], loss=66.8484
	step [151/192], loss=69.2032
	step [152/192], loss=70.5717
	step [153/192], loss=70.2534
	step [154/192], loss=62.1787
	step [155/192], loss=70.6772
	step [156/192], loss=55.2801
	step [157/192], loss=57.2961
	step [158/192], loss=71.6253
	step [159/192], loss=73.2987
	step [160/192], loss=71.5471
	step [161/192], loss=72.5680
	step [162/192], loss=65.2051
	step [163/192], loss=75.3783
	step [164/192], loss=70.8165
	step [165/192], loss=55.1419
	step [166/192], loss=64.7267
	step [167/192], loss=62.2318
	step [168/192], loss=84.9351
	step [169/192], loss=53.5964
	step [170/192], loss=68.9960
	step [171/192], loss=80.4045
	step [172/192], loss=62.0151
	step [173/192], loss=69.7551
	step [174/192], loss=64.6168
	step [175/192], loss=73.0427
	step [176/192], loss=69.9811
	step [177/192], loss=70.0826
	step [178/192], loss=55.2862
	step [179/192], loss=67.9520
	step [180/192], loss=72.0434
	step [181/192], loss=59.2170
	step [182/192], loss=68.2572
	step [183/192], loss=64.2354
	step [184/192], loss=64.7415
	step [185/192], loss=76.1302
	step [186/192], loss=59.1172
	step [187/192], loss=60.0507
	step [188/192], loss=77.8587
	step [189/192], loss=58.1925
	step [190/192], loss=71.3669
	step [191/192], loss=61.5405
	step [192/192], loss=61.1047
	Evaluating
	loss=0.0065, precision=0.3745, recall=0.8654, f1=0.5227
Training epoch 118
	step [1/192], loss=58.4549
	step [2/192], loss=62.1296
	step [3/192], loss=68.2662
	step [4/192], loss=64.9586
	step [5/192], loss=69.1816
	step [6/192], loss=68.2576
	step [7/192], loss=65.4597
	step [8/192], loss=67.4448
	step [9/192], loss=62.5449
	step [10/192], loss=61.0062
	step [11/192], loss=68.3924
	step [12/192], loss=66.2005
	step [13/192], loss=72.3808
	step [14/192], loss=61.7030
	step [15/192], loss=68.9261
	step [16/192], loss=64.3110
	step [17/192], loss=61.8465
	step [18/192], loss=69.1205
	step [19/192], loss=69.4055
	step [20/192], loss=56.2087
	step [21/192], loss=64.9691
	step [22/192], loss=61.6475
	step [23/192], loss=61.2399
	step [24/192], loss=65.0197
	step [25/192], loss=68.9222
	step [26/192], loss=70.4825
	step [27/192], loss=64.5342
	step [28/192], loss=71.4710
	step [29/192], loss=74.0918
	step [30/192], loss=62.0512
	step [31/192], loss=66.9663
	step [32/192], loss=75.5767
	step [33/192], loss=71.3709
	step [34/192], loss=70.9385
	step [35/192], loss=77.5316
	step [36/192], loss=62.7334
	step [37/192], loss=69.7880
	step [38/192], loss=51.9878
	step [39/192], loss=66.6460
	step [40/192], loss=77.1608
	step [41/192], loss=74.0831
	step [42/192], loss=65.3339
	step [43/192], loss=54.7048
	step [44/192], loss=58.9466
	step [45/192], loss=63.0724
	step [46/192], loss=60.3729
	step [47/192], loss=68.0688
	step [48/192], loss=59.2305
	step [49/192], loss=63.0554
	step [50/192], loss=73.4366
	step [51/192], loss=51.2379
	step [52/192], loss=57.6459
	step [53/192], loss=74.4996
	step [54/192], loss=64.8770
	step [55/192], loss=89.5395
	step [56/192], loss=63.4636
	step [57/192], loss=66.4368
	step [58/192], loss=62.5585
	step [59/192], loss=64.4453
	step [60/192], loss=68.0902
	step [61/192], loss=65.8542
	step [62/192], loss=79.0061
	step [63/192], loss=59.9671
	step [64/192], loss=60.7158
	step [65/192], loss=60.5399
	step [66/192], loss=67.1908
	step [67/192], loss=65.6628
	step [68/192], loss=68.5805
	step [69/192], loss=63.6406
	step [70/192], loss=75.0169
	step [71/192], loss=65.8959
	step [72/192], loss=63.8353
	step [73/192], loss=66.9502
	step [74/192], loss=68.7877
	step [75/192], loss=62.3883
	step [76/192], loss=62.0423
	step [77/192], loss=67.0791
	step [78/192], loss=61.8799
	step [79/192], loss=56.6980
	step [80/192], loss=58.4202
	step [81/192], loss=63.5931
	step [82/192], loss=67.2589
	step [83/192], loss=54.7843
	step [84/192], loss=71.2354
	step [85/192], loss=63.5624
	step [86/192], loss=56.1755
	step [87/192], loss=65.9085
	step [88/192], loss=66.6698
	step [89/192], loss=67.1055
	step [90/192], loss=63.5161
	step [91/192], loss=59.6831
	step [92/192], loss=61.4264
	step [93/192], loss=55.0643
	step [94/192], loss=63.9098
	step [95/192], loss=73.1586
	step [96/192], loss=64.7829
	step [97/192], loss=65.3721
	step [98/192], loss=73.0076
	step [99/192], loss=69.7593
	step [100/192], loss=67.1831
	step [101/192], loss=67.2964
	step [102/192], loss=65.4260
	step [103/192], loss=70.1942
	step [104/192], loss=57.5396
	step [105/192], loss=64.9120
	step [106/192], loss=79.2366
	step [107/192], loss=72.0243
	step [108/192], loss=79.5694
	step [109/192], loss=65.5755
	step [110/192], loss=68.7908
	step [111/192], loss=62.4769
	step [112/192], loss=68.4301
	step [113/192], loss=71.2753
	step [114/192], loss=66.1615
	step [115/192], loss=69.7133
	step [116/192], loss=64.3560
	step [117/192], loss=66.5363
	step [118/192], loss=75.5003
	step [119/192], loss=73.6186
	step [120/192], loss=54.7841
	step [121/192], loss=68.9357
	step [122/192], loss=63.6910
	step [123/192], loss=55.5913
	step [124/192], loss=76.3612
	step [125/192], loss=59.4708
	step [126/192], loss=54.2603
	step [127/192], loss=72.8116
	step [128/192], loss=60.7575
	step [129/192], loss=74.8325
	step [130/192], loss=68.4707
	step [131/192], loss=71.1132
	step [132/192], loss=70.1412
	step [133/192], loss=66.4817
	step [134/192], loss=58.5534
	step [135/192], loss=59.3212
	step [136/192], loss=57.7052
	step [137/192], loss=82.8948
	step [138/192], loss=64.9751
	step [139/192], loss=59.5068
	step [140/192], loss=63.1254
	step [141/192], loss=60.8213
	step [142/192], loss=64.3731
	step [143/192], loss=69.9850
	step [144/192], loss=61.8050
	step [145/192], loss=63.9257
	step [146/192], loss=80.0309
	step [147/192], loss=71.1037
	step [148/192], loss=74.1187
	step [149/192], loss=74.0732
	step [150/192], loss=63.5487
	step [151/192], loss=72.4688
	step [152/192], loss=77.5502
	step [153/192], loss=71.5126
	step [154/192], loss=66.8634
	step [155/192], loss=71.9466
	step [156/192], loss=62.3044
	step [157/192], loss=73.5922
	step [158/192], loss=67.4825
	step [159/192], loss=80.8564
	step [160/192], loss=67.2245
	step [161/192], loss=69.8616
	step [162/192], loss=68.6687
	step [163/192], loss=75.9981
	step [164/192], loss=57.0923
	step [165/192], loss=61.2504
	step [166/192], loss=63.0924
	step [167/192], loss=59.1842
	step [168/192], loss=53.8728
	step [169/192], loss=61.7780
	step [170/192], loss=65.0704
	step [171/192], loss=65.4552
	step [172/192], loss=66.1519
	step [173/192], loss=60.5358
	step [174/192], loss=78.2938
	step [175/192], loss=65.6110
	step [176/192], loss=65.9522
	step [177/192], loss=57.0631
	step [178/192], loss=64.2368
	step [179/192], loss=55.9274
	step [180/192], loss=59.6823
	step [181/192], loss=67.0043
	step [182/192], loss=66.1279
	step [183/192], loss=73.9307
	step [184/192], loss=53.9667
	step [185/192], loss=57.0644
	step [186/192], loss=73.5845
	step [187/192], loss=67.7567
	step [188/192], loss=70.8679
	step [189/192], loss=63.5567
	step [190/192], loss=74.7465
	step [191/192], loss=72.9466
	step [192/192], loss=64.0545
	Evaluating
	loss=0.0063, precision=0.3762, recall=0.8633, f1=0.5240
Training epoch 119
	step [1/192], loss=66.5099
	step [2/192], loss=54.6737
	step [3/192], loss=67.0785
	step [4/192], loss=62.6478
	step [5/192], loss=71.9016
	step [6/192], loss=57.9845
	step [7/192], loss=67.2072
	step [8/192], loss=70.4418
	step [9/192], loss=58.5924
	step [10/192], loss=57.5829
	step [11/192], loss=71.6774
	step [12/192], loss=64.1544
	step [13/192], loss=74.5097
	step [14/192], loss=64.1384
	step [15/192], loss=64.3095
	step [16/192], loss=63.2886
	step [17/192], loss=67.0180
	step [18/192], loss=65.8129
	step [19/192], loss=72.1752
	step [20/192], loss=68.9956
	step [21/192], loss=61.6398
	step [22/192], loss=63.7508
	step [23/192], loss=62.8063
	step [24/192], loss=65.1707
	step [25/192], loss=60.8274
	step [26/192], loss=74.9703
	step [27/192], loss=62.3817
	step [28/192], loss=63.0364
	step [29/192], loss=70.3272
	step [30/192], loss=65.3548
	step [31/192], loss=66.9779
	step [32/192], loss=72.3927
	step [33/192], loss=65.0936
	step [34/192], loss=57.5159
	step [35/192], loss=67.5138
	step [36/192], loss=57.1103
	step [37/192], loss=70.9780
	step [38/192], loss=70.1455
	step [39/192], loss=66.0830
	step [40/192], loss=69.9343
	step [41/192], loss=69.7098
	step [42/192], loss=52.6655
	step [43/192], loss=63.8626
	step [44/192], loss=72.8050
	step [45/192], loss=60.3227
	step [46/192], loss=75.8837
	step [47/192], loss=71.8348
	step [48/192], loss=62.5098
	step [49/192], loss=77.2508
	step [50/192], loss=66.7317
	step [51/192], loss=59.4716
	step [52/192], loss=63.2414
	step [53/192], loss=69.9547
	step [54/192], loss=56.0060
	step [55/192], loss=69.3679
	step [56/192], loss=67.0854
	step [57/192], loss=60.0240
	step [58/192], loss=68.0183
	step [59/192], loss=57.6111
	step [60/192], loss=64.7567
	step [61/192], loss=76.9298
	step [62/192], loss=66.1144
	step [63/192], loss=65.4981
	step [64/192], loss=67.5056
	step [65/192], loss=70.3640
	step [66/192], loss=56.4737
	step [67/192], loss=79.4075
	step [68/192], loss=56.9641
	step [69/192], loss=58.3221
	step [70/192], loss=90.0243
	step [71/192], loss=74.4322
	step [72/192], loss=67.7972
	step [73/192], loss=72.5662
	step [74/192], loss=58.3509
	step [75/192], loss=74.7357
	step [76/192], loss=68.4738
	step [77/192], loss=68.6544
	step [78/192], loss=66.3197
	step [79/192], loss=77.2538
	step [80/192], loss=71.8370
	step [81/192], loss=70.8038
	step [82/192], loss=69.1486
	step [83/192], loss=68.6615
	step [84/192], loss=49.0940
	step [85/192], loss=68.4818
	step [86/192], loss=71.0739
	step [87/192], loss=65.4653
	step [88/192], loss=57.3863
	step [89/192], loss=59.2402
	step [90/192], loss=76.1690
	step [91/192], loss=63.3651
	step [92/192], loss=57.8988
	step [93/192], loss=63.7354
	step [94/192], loss=60.7587
	step [95/192], loss=64.5896
	step [96/192], loss=71.7949
	step [97/192], loss=66.9063
	step [98/192], loss=63.5851
	step [99/192], loss=62.1908
	step [100/192], loss=71.5245
	step [101/192], loss=56.8109
	step [102/192], loss=67.0602
	step [103/192], loss=70.9352
	step [104/192], loss=66.9989
	step [105/192], loss=61.2346
	step [106/192], loss=65.6193
	step [107/192], loss=69.8457
	step [108/192], loss=72.8521
	step [109/192], loss=64.9322
	step [110/192], loss=58.7539
	step [111/192], loss=64.4103
	step [112/192], loss=73.8196
	step [113/192], loss=64.6560
	step [114/192], loss=60.3814
	step [115/192], loss=58.5887
	step [116/192], loss=74.6793
	step [117/192], loss=72.1695
	step [118/192], loss=73.0798
	step [119/192], loss=74.2255
	step [120/192], loss=70.8686
	step [121/192], loss=57.2022
	step [122/192], loss=53.9959
	step [123/192], loss=64.5551
	step [124/192], loss=74.1953
	step [125/192], loss=65.7346
	step [126/192], loss=64.4927
	step [127/192], loss=59.6493
	step [128/192], loss=78.7678
	step [129/192], loss=67.4397
	step [130/192], loss=75.9146
	step [131/192], loss=67.6884
	step [132/192], loss=55.0773
	step [133/192], loss=73.5766
	step [134/192], loss=53.8368
	step [135/192], loss=71.7456
	step [136/192], loss=57.9833
	step [137/192], loss=70.8716
	step [138/192], loss=62.2134
	step [139/192], loss=66.6136
	step [140/192], loss=69.6513
	step [141/192], loss=58.8216
	step [142/192], loss=69.1355
	step [143/192], loss=63.4423
	step [144/192], loss=64.0486
	step [145/192], loss=68.9277
	step [146/192], loss=75.7463
	step [147/192], loss=76.3392
	step [148/192], loss=55.6450
	step [149/192], loss=61.3843
	step [150/192], loss=72.0640
	step [151/192], loss=67.7022
	step [152/192], loss=63.2576
	step [153/192], loss=73.5641
	step [154/192], loss=65.7007
	step [155/192], loss=55.7435
	step [156/192], loss=61.8717
	step [157/192], loss=59.6309
	step [158/192], loss=66.5744
	step [159/192], loss=72.9566
	step [160/192], loss=56.0583
	step [161/192], loss=68.0037
	step [162/192], loss=58.1415
	step [163/192], loss=79.2882
	step [164/192], loss=65.4443
	step [165/192], loss=61.1723
	step [166/192], loss=56.0341
	step [167/192], loss=63.2607
	step [168/192], loss=58.1820
	step [169/192], loss=54.7196
	step [170/192], loss=70.9872
	step [171/192], loss=65.8645
	step [172/192], loss=58.4032
	step [173/192], loss=71.2377
	step [174/192], loss=60.7438
	step [175/192], loss=59.1137
	step [176/192], loss=56.2018
	step [177/192], loss=75.2021
	step [178/192], loss=62.5961
	step [179/192], loss=70.2976
	step [180/192], loss=63.9051
	step [181/192], loss=60.4629
	step [182/192], loss=62.2646
	step [183/192], loss=70.0966
	step [184/192], loss=65.6333
	step [185/192], loss=53.5214
	step [186/192], loss=61.2234
	step [187/192], loss=59.9850
	step [188/192], loss=68.2039
	step [189/192], loss=67.0866
	step [190/192], loss=67.7450
	step [191/192], loss=65.1856
	step [192/192], loss=64.1915
	Evaluating
	loss=0.0060, precision=0.4209, recall=0.8681, f1=0.5669
Training epoch 120
	step [1/192], loss=68.8831
	step [2/192], loss=66.4990
	step [3/192], loss=61.5668
	step [4/192], loss=66.4474
	step [5/192], loss=68.9968
	step [6/192], loss=62.5506
	step [7/192], loss=60.2704
	step [8/192], loss=73.2181
	step [9/192], loss=59.5034
	step [10/192], loss=71.0912
	step [11/192], loss=62.4480
	step [12/192], loss=58.1013
	step [13/192], loss=59.5006
	step [14/192], loss=77.7967
	step [15/192], loss=68.8127
	step [16/192], loss=66.5849
	step [17/192], loss=70.0780
	step [18/192], loss=52.4597
	step [19/192], loss=42.8431
	step [20/192], loss=60.3657
	step [21/192], loss=71.1372
	step [22/192], loss=64.4270
	step [23/192], loss=66.7289
	step [24/192], loss=73.6430
	step [25/192], loss=75.2098
	step [26/192], loss=67.8447
	step [27/192], loss=55.0478
	step [28/192], loss=72.3621
	step [29/192], loss=75.6266
	step [30/192], loss=49.1371
	step [31/192], loss=62.1372
	step [32/192], loss=66.6696
	step [33/192], loss=54.3637
	step [34/192], loss=71.1062
	step [35/192], loss=52.5189
	step [36/192], loss=69.9639
	step [37/192], loss=80.7189
	step [38/192], loss=54.1753
	step [39/192], loss=67.0082
	step [40/192], loss=54.3975
	step [41/192], loss=74.4199
	step [42/192], loss=64.4302
	step [43/192], loss=61.9693
	step [44/192], loss=84.5064
	step [45/192], loss=63.9297
	step [46/192], loss=72.4110
	step [47/192], loss=69.5014
	step [48/192], loss=64.2717
	step [49/192], loss=60.0361
	step [50/192], loss=65.4063
	step [51/192], loss=64.2123
	step [52/192], loss=72.7475
	step [53/192], loss=62.3524
	step [54/192], loss=70.4591
	step [55/192], loss=57.6068
	step [56/192], loss=67.0002
	step [57/192], loss=78.4823
	step [58/192], loss=66.2921
	step [59/192], loss=66.1158
	step [60/192], loss=63.4477
	step [61/192], loss=77.3802
	step [62/192], loss=65.8053
	step [63/192], loss=60.7427
	step [64/192], loss=68.4784
	step [65/192], loss=76.0555
	step [66/192], loss=67.7993
	step [67/192], loss=74.4982
	step [68/192], loss=63.6284
	step [69/192], loss=65.2965
	step [70/192], loss=60.0827
	step [71/192], loss=68.4777
	step [72/192], loss=69.6596
	step [73/192], loss=63.7521
	step [74/192], loss=74.8493
	step [75/192], loss=72.3645
	step [76/192], loss=65.2093
	step [77/192], loss=58.9441
	step [78/192], loss=57.3927
	step [79/192], loss=60.3667
	step [80/192], loss=70.2096
	step [81/192], loss=65.0672
	step [82/192], loss=64.5429
	step [83/192], loss=71.5706
	step [84/192], loss=59.1449
	step [85/192], loss=59.1718
	step [86/192], loss=68.5202
	step [87/192], loss=59.7325
	step [88/192], loss=63.8437
	step [89/192], loss=65.8521
	step [90/192], loss=53.1568
	step [91/192], loss=66.7440
	step [92/192], loss=71.7521
	step [93/192], loss=59.1469
	step [94/192], loss=49.8513
	step [95/192], loss=76.3609
	step [96/192], loss=76.8990
	step [97/192], loss=55.4946
	step [98/192], loss=52.6197
	step [99/192], loss=55.5990
	step [100/192], loss=62.4739
	step [101/192], loss=67.0896
	step [102/192], loss=61.4656
	step [103/192], loss=76.0553
	step [104/192], loss=63.5011
	step [105/192], loss=59.0116
	step [106/192], loss=69.6133
	step [107/192], loss=57.4383
	step [108/192], loss=63.7266
	step [109/192], loss=71.2312
	step [110/192], loss=68.9208
	step [111/192], loss=80.3117
	step [112/192], loss=59.7708
	step [113/192], loss=61.1103
	step [114/192], loss=56.8416
	step [115/192], loss=66.9877
	step [116/192], loss=65.6398
	step [117/192], loss=64.4710
	step [118/192], loss=78.8743
	step [119/192], loss=69.3844
	step [120/192], loss=66.3993
	step [121/192], loss=77.7400
	step [122/192], loss=67.1634
	step [123/192], loss=53.8881
	step [124/192], loss=67.4764
	step [125/192], loss=62.7837
	step [126/192], loss=62.5132
	step [127/192], loss=67.8862
	step [128/192], loss=66.7520
	step [129/192], loss=63.1817
	step [130/192], loss=78.5830
	step [131/192], loss=61.3256
	step [132/192], loss=53.2741
	step [133/192], loss=67.5217
	step [134/192], loss=62.2148
	step [135/192], loss=59.5831
	step [136/192], loss=57.9350
	step [137/192], loss=71.1989
	step [138/192], loss=55.9762
	step [139/192], loss=53.8814
	step [140/192], loss=57.5559
	step [141/192], loss=62.8167
	step [142/192], loss=58.9270
	step [143/192], loss=56.4244
	step [144/192], loss=62.7082
	step [145/192], loss=70.1513
	step [146/192], loss=72.5626
	step [147/192], loss=68.9119
	step [148/192], loss=71.2451
	step [149/192], loss=76.1905
	step [150/192], loss=68.1815
	step [151/192], loss=60.9273
	step [152/192], loss=73.7137
	step [153/192], loss=59.5463
	step [154/192], loss=61.8133
	step [155/192], loss=71.5691
	step [156/192], loss=64.6098
	step [157/192], loss=70.7943
	step [158/192], loss=58.1457
	step [159/192], loss=60.2085
	step [160/192], loss=73.5321
	step [161/192], loss=58.0546
	step [162/192], loss=61.3115
	step [163/192], loss=78.1215
	step [164/192], loss=64.0787
	step [165/192], loss=61.6112
	step [166/192], loss=65.5974
	step [167/192], loss=67.1725
	step [168/192], loss=56.2400
	step [169/192], loss=85.9500
	step [170/192], loss=66.4821
	step [171/192], loss=76.4505
	step [172/192], loss=63.9094
	step [173/192], loss=62.5760
	step [174/192], loss=76.5207
	step [175/192], loss=57.7071
	step [176/192], loss=68.6267
	step [177/192], loss=68.6748
	step [178/192], loss=63.6979
	step [179/192], loss=64.0683
	step [180/192], loss=61.4640
	step [181/192], loss=53.3215
	step [182/192], loss=76.0636
	step [183/192], loss=72.0036
	step [184/192], loss=61.1048
	step [185/192], loss=56.7458
	step [186/192], loss=59.8755
	step [187/192], loss=75.5432
	step [188/192], loss=70.0339
	step [189/192], loss=70.0123
	step [190/192], loss=88.8460
	step [191/192], loss=69.3416
	step [192/192], loss=53.8345
	Evaluating
	loss=0.0062, precision=0.4006, recall=0.8612, f1=0.5468
Training epoch 121
	step [1/192], loss=64.7090
	step [2/192], loss=71.5008
	step [3/192], loss=62.5177
	step [4/192], loss=60.3903
	step [5/192], loss=67.0062
	step [6/192], loss=61.1352
	step [7/192], loss=63.8643
	step [8/192], loss=73.2724
	step [9/192], loss=78.7279
	step [10/192], loss=63.5778
	step [11/192], loss=62.1397
	step [12/192], loss=50.5903
	step [13/192], loss=67.4274
	step [14/192], loss=66.4681
	step [15/192], loss=58.2685
	step [16/192], loss=68.8288
	step [17/192], loss=64.9942
	step [18/192], loss=58.4350
	step [19/192], loss=70.4453
	step [20/192], loss=79.8418
	step [21/192], loss=72.2202
	step [22/192], loss=66.0789
	step [23/192], loss=72.4450
	step [24/192], loss=66.0439
	step [25/192], loss=74.9141
	step [26/192], loss=59.2900
	step [27/192], loss=52.2284
	step [28/192], loss=62.5782
	step [29/192], loss=70.9352
	step [30/192], loss=64.9052
	step [31/192], loss=56.3657
	step [32/192], loss=64.9340
	step [33/192], loss=62.2002
	step [34/192], loss=70.9477
	step [35/192], loss=70.0996
	step [36/192], loss=82.9479
	step [37/192], loss=58.3907
	step [38/192], loss=69.2177
	step [39/192], loss=63.7792
	step [40/192], loss=59.3705
	step [41/192], loss=61.5696
	step [42/192], loss=54.6900
	step [43/192], loss=85.7829
	step [44/192], loss=70.7941
	step [45/192], loss=60.7723
	step [46/192], loss=55.1071
	step [47/192], loss=68.9866
	step [48/192], loss=66.5282
	step [49/192], loss=66.6779
	step [50/192], loss=66.8728
	step [51/192], loss=71.2537
	step [52/192], loss=67.6688
	step [53/192], loss=72.1818
	step [54/192], loss=64.2424
	step [55/192], loss=59.9038
	step [56/192], loss=74.4518
	step [57/192], loss=73.2047
	step [58/192], loss=60.7170
	step [59/192], loss=66.4854
	step [60/192], loss=61.3298
	step [61/192], loss=82.8812
	step [62/192], loss=60.9479
	step [63/192], loss=59.8015
	step [64/192], loss=67.9060
	step [65/192], loss=72.7733
	step [66/192], loss=64.4299
	step [67/192], loss=63.0161
	step [68/192], loss=65.5138
	step [69/192], loss=67.4832
	step [70/192], loss=70.2454
	step [71/192], loss=61.5381
	step [72/192], loss=69.8453
	step [73/192], loss=71.7654
	step [74/192], loss=63.6200
	step [75/192], loss=73.7273
	step [76/192], loss=60.7529
	step [77/192], loss=58.3770
	step [78/192], loss=65.1993
	step [79/192], loss=69.6316
	step [80/192], loss=66.3938
	step [81/192], loss=69.9815
	step [82/192], loss=56.5853
	step [83/192], loss=73.5854
	step [84/192], loss=59.5800
	step [85/192], loss=67.4100
	step [86/192], loss=69.9196
	step [87/192], loss=69.2902
	step [88/192], loss=69.7037
	step [89/192], loss=66.5292
	step [90/192], loss=64.0334
	step [91/192], loss=60.4475
	step [92/192], loss=68.7001
	step [93/192], loss=66.0169
	step [94/192], loss=58.1097
	step [95/192], loss=56.2715
	step [96/192], loss=57.9882
	step [97/192], loss=81.7910
	step [98/192], loss=65.7066
	step [99/192], loss=73.3538
	step [100/192], loss=59.4325
	step [101/192], loss=59.2623
	step [102/192], loss=72.7528
	step [103/192], loss=63.1118
	step [104/192], loss=59.8068
	step [105/192], loss=70.1195
	step [106/192], loss=69.7761
	step [107/192], loss=56.3063
	step [108/192], loss=60.3566
	step [109/192], loss=63.2031
	step [110/192], loss=58.1226
	step [111/192], loss=79.7266
	step [112/192], loss=61.7434
	step [113/192], loss=71.9892
	step [114/192], loss=69.5066
	step [115/192], loss=78.2768
	step [116/192], loss=60.4844
	step [117/192], loss=64.2643
	step [118/192], loss=59.4930
	step [119/192], loss=72.4601
	step [120/192], loss=63.7011
	step [121/192], loss=61.5125
	step [122/192], loss=60.3507
	step [123/192], loss=51.0860
	step [124/192], loss=69.8976
	step [125/192], loss=56.7179
	step [126/192], loss=62.7928
	step [127/192], loss=70.3249
	step [128/192], loss=57.5289
	step [129/192], loss=64.3884
	step [130/192], loss=68.1834
	step [131/192], loss=75.1842
	step [132/192], loss=65.4080
	step [133/192], loss=67.8944
	step [134/192], loss=69.6252
	step [135/192], loss=64.6869
	step [136/192], loss=60.3853
	step [137/192], loss=62.9719
	step [138/192], loss=61.4364
	step [139/192], loss=68.4592
	step [140/192], loss=71.0793
	step [141/192], loss=62.7829
	step [142/192], loss=60.8647
	step [143/192], loss=66.1454
	step [144/192], loss=57.9721
	step [145/192], loss=75.9357
	step [146/192], loss=68.4761
	step [147/192], loss=66.5870
	step [148/192], loss=59.4684
	step [149/192], loss=76.9269
	step [150/192], loss=66.9737
	step [151/192], loss=69.2934
	step [152/192], loss=55.3997
	step [153/192], loss=60.3357
	step [154/192], loss=67.0148
	step [155/192], loss=63.0033
	step [156/192], loss=70.7764
	step [157/192], loss=59.3706
	step [158/192], loss=62.2976
	step [159/192], loss=73.5085
	step [160/192], loss=57.3491
	step [161/192], loss=55.2973
	step [162/192], loss=73.2441
	step [163/192], loss=68.5280
	step [164/192], loss=61.0099
	step [165/192], loss=65.7668
	step [166/192], loss=67.2163
	step [167/192], loss=70.4268
	step [168/192], loss=62.1347
	step [169/192], loss=66.1162
	step [170/192], loss=67.9329
	step [171/192], loss=58.0758
	step [172/192], loss=72.1455
	step [173/192], loss=61.7611
	step [174/192], loss=70.3361
	step [175/192], loss=60.3917
	step [176/192], loss=66.8323
	step [177/192], loss=51.1222
	step [178/192], loss=61.4044
	step [179/192], loss=63.2916
	step [180/192], loss=62.4570
	step [181/192], loss=61.8240
	step [182/192], loss=64.3382
	step [183/192], loss=68.1346
	step [184/192], loss=59.2094
	step [185/192], loss=67.5967
	step [186/192], loss=55.4876
	step [187/192], loss=68.3794
	step [188/192], loss=69.3003
	step [189/192], loss=60.5016
	step [190/192], loss=59.1091
	step [191/192], loss=66.2926
	step [192/192], loss=57.4183
	Evaluating
	loss=0.0060, precision=0.4005, recall=0.8705, f1=0.5486
Training epoch 122
	step [1/192], loss=72.4492
	step [2/192], loss=59.1330
	step [3/192], loss=61.9424
	step [4/192], loss=71.5768
	step [5/192], loss=60.6954
	step [6/192], loss=79.4678
	step [7/192], loss=69.1730
	step [8/192], loss=64.6012
	step [9/192], loss=77.1805
	step [10/192], loss=67.5466
	step [11/192], loss=69.2323
	step [12/192], loss=65.1195
	step [13/192], loss=68.8013
	step [14/192], loss=63.2095
	step [15/192], loss=64.1259
	step [16/192], loss=68.3933
	step [17/192], loss=68.3581
	step [18/192], loss=63.0420
	step [19/192], loss=60.6471
	step [20/192], loss=59.5189
	step [21/192], loss=67.2216
	step [22/192], loss=62.3649
	step [23/192], loss=68.2711
	step [24/192], loss=60.5504
	step [25/192], loss=52.8249
	step [26/192], loss=54.3646
	step [27/192], loss=66.1445
	step [28/192], loss=59.6308
	step [29/192], loss=61.9328
	step [30/192], loss=56.8629
	step [31/192], loss=65.0087
	step [32/192], loss=80.6868
	step [33/192], loss=64.0554
	step [34/192], loss=64.2806
	step [35/192], loss=62.0264
	step [36/192], loss=54.4974
	step [37/192], loss=69.8173
	step [38/192], loss=62.1501
	step [39/192], loss=61.2772
	step [40/192], loss=69.4956
	step [41/192], loss=71.5410
	step [42/192], loss=67.0930
	step [43/192], loss=72.0505
	step [44/192], loss=61.4716
	step [45/192], loss=60.1502
	step [46/192], loss=60.9156
	step [47/192], loss=68.1639
	step [48/192], loss=51.5303
	step [49/192], loss=66.8333
	step [50/192], loss=62.1412
	step [51/192], loss=52.4119
	step [52/192], loss=64.1214
	step [53/192], loss=72.0739
	step [54/192], loss=64.7595
	step [55/192], loss=66.6139
	step [56/192], loss=73.6866
	step [57/192], loss=82.6137
	step [58/192], loss=75.7729
	step [59/192], loss=68.9984
	step [60/192], loss=68.2531
	step [61/192], loss=74.9681
	step [62/192], loss=58.5706
	step [63/192], loss=72.7694
	step [64/192], loss=63.8387
	step [65/192], loss=62.8220
	step [66/192], loss=62.0845
	step [67/192], loss=55.7840
	step [68/192], loss=66.2013
	step [69/192], loss=63.6318
	step [70/192], loss=55.3761
	step [71/192], loss=62.7318
	step [72/192], loss=70.5003
	step [73/192], loss=68.9331
	step [74/192], loss=60.8675
	step [75/192], loss=68.0944
	step [76/192], loss=55.2642
	step [77/192], loss=69.7407
	step [78/192], loss=63.7471
	step [79/192], loss=57.7972
	step [80/192], loss=57.9970
	step [81/192], loss=61.3317
	step [82/192], loss=68.2296
	step [83/192], loss=60.6279
	step [84/192], loss=61.9067
	step [85/192], loss=65.4987
	step [86/192], loss=64.0023
	step [87/192], loss=56.6200
	step [88/192], loss=60.8623
	step [89/192], loss=64.8433
	step [90/192], loss=66.2636
	step [91/192], loss=64.2118
	step [92/192], loss=62.5879
	step [93/192], loss=69.3235
	step [94/192], loss=59.1946
	step [95/192], loss=65.7026
	step [96/192], loss=62.3796
	step [97/192], loss=66.0406
	step [98/192], loss=62.9635
	step [99/192], loss=61.4811
	step [100/192], loss=68.6435
	step [101/192], loss=58.1204
	step [102/192], loss=63.4513
	step [103/192], loss=66.3816
	step [104/192], loss=60.6406
	step [105/192], loss=72.2228
	step [106/192], loss=68.2955
	step [107/192], loss=64.0761
	step [108/192], loss=61.3298
	step [109/192], loss=53.0101
	step [110/192], loss=62.6534
	step [111/192], loss=69.3852
	step [112/192], loss=69.6602
	step [113/192], loss=78.7791
	step [114/192], loss=66.1212
	step [115/192], loss=68.3957
	step [116/192], loss=59.4529
	step [117/192], loss=63.4096
	step [118/192], loss=69.0385
	step [119/192], loss=57.3699
	step [120/192], loss=69.9514
	step [121/192], loss=66.6179
	step [122/192], loss=74.7867
	step [123/192], loss=56.4569
	step [124/192], loss=61.5431
	step [125/192], loss=55.7463
	step [126/192], loss=67.1450
	step [127/192], loss=67.9220
	step [128/192], loss=73.5115
	step [129/192], loss=66.2067
	step [130/192], loss=66.7568
	step [131/192], loss=50.5783
	step [132/192], loss=58.9991
	step [133/192], loss=67.7204
	step [134/192], loss=58.2921
	step [135/192], loss=61.7335
	step [136/192], loss=71.6297
	step [137/192], loss=69.2248
	step [138/192], loss=67.1800
	step [139/192], loss=75.9293
	step [140/192], loss=81.0975
	step [141/192], loss=64.8808
	step [142/192], loss=60.6046
	step [143/192], loss=56.2063
	step [144/192], loss=61.4206
	step [145/192], loss=72.2811
	step [146/192], loss=72.5736
	step [147/192], loss=60.2466
	step [148/192], loss=60.5907
	step [149/192], loss=70.3005
	step [150/192], loss=65.1646
	step [151/192], loss=73.4351
	step [152/192], loss=65.6478
	step [153/192], loss=62.9181
	step [154/192], loss=56.3953
	step [155/192], loss=72.5552
	step [156/192], loss=67.3832
	step [157/192], loss=81.5133
	step [158/192], loss=64.6212
	step [159/192], loss=75.3791
	step [160/192], loss=64.7397
	step [161/192], loss=62.4803
	step [162/192], loss=67.3496
	step [163/192], loss=61.9232
	step [164/192], loss=65.3592
	step [165/192], loss=67.5216
	step [166/192], loss=56.3967
	step [167/192], loss=69.7326
	step [168/192], loss=69.1399
	step [169/192], loss=63.9415
	step [170/192], loss=69.0532
	step [171/192], loss=69.5957
	step [172/192], loss=65.0037
	step [173/192], loss=60.4684
	step [174/192], loss=86.8024
	step [175/192], loss=61.4640
	step [176/192], loss=66.0971
	step [177/192], loss=64.9881
	step [178/192], loss=60.1780
	step [179/192], loss=70.9296
	step [180/192], loss=63.4018
	step [181/192], loss=61.6999
	step [182/192], loss=83.0560
	step [183/192], loss=64.4070
	step [184/192], loss=74.6324
	step [185/192], loss=66.7710
	step [186/192], loss=66.7744
	step [187/192], loss=69.6773
	step [188/192], loss=53.6136
	step [189/192], loss=63.6165
	step [190/192], loss=68.5654
	step [191/192], loss=57.0171
	step [192/192], loss=50.9051
	Evaluating
	loss=0.0063, precision=0.3577, recall=0.8587, f1=0.5051
Training epoch 123
	step [1/192], loss=69.9313
	step [2/192], loss=57.2251
	step [3/192], loss=61.1198
	step [4/192], loss=62.6468
	step [5/192], loss=72.4618
	step [6/192], loss=72.9298
	step [7/192], loss=74.2076
	step [8/192], loss=66.9813
	step [9/192], loss=64.6286
	step [10/192], loss=73.9611
	step [11/192], loss=62.2017
	step [12/192], loss=56.1873
	step [13/192], loss=72.4504
	step [14/192], loss=57.0563
	step [15/192], loss=58.5521
	step [16/192], loss=54.8389
	step [17/192], loss=62.5733
	step [18/192], loss=62.1623
	step [19/192], loss=72.5573
	step [20/192], loss=72.9951
	step [21/192], loss=71.9297
	step [22/192], loss=68.8095
	step [23/192], loss=63.6738
	step [24/192], loss=66.6781
	step [25/192], loss=65.3361
	step [26/192], loss=65.9992
	step [27/192], loss=73.8948
	step [28/192], loss=69.6292
	step [29/192], loss=65.2583
	step [30/192], loss=68.1310
	step [31/192], loss=56.9248
	step [32/192], loss=54.5686
	step [33/192], loss=69.1788
	step [34/192], loss=66.3769
	step [35/192], loss=63.8502
	step [36/192], loss=76.3391
	step [37/192], loss=68.1768
	step [38/192], loss=65.8236
	step [39/192], loss=65.0749
	step [40/192], loss=52.8441
	step [41/192], loss=55.0190
	step [42/192], loss=58.6124
	step [43/192], loss=73.5683
	step [44/192], loss=65.3913
	step [45/192], loss=69.8178
	step [46/192], loss=60.3400
	step [47/192], loss=67.6152
	step [48/192], loss=60.3526
	step [49/192], loss=66.9905
	step [50/192], loss=72.3970
	step [51/192], loss=60.5454
	step [52/192], loss=62.0561
	step [53/192], loss=70.7045
	step [54/192], loss=59.7482
	step [55/192], loss=66.7166
	step [56/192], loss=72.8557
	step [57/192], loss=59.4985
	step [58/192], loss=73.2595
	step [59/192], loss=60.5787
	step [60/192], loss=69.6024
	step [61/192], loss=56.7068
	step [62/192], loss=53.4138
	step [63/192], loss=66.2978
	step [64/192], loss=70.7309
	step [65/192], loss=81.3734
	step [66/192], loss=67.2799
	step [67/192], loss=68.6315
	step [68/192], loss=74.1165
	step [69/192], loss=57.5789
	step [70/192], loss=66.2697
	step [71/192], loss=62.0652
	step [72/192], loss=60.2974
	step [73/192], loss=64.0382
	step [74/192], loss=58.9859
	step [75/192], loss=68.4457
	step [76/192], loss=67.6777
	step [77/192], loss=60.2707
	step [78/192], loss=68.3744
	step [79/192], loss=78.0908
	step [80/192], loss=61.4292
	step [81/192], loss=56.6172
	step [82/192], loss=61.9647
	step [83/192], loss=65.7620
	step [84/192], loss=59.8774
	step [85/192], loss=75.5728
	step [86/192], loss=63.7410
	step [87/192], loss=58.4491
	step [88/192], loss=63.7716
	step [89/192], loss=79.8662
	step [90/192], loss=56.2469
	step [91/192], loss=76.8341
	step [92/192], loss=62.3062
	step [93/192], loss=65.9356
	step [94/192], loss=58.7457
	step [95/192], loss=59.9135
	step [96/192], loss=59.6816
	step [97/192], loss=64.7365
	step [98/192], loss=55.4071
	step [99/192], loss=56.2963
	step [100/192], loss=68.8377
	step [101/192], loss=72.5425
	step [102/192], loss=62.9333
	step [103/192], loss=65.7425
	step [104/192], loss=64.9906
	step [105/192], loss=69.0233
	step [106/192], loss=71.1551
	step [107/192], loss=58.6012
	step [108/192], loss=53.8157
	step [109/192], loss=71.7775
	step [110/192], loss=61.3016
	step [111/192], loss=61.7391
	step [112/192], loss=60.8104
	step [113/192], loss=68.7097
	step [114/192], loss=69.2065
	step [115/192], loss=64.8700
	step [116/192], loss=69.7965
	step [117/192], loss=59.5703
	step [118/192], loss=86.0236
	step [119/192], loss=60.1396
	step [120/192], loss=64.3663
	step [121/192], loss=64.7767
	step [122/192], loss=64.1815
	step [123/192], loss=70.2463
	step [124/192], loss=59.2919
	step [125/192], loss=65.3412
	step [126/192], loss=70.9621
	step [127/192], loss=68.5035
	step [128/192], loss=62.0223
	step [129/192], loss=70.4485
	step [130/192], loss=60.9192
	step [131/192], loss=62.0694
	step [132/192], loss=65.3278
	step [133/192], loss=50.6997
	step [134/192], loss=64.3533
	step [135/192], loss=59.1205
	step [136/192], loss=74.4986
	step [137/192], loss=58.5215
	step [138/192], loss=68.3945
	step [139/192], loss=63.3569
	step [140/192], loss=60.6626
	step [141/192], loss=63.7965
	step [142/192], loss=63.2826
	step [143/192], loss=65.1751
	step [144/192], loss=70.8869
	step [145/192], loss=66.9436
	step [146/192], loss=69.7166
	step [147/192], loss=62.2703
	step [148/192], loss=73.6530
	step [149/192], loss=59.3395
	step [150/192], loss=73.3791
	step [151/192], loss=69.5758
	step [152/192], loss=69.4188
	step [153/192], loss=52.3425
	step [154/192], loss=62.9550
	step [155/192], loss=64.6414
	step [156/192], loss=59.5085
	step [157/192], loss=64.1916
	step [158/192], loss=62.9532
	step [159/192], loss=71.6415
	step [160/192], loss=61.3317
	step [161/192], loss=58.1950
	step [162/192], loss=83.7863
	step [163/192], loss=69.5346
	step [164/192], loss=65.0690
	step [165/192], loss=69.0891
	step [166/192], loss=72.7396
	step [167/192], loss=61.1969
	step [168/192], loss=56.1133
	step [169/192], loss=58.3012
	step [170/192], loss=64.9845
	step [171/192], loss=65.7768
	step [172/192], loss=64.8596
	step [173/192], loss=61.3081
	step [174/192], loss=68.2821
	step [175/192], loss=65.9105
	step [176/192], loss=65.1438
	step [177/192], loss=78.8120
	step [178/192], loss=56.4072
	step [179/192], loss=68.9540
	step [180/192], loss=52.8516
	step [181/192], loss=63.4576
	step [182/192], loss=60.1171
	step [183/192], loss=60.9868
	step [184/192], loss=61.5832
	step [185/192], loss=68.8730
	step [186/192], loss=59.3826
	step [187/192], loss=67.7680
	step [188/192], loss=69.5673
	step [189/192], loss=74.1514
	step [190/192], loss=61.2286
	step [191/192], loss=76.8043
	step [192/192], loss=59.6860
	Evaluating
	loss=0.0069, precision=0.3500, recall=0.8607, f1=0.4976
Training epoch 124
	step [1/192], loss=74.5588
	step [2/192], loss=69.2806
	step [3/192], loss=67.8555
	step [4/192], loss=66.0355
	step [5/192], loss=65.1023
	step [6/192], loss=66.4260
	step [7/192], loss=60.8145
	step [8/192], loss=71.6387
	step [9/192], loss=53.0301
	step [10/192], loss=68.3793
	step [11/192], loss=62.2059
	step [12/192], loss=59.9047
	step [13/192], loss=68.4895
	step [14/192], loss=77.6823
	step [15/192], loss=68.0205
	step [16/192], loss=65.8270
	step [17/192], loss=71.0388
	step [18/192], loss=68.6712
	step [19/192], loss=71.6901
	step [20/192], loss=58.3610
	step [21/192], loss=66.3980
	step [22/192], loss=69.8165
	step [23/192], loss=69.3673
	step [24/192], loss=64.1036
	step [25/192], loss=66.3841
	step [26/192], loss=58.2171
	step [27/192], loss=64.8395
	step [28/192], loss=60.5977
	step [29/192], loss=68.2192
	step [30/192], loss=63.2907
	step [31/192], loss=60.7639
	step [32/192], loss=66.1653
	step [33/192], loss=52.0574
	step [34/192], loss=67.3205
	step [35/192], loss=67.3922
	step [36/192], loss=65.8313
	step [37/192], loss=68.3835
	step [38/192], loss=56.0805
	step [39/192], loss=66.2718
	step [40/192], loss=76.7811
	step [41/192], loss=65.8719
	step [42/192], loss=60.9200
	step [43/192], loss=63.8041
	step [44/192], loss=62.7357
	step [45/192], loss=67.1342
	step [46/192], loss=68.4434
	step [47/192], loss=66.6763
	step [48/192], loss=65.5567
	step [49/192], loss=62.0379
	step [50/192], loss=61.4383
	step [51/192], loss=68.6692
	step [52/192], loss=65.6683
	step [53/192], loss=67.2706
	step [54/192], loss=65.9489
	step [55/192], loss=57.6517
	step [56/192], loss=61.0290
	step [57/192], loss=75.1825
	step [58/192], loss=65.7673
	step [59/192], loss=72.3121
	step [60/192], loss=61.2764
	step [61/192], loss=67.2693
	step [62/192], loss=75.1880
	step [63/192], loss=70.9635
	step [64/192], loss=77.3679
	step [65/192], loss=59.7718
	step [66/192], loss=77.1588
	step [67/192], loss=57.4795
	step [68/192], loss=58.6806
	step [69/192], loss=56.8375
	step [70/192], loss=69.9244
	step [71/192], loss=59.8066
	step [72/192], loss=65.8963
	step [73/192], loss=58.1566
	step [74/192], loss=65.4062
	step [75/192], loss=62.3500
	step [76/192], loss=73.4275
	step [77/192], loss=75.0468
	step [78/192], loss=57.5859
	step [79/192], loss=64.5033
	step [80/192], loss=56.6778
	step [81/192], loss=71.7686
	step [82/192], loss=66.8538
	step [83/192], loss=63.4083
	step [84/192], loss=70.7076
	step [85/192], loss=64.1502
	step [86/192], loss=64.4087
	step [87/192], loss=58.7711
	step [88/192], loss=71.4873
	step [89/192], loss=63.1521
	step [90/192], loss=67.5218
	step [91/192], loss=78.2820
	step [92/192], loss=61.1569
	step [93/192], loss=76.9147
	step [94/192], loss=59.3148
	step [95/192], loss=69.6523
	step [96/192], loss=69.5346
	step [97/192], loss=71.2172
	step [98/192], loss=62.5763
	step [99/192], loss=63.6171
	step [100/192], loss=76.9768
	step [101/192], loss=63.2942
	step [102/192], loss=53.2376
	step [103/192], loss=57.6693
	step [104/192], loss=69.9496
	step [105/192], loss=74.8319
	step [106/192], loss=65.0538
	step [107/192], loss=64.2642
	step [108/192], loss=64.6060
	step [109/192], loss=77.7629
	step [110/192], loss=64.9773
	step [111/192], loss=73.7224
	step [112/192], loss=64.1250
	step [113/192], loss=68.6410
	step [114/192], loss=65.9315
	step [115/192], loss=68.7640
	step [116/192], loss=60.8102
	step [117/192], loss=66.8376
	step [118/192], loss=72.0164
	step [119/192], loss=56.6458
	step [120/192], loss=63.5630
	step [121/192], loss=60.6822
	step [122/192], loss=56.6100
	step [123/192], loss=56.4142
	step [124/192], loss=62.7568
	step [125/192], loss=69.5500
	step [126/192], loss=62.7016
	step [127/192], loss=59.8957
	step [128/192], loss=57.9037
	step [129/192], loss=61.2735
	step [130/192], loss=80.2374
	step [131/192], loss=62.1687
	step [132/192], loss=72.5552
	step [133/192], loss=67.9760
	step [134/192], loss=66.8544
	step [135/192], loss=65.9203
	step [136/192], loss=65.5782
	step [137/192], loss=57.2250
	step [138/192], loss=68.4046
	step [139/192], loss=75.8559
	step [140/192], loss=59.8015
	step [141/192], loss=54.6242
	step [142/192], loss=63.0621
	step [143/192], loss=78.4999
	step [144/192], loss=59.6244
	step [145/192], loss=68.2524
	step [146/192], loss=60.1888
	step [147/192], loss=76.5752
	step [148/192], loss=58.6217
	step [149/192], loss=63.4095
	step [150/192], loss=63.6956
	step [151/192], loss=66.6712
	step [152/192], loss=63.8649
	step [153/192], loss=64.1669
	step [154/192], loss=59.6090
	step [155/192], loss=69.2665
	step [156/192], loss=69.8860
	step [157/192], loss=62.2864
	step [158/192], loss=67.4199
	step [159/192], loss=55.1222
	step [160/192], loss=60.3504
	step [161/192], loss=56.4840
	step [162/192], loss=59.4354
	step [163/192], loss=65.3906
	step [164/192], loss=62.0452
	step [165/192], loss=62.0981
	step [166/192], loss=67.6785
	step [167/192], loss=62.3590
	step [168/192], loss=64.3744
	step [169/192], loss=70.3422
	step [170/192], loss=54.6235
	step [171/192], loss=74.4539
	step [172/192], loss=62.4763
	step [173/192], loss=72.9797
	step [174/192], loss=62.7794
	step [175/192], loss=68.8423
	step [176/192], loss=53.8014
	step [177/192], loss=75.5097
	step [178/192], loss=58.8681
	step [179/192], loss=58.1254
	step [180/192], loss=62.6001
	step [181/192], loss=68.2895
	step [182/192], loss=60.5575
	step [183/192], loss=58.6622
	step [184/192], loss=68.8857
	step [185/192], loss=62.6948
	step [186/192], loss=59.5796
	step [187/192], loss=69.9692
	step [188/192], loss=66.7035
	step [189/192], loss=59.1034
	step [190/192], loss=63.3691
	step [191/192], loss=61.0780
	step [192/192], loss=58.2917
	Evaluating
	loss=0.0077, precision=0.3119, recall=0.8716, f1=0.4594
Training epoch 125
	step [1/192], loss=60.5853
	step [2/192], loss=60.1593
	step [3/192], loss=60.4858
	step [4/192], loss=58.0804
	step [5/192], loss=59.4762
	step [6/192], loss=63.2224
	step [7/192], loss=66.0055
	step [8/192], loss=77.2537
	step [9/192], loss=58.2855
	step [10/192], loss=59.0840
	step [11/192], loss=68.3651
	step [12/192], loss=62.8605
	step [13/192], loss=67.7995
	step [14/192], loss=54.8079
	step [15/192], loss=72.8059
	step [16/192], loss=67.3734
	step [17/192], loss=74.1468
	step [18/192], loss=61.2995
	step [19/192], loss=69.6822
	step [20/192], loss=64.7609
	step [21/192], loss=74.9908
	step [22/192], loss=68.7568
	step [23/192], loss=55.1868
	step [24/192], loss=57.8581
	step [25/192], loss=63.1219
	step [26/192], loss=59.1162
	step [27/192], loss=60.1978
	step [28/192], loss=62.2790
	step [29/192], loss=48.4481
	step [30/192], loss=58.5776
	step [31/192], loss=72.4362
	step [32/192], loss=65.1318
	step [33/192], loss=61.1516
	step [34/192], loss=74.3285
	step [35/192], loss=54.0140
	step [36/192], loss=67.4301
	step [37/192], loss=62.2953
	step [38/192], loss=71.0393
	step [39/192], loss=65.0000
	step [40/192], loss=73.4067
	step [41/192], loss=55.7426
	step [42/192], loss=77.7960
	step [43/192], loss=67.5674
	step [44/192], loss=57.1665
	step [45/192], loss=69.9130
	step [46/192], loss=59.7700
	step [47/192], loss=74.9823
	step [48/192], loss=55.0195
	step [49/192], loss=57.9351
	step [50/192], loss=68.2446
	step [51/192], loss=68.3987
	step [52/192], loss=65.5149
	step [53/192], loss=68.5229
	step [54/192], loss=61.0181
	step [55/192], loss=70.5544
	step [56/192], loss=63.0445
	step [57/192], loss=66.3277
	step [58/192], loss=62.0203
	step [59/192], loss=70.9796
	step [60/192], loss=62.6858
	step [61/192], loss=66.6610
	step [62/192], loss=69.2027
	step [63/192], loss=65.1416
	step [64/192], loss=61.6128
	step [65/192], loss=77.9734
	step [66/192], loss=58.5949
	step [67/192], loss=64.3465
	step [68/192], loss=63.3164
	step [69/192], loss=71.5079
	step [70/192], loss=65.5435
	step [71/192], loss=65.5208
	step [72/192], loss=72.3043
	step [73/192], loss=52.0986
	step [74/192], loss=66.6531
	step [75/192], loss=69.2286
	step [76/192], loss=67.5166
	step [77/192], loss=72.7225
	step [78/192], loss=53.9367
	step [79/192], loss=64.3901
	step [80/192], loss=55.1948
	step [81/192], loss=75.7106
	step [82/192], loss=62.8306
	step [83/192], loss=59.9491
	step [84/192], loss=57.3434
	step [85/192], loss=53.2899
	step [86/192], loss=57.7388
	step [87/192], loss=53.9113
	step [88/192], loss=68.5789
	step [89/192], loss=61.3966
	step [90/192], loss=69.7524
	step [91/192], loss=68.9743
	step [92/192], loss=80.3913
	step [93/192], loss=74.7578
	step [94/192], loss=60.3247
	step [95/192], loss=66.7759
	step [96/192], loss=64.4871
	step [97/192], loss=67.3290
	step [98/192], loss=64.2177
	step [99/192], loss=73.5290
	step [100/192], loss=78.1048
	step [101/192], loss=68.8222
	step [102/192], loss=68.9084
	step [103/192], loss=72.2654
	step [104/192], loss=67.1355
	step [105/192], loss=62.6986
	step [106/192], loss=64.0476
	step [107/192], loss=62.9481
	step [108/192], loss=65.1495
	step [109/192], loss=76.6579
	step [110/192], loss=50.0500
	step [111/192], loss=67.6792
	step [112/192], loss=68.7771
	step [113/192], loss=66.0879
	step [114/192], loss=67.7981
	step [115/192], loss=63.1625
	step [116/192], loss=60.4151
	step [117/192], loss=82.4042
	step [118/192], loss=53.6536
	step [119/192], loss=78.7839
	step [120/192], loss=63.9947
	step [121/192], loss=65.2284
	step [122/192], loss=55.1871
	step [123/192], loss=59.3945
	step [124/192], loss=53.8132
	step [125/192], loss=67.7597
	step [126/192], loss=70.5697
	step [127/192], loss=57.7378
	step [128/192], loss=59.1318
	step [129/192], loss=50.3991
	step [130/192], loss=63.9418
	step [131/192], loss=60.1109
	step [132/192], loss=67.2803
	step [133/192], loss=64.7065
	step [134/192], loss=68.7944
	step [135/192], loss=71.2794
	step [136/192], loss=71.7133
	step [137/192], loss=70.5298
	step [138/192], loss=70.0453
	step [139/192], loss=69.2915
	step [140/192], loss=63.8173
	step [141/192], loss=68.5234
	step [142/192], loss=61.3089
	step [143/192], loss=57.2603
	step [144/192], loss=67.4189
	step [145/192], loss=53.0624
	step [146/192], loss=61.5077
	step [147/192], loss=83.6270
	step [148/192], loss=63.1262
	step [149/192], loss=64.7206
	step [150/192], loss=69.4693
	step [151/192], loss=49.6050
	step [152/192], loss=66.1888
	step [153/192], loss=72.4400
	step [154/192], loss=67.7553
	step [155/192], loss=63.2017
	step [156/192], loss=61.7505
	step [157/192], loss=60.5227
	step [158/192], loss=71.8882
	step [159/192], loss=75.4014
	step [160/192], loss=60.3863
	step [161/192], loss=62.6674
	step [162/192], loss=89.8859
	step [163/192], loss=64.8973
	step [164/192], loss=64.6624
	step [165/192], loss=74.1917
	step [166/192], loss=60.7917
	step [167/192], loss=64.0058
	step [168/192], loss=69.8179
	step [169/192], loss=53.8483
	step [170/192], loss=74.3227
	step [171/192], loss=57.7290
	step [172/192], loss=73.6850
	step [173/192], loss=64.8865
	step [174/192], loss=58.6355
	step [175/192], loss=57.6602
	step [176/192], loss=67.0511
	step [177/192], loss=62.9002
	step [178/192], loss=56.5763
	step [179/192], loss=67.7786
	step [180/192], loss=64.4663
	step [181/192], loss=62.5945
	step [182/192], loss=56.2283
	step [183/192], loss=61.9502
	step [184/192], loss=71.5751
	step [185/192], loss=69.5918
	step [186/192], loss=76.4671
	step [187/192], loss=59.9235
	step [188/192], loss=67.9298
	step [189/192], loss=64.3217
	step [190/192], loss=60.3069
	step [191/192], loss=65.8670
	step [192/192], loss=52.5555
	Evaluating
	loss=0.0066, precision=0.3510, recall=0.8609, f1=0.4987
Training epoch 126
	step [1/192], loss=62.1520
	step [2/192], loss=73.0826
	step [3/192], loss=59.2284
	step [4/192], loss=64.8256
	step [5/192], loss=60.7965
	step [6/192], loss=66.6011
	step [7/192], loss=64.9725
	step [8/192], loss=81.8223
	step [9/192], loss=59.0914
	step [10/192], loss=56.4294
	step [11/192], loss=61.8801
	step [12/192], loss=60.4963
	step [13/192], loss=63.4095
	step [14/192], loss=63.9399
	step [15/192], loss=59.1120
	step [16/192], loss=68.2510
	step [17/192], loss=65.4508
	step [18/192], loss=55.8318
	step [19/192], loss=59.5123
	step [20/192], loss=60.2280
	step [21/192], loss=69.6334
	step [22/192], loss=71.5988
	step [23/192], loss=60.9945
	step [24/192], loss=61.8523
	step [25/192], loss=63.3910
	step [26/192], loss=62.2743
	step [27/192], loss=75.1876
	step [28/192], loss=71.5596
	step [29/192], loss=66.2974
	step [30/192], loss=73.6399
	step [31/192], loss=67.4324
	step [32/192], loss=70.8801
	step [33/192], loss=68.5972
	step [34/192], loss=69.2842
	step [35/192], loss=57.9454
	step [36/192], loss=57.2334
	step [37/192], loss=59.6366
	step [38/192], loss=63.9376
	step [39/192], loss=63.4089
	step [40/192], loss=64.6889
	step [41/192], loss=60.8095
	step [42/192], loss=60.4643
	step [43/192], loss=68.9373
	step [44/192], loss=59.8987
	step [45/192], loss=65.3265
	step [46/192], loss=71.5952
	step [47/192], loss=70.4436
	step [48/192], loss=64.0185
	step [49/192], loss=66.0319
	step [50/192], loss=58.2479
	step [51/192], loss=68.4409
	step [52/192], loss=54.9424
	step [53/192], loss=64.3392
	step [54/192], loss=76.4752
	step [55/192], loss=62.2453
	step [56/192], loss=52.6893
	step [57/192], loss=66.4785
	step [58/192], loss=53.5270
	step [59/192], loss=57.9963
	step [60/192], loss=61.0807
	step [61/192], loss=61.6752
	step [62/192], loss=63.8134
	step [63/192], loss=73.0622
	step [64/192], loss=63.1646
	step [65/192], loss=63.7415
	step [66/192], loss=68.0318
	step [67/192], loss=65.8794
	step [68/192], loss=61.1457
	step [69/192], loss=59.9387
	step [70/192], loss=62.3870
	step [71/192], loss=72.2437
	step [72/192], loss=54.1984
	step [73/192], loss=61.9402
	step [74/192], loss=72.5032
	step [75/192], loss=72.3756
	step [76/192], loss=57.7844
	step [77/192], loss=65.5033
	step [78/192], loss=56.6301
	step [79/192], loss=56.1735
	step [80/192], loss=70.1069
	step [81/192], loss=63.4506
	step [82/192], loss=64.6071
	step [83/192], loss=62.2108
	step [84/192], loss=77.7698
	step [85/192], loss=78.3697
	step [86/192], loss=57.3035
	step [87/192], loss=65.9717
	step [88/192], loss=47.2482
	step [89/192], loss=60.8774
	step [90/192], loss=58.6557
	step [91/192], loss=79.7102
	step [92/192], loss=74.5822
	step [93/192], loss=65.5350
	step [94/192], loss=60.9690
	step [95/192], loss=57.7503
	step [96/192], loss=74.7347
	step [97/192], loss=77.1316
	step [98/192], loss=79.2194
	step [99/192], loss=72.9650
	step [100/192], loss=61.4556
	step [101/192], loss=65.1627
	step [102/192], loss=63.1823
	step [103/192], loss=68.3960
	step [104/192], loss=68.6236
	step [105/192], loss=55.2579
	step [106/192], loss=61.0383
	step [107/192], loss=66.0642
	step [108/192], loss=64.1824
	step [109/192], loss=76.4628
	step [110/192], loss=62.6187
	step [111/192], loss=75.7855
	step [112/192], loss=67.2012
	step [113/192], loss=57.5294
	step [114/192], loss=75.1077
	step [115/192], loss=58.9215
	step [116/192], loss=66.2449
	step [117/192], loss=64.8951
	step [118/192], loss=58.4484
	step [119/192], loss=61.4922
	step [120/192], loss=58.0509
	step [121/192], loss=73.7727
	step [122/192], loss=66.2375
	step [123/192], loss=57.1083
	step [124/192], loss=66.3046
	step [125/192], loss=65.0782
	step [126/192], loss=60.2283
	step [127/192], loss=81.8826
	step [128/192], loss=69.2381
	step [129/192], loss=65.5453
	step [130/192], loss=70.5373
	step [131/192], loss=51.4248
	step [132/192], loss=67.4683
	step [133/192], loss=70.4720
	step [134/192], loss=68.4593
	step [135/192], loss=85.7593
	step [136/192], loss=71.1017
	step [137/192], loss=66.2279
	step [138/192], loss=62.9905
	step [139/192], loss=63.6785
	step [140/192], loss=64.4625
	step [141/192], loss=60.2928
	step [142/192], loss=65.9015
	step [143/192], loss=72.8954
	step [144/192], loss=60.6286
	step [145/192], loss=68.7449
	step [146/192], loss=67.8199
	step [147/192], loss=63.2150
	step [148/192], loss=69.5275
	step [149/192], loss=52.8051
	step [150/192], loss=64.8695
	step [151/192], loss=80.4730
	step [152/192], loss=59.5427
	step [153/192], loss=55.7426
	step [154/192], loss=74.3153
	step [155/192], loss=61.2599
	step [156/192], loss=59.5305
	step [157/192], loss=67.7736
	step [158/192], loss=57.0184
	step [159/192], loss=66.1320
	step [160/192], loss=65.6970
	step [161/192], loss=67.3083
	step [162/192], loss=69.2720
	step [163/192], loss=69.2018
	step [164/192], loss=56.7217
	step [165/192], loss=66.6986
	step [166/192], loss=66.3593
	step [167/192], loss=70.9151
	step [168/192], loss=64.6589
	step [169/192], loss=58.8470
	step [170/192], loss=58.0161
	step [171/192], loss=62.2336
	step [172/192], loss=67.4408
	step [173/192], loss=70.5159
	step [174/192], loss=69.6327
	step [175/192], loss=56.5098
	step [176/192], loss=70.5925
	step [177/192], loss=67.5303
	step [178/192], loss=74.6150
	step [179/192], loss=56.9486
	step [180/192], loss=59.6113
	step [181/192], loss=60.6756
	step [182/192], loss=75.7205
	step [183/192], loss=66.8092
	step [184/192], loss=68.6302
	step [185/192], loss=67.8125
	step [186/192], loss=68.4294
	step [187/192], loss=78.3799
	step [188/192], loss=59.2685
	step [189/192], loss=58.5369
	step [190/192], loss=54.1485
	step [191/192], loss=57.2258
	step [192/192], loss=49.6489
	Evaluating
	loss=0.0058, precision=0.4239, recall=0.8672, f1=0.5694
Training epoch 127
	step [1/192], loss=54.0012
	step [2/192], loss=59.2220
	step [3/192], loss=79.3889
	step [4/192], loss=66.3825
	step [5/192], loss=65.5260
	step [6/192], loss=65.4762
	step [7/192], loss=66.9443
	step [8/192], loss=54.9006
	step [9/192], loss=54.9996
	step [10/192], loss=78.3615
	step [11/192], loss=81.6054
	step [12/192], loss=65.3842
	step [13/192], loss=62.3139
	step [14/192], loss=65.8820
	step [15/192], loss=65.9283
	step [16/192], loss=71.7320
	step [17/192], loss=63.8204
	step [18/192], loss=56.4990
	step [19/192], loss=69.9881
	step [20/192], loss=71.2072
	step [21/192], loss=64.1436
	step [22/192], loss=65.9635
	step [23/192], loss=59.3311
	step [24/192], loss=54.5383
	step [25/192], loss=64.9482
	step [26/192], loss=56.0366
	step [27/192], loss=65.4509
	step [28/192], loss=63.4511
	step [29/192], loss=68.0393
	step [30/192], loss=59.9591
	step [31/192], loss=59.3131
	step [32/192], loss=77.3643
	step [33/192], loss=70.7656
	step [34/192], loss=72.3683
	step [35/192], loss=65.8410
	step [36/192], loss=54.4828
	step [37/192], loss=55.5430
	step [38/192], loss=59.8325
	step [39/192], loss=62.4434
	step [40/192], loss=60.5486
	step [41/192], loss=53.4418
	step [42/192], loss=69.3155
	step [43/192], loss=64.5414
	step [44/192], loss=79.1248
	step [45/192], loss=57.7030
	step [46/192], loss=69.9887
	step [47/192], loss=59.0407
	step [48/192], loss=67.3981
	step [49/192], loss=73.4142
	step [50/192], loss=61.2053
	step [51/192], loss=63.8862
	step [52/192], loss=68.9099
	step [53/192], loss=62.8242
	step [54/192], loss=63.8927
	step [55/192], loss=70.7069
	step [56/192], loss=67.4203
	step [57/192], loss=63.6362
	step [58/192], loss=73.8429
	step [59/192], loss=52.6179
	step [60/192], loss=51.3294
	step [61/192], loss=61.9511
	step [62/192], loss=66.1144
	step [63/192], loss=61.1061
	step [64/192], loss=64.3047
	step [65/192], loss=58.8558
	step [66/192], loss=66.2840
	step [67/192], loss=58.4345
	step [68/192], loss=68.7177
	step [69/192], loss=61.8228
	step [70/192], loss=70.6814
	step [71/192], loss=69.3291
	step [72/192], loss=65.1016
	step [73/192], loss=63.3799
	step [74/192], loss=64.0307
	step [75/192], loss=59.0911
	step [76/192], loss=72.1977
	step [77/192], loss=69.0511
	step [78/192], loss=66.8307
	step [79/192], loss=63.3570
	step [80/192], loss=56.2049
	step [81/192], loss=58.5822
	step [82/192], loss=74.5422
	step [83/192], loss=66.1897
	step [84/192], loss=60.2024
	step [85/192], loss=67.0471
	step [86/192], loss=61.0845
	step [87/192], loss=60.2095
	step [88/192], loss=60.1591
	step [89/192], loss=72.2636
	step [90/192], loss=75.7993
	step [91/192], loss=71.0236
	step [92/192], loss=59.6852
	step [93/192], loss=68.9580
	step [94/192], loss=67.9876
	step [95/192], loss=67.0181
	step [96/192], loss=72.8052
	step [97/192], loss=68.1889
	step [98/192], loss=60.6554
	step [99/192], loss=64.2562
	step [100/192], loss=57.4981
	step [101/192], loss=68.7442
	step [102/192], loss=68.1301
	step [103/192], loss=63.9501
	step [104/192], loss=69.3169
	step [105/192], loss=68.0347
	step [106/192], loss=60.4937
	step [107/192], loss=65.0335
	step [108/192], loss=57.1614
	step [109/192], loss=61.1799
	step [110/192], loss=74.1428
	step [111/192], loss=65.4677
	step [112/192], loss=72.3414
	step [113/192], loss=62.5219
	step [114/192], loss=60.2689
	step [115/192], loss=69.2593
	step [116/192], loss=63.1798
	step [117/192], loss=64.9883
	step [118/192], loss=48.9565
	step [119/192], loss=59.2766
	step [120/192], loss=54.1765
	step [121/192], loss=60.3848
	step [122/192], loss=61.7699
	step [123/192], loss=64.0675
	step [124/192], loss=70.6887
	step [125/192], loss=74.0789
	step [126/192], loss=62.6191
	step [127/192], loss=58.1667
	step [128/192], loss=64.1807
	step [129/192], loss=61.2966
	step [130/192], loss=63.8381
	step [131/192], loss=70.6602
	step [132/192], loss=68.4241
	step [133/192], loss=57.0605
	step [134/192], loss=66.4648
	step [135/192], loss=73.9130
	step [136/192], loss=69.6412
	step [137/192], loss=69.2257
	step [138/192], loss=58.7664
	step [139/192], loss=53.4930
	step [140/192], loss=59.0351
	step [141/192], loss=58.3400
	step [142/192], loss=68.7292
	step [143/192], loss=58.2048
	step [144/192], loss=57.4041
	step [145/192], loss=67.3820
	step [146/192], loss=55.5027
	step [147/192], loss=61.0547
	step [148/192], loss=67.2953
	step [149/192], loss=59.9917
	step [150/192], loss=59.9184
	step [151/192], loss=68.1673
	step [152/192], loss=58.4453
	step [153/192], loss=68.7852
	step [154/192], loss=70.7639
	step [155/192], loss=68.9478
	step [156/192], loss=58.8534
	step [157/192], loss=66.6993
	step [158/192], loss=70.6013
	step [159/192], loss=78.3769
	step [160/192], loss=62.6060
	step [161/192], loss=61.6509
	step [162/192], loss=70.0668
	step [163/192], loss=65.1464
	step [164/192], loss=66.2098
	step [165/192], loss=63.1178
	step [166/192], loss=62.7303
	step [167/192], loss=65.7096
	step [168/192], loss=77.9670
	step [169/192], loss=72.6460
	step [170/192], loss=62.5963
	step [171/192], loss=69.8214
	step [172/192], loss=78.0180
	step [173/192], loss=48.3036
	step [174/192], loss=63.1335
	step [175/192], loss=83.3589
	step [176/192], loss=78.5980
	step [177/192], loss=66.3754
	step [178/192], loss=64.3262
	step [179/192], loss=74.3358
	step [180/192], loss=68.8645
	step [181/192], loss=60.5676
	step [182/192], loss=53.6993
	step [183/192], loss=69.0126
	step [184/192], loss=66.2970
	step [185/192], loss=66.8416
	step [186/192], loss=65.3227
	step [187/192], loss=62.6586
	step [188/192], loss=67.8933
	step [189/192], loss=65.9725
	step [190/192], loss=62.8830
	step [191/192], loss=55.6108
	step [192/192], loss=44.1973
	Evaluating
	loss=0.0070, precision=0.3278, recall=0.8651, f1=0.4755
Training epoch 128
	step [1/192], loss=63.3415
	step [2/192], loss=62.4672
	step [3/192], loss=70.1652
	step [4/192], loss=66.2100
	step [5/192], loss=63.0377
	step [6/192], loss=74.0667
	step [7/192], loss=64.8144
	step [8/192], loss=63.2358
	step [9/192], loss=66.1543
	step [10/192], loss=65.3113
	step [11/192], loss=58.0084
	step [12/192], loss=62.0642
	step [13/192], loss=49.5178
	step [14/192], loss=70.9159
	step [15/192], loss=69.0853
	step [16/192], loss=64.5922
	step [17/192], loss=59.9738
	step [18/192], loss=65.7069
	step [19/192], loss=67.9593
	step [20/192], loss=70.3422
	step [21/192], loss=62.2006
	step [22/192], loss=67.3878
	step [23/192], loss=60.6431
	step [24/192], loss=57.9889
	step [25/192], loss=56.7608
	step [26/192], loss=58.2778
	step [27/192], loss=54.9983
	step [28/192], loss=68.9325
	step [29/192], loss=67.8903
	step [30/192], loss=66.4288
	step [31/192], loss=63.9359
	step [32/192], loss=65.8028
	step [33/192], loss=60.3937
	step [34/192], loss=53.4838
	step [35/192], loss=59.3447
	step [36/192], loss=66.0303
	step [37/192], loss=69.8762
	step [38/192], loss=75.3340
	step [39/192], loss=50.6242
	step [40/192], loss=74.0663
	step [41/192], loss=66.6095
	step [42/192], loss=59.3502
	step [43/192], loss=79.5667
	step [44/192], loss=62.6838
	step [45/192], loss=66.0385
	step [46/192], loss=64.1918
	step [47/192], loss=73.6355
	step [48/192], loss=59.2415
	step [49/192], loss=66.8349
	step [50/192], loss=54.6550
	step [51/192], loss=55.9065
	step [52/192], loss=61.7608
	step [53/192], loss=62.5539
	step [54/192], loss=77.5917
	step [55/192], loss=56.7631
	step [56/192], loss=60.8095
	step [57/192], loss=63.2965
	step [58/192], loss=55.1093
	step [59/192], loss=54.8792
	step [60/192], loss=48.8929
	step [61/192], loss=68.2891
	step [62/192], loss=67.0464
	step [63/192], loss=64.7830
	step [64/192], loss=62.9163
	step [65/192], loss=62.0866
	step [66/192], loss=64.4581
	step [67/192], loss=68.6615
	step [68/192], loss=59.6666
	step [69/192], loss=63.2806
	step [70/192], loss=64.3199
	step [71/192], loss=71.4463
	step [72/192], loss=61.4892
	step [73/192], loss=68.1013
	step [74/192], loss=74.4739
	step [75/192], loss=58.4917
	step [76/192], loss=70.9182
	step [77/192], loss=56.7919
	step [78/192], loss=72.8100
	step [79/192], loss=64.4463
	step [80/192], loss=59.8749
	step [81/192], loss=73.0877
	step [82/192], loss=69.8177
	step [83/192], loss=61.9179
	step [84/192], loss=67.8006
	step [85/192], loss=62.4811
	step [86/192], loss=58.9923
	step [87/192], loss=62.3510
	step [88/192], loss=57.1095
	step [89/192], loss=66.6623
	step [90/192], loss=64.1678
	step [91/192], loss=61.9632
	step [92/192], loss=66.8389
	step [93/192], loss=63.2840
	step [94/192], loss=62.9140
	step [95/192], loss=57.8656
	step [96/192], loss=88.9062
	step [97/192], loss=73.2796
	step [98/192], loss=68.4967
	step [99/192], loss=58.0404
	step [100/192], loss=61.5317
	step [101/192], loss=80.4924
	step [102/192], loss=69.7101
	step [103/192], loss=74.6045
	step [104/192], loss=65.5977
	step [105/192], loss=55.5337
	step [106/192], loss=74.7555
	step [107/192], loss=55.1112
	step [108/192], loss=52.6538
	step [109/192], loss=61.7947
	step [110/192], loss=67.5987
	step [111/192], loss=65.1406
	step [112/192], loss=70.8448
	step [113/192], loss=63.8861
	step [114/192], loss=60.4222
	step [115/192], loss=54.1413
	step [116/192], loss=67.9002
	step [117/192], loss=69.4762
	step [118/192], loss=69.3678
	step [119/192], loss=62.5681
	step [120/192], loss=61.0097
	step [121/192], loss=69.6685
	step [122/192], loss=66.8026
	step [123/192], loss=56.3344
	step [124/192], loss=59.8145
	step [125/192], loss=49.1995
	step [126/192], loss=63.2485
	step [127/192], loss=54.1917
	step [128/192], loss=69.6671
	step [129/192], loss=56.7823
	step [130/192], loss=65.5761
	step [131/192], loss=56.9543
	step [132/192], loss=63.7793
	step [133/192], loss=65.3573
	step [134/192], loss=81.2785
	step [135/192], loss=74.7893
	step [136/192], loss=70.1715
	step [137/192], loss=65.1447
	step [138/192], loss=63.0475
	step [139/192], loss=66.2402
	step [140/192], loss=56.7945
	step [141/192], loss=70.8370
	step [142/192], loss=73.9366
	step [143/192], loss=57.0763
	step [144/192], loss=63.3622
	step [145/192], loss=52.8992
	step [146/192], loss=61.1948
	step [147/192], loss=63.2019
	step [148/192], loss=66.8147
	step [149/192], loss=66.0291
	step [150/192], loss=64.1596
	step [151/192], loss=79.7932
	step [152/192], loss=56.7273
	step [153/192], loss=64.4722
	step [154/192], loss=54.4342
	step [155/192], loss=73.1315
	step [156/192], loss=67.7673
	step [157/192], loss=62.2025
	step [158/192], loss=68.1579
	step [159/192], loss=69.1788
	step [160/192], loss=62.1695
	step [161/192], loss=72.0522
	step [162/192], loss=71.8752
	step [163/192], loss=76.2178
	step [164/192], loss=70.7447
	step [165/192], loss=72.9706
	step [166/192], loss=62.3404
	step [167/192], loss=68.9489
	step [168/192], loss=75.8698
	step [169/192], loss=63.1063
	step [170/192], loss=71.4271
	step [171/192], loss=64.1008
	step [172/192], loss=59.9515
	step [173/192], loss=76.3442
	step [174/192], loss=71.5624
	step [175/192], loss=59.4997
	step [176/192], loss=58.4137
	step [177/192], loss=64.1633
	step [178/192], loss=64.2976
	step [179/192], loss=60.2329
	step [180/192], loss=74.4950
	step [181/192], loss=61.0639
	step [182/192], loss=68.4765
	step [183/192], loss=63.6412
	step [184/192], loss=73.9747
	step [185/192], loss=53.8431
	step [186/192], loss=67.7406
	step [187/192], loss=62.5778
	step [188/192], loss=74.3387
	step [189/192], loss=54.4823
	step [190/192], loss=53.5355
	step [191/192], loss=59.0498
	step [192/192], loss=47.2194
	Evaluating
	loss=0.0071, precision=0.3353, recall=0.8585, f1=0.4823
Training epoch 129
	step [1/192], loss=65.0024
	step [2/192], loss=62.1462
	step [3/192], loss=64.3013
	step [4/192], loss=64.7483
	step [5/192], loss=61.5641
	step [6/192], loss=62.6953
	step [7/192], loss=58.2183
	step [8/192], loss=69.6977
	step [9/192], loss=61.7346
	step [10/192], loss=61.8987
	step [11/192], loss=62.0298
	step [12/192], loss=70.5366
	step [13/192], loss=66.0745
	step [14/192], loss=51.9336
	step [15/192], loss=62.8451
	step [16/192], loss=62.1234
	step [17/192], loss=49.6643
	step [18/192], loss=61.8070
	step [19/192], loss=66.5289
	step [20/192], loss=61.3387
	step [21/192], loss=57.2079
	step [22/192], loss=67.2639
	step [23/192], loss=65.2208
	step [24/192], loss=66.9565
	step [25/192], loss=62.2700
	step [26/192], loss=61.9929
	step [27/192], loss=68.5478
	step [28/192], loss=64.4080
	step [29/192], loss=56.6137
	step [30/192], loss=69.3813
	step [31/192], loss=64.7492
	step [32/192], loss=65.4978
	step [33/192], loss=61.0562
	step [34/192], loss=61.8838
	step [35/192], loss=65.0150
	step [36/192], loss=59.7334
	step [37/192], loss=65.0005
	step [38/192], loss=67.0842
	step [39/192], loss=71.2798
	step [40/192], loss=68.4025
	step [41/192], loss=63.5331
	step [42/192], loss=69.0080
	step [43/192], loss=70.2147
	step [44/192], loss=62.5259
	step [45/192], loss=52.3678
	step [46/192], loss=61.2431
	step [47/192], loss=63.4057
	step [48/192], loss=68.6164
	step [49/192], loss=61.8510
	step [50/192], loss=65.5306
	step [51/192], loss=71.3954
	step [52/192], loss=73.3934
	step [53/192], loss=65.8669
	step [54/192], loss=66.9886
	step [55/192], loss=67.4592
	step [56/192], loss=63.3541
	step [57/192], loss=73.6429
	step [58/192], loss=73.9043
	step [59/192], loss=67.7729
	step [60/192], loss=62.4839
	step [61/192], loss=70.6987
	step [62/192], loss=65.8083
	step [63/192], loss=65.2591
	step [64/192], loss=59.1362
	step [65/192], loss=62.4622
	step [66/192], loss=55.9137
	step [67/192], loss=84.1327
	step [68/192], loss=68.3605
	step [69/192], loss=70.3642
	step [70/192], loss=55.5434
	step [71/192], loss=62.4419
	step [72/192], loss=57.1821
	step [73/192], loss=68.3117
	step [74/192], loss=69.4914
	step [75/192], loss=60.7698
	step [76/192], loss=57.2049
	step [77/192], loss=59.2087
	step [78/192], loss=62.5457
	step [79/192], loss=65.8717
	step [80/192], loss=64.5246
	step [81/192], loss=67.5632
	step [82/192], loss=79.7297
	step [83/192], loss=60.8964
	step [84/192], loss=50.8602
	step [85/192], loss=69.4743
	step [86/192], loss=62.7119
	step [87/192], loss=55.7776
	step [88/192], loss=60.3532
	step [89/192], loss=65.8430
	step [90/192], loss=75.6858
	step [91/192], loss=57.9379
	step [92/192], loss=72.4958
	step [93/192], loss=65.4686
	step [94/192], loss=55.7938
	step [95/192], loss=74.0278
	step [96/192], loss=72.4281
	step [97/192], loss=71.3735
	step [98/192], loss=54.5584
	step [99/192], loss=60.4277
	step [100/192], loss=63.5701
	step [101/192], loss=61.1788
	step [102/192], loss=59.2925
	step [103/192], loss=61.3493
	step [104/192], loss=78.8320
	step [105/192], loss=61.2464
	step [106/192], loss=66.3555
	step [107/192], loss=77.5427
	step [108/192], loss=65.1375
	step [109/192], loss=56.4761
	step [110/192], loss=86.1347
	step [111/192], loss=54.9670
	step [112/192], loss=70.2212
	step [113/192], loss=71.6469
	step [114/192], loss=60.4192
	step [115/192], loss=70.0828
	step [116/192], loss=51.6097
	step [117/192], loss=53.5003
	step [118/192], loss=67.0641
	step [119/192], loss=54.3580
	step [120/192], loss=54.9068
	step [121/192], loss=73.4966
	step [122/192], loss=65.4350
	step [123/192], loss=67.9625
	step [124/192], loss=69.4318
	step [125/192], loss=81.9423
	step [126/192], loss=70.2155
	step [127/192], loss=63.7399
	step [128/192], loss=66.7832
	step [129/192], loss=53.1386
	step [130/192], loss=61.4021
	step [131/192], loss=57.2837
	step [132/192], loss=70.7154
	step [133/192], loss=62.1039
	step [134/192], loss=67.7903
	step [135/192], loss=64.7829
	step [136/192], loss=58.2790
	step [137/192], loss=63.7569
	step [138/192], loss=65.5799
	step [139/192], loss=69.6844
	step [140/192], loss=56.7836
	step [141/192], loss=70.3295
	step [142/192], loss=54.9732
	step [143/192], loss=58.9642
	step [144/192], loss=69.3198
	step [145/192], loss=65.6290
	step [146/192], loss=67.7374
	step [147/192], loss=62.0392
	step [148/192], loss=54.5277
	step [149/192], loss=74.7290
	step [150/192], loss=58.6586
	step [151/192], loss=71.1035
	step [152/192], loss=72.3496
	step [153/192], loss=51.4362
	step [154/192], loss=63.7650
	step [155/192], loss=70.8262
	step [156/192], loss=61.7126
	step [157/192], loss=77.0223
	step [158/192], loss=58.0714
	step [159/192], loss=55.3608
	step [160/192], loss=69.3653
	step [161/192], loss=61.0138
	step [162/192], loss=60.5312
	step [163/192], loss=63.6396
	step [164/192], loss=77.8829
	step [165/192], loss=63.8318
	step [166/192], loss=61.4511
	step [167/192], loss=60.0544
	step [168/192], loss=60.7387
	step [169/192], loss=58.6760
	step [170/192], loss=61.5540
	step [171/192], loss=65.1253
	step [172/192], loss=59.8823
	step [173/192], loss=58.4354
	step [174/192], loss=58.5232
	step [175/192], loss=51.4963
	step [176/192], loss=54.5413
	step [177/192], loss=86.9452
	step [178/192], loss=68.5190
	step [179/192], loss=60.8516
	step [180/192], loss=66.9672
	step [181/192], loss=72.7305
	step [182/192], loss=71.2423
	step [183/192], loss=72.7087
	step [184/192], loss=67.5807
	step [185/192], loss=58.6592
	step [186/192], loss=72.7613
	step [187/192], loss=67.5119
	step [188/192], loss=73.3236
	step [189/192], loss=57.8990
	step [190/192], loss=58.8994
	step [191/192], loss=67.2340
	step [192/192], loss=64.5146
	Evaluating
	loss=0.0068, precision=0.3813, recall=0.8653, f1=0.5294
Training epoch 130
	step [1/192], loss=58.8587
	step [2/192], loss=74.0666
	step [3/192], loss=70.9837
	step [4/192], loss=63.9895
	step [5/192], loss=58.8280
	step [6/192], loss=65.7325
	step [7/192], loss=63.8263
	step [8/192], loss=66.5699
	step [9/192], loss=65.2648
	step [10/192], loss=56.4287
	step [11/192], loss=68.0674
	step [12/192], loss=65.5312
	step [13/192], loss=67.3412
	step [14/192], loss=69.7902
	step [15/192], loss=60.6297
	step [16/192], loss=56.3660
	step [17/192], loss=71.8273
	step [18/192], loss=71.1546
	step [19/192], loss=65.3874
	step [20/192], loss=62.2082
	step [21/192], loss=70.5532
	step [22/192], loss=57.5628
	step [23/192], loss=66.8252
	step [24/192], loss=62.8773
	step [25/192], loss=58.6688
	step [26/192], loss=59.3988
	step [27/192], loss=65.1864
	step [28/192], loss=58.9054
	step [29/192], loss=57.2469
	step [30/192], loss=62.6335
	step [31/192], loss=68.7622
	step [32/192], loss=62.7238
	step [33/192], loss=72.5050
	step [34/192], loss=64.8944
	step [35/192], loss=56.2138
	step [36/192], loss=59.4640
	step [37/192], loss=63.9415
	step [38/192], loss=58.7991
	step [39/192], loss=71.5638
	step [40/192], loss=66.3481
	step [41/192], loss=64.8072
	step [42/192], loss=63.6200
	step [43/192], loss=62.5715
	step [44/192], loss=70.5172
	step [45/192], loss=58.2036
	step [46/192], loss=65.1309
	step [47/192], loss=74.1246
	step [48/192], loss=73.3199
	step [49/192], loss=59.8476
	step [50/192], loss=62.6560
	step [51/192], loss=61.8889
	step [52/192], loss=67.0431
	step [53/192], loss=62.3853
	step [54/192], loss=51.5075
	step [55/192], loss=69.5525
	step [56/192], loss=68.1212
	step [57/192], loss=59.4425
	step [58/192], loss=68.1244
	step [59/192], loss=69.0494
	step [60/192], loss=61.2636
	step [61/192], loss=62.7874
	step [62/192], loss=77.5774
	step [63/192], loss=62.8597
	step [64/192], loss=62.6556
	step [65/192], loss=69.0763
	step [66/192], loss=66.4475
	step [67/192], loss=57.5289
	step [68/192], loss=54.7515
	step [69/192], loss=77.8423
	step [70/192], loss=69.6246
	step [71/192], loss=60.5309
	step [72/192], loss=61.5450
	step [73/192], loss=73.7822
	step [74/192], loss=60.1214
	step [75/192], loss=71.9996
	step [76/192], loss=61.8309
	step [77/192], loss=68.0338
	step [78/192], loss=60.8287
	step [79/192], loss=68.5824
	step [80/192], loss=58.7861
	step [81/192], loss=73.0727
	step [82/192], loss=70.8614
	step [83/192], loss=66.5645
	step [84/192], loss=43.7172
	step [85/192], loss=67.1557
	step [86/192], loss=58.8774
	step [87/192], loss=64.4591
	step [88/192], loss=68.4227
	step [89/192], loss=68.3387
	step [90/192], loss=57.5213
	step [91/192], loss=54.9138
	step [92/192], loss=70.7533
	step [93/192], loss=76.3760
	step [94/192], loss=72.2300
	step [95/192], loss=64.4581
	step [96/192], loss=69.0512
	step [97/192], loss=57.1510
	step [98/192], loss=49.0100
	step [99/192], loss=75.1773
	step [100/192], loss=68.9184
	step [101/192], loss=57.2152
	step [102/192], loss=66.0930
	step [103/192], loss=61.3040
	step [104/192], loss=68.6699
	step [105/192], loss=66.0378
	step [106/192], loss=61.0038
	step [107/192], loss=71.4968
	step [108/192], loss=61.0268
	step [109/192], loss=54.9106
	step [110/192], loss=73.6819
	step [111/192], loss=67.0159
	step [112/192], loss=63.7003
	step [113/192], loss=72.0088
	step [114/192], loss=64.1136
	step [115/192], loss=74.1982
	step [116/192], loss=58.9278
	step [117/192], loss=63.9635
	step [118/192], loss=59.2725
	step [119/192], loss=67.7888
	step [120/192], loss=65.5196
	step [121/192], loss=70.1468
	step [122/192], loss=72.4606
	step [123/192], loss=58.7445
	step [124/192], loss=65.1185
	step [125/192], loss=75.9592
	step [126/192], loss=72.9970
	step [127/192], loss=66.5242
	step [128/192], loss=59.1094
	step [129/192], loss=48.4100
	step [130/192], loss=59.1415
	step [131/192], loss=55.3576
	step [132/192], loss=58.9413
	step [133/192], loss=76.0155
	step [134/192], loss=70.2736
	step [135/192], loss=59.5083
	step [136/192], loss=64.2005
	step [137/192], loss=67.2771
	step [138/192], loss=60.0673
	step [139/192], loss=60.4271
	step [140/192], loss=66.2027
	step [141/192], loss=69.4801
	step [142/192], loss=72.3570
	step [143/192], loss=64.1194
	step [144/192], loss=65.9630
	step [145/192], loss=61.4272
	step [146/192], loss=64.9117
	step [147/192], loss=58.6705
	step [148/192], loss=50.8064
	step [149/192], loss=69.9900
	step [150/192], loss=56.8342
	step [151/192], loss=58.4658
	step [152/192], loss=67.5279
	step [153/192], loss=60.3049
	step [154/192], loss=65.1421
	step [155/192], loss=53.6081
	step [156/192], loss=66.1415
	step [157/192], loss=63.8146
	step [158/192], loss=51.0321
	step [159/192], loss=73.6392
	step [160/192], loss=65.4693
	step [161/192], loss=77.5726
	step [162/192], loss=73.7403
	step [163/192], loss=71.1226
	step [164/192], loss=63.2142
	step [165/192], loss=69.5828
	step [166/192], loss=62.4091
	step [167/192], loss=57.1633
	step [168/192], loss=58.2565
	step [169/192], loss=66.0985
	step [170/192], loss=71.0832
	step [171/192], loss=55.2450
	step [172/192], loss=66.3624
	step [173/192], loss=59.7448
	step [174/192], loss=67.4227
	step [175/192], loss=54.8492
	step [176/192], loss=57.2870
	step [177/192], loss=56.6003
	step [178/192], loss=58.6734
	step [179/192], loss=64.6314
	step [180/192], loss=69.0931
	step [181/192], loss=70.5801
	step [182/192], loss=62.0603
	step [183/192], loss=52.7772
	step [184/192], loss=79.4492
	step [185/192], loss=67.6199
	step [186/192], loss=70.9901
	step [187/192], loss=64.5391
	step [188/192], loss=64.4035
	step [189/192], loss=69.5907
	step [190/192], loss=55.7301
	step [191/192], loss=61.6779
	step [192/192], loss=52.5153
	Evaluating
	loss=0.0057, precision=0.4243, recall=0.8634, f1=0.5690
Training epoch 131
	step [1/192], loss=66.5046
	step [2/192], loss=69.3669
	step [3/192], loss=61.1898
	step [4/192], loss=76.1417
	step [5/192], loss=59.2869
	step [6/192], loss=74.2108
	step [7/192], loss=66.0261
	step [8/192], loss=57.3821
	step [9/192], loss=60.0394
	step [10/192], loss=69.2294
	step [11/192], loss=69.7515
	step [12/192], loss=55.7350
	step [13/192], loss=61.7933
	step [14/192], loss=64.7860
	step [15/192], loss=79.0179
	step [16/192], loss=73.5502
	step [17/192], loss=68.4416
	step [18/192], loss=64.0189
	step [19/192], loss=70.8870
	step [20/192], loss=58.8720
	step [21/192], loss=64.6755
	step [22/192], loss=72.9715
	step [23/192], loss=60.5646
	step [24/192], loss=50.8268
	step [25/192], loss=58.9377
	step [26/192], loss=66.0557
	step [27/192], loss=75.7930
	step [28/192], loss=79.2391
	step [29/192], loss=57.9854
	step [30/192], loss=53.3398
	step [31/192], loss=70.0202
	step [32/192], loss=59.3216
	step [33/192], loss=67.3794
	step [34/192], loss=58.0246
	step [35/192], loss=65.0400
	step [36/192], loss=72.1437
	step [37/192], loss=63.7231
	step [38/192], loss=80.4560
	step [39/192], loss=59.2085
	step [40/192], loss=62.6372
	step [41/192], loss=68.0516
	step [42/192], loss=51.8681
	step [43/192], loss=77.7647
	step [44/192], loss=66.6813
	step [45/192], loss=62.8207
	step [46/192], loss=61.2397
	step [47/192], loss=66.5975
	step [48/192], loss=59.2860
	step [49/192], loss=66.6538
	step [50/192], loss=53.5472
	step [51/192], loss=53.0425
	step [52/192], loss=60.3018
	step [53/192], loss=67.8141
	step [54/192], loss=69.6637
	step [55/192], loss=59.3700
	step [56/192], loss=55.6578
	step [57/192], loss=56.6531
	step [58/192], loss=74.4439
	step [59/192], loss=53.4418
	step [60/192], loss=73.9007
	step [61/192], loss=70.8235
	step [62/192], loss=56.9235
	step [63/192], loss=65.6575
	step [64/192], loss=68.0420
	step [65/192], loss=61.5727
	step [66/192], loss=65.1556
	step [67/192], loss=62.5585
	step [68/192], loss=55.5606
	step [69/192], loss=58.1890
	step [70/192], loss=59.6734
	step [71/192], loss=63.0196
	step [72/192], loss=67.3521
	step [73/192], loss=66.5113
	step [74/192], loss=56.6274
	step [75/192], loss=65.7735
	step [76/192], loss=66.4348
	step [77/192], loss=65.7415
	step [78/192], loss=61.3864
	step [79/192], loss=63.1128
	step [80/192], loss=72.0430
	step [81/192], loss=67.9976
	step [82/192], loss=56.0071
	step [83/192], loss=61.6138
	step [84/192], loss=61.7728
	step [85/192], loss=58.3779
	step [86/192], loss=67.2784
	step [87/192], loss=65.0189
	step [88/192], loss=57.7569
	step [89/192], loss=63.6320
	step [90/192], loss=63.6216
	step [91/192], loss=51.6017
	step [92/192], loss=63.4223
	step [93/192], loss=69.3477
	step [94/192], loss=65.0118
	step [95/192], loss=61.3349
	step [96/192], loss=69.0060
	step [97/192], loss=66.8782
	step [98/192], loss=55.8229
	step [99/192], loss=56.0984
	step [100/192], loss=57.7227
	step [101/192], loss=52.8752
	step [102/192], loss=60.2474
	step [103/192], loss=79.8673
	step [104/192], loss=70.0191
	step [105/192], loss=57.8992
	step [106/192], loss=75.7775
	step [107/192], loss=67.4124
	step [108/192], loss=64.3965
	step [109/192], loss=69.7931
	step [110/192], loss=65.7237
	step [111/192], loss=63.6216
	step [112/192], loss=78.8504
	step [113/192], loss=58.9958
	step [114/192], loss=71.3934
	step [115/192], loss=60.8496
	step [116/192], loss=66.1264
	step [117/192], loss=63.5652
	step [118/192], loss=79.0164
	step [119/192], loss=72.6578
	step [120/192], loss=59.4898
	step [121/192], loss=68.8926
	step [122/192], loss=63.9805
	step [123/192], loss=62.6782
	step [124/192], loss=68.1660
	step [125/192], loss=75.8807
	step [126/192], loss=72.6990
	step [127/192], loss=68.3554
	step [128/192], loss=59.4788
	step [129/192], loss=60.0558
	step [130/192], loss=56.3575
	step [131/192], loss=57.3515
	step [132/192], loss=70.8755
	step [133/192], loss=56.7103
	step [134/192], loss=65.8202
	step [135/192], loss=60.3134
	step [136/192], loss=56.5496
	step [137/192], loss=56.2491
	step [138/192], loss=61.2881
	step [139/192], loss=63.9769
	step [140/192], loss=54.3806
	step [141/192], loss=69.9750
	step [142/192], loss=72.4296
	step [143/192], loss=51.7511
	step [144/192], loss=70.7704
	step [145/192], loss=66.8101
	step [146/192], loss=58.4108
	step [147/192], loss=68.5099
	step [148/192], loss=62.1719
	step [149/192], loss=67.4961
	step [150/192], loss=69.8425
	step [151/192], loss=67.4748
	step [152/192], loss=69.6678
	step [153/192], loss=78.0965
	step [154/192], loss=64.2006
	step [155/192], loss=72.3428
	step [156/192], loss=68.5927
	step [157/192], loss=62.3072
	step [158/192], loss=66.9204
	step [159/192], loss=68.7657
	step [160/192], loss=54.8523
	step [161/192], loss=57.9148
	step [162/192], loss=65.8737
	step [163/192], loss=73.1903
	step [164/192], loss=64.6292
	step [165/192], loss=74.9245
	step [166/192], loss=72.3884
	step [167/192], loss=57.9819
	step [168/192], loss=67.7826
	step [169/192], loss=59.7620
	step [170/192], loss=64.7192
	step [171/192], loss=65.3950
	step [172/192], loss=61.7571
	step [173/192], loss=73.8143
	step [174/192], loss=57.1577
	step [175/192], loss=58.3909
	step [176/192], loss=65.5260
	step [177/192], loss=62.1543
	step [178/192], loss=78.3997
	step [179/192], loss=66.3730
	step [180/192], loss=57.6061
	step [181/192], loss=50.2369
	step [182/192], loss=72.2137
	step [183/192], loss=57.0928
	step [184/192], loss=59.5799
	step [185/192], loss=69.3141
	step [186/192], loss=58.0781
	step [187/192], loss=60.9784
	step [188/192], loss=55.1032
	step [189/192], loss=63.6567
	step [190/192], loss=63.1744
	step [191/192], loss=55.0158
	step [192/192], loss=49.2861
	Evaluating
	loss=0.0057, precision=0.4309, recall=0.8706, f1=0.5765
saving model as: 0_saved_model.pth
Training epoch 132
	step [1/192], loss=66.9888
	step [2/192], loss=60.0705
	step [3/192], loss=58.8925
	step [4/192], loss=58.0916
	step [5/192], loss=64.2474
	step [6/192], loss=53.4436
	step [7/192], loss=73.9596
	step [8/192], loss=58.6758
	step [9/192], loss=54.7211
	step [10/192], loss=56.0129
	step [11/192], loss=59.5360
	step [12/192], loss=60.6542
	step [13/192], loss=71.7974
	step [14/192], loss=72.4059
	step [15/192], loss=58.1803
	step [16/192], loss=60.2529
	step [17/192], loss=73.5183
	step [18/192], loss=74.6965
	step [19/192], loss=61.2584
	step [20/192], loss=55.9986
	step [21/192], loss=76.5165
	step [22/192], loss=67.5681
	step [23/192], loss=80.2350
	step [24/192], loss=69.6998
	step [25/192], loss=56.4074
	step [26/192], loss=60.1594
	step [27/192], loss=52.5556
	step [28/192], loss=60.3633
	step [29/192], loss=73.6623
	step [30/192], loss=66.3948
	step [31/192], loss=43.6684
	step [32/192], loss=73.4328
	step [33/192], loss=66.6259
	step [34/192], loss=69.0718
	step [35/192], loss=65.8980
	step [36/192], loss=63.2298
	step [37/192], loss=76.7931
	step [38/192], loss=58.4850
	step [39/192], loss=60.7226
	step [40/192], loss=62.4314
	step [41/192], loss=69.3837
	step [42/192], loss=73.2231
	step [43/192], loss=68.9143
	step [44/192], loss=55.8613
	step [45/192], loss=71.9046
	step [46/192], loss=70.0380
	step [47/192], loss=61.8420
	step [48/192], loss=71.5409
	step [49/192], loss=56.3612
	step [50/192], loss=75.2162
	step [51/192], loss=61.9809
	step [52/192], loss=73.9820
	step [53/192], loss=52.6235
	step [54/192], loss=69.0880
	step [55/192], loss=57.2692
	step [56/192], loss=59.4939
	step [57/192], loss=70.5000
	step [58/192], loss=65.8946
	step [59/192], loss=70.3172
	step [60/192], loss=56.0270
	step [61/192], loss=76.4670
	step [62/192], loss=76.6361
	step [63/192], loss=62.2095
	step [64/192], loss=63.6246
	step [65/192], loss=67.3179
	step [66/192], loss=68.0797
	step [67/192], loss=61.7500
	step [68/192], loss=65.0866
	step [69/192], loss=55.4901
	step [70/192], loss=59.9686
	step [71/192], loss=89.6157
	step [72/192], loss=55.5939
	step [73/192], loss=51.8114
	step [74/192], loss=54.7667
	step [75/192], loss=61.0889
	step [76/192], loss=65.9998
	step [77/192], loss=61.0395
	step [78/192], loss=51.6501
	step [79/192], loss=62.9979
	step [80/192], loss=64.4726
	step [81/192], loss=66.6754
	step [82/192], loss=62.5282
	step [83/192], loss=58.2216
	step [84/192], loss=67.0704
	step [85/192], loss=62.7545
	step [86/192], loss=57.1618
	step [87/192], loss=52.9475
	step [88/192], loss=61.7682
	step [89/192], loss=62.2888
	step [90/192], loss=65.7116
	step [91/192], loss=62.8517
	step [92/192], loss=69.5351
	step [93/192], loss=65.3686
	step [94/192], loss=74.5631
	step [95/192], loss=70.7549
	step [96/192], loss=65.8416
	step [97/192], loss=61.3195
	step [98/192], loss=66.5894
	step [99/192], loss=65.8003
	step [100/192], loss=55.7581
	step [101/192], loss=69.0234
	step [102/192], loss=51.7439
	step [103/192], loss=54.1577
	step [104/192], loss=68.4371
	step [105/192], loss=56.2331
	step [106/192], loss=75.7883
	step [107/192], loss=62.5517
	step [108/192], loss=69.8407
	step [109/192], loss=68.9534
	step [110/192], loss=53.5491
	step [111/192], loss=66.6711
	step [112/192], loss=64.7820
	step [113/192], loss=59.3696
	step [114/192], loss=60.6517
	step [115/192], loss=65.5924
	step [116/192], loss=63.6128
	step [117/192], loss=66.9999
	step [118/192], loss=50.6838
	step [119/192], loss=61.9995
	step [120/192], loss=57.5554
	step [121/192], loss=72.1912
	step [122/192], loss=60.8561
	step [123/192], loss=64.5744
	step [124/192], loss=57.6669
	step [125/192], loss=60.7886
	step [126/192], loss=72.8563
	step [127/192], loss=68.9593
	step [128/192], loss=81.1443
	step [129/192], loss=76.5424
	step [130/192], loss=58.0907
	step [131/192], loss=75.8818
	step [132/192], loss=60.5966
	step [133/192], loss=55.3224
	step [134/192], loss=66.1696
	step [135/192], loss=58.4152
	step [136/192], loss=57.5851
	step [137/192], loss=60.7165
	step [138/192], loss=60.1897
	step [139/192], loss=74.2954
	step [140/192], loss=54.7485
	step [141/192], loss=58.7955
	step [142/192], loss=63.5969
	step [143/192], loss=69.0983
	step [144/192], loss=62.4517
	step [145/192], loss=66.8936
	step [146/192], loss=65.7607
	step [147/192], loss=72.6503
	step [148/192], loss=70.4291
	step [149/192], loss=62.3538
	step [150/192], loss=77.2290
	step [151/192], loss=65.1383
	step [152/192], loss=67.1138
	step [153/192], loss=61.6460
	step [154/192], loss=66.4475
	step [155/192], loss=62.0310
	step [156/192], loss=56.3059
	step [157/192], loss=81.5161
	step [158/192], loss=73.0881
	step [159/192], loss=65.3187
	step [160/192], loss=64.3686
	step [161/192], loss=54.8948
	step [162/192], loss=65.4065
	step [163/192], loss=62.7199
	step [164/192], loss=73.3407
	step [165/192], loss=63.7896
	step [166/192], loss=66.4176
	step [167/192], loss=73.3874
	step [168/192], loss=66.4279
	step [169/192], loss=56.7860
	step [170/192], loss=66.4592
	step [171/192], loss=59.6216
	step [172/192], loss=71.1573
	step [173/192], loss=66.3382
	step [174/192], loss=66.2517
	step [175/192], loss=66.4007
	step [176/192], loss=56.7990
	step [177/192], loss=57.3797
	step [178/192], loss=72.2966
	step [179/192], loss=70.1466
	step [180/192], loss=63.7822
	step [181/192], loss=56.6676
	step [182/192], loss=59.6674
	step [183/192], loss=65.6005
	step [184/192], loss=63.8443
	step [185/192], loss=67.6563
	step [186/192], loss=63.3880
	step [187/192], loss=54.2931
	step [188/192], loss=62.5688
	step [189/192], loss=59.4156
	step [190/192], loss=60.7741
	step [191/192], loss=61.9339
	step [192/192], loss=57.4916
	Evaluating
	loss=0.0064, precision=0.3657, recall=0.8661, f1=0.5143
Training epoch 133
	step [1/192], loss=62.9739
	step [2/192], loss=63.5795
	step [3/192], loss=65.8211
	step [4/192], loss=62.2170
	step [5/192], loss=63.3947
	step [6/192], loss=62.0193
	step [7/192], loss=60.3835
	step [8/192], loss=75.0257
	step [9/192], loss=71.7932
	step [10/192], loss=63.3153
	step [11/192], loss=65.0465
	step [12/192], loss=62.8282
	step [13/192], loss=59.7930
	step [14/192], loss=70.8338
	step [15/192], loss=61.8315
	step [16/192], loss=66.3980
	step [17/192], loss=52.7571
	step [18/192], loss=56.6813
	step [19/192], loss=70.8493
	step [20/192], loss=64.1347
	step [21/192], loss=57.8660
	step [22/192], loss=60.8756
	step [23/192], loss=66.9203
	step [24/192], loss=68.4917
	step [25/192], loss=69.1083
	step [26/192], loss=66.8408
	step [27/192], loss=69.3388
	step [28/192], loss=55.6912
	step [29/192], loss=73.6685
	step [30/192], loss=53.1052
	step [31/192], loss=64.3934
	step [32/192], loss=63.7305
	step [33/192], loss=59.1778
	step [34/192], loss=72.5337
	step [35/192], loss=66.3325
	step [36/192], loss=57.6751
	step [37/192], loss=61.2523
	step [38/192], loss=70.4237
	step [39/192], loss=68.4422
	step [40/192], loss=50.1174
	step [41/192], loss=51.0504
	step [42/192], loss=62.2704
	step [43/192], loss=67.7906
	step [44/192], loss=66.7171
	step [45/192], loss=58.6417
	step [46/192], loss=61.8414
	step [47/192], loss=51.6769
	step [48/192], loss=58.1917
	step [49/192], loss=72.3159
	step [50/192], loss=65.1493
	step [51/192], loss=66.1648
	step [52/192], loss=65.4502
	step [53/192], loss=55.5474
	step [54/192], loss=52.0466
	step [55/192], loss=62.4806
	step [56/192], loss=64.0379
	step [57/192], loss=61.8097
	step [58/192], loss=65.5392
	step [59/192], loss=59.3887
	step [60/192], loss=76.5380
	step [61/192], loss=59.4650
	step [62/192], loss=64.3902
	step [63/192], loss=66.7173
	step [64/192], loss=60.3482
	step [65/192], loss=65.4291
	step [66/192], loss=57.8957
	step [67/192], loss=67.8548
	step [68/192], loss=52.9579
	step [69/192], loss=56.7655
	step [70/192], loss=65.7276
	step [71/192], loss=56.7431
	step [72/192], loss=62.8632
	step [73/192], loss=71.5357
	step [74/192], loss=60.5820
	step [75/192], loss=66.1071
	step [76/192], loss=65.2289
	step [77/192], loss=57.1136
	step [78/192], loss=74.6252
	step [79/192], loss=67.3992
	step [80/192], loss=61.7430
	step [81/192], loss=78.0418
	step [82/192], loss=67.5224
	step [83/192], loss=62.0379
	step [84/192], loss=62.8150
	step [85/192], loss=61.7763
	step [86/192], loss=61.1613
	step [87/192], loss=69.7690
	step [88/192], loss=64.5531
	step [89/192], loss=65.2313
	step [90/192], loss=69.1525
	step [91/192], loss=60.9362
	step [92/192], loss=68.2037
	step [93/192], loss=62.0930
	step [94/192], loss=66.2496
	step [95/192], loss=58.8900
	step [96/192], loss=57.2860
	step [97/192], loss=69.6166
	step [98/192], loss=66.0494
	step [99/192], loss=57.2590
	step [100/192], loss=64.2027
	step [101/192], loss=65.9208
	step [102/192], loss=56.8511
	step [103/192], loss=59.0541
	step [104/192], loss=65.4995
	step [105/192], loss=47.9582
	step [106/192], loss=60.8474
	step [107/192], loss=68.5187
	step [108/192], loss=71.7228
	step [109/192], loss=75.7273
	step [110/192], loss=61.1618
	step [111/192], loss=68.3535
	step [112/192], loss=69.0194
	step [113/192], loss=71.2799
	step [114/192], loss=69.9579
	step [115/192], loss=65.8173
	step [116/192], loss=70.5870
	step [117/192], loss=61.1424
	step [118/192], loss=60.9731
	step [119/192], loss=61.6841
	step [120/192], loss=64.3757
	step [121/192], loss=60.7507
	step [122/192], loss=71.5915
	step [123/192], loss=59.1205
	step [124/192], loss=47.2423
	step [125/192], loss=74.4046
	step [126/192], loss=65.9098
	step [127/192], loss=70.3585
	step [128/192], loss=57.2897
	step [129/192], loss=59.1682
	step [130/192], loss=60.6841
	step [131/192], loss=68.3654
	step [132/192], loss=73.7331
	step [133/192], loss=70.2410
	step [134/192], loss=60.9299
	step [135/192], loss=62.7893
	step [136/192], loss=65.4908
	step [137/192], loss=64.7174
	step [138/192], loss=66.9529
	step [139/192], loss=61.0243
	step [140/192], loss=50.8640
	step [141/192], loss=62.5244
	step [142/192], loss=68.9967
	step [143/192], loss=73.2875
	step [144/192], loss=57.9317
	step [145/192], loss=70.2419
	step [146/192], loss=65.9703
	step [147/192], loss=67.2259
	step [148/192], loss=60.1725
	step [149/192], loss=67.1635
	step [150/192], loss=71.2511
	step [151/192], loss=61.3050
	step [152/192], loss=73.6264
	step [153/192], loss=55.3633
	step [154/192], loss=65.8509
	step [155/192], loss=60.1911
	step [156/192], loss=64.3272
	step [157/192], loss=65.6730
	step [158/192], loss=65.3897
	step [159/192], loss=80.7803
	step [160/192], loss=64.7834
	step [161/192], loss=65.8879
	step [162/192], loss=70.6102
	step [163/192], loss=69.6893
	step [164/192], loss=62.6010
	step [165/192], loss=64.9216
	step [166/192], loss=70.8941
	step [167/192], loss=58.8950
	step [168/192], loss=57.7866
	step [169/192], loss=55.8304
	step [170/192], loss=57.6552
	step [171/192], loss=60.0057
	step [172/192], loss=58.7116
	step [173/192], loss=70.7449
	step [174/192], loss=67.1503
	step [175/192], loss=63.8649
	step [176/192], loss=67.4033
	step [177/192], loss=64.6145
	step [178/192], loss=74.1534
	step [179/192], loss=65.4545
	step [180/192], loss=58.0824
	step [181/192], loss=64.8264
	step [182/192], loss=62.6470
	step [183/192], loss=59.8205
	step [184/192], loss=74.3921
	step [185/192], loss=66.2752
	step [186/192], loss=60.8899
	step [187/192], loss=64.3063
	step [188/192], loss=63.5589
	step [189/192], loss=58.6271
	step [190/192], loss=71.6226
	step [191/192], loss=50.3296
	step [192/192], loss=58.0541
	Evaluating
	loss=0.0057, precision=0.4325, recall=0.8557, f1=0.5745
Training epoch 134
	step [1/192], loss=61.2577
	step [2/192], loss=70.7992
	step [3/192], loss=65.9465
	step [4/192], loss=61.4162
	step [5/192], loss=71.7590
	step [6/192], loss=69.0064
	step [7/192], loss=62.0016
	step [8/192], loss=52.7481
	step [9/192], loss=57.8033
	step [10/192], loss=80.9023
	step [11/192], loss=61.0633
	step [12/192], loss=70.0736
	step [13/192], loss=65.8998
	step [14/192], loss=57.6899
	step [15/192], loss=52.3674
	step [16/192], loss=74.3203
	step [17/192], loss=64.6450
	step [18/192], loss=60.1419
	step [19/192], loss=70.2017
	step [20/192], loss=59.3509
	step [21/192], loss=60.6723
	step [22/192], loss=59.2532
	step [23/192], loss=55.9417
	step [24/192], loss=58.2313
	step [25/192], loss=65.7455
	step [26/192], loss=81.2395
	step [27/192], loss=67.3874
	step [28/192], loss=62.1675
	step [29/192], loss=66.0254
	step [30/192], loss=56.8363
	step [31/192], loss=65.6953
	step [32/192], loss=59.7370
	step [33/192], loss=60.5253
	step [34/192], loss=71.1869
	step [35/192], loss=72.9843
	step [36/192], loss=54.3695
	step [37/192], loss=59.5478
	step [38/192], loss=61.4166
	step [39/192], loss=68.0077
	step [40/192], loss=70.9771
	step [41/192], loss=61.9177
	step [42/192], loss=56.9061
	step [43/192], loss=67.3068
	step [44/192], loss=63.2731
	step [45/192], loss=57.0838
	step [46/192], loss=67.8464
	step [47/192], loss=60.1661
	step [48/192], loss=58.3824
	step [49/192], loss=57.6646
	step [50/192], loss=59.4820
	step [51/192], loss=77.4329
	step [52/192], loss=67.0236
	step [53/192], loss=60.3613
	step [54/192], loss=74.9165
	step [55/192], loss=60.3000
	step [56/192], loss=61.5406
	step [57/192], loss=49.7042
	step [58/192], loss=59.1768
	step [59/192], loss=67.7172
	step [60/192], loss=71.6121
	step [61/192], loss=64.1300
	step [62/192], loss=68.5935
	step [63/192], loss=74.0352
	step [64/192], loss=74.5141
	step [65/192], loss=58.8265
	step [66/192], loss=64.8480
	step [67/192], loss=60.7887
	step [68/192], loss=57.9531
	step [69/192], loss=69.7321
	step [70/192], loss=71.2876
	step [71/192], loss=57.5435
	step [72/192], loss=59.8929
	step [73/192], loss=62.6156
	step [74/192], loss=56.0262
	step [75/192], loss=55.2803
	step [76/192], loss=61.9882
	step [77/192], loss=69.6557
	step [78/192], loss=68.9040
	step [79/192], loss=68.7958
	step [80/192], loss=66.8884
	step [81/192], loss=66.3223
	step [82/192], loss=68.7948
	step [83/192], loss=67.5823
	step [84/192], loss=72.6354
	step [85/192], loss=56.5157
	step [86/192], loss=61.8306
	step [87/192], loss=65.8922
	step [88/192], loss=70.8667
	step [89/192], loss=74.5355
	step [90/192], loss=65.9785
	step [91/192], loss=49.3089
	step [92/192], loss=69.7734
	step [93/192], loss=68.1647
	step [94/192], loss=54.5826
	step [95/192], loss=63.4913
	step [96/192], loss=59.5908
	step [97/192], loss=56.2082
	step [98/192], loss=59.1758
	step [99/192], loss=61.7735
	step [100/192], loss=69.2678
	step [101/192], loss=52.9970
	step [102/192], loss=63.4413
	step [103/192], loss=58.5388
	step [104/192], loss=61.7328
	step [105/192], loss=72.8236
	step [106/192], loss=66.6395
	step [107/192], loss=57.7533
	step [108/192], loss=65.8199
	step [109/192], loss=64.5332
	step [110/192], loss=54.5046
	step [111/192], loss=76.9168
	step [112/192], loss=65.7232
	step [113/192], loss=64.0087
	step [114/192], loss=70.4176
	step [115/192], loss=64.4062
	step [116/192], loss=63.6079
	step [117/192], loss=67.4789
	step [118/192], loss=63.3945
	step [119/192], loss=72.0064
	step [120/192], loss=69.4640
	step [121/192], loss=49.8353
	step [122/192], loss=61.8879
	step [123/192], loss=56.7616
	step [124/192], loss=72.5249
	step [125/192], loss=61.4278
	step [126/192], loss=66.9903
	step [127/192], loss=70.5883
	step [128/192], loss=60.0185
	step [129/192], loss=50.3126
	step [130/192], loss=78.6213
	step [131/192], loss=63.5837
	step [132/192], loss=66.6770
	step [133/192], loss=66.7158
	step [134/192], loss=62.0678
	step [135/192], loss=67.7128
	step [136/192], loss=71.5485
	step [137/192], loss=65.5076
	step [138/192], loss=69.0682
	step [139/192], loss=65.6444
	step [140/192], loss=60.7736
	step [141/192], loss=65.3624
	step [142/192], loss=62.5343
	step [143/192], loss=57.6228
	step [144/192], loss=54.3351
	step [145/192], loss=55.5331
	step [146/192], loss=61.8807
	step [147/192], loss=68.6832
	step [148/192], loss=69.3802
	step [149/192], loss=67.1109
	step [150/192], loss=66.9086
	step [151/192], loss=53.3320
	step [152/192], loss=56.5231
	step [153/192], loss=47.7588
	step [154/192], loss=59.7372
	step [155/192], loss=66.3266
	step [156/192], loss=65.8457
	step [157/192], loss=57.0010
	step [158/192], loss=62.8774
	step [159/192], loss=70.9890
	step [160/192], loss=61.9964
	step [161/192], loss=56.4097
	step [162/192], loss=59.8346
	step [163/192], loss=63.6305
	step [164/192], loss=56.1488
	step [165/192], loss=67.1535
	step [166/192], loss=57.2406
	step [167/192], loss=61.8462
	step [168/192], loss=52.9033
	step [169/192], loss=67.9955
	step [170/192], loss=64.5225
	step [171/192], loss=66.0056
	step [172/192], loss=66.9903
	step [173/192], loss=76.5821
	step [174/192], loss=58.7928
	step [175/192], loss=66.5564
	step [176/192], loss=56.3836
	step [177/192], loss=70.5520
	step [178/192], loss=70.3277
	step [179/192], loss=76.1295
	step [180/192], loss=77.1308
	step [181/192], loss=59.7738
	step [182/192], loss=62.7521
	step [183/192], loss=69.9586
	step [184/192], loss=63.6741
	step [185/192], loss=57.7187
	step [186/192], loss=53.5783
	step [187/192], loss=64.2118
	step [188/192], loss=59.0209
	step [189/192], loss=62.2987
	step [190/192], loss=61.2887
	step [191/192], loss=72.6436
	step [192/192], loss=58.8484
	Evaluating
	loss=0.0055, precision=0.4283, recall=0.8594, f1=0.5717
Training epoch 135
	step [1/192], loss=68.8768
	step [2/192], loss=65.4717
	step [3/192], loss=62.3470
	step [4/192], loss=65.6659
	step [5/192], loss=67.9931
	step [6/192], loss=56.9402
	step [7/192], loss=58.0246
	step [8/192], loss=55.3953
	step [9/192], loss=55.6040
	step [10/192], loss=76.3449
	step [11/192], loss=56.4693
	step [12/192], loss=64.8375
	step [13/192], loss=65.9598
	step [14/192], loss=65.8729
	step [15/192], loss=63.8584
	step [16/192], loss=56.5811
	step [17/192], loss=53.9652
	step [18/192], loss=75.6907
	step [19/192], loss=62.8438
	step [20/192], loss=64.6052
	step [21/192], loss=54.9837
	step [22/192], loss=64.6278
	step [23/192], loss=68.3092
	step [24/192], loss=62.6867
	step [25/192], loss=72.2820
	step [26/192], loss=62.3909
	step [27/192], loss=63.9808
	step [28/192], loss=63.3502
	step [29/192], loss=74.0688
	step [30/192], loss=54.7918
	step [31/192], loss=66.5904
	step [32/192], loss=55.5606
	step [33/192], loss=62.5732
	step [34/192], loss=71.0824
	step [35/192], loss=55.1537
	step [36/192], loss=56.7313
	step [37/192], loss=58.8644
	step [38/192], loss=47.6948
	step [39/192], loss=55.0257
	step [40/192], loss=57.1966
	step [41/192], loss=61.1580
	step [42/192], loss=58.0617
	step [43/192], loss=72.7260
	step [44/192], loss=57.3532
	step [45/192], loss=69.0036
	step [46/192], loss=66.6166
	step [47/192], loss=56.7292
	step [48/192], loss=55.5527
	step [49/192], loss=60.9262
	step [50/192], loss=67.5598
	step [51/192], loss=66.6850
	step [52/192], loss=57.7429
	step [53/192], loss=69.4136
	step [54/192], loss=63.9074
	step [55/192], loss=60.6537
	step [56/192], loss=63.6641
	step [57/192], loss=64.5787
	step [58/192], loss=63.3193
	step [59/192], loss=75.7061
	step [60/192], loss=61.4305
	step [61/192], loss=56.8837
	step [62/192], loss=65.1560
	step [63/192], loss=61.0382
	step [64/192], loss=65.6525
	step [65/192], loss=56.7821
	step [66/192], loss=59.7820
	step [67/192], loss=60.6645
	step [68/192], loss=70.8825
	step [69/192], loss=55.5755
	step [70/192], loss=61.5040
	step [71/192], loss=60.1090
	step [72/192], loss=68.2632
	step [73/192], loss=78.0813
	step [74/192], loss=78.0597
	step [75/192], loss=68.2940
	step [76/192], loss=65.6669
	step [77/192], loss=67.6168
	step [78/192], loss=66.6909
	step [79/192], loss=63.7539
	step [80/192], loss=58.9360
	step [81/192], loss=55.7038
	step [82/192], loss=61.9743
	step [83/192], loss=64.4629
	step [84/192], loss=79.6858
	step [85/192], loss=58.9881
	step [86/192], loss=60.6594
	step [87/192], loss=72.8599
	step [88/192], loss=67.6747
	step [89/192], loss=60.1620
	step [90/192], loss=73.9203
	step [91/192], loss=74.6836
	step [92/192], loss=74.5894
	step [93/192], loss=63.1443
	step [94/192], loss=64.3268
	step [95/192], loss=58.2497
	step [96/192], loss=54.0289
	step [97/192], loss=66.4777
	step [98/192], loss=63.5350
	step [99/192], loss=79.7220
	step [100/192], loss=65.7908
	step [101/192], loss=60.2286
	step [102/192], loss=63.3451
	step [103/192], loss=63.0123
	step [104/192], loss=65.4331
	step [105/192], loss=64.1309
	step [106/192], loss=51.6487
	step [107/192], loss=64.1772
	step [108/192], loss=54.1737
	step [109/192], loss=65.1582
	step [110/192], loss=63.0040
	step [111/192], loss=82.0075
	step [112/192], loss=56.4247
	step [113/192], loss=53.7649
	step [114/192], loss=59.1442
	step [115/192], loss=61.3174
	step [116/192], loss=61.3423
	step [117/192], loss=67.6880
	step [118/192], loss=61.2863
	step [119/192], loss=61.9925
	step [120/192], loss=64.0813
	step [121/192], loss=71.9016
	step [122/192], loss=67.2708
	step [123/192], loss=66.0359
	step [124/192], loss=59.3201
	step [125/192], loss=71.0586
	step [126/192], loss=59.6621
	step [127/192], loss=58.5703
	step [128/192], loss=47.1452
	step [129/192], loss=71.8676
	step [130/192], loss=56.8520
	step [131/192], loss=67.1944
	step [132/192], loss=68.1242
	step [133/192], loss=69.5041
	step [134/192], loss=60.4592
	step [135/192], loss=68.6189
	step [136/192], loss=62.9631
	step [137/192], loss=81.2454
	step [138/192], loss=52.4137
	step [139/192], loss=60.4146
	step [140/192], loss=65.8558
	step [141/192], loss=58.3503
	step [142/192], loss=66.6565
	step [143/192], loss=63.2766
	step [144/192], loss=60.1531
	step [145/192], loss=53.7470
	step [146/192], loss=67.5924
	step [147/192], loss=61.8247
	step [148/192], loss=61.2653
	step [149/192], loss=66.1183
	step [150/192], loss=57.3496
	step [151/192], loss=67.9159
	step [152/192], loss=75.6218
	step [153/192], loss=50.2599
	step [154/192], loss=69.0598
	step [155/192], loss=77.2115
	step [156/192], loss=62.3712
	step [157/192], loss=65.9002
	step [158/192], loss=61.8293
	step [159/192], loss=63.6512
	step [160/192], loss=67.2873
	step [161/192], loss=59.4657
	step [162/192], loss=67.7425
	step [163/192], loss=63.9015
	step [164/192], loss=74.7155
	step [165/192], loss=65.8011
	step [166/192], loss=56.5789
	step [167/192], loss=56.1733
	step [168/192], loss=68.2505
	step [169/192], loss=76.9719
	step [170/192], loss=64.7994
	step [171/192], loss=63.6625
	step [172/192], loss=67.3951
	step [173/192], loss=71.9781
	step [174/192], loss=63.9666
	step [175/192], loss=54.1744
	step [176/192], loss=60.9629
	step [177/192], loss=78.0719
	step [178/192], loss=66.1147
	step [179/192], loss=66.4143
	step [180/192], loss=62.3036
	step [181/192], loss=64.5779
	step [182/192], loss=64.2182
	step [183/192], loss=56.5907
	step [184/192], loss=58.7052
	step [185/192], loss=65.6852
	step [186/192], loss=62.7031
	step [187/192], loss=50.9950
	step [188/192], loss=56.0848
	step [189/192], loss=73.2673
	step [190/192], loss=59.3001
	step [191/192], loss=63.7666
	step [192/192], loss=85.7493
	Evaluating
	loss=0.0060, precision=0.4135, recall=0.8678, f1=0.5601
Training epoch 136
	step [1/192], loss=77.2443
	step [2/192], loss=73.4204
	step [3/192], loss=60.2306
	step [4/192], loss=59.5387
	step [5/192], loss=65.8315
	step [6/192], loss=66.1317
	step [7/192], loss=60.9117
	step [8/192], loss=64.6775
	step [9/192], loss=56.9508
	step [10/192], loss=63.6057
	step [11/192], loss=76.5798
	step [12/192], loss=67.0056
	step [13/192], loss=49.2431
	step [14/192], loss=70.7474
	step [15/192], loss=71.2475
	step [16/192], loss=53.2608
	step [17/192], loss=77.1696
	step [18/192], loss=58.6048
	step [19/192], loss=70.2546
	step [20/192], loss=61.4424
	step [21/192], loss=67.6508
	step [22/192], loss=67.5896
	step [23/192], loss=60.4687
	step [24/192], loss=53.7873
	step [25/192], loss=71.6532
	step [26/192], loss=67.0596
	step [27/192], loss=67.5029
	step [28/192], loss=61.2612
	step [29/192], loss=64.4044
	step [30/192], loss=60.9929
	step [31/192], loss=55.2389
	step [32/192], loss=61.6710
	step [33/192], loss=64.5748
	step [34/192], loss=64.8466
	step [35/192], loss=53.1927
	step [36/192], loss=65.1353
	step [37/192], loss=61.3303
	step [38/192], loss=72.4176
	step [39/192], loss=73.4637
	step [40/192], loss=54.5430
	step [41/192], loss=69.3204
	step [42/192], loss=63.6882
	step [43/192], loss=66.6511
	step [44/192], loss=72.1895
	step [45/192], loss=65.3963
	step [46/192], loss=71.8499
	step [47/192], loss=64.5501
	step [48/192], loss=48.6778
	step [49/192], loss=63.0136
	step [50/192], loss=74.1849
	step [51/192], loss=71.9528
	step [52/192], loss=60.2325
	step [53/192], loss=64.5572
	step [54/192], loss=66.3279
	step [55/192], loss=65.7703
	step [56/192], loss=68.0876
	step [57/192], loss=70.4856
	step [58/192], loss=65.4611
	step [59/192], loss=60.1794
	step [60/192], loss=62.1928
	step [61/192], loss=71.3895
	step [62/192], loss=61.7880
	step [63/192], loss=63.1989
	step [64/192], loss=64.2954
	step [65/192], loss=56.1172
	step [66/192], loss=68.6910
	step [67/192], loss=70.7192
	step [68/192], loss=59.6468
	step [69/192], loss=71.9415
	step [70/192], loss=54.8932
	step [71/192], loss=63.3182
	step [72/192], loss=63.3891
	step [73/192], loss=62.6799
	step [74/192], loss=68.2524
	step [75/192], loss=63.7311
	step [76/192], loss=59.4192
	step [77/192], loss=62.1088
	step [78/192], loss=52.6021
	step [79/192], loss=67.1068
	step [80/192], loss=67.6186
	step [81/192], loss=65.4097
	step [82/192], loss=64.6009
	step [83/192], loss=52.0242
	step [84/192], loss=69.3334
	step [85/192], loss=63.7628
	step [86/192], loss=62.0958
	step [87/192], loss=60.2000
	step [88/192], loss=59.9669
	step [89/192], loss=61.4274
	step [90/192], loss=65.6608
	step [91/192], loss=66.7148
	step [92/192], loss=55.5450
	step [93/192], loss=70.2312
	step [94/192], loss=70.6965
	step [95/192], loss=59.7169
	step [96/192], loss=65.7309
	step [97/192], loss=69.7208
	step [98/192], loss=62.1341
	step [99/192], loss=58.7178
	step [100/192], loss=62.5286
	step [101/192], loss=59.8529
	step [102/192], loss=68.4584
	step [103/192], loss=68.1613
	step [104/192], loss=61.8242
	step [105/192], loss=51.8558
	step [106/192], loss=57.0248
	step [107/192], loss=58.4216
	step [108/192], loss=62.7583
	step [109/192], loss=61.4430
	step [110/192], loss=55.4606
	step [111/192], loss=60.7051
	step [112/192], loss=59.6056
	step [113/192], loss=65.0544
	step [114/192], loss=49.7242
	step [115/192], loss=57.4030
	step [116/192], loss=70.4249
	step [117/192], loss=59.5461
	step [118/192], loss=65.7254
	step [119/192], loss=61.8027
	step [120/192], loss=68.9909
	step [121/192], loss=57.1654
	step [122/192], loss=52.9654
	step [123/192], loss=60.6168
	step [124/192], loss=65.3605
	step [125/192], loss=60.6769
	step [126/192], loss=68.9938
	step [127/192], loss=61.4024
	step [128/192], loss=62.5343
	step [129/192], loss=63.0783
	step [130/192], loss=64.5990
	step [131/192], loss=58.2182
	step [132/192], loss=52.3370
	step [133/192], loss=62.6520
	step [134/192], loss=69.1247
	step [135/192], loss=73.2856
	step [136/192], loss=58.6308
	step [137/192], loss=72.9690
	step [138/192], loss=68.6216
	step [139/192], loss=60.4656
	step [140/192], loss=64.0678
	step [141/192], loss=58.3652
	step [142/192], loss=71.4360
	step [143/192], loss=68.3027
	step [144/192], loss=79.1527
	step [145/192], loss=64.0906
	step [146/192], loss=73.1343
	step [147/192], loss=54.8782
	step [148/192], loss=72.7106
	step [149/192], loss=74.6012
	step [150/192], loss=62.2349
	step [151/192], loss=55.9243
	step [152/192], loss=49.6786
	step [153/192], loss=56.8441
	step [154/192], loss=75.6830
	step [155/192], loss=54.5951
	step [156/192], loss=66.5455
	step [157/192], loss=62.4241
	step [158/192], loss=64.8040
	step [159/192], loss=62.4550
	step [160/192], loss=60.4905
	step [161/192], loss=66.3296
	step [162/192], loss=60.5201
	step [163/192], loss=56.1291
	step [164/192], loss=58.3627
	step [165/192], loss=57.9924
	step [166/192], loss=66.4275
	step [167/192], loss=67.6324
	step [168/192], loss=57.0872
	step [169/192], loss=52.1504
	step [170/192], loss=67.1227
	step [171/192], loss=74.6564
	step [172/192], loss=70.8190
	step [173/192], loss=69.6886
	step [174/192], loss=67.8979
	step [175/192], loss=62.9576
	step [176/192], loss=50.4640
	step [177/192], loss=74.5176
	step [178/192], loss=59.2450
	step [179/192], loss=65.1505
	step [180/192], loss=74.0002
	step [181/192], loss=65.0679
	step [182/192], loss=61.7474
	step [183/192], loss=63.6250
	step [184/192], loss=72.6633
	step [185/192], loss=62.6428
	step [186/192], loss=64.1561
	step [187/192], loss=65.0529
	step [188/192], loss=65.8536
	step [189/192], loss=65.2619
	step [190/192], loss=66.0846
	step [191/192], loss=59.5888
	step [192/192], loss=55.0765
	Evaluating
	loss=0.0062, precision=0.3931, recall=0.8672, f1=0.5410
Training epoch 137
	step [1/192], loss=57.5753
	step [2/192], loss=58.0121
	step [3/192], loss=58.3087
	step [4/192], loss=55.4108
	step [5/192], loss=65.8389
	step [6/192], loss=63.1110
	step [7/192], loss=66.5877
	step [8/192], loss=58.5139
	step [9/192], loss=77.2841
	step [10/192], loss=56.5070
	step [11/192], loss=70.2823
	step [12/192], loss=53.5466
	step [13/192], loss=56.5839
	step [14/192], loss=64.6698
	step [15/192], loss=60.1182
	step [16/192], loss=61.4806
	step [17/192], loss=50.2811
	step [18/192], loss=64.5303
	step [19/192], loss=66.7507
	step [20/192], loss=70.7339
	step [21/192], loss=57.8881
	step [22/192], loss=71.9538
	step [23/192], loss=59.9805
	step [24/192], loss=64.4133
	step [25/192], loss=62.0296
	step [26/192], loss=66.8375
	step [27/192], loss=66.4279
	step [28/192], loss=50.8613
	step [29/192], loss=72.7382
	step [30/192], loss=65.8449
	step [31/192], loss=57.2149
	step [32/192], loss=60.1720
	step [33/192], loss=57.3059
	step [34/192], loss=66.2104
	step [35/192], loss=65.4443
	step [36/192], loss=64.3885
	step [37/192], loss=73.9541
	step [38/192], loss=72.2136
	step [39/192], loss=61.3945
	step [40/192], loss=57.3456
	step [41/192], loss=65.9610
	step [42/192], loss=57.9558
	step [43/192], loss=68.2633
	step [44/192], loss=68.6532
	step [45/192], loss=71.8070
	step [46/192], loss=70.5528
	step [47/192], loss=72.5598
	step [48/192], loss=78.0026
	step [49/192], loss=71.0694
	step [50/192], loss=75.6068
	step [51/192], loss=54.9893
	step [52/192], loss=69.3612
	step [53/192], loss=65.2469
	step [54/192], loss=63.2625
	step [55/192], loss=74.3746
	step [56/192], loss=64.2789
	step [57/192], loss=57.0079
	step [58/192], loss=69.2279
	step [59/192], loss=70.7844
	step [60/192], loss=57.3263
	step [61/192], loss=59.4333
	step [62/192], loss=71.7621
	step [63/192], loss=53.2623
	step [64/192], loss=47.8849
	step [65/192], loss=63.2744
	step [66/192], loss=58.6612
	step [67/192], loss=70.6015
	step [68/192], loss=59.4123
	step [69/192], loss=65.1783
	step [70/192], loss=63.2956
	step [71/192], loss=57.7678
	step [72/192], loss=64.3654
	step [73/192], loss=57.8333
	step [74/192], loss=67.1842
	step [75/192], loss=64.5404
	step [76/192], loss=57.5718
	step [77/192], loss=64.4936
	step [78/192], loss=65.7251
	step [79/192], loss=66.4708
	step [80/192], loss=68.6498
	step [81/192], loss=66.0984
	step [82/192], loss=70.1874
	step [83/192], loss=66.7522
	step [84/192], loss=63.4930
	step [85/192], loss=65.7990
	step [86/192], loss=64.4295
	step [87/192], loss=59.6256
	step [88/192], loss=64.2685
	step [89/192], loss=64.4221
	step [90/192], loss=64.5519
	step [91/192], loss=61.7478
	step [92/192], loss=58.9022
	step [93/192], loss=68.8674
	step [94/192], loss=54.8403
	step [95/192], loss=66.1443
	step [96/192], loss=63.9721
	step [97/192], loss=67.1815
	step [98/192], loss=56.6121
	step [99/192], loss=66.9846
	step [100/192], loss=72.1679
	step [101/192], loss=67.3386
	step [102/192], loss=64.4433
	step [103/192], loss=57.4510
	step [104/192], loss=52.9624
	step [105/192], loss=59.7624
	step [106/192], loss=62.2118
	step [107/192], loss=54.2544
	step [108/192], loss=55.0423
	step [109/192], loss=63.3020
	step [110/192], loss=72.7351
	step [111/192], loss=54.7155
	step [112/192], loss=67.3164
	step [113/192], loss=65.8643
	step [114/192], loss=71.2060
	step [115/192], loss=65.2740
	step [116/192], loss=63.0946
	step [117/192], loss=59.1325
	step [118/192], loss=72.0590
	step [119/192], loss=54.2607
	step [120/192], loss=62.7555
	step [121/192], loss=74.1547
	step [122/192], loss=61.7868
	step [123/192], loss=54.6234
	step [124/192], loss=52.9708
	step [125/192], loss=73.4742
	step [126/192], loss=63.6976
	step [127/192], loss=61.6856
	step [128/192], loss=59.1499
	step [129/192], loss=74.4436
	step [130/192], loss=62.2597
	step [131/192], loss=48.4877
	step [132/192], loss=50.1093
	step [133/192], loss=55.2529
	step [134/192], loss=66.5441
	step [135/192], loss=70.2963
	step [136/192], loss=62.2130
	step [137/192], loss=63.9816
	step [138/192], loss=63.9604
	step [139/192], loss=51.0963
	step [140/192], loss=65.1647
	step [141/192], loss=61.2786
	step [142/192], loss=60.4199
	step [143/192], loss=70.6837
	step [144/192], loss=59.5195
	step [145/192], loss=44.8896
	step [146/192], loss=63.0319
	step [147/192], loss=61.6760
	step [148/192], loss=61.8346
	step [149/192], loss=65.0792
	step [150/192], loss=66.5397
	step [151/192], loss=56.6996
	step [152/192], loss=67.2878
	step [153/192], loss=71.0615
	step [154/192], loss=70.3879
	step [155/192], loss=65.0197
	step [156/192], loss=65.0306
	step [157/192], loss=62.6534
	step [158/192], loss=59.5351
	step [159/192], loss=61.1764
	step [160/192], loss=62.8370
	step [161/192], loss=68.1423
	step [162/192], loss=65.8053
	step [163/192], loss=60.5940
	step [164/192], loss=74.8824
	step [165/192], loss=66.5018
	step [166/192], loss=74.4395
	step [167/192], loss=58.7566
	step [168/192], loss=69.1566
	step [169/192], loss=63.3618
	step [170/192], loss=60.3310
	step [171/192], loss=67.4935
	step [172/192], loss=64.3077
	step [173/192], loss=71.4466
	step [174/192], loss=66.6487
	step [175/192], loss=65.6803
	step [176/192], loss=57.6919
	step [177/192], loss=55.9397
	step [178/192], loss=60.5449
	step [179/192], loss=62.3791
	step [180/192], loss=64.7148
	step [181/192], loss=65.6014
	step [182/192], loss=74.5371
	step [183/192], loss=65.4734
	step [184/192], loss=66.4237
	step [185/192], loss=66.0795
	step [186/192], loss=68.4106
	step [187/192], loss=57.3791
	step [188/192], loss=68.4168
	step [189/192], loss=71.6766
	step [190/192], loss=65.6368
	step [191/192], loss=68.2756
	step [192/192], loss=54.7973
	Evaluating
	loss=0.0060, precision=0.3921, recall=0.8554, f1=0.5377
Training epoch 138
	step [1/192], loss=61.3776
	step [2/192], loss=64.1515
	step [3/192], loss=62.2614
	step [4/192], loss=59.4293
	step [5/192], loss=56.5868
	step [6/192], loss=63.2529
	step [7/192], loss=65.9922
	step [8/192], loss=57.6833
	step [9/192], loss=70.1565
	step [10/192], loss=60.2047
	step [11/192], loss=64.5421
	step [12/192], loss=67.9503
	step [13/192], loss=58.7896
	step [14/192], loss=59.7783
	step [15/192], loss=64.4963
	step [16/192], loss=59.4142
	step [17/192], loss=66.2828
	step [18/192], loss=69.3912
	step [19/192], loss=63.4257
	step [20/192], loss=51.8637
	step [21/192], loss=59.5933
	step [22/192], loss=58.4782
	step [23/192], loss=66.5394
	step [24/192], loss=62.7621
	step [25/192], loss=72.9343
	step [26/192], loss=62.2950
	step [27/192], loss=70.0759
	step [28/192], loss=61.2901
	step [29/192], loss=65.7906
	step [30/192], loss=55.0997
	step [31/192], loss=63.4673
	step [32/192], loss=62.8932
	step [33/192], loss=66.2037
	step [34/192], loss=57.1434
	step [35/192], loss=60.7394
	step [36/192], loss=72.2841
	step [37/192], loss=63.8208
	step [38/192], loss=63.4948
	step [39/192], loss=59.7332
	step [40/192], loss=63.6890
	step [41/192], loss=72.3356
	step [42/192], loss=64.1169
	step [43/192], loss=62.0961
	step [44/192], loss=55.8538
	step [45/192], loss=61.0976
	step [46/192], loss=61.2259
	step [47/192], loss=57.9950
	step [48/192], loss=64.5897
	step [49/192], loss=57.9876
	step [50/192], loss=62.4949
	step [51/192], loss=67.8593
	step [52/192], loss=60.3160
	step [53/192], loss=72.3204
	step [54/192], loss=68.8740
	step [55/192], loss=68.4249
	step [56/192], loss=63.4291
	step [57/192], loss=58.9886
	step [58/192], loss=66.0108
	step [59/192], loss=65.6805
	step [60/192], loss=74.2724
	step [61/192], loss=59.4242
	step [62/192], loss=70.2533
	step [63/192], loss=55.8930
	step [64/192], loss=54.4094
	step [65/192], loss=68.6180
	step [66/192], loss=58.0044
	step [67/192], loss=67.5633
	step [68/192], loss=53.2459
	step [69/192], loss=64.9043
	step [70/192], loss=69.7959
	step [71/192], loss=71.0670
	step [72/192], loss=73.1219
	step [73/192], loss=59.9837
	step [74/192], loss=67.8529
	step [75/192], loss=66.4068
	step [76/192], loss=61.7637
	step [77/192], loss=61.0054
	step [78/192], loss=65.6195
	step [79/192], loss=62.8955
	step [80/192], loss=64.1557
	step [81/192], loss=67.4166
	step [82/192], loss=66.4801
	step [83/192], loss=63.2072
	step [84/192], loss=59.1631
	step [85/192], loss=55.9094
	step [86/192], loss=62.8421
	step [87/192], loss=72.6808
	step [88/192], loss=57.6359
	step [89/192], loss=65.9123
	step [90/192], loss=68.2881
	step [91/192], loss=55.1592
	step [92/192], loss=52.0354
	step [93/192], loss=58.7152
	step [94/192], loss=54.7389
	step [95/192], loss=70.3257
	step [96/192], loss=58.7244
	step [97/192], loss=53.2038
	step [98/192], loss=65.1307
	step [99/192], loss=66.9541
	step [100/192], loss=70.2927
	step [101/192], loss=69.7243
	step [102/192], loss=58.4480
	step [103/192], loss=64.8521
	step [104/192], loss=67.4493
	step [105/192], loss=63.1404
	step [106/192], loss=74.0966
	step [107/192], loss=65.5722
	step [108/192], loss=62.7631
	step [109/192], loss=71.3941
	step [110/192], loss=64.7787
	step [111/192], loss=57.0145
	step [112/192], loss=58.5593
	step [113/192], loss=80.6833
	step [114/192], loss=65.7124
	step [115/192], loss=64.8163
	step [116/192], loss=58.1463
	step [117/192], loss=64.9943
	step [118/192], loss=64.9667
	step [119/192], loss=54.7114
	step [120/192], loss=63.1744
	step [121/192], loss=68.8042
	step [122/192], loss=59.8648
	step [123/192], loss=60.2351
	step [124/192], loss=57.1598
	step [125/192], loss=56.3163
	step [126/192], loss=70.1091
	step [127/192], loss=71.2682
	step [128/192], loss=60.6765
	step [129/192], loss=67.2262
	step [130/192], loss=66.5889
	step [131/192], loss=63.4475
	step [132/192], loss=53.7131
	step [133/192], loss=60.9364
	step [134/192], loss=64.4622
	step [135/192], loss=59.0867
	step [136/192], loss=74.5603
	step [137/192], loss=67.1255
	step [138/192], loss=61.4352
	step [139/192], loss=65.6351
	step [140/192], loss=78.6654
	step [141/192], loss=70.1697
	step [142/192], loss=66.5423
	step [143/192], loss=80.5178
	step [144/192], loss=70.8139
	step [145/192], loss=58.8596
	step [146/192], loss=63.1040
	step [147/192], loss=61.3203
	step [148/192], loss=52.1221
	step [149/192], loss=63.7157
	step [150/192], loss=63.9552
	step [151/192], loss=62.8788
	step [152/192], loss=62.7207
	step [153/192], loss=63.5293
	step [154/192], loss=59.1891
	step [155/192], loss=67.7296
	step [156/192], loss=60.2344
	step [157/192], loss=60.6721
	step [158/192], loss=63.2770
	step [159/192], loss=57.0469
	step [160/192], loss=61.5048
	step [161/192], loss=64.6595
	step [162/192], loss=63.2063
	step [163/192], loss=56.8316
	step [164/192], loss=62.6715
	step [165/192], loss=58.3459
	step [166/192], loss=58.9223
	step [167/192], loss=67.9836
	step [168/192], loss=73.3465
	step [169/192], loss=71.2782
	step [170/192], loss=68.1506
	step [171/192], loss=58.1461
	step [172/192], loss=68.7637
	step [173/192], loss=65.0490
	step [174/192], loss=61.2952
	step [175/192], loss=63.2177
	step [176/192], loss=54.1826
	step [177/192], loss=64.2618
	step [178/192], loss=69.6607
	step [179/192], loss=64.9499
	step [180/192], loss=58.2447
	step [181/192], loss=68.7743
	step [182/192], loss=63.6104
	step [183/192], loss=68.7572
	step [184/192], loss=61.4836
	step [185/192], loss=67.4985
	step [186/192], loss=63.9773
	step [187/192], loss=56.9314
	step [188/192], loss=72.2569
	step [189/192], loss=65.0942
	step [190/192], loss=62.3339
	step [191/192], loss=59.0825
	step [192/192], loss=62.9437
	Evaluating
	loss=0.0056, precision=0.4333, recall=0.8703, f1=0.5785
saving model as: 0_saved_model.pth
Training epoch 139
	step [1/192], loss=68.3406
	step [2/192], loss=58.9251
	step [3/192], loss=74.9565
	step [4/192], loss=57.5609
	step [5/192], loss=70.4457
	step [6/192], loss=69.8329
	step [7/192], loss=75.2730
	step [8/192], loss=62.3588
	step [9/192], loss=60.3173
	step [10/192], loss=61.1130
	step [11/192], loss=72.0262
	step [12/192], loss=67.7055
	step [13/192], loss=72.8434
	step [14/192], loss=53.6257
	step [15/192], loss=70.4892
	step [16/192], loss=63.1292
	step [17/192], loss=70.2068
	step [18/192], loss=49.7201
	step [19/192], loss=60.1311
	step [20/192], loss=61.5069
	step [21/192], loss=57.0363
	step [22/192], loss=53.5434
	step [23/192], loss=58.6726
	step [24/192], loss=61.5905
	step [25/192], loss=69.4729
	step [26/192], loss=58.5683
	step [27/192], loss=47.3477
	step [28/192], loss=67.3160
	step [29/192], loss=63.7488
	step [30/192], loss=57.8491
	step [31/192], loss=58.0637
	step [32/192], loss=63.0235
	step [33/192], loss=54.9393
	step [34/192], loss=60.2299
	step [35/192], loss=62.3991
	step [36/192], loss=68.4940
	step [37/192], loss=55.7146
	step [38/192], loss=70.0016
	step [39/192], loss=65.5542
	step [40/192], loss=63.7227
	step [41/192], loss=65.2079
	step [42/192], loss=68.9781
	step [43/192], loss=60.4468
	step [44/192], loss=59.6731
	step [45/192], loss=53.4627
	step [46/192], loss=77.7747
	step [47/192], loss=70.5471
	step [48/192], loss=63.8176
	step [49/192], loss=71.1420
	step [50/192], loss=57.6489
	step [51/192], loss=73.5871
	step [52/192], loss=54.7579
	step [53/192], loss=56.8371
	step [54/192], loss=66.3124
	step [55/192], loss=64.6422
	step [56/192], loss=67.9422
	step [57/192], loss=53.8314
	step [58/192], loss=59.0782
	step [59/192], loss=64.4483
	step [60/192], loss=58.4921
	step [61/192], loss=59.4403
	step [62/192], loss=59.5645
	step [63/192], loss=55.2894
	step [64/192], loss=68.1979
	step [65/192], loss=82.1299
	step [66/192], loss=62.6444
	step [67/192], loss=64.2361
	step [68/192], loss=51.6847
	step [69/192], loss=66.6582
	step [70/192], loss=69.5103
	step [71/192], loss=62.5287
	step [72/192], loss=71.0387
	step [73/192], loss=62.5084
	step [74/192], loss=63.2916
	step [75/192], loss=67.6254
	step [76/192], loss=69.6010
	step [77/192], loss=61.8770
	step [78/192], loss=61.7724
	step [79/192], loss=60.5343
	step [80/192], loss=58.0869
	step [81/192], loss=67.2209
	step [82/192], loss=61.1325
	step [83/192], loss=56.2564
	step [84/192], loss=63.2859
	step [85/192], loss=72.5889
	step [86/192], loss=59.5350
	step [87/192], loss=56.3635
	step [88/192], loss=58.9597
	step [89/192], loss=72.7576
	step [90/192], loss=56.7075
	step [91/192], loss=68.3981
	step [92/192], loss=65.3736
	step [93/192], loss=52.4245
	step [94/192], loss=56.4363
	step [95/192], loss=58.2257
	step [96/192], loss=61.7785
	step [97/192], loss=70.5991
	step [98/192], loss=65.3661
	step [99/192], loss=65.8764
	step [100/192], loss=61.4387
	step [101/192], loss=77.7880
	step [102/192], loss=61.2683
	step [103/192], loss=62.6666
	step [104/192], loss=61.2093
	step [105/192], loss=69.6885
	step [106/192], loss=61.3992
	step [107/192], loss=68.6171
	step [108/192], loss=56.6308
	step [109/192], loss=61.5178
	step [110/192], loss=64.2337
	step [111/192], loss=72.0812
	step [112/192], loss=56.2867
	step [113/192], loss=69.0274
	step [114/192], loss=66.9779
	step [115/192], loss=62.0474
	step [116/192], loss=55.6829
	step [117/192], loss=69.5616
	step [118/192], loss=64.3741
	step [119/192], loss=71.9684
	step [120/192], loss=67.6425
	step [121/192], loss=59.6382
	step [122/192], loss=59.1807
	step [123/192], loss=58.3913
	step [124/192], loss=67.1527
	step [125/192], loss=67.1575
	step [126/192], loss=54.7479
	step [127/192], loss=66.0217
	step [128/192], loss=68.9253
	step [129/192], loss=77.4620
	step [130/192], loss=61.4313
	step [131/192], loss=67.6304
	step [132/192], loss=67.0662
	step [133/192], loss=63.5919
	step [134/192], loss=72.4466
	step [135/192], loss=62.3215
	step [136/192], loss=52.4460
	step [137/192], loss=73.4998
	step [138/192], loss=67.5338
	step [139/192], loss=60.3976
	step [140/192], loss=63.5489
	step [141/192], loss=72.2471
	step [142/192], loss=57.5245
	step [143/192], loss=58.3401
	step [144/192], loss=77.3807
	step [145/192], loss=55.1475
	step [146/192], loss=65.6154
	step [147/192], loss=64.1996
	step [148/192], loss=58.5671
	step [149/192], loss=53.0386
	step [150/192], loss=62.4647
	step [151/192], loss=56.7109
	step [152/192], loss=61.4102
	step [153/192], loss=62.6318
	step [154/192], loss=58.8358
	step [155/192], loss=60.1442
	step [156/192], loss=74.5610
	step [157/192], loss=65.3864
	step [158/192], loss=68.1016
	step [159/192], loss=49.5757
	step [160/192], loss=55.9857
	step [161/192], loss=63.7323
	step [162/192], loss=65.4751
	step [163/192], loss=67.7118
	step [164/192], loss=60.5690
	step [165/192], loss=65.4667
	step [166/192], loss=54.5678
	step [167/192], loss=65.0510
	step [168/192], loss=68.7716
	step [169/192], loss=47.6085
	step [170/192], loss=60.3328
	step [171/192], loss=63.7910
	step [172/192], loss=50.6558
	step [173/192], loss=67.0280
	step [174/192], loss=61.9913
	step [175/192], loss=68.3531
	step [176/192], loss=62.5913
	step [177/192], loss=65.8605
	step [178/192], loss=81.5715
	step [179/192], loss=64.1685
	step [180/192], loss=68.8992
	step [181/192], loss=52.9066
	step [182/192], loss=64.7972
	step [183/192], loss=63.2339
	step [184/192], loss=69.1909
	step [185/192], loss=54.2607
	step [186/192], loss=66.4512
	step [187/192], loss=77.9972
	step [188/192], loss=56.6521
	step [189/192], loss=65.3483
	step [190/192], loss=56.6367
	step [191/192], loss=72.0544
	step [192/192], loss=51.6116
	Evaluating
	loss=0.0065, precision=0.3688, recall=0.8671, f1=0.5175
Training epoch 140
	step [1/192], loss=62.6920
	step [2/192], loss=59.0970
	step [3/192], loss=54.2702
	step [4/192], loss=60.4403
	step [5/192], loss=53.2459
	step [6/192], loss=75.1142
	step [7/192], loss=71.5007
	step [8/192], loss=67.4761
	step [9/192], loss=55.2422
	step [10/192], loss=66.1583
	step [11/192], loss=65.5298
	step [12/192], loss=74.0203
	step [13/192], loss=60.2495
	step [14/192], loss=56.2272
	step [15/192], loss=68.6628
	step [16/192], loss=66.0311
	step [17/192], loss=61.5408
	step [18/192], loss=61.6975
	step [19/192], loss=55.8490
	step [20/192], loss=60.3991
	step [21/192], loss=53.9959
	step [22/192], loss=57.8656
	step [23/192], loss=55.6651
	step [24/192], loss=67.2487
	step [25/192], loss=65.2491
	step [26/192], loss=58.9705
	step [27/192], loss=58.9468
	step [28/192], loss=64.0399
	step [29/192], loss=54.6897
	step [30/192], loss=80.3241
	step [31/192], loss=69.2768
	step [32/192], loss=58.4048
	step [33/192], loss=71.1545
	step [34/192], loss=69.5931
	step [35/192], loss=71.6784
	step [36/192], loss=66.0686
	step [37/192], loss=62.8127
	step [38/192], loss=60.5966
	step [39/192], loss=60.9935
	step [40/192], loss=52.2487
	step [41/192], loss=74.2067
	step [42/192], loss=61.8063
	step [43/192], loss=68.3505
	step [44/192], loss=71.7315
	step [45/192], loss=62.3483
	step [46/192], loss=72.5195
	step [47/192], loss=67.4966
	step [48/192], loss=67.3848
	step [49/192], loss=75.3731
	step [50/192], loss=62.6663
	step [51/192], loss=59.8976
	step [52/192], loss=62.5672
	step [53/192], loss=71.7624
	step [54/192], loss=55.4604
	step [55/192], loss=66.8956
	step [56/192], loss=64.1730
	step [57/192], loss=61.0042
	step [58/192], loss=70.9968
	step [59/192], loss=62.2108
	step [60/192], loss=52.9846
	step [61/192], loss=67.9459
	step [62/192], loss=48.5179
	step [63/192], loss=62.4109
	step [64/192], loss=52.8107
	step [65/192], loss=59.1250
	step [66/192], loss=57.6908
	step [67/192], loss=61.9157
	step [68/192], loss=59.4322
	step [69/192], loss=55.3570
	step [70/192], loss=63.1337
	step [71/192], loss=57.4060
	step [72/192], loss=61.4182
	step [73/192], loss=57.7364
	step [74/192], loss=74.8569
	step [75/192], loss=54.3195
	step [76/192], loss=62.9565
	step [77/192], loss=58.0541
	step [78/192], loss=75.9074
	step [79/192], loss=75.4292
	step [80/192], loss=56.0874
	step [81/192], loss=70.0380
	step [82/192], loss=72.1837
	step [83/192], loss=60.9947
	step [84/192], loss=60.1693
	step [85/192], loss=60.4097
	step [86/192], loss=70.8985
	step [87/192], loss=68.0012
	step [88/192], loss=46.2019
	step [89/192], loss=66.7573
	step [90/192], loss=62.9843
	step [91/192], loss=72.3656
	step [92/192], loss=76.4714
	step [93/192], loss=54.1999
	step [94/192], loss=69.6420
	step [95/192], loss=54.9577
	step [96/192], loss=69.0136
	step [97/192], loss=63.8663
	step [98/192], loss=76.8627
	step [99/192], loss=48.9075
	step [100/192], loss=67.8383
	step [101/192], loss=57.1127
	step [102/192], loss=68.9046
	step [103/192], loss=61.5888
	step [104/192], loss=61.8664
	step [105/192], loss=69.8892
	step [106/192], loss=72.3475
	step [107/192], loss=63.3746
	step [108/192], loss=55.2611
	step [109/192], loss=61.2778
	step [110/192], loss=59.4254
	step [111/192], loss=65.3874
	step [112/192], loss=59.3667
	step [113/192], loss=62.3121
	step [114/192], loss=68.8032
	step [115/192], loss=54.7544
	step [116/192], loss=55.0067
	step [117/192], loss=59.5759
	step [118/192], loss=69.0280
	step [119/192], loss=59.9630
	step [120/192], loss=67.6652
	step [121/192], loss=58.1551
	step [122/192], loss=65.0673
	step [123/192], loss=76.0386
	step [124/192], loss=56.6085
	step [125/192], loss=69.1446
	step [126/192], loss=63.1507
	step [127/192], loss=61.1406
	step [128/192], loss=57.0367
	step [129/192], loss=59.4442
	step [130/192], loss=64.3459
	step [131/192], loss=67.7953
	step [132/192], loss=60.6845
	step [133/192], loss=59.1053
	step [134/192], loss=60.0908
	step [135/192], loss=68.4981
	step [136/192], loss=65.1792
	step [137/192], loss=78.3080
	step [138/192], loss=62.7527
	step [139/192], loss=68.0603
	step [140/192], loss=53.0053
	step [141/192], loss=70.4883
	step [142/192], loss=61.3206
	step [143/192], loss=69.6418
	step [144/192], loss=63.8236
	step [145/192], loss=74.9850
	step [146/192], loss=54.9759
	step [147/192], loss=52.0953
	step [148/192], loss=66.0672
	step [149/192], loss=62.8159
	step [150/192], loss=60.1264
	step [151/192], loss=61.6795
	step [152/192], loss=55.9208
	step [153/192], loss=61.6871
	step [154/192], loss=72.0585
	step [155/192], loss=71.8069
	step [156/192], loss=66.8328
	step [157/192], loss=59.6082
	step [158/192], loss=75.9180
	step [159/192], loss=67.1115
	step [160/192], loss=66.2124
	step [161/192], loss=55.7059
	step [162/192], loss=81.7238
	step [163/192], loss=62.0197
	step [164/192], loss=66.2413
	step [165/192], loss=63.2347
	step [166/192], loss=59.4624
	step [167/192], loss=63.9884
	step [168/192], loss=63.0971
	step [169/192], loss=52.9248
	step [170/192], loss=61.3373
	step [171/192], loss=73.3424
	step [172/192], loss=63.8315
	step [173/192], loss=74.3239
	step [174/192], loss=55.4303
	step [175/192], loss=63.1057
	step [176/192], loss=57.3906
	step [177/192], loss=59.2536
	step [178/192], loss=66.8231
	step [179/192], loss=59.9675
	step [180/192], loss=63.0268
	step [181/192], loss=72.5092
	step [182/192], loss=57.2628
	step [183/192], loss=64.4870
	step [184/192], loss=67.7591
	step [185/192], loss=78.8772
	step [186/192], loss=58.9535
	step [187/192], loss=60.7437
	step [188/192], loss=60.7444
	step [189/192], loss=60.0524
	step [190/192], loss=47.4784
	step [191/192], loss=58.6843
	step [192/192], loss=53.9465
	Evaluating
	loss=0.0067, precision=0.3752, recall=0.8656, f1=0.5235
Training epoch 141
	step [1/192], loss=63.8769
	step [2/192], loss=67.5182
	step [3/192], loss=57.3407
	step [4/192], loss=60.7519
	step [5/192], loss=66.2520
	step [6/192], loss=71.6305
	step [7/192], loss=67.0395
	step [8/192], loss=59.2477
	step [9/192], loss=60.1782
	step [10/192], loss=68.0526
	step [11/192], loss=50.6304
	step [12/192], loss=67.5491
	step [13/192], loss=62.7566
	step [14/192], loss=70.6651
	step [15/192], loss=70.5487
	step [16/192], loss=47.8530
	step [17/192], loss=68.3717
	step [18/192], loss=56.5320
	step [19/192], loss=67.2908
	step [20/192], loss=56.9437
	step [21/192], loss=73.7076
	step [22/192], loss=75.4407
	step [23/192], loss=64.7282
	step [24/192], loss=58.5859
	step [25/192], loss=74.0447
	step [26/192], loss=64.9460
	step [27/192], loss=63.0669
	step [28/192], loss=60.0378
	step [29/192], loss=62.2153
	step [30/192], loss=53.2053
	step [31/192], loss=72.2732
	step [32/192], loss=58.0293
	step [33/192], loss=69.1033
	step [34/192], loss=65.8802
	step [35/192], loss=57.6958
	step [36/192], loss=77.7463
	step [37/192], loss=63.5602
	step [38/192], loss=62.1483
	step [39/192], loss=61.5153
	step [40/192], loss=61.8140
	step [41/192], loss=65.4274
	step [42/192], loss=65.6638
	step [43/192], loss=74.9854
	step [44/192], loss=72.9030
	step [45/192], loss=62.3531
	step [46/192], loss=51.3278
	step [47/192], loss=60.2316
	step [48/192], loss=70.7254
	step [49/192], loss=62.4605
	step [50/192], loss=57.1876
	step [51/192], loss=64.0724
	step [52/192], loss=57.9683
	step [53/192], loss=59.2497
	step [54/192], loss=58.9518
	step [55/192], loss=58.4432
	step [56/192], loss=73.5982
	step [57/192], loss=68.3797
	step [58/192], loss=59.7702
	step [59/192], loss=61.7856
	step [60/192], loss=61.6280
	step [61/192], loss=58.6141
	step [62/192], loss=64.7606
	step [63/192], loss=70.9230
	step [64/192], loss=70.2265
	step [65/192], loss=69.8102
	step [66/192], loss=62.3763
	step [67/192], loss=68.0397
	step [68/192], loss=63.9235
	step [69/192], loss=70.8979
	step [70/192], loss=62.2841
	step [71/192], loss=58.9519
	step [72/192], loss=69.0917
	step [73/192], loss=58.6532
	step [74/192], loss=68.8451
	step [75/192], loss=62.8463
	step [76/192], loss=55.9982
	step [77/192], loss=66.9005
	step [78/192], loss=56.0175
	step [79/192], loss=60.5725
	step [80/192], loss=53.5918
	step [81/192], loss=74.9898
	step [82/192], loss=61.2153
	step [83/192], loss=68.1561
	step [84/192], loss=57.0133
	step [85/192], loss=64.2054
	step [86/192], loss=73.5336
	step [87/192], loss=61.1992
	step [88/192], loss=58.7056
	step [89/192], loss=60.9389
	step [90/192], loss=61.6370
	step [91/192], loss=56.9537
	step [92/192], loss=66.6261
	step [93/192], loss=68.4629
	step [94/192], loss=62.5386
	step [95/192], loss=69.2083
	step [96/192], loss=70.3925
	step [97/192], loss=61.3346
	step [98/192], loss=72.7851
	step [99/192], loss=59.0203
	step [100/192], loss=68.0934
	step [101/192], loss=63.0212
	step [102/192], loss=58.4635
	step [103/192], loss=58.3955
	step [104/192], loss=62.9391
	step [105/192], loss=64.8192
	step [106/192], loss=67.5543
	step [107/192], loss=65.4258
	step [108/192], loss=57.0822
	step [109/192], loss=58.0567
	step [110/192], loss=75.8407
	step [111/192], loss=61.6222
	step [112/192], loss=52.6642
	step [113/192], loss=52.6381
	step [114/192], loss=58.5287
	step [115/192], loss=52.3237
	step [116/192], loss=58.7197
	step [117/192], loss=54.8265
	step [118/192], loss=61.9550
	step [119/192], loss=51.1281
	step [120/192], loss=55.7162
	step [121/192], loss=57.5036
	step [122/192], loss=75.5802
	step [123/192], loss=62.8728
	step [124/192], loss=72.2735
	step [125/192], loss=62.1445
	step [126/192], loss=72.9833
	step [127/192], loss=67.3639
	step [128/192], loss=62.4369
	step [129/192], loss=57.4473
	step [130/192], loss=58.4500
	step [131/192], loss=65.9258
	step [132/192], loss=62.0636
	step [133/192], loss=60.5039
	step [134/192], loss=66.6312
	step [135/192], loss=62.1330
	step [136/192], loss=74.6375
	step [137/192], loss=76.1488
	step [138/192], loss=65.8132
	step [139/192], loss=59.9411
	step [140/192], loss=67.3534
	step [141/192], loss=67.5080
	step [142/192], loss=60.5867
	step [143/192], loss=49.2754
	step [144/192], loss=55.2465
	step [145/192], loss=64.0591
	step [146/192], loss=57.8378
	step [147/192], loss=66.0862
	step [148/192], loss=60.2809
	step [149/192], loss=69.1516
	step [150/192], loss=64.3311
	step [151/192], loss=63.3410
	step [152/192], loss=63.5122
	step [153/192], loss=62.1492
	step [154/192], loss=55.2829
	step [155/192], loss=59.0832
	step [156/192], loss=50.0768
	step [157/192], loss=59.9181
	step [158/192], loss=60.0238
	step [159/192], loss=61.0453
	step [160/192], loss=68.3680
	step [161/192], loss=51.7060
	step [162/192], loss=62.2197
	step [163/192], loss=68.1217
	step [164/192], loss=65.9896
	step [165/192], loss=66.4173
	step [166/192], loss=59.8622
	step [167/192], loss=56.7370
	step [168/192], loss=69.8097
	step [169/192], loss=65.5441
	step [170/192], loss=58.3733
	step [171/192], loss=79.6626
	step [172/192], loss=67.3740
	step [173/192], loss=72.7608
	step [174/192], loss=64.6588
	step [175/192], loss=63.8586
	step [176/192], loss=47.3753
	step [177/192], loss=61.4279
	step [178/192], loss=65.3236
	step [179/192], loss=67.4771
	step [180/192], loss=71.6560
	step [181/192], loss=63.4903
	step [182/192], loss=67.6123
	step [183/192], loss=68.5213
	step [184/192], loss=63.9425
	step [185/192], loss=46.3160
	step [186/192], loss=85.1368
	step [187/192], loss=74.1070
	step [188/192], loss=63.9934
	step [189/192], loss=50.2291
	step [190/192], loss=54.9300
	step [191/192], loss=57.0285
	step [192/192], loss=39.1869
	Evaluating
	loss=0.0072, precision=0.3191, recall=0.8608, f1=0.4656
Training epoch 142
	step [1/192], loss=61.4316
	step [2/192], loss=63.8478
	step [3/192], loss=65.8161
	step [4/192], loss=58.5070
	step [5/192], loss=66.9463
	step [6/192], loss=57.2130
	step [7/192], loss=61.4040
	step [8/192], loss=54.4379
	step [9/192], loss=69.1287
	step [10/192], loss=71.7741
	step [11/192], loss=61.1952
	step [12/192], loss=62.5714
	step [13/192], loss=68.2038
	step [14/192], loss=59.1979
	step [15/192], loss=66.4401
	step [16/192], loss=60.5748
	step [17/192], loss=57.1138
	step [18/192], loss=72.5596
	step [19/192], loss=52.3960
	step [20/192], loss=56.1758
	step [21/192], loss=65.4212
	step [22/192], loss=60.7136
	step [23/192], loss=64.6637
	step [24/192], loss=70.8752
	step [25/192], loss=80.8584
	step [26/192], loss=64.9300
	step [27/192], loss=61.3236
	step [28/192], loss=72.5063
	step [29/192], loss=65.2231
	step [30/192], loss=71.1378
	step [31/192], loss=53.2455
	step [32/192], loss=56.8004
	step [33/192], loss=72.2939
	step [34/192], loss=61.3458
	step [35/192], loss=66.7664
	step [36/192], loss=59.4193
	step [37/192], loss=78.8359
	step [38/192], loss=61.6348
	step [39/192], loss=60.4481
	step [40/192], loss=57.0313
	step [41/192], loss=66.2508
	step [42/192], loss=73.2255
	step [43/192], loss=62.4735
	step [44/192], loss=75.8994
	step [45/192], loss=63.7968
	step [46/192], loss=59.8323
	step [47/192], loss=66.4749
	step [48/192], loss=63.6037
	step [49/192], loss=65.3925
	step [50/192], loss=62.7814
	step [51/192], loss=64.2640
	step [52/192], loss=54.1320
	step [53/192], loss=60.2168
	step [54/192], loss=65.8971
	step [55/192], loss=55.1866
	step [56/192], loss=55.8197
	step [57/192], loss=61.1239
	step [58/192], loss=57.5489
	step [59/192], loss=59.0277
	step [60/192], loss=61.4664
	step [61/192], loss=58.6200
	step [62/192], loss=78.4790
	step [63/192], loss=58.2238
	step [64/192], loss=62.7045
	step [65/192], loss=58.8204
	step [66/192], loss=57.8118
	step [67/192], loss=55.0107
	step [68/192], loss=48.9839
	step [69/192], loss=63.2946
	step [70/192], loss=54.2346
	step [71/192], loss=62.5895
	step [72/192], loss=63.7889
	step [73/192], loss=60.3962
	step [74/192], loss=66.3219
	step [75/192], loss=61.9347
	step [76/192], loss=74.2150
	step [77/192], loss=71.2691
	step [78/192], loss=70.0906
	step [79/192], loss=58.5182
	step [80/192], loss=57.4228
	step [81/192], loss=69.8800
	step [82/192], loss=68.9564
	step [83/192], loss=67.2281
	step [84/192], loss=56.8399
	step [85/192], loss=61.8636
	step [86/192], loss=47.0968
	step [87/192], loss=56.3189
	step [88/192], loss=51.8271
	step [89/192], loss=56.9349
	step [90/192], loss=83.6698
	step [91/192], loss=70.2458
	step [92/192], loss=76.4666
	step [93/192], loss=54.3833
	step [94/192], loss=54.7003
	step [95/192], loss=64.0371
	step [96/192], loss=57.6183
	step [97/192], loss=65.3739
	step [98/192], loss=75.6031
	step [99/192], loss=53.4717
	step [100/192], loss=55.8384
	step [101/192], loss=60.2260
	step [102/192], loss=58.5109
	step [103/192], loss=60.0511
	step [104/192], loss=60.7934
	step [105/192], loss=58.9931
	step [106/192], loss=59.5998
	step [107/192], loss=61.8225
	step [108/192], loss=70.7635
	step [109/192], loss=64.6035
	step [110/192], loss=65.8682
	step [111/192], loss=61.2361
	step [112/192], loss=63.4681
	step [113/192], loss=64.4310
	step [114/192], loss=60.0210
	step [115/192], loss=60.7500
	step [116/192], loss=64.6509
	step [117/192], loss=59.9436
	step [118/192], loss=59.8186
	step [119/192], loss=58.7086
	step [120/192], loss=67.9444
	step [121/192], loss=61.3975
	step [122/192], loss=67.5531
	step [123/192], loss=70.9721
	step [124/192], loss=62.9148
	step [125/192], loss=65.2300
	step [126/192], loss=64.0201
	step [127/192], loss=71.3646
	step [128/192], loss=56.4021
	step [129/192], loss=58.5980
	step [130/192], loss=65.2225
	step [131/192], loss=58.4807
	step [132/192], loss=63.0767
	step [133/192], loss=52.7485
	step [134/192], loss=56.2060
	step [135/192], loss=71.9219
	step [136/192], loss=68.5052
	step [137/192], loss=59.2740
	step [138/192], loss=64.5392
	step [139/192], loss=66.9468
	step [140/192], loss=62.3599
	step [141/192], loss=71.1834
	step [142/192], loss=69.5615
	step [143/192], loss=63.8345
	step [144/192], loss=66.9011
	step [145/192], loss=68.1018
	step [146/192], loss=63.2904
	step [147/192], loss=58.3725
	step [148/192], loss=70.7120
	step [149/192], loss=65.7663
	step [150/192], loss=62.2591
	step [151/192], loss=57.5761
	step [152/192], loss=63.2042
	step [153/192], loss=60.9131
	step [154/192], loss=52.7951
	step [155/192], loss=60.6809
	step [156/192], loss=60.5914
	step [157/192], loss=56.3540
	step [158/192], loss=50.5938
	step [159/192], loss=69.3382
	step [160/192], loss=62.3310
	step [161/192], loss=61.4924
	step [162/192], loss=58.9480
	step [163/192], loss=67.6289
	step [164/192], loss=60.8770
	step [165/192], loss=60.3161
	step [166/192], loss=64.1480
	step [167/192], loss=63.0952
	step [168/192], loss=61.4898
	step [169/192], loss=74.0152
	step [170/192], loss=71.6150
	step [171/192], loss=58.2271
	step [172/192], loss=64.7760
	step [173/192], loss=66.3368
	step [174/192], loss=62.2595
	step [175/192], loss=67.1618
	step [176/192], loss=66.1338
	step [177/192], loss=66.5472
	step [178/192], loss=55.6866
	step [179/192], loss=79.2439
	step [180/192], loss=61.6046
	step [181/192], loss=65.5425
	step [182/192], loss=48.1225
	step [183/192], loss=69.9088
	step [184/192], loss=60.4341
	step [185/192], loss=67.2964
	step [186/192], loss=62.9629
	step [187/192], loss=55.7961
	step [188/192], loss=68.6404
	step [189/192], loss=60.7969
	step [190/192], loss=67.1351
	step [191/192], loss=79.3946
	step [192/192], loss=52.7711
	Evaluating
	loss=0.0061, precision=0.3788, recall=0.8716, f1=0.5281
Training epoch 143
	step [1/192], loss=70.3260
	step [2/192], loss=66.5455
	step [3/192], loss=54.9985
	step [4/192], loss=55.2641
	step [5/192], loss=59.5697
	step [6/192], loss=62.9680
	step [7/192], loss=69.1480
	step [8/192], loss=61.4212
	step [9/192], loss=67.2955
	step [10/192], loss=62.7637
	step [11/192], loss=52.3201
	step [12/192], loss=66.3520
	step [13/192], loss=52.8403
	step [14/192], loss=59.9310
	step [15/192], loss=63.3427
	step [16/192], loss=56.2220
	step [17/192], loss=52.3958
	step [18/192], loss=68.3698
	step [19/192], loss=59.1069
	step [20/192], loss=62.1139
	step [21/192], loss=62.8941
	step [22/192], loss=63.7344
	step [23/192], loss=62.6911
	step [24/192], loss=59.3997
	step [25/192], loss=63.5197
	step [26/192], loss=65.1964
	step [27/192], loss=73.9488
	step [28/192], loss=65.5190
	step [29/192], loss=61.0845
	step [30/192], loss=60.1321
	step [31/192], loss=80.4765
	step [32/192], loss=65.4259
	step [33/192], loss=50.7586
	step [34/192], loss=68.6636
	step [35/192], loss=57.7041
	step [36/192], loss=64.3438
	step [37/192], loss=53.9780
	step [38/192], loss=71.6564
	step [39/192], loss=54.5621
	step [40/192], loss=55.6476
	step [41/192], loss=68.0265
	step [42/192], loss=58.3228
	step [43/192], loss=54.7108
	step [44/192], loss=58.7198
	step [45/192], loss=68.0100
	step [46/192], loss=71.2775
	step [47/192], loss=59.1366
	step [48/192], loss=67.0786
	step [49/192], loss=62.9787
	step [50/192], loss=60.2408
	step [51/192], loss=77.2426
	step [52/192], loss=69.7401
	step [53/192], loss=53.6883
	step [54/192], loss=60.9096
	step [55/192], loss=70.3569
	step [56/192], loss=72.8616
	step [57/192], loss=55.2723
	step [58/192], loss=57.9077
	step [59/192], loss=69.4177
	step [60/192], loss=57.3210
	step [61/192], loss=60.1827
	step [62/192], loss=76.5320
	step [63/192], loss=62.2080
	step [64/192], loss=68.6488
	step [65/192], loss=58.6567
	step [66/192], loss=65.4187
	step [67/192], loss=64.3880
	step [68/192], loss=67.0978
	step [69/192], loss=76.4464
	step [70/192], loss=60.6523
	step [71/192], loss=60.9000
	step [72/192], loss=66.1095
	step [73/192], loss=68.8232
	step [74/192], loss=68.0349
	step [75/192], loss=70.5126
	step [76/192], loss=66.0225
	step [77/192], loss=59.0600
	step [78/192], loss=73.3175
	step [79/192], loss=87.9601
	step [80/192], loss=68.6550
	step [81/192], loss=49.3961
	step [82/192], loss=63.4399
	step [83/192], loss=63.5944
	step [84/192], loss=61.6606
	step [85/192], loss=66.0713
	step [86/192], loss=49.4428
	step [87/192], loss=69.4488
	step [88/192], loss=74.9724
	step [89/192], loss=63.7127
	step [90/192], loss=56.6859
	step [91/192], loss=67.1946
	step [92/192], loss=49.4626
	step [93/192], loss=68.0430
	step [94/192], loss=59.7147
	step [95/192], loss=67.1281
	step [96/192], loss=64.6808
	step [97/192], loss=59.3697
	step [98/192], loss=71.7779
	step [99/192], loss=62.0537
	step [100/192], loss=74.9163
	step [101/192], loss=69.1144
	step [102/192], loss=65.8839
	step [103/192], loss=59.8271
	step [104/192], loss=58.1076
	step [105/192], loss=55.8923
	step [106/192], loss=60.3906
	step [107/192], loss=63.4902
	step [108/192], loss=69.6924
	step [109/192], loss=71.2816
	step [110/192], loss=62.5188
	step [111/192], loss=55.0271
	step [112/192], loss=60.7154
	step [113/192], loss=58.9462
	step [114/192], loss=63.5119
	step [115/192], loss=59.5950
	step [116/192], loss=73.5860
	step [117/192], loss=63.7831
	step [118/192], loss=70.8318
	step [119/192], loss=60.1835
	step [120/192], loss=57.5359
	step [121/192], loss=68.1662
	step [122/192], loss=49.3599
	step [123/192], loss=61.3969
	step [124/192], loss=59.7130
	step [125/192], loss=56.2601
	step [126/192], loss=59.5606
	step [127/192], loss=63.2787
	step [128/192], loss=62.5399
	step [129/192], loss=66.5481
	step [130/192], loss=64.3570
	step [131/192], loss=54.7922
	step [132/192], loss=63.2935
	step [133/192], loss=72.0792
	step [134/192], loss=66.9113
	step [135/192], loss=65.5141
	step [136/192], loss=62.9504
	step [137/192], loss=65.1873
	step [138/192], loss=55.0201
	step [139/192], loss=46.3509
	step [140/192], loss=58.7402
	step [141/192], loss=73.3242
	step [142/192], loss=79.1797
	step [143/192], loss=58.3876
	step [144/192], loss=53.5629
	step [145/192], loss=67.9525
	step [146/192], loss=60.2355
	step [147/192], loss=64.5557
	step [148/192], loss=58.2323
	step [149/192], loss=70.6607
	step [150/192], loss=70.2027
	step [151/192], loss=50.8303
	step [152/192], loss=62.5424
	step [153/192], loss=54.8969
	step [154/192], loss=59.3335
	step [155/192], loss=54.1215
	step [156/192], loss=54.7146
	step [157/192], loss=70.6753
	step [158/192], loss=50.8502
	step [159/192], loss=58.6806
	step [160/192], loss=55.8991
	step [161/192], loss=71.4935
	step [162/192], loss=58.1934
	step [163/192], loss=74.6130
	step [164/192], loss=62.9639
	step [165/192], loss=55.8029
	step [166/192], loss=74.8584
	step [167/192], loss=64.8155
	step [168/192], loss=60.2587
	step [169/192], loss=75.4994
	step [170/192], loss=60.9002
	step [171/192], loss=58.0018
	step [172/192], loss=64.4996
	step [173/192], loss=69.5078
	step [174/192], loss=62.4029
	step [175/192], loss=70.7089
	step [176/192], loss=55.4046
	step [177/192], loss=64.0546
	step [178/192], loss=62.6408
	step [179/192], loss=59.1653
	step [180/192], loss=53.8635
	step [181/192], loss=71.7093
	step [182/192], loss=63.2075
	step [183/192], loss=63.0127
	step [184/192], loss=66.2552
	step [185/192], loss=50.9008
	step [186/192], loss=70.3731
	step [187/192], loss=58.5062
	step [188/192], loss=67.3534
	step [189/192], loss=59.2769
	step [190/192], loss=67.5277
	step [191/192], loss=56.8715
	step [192/192], loss=65.1334
	Evaluating
	loss=0.0058, precision=0.4115, recall=0.8589, f1=0.5564
Training epoch 144
	step [1/192], loss=70.1839
	step [2/192], loss=60.3305
	step [3/192], loss=64.9422
	step [4/192], loss=61.7304
	step [5/192], loss=68.6067
	step [6/192], loss=61.5870
	step [7/192], loss=70.0270
	step [8/192], loss=52.7477
	step [9/192], loss=65.8589
	step [10/192], loss=57.0463
	step [11/192], loss=60.9757
	step [12/192], loss=59.7834
	step [13/192], loss=58.6556
	step [14/192], loss=66.3914
	step [15/192], loss=61.1569
	step [16/192], loss=81.6886
	step [17/192], loss=71.7686
	step [18/192], loss=61.5204
	step [19/192], loss=61.5207
	step [20/192], loss=62.1068
	step [21/192], loss=64.0252
	step [22/192], loss=63.9221
	step [23/192], loss=72.5631
	step [24/192], loss=59.7500
	step [25/192], loss=58.2806
	step [26/192], loss=57.3545
	step [27/192], loss=52.9754
	step [28/192], loss=57.1140
	step [29/192], loss=64.6009
	step [30/192], loss=65.0545
	step [31/192], loss=56.5717
	step [32/192], loss=71.3482
	step [33/192], loss=74.8072
	step [34/192], loss=67.9510
	step [35/192], loss=61.3211
	step [36/192], loss=64.5894
	step [37/192], loss=60.8332
	step [38/192], loss=65.2614
	step [39/192], loss=63.8256
	step [40/192], loss=59.4526
	step [41/192], loss=66.2334
	step [42/192], loss=53.7633
	step [43/192], loss=65.4798
	step [44/192], loss=64.2423
	step [45/192], loss=67.7088
	step [46/192], loss=73.4456
	step [47/192], loss=59.5508
	step [48/192], loss=75.9784
	step [49/192], loss=67.7699
	step [50/192], loss=58.9979
	step [51/192], loss=68.5257
	step [52/192], loss=53.2459
	step [53/192], loss=54.1438
	step [54/192], loss=61.7833
	step [55/192], loss=61.0761
	step [56/192], loss=60.2046
	step [57/192], loss=73.2826
	step [58/192], loss=70.9853
	step [59/192], loss=55.0059
	step [60/192], loss=59.8948
	step [61/192], loss=62.8031
	step [62/192], loss=63.0056
	step [63/192], loss=78.4213
	step [64/192], loss=53.5282
	step [65/192], loss=65.6026
	step [66/192], loss=63.2669
	step [67/192], loss=56.6184
	step [68/192], loss=70.0145
	step [69/192], loss=70.5251
	step [70/192], loss=67.6249
	step [71/192], loss=47.6217
	step [72/192], loss=71.5099
	step [73/192], loss=68.7461
	step [74/192], loss=62.1343
	step [75/192], loss=56.1099
	step [76/192], loss=57.6200
	step [77/192], loss=60.5010
	step [78/192], loss=57.1532
	step [79/192], loss=57.8681
	step [80/192], loss=57.0418
	step [81/192], loss=62.2516
	step [82/192], loss=65.1725
	step [83/192], loss=68.3964
	step [84/192], loss=62.2002
	step [85/192], loss=57.7265
	step [86/192], loss=61.7772
	step [87/192], loss=51.0284
	step [88/192], loss=69.8472
	step [89/192], loss=61.1206
	step [90/192], loss=65.4941
	step [91/192], loss=62.5314
	step [92/192], loss=68.6076
	step [93/192], loss=64.4580
	step [94/192], loss=57.6792
	step [95/192], loss=67.0461
	step [96/192], loss=61.2115
	step [97/192], loss=55.3247
	step [98/192], loss=60.6467
	step [99/192], loss=60.0452
	step [100/192], loss=60.0503
	step [101/192], loss=67.9413
	step [102/192], loss=65.2259
	step [103/192], loss=63.8296
	step [104/192], loss=60.3917
	step [105/192], loss=63.7962
	step [106/192], loss=63.6209
	step [107/192], loss=61.0325
	step [108/192], loss=73.8434
	step [109/192], loss=78.1301
	step [110/192], loss=66.2356
	step [111/192], loss=68.3168
	step [112/192], loss=57.2383
	step [113/192], loss=61.4753
	step [114/192], loss=62.1755
	step [115/192], loss=61.5315
	step [116/192], loss=61.8015
	step [117/192], loss=50.9682
	step [118/192], loss=56.0650
	step [119/192], loss=59.8435
	step [120/192], loss=62.9721
	step [121/192], loss=62.5038
	step [122/192], loss=57.3079
	step [123/192], loss=54.5195
	step [124/192], loss=58.2548
	step [125/192], loss=57.7854
	step [126/192], loss=53.3177
	step [127/192], loss=60.1811
	step [128/192], loss=67.9770
	step [129/192], loss=57.0097
	step [130/192], loss=67.1579
	step [131/192], loss=55.1509
	step [132/192], loss=68.0283
	step [133/192], loss=61.0773
	step [134/192], loss=70.9399
	step [135/192], loss=63.0753
	step [136/192], loss=62.9186
	step [137/192], loss=64.2393
	step [138/192], loss=56.0958
	step [139/192], loss=60.4128
	step [140/192], loss=66.6566
	step [141/192], loss=67.4007
	step [142/192], loss=70.8052
	step [143/192], loss=59.9094
	step [144/192], loss=64.1221
	step [145/192], loss=60.0432
	step [146/192], loss=62.4826
	step [147/192], loss=60.1848
	step [148/192], loss=68.0087
	step [149/192], loss=63.0154
	step [150/192], loss=63.1696
	step [151/192], loss=68.3386
	step [152/192], loss=62.9749
	step [153/192], loss=62.0574
	step [154/192], loss=73.6633
	step [155/192], loss=59.4531
	step [156/192], loss=67.4328
	step [157/192], loss=58.9978
	step [158/192], loss=72.7091
	step [159/192], loss=65.7208
	step [160/192], loss=64.3394
	step [161/192], loss=50.4501
	step [162/192], loss=62.2067
	step [163/192], loss=61.0217
	step [164/192], loss=53.4959
	step [165/192], loss=61.7050
	step [166/192], loss=56.2980
	step [167/192], loss=62.0472
	step [168/192], loss=60.1847
	step [169/192], loss=61.7883
	step [170/192], loss=63.0222
	step [171/192], loss=68.4594
	step [172/192], loss=73.0450
	step [173/192], loss=69.8824
	step [174/192], loss=59.8609
	step [175/192], loss=66.0032
	step [176/192], loss=66.2606
	step [177/192], loss=51.7055
	step [178/192], loss=61.0488
	step [179/192], loss=63.8758
	step [180/192], loss=69.0675
	step [181/192], loss=65.7811
	step [182/192], loss=63.7469
	step [183/192], loss=63.6463
	step [184/192], loss=68.9032
	step [185/192], loss=55.5210
	step [186/192], loss=63.0369
	step [187/192], loss=54.1295
	step [188/192], loss=62.5127
	step [189/192], loss=63.2524
	step [190/192], loss=60.3962
	step [191/192], loss=62.8856
	step [192/192], loss=60.6631
	Evaluating
	loss=0.0073, precision=0.3230, recall=0.8549, f1=0.4689
Training epoch 145
	step [1/192], loss=56.3489
	step [2/192], loss=65.5883
	step [3/192], loss=67.3784
	step [4/192], loss=81.1882
	step [5/192], loss=61.0370
	step [6/192], loss=62.3947
	step [7/192], loss=58.4414
	step [8/192], loss=67.3773
	step [9/192], loss=64.9106
	step [10/192], loss=54.9667
	step [11/192], loss=64.7006
	step [12/192], loss=69.4413
	step [13/192], loss=57.0379
	step [14/192], loss=56.7210
	step [15/192], loss=73.8203
	step [16/192], loss=49.3892
	step [17/192], loss=66.9230
	step [18/192], loss=62.9057
	step [19/192], loss=54.2058
	step [20/192], loss=65.7448
	step [21/192], loss=60.2301
	step [22/192], loss=74.8616
	step [23/192], loss=50.7564
	step [24/192], loss=72.6672
	step [25/192], loss=64.9410
	step [26/192], loss=67.0515
	step [27/192], loss=68.0701
	step [28/192], loss=62.6099
	step [29/192], loss=62.8883
	step [30/192], loss=68.9815
	step [31/192], loss=52.8730
	step [32/192], loss=54.2739
	step [33/192], loss=57.2803
	step [34/192], loss=58.1552
	step [35/192], loss=74.0371
	step [36/192], loss=56.9623
	step [37/192], loss=63.1405
	step [38/192], loss=65.3253
	step [39/192], loss=64.1049
	step [40/192], loss=60.2311
	step [41/192], loss=68.0267
	step [42/192], loss=59.5020
	step [43/192], loss=54.9766
	step [44/192], loss=60.9068
	step [45/192], loss=61.4961
	step [46/192], loss=60.9387
	step [47/192], loss=73.8155
	step [48/192], loss=63.1501
	step [49/192], loss=57.9380
	step [50/192], loss=66.0373
	step [51/192], loss=54.7041
	step [52/192], loss=67.2188
	step [53/192], loss=61.1102
	step [54/192], loss=72.2169
	step [55/192], loss=56.2950
	step [56/192], loss=65.0556
	step [57/192], loss=63.4423
	step [58/192], loss=57.5101
	step [59/192], loss=56.7495
	step [60/192], loss=62.1841
	step [61/192], loss=65.3852
	step [62/192], loss=50.3680
	step [63/192], loss=58.0670
	step [64/192], loss=61.1174
	step [65/192], loss=61.7735
	step [66/192], loss=59.2393
	step [67/192], loss=56.5369
	step [68/192], loss=66.2178
	step [69/192], loss=56.6712
	step [70/192], loss=67.8306
	step [71/192], loss=62.7753
	step [72/192], loss=58.4747
	step [73/192], loss=77.0804
	step [74/192], loss=59.5664
	step [75/192], loss=62.3740
	step [76/192], loss=73.2373
	step [77/192], loss=62.7555
	step [78/192], loss=56.8299
	step [79/192], loss=77.9510
	step [80/192], loss=54.8701
	step [81/192], loss=64.8366
	step [82/192], loss=60.9741
	step [83/192], loss=62.9553
	step [84/192], loss=75.5487
	step [85/192], loss=51.8551
	step [86/192], loss=57.6237
	step [87/192], loss=49.7983
	step [88/192], loss=66.3998
	step [89/192], loss=57.1242
	step [90/192], loss=64.0722
	step [91/192], loss=63.2448
	step [92/192], loss=68.9120
	step [93/192], loss=54.0754
	step [94/192], loss=67.0313
	step [95/192], loss=58.0658
	step [96/192], loss=67.4306
	step [97/192], loss=59.3884
	step [98/192], loss=61.9896
	step [99/192], loss=60.5341
	step [100/192], loss=56.9005
	step [101/192], loss=66.0770
	step [102/192], loss=57.4219
	step [103/192], loss=54.7792
	step [104/192], loss=74.9565
	step [105/192], loss=61.5022
	step [106/192], loss=66.5493
	step [107/192], loss=67.8135
	step [108/192], loss=70.5106
	step [109/192], loss=69.8550
	step [110/192], loss=70.2168
	step [111/192], loss=62.2455
	step [112/192], loss=57.0704
	step [113/192], loss=58.9325
	step [114/192], loss=52.1262
	step [115/192], loss=60.5740
	step [116/192], loss=62.9409
	step [117/192], loss=51.9929
	step [118/192], loss=56.9577
	step [119/192], loss=71.8928
	step [120/192], loss=60.2637
	step [121/192], loss=57.0439
	step [122/192], loss=75.4653
	step [123/192], loss=60.0853
	step [124/192], loss=59.2083
	step [125/192], loss=65.6084
	step [126/192], loss=56.2565
	step [127/192], loss=57.5514
	step [128/192], loss=66.5431
	step [129/192], loss=66.8835
	step [130/192], loss=63.9998
	step [131/192], loss=55.7097
	step [132/192], loss=69.0019
	step [133/192], loss=61.3497
	step [134/192], loss=63.9780
	step [135/192], loss=70.9990
	step [136/192], loss=58.8455
	step [137/192], loss=62.0249
	step [138/192], loss=64.5809
	step [139/192], loss=62.7087
	step [140/192], loss=67.8172
	step [141/192], loss=60.2976
	step [142/192], loss=65.3546
	step [143/192], loss=56.4844
	step [144/192], loss=64.9235
	step [145/192], loss=71.1850
	step [146/192], loss=65.1129
	step [147/192], loss=61.2290
	step [148/192], loss=65.4336
	step [149/192], loss=72.4171
	step [150/192], loss=70.9185
	step [151/192], loss=61.2626
	step [152/192], loss=67.3222
	step [153/192], loss=68.3333
	step [154/192], loss=56.5678
	step [155/192], loss=57.1929
	step [156/192], loss=62.9113
	step [157/192], loss=57.7717
	step [158/192], loss=72.1187
	step [159/192], loss=64.9710
	step [160/192], loss=65.9188
	step [161/192], loss=57.2414
	step [162/192], loss=73.2232
	step [163/192], loss=68.8620
	step [164/192], loss=69.4010
	step [165/192], loss=70.1024
	step [166/192], loss=61.4887
	step [167/192], loss=61.4027
	step [168/192], loss=63.3727
	step [169/192], loss=61.5049
	step [170/192], loss=59.3469
	step [171/192], loss=62.6902
	step [172/192], loss=60.6656
	step [173/192], loss=67.2346
	step [174/192], loss=63.5739
	step [175/192], loss=59.2908
	step [176/192], loss=69.2129
	step [177/192], loss=61.9493
	step [178/192], loss=65.4179
	step [179/192], loss=67.3157
	step [180/192], loss=63.5856
	step [181/192], loss=66.5489
	step [182/192], loss=63.9813
	step [183/192], loss=60.9516
	step [184/192], loss=63.8494
	step [185/192], loss=65.5338
	step [186/192], loss=55.3882
	step [187/192], loss=63.3090
	step [188/192], loss=54.3980
	step [189/192], loss=54.4406
	step [190/192], loss=55.7197
	step [191/192], loss=55.9721
	step [192/192], loss=56.1205
	Evaluating
	loss=0.0056, precision=0.4199, recall=0.8572, f1=0.5636
Training epoch 146
	step [1/192], loss=60.2096
	step [2/192], loss=63.5186
	step [3/192], loss=63.8499
	step [4/192], loss=65.5080
	step [5/192], loss=71.9657
	step [6/192], loss=56.7140
	step [7/192], loss=72.7768
	step [8/192], loss=83.3930
	step [9/192], loss=70.1485
	step [10/192], loss=53.1764
	step [11/192], loss=55.4413
	step [12/192], loss=56.0903
	step [13/192], loss=64.0133
	step [14/192], loss=70.0842
	step [15/192], loss=55.2186
	step [16/192], loss=65.7229
	step [17/192], loss=67.7768
	step [18/192], loss=55.9859
	step [19/192], loss=65.3344
	step [20/192], loss=64.3503
	step [21/192], loss=62.2272
	step [22/192], loss=62.4936
	step [23/192], loss=68.9837
	step [24/192], loss=67.1162
	step [25/192], loss=57.4011
	step [26/192], loss=67.0876
	step [27/192], loss=47.4571
	step [28/192], loss=64.7794
	step [29/192], loss=59.4059
	step [30/192], loss=59.9905
	step [31/192], loss=69.0080
	step [32/192], loss=69.1673
	step [33/192], loss=57.1600
	step [34/192], loss=65.8183
	step [35/192], loss=62.2292
	step [36/192], loss=60.9576
	step [37/192], loss=65.0059
	step [38/192], loss=65.9863
	step [39/192], loss=67.5428
	step [40/192], loss=63.1896
	step [41/192], loss=60.9522
	step [42/192], loss=56.2996
	step [43/192], loss=67.5259
	step [44/192], loss=69.7877
	step [45/192], loss=67.3532
	step [46/192], loss=62.2157
	step [47/192], loss=64.7897
	step [48/192], loss=62.0792
	step [49/192], loss=68.2186
	step [50/192], loss=52.3179
	step [51/192], loss=49.9497
	step [52/192], loss=53.9752
	step [53/192], loss=64.7703
	step [54/192], loss=64.7767
	step [55/192], loss=63.2675
	step [56/192], loss=59.3908
	step [57/192], loss=63.3315
	step [58/192], loss=65.2372
	step [59/192], loss=67.6827
	step [60/192], loss=60.5395
	step [61/192], loss=73.3989
	step [62/192], loss=61.5240
	step [63/192], loss=61.4200
	step [64/192], loss=69.7522
	step [65/192], loss=62.1093
	step [66/192], loss=64.5917
	step [67/192], loss=62.7639
	step [68/192], loss=62.1618
	step [69/192], loss=64.5693
	step [70/192], loss=63.3192
	step [71/192], loss=60.0850
	step [72/192], loss=54.8147
	step [73/192], loss=55.4078
	step [74/192], loss=64.6007
	step [75/192], loss=65.6534
	step [76/192], loss=57.7285
	step [77/192], loss=61.1891
	step [78/192], loss=61.6905
	step [79/192], loss=67.3215
	step [80/192], loss=62.1446
	step [81/192], loss=55.9794
	step [82/192], loss=54.6788
	step [83/192], loss=70.9503
	step [84/192], loss=64.1918
	step [85/192], loss=52.4880
	step [86/192], loss=60.9795
	step [87/192], loss=71.4948
	step [88/192], loss=51.1235
	step [89/192], loss=72.6236
	step [90/192], loss=61.8666
	step [91/192], loss=58.5138
	step [92/192], loss=57.8942
	step [93/192], loss=53.5444
	step [94/192], loss=66.6924
	step [95/192], loss=69.3865
	step [96/192], loss=62.6371
	step [97/192], loss=58.6661
	step [98/192], loss=71.8551
	step [99/192], loss=67.0853
	step [100/192], loss=64.9976
	step [101/192], loss=64.5230
	step [102/192], loss=56.8440
	step [103/192], loss=52.4871
	step [104/192], loss=63.9111
	step [105/192], loss=56.0611
	step [106/192], loss=72.0895
	step [107/192], loss=70.9926
	step [108/192], loss=59.5824
	step [109/192], loss=74.1294
	step [110/192], loss=73.9603
	step [111/192], loss=61.5216
	step [112/192], loss=62.9607
	step [113/192], loss=61.7941
	step [114/192], loss=89.7531
	step [115/192], loss=67.4779
	step [116/192], loss=57.3794
	step [117/192], loss=61.1548
	step [118/192], loss=63.6643
	step [119/192], loss=54.2073
	step [120/192], loss=64.5146
	step [121/192], loss=61.6017
	step [122/192], loss=57.1274
	step [123/192], loss=61.2996
	step [124/192], loss=72.4039
	step [125/192], loss=55.6383
	step [126/192], loss=68.7283
	step [127/192], loss=64.9186
	step [128/192], loss=57.8713
	step [129/192], loss=57.7446
	step [130/192], loss=58.8118
	step [131/192], loss=57.3430
	step [132/192], loss=56.9415
	step [133/192], loss=73.9285
	step [134/192], loss=61.2070
	step [135/192], loss=49.6255
	step [136/192], loss=56.6886
	step [137/192], loss=62.9214
	step [138/192], loss=72.1814
	step [139/192], loss=59.8535
	step [140/192], loss=70.2787
	step [141/192], loss=68.7343
	step [142/192], loss=61.9485
	step [143/192], loss=59.5026
	step [144/192], loss=62.9135
	step [145/192], loss=64.3719
	step [146/192], loss=57.7432
	step [147/192], loss=62.8730
	step [148/192], loss=68.4317
	step [149/192], loss=76.5355
	step [150/192], loss=65.3525
	step [151/192], loss=64.1935
	step [152/192], loss=55.7778
	step [153/192], loss=71.3534
	step [154/192], loss=60.1877
	step [155/192], loss=55.2078
	step [156/192], loss=64.9385
	step [157/192], loss=52.1285
	step [158/192], loss=67.0724
	step [159/192], loss=71.9491
	step [160/192], loss=53.5937
	step [161/192], loss=62.6342
	step [162/192], loss=60.9854
	step [163/192], loss=76.7177
	step [164/192], loss=55.2371
	step [165/192], loss=54.1066
	step [166/192], loss=58.1304
	step [167/192], loss=59.9648
	step [168/192], loss=61.4634
	step [169/192], loss=60.4887
	step [170/192], loss=62.4580
	step [171/192], loss=54.6250
	step [172/192], loss=65.9859
	step [173/192], loss=76.9677
	step [174/192], loss=69.0488
	step [175/192], loss=68.6268
	step [176/192], loss=61.0606
	step [177/192], loss=63.2523
	step [178/192], loss=68.0150
	step [179/192], loss=70.9332
	step [180/192], loss=56.9866
	step [181/192], loss=58.9207
	step [182/192], loss=64.6064
	step [183/192], loss=58.1563
	step [184/192], loss=46.2524
	step [185/192], loss=60.7324
	step [186/192], loss=54.4905
	step [187/192], loss=64.0831
	step [188/192], loss=53.1191
	step [189/192], loss=69.4421
	step [190/192], loss=55.7287
	step [191/192], loss=63.7024
	step [192/192], loss=50.0256
	Evaluating
	loss=0.0063, precision=0.3807, recall=0.8633, f1=0.5284
Training epoch 147
	step [1/192], loss=69.7691
	step [2/192], loss=72.0797
	step [3/192], loss=61.1907
	step [4/192], loss=62.5104
	step [5/192], loss=67.2859
	step [6/192], loss=57.8647
	step [7/192], loss=60.5616
	step [8/192], loss=71.4714
	step [9/192], loss=59.9928
	step [10/192], loss=68.2299
	step [11/192], loss=65.9770
	step [12/192], loss=65.4797
	step [13/192], loss=67.9885
	step [14/192], loss=69.6510
	step [15/192], loss=64.0646
	step [16/192], loss=67.7028
	step [17/192], loss=67.3024
	step [18/192], loss=52.6204
	step [19/192], loss=60.2946
	step [20/192], loss=81.2753
	step [21/192], loss=66.7612
	step [22/192], loss=54.4148
	step [23/192], loss=53.9539
	step [24/192], loss=66.5532
	step [25/192], loss=63.1776
	step [26/192], loss=58.5535
	step [27/192], loss=62.0044
	step [28/192], loss=53.3810
	step [29/192], loss=66.4009
	step [30/192], loss=67.8416
	step [31/192], loss=59.2706
	step [32/192], loss=59.4542
	step [33/192], loss=70.1859
	step [34/192], loss=57.1968
	step [35/192], loss=58.7173
	step [36/192], loss=75.1102
	step [37/192], loss=51.6104
	step [38/192], loss=58.9656
	step [39/192], loss=67.0752
	step [40/192], loss=63.5108
	step [41/192], loss=63.5523
	step [42/192], loss=56.1959
	step [43/192], loss=78.1403
	step [44/192], loss=48.1697
	step [45/192], loss=58.9991
	step [46/192], loss=79.0736
	step [47/192], loss=65.6360
	step [48/192], loss=63.3332
	step [49/192], loss=63.7808
	step [50/192], loss=61.0648
	step [51/192], loss=55.3817
	step [52/192], loss=66.4086
	step [53/192], loss=61.3878
	step [54/192], loss=44.2563
	step [55/192], loss=58.9422
	step [56/192], loss=54.7078
	step [57/192], loss=61.8152
	step [58/192], loss=67.4121
	step [59/192], loss=62.9880
	step [60/192], loss=59.5204
	step [61/192], loss=63.0497
	step [62/192], loss=65.9302
	step [63/192], loss=69.9355
	step [64/192], loss=66.7595
	step [65/192], loss=67.2120
	step [66/192], loss=57.1338
	step [67/192], loss=56.7644
	step [68/192], loss=77.2768
	step [69/192], loss=56.5380
	step [70/192], loss=57.2811
	step [71/192], loss=53.5184
	step [72/192], loss=54.2786
	step [73/192], loss=65.6773
	step [74/192], loss=63.9671
	step [75/192], loss=63.5429
	step [76/192], loss=63.7034
	step [77/192], loss=59.1567
	step [78/192], loss=62.8881
	step [79/192], loss=68.0203
	step [80/192], loss=75.3920
	step [81/192], loss=61.1190
	step [82/192], loss=63.7684
	step [83/192], loss=63.0883
	step [84/192], loss=54.3376
	step [85/192], loss=70.1619
	step [86/192], loss=59.5430
	step [87/192], loss=67.0091
	step [88/192], loss=65.3231
	step [89/192], loss=52.1653
	step [90/192], loss=52.7949
	step [91/192], loss=53.5113
	step [92/192], loss=61.1936
	step [93/192], loss=60.6152
	step [94/192], loss=56.5918
	step [95/192], loss=58.1364
	step [96/192], loss=74.3015
	step [97/192], loss=65.6372
	step [98/192], loss=59.5216
	step [99/192], loss=64.0836
	step [100/192], loss=60.7801
	step [101/192], loss=67.4440
	step [102/192], loss=50.2993
	step [103/192], loss=57.2299
	step [104/192], loss=67.9880
	step [105/192], loss=71.5013
	step [106/192], loss=64.6147
	step [107/192], loss=66.3405
	step [108/192], loss=49.8051
	step [109/192], loss=70.6472
	step [110/192], loss=53.8486
	step [111/192], loss=54.6465
	step [112/192], loss=68.5826
	step [113/192], loss=59.3294
	step [114/192], loss=64.3687
	step [115/192], loss=58.4506
	step [116/192], loss=66.1903
	step [117/192], loss=64.8646
	step [118/192], loss=71.5833
	step [119/192], loss=71.1906
	step [120/192], loss=67.5366
	step [121/192], loss=63.1874
	step [122/192], loss=67.1649
	step [123/192], loss=69.8311
	step [124/192], loss=60.4529
	step [125/192], loss=62.9253
	step [126/192], loss=71.5302
	step [127/192], loss=63.7764
	step [128/192], loss=72.7988
	step [129/192], loss=49.7104
	step [130/192], loss=63.7428
	step [131/192], loss=58.9133
	step [132/192], loss=56.5650
	step [133/192], loss=61.6178
	step [134/192], loss=53.8498
	step [135/192], loss=55.0374
	step [136/192], loss=54.0652
	step [137/192], loss=60.2794
	step [138/192], loss=65.1958
	step [139/192], loss=58.0412
	step [140/192], loss=76.8641
	step [141/192], loss=67.0810
	step [142/192], loss=73.0953
	step [143/192], loss=61.7457
	step [144/192], loss=60.6040
	step [145/192], loss=64.7877
	step [146/192], loss=53.4967
	step [147/192], loss=60.9506
	step [148/192], loss=60.2887
	step [149/192], loss=67.6123
	step [150/192], loss=63.8453
	step [151/192], loss=74.6337
	step [152/192], loss=63.6883
	step [153/192], loss=60.6976
	step [154/192], loss=62.3486
	step [155/192], loss=69.0319
	step [156/192], loss=58.6271
	step [157/192], loss=63.1139
	step [158/192], loss=52.4476
	step [159/192], loss=57.1325
	step [160/192], loss=59.0692
	step [161/192], loss=62.5045
	step [162/192], loss=63.2698
	step [163/192], loss=65.4612
	step [164/192], loss=67.0826
	step [165/192], loss=66.0938
	step [166/192], loss=63.7461
	step [167/192], loss=71.4201
	step [168/192], loss=57.3791
	step [169/192], loss=72.8916
	step [170/192], loss=61.9499
	step [171/192], loss=61.8514
	step [172/192], loss=51.2026
	step [173/192], loss=50.8980
	step [174/192], loss=68.9985
	step [175/192], loss=72.7045
	step [176/192], loss=58.4090
	step [177/192], loss=55.5473
	step [178/192], loss=59.7960
	step [179/192], loss=61.3924
	step [180/192], loss=57.6992
	step [181/192], loss=66.6573
	step [182/192], loss=72.4230
	step [183/192], loss=63.5840
	step [184/192], loss=59.9368
	step [185/192], loss=51.3203
	step [186/192], loss=53.8135
	step [187/192], loss=56.4611
	step [188/192], loss=54.4576
	step [189/192], loss=67.8878
	step [190/192], loss=57.8506
	step [191/192], loss=70.1427
	step [192/192], loss=56.7114
	Evaluating
	loss=0.0054, precision=0.4333, recall=0.8609, f1=0.5765
Training epoch 148
	step [1/192], loss=59.5963
	step [2/192], loss=64.8815
	step [3/192], loss=66.6766
	step [4/192], loss=65.9115
	step [5/192], loss=64.9281
	step [6/192], loss=69.7333
	step [7/192], loss=61.3815
	step [8/192], loss=62.4852
	step [9/192], loss=62.4195
	step [10/192], loss=62.6773
	step [11/192], loss=59.3379
	step [12/192], loss=63.2008
	step [13/192], loss=67.1893
	step [14/192], loss=57.7992
	step [15/192], loss=64.7124
	step [16/192], loss=71.6588
	step [17/192], loss=62.1979
	step [18/192], loss=72.5287
	step [19/192], loss=56.6583
	step [20/192], loss=78.5108
	step [21/192], loss=63.9031
	step [22/192], loss=56.8126
	step [23/192], loss=55.1517
	step [24/192], loss=65.9684
	step [25/192], loss=63.8180
	step [26/192], loss=74.3511
	step [27/192], loss=57.3331
	step [28/192], loss=57.5017
	step [29/192], loss=66.9585
	step [30/192], loss=58.3679
	step [31/192], loss=47.5228
	step [32/192], loss=60.7793
	step [33/192], loss=59.4529
	step [34/192], loss=53.3859
	step [35/192], loss=64.1493
	step [36/192], loss=58.6780
	step [37/192], loss=61.6598
	step [38/192], loss=58.4550
	step [39/192], loss=64.5775
	step [40/192], loss=69.4381
	step [41/192], loss=59.8716
	step [42/192], loss=62.7368
	step [43/192], loss=79.9215
	step [44/192], loss=56.0698
	step [45/192], loss=63.3066
	step [46/192], loss=53.3539
	step [47/192], loss=61.3782
	step [48/192], loss=70.5037
	step [49/192], loss=52.8543
	step [50/192], loss=72.3003
	step [51/192], loss=67.4728
	step [52/192], loss=58.7257
	step [53/192], loss=54.3435
	step [54/192], loss=59.2315
	step [55/192], loss=58.1289
	step [56/192], loss=64.1387
	step [57/192], loss=54.4955
	step [58/192], loss=57.0817
	step [59/192], loss=57.0477
	step [60/192], loss=58.3655
	step [61/192], loss=65.9319
	step [62/192], loss=66.4498
	step [63/192], loss=68.1887
	step [64/192], loss=62.5272
	step [65/192], loss=56.4672
	step [66/192], loss=59.9836
	step [67/192], loss=68.7970
	step [68/192], loss=65.0089
	step [69/192], loss=54.2452
	step [70/192], loss=72.6672
	step [71/192], loss=58.8440
	step [72/192], loss=63.3885
	step [73/192], loss=58.9785
	step [74/192], loss=72.7895
	step [75/192], loss=61.5046
	step [76/192], loss=60.1191
	step [77/192], loss=63.7285
	step [78/192], loss=65.3520
	step [79/192], loss=61.5784
	step [80/192], loss=59.6563
	step [81/192], loss=52.4928
	step [82/192], loss=51.3152
	step [83/192], loss=65.0022
	step [84/192], loss=61.4886
	step [85/192], loss=78.3550
	step [86/192], loss=57.1278
	step [87/192], loss=63.8538
	step [88/192], loss=62.5506
	step [89/192], loss=61.6708
	step [90/192], loss=66.3386
	step [91/192], loss=57.6552
	step [92/192], loss=55.4178
	step [93/192], loss=58.9022
	step [94/192], loss=65.9200
	step [95/192], loss=62.5014
	step [96/192], loss=59.6329
	step [97/192], loss=68.4136
	step [98/192], loss=60.8688
	step [99/192], loss=56.0645
	step [100/192], loss=61.5015
	step [101/192], loss=57.5973
	step [102/192], loss=59.4746
	step [103/192], loss=61.2374
	step [104/192], loss=67.5332
	step [105/192], loss=66.3212
	step [106/192], loss=60.9644
	step [107/192], loss=68.0639
	step [108/192], loss=64.3916
	step [109/192], loss=54.3441
	step [110/192], loss=62.4497
	step [111/192], loss=67.1896
	step [112/192], loss=60.8702
	step [113/192], loss=54.3624
	step [114/192], loss=74.8152
	step [115/192], loss=67.0867
	step [116/192], loss=72.6871
	step [117/192], loss=60.5581
	step [118/192], loss=60.5543
	step [119/192], loss=75.1187
	step [120/192], loss=69.3409
	step [121/192], loss=69.4721
	step [122/192], loss=63.3967
	step [123/192], loss=61.4813
	step [124/192], loss=51.6933
	step [125/192], loss=63.9552
	step [126/192], loss=60.9542
	step [127/192], loss=71.0483
	step [128/192], loss=62.4692
	step [129/192], loss=62.0111
	step [130/192], loss=55.7840
	step [131/192], loss=51.2255
	step [132/192], loss=64.0709
	step [133/192], loss=66.5460
	step [134/192], loss=46.3712
	step [135/192], loss=57.9271
	step [136/192], loss=61.3087
	step [137/192], loss=67.8602
	step [138/192], loss=65.5185
	step [139/192], loss=56.9527
	step [140/192], loss=68.4912
	step [141/192], loss=51.6169
	step [142/192], loss=61.6355
	step [143/192], loss=68.5688
	step [144/192], loss=63.0830
	step [145/192], loss=59.1034
	step [146/192], loss=56.9207
	step [147/192], loss=60.2486
	step [148/192], loss=61.3681
	step [149/192], loss=61.5197
	step [150/192], loss=58.8178
	step [151/192], loss=58.0144
	step [152/192], loss=63.7494
	step [153/192], loss=57.6912
	step [154/192], loss=56.5031
	step [155/192], loss=57.9876
	step [156/192], loss=67.0126
	step [157/192], loss=51.0358
	step [158/192], loss=54.7918
	step [159/192], loss=61.7831
	step [160/192], loss=55.5935
	step [161/192], loss=58.7127
	step [162/192], loss=56.4087
	step [163/192], loss=44.7540
	step [164/192], loss=71.8419
	step [165/192], loss=67.0785
	step [166/192], loss=70.0655
	step [167/192], loss=65.7042
	step [168/192], loss=51.5597
	step [169/192], loss=68.1374
	step [170/192], loss=77.6577
	step [171/192], loss=55.8352
	step [172/192], loss=64.0733
	step [173/192], loss=63.1470
	step [174/192], loss=68.1016
	step [175/192], loss=59.0782
	step [176/192], loss=75.9168
	step [177/192], loss=70.3523
	step [178/192], loss=64.9791
	step [179/192], loss=68.6170
	step [180/192], loss=62.7271
	step [181/192], loss=65.4304
	step [182/192], loss=64.1995
	step [183/192], loss=68.5042
	step [184/192], loss=60.3527
	step [185/192], loss=60.3670
	step [186/192], loss=70.0445
	step [187/192], loss=64.6226
	step [188/192], loss=74.6172
	step [189/192], loss=73.7121
	step [190/192], loss=66.8169
	step [191/192], loss=55.8606
	step [192/192], loss=66.4761
	Evaluating
	loss=0.0060, precision=0.3969, recall=0.8645, f1=0.5441
Training epoch 149
	step [1/192], loss=64.6497
	step [2/192], loss=63.2035
	step [3/192], loss=62.1567
	step [4/192], loss=72.0875
	step [5/192], loss=68.3186
	step [6/192], loss=61.6650
	step [7/192], loss=60.5188
	step [8/192], loss=66.6409
	step [9/192], loss=62.9324
	step [10/192], loss=63.4645
	step [11/192], loss=67.7042
	step [12/192], loss=69.8403
	step [13/192], loss=68.7179
	step [14/192], loss=59.3746
	step [15/192], loss=77.4353
	step [16/192], loss=64.3235
	step [17/192], loss=48.1652
	step [18/192], loss=61.1871
	step [19/192], loss=57.6871
	step [20/192], loss=59.0282
	step [21/192], loss=68.6923
	step [22/192], loss=69.2879
	step [23/192], loss=76.9714
	step [24/192], loss=54.4197
	step [25/192], loss=67.5835
	step [26/192], loss=67.5637
	step [27/192], loss=55.4829
	step [28/192], loss=57.9742
	step [29/192], loss=57.4991
	step [30/192], loss=55.4110
	step [31/192], loss=53.5446
	step [32/192], loss=61.7646
	step [33/192], loss=64.0768
	step [34/192], loss=62.3711
	step [35/192], loss=65.0853
	step [36/192], loss=59.2806
	step [37/192], loss=63.9060
	step [38/192], loss=56.7726
	step [39/192], loss=57.5032
	step [40/192], loss=57.8581
	step [41/192], loss=62.5892
	step [42/192], loss=57.1236
	step [43/192], loss=68.6714
	step [44/192], loss=67.1732
	step [45/192], loss=60.4515
	step [46/192], loss=59.0320
	step [47/192], loss=52.3141
	step [48/192], loss=63.4779
	step [49/192], loss=65.4539
	step [50/192], loss=64.6783
	step [51/192], loss=67.1760
	step [52/192], loss=65.5371
	step [53/192], loss=54.9410
	step [54/192], loss=53.3638
	step [55/192], loss=57.7599
	step [56/192], loss=63.2596
	step [57/192], loss=71.6783
	step [58/192], loss=65.6406
	step [59/192], loss=56.2558
	step [60/192], loss=62.6419
	step [61/192], loss=53.5986
	step [62/192], loss=64.0919
	step [63/192], loss=62.0649
	step [64/192], loss=65.1611
	step [65/192], loss=69.2553
	step [66/192], loss=60.8300
	step [67/192], loss=66.0265
	step [68/192], loss=58.2247
	step [69/192], loss=62.6778
	step [70/192], loss=65.0685
	step [71/192], loss=54.6435
	step [72/192], loss=63.6728
	step [73/192], loss=61.2957
	step [74/192], loss=56.8069
	step [75/192], loss=66.2489
	step [76/192], loss=63.3267
	step [77/192], loss=49.4210
	step [78/192], loss=60.8434
	step [79/192], loss=51.9301
	step [80/192], loss=47.5148
	step [81/192], loss=64.1387
	step [82/192], loss=70.6105
	step [83/192], loss=60.9022
	step [84/192], loss=63.6149
	step [85/192], loss=69.2317
	step [86/192], loss=68.8819
	step [87/192], loss=54.5396
	step [88/192], loss=50.2777
	step [89/192], loss=66.2550
	step [90/192], loss=63.1096
	step [91/192], loss=60.5693
	step [92/192], loss=62.2121
	step [93/192], loss=57.6823
	step [94/192], loss=61.8906
	step [95/192], loss=62.8917
	step [96/192], loss=63.0063
	step [97/192], loss=60.1663
	step [98/192], loss=64.5338
	step [99/192], loss=61.2985
	step [100/192], loss=71.1973
	step [101/192], loss=52.5744
	step [102/192], loss=57.6249
	step [103/192], loss=77.2212
	step [104/192], loss=57.0550
	step [105/192], loss=62.3025
	step [106/192], loss=53.2552
	step [107/192], loss=60.6930
	step [108/192], loss=67.5274
	step [109/192], loss=70.1587
	step [110/192], loss=57.5396
	step [111/192], loss=52.0029
	step [112/192], loss=61.9868
	step [113/192], loss=64.9856
	step [114/192], loss=69.1894
	step [115/192], loss=80.9996
	step [116/192], loss=67.7273
	step [117/192], loss=58.6820
	step [118/192], loss=59.7527
	step [119/192], loss=66.8713
	step [120/192], loss=62.0573
	step [121/192], loss=58.2973
	step [122/192], loss=69.6725
	step [123/192], loss=73.0598
	step [124/192], loss=71.6198
	step [125/192], loss=60.9297
	step [126/192], loss=53.2109
	step [127/192], loss=62.6655
	step [128/192], loss=64.2413
	step [129/192], loss=64.6674
	step [130/192], loss=73.7792
	step [131/192], loss=60.0823
	step [132/192], loss=58.5852
	step [133/192], loss=59.9943
	step [134/192], loss=63.4885
	step [135/192], loss=63.8369
	step [136/192], loss=55.6605
	step [137/192], loss=63.0991
	step [138/192], loss=62.4357
	step [139/192], loss=61.1727
	step [140/192], loss=67.7604
	step [141/192], loss=69.0894
	step [142/192], loss=62.5557
	step [143/192], loss=61.0564
	step [144/192], loss=60.8854
	step [145/192], loss=70.8034
	step [146/192], loss=55.2817
	step [147/192], loss=69.4801
	step [148/192], loss=63.8433
	step [149/192], loss=64.5416
	step [150/192], loss=55.3739
	step [151/192], loss=60.2529
	step [152/192], loss=61.3442
	step [153/192], loss=59.6121
	step [154/192], loss=55.7104
	step [155/192], loss=73.1742
	step [156/192], loss=66.6224
	step [157/192], loss=67.7722
	step [158/192], loss=65.9790
	step [159/192], loss=58.0087
	step [160/192], loss=62.8350
	step [161/192], loss=50.4706
	step [162/192], loss=61.8512
	step [163/192], loss=70.8164
	step [164/192], loss=61.1909
	step [165/192], loss=65.1885
	step [166/192], loss=70.4404
	step [167/192], loss=64.9543
	step [168/192], loss=61.5610
	step [169/192], loss=61.3268
	step [170/192], loss=68.0270
	step [171/192], loss=59.8668
	step [172/192], loss=65.9140
	step [173/192], loss=64.9794
	step [174/192], loss=60.4512
	step [175/192], loss=61.1259
	step [176/192], loss=61.6206
	step [177/192], loss=63.4651
	step [178/192], loss=56.6416
	step [179/192], loss=56.2895
	step [180/192], loss=69.1179
	step [181/192], loss=49.8416
	step [182/192], loss=57.8692
	step [183/192], loss=63.9015
	step [184/192], loss=66.4559
	step [185/192], loss=58.3163
	step [186/192], loss=57.1816
	step [187/192], loss=61.8765
	step [188/192], loss=62.7264
	step [189/192], loss=58.6403
	step [190/192], loss=58.1028
	step [191/192], loss=65.4613
	step [192/192], loss=54.0230
	Evaluating
	loss=0.0059, precision=0.3860, recall=0.8597, f1=0.5327
Training epoch 150
	step [1/192], loss=64.8077
	step [2/192], loss=61.2981
	step [3/192], loss=64.9975
	step [4/192], loss=59.1742
	step [5/192], loss=74.2765
	step [6/192], loss=72.9418
	step [7/192], loss=71.9481
	step [8/192], loss=70.0379
	step [9/192], loss=75.4974
	step [10/192], loss=61.6191
	step [11/192], loss=66.2122
	step [12/192], loss=57.2148
	step [13/192], loss=56.3685
	step [14/192], loss=62.6927
	step [15/192], loss=66.7696
	step [16/192], loss=62.0042
	step [17/192], loss=64.4356
	step [18/192], loss=53.7392
	step [19/192], loss=67.5295
	step [20/192], loss=56.5450
	step [21/192], loss=74.1811
	step [22/192], loss=66.9126
	step [23/192], loss=58.7957
	step [24/192], loss=74.7370
	step [25/192], loss=59.6752
	step [26/192], loss=67.8050
	step [27/192], loss=58.8686
	step [28/192], loss=53.6358
	step [29/192], loss=65.2322
	step [30/192], loss=59.9279
	step [31/192], loss=57.3584
	step [32/192], loss=61.9207
	step [33/192], loss=58.5048
	step [34/192], loss=53.2021
	step [35/192], loss=65.8867
	step [36/192], loss=52.9329
	step [37/192], loss=61.3029
	step [38/192], loss=68.5474
	step [39/192], loss=62.2611
	step [40/192], loss=58.5014
	step [41/192], loss=61.7771
	step [42/192], loss=63.5740
	step [43/192], loss=55.3039
	step [44/192], loss=64.5331
	step [45/192], loss=60.6683
	step [46/192], loss=66.1102
	step [47/192], loss=62.4212
	step [48/192], loss=59.3321
	step [49/192], loss=71.3605
	step [50/192], loss=61.4306
	step [51/192], loss=67.0471
	step [52/192], loss=56.4082
	step [53/192], loss=56.7374
	step [54/192], loss=61.4642
	step [55/192], loss=69.3654
	step [56/192], loss=66.4659
	step [57/192], loss=67.7661
	step [58/192], loss=61.3511
	step [59/192], loss=56.5444
	step [60/192], loss=66.4379
	step [61/192], loss=57.4890
	step [62/192], loss=60.0574
	step [63/192], loss=69.5030
	step [64/192], loss=71.8397
	step [65/192], loss=65.0586
	step [66/192], loss=57.0173
	step [67/192], loss=52.6423
	step [68/192], loss=69.3028
	step [69/192], loss=62.9311
	step [70/192], loss=71.6463
	step [71/192], loss=59.5962
	step [72/192], loss=59.1113
	step [73/192], loss=62.6623
	step [74/192], loss=55.8757
	step [75/192], loss=58.6734
	step [76/192], loss=58.4572
	step [77/192], loss=65.1158
	step [78/192], loss=70.7175
	step [79/192], loss=57.4740
	step [80/192], loss=68.0185
	step [81/192], loss=48.0863
	step [82/192], loss=82.0182
	step [83/192], loss=61.8120
	step [84/192], loss=66.9361
	step [85/192], loss=71.8271
	step [86/192], loss=54.2375
	step [87/192], loss=75.9430
	step [88/192], loss=55.5433
	step [89/192], loss=55.9443
	step [90/192], loss=54.4674
	step [91/192], loss=53.5528
	step [92/192], loss=65.8600
	step [93/192], loss=71.6875
	step [94/192], loss=69.1258
	step [95/192], loss=66.3895
	step [96/192], loss=66.7576
	step [97/192], loss=68.3001
	step [98/192], loss=64.1868
	step [99/192], loss=58.0748
	step [100/192], loss=55.3673
	step [101/192], loss=63.1112
	step [102/192], loss=73.3947
	step [103/192], loss=55.7602
	step [104/192], loss=69.4777
	step [105/192], loss=65.1951
	step [106/192], loss=52.0622
	step [107/192], loss=67.6907
	step [108/192], loss=56.6946
	step [109/192], loss=64.3381
	step [110/192], loss=56.2964
	step [111/192], loss=60.0297
	step [112/192], loss=56.6753
	step [113/192], loss=61.7546
	step [114/192], loss=73.4085
	step [115/192], loss=54.7815
	step [116/192], loss=71.5943
	step [117/192], loss=55.5461
	step [118/192], loss=53.8189
	step [119/192], loss=49.4628
	step [120/192], loss=61.7030
	step [121/192], loss=56.9972
	step [122/192], loss=64.6441
	step [123/192], loss=66.1961
	step [124/192], loss=69.8727
	step [125/192], loss=72.8260
	step [126/192], loss=64.4828
	step [127/192], loss=66.4077
	step [128/192], loss=58.7124
	step [129/192], loss=75.7790
	step [130/192], loss=57.7982
	step [131/192], loss=57.3169
	step [132/192], loss=81.4969
	step [133/192], loss=59.6702
	step [134/192], loss=56.1830
	step [135/192], loss=65.7664
	step [136/192], loss=60.6557
	step [137/192], loss=64.4997
	step [138/192], loss=58.4021
	step [139/192], loss=55.4775
	step [140/192], loss=64.3613
	step [141/192], loss=55.5713
	step [142/192], loss=51.2496
	step [143/192], loss=67.8143
	step [144/192], loss=50.6334
	step [145/192], loss=67.3469
	step [146/192], loss=61.1291
	step [147/192], loss=68.2910
	step [148/192], loss=68.6246
	step [149/192], loss=57.1173
	step [150/192], loss=63.3133
	step [151/192], loss=62.8046
	step [152/192], loss=50.3269
	step [153/192], loss=56.0801
	step [154/192], loss=53.7541
	step [155/192], loss=70.2732
	step [156/192], loss=57.7103
	step [157/192], loss=60.2595
	step [158/192], loss=63.7797
	step [159/192], loss=57.4718
	step [160/192], loss=67.6858
	step [161/192], loss=54.7379
	step [162/192], loss=56.1656
	step [163/192], loss=49.8957
	step [164/192], loss=64.6748
	step [165/192], loss=68.0626
	step [166/192], loss=62.1067
	step [167/192], loss=63.3236
	step [168/192], loss=54.2326
	step [169/192], loss=60.9518
	step [170/192], loss=65.3571
	step [171/192], loss=52.0677
	step [172/192], loss=60.2090
	step [173/192], loss=56.8845
	step [174/192], loss=63.1331
	step [175/192], loss=62.8111
	step [176/192], loss=55.4306
	step [177/192], loss=55.9593
	step [178/192], loss=63.0749
	step [179/192], loss=70.4706
	step [180/192], loss=53.0248
	step [181/192], loss=53.6731
	step [182/192], loss=60.6996
	step [183/192], loss=55.9794
	step [184/192], loss=62.0196
	step [185/192], loss=71.6893
	step [186/192], loss=62.6373
	step [187/192], loss=57.6638
	step [188/192], loss=55.9379
	step [189/192], loss=64.4854
	step [190/192], loss=60.0167
	step [191/192], loss=62.7972
	step [192/192], loss=54.9151
	Evaluating
	loss=0.0059, precision=0.4167, recall=0.8677, f1=0.5630
Training finished
best_f1: 0.578524688245307
directing: Z rim_enhanced: True test_id 1
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12192 # image files with weight 12192
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3352 # image files with weight 3352
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12192
Using 4 GPUs
Going to train epochs [65-114]
Training epoch 65
	step [1/191], loss=87.2648
	step [2/191], loss=65.1090
	step [3/191], loss=71.4607
	step [4/191], loss=54.3010
	step [5/191], loss=83.6166
	step [6/191], loss=72.0334
	step [7/191], loss=61.7104
	step [8/191], loss=61.6325
	step [9/191], loss=62.0184
	step [10/191], loss=73.3688
	step [11/191], loss=75.7906
	step [12/191], loss=65.7845
	step [13/191], loss=59.0734
	step [14/191], loss=60.4306
	step [15/191], loss=78.3665
	step [16/191], loss=82.7735
	step [17/191], loss=55.5860
	step [18/191], loss=67.2122
	step [19/191], loss=69.7441
	step [20/191], loss=60.0055
	step [21/191], loss=62.1985
	step [22/191], loss=74.3092
	step [23/191], loss=73.0091
	step [24/191], loss=69.6653
	step [25/191], loss=73.9009
	step [26/191], loss=65.1179
	step [27/191], loss=67.3199
	step [28/191], loss=83.8235
	step [29/191], loss=71.3243
	step [30/191], loss=56.4748
	step [31/191], loss=83.4138
	step [32/191], loss=75.5292
	step [33/191], loss=73.4995
	step [34/191], loss=60.5932
	step [35/191], loss=82.0963
	step [36/191], loss=63.2330
	step [37/191], loss=86.6092
	step [38/191], loss=70.5102
	step [39/191], loss=57.6177
	step [40/191], loss=70.8371
	step [41/191], loss=67.7546
	step [42/191], loss=77.8109
	step [43/191], loss=72.5504
	step [44/191], loss=66.3528
	step [45/191], loss=72.9104
	step [46/191], loss=76.0305
	step [47/191], loss=63.0827
	step [48/191], loss=67.6043
	step [49/191], loss=74.9681
	step [50/191], loss=62.5618
	step [51/191], loss=62.5745
	step [52/191], loss=64.4753
	step [53/191], loss=64.5957
	step [54/191], loss=70.6073
	step [55/191], loss=81.0200
	step [56/191], loss=66.3972
	step [57/191], loss=60.3098
	step [58/191], loss=71.9269
	step [59/191], loss=71.3968
	step [60/191], loss=74.8029
	step [61/191], loss=64.2436
	step [62/191], loss=77.3898
	step [63/191], loss=59.9278
	step [64/191], loss=83.2258
	step [65/191], loss=68.9359
	step [66/191], loss=76.5084
	step [67/191], loss=77.9311
	step [68/191], loss=72.4894
	step [69/191], loss=59.0156
	step [70/191], loss=71.0861
	step [71/191], loss=72.9644
	step [72/191], loss=54.1095
	step [73/191], loss=65.0858
	step [74/191], loss=61.7477
	step [75/191], loss=67.5077
	step [76/191], loss=62.8126
	step [77/191], loss=61.9307
	step [78/191], loss=80.6757
	step [79/191], loss=75.1796
	step [80/191], loss=71.3045
	step [81/191], loss=70.6543
	step [82/191], loss=81.2160
	step [83/191], loss=76.1582
	step [84/191], loss=79.4589
	step [85/191], loss=75.6075
	step [86/191], loss=65.4634
	step [87/191], loss=77.7178
	step [88/191], loss=69.3757
	step [89/191], loss=75.1644
	step [90/191], loss=69.6002
	step [91/191], loss=67.1610
	step [92/191], loss=65.4116
	step [93/191], loss=64.3977
	step [94/191], loss=76.1964
	step [95/191], loss=76.1043
	step [96/191], loss=67.9546
	step [97/191], loss=70.1104
	step [98/191], loss=77.5594
	step [99/191], loss=84.0347
	step [100/191], loss=77.4348
	step [101/191], loss=70.5259
	step [102/191], loss=80.4060
	step [103/191], loss=86.3382
	step [104/191], loss=67.8496
	step [105/191], loss=78.6479
	step [106/191], loss=75.5036
	step [107/191], loss=81.0660
	step [108/191], loss=88.7362
	step [109/191], loss=77.2727
	step [110/191], loss=67.6936
	step [111/191], loss=66.7027
	step [112/191], loss=64.3644
	step [113/191], loss=72.7285
	step [114/191], loss=79.9411
	step [115/191], loss=84.0271
	step [116/191], loss=80.0611
	step [117/191], loss=71.3768
	step [118/191], loss=66.9174
	step [119/191], loss=61.4150
	step [120/191], loss=63.9065
	step [121/191], loss=81.9200
	step [122/191], loss=79.4892
	step [123/191], loss=59.2925
	step [124/191], loss=69.2477
	step [125/191], loss=67.2948
	step [126/191], loss=70.8040
	step [127/191], loss=65.4619
	step [128/191], loss=69.5297
	step [129/191], loss=73.5045
	step [130/191], loss=67.5929
	step [131/191], loss=68.2186
	step [132/191], loss=68.1121
	step [133/191], loss=70.9858
	step [134/191], loss=81.4655
	step [135/191], loss=63.8574
	step [136/191], loss=74.4132
	step [137/191], loss=56.7752
	step [138/191], loss=73.4782
	step [139/191], loss=78.4063
	step [140/191], loss=70.2767
	step [141/191], loss=63.2631
	step [142/191], loss=74.0999
	step [143/191], loss=73.3242
	step [144/191], loss=68.3911
	step [145/191], loss=71.6497
	step [146/191], loss=67.4747
	step [147/191], loss=67.9256
	step [148/191], loss=66.4375
	step [149/191], loss=87.3724
	step [150/191], loss=62.7037
	step [151/191], loss=70.4735
	step [152/191], loss=64.9381
	step [153/191], loss=62.2627
	step [154/191], loss=65.5197
	step [155/191], loss=76.4173
	step [156/191], loss=78.8573
	step [157/191], loss=70.0072
	step [158/191], loss=63.3058
	step [159/191], loss=73.5421
	step [160/191], loss=74.2738
	step [161/191], loss=62.6191
	step [162/191], loss=74.1581
	step [163/191], loss=78.9685
	step [164/191], loss=75.7322
	step [165/191], loss=67.8652
	step [166/191], loss=80.6000
	step [167/191], loss=78.4487
	step [168/191], loss=80.8000
	step [169/191], loss=74.0948
	step [170/191], loss=64.3719
	step [171/191], loss=82.1340
	step [172/191], loss=69.5341
	step [173/191], loss=67.1668
	step [174/191], loss=64.5240
	step [175/191], loss=63.8776
	step [176/191], loss=80.6193
	step [177/191], loss=54.1527
	step [178/191], loss=69.4776
	step [179/191], loss=78.3959
	step [180/191], loss=58.1468
	step [181/191], loss=77.0692
	step [182/191], loss=63.0462
	step [183/191], loss=54.6024
	step [184/191], loss=72.4082
	step [185/191], loss=79.9958
	step [186/191], loss=81.4020
	step [187/191], loss=59.6780
	step [188/191], loss=79.1295
	step [189/191], loss=62.5721
	step [190/191], loss=59.3098
	step [191/191], loss=35.4602
	Evaluating
	loss=0.0088, precision=0.4133, recall=0.8495, f1=0.5560
saving model as: 1_saved_model.pth
Training epoch 66
	step [1/191], loss=75.2911
	step [2/191], loss=80.0345
	step [3/191], loss=74.5099
	step [4/191], loss=74.9529
	step [5/191], loss=64.9664
	step [6/191], loss=68.8811
	step [7/191], loss=57.4653
	step [8/191], loss=70.4158
	step [9/191], loss=53.7119
	step [10/191], loss=83.5128
	step [11/191], loss=68.2274
	step [12/191], loss=78.3083
	step [13/191], loss=64.3552
	step [14/191], loss=84.8263
	step [15/191], loss=71.2458
	step [16/191], loss=64.0949
	step [17/191], loss=75.4360
	step [18/191], loss=58.0239
	step [19/191], loss=68.7421
	step [20/191], loss=81.1616
	step [21/191], loss=55.7563
	step [22/191], loss=75.1494
	step [23/191], loss=57.4890
	step [24/191], loss=68.0915
	step [25/191], loss=66.1112
	step [26/191], loss=72.7619
	step [27/191], loss=66.1398
	step [28/191], loss=79.1118
	step [29/191], loss=65.5548
	step [30/191], loss=60.8928
	step [31/191], loss=74.1772
	step [32/191], loss=63.7100
	step [33/191], loss=65.7232
	step [34/191], loss=71.2389
	step [35/191], loss=74.9900
	step [36/191], loss=67.7246
	step [37/191], loss=81.8778
	step [38/191], loss=68.3947
	step [39/191], loss=60.9601
	step [40/191], loss=76.5712
	step [41/191], loss=65.6459
	step [42/191], loss=77.9845
	step [43/191], loss=68.5197
	step [44/191], loss=79.5159
	step [45/191], loss=80.2861
	step [46/191], loss=72.9711
	step [47/191], loss=67.0914
	step [48/191], loss=66.9519
	step [49/191], loss=78.7991
	step [50/191], loss=86.3754
	step [51/191], loss=78.5804
	step [52/191], loss=81.7350
	step [53/191], loss=75.1322
	step [54/191], loss=83.3768
	step [55/191], loss=76.4128
	step [56/191], loss=74.2938
	step [57/191], loss=81.8401
	step [58/191], loss=73.7238
	step [59/191], loss=62.9521
	step [60/191], loss=67.4993
	step [61/191], loss=59.6173
	step [62/191], loss=64.5866
	step [63/191], loss=72.2000
	step [64/191], loss=82.3559
	step [65/191], loss=72.4237
	step [66/191], loss=79.3722
	step [67/191], loss=68.9598
	step [68/191], loss=68.7127
	step [69/191], loss=60.9034
	step [70/191], loss=74.1826
	step [71/191], loss=71.5632
	step [72/191], loss=62.0537
	step [73/191], loss=71.7118
	step [74/191], loss=69.0134
	step [75/191], loss=82.4694
	step [76/191], loss=72.1185
	step [77/191], loss=79.2328
	step [78/191], loss=67.8320
	step [79/191], loss=69.6615
	step [80/191], loss=75.0614
	step [81/191], loss=69.1686
	step [82/191], loss=67.3981
	step [83/191], loss=71.7903
	step [84/191], loss=65.7054
	step [85/191], loss=67.5048
	step [86/191], loss=84.7094
	step [87/191], loss=53.3967
	step [88/191], loss=64.3265
	step [89/191], loss=61.4567
	step [90/191], loss=78.8760
	step [91/191], loss=75.8328
	step [92/191], loss=73.0938
	step [93/191], loss=62.9420
	step [94/191], loss=72.5733
	step [95/191], loss=74.4181
	step [96/191], loss=67.5152
	step [97/191], loss=75.9151
	step [98/191], loss=76.2645
	step [99/191], loss=52.6121
	step [100/191], loss=57.9882
	step [101/191], loss=68.4752
	step [102/191], loss=74.7964
	step [103/191], loss=74.7571
	step [104/191], loss=68.8253
	step [105/191], loss=71.5825
	step [106/191], loss=92.2338
	step [107/191], loss=69.3412
	step [108/191], loss=66.0963
	step [109/191], loss=69.4932
	step [110/191], loss=77.8814
	step [111/191], loss=85.8540
	step [112/191], loss=66.5872
	step [113/191], loss=83.8702
	step [114/191], loss=67.2616
	step [115/191], loss=62.1251
	step [116/191], loss=69.8002
	step [117/191], loss=55.9273
	step [118/191], loss=64.4711
	step [119/191], loss=73.3692
	step [120/191], loss=86.4462
	step [121/191], loss=60.1915
	step [122/191], loss=68.7508
	step [123/191], loss=72.7204
	step [124/191], loss=68.9486
	step [125/191], loss=62.0712
	step [126/191], loss=80.0178
	step [127/191], loss=81.8069
	step [128/191], loss=69.9106
	step [129/191], loss=72.3301
	step [130/191], loss=71.8726
	step [131/191], loss=63.0733
	step [132/191], loss=54.1936
	step [133/191], loss=77.5319
	step [134/191], loss=59.3806
	step [135/191], loss=64.8982
	step [136/191], loss=66.0994
	step [137/191], loss=66.8616
	step [138/191], loss=61.6697
	step [139/191], loss=67.1315
	step [140/191], loss=61.4727
	step [141/191], loss=68.2781
	step [142/191], loss=69.1594
	step [143/191], loss=80.1904
	step [144/191], loss=71.6014
	step [145/191], loss=70.4818
	step [146/191], loss=63.4461
	step [147/191], loss=78.3158
	step [148/191], loss=70.3923
	step [149/191], loss=81.3039
	step [150/191], loss=70.1160
	step [151/191], loss=71.0936
	step [152/191], loss=62.2128
	step [153/191], loss=62.8983
	step [154/191], loss=69.8897
	step [155/191], loss=67.9977
	step [156/191], loss=67.5348
	step [157/191], loss=73.1801
	step [158/191], loss=72.1999
	step [159/191], loss=66.9264
	step [160/191], loss=72.9137
	step [161/191], loss=59.6030
	step [162/191], loss=73.2993
	step [163/191], loss=57.0583
	step [164/191], loss=70.1407
	step [165/191], loss=77.0769
	step [166/191], loss=69.5686
	step [167/191], loss=85.7728
	step [168/191], loss=66.1429
	step [169/191], loss=68.2208
	step [170/191], loss=63.4370
	step [171/191], loss=77.9684
	step [172/191], loss=63.2960
	step [173/191], loss=70.4793
	step [174/191], loss=61.6910
	step [175/191], loss=73.9461
	step [176/191], loss=64.3692
	step [177/191], loss=85.1332
	step [178/191], loss=71.6788
	step [179/191], loss=79.2134
	step [180/191], loss=68.4855
	step [181/191], loss=63.1818
	step [182/191], loss=63.8816
	step [183/191], loss=73.3070
	step [184/191], loss=69.0201
	step [185/191], loss=64.3759
	step [186/191], loss=69.8556
	step [187/191], loss=67.5008
	step [188/191], loss=68.7790
	step [189/191], loss=62.0779
	step [190/191], loss=65.9318
	step [191/191], loss=37.8438
	Evaluating
	loss=0.0082, precision=0.4319, recall=0.8608, f1=0.5752
saving model as: 1_saved_model.pth
Training epoch 67
	step [1/191], loss=66.6682
	step [2/191], loss=84.2112
	step [3/191], loss=64.3078
	step [4/191], loss=72.5167
	step [5/191], loss=72.6726
	step [6/191], loss=70.1000
	step [7/191], loss=70.3173
	step [8/191], loss=67.8637
	step [9/191], loss=77.4200
	step [10/191], loss=71.0419
	step [11/191], loss=82.4481
	step [12/191], loss=76.8712
	step [13/191], loss=71.8622
	step [14/191], loss=83.5215
	step [15/191], loss=74.4536
	step [16/191], loss=74.4268
	step [17/191], loss=63.5386
	step [18/191], loss=82.1206
	step [19/191], loss=69.7972
	step [20/191], loss=72.8832
	step [21/191], loss=66.7264
	step [22/191], loss=58.8276
	step [23/191], loss=60.6677
	step [24/191], loss=75.5499
	step [25/191], loss=66.3650
	step [26/191], loss=70.1414
	step [27/191], loss=66.9444
	step [28/191], loss=68.5832
	step [29/191], loss=63.8566
	step [30/191], loss=60.7182
	step [31/191], loss=70.2045
	step [32/191], loss=66.8269
	step [33/191], loss=74.6517
	step [34/191], loss=75.3602
	step [35/191], loss=66.7993
	step [36/191], loss=68.8574
	step [37/191], loss=69.2668
	step [38/191], loss=87.4133
	step [39/191], loss=70.7104
	step [40/191], loss=76.2168
	step [41/191], loss=82.6425
	step [42/191], loss=74.2967
	step [43/191], loss=62.4617
	step [44/191], loss=72.0210
	step [45/191], loss=73.9796
	step [46/191], loss=71.0681
	step [47/191], loss=75.6007
	step [48/191], loss=64.1765
	step [49/191], loss=72.9263
	step [50/191], loss=66.0097
	step [51/191], loss=73.2230
	step [52/191], loss=62.5240
	step [53/191], loss=59.5481
	step [54/191], loss=81.9165
	step [55/191], loss=66.7775
	step [56/191], loss=61.1555
	step [57/191], loss=71.2687
	step [58/191], loss=69.1989
	step [59/191], loss=58.0493
	step [60/191], loss=69.9146
	step [61/191], loss=62.8234
	step [62/191], loss=63.3886
	step [63/191], loss=73.1436
	step [64/191], loss=70.7905
	step [65/191], loss=67.2807
	step [66/191], loss=71.5642
	step [67/191], loss=70.4281
	step [68/191], loss=68.4460
	step [69/191], loss=80.1697
	step [70/191], loss=75.6642
	step [71/191], loss=58.5981
	step [72/191], loss=62.5739
	step [73/191], loss=78.4311
	step [74/191], loss=83.0617
	step [75/191], loss=53.5231
	step [76/191], loss=67.5611
	step [77/191], loss=58.8140
	step [78/191], loss=71.1781
	step [79/191], loss=76.3424
	step [80/191], loss=69.6318
	step [81/191], loss=72.8209
	step [82/191], loss=82.0317
	step [83/191], loss=67.4726
	step [84/191], loss=59.1460
	step [85/191], loss=72.2046
	step [86/191], loss=69.6807
	step [87/191], loss=80.8080
	step [88/191], loss=70.1394
	step [89/191], loss=69.0521
	step [90/191], loss=62.8436
	step [91/191], loss=72.0385
	step [92/191], loss=70.9927
	step [93/191], loss=70.5678
	step [94/191], loss=67.2786
	step [95/191], loss=73.1794
	step [96/191], loss=64.7130
	step [97/191], loss=57.4304
	step [98/191], loss=82.0378
	step [99/191], loss=69.5780
	step [100/191], loss=85.4204
	step [101/191], loss=60.7744
	step [102/191], loss=72.8763
	step [103/191], loss=73.5809
	step [104/191], loss=58.3674
	step [105/191], loss=76.1953
	step [106/191], loss=71.6634
	step [107/191], loss=64.6595
	step [108/191], loss=71.8563
	step [109/191], loss=68.5918
	step [110/191], loss=59.2738
	step [111/191], loss=64.6504
	step [112/191], loss=69.1788
	step [113/191], loss=72.9089
	step [114/191], loss=68.7459
	step [115/191], loss=71.9007
	step [116/191], loss=76.3523
	step [117/191], loss=77.7565
	step [118/191], loss=73.5658
	step [119/191], loss=69.4181
	step [120/191], loss=70.8546
	step [121/191], loss=67.4230
	step [122/191], loss=63.2033
	step [123/191], loss=70.4165
	step [124/191], loss=71.3397
	step [125/191], loss=70.5199
	step [126/191], loss=71.6940
	step [127/191], loss=68.2645
	step [128/191], loss=84.3158
	step [129/191], loss=64.3506
	step [130/191], loss=60.5891
	step [131/191], loss=75.7068
	step [132/191], loss=66.6034
	step [133/191], loss=70.8827
	step [134/191], loss=74.5939
	step [135/191], loss=73.7570
	step [136/191], loss=83.1607
	step [137/191], loss=66.1803
	step [138/191], loss=66.6343
	step [139/191], loss=76.2672
	step [140/191], loss=65.0523
	step [141/191], loss=70.4625
	step [142/191], loss=58.7369
	step [143/191], loss=63.6147
	step [144/191], loss=67.9434
	step [145/191], loss=68.7469
	step [146/191], loss=75.6987
	step [147/191], loss=63.8112
	step [148/191], loss=67.4057
	step [149/191], loss=68.7254
	step [150/191], loss=61.3602
	step [151/191], loss=78.8006
	step [152/191], loss=77.9872
	step [153/191], loss=61.1034
	step [154/191], loss=77.3418
	step [155/191], loss=67.9340
	step [156/191], loss=74.1901
	step [157/191], loss=67.0280
	step [158/191], loss=75.7460
	step [159/191], loss=62.0978
	step [160/191], loss=77.6945
	step [161/191], loss=74.3183
	step [162/191], loss=61.9305
	step [163/191], loss=69.1817
	step [164/191], loss=82.4605
	step [165/191], loss=60.6060
	step [166/191], loss=78.2486
	step [167/191], loss=59.4131
	step [168/191], loss=82.9750
	step [169/191], loss=67.9226
	step [170/191], loss=66.5186
	step [171/191], loss=73.9568
	step [172/191], loss=73.9712
	step [173/191], loss=60.0991
	step [174/191], loss=87.5824
	step [175/191], loss=83.1082
	step [176/191], loss=58.8589
	step [177/191], loss=71.7967
	step [178/191], loss=72.1174
	step [179/191], loss=66.2508
	step [180/191], loss=75.1873
	step [181/191], loss=62.2676
	step [182/191], loss=77.7192
	step [183/191], loss=70.6716
	step [184/191], loss=65.3874
	step [185/191], loss=79.3465
	step [186/191], loss=53.5150
	step [187/191], loss=75.7314
	step [188/191], loss=61.6364
	step [189/191], loss=73.3638
	step [190/191], loss=71.0434
	step [191/191], loss=42.3086
	Evaluating
	loss=0.0099, precision=0.3624, recall=0.8679, f1=0.5113
Training epoch 68
	step [1/191], loss=64.9133
	step [2/191], loss=65.7456
	step [3/191], loss=72.7030
	step [4/191], loss=72.0209
	step [5/191], loss=57.6011
	step [6/191], loss=69.5083
	step [7/191], loss=69.2316
	step [8/191], loss=91.0525
	step [9/191], loss=75.0813
	step [10/191], loss=74.2689
	step [11/191], loss=62.8764
	step [12/191], loss=71.6972
	step [13/191], loss=60.1532
	step [14/191], loss=62.5657
	step [15/191], loss=80.0444
	step [16/191], loss=74.3742
	step [17/191], loss=68.0302
	step [18/191], loss=71.4246
	step [19/191], loss=77.2611
	step [20/191], loss=73.8478
	step [21/191], loss=70.4893
	step [22/191], loss=70.0914
	step [23/191], loss=70.6641
	step [24/191], loss=61.6173
	step [25/191], loss=57.6608
	step [26/191], loss=71.9298
	step [27/191], loss=65.7626
	step [28/191], loss=65.1356
	step [29/191], loss=74.3216
	step [30/191], loss=69.0290
	step [31/191], loss=73.2146
	step [32/191], loss=73.9285
	step [33/191], loss=60.3968
	step [34/191], loss=63.3100
	step [35/191], loss=75.0328
	step [36/191], loss=63.4832
	step [37/191], loss=64.7740
	step [38/191], loss=67.2806
	step [39/191], loss=71.1215
	step [40/191], loss=67.6144
	step [41/191], loss=67.3395
	step [42/191], loss=72.8918
	step [43/191], loss=71.8262
	step [44/191], loss=65.9559
	step [45/191], loss=69.0753
	step [46/191], loss=85.8075
	step [47/191], loss=66.4060
	step [48/191], loss=75.9121
	step [49/191], loss=70.4006
	step [50/191], loss=62.6409
	step [51/191], loss=75.3943
	step [52/191], loss=76.0040
	step [53/191], loss=72.5366
	step [54/191], loss=68.8627
	step [55/191], loss=65.7922
	step [56/191], loss=69.1037
	step [57/191], loss=60.2905
	step [58/191], loss=69.0744
	step [59/191], loss=77.8367
	step [60/191], loss=79.8866
	step [61/191], loss=70.3828
	step [62/191], loss=62.3346
	step [63/191], loss=82.0971
	step [64/191], loss=74.6988
	step [65/191], loss=60.9415
	step [66/191], loss=64.9742
	step [67/191], loss=78.3999
	step [68/191], loss=74.9221
	step [69/191], loss=78.4987
	step [70/191], loss=69.2848
	step [71/191], loss=79.0510
	step [72/191], loss=83.8275
	step [73/191], loss=73.3517
	step [74/191], loss=75.4857
	step [75/191], loss=77.2537
	step [76/191], loss=81.3238
	step [77/191], loss=68.8211
	step [78/191], loss=68.9685
	step [79/191], loss=77.3159
	step [80/191], loss=57.2107
	step [81/191], loss=67.0452
	step [82/191], loss=70.5399
	step [83/191], loss=63.2545
	step [84/191], loss=66.4412
	step [85/191], loss=66.8208
	step [86/191], loss=66.6371
	step [87/191], loss=70.7641
	step [88/191], loss=83.0210
	step [89/191], loss=68.7812
	step [90/191], loss=67.7008
	step [91/191], loss=67.1764
	step [92/191], loss=71.0575
	step [93/191], loss=73.7642
	step [94/191], loss=63.6680
	step [95/191], loss=76.3015
	step [96/191], loss=68.9585
	step [97/191], loss=66.7996
	step [98/191], loss=73.8776
	step [99/191], loss=62.9920
	step [100/191], loss=72.0150
	step [101/191], loss=73.3142
	step [102/191], loss=65.0812
	step [103/191], loss=65.7865
	step [104/191], loss=73.6202
	step [105/191], loss=74.9291
	step [106/191], loss=68.2345
	step [107/191], loss=63.7687
	step [108/191], loss=64.5316
	step [109/191], loss=66.4874
	step [110/191], loss=80.7240
	step [111/191], loss=76.5265
	step [112/191], loss=57.6725
	step [113/191], loss=66.1316
	step [114/191], loss=82.3291
	step [115/191], loss=74.1292
	step [116/191], loss=61.1124
	step [117/191], loss=72.2638
	step [118/191], loss=72.3340
	step [119/191], loss=70.9560
	step [120/191], loss=70.7853
	step [121/191], loss=65.4428
	step [122/191], loss=70.7825
	step [123/191], loss=87.2169
	step [124/191], loss=63.3185
	step [125/191], loss=69.6039
	step [126/191], loss=74.9385
	step [127/191], loss=64.0167
	step [128/191], loss=72.1658
	step [129/191], loss=66.5646
	step [130/191], loss=73.5314
	step [131/191], loss=78.5360
	step [132/191], loss=65.0909
	step [133/191], loss=71.6042
	step [134/191], loss=90.3287
	step [135/191], loss=56.4409
	step [136/191], loss=82.2599
	step [137/191], loss=65.6473
	step [138/191], loss=66.9897
	step [139/191], loss=76.4243
	step [140/191], loss=80.4371
	step [141/191], loss=62.2233
	step [142/191], loss=63.8317
	step [143/191], loss=71.5799
	step [144/191], loss=78.7094
	step [145/191], loss=75.1698
	step [146/191], loss=64.9947
	step [147/191], loss=79.3016
	step [148/191], loss=68.2254
	step [149/191], loss=62.1544
	step [150/191], loss=73.4878
	step [151/191], loss=70.6284
	step [152/191], loss=62.3007
	step [153/191], loss=74.8850
	step [154/191], loss=66.1171
	step [155/191], loss=75.6553
	step [156/191], loss=78.0179
	step [157/191], loss=80.0447
	step [158/191], loss=62.1997
	step [159/191], loss=69.7470
	step [160/191], loss=66.5972
	step [161/191], loss=76.7010
	step [162/191], loss=68.5337
	step [163/191], loss=62.4465
	step [164/191], loss=62.6386
	step [165/191], loss=62.4970
	step [166/191], loss=60.6094
	step [167/191], loss=63.3554
	step [168/191], loss=67.2259
	step [169/191], loss=68.8565
	step [170/191], loss=62.7802
	step [171/191], loss=79.9472
	step [172/191], loss=76.0623
	step [173/191], loss=70.2864
	step [174/191], loss=65.6931
	step [175/191], loss=77.6594
	step [176/191], loss=56.5202
	step [177/191], loss=76.0101
	step [178/191], loss=64.7288
	step [179/191], loss=80.3627
	step [180/191], loss=69.4430
	step [181/191], loss=76.2636
	step [182/191], loss=66.8681
	step [183/191], loss=73.9484
	step [184/191], loss=67.7118
	step [185/191], loss=65.2112
	step [186/191], loss=60.9750
	step [187/191], loss=49.4262
	step [188/191], loss=60.4430
	step [189/191], loss=62.3155
	step [190/191], loss=65.6251
	step [191/191], loss=37.8059
	Evaluating
	loss=0.0081, precision=0.4408, recall=0.8612, f1=0.5831
saving model as: 1_saved_model.pth
Training epoch 69
	step [1/191], loss=66.8432
	step [2/191], loss=60.2837
	step [3/191], loss=68.3218
	step [4/191], loss=78.2313
	step [5/191], loss=72.7316
	step [6/191], loss=69.5433
	step [7/191], loss=75.6901
	step [8/191], loss=70.3310
	step [9/191], loss=70.1569
	step [10/191], loss=75.6557
	step [11/191], loss=66.8751
	step [12/191], loss=65.7221
	step [13/191], loss=65.6985
	step [14/191], loss=68.0517
	step [15/191], loss=73.1017
	step [16/191], loss=72.9274
	step [17/191], loss=79.4677
	step [18/191], loss=64.0990
	step [19/191], loss=60.5720
	step [20/191], loss=57.9242
	step [21/191], loss=69.0146
	step [22/191], loss=76.1695
	step [23/191], loss=68.2853
	step [24/191], loss=61.5099
	step [25/191], loss=60.5978
	step [26/191], loss=67.4503
	step [27/191], loss=78.2761
	step [28/191], loss=60.8010
	step [29/191], loss=66.1820
	step [30/191], loss=82.0176
	step [31/191], loss=79.8762
	step [32/191], loss=72.7671
	step [33/191], loss=69.7265
	step [34/191], loss=71.8555
	step [35/191], loss=67.5381
	step [36/191], loss=71.3113
	step [37/191], loss=63.4379
	step [38/191], loss=80.8812
	step [39/191], loss=55.6031
	step [40/191], loss=72.3188
	step [41/191], loss=71.4954
	step [42/191], loss=73.5182
	step [43/191], loss=72.9957
	step [44/191], loss=68.8064
	step [45/191], loss=74.0907
	step [46/191], loss=68.4638
	step [47/191], loss=74.7232
	step [48/191], loss=68.3457
	step [49/191], loss=72.4778
	step [50/191], loss=71.1308
	step [51/191], loss=67.6010
	step [52/191], loss=68.8338
	step [53/191], loss=79.2252
	step [54/191], loss=56.6990
	step [55/191], loss=69.2961
	step [56/191], loss=66.2351
	step [57/191], loss=67.7614
	step [58/191], loss=67.5208
	step [59/191], loss=71.2752
	step [60/191], loss=61.2262
	step [61/191], loss=71.4005
	step [62/191], loss=69.6770
	step [63/191], loss=59.4821
	step [64/191], loss=66.5837
	step [65/191], loss=66.1949
	step [66/191], loss=73.2065
	step [67/191], loss=71.6086
	step [68/191], loss=64.8142
	step [69/191], loss=70.6041
	step [70/191], loss=61.2455
	step [71/191], loss=75.5449
	step [72/191], loss=70.9866
	step [73/191], loss=67.1127
	step [74/191], loss=58.8092
	step [75/191], loss=58.3979
	step [76/191], loss=74.1960
	step [77/191], loss=68.7382
	step [78/191], loss=72.9733
	step [79/191], loss=76.9557
	step [80/191], loss=67.8746
	step [81/191], loss=85.0377
	step [82/191], loss=61.2343
	step [83/191], loss=67.0506
	step [84/191], loss=63.0991
	step [85/191], loss=76.2615
	step [86/191], loss=72.4326
	step [87/191], loss=77.3736
	step [88/191], loss=72.1634
	step [89/191], loss=70.1785
	step [90/191], loss=65.1490
	step [91/191], loss=65.2816
	step [92/191], loss=59.6034
	step [93/191], loss=83.9724
	step [94/191], loss=64.4172
	step [95/191], loss=63.6858
	step [96/191], loss=63.2319
	step [97/191], loss=71.0423
	step [98/191], loss=73.5458
	step [99/191], loss=68.7389
	step [100/191], loss=62.0931
	step [101/191], loss=73.6103
	step [102/191], loss=70.5767
	step [103/191], loss=70.1788
	step [104/191], loss=73.8301
	step [105/191], loss=69.6748
	step [106/191], loss=62.7247
	step [107/191], loss=59.2997
	step [108/191], loss=72.9156
	step [109/191], loss=70.4030
	step [110/191], loss=74.6712
	step [111/191], loss=67.4705
	step [112/191], loss=83.0694
	step [113/191], loss=55.8450
	step [114/191], loss=64.5044
	step [115/191], loss=64.9813
	step [116/191], loss=59.0354
	step [117/191], loss=72.6098
	step [118/191], loss=69.0965
	step [119/191], loss=61.0265
	step [120/191], loss=69.9682
	step [121/191], loss=69.6077
	step [122/191], loss=87.9673
	step [123/191], loss=71.1486
	step [124/191], loss=69.2694
	step [125/191], loss=80.1026
	step [126/191], loss=68.6360
	step [127/191], loss=79.6244
	step [128/191], loss=75.0713
	step [129/191], loss=68.4836
	step [130/191], loss=70.2397
	step [131/191], loss=68.4672
	step [132/191], loss=74.8263
	step [133/191], loss=73.8799
	step [134/191], loss=74.6388
	step [135/191], loss=80.0467
	step [136/191], loss=74.7213
	step [137/191], loss=60.1263
	step [138/191], loss=79.7488
	step [139/191], loss=67.0777
	step [140/191], loss=69.5181
	step [141/191], loss=75.0179
	step [142/191], loss=69.6346
	step [143/191], loss=72.2617
	step [144/191], loss=72.3863
	step [145/191], loss=71.8230
	step [146/191], loss=75.5943
	step [147/191], loss=72.8655
	step [148/191], loss=69.0221
	step [149/191], loss=68.7140
	step [150/191], loss=68.1738
	step [151/191], loss=56.0647
	step [152/191], loss=78.4763
	step [153/191], loss=59.7939
	step [154/191], loss=71.1838
	step [155/191], loss=75.7161
	step [156/191], loss=70.0945
	step [157/191], loss=76.5999
	step [158/191], loss=73.3607
	step [159/191], loss=69.2075
	step [160/191], loss=75.5998
	step [161/191], loss=69.0400
	step [162/191], loss=65.8983
	step [163/191], loss=70.6855
	step [164/191], loss=81.3813
	step [165/191], loss=69.9839
	step [166/191], loss=84.8458
	step [167/191], loss=62.9738
	step [168/191], loss=63.2707
	step [169/191], loss=69.7805
	step [170/191], loss=64.8143
	step [171/191], loss=78.9518
	step [172/191], loss=70.2710
	step [173/191], loss=72.3373
	step [174/191], loss=60.7234
	step [175/191], loss=72.2522
	step [176/191], loss=69.6502
	step [177/191], loss=73.2548
	step [178/191], loss=78.4689
	step [179/191], loss=65.5752
	step [180/191], loss=66.1894
	step [181/191], loss=79.1154
	step [182/191], loss=68.0481
	step [183/191], loss=55.3119
	step [184/191], loss=51.5253
	step [185/191], loss=75.5392
	step [186/191], loss=66.1266
	step [187/191], loss=63.2646
	step [188/191], loss=74.2380
	step [189/191], loss=69.6178
	step [190/191], loss=67.8963
	step [191/191], loss=34.8909
	Evaluating
	loss=0.0079, precision=0.4296, recall=0.8549, f1=0.5718
Training epoch 70
	step [1/191], loss=70.9928
	step [2/191], loss=77.0603
	step [3/191], loss=73.4709
	step [4/191], loss=65.4981
	step [5/191], loss=67.6751
	step [6/191], loss=56.1541
	step [7/191], loss=76.7289
	step [8/191], loss=56.1796
	step [9/191], loss=80.8833
	step [10/191], loss=67.1824
	step [11/191], loss=71.6513
	step [12/191], loss=67.6314
	step [13/191], loss=72.7573
	step [14/191], loss=67.9463
	step [15/191], loss=64.2308
	step [16/191], loss=78.1979
	step [17/191], loss=58.2097
	step [18/191], loss=76.7111
	step [19/191], loss=77.7459
	step [20/191], loss=69.3085
	step [21/191], loss=69.5875
	step [22/191], loss=76.4273
	step [23/191], loss=51.2101
	step [24/191], loss=60.4024
	step [25/191], loss=74.1314
	step [26/191], loss=70.7383
	step [27/191], loss=54.7292
	step [28/191], loss=75.0562
	step [29/191], loss=62.2723
	step [30/191], loss=77.9859
	step [31/191], loss=80.3702
	step [32/191], loss=69.1466
	step [33/191], loss=59.5286
	step [34/191], loss=63.9433
	step [35/191], loss=60.8906
	step [36/191], loss=74.6628
	step [37/191], loss=74.4899
	step [38/191], loss=86.6410
	step [39/191], loss=60.7419
	step [40/191], loss=65.9433
	step [41/191], loss=69.1915
	step [42/191], loss=84.1658
	step [43/191], loss=66.1085
	step [44/191], loss=62.4274
	step [45/191], loss=75.0073
	step [46/191], loss=70.5535
	step [47/191], loss=61.1404
	step [48/191], loss=67.9997
	step [49/191], loss=62.0188
	step [50/191], loss=78.9103
	step [51/191], loss=69.7013
	step [52/191], loss=60.5878
	step [53/191], loss=63.1468
	step [54/191], loss=86.2896
	step [55/191], loss=58.0306
	step [56/191], loss=73.3162
	step [57/191], loss=81.6153
	step [58/191], loss=74.1989
	step [59/191], loss=74.5223
	step [60/191], loss=89.0225
	step [61/191], loss=74.5288
	step [62/191], loss=63.0128
	step [63/191], loss=61.9230
	step [64/191], loss=83.9494
	step [65/191], loss=68.7630
	step [66/191], loss=75.9283
	step [67/191], loss=69.3795
	step [68/191], loss=64.5739
	step [69/191], loss=83.5195
	step [70/191], loss=76.4294
	step [71/191], loss=77.1034
	step [72/191], loss=62.0317
	step [73/191], loss=70.7936
	step [74/191], loss=72.0389
	step [75/191], loss=65.6171
	step [76/191], loss=57.6477
	step [77/191], loss=67.9219
	step [78/191], loss=62.3513
	step [79/191], loss=72.1201
	step [80/191], loss=57.2993
	step [81/191], loss=69.4726
	step [82/191], loss=66.6355
	step [83/191], loss=76.6775
	step [84/191], loss=74.0296
	step [85/191], loss=60.3544
	step [86/191], loss=70.2162
	step [87/191], loss=76.8994
	step [88/191], loss=68.5952
	step [89/191], loss=58.3967
	step [90/191], loss=71.6188
	step [91/191], loss=66.7362
	step [92/191], loss=86.1946
	step [93/191], loss=85.3370
	step [94/191], loss=65.9634
	step [95/191], loss=66.9492
	step [96/191], loss=70.8996
	step [97/191], loss=64.2094
	step [98/191], loss=65.6407
	step [99/191], loss=77.0394
	step [100/191], loss=66.6803
	step [101/191], loss=48.7008
	step [102/191], loss=55.7375
	step [103/191], loss=64.9025
	step [104/191], loss=73.4182
	step [105/191], loss=79.5668
	step [106/191], loss=82.2402
	step [107/191], loss=61.7118
	step [108/191], loss=67.7427
	step [109/191], loss=69.9720
	step [110/191], loss=59.3817
	step [111/191], loss=66.6516
	step [112/191], loss=61.8208
	step [113/191], loss=70.6849
	step [114/191], loss=62.1191
	step [115/191], loss=64.9809
	step [116/191], loss=61.0063
	step [117/191], loss=71.9445
	step [118/191], loss=68.8565
	step [119/191], loss=74.0619
	step [120/191], loss=66.1568
	step [121/191], loss=69.3630
	step [122/191], loss=65.5383
	step [123/191], loss=78.4321
	step [124/191], loss=59.4062
	step [125/191], loss=60.9240
	step [126/191], loss=81.4989
	step [127/191], loss=65.4264
	step [128/191], loss=70.6882
	step [129/191], loss=68.0265
	step [130/191], loss=68.6890
	step [131/191], loss=60.3414
	step [132/191], loss=84.4632
	step [133/191], loss=66.1570
	step [134/191], loss=59.9047
	step [135/191], loss=77.5824
	step [136/191], loss=67.2531
	step [137/191], loss=74.1129
	step [138/191], loss=79.1924
	step [139/191], loss=93.7921
	step [140/191], loss=60.6012
	step [141/191], loss=71.3763
	step [142/191], loss=65.2131
	step [143/191], loss=56.6039
	step [144/191], loss=75.5879
	step [145/191], loss=62.5997
	step [146/191], loss=71.9229
	step [147/191], loss=54.2317
	step [148/191], loss=80.1196
	step [149/191], loss=63.0283
	step [150/191], loss=77.9788
	step [151/191], loss=70.6417
	step [152/191], loss=66.6555
	step [153/191], loss=78.6853
	step [154/191], loss=72.0800
	step [155/191], loss=63.8421
	step [156/191], loss=69.9919
	step [157/191], loss=58.2296
	step [158/191], loss=72.3274
	step [159/191], loss=57.1477
	step [160/191], loss=82.9439
	step [161/191], loss=60.1577
	step [162/191], loss=71.6512
	step [163/191], loss=80.8810
	step [164/191], loss=75.2802
	step [165/191], loss=63.5082
	step [166/191], loss=60.5802
	step [167/191], loss=75.3680
	step [168/191], loss=72.1616
	step [169/191], loss=83.4660
	step [170/191], loss=67.7789
	step [171/191], loss=71.2589
	step [172/191], loss=73.2240
	step [173/191], loss=70.2398
	step [174/191], loss=72.8223
	step [175/191], loss=68.1360
	step [176/191], loss=75.8991
	step [177/191], loss=67.2048
	step [178/191], loss=75.0131
	step [179/191], loss=82.9820
	step [180/191], loss=66.4579
	step [181/191], loss=69.3139
	step [182/191], loss=66.7796
	step [183/191], loss=59.5738
	step [184/191], loss=83.8748
	step [185/191], loss=72.6626
	step [186/191], loss=74.9774
	step [187/191], loss=69.4519
	step [188/191], loss=67.3640
	step [189/191], loss=71.9565
	step [190/191], loss=72.0571
	step [191/191], loss=33.8477
	Evaluating
	loss=0.0086, precision=0.4071, recall=0.8624, f1=0.5531
Training epoch 71
	step [1/191], loss=69.6803
	step [2/191], loss=77.6604
	step [3/191], loss=67.6212
	step [4/191], loss=65.2917
	step [5/191], loss=71.6933
	step [6/191], loss=73.7504
	step [7/191], loss=56.6899
	step [8/191], loss=63.9660
	step [9/191], loss=81.6293
	step [10/191], loss=59.1884
	step [11/191], loss=67.9573
	step [12/191], loss=67.1464
	step [13/191], loss=73.1709
	step [14/191], loss=62.2509
	step [15/191], loss=72.8855
	step [16/191], loss=69.1844
	step [17/191], loss=84.7499
	step [18/191], loss=53.8587
	step [19/191], loss=65.4399
	step [20/191], loss=63.7980
	step [21/191], loss=77.5295
	step [22/191], loss=68.2304
	step [23/191], loss=76.1901
	step [24/191], loss=65.9914
	step [25/191], loss=66.9438
	step [26/191], loss=74.3773
	step [27/191], loss=72.3781
	step [28/191], loss=70.7012
	step [29/191], loss=72.4981
	step [30/191], loss=86.9815
	step [31/191], loss=55.0783
	step [32/191], loss=69.4060
	step [33/191], loss=66.2325
	step [34/191], loss=78.7745
	step [35/191], loss=73.9969
	step [36/191], loss=71.1299
	step [37/191], loss=69.8121
	step [38/191], loss=66.0740
	step [39/191], loss=69.3622
	step [40/191], loss=69.0896
	step [41/191], loss=63.3370
	step [42/191], loss=84.7290
	step [43/191], loss=66.8754
	step [44/191], loss=65.3514
	step [45/191], loss=83.3271
	step [46/191], loss=73.0424
	step [47/191], loss=67.3755
	step [48/191], loss=66.1469
	step [49/191], loss=69.0867
	step [50/191], loss=66.1933
	step [51/191], loss=68.5091
	step [52/191], loss=73.9017
	step [53/191], loss=55.4161
	step [54/191], loss=72.8380
	step [55/191], loss=71.9172
	step [56/191], loss=69.5599
	step [57/191], loss=67.4159
	step [58/191], loss=74.5877
	step [59/191], loss=51.7225
	step [60/191], loss=62.7562
	step [61/191], loss=55.3231
	step [62/191], loss=71.1150
	step [63/191], loss=63.7842
	step [64/191], loss=69.7040
	step [65/191], loss=74.1239
	step [66/191], loss=75.4987
	step [67/191], loss=72.4073
	step [68/191], loss=58.1558
	step [69/191], loss=76.8798
	step [70/191], loss=77.3416
	step [71/191], loss=61.0523
	step [72/191], loss=75.1698
	step [73/191], loss=78.1851
	step [74/191], loss=64.7290
	step [75/191], loss=77.5249
	step [76/191], loss=59.7278
	step [77/191], loss=57.2747
	step [78/191], loss=63.1218
	step [79/191], loss=67.1434
	step [80/191], loss=72.8485
	step [81/191], loss=74.4416
	step [82/191], loss=74.8024
	step [83/191], loss=60.9656
	step [84/191], loss=61.2725
	step [85/191], loss=60.9871
	step [86/191], loss=71.8905
	step [87/191], loss=64.1712
	step [88/191], loss=50.1640
	step [89/191], loss=68.2791
	step [90/191], loss=71.4324
	step [91/191], loss=69.9799
	step [92/191], loss=61.2395
	step [93/191], loss=63.4674
	step [94/191], loss=67.3634
	step [95/191], loss=67.5238
	step [96/191], loss=59.3961
	step [97/191], loss=68.8186
	step [98/191], loss=81.8159
	step [99/191], loss=69.7807
	step [100/191], loss=58.3763
	step [101/191], loss=69.5448
	step [102/191], loss=70.8269
	step [103/191], loss=85.4577
	step [104/191], loss=68.7160
	step [105/191], loss=72.1031
	step [106/191], loss=57.1794
	step [107/191], loss=56.7105
	step [108/191], loss=72.3808
	step [109/191], loss=67.6948
	step [110/191], loss=72.8803
	step [111/191], loss=67.9833
	step [112/191], loss=59.9048
	step [113/191], loss=74.5607
	step [114/191], loss=66.6046
	step [115/191], loss=67.0849
	step [116/191], loss=76.4603
	step [117/191], loss=71.2921
	step [118/191], loss=68.0285
	step [119/191], loss=74.2923
	step [120/191], loss=67.1354
	step [121/191], loss=71.6555
	step [122/191], loss=67.3847
	step [123/191], loss=68.9634
	step [124/191], loss=68.9158
	step [125/191], loss=70.2043
	step [126/191], loss=71.2413
	step [127/191], loss=64.8534
	step [128/191], loss=67.5107
	step [129/191], loss=88.0806
	step [130/191], loss=67.5066
	step [131/191], loss=74.3543
	step [132/191], loss=73.0975
	step [133/191], loss=82.4557
	step [134/191], loss=83.4912
	step [135/191], loss=57.8898
	step [136/191], loss=75.3733
	step [137/191], loss=68.4978
	step [138/191], loss=66.7297
	step [139/191], loss=68.1204
	step [140/191], loss=78.2146
	step [141/191], loss=75.3496
	step [142/191], loss=74.7975
	step [143/191], loss=69.2973
	step [144/191], loss=66.2698
	step [145/191], loss=74.1451
	step [146/191], loss=68.2027
	step [147/191], loss=74.8949
	step [148/191], loss=71.0291
	step [149/191], loss=65.2321
	step [150/191], loss=56.8123
	step [151/191], loss=67.4709
	step [152/191], loss=69.7578
	step [153/191], loss=75.1250
	step [154/191], loss=70.4839
	step [155/191], loss=83.9133
	step [156/191], loss=66.2845
	step [157/191], loss=67.3083
	step [158/191], loss=62.9799
	step [159/191], loss=80.6286
	step [160/191], loss=54.9710
	step [161/191], loss=62.0666
	step [162/191], loss=73.6521
	step [163/191], loss=73.5942
	step [164/191], loss=73.3318
	step [165/191], loss=71.8884
	step [166/191], loss=72.5788
	step [167/191], loss=73.8979
	step [168/191], loss=76.4927
	step [169/191], loss=62.0342
	step [170/191], loss=76.0343
	step [171/191], loss=70.4609
	step [172/191], loss=65.1037
	step [173/191], loss=71.0269
	step [174/191], loss=60.7422
	step [175/191], loss=66.9563
	step [176/191], loss=56.2917
	step [177/191], loss=58.7253
	step [178/191], loss=73.1072
	step [179/191], loss=85.9223
	step [180/191], loss=69.7763
	step [181/191], loss=69.3937
	step [182/191], loss=76.3200
	step [183/191], loss=67.0763
	step [184/191], loss=68.9074
	step [185/191], loss=69.7668
	step [186/191], loss=74.3603
	step [187/191], loss=76.4437
	step [188/191], loss=71.9522
	step [189/191], loss=63.4849
	step [190/191], loss=67.8583
	step [191/191], loss=37.4465
	Evaluating
	loss=0.0072, precision=0.4622, recall=0.8476, f1=0.5982
saving model as: 1_saved_model.pth
Training epoch 72
	step [1/191], loss=79.6030
	step [2/191], loss=58.2148
	step [3/191], loss=78.5847
	step [4/191], loss=77.9144
	step [5/191], loss=61.5477
	step [6/191], loss=87.1360
	step [7/191], loss=79.2911
	step [8/191], loss=74.6115
	step [9/191], loss=77.7638
	step [10/191], loss=77.4048
	step [11/191], loss=63.7845
	step [12/191], loss=79.9606
	step [13/191], loss=60.6885
	step [14/191], loss=55.8955
	step [15/191], loss=81.0851
	step [16/191], loss=67.2107
	step [17/191], loss=62.8272
	step [18/191], loss=60.1486
	step [19/191], loss=70.2193
	step [20/191], loss=65.7376
	step [21/191], loss=65.6495
	step [22/191], loss=61.6826
	step [23/191], loss=74.1358
	step [24/191], loss=64.0155
	step [25/191], loss=67.1134
	step [26/191], loss=77.1953
	step [27/191], loss=68.8812
	step [28/191], loss=70.1759
	step [29/191], loss=64.0841
	step [30/191], loss=70.0220
	step [31/191], loss=70.1360
	step [32/191], loss=67.2399
	step [33/191], loss=76.1705
	step [34/191], loss=69.9420
	step [35/191], loss=78.8374
	step [36/191], loss=64.5405
	step [37/191], loss=68.8079
	step [38/191], loss=72.7241
	step [39/191], loss=65.9888
	step [40/191], loss=60.4664
	step [41/191], loss=68.7478
	step [42/191], loss=64.5186
	step [43/191], loss=58.5767
	step [44/191], loss=63.0987
	step [45/191], loss=62.5197
	step [46/191], loss=69.0325
	step [47/191], loss=68.9490
	step [48/191], loss=64.1945
	step [49/191], loss=64.2857
	step [50/191], loss=74.1351
	step [51/191], loss=71.6682
	step [52/191], loss=79.1995
	step [53/191], loss=72.8481
	step [54/191], loss=62.8366
	step [55/191], loss=64.1695
	step [56/191], loss=69.2219
	step [57/191], loss=64.4368
	step [58/191], loss=69.5905
	step [59/191], loss=91.8973
	step [60/191], loss=79.4637
	step [61/191], loss=62.4458
	step [62/191], loss=61.9936
	step [63/191], loss=71.7265
	step [64/191], loss=64.6872
	step [65/191], loss=71.5960
	step [66/191], loss=63.3515
	step [67/191], loss=58.7283
	step [68/191], loss=64.9965
	step [69/191], loss=76.4700
	step [70/191], loss=70.6319
	step [71/191], loss=71.1132
	step [72/191], loss=59.7664
	step [73/191], loss=79.9542
	step [74/191], loss=72.0268
	step [75/191], loss=72.0919
	step [76/191], loss=66.5664
	step [77/191], loss=70.5171
	step [78/191], loss=69.6168
	step [79/191], loss=55.3371
	step [80/191], loss=66.4189
	step [81/191], loss=85.5508
	step [82/191], loss=73.7127
	step [83/191], loss=72.4366
	step [84/191], loss=63.0963
	step [85/191], loss=74.6351
	step [86/191], loss=65.4803
	step [87/191], loss=67.8981
	step [88/191], loss=80.0677
	step [89/191], loss=59.7151
	step [90/191], loss=62.0335
	step [91/191], loss=74.6404
	step [92/191], loss=60.4898
	step [93/191], loss=71.4045
	step [94/191], loss=76.5932
	step [95/191], loss=57.8740
	step [96/191], loss=68.5895
	step [97/191], loss=65.9718
	step [98/191], loss=70.9557
	step [99/191], loss=73.1555
	step [100/191], loss=72.2535
	step [101/191], loss=80.3494
	step [102/191], loss=72.0722
	step [103/191], loss=69.3202
	step [104/191], loss=73.4903
	step [105/191], loss=78.4533
	step [106/191], loss=72.3921
	step [107/191], loss=61.5423
	step [108/191], loss=64.7713
	step [109/191], loss=61.2641
	step [110/191], loss=67.6923
	step [111/191], loss=88.8851
	step [112/191], loss=62.3401
	step [113/191], loss=65.5126
	step [114/191], loss=77.9441
	step [115/191], loss=71.8332
	step [116/191], loss=64.2194
	step [117/191], loss=63.8684
	step [118/191], loss=65.2949
	step [119/191], loss=65.3217
	step [120/191], loss=54.9961
	step [121/191], loss=61.5496
	step [122/191], loss=70.3066
	step [123/191], loss=60.9065
	step [124/191], loss=72.8810
	step [125/191], loss=67.0008
	step [126/191], loss=45.5350
	step [127/191], loss=75.7765
	step [128/191], loss=64.3021
	step [129/191], loss=67.5967
	step [130/191], loss=69.0645
	step [131/191], loss=74.0012
	step [132/191], loss=63.2955
	step [133/191], loss=77.5014
	step [134/191], loss=59.9392
	step [135/191], loss=70.0923
	step [136/191], loss=74.4484
	step [137/191], loss=66.2845
	step [138/191], loss=69.9131
	step [139/191], loss=71.8082
	step [140/191], loss=85.9198
	step [141/191], loss=68.8122
	step [142/191], loss=68.5666
	step [143/191], loss=71.0342
	step [144/191], loss=68.6135
	step [145/191], loss=58.4609
	step [146/191], loss=74.6376
	step [147/191], loss=80.8197
	step [148/191], loss=68.0895
	step [149/191], loss=71.0894
	step [150/191], loss=68.7761
	step [151/191], loss=68.5682
	step [152/191], loss=66.2273
	step [153/191], loss=63.4185
	step [154/191], loss=81.8374
	step [155/191], loss=71.7564
	step [156/191], loss=66.5613
	step [157/191], loss=59.9364
	step [158/191], loss=66.0138
	step [159/191], loss=61.2487
	step [160/191], loss=67.7373
	step [161/191], loss=68.5787
	step [162/191], loss=67.4012
	step [163/191], loss=72.5069
	step [164/191], loss=81.8546
	step [165/191], loss=67.0662
	step [166/191], loss=77.8566
	step [167/191], loss=60.8300
	step [168/191], loss=77.0841
	step [169/191], loss=66.1870
	step [170/191], loss=69.8024
	step [171/191], loss=73.5240
	step [172/191], loss=69.8327
	step [173/191], loss=70.5514
	step [174/191], loss=70.9291
	step [175/191], loss=61.7727
	step [176/191], loss=67.5651
	step [177/191], loss=72.4161
	step [178/191], loss=66.0272
	step [179/191], loss=73.7684
	step [180/191], loss=80.9139
	step [181/191], loss=69.9627
	step [182/191], loss=57.9062
	step [183/191], loss=74.2570
	step [184/191], loss=62.3617
	step [185/191], loss=69.5826
	step [186/191], loss=71.9705
	step [187/191], loss=65.7639
	step [188/191], loss=80.2069
	step [189/191], loss=58.7891
	step [190/191], loss=75.7583
	step [191/191], loss=27.5924
	Evaluating
	loss=0.0076, precision=0.4455, recall=0.8541, f1=0.5856
Training epoch 73
	step [1/191], loss=89.3679
	step [2/191], loss=63.6145
	step [3/191], loss=68.5675
	step [4/191], loss=76.5999
	step [5/191], loss=73.5639
	step [6/191], loss=74.8795
	step [7/191], loss=68.1292
	step [8/191], loss=69.6606
	step [9/191], loss=65.2286
	step [10/191], loss=62.1908
	step [11/191], loss=79.5589
	step [12/191], loss=62.7978
	step [13/191], loss=62.2286
	step [14/191], loss=80.0199
	step [15/191], loss=68.1442
	step [16/191], loss=68.1135
	step [17/191], loss=83.0632
	step [18/191], loss=70.6096
	step [19/191], loss=64.3723
	step [20/191], loss=61.3074
	step [21/191], loss=59.5086
	step [22/191], loss=63.7707
	step [23/191], loss=62.4592
	step [24/191], loss=66.5484
	step [25/191], loss=71.0872
	step [26/191], loss=63.5006
	step [27/191], loss=70.7663
	step [28/191], loss=86.6773
	step [29/191], loss=66.7920
	step [30/191], loss=65.9406
	step [31/191], loss=77.0400
	step [32/191], loss=85.7068
	step [33/191], loss=60.2995
	step [34/191], loss=77.1423
	step [35/191], loss=70.4033
	step [36/191], loss=59.0267
	step [37/191], loss=65.3365
	step [38/191], loss=62.3028
	step [39/191], loss=67.4702
	step [40/191], loss=70.0222
	step [41/191], loss=69.2062
	step [42/191], loss=54.2499
	step [43/191], loss=65.1715
	step [44/191], loss=50.3206
	step [45/191], loss=64.1742
	step [46/191], loss=62.0391
	step [47/191], loss=78.9777
	step [48/191], loss=73.7739
	step [49/191], loss=74.6069
	step [50/191], loss=64.9252
	step [51/191], loss=81.1662
	step [52/191], loss=73.1318
	step [53/191], loss=69.4201
	step [54/191], loss=69.4350
	step [55/191], loss=79.4044
	step [56/191], loss=56.1546
	step [57/191], loss=73.6113
	step [58/191], loss=55.9079
	step [59/191], loss=73.5047
	step [60/191], loss=82.9309
	step [61/191], loss=64.5069
	step [62/191], loss=60.6649
	step [63/191], loss=66.1301
	step [64/191], loss=66.1058
	step [65/191], loss=70.0497
	step [66/191], loss=62.6000
	step [67/191], loss=67.3783
	step [68/191], loss=64.8859
	step [69/191], loss=76.5138
	step [70/191], loss=66.7635
	step [71/191], loss=70.4577
	step [72/191], loss=72.1490
	step [73/191], loss=75.7315
	step [74/191], loss=69.5348
	step [75/191], loss=67.7381
	step [76/191], loss=61.3441
	step [77/191], loss=80.1348
	step [78/191], loss=68.0042
	step [79/191], loss=77.2037
	step [80/191], loss=63.4241
	step [81/191], loss=57.4649
	step [82/191], loss=61.1160
	step [83/191], loss=61.4523
	step [84/191], loss=63.7682
	step [85/191], loss=69.6749
	step [86/191], loss=79.7960
	step [87/191], loss=76.2697
	step [88/191], loss=59.7159
	step [89/191], loss=59.6228
	step [90/191], loss=71.7387
	step [91/191], loss=61.9318
	step [92/191], loss=66.3646
	step [93/191], loss=59.8417
	step [94/191], loss=82.2983
	step [95/191], loss=63.3776
	step [96/191], loss=65.7121
	step [97/191], loss=73.9652
	step [98/191], loss=70.6800
	step [99/191], loss=75.4003
	step [100/191], loss=63.6522
	step [101/191], loss=58.6883
	step [102/191], loss=67.4953
	step [103/191], loss=61.0759
	step [104/191], loss=58.3604
	step [105/191], loss=87.1200
	step [106/191], loss=83.6895
	step [107/191], loss=56.3476
	step [108/191], loss=82.0079
	step [109/191], loss=62.0547
	step [110/191], loss=71.2566
	step [111/191], loss=69.0791
	step [112/191], loss=71.6658
	step [113/191], loss=74.3811
	step [114/191], loss=62.2523
	step [115/191], loss=71.0865
	step [116/191], loss=84.6080
	step [117/191], loss=64.4549
	step [118/191], loss=60.5663
	step [119/191], loss=74.9319
	step [120/191], loss=65.7213
	step [121/191], loss=63.1740
	step [122/191], loss=77.4428
	step [123/191], loss=63.9318
	step [124/191], loss=67.4436
	step [125/191], loss=71.8860
	step [126/191], loss=70.4995
	step [127/191], loss=71.9772
	step [128/191], loss=75.4138
	step [129/191], loss=73.7537
	step [130/191], loss=53.2941
	step [131/191], loss=62.2248
	step [132/191], loss=61.0270
	step [133/191], loss=79.4296
	step [134/191], loss=70.3505
	step [135/191], loss=77.0182
	step [136/191], loss=67.9926
	step [137/191], loss=68.4201
	step [138/191], loss=68.4477
	step [139/191], loss=66.0721
	step [140/191], loss=70.7507
	step [141/191], loss=76.7234
	step [142/191], loss=77.4874
	step [143/191], loss=78.6933
	step [144/191], loss=61.3269
	step [145/191], loss=70.7161
	step [146/191], loss=78.7966
	step [147/191], loss=63.6930
	step [148/191], loss=61.2303
	step [149/191], loss=61.5588
	step [150/191], loss=62.3965
	step [151/191], loss=50.7164
	step [152/191], loss=74.1042
	step [153/191], loss=66.8673
	step [154/191], loss=69.5996
	step [155/191], loss=70.0635
	step [156/191], loss=67.0937
	step [157/191], loss=73.0300
	step [158/191], loss=73.4630
	step [159/191], loss=74.9360
	step [160/191], loss=71.0206
	step [161/191], loss=61.7234
	step [162/191], loss=70.1699
	step [163/191], loss=71.8601
	step [164/191], loss=65.0736
	step [165/191], loss=93.6344
	step [166/191], loss=72.3694
	step [167/191], loss=67.7958
	step [168/191], loss=72.0333
	step [169/191], loss=71.7517
	step [170/191], loss=67.9645
	step [171/191], loss=72.8316
	step [172/191], loss=82.0088
	step [173/191], loss=79.4219
	step [174/191], loss=74.1906
	step [175/191], loss=72.0833
	step [176/191], loss=69.2776
	step [177/191], loss=76.8686
	step [178/191], loss=50.0508
	step [179/191], loss=64.0832
	step [180/191], loss=61.4343
	step [181/191], loss=63.9885
	step [182/191], loss=66.8185
	step [183/191], loss=70.3899
	step [184/191], loss=61.3829
	step [185/191], loss=73.6665
	step [186/191], loss=63.7076
	step [187/191], loss=66.9852
	step [188/191], loss=81.6819
	step [189/191], loss=70.0784
	step [190/191], loss=66.6653
	step [191/191], loss=29.4508
	Evaluating
	loss=0.0073, precision=0.4662, recall=0.8648, f1=0.6058
saving model as: 1_saved_model.pth
Training epoch 74
	step [1/191], loss=71.1697
	step [2/191], loss=80.1818
	step [3/191], loss=73.8993
	step [4/191], loss=76.6301
	step [5/191], loss=66.2456
	step [6/191], loss=70.1410
	step [7/191], loss=74.4797
	step [8/191], loss=74.4008
	step [9/191], loss=79.6530
	step [10/191], loss=68.7143
	step [11/191], loss=64.6128
	step [12/191], loss=57.1486
	step [13/191], loss=68.0643
	step [14/191], loss=79.1628
	step [15/191], loss=70.0708
	step [16/191], loss=81.7749
	step [17/191], loss=74.0702
	step [18/191], loss=78.3984
	step [19/191], loss=70.3515
	step [20/191], loss=71.6128
	step [21/191], loss=68.4446
	step [22/191], loss=76.9673
	step [23/191], loss=66.3483
	step [24/191], loss=59.3629
	step [25/191], loss=72.7127
	step [26/191], loss=77.9497
	step [27/191], loss=71.7874
	step [28/191], loss=61.1579
	step [29/191], loss=60.5568
	step [30/191], loss=77.6865
	step [31/191], loss=73.1732
	step [32/191], loss=63.7450
	step [33/191], loss=66.4987
	step [34/191], loss=78.5139
	step [35/191], loss=70.2390
	step [36/191], loss=61.2416
	step [37/191], loss=70.5319
	step [38/191], loss=63.4479
	step [39/191], loss=58.6959
	step [40/191], loss=68.0697
	step [41/191], loss=72.9747
	step [42/191], loss=68.5200
	step [43/191], loss=78.5504
	step [44/191], loss=72.1358
	step [45/191], loss=71.0383
	step [46/191], loss=61.6137
	step [47/191], loss=60.2465
	step [48/191], loss=66.9054
	step [49/191], loss=77.8977
	step [50/191], loss=65.0361
	step [51/191], loss=63.1409
	step [52/191], loss=68.6186
	step [53/191], loss=65.4017
	step [54/191], loss=59.7275
	step [55/191], loss=68.5398
	step [56/191], loss=70.6573
	step [57/191], loss=75.5817
	step [58/191], loss=71.2099
	step [59/191], loss=65.1159
	step [60/191], loss=66.6271
	step [61/191], loss=63.6195
	step [62/191], loss=66.1036
	step [63/191], loss=74.1233
	step [64/191], loss=60.2690
	step [65/191], loss=62.3861
	step [66/191], loss=60.9115
	step [67/191], loss=57.9910
	step [68/191], loss=67.4821
	step [69/191], loss=64.3402
	step [70/191], loss=65.5109
	step [71/191], loss=66.0932
	step [72/191], loss=66.4583
	step [73/191], loss=68.2422
	step [74/191], loss=65.1977
	step [75/191], loss=55.8250
	step [76/191], loss=66.5996
	step [77/191], loss=74.5888
	step [78/191], loss=65.3901
	step [79/191], loss=75.7131
	step [80/191], loss=61.4340
	step [81/191], loss=72.1605
	step [82/191], loss=73.4749
	step [83/191], loss=77.2702
	step [84/191], loss=64.6153
	step [85/191], loss=55.4423
	step [86/191], loss=70.3644
	step [87/191], loss=62.8667
	step [88/191], loss=67.3436
	step [89/191], loss=67.3537
	step [90/191], loss=69.4682
	step [91/191], loss=65.8391
	step [92/191], loss=63.8341
	step [93/191], loss=66.6837
	step [94/191], loss=68.0225
	step [95/191], loss=65.9210
	step [96/191], loss=67.4266
	step [97/191], loss=65.9114
	step [98/191], loss=72.3060
	step [99/191], loss=73.3156
	step [100/191], loss=70.6691
	step [101/191], loss=77.1703
	step [102/191], loss=68.8197
	step [103/191], loss=71.4560
	step [104/191], loss=60.7470
	step [105/191], loss=62.7995
	step [106/191], loss=58.0420
	step [107/191], loss=59.3045
	step [108/191], loss=57.5601
	step [109/191], loss=56.9666
	step [110/191], loss=68.3083
	step [111/191], loss=68.0186
	step [112/191], loss=82.6290
	step [113/191], loss=72.1893
	step [114/191], loss=58.6157
	step [115/191], loss=63.8626
	step [116/191], loss=68.4582
	step [117/191], loss=65.6300
	step [118/191], loss=67.5640
	step [119/191], loss=73.9725
	step [120/191], loss=67.1562
	step [121/191], loss=64.1438
	step [122/191], loss=73.7889
	step [123/191], loss=66.1771
	step [124/191], loss=63.6249
	step [125/191], loss=62.6413
	step [126/191], loss=81.7597
	step [127/191], loss=74.0022
	step [128/191], loss=61.6646
	step [129/191], loss=80.2869
	step [130/191], loss=70.0485
	step [131/191], loss=69.4365
	step [132/191], loss=58.4318
	step [133/191], loss=69.6412
	step [134/191], loss=70.3452
	step [135/191], loss=61.5950
	step [136/191], loss=76.9214
	step [137/191], loss=62.5260
	step [138/191], loss=68.5307
	step [139/191], loss=68.4511
	step [140/191], loss=62.9874
	step [141/191], loss=69.0926
	step [142/191], loss=66.6951
	step [143/191], loss=65.5851
	step [144/191], loss=67.1997
	step [145/191], loss=63.9037
	step [146/191], loss=65.8496
	step [147/191], loss=64.1810
	step [148/191], loss=73.0457
	step [149/191], loss=60.8711
	step [150/191], loss=68.7377
	step [151/191], loss=65.7557
	step [152/191], loss=79.5533
	step [153/191], loss=63.6164
	step [154/191], loss=68.1112
	step [155/191], loss=66.9567
	step [156/191], loss=72.6818
	step [157/191], loss=62.1965
	step [158/191], loss=79.8304
	step [159/191], loss=80.3569
	step [160/191], loss=82.3205
	step [161/191], loss=68.7868
	step [162/191], loss=59.0014
	step [163/191], loss=70.8059
	step [164/191], loss=70.2733
	step [165/191], loss=68.7536
	step [166/191], loss=69.4452
	step [167/191], loss=66.2201
	step [168/191], loss=70.7433
	step [169/191], loss=69.9203
	step [170/191], loss=59.8819
	step [171/191], loss=69.9727
	step [172/191], loss=68.3975
	step [173/191], loss=67.2367
	step [174/191], loss=62.2241
	step [175/191], loss=73.4545
	step [176/191], loss=80.4668
	step [177/191], loss=63.9425
	step [178/191], loss=80.8109
	step [179/191], loss=71.7720
	step [180/191], loss=73.0186
	step [181/191], loss=63.8206
	step [182/191], loss=68.0576
	step [183/191], loss=83.3653
	step [184/191], loss=58.9703
	step [185/191], loss=69.1448
	step [186/191], loss=83.6159
	step [187/191], loss=62.2305
	step [188/191], loss=63.1057
	step [189/191], loss=85.3068
	step [190/191], loss=69.9707
	step [191/191], loss=35.2240
	Evaluating
	loss=0.0076, precision=0.4522, recall=0.8565, f1=0.5919
Training epoch 75
	step [1/191], loss=68.6889
	step [2/191], loss=69.6139
	step [3/191], loss=75.1808
	step [4/191], loss=78.5678
	step [5/191], loss=80.0177
	step [6/191], loss=55.6027
	step [7/191], loss=71.2513
	step [8/191], loss=62.7261
	step [9/191], loss=64.5983
	step [10/191], loss=82.0394
	step [11/191], loss=65.9602
	step [12/191], loss=69.5879
	step [13/191], loss=66.3851
	step [14/191], loss=68.6241
	step [15/191], loss=75.3473
	step [16/191], loss=67.3568
	step [17/191], loss=65.4496
	step [18/191], loss=71.5874
	step [19/191], loss=66.8645
	step [20/191], loss=74.1336
	step [21/191], loss=84.9171
	step [22/191], loss=53.2876
	step [23/191], loss=68.7677
	step [24/191], loss=66.7507
	step [25/191], loss=76.1241
	step [26/191], loss=68.1730
	step [27/191], loss=69.2454
	step [28/191], loss=82.0163
	step [29/191], loss=64.5046
	step [30/191], loss=66.5559
	step [31/191], loss=76.1064
	step [32/191], loss=59.5728
	step [33/191], loss=63.2377
	step [34/191], loss=65.5918
	step [35/191], loss=69.9330
	step [36/191], loss=64.6446
	step [37/191], loss=66.9890
	step [38/191], loss=80.0119
	step [39/191], loss=58.8721
	step [40/191], loss=63.7321
	step [41/191], loss=60.3415
	step [42/191], loss=70.1275
	step [43/191], loss=68.3054
	step [44/191], loss=66.7775
	step [45/191], loss=67.6440
	step [46/191], loss=61.8974
	step [47/191], loss=70.9854
	step [48/191], loss=73.6075
	step [49/191], loss=81.3380
	step [50/191], loss=67.7870
	step [51/191], loss=61.4183
	step [52/191], loss=69.1838
	step [53/191], loss=62.1207
	step [54/191], loss=69.8463
	step [55/191], loss=69.1477
	step [56/191], loss=81.0598
	step [57/191], loss=55.4585
	step [58/191], loss=60.2752
	step [59/191], loss=65.1079
	step [60/191], loss=65.1828
	step [61/191], loss=59.9718
	step [62/191], loss=62.5338
	step [63/191], loss=65.4300
	step [64/191], loss=70.7558
	step [65/191], loss=70.6489
	step [66/191], loss=60.3231
	step [67/191], loss=67.0374
	step [68/191], loss=62.2008
	step [69/191], loss=75.4463
	step [70/191], loss=71.7676
	step [71/191], loss=65.2436
	step [72/191], loss=75.8186
	step [73/191], loss=75.3026
	step [74/191], loss=79.4775
	step [75/191], loss=68.0688
	step [76/191], loss=70.6149
	step [77/191], loss=65.9148
	step [78/191], loss=67.5965
	step [79/191], loss=68.2102
	step [80/191], loss=69.4794
	step [81/191], loss=65.2533
	step [82/191], loss=73.6124
	step [83/191], loss=51.9928
	step [84/191], loss=69.1056
	step [85/191], loss=61.6629
	step [86/191], loss=55.0635
	step [87/191], loss=67.2496
	step [88/191], loss=75.3935
	step [89/191], loss=68.3326
	step [90/191], loss=63.5300
	step [91/191], loss=80.1204
	step [92/191], loss=65.1324
	step [93/191], loss=86.7774
	step [94/191], loss=77.5170
	step [95/191], loss=63.7130
	step [96/191], loss=74.7319
	step [97/191], loss=73.8240
	step [98/191], loss=69.5549
	step [99/191], loss=82.9423
	step [100/191], loss=62.4013
	step [101/191], loss=62.4095
	step [102/191], loss=68.0042
	step [103/191], loss=74.7028
	step [104/191], loss=62.3203
	step [105/191], loss=71.6196
	step [106/191], loss=66.5810
	step [107/191], loss=71.1047
	step [108/191], loss=56.5054
	step [109/191], loss=74.4113
	step [110/191], loss=73.1786
	step [111/191], loss=72.4986
	step [112/191], loss=77.7070
	step [113/191], loss=60.7717
	step [114/191], loss=65.4582
	step [115/191], loss=71.4111
	step [116/191], loss=63.8339
	step [117/191], loss=73.7277
	step [118/191], loss=74.1190
	step [119/191], loss=53.3040
	step [120/191], loss=76.3840
	step [121/191], loss=77.7633
	step [122/191], loss=68.8045
	step [123/191], loss=73.4963
	step [124/191], loss=65.4782
	step [125/191], loss=68.8295
	step [126/191], loss=70.5201
	step [127/191], loss=75.4738
	step [128/191], loss=75.1668
	step [129/191], loss=66.7400
	step [130/191], loss=64.2965
	step [131/191], loss=63.7762
	step [132/191], loss=62.2337
	step [133/191], loss=77.7238
	step [134/191], loss=61.5989
	step [135/191], loss=60.2058
	step [136/191], loss=60.1289
	step [137/191], loss=61.7413
	step [138/191], loss=69.3117
	step [139/191], loss=61.8146
	step [140/191], loss=73.1083
	step [141/191], loss=67.3136
	step [142/191], loss=64.4632
	step [143/191], loss=75.9839
	step [144/191], loss=77.1776
	step [145/191], loss=62.4596
	step [146/191], loss=64.7308
	step [147/191], loss=71.8666
	step [148/191], loss=62.8904
	step [149/191], loss=54.7826
	step [150/191], loss=73.8730
	step [151/191], loss=73.0966
	step [152/191], loss=62.7579
	step [153/191], loss=58.9218
	step [154/191], loss=59.4799
	step [155/191], loss=63.1178
	step [156/191], loss=82.2433
	step [157/191], loss=73.8987
	step [158/191], loss=70.0472
	step [159/191], loss=62.8774
	step [160/191], loss=70.4673
	step [161/191], loss=64.9153
	step [162/191], loss=68.9816
	step [163/191], loss=63.0362
	step [164/191], loss=71.5639
	step [165/191], loss=55.2429
	step [166/191], loss=50.3945
	step [167/191], loss=60.4318
	step [168/191], loss=63.9527
	step [169/191], loss=64.6123
	step [170/191], loss=70.2232
	step [171/191], loss=68.1741
	step [172/191], loss=73.7910
	step [173/191], loss=58.2824
	step [174/191], loss=75.2540
	step [175/191], loss=71.0120
	step [176/191], loss=63.4647
	step [177/191], loss=64.5207
	step [178/191], loss=72.6470
	step [179/191], loss=83.7108
	step [180/191], loss=66.5737
	step [181/191], loss=65.9449
	step [182/191], loss=62.2563
	step [183/191], loss=85.9971
	step [184/191], loss=80.7875
	step [185/191], loss=68.0786
	step [186/191], loss=72.6835
	step [187/191], loss=68.1560
	step [188/191], loss=74.2137
	step [189/191], loss=55.3270
	step [190/191], loss=72.6619
	step [191/191], loss=29.8841
	Evaluating
	loss=0.0095, precision=0.3748, recall=0.8647, f1=0.5229
Training epoch 76
	step [1/191], loss=71.2805
	step [2/191], loss=54.5785
	step [3/191], loss=67.9579
	step [4/191], loss=70.3811
	step [5/191], loss=64.9291
	step [6/191], loss=69.0956
	step [7/191], loss=67.6149
	step [8/191], loss=69.7155
	step [9/191], loss=56.6623
	step [10/191], loss=74.2520
	step [11/191], loss=80.9431
	step [12/191], loss=57.8856
	step [13/191], loss=66.1120
	step [14/191], loss=62.7827
	step [15/191], loss=72.9217
	step [16/191], loss=69.8403
	step [17/191], loss=65.1237
	step [18/191], loss=66.6473
	step [19/191], loss=72.3529
	step [20/191], loss=69.2747
	step [21/191], loss=75.4157
	step [22/191], loss=78.7173
	step [23/191], loss=73.7430
	step [24/191], loss=67.1857
	step [25/191], loss=64.9479
	step [26/191], loss=63.8710
	step [27/191], loss=62.1163
	step [28/191], loss=65.9747
	step [29/191], loss=69.8400
	step [30/191], loss=60.7691
	step [31/191], loss=69.1407
	step [32/191], loss=75.7617
	step [33/191], loss=68.7875
	step [34/191], loss=85.5415
	step [35/191], loss=72.3103
	step [36/191], loss=77.0782
	step [37/191], loss=69.3023
	step [38/191], loss=66.3663
	step [39/191], loss=62.4150
	step [40/191], loss=69.3158
	step [41/191], loss=70.5824
	step [42/191], loss=57.1033
	step [43/191], loss=72.1531
	step [44/191], loss=66.7641
	step [45/191], loss=83.4282
	step [46/191], loss=65.6487
	step [47/191], loss=56.8989
	step [48/191], loss=54.0208
	step [49/191], loss=53.5247
	step [50/191], loss=55.0773
	step [51/191], loss=73.9379
	step [52/191], loss=62.3387
	step [53/191], loss=72.6031
	step [54/191], loss=62.7622
	step [55/191], loss=73.1598
	step [56/191], loss=61.7967
	step [57/191], loss=72.3153
	step [58/191], loss=61.3523
	step [59/191], loss=61.0613
	step [60/191], loss=71.7996
	step [61/191], loss=60.0761
	step [62/191], loss=69.0351
	step [63/191], loss=62.7826
	step [64/191], loss=54.0053
	step [65/191], loss=67.4514
	step [66/191], loss=68.8618
	step [67/191], loss=81.6555
	step [68/191], loss=83.5228
	step [69/191], loss=66.6743
	step [70/191], loss=59.4263
	step [71/191], loss=66.0892
	step [72/191], loss=79.1295
	step [73/191], loss=75.4323
	step [74/191], loss=59.1269
	step [75/191], loss=68.7782
	step [76/191], loss=70.6583
	step [77/191], loss=71.3121
	step [78/191], loss=65.3264
	step [79/191], loss=73.5530
	step [80/191], loss=73.3769
	step [81/191], loss=79.0614
	step [82/191], loss=71.4761
	step [83/191], loss=68.0038
	step [84/191], loss=65.8713
	step [85/191], loss=59.7153
	step [86/191], loss=80.0032
	step [87/191], loss=69.1838
	step [88/191], loss=66.9543
	step [89/191], loss=64.0377
	step [90/191], loss=62.6092
	step [91/191], loss=55.9050
	step [92/191], loss=72.4906
	step [93/191], loss=68.0982
	step [94/191], loss=66.7194
	step [95/191], loss=69.0530
	step [96/191], loss=61.9987
	step [97/191], loss=65.2938
	step [98/191], loss=60.3142
	step [99/191], loss=75.8748
	step [100/191], loss=75.7299
	step [101/191], loss=67.1908
	step [102/191], loss=74.2205
	step [103/191], loss=62.6280
	step [104/191], loss=68.8958
	step [105/191], loss=63.6538
	step [106/191], loss=64.1684
	step [107/191], loss=69.7865
	step [108/191], loss=57.8541
	step [109/191], loss=59.9859
	step [110/191], loss=60.9152
	step [111/191], loss=71.9005
	step [112/191], loss=61.1852
	step [113/191], loss=78.1383
	step [114/191], loss=80.5739
	step [115/191], loss=69.9111
	step [116/191], loss=75.7369
	step [117/191], loss=67.4677
	step [118/191], loss=68.6569
	step [119/191], loss=70.4840
	step [120/191], loss=80.5789
	step [121/191], loss=64.4590
	step [122/191], loss=65.2167
	step [123/191], loss=63.8300
	step [124/191], loss=54.7124
	step [125/191], loss=77.2655
	step [126/191], loss=70.7816
	step [127/191], loss=79.8337
	step [128/191], loss=69.7248
	step [129/191], loss=69.3849
	step [130/191], loss=71.7650
	step [131/191], loss=58.2647
	step [132/191], loss=70.8169
	step [133/191], loss=63.0863
	step [134/191], loss=66.6976
	step [135/191], loss=74.1960
	step [136/191], loss=69.1351
	step [137/191], loss=69.1957
	step [138/191], loss=59.5184
	step [139/191], loss=63.5583
	step [140/191], loss=74.0998
	step [141/191], loss=56.5934
	step [142/191], loss=66.5130
	step [143/191], loss=80.8632
	step [144/191], loss=58.2477
	step [145/191], loss=49.5419
	step [146/191], loss=85.6720
	step [147/191], loss=62.5727
	step [148/191], loss=76.0005
	step [149/191], loss=64.4668
	step [150/191], loss=58.5657
	step [151/191], loss=76.6234
	step [152/191], loss=73.4212
	step [153/191], loss=63.8102
	step [154/191], loss=63.0182
	step [155/191], loss=66.1315
	step [156/191], loss=62.2933
	step [157/191], loss=68.1268
	step [158/191], loss=80.1289
	step [159/191], loss=72.7111
	step [160/191], loss=71.5758
	step [161/191], loss=75.1621
	step [162/191], loss=79.3134
	step [163/191], loss=54.8675
	step [164/191], loss=60.1706
	step [165/191], loss=79.2346
	step [166/191], loss=76.4458
	step [167/191], loss=58.5503
	step [168/191], loss=74.7940
	step [169/191], loss=65.0352
	step [170/191], loss=74.5058
	step [171/191], loss=76.5906
	step [172/191], loss=69.0221
	step [173/191], loss=76.7847
	step [174/191], loss=63.2038
	step [175/191], loss=64.0841
	step [176/191], loss=69.7140
	step [177/191], loss=71.6641
	step [178/191], loss=69.1088
	step [179/191], loss=64.4758
	step [180/191], loss=72.4362
	step [181/191], loss=64.4924
	step [182/191], loss=64.8801
	step [183/191], loss=82.5574
	step [184/191], loss=71.2520
	step [185/191], loss=66.8601
	step [186/191], loss=81.1300
	step [187/191], loss=65.9651
	step [188/191], loss=78.6267
	step [189/191], loss=65.7113
	step [190/191], loss=70.2575
	step [191/191], loss=32.9337
	Evaluating
	loss=0.0067, precision=0.4960, recall=0.8515, f1=0.6269
saving model as: 1_saved_model.pth
Training epoch 77
	step [1/191], loss=66.2990
	step [2/191], loss=61.2576
	step [3/191], loss=75.6542
	step [4/191], loss=66.2458
	step [5/191], loss=87.7107
	step [6/191], loss=58.7416
	step [7/191], loss=71.1913
	step [8/191], loss=64.0902
	step [9/191], loss=79.4312
	step [10/191], loss=68.0137
	step [11/191], loss=67.2244
	step [12/191], loss=61.0081
	step [13/191], loss=60.8826
	step [14/191], loss=61.1791
	step [15/191], loss=75.8161
	step [16/191], loss=69.7547
	step [17/191], loss=67.8931
	step [18/191], loss=67.4994
	step [19/191], loss=67.5175
	step [20/191], loss=65.6831
	step [21/191], loss=70.6190
	step [22/191], loss=61.8034
	step [23/191], loss=59.8213
	step [24/191], loss=77.5823
	step [25/191], loss=55.5246
	step [26/191], loss=66.9278
	step [27/191], loss=68.1036
	step [28/191], loss=60.6389
	step [29/191], loss=70.2228
	step [30/191], loss=77.1539
	step [31/191], loss=76.6984
	step [32/191], loss=65.3087
	step [33/191], loss=79.7916
	step [34/191], loss=70.1549
	step [35/191], loss=61.4723
	step [36/191], loss=60.0788
	step [37/191], loss=62.6897
	step [38/191], loss=74.0574
	step [39/191], loss=72.9646
	step [40/191], loss=61.1726
	step [41/191], loss=71.1203
	step [42/191], loss=74.7907
	step [43/191], loss=61.2202
	step [44/191], loss=62.9946
	step [45/191], loss=64.0412
	step [46/191], loss=64.8380
	step [47/191], loss=65.3649
	step [48/191], loss=74.2801
	step [49/191], loss=77.5254
	step [50/191], loss=68.7640
	step [51/191], loss=65.5831
	step [52/191], loss=54.5478
	step [53/191], loss=63.4685
	step [54/191], loss=73.7166
	step [55/191], loss=79.2254
	step [56/191], loss=64.0031
	step [57/191], loss=69.8354
	step [58/191], loss=68.0681
	step [59/191], loss=63.8525
	step [60/191], loss=59.9909
	step [61/191], loss=58.7314
	step [62/191], loss=67.4340
	step [63/191], loss=73.4613
	step [64/191], loss=67.7134
	step [65/191], loss=70.4580
	step [66/191], loss=61.7774
	step [67/191], loss=69.5434
	step [68/191], loss=60.7739
	step [69/191], loss=61.9082
	step [70/191], loss=65.3694
	step [71/191], loss=75.0881
	step [72/191], loss=77.7920
	step [73/191], loss=65.9831
	step [74/191], loss=64.0792
	step [75/191], loss=68.0354
	step [76/191], loss=65.7576
	step [77/191], loss=82.5288
	step [78/191], loss=61.8305
	step [79/191], loss=59.5153
	step [80/191], loss=75.3649
	step [81/191], loss=59.8973
	step [82/191], loss=62.7005
	step [83/191], loss=76.4978
	step [84/191], loss=70.2230
	step [85/191], loss=72.1929
	step [86/191], loss=66.7906
	step [87/191], loss=67.5979
	step [88/191], loss=61.9873
	step [89/191], loss=75.1068
	step [90/191], loss=72.3655
	step [91/191], loss=79.3462
	step [92/191], loss=66.5527
	step [93/191], loss=63.4723
	step [94/191], loss=68.7597
	step [95/191], loss=59.6364
	step [96/191], loss=59.5648
	step [97/191], loss=66.7841
	step [98/191], loss=63.2260
	step [99/191], loss=76.2263
	step [100/191], loss=66.4108
	step [101/191], loss=59.3684
	step [102/191], loss=64.1449
	step [103/191], loss=65.0124
	step [104/191], loss=66.3806
	step [105/191], loss=59.9800
	step [106/191], loss=62.8452
	step [107/191], loss=76.8729
	step [108/191], loss=61.0537
	step [109/191], loss=86.6015
	step [110/191], loss=69.9961
	step [111/191], loss=68.9304
	step [112/191], loss=69.7346
	step [113/191], loss=61.3272
	step [114/191], loss=72.3038
	step [115/191], loss=64.4597
	step [116/191], loss=70.7396
	step [117/191], loss=60.8110
	step [118/191], loss=65.0440
	step [119/191], loss=64.7603
	step [120/191], loss=57.3692
	step [121/191], loss=65.7202
	step [122/191], loss=62.4408
	step [123/191], loss=63.2091
	step [124/191], loss=74.1789
	step [125/191], loss=72.6649
	step [126/191], loss=59.0311
	step [127/191], loss=66.9647
	step [128/191], loss=86.5855
	step [129/191], loss=65.6287
	step [130/191], loss=55.4343
	step [131/191], loss=69.9742
	step [132/191], loss=71.6351
	step [133/191], loss=56.6989
	step [134/191], loss=59.0269
	step [135/191], loss=69.6981
	step [136/191], loss=59.0393
	step [137/191], loss=63.6511
	step [138/191], loss=60.0585
	step [139/191], loss=66.5873
	step [140/191], loss=70.4066
	step [141/191], loss=76.5396
	step [142/191], loss=68.8647
	step [143/191], loss=64.8572
	step [144/191], loss=62.0340
	step [145/191], loss=84.1758
	step [146/191], loss=67.4933
	step [147/191], loss=63.8591
	step [148/191], loss=63.0889
	step [149/191], loss=67.4507
	step [150/191], loss=73.8856
	step [151/191], loss=72.3901
	step [152/191], loss=76.4037
	step [153/191], loss=77.1722
	step [154/191], loss=66.7005
	step [155/191], loss=63.7546
	step [156/191], loss=84.6039
	step [157/191], loss=66.7542
	step [158/191], loss=69.4494
	step [159/191], loss=63.8373
	step [160/191], loss=74.8318
	step [161/191], loss=81.9244
	step [162/191], loss=55.6674
	step [163/191], loss=60.5178
	step [164/191], loss=75.5027
	step [165/191], loss=71.7161
	step [166/191], loss=64.5307
	step [167/191], loss=71.5608
	step [168/191], loss=70.8603
	step [169/191], loss=65.1531
	step [170/191], loss=60.6058
	step [171/191], loss=66.8544
	step [172/191], loss=71.1479
	step [173/191], loss=80.5177
	step [174/191], loss=83.3667
	step [175/191], loss=63.8294
	step [176/191], loss=74.4853
	step [177/191], loss=62.0061
	step [178/191], loss=61.7457
	step [179/191], loss=74.3800
	step [180/191], loss=80.6810
	step [181/191], loss=66.6032
	step [182/191], loss=63.5296
	step [183/191], loss=64.7918
	step [184/191], loss=76.0985
	step [185/191], loss=65.4165
	step [186/191], loss=61.4784
	step [187/191], loss=64.7610
	step [188/191], loss=77.2075
	step [189/191], loss=57.3419
	step [190/191], loss=76.8209
	step [191/191], loss=37.5216
	Evaluating
	loss=0.0083, precision=0.4152, recall=0.8665, f1=0.5614
Training epoch 78
	step [1/191], loss=58.9702
	step [2/191], loss=78.2224
	step [3/191], loss=76.4305
	step [4/191], loss=68.7080
	step [5/191], loss=66.3415
	step [6/191], loss=71.7764
	step [7/191], loss=62.5591
	step [8/191], loss=60.3673
	step [9/191], loss=73.2084
	step [10/191], loss=82.8859
	step [11/191], loss=62.7001
	step [12/191], loss=77.6586
	step [13/191], loss=64.6251
	step [14/191], loss=71.9886
	step [15/191], loss=69.2436
	step [16/191], loss=77.2519
	step [17/191], loss=72.6532
	step [18/191], loss=59.0083
	step [19/191], loss=66.7808
	step [20/191], loss=64.1257
	step [21/191], loss=66.2783
	step [22/191], loss=65.6261
	step [23/191], loss=63.9991
	step [24/191], loss=66.4387
	step [25/191], loss=60.6596
	step [26/191], loss=75.1749
	step [27/191], loss=73.1932
	step [28/191], loss=59.1871
	step [29/191], loss=71.1499
	step [30/191], loss=63.2947
	step [31/191], loss=70.8117
	step [32/191], loss=73.8903
	step [33/191], loss=62.8470
	step [34/191], loss=60.8146
	step [35/191], loss=66.0062
	step [36/191], loss=61.2102
	step [37/191], loss=65.6795
	step [38/191], loss=73.2544
	step [39/191], loss=68.5357
	step [40/191], loss=64.2146
	step [41/191], loss=55.3229
	step [42/191], loss=77.4911
	step [43/191], loss=70.4460
	step [44/191], loss=76.0062
	step [45/191], loss=75.2312
	step [46/191], loss=64.6938
	step [47/191], loss=66.5732
	step [48/191], loss=67.9104
	step [49/191], loss=60.3275
	step [50/191], loss=76.0616
	step [51/191], loss=70.0113
	step [52/191], loss=69.7621
	step [53/191], loss=66.9530
	step [54/191], loss=52.6327
	step [55/191], loss=60.2490
	step [56/191], loss=66.8242
	step [57/191], loss=62.9060
	step [58/191], loss=64.1466
	step [59/191], loss=74.0678
	step [60/191], loss=65.0020
	step [61/191], loss=67.0613
	step [62/191], loss=70.2380
	step [63/191], loss=61.1903
	step [64/191], loss=73.0263
	step [65/191], loss=68.2576
	step [66/191], loss=68.8452
	step [67/191], loss=77.5472
	step [68/191], loss=71.2000
	step [69/191], loss=79.0100
	step [70/191], loss=70.7480
	step [71/191], loss=72.5745
	step [72/191], loss=66.1086
	step [73/191], loss=59.5678
	step [74/191], loss=76.5064
	step [75/191], loss=62.7253
	step [76/191], loss=81.0251
	step [77/191], loss=60.0363
	step [78/191], loss=61.1574
	step [79/191], loss=69.3789
	step [80/191], loss=55.9877
	step [81/191], loss=61.8906
	step [82/191], loss=69.0762
	step [83/191], loss=72.7531
	step [84/191], loss=70.8772
	step [85/191], loss=58.3700
	step [86/191], loss=70.0585
	step [87/191], loss=70.2880
	step [88/191], loss=75.3740
	step [89/191], loss=57.9302
	step [90/191], loss=74.5145
	step [91/191], loss=62.3794
	step [92/191], loss=67.0759
	step [93/191], loss=80.0389
	step [94/191], loss=68.2879
	step [95/191], loss=67.6060
	step [96/191], loss=64.8582
	step [97/191], loss=57.6837
	step [98/191], loss=67.1543
	step [99/191], loss=58.5607
	step [100/191], loss=73.0362
	step [101/191], loss=72.1046
	step [102/191], loss=66.9352
	step [103/191], loss=64.3856
	step [104/191], loss=70.4366
	step [105/191], loss=61.8690
	step [106/191], loss=76.6540
	step [107/191], loss=59.4727
	step [108/191], loss=73.8681
	step [109/191], loss=81.6459
	step [110/191], loss=71.2217
	step [111/191], loss=63.5888
	step [112/191], loss=68.9575
	step [113/191], loss=72.6504
	step [114/191], loss=80.4358
	step [115/191], loss=54.0899
	step [116/191], loss=69.4617
	step [117/191], loss=55.5843
	step [118/191], loss=64.0645
	step [119/191], loss=70.5371
	step [120/191], loss=66.7296
	step [121/191], loss=66.7300
	step [122/191], loss=64.0995
	step [123/191], loss=64.3817
	step [124/191], loss=57.8071
	step [125/191], loss=73.9642
	step [126/191], loss=68.6097
	step [127/191], loss=77.0902
	step [128/191], loss=77.4883
	step [129/191], loss=67.9498
	step [130/191], loss=76.2920
	step [131/191], loss=77.3632
	step [132/191], loss=76.1559
	step [133/191], loss=54.9361
	step [134/191], loss=66.4599
	step [135/191], loss=57.3403
	step [136/191], loss=61.4286
	step [137/191], loss=68.8613
	step [138/191], loss=66.2797
	step [139/191], loss=66.2883
	step [140/191], loss=62.9514
	step [141/191], loss=64.2647
	step [142/191], loss=69.1508
	step [143/191], loss=71.0992
	step [144/191], loss=63.0486
	step [145/191], loss=73.6651
	step [146/191], loss=64.9451
	step [147/191], loss=64.9905
	step [148/191], loss=57.5365
	step [149/191], loss=79.3196
	step [150/191], loss=61.0174
	step [151/191], loss=70.2003
	step [152/191], loss=73.0685
	step [153/191], loss=76.7963
	step [154/191], loss=82.1704
	step [155/191], loss=57.1375
	step [156/191], loss=65.2282
	step [157/191], loss=69.5713
	step [158/191], loss=72.0104
	step [159/191], loss=68.5425
	step [160/191], loss=75.6754
	step [161/191], loss=67.6468
	step [162/191], loss=68.4534
	step [163/191], loss=80.9403
	step [164/191], loss=72.8167
	step [165/191], loss=69.6457
	step [166/191], loss=63.8746
	step [167/191], loss=75.9106
	step [168/191], loss=69.4087
	step [169/191], loss=62.8723
	step [170/191], loss=69.8567
	step [171/191], loss=59.7168
	step [172/191], loss=56.6097
	step [173/191], loss=72.3633
	step [174/191], loss=60.6556
	step [175/191], loss=68.8129
	step [176/191], loss=68.0394
	step [177/191], loss=67.5640
	step [178/191], loss=67.1082
	step [179/191], loss=58.7450
	step [180/191], loss=64.9517
	step [181/191], loss=64.1898
	step [182/191], loss=78.5707
	step [183/191], loss=70.4405
	step [184/191], loss=66.8774
	step [185/191], loss=67.1639
	step [186/191], loss=64.1368
	step [187/191], loss=66.9697
	step [188/191], loss=57.6104
	step [189/191], loss=68.4442
	step [190/191], loss=64.8573
	step [191/191], loss=35.0823
	Evaluating
	loss=0.0069, precision=0.4775, recall=0.8537, f1=0.6125
Training epoch 79
	step [1/191], loss=76.1472
	step [2/191], loss=69.3013
	step [3/191], loss=73.0125
	step [4/191], loss=76.6803
	step [5/191], loss=62.3822
	step [6/191], loss=70.9262
	step [7/191], loss=62.1753
	step [8/191], loss=70.0867
	step [9/191], loss=68.3411
	step [10/191], loss=64.0220
	step [11/191], loss=78.7045
	step [12/191], loss=58.0316
	step [13/191], loss=67.3067
	step [14/191], loss=81.1001
	step [15/191], loss=48.3310
	step [16/191], loss=69.4883
	step [17/191], loss=65.5363
	step [18/191], loss=66.7873
	step [19/191], loss=69.1403
	step [20/191], loss=55.2163
	step [21/191], loss=56.4150
	step [22/191], loss=72.7465
	step [23/191], loss=71.9984
	step [24/191], loss=83.7323
	step [25/191], loss=69.0106
	step [26/191], loss=66.4120
	step [27/191], loss=67.0617
	step [28/191], loss=70.1789
	step [29/191], loss=60.7971
	step [30/191], loss=66.1211
	step [31/191], loss=68.9233
	step [32/191], loss=69.8073
	step [33/191], loss=65.5957
	step [34/191], loss=82.2542
	step [35/191], loss=61.8409
	step [36/191], loss=57.7174
	step [37/191], loss=61.9698
	step [38/191], loss=68.7204
	step [39/191], loss=68.6441
	step [40/191], loss=68.5274
	step [41/191], loss=63.8865
	step [42/191], loss=71.3009
	step [43/191], loss=68.0224
	step [44/191], loss=68.7627
	step [45/191], loss=70.5703
	step [46/191], loss=66.2534
	step [47/191], loss=70.6854
	step [48/191], loss=71.0732
	step [49/191], loss=69.5092
	step [50/191], loss=76.0273
	step [51/191], loss=55.3493
	step [52/191], loss=69.6772
	step [53/191], loss=60.3143
	step [54/191], loss=64.0792
	step [55/191], loss=70.7186
	step [56/191], loss=68.5167
	step [57/191], loss=58.6158
	step [58/191], loss=61.6949
	step [59/191], loss=67.8180
	step [60/191], loss=61.9058
	step [61/191], loss=64.7881
	step [62/191], loss=65.4043
	step [63/191], loss=61.1617
	step [64/191], loss=75.1657
	step [65/191], loss=61.4015
	step [66/191], loss=67.5353
	step [67/191], loss=58.6656
	step [68/191], loss=58.9614
	step [69/191], loss=67.5047
	step [70/191], loss=83.6744
	step [71/191], loss=75.5415
	step [72/191], loss=69.5763
	step [73/191], loss=66.5811
	step [74/191], loss=67.3158
	step [75/191], loss=77.6162
	step [76/191], loss=60.0872
	step [77/191], loss=58.8910
	step [78/191], loss=60.6545
	step [79/191], loss=76.7894
	step [80/191], loss=69.3944
	step [81/191], loss=68.7400
	step [82/191], loss=69.9319
	step [83/191], loss=61.9857
	step [84/191], loss=61.0152
	step [85/191], loss=64.5268
	step [86/191], loss=77.3338
	step [87/191], loss=66.6442
	step [88/191], loss=59.3863
	step [89/191], loss=63.9586
	step [90/191], loss=59.9231
	step [91/191], loss=69.3213
	step [92/191], loss=68.4269
	step [93/191], loss=74.1484
	step [94/191], loss=72.1181
	step [95/191], loss=66.9425
	step [96/191], loss=53.8939
	step [97/191], loss=67.8381
	step [98/191], loss=70.4907
	step [99/191], loss=67.6143
	step [100/191], loss=58.0894
	step [101/191], loss=70.7374
	step [102/191], loss=80.0815
	step [103/191], loss=67.8722
	step [104/191], loss=72.2104
	step [105/191], loss=64.1228
	step [106/191], loss=66.9222
	step [107/191], loss=81.4478
	step [108/191], loss=66.5945
	step [109/191], loss=70.8310
	step [110/191], loss=62.7912
	step [111/191], loss=71.8827
	step [112/191], loss=71.7327
	step [113/191], loss=66.7155
	step [114/191], loss=71.6606
	step [115/191], loss=71.9674
	step [116/191], loss=59.7708
	step [117/191], loss=63.8030
	step [118/191], loss=84.2529
	step [119/191], loss=68.3031
	step [120/191], loss=67.8252
	step [121/191], loss=72.8731
	step [122/191], loss=75.5216
	step [123/191], loss=72.6333
	step [124/191], loss=73.3958
	step [125/191], loss=68.0709
	step [126/191], loss=58.1636
	step [127/191], loss=59.2677
	step [128/191], loss=70.9240
	step [129/191], loss=72.1747
	step [130/191], loss=56.6547
	step [131/191], loss=64.7442
	step [132/191], loss=63.7637
	step [133/191], loss=69.5266
	step [134/191], loss=63.3070
	step [135/191], loss=54.4764
	step [136/191], loss=80.7335
	step [137/191], loss=64.6583
	step [138/191], loss=66.1376
	step [139/191], loss=80.4926
	step [140/191], loss=61.7364
	step [141/191], loss=71.5842
	step [142/191], loss=61.9558
	step [143/191], loss=74.4758
	step [144/191], loss=71.0701
	step [145/191], loss=54.1329
	step [146/191], loss=62.6172
	step [147/191], loss=69.8334
	step [148/191], loss=68.2514
	step [149/191], loss=69.9583
	step [150/191], loss=65.9028
	step [151/191], loss=74.9104
	step [152/191], loss=77.3166
	step [153/191], loss=55.6727
	step [154/191], loss=68.2458
	step [155/191], loss=61.1036
	step [156/191], loss=64.7850
	step [157/191], loss=64.4174
	step [158/191], loss=77.7186
	step [159/191], loss=58.7976
	step [160/191], loss=67.8326
	step [161/191], loss=69.3774
	step [162/191], loss=70.4530
	step [163/191], loss=75.7407
	step [164/191], loss=81.0298
	step [165/191], loss=63.4899
	step [166/191], loss=63.2264
	step [167/191], loss=65.5112
	step [168/191], loss=60.5415
	step [169/191], loss=71.3101
	step [170/191], loss=61.2221
	step [171/191], loss=82.3647
	step [172/191], loss=59.9581
	step [173/191], loss=63.8892
	step [174/191], loss=50.0162
	step [175/191], loss=63.3039
	step [176/191], loss=63.8089
	step [177/191], loss=67.4893
	step [178/191], loss=70.3336
	step [179/191], loss=67.6444
	step [180/191], loss=72.2453
	step [181/191], loss=69.1246
	step [182/191], loss=58.8433
	step [183/191], loss=56.0024
	step [184/191], loss=62.4196
	step [185/191], loss=68.2311
	step [186/191], loss=73.2288
	step [187/191], loss=62.5790
	step [188/191], loss=67.4987
	step [189/191], loss=75.6228
	step [190/191], loss=64.7901
	step [191/191], loss=28.9197
	Evaluating
	loss=0.0074, precision=0.4450, recall=0.8499, f1=0.5841
Training epoch 80
	step [1/191], loss=71.2312
	step [2/191], loss=78.2434
	step [3/191], loss=63.2818
	step [4/191], loss=74.1593
	step [5/191], loss=64.5315
	step [6/191], loss=69.4740
	step [7/191], loss=67.3982
	step [8/191], loss=53.8949
	step [9/191], loss=68.5104
	step [10/191], loss=63.3372
	step [11/191], loss=71.8571
	step [12/191], loss=62.1354
	step [13/191], loss=59.8122
	step [14/191], loss=63.7356
	step [15/191], loss=69.9004
	step [16/191], loss=76.5544
	step [17/191], loss=62.8022
	step [18/191], loss=73.2048
	step [19/191], loss=67.1812
	step [20/191], loss=64.3756
	step [21/191], loss=69.2759
	step [22/191], loss=70.3284
	step [23/191], loss=64.0435
	step [24/191], loss=69.3122
	step [25/191], loss=64.2986
	step [26/191], loss=69.9767
	step [27/191], loss=70.6414
	step [28/191], loss=64.4538
	step [29/191], loss=74.9499
	step [30/191], loss=73.6729
	step [31/191], loss=67.5086
	step [32/191], loss=58.7564
	step [33/191], loss=58.0117
	step [34/191], loss=71.0285
	step [35/191], loss=65.0373
	step [36/191], loss=66.8474
	step [37/191], loss=68.6225
	step [38/191], loss=56.1235
	step [39/191], loss=69.8982
	step [40/191], loss=64.3583
	step [41/191], loss=67.7560
	step [42/191], loss=65.5925
	step [43/191], loss=59.5458
	step [44/191], loss=62.1804
	step [45/191], loss=61.5212
	step [46/191], loss=67.7805
	step [47/191], loss=61.5143
	step [48/191], loss=58.6379
	step [49/191], loss=69.5199
	step [50/191], loss=60.8455
	step [51/191], loss=78.2654
	step [52/191], loss=68.2787
	step [53/191], loss=67.6104
	step [54/191], loss=71.7034
	step [55/191], loss=70.1617
	step [56/191], loss=66.5136
	step [57/191], loss=67.5448
	step [58/191], loss=60.8703
	step [59/191], loss=66.6602
	step [60/191], loss=62.5621
	step [61/191], loss=64.7737
	step [62/191], loss=69.2806
	step [63/191], loss=56.2681
	step [64/191], loss=55.2862
	step [65/191], loss=71.3188
	step [66/191], loss=61.5278
	step [67/191], loss=61.5372
	step [68/191], loss=66.7736
	step [69/191], loss=68.5402
	step [70/191], loss=63.2696
	step [71/191], loss=78.6317
	step [72/191], loss=67.8069
	step [73/191], loss=69.9829
	step [74/191], loss=68.1001
	step [75/191], loss=81.4468
	step [76/191], loss=76.6073
	step [77/191], loss=67.7076
	step [78/191], loss=68.1085
	step [79/191], loss=73.6775
	step [80/191], loss=71.4026
	step [81/191], loss=66.8815
	step [82/191], loss=73.2110
	step [83/191], loss=64.1039
	step [84/191], loss=64.0129
	step [85/191], loss=55.5949
	step [86/191], loss=59.1495
	step [87/191], loss=61.2036
	step [88/191], loss=68.2063
	step [89/191], loss=60.6252
	step [90/191], loss=59.9445
	step [91/191], loss=69.4753
	step [92/191], loss=83.3558
	step [93/191], loss=74.4543
	step [94/191], loss=68.6656
	step [95/191], loss=63.2848
	step [96/191], loss=63.9960
	step [97/191], loss=74.0640
	step [98/191], loss=62.3298
	step [99/191], loss=77.9404
	step [100/191], loss=65.7000
	step [101/191], loss=67.7619
	step [102/191], loss=63.2060
	step [103/191], loss=68.3198
	step [104/191], loss=78.7747
	step [105/191], loss=79.0971
	step [106/191], loss=68.0845
	step [107/191], loss=60.3051
	step [108/191], loss=72.8004
	step [109/191], loss=52.3911
	step [110/191], loss=75.1931
	step [111/191], loss=73.5796
	step [112/191], loss=56.3179
	step [113/191], loss=72.6190
	step [114/191], loss=65.4329
	step [115/191], loss=78.0268
	step [116/191], loss=69.2186
	step [117/191], loss=58.5947
	step [118/191], loss=68.7515
	step [119/191], loss=85.5742
	step [120/191], loss=74.0745
	step [121/191], loss=72.2266
	step [122/191], loss=74.6686
	step [123/191], loss=64.1616
	step [124/191], loss=63.4289
	step [125/191], loss=70.3280
	step [126/191], loss=57.4715
	step [127/191], loss=61.7229
	step [128/191], loss=58.4689
	step [129/191], loss=69.8188
	step [130/191], loss=64.4348
	step [131/191], loss=79.5185
	step [132/191], loss=72.4744
	step [133/191], loss=66.5017
	step [134/191], loss=75.8515
	step [135/191], loss=64.4127
	step [136/191], loss=79.0457
	step [137/191], loss=73.1306
	step [138/191], loss=59.4825
	step [139/191], loss=68.9711
	step [140/191], loss=72.9799
	step [141/191], loss=67.9816
	step [142/191], loss=63.6687
	step [143/191], loss=67.3074
	step [144/191], loss=60.8733
	step [145/191], loss=71.6972
	step [146/191], loss=59.2522
	step [147/191], loss=73.7013
	step [148/191], loss=59.8659
	step [149/191], loss=67.8046
	step [150/191], loss=65.4980
	step [151/191], loss=58.9042
	step [152/191], loss=58.1560
	step [153/191], loss=58.5949
	step [154/191], loss=68.1209
	step [155/191], loss=75.7922
	step [156/191], loss=50.4737
	step [157/191], loss=70.6246
	step [158/191], loss=80.7668
	step [159/191], loss=68.9770
	step [160/191], loss=68.1041
	step [161/191], loss=62.2982
	step [162/191], loss=57.3431
	step [163/191], loss=64.9384
	step [164/191], loss=64.7821
	step [165/191], loss=69.6869
	step [166/191], loss=59.5016
	step [167/191], loss=65.0466
	step [168/191], loss=77.5871
	step [169/191], loss=62.1649
	step [170/191], loss=80.1471
	step [171/191], loss=62.8807
	step [172/191], loss=63.0800
	step [173/191], loss=56.4674
	step [174/191], loss=64.6645
	step [175/191], loss=57.6479
	step [176/191], loss=69.7562
	step [177/191], loss=74.3323
	step [178/191], loss=76.1915
	step [179/191], loss=71.2156
	step [180/191], loss=63.7206
	step [181/191], loss=68.8113
	step [182/191], loss=71.4631
	step [183/191], loss=65.0457
	step [184/191], loss=55.9386
	step [185/191], loss=75.2841
	step [186/191], loss=56.2271
	step [187/191], loss=75.3315
	step [188/191], loss=77.3910
	step [189/191], loss=56.0335
	step [190/191], loss=69.2058
	step [191/191], loss=34.3812
	Evaluating
	loss=0.0072, precision=0.4520, recall=0.8616, f1=0.5929
Training epoch 81
	step [1/191], loss=65.9385
	step [2/191], loss=61.6222
	step [3/191], loss=74.6374
	step [4/191], loss=65.4688
	step [5/191], loss=70.2595
	step [6/191], loss=51.6912
	step [7/191], loss=60.7661
	step [8/191], loss=64.0489
	step [9/191], loss=63.6609
	step [10/191], loss=55.9402
	step [11/191], loss=57.9630
	step [12/191], loss=65.0348
	step [13/191], loss=65.6311
	step [14/191], loss=68.0229
	step [15/191], loss=68.2535
	step [16/191], loss=62.9372
	step [17/191], loss=66.3428
	step [18/191], loss=60.0196
	step [19/191], loss=61.6779
	step [20/191], loss=62.9910
	step [21/191], loss=67.4514
	step [22/191], loss=54.2221
	step [23/191], loss=63.8298
	step [24/191], loss=67.3643
	step [25/191], loss=68.1094
	step [26/191], loss=77.9653
	step [27/191], loss=67.5460
	step [28/191], loss=66.7475
	step [29/191], loss=62.2802
	step [30/191], loss=62.8065
	step [31/191], loss=84.3340
	step [32/191], loss=69.8656
	step [33/191], loss=76.5962
	step [34/191], loss=67.6581
	step [35/191], loss=75.7597
	step [36/191], loss=54.3330
	step [37/191], loss=63.2199
	step [38/191], loss=73.2673
	step [39/191], loss=65.8903
	step [40/191], loss=69.7706
	step [41/191], loss=61.9544
	step [42/191], loss=54.4973
	step [43/191], loss=58.3496
	step [44/191], loss=74.5293
	step [45/191], loss=75.6127
	step [46/191], loss=83.7822
	step [47/191], loss=61.5209
	step [48/191], loss=63.2384
	step [49/191], loss=67.6240
	step [50/191], loss=69.0671
	step [51/191], loss=74.6128
	step [52/191], loss=60.6221
	step [53/191], loss=56.6977
	step [54/191], loss=70.3843
	step [55/191], loss=68.8495
	step [56/191], loss=66.3914
	step [57/191], loss=69.7117
	step [58/191], loss=66.9652
	step [59/191], loss=86.2202
	step [60/191], loss=62.3444
	step [61/191], loss=70.0212
	step [62/191], loss=60.9812
	step [63/191], loss=74.5509
	step [64/191], loss=56.8334
	step [65/191], loss=68.7327
	step [66/191], loss=69.5769
	step [67/191], loss=60.3606
	step [68/191], loss=65.7124
	step [69/191], loss=67.2047
	step [70/191], loss=68.1427
	step [71/191], loss=70.5494
	step [72/191], loss=65.2656
	step [73/191], loss=71.5745
	step [74/191], loss=81.3040
	step [75/191], loss=73.2910
	step [76/191], loss=61.4022
	step [77/191], loss=69.6071
	step [78/191], loss=59.8467
	step [79/191], loss=72.6884
	step [80/191], loss=70.0390
	step [81/191], loss=60.2681
	step [82/191], loss=66.2190
	step [83/191], loss=67.2066
	step [84/191], loss=66.1801
	step [85/191], loss=58.3521
	step [86/191], loss=68.4519
	step [87/191], loss=61.5742
	step [88/191], loss=76.8815
	step [89/191], loss=67.0733
	step [90/191], loss=62.0446
	step [91/191], loss=67.3599
	step [92/191], loss=68.9138
	step [93/191], loss=50.0282
	step [94/191], loss=68.4562
	step [95/191], loss=50.4088
	step [96/191], loss=69.2423
	step [97/191], loss=65.7624
	step [98/191], loss=62.9008
	step [99/191], loss=64.1439
	step [100/191], loss=75.4823
	step [101/191], loss=61.7537
	step [102/191], loss=72.7112
	step [103/191], loss=71.1909
	step [104/191], loss=61.6024
	step [105/191], loss=58.8746
	step [106/191], loss=69.5533
	step [107/191], loss=62.2381
	step [108/191], loss=72.4808
	step [109/191], loss=67.9324
	step [110/191], loss=68.5826
	step [111/191], loss=59.7097
	step [112/191], loss=74.6984
	step [113/191], loss=56.6156
	step [114/191], loss=77.8055
	step [115/191], loss=60.0805
	step [116/191], loss=56.3978
	step [117/191], loss=61.4836
	step [118/191], loss=56.8391
	step [119/191], loss=68.3566
	step [120/191], loss=67.1574
	step [121/191], loss=66.7600
	step [122/191], loss=77.7627
	step [123/191], loss=78.7879
	step [124/191], loss=60.2039
	step [125/191], loss=70.6066
	step [126/191], loss=67.9478
	step [127/191], loss=62.4971
	step [128/191], loss=53.6722
	step [129/191], loss=83.6958
	step [130/191], loss=55.8399
	step [131/191], loss=63.6146
	step [132/191], loss=60.4501
	step [133/191], loss=82.0123
	step [134/191], loss=75.6473
	step [135/191], loss=71.0093
	step [136/191], loss=57.1643
	step [137/191], loss=68.3159
	step [138/191], loss=55.1371
	step [139/191], loss=63.4629
	step [140/191], loss=71.7706
	step [141/191], loss=57.2683
	step [142/191], loss=64.1369
	step [143/191], loss=66.8701
	step [144/191], loss=62.6322
	step [145/191], loss=64.5009
	step [146/191], loss=63.7191
	step [147/191], loss=74.8564
	step [148/191], loss=60.7563
	step [149/191], loss=75.5656
	step [150/191], loss=69.1592
	step [151/191], loss=76.5590
	step [152/191], loss=66.9392
	step [153/191], loss=58.5883
	step [154/191], loss=63.8510
	step [155/191], loss=78.2280
	step [156/191], loss=62.6845
	step [157/191], loss=64.7787
	step [158/191], loss=74.3171
	step [159/191], loss=75.8983
	step [160/191], loss=57.4833
	step [161/191], loss=74.5977
	step [162/191], loss=69.5096
	step [163/191], loss=66.5758
	step [164/191], loss=67.6675
	step [165/191], loss=65.5518
	step [166/191], loss=66.9751
	step [167/191], loss=80.2952
	step [168/191], loss=75.7281
	step [169/191], loss=79.9625
	step [170/191], loss=81.4929
	step [171/191], loss=70.4995
	step [172/191], loss=76.0337
	step [173/191], loss=72.6155
	step [174/191], loss=77.2543
	step [175/191], loss=70.8317
	step [176/191], loss=61.6448
	step [177/191], loss=75.3951
	step [178/191], loss=69.4321
	step [179/191], loss=71.2751
	step [180/191], loss=76.4156
	step [181/191], loss=66.9127
	step [182/191], loss=72.7522
	step [183/191], loss=74.1189
	step [184/191], loss=63.3308
	step [185/191], loss=63.9266
	step [186/191], loss=70.7534
	step [187/191], loss=65.4700
	step [188/191], loss=69.5987
	step [189/191], loss=62.6655
	step [190/191], loss=60.2695
	step [191/191], loss=32.9893
	Evaluating
	loss=0.0065, precision=0.5021, recall=0.8500, f1=0.6313
saving model as: 1_saved_model.pth
Training epoch 82
	step [1/191], loss=66.1495
	step [2/191], loss=75.4302
	step [3/191], loss=85.5834
	step [4/191], loss=69.8088
	step [5/191], loss=60.4758
	step [6/191], loss=66.7804
	step [7/191], loss=67.7825
	step [8/191], loss=71.3810
	step [9/191], loss=52.2331
	step [10/191], loss=74.0224
	step [11/191], loss=62.6122
	step [12/191], loss=60.1591
	step [13/191], loss=58.7689
	step [14/191], loss=60.7961
	step [15/191], loss=63.9169
	step [16/191], loss=66.3520
	step [17/191], loss=65.3301
	step [18/191], loss=68.9132
	step [19/191], loss=85.4395
	step [20/191], loss=77.9137
	step [21/191], loss=61.0204
	step [22/191], loss=72.2513
	step [23/191], loss=66.0623
	step [24/191], loss=59.6080
	step [25/191], loss=63.1188
	step [26/191], loss=67.8710
	step [27/191], loss=67.1977
	step [28/191], loss=61.2067
	step [29/191], loss=74.3354
	step [30/191], loss=60.6266
	step [31/191], loss=60.9229
	step [32/191], loss=62.6567
	step [33/191], loss=85.0924
	step [34/191], loss=68.0354
	step [35/191], loss=62.2797
	step [36/191], loss=62.6515
	step [37/191], loss=68.3384
	step [38/191], loss=76.3673
	step [39/191], loss=66.1777
	step [40/191], loss=72.9055
	step [41/191], loss=56.2708
	step [42/191], loss=61.0475
	step [43/191], loss=70.5702
	step [44/191], loss=68.5196
	step [45/191], loss=72.1791
	step [46/191], loss=64.8147
	step [47/191], loss=47.8383
	step [48/191], loss=55.5784
	step [49/191], loss=67.5333
	step [50/191], loss=63.4469
	step [51/191], loss=49.1146
	step [52/191], loss=69.8163
	step [53/191], loss=60.9157
	step [54/191], loss=55.0934
	step [55/191], loss=62.5077
	step [56/191], loss=75.5241
	step [57/191], loss=63.5286
	step [58/191], loss=70.1501
	step [59/191], loss=84.9395
	step [60/191], loss=63.3407
	step [61/191], loss=61.1577
	step [62/191], loss=72.7364
	step [63/191], loss=66.0136
	step [64/191], loss=65.7223
	step [65/191], loss=62.9247
	step [66/191], loss=60.6212
	step [67/191], loss=71.1190
	step [68/191], loss=75.9507
	step [69/191], loss=74.1970
	step [70/191], loss=70.5392
	step [71/191], loss=72.7713
	step [72/191], loss=58.7210
	step [73/191], loss=77.0851
	step [74/191], loss=62.8866
	step [75/191], loss=65.6520
	step [76/191], loss=64.6210
	step [77/191], loss=61.1332
	step [78/191], loss=65.2675
	step [79/191], loss=64.2643
	step [80/191], loss=62.7175
	step [81/191], loss=62.2703
	step [82/191], loss=55.5384
	step [83/191], loss=68.9615
	step [84/191], loss=75.4539
	step [85/191], loss=76.5251
	step [86/191], loss=72.5996
	step [87/191], loss=58.8947
	step [88/191], loss=72.3337
	step [89/191], loss=68.0222
	step [90/191], loss=73.8911
	step [91/191], loss=67.1762
	step [92/191], loss=70.6909
	step [93/191], loss=64.6404
	step [94/191], loss=74.3906
	step [95/191], loss=69.0041
	step [96/191], loss=62.9464
	step [97/191], loss=64.2557
	step [98/191], loss=76.3286
	step [99/191], loss=56.2607
	step [100/191], loss=62.7004
	step [101/191], loss=60.2271
	step [102/191], loss=79.7954
	step [103/191], loss=76.0774
	step [104/191], loss=73.5965
	step [105/191], loss=75.6819
	step [106/191], loss=54.1995
	step [107/191], loss=76.5243
	step [108/191], loss=60.2722
	step [109/191], loss=63.3118
	step [110/191], loss=66.8179
	step [111/191], loss=58.0262
	step [112/191], loss=74.9838
	step [113/191], loss=47.5754
	step [114/191], loss=80.0280
	step [115/191], loss=77.3791
	step [116/191], loss=70.7895
	step [117/191], loss=63.2079
	step [118/191], loss=61.8342
	step [119/191], loss=58.8785
	step [120/191], loss=60.1149
	step [121/191], loss=45.9593
	step [122/191], loss=67.2057
	step [123/191], loss=67.8660
	step [124/191], loss=60.5747
	step [125/191], loss=63.9573
	step [126/191], loss=65.8857
	step [127/191], loss=81.9715
	step [128/191], loss=69.2320
	step [129/191], loss=75.5890
	step [130/191], loss=73.2724
	step [131/191], loss=60.5003
	step [132/191], loss=62.5106
	step [133/191], loss=56.6237
	step [134/191], loss=81.5491
	step [135/191], loss=71.9859
	step [136/191], loss=64.8995
	step [137/191], loss=73.3058
	step [138/191], loss=74.1296
	step [139/191], loss=61.1917
	step [140/191], loss=74.1538
	step [141/191], loss=65.9321
	step [142/191], loss=66.8412
	step [143/191], loss=61.5625
	step [144/191], loss=62.3661
	step [145/191], loss=67.1919
	step [146/191], loss=64.8516
	step [147/191], loss=67.7559
	step [148/191], loss=58.3392
	step [149/191], loss=63.2688
	step [150/191], loss=66.2213
	step [151/191], loss=67.0239
	step [152/191], loss=74.1032
	step [153/191], loss=55.5596
	step [154/191], loss=68.5310
	step [155/191], loss=62.9039
	step [156/191], loss=50.6938
	step [157/191], loss=76.7578
	step [158/191], loss=72.4061
	step [159/191], loss=69.9826
	step [160/191], loss=49.3524
	step [161/191], loss=66.3671
	step [162/191], loss=60.1860
	step [163/191], loss=67.4611
	step [164/191], loss=70.7917
	step [165/191], loss=61.5412
	step [166/191], loss=74.5019
	step [167/191], loss=71.6420
	step [168/191], loss=68.0144
	step [169/191], loss=67.7177
	step [170/191], loss=60.9778
	step [171/191], loss=79.6693
	step [172/191], loss=60.8340
	step [173/191], loss=80.7592
	step [174/191], loss=66.8954
	step [175/191], loss=71.8293
	step [176/191], loss=75.1060
	step [177/191], loss=67.8274
	step [178/191], loss=79.3802
	step [179/191], loss=68.8209
	step [180/191], loss=59.7394
	step [181/191], loss=72.1851
	step [182/191], loss=66.3439
	step [183/191], loss=62.4858
	step [184/191], loss=67.5214
	step [185/191], loss=78.2862
	step [186/191], loss=54.5038
	step [187/191], loss=65.9077
	step [188/191], loss=70.1061
	step [189/191], loss=63.8815
	step [190/191], loss=63.3003
	step [191/191], loss=38.7485
	Evaluating
	loss=0.0079, precision=0.4218, recall=0.8418, f1=0.5620
Training epoch 83
	step [1/191], loss=82.6824
	step [2/191], loss=69.9138
	step [3/191], loss=55.9706
	step [4/191], loss=64.5203
	step [5/191], loss=75.8440
	step [6/191], loss=65.3141
	step [7/191], loss=71.3630
	step [8/191], loss=74.7990
	step [9/191], loss=92.7295
	step [10/191], loss=63.9366
	step [11/191], loss=78.8909
	step [12/191], loss=64.7045
	step [13/191], loss=65.3077
	step [14/191], loss=57.1113
	step [15/191], loss=54.6978
	step [16/191], loss=76.1023
	step [17/191], loss=64.3298
	step [18/191], loss=68.0774
	step [19/191], loss=73.6009
	step [20/191], loss=67.4604
	step [21/191], loss=62.9489
	step [22/191], loss=76.8458
	step [23/191], loss=67.7452
	step [24/191], loss=60.5300
	step [25/191], loss=66.2561
	step [26/191], loss=54.6842
	step [27/191], loss=57.6767
	step [28/191], loss=58.6263
	step [29/191], loss=71.9900
	step [30/191], loss=59.8350
	step [31/191], loss=68.9571
	step [32/191], loss=59.2105
	step [33/191], loss=73.5215
	step [34/191], loss=71.9581
	step [35/191], loss=65.8121
	step [36/191], loss=59.4119
	step [37/191], loss=75.0954
	step [38/191], loss=60.7826
	step [39/191], loss=57.8062
	step [40/191], loss=61.5548
	step [41/191], loss=53.5259
	step [42/191], loss=64.6744
	step [43/191], loss=70.9632
	step [44/191], loss=82.7078
	step [45/191], loss=66.7701
	step [46/191], loss=63.5012
	step [47/191], loss=57.5298
	step [48/191], loss=58.3482
	step [49/191], loss=56.9372
	step [50/191], loss=68.1079
	step [51/191], loss=71.6569
	step [52/191], loss=62.9215
	step [53/191], loss=60.8823
	step [54/191], loss=53.9039
	step [55/191], loss=77.6743
	step [56/191], loss=55.7179
	step [57/191], loss=59.1160
	step [58/191], loss=59.9039
	step [59/191], loss=65.0388
	step [60/191], loss=68.5194
	step [61/191], loss=65.4867
	step [62/191], loss=68.9004
	step [63/191], loss=72.6980
	step [64/191], loss=70.4833
	step [65/191], loss=78.1902
	step [66/191], loss=71.0182
	step [67/191], loss=62.3797
	step [68/191], loss=58.6961
	step [69/191], loss=62.5876
	step [70/191], loss=75.0784
	step [71/191], loss=82.7595
	step [72/191], loss=59.7518
	step [73/191], loss=71.3681
	step [74/191], loss=70.1881
	step [75/191], loss=61.4268
	step [76/191], loss=67.2278
	step [77/191], loss=72.0616
	step [78/191], loss=75.1188
	step [79/191], loss=65.7342
	step [80/191], loss=68.9971
	step [81/191], loss=65.1030
	step [82/191], loss=48.1127
	step [83/191], loss=57.6597
	step [84/191], loss=73.2955
	step [85/191], loss=73.7075
	step [86/191], loss=52.0437
	step [87/191], loss=61.0064
	step [88/191], loss=70.3234
	step [89/191], loss=49.2398
	step [90/191], loss=64.0518
	step [91/191], loss=69.6167
	step [92/191], loss=61.2934
	step [93/191], loss=60.2680
	step [94/191], loss=63.6501
	step [95/191], loss=61.4532
	step [96/191], loss=75.1377
	step [97/191], loss=66.4444
	step [98/191], loss=66.7782
	step [99/191], loss=64.2944
	step [100/191], loss=73.8107
	step [101/191], loss=59.9189
	step [102/191], loss=76.3509
	step [103/191], loss=66.6148
	step [104/191], loss=65.6118
	step [105/191], loss=62.6523
	step [106/191], loss=65.1926
	step [107/191], loss=54.7036
	step [108/191], loss=64.2941
	step [109/191], loss=61.7255
	step [110/191], loss=66.5446
	step [111/191], loss=62.6042
	step [112/191], loss=69.9385
	step [113/191], loss=65.1677
	step [114/191], loss=81.3002
	step [115/191], loss=74.7223
	step [116/191], loss=65.0905
	step [117/191], loss=73.3498
	step [118/191], loss=59.4515
	step [119/191], loss=62.1595
	step [120/191], loss=68.9403
	step [121/191], loss=74.7986
	step [122/191], loss=83.5252
	step [123/191], loss=67.8384
	step [124/191], loss=74.2397
	step [125/191], loss=60.5109
	step [126/191], loss=63.1992
	step [127/191], loss=68.3575
	step [128/191], loss=57.3551
	step [129/191], loss=70.7536
	step [130/191], loss=68.2815
	step [131/191], loss=71.1436
	step [132/191], loss=70.9378
	step [133/191], loss=65.7839
	step [134/191], loss=64.4552
	step [135/191], loss=71.4388
	step [136/191], loss=67.4649
	step [137/191], loss=71.1391
	step [138/191], loss=64.3816
	step [139/191], loss=64.5464
	step [140/191], loss=71.7716
	step [141/191], loss=63.5843
	step [142/191], loss=75.6566
	step [143/191], loss=71.4483
	step [144/191], loss=67.0890
	step [145/191], loss=77.5762
	step [146/191], loss=65.3062
	step [147/191], loss=63.6774
	step [148/191], loss=83.4225
	step [149/191], loss=71.6143
	step [150/191], loss=71.9205
	step [151/191], loss=58.0291
	step [152/191], loss=64.1445
	step [153/191], loss=61.1360
	step [154/191], loss=68.2233
	step [155/191], loss=63.0949
	step [156/191], loss=60.9783
	step [157/191], loss=65.8147
	step [158/191], loss=79.2337
	step [159/191], loss=71.2853
	step [160/191], loss=67.3863
	step [161/191], loss=68.2366
	step [162/191], loss=72.0489
	step [163/191], loss=71.9984
	step [164/191], loss=71.8689
	step [165/191], loss=70.3816
	step [166/191], loss=67.8388
	step [167/191], loss=63.2332
	step [168/191], loss=68.3220
	step [169/191], loss=60.5870
	step [170/191], loss=62.9710
	step [171/191], loss=60.0565
	step [172/191], loss=65.9731
	step [173/191], loss=55.9115
	step [174/191], loss=60.3317
	step [175/191], loss=78.4311
	step [176/191], loss=85.9260
	step [177/191], loss=50.4698
	step [178/191], loss=57.9169
	step [179/191], loss=76.5574
	step [180/191], loss=78.3850
	step [181/191], loss=65.8560
	step [182/191], loss=54.9525
	step [183/191], loss=69.0850
	step [184/191], loss=67.5454
	step [185/191], loss=70.5556
	step [186/191], loss=60.2914
	step [187/191], loss=67.4920
	step [188/191], loss=58.8414
	step [189/191], loss=68.1517
	step [190/191], loss=58.9337
	step [191/191], loss=31.5714
	Evaluating
	loss=0.0078, precision=0.4361, recall=0.8532, f1=0.5772
Training epoch 84
	step [1/191], loss=61.2985
	step [2/191], loss=63.2080
	step [3/191], loss=60.9176
	step [4/191], loss=71.5338
	step [5/191], loss=62.4161
	step [6/191], loss=58.1887
	step [7/191], loss=66.1721
	step [8/191], loss=64.9850
	step [9/191], loss=77.5268
	step [10/191], loss=58.8306
	step [11/191], loss=55.9643
	step [12/191], loss=75.0012
	step [13/191], loss=56.9445
	step [14/191], loss=59.1376
	step [15/191], loss=70.7133
	step [16/191], loss=58.3902
	step [17/191], loss=57.9209
	step [18/191], loss=65.2598
	step [19/191], loss=74.9376
	step [20/191], loss=76.7110
	step [21/191], loss=65.0415
	step [22/191], loss=65.7378
	step [23/191], loss=65.0558
	step [24/191], loss=62.1277
	step [25/191], loss=65.1053
	step [26/191], loss=58.3886
	step [27/191], loss=69.8088
	step [28/191], loss=63.5858
	step [29/191], loss=63.0014
	step [30/191], loss=75.9411
	step [31/191], loss=70.7470
	step [32/191], loss=76.9713
	step [33/191], loss=69.6586
	step [34/191], loss=79.1502
	step [35/191], loss=73.9845
	step [36/191], loss=62.6341
	step [37/191], loss=77.4691
	step [38/191], loss=68.7832
	step [39/191], loss=60.9577
	step [40/191], loss=56.1016
	step [41/191], loss=64.3883
	step [42/191], loss=64.1373
	step [43/191], loss=67.1276
	step [44/191], loss=77.7123
	step [45/191], loss=67.3439
	step [46/191], loss=64.5547
	step [47/191], loss=62.4420
	step [48/191], loss=58.1021
	step [49/191], loss=67.5173
	step [50/191], loss=60.5244
	step [51/191], loss=67.8651
	step [52/191], loss=71.1529
	step [53/191], loss=72.9108
	step [54/191], loss=77.2259
	step [55/191], loss=62.7881
	step [56/191], loss=57.2582
	step [57/191], loss=76.4747
	step [58/191], loss=51.5196
	step [59/191], loss=77.3725
	step [60/191], loss=61.8425
	step [61/191], loss=75.7844
	step [62/191], loss=66.0794
	step [63/191], loss=65.9885
	step [64/191], loss=76.4706
	step [65/191], loss=61.4768
	step [66/191], loss=71.7024
	step [67/191], loss=68.7882
	step [68/191], loss=54.8890
	step [69/191], loss=60.9112
	step [70/191], loss=66.9619
	step [71/191], loss=73.9382
	step [72/191], loss=59.8337
	step [73/191], loss=69.3329
	step [74/191], loss=55.9677
	step [75/191], loss=80.3079
	step [76/191], loss=68.0609
	step [77/191], loss=54.5730
	step [78/191], loss=63.6657
	step [79/191], loss=64.5815
	step [80/191], loss=53.0250
	step [81/191], loss=72.9222
	step [82/191], loss=72.7957
	step [83/191], loss=66.9789
	step [84/191], loss=68.1119
	step [85/191], loss=71.3579
	step [86/191], loss=61.3226
	step [87/191], loss=80.4238
	step [88/191], loss=71.3741
	step [89/191], loss=64.5017
	step [90/191], loss=66.6236
	step [91/191], loss=67.1953
	step [92/191], loss=63.8549
	step [93/191], loss=73.6339
	step [94/191], loss=58.6754
	step [95/191], loss=59.7605
	step [96/191], loss=68.4787
	step [97/191], loss=80.7388
	step [98/191], loss=68.3014
	step [99/191], loss=77.1097
	step [100/191], loss=52.7458
	step [101/191], loss=69.7024
	step [102/191], loss=77.9186
	step [103/191], loss=74.0175
	step [104/191], loss=76.3303
	step [105/191], loss=56.4562
	step [106/191], loss=60.7204
	step [107/191], loss=64.1023
	step [108/191], loss=72.1070
	step [109/191], loss=65.2827
	step [110/191], loss=65.1867
	step [111/191], loss=72.1242
	step [112/191], loss=76.0700
	step [113/191], loss=60.9797
	step [114/191], loss=67.5002
	step [115/191], loss=67.4792
	step [116/191], loss=68.4433
	step [117/191], loss=71.9939
	step [118/191], loss=69.5603
	step [119/191], loss=68.9543
	step [120/191], loss=67.5136
	step [121/191], loss=77.3979
	step [122/191], loss=68.4189
	step [123/191], loss=66.0955
	step [124/191], loss=59.9377
	step [125/191], loss=64.1669
	step [126/191], loss=59.7447
	step [127/191], loss=57.6965
	step [128/191], loss=53.4570
	step [129/191], loss=65.8027
	step [130/191], loss=72.1646
	step [131/191], loss=67.8936
	step [132/191], loss=62.2546
	step [133/191], loss=78.8837
	step [134/191], loss=71.4566
	step [135/191], loss=61.9876
	step [136/191], loss=60.7169
	step [137/191], loss=63.7768
	step [138/191], loss=64.5549
	step [139/191], loss=69.5101
	step [140/191], loss=57.3168
	step [141/191], loss=61.3231
	step [142/191], loss=65.8255
	step [143/191], loss=58.7383
	step [144/191], loss=57.4522
	step [145/191], loss=65.1381
	step [146/191], loss=66.8035
	step [147/191], loss=66.4515
	step [148/191], loss=74.3808
	step [149/191], loss=63.0083
	step [150/191], loss=65.1333
	step [151/191], loss=76.3484
	step [152/191], loss=60.6753
	step [153/191], loss=75.3843
	step [154/191], loss=65.0171
	step [155/191], loss=67.0121
	step [156/191], loss=67.1881
	step [157/191], loss=67.5866
	step [158/191], loss=59.0508
	step [159/191], loss=65.4595
	step [160/191], loss=64.3052
	step [161/191], loss=65.0705
	step [162/191], loss=51.3669
	step [163/191], loss=69.8510
	step [164/191], loss=69.3441
	step [165/191], loss=70.9632
	step [166/191], loss=67.1665
	step [167/191], loss=60.1281
	step [168/191], loss=67.4947
	step [169/191], loss=56.3915
	step [170/191], loss=71.6770
	step [171/191], loss=73.4970
	step [172/191], loss=77.1043
	step [173/191], loss=66.0359
	step [174/191], loss=55.8823
	step [175/191], loss=64.4193
	step [176/191], loss=76.4434
	step [177/191], loss=72.4968
	step [178/191], loss=64.3362
	step [179/191], loss=66.8026
	step [180/191], loss=66.2154
	step [181/191], loss=59.7884
	step [182/191], loss=65.3354
	step [183/191], loss=69.2011
	step [184/191], loss=56.0525
	step [185/191], loss=67.5351
	step [186/191], loss=75.8900
	step [187/191], loss=62.7530
	step [188/191], loss=69.3933
	step [189/191], loss=64.5253
	step [190/191], loss=74.7734
	step [191/191], loss=36.2870
	Evaluating
	loss=0.0081, precision=0.4166, recall=0.8552, f1=0.5602
Training epoch 85
	step [1/191], loss=56.5674
	step [2/191], loss=63.2986
	step [3/191], loss=67.0763
	step [4/191], loss=57.2041
	step [5/191], loss=60.6483
	step [6/191], loss=73.2840
	step [7/191], loss=73.5979
	step [8/191], loss=65.4253
	step [9/191], loss=63.8349
	step [10/191], loss=53.9159
	step [11/191], loss=72.4832
	step [12/191], loss=56.4262
	step [13/191], loss=74.8180
	step [14/191], loss=64.5152
	step [15/191], loss=75.3213
	step [16/191], loss=65.7151
	step [17/191], loss=65.6301
	step [18/191], loss=74.5581
	step [19/191], loss=67.1952
	step [20/191], loss=59.6677
	step [21/191], loss=68.2712
	step [22/191], loss=64.0243
	step [23/191], loss=75.0465
	step [24/191], loss=63.9657
	step [25/191], loss=61.5122
	step [26/191], loss=51.9757
	step [27/191], loss=67.8389
	step [28/191], loss=63.7903
	step [29/191], loss=58.8031
	step [30/191], loss=63.6781
	step [31/191], loss=58.8078
	step [32/191], loss=55.6325
	step [33/191], loss=70.1564
	step [34/191], loss=64.3610
	step [35/191], loss=68.9169
	step [36/191], loss=64.1506
	step [37/191], loss=70.7247
	step [38/191], loss=63.7395
	step [39/191], loss=59.6766
	step [40/191], loss=60.0530
	step [41/191], loss=61.3888
	step [42/191], loss=64.4592
	step [43/191], loss=52.1821
	step [44/191], loss=67.1362
	step [45/191], loss=52.4274
	step [46/191], loss=58.6081
	step [47/191], loss=56.7386
	step [48/191], loss=78.3820
	step [49/191], loss=60.8753
	step [50/191], loss=50.1290
	step [51/191], loss=78.2321
	step [52/191], loss=57.0837
	step [53/191], loss=76.6235
	step [54/191], loss=62.4419
	step [55/191], loss=65.4455
	step [56/191], loss=64.4300
	step [57/191], loss=78.5853
	step [58/191], loss=75.5021
	step [59/191], loss=73.2553
	step [60/191], loss=72.0955
	step [61/191], loss=62.4115
	step [62/191], loss=56.9460
	step [63/191], loss=73.3675
	step [64/191], loss=65.4278
	step [65/191], loss=62.6380
	step [66/191], loss=60.5330
	step [67/191], loss=58.5056
	step [68/191], loss=62.1958
	step [69/191], loss=76.9992
	step [70/191], loss=63.7302
	step [71/191], loss=64.9141
	step [72/191], loss=64.2698
	step [73/191], loss=60.1770
	step [74/191], loss=70.8516
	step [75/191], loss=66.3289
	step [76/191], loss=64.6882
	step [77/191], loss=64.5203
	step [78/191], loss=73.3909
	step [79/191], loss=56.7336
	step [80/191], loss=75.8718
	step [81/191], loss=62.9204
	step [82/191], loss=74.2832
	step [83/191], loss=65.1816
	step [84/191], loss=66.2852
	step [85/191], loss=78.6189
	step [86/191], loss=64.1811
	step [87/191], loss=58.2590
	step [88/191], loss=79.5804
	step [89/191], loss=69.0566
	step [90/191], loss=69.2851
	step [91/191], loss=71.2470
	step [92/191], loss=63.7114
	step [93/191], loss=75.1781
	step [94/191], loss=69.2943
	step [95/191], loss=79.3533
	step [96/191], loss=70.1798
	step [97/191], loss=64.4244
	step [98/191], loss=67.2145
	step [99/191], loss=61.6669
	step [100/191], loss=74.6338
	step [101/191], loss=66.2837
	step [102/191], loss=79.7504
	step [103/191], loss=69.7271
	step [104/191], loss=62.5555
	step [105/191], loss=78.4716
	step [106/191], loss=56.6956
	step [107/191], loss=66.7410
	step [108/191], loss=80.8813
	step [109/191], loss=67.9788
	step [110/191], loss=75.2737
	step [111/191], loss=69.0745
	step [112/191], loss=69.1846
	step [113/191], loss=67.4372
	step [114/191], loss=70.7532
	step [115/191], loss=63.7582
	step [116/191], loss=62.0119
	step [117/191], loss=65.1331
	step [118/191], loss=73.9456
	step [119/191], loss=54.1050
	step [120/191], loss=69.6024
	step [121/191], loss=67.7689
	step [122/191], loss=59.7086
	step [123/191], loss=62.5329
	step [124/191], loss=63.1716
	step [125/191], loss=59.3936
	step [126/191], loss=75.2098
	step [127/191], loss=63.0388
	step [128/191], loss=63.6762
	step [129/191], loss=67.3451
	step [130/191], loss=66.3607
	step [131/191], loss=73.0408
	step [132/191], loss=66.9244
	step [133/191], loss=61.6037
	step [134/191], loss=72.4639
	step [135/191], loss=74.3492
	step [136/191], loss=63.5965
	step [137/191], loss=63.1900
	step [138/191], loss=54.5930
	step [139/191], loss=69.5348
	step [140/191], loss=61.2596
	step [141/191], loss=49.4652
	step [142/191], loss=57.6670
	step [143/191], loss=65.5695
	step [144/191], loss=69.9469
	step [145/191], loss=61.7585
	step [146/191], loss=54.6158
	step [147/191], loss=72.4693
	step [148/191], loss=61.1110
	step [149/191], loss=70.1113
	step [150/191], loss=62.5937
	step [151/191], loss=67.7518
	step [152/191], loss=63.3919
	step [153/191], loss=82.7895
	step [154/191], loss=65.6767
	step [155/191], loss=58.8870
	step [156/191], loss=78.7736
	step [157/191], loss=75.1747
	step [158/191], loss=67.4351
	step [159/191], loss=56.5566
	step [160/191], loss=65.3703
	step [161/191], loss=63.5553
	step [162/191], loss=68.3443
	step [163/191], loss=61.9319
	step [164/191], loss=68.6020
	step [165/191], loss=72.8145
	step [166/191], loss=75.3734
	step [167/191], loss=61.3903
	step [168/191], loss=59.0365
	step [169/191], loss=57.7736
	step [170/191], loss=62.9161
	step [171/191], loss=60.8338
	step [172/191], loss=69.8173
	step [173/191], loss=66.8710
	step [174/191], loss=64.1804
	step [175/191], loss=74.5724
	step [176/191], loss=70.6062
	step [177/191], loss=65.8659
	step [178/191], loss=75.4211
	step [179/191], loss=58.0955
	step [180/191], loss=69.3965
	step [181/191], loss=58.0512
	step [182/191], loss=68.9176
	step [183/191], loss=60.3016
	step [184/191], loss=67.5226
	step [185/191], loss=61.2339
	step [186/191], loss=85.8005
	step [187/191], loss=68.1042
	step [188/191], loss=67.6870
	step [189/191], loss=68.0112
	step [190/191], loss=68.5057
	step [191/191], loss=37.0333
	Evaluating
	loss=0.0071, precision=0.4682, recall=0.8604, f1=0.6064
Training epoch 86
	step [1/191], loss=65.7504
	step [2/191], loss=69.1636
	step [3/191], loss=62.1281
	step [4/191], loss=65.5289
	step [5/191], loss=66.9229
	step [6/191], loss=70.7087
	step [7/191], loss=80.2334
	step [8/191], loss=58.8668
	step [9/191], loss=70.9966
	step [10/191], loss=77.4905
	step [11/191], loss=66.7789
	step [12/191], loss=65.5625
	step [13/191], loss=64.8840
	step [14/191], loss=57.1796
	step [15/191], loss=77.4668
	step [16/191], loss=59.7413
	step [17/191], loss=66.5481
	step [18/191], loss=65.6732
	step [19/191], loss=57.0005
	step [20/191], loss=85.1932
	step [21/191], loss=74.5128
	step [22/191], loss=64.0197
	step [23/191], loss=71.9482
	step [24/191], loss=56.4549
	step [25/191], loss=76.8543
	step [26/191], loss=73.8862
	step [27/191], loss=70.6852
	step [28/191], loss=66.7074
	step [29/191], loss=68.5546
	step [30/191], loss=61.4281
	step [31/191], loss=56.2245
	step [32/191], loss=66.4934
	step [33/191], loss=67.9348
	step [34/191], loss=68.0496
	step [35/191], loss=70.7631
	step [36/191], loss=69.1887
	step [37/191], loss=68.4836
	step [38/191], loss=62.5113
	step [39/191], loss=80.9346
	step [40/191], loss=78.9451
	step [41/191], loss=62.9336
	step [42/191], loss=69.1484
	step [43/191], loss=75.9954
	step [44/191], loss=55.4395
	step [45/191], loss=72.3579
	step [46/191], loss=51.2844
	step [47/191], loss=63.3508
	step [48/191], loss=73.2751
	step [49/191], loss=53.3223
	step [50/191], loss=71.9955
	step [51/191], loss=70.8896
	step [52/191], loss=62.4524
	step [53/191], loss=58.6057
	step [54/191], loss=62.8213
	step [55/191], loss=61.9678
	step [56/191], loss=80.0804
	step [57/191], loss=67.9389
	step [58/191], loss=65.7325
	step [59/191], loss=67.5621
	step [60/191], loss=62.0245
	step [61/191], loss=74.9209
	step [62/191], loss=54.9867
	step [63/191], loss=60.6716
	step [64/191], loss=71.9335
	step [65/191], loss=60.4265
	step [66/191], loss=67.1808
	step [67/191], loss=61.2415
	step [68/191], loss=72.7755
	step [69/191], loss=56.3871
	step [70/191], loss=67.7899
	step [71/191], loss=65.9165
	step [72/191], loss=60.8245
	step [73/191], loss=63.3208
	step [74/191], loss=57.7088
	step [75/191], loss=64.7655
	step [76/191], loss=66.6722
	step [77/191], loss=74.2723
	step [78/191], loss=72.5575
	step [79/191], loss=68.8604
	step [80/191], loss=71.1138
	step [81/191], loss=73.1310
	step [82/191], loss=64.1164
	step [83/191], loss=65.9783
	step [84/191], loss=57.7826
	step [85/191], loss=73.4423
	step [86/191], loss=59.1515
	step [87/191], loss=60.5135
	step [88/191], loss=66.7925
	step [89/191], loss=68.2673
	step [90/191], loss=66.1245
	step [91/191], loss=73.2168
	step [92/191], loss=62.6515
	step [93/191], loss=57.3398
	step [94/191], loss=73.7614
	step [95/191], loss=60.1380
	step [96/191], loss=59.0569
	step [97/191], loss=82.0159
	step [98/191], loss=69.3663
	step [99/191], loss=70.4788
	step [100/191], loss=65.6966
	step [101/191], loss=68.1562
	step [102/191], loss=58.7771
	step [103/191], loss=70.6486
	step [104/191], loss=56.7798
	step [105/191], loss=56.9732
	step [106/191], loss=75.4593
	step [107/191], loss=62.9558
	step [108/191], loss=74.0038
	step [109/191], loss=63.5089
	step [110/191], loss=60.9652
	step [111/191], loss=61.6211
	step [112/191], loss=61.9887
	step [113/191], loss=60.3145
	step [114/191], loss=59.4498
	step [115/191], loss=59.2930
	step [116/191], loss=75.9918
	step [117/191], loss=66.6862
	step [118/191], loss=56.2278
	step [119/191], loss=73.0450
	step [120/191], loss=68.8194
	step [121/191], loss=51.9780
	step [122/191], loss=77.7063
	step [123/191], loss=73.2663
	step [124/191], loss=62.1069
	step [125/191], loss=60.1618
	step [126/191], loss=51.9831
	step [127/191], loss=79.9181
	step [128/191], loss=72.5534
	step [129/191], loss=67.4102
	step [130/191], loss=58.6173
	step [131/191], loss=74.0111
	step [132/191], loss=68.2136
	step [133/191], loss=69.2026
	step [134/191], loss=66.7126
	step [135/191], loss=60.4624
	step [136/191], loss=66.6764
	step [137/191], loss=69.3766
	step [138/191], loss=68.3226
	step [139/191], loss=62.0114
	step [140/191], loss=70.5632
	step [141/191], loss=62.2110
	step [142/191], loss=74.0162
	step [143/191], loss=61.3001
	step [144/191], loss=57.8969
	step [145/191], loss=65.7664
	step [146/191], loss=61.4452
	step [147/191], loss=72.6557
	step [148/191], loss=73.8903
	step [149/191], loss=72.0637
	step [150/191], loss=58.3800
	step [151/191], loss=74.5280
	step [152/191], loss=62.3021
	step [153/191], loss=68.6504
	step [154/191], loss=70.0030
	step [155/191], loss=56.6465
	step [156/191], loss=59.6969
	step [157/191], loss=62.5807
	step [158/191], loss=72.4037
	step [159/191], loss=67.2231
	step [160/191], loss=73.7881
	step [161/191], loss=63.1139
	step [162/191], loss=62.8303
	step [163/191], loss=64.6192
	step [164/191], loss=62.5924
	step [165/191], loss=54.6950
	step [166/191], loss=63.8314
	step [167/191], loss=63.6624
	step [168/191], loss=71.2320
	step [169/191], loss=76.1636
	step [170/191], loss=72.0797
	step [171/191], loss=63.8180
	step [172/191], loss=64.9882
	step [173/191], loss=55.9114
	step [174/191], loss=60.7206
	step [175/191], loss=58.1991
	step [176/191], loss=74.1475
	step [177/191], loss=61.7935
	step [178/191], loss=63.7898
	step [179/191], loss=68.0582
	step [180/191], loss=73.2961
	step [181/191], loss=57.9642
	step [182/191], loss=56.0109
	step [183/191], loss=71.9502
	step [184/191], loss=59.6629
	step [185/191], loss=59.7106
	step [186/191], loss=62.8087
	step [187/191], loss=49.7540
	step [188/191], loss=71.3344
	step [189/191], loss=67.7006
	step [190/191], loss=68.8755
	step [191/191], loss=31.1935
	Evaluating
	loss=0.0071, precision=0.4563, recall=0.8515, f1=0.5942
Training epoch 87
	step [1/191], loss=61.8016
	step [2/191], loss=69.8333
	step [3/191], loss=65.1072
	step [4/191], loss=71.4622
	step [5/191], loss=60.8976
	step [6/191], loss=74.7476
	step [7/191], loss=82.0424
	step [8/191], loss=65.9301
	step [9/191], loss=63.4426
	step [10/191], loss=68.9275
	step [11/191], loss=65.5830
	step [12/191], loss=70.1008
	step [13/191], loss=58.5814
	step [14/191], loss=61.2931
	step [15/191], loss=68.4548
	step [16/191], loss=58.5395
	step [17/191], loss=71.1243
	step [18/191], loss=65.5542
	step [19/191], loss=70.5803
	step [20/191], loss=59.9689
	step [21/191], loss=55.2986
	step [22/191], loss=71.7066
	step [23/191], loss=60.9276
	step [24/191], loss=62.4799
	step [25/191], loss=75.4443
	step [26/191], loss=62.9939
	step [27/191], loss=60.4518
	step [28/191], loss=66.6517
	step [29/191], loss=63.3429
	step [30/191], loss=78.1535
	step [31/191], loss=68.9261
	step [32/191], loss=65.1995
	step [33/191], loss=65.9770
	step [34/191], loss=68.0092
	step [35/191], loss=69.1048
	step [36/191], loss=68.0475
	step [37/191], loss=80.6543
	step [38/191], loss=69.1825
	step [39/191], loss=63.2518
	step [40/191], loss=67.3437
	step [41/191], loss=57.3618
	step [42/191], loss=72.1260
	step [43/191], loss=58.6848
	step [44/191], loss=70.2815
	step [45/191], loss=64.7274
	step [46/191], loss=64.1485
	step [47/191], loss=70.8989
	step [48/191], loss=58.6131
	step [49/191], loss=67.3908
	step [50/191], loss=65.3389
	step [51/191], loss=67.5029
	step [52/191], loss=64.4473
	step [53/191], loss=58.6093
	step [54/191], loss=73.9262
	step [55/191], loss=69.9212
	step [56/191], loss=60.8666
	step [57/191], loss=64.8210
	step [58/191], loss=64.4885
	step [59/191], loss=72.2065
	step [60/191], loss=69.2022
	step [61/191], loss=64.7829
	step [62/191], loss=63.8936
	step [63/191], loss=55.4272
	step [64/191], loss=74.1228
	step [65/191], loss=68.9621
	step [66/191], loss=68.5286
	step [67/191], loss=62.2521
	step [68/191], loss=66.7340
	step [69/191], loss=61.6234
	step [70/191], loss=68.1290
	step [71/191], loss=44.5510
	step [72/191], loss=70.5941
	step [73/191], loss=75.5065
	step [74/191], loss=75.8299
	step [75/191], loss=69.0390
	step [76/191], loss=62.0351
	step [77/191], loss=66.0566
	step [78/191], loss=65.3766
	step [79/191], loss=59.4183
	step [80/191], loss=64.4346
	step [81/191], loss=69.0370
	step [82/191], loss=63.5268
	step [83/191], loss=64.3221
	step [84/191], loss=70.1776
	step [85/191], loss=63.0305
	step [86/191], loss=61.0867
	step [87/191], loss=67.6792
	step [88/191], loss=53.1914
	step [89/191], loss=75.9675
	step [90/191], loss=60.6296
	step [91/191], loss=67.9832
	step [92/191], loss=82.4852
	step [93/191], loss=55.6761
	step [94/191], loss=69.1612
	step [95/191], loss=59.6216
	step [96/191], loss=73.3971
	step [97/191], loss=70.7084
	step [98/191], loss=64.0060
	step [99/191], loss=63.2464
	step [100/191], loss=64.8820
	step [101/191], loss=64.0495
	step [102/191], loss=45.2196
	step [103/191], loss=52.4907
	step [104/191], loss=72.5412
	step [105/191], loss=63.5598
	step [106/191], loss=58.1180
	step [107/191], loss=69.8775
	step [108/191], loss=65.9966
	step [109/191], loss=65.2708
	step [110/191], loss=52.4714
	step [111/191], loss=70.2047
	step [112/191], loss=54.8865
	step [113/191], loss=73.0729
	step [114/191], loss=53.7302
	step [115/191], loss=74.0920
	step [116/191], loss=61.0388
	step [117/191], loss=63.1385
	step [118/191], loss=64.4862
	step [119/191], loss=68.7114
	step [120/191], loss=61.1481
	step [121/191], loss=74.8551
	step [122/191], loss=65.3797
	step [123/191], loss=73.4562
	step [124/191], loss=68.9610
	step [125/191], loss=82.4512
	step [126/191], loss=70.5238
	step [127/191], loss=69.1292
	step [128/191], loss=71.9582
	step [129/191], loss=62.4841
	step [130/191], loss=71.1108
	step [131/191], loss=72.7841
	step [132/191], loss=67.2165
	step [133/191], loss=66.1257
	step [134/191], loss=64.2746
	step [135/191], loss=57.1686
	step [136/191], loss=69.6306
	step [137/191], loss=70.9742
	step [138/191], loss=60.3000
	step [139/191], loss=65.2888
	step [140/191], loss=73.7632
	step [141/191], loss=64.9034
	step [142/191], loss=60.0282
	step [143/191], loss=66.8691
	step [144/191], loss=61.3452
	step [145/191], loss=62.2029
	step [146/191], loss=64.9539
	step [147/191], loss=67.0115
	step [148/191], loss=57.9019
	step [149/191], loss=60.9613
	step [150/191], loss=55.6717
	step [151/191], loss=68.9251
	step [152/191], loss=63.3415
	step [153/191], loss=64.7768
	step [154/191], loss=76.3350
	step [155/191], loss=57.5830
	step [156/191], loss=75.1912
	step [157/191], loss=74.0351
	step [158/191], loss=66.3769
	step [159/191], loss=74.8980
	step [160/191], loss=63.1417
	step [161/191], loss=67.1573
	step [162/191], loss=56.5247
	step [163/191], loss=64.0201
	step [164/191], loss=71.1414
	step [165/191], loss=47.9728
	step [166/191], loss=59.3333
	step [167/191], loss=64.4066
	step [168/191], loss=65.4197
	step [169/191], loss=65.6511
	step [170/191], loss=68.3030
	step [171/191], loss=62.5474
	step [172/191], loss=56.4323
	step [173/191], loss=71.7580
	step [174/191], loss=68.0207
	step [175/191], loss=55.1591
	step [176/191], loss=52.0814
	step [177/191], loss=69.9978
	step [178/191], loss=71.5484
	step [179/191], loss=63.6169
	step [180/191], loss=60.1380
	step [181/191], loss=70.9381
	step [182/191], loss=71.2093
	step [183/191], loss=64.9903
	step [184/191], loss=71.0654
	step [185/191], loss=75.1947
	step [186/191], loss=63.2698
	step [187/191], loss=54.9244
	step [188/191], loss=60.2400
	step [189/191], loss=72.4380
	step [190/191], loss=71.4468
	step [191/191], loss=34.9856
	Evaluating
	loss=0.0070, precision=0.4653, recall=0.8555, f1=0.6028
Training epoch 88
	step [1/191], loss=61.7595
	step [2/191], loss=73.4679
	step [3/191], loss=76.2988
	step [4/191], loss=64.7418
	step [5/191], loss=62.1380
	step [6/191], loss=65.2319
	step [7/191], loss=67.4763
	step [8/191], loss=58.0619
	step [9/191], loss=62.6209
	step [10/191], loss=67.0942
	step [11/191], loss=74.2994
	step [12/191], loss=64.8557
	step [13/191], loss=63.3311
	step [14/191], loss=63.6411
	step [15/191], loss=57.0420
	step [16/191], loss=56.4712
	step [17/191], loss=60.9942
	step [18/191], loss=74.2859
	step [19/191], loss=71.7048
	step [20/191], loss=69.3307
	step [21/191], loss=61.3161
	step [22/191], loss=58.7244
	step [23/191], loss=69.2151
	step [24/191], loss=60.6301
	step [25/191], loss=84.1145
	step [26/191], loss=51.8670
	step [27/191], loss=72.3339
	step [28/191], loss=67.4408
	step [29/191], loss=79.1829
	step [30/191], loss=57.0818
	step [31/191], loss=57.8056
	step [32/191], loss=73.2907
	step [33/191], loss=62.3885
	step [34/191], loss=67.1189
	step [35/191], loss=65.8512
	step [36/191], loss=64.9018
	step [37/191], loss=63.1374
	step [38/191], loss=66.0961
	step [39/191], loss=70.8970
	step [40/191], loss=81.3286
	step [41/191], loss=68.9240
	step [42/191], loss=65.2117
	step [43/191], loss=77.2809
	step [44/191], loss=62.5945
	step [45/191], loss=73.5571
	step [46/191], loss=80.1467
	step [47/191], loss=64.2725
	step [48/191], loss=62.2084
	step [49/191], loss=60.7484
	step [50/191], loss=64.4227
	step [51/191], loss=77.5951
	step [52/191], loss=68.0684
	step [53/191], loss=65.4939
	step [54/191], loss=47.7190
	step [55/191], loss=64.3285
	step [56/191], loss=65.2517
	step [57/191], loss=64.7493
	step [58/191], loss=60.6932
	step [59/191], loss=75.8745
	step [60/191], loss=76.8115
	step [61/191], loss=65.7792
	step [62/191], loss=62.5081
	step [63/191], loss=56.4896
	step [64/191], loss=68.2714
	step [65/191], loss=53.8410
	step [66/191], loss=71.2079
	step [67/191], loss=63.0992
	step [68/191], loss=68.6925
	step [69/191], loss=72.4427
	step [70/191], loss=69.9313
	step [71/191], loss=66.6354
	step [72/191], loss=72.2965
	step [73/191], loss=73.0041
	step [74/191], loss=55.8650
	step [75/191], loss=57.9029
	step [76/191], loss=75.3675
	step [77/191], loss=65.2627
	step [78/191], loss=60.3094
	step [79/191], loss=60.0544
	step [80/191], loss=73.9520
	step [81/191], loss=70.3651
	step [82/191], loss=56.0150
	step [83/191], loss=65.6289
	step [84/191], loss=70.7469
	step [85/191], loss=59.0140
	step [86/191], loss=70.2378
	step [87/191], loss=61.0850
	step [88/191], loss=73.7329
	step [89/191], loss=75.5759
	step [90/191], loss=56.0139
	step [91/191], loss=76.7391
	step [92/191], loss=69.4926
	step [93/191], loss=60.9534
	step [94/191], loss=63.4002
	step [95/191], loss=64.4564
	step [96/191], loss=66.5546
	step [97/191], loss=59.3507
	step [98/191], loss=57.9836
	step [99/191], loss=63.4942
	step [100/191], loss=59.7351
	step [101/191], loss=68.8195
	step [102/191], loss=54.5298
	step [103/191], loss=60.6763
	step [104/191], loss=70.9369
	step [105/191], loss=72.8210
	step [106/191], loss=62.8428
	step [107/191], loss=61.3007
	step [108/191], loss=50.9312
	step [109/191], loss=70.3093
	step [110/191], loss=46.5741
	step [111/191], loss=73.0287
	step [112/191], loss=68.9024
	step [113/191], loss=72.9908
	step [114/191], loss=52.1773
	step [115/191], loss=65.0120
	step [116/191], loss=65.3849
	step [117/191], loss=62.6003
	step [118/191], loss=71.1950
	step [119/191], loss=73.4317
	step [120/191], loss=69.7655
	step [121/191], loss=65.0924
	step [122/191], loss=48.0256
	step [123/191], loss=59.6073
	step [124/191], loss=76.9507
	step [125/191], loss=54.2772
	step [126/191], loss=69.1880
	step [127/191], loss=60.5137
	step [128/191], loss=81.7989
	step [129/191], loss=56.6448
	step [130/191], loss=60.3918
	step [131/191], loss=77.6013
	step [132/191], loss=80.3332
	step [133/191], loss=69.3159
	step [134/191], loss=61.4393
	step [135/191], loss=79.1162
	step [136/191], loss=69.7018
	step [137/191], loss=64.9032
	step [138/191], loss=57.7157
	step [139/191], loss=52.3831
	step [140/191], loss=68.6507
	step [141/191], loss=55.4963
	step [142/191], loss=65.6290
	step [143/191], loss=70.7905
	step [144/191], loss=72.2554
	step [145/191], loss=59.3488
	step [146/191], loss=61.6675
	step [147/191], loss=58.7701
	step [148/191], loss=63.8421
	step [149/191], loss=73.3013
	step [150/191], loss=67.7896
	step [151/191], loss=64.6574
	step [152/191], loss=63.7859
	step [153/191], loss=72.6306
	step [154/191], loss=68.4809
	step [155/191], loss=74.9488
	step [156/191], loss=65.5205
	step [157/191], loss=70.7448
	step [158/191], loss=65.9352
	step [159/191], loss=64.1795
	step [160/191], loss=81.1414
	step [161/191], loss=62.7008
	step [162/191], loss=58.3342
	step [163/191], loss=65.3913
	step [164/191], loss=58.0150
	step [165/191], loss=62.9474
	step [166/191], loss=57.5009
	step [167/191], loss=62.9479
	step [168/191], loss=59.0779
	step [169/191], loss=68.4570
	step [170/191], loss=64.6672
	step [171/191], loss=62.8094
	step [172/191], loss=57.0802
	step [173/191], loss=83.8345
	step [174/191], loss=66.4107
	step [175/191], loss=59.2363
	step [176/191], loss=70.1470
	step [177/191], loss=78.7589
	step [178/191], loss=68.1492
	step [179/191], loss=67.2832
	step [180/191], loss=69.0666
	step [181/191], loss=80.2504
	step [182/191], loss=68.9672
	step [183/191], loss=66.6461
	step [184/191], loss=76.4365
	step [185/191], loss=74.3483
	step [186/191], loss=71.6341
	step [187/191], loss=68.6217
	step [188/191], loss=76.1731
	step [189/191], loss=65.9984
	step [190/191], loss=67.7543
	step [191/191], loss=29.4181
	Evaluating
	loss=0.0077, precision=0.4401, recall=0.8512, f1=0.5802
Training epoch 89
	step [1/191], loss=60.0413
	step [2/191], loss=65.7527
	step [3/191], loss=60.2737
	step [4/191], loss=66.9292
	step [5/191], loss=66.6997
	step [6/191], loss=76.8175
	step [7/191], loss=48.6359
	step [8/191], loss=68.2638
	step [9/191], loss=62.3716
	step [10/191], loss=55.7792
	step [11/191], loss=70.2481
	step [12/191], loss=64.4675
	step [13/191], loss=50.9130
	step [14/191], loss=73.8912
	step [15/191], loss=60.0006
	step [16/191], loss=58.9545
	step [17/191], loss=61.0570
	step [18/191], loss=69.6283
	step [19/191], loss=58.3927
	step [20/191], loss=69.1053
	step [21/191], loss=57.7565
	step [22/191], loss=61.1039
	step [23/191], loss=71.5581
	step [24/191], loss=69.2482
	step [25/191], loss=68.7028
	step [26/191], loss=66.6694
	step [27/191], loss=66.0764
	step [28/191], loss=76.0792
	step [29/191], loss=67.4838
	step [30/191], loss=65.8913
	step [31/191], loss=68.1986
	step [32/191], loss=74.5460
	step [33/191], loss=69.5325
	step [34/191], loss=77.1999
	step [35/191], loss=66.3212
	step [36/191], loss=65.9734
	step [37/191], loss=57.5602
	step [38/191], loss=57.7100
	step [39/191], loss=76.7444
	step [40/191], loss=61.0461
	step [41/191], loss=69.7549
	step [42/191], loss=61.6833
	step [43/191], loss=79.1038
	step [44/191], loss=61.3073
	step [45/191], loss=61.3601
	step [46/191], loss=75.0646
	step [47/191], loss=54.9347
	step [48/191], loss=63.4723
	step [49/191], loss=70.0420
	step [50/191], loss=62.9841
	step [51/191], loss=65.4956
	step [52/191], loss=67.7282
	step [53/191], loss=66.6888
	step [54/191], loss=69.4723
	step [55/191], loss=71.0145
	step [56/191], loss=69.1378
	step [57/191], loss=66.4116
	step [58/191], loss=83.7916
	step [59/191], loss=74.4974
	step [60/191], loss=57.2849
	step [61/191], loss=58.6114
	step [62/191], loss=59.6244
	step [63/191], loss=68.9622
	step [64/191], loss=53.1744
	step [65/191], loss=66.4310
	step [66/191], loss=60.7465
	step [67/191], loss=66.8007
	step [68/191], loss=66.1484
	step [69/191], loss=65.0425
	step [70/191], loss=72.4880
	step [71/191], loss=74.3024
	step [72/191], loss=62.3763
	step [73/191], loss=58.3005
	step [74/191], loss=58.5570
	step [75/191], loss=60.8721
	step [76/191], loss=59.9817
	step [77/191], loss=73.9268
	step [78/191], loss=60.5526
	step [79/191], loss=64.7481
	step [80/191], loss=66.7729
	step [81/191], loss=66.7312
	step [82/191], loss=68.6772
	step [83/191], loss=65.4381
	step [84/191], loss=73.3097
	step [85/191], loss=71.4156
	step [86/191], loss=65.7169
	step [87/191], loss=62.5344
	step [88/191], loss=69.9816
	step [89/191], loss=67.2483
	step [90/191], loss=65.5186
	step [91/191], loss=56.8102
	step [92/191], loss=69.0539
	step [93/191], loss=56.2241
	step [94/191], loss=79.7224
	step [95/191], loss=56.1978
	step [96/191], loss=75.5830
	step [97/191], loss=61.8790
	step [98/191], loss=64.0678
	step [99/191], loss=83.1098
	step [100/191], loss=63.5606
	step [101/191], loss=56.0904
	step [102/191], loss=69.4091
	step [103/191], loss=67.3010
	step [104/191], loss=72.7720
	step [105/191], loss=62.0001
	step [106/191], loss=76.2444
	step [107/191], loss=59.6912
	step [108/191], loss=73.7683
	step [109/191], loss=70.0811
	step [110/191], loss=56.4030
	step [111/191], loss=75.4497
	step [112/191], loss=63.8517
	step [113/191], loss=62.0457
	step [114/191], loss=77.8116
	step [115/191], loss=78.2840
	step [116/191], loss=64.6453
	step [117/191], loss=59.1716
	step [118/191], loss=67.8572
	step [119/191], loss=59.6855
	step [120/191], loss=59.8436
	step [121/191], loss=66.7210
	step [122/191], loss=63.8274
	step [123/191], loss=68.1348
	step [124/191], loss=67.5506
	step [125/191], loss=60.2579
	step [126/191], loss=56.3902
	step [127/191], loss=69.1982
	step [128/191], loss=78.2084
	step [129/191], loss=62.2077
	step [130/191], loss=61.1575
	step [131/191], loss=60.9022
	step [132/191], loss=64.9967
	step [133/191], loss=66.5858
	step [134/191], loss=71.7562
	step [135/191], loss=65.3370
	step [136/191], loss=70.5479
	step [137/191], loss=54.7424
	step [138/191], loss=55.0126
	step [139/191], loss=59.2938
	step [140/191], loss=69.1170
	step [141/191], loss=60.2066
	step [142/191], loss=68.5434
	step [143/191], loss=68.1919
	step [144/191], loss=64.2729
	step [145/191], loss=60.6288
	step [146/191], loss=64.7738
	step [147/191], loss=80.4319
	step [148/191], loss=68.3315
	step [149/191], loss=65.1133
	step [150/191], loss=64.8562
	step [151/191], loss=72.8370
	step [152/191], loss=60.4064
	step [153/191], loss=64.9306
	step [154/191], loss=60.6865
	step [155/191], loss=70.6735
	step [156/191], loss=61.6113
	step [157/191], loss=54.1368
	step [158/191], loss=68.9148
	step [159/191], loss=60.1358
	step [160/191], loss=79.3807
	step [161/191], loss=76.6135
	step [162/191], loss=65.1782
	step [163/191], loss=50.2507
	step [164/191], loss=68.4687
	step [165/191], loss=76.0478
	step [166/191], loss=61.8910
	step [167/191], loss=56.6594
	step [168/191], loss=66.3187
	step [169/191], loss=54.8743
	step [170/191], loss=54.7193
	step [171/191], loss=58.4939
	step [172/191], loss=67.3184
	step [173/191], loss=61.8982
	step [174/191], loss=67.4330
	step [175/191], loss=70.7746
	step [176/191], loss=60.8517
	step [177/191], loss=61.1082
	step [178/191], loss=69.5500
	step [179/191], loss=67.2079
	step [180/191], loss=64.2314
	step [181/191], loss=71.8199
	step [182/191], loss=50.9419
	step [183/191], loss=54.5429
	step [184/191], loss=64.6192
	step [185/191], loss=64.6289
	step [186/191], loss=62.0663
	step [187/191], loss=72.2100
	step [188/191], loss=70.4034
	step [189/191], loss=67.7522
	step [190/191], loss=60.1754
	step [191/191], loss=29.9952
	Evaluating
	loss=0.0070, precision=0.4581, recall=0.8509, f1=0.5956
Training epoch 90
	step [1/191], loss=54.9324
	step [2/191], loss=68.2866
	step [3/191], loss=70.9914
	step [4/191], loss=65.0883
	step [5/191], loss=61.7412
	step [6/191], loss=75.5533
	step [7/191], loss=57.7205
	step [8/191], loss=59.2197
	step [9/191], loss=72.7768
	step [10/191], loss=74.4990
	step [11/191], loss=60.0217
	step [12/191], loss=57.0435
	step [13/191], loss=60.6323
	step [14/191], loss=64.1712
	step [15/191], loss=60.4043
	step [16/191], loss=69.9186
	step [17/191], loss=65.4037
	step [18/191], loss=58.7717
	step [19/191], loss=66.2751
	step [20/191], loss=54.9945
	step [21/191], loss=56.7165
	step [22/191], loss=71.5291
	step [23/191], loss=61.1566
	step [24/191], loss=60.3795
	step [25/191], loss=57.3989
	step [26/191], loss=57.2979
	step [27/191], loss=59.7097
	step [28/191], loss=58.8674
	step [29/191], loss=76.5833
	step [30/191], loss=55.2708
	step [31/191], loss=64.9226
	step [32/191], loss=66.8423
	step [33/191], loss=52.9869
	step [34/191], loss=68.6592
	step [35/191], loss=58.3926
	step [36/191], loss=68.7852
	step [37/191], loss=81.1660
	step [38/191], loss=62.4493
	step [39/191], loss=86.7819
	step [40/191], loss=62.1280
	step [41/191], loss=75.5548
	step [42/191], loss=48.5564
	step [43/191], loss=75.7580
	step [44/191], loss=67.3329
	step [45/191], loss=72.5208
	step [46/191], loss=72.3872
	step [47/191], loss=56.7497
	step [48/191], loss=70.5865
	step [49/191], loss=58.2478
	step [50/191], loss=60.1240
	step [51/191], loss=69.0389
	step [52/191], loss=69.6716
	step [53/191], loss=60.1328
	step [54/191], loss=65.9550
	step [55/191], loss=62.3870
	step [56/191], loss=67.3299
	step [57/191], loss=67.0297
	step [58/191], loss=58.2997
	step [59/191], loss=71.9591
	step [60/191], loss=61.1058
	step [61/191], loss=60.7927
	step [62/191], loss=69.6732
	step [63/191], loss=82.6833
	step [64/191], loss=67.8811
	step [65/191], loss=64.9081
	step [66/191], loss=67.4979
	step [67/191], loss=65.3931
	step [68/191], loss=75.3879
	step [69/191], loss=69.3339
	step [70/191], loss=66.8062
	step [71/191], loss=61.8576
	step [72/191], loss=62.2519
	step [73/191], loss=58.3631
	step [74/191], loss=66.1678
	step [75/191], loss=60.5609
	step [76/191], loss=60.7790
	step [77/191], loss=58.8037
	step [78/191], loss=62.9514
	step [79/191], loss=72.0760
	step [80/191], loss=59.3166
	step [81/191], loss=60.9331
	step [82/191], loss=67.9752
	step [83/191], loss=66.4692
	step [84/191], loss=60.8047
	step [85/191], loss=61.7425
	step [86/191], loss=71.3506
	step [87/191], loss=56.9661
	step [88/191], loss=63.6680
	step [89/191], loss=68.2180
	step [90/191], loss=67.3197
	step [91/191], loss=65.2478
	step [92/191], loss=63.7323
	step [93/191], loss=60.4096
	step [94/191], loss=74.4082
	step [95/191], loss=57.7980
	step [96/191], loss=64.5241
	step [97/191], loss=59.8210
	step [98/191], loss=68.2238
	step [99/191], loss=66.0600
	step [100/191], loss=73.3519
	step [101/191], loss=69.5886
	step [102/191], loss=70.4610
	step [103/191], loss=71.0533
	step [104/191], loss=74.7258
	step [105/191], loss=73.8588
	step [106/191], loss=65.8107
	step [107/191], loss=67.7766
	step [108/191], loss=62.6467
	step [109/191], loss=70.1782
	step [110/191], loss=59.6827
	step [111/191], loss=73.4143
	step [112/191], loss=65.6127
	step [113/191], loss=86.2389
	step [114/191], loss=56.7768
	step [115/191], loss=70.4794
	step [116/191], loss=59.2371
	step [117/191], loss=63.9711
	step [118/191], loss=69.8840
	step [119/191], loss=82.0520
	step [120/191], loss=69.0871
	step [121/191], loss=68.4861
	step [122/191], loss=64.2428
	step [123/191], loss=64.7953
	step [124/191], loss=64.7808
	step [125/191], loss=60.4348
	step [126/191], loss=56.0098
	step [127/191], loss=66.7031
	step [128/191], loss=71.9555
	step [129/191], loss=78.8944
	step [130/191], loss=65.5098
	step [131/191], loss=69.5402
	step [132/191], loss=70.9960
	step [133/191], loss=65.6375
	step [134/191], loss=66.7877
	step [135/191], loss=65.1882
	step [136/191], loss=61.8294
	step [137/191], loss=69.4022
	step [138/191], loss=66.7352
	step [139/191], loss=64.4200
	step [140/191], loss=61.3113
	step [141/191], loss=70.7177
	step [142/191], loss=59.2546
	step [143/191], loss=57.1860
	step [144/191], loss=63.0024
	step [145/191], loss=74.6384
	step [146/191], loss=60.8853
	step [147/191], loss=59.6469
	step [148/191], loss=57.3861
	step [149/191], loss=60.6457
	step [150/191], loss=58.4164
	step [151/191], loss=55.3398
	step [152/191], loss=65.8838
	step [153/191], loss=70.8121
	step [154/191], loss=63.9680
	step [155/191], loss=76.3984
	step [156/191], loss=67.0348
	step [157/191], loss=58.9690
	step [158/191], loss=76.3529
	step [159/191], loss=61.7875
	step [160/191], loss=61.9132
	step [161/191], loss=66.3477
	step [162/191], loss=54.8838
	step [163/191], loss=60.6333
	step [164/191], loss=57.4587
	step [165/191], loss=77.2657
	step [166/191], loss=53.5774
	step [167/191], loss=69.4128
	step [168/191], loss=65.5229
	step [169/191], loss=69.0892
	step [170/191], loss=59.0165
	step [171/191], loss=70.3673
	step [172/191], loss=69.7438
	step [173/191], loss=56.6323
	step [174/191], loss=60.1658
	step [175/191], loss=54.9917
	step [176/191], loss=75.0348
	step [177/191], loss=59.9406
	step [178/191], loss=67.8893
	step [179/191], loss=68.3674
	step [180/191], loss=73.2041
	step [181/191], loss=72.6343
	step [182/191], loss=66.7164
	step [183/191], loss=59.2154
	step [184/191], loss=61.3001
	step [185/191], loss=63.4600
	step [186/191], loss=67.4888
	step [187/191], loss=65.8134
	step [188/191], loss=65.7860
	step [189/191], loss=61.0313
	step [190/191], loss=66.6748
	step [191/191], loss=30.8799
	Evaluating
	loss=0.0059, precision=0.5264, recall=0.8493, f1=0.6499
saving model as: 1_saved_model.pth
Training epoch 91
	step [1/191], loss=71.3903
	step [2/191], loss=52.4322
	step [3/191], loss=66.3895
	step [4/191], loss=75.4621
	step [5/191], loss=61.3358
	step [6/191], loss=67.9121
	step [7/191], loss=65.4361
	step [8/191], loss=66.1497
	step [9/191], loss=62.8439
	step [10/191], loss=59.9300
	step [11/191], loss=74.0502
	step [12/191], loss=71.5695
	step [13/191], loss=71.5439
	step [14/191], loss=70.2154
	step [15/191], loss=62.7306
	step [16/191], loss=58.0853
	step [17/191], loss=50.9934
	step [18/191], loss=64.3057
	step [19/191], loss=60.9868
	step [20/191], loss=66.2911
	step [21/191], loss=53.3256
	step [22/191], loss=65.6799
	step [23/191], loss=60.9823
	step [24/191], loss=62.6168
	step [25/191], loss=56.0976
	step [26/191], loss=59.8809
	step [27/191], loss=65.9427
	step [28/191], loss=74.3438
	step [29/191], loss=60.2146
	step [30/191], loss=65.7173
	step [31/191], loss=60.8971
	step [32/191], loss=67.0182
	step [33/191], loss=70.8987
	step [34/191], loss=69.2956
	step [35/191], loss=49.4604
	step [36/191], loss=67.9839
	step [37/191], loss=66.2988
	step [38/191], loss=78.9258
	step [39/191], loss=62.1393
	step [40/191], loss=59.3606
	step [41/191], loss=57.9526
	step [42/191], loss=65.4463
	step [43/191], loss=69.5934
	step [44/191], loss=58.8991
	step [45/191], loss=63.5265
	step [46/191], loss=60.0147
	step [47/191], loss=66.9771
	step [48/191], loss=66.1958
	step [49/191], loss=53.3033
	step [50/191], loss=58.9595
	step [51/191], loss=62.5613
	step [52/191], loss=69.6675
	step [53/191], loss=59.9085
	step [54/191], loss=58.3704
	step [55/191], loss=52.0680
	step [56/191], loss=65.5629
	step [57/191], loss=65.6095
	step [58/191], loss=67.1704
	step [59/191], loss=65.9906
	step [60/191], loss=63.5890
	step [61/191], loss=73.4988
	step [62/191], loss=64.2529
	step [63/191], loss=73.7297
	step [64/191], loss=79.3164
	step [65/191], loss=52.3066
	step [66/191], loss=58.8691
	step [67/191], loss=61.7266
	step [68/191], loss=65.7562
	step [69/191], loss=66.6965
	step [70/191], loss=62.6688
	step [71/191], loss=64.3693
	step [72/191], loss=65.4549
	step [73/191], loss=57.5183
	step [74/191], loss=61.7546
	step [75/191], loss=68.7103
	step [76/191], loss=59.8206
	step [77/191], loss=61.3920
	step [78/191], loss=59.2742
	step [79/191], loss=63.9192
	step [80/191], loss=69.0518
	step [81/191], loss=61.7409
	step [82/191], loss=65.6378
	step [83/191], loss=58.5928
	step [84/191], loss=58.5578
	step [85/191], loss=70.7233
	step [86/191], loss=64.0728
	step [87/191], loss=62.0977
	step [88/191], loss=56.3971
	step [89/191], loss=60.1595
	step [90/191], loss=67.9890
	step [91/191], loss=84.1819
	step [92/191], loss=62.4851
	step [93/191], loss=62.7112
	step [94/191], loss=80.4134
	step [95/191], loss=75.3583
	step [96/191], loss=64.1594
	step [97/191], loss=69.6728
	step [98/191], loss=65.7959
	step [99/191], loss=55.2584
	step [100/191], loss=62.1652
	step [101/191], loss=62.5780
	step [102/191], loss=52.0168
	step [103/191], loss=71.9977
	step [104/191], loss=66.9428
	step [105/191], loss=80.8180
	step [106/191], loss=68.0836
	step [107/191], loss=56.0219
	step [108/191], loss=63.8985
	step [109/191], loss=70.5561
	step [110/191], loss=81.4335
	step [111/191], loss=64.2364
	step [112/191], loss=54.3958
	step [113/191], loss=63.4042
	step [114/191], loss=72.2503
	step [115/191], loss=70.8131
	step [116/191], loss=72.9539
	step [117/191], loss=64.4697
	step [118/191], loss=63.7753
	step [119/191], loss=81.3726
	step [120/191], loss=62.9342
	step [121/191], loss=65.0618
	step [122/191], loss=74.0772
	step [123/191], loss=74.7610
	step [124/191], loss=71.3543
	step [125/191], loss=81.6646
	step [126/191], loss=63.7141
	step [127/191], loss=59.9864
	step [128/191], loss=74.9770
	step [129/191], loss=66.2019
	step [130/191], loss=70.3229
	step [131/191], loss=65.3999
	step [132/191], loss=81.6668
	step [133/191], loss=64.5293
	step [134/191], loss=69.8108
	step [135/191], loss=76.8823
	step [136/191], loss=65.7625
	step [137/191], loss=66.0325
	step [138/191], loss=69.0063
	step [139/191], loss=71.7704
	step [140/191], loss=74.8777
	step [141/191], loss=65.2308
	step [142/191], loss=67.7455
	step [143/191], loss=62.0962
	step [144/191], loss=66.1218
	step [145/191], loss=61.2228
	step [146/191], loss=70.0274
	step [147/191], loss=58.3934
	step [148/191], loss=61.6789
	step [149/191], loss=52.3782
	step [150/191], loss=72.7472
	step [151/191], loss=72.2940
	step [152/191], loss=59.9079
	step [153/191], loss=57.3456
	step [154/191], loss=72.8544
	step [155/191], loss=61.9488
	step [156/191], loss=61.2537
	step [157/191], loss=67.1388
	step [158/191], loss=58.0985
	step [159/191], loss=63.1422
	step [160/191], loss=73.0778
	step [161/191], loss=67.0983
	step [162/191], loss=77.4674
	step [163/191], loss=73.5128
	step [164/191], loss=61.4034
	step [165/191], loss=65.1024
	step [166/191], loss=71.1068
	step [167/191], loss=63.6192
	step [168/191], loss=64.4928
	step [169/191], loss=64.6457
	step [170/191], loss=61.7484
	step [171/191], loss=74.1969
	step [172/191], loss=58.6654
	step [173/191], loss=66.1670
	step [174/191], loss=71.8079
	step [175/191], loss=62.7660
	step [176/191], loss=65.1155
	step [177/191], loss=54.4627
	step [178/191], loss=64.1718
	step [179/191], loss=62.5321
	step [180/191], loss=59.7740
	step [181/191], loss=58.4749
	step [182/191], loss=53.3122
	step [183/191], loss=62.8806
	step [184/191], loss=62.2997
	step [185/191], loss=57.4497
	step [186/191], loss=70.8872
	step [187/191], loss=68.3482
	step [188/191], loss=55.6176
	step [189/191], loss=62.8860
	step [190/191], loss=58.9350
	step [191/191], loss=29.2433
	Evaluating
	loss=0.0066, precision=0.4741, recall=0.8527, f1=0.6094
Training epoch 92
	step [1/191], loss=68.4019
	step [2/191], loss=73.5498
	step [3/191], loss=48.8619
	step [4/191], loss=62.1590
	step [5/191], loss=59.1188
	step [6/191], loss=68.2148
	step [7/191], loss=60.0270
	step [8/191], loss=77.4119
	step [9/191], loss=69.8044
	step [10/191], loss=58.4988
	step [11/191], loss=63.7052
	step [12/191], loss=60.9517
	step [13/191], loss=73.7376
	step [14/191], loss=70.2901
	step [15/191], loss=59.2844
	step [16/191], loss=60.0518
	step [17/191], loss=65.5982
	step [18/191], loss=74.5084
	step [19/191], loss=61.0031
	step [20/191], loss=71.3230
	step [21/191], loss=62.4528
	step [22/191], loss=73.1703
	step [23/191], loss=70.2850
	step [24/191], loss=68.3262
	step [25/191], loss=63.7028
	step [26/191], loss=66.5102
	step [27/191], loss=61.7799
	step [28/191], loss=72.5732
	step [29/191], loss=61.8552
	step [30/191], loss=67.6722
	step [31/191], loss=63.8799
	step [32/191], loss=74.0144
	step [33/191], loss=56.9703
	step [34/191], loss=52.9322
	step [35/191], loss=57.6943
	step [36/191], loss=91.2598
	step [37/191], loss=55.9829
	step [38/191], loss=65.4565
	step [39/191], loss=57.2305
	step [40/191], loss=59.1617
	step [41/191], loss=68.1792
	step [42/191], loss=68.5182
	step [43/191], loss=56.7384
	step [44/191], loss=65.4413
	step [45/191], loss=69.1856
	step [46/191], loss=63.0695
	step [47/191], loss=63.9095
	step [48/191], loss=61.0548
	step [49/191], loss=63.7660
	step [50/191], loss=63.2047
	step [51/191], loss=58.5686
	step [52/191], loss=61.4815
	step [53/191], loss=63.9810
	step [54/191], loss=70.4213
	step [55/191], loss=56.7762
	step [56/191], loss=62.7782
	step [57/191], loss=65.5744
	step [58/191], loss=58.1161
	step [59/191], loss=58.4698
	step [60/191], loss=58.4977
	step [61/191], loss=65.5510
	step [62/191], loss=64.7981
	step [63/191], loss=56.5917
	step [64/191], loss=67.0969
	step [65/191], loss=69.0200
	step [66/191], loss=66.6572
	step [67/191], loss=61.5913
	step [68/191], loss=69.3501
	step [69/191], loss=58.2951
	step [70/191], loss=75.8554
	step [71/191], loss=63.3355
	step [72/191], loss=56.4452
	step [73/191], loss=60.4499
	step [74/191], loss=63.3690
	step [75/191], loss=66.8473
	step [76/191], loss=55.3181
	step [77/191], loss=56.4953
	step [78/191], loss=68.1381
	step [79/191], loss=53.4533
	step [80/191], loss=71.9596
	step [81/191], loss=56.6133
	step [82/191], loss=73.3516
	step [83/191], loss=66.2042
	step [84/191], loss=71.9861
	step [85/191], loss=60.2503
	step [86/191], loss=63.7168
	step [87/191], loss=54.1669
	step [88/191], loss=65.0250
	step [89/191], loss=59.2801
	step [90/191], loss=60.7900
	step [91/191], loss=66.5028
	step [92/191], loss=57.3095
	step [93/191], loss=66.6247
	step [94/191], loss=57.7382
	step [95/191], loss=64.8396
	step [96/191], loss=74.2219
	step [97/191], loss=58.5177
	step [98/191], loss=52.2113
	step [99/191], loss=61.6964
	step [100/191], loss=66.3717
	step [101/191], loss=74.7197
	step [102/191], loss=77.0521
	step [103/191], loss=63.6638
	step [104/191], loss=61.6557
	step [105/191], loss=61.8920
	step [106/191], loss=75.4842
	step [107/191], loss=75.7680
	step [108/191], loss=57.8978
	step [109/191], loss=61.6828
	step [110/191], loss=59.5499
	step [111/191], loss=75.4059
	step [112/191], loss=73.9348
	step [113/191], loss=52.3174
	step [114/191], loss=61.7937
	step [115/191], loss=66.4990
	step [116/191], loss=69.1443
	step [117/191], loss=56.4445
	step [118/191], loss=59.6313
	step [119/191], loss=74.4128
	step [120/191], loss=65.2423
	step [121/191], loss=72.6020
	step [122/191], loss=72.7951
	step [123/191], loss=76.2547
	step [124/191], loss=60.4257
	step [125/191], loss=59.4851
	step [126/191], loss=71.4185
	step [127/191], loss=78.5673
	step [128/191], loss=60.1945
	step [129/191], loss=62.2156
	step [130/191], loss=62.3228
	step [131/191], loss=64.5004
	step [132/191], loss=83.3247
	step [133/191], loss=66.7129
	step [134/191], loss=74.9226
	step [135/191], loss=75.8734
	step [136/191], loss=59.1784
	step [137/191], loss=67.5618
	step [138/191], loss=81.1001
	step [139/191], loss=60.5153
	step [140/191], loss=66.5764
	step [141/191], loss=60.0581
	step [142/191], loss=61.0293
	step [143/191], loss=63.2996
	step [144/191], loss=72.4920
	step [145/191], loss=62.0307
	step [146/191], loss=53.6628
	step [147/191], loss=63.5405
	step [148/191], loss=58.3465
	step [149/191], loss=59.6492
	step [150/191], loss=81.4877
	step [151/191], loss=62.2752
	step [152/191], loss=71.9458
	step [153/191], loss=67.7908
	step [154/191], loss=69.0707
	step [155/191], loss=72.3491
	step [156/191], loss=59.4745
	step [157/191], loss=61.5949
	step [158/191], loss=69.6402
	step [159/191], loss=65.9446
	step [160/191], loss=60.2702
	step [161/191], loss=73.4212
	step [162/191], loss=64.2474
	step [163/191], loss=59.9828
	step [164/191], loss=69.8505
	step [165/191], loss=66.6976
	step [166/191], loss=70.8834
	step [167/191], loss=58.4179
	step [168/191], loss=69.3821
	step [169/191], loss=71.1834
	step [170/191], loss=61.4168
	step [171/191], loss=63.0021
	step [172/191], loss=60.5555
	step [173/191], loss=61.4308
	step [174/191], loss=60.6292
	step [175/191], loss=73.1337
	step [176/191], loss=69.4817
	step [177/191], loss=74.4388
	step [178/191], loss=58.4252
	step [179/191], loss=59.6089
	step [180/191], loss=68.9391
	step [181/191], loss=64.1764
	step [182/191], loss=69.5683
	step [183/191], loss=59.4327
	step [184/191], loss=73.6414
	step [185/191], loss=60.2945
	step [186/191], loss=67.9743
	step [187/191], loss=57.9784
	step [188/191], loss=78.4190
	step [189/191], loss=67.2910
	step [190/191], loss=62.6466
	step [191/191], loss=29.6373
	Evaluating
	loss=0.0074, precision=0.4441, recall=0.8586, f1=0.5854
Training epoch 93
	step [1/191], loss=64.5329
	step [2/191], loss=68.7102
	step [3/191], loss=53.2178
	step [4/191], loss=65.8592
	step [5/191], loss=58.1191
	step [6/191], loss=71.5510
	step [7/191], loss=63.0559
	step [8/191], loss=65.4911
	step [9/191], loss=63.6236
	step [10/191], loss=66.7415
	step [11/191], loss=62.0445
	step [12/191], loss=65.9118
	step [13/191], loss=64.3448
	step [14/191], loss=64.6608
	step [15/191], loss=56.6127
	step [16/191], loss=66.8238
	step [17/191], loss=66.4146
	step [18/191], loss=65.6208
	step [19/191], loss=66.8156
	step [20/191], loss=64.3480
	step [21/191], loss=71.2359
	step [22/191], loss=60.2817
	step [23/191], loss=53.4013
	step [24/191], loss=57.8788
	step [25/191], loss=77.0449
	step [26/191], loss=67.4732
	step [27/191], loss=53.7797
	step [28/191], loss=60.2052
	step [29/191], loss=68.6993
	step [30/191], loss=68.7089
	step [31/191], loss=66.0557
	step [32/191], loss=64.4321
	step [33/191], loss=61.1267
	step [34/191], loss=62.7403
	step [35/191], loss=57.2307
	step [36/191], loss=67.6548
	step [37/191], loss=72.9076
	step [38/191], loss=73.3650
	step [39/191], loss=55.8314
	step [40/191], loss=61.4603
	step [41/191], loss=62.0964
	step [42/191], loss=72.7811
	step [43/191], loss=66.4635
	step [44/191], loss=73.4033
	step [45/191], loss=62.3969
	step [46/191], loss=60.0001
	step [47/191], loss=64.5660
	step [48/191], loss=76.9263
	step [49/191], loss=56.9268
	step [50/191], loss=68.5173
	step [51/191], loss=65.4545
	step [52/191], loss=57.4755
	step [53/191], loss=61.6201
	step [54/191], loss=64.6150
	step [55/191], loss=55.1978
	step [56/191], loss=63.6481
	step [57/191], loss=82.2679
	step [58/191], loss=62.3522
	step [59/191], loss=75.4744
	step [60/191], loss=69.1851
	step [61/191], loss=57.9740
	step [62/191], loss=67.3834
	step [63/191], loss=68.4379
	step [64/191], loss=58.6153
	step [65/191], loss=55.2322
	step [66/191], loss=66.8185
	step [67/191], loss=69.3002
	step [68/191], loss=76.2925
	step [69/191], loss=61.4149
	step [70/191], loss=51.4865
	step [71/191], loss=57.1375
	step [72/191], loss=66.4226
	step [73/191], loss=62.4716
	step [74/191], loss=68.4940
	step [75/191], loss=67.3378
	step [76/191], loss=56.9324
	step [77/191], loss=61.2300
	step [78/191], loss=72.0784
	step [79/191], loss=62.2143
	step [80/191], loss=58.1090
	step [81/191], loss=62.6551
	step [82/191], loss=71.1300
	step [83/191], loss=59.5102
	step [84/191], loss=68.6527
	step [85/191], loss=68.7888
	step [86/191], loss=70.3656
	step [87/191], loss=62.9131
	step [88/191], loss=57.7678
	step [89/191], loss=65.3249
	step [90/191], loss=69.8828
	step [91/191], loss=64.1264
	step [92/191], loss=54.2101
	step [93/191], loss=67.8904
	step [94/191], loss=58.6527
	step [95/191], loss=69.5547
	step [96/191], loss=72.6386
	step [97/191], loss=63.7523
	step [98/191], loss=63.0013
	step [99/191], loss=70.3813
	step [100/191], loss=72.3889
	step [101/191], loss=72.5033
	step [102/191], loss=60.4518
	step [103/191], loss=74.9174
	step [104/191], loss=61.8047
	step [105/191], loss=66.9071
	step [106/191], loss=80.9664
	step [107/191], loss=66.5669
	step [108/191], loss=63.2196
	step [109/191], loss=62.9287
	step [110/191], loss=70.2042
	step [111/191], loss=59.2642
	step [112/191], loss=56.2179
	step [113/191], loss=59.5492
	step [114/191], loss=66.3398
	step [115/191], loss=58.5115
	step [116/191], loss=64.3429
	step [117/191], loss=72.3997
	step [118/191], loss=67.9052
	step [119/191], loss=63.8560
	step [120/191], loss=59.4047
	step [121/191], loss=56.9240
	step [122/191], loss=53.3157
	step [123/191], loss=71.4766
	step [124/191], loss=70.8022
	step [125/191], loss=54.5518
	step [126/191], loss=65.0510
	step [127/191], loss=56.4002
	step [128/191], loss=58.5437
	step [129/191], loss=66.1168
	step [130/191], loss=61.8368
	step [131/191], loss=62.7645
	step [132/191], loss=74.9137
	step [133/191], loss=55.8937
	step [134/191], loss=62.4301
	step [135/191], loss=69.7041
	step [136/191], loss=60.2810
	step [137/191], loss=74.4849
	step [138/191], loss=69.2527
	step [139/191], loss=63.7815
	step [140/191], loss=72.7932
	step [141/191], loss=74.6812
	step [142/191], loss=69.9914
	step [143/191], loss=65.9959
	step [144/191], loss=51.0060
	step [145/191], loss=84.6287
	step [146/191], loss=67.8142
	step [147/191], loss=69.3336
	step [148/191], loss=62.4811
	step [149/191], loss=65.4241
	step [150/191], loss=78.2576
	step [151/191], loss=70.6772
	step [152/191], loss=72.1439
	step [153/191], loss=61.1170
	step [154/191], loss=59.5728
	step [155/191], loss=68.9487
	step [156/191], loss=64.6259
	step [157/191], loss=73.9628
	step [158/191], loss=58.1878
	step [159/191], loss=60.9843
	step [160/191], loss=59.4218
	step [161/191], loss=59.7349
	step [162/191], loss=53.5532
	step [163/191], loss=67.8129
	step [164/191], loss=63.3969
	step [165/191], loss=56.1144
	step [166/191], loss=64.5813
	step [167/191], loss=67.0782
	step [168/191], loss=46.9315
	step [169/191], loss=60.8873
	step [170/191], loss=60.3307
	step [171/191], loss=73.0737
	step [172/191], loss=66.8964
	step [173/191], loss=64.5415
	step [174/191], loss=65.7135
	step [175/191], loss=67.1041
	step [176/191], loss=77.0179
	step [177/191], loss=59.3086
	step [178/191], loss=63.3432
	step [179/191], loss=53.6441
	step [180/191], loss=73.6374
	step [181/191], loss=72.2094
	step [182/191], loss=67.4634
	step [183/191], loss=65.0372
	step [184/191], loss=68.7336
	step [185/191], loss=63.8763
	step [186/191], loss=67.4256
	step [187/191], loss=68.0208
	step [188/191], loss=55.2785
	step [189/191], loss=68.9567
	step [190/191], loss=64.9674
	step [191/191], loss=26.1296
	Evaluating
	loss=0.0069, precision=0.4640, recall=0.8586, f1=0.6024
Training epoch 94
	step [1/191], loss=55.5881
	step [2/191], loss=67.6689
	step [3/191], loss=66.2422
	step [4/191], loss=55.4908
	step [5/191], loss=56.2601
	step [6/191], loss=62.8304
	step [7/191], loss=61.7427
	step [8/191], loss=57.3285
	step [9/191], loss=56.8261
	step [10/191], loss=60.1755
	step [11/191], loss=58.5922
	step [12/191], loss=65.6762
	step [13/191], loss=61.4307
	step [14/191], loss=58.4893
	step [15/191], loss=72.8080
	step [16/191], loss=66.1986
	step [17/191], loss=60.0732
	step [18/191], loss=61.1481
	step [19/191], loss=67.7391
	step [20/191], loss=69.6452
	step [21/191], loss=60.2409
	step [22/191], loss=56.5025
	step [23/191], loss=64.3239
	step [24/191], loss=68.8915
	step [25/191], loss=59.7394
	step [26/191], loss=61.2870
	step [27/191], loss=53.7227
	step [28/191], loss=60.7371
	step [29/191], loss=68.1204
	step [30/191], loss=72.6679
	step [31/191], loss=66.4452
	step [32/191], loss=59.1879
	step [33/191], loss=61.1150
	step [34/191], loss=57.9178
	step [35/191], loss=58.2061
	step [36/191], loss=63.4108
	step [37/191], loss=56.7538
	step [38/191], loss=66.7203
	step [39/191], loss=54.3716
	step [40/191], loss=62.1885
	step [41/191], loss=60.4173
	step [42/191], loss=60.8446
	step [43/191], loss=70.3781
	step [44/191], loss=75.5793
	step [45/191], loss=71.3145
	step [46/191], loss=79.0662
	step [47/191], loss=65.3557
	step [48/191], loss=57.6032
	step [49/191], loss=61.8567
	step [50/191], loss=59.1280
	step [51/191], loss=65.1584
	step [52/191], loss=73.9166
	step [53/191], loss=52.2398
	step [54/191], loss=64.4880
	step [55/191], loss=59.7495
	step [56/191], loss=65.9582
	step [57/191], loss=63.8068
	step [58/191], loss=70.9053
	step [59/191], loss=79.4450
	step [60/191], loss=62.0591
	step [61/191], loss=67.7004
	step [62/191], loss=58.3423
	step [63/191], loss=65.9050
	step [64/191], loss=60.1850
	step [65/191], loss=75.5673
	step [66/191], loss=71.3855
	step [67/191], loss=63.1407
	step [68/191], loss=74.6713
	step [69/191], loss=60.4246
	step [70/191], loss=76.0828
	step [71/191], loss=58.4903
	step [72/191], loss=62.1517
	step [73/191], loss=60.8239
	step [74/191], loss=67.3780
	step [75/191], loss=63.8024
	step [76/191], loss=61.4178
	step [77/191], loss=62.4318
	step [78/191], loss=67.4378
	step [79/191], loss=61.0766
	step [80/191], loss=61.2805
	step [81/191], loss=58.7953
	step [82/191], loss=58.3448
	step [83/191], loss=61.4382
	step [84/191], loss=66.0759
	step [85/191], loss=87.8966
	step [86/191], loss=79.5538
	step [87/191], loss=69.2349
	step [88/191], loss=84.6333
	step [89/191], loss=65.5437
	step [90/191], loss=56.4377
	step [91/191], loss=76.4678
	step [92/191], loss=76.4604
	step [93/191], loss=48.7264
	step [94/191], loss=74.8818
	step [95/191], loss=77.3300
	step [96/191], loss=58.1542
	step [97/191], loss=65.0168
	step [98/191], loss=62.9464
	step [99/191], loss=89.0282
	step [100/191], loss=72.6496
	step [101/191], loss=74.7557
	step [102/191], loss=56.6290
	step [103/191], loss=66.1434
	step [104/191], loss=67.6690
	step [105/191], loss=63.3617
	step [106/191], loss=61.4387
	step [107/191], loss=59.1062
	step [108/191], loss=70.1236
	step [109/191], loss=71.5938
	step [110/191], loss=67.0198
	step [111/191], loss=56.0172
	step [112/191], loss=68.7169
	step [113/191], loss=57.9588
	step [114/191], loss=59.0923
	step [115/191], loss=55.9501
	step [116/191], loss=67.0211
	step [117/191], loss=72.3113
	step [118/191], loss=70.1740
	step [119/191], loss=49.8971
	step [120/191], loss=74.2944
	step [121/191], loss=72.9629
	step [122/191], loss=61.6164
	step [123/191], loss=64.9934
	step [124/191], loss=59.1739
	step [125/191], loss=63.8932
	step [126/191], loss=65.9666
	step [127/191], loss=58.3188
	step [128/191], loss=61.1886
	step [129/191], loss=66.0447
	step [130/191], loss=56.5846
	step [131/191], loss=64.1699
	step [132/191], loss=62.0532
	step [133/191], loss=75.8003
	step [134/191], loss=64.0899
	step [135/191], loss=67.8801
	step [136/191], loss=55.0934
	step [137/191], loss=68.6721
	step [138/191], loss=49.5656
	step [139/191], loss=66.0147
	step [140/191], loss=67.7110
	step [141/191], loss=69.0952
	step [142/191], loss=65.5724
	step [143/191], loss=61.5278
	step [144/191], loss=57.6448
	step [145/191], loss=65.2703
	step [146/191], loss=59.2884
	step [147/191], loss=64.7943
	step [148/191], loss=71.3052
	step [149/191], loss=70.3770
	step [150/191], loss=60.8408
	step [151/191], loss=58.9549
	step [152/191], loss=70.4141
	step [153/191], loss=69.8435
	step [154/191], loss=68.5671
	step [155/191], loss=71.0197
	step [156/191], loss=74.9089
	step [157/191], loss=58.8825
	step [158/191], loss=62.7382
	step [159/191], loss=83.5189
	step [160/191], loss=59.6125
	step [161/191], loss=53.8024
	step [162/191], loss=69.6957
	step [163/191], loss=65.6260
	step [164/191], loss=58.6313
	step [165/191], loss=60.8629
	step [166/191], loss=73.7619
	step [167/191], loss=80.3579
	step [168/191], loss=64.5454
	step [169/191], loss=68.2681
	step [170/191], loss=65.3391
	step [171/191], loss=64.6830
	step [172/191], loss=58.6295
	step [173/191], loss=71.1314
	step [174/191], loss=50.3950
	step [175/191], loss=67.6979
	step [176/191], loss=72.1760
	step [177/191], loss=75.2568
	step [178/191], loss=55.2996
	step [179/191], loss=60.2999
	step [180/191], loss=73.5387
	step [181/191], loss=60.6016
	step [182/191], loss=56.7537
	step [183/191], loss=66.8478
	step [184/191], loss=70.7788
	step [185/191], loss=57.9483
	step [186/191], loss=65.5104
	step [187/191], loss=62.3964
	step [188/191], loss=65.4202
	step [189/191], loss=48.4570
	step [190/191], loss=58.4373
	step [191/191], loss=44.5165
	Evaluating
	loss=0.0069, precision=0.4674, recall=0.8536, f1=0.6040
Training epoch 95
	step [1/191], loss=60.9211
	step [2/191], loss=63.7453
	step [3/191], loss=57.5912
	step [4/191], loss=62.8774
	step [5/191], loss=55.1637
	step [6/191], loss=69.0050
	step [7/191], loss=59.2303
	step [8/191], loss=64.8265
	step [9/191], loss=64.9860
	step [10/191], loss=68.6307
	step [11/191], loss=58.1199
	step [12/191], loss=66.5793
	step [13/191], loss=57.5108
	step [14/191], loss=68.9009
	step [15/191], loss=64.7356
	step [16/191], loss=60.4617
	step [17/191], loss=65.2299
	step [18/191], loss=72.6011
	step [19/191], loss=63.9463
	step [20/191], loss=64.6697
	step [21/191], loss=64.3996
	step [22/191], loss=59.9001
	step [23/191], loss=73.0074
	step [24/191], loss=64.3362
	step [25/191], loss=64.7349
	step [26/191], loss=73.6771
	step [27/191], loss=71.9065
	step [28/191], loss=65.2468
	step [29/191], loss=78.3231
	step [30/191], loss=69.9136
	step [31/191], loss=57.8097
	step [32/191], loss=72.6388
	step [33/191], loss=75.0543
	step [34/191], loss=77.2878
	step [35/191], loss=58.3111
	step [36/191], loss=65.3654
	step [37/191], loss=54.0689
	step [38/191], loss=60.9119
	step [39/191], loss=70.3146
	step [40/191], loss=63.4297
	step [41/191], loss=70.0403
	step [42/191], loss=68.8862
	step [43/191], loss=64.3501
	step [44/191], loss=64.3950
	step [45/191], loss=76.5973
	step [46/191], loss=66.4183
	step [47/191], loss=68.0442
	step [48/191], loss=59.5936
	step [49/191], loss=68.1492
	step [50/191], loss=48.2878
	step [51/191], loss=65.3293
	step [52/191], loss=61.0914
	step [53/191], loss=63.1502
	step [54/191], loss=59.6907
	step [55/191], loss=58.5727
	step [56/191], loss=65.5624
	step [57/191], loss=65.7860
	step [58/191], loss=65.7316
	step [59/191], loss=69.3661
	step [60/191], loss=65.8698
	step [61/191], loss=63.3825
	step [62/191], loss=71.9010
	step [63/191], loss=56.5273
	step [64/191], loss=62.4150
	step [65/191], loss=62.8959
	step [66/191], loss=68.2987
	step [67/191], loss=68.0498
	step [68/191], loss=79.8386
	step [69/191], loss=70.1052
	step [70/191], loss=54.8933
	step [71/191], loss=63.5021
	step [72/191], loss=65.9458
	step [73/191], loss=78.0041
	step [74/191], loss=68.5066
	step [75/191], loss=61.9368
	step [76/191], loss=55.8815
	step [77/191], loss=62.4955
	step [78/191], loss=77.3676
	step [79/191], loss=76.7469
	step [80/191], loss=50.9088
	step [81/191], loss=60.5819
	step [82/191], loss=69.7800
	step [83/191], loss=63.2956
	step [84/191], loss=70.9323
	step [85/191], loss=77.9407
	step [86/191], loss=69.9709
	step [87/191], loss=73.1395
	step [88/191], loss=71.4351
	step [89/191], loss=45.2183
	step [90/191], loss=61.4782
	step [91/191], loss=64.0706
	step [92/191], loss=62.8951
	step [93/191], loss=54.0387
	step [94/191], loss=64.7643
	step [95/191], loss=73.9129
	step [96/191], loss=62.8113
	step [97/191], loss=60.9043
	step [98/191], loss=73.1834
	step [99/191], loss=68.0448
	step [100/191], loss=68.1883
	step [101/191], loss=60.1399
	step [102/191], loss=63.8460
	step [103/191], loss=65.1150
	step [104/191], loss=63.4659
	step [105/191], loss=65.9487
	step [106/191], loss=69.9608
	step [107/191], loss=63.2211
	step [108/191], loss=68.5465
	step [109/191], loss=66.2479
	step [110/191], loss=54.9187
	step [111/191], loss=66.6276
	step [112/191], loss=61.1434
	step [113/191], loss=68.8140
	step [114/191], loss=76.8586
	step [115/191], loss=59.7951
	step [116/191], loss=70.1301
	step [117/191], loss=72.3186
	step [118/191], loss=64.8701
	step [119/191], loss=68.3107
	step [120/191], loss=72.0545
	step [121/191], loss=68.5439
	step [122/191], loss=72.4682
	step [123/191], loss=59.3588
	step [124/191], loss=48.9860
	step [125/191], loss=66.0235
	step [126/191], loss=66.7013
	step [127/191], loss=58.1364
	step [128/191], loss=62.4054
	step [129/191], loss=67.5688
	step [130/191], loss=61.4885
	step [131/191], loss=64.2221
	step [132/191], loss=68.0934
	step [133/191], loss=75.7742
	step [134/191], loss=73.5841
	step [135/191], loss=66.1997
	step [136/191], loss=63.6447
	step [137/191], loss=60.6401
	step [138/191], loss=67.6091
	step [139/191], loss=64.2021
	step [140/191], loss=64.8177
	step [141/191], loss=61.2426
	step [142/191], loss=74.0901
	step [143/191], loss=58.0262
	step [144/191], loss=63.3920
	step [145/191], loss=58.8064
	step [146/191], loss=61.1002
	step [147/191], loss=55.1949
	step [148/191], loss=67.3546
	step [149/191], loss=68.4021
	step [150/191], loss=62.5330
	step [151/191], loss=70.5549
	step [152/191], loss=65.9744
	step [153/191], loss=72.3373
	step [154/191], loss=68.4190
	step [155/191], loss=74.1634
	step [156/191], loss=77.8666
	step [157/191], loss=74.6897
	step [158/191], loss=66.2959
	step [159/191], loss=62.8496
	step [160/191], loss=58.8376
	step [161/191], loss=66.9793
	step [162/191], loss=63.4234
	step [163/191], loss=67.0513
	step [164/191], loss=57.5089
	step [165/191], loss=62.1990
	step [166/191], loss=61.3785
	step [167/191], loss=54.9368
	step [168/191], loss=77.5818
	step [169/191], loss=60.5867
	step [170/191], loss=68.3401
	step [171/191], loss=62.5254
	step [172/191], loss=60.4331
	step [173/191], loss=55.5622
	step [174/191], loss=56.6533
	step [175/191], loss=62.6961
	step [176/191], loss=52.7169
	step [177/191], loss=55.2462
	step [178/191], loss=66.1260
	step [179/191], loss=64.3807
	step [180/191], loss=67.7010
	step [181/191], loss=70.2208
	step [182/191], loss=64.3649
	step [183/191], loss=50.5971
	step [184/191], loss=55.8774
	step [185/191], loss=65.0182
	step [186/191], loss=53.0646
	step [187/191], loss=65.3465
	step [188/191], loss=55.6374
	step [189/191], loss=70.0745
	step [190/191], loss=72.9344
	step [191/191], loss=29.1789
	Evaluating
	loss=0.0063, precision=0.4910, recall=0.8586, f1=0.6247
Training epoch 96
	step [1/191], loss=77.5207
	step [2/191], loss=54.4248
	step [3/191], loss=62.9243
	step [4/191], loss=68.1053
	step [5/191], loss=60.9907
	step [6/191], loss=54.5881
	step [7/191], loss=73.2228
	step [8/191], loss=63.0079
	step [9/191], loss=67.5041
	step [10/191], loss=60.2445
	step [11/191], loss=56.9499
	step [12/191], loss=78.6460
	step [13/191], loss=68.2962
	step [14/191], loss=55.6113
	step [15/191], loss=60.9093
	step [16/191], loss=67.3661
	step [17/191], loss=63.8937
	step [18/191], loss=74.9728
	step [19/191], loss=78.2906
	step [20/191], loss=72.5360
	step [21/191], loss=62.9420
	step [22/191], loss=62.6765
	step [23/191], loss=68.9591
	step [24/191], loss=65.8139
	step [25/191], loss=58.3186
	step [26/191], loss=61.6873
	step [27/191], loss=72.3013
	step [28/191], loss=53.8220
	step [29/191], loss=61.2929
	step [30/191], loss=63.4368
	step [31/191], loss=66.6639
	step [32/191], loss=62.6761
	step [33/191], loss=56.0143
	step [34/191], loss=60.9800
	step [35/191], loss=63.0961
	step [36/191], loss=68.7278
	step [37/191], loss=57.2669
	step [38/191], loss=81.8717
	step [39/191], loss=69.9708
	step [40/191], loss=65.9028
	step [41/191], loss=63.1259
	step [42/191], loss=71.0019
	step [43/191], loss=60.0862
	step [44/191], loss=54.0145
	step [45/191], loss=64.6499
	step [46/191], loss=58.7923
	step [47/191], loss=64.8058
	step [48/191], loss=62.8711
	step [49/191], loss=52.9850
	step [50/191], loss=54.2741
	step [51/191], loss=59.1075
	step [52/191], loss=85.7034
	step [53/191], loss=60.2511
	step [54/191], loss=69.0961
	step [55/191], loss=64.7091
	step [56/191], loss=59.4937
	step [57/191], loss=76.4172
	step [58/191], loss=55.1720
	step [59/191], loss=72.2906
	step [60/191], loss=72.1493
	step [61/191], loss=62.6649
	step [62/191], loss=70.4914
	step [63/191], loss=70.1280
	step [64/191], loss=61.7917
	step [65/191], loss=61.0127
	step [66/191], loss=71.1180
	step [67/191], loss=62.0916
	step [68/191], loss=69.5638
	step [69/191], loss=68.5230
	step [70/191], loss=64.9669
	step [71/191], loss=66.4056
	step [72/191], loss=66.9489
	step [73/191], loss=54.1476
	step [74/191], loss=60.5732
	step [75/191], loss=64.8783
	step [76/191], loss=59.4335
	step [77/191], loss=58.3668
	step [78/191], loss=65.7912
	step [79/191], loss=60.3753
	step [80/191], loss=63.2281
	step [81/191], loss=54.3962
	step [82/191], loss=59.5552
	step [83/191], loss=68.1609
	step [84/191], loss=54.0894
	step [85/191], loss=76.1157
	step [86/191], loss=64.2322
	step [87/191], loss=68.3022
	step [88/191], loss=65.5385
	step [89/191], loss=66.4547
	step [90/191], loss=58.4956
	step [91/191], loss=74.8395
	step [92/191], loss=60.0743
	step [93/191], loss=59.3103
	step [94/191], loss=55.6176
	step [95/191], loss=59.1712
	step [96/191], loss=61.5686
	step [97/191], loss=58.4438
	step [98/191], loss=66.0653
	step [99/191], loss=60.3981
	step [100/191], loss=68.9052
	step [101/191], loss=62.8831
	step [102/191], loss=62.1694
	step [103/191], loss=62.5910
	step [104/191], loss=64.6827
	step [105/191], loss=64.5568
	step [106/191], loss=56.5104
	step [107/191], loss=62.2821
	step [108/191], loss=68.1600
	step [109/191], loss=68.8065
	step [110/191], loss=69.4149
	step [111/191], loss=60.5391
	step [112/191], loss=69.0191
	step [113/191], loss=69.5401
	step [114/191], loss=66.0200
	step [115/191], loss=57.8728
	step [116/191], loss=55.3978
	step [117/191], loss=67.6850
	step [118/191], loss=62.0050
	step [119/191], loss=61.4487
	step [120/191], loss=65.9547
	step [121/191], loss=57.6835
	step [122/191], loss=67.3334
	step [123/191], loss=55.4690
	step [124/191], loss=60.7473
	step [125/191], loss=77.1190
	step [126/191], loss=58.8516
	step [127/191], loss=57.7788
	step [128/191], loss=83.1886
	step [129/191], loss=56.3652
	step [130/191], loss=55.2325
	step [131/191], loss=55.6710
	step [132/191], loss=52.9141
	step [133/191], loss=66.5074
	step [134/191], loss=59.0214
	step [135/191], loss=61.4547
	step [136/191], loss=64.1378
	step [137/191], loss=55.1088
	step [138/191], loss=63.2210
	step [139/191], loss=74.7972
	step [140/191], loss=68.8153
	step [141/191], loss=63.2424
	step [142/191], loss=79.0751
	step [143/191], loss=59.3606
	step [144/191], loss=67.4818
	step [145/191], loss=61.8231
	step [146/191], loss=66.9412
	step [147/191], loss=59.1305
	step [148/191], loss=72.1453
	step [149/191], loss=61.2664
	step [150/191], loss=57.4802
	step [151/191], loss=80.8308
	step [152/191], loss=75.7537
	step [153/191], loss=69.8880
	step [154/191], loss=65.0245
	step [155/191], loss=58.5434
	step [156/191], loss=56.9126
	step [157/191], loss=68.7419
	step [158/191], loss=60.6570
	step [159/191], loss=70.9055
	step [160/191], loss=77.5022
	step [161/191], loss=74.2902
	step [162/191], loss=67.5048
	step [163/191], loss=70.2179
	step [164/191], loss=65.6136
	step [165/191], loss=73.9604
	step [166/191], loss=64.0946
	step [167/191], loss=63.9968
	step [168/191], loss=75.2270
	step [169/191], loss=67.7607
	step [170/191], loss=63.0939
	step [171/191], loss=77.2298
	step [172/191], loss=57.9848
	step [173/191], loss=68.3713
	step [174/191], loss=64.9958
	step [175/191], loss=59.5266
	step [176/191], loss=59.8117
	step [177/191], loss=60.0076
	step [178/191], loss=63.2992
	step [179/191], loss=79.4047
	step [180/191], loss=62.3027
	step [181/191], loss=66.0393
	step [182/191], loss=64.9905
	step [183/191], loss=61.9792
	step [184/191], loss=61.3153
	step [185/191], loss=65.0757
	step [186/191], loss=55.2674
	step [187/191], loss=68.6703
	step [188/191], loss=57.7448
	step [189/191], loss=61.8467
	step [190/191], loss=61.2521
	step [191/191], loss=32.6875
	Evaluating
	loss=0.0068, precision=0.4748, recall=0.8512, f1=0.6096
Training epoch 97
	step [1/191], loss=64.1088
	step [2/191], loss=52.6300
	step [3/191], loss=80.3755
	step [4/191], loss=60.4957
	step [5/191], loss=59.6091
	step [6/191], loss=64.8933
	step [7/191], loss=73.4343
	step [8/191], loss=81.8137
	step [9/191], loss=67.1437
	step [10/191], loss=58.0118
	step [11/191], loss=68.4334
	step [12/191], loss=55.9289
	step [13/191], loss=60.4483
	step [14/191], loss=65.9875
	step [15/191], loss=58.4551
	step [16/191], loss=61.8570
	step [17/191], loss=66.9538
	step [18/191], loss=65.0387
	step [19/191], loss=70.0555
	step [20/191], loss=59.4771
	step [21/191], loss=76.9939
	step [22/191], loss=56.5385
	step [23/191], loss=64.2058
	step [24/191], loss=55.3887
	step [25/191], loss=55.1793
	step [26/191], loss=59.2387
	step [27/191], loss=66.0895
	step [28/191], loss=63.9740
	step [29/191], loss=64.8223
	step [30/191], loss=57.6924
	step [31/191], loss=58.1311
	step [32/191], loss=64.0611
	step [33/191], loss=60.1719
	step [34/191], loss=63.9261
	step [35/191], loss=62.4312
	step [36/191], loss=55.2282
	step [37/191], loss=60.2143
	step [38/191], loss=63.6783
	step [39/191], loss=54.6279
	step [40/191], loss=56.8514
	step [41/191], loss=67.7194
	step [42/191], loss=62.7126
	step [43/191], loss=75.1253
	step [44/191], loss=58.9914
	step [45/191], loss=51.9693
	step [46/191], loss=65.6666
	step [47/191], loss=66.4550
	step [48/191], loss=69.2772
	step [49/191], loss=64.8284
	step [50/191], loss=56.0086
	step [51/191], loss=55.7011
	step [52/191], loss=65.1426
	step [53/191], loss=59.1085
	step [54/191], loss=67.1015
	step [55/191], loss=57.0533
	step [56/191], loss=51.2754
	step [57/191], loss=56.9985
	step [58/191], loss=67.5163
	step [59/191], loss=71.0004
	step [60/191], loss=57.6772
	step [61/191], loss=60.0699
	step [62/191], loss=60.0010
	step [63/191], loss=68.8316
	step [64/191], loss=73.5552
	step [65/191], loss=69.1284
	step [66/191], loss=71.4693
	step [67/191], loss=60.4881
	step [68/191], loss=73.3614
	step [69/191], loss=52.7332
	step [70/191], loss=69.6025
	step [71/191], loss=74.2660
	step [72/191], loss=69.2113
	step [73/191], loss=62.2172
	step [74/191], loss=64.0094
	step [75/191], loss=79.3464
	step [76/191], loss=50.7066
	step [77/191], loss=60.6144
	step [78/191], loss=72.4843
	step [79/191], loss=67.5166
	step [80/191], loss=64.7903
	step [81/191], loss=64.8271
	step [82/191], loss=61.0386
	step [83/191], loss=49.6646
	step [84/191], loss=68.0077
	step [85/191], loss=55.3595
	step [86/191], loss=71.5373
	step [87/191], loss=65.8722
	step [88/191], loss=62.1759
	step [89/191], loss=62.9147
	step [90/191], loss=73.1542
	step [91/191], loss=63.8486
	step [92/191], loss=63.5577
	step [93/191], loss=70.1165
	step [94/191], loss=53.6273
	step [95/191], loss=64.7478
	step [96/191], loss=63.9886
	step [97/191], loss=61.7723
	step [98/191], loss=66.8526
	step [99/191], loss=55.9782
	step [100/191], loss=66.0473
	step [101/191], loss=59.9098
	step [102/191], loss=66.9894
	step [103/191], loss=69.1032
	step [104/191], loss=57.1830
	step [105/191], loss=64.1577
	step [106/191], loss=66.7422
	step [107/191], loss=71.5288
	step [108/191], loss=63.1294
	step [109/191], loss=80.0850
	step [110/191], loss=67.5717
	step [111/191], loss=58.1362
	step [112/191], loss=69.4979
	step [113/191], loss=68.0841
	step [114/191], loss=64.8393
	step [115/191], loss=62.9437
	step [116/191], loss=57.9597
	step [117/191], loss=59.7796
	step [118/191], loss=71.3067
	step [119/191], loss=66.0269
	step [120/191], loss=62.0804
	step [121/191], loss=65.7915
	step [122/191], loss=57.3054
	step [123/191], loss=69.5144
	step [124/191], loss=60.3887
	step [125/191], loss=58.4168
	step [126/191], loss=70.1850
	step [127/191], loss=71.0668
	step [128/191], loss=67.9865
	step [129/191], loss=62.0249
	step [130/191], loss=71.0435
	step [131/191], loss=70.6563
	step [132/191], loss=55.6565
	step [133/191], loss=52.1244
	step [134/191], loss=57.7721
	step [135/191], loss=66.7821
	step [136/191], loss=69.7850
	step [137/191], loss=60.2480
	step [138/191], loss=58.8274
	step [139/191], loss=67.2943
	step [140/191], loss=65.7907
	step [141/191], loss=65.5750
	step [142/191], loss=66.2257
	step [143/191], loss=61.1068
	step [144/191], loss=71.1220
	step [145/191], loss=70.5314
	step [146/191], loss=68.3502
	step [147/191], loss=73.0842
	step [148/191], loss=67.2178
	step [149/191], loss=54.9837
	step [150/191], loss=51.1279
	step [151/191], loss=74.3611
	step [152/191], loss=71.9723
	step [153/191], loss=57.1776
	step [154/191], loss=65.1621
	step [155/191], loss=63.9051
	step [156/191], loss=63.3587
	step [157/191], loss=60.8198
	step [158/191], loss=65.9737
	step [159/191], loss=51.6890
	step [160/191], loss=67.5132
	step [161/191], loss=60.2766
	step [162/191], loss=64.6963
	step [163/191], loss=67.2904
	step [164/191], loss=68.1882
	step [165/191], loss=59.2754
	step [166/191], loss=62.6809
	step [167/191], loss=55.3381
	step [168/191], loss=53.8942
	step [169/191], loss=60.1842
	step [170/191], loss=64.7889
	step [171/191], loss=73.0323
	step [172/191], loss=54.8173
	step [173/191], loss=66.4419
	step [174/191], loss=58.7912
	step [175/191], loss=60.5491
	step [176/191], loss=69.1980
	step [177/191], loss=66.4689
	step [178/191], loss=63.5287
	step [179/191], loss=77.1077
	step [180/191], loss=64.8879
	step [181/191], loss=68.1895
	step [182/191], loss=67.6016
	step [183/191], loss=64.2013
	step [184/191], loss=59.7299
	step [185/191], loss=74.7826
	step [186/191], loss=64.8034
	step [187/191], loss=55.5751
	step [188/191], loss=70.6801
	step [189/191], loss=63.7354
	step [190/191], loss=71.3473
	step [191/191], loss=27.4715
	Evaluating
	loss=0.0067, precision=0.4741, recall=0.8524, f1=0.6093
Training epoch 98
	step [1/191], loss=51.5028
	step [2/191], loss=63.6775
	step [3/191], loss=62.9169
	step [4/191], loss=65.3856
	step [5/191], loss=66.8700
	step [6/191], loss=64.8457
	step [7/191], loss=62.6025
	step [8/191], loss=58.1129
	step [9/191], loss=56.8972
	step [10/191], loss=56.5540
	step [11/191], loss=61.9241
	step [12/191], loss=65.5938
	step [13/191], loss=61.3234
	step [14/191], loss=51.2492
	step [15/191], loss=56.5723
	step [16/191], loss=56.0205
	step [17/191], loss=77.4491
	step [18/191], loss=54.2867
	step [19/191], loss=71.7739
	step [20/191], loss=66.0032
	step [21/191], loss=48.2403
	step [22/191], loss=74.1461
	step [23/191], loss=54.7641
	step [24/191], loss=70.1510
	step [25/191], loss=65.1541
	step [26/191], loss=74.7108
	step [27/191], loss=65.4010
	step [28/191], loss=55.1694
	step [29/191], loss=65.3771
	step [30/191], loss=68.9018
	step [31/191], loss=69.5248
	step [32/191], loss=72.5640
	step [33/191], loss=61.8240
	step [34/191], loss=60.0948
	step [35/191], loss=62.5056
	step [36/191], loss=62.8445
	step [37/191], loss=68.3695
	step [38/191], loss=65.6440
	step [39/191], loss=46.3350
	step [40/191], loss=61.7943
	step [41/191], loss=59.7728
	step [42/191], loss=63.0571
	step [43/191], loss=56.1168
	step [44/191], loss=65.3175
	step [45/191], loss=64.4128
	step [46/191], loss=59.0455
	step [47/191], loss=63.9074
	step [48/191], loss=69.3683
	step [49/191], loss=65.4447
	step [50/191], loss=71.4129
	step [51/191], loss=54.4862
	step [52/191], loss=66.6631
	step [53/191], loss=59.5352
	step [54/191], loss=74.9331
	step [55/191], loss=62.1321
	step [56/191], loss=60.3876
	step [57/191], loss=58.5344
	step [58/191], loss=65.8494
	step [59/191], loss=61.3938
	step [60/191], loss=58.9376
	step [61/191], loss=74.5272
	step [62/191], loss=62.4695
	step [63/191], loss=78.7650
	step [64/191], loss=62.4478
	step [65/191], loss=60.7394
	step [66/191], loss=61.0313
	step [67/191], loss=55.9204
	step [68/191], loss=71.8278
	step [69/191], loss=63.0235
	step [70/191], loss=60.5841
	step [71/191], loss=56.1753
	step [72/191], loss=70.0374
	step [73/191], loss=61.8930
	step [74/191], loss=52.6944
	step [75/191], loss=62.2983
	step [76/191], loss=66.5257
	step [77/191], loss=67.3567
	step [78/191], loss=67.3581
	step [79/191], loss=69.1044
	step [80/191], loss=57.6984
	step [81/191], loss=60.4312
	step [82/191], loss=59.6459
	step [83/191], loss=69.3885
	step [84/191], loss=68.2658
	step [85/191], loss=55.6027
	step [86/191], loss=59.0420
	step [87/191], loss=66.1217
	step [88/191], loss=51.6127
	step [89/191], loss=65.0001
	step [90/191], loss=50.4386
	step [91/191], loss=71.8086
	step [92/191], loss=62.3958
	step [93/191], loss=61.1303
	step [94/191], loss=73.1423
	step [95/191], loss=68.1636
	step [96/191], loss=77.9455
	step [97/191], loss=60.3190
	step [98/191], loss=54.8845
	step [99/191], loss=72.6337
	step [100/191], loss=74.3864
	step [101/191], loss=60.1505
	step [102/191], loss=74.2571
	step [103/191], loss=59.1253
	step [104/191], loss=70.2658
	step [105/191], loss=68.7036
	step [106/191], loss=67.6850
	step [107/191], loss=74.6576
	step [108/191], loss=64.4868
	step [109/191], loss=67.1451
	step [110/191], loss=75.8265
	step [111/191], loss=73.3011
	step [112/191], loss=76.0934
	step [113/191], loss=71.3549
	step [114/191], loss=61.1647
	step [115/191], loss=63.0776
	step [116/191], loss=58.7749
	step [117/191], loss=72.5412
	step [118/191], loss=72.8565
	step [119/191], loss=61.6849
	step [120/191], loss=58.8403
	step [121/191], loss=59.9793
	step [122/191], loss=65.0059
	step [123/191], loss=70.6763
	step [124/191], loss=62.7306
	step [125/191], loss=57.6890
	step [126/191], loss=60.7897
	step [127/191], loss=62.6721
	step [128/191], loss=69.5675
	step [129/191], loss=67.6719
	step [130/191], loss=67.1555
	step [131/191], loss=67.8861
	step [132/191], loss=69.9688
	step [133/191], loss=70.3667
	step [134/191], loss=53.2656
	step [135/191], loss=53.7653
	step [136/191], loss=60.2978
	step [137/191], loss=62.6529
	step [138/191], loss=58.5718
	step [139/191], loss=58.8907
	step [140/191], loss=75.1995
	step [141/191], loss=56.4622
	step [142/191], loss=60.9593
	step [143/191], loss=70.8407
	step [144/191], loss=57.0120
	step [145/191], loss=63.3483
	step [146/191], loss=63.1896
	step [147/191], loss=72.4624
	step [148/191], loss=60.9594
	step [149/191], loss=69.6943
	step [150/191], loss=71.3329
	step [151/191], loss=61.8999
	step [152/191], loss=53.5016
	step [153/191], loss=63.9049
	step [154/191], loss=58.8239
	step [155/191], loss=65.6843
	step [156/191], loss=65.6063
	step [157/191], loss=73.1379
	step [158/191], loss=68.0414
	step [159/191], loss=79.2509
	step [160/191], loss=65.5187
	step [161/191], loss=58.1917
	step [162/191], loss=59.6186
	step [163/191], loss=62.9019
	step [164/191], loss=64.2613
	step [165/191], loss=71.6322
	step [166/191], loss=60.3228
	step [167/191], loss=48.4475
	step [168/191], loss=70.2766
	step [169/191], loss=53.7303
	step [170/191], loss=70.3034
	step [171/191], loss=57.6260
	step [172/191], loss=74.3167
	step [173/191], loss=52.2615
	step [174/191], loss=58.8926
	step [175/191], loss=52.5650
	step [176/191], loss=74.7878
	step [177/191], loss=58.1416
	step [178/191], loss=61.1689
	step [179/191], loss=68.1226
	step [180/191], loss=67.1590
	step [181/191], loss=58.7683
	step [182/191], loss=64.4619
	step [183/191], loss=66.9384
	step [184/191], loss=60.6289
	step [185/191], loss=72.0706
	step [186/191], loss=66.7004
	step [187/191], loss=59.1256
	step [188/191], loss=65.7895
	step [189/191], loss=57.3821
	step [190/191], loss=72.1445
	step [191/191], loss=38.1430
	Evaluating
	loss=0.0066, precision=0.4805, recall=0.8580, f1=0.6160
Training epoch 99
	step [1/191], loss=65.6126
	step [2/191], loss=72.0088
	step [3/191], loss=55.3639
	step [4/191], loss=78.3426
	step [5/191], loss=55.5032
	step [6/191], loss=59.6521
	step [7/191], loss=58.8927
	step [8/191], loss=77.9426
	step [9/191], loss=65.7197
	step [10/191], loss=65.1847
	step [11/191], loss=75.7302
	step [12/191], loss=61.2609
	step [13/191], loss=59.6578
	step [14/191], loss=67.0093
	step [15/191], loss=72.3285
	step [16/191], loss=45.8233
	step [17/191], loss=64.0743
	step [18/191], loss=61.3341
	step [19/191], loss=62.9065
	step [20/191], loss=65.6994
	step [21/191], loss=69.9281
	step [22/191], loss=59.0895
	step [23/191], loss=74.9523
	step [24/191], loss=61.7940
	step [25/191], loss=63.8504
	step [26/191], loss=71.5155
	step [27/191], loss=67.7909
	step [28/191], loss=63.4099
	step [29/191], loss=56.6710
	step [30/191], loss=65.7654
	step [31/191], loss=63.9308
	step [32/191], loss=65.1824
	step [33/191], loss=60.9988
	step [34/191], loss=65.2298
	step [35/191], loss=63.8258
	step [36/191], loss=50.9536
	step [37/191], loss=63.2334
	step [38/191], loss=74.3677
	step [39/191], loss=63.0784
	step [40/191], loss=67.3396
	step [41/191], loss=60.9358
	step [42/191], loss=53.6061
	step [43/191], loss=62.9357
	step [44/191], loss=60.7708
	step [45/191], loss=62.1311
	step [46/191], loss=61.7652
	step [47/191], loss=52.7283
	step [48/191], loss=73.0220
	step [49/191], loss=77.1179
	step [50/191], loss=55.2605
	step [51/191], loss=57.6110
	step [52/191], loss=79.7950
	step [53/191], loss=73.2033
	step [54/191], loss=76.7023
	step [55/191], loss=62.9194
	step [56/191], loss=71.3572
	step [57/191], loss=53.9089
	step [58/191], loss=62.4070
	step [59/191], loss=68.4725
	step [60/191], loss=62.5140
	step [61/191], loss=64.7051
	step [62/191], loss=63.9047
	step [63/191], loss=64.4536
	step [64/191], loss=73.7820
	step [65/191], loss=60.1663
	step [66/191], loss=66.4393
	step [67/191], loss=59.0714
	step [68/191], loss=67.3184
	step [69/191], loss=68.9174
	step [70/191], loss=79.2395
	step [71/191], loss=68.2098
	step [72/191], loss=59.4834
	step [73/191], loss=63.3521
	step [74/191], loss=77.5822
	step [75/191], loss=61.0086
	step [76/191], loss=63.5837
	step [77/191], loss=58.9917
	step [78/191], loss=57.8836
	step [79/191], loss=60.8391
	step [80/191], loss=79.0953
	step [81/191], loss=55.0560
	step [82/191], loss=65.7157
	step [83/191], loss=67.6119
	step [84/191], loss=63.6462
	step [85/191], loss=52.2004
	step [86/191], loss=68.0818
	step [87/191], loss=69.4629
	step [88/191], loss=63.3225
	step [89/191], loss=62.9708
	step [90/191], loss=67.8321
	step [91/191], loss=63.4569
	step [92/191], loss=63.0796
	step [93/191], loss=61.6764
	step [94/191], loss=55.3320
	step [95/191], loss=63.5374
	step [96/191], loss=61.0886
	step [97/191], loss=52.7084
	step [98/191], loss=74.4930
	step [99/191], loss=62.8678
	step [100/191], loss=63.2512
	step [101/191], loss=68.8689
	step [102/191], loss=64.3832
	step [103/191], loss=66.9089
	step [104/191], loss=59.8471
	step [105/191], loss=70.3687
	step [106/191], loss=54.5647
	step [107/191], loss=66.4572
	step [108/191], loss=56.4099
	step [109/191], loss=70.8638
	step [110/191], loss=61.3753
	step [111/191], loss=66.6666
	step [112/191], loss=61.2544
	step [113/191], loss=64.0084
	step [114/191], loss=69.1646
	step [115/191], loss=60.6411
	step [116/191], loss=68.1244
	step [117/191], loss=63.5091
	step [118/191], loss=77.6540
	step [119/191], loss=56.6073
	step [120/191], loss=71.9768
	step [121/191], loss=67.0661
	step [122/191], loss=66.0259
	step [123/191], loss=66.2696
	step [124/191], loss=66.5592
	step [125/191], loss=62.9408
	step [126/191], loss=59.4406
	step [127/191], loss=75.4210
	step [128/191], loss=61.2507
	step [129/191], loss=59.3164
	step [130/191], loss=63.7981
	step [131/191], loss=62.9136
	step [132/191], loss=66.7142
	step [133/191], loss=66.9510
	step [134/191], loss=57.0474
	step [135/191], loss=70.7851
	step [136/191], loss=71.8133
	step [137/191], loss=65.4338
	step [138/191], loss=68.7497
	step [139/191], loss=60.2239
	step [140/191], loss=51.7328
	step [141/191], loss=59.2453
	step [142/191], loss=63.7644
	step [143/191], loss=58.0353
	step [144/191], loss=69.5081
	step [145/191], loss=59.1969
	step [146/191], loss=53.9520
	step [147/191], loss=64.0835
	step [148/191], loss=62.6407
	step [149/191], loss=55.4963
	step [150/191], loss=65.3361
	step [151/191], loss=61.5727
	step [152/191], loss=63.9383
	step [153/191], loss=61.8224
	step [154/191], loss=59.3710
	step [155/191], loss=70.7036
	step [156/191], loss=59.8200
	step [157/191], loss=49.6220
	step [158/191], loss=52.4712
	step [159/191], loss=64.7253
	step [160/191], loss=61.3354
	step [161/191], loss=60.9444
	step [162/191], loss=63.8188
	step [163/191], loss=69.8868
	step [164/191], loss=76.6422
	step [165/191], loss=56.4435
	step [166/191], loss=58.3867
	step [167/191], loss=69.1693
	step [168/191], loss=63.9418
	step [169/191], loss=61.1728
	step [170/191], loss=68.7619
	step [171/191], loss=67.0190
	step [172/191], loss=63.5745
	step [173/191], loss=58.7923
	step [174/191], loss=56.0172
	step [175/191], loss=60.5928
	step [176/191], loss=66.3018
	step [177/191], loss=65.6887
	step [178/191], loss=68.7537
	step [179/191], loss=64.8945
	step [180/191], loss=58.6747
	step [181/191], loss=61.9424
	step [182/191], loss=60.4969
	step [183/191], loss=64.4335
	step [184/191], loss=59.9433
	step [185/191], loss=58.3972
	step [186/191], loss=71.3650
	step [187/191], loss=66.7812
	step [188/191], loss=58.2527
	step [189/191], loss=64.6106
	step [190/191], loss=53.6980
	step [191/191], loss=28.5620
	Evaluating
	loss=0.0058, precision=0.5239, recall=0.8393, f1=0.6451
Training epoch 100
	step [1/191], loss=60.9593
	step [2/191], loss=59.7265
	step [3/191], loss=69.0856
	step [4/191], loss=62.0260
	step [5/191], loss=67.2151
	step [6/191], loss=57.0218
	step [7/191], loss=60.0872
	step [8/191], loss=55.5121
	step [9/191], loss=58.7441
	step [10/191], loss=59.9944
	step [11/191], loss=67.3084
	step [12/191], loss=71.1813
	step [13/191], loss=77.1686
	step [14/191], loss=65.2464
	step [15/191], loss=71.5957
	step [16/191], loss=67.9258
	step [17/191], loss=64.8243
	step [18/191], loss=58.5685
	step [19/191], loss=61.6540
	step [20/191], loss=68.5719
	step [21/191], loss=67.5282
	step [22/191], loss=58.0218
	step [23/191], loss=67.9191
	step [24/191], loss=58.1808
	step [25/191], loss=73.5086
	step [26/191], loss=57.1192
	step [27/191], loss=68.4975
	step [28/191], loss=63.3479
	step [29/191], loss=77.1261
	step [30/191], loss=66.5960
	step [31/191], loss=67.7307
	step [32/191], loss=77.2457
	step [33/191], loss=61.9525
	step [34/191], loss=66.9850
	step [35/191], loss=67.8555
	step [36/191], loss=67.9638
	step [37/191], loss=60.0491
	step [38/191], loss=63.6309
	step [39/191], loss=69.6082
	step [40/191], loss=59.4423
	step [41/191], loss=63.8406
	step [42/191], loss=61.6610
	step [43/191], loss=64.8731
	step [44/191], loss=69.5499
	step [45/191], loss=68.8613
	step [46/191], loss=68.1163
	step [47/191], loss=46.4664
	step [48/191], loss=58.8236
	step [49/191], loss=66.0792
	step [50/191], loss=55.5711
	step [51/191], loss=57.0978
	step [52/191], loss=60.4746
	step [53/191], loss=66.1359
	step [54/191], loss=64.9909
	step [55/191], loss=67.1849
	step [56/191], loss=61.9071
	step [57/191], loss=67.6403
	step [58/191], loss=64.6869
	step [59/191], loss=57.8696
	step [60/191], loss=60.5783
	step [61/191], loss=54.7487
	step [62/191], loss=68.1482
	step [63/191], loss=70.6095
	step [64/191], loss=55.4032
	step [65/191], loss=75.0973
	step [66/191], loss=78.2102
	step [67/191], loss=55.6108
	step [68/191], loss=58.7818
	step [69/191], loss=67.6469
	step [70/191], loss=58.2081
	step [71/191], loss=66.6496
	step [72/191], loss=66.0911
	step [73/191], loss=59.4127
	step [74/191], loss=73.8886
	step [75/191], loss=67.9256
	step [76/191], loss=61.7503
	step [77/191], loss=62.6217
	step [78/191], loss=61.6077
	step [79/191], loss=59.2559
	step [80/191], loss=53.0751
	step [81/191], loss=56.2990
	step [82/191], loss=65.4204
	step [83/191], loss=63.8631
	step [84/191], loss=66.2695
	step [85/191], loss=56.5429
	step [86/191], loss=55.9950
	step [87/191], loss=71.4433
	step [88/191], loss=65.1863
	step [89/191], loss=65.7976
	step [90/191], loss=69.7952
	step [91/191], loss=67.3979
	step [92/191], loss=66.3241
	step [93/191], loss=56.4993
	step [94/191], loss=56.9889
	step [95/191], loss=63.4852
	step [96/191], loss=66.3005
	step [97/191], loss=63.8653
	step [98/191], loss=65.0465
	step [99/191], loss=54.1293
	step [100/191], loss=70.5552
	step [101/191], loss=66.6860
	step [102/191], loss=68.2976
	step [103/191], loss=53.3341
	step [104/191], loss=67.6627
	step [105/191], loss=62.6993
	step [106/191], loss=70.1517
	step [107/191], loss=57.6418
	step [108/191], loss=65.6806
	step [109/191], loss=75.9293
	step [110/191], loss=46.8244
	step [111/191], loss=59.0647
	step [112/191], loss=66.9404
	step [113/191], loss=69.1576
	step [114/191], loss=68.7030
	step [115/191], loss=61.7218
	step [116/191], loss=66.4566
	step [117/191], loss=61.7621
	step [118/191], loss=59.3863
	step [119/191], loss=65.8139
	step [120/191], loss=58.9585
	step [121/191], loss=68.2020
	step [122/191], loss=61.7792
	step [123/191], loss=63.1491
	step [124/191], loss=57.8771
	step [125/191], loss=68.5929
	step [126/191], loss=53.7040
	step [127/191], loss=55.4516
	step [128/191], loss=62.5833
	step [129/191], loss=54.9015
	step [130/191], loss=57.2711
	step [131/191], loss=62.4445
	step [132/191], loss=61.1808
	step [133/191], loss=63.5562
	step [134/191], loss=66.5055
	step [135/191], loss=79.2016
	step [136/191], loss=54.9605
	step [137/191], loss=54.4536
	step [138/191], loss=64.0576
	step [139/191], loss=66.0962
	step [140/191], loss=65.8407
	step [141/191], loss=52.6991
	step [142/191], loss=68.9233
	step [143/191], loss=56.4723
	step [144/191], loss=71.5743
	step [145/191], loss=61.8469
	step [146/191], loss=79.8544
	step [147/191], loss=63.1744
	step [148/191], loss=60.5013
	step [149/191], loss=55.2968
	step [150/191], loss=63.8191
	step [151/191], loss=55.7454
	step [152/191], loss=70.0940
	step [153/191], loss=60.0382
	step [154/191], loss=63.3750
	step [155/191], loss=56.0065
	step [156/191], loss=56.9823
	step [157/191], loss=65.1974
	step [158/191], loss=61.5211
	step [159/191], loss=57.3611
	step [160/191], loss=63.4714
	step [161/191], loss=61.6981
	step [162/191], loss=60.7693
	step [163/191], loss=66.4103
	step [164/191], loss=66.1323
	step [165/191], loss=67.9723
	step [166/191], loss=64.8236
	step [167/191], loss=58.1820
	step [168/191], loss=61.7900
	step [169/191], loss=69.4302
	step [170/191], loss=59.3590
	step [171/191], loss=70.5736
	step [172/191], loss=71.7209
	step [173/191], loss=60.7467
	step [174/191], loss=57.9796
	step [175/191], loss=58.5846
	step [176/191], loss=63.5837
	step [177/191], loss=56.2628
	step [178/191], loss=69.3585
	step [179/191], loss=69.4605
	step [180/191], loss=54.9648
	step [181/191], loss=60.7792
	step [182/191], loss=56.5674
	step [183/191], loss=57.7897
	step [184/191], loss=61.8534
	step [185/191], loss=61.7293
	step [186/191], loss=62.2733
	step [187/191], loss=65.8303
	step [188/191], loss=64.6829
	step [189/191], loss=63.2250
	step [190/191], loss=80.0956
	step [191/191], loss=31.6883
	Evaluating
	loss=0.0063, precision=0.4929, recall=0.8520, f1=0.6245
Training epoch 101
	step [1/191], loss=64.7113
	step [2/191], loss=74.3856
	step [3/191], loss=48.8943
	step [4/191], loss=65.6196
	step [5/191], loss=71.4258
	step [6/191], loss=53.1305
	step [7/191], loss=67.1864
	step [8/191], loss=76.3397
	step [9/191], loss=63.4820
	step [10/191], loss=72.0179
	step [11/191], loss=63.1084
	step [12/191], loss=65.2328
	step [13/191], loss=60.4648
	step [14/191], loss=70.2403
	step [15/191], loss=64.7587
	step [16/191], loss=60.4459
	step [17/191], loss=64.7440
	step [18/191], loss=59.2003
	step [19/191], loss=62.1636
	step [20/191], loss=66.1833
	step [21/191], loss=62.4865
	step [22/191], loss=71.5219
	step [23/191], loss=59.3548
	step [24/191], loss=71.7526
	step [25/191], loss=67.9909
	step [26/191], loss=62.9024
	step [27/191], loss=68.4240
	step [28/191], loss=63.6914
	step [29/191], loss=65.9748
	step [30/191], loss=59.8417
	step [31/191], loss=66.7389
	step [32/191], loss=49.3065
	step [33/191], loss=69.1754
	step [34/191], loss=53.9027
	step [35/191], loss=65.0937
	step [36/191], loss=73.7291
	step [37/191], loss=69.6406
	step [38/191], loss=58.1776
	step [39/191], loss=55.8884
	step [40/191], loss=57.1868
	step [41/191], loss=47.2178
	step [42/191], loss=62.8726
	step [43/191], loss=60.9552
	step [44/191], loss=55.6567
	step [45/191], loss=64.7092
	step [46/191], loss=67.3819
	step [47/191], loss=82.5829
	step [48/191], loss=67.9206
	step [49/191], loss=53.0650
	step [50/191], loss=67.4333
	step [51/191], loss=63.0178
	step [52/191], loss=69.0905
	step [53/191], loss=73.8652
	step [54/191], loss=47.3323
	step [55/191], loss=62.0056
	step [56/191], loss=57.1375
	step [57/191], loss=58.8058
	step [58/191], loss=72.2215
	step [59/191], loss=57.2758
	step [60/191], loss=68.8404
	step [61/191], loss=61.4378
	step [62/191], loss=73.0033
	step [63/191], loss=62.7799
	step [64/191], loss=67.5943
	step [65/191], loss=58.8173
	step [66/191], loss=62.6704
	step [67/191], loss=59.5065
	step [68/191], loss=61.3764
	step [69/191], loss=60.6140
	step [70/191], loss=50.4105
	step [71/191], loss=69.5492
	step [72/191], loss=61.7703
	step [73/191], loss=69.3671
	step [74/191], loss=59.9280
	step [75/191], loss=56.7661
	step [76/191], loss=60.8349
	step [77/191], loss=62.9769
	step [78/191], loss=64.9545
	step [79/191], loss=67.6781
	step [80/191], loss=60.1970
	step [81/191], loss=68.9823
	step [82/191], loss=69.2295
	step [83/191], loss=60.1784
	step [84/191], loss=61.0438
	step [85/191], loss=62.8446
	step [86/191], loss=67.7788
	step [87/191], loss=64.0719
	step [88/191], loss=64.5994
	step [89/191], loss=55.2989
	step [90/191], loss=68.2027
	step [91/191], loss=64.1821
	step [92/191], loss=73.9618
	step [93/191], loss=61.7159
	step [94/191], loss=69.0275
	step [95/191], loss=67.1789
	step [96/191], loss=62.8797
	step [97/191], loss=62.0434
	step [98/191], loss=66.2954
	step [99/191], loss=60.8179
	step [100/191], loss=60.9136
	step [101/191], loss=71.8261
	step [102/191], loss=61.3717
	step [103/191], loss=72.6641
	step [104/191], loss=69.6521
	step [105/191], loss=62.6631
	step [106/191], loss=59.3515
	step [107/191], loss=54.5443
	step [108/191], loss=64.0789
	step [109/191], loss=64.1438
	step [110/191], loss=51.9420
	step [111/191], loss=63.5506
	step [112/191], loss=52.3307
	step [113/191], loss=64.2099
	step [114/191], loss=59.5080
	step [115/191], loss=69.0507
	step [116/191], loss=63.5359
	step [117/191], loss=65.4818
	step [118/191], loss=62.3316
	step [119/191], loss=73.5903
	step [120/191], loss=65.1536
	step [121/191], loss=56.3684
	step [122/191], loss=70.4467
	step [123/191], loss=60.2473
	step [124/191], loss=67.6939
	step [125/191], loss=55.6067
	step [126/191], loss=68.8177
	step [127/191], loss=71.3165
	step [128/191], loss=66.4815
	step [129/191], loss=68.7704
	step [130/191], loss=65.5040
	step [131/191], loss=74.6618
	step [132/191], loss=60.3288
	step [133/191], loss=46.4226
	step [134/191], loss=53.6711
	step [135/191], loss=73.7798
	step [136/191], loss=58.8410
	step [137/191], loss=55.7866
	step [138/191], loss=61.5660
	step [139/191], loss=54.6985
	step [140/191], loss=64.3848
	step [141/191], loss=74.4432
	step [142/191], loss=61.9114
	step [143/191], loss=54.0996
	step [144/191], loss=55.7657
	step [145/191], loss=66.5440
	step [146/191], loss=62.8579
	step [147/191], loss=59.4273
	step [148/191], loss=59.3941
	step [149/191], loss=66.5524
	step [150/191], loss=77.1498
	step [151/191], loss=63.2488
	step [152/191], loss=66.9359
	step [153/191], loss=60.9489
	step [154/191], loss=67.1273
	step [155/191], loss=63.2856
	step [156/191], loss=65.0662
	step [157/191], loss=53.5348
	step [158/191], loss=66.8139
	step [159/191], loss=65.3151
	step [160/191], loss=57.4337
	step [161/191], loss=71.0079
	step [162/191], loss=70.3970
	step [163/191], loss=63.8076
	step [164/191], loss=62.3100
	step [165/191], loss=74.5319
	step [166/191], loss=60.2112
	step [167/191], loss=65.1652
	step [168/191], loss=51.5665
	step [169/191], loss=57.9260
	step [170/191], loss=64.2123
	step [171/191], loss=68.5887
	step [172/191], loss=52.9114
	step [173/191], loss=72.6287
	step [174/191], loss=66.7473
	step [175/191], loss=62.9722
	step [176/191], loss=69.8444
	step [177/191], loss=65.3957
	step [178/191], loss=68.7787
	step [179/191], loss=60.0500
	step [180/191], loss=58.9370
	step [181/191], loss=71.3566
	step [182/191], loss=60.2999
	step [183/191], loss=63.7771
	step [184/191], loss=70.0465
	step [185/191], loss=73.8122
	step [186/191], loss=61.2880
	step [187/191], loss=61.6627
	step [188/191], loss=65.0804
	step [189/191], loss=72.1572
	step [190/191], loss=58.4787
	step [191/191], loss=32.8866
	Evaluating
	loss=0.0059, precision=0.5156, recall=0.8422, f1=0.6396
Training epoch 102
	step [1/191], loss=74.1297
	step [2/191], loss=56.1174
	step [3/191], loss=66.4504
	step [4/191], loss=62.4264
	step [5/191], loss=64.2955
	step [6/191], loss=71.8651
	step [7/191], loss=51.0206
	step [8/191], loss=69.2704
	step [9/191], loss=61.5182
	step [10/191], loss=62.1861
	step [11/191], loss=51.9572
	step [12/191], loss=56.6655
	step [13/191], loss=58.0501
	step [14/191], loss=63.5156
	step [15/191], loss=58.9883
	step [16/191], loss=53.7100
	step [17/191], loss=66.3383
	step [18/191], loss=57.6253
	step [19/191], loss=71.7130
	step [20/191], loss=61.4116
	step [21/191], loss=57.9516
	step [22/191], loss=67.5155
	step [23/191], loss=69.8439
	step [24/191], loss=49.8743
	step [25/191], loss=57.6191
	step [26/191], loss=73.0600
	step [27/191], loss=69.8740
	step [28/191], loss=64.9586
	step [29/191], loss=61.2635
	step [30/191], loss=68.7842
	step [31/191], loss=63.9482
	step [32/191], loss=61.5799
	step [33/191], loss=74.9919
	step [34/191], loss=61.8307
	step [35/191], loss=55.9125
	step [36/191], loss=60.3998
	step [37/191], loss=59.0072
	step [38/191], loss=78.4314
	step [39/191], loss=63.0720
	step [40/191], loss=58.3903
	step [41/191], loss=58.2003
	step [42/191], loss=66.5833
	step [43/191], loss=73.0510
	step [44/191], loss=71.1434
	step [45/191], loss=69.2344
	step [46/191], loss=63.2269
	step [47/191], loss=64.9171
	step [48/191], loss=54.0338
	step [49/191], loss=78.5522
	step [50/191], loss=56.5697
	step [51/191], loss=61.5925
	step [52/191], loss=52.2315
	step [53/191], loss=74.0856
	step [54/191], loss=61.8480
	step [55/191], loss=57.9867
	step [56/191], loss=58.8309
	step [57/191], loss=60.1290
	step [58/191], loss=67.6900
	step [59/191], loss=66.8252
	step [60/191], loss=66.7841
	step [61/191], loss=66.5849
	step [62/191], loss=72.6663
	step [63/191], loss=57.3734
	step [64/191], loss=53.7224
	step [65/191], loss=73.2925
	step [66/191], loss=67.4415
	step [67/191], loss=58.7317
	step [68/191], loss=66.2938
	step [69/191], loss=51.1400
	step [70/191], loss=69.8893
	step [71/191], loss=58.1404
	step [72/191], loss=63.7571
	step [73/191], loss=61.4564
	step [74/191], loss=64.3572
	step [75/191], loss=67.7330
	step [76/191], loss=67.3645
	step [77/191], loss=74.6470
	step [78/191], loss=58.9254
	step [79/191], loss=63.9286
	step [80/191], loss=73.2753
	step [81/191], loss=60.7927
	step [82/191], loss=70.5452
	step [83/191], loss=58.6987
	step [84/191], loss=52.0767
	step [85/191], loss=72.6931
	step [86/191], loss=61.4488
	step [87/191], loss=69.0073
	step [88/191], loss=63.6197
	step [89/191], loss=62.3107
	step [90/191], loss=72.4122
	step [91/191], loss=62.6208
	step [92/191], loss=63.8691
	step [93/191], loss=65.2121
	step [94/191], loss=50.0754
	step [95/191], loss=62.1628
	step [96/191], loss=73.5964
	step [97/191], loss=69.9184
	step [98/191], loss=52.8205
	step [99/191], loss=67.3347
	step [100/191], loss=73.8354
	step [101/191], loss=70.6749
	step [102/191], loss=66.6404
	step [103/191], loss=67.6799
	step [104/191], loss=70.4852
	step [105/191], loss=75.4860
	step [106/191], loss=60.9205
	step [107/191], loss=62.4740
	step [108/191], loss=63.3259
	step [109/191], loss=64.3209
	step [110/191], loss=66.7867
	step [111/191], loss=52.9744
	step [112/191], loss=51.6443
	step [113/191], loss=57.2139
	step [114/191], loss=61.8230
	step [115/191], loss=60.4586
	step [116/191], loss=57.3856
	step [117/191], loss=57.4372
	step [118/191], loss=64.3510
	step [119/191], loss=62.4539
	step [120/191], loss=59.0245
	step [121/191], loss=49.3150
	step [122/191], loss=56.0053
	step [123/191], loss=62.0177
	step [124/191], loss=63.3457
	step [125/191], loss=52.3674
	step [126/191], loss=74.1768
	step [127/191], loss=63.0914
	step [128/191], loss=65.7915
	step [129/191], loss=66.0834
	step [130/191], loss=57.5925
	step [131/191], loss=61.4738
	step [132/191], loss=69.3060
	step [133/191], loss=58.4185
	step [134/191], loss=68.7839
	step [135/191], loss=71.7284
	step [136/191], loss=55.5131
	step [137/191], loss=53.3964
	step [138/191], loss=73.8275
	step [139/191], loss=69.4120
	step [140/191], loss=60.0385
	step [141/191], loss=67.0793
	step [142/191], loss=70.4049
	step [143/191], loss=67.0589
	step [144/191], loss=59.7918
	step [145/191], loss=60.4491
	step [146/191], loss=58.5555
	step [147/191], loss=69.5585
	step [148/191], loss=64.8317
	step [149/191], loss=64.3140
	step [150/191], loss=69.1664
	step [151/191], loss=57.2537
	step [152/191], loss=64.8145
	step [153/191], loss=60.1631
	step [154/191], loss=64.5758
	step [155/191], loss=61.2592
	step [156/191], loss=60.0395
	step [157/191], loss=57.1136
	step [158/191], loss=71.5920
	step [159/191], loss=62.4802
	step [160/191], loss=58.3328
	step [161/191], loss=65.0259
	step [162/191], loss=68.7617
	step [163/191], loss=55.7876
	step [164/191], loss=50.4701
	step [165/191], loss=56.6536
	step [166/191], loss=52.2101
	step [167/191], loss=74.7375
	step [168/191], loss=57.2013
	step [169/191], loss=63.9032
	step [170/191], loss=57.5610
	step [171/191], loss=60.8624
	step [172/191], loss=55.8622
	step [173/191], loss=78.5080
	step [174/191], loss=63.3613
	step [175/191], loss=71.7348
	step [176/191], loss=55.0955
	step [177/191], loss=70.1987
	step [178/191], loss=61.4821
	step [179/191], loss=70.5463
	step [180/191], loss=59.3411
	step [181/191], loss=64.6376
	step [182/191], loss=64.3115
	step [183/191], loss=76.5078
	step [184/191], loss=78.7582
	step [185/191], loss=65.7957
	step [186/191], loss=53.2133
	step [187/191], loss=62.0991
	step [188/191], loss=70.1132
	step [189/191], loss=54.7508
	step [190/191], loss=64.8098
	step [191/191], loss=30.8255
	Evaluating
	loss=0.0069, precision=0.4642, recall=0.8490, f1=0.6002
Training epoch 103
	step [1/191], loss=51.5424
	step [2/191], loss=74.2404
	step [3/191], loss=59.2081
	step [4/191], loss=67.8568
	step [5/191], loss=76.3411
	step [6/191], loss=59.3105
	step [7/191], loss=65.1737
	step [8/191], loss=62.0173
	step [9/191], loss=64.0005
	step [10/191], loss=73.9069
	step [11/191], loss=70.7591
	step [12/191], loss=55.2472
	step [13/191], loss=64.2120
	step [14/191], loss=66.5833
	step [15/191], loss=67.5681
	step [16/191], loss=60.3515
	step [17/191], loss=73.2294
	step [18/191], loss=64.0551
	step [19/191], loss=52.9830
	step [20/191], loss=70.3923
	step [21/191], loss=64.2960
	step [22/191], loss=58.0697
	step [23/191], loss=65.5885
	step [24/191], loss=58.2513
	step [25/191], loss=58.1881
	step [26/191], loss=58.0826
	step [27/191], loss=60.5614
	step [28/191], loss=69.2110
	step [29/191], loss=58.1344
	step [30/191], loss=64.6942
	step [31/191], loss=62.4856
	step [32/191], loss=67.1473
	step [33/191], loss=77.8275
	step [34/191], loss=58.2373
	step [35/191], loss=57.1087
	step [36/191], loss=54.1135
	step [37/191], loss=60.4572
	step [38/191], loss=64.0787
	step [39/191], loss=81.0387
	step [40/191], loss=66.0480
	step [41/191], loss=58.8626
	step [42/191], loss=46.0625
	step [43/191], loss=67.2544
	step [44/191], loss=67.9106
	step [45/191], loss=62.8898
	step [46/191], loss=59.8182
	step [47/191], loss=58.3046
	step [48/191], loss=56.1392
	step [49/191], loss=61.6923
	step [50/191], loss=74.7850
	step [51/191], loss=55.3598
	step [52/191], loss=70.0582
	step [53/191], loss=54.6614
	step [54/191], loss=60.1120
	step [55/191], loss=67.8296
	step [56/191], loss=64.6955
	step [57/191], loss=65.8022
	step [58/191], loss=57.2959
	step [59/191], loss=54.8208
	step [60/191], loss=57.0960
	step [61/191], loss=65.4773
	step [62/191], loss=61.1163
	step [63/191], loss=55.1577
	step [64/191], loss=63.3822
	step [65/191], loss=69.3380
	step [66/191], loss=69.1660
	step [67/191], loss=60.7587
	step [68/191], loss=60.3487
	step [69/191], loss=63.3409
	step [70/191], loss=63.9139
	step [71/191], loss=69.0006
	step [72/191], loss=69.4859
	step [73/191], loss=77.2182
	step [74/191], loss=66.6642
	step [75/191], loss=70.9894
	step [76/191], loss=59.8065
	step [77/191], loss=66.5997
	step [78/191], loss=60.0754
	step [79/191], loss=65.3398
	step [80/191], loss=49.5363
	step [81/191], loss=59.5097
	step [82/191], loss=58.7491
	step [83/191], loss=76.2471
	step [84/191], loss=61.7271
	step [85/191], loss=66.0106
	step [86/191], loss=54.9097
	step [87/191], loss=64.5024
	step [88/191], loss=58.4299
	step [89/191], loss=66.2662
	step [90/191], loss=77.1543
	step [91/191], loss=64.9106
	step [92/191], loss=67.0091
	step [93/191], loss=62.3570
	step [94/191], loss=63.7329
	step [95/191], loss=67.4856
	step [96/191], loss=78.2981
	step [97/191], loss=63.0890
	step [98/191], loss=71.8860
	step [99/191], loss=60.4288
	step [100/191], loss=67.8591
	step [101/191], loss=66.5459
	step [102/191], loss=68.7260
	step [103/191], loss=58.2775
	step [104/191], loss=61.8173
	step [105/191], loss=55.6015
	step [106/191], loss=78.6334
	step [107/191], loss=76.7363
	step [108/191], loss=62.2085
	step [109/191], loss=56.8487
	step [110/191], loss=65.2205
	step [111/191], loss=61.3628
	step [112/191], loss=61.1118
	step [113/191], loss=54.8381
	step [114/191], loss=70.5166
	step [115/191], loss=73.1753
	step [116/191], loss=75.1438
	step [117/191], loss=62.6528
	step [118/191], loss=67.9648
	step [119/191], loss=63.3230
	step [120/191], loss=68.4789
	step [121/191], loss=66.5212
	step [122/191], loss=64.5764
	step [123/191], loss=73.2996
	step [124/191], loss=59.0901
	step [125/191], loss=61.8561
	step [126/191], loss=63.5207
	step [127/191], loss=62.1051
	step [128/191], loss=61.1248
	step [129/191], loss=73.2679
	step [130/191], loss=61.3295
	step [131/191], loss=65.7965
	step [132/191], loss=57.4113
	step [133/191], loss=59.2385
	step [134/191], loss=63.8331
	step [135/191], loss=59.2137
	step [136/191], loss=65.1596
	step [137/191], loss=63.4661
	step [138/191], loss=62.8398
	step [139/191], loss=56.0342
	step [140/191], loss=67.7663
	step [141/191], loss=55.9978
	step [142/191], loss=60.0609
	step [143/191], loss=63.1226
	step [144/191], loss=68.5427
	step [145/191], loss=58.9085
	step [146/191], loss=53.4173
	step [147/191], loss=51.6731
	step [148/191], loss=59.1294
	step [149/191], loss=58.9275
	step [150/191], loss=65.1738
	step [151/191], loss=64.3927
	step [152/191], loss=58.1259
	step [153/191], loss=59.6642
	step [154/191], loss=64.6111
	step [155/191], loss=56.8704
	step [156/191], loss=76.9547
	step [157/191], loss=63.6505
	step [158/191], loss=60.0277
	step [159/191], loss=72.3979
	step [160/191], loss=72.1559
	step [161/191], loss=76.1245
	step [162/191], loss=59.5338
	step [163/191], loss=55.3476
	step [164/191], loss=60.0774
	step [165/191], loss=58.0616
	step [166/191], loss=81.0566
	step [167/191], loss=62.9781
	step [168/191], loss=65.2379
	step [169/191], loss=51.2830
	step [170/191], loss=56.8543
	step [171/191], loss=56.8928
	step [172/191], loss=51.4368
	step [173/191], loss=53.7155
	step [174/191], loss=56.8856
	step [175/191], loss=58.0237
	step [176/191], loss=65.8673
	step [177/191], loss=62.2385
	step [178/191], loss=61.8180
	step [179/191], loss=58.4562
	step [180/191], loss=52.5645
	step [181/191], loss=56.6067
	step [182/191], loss=64.7477
	step [183/191], loss=65.3369
	step [184/191], loss=78.2359
	step [185/191], loss=73.8090
	step [186/191], loss=60.1587
	step [187/191], loss=50.5422
	step [188/191], loss=63.0944
	step [189/191], loss=58.3252
	step [190/191], loss=72.8909
	step [191/191], loss=31.4623
	Evaluating
	loss=0.0075, precision=0.4343, recall=0.8633, f1=0.5779
Training epoch 104
	step [1/191], loss=68.0943
	step [2/191], loss=57.9018
	step [3/191], loss=68.1692
	step [4/191], loss=61.9422
	step [5/191], loss=57.7129
	step [6/191], loss=65.7562
	step [7/191], loss=51.0670
	step [8/191], loss=67.0393
	step [9/191], loss=65.8417
	step [10/191], loss=57.3952
	step [11/191], loss=64.8970
	step [12/191], loss=72.5437
	step [13/191], loss=66.3170
	step [14/191], loss=59.2391
	step [15/191], loss=66.2800
	step [16/191], loss=67.4326
	step [17/191], loss=67.2281
	step [18/191], loss=67.7064
	step [19/191], loss=54.7273
	step [20/191], loss=58.1031
	step [21/191], loss=61.2451
	step [22/191], loss=55.9160
	step [23/191], loss=61.9590
	step [24/191], loss=62.3877
	step [25/191], loss=66.1969
	step [26/191], loss=54.3416
	step [27/191], loss=71.1998
	step [28/191], loss=76.2633
	step [29/191], loss=61.5871
	step [30/191], loss=62.2557
	step [31/191], loss=52.7271
	step [32/191], loss=70.3400
	step [33/191], loss=52.7598
	step [34/191], loss=53.0533
	step [35/191], loss=64.0708
	step [36/191], loss=56.6004
	step [37/191], loss=69.6778
	step [38/191], loss=58.6419
	step [39/191], loss=68.3496
	step [40/191], loss=55.7009
	step [41/191], loss=54.5509
	step [42/191], loss=64.0428
	step [43/191], loss=78.8619
	step [44/191], loss=54.6375
	step [45/191], loss=68.2326
	step [46/191], loss=55.0263
	step [47/191], loss=54.1719
	step [48/191], loss=52.4883
	step [49/191], loss=61.9815
	step [50/191], loss=68.8982
	step [51/191], loss=58.2259
	step [52/191], loss=64.6410
	step [53/191], loss=65.0693
	step [54/191], loss=49.7133
	step [55/191], loss=59.6658
	step [56/191], loss=56.7877
	step [57/191], loss=71.2568
	step [58/191], loss=68.5429
	step [59/191], loss=69.1240
	step [60/191], loss=51.0914
	step [61/191], loss=62.7666
	step [62/191], loss=72.3939
	step [63/191], loss=60.0118
	step [64/191], loss=63.5846
	step [65/191], loss=67.5265
	step [66/191], loss=65.6693
	step [67/191], loss=58.0373
	step [68/191], loss=72.8519
	step [69/191], loss=63.4946
	step [70/191], loss=64.2957
	step [71/191], loss=66.1831
	step [72/191], loss=59.6030
	step [73/191], loss=63.5506
	step [74/191], loss=60.5738
	step [75/191], loss=56.9766
	step [76/191], loss=58.6305
	step [77/191], loss=71.1051
	step [78/191], loss=62.8631
	step [79/191], loss=53.9574
	step [80/191], loss=57.4484
	step [81/191], loss=70.1914
	step [82/191], loss=66.9775
	step [83/191], loss=57.4638
	step [84/191], loss=62.4321
	step [85/191], loss=64.7031
	step [86/191], loss=77.3298
	step [87/191], loss=82.8665
	step [88/191], loss=61.0580
	step [89/191], loss=60.6526
	step [90/191], loss=65.8739
	step [91/191], loss=72.2446
	step [92/191], loss=54.3960
	step [93/191], loss=63.8464
	step [94/191], loss=52.1120
	step [95/191], loss=55.2935
	step [96/191], loss=65.1895
	step [97/191], loss=61.1244
	step [98/191], loss=67.1894
	step [99/191], loss=71.3587
	step [100/191], loss=76.0624
	step [101/191], loss=60.7684
	step [102/191], loss=73.4407
	step [103/191], loss=68.1620
	step [104/191], loss=59.8490
	step [105/191], loss=61.9724
	step [106/191], loss=60.5586
	step [107/191], loss=68.9140
	step [108/191], loss=64.8685
	step [109/191], loss=76.3052
	step [110/191], loss=65.8661
	step [111/191], loss=68.9409
	step [112/191], loss=59.8765
	step [113/191], loss=61.0584
	step [114/191], loss=57.2823
	step [115/191], loss=67.2024
	step [116/191], loss=58.2709
	step [117/191], loss=57.6249
	step [118/191], loss=60.8185
	step [119/191], loss=51.7892
	step [120/191], loss=60.7812
	step [121/191], loss=64.7217
	step [122/191], loss=57.2700
	step [123/191], loss=67.3619
	step [124/191], loss=57.6832
	step [125/191], loss=78.1182
	step [126/191], loss=67.1290
	step [127/191], loss=61.0391
	step [128/191], loss=64.3386
	step [129/191], loss=64.4125
	step [130/191], loss=58.5448
	step [131/191], loss=66.4651
	step [132/191], loss=65.4671
	step [133/191], loss=50.9237
	step [134/191], loss=71.3450
	step [135/191], loss=72.3930
	step [136/191], loss=69.5977
	step [137/191], loss=76.2317
	step [138/191], loss=60.9036
	step [139/191], loss=61.0754
	step [140/191], loss=63.9478
	step [141/191], loss=50.6687
	step [142/191], loss=63.7637
	step [143/191], loss=68.6271
	step [144/191], loss=64.9057
	step [145/191], loss=57.2476
	step [146/191], loss=67.0179
	step [147/191], loss=53.6540
	step [148/191], loss=57.7760
	step [149/191], loss=64.0439
	step [150/191], loss=70.3344
	step [151/191], loss=69.6707
	step [152/191], loss=58.0321
	step [153/191], loss=55.6336
	step [154/191], loss=76.9414
	step [155/191], loss=60.1986
	step [156/191], loss=66.1831
	step [157/191], loss=65.7228
	step [158/191], loss=62.7862
	step [159/191], loss=60.1786
	step [160/191], loss=64.2187
	step [161/191], loss=66.4912
	step [162/191], loss=63.3712
	step [163/191], loss=49.2787
	step [164/191], loss=63.1300
	step [165/191], loss=57.0088
	step [166/191], loss=66.9171
	step [167/191], loss=63.0364
	step [168/191], loss=55.0430
	step [169/191], loss=62.4396
	step [170/191], loss=72.7656
	step [171/191], loss=55.9217
	step [172/191], loss=64.4583
	step [173/191], loss=64.3489
	step [174/191], loss=56.2468
	step [175/191], loss=58.0924
	step [176/191], loss=72.0783
	step [177/191], loss=56.3483
	step [178/191], loss=60.8920
	step [179/191], loss=58.9682
	step [180/191], loss=56.2795
	step [181/191], loss=60.4240
	step [182/191], loss=69.6528
	step [183/191], loss=67.7735
	step [184/191], loss=61.1382
	step [185/191], loss=64.8895
	step [186/191], loss=63.5811
	step [187/191], loss=60.2625
	step [188/191], loss=73.2028
	step [189/191], loss=64.5190
	step [190/191], loss=74.2403
	step [191/191], loss=30.0660
	Evaluating
	loss=0.0057, precision=0.5369, recall=0.8497, f1=0.6580
saving model as: 1_saved_model.pth
Training epoch 105
	step [1/191], loss=63.1268
	step [2/191], loss=64.0412
	step [3/191], loss=56.6847
	step [4/191], loss=71.2017
	step [5/191], loss=65.8087
	step [6/191], loss=58.0745
	step [7/191], loss=58.1631
	step [8/191], loss=70.7787
	step [9/191], loss=61.3916
	step [10/191], loss=56.5110
	step [11/191], loss=57.0035
	step [12/191], loss=62.4391
	step [13/191], loss=56.3017
	step [14/191], loss=65.1269
	step [15/191], loss=61.3306
	step [16/191], loss=66.7581
	step [17/191], loss=70.2549
	step [18/191], loss=61.0565
	step [19/191], loss=59.3880
	step [20/191], loss=54.5210
	step [21/191], loss=54.4450
	step [22/191], loss=69.6317
	step [23/191], loss=54.4208
	step [24/191], loss=60.3637
	step [25/191], loss=64.4206
	step [26/191], loss=55.6717
	step [27/191], loss=58.0052
	step [28/191], loss=60.8369
	step [29/191], loss=61.3391
	step [30/191], loss=61.1395
	step [31/191], loss=64.2273
	step [32/191], loss=55.4939
	step [33/191], loss=70.0746
	step [34/191], loss=70.6656
	step [35/191], loss=61.5137
	step [36/191], loss=80.0053
	step [37/191], loss=54.0288
	step [38/191], loss=60.8315
	step [39/191], loss=66.3193
	step [40/191], loss=66.3593
	step [41/191], loss=62.3490
	step [42/191], loss=64.6756
	step [43/191], loss=63.6193
	step [44/191], loss=63.5790
	step [45/191], loss=69.4249
	step [46/191], loss=63.1018
	step [47/191], loss=60.2517
	step [48/191], loss=76.4019
	step [49/191], loss=56.9861
	step [50/191], loss=57.5516
	step [51/191], loss=51.3421
	step [52/191], loss=67.9970
	step [53/191], loss=57.9871
	step [54/191], loss=58.3891
	step [55/191], loss=68.4167
	step [56/191], loss=61.7867
	step [57/191], loss=67.5074
	step [58/191], loss=58.9960
	step [59/191], loss=57.4865
	step [60/191], loss=58.5123
	step [61/191], loss=70.9021
	step [62/191], loss=71.8240
	step [63/191], loss=69.3624
	step [64/191], loss=65.4686
	step [65/191], loss=56.1476
	step [66/191], loss=68.9185
	step [67/191], loss=58.8779
	step [68/191], loss=61.5720
	step [69/191], loss=66.1255
	step [70/191], loss=63.1441
	step [71/191], loss=57.8079
	step [72/191], loss=62.6630
	step [73/191], loss=75.1116
	step [74/191], loss=66.0993
	step [75/191], loss=68.5904
	step [76/191], loss=57.5892
	step [77/191], loss=69.2677
	step [78/191], loss=56.5701
	step [79/191], loss=70.1383
	step [80/191], loss=59.9982
	step [81/191], loss=66.8065
	step [82/191], loss=65.2816
	step [83/191], loss=65.8668
	step [84/191], loss=50.6927
	step [85/191], loss=63.5308
	step [86/191], loss=61.1774
	step [87/191], loss=69.0291
	step [88/191], loss=75.8828
	step [89/191], loss=61.4376
	step [90/191], loss=58.0747
	step [91/191], loss=69.4602
	step [92/191], loss=70.9848
	step [93/191], loss=56.4278
	step [94/191], loss=61.1682
	step [95/191], loss=54.9883
	step [96/191], loss=64.5216
	step [97/191], loss=61.6415
	step [98/191], loss=61.9446
	step [99/191], loss=55.8566
	step [100/191], loss=62.5906
	step [101/191], loss=54.8957
	step [102/191], loss=65.4424
	step [103/191], loss=56.0532
	step [104/191], loss=56.7402
	step [105/191], loss=66.2577
	step [106/191], loss=52.8108
	step [107/191], loss=70.7356
	step [108/191], loss=67.2656
	step [109/191], loss=60.3774
	step [110/191], loss=57.3890
	step [111/191], loss=67.2269
	step [112/191], loss=63.4173
	step [113/191], loss=63.7999
	step [114/191], loss=66.9128
	step [115/191], loss=69.8556
	step [116/191], loss=56.1354
	step [117/191], loss=71.5993
	step [118/191], loss=63.7675
	step [119/191], loss=66.4826
	step [120/191], loss=61.4714
	step [121/191], loss=62.8825
	step [122/191], loss=66.0733
	step [123/191], loss=59.5725
	step [124/191], loss=69.3168
	step [125/191], loss=60.7338
	step [126/191], loss=63.5414
	step [127/191], loss=54.9771
	step [128/191], loss=74.5346
	step [129/191], loss=54.9651
	step [130/191], loss=60.2543
	step [131/191], loss=56.3601
	step [132/191], loss=67.0846
	step [133/191], loss=66.9657
	step [134/191], loss=63.7262
	step [135/191], loss=59.7083
	step [136/191], loss=62.5215
	step [137/191], loss=74.8025
	step [138/191], loss=60.0341
	step [139/191], loss=70.8525
	step [140/191], loss=65.6176
	step [141/191], loss=56.0035
	step [142/191], loss=62.2198
	step [143/191], loss=63.8168
	step [144/191], loss=49.8265
	step [145/191], loss=62.1302
	step [146/191], loss=63.5274
	step [147/191], loss=69.5264
	step [148/191], loss=66.2600
	step [149/191], loss=67.2354
	step [150/191], loss=66.7765
	step [151/191], loss=67.1058
	step [152/191], loss=60.1427
	step [153/191], loss=59.4205
	step [154/191], loss=56.3870
	step [155/191], loss=69.0596
	step [156/191], loss=60.2092
	step [157/191], loss=54.5279
	step [158/191], loss=67.8247
	step [159/191], loss=58.1490
	step [160/191], loss=68.1656
	step [161/191], loss=56.3055
	step [162/191], loss=65.4049
	step [163/191], loss=57.2929
	step [164/191], loss=61.0819
	step [165/191], loss=60.5887
	step [166/191], loss=65.0948
	step [167/191], loss=69.1176
	step [168/191], loss=67.5463
	step [169/191], loss=59.7276
	step [170/191], loss=63.4735
	step [171/191], loss=71.1902
	step [172/191], loss=62.9888
	step [173/191], loss=60.1838
	step [174/191], loss=60.8877
	step [175/191], loss=58.6126
	step [176/191], loss=51.1703
	step [177/191], loss=54.9841
	step [178/191], loss=58.5563
	step [179/191], loss=61.3663
	step [180/191], loss=57.8133
	step [181/191], loss=67.7189
	step [182/191], loss=66.2130
	step [183/191], loss=62.0397
	step [184/191], loss=62.2271
	step [185/191], loss=65.2374
	step [186/191], loss=73.7790
	step [187/191], loss=64.5414
	step [188/191], loss=63.2527
	step [189/191], loss=61.3097
	step [190/191], loss=72.5146
	step [191/191], loss=30.2951
	Evaluating
	loss=0.0059, precision=0.5182, recall=0.8471, f1=0.6430
Training epoch 106
	step [1/191], loss=63.5032
	step [2/191], loss=59.8406
	step [3/191], loss=62.2607
	step [4/191], loss=72.9020
	step [5/191], loss=70.5833
	step [6/191], loss=71.4041
	step [7/191], loss=46.2926
	step [8/191], loss=51.1906
	step [9/191], loss=56.9571
	step [10/191], loss=43.5103
	step [11/191], loss=72.4335
	step [12/191], loss=65.2744
	step [13/191], loss=68.6014
	step [14/191], loss=67.0783
	step [15/191], loss=63.6262
	step [16/191], loss=56.7982
	step [17/191], loss=60.8208
	step [18/191], loss=55.9010
	step [19/191], loss=55.3392
	step [20/191], loss=63.9038
	step [21/191], loss=66.1384
	step [22/191], loss=73.3851
	step [23/191], loss=63.6231
	step [24/191], loss=62.4877
	step [25/191], loss=56.8258
	step [26/191], loss=55.5037
	step [27/191], loss=67.0899
	step [28/191], loss=67.6968
	step [29/191], loss=66.8698
	step [30/191], loss=60.4565
	step [31/191], loss=59.7466
	step [32/191], loss=63.9070
	step [33/191], loss=63.5133
	step [34/191], loss=56.2938
	step [35/191], loss=62.7871
	step [36/191], loss=48.9144
	step [37/191], loss=66.1051
	step [38/191], loss=68.4654
	step [39/191], loss=64.8583
	step [40/191], loss=62.5753
	step [41/191], loss=68.9262
	step [42/191], loss=69.1845
	step [43/191], loss=57.6058
	step [44/191], loss=67.4839
	step [45/191], loss=56.2845
	step [46/191], loss=61.9523
	step [47/191], loss=50.1841
	step [48/191], loss=70.1104
	step [49/191], loss=69.1725
	step [50/191], loss=72.7706
	step [51/191], loss=60.1880
	step [52/191], loss=76.4332
	step [53/191], loss=58.4560
	step [54/191], loss=58.2732
	step [55/191], loss=69.5778
	step [56/191], loss=58.7084
	step [57/191], loss=62.7614
	step [58/191], loss=54.9907
	step [59/191], loss=66.1684
	step [60/191], loss=67.7971
	step [61/191], loss=64.3413
	step [62/191], loss=58.0424
	step [63/191], loss=55.5028
	step [64/191], loss=76.3301
	step [65/191], loss=67.2876
	step [66/191], loss=64.4539
	step [67/191], loss=76.0048
	step [68/191], loss=58.6207
	step [69/191], loss=62.8584
	step [70/191], loss=60.3377
	step [71/191], loss=62.9388
	step [72/191], loss=62.8527
	step [73/191], loss=72.9637
	step [74/191], loss=67.6699
	step [75/191], loss=71.0155
	step [76/191], loss=57.1074
	step [77/191], loss=62.9036
	step [78/191], loss=76.2767
	step [79/191], loss=67.4710
	step [80/191], loss=64.1534
	step [81/191], loss=64.9599
	step [82/191], loss=72.4803
	step [83/191], loss=50.0105
	step [84/191], loss=70.8717
	step [85/191], loss=66.4116
	step [86/191], loss=64.0268
	step [87/191], loss=67.8137
	step [88/191], loss=57.2931
	step [89/191], loss=81.3784
	step [90/191], loss=69.8194
	step [91/191], loss=65.4652
	step [92/191], loss=60.7597
	step [93/191], loss=73.7225
	step [94/191], loss=65.8106
	step [95/191], loss=63.4530
	step [96/191], loss=58.9491
	step [97/191], loss=70.7295
	step [98/191], loss=56.5808
	step [99/191], loss=64.8759
	step [100/191], loss=64.6998
	step [101/191], loss=61.2551
	step [102/191], loss=60.8954
	step [103/191], loss=61.0484
	step [104/191], loss=69.0336
	step [105/191], loss=56.0465
	step [106/191], loss=58.0496
	step [107/191], loss=54.2440
	step [108/191], loss=51.5808
	step [109/191], loss=66.3613
	step [110/191], loss=59.4926
	step [111/191], loss=69.0016
	step [112/191], loss=50.8017
	step [113/191], loss=64.8797
	step [114/191], loss=69.1857
	step [115/191], loss=62.9663
	step [116/191], loss=54.8844
	step [117/191], loss=62.7210
	step [118/191], loss=70.8600
	step [119/191], loss=60.7267
	step [120/191], loss=65.6187
	step [121/191], loss=59.6946
	step [122/191], loss=64.5244
	step [123/191], loss=53.9403
	step [124/191], loss=59.8253
	step [125/191], loss=54.7829
	step [126/191], loss=53.6351
	step [127/191], loss=59.8190
	step [128/191], loss=51.9615
	step [129/191], loss=55.0791
	step [130/191], loss=55.3171
	step [131/191], loss=60.1029
	step [132/191], loss=64.8473
	step [133/191], loss=65.5241
	step [134/191], loss=65.3801
	step [135/191], loss=61.9338
	step [136/191], loss=58.6681
	step [137/191], loss=61.7237
	step [138/191], loss=59.2811
	step [139/191], loss=66.0686
	step [140/191], loss=55.0631
	step [141/191], loss=68.1044
	step [142/191], loss=65.1439
	step [143/191], loss=59.2713
	step [144/191], loss=66.0496
	step [145/191], loss=60.2291
	step [146/191], loss=57.7524
	step [147/191], loss=63.4165
	step [148/191], loss=56.3315
	step [149/191], loss=64.6237
	step [150/191], loss=76.6198
	step [151/191], loss=73.4560
	step [152/191], loss=65.3502
	step [153/191], loss=68.7966
	step [154/191], loss=64.0894
	step [155/191], loss=68.1516
	step [156/191], loss=57.1264
	step [157/191], loss=63.5364
	step [158/191], loss=50.3536
	step [159/191], loss=69.3728
	step [160/191], loss=63.1327
	step [161/191], loss=60.5496
	step [162/191], loss=62.6658
	step [163/191], loss=58.8550
	step [164/191], loss=59.4701
	step [165/191], loss=54.9967
	step [166/191], loss=63.8990
	step [167/191], loss=66.4955
	step [168/191], loss=66.2143
	step [169/191], loss=59.2192
	step [170/191], loss=57.8748
	step [171/191], loss=50.2713
	step [172/191], loss=59.1735
	step [173/191], loss=66.4510
	step [174/191], loss=64.5667
	step [175/191], loss=69.6695
	step [176/191], loss=64.4217
	step [177/191], loss=66.2786
	step [178/191], loss=52.2563
	step [179/191], loss=54.2999
	step [180/191], loss=65.6187
	step [181/191], loss=61.4408
	step [182/191], loss=62.0794
	step [183/191], loss=57.6558
	step [184/191], loss=59.0750
	step [185/191], loss=65.0528
	step [186/191], loss=67.5010
	step [187/191], loss=70.6858
	step [188/191], loss=65.4162
	step [189/191], loss=64.4143
	step [190/191], loss=64.4613
	step [191/191], loss=30.0646
	Evaluating
	loss=0.0066, precision=0.4799, recall=0.8492, f1=0.6133
Training epoch 107
	step [1/191], loss=68.0552
	step [2/191], loss=71.4129
	step [3/191], loss=56.7475
	step [4/191], loss=53.5002
	step [5/191], loss=59.7967
	step [6/191], loss=60.9847
	step [7/191], loss=73.2973
	step [8/191], loss=64.0406
	step [9/191], loss=58.7108
	step [10/191], loss=64.3772
	step [11/191], loss=53.6942
	step [12/191], loss=65.6449
	step [13/191], loss=64.9806
	step [14/191], loss=65.0275
	step [15/191], loss=58.1766
	step [16/191], loss=65.4150
	step [17/191], loss=59.8211
	step [18/191], loss=69.5512
	step [19/191], loss=65.6830
	step [20/191], loss=57.1933
	step [21/191], loss=49.2524
	step [22/191], loss=73.7781
	step [23/191], loss=48.6570
	step [24/191], loss=76.9519
	step [25/191], loss=68.7355
	step [26/191], loss=72.3489
	step [27/191], loss=55.2890
	step [28/191], loss=53.8938
	step [29/191], loss=64.9554
	step [30/191], loss=64.6524
	step [31/191], loss=69.7754
	step [32/191], loss=62.4633
	step [33/191], loss=49.5839
	step [34/191], loss=66.7412
	step [35/191], loss=73.7423
	step [36/191], loss=59.5506
	step [37/191], loss=78.8288
	step [38/191], loss=55.6163
	step [39/191], loss=70.7254
	step [40/191], loss=75.1507
	step [41/191], loss=70.9879
	step [42/191], loss=59.5606
	step [43/191], loss=59.1135
	step [44/191], loss=66.7202
	step [45/191], loss=63.9641
	step [46/191], loss=62.9751
	step [47/191], loss=57.6587
	step [48/191], loss=56.0194
	step [49/191], loss=59.3952
	step [50/191], loss=51.0836
	step [51/191], loss=65.5102
	step [52/191], loss=59.6971
	step [53/191], loss=65.1728
	step [54/191], loss=74.2889
	step [55/191], loss=65.7142
	step [56/191], loss=49.5920
	step [57/191], loss=75.0660
	step [58/191], loss=63.3118
	step [59/191], loss=57.7441
	step [60/191], loss=69.2623
	step [61/191], loss=67.2649
	step [62/191], loss=56.0624
	step [63/191], loss=56.4445
	step [64/191], loss=67.0553
	step [65/191], loss=61.7046
	step [66/191], loss=56.9918
	step [67/191], loss=65.0349
	step [68/191], loss=59.8602
	step [69/191], loss=58.1113
	step [70/191], loss=66.4986
	step [71/191], loss=67.1429
	step [72/191], loss=57.7162
	step [73/191], loss=62.4316
	step [74/191], loss=62.7843
	step [75/191], loss=61.9053
	step [76/191], loss=60.7089
	step [77/191], loss=62.3641
	step [78/191], loss=67.8533
	step [79/191], loss=55.2502
	step [80/191], loss=64.9464
	step [81/191], loss=55.4585
	step [82/191], loss=72.0877
	step [83/191], loss=59.1343
	step [84/191], loss=72.5454
	step [85/191], loss=50.7850
	step [86/191], loss=59.7150
	step [87/191], loss=60.3444
	step [88/191], loss=54.2355
	step [89/191], loss=62.6505
	step [90/191], loss=65.1572
	step [91/191], loss=65.9836
	step [92/191], loss=60.9849
	step [93/191], loss=69.5636
	step [94/191], loss=57.0160
	step [95/191], loss=69.4319
	step [96/191], loss=59.1924
	step [97/191], loss=65.4290
	step [98/191], loss=54.4453
	step [99/191], loss=57.3916
	step [100/191], loss=60.8154
	step [101/191], loss=71.6344
	step [102/191], loss=67.3281
	step [103/191], loss=58.4276
	step [104/191], loss=55.4879
	step [105/191], loss=58.6177
	step [106/191], loss=60.0307
	step [107/191], loss=66.9223
	step [108/191], loss=57.4130
	step [109/191], loss=73.9001
	step [110/191], loss=55.2807
	step [111/191], loss=52.7739
	step [112/191], loss=59.2434
	step [113/191], loss=64.8547
	step [114/191], loss=71.0412
	step [115/191], loss=58.2031
	step [116/191], loss=76.6420
	step [117/191], loss=61.2049
	step [118/191], loss=62.2677
	step [119/191], loss=75.6178
	step [120/191], loss=61.7901
	step [121/191], loss=58.1804
	step [122/191], loss=53.9882
	step [123/191], loss=66.3197
	step [124/191], loss=63.7148
	step [125/191], loss=72.6930
	step [126/191], loss=61.1207
	step [127/191], loss=55.1615
	step [128/191], loss=59.5055
	step [129/191], loss=62.3331
	step [130/191], loss=55.1214
	step [131/191], loss=57.8437
	step [132/191], loss=74.1381
	step [133/191], loss=72.6583
	step [134/191], loss=65.2010
	step [135/191], loss=73.2281
	step [136/191], loss=66.8346
	step [137/191], loss=64.4457
	step [138/191], loss=54.3591
	step [139/191], loss=63.4117
	step [140/191], loss=55.3113
	step [141/191], loss=56.4143
	step [142/191], loss=55.5446
	step [143/191], loss=70.0361
	step [144/191], loss=71.8107
	step [145/191], loss=55.9099
	step [146/191], loss=63.3630
	step [147/191], loss=60.5901
	step [148/191], loss=68.9757
	step [149/191], loss=69.1319
	step [150/191], loss=58.0389
	step [151/191], loss=57.6248
	step [152/191], loss=57.9289
	step [153/191], loss=56.4143
	step [154/191], loss=56.5107
	step [155/191], loss=63.8493
	step [156/191], loss=61.5066
	step [157/191], loss=64.0641
	step [158/191], loss=67.1539
	step [159/191], loss=63.8508
	step [160/191], loss=65.5823
	step [161/191], loss=77.7980
	step [162/191], loss=61.0362
	step [163/191], loss=59.1493
	step [164/191], loss=58.2229
	step [165/191], loss=60.8906
	step [166/191], loss=59.3272
	step [167/191], loss=56.8028
	step [168/191], loss=67.5100
	step [169/191], loss=56.7872
	step [170/191], loss=57.3595
	step [171/191], loss=67.9644
	step [172/191], loss=56.7510
	step [173/191], loss=54.2002
	step [174/191], loss=58.4256
	step [175/191], loss=61.2406
	step [176/191], loss=71.3919
	step [177/191], loss=55.6167
	step [178/191], loss=59.5912
	step [179/191], loss=59.0938
	step [180/191], loss=74.7286
	step [181/191], loss=68.0804
	step [182/191], loss=68.0397
	step [183/191], loss=66.1089
	step [184/191], loss=68.4824
	step [185/191], loss=61.3006
	step [186/191], loss=65.2954
	step [187/191], loss=55.7442
	step [188/191], loss=65.4316
	step [189/191], loss=63.0624
	step [190/191], loss=51.4281
	step [191/191], loss=31.5904
	Evaluating
	loss=0.0065, precision=0.4827, recall=0.8550, f1=0.6171
Training epoch 108
	step [1/191], loss=60.8541
	step [2/191], loss=53.2272
	step [3/191], loss=48.8575
	step [4/191], loss=53.1972
	step [5/191], loss=57.5828
	step [6/191], loss=56.0202
	step [7/191], loss=53.6025
	step [8/191], loss=51.1267
	step [9/191], loss=64.5926
	step [10/191], loss=56.9746
	step [11/191], loss=50.7468
	step [12/191], loss=48.8591
	step [13/191], loss=51.0518
	step [14/191], loss=68.2379
	step [15/191], loss=55.8225
	step [16/191], loss=65.3873
	step [17/191], loss=59.6478
	step [18/191], loss=72.6763
	step [19/191], loss=59.9648
	step [20/191], loss=64.5730
	step [21/191], loss=51.1082
	step [22/191], loss=52.5224
	step [23/191], loss=56.8404
	step [24/191], loss=69.2202
	step [25/191], loss=58.8547
	step [26/191], loss=59.4499
	step [27/191], loss=60.9165
	step [28/191], loss=69.3359
	step [29/191], loss=59.9499
	step [30/191], loss=67.4914
	step [31/191], loss=56.1318
	step [32/191], loss=57.7496
	step [33/191], loss=56.5785
	step [34/191], loss=54.3333
	step [35/191], loss=65.8753
	step [36/191], loss=65.5059
	step [37/191], loss=54.4528
	step [38/191], loss=60.7752
	step [39/191], loss=61.5698
	step [40/191], loss=63.0291
	step [41/191], loss=66.2779
	step [42/191], loss=66.8682
	step [43/191], loss=54.8015
	step [44/191], loss=71.1413
	step [45/191], loss=70.8081
	step [46/191], loss=73.5877
	step [47/191], loss=61.7437
	step [48/191], loss=59.4308
	step [49/191], loss=56.8303
	step [50/191], loss=72.8459
	step [51/191], loss=68.5132
	step [52/191], loss=59.6577
	step [53/191], loss=66.6333
	step [54/191], loss=65.2779
	step [55/191], loss=53.9488
	step [56/191], loss=70.0118
	step [57/191], loss=58.0005
	step [58/191], loss=70.1060
	step [59/191], loss=64.6345
	step [60/191], loss=65.0204
	step [61/191], loss=77.9292
	step [62/191], loss=54.5613
	step [63/191], loss=70.4974
	step [64/191], loss=55.8555
	step [65/191], loss=62.9470
	step [66/191], loss=68.6248
	step [67/191], loss=51.6562
	step [68/191], loss=72.3841
	step [69/191], loss=67.0724
	step [70/191], loss=59.8608
	step [71/191], loss=63.6805
	step [72/191], loss=60.0955
	step [73/191], loss=61.5921
	step [74/191], loss=64.6216
	step [75/191], loss=58.6814
	step [76/191], loss=60.3414
	step [77/191], loss=65.4908
	step [78/191], loss=70.4053
	step [79/191], loss=53.0861
	step [80/191], loss=68.9195
	step [81/191], loss=50.6171
	step [82/191], loss=61.5815
	step [83/191], loss=60.8153
	step [84/191], loss=61.1348
	step [85/191], loss=70.8554
	step [86/191], loss=51.9367
	step [87/191], loss=64.2262
	step [88/191], loss=63.8842
	step [89/191], loss=64.3345
	step [90/191], loss=71.3290
	step [91/191], loss=53.2611
	step [92/191], loss=61.9340
	step [93/191], loss=55.9129
	step [94/191], loss=67.8286
	step [95/191], loss=56.3840
	step [96/191], loss=70.5862
	step [97/191], loss=67.3110
	step [98/191], loss=57.5059
	step [99/191], loss=73.9718
	step [100/191], loss=61.9195
	step [101/191], loss=71.0868
	step [102/191], loss=53.4264
	step [103/191], loss=61.3271
	step [104/191], loss=59.9518
	step [105/191], loss=55.6357
	step [106/191], loss=60.2196
	step [107/191], loss=68.8601
	step [108/191], loss=62.1450
	step [109/191], loss=65.8005
	step [110/191], loss=56.9274
	step [111/191], loss=71.8792
	step [112/191], loss=68.5779
	step [113/191], loss=55.7440
	step [114/191], loss=65.5487
	step [115/191], loss=61.1060
	step [116/191], loss=59.1592
	step [117/191], loss=73.9376
	step [118/191], loss=63.2687
	step [119/191], loss=54.7558
	step [120/191], loss=66.0134
	step [121/191], loss=60.4594
	step [122/191], loss=67.3828
	step [123/191], loss=71.7326
	step [124/191], loss=54.4383
	step [125/191], loss=65.0838
	step [126/191], loss=70.8155
	step [127/191], loss=63.6173
	step [128/191], loss=57.9340
	step [129/191], loss=47.0539
	step [130/191], loss=67.7934
	step [131/191], loss=61.2050
	step [132/191], loss=63.3121
	step [133/191], loss=74.3869
	step [134/191], loss=68.8934
	step [135/191], loss=57.4263
	step [136/191], loss=62.5910
	step [137/191], loss=56.6644
	step [138/191], loss=64.9267
	step [139/191], loss=56.3122
	step [140/191], loss=64.1001
	step [141/191], loss=58.4638
	step [142/191], loss=58.0231
	step [143/191], loss=74.3512
	step [144/191], loss=63.8173
	step [145/191], loss=66.1443
	step [146/191], loss=57.3081
	step [147/191], loss=67.4595
	step [148/191], loss=68.0740
	step [149/191], loss=52.9068
	step [150/191], loss=58.7900
	step [151/191], loss=59.9698
	step [152/191], loss=75.8584
	step [153/191], loss=64.4596
	step [154/191], loss=66.9940
	step [155/191], loss=56.2769
	step [156/191], loss=68.8158
	step [157/191], loss=67.1947
	step [158/191], loss=61.0878
	step [159/191], loss=69.7441
	step [160/191], loss=69.6796
	step [161/191], loss=57.5678
	step [162/191], loss=57.2901
	step [163/191], loss=71.3301
	step [164/191], loss=58.6264
	step [165/191], loss=59.3101
	step [166/191], loss=67.1349
	step [167/191], loss=70.5750
	step [168/191], loss=62.0801
	step [169/191], loss=56.8905
	step [170/191], loss=63.7810
	step [171/191], loss=65.3083
	step [172/191], loss=64.4379
	step [173/191], loss=72.4214
	step [174/191], loss=65.3769
	step [175/191], loss=67.4984
	step [176/191], loss=55.4027
	step [177/191], loss=59.3614
	step [178/191], loss=63.3918
	step [179/191], loss=62.2780
	step [180/191], loss=61.8180
	step [181/191], loss=59.6948
	step [182/191], loss=67.1966
	step [183/191], loss=64.4435
	step [184/191], loss=56.1342
	step [185/191], loss=66.2351
	step [186/191], loss=61.7566
	step [187/191], loss=62.5269
	step [188/191], loss=57.3526
	step [189/191], loss=63.9293
	step [190/191], loss=65.5985
	step [191/191], loss=41.6274
	Evaluating
	loss=0.0075, precision=0.4337, recall=0.8589, f1=0.5764
Training epoch 109
	step [1/191], loss=66.2289
	step [2/191], loss=58.7318
	step [3/191], loss=66.8362
	step [4/191], loss=67.0351
	step [5/191], loss=62.1326
	step [6/191], loss=62.1045
	step [7/191], loss=54.7387
	step [8/191], loss=59.6943
	step [9/191], loss=61.9976
	step [10/191], loss=61.1620
	step [11/191], loss=53.8224
	step [12/191], loss=62.9620
	step [13/191], loss=63.1665
	step [14/191], loss=59.2255
	step [15/191], loss=70.9778
	step [16/191], loss=58.6004
	step [17/191], loss=68.7317
	step [18/191], loss=55.5193
	step [19/191], loss=63.2968
	step [20/191], loss=47.6439
	step [21/191], loss=52.7372
	step [22/191], loss=63.3042
	step [23/191], loss=61.4290
	step [24/191], loss=58.4307
	step [25/191], loss=65.4365
	step [26/191], loss=62.1554
	step [27/191], loss=66.1855
	step [28/191], loss=56.9489
	step [29/191], loss=65.8226
	step [30/191], loss=62.5753
	step [31/191], loss=56.6060
	step [32/191], loss=61.0855
	step [33/191], loss=72.3069
	step [34/191], loss=49.0955
	step [35/191], loss=62.1810
	step [36/191], loss=68.4965
	step [37/191], loss=71.0937
	step [38/191], loss=71.8202
	step [39/191], loss=64.0380
	step [40/191], loss=55.3252
	step [41/191], loss=57.4705
	step [42/191], loss=69.7136
	step [43/191], loss=69.5039
	step [44/191], loss=49.1601
	step [45/191], loss=66.7773
	step [46/191], loss=54.0996
	step [47/191], loss=63.8709
	step [48/191], loss=64.2177
	step [49/191], loss=67.8609
	step [50/191], loss=63.0363
	step [51/191], loss=56.8135
	step [52/191], loss=61.0450
	step [53/191], loss=54.4781
	step [54/191], loss=56.2057
	step [55/191], loss=70.9192
	step [56/191], loss=75.8410
	step [57/191], loss=63.1001
	step [58/191], loss=82.7989
	step [59/191], loss=67.4841
	step [60/191], loss=57.5506
	step [61/191], loss=67.1823
	step [62/191], loss=61.9404
	step [63/191], loss=67.8783
	step [64/191], loss=60.7134
	step [65/191], loss=61.7803
	step [66/191], loss=63.9987
	step [67/191], loss=62.4526
	step [68/191], loss=66.0508
	step [69/191], loss=56.1528
	step [70/191], loss=55.6666
	step [71/191], loss=64.1151
	step [72/191], loss=56.4809
	step [73/191], loss=62.0174
	step [74/191], loss=63.5686
	step [75/191], loss=79.0355
	step [76/191], loss=61.6263
	step [77/191], loss=55.1472
	step [78/191], loss=59.1672
	step [79/191], loss=64.0870
	step [80/191], loss=51.4604
	step [81/191], loss=71.4620
	step [82/191], loss=50.5298
	step [83/191], loss=59.5801
	step [84/191], loss=69.7569
	step [85/191], loss=65.7579
	step [86/191], loss=63.9202
	step [87/191], loss=73.3279
	step [88/191], loss=49.1016
	step [89/191], loss=59.7143
	step [90/191], loss=54.6182
	step [91/191], loss=67.4552
	step [92/191], loss=57.5445
	step [93/191], loss=49.5470
	step [94/191], loss=72.2641
	step [95/191], loss=70.4815
	step [96/191], loss=57.4408
	step [97/191], loss=59.7495
	step [98/191], loss=59.6425
	step [99/191], loss=59.1966
	step [100/191], loss=60.5719
	step [101/191], loss=63.2592
	step [102/191], loss=59.5199
	step [103/191], loss=52.0418
	step [104/191], loss=56.0187
	step [105/191], loss=58.7947
	step [106/191], loss=59.1410
	step [107/191], loss=59.6352
	step [108/191], loss=73.3947
	step [109/191], loss=59.2021
	step [110/191], loss=64.0070
	step [111/191], loss=65.1145
	step [112/191], loss=81.2434
	step [113/191], loss=54.9664
	step [114/191], loss=60.1828
	step [115/191], loss=62.4873
	step [116/191], loss=61.1631
	step [117/191], loss=66.6237
	step [118/191], loss=67.4191
	step [119/191], loss=66.7346
	step [120/191], loss=60.4267
	step [121/191], loss=67.0778
	step [122/191], loss=65.2399
	step [123/191], loss=62.9609
	step [124/191], loss=69.5139
	step [125/191], loss=67.1247
	step [126/191], loss=64.7353
	step [127/191], loss=59.7796
	step [128/191], loss=58.4388
	step [129/191], loss=64.5278
	step [130/191], loss=68.4477
	step [131/191], loss=55.2363
	step [132/191], loss=56.8527
	step [133/191], loss=53.0236
	step [134/191], loss=63.4403
	step [135/191], loss=78.1539
	step [136/191], loss=60.8916
	step [137/191], loss=64.3636
	step [138/191], loss=58.0008
	step [139/191], loss=56.1111
	step [140/191], loss=61.2906
	step [141/191], loss=51.9512
	step [142/191], loss=57.7919
	step [143/191], loss=66.0480
	step [144/191], loss=67.6651
	step [145/191], loss=53.6851
	step [146/191], loss=61.2503
	step [147/191], loss=61.2203
	step [148/191], loss=69.0500
	step [149/191], loss=69.9156
	step [150/191], loss=64.0319
	step [151/191], loss=70.6615
	step [152/191], loss=67.8301
	step [153/191], loss=55.3330
	step [154/191], loss=66.3929
	step [155/191], loss=62.2338
	step [156/191], loss=66.5456
	step [157/191], loss=58.8387
	step [158/191], loss=67.3326
	step [159/191], loss=57.0298
	step [160/191], loss=65.9748
	step [161/191], loss=65.2483
	step [162/191], loss=69.3680
	step [163/191], loss=64.9387
	step [164/191], loss=63.2450
	step [165/191], loss=60.7515
	step [166/191], loss=67.1623
	step [167/191], loss=65.2283
	step [168/191], loss=65.1999
	step [169/191], loss=62.2631
	step [170/191], loss=60.6397
	step [171/191], loss=52.5413
	step [172/191], loss=76.3912
	step [173/191], loss=65.8491
	step [174/191], loss=62.3429
	step [175/191], loss=62.4190
	step [176/191], loss=59.2425
	step [177/191], loss=58.1469
	step [178/191], loss=57.0013
	step [179/191], loss=53.7614
	step [180/191], loss=58.8217
	step [181/191], loss=66.7907
	step [182/191], loss=60.0113
	step [183/191], loss=58.0917
	step [184/191], loss=63.3139
	step [185/191], loss=42.6052
	step [186/191], loss=61.6341
	step [187/191], loss=60.1567
	step [188/191], loss=62.2523
	step [189/191], loss=70.4545
	step [190/191], loss=59.1893
	step [191/191], loss=39.6318
	Evaluating
	loss=0.0060, precision=0.5031, recall=0.8486, f1=0.6317
Training epoch 110
	step [1/191], loss=62.2100
	step [2/191], loss=57.7206
	step [3/191], loss=61.8861
	step [4/191], loss=53.1337
	step [5/191], loss=65.2563
	step [6/191], loss=56.6880
	step [7/191], loss=61.5899
	step [8/191], loss=85.0900
	step [9/191], loss=69.7537
	step [10/191], loss=81.5697
	step [11/191], loss=67.0442
	step [12/191], loss=72.6845
	step [13/191], loss=50.9502
	step [14/191], loss=57.7634
	step [15/191], loss=58.0984
	step [16/191], loss=68.1149
	step [17/191], loss=66.9906
	step [18/191], loss=65.5784
	step [19/191], loss=65.3518
	step [20/191], loss=46.6730
	step [21/191], loss=66.2813
	step [22/191], loss=56.3230
	step [23/191], loss=48.6738
	step [24/191], loss=65.5273
	step [25/191], loss=69.6998
	step [26/191], loss=65.9008
	step [27/191], loss=66.8279
	step [28/191], loss=68.3769
	step [29/191], loss=55.3836
	step [30/191], loss=62.8633
	step [31/191], loss=66.0629
	step [32/191], loss=68.3879
	step [33/191], loss=58.3113
	step [34/191], loss=57.7043
	step [35/191], loss=70.8379
	step [36/191], loss=53.4998
	step [37/191], loss=56.0454
	step [38/191], loss=59.3168
	step [39/191], loss=59.1292
	step [40/191], loss=70.7486
	step [41/191], loss=65.8474
	step [42/191], loss=60.1684
	step [43/191], loss=52.8791
	step [44/191], loss=78.2736
	step [45/191], loss=48.2377
	step [46/191], loss=69.4054
	step [47/191], loss=55.5170
	step [48/191], loss=58.3691
	step [49/191], loss=69.3598
	step [50/191], loss=72.3946
	step [51/191], loss=62.3972
	step [52/191], loss=61.8422
	step [53/191], loss=68.0720
	step [54/191], loss=61.1854
	step [55/191], loss=80.0461
	step [56/191], loss=73.0978
	step [57/191], loss=61.0219
	step [58/191], loss=57.9727
	step [59/191], loss=61.3414
	step [60/191], loss=54.8774
	step [61/191], loss=50.2155
	step [62/191], loss=51.9983
	step [63/191], loss=54.7033
	step [64/191], loss=58.5092
	step [65/191], loss=72.6122
	step [66/191], loss=58.6024
	step [67/191], loss=67.1290
	step [68/191], loss=66.4597
	step [69/191], loss=69.5417
	step [70/191], loss=72.2189
	step [71/191], loss=62.8198
	step [72/191], loss=61.3621
	step [73/191], loss=60.1593
	step [74/191], loss=67.4678
	step [75/191], loss=53.9014
	step [76/191], loss=64.3577
	step [77/191], loss=48.5103
	step [78/191], loss=56.6133
	step [79/191], loss=52.3436
	step [80/191], loss=69.8766
	step [81/191], loss=55.8985
	step [82/191], loss=61.7158
	step [83/191], loss=65.1628
	step [84/191], loss=49.6374
	step [85/191], loss=61.8962
	step [86/191], loss=72.3437
	step [87/191], loss=56.7826
	step [88/191], loss=57.3521
	step [89/191], loss=58.0922
	step [90/191], loss=55.3095
	step [91/191], loss=65.1879
	step [92/191], loss=59.2461
	step [93/191], loss=61.2219
	step [94/191], loss=60.3383
	step [95/191], loss=61.7884
	step [96/191], loss=61.0629
	step [97/191], loss=65.4725
	step [98/191], loss=65.3796
	step [99/191], loss=56.7745
	step [100/191], loss=63.6734
	step [101/191], loss=61.5588
	step [102/191], loss=62.3715
	step [103/191], loss=61.4348
	step [104/191], loss=66.6374
	step [105/191], loss=63.1182
	step [106/191], loss=68.2943
	step [107/191], loss=59.8059
	step [108/191], loss=63.9020
	step [109/191], loss=60.0243
	step [110/191], loss=76.8398
	step [111/191], loss=67.3127
	step [112/191], loss=58.7432
	step [113/191], loss=61.6252
	step [114/191], loss=63.1163
	step [115/191], loss=68.5695
	step [116/191], loss=64.9987
	step [117/191], loss=66.9602
	step [118/191], loss=56.3282
	step [119/191], loss=58.7626
	step [120/191], loss=62.8787
	step [121/191], loss=54.2024
	step [122/191], loss=60.4212
	step [123/191], loss=60.5207
	step [124/191], loss=53.1993
	step [125/191], loss=76.6549
	step [126/191], loss=57.6689
	step [127/191], loss=61.9552
	step [128/191], loss=57.6866
	step [129/191], loss=65.2157
	step [130/191], loss=59.9862
	step [131/191], loss=60.8732
	step [132/191], loss=54.1371
	step [133/191], loss=66.7321
	step [134/191], loss=57.1857
	step [135/191], loss=58.5046
	step [136/191], loss=74.9030
	step [137/191], loss=73.1138
	step [138/191], loss=68.3656
	step [139/191], loss=67.9067
	step [140/191], loss=52.9965
	step [141/191], loss=80.3559
	step [142/191], loss=68.7620
	step [143/191], loss=58.4595
	step [144/191], loss=57.4845
	step [145/191], loss=52.5620
	step [146/191], loss=66.1891
	step [147/191], loss=53.2474
	step [148/191], loss=55.3945
	step [149/191], loss=53.1616
	step [150/191], loss=73.4552
	step [151/191], loss=61.4523
	step [152/191], loss=62.8184
	step [153/191], loss=58.5490
	step [154/191], loss=67.0106
	step [155/191], loss=71.1400
	step [156/191], loss=68.9528
	step [157/191], loss=65.3459
	step [158/191], loss=59.9669
	step [159/191], loss=58.2095
	step [160/191], loss=60.0073
	step [161/191], loss=60.6250
	step [162/191], loss=60.5008
	step [163/191], loss=60.0508
	step [164/191], loss=53.4853
	step [165/191], loss=58.7082
	step [166/191], loss=59.7868
	step [167/191], loss=67.3936
	step [168/191], loss=67.4805
	step [169/191], loss=63.9738
	step [170/191], loss=58.5464
	step [171/191], loss=58.9160
	step [172/191], loss=60.3796
	step [173/191], loss=58.6725
	step [174/191], loss=52.7374
	step [175/191], loss=58.9278
	step [176/191], loss=57.8314
	step [177/191], loss=60.5789
	step [178/191], loss=69.6547
	step [179/191], loss=74.4039
	step [180/191], loss=62.2098
	step [181/191], loss=57.7289
	step [182/191], loss=53.1104
	step [183/191], loss=61.6623
	step [184/191], loss=58.4004
	step [185/191], loss=50.8394
	step [186/191], loss=71.0622
	step [187/191], loss=71.8921
	step [188/191], loss=62.6874
	step [189/191], loss=58.0902
	step [190/191], loss=60.4977
	step [191/191], loss=30.6853
	Evaluating
	loss=0.0067, precision=0.4801, recall=0.8509, f1=0.6139
Training epoch 111
	step [1/191], loss=72.7495
	step [2/191], loss=60.1040
	step [3/191], loss=59.8823
	step [4/191], loss=64.7306
	step [5/191], loss=60.7687
	step [6/191], loss=68.1145
	step [7/191], loss=58.9493
	step [8/191], loss=60.3830
	step [9/191], loss=62.1419
	step [10/191], loss=77.6995
	step [11/191], loss=58.8139
	step [12/191], loss=63.1917
	step [13/191], loss=59.7881
	step [14/191], loss=56.9966
	step [15/191], loss=63.7149
	step [16/191], loss=59.3320
	step [17/191], loss=63.3278
	step [18/191], loss=64.8572
	step [19/191], loss=66.0237
	step [20/191], loss=54.6214
	step [21/191], loss=62.3420
	step [22/191], loss=61.5283
	step [23/191], loss=65.0212
	step [24/191], loss=63.8677
	step [25/191], loss=60.9475
	step [26/191], loss=60.6918
	step [27/191], loss=72.7638
	step [28/191], loss=63.3221
	step [29/191], loss=62.6208
	step [30/191], loss=57.2029
	step [31/191], loss=60.9670
	step [32/191], loss=61.7897
	step [33/191], loss=60.6739
	step [34/191], loss=62.9416
	step [35/191], loss=59.2525
	step [36/191], loss=58.0728
	step [37/191], loss=55.4039
	step [38/191], loss=62.9411
	step [39/191], loss=60.2571
	step [40/191], loss=69.8549
	step [41/191], loss=60.5068
	step [42/191], loss=52.1904
	step [43/191], loss=64.4309
	step [44/191], loss=65.4234
	step [45/191], loss=69.9793
	step [46/191], loss=64.0478
	step [47/191], loss=55.7014
	step [48/191], loss=63.5950
	step [49/191], loss=65.5575
	step [50/191], loss=63.9106
	step [51/191], loss=61.0188
	step [52/191], loss=66.2941
	step [53/191], loss=60.5239
	step [54/191], loss=63.6119
	step [55/191], loss=62.0495
	step [56/191], loss=68.0750
	step [57/191], loss=52.1443
	step [58/191], loss=55.9452
	step [59/191], loss=56.4405
	step [60/191], loss=67.1212
	step [61/191], loss=69.7188
	step [62/191], loss=58.8887
	step [63/191], loss=65.0800
	step [64/191], loss=55.9221
	step [65/191], loss=66.1273
	step [66/191], loss=67.4070
	step [67/191], loss=63.0270
	step [68/191], loss=60.0852
	step [69/191], loss=51.9027
	step [70/191], loss=59.0636
	step [71/191], loss=54.3078
	step [72/191], loss=57.7285
	step [73/191], loss=61.3247
	step [74/191], loss=68.0359
	step [75/191], loss=61.1165
	step [76/191], loss=57.2150
	step [77/191], loss=53.0437
	step [78/191], loss=53.9664
	step [79/191], loss=65.0562
	step [80/191], loss=46.7299
	step [81/191], loss=60.5622
	step [82/191], loss=63.4743
	step [83/191], loss=67.8188
	step [84/191], loss=65.5152
	step [85/191], loss=57.8651
	step [86/191], loss=67.1519
	step [87/191], loss=50.9205
	step [88/191], loss=69.3094
	step [89/191], loss=66.9860
	step [90/191], loss=65.3885
	step [91/191], loss=53.5133
	step [92/191], loss=68.4612
	step [93/191], loss=64.4107
	step [94/191], loss=51.3147
	step [95/191], loss=63.0172
	step [96/191], loss=68.8894
	step [97/191], loss=56.3030
	step [98/191], loss=59.0625
	step [99/191], loss=72.7470
	step [100/191], loss=53.6548
	step [101/191], loss=71.4813
	step [102/191], loss=45.0895
	step [103/191], loss=69.3376
	step [104/191], loss=56.2447
	step [105/191], loss=54.5127
	step [106/191], loss=57.1333
	step [107/191], loss=56.8016
	step [108/191], loss=66.1420
	step [109/191], loss=57.2337
	step [110/191], loss=49.5177
	step [111/191], loss=66.8391
	step [112/191], loss=69.4382
	step [113/191], loss=76.4621
	step [114/191], loss=67.8379
	step [115/191], loss=70.4191
	step [116/191], loss=60.5580
	step [117/191], loss=54.9467
	step [118/191], loss=65.4557
	step [119/191], loss=47.8756
	step [120/191], loss=64.1344
	step [121/191], loss=62.3127
	step [122/191], loss=63.3632
	step [123/191], loss=52.3290
	step [124/191], loss=63.6827
	step [125/191], loss=60.3786
	step [126/191], loss=55.9947
	step [127/191], loss=68.2512
	step [128/191], loss=68.7045
	step [129/191], loss=46.4534
	step [130/191], loss=60.0816
	step [131/191], loss=56.7971
	step [132/191], loss=68.7103
	step [133/191], loss=66.8641
	step [134/191], loss=70.2855
	step [135/191], loss=74.6986
	step [136/191], loss=59.9985
	step [137/191], loss=62.5844
	step [138/191], loss=45.3936
	step [139/191], loss=67.4561
	step [140/191], loss=59.8102
	step [141/191], loss=78.2451
	step [142/191], loss=63.7297
	step [143/191], loss=58.7775
	step [144/191], loss=58.7452
	step [145/191], loss=61.8006
	step [146/191], loss=60.2293
	step [147/191], loss=65.7328
	step [148/191], loss=61.8326
	step [149/191], loss=61.9663
	step [150/191], loss=62.9372
	step [151/191], loss=60.0836
	step [152/191], loss=62.5638
	step [153/191], loss=68.0821
	step [154/191], loss=60.4904
	step [155/191], loss=59.9557
	step [156/191], loss=68.1169
	step [157/191], loss=49.8540
	step [158/191], loss=60.4444
	step [159/191], loss=62.3152
	step [160/191], loss=53.8623
	step [161/191], loss=56.4274
	step [162/191], loss=57.7685
	step [163/191], loss=64.0263
	step [164/191], loss=67.0026
	step [165/191], loss=64.5423
	step [166/191], loss=62.9995
	step [167/191], loss=68.9011
	step [168/191], loss=60.0597
	step [169/191], loss=57.5531
	step [170/191], loss=49.8713
	step [171/191], loss=78.9191
	step [172/191], loss=60.6983
	step [173/191], loss=71.8509
	step [174/191], loss=58.8601
	step [175/191], loss=63.8723
	step [176/191], loss=74.6070
	step [177/191], loss=66.3105
	step [178/191], loss=62.1766
	step [179/191], loss=66.3914
	step [180/191], loss=66.7563
	step [181/191], loss=60.6821
	step [182/191], loss=61.6282
	step [183/191], loss=57.2121
	step [184/191], loss=66.8683
	step [185/191], loss=68.4610
	step [186/191], loss=59.0606
	step [187/191], loss=60.3314
	step [188/191], loss=59.9021
	step [189/191], loss=71.4497
	step [190/191], loss=82.0586
	step [191/191], loss=30.2583
	Evaluating
	loss=0.0066, precision=0.4790, recall=0.8446, f1=0.6113
Training epoch 112
	step [1/191], loss=59.9937
	step [2/191], loss=66.7004
	step [3/191], loss=61.4768
	step [4/191], loss=58.8214
	step [5/191], loss=56.7896
	step [6/191], loss=70.4398
	step [7/191], loss=61.7374
	step [8/191], loss=50.2117
	step [9/191], loss=59.3430
	step [10/191], loss=53.6918
	step [11/191], loss=60.3110
	step [12/191], loss=62.3075
	step [13/191], loss=57.5208
	step [14/191], loss=57.8308
	step [15/191], loss=57.1250
	step [16/191], loss=56.4915
	step [17/191], loss=75.5478
	step [18/191], loss=58.4931
	step [19/191], loss=70.3225
	step [20/191], loss=49.4239
	step [21/191], loss=68.1841
	step [22/191], loss=75.1129
	step [23/191], loss=59.1872
	step [24/191], loss=66.9064
	step [25/191], loss=67.1670
	step [26/191], loss=57.4276
	step [27/191], loss=60.4666
	step [28/191], loss=59.7625
	step [29/191], loss=57.4273
	step [30/191], loss=61.6704
	step [31/191], loss=56.2640
	step [32/191], loss=67.2314
	step [33/191], loss=60.2613
	step [34/191], loss=57.8770
	step [35/191], loss=63.7563
	step [36/191], loss=67.9101
	step [37/191], loss=61.6025
	step [38/191], loss=74.9836
	step [39/191], loss=59.3032
	step [40/191], loss=68.8309
	step [41/191], loss=73.3648
	step [42/191], loss=53.5088
	step [43/191], loss=56.9032
	step [44/191], loss=54.4016
	step [45/191], loss=65.5895
	step [46/191], loss=57.2135
	step [47/191], loss=58.2698
	step [48/191], loss=76.9997
	step [49/191], loss=63.4914
	step [50/191], loss=53.2251
	step [51/191], loss=69.3055
	step [52/191], loss=62.5180
	step [53/191], loss=60.8889
	step [54/191], loss=58.6220
	step [55/191], loss=50.2339
	step [56/191], loss=61.9321
	step [57/191], loss=57.8464
	step [58/191], loss=62.3057
	step [59/191], loss=68.7783
	step [60/191], loss=56.9829
	step [61/191], loss=61.8429
	step [62/191], loss=51.5719
	step [63/191], loss=61.1610
	step [64/191], loss=58.4573
	step [65/191], loss=64.4353
	step [66/191], loss=63.7909
	step [67/191], loss=57.0109
	step [68/191], loss=65.7710
	step [69/191], loss=67.1225
	step [70/191], loss=62.9045
	step [71/191], loss=65.8869
	step [72/191], loss=53.5896
	step [73/191], loss=65.4964
	step [74/191], loss=74.7171
	step [75/191], loss=71.6371
	step [76/191], loss=63.8048
	step [77/191], loss=58.0310
	step [78/191], loss=68.2937
	step [79/191], loss=61.9546
	step [80/191], loss=54.7055
	step [81/191], loss=62.5666
	step [82/191], loss=58.3390
	step [83/191], loss=60.6461
	step [84/191], loss=59.7551
	step [85/191], loss=67.6325
	step [86/191], loss=64.9601
	step [87/191], loss=74.6669
	step [88/191], loss=57.4230
	step [89/191], loss=65.3630
	step [90/191], loss=60.3341
	step [91/191], loss=65.5044
	step [92/191], loss=75.1809
	step [93/191], loss=65.0217
	step [94/191], loss=55.0708
	step [95/191], loss=61.6322
	step [96/191], loss=62.1768
	step [97/191], loss=71.5330
	step [98/191], loss=56.1630
	step [99/191], loss=63.1221
	step [100/191], loss=58.0395
	step [101/191], loss=59.9902
	step [102/191], loss=60.1672
	step [103/191], loss=57.7003
	step [104/191], loss=61.2725
	step [105/191], loss=54.5827
	step [106/191], loss=68.5094
	step [107/191], loss=57.8542
	step [108/191], loss=60.8848
	step [109/191], loss=66.8576
	step [110/191], loss=58.2097
	step [111/191], loss=57.4685
	step [112/191], loss=64.7601
	step [113/191], loss=64.9612
	step [114/191], loss=64.8671
	step [115/191], loss=64.9865
	step [116/191], loss=63.3124
	step [117/191], loss=58.7088
	step [118/191], loss=67.1345
	step [119/191], loss=54.9173
	step [120/191], loss=66.7032
	step [121/191], loss=56.3866
	step [122/191], loss=70.5162
	step [123/191], loss=78.5247
	step [124/191], loss=56.4676
	step [125/191], loss=57.6707
	step [126/191], loss=60.1379
	step [127/191], loss=63.8059
	step [128/191], loss=59.8047
	step [129/191], loss=69.9112
	step [130/191], loss=54.7529
	step [131/191], loss=58.9958
	step [132/191], loss=55.0559
	step [133/191], loss=57.7983
	step [134/191], loss=66.9286
	step [135/191], loss=60.2632
	step [136/191], loss=57.0958
	step [137/191], loss=54.5655
	step [138/191], loss=57.0766
	step [139/191], loss=58.4768
	step [140/191], loss=54.8082
	step [141/191], loss=63.7387
	step [142/191], loss=53.2154
	step [143/191], loss=62.5282
	step [144/191], loss=56.9375
	step [145/191], loss=63.0148
	step [146/191], loss=68.1135
	step [147/191], loss=62.9513
	step [148/191], loss=56.4345
	step [149/191], loss=63.1394
	step [150/191], loss=57.3577
	step [151/191], loss=71.2707
	step [152/191], loss=64.6208
	step [153/191], loss=68.6141
	step [154/191], loss=67.8269
	step [155/191], loss=57.9037
	step [156/191], loss=59.8900
	step [157/191], loss=49.5369
	step [158/191], loss=77.7596
	step [159/191], loss=56.9987
	step [160/191], loss=52.9174
	step [161/191], loss=65.2033
	step [162/191], loss=57.0968
	step [163/191], loss=75.0729
	step [164/191], loss=64.6999
	step [165/191], loss=55.3938
	step [166/191], loss=55.1709
	step [167/191], loss=66.2304
	step [168/191], loss=52.8664
	step [169/191], loss=70.4566
	step [170/191], loss=57.6661
	step [171/191], loss=73.4034
	step [172/191], loss=60.3357
	step [173/191], loss=68.9465
	step [174/191], loss=62.7300
	step [175/191], loss=63.4516
	step [176/191], loss=69.9945
	step [177/191], loss=56.2599
	step [178/191], loss=60.5008
	step [179/191], loss=67.2207
	step [180/191], loss=55.1368
	step [181/191], loss=54.1428
	step [182/191], loss=62.4573
	step [183/191], loss=66.4867
	step [184/191], loss=72.1873
	step [185/191], loss=65.3957
	step [186/191], loss=67.7128
	step [187/191], loss=66.1007
	step [188/191], loss=63.0240
	step [189/191], loss=48.0643
	step [190/191], loss=63.7702
	step [191/191], loss=37.0401
	Evaluating
	loss=0.0062, precision=0.4948, recall=0.8452, f1=0.6242
Training epoch 113
	step [1/191], loss=64.8680
	step [2/191], loss=56.7059
	step [3/191], loss=64.5526
	step [4/191], loss=57.3721
	step [5/191], loss=53.8273
	step [6/191], loss=51.9727
	step [7/191], loss=53.6282
	step [8/191], loss=61.0236
	step [9/191], loss=65.5415
	step [10/191], loss=57.7459
	step [11/191], loss=55.6435
	step [12/191], loss=66.6483
	step [13/191], loss=67.0568
	step [14/191], loss=69.3365
	step [15/191], loss=61.0649
	step [16/191], loss=70.7018
	step [17/191], loss=51.6753
	step [18/191], loss=62.1750
	step [19/191], loss=58.4578
	step [20/191], loss=65.1666
	step [21/191], loss=61.9312
	step [22/191], loss=64.6807
	step [23/191], loss=65.0630
	step [24/191], loss=53.9368
	step [25/191], loss=57.3656
	step [26/191], loss=64.1672
	step [27/191], loss=65.5164
	step [28/191], loss=58.0088
	step [29/191], loss=56.5759
	step [30/191], loss=66.0904
	step [31/191], loss=62.5320
	step [32/191], loss=77.0605
	step [33/191], loss=63.2531
	step [34/191], loss=70.4444
	step [35/191], loss=65.3662
	step [36/191], loss=57.6984
	step [37/191], loss=63.2659
	step [38/191], loss=63.3563
	step [39/191], loss=65.2240
	step [40/191], loss=58.8805
	step [41/191], loss=59.8148
	step [42/191], loss=77.5831
	step [43/191], loss=64.0592
	step [44/191], loss=63.4437
	step [45/191], loss=67.6674
	step [46/191], loss=51.2298
	step [47/191], loss=64.7464
	step [48/191], loss=49.5889
	step [49/191], loss=56.4034
	step [50/191], loss=50.7068
	step [51/191], loss=55.6963
	step [52/191], loss=61.6751
	step [53/191], loss=58.0248
	step [54/191], loss=62.2221
	step [55/191], loss=58.3236
	step [56/191], loss=61.0070
	step [57/191], loss=55.0391
	step [58/191], loss=58.9796
	step [59/191], loss=54.5279
	step [60/191], loss=60.0222
	step [61/191], loss=66.3648
	step [62/191], loss=56.1783
	step [63/191], loss=71.8999
	step [64/191], loss=64.8452
	step [65/191], loss=59.6844
	step [66/191], loss=60.1116
	step [67/191], loss=61.9000
	step [68/191], loss=46.9764
	step [69/191], loss=61.4968
	step [70/191], loss=66.7237
	step [71/191], loss=68.6155
	step [72/191], loss=69.5294
	step [73/191], loss=63.9008
	step [74/191], loss=58.3385
	step [75/191], loss=65.8500
	step [76/191], loss=53.8331
	step [77/191], loss=55.2236
	step [78/191], loss=63.5662
	step [79/191], loss=54.7683
	step [80/191], loss=69.6911
	step [81/191], loss=67.9133
	step [82/191], loss=48.7655
	step [83/191], loss=66.9637
	step [84/191], loss=60.7371
	step [85/191], loss=62.0108
	step [86/191], loss=65.0048
	step [87/191], loss=61.1122
	step [88/191], loss=61.1026
	step [89/191], loss=70.1505
	step [90/191], loss=69.0163
	step [91/191], loss=51.8815
	step [92/191], loss=67.2795
	step [93/191], loss=66.5607
	step [94/191], loss=67.5250
	step [95/191], loss=61.1363
	step [96/191], loss=56.4512
	step [97/191], loss=53.3038
	step [98/191], loss=64.9359
	step [99/191], loss=63.1218
	step [100/191], loss=62.4499
	step [101/191], loss=58.8788
	step [102/191], loss=62.5493
	step [103/191], loss=67.6900
	step [104/191], loss=58.9611
	step [105/191], loss=64.7622
	step [106/191], loss=85.3011
	step [107/191], loss=61.9200
	step [108/191], loss=70.1871
	step [109/191], loss=54.1124
	step [110/191], loss=59.3362
	step [111/191], loss=52.5497
	step [112/191], loss=62.9849
	step [113/191], loss=61.4407
	step [114/191], loss=63.8955
	step [115/191], loss=71.4731
	step [116/191], loss=59.5079
	step [117/191], loss=52.3732
	step [118/191], loss=63.9574
	step [119/191], loss=69.4639
	step [120/191], loss=64.4189
	step [121/191], loss=54.4691
	step [122/191], loss=60.3862
	step [123/191], loss=69.0588
	step [124/191], loss=65.2222
	step [125/191], loss=74.4135
	step [126/191], loss=63.7998
	step [127/191], loss=61.3084
	step [128/191], loss=61.6343
	step [129/191], loss=60.2907
	step [130/191], loss=66.1563
	step [131/191], loss=63.0348
	step [132/191], loss=61.5976
	step [133/191], loss=58.0743
	step [134/191], loss=53.6709
	step [135/191], loss=61.9992
	step [136/191], loss=58.6342
	step [137/191], loss=61.9259
	step [138/191], loss=62.7808
	step [139/191], loss=62.3875
	step [140/191], loss=58.5647
	step [141/191], loss=55.5950
	step [142/191], loss=73.8542
	step [143/191], loss=58.6895
	step [144/191], loss=68.1383
	step [145/191], loss=56.8601
	step [146/191], loss=75.5544
	step [147/191], loss=56.4915
	step [148/191], loss=68.5577
	step [149/191], loss=59.5758
	step [150/191], loss=71.3370
	step [151/191], loss=65.3120
	step [152/191], loss=69.7349
	step [153/191], loss=65.5228
	step [154/191], loss=53.3997
	step [155/191], loss=62.8107
	step [156/191], loss=49.7055
	step [157/191], loss=74.5315
	step [158/191], loss=62.1620
	step [159/191], loss=53.5484
	step [160/191], loss=64.0530
	step [161/191], loss=63.4426
	step [162/191], loss=60.8152
	step [163/191], loss=65.7461
	step [164/191], loss=75.1370
	step [165/191], loss=65.9914
	step [166/191], loss=64.8946
	step [167/191], loss=57.4843
	step [168/191], loss=66.5772
	step [169/191], loss=68.3440
	step [170/191], loss=59.9473
	step [171/191], loss=59.1328
	step [172/191], loss=55.3312
	step [173/191], loss=52.8608
	step [174/191], loss=55.0301
	step [175/191], loss=74.0188
	step [176/191], loss=72.9953
	step [177/191], loss=57.7213
	step [178/191], loss=64.6392
	step [179/191], loss=57.4037
	step [180/191], loss=63.4248
	step [181/191], loss=45.2376
	step [182/191], loss=59.0207
	step [183/191], loss=48.9056
	step [184/191], loss=54.7542
	step [185/191], loss=59.7678
	step [186/191], loss=56.8270
	step [187/191], loss=67.4621
	step [188/191], loss=62.1246
	step [189/191], loss=52.8619
	step [190/191], loss=58.0067
	step [191/191], loss=32.0222
	Evaluating
	loss=0.0062, precision=0.4971, recall=0.8441, f1=0.6257
Training epoch 114
	step [1/191], loss=52.6516
	step [2/191], loss=48.8869
	step [3/191], loss=63.6632
	step [4/191], loss=68.8345
	step [5/191], loss=59.0933
	step [6/191], loss=67.5115
	step [7/191], loss=60.5362
	step [8/191], loss=60.6254
	step [9/191], loss=73.9409
	step [10/191], loss=56.2229
	step [11/191], loss=58.7791
	step [12/191], loss=64.1478
	step [13/191], loss=60.4596
	step [14/191], loss=61.1108
	step [15/191], loss=61.7213
	step [16/191], loss=79.0774
	step [17/191], loss=58.0672
	step [18/191], loss=67.3518
	step [19/191], loss=61.9791
	step [20/191], loss=55.7484
	step [21/191], loss=57.0271
	step [22/191], loss=67.4871
	step [23/191], loss=63.8143
	step [24/191], loss=58.5488
	step [25/191], loss=54.7675
	step [26/191], loss=60.0740
	step [27/191], loss=70.3727
	step [28/191], loss=71.2372
	step [29/191], loss=66.6975
	step [30/191], loss=52.4879
	step [31/191], loss=63.5107
	step [32/191], loss=64.0788
	step [33/191], loss=70.3982
	step [34/191], loss=71.5623
	step [35/191], loss=71.1639
	step [36/191], loss=77.2045
	step [37/191], loss=60.8870
	step [38/191], loss=61.0919
	step [39/191], loss=69.1185
	step [40/191], loss=60.5508
	step [41/191], loss=52.7394
	step [42/191], loss=63.9472
	step [43/191], loss=57.7076
	step [44/191], loss=68.6089
	step [45/191], loss=63.4979
	step [46/191], loss=54.4508
	step [47/191], loss=60.3943
	step [48/191], loss=69.1090
	step [49/191], loss=61.5652
	step [50/191], loss=65.0367
	step [51/191], loss=57.4711
	step [52/191], loss=63.6806
	step [53/191], loss=56.8124
	step [54/191], loss=66.0423
	step [55/191], loss=52.6160
	step [56/191], loss=61.6825
	step [57/191], loss=50.2992
	step [58/191], loss=58.2369
	step [59/191], loss=60.9537
	step [60/191], loss=58.1398
	step [61/191], loss=60.5750
	step [62/191], loss=61.7896
	step [63/191], loss=59.6122
	step [64/191], loss=53.7120
	step [65/191], loss=50.2033
	step [66/191], loss=50.5633
	step [67/191], loss=57.4551
	step [68/191], loss=51.7605
	step [69/191], loss=59.8100
	step [70/191], loss=62.2400
	step [71/191], loss=54.5692
	step [72/191], loss=63.7667
	step [73/191], loss=66.2019
	step [74/191], loss=57.8834
	step [75/191], loss=66.0467
	step [76/191], loss=72.3156
	step [77/191], loss=67.6703
	step [78/191], loss=59.8046
	step [79/191], loss=51.2864
	step [80/191], loss=69.9054
	step [81/191], loss=68.2549
	step [82/191], loss=66.0815
	step [83/191], loss=69.6638
	step [84/191], loss=55.9352
	step [85/191], loss=56.2202
	step [86/191], loss=67.0181
	step [87/191], loss=52.3984
	step [88/191], loss=66.3984
	step [89/191], loss=54.4708
	step [90/191], loss=70.3717
	step [91/191], loss=55.1917
	step [92/191], loss=60.7310
	step [93/191], loss=57.4834
	step [94/191], loss=71.3415
	step [95/191], loss=60.1545
	step [96/191], loss=60.4892
	step [97/191], loss=65.7239
	step [98/191], loss=66.9722
	step [99/191], loss=66.1938
	step [100/191], loss=50.6385
	step [101/191], loss=47.7829
	step [102/191], loss=58.3479
	step [103/191], loss=72.2947
	step [104/191], loss=67.6494
	step [105/191], loss=63.1839
	step [106/191], loss=58.1493
	step [107/191], loss=57.3932
	step [108/191], loss=55.8858
	step [109/191], loss=62.8864
	step [110/191], loss=54.4139
	step [111/191], loss=63.2115
	step [112/191], loss=52.1434
	step [113/191], loss=64.2484
	step [114/191], loss=71.1431
	step [115/191], loss=54.0896
	step [116/191], loss=65.2116
	step [117/191], loss=65.1872
	step [118/191], loss=59.5683
	step [119/191], loss=63.1689
	step [120/191], loss=70.6114
	step [121/191], loss=54.5250
	step [122/191], loss=64.3860
	step [123/191], loss=51.8129
	step [124/191], loss=58.8275
	step [125/191], loss=70.5118
	step [126/191], loss=66.3628
	step [127/191], loss=57.1648
	step [128/191], loss=56.4693
	step [129/191], loss=60.2031
	step [130/191], loss=60.2444
	step [131/191], loss=52.5629
	step [132/191], loss=79.0372
	step [133/191], loss=69.0460
	step [134/191], loss=66.4287
	step [135/191], loss=59.7243
	step [136/191], loss=54.3842
	step [137/191], loss=65.4121
	step [138/191], loss=55.0256
	step [139/191], loss=54.3097
	step [140/191], loss=62.8309
	step [141/191], loss=49.4647
	step [142/191], loss=64.1969
	step [143/191], loss=62.5794
	step [144/191], loss=63.0603
	step [145/191], loss=67.6438
	step [146/191], loss=58.1390
	step [147/191], loss=51.6699
	step [148/191], loss=61.3013
	step [149/191], loss=56.8630
	step [150/191], loss=65.9411
	step [151/191], loss=65.5800
	step [152/191], loss=66.3189
	step [153/191], loss=48.9597
	step [154/191], loss=63.5462
	step [155/191], loss=56.7501
	step [156/191], loss=59.6300
	step [157/191], loss=61.6945
	step [158/191], loss=58.8037
	step [159/191], loss=75.6159
	step [160/191], loss=59.9965
	step [161/191], loss=60.3243
	step [162/191], loss=71.5485
	step [163/191], loss=58.7981
	step [164/191], loss=52.8457
	step [165/191], loss=63.8803
	step [166/191], loss=72.7689
	step [167/191], loss=75.9816
	step [168/191], loss=72.0748
	step [169/191], loss=64.6203
	step [170/191], loss=67.4692
	step [171/191], loss=62.1575
	step [172/191], loss=61.1786
	step [173/191], loss=54.6050
	step [174/191], loss=59.4105
	step [175/191], loss=59.9452
	step [176/191], loss=54.2225
	step [177/191], loss=56.7139
	step [178/191], loss=60.5521
	step [179/191], loss=57.7297
	step [180/191], loss=62.1557
	step [181/191], loss=53.9771
	step [182/191], loss=70.6555
	step [183/191], loss=62.6011
	step [184/191], loss=65.0298
	step [185/191], loss=55.9019
	step [186/191], loss=65.4990
	step [187/191], loss=65.5625
	step [188/191], loss=71.8109
	step [189/191], loss=72.2066
	step [190/191], loss=72.3584
	step [191/191], loss=30.1477
	Evaluating
	loss=0.0055, precision=0.5373, recall=0.8493, f1=0.6582
saving model as: 1_saved_model.pth
Training finished
best_f1: 0.6582174139000642
directing: Z rim_enhanced: True test_id 2
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12056 # image files with weight 12056
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3488 # image files with weight 3488
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12056
Using 4 GPUs
Going to train epochs [60-109]
Training epoch 60
	step [1/189], loss=64.6183
	step [2/189], loss=73.5269
	step [3/189], loss=94.7417
	step [4/189], loss=76.2825
	step [5/189], loss=71.4467
	step [6/189], loss=77.1798
	step [7/189], loss=91.4828
	step [8/189], loss=80.4303
	step [9/189], loss=86.3564
	step [10/189], loss=72.3479
	step [11/189], loss=73.8052
	step [12/189], loss=73.5745
	step [13/189], loss=74.1721
	step [14/189], loss=90.2629
	step [15/189], loss=86.8831
	step [16/189], loss=84.6995
	step [17/189], loss=82.7628
	step [18/189], loss=78.7134
	step [19/189], loss=63.6798
	step [20/189], loss=69.3484
	step [21/189], loss=79.2510
	step [22/189], loss=78.4610
	step [23/189], loss=76.5061
	step [24/189], loss=79.1707
	step [25/189], loss=73.5195
	step [26/189], loss=77.1461
	step [27/189], loss=88.8482
	step [28/189], loss=89.2112
	step [29/189], loss=78.0872
	step [30/189], loss=69.4108
	step [31/189], loss=71.8771
	step [32/189], loss=71.7111
	step [33/189], loss=92.9780
	step [34/189], loss=73.0324
	step [35/189], loss=78.2373
	step [36/189], loss=65.3928
	step [37/189], loss=83.7029
	step [38/189], loss=95.3006
	step [39/189], loss=82.1769
	step [40/189], loss=73.8130
	step [41/189], loss=67.7272
	step [42/189], loss=78.0289
	step [43/189], loss=82.7173
	step [44/189], loss=76.3350
	step [45/189], loss=81.0990
	step [46/189], loss=68.9802
	step [47/189], loss=79.6693
	step [48/189], loss=81.9264
	step [49/189], loss=74.6552
	step [50/189], loss=69.7845
	step [51/189], loss=80.5068
	step [52/189], loss=80.5992
	step [53/189], loss=78.5373
	step [54/189], loss=95.1490
	step [55/189], loss=76.2139
	step [56/189], loss=88.5937
	step [57/189], loss=83.1736
	step [58/189], loss=92.6983
	step [59/189], loss=67.8612
	step [60/189], loss=80.2620
	step [61/189], loss=66.1398
	step [62/189], loss=80.0282
	step [63/189], loss=75.1465
	step [64/189], loss=69.1670
	step [65/189], loss=74.6017
	step [66/189], loss=74.9022
	step [67/189], loss=86.3827
	step [68/189], loss=62.5171
	step [69/189], loss=77.2198
	step [70/189], loss=76.6041
	step [71/189], loss=77.1386
	step [72/189], loss=79.6285
	step [73/189], loss=76.0560
	step [74/189], loss=82.7845
	step [75/189], loss=89.7796
	step [76/189], loss=84.7623
	step [77/189], loss=81.2726
	step [78/189], loss=81.1356
	step [79/189], loss=73.5004
	step [80/189], loss=71.4228
	step [81/189], loss=86.1863
	step [82/189], loss=66.9148
	step [83/189], loss=78.1259
	step [84/189], loss=74.5596
	step [85/189], loss=71.2206
	step [86/189], loss=92.2805
	step [87/189], loss=81.6734
	step [88/189], loss=66.4710
	step [89/189], loss=73.0358
	step [90/189], loss=84.8543
	step [91/189], loss=82.2360
	step [92/189], loss=87.8748
	step [93/189], loss=78.5256
	step [94/189], loss=76.5562
	step [95/189], loss=78.6291
	step [96/189], loss=88.5165
	step [97/189], loss=65.3754
	step [98/189], loss=80.8801
	step [99/189], loss=89.4468
	step [100/189], loss=65.9525
	step [101/189], loss=87.0910
	step [102/189], loss=82.7803
	step [103/189], loss=91.7555
	step [104/189], loss=63.3329
	step [105/189], loss=84.9595
	step [106/189], loss=79.8145
	step [107/189], loss=70.0623
	step [108/189], loss=81.3063
	step [109/189], loss=62.1896
	step [110/189], loss=80.6944
	step [111/189], loss=84.3070
	step [112/189], loss=80.8841
	step [113/189], loss=78.7423
	step [114/189], loss=66.1405
	step [115/189], loss=84.0347
	step [116/189], loss=88.8244
	step [117/189], loss=74.9429
	step [118/189], loss=74.1088
	step [119/189], loss=65.2608
	step [120/189], loss=83.0161
	step [121/189], loss=82.2922
	step [122/189], loss=87.2229
	step [123/189], loss=66.9718
	step [124/189], loss=77.4524
	step [125/189], loss=89.7412
	step [126/189], loss=98.3344
	step [127/189], loss=79.1665
	step [128/189], loss=90.1436
	step [129/189], loss=78.0772
	step [130/189], loss=71.2747
	step [131/189], loss=75.6171
	step [132/189], loss=81.7436
	step [133/189], loss=75.4619
	step [134/189], loss=83.7076
	step [135/189], loss=69.9236
	step [136/189], loss=73.8959
	step [137/189], loss=83.4757
	step [138/189], loss=63.1782
	step [139/189], loss=75.4703
	step [140/189], loss=75.4683
	step [141/189], loss=78.6395
	step [142/189], loss=69.1237
	step [143/189], loss=82.6013
	step [144/189], loss=77.9232
	step [145/189], loss=73.9573
	step [146/189], loss=92.3100
	step [147/189], loss=83.2974
	step [148/189], loss=87.4867
	step [149/189], loss=69.9062
	step [150/189], loss=78.8344
	step [151/189], loss=76.3314
	step [152/189], loss=65.4923
	step [153/189], loss=89.4251
	step [154/189], loss=74.7174
	step [155/189], loss=78.7200
	step [156/189], loss=83.3651
	step [157/189], loss=91.2470
	step [158/189], loss=75.2461
	step [159/189], loss=78.3025
	step [160/189], loss=79.4551
	step [161/189], loss=77.3516
	step [162/189], loss=84.0537
	step [163/189], loss=88.5691
	step [164/189], loss=71.4507
	step [165/189], loss=80.0309
	step [166/189], loss=75.4178
	step [167/189], loss=75.1921
	step [168/189], loss=68.6278
	step [169/189], loss=74.4223
	step [170/189], loss=80.3573
	step [171/189], loss=74.8190
	step [172/189], loss=81.0546
	step [173/189], loss=68.9796
	step [174/189], loss=79.2531
	step [175/189], loss=78.2462
	step [176/189], loss=83.7371
	step [177/189], loss=78.8064
	step [178/189], loss=79.9258
	step [179/189], loss=74.5524
	step [180/189], loss=71.8604
	step [181/189], loss=65.4695
	step [182/189], loss=85.4875
	step [183/189], loss=77.1595
	step [184/189], loss=77.7860
	step [185/189], loss=72.5248
	step [186/189], loss=90.7641
	step [187/189], loss=73.8850
	step [188/189], loss=82.6337
	step [189/189], loss=35.2528
	Evaluating
	loss=0.0103, precision=0.3557, recall=0.9036, f1=0.5105
saving model as: 2_saved_model.pth
Training epoch 61
	step [1/189], loss=70.4512
	step [2/189], loss=70.4140
	step [3/189], loss=81.5007
	step [4/189], loss=76.5232
	step [5/189], loss=76.1604
	step [6/189], loss=78.7100
	step [7/189], loss=87.4080
	step [8/189], loss=72.3937
	step [9/189], loss=86.1288
	step [10/189], loss=68.0515
	step [11/189], loss=73.9691
	step [12/189], loss=77.6952
	step [13/189], loss=81.4170
	step [14/189], loss=74.1527
	step [15/189], loss=86.4499
	step [16/189], loss=72.1204
	step [17/189], loss=84.0264
	step [18/189], loss=88.8006
	step [19/189], loss=75.4323
	step [20/189], loss=67.8590
	step [21/189], loss=95.3323
	step [22/189], loss=77.1305
	step [23/189], loss=78.3519
	step [24/189], loss=85.0109
	step [25/189], loss=70.5135
	step [26/189], loss=83.2103
	step [27/189], loss=75.7121
	step [28/189], loss=80.2152
	step [29/189], loss=80.8976
	step [30/189], loss=74.6781
	step [31/189], loss=81.8434
	step [32/189], loss=66.9421
	step [33/189], loss=78.8437
	step [34/189], loss=89.8805
	step [35/189], loss=90.8404
	step [36/189], loss=76.1764
	step [37/189], loss=94.3194
	step [38/189], loss=77.4769
	step [39/189], loss=78.7540
	step [40/189], loss=77.4985
	step [41/189], loss=83.8128
	step [42/189], loss=76.4964
	step [43/189], loss=69.7016
	step [44/189], loss=78.1645
	step [45/189], loss=81.6638
	step [46/189], loss=89.3221
	step [47/189], loss=62.1411
	step [48/189], loss=77.6870
	step [49/189], loss=76.9771
	step [50/189], loss=73.3763
	step [51/189], loss=72.4475
	step [52/189], loss=79.1521
	step [53/189], loss=73.5010
	step [54/189], loss=79.5313
	step [55/189], loss=86.2021
	step [56/189], loss=75.5619
	step [57/189], loss=77.5704
	step [58/189], loss=74.8471
	step [59/189], loss=61.6056
	step [60/189], loss=75.7270
	step [61/189], loss=82.2047
	step [62/189], loss=75.9948
	step [63/189], loss=77.1784
	step [64/189], loss=83.5234
	step [65/189], loss=68.4459
	step [66/189], loss=81.3411
	step [67/189], loss=64.3174
	step [68/189], loss=70.3885
	step [69/189], loss=78.5658
	step [70/189], loss=80.3432
	step [71/189], loss=72.6046
	step [72/189], loss=72.4100
	step [73/189], loss=80.2932
	step [74/189], loss=81.7002
	step [75/189], loss=74.4928
	step [76/189], loss=87.2388
	step [77/189], loss=80.6787
	step [78/189], loss=68.8861
	step [79/189], loss=93.0494
	step [80/189], loss=77.1533
	step [81/189], loss=88.6269
	step [82/189], loss=78.1676
	step [83/189], loss=64.6868
	step [84/189], loss=89.8089
	step [85/189], loss=69.6729
	step [86/189], loss=70.8107
	step [87/189], loss=85.6042
	step [88/189], loss=79.8088
	step [89/189], loss=82.5750
	step [90/189], loss=78.3005
	step [91/189], loss=80.8849
	step [92/189], loss=76.6763
	step [93/189], loss=75.4695
	step [94/189], loss=92.3112
	step [95/189], loss=78.2906
	step [96/189], loss=69.1718
	step [97/189], loss=75.1238
	step [98/189], loss=71.4421
	step [99/189], loss=68.9368
	step [100/189], loss=85.4498
	step [101/189], loss=91.4932
	step [102/189], loss=81.8455
	step [103/189], loss=76.8975
	step [104/189], loss=77.6560
	step [105/189], loss=79.5922
	step [106/189], loss=91.1996
	step [107/189], loss=73.1290
	step [108/189], loss=69.6028
	step [109/189], loss=75.4059
	step [110/189], loss=85.7544
	step [111/189], loss=85.5947
	step [112/189], loss=82.5466
	step [113/189], loss=73.9216
	step [114/189], loss=78.4194
	step [115/189], loss=87.0066
	step [116/189], loss=108.9689
	step [117/189], loss=67.8028
	step [118/189], loss=78.4142
	step [119/189], loss=86.2635
	step [120/189], loss=70.9997
	step [121/189], loss=95.0130
	step [122/189], loss=72.5451
	step [123/189], loss=75.1823
	step [124/189], loss=79.3132
	step [125/189], loss=79.4428
	step [126/189], loss=78.4676
	step [127/189], loss=70.7335
	step [128/189], loss=80.2736
	step [129/189], loss=75.8471
	step [130/189], loss=70.9823
	step [131/189], loss=83.0338
	step [132/189], loss=85.1651
	step [133/189], loss=78.5400
	step [134/189], loss=84.5138
	step [135/189], loss=80.2021
	step [136/189], loss=80.9611
	step [137/189], loss=80.6925
	step [138/189], loss=75.0476
	step [139/189], loss=71.8266
	step [140/189], loss=88.1044
	step [141/189], loss=77.2538
	step [142/189], loss=79.7388
	step [143/189], loss=74.4647
	step [144/189], loss=75.6989
	step [145/189], loss=71.0235
	step [146/189], loss=56.5557
	step [147/189], loss=85.5619
	step [148/189], loss=83.5725
	step [149/189], loss=77.8143
	step [150/189], loss=88.5043
	step [151/189], loss=77.8920
	step [152/189], loss=73.4451
	step [153/189], loss=70.4218
	step [154/189], loss=80.6717
	step [155/189], loss=67.4679
	step [156/189], loss=76.1645
	step [157/189], loss=84.9356
	step [158/189], loss=77.7590
	step [159/189], loss=87.8843
	step [160/189], loss=78.3978
	step [161/189], loss=81.4547
	step [162/189], loss=87.2825
	step [163/189], loss=73.6805
	step [164/189], loss=70.8116
	step [165/189], loss=70.5418
	step [166/189], loss=87.3065
	step [167/189], loss=63.7990
	step [168/189], loss=88.8408
	step [169/189], loss=84.2103
	step [170/189], loss=76.1250
	step [171/189], loss=68.9306
	step [172/189], loss=68.7640
	step [173/189], loss=68.4372
	step [174/189], loss=84.7859
	step [175/189], loss=77.3641
	step [176/189], loss=79.3780
	step [177/189], loss=77.7446
	step [178/189], loss=75.6510
	step [179/189], loss=71.4083
	step [180/189], loss=84.8987
	step [181/189], loss=85.1080
	step [182/189], loss=86.8734
	step [183/189], loss=77.3297
	step [184/189], loss=90.8812
	step [185/189], loss=73.9306
	step [186/189], loss=81.2324
	step [187/189], loss=68.8028
	step [188/189], loss=84.4022
	step [189/189], loss=31.3011
	Evaluating
	loss=0.0104, precision=0.3481, recall=0.8995, f1=0.5019
Training epoch 62
	step [1/189], loss=83.0041
	step [2/189], loss=66.3595
	step [3/189], loss=73.0255
	step [4/189], loss=77.4983
	step [5/189], loss=77.4833
	step [6/189], loss=64.1707
	step [7/189], loss=75.4744
	step [8/189], loss=76.6814
	step [9/189], loss=91.7537
	step [10/189], loss=72.9512
	step [11/189], loss=93.6951
	step [12/189], loss=81.7389
	step [13/189], loss=66.3962
	step [14/189], loss=70.5449
	step [15/189], loss=88.9126
	step [16/189], loss=84.5280
	step [17/189], loss=78.9331
	step [18/189], loss=70.5749
	step [19/189], loss=89.2999
	step [20/189], loss=89.3664
	step [21/189], loss=69.4581
	step [22/189], loss=84.6775
	step [23/189], loss=70.1241
	step [24/189], loss=72.9544
	step [25/189], loss=76.0645
	step [26/189], loss=77.0437
	step [27/189], loss=80.7783
	step [28/189], loss=84.1508
	step [29/189], loss=78.0064
	step [30/189], loss=83.1871
	step [31/189], loss=88.1919
	step [32/189], loss=76.0532
	step [33/189], loss=81.3294
	step [34/189], loss=79.0958
	step [35/189], loss=87.7229
	step [36/189], loss=75.1134
	step [37/189], loss=77.1007
	step [38/189], loss=62.0861
	step [39/189], loss=69.3317
	step [40/189], loss=75.0760
	step [41/189], loss=80.3375
	step [42/189], loss=83.9148
	step [43/189], loss=66.1002
	step [44/189], loss=74.3086
	step [45/189], loss=86.8245
	step [46/189], loss=78.5360
	step [47/189], loss=66.4659
	step [48/189], loss=75.1476
	step [49/189], loss=79.2912
	step [50/189], loss=75.6467
	step [51/189], loss=77.5571
	step [52/189], loss=83.5232
	step [53/189], loss=83.3814
	step [54/189], loss=81.3850
	step [55/189], loss=63.8181
	step [56/189], loss=82.1503
	step [57/189], loss=72.4820
	step [58/189], loss=81.9548
	step [59/189], loss=87.3872
	step [60/189], loss=71.3777
	step [61/189], loss=66.5843
	step [62/189], loss=72.6000
	step [63/189], loss=88.1547
	step [64/189], loss=84.7325
	step [65/189], loss=75.1400
	step [66/189], loss=88.3563
	step [67/189], loss=66.4680
	step [68/189], loss=82.1066
	step [69/189], loss=86.7680
	step [70/189], loss=71.3499
	step [71/189], loss=92.6824
	step [72/189], loss=93.1567
	step [73/189], loss=85.7205
	step [74/189], loss=81.8418
	step [75/189], loss=69.3317
	step [76/189], loss=67.1577
	step [77/189], loss=75.2597
	step [78/189], loss=69.5552
	step [79/189], loss=79.7275
	step [80/189], loss=82.7533
	step [81/189], loss=78.8321
	step [82/189], loss=80.4417
	step [83/189], loss=74.2271
	step [84/189], loss=70.3355
	step [85/189], loss=77.5871
	step [86/189], loss=96.8885
	step [87/189], loss=69.2395
	step [88/189], loss=75.5160
	step [89/189], loss=75.7950
	step [90/189], loss=75.7744
	step [91/189], loss=88.3572
	step [92/189], loss=75.6563
	step [93/189], loss=58.9921
	step [94/189], loss=84.7295
	step [95/189], loss=72.5923
	step [96/189], loss=80.3797
	step [97/189], loss=65.3294
	step [98/189], loss=74.1737
	step [99/189], loss=71.9845
	step [100/189], loss=93.2646
	step [101/189], loss=83.1045
	step [102/189], loss=69.5286
	step [103/189], loss=68.5309
	step [104/189], loss=67.3671
	step [105/189], loss=78.7409
	step [106/189], loss=80.7443
	step [107/189], loss=82.3976
	step [108/189], loss=63.3262
	step [109/189], loss=87.1755
	step [110/189], loss=77.7895
	step [111/189], loss=73.5447
	step [112/189], loss=69.7365
	step [113/189], loss=80.8581
	step [114/189], loss=76.6555
	step [115/189], loss=84.4638
	step [116/189], loss=83.5276
	step [117/189], loss=73.0462
	step [118/189], loss=66.0407
	step [119/189], loss=79.7589
	step [120/189], loss=80.6923
	step [121/189], loss=85.4241
	step [122/189], loss=86.5984
	step [123/189], loss=77.2221
	step [124/189], loss=76.3438
	step [125/189], loss=85.4613
	step [126/189], loss=86.9061
	step [127/189], loss=91.1079
	step [128/189], loss=77.5582
	step [129/189], loss=89.7201
	step [130/189], loss=87.4060
	step [131/189], loss=88.4515
	step [132/189], loss=70.2782
	step [133/189], loss=60.4059
	step [134/189], loss=80.3887
	step [135/189], loss=65.3101
	step [136/189], loss=87.2829
	step [137/189], loss=84.6672
	step [138/189], loss=74.4519
	step [139/189], loss=76.4768
	step [140/189], loss=102.5107
	step [141/189], loss=68.2397
	step [142/189], loss=65.4454
	step [143/189], loss=62.0271
	step [144/189], loss=83.5710
	step [145/189], loss=72.8085
	step [146/189], loss=82.5415
	step [147/189], loss=76.6619
	step [148/189], loss=85.4185
	step [149/189], loss=63.0565
	step [150/189], loss=75.4083
	step [151/189], loss=65.7020
	step [152/189], loss=79.2106
	step [153/189], loss=89.9991
	step [154/189], loss=82.2369
	step [155/189], loss=75.2557
	step [156/189], loss=75.0538
	step [157/189], loss=71.4599
	step [158/189], loss=86.4760
	step [159/189], loss=73.6878
	step [160/189], loss=81.5387
	step [161/189], loss=82.2720
	step [162/189], loss=73.6627
	step [163/189], loss=81.0757
	step [164/189], loss=78.6755
	step [165/189], loss=68.8317
	step [166/189], loss=78.9358
	step [167/189], loss=75.7420
	step [168/189], loss=76.0594
	step [169/189], loss=76.4671
	step [170/189], loss=70.2408
	step [171/189], loss=77.4488
	step [172/189], loss=74.9307
	step [173/189], loss=82.9188
	step [174/189], loss=86.1876
	step [175/189], loss=80.4314
	step [176/189], loss=83.4436
	step [177/189], loss=91.7776
	step [178/189], loss=75.6717
	step [179/189], loss=92.7044
	step [180/189], loss=70.0188
	step [181/189], loss=74.8436
	step [182/189], loss=85.4546
	step [183/189], loss=77.2552
	step [184/189], loss=67.8398
	step [185/189], loss=66.4743
	step [186/189], loss=67.0982
	step [187/189], loss=68.7961
	step [188/189], loss=86.7302
	step [189/189], loss=34.2501
	Evaluating
	loss=0.0099, precision=0.3822, recall=0.8995, f1=0.5365
saving model as: 2_saved_model.pth
Training epoch 63
	step [1/189], loss=80.6166
	step [2/189], loss=68.1207
	step [3/189], loss=81.5237
	step [4/189], loss=55.0687
	step [5/189], loss=86.2876
	step [6/189], loss=68.1005
	step [7/189], loss=74.2412
	step [8/189], loss=77.8316
	step [9/189], loss=77.3677
	step [10/189], loss=77.4847
	step [11/189], loss=81.8606
	step [12/189], loss=78.9987
	step [13/189], loss=91.6419
	step [14/189], loss=68.7668
	step [15/189], loss=76.0895
	step [16/189], loss=68.5190
	step [17/189], loss=80.4077
	step [18/189], loss=77.0483
	step [19/189], loss=82.2856
	step [20/189], loss=89.0819
	step [21/189], loss=94.4437
	step [22/189], loss=63.6295
	step [23/189], loss=66.2047
	step [24/189], loss=74.0707
	step [25/189], loss=70.2927
	step [26/189], loss=73.2637
	step [27/189], loss=71.8850
	step [28/189], loss=80.5208
	step [29/189], loss=85.7973
	step [30/189], loss=81.0045
	step [31/189], loss=83.8919
	step [32/189], loss=64.7515
	step [33/189], loss=76.8362
	step [34/189], loss=86.7946
	step [35/189], loss=80.9663
	step [36/189], loss=77.4905
	step [37/189], loss=81.4124
	step [38/189], loss=82.6754
	step [39/189], loss=72.0547
	step [40/189], loss=81.2333
	step [41/189], loss=82.8377
	step [42/189], loss=69.2319
	step [43/189], loss=78.4559
	step [44/189], loss=79.6361
	step [45/189], loss=80.1079
	step [46/189], loss=66.2120
	step [47/189], loss=98.9809
	step [48/189], loss=69.8707
	step [49/189], loss=80.2002
	step [50/189], loss=69.7541
	step [51/189], loss=78.5156
	step [52/189], loss=82.0863
	step [53/189], loss=77.3155
	step [54/189], loss=81.2496
	step [55/189], loss=71.3413
	step [56/189], loss=74.3351
	step [57/189], loss=79.1245
	step [58/189], loss=85.2615
	step [59/189], loss=75.0800
	step [60/189], loss=88.1931
	step [61/189], loss=85.0116
	step [62/189], loss=85.6076
	step [63/189], loss=67.1336
	step [64/189], loss=70.1387
	step [65/189], loss=87.0388
	step [66/189], loss=86.7238
	step [67/189], loss=81.5950
	step [68/189], loss=85.1591
	step [69/189], loss=82.2576
	step [70/189], loss=88.0666
	step [71/189], loss=82.7365
	step [72/189], loss=77.3062
	step [73/189], loss=81.7234
	step [74/189], loss=79.9692
	step [75/189], loss=67.0236
	step [76/189], loss=76.3106
	step [77/189], loss=71.0323
	step [78/189], loss=81.0108
	step [79/189], loss=83.4873
	step [80/189], loss=88.1046
	step [81/189], loss=77.1426
	step [82/189], loss=76.7335
	step [83/189], loss=71.8031
	step [84/189], loss=80.7121
	step [85/189], loss=69.6874
	step [86/189], loss=71.2719
	step [87/189], loss=68.2474
	step [88/189], loss=71.5381
	step [89/189], loss=63.7025
	step [90/189], loss=72.1731
	step [91/189], loss=80.6225
	step [92/189], loss=77.2236
	step [93/189], loss=79.9110
	step [94/189], loss=72.0358
	step [95/189], loss=78.2029
	step [96/189], loss=71.2239
	step [97/189], loss=78.1323
	step [98/189], loss=79.8609
	step [99/189], loss=74.1007
	step [100/189], loss=72.8442
	step [101/189], loss=84.7312
	step [102/189], loss=76.6656
	step [103/189], loss=91.6394
	step [104/189], loss=60.7031
	step [105/189], loss=79.9127
	step [106/189], loss=63.2032
	step [107/189], loss=61.8562
	step [108/189], loss=67.2919
	step [109/189], loss=71.4998
	step [110/189], loss=72.1994
	step [111/189], loss=87.5518
	step [112/189], loss=74.0152
	step [113/189], loss=66.8388
	step [114/189], loss=98.6747
	step [115/189], loss=67.0256
	step [116/189], loss=81.1756
	step [117/189], loss=77.2249
	step [118/189], loss=61.3103
	step [119/189], loss=81.7767
	step [120/189], loss=83.2513
	step [121/189], loss=75.6106
	step [122/189], loss=74.4570
	step [123/189], loss=79.3959
	step [124/189], loss=70.6279
	step [125/189], loss=77.7714
	step [126/189], loss=75.5910
	step [127/189], loss=78.6208
	step [128/189], loss=80.8945
	step [129/189], loss=88.2762
	step [130/189], loss=68.5071
	step [131/189], loss=80.2999
	step [132/189], loss=81.5172
	step [133/189], loss=65.7334
	step [134/189], loss=91.3669
	step [135/189], loss=73.3119
	step [136/189], loss=62.5009
	step [137/189], loss=88.1337
	step [138/189], loss=75.0658
	step [139/189], loss=87.8130
	step [140/189], loss=76.3786
	step [141/189], loss=98.7135
	step [142/189], loss=69.5120
	step [143/189], loss=78.6185
	step [144/189], loss=70.1686
	step [145/189], loss=92.1008
	step [146/189], loss=80.7677
	step [147/189], loss=84.4810
	step [148/189], loss=86.0174
	step [149/189], loss=73.5235
	step [150/189], loss=89.4141
	step [151/189], loss=70.3893
	step [152/189], loss=82.3125
	step [153/189], loss=72.1086
	step [154/189], loss=74.8815
	step [155/189], loss=73.4353
	step [156/189], loss=73.0726
	step [157/189], loss=70.3818
	step [158/189], loss=68.4990
	step [159/189], loss=86.4420
	step [160/189], loss=67.8124
	step [161/189], loss=78.6063
	step [162/189], loss=78.8284
	step [163/189], loss=79.3678
	step [164/189], loss=90.5865
	step [165/189], loss=68.3142
	step [166/189], loss=64.0201
	step [167/189], loss=79.9663
	step [168/189], loss=60.9677
	step [169/189], loss=63.9474
	step [170/189], loss=79.4065
	step [171/189], loss=65.9222
	step [172/189], loss=76.0789
	step [173/189], loss=82.0825
	step [174/189], loss=74.3529
	step [175/189], loss=80.0544
	step [176/189], loss=81.1511
	step [177/189], loss=84.0403
	step [178/189], loss=82.9749
	step [179/189], loss=74.8851
	step [180/189], loss=68.7494
	step [181/189], loss=79.6217
	step [182/189], loss=86.9199
	step [183/189], loss=81.9132
	step [184/189], loss=72.4861
	step [185/189], loss=78.3554
	step [186/189], loss=82.2438
	step [187/189], loss=73.9853
	step [188/189], loss=73.0041
	step [189/189], loss=30.0622
	Evaluating
	loss=0.0113, precision=0.3158, recall=0.8928, f1=0.4666
Training epoch 64
	step [1/189], loss=71.9614
	step [2/189], loss=85.1353
	step [3/189], loss=83.3485
	step [4/189], loss=85.0133
	step [5/189], loss=61.8530
	step [6/189], loss=92.2771
	step [7/189], loss=68.9302
	step [8/189], loss=76.5957
	step [9/189], loss=85.5270
	step [10/189], loss=85.7238
	step [11/189], loss=89.2737
	step [12/189], loss=78.3646
	step [13/189], loss=72.6217
	step [14/189], loss=80.5312
	step [15/189], loss=88.9215
	step [16/189], loss=70.6650
	step [17/189], loss=90.5156
	step [18/189], loss=81.5976
	step [19/189], loss=77.9429
	step [20/189], loss=76.8189
	step [21/189], loss=61.8227
	step [22/189], loss=67.3675
	step [23/189], loss=71.7627
	step [24/189], loss=75.1721
	step [25/189], loss=64.7902
	step [26/189], loss=78.8177
	step [27/189], loss=72.3385
	step [28/189], loss=78.0457
	step [29/189], loss=67.7271
	step [30/189], loss=73.1753
	step [31/189], loss=69.4252
	step [32/189], loss=67.1180
	step [33/189], loss=64.5618
	step [34/189], loss=75.6503
	step [35/189], loss=73.3831
	step [36/189], loss=74.5587
	step [37/189], loss=75.4935
	step [38/189], loss=85.0682
	step [39/189], loss=81.1196
	step [40/189], loss=78.9651
	step [41/189], loss=71.5261
	step [42/189], loss=72.8890
	step [43/189], loss=78.2363
	step [44/189], loss=80.2883
	step [45/189], loss=78.4486
	step [46/189], loss=75.6774
	step [47/189], loss=72.6333
	step [48/189], loss=73.6338
	step [49/189], loss=81.7746
	step [50/189], loss=70.5141
	step [51/189], loss=81.0192
	step [52/189], loss=55.5204
	step [53/189], loss=78.1947
	step [54/189], loss=74.2363
	step [55/189], loss=76.5972
	step [56/189], loss=82.5268
	step [57/189], loss=75.9755
	step [58/189], loss=69.2328
	step [59/189], loss=82.2593
	step [60/189], loss=79.1052
	step [61/189], loss=86.7627
	step [62/189], loss=72.2901
	step [63/189], loss=75.3476
	step [64/189], loss=81.3676
	step [65/189], loss=73.4746
	step [66/189], loss=77.3897
	step [67/189], loss=82.6673
	step [68/189], loss=83.2946
	step [69/189], loss=89.9955
	step [70/189], loss=70.8065
	step [71/189], loss=73.3193
	step [72/189], loss=91.6986
	step [73/189], loss=72.6884
	step [74/189], loss=72.3372
	step [75/189], loss=80.2516
	step [76/189], loss=78.1864
	step [77/189], loss=79.1621
	step [78/189], loss=71.0992
	step [79/189], loss=95.5022
	step [80/189], loss=63.9167
	step [81/189], loss=77.0909
	step [82/189], loss=74.5498
	step [83/189], loss=78.6681
	step [84/189], loss=82.8095
	step [85/189], loss=83.9418
	step [86/189], loss=76.4377
	step [87/189], loss=84.0245
	step [88/189], loss=73.3619
	step [89/189], loss=76.5990
	step [90/189], loss=80.9997
	step [91/189], loss=84.8761
	step [92/189], loss=80.6556
	step [93/189], loss=82.0728
	step [94/189], loss=72.1140
	step [95/189], loss=88.4883
	step [96/189], loss=80.9254
	step [97/189], loss=69.9567
	step [98/189], loss=64.2010
	step [99/189], loss=96.3779
	step [100/189], loss=71.8500
	step [101/189], loss=66.1893
	step [102/189], loss=79.3222
	step [103/189], loss=81.2759
	step [104/189], loss=73.7410
	step [105/189], loss=86.4282
	step [106/189], loss=77.6910
	step [107/189], loss=81.1180
	step [108/189], loss=72.2673
	step [109/189], loss=74.5709
	step [110/189], loss=83.9061
	step [111/189], loss=85.6549
	step [112/189], loss=76.9305
	step [113/189], loss=81.2522
	step [114/189], loss=76.4060
	step [115/189], loss=68.7543
	step [116/189], loss=80.6824
	step [117/189], loss=70.4776
	step [118/189], loss=75.2002
	step [119/189], loss=89.7005
	step [120/189], loss=77.6256
	step [121/189], loss=67.9393
	step [122/189], loss=90.9275
	step [123/189], loss=73.7159
	step [124/189], loss=84.3321
	step [125/189], loss=63.1486
	step [126/189], loss=90.6890
	step [127/189], loss=75.5117
	step [128/189], loss=75.4175
	step [129/189], loss=68.1289
	step [130/189], loss=83.6978
	step [131/189], loss=72.7700
	step [132/189], loss=81.9501
	step [133/189], loss=74.4099
	step [134/189], loss=69.1173
	step [135/189], loss=73.5749
	step [136/189], loss=79.9888
	step [137/189], loss=81.2510
	step [138/189], loss=78.2941
	step [139/189], loss=77.5106
	step [140/189], loss=76.2516
	step [141/189], loss=79.6220
	step [142/189], loss=74.2166
	step [143/189], loss=79.0412
	step [144/189], loss=79.1094
	step [145/189], loss=66.1132
	step [146/189], loss=70.6208
	step [147/189], loss=83.3330
	step [148/189], loss=69.9742
	step [149/189], loss=60.7271
	step [150/189], loss=70.2852
	step [151/189], loss=70.8468
	step [152/189], loss=97.2815
	step [153/189], loss=66.9677
	step [154/189], loss=77.6123
	step [155/189], loss=84.0631
	step [156/189], loss=77.8562
	step [157/189], loss=73.6509
	step [158/189], loss=100.8304
	step [159/189], loss=69.2138
	step [160/189], loss=89.2825
	step [161/189], loss=76.1995
	step [162/189], loss=71.9863
	step [163/189], loss=83.3054
	step [164/189], loss=88.9051
	step [165/189], loss=83.9868
	step [166/189], loss=75.8465
	step [167/189], loss=60.5292
	step [168/189], loss=65.8607
	step [169/189], loss=62.5197
	step [170/189], loss=73.3491
	step [171/189], loss=73.0296
	step [172/189], loss=69.6773
	step [173/189], loss=93.8675
	step [174/189], loss=71.2950
	step [175/189], loss=94.3978
	step [176/189], loss=86.1535
	step [177/189], loss=68.1052
	step [178/189], loss=72.3482
	step [179/189], loss=67.0180
	step [180/189], loss=80.7006
	step [181/189], loss=77.9040
	step [182/189], loss=89.8401
	step [183/189], loss=71.9175
	step [184/189], loss=74.7644
	step [185/189], loss=71.1022
	step [186/189], loss=76.4537
	step [187/189], loss=56.5302
	step [188/189], loss=82.1618
	step [189/189], loss=22.0286
	Evaluating
	loss=0.0121, precision=0.3153, recall=0.8895, f1=0.4656
Training epoch 65
	step [1/189], loss=78.4728
	step [2/189], loss=71.3157
	step [3/189], loss=89.7350
	step [4/189], loss=75.0596
	step [5/189], loss=84.3837
	step [6/189], loss=63.5775
	step [7/189], loss=85.7233
	step [8/189], loss=77.5224
	step [9/189], loss=71.9556
	step [10/189], loss=77.3001
	step [11/189], loss=72.6630
	step [12/189], loss=90.1607
	step [13/189], loss=71.9412
	step [14/189], loss=71.0772
	step [15/189], loss=68.6653
	step [16/189], loss=79.4953
	step [17/189], loss=72.2583
	step [18/189], loss=70.9438
	step [19/189], loss=70.4938
	step [20/189], loss=66.4323
	step [21/189], loss=75.9129
	step [22/189], loss=74.4533
	step [23/189], loss=76.5414
	step [24/189], loss=86.6510
	step [25/189], loss=82.6939
	step [26/189], loss=84.0473
	step [27/189], loss=71.4297
	step [28/189], loss=63.8524
	step [29/189], loss=67.8169
	step [30/189], loss=84.4033
	step [31/189], loss=69.0859
	step [32/189], loss=81.0743
	step [33/189], loss=84.8199
	step [34/189], loss=72.8512
	step [35/189], loss=84.7743
	step [36/189], loss=73.8100
	step [37/189], loss=76.8555
	step [38/189], loss=61.9806
	step [39/189], loss=78.2619
	step [40/189], loss=78.7456
	step [41/189], loss=69.8280
	step [42/189], loss=81.9656
	step [43/189], loss=71.9520
	step [44/189], loss=79.2469
	step [45/189], loss=84.5700
	step [46/189], loss=74.5002
	step [47/189], loss=82.0974
	step [48/189], loss=78.7359
	step [49/189], loss=59.9891
	step [50/189], loss=80.2523
	step [51/189], loss=75.0212
	step [52/189], loss=78.6059
	step [53/189], loss=71.8318
	step [54/189], loss=70.4019
	step [55/189], loss=73.6134
	step [56/189], loss=77.2184
	step [57/189], loss=96.0914
	step [58/189], loss=78.5163
	step [59/189], loss=89.0558
	step [60/189], loss=83.0844
	step [61/189], loss=69.2465
	step [62/189], loss=80.1016
	step [63/189], loss=70.4224
	step [64/189], loss=75.3537
	step [65/189], loss=81.1152
	step [66/189], loss=85.9855
	step [67/189], loss=72.3391
	step [68/189], loss=82.1397
	step [69/189], loss=69.5591
	step [70/189], loss=61.3046
	step [71/189], loss=68.5075
	step [72/189], loss=82.4901
	step [73/189], loss=73.3586
	step [74/189], loss=74.6499
	step [75/189], loss=76.7847
	step [76/189], loss=78.7867
	step [77/189], loss=79.8009
	step [78/189], loss=84.3650
	step [79/189], loss=85.9319
	step [80/189], loss=81.0740
	step [81/189], loss=92.9548
	step [82/189], loss=70.7759
	step [83/189], loss=71.9535
	step [84/189], loss=87.7728
	step [85/189], loss=67.0470
	step [86/189], loss=67.2020
	step [87/189], loss=68.6191
	step [88/189], loss=82.8361
	step [89/189], loss=65.6207
	step [90/189], loss=94.7868
	step [91/189], loss=73.5901
	step [92/189], loss=74.6206
	step [93/189], loss=71.1320
	step [94/189], loss=74.1156
	step [95/189], loss=79.6968
	step [96/189], loss=74.9588
	step [97/189], loss=91.8667
	step [98/189], loss=58.5825
	step [99/189], loss=67.3186
	step [100/189], loss=61.7720
	step [101/189], loss=89.0764
	step [102/189], loss=73.8309
	step [103/189], loss=67.3264
	step [104/189], loss=71.2520
	step [105/189], loss=79.6651
	step [106/189], loss=71.4957
	step [107/189], loss=83.9446
	step [108/189], loss=78.9724
	step [109/189], loss=81.4458
	step [110/189], loss=79.7893
	step [111/189], loss=79.2431
	step [112/189], loss=84.6034
	step [113/189], loss=73.8342
	step [114/189], loss=80.5007
	step [115/189], loss=66.4386
	step [116/189], loss=86.5162
	step [117/189], loss=64.7196
	step [118/189], loss=79.2654
	step [119/189], loss=72.7918
	step [120/189], loss=88.8790
	step [121/189], loss=69.7709
	step [122/189], loss=78.3686
	step [123/189], loss=78.6572
	step [124/189], loss=89.6833
	step [125/189], loss=84.0156
	step [126/189], loss=73.8376
	step [127/189], loss=78.0141
	step [128/189], loss=65.5822
	step [129/189], loss=73.7826
	step [130/189], loss=66.4135
	step [131/189], loss=72.8021
	step [132/189], loss=80.0509
	step [133/189], loss=73.1189
	step [134/189], loss=79.4407
	step [135/189], loss=74.1950
	step [136/189], loss=87.4192
	step [137/189], loss=64.9511
	step [138/189], loss=84.0102
	step [139/189], loss=88.8784
	step [140/189], loss=68.4656
	step [141/189], loss=81.8077
	step [142/189], loss=78.0169
	step [143/189], loss=80.6333
	step [144/189], loss=77.0564
	step [145/189], loss=70.4943
	step [146/189], loss=82.1442
	step [147/189], loss=86.3108
	step [148/189], loss=71.4494
	step [149/189], loss=80.0921
	step [150/189], loss=89.8531
	step [151/189], loss=75.3547
	step [152/189], loss=82.3565
	step [153/189], loss=69.4368
	step [154/189], loss=74.1330
	step [155/189], loss=77.2986
	step [156/189], loss=74.6272
	step [157/189], loss=82.4987
	step [158/189], loss=80.1772
	step [159/189], loss=76.2866
	step [160/189], loss=82.5633
	step [161/189], loss=78.5872
	step [162/189], loss=78.9483
	step [163/189], loss=79.6141
	step [164/189], loss=73.3722
	step [165/189], loss=70.4124
	step [166/189], loss=88.8257
	step [167/189], loss=101.6099
	step [168/189], loss=78.9990
	step [169/189], loss=76.1832
	step [170/189], loss=70.4510
	step [171/189], loss=82.7677
	step [172/189], loss=64.3337
	step [173/189], loss=71.6060
	step [174/189], loss=88.3303
	step [175/189], loss=83.5493
	step [176/189], loss=73.8244
	step [177/189], loss=62.2333
	step [178/189], loss=67.7371
	step [179/189], loss=88.6375
	step [180/189], loss=74.1365
	step [181/189], loss=76.2654
	step [182/189], loss=64.8404
	step [183/189], loss=87.3906
	step [184/189], loss=93.2320
	step [185/189], loss=67.6773
	step [186/189], loss=70.6078
	step [187/189], loss=85.4532
	step [188/189], loss=77.8807
	step [189/189], loss=32.6292
	Evaluating
	loss=0.0108, precision=0.3473, recall=0.8990, f1=0.5011
Training epoch 66
	step [1/189], loss=80.3247
	step [2/189], loss=86.3590
	step [3/189], loss=75.7772
	step [4/189], loss=70.0442
	step [5/189], loss=77.1913
	step [6/189], loss=72.6882
	step [7/189], loss=90.4250
	step [8/189], loss=70.1756
	step [9/189], loss=80.0701
	step [10/189], loss=72.1711
	step [11/189], loss=71.6647
	step [12/189], loss=64.7036
	step [13/189], loss=69.9240
	step [14/189], loss=75.4094
	step [15/189], loss=69.7412
	step [16/189], loss=75.1791
	step [17/189], loss=79.1693
	step [18/189], loss=85.8753
	step [19/189], loss=75.7101
	step [20/189], loss=68.4496
	step [21/189], loss=80.9783
	step [22/189], loss=67.9135
	step [23/189], loss=72.4097
	step [24/189], loss=66.8386
	step [25/189], loss=70.7183
	step [26/189], loss=64.8562
	step [27/189], loss=71.2794
	step [28/189], loss=91.7284
	step [29/189], loss=79.0987
	step [30/189], loss=84.9285
	step [31/189], loss=75.7456
	step [32/189], loss=66.7602
	step [33/189], loss=77.1082
	step [34/189], loss=83.8392
	step [35/189], loss=77.7882
	step [36/189], loss=62.6320
	step [37/189], loss=83.4944
	step [38/189], loss=91.6569
	step [39/189], loss=72.4437
	step [40/189], loss=83.3758
	step [41/189], loss=83.9568
	step [42/189], loss=83.4527
	step [43/189], loss=80.6985
	step [44/189], loss=83.9808
	step [45/189], loss=73.2788
	step [46/189], loss=67.6559
	step [47/189], loss=70.4697
	step [48/189], loss=84.9300
	step [49/189], loss=80.8924
	step [50/189], loss=80.9480
	step [51/189], loss=90.6415
	step [52/189], loss=88.3185
	step [53/189], loss=66.8490
	step [54/189], loss=80.0972
	step [55/189], loss=78.7008
	step [56/189], loss=68.9061
	step [57/189], loss=69.1241
	step [58/189], loss=104.3825
	step [59/189], loss=81.3436
	step [60/189], loss=79.5681
	step [61/189], loss=70.5601
	step [62/189], loss=71.6678
	step [63/189], loss=81.4627
	step [64/189], loss=58.8638
	step [65/189], loss=76.8721
	step [66/189], loss=88.1237
	step [67/189], loss=75.6220
	step [68/189], loss=59.3984
	step [69/189], loss=86.4465
	step [70/189], loss=75.8044
	step [71/189], loss=75.3289
	step [72/189], loss=55.6678
	step [73/189], loss=89.7148
	step [74/189], loss=67.5764
	step [75/189], loss=79.0672
	step [76/189], loss=73.6143
	step [77/189], loss=74.9175
	step [78/189], loss=83.4606
	step [79/189], loss=56.5929
	step [80/189], loss=70.0570
	step [81/189], loss=70.2195
	step [82/189], loss=63.7497
	step [83/189], loss=76.7178
	step [84/189], loss=72.8426
	step [85/189], loss=70.6331
	step [86/189], loss=66.9667
	step [87/189], loss=76.4754
	step [88/189], loss=70.7638
	step [89/189], loss=78.2185
	step [90/189], loss=77.0174
	step [91/189], loss=72.9017
	step [92/189], loss=75.8620
	step [93/189], loss=78.3686
	step [94/189], loss=74.4121
	step [95/189], loss=75.0283
	step [96/189], loss=69.9796
	step [97/189], loss=86.7709
	step [98/189], loss=76.1739
	step [99/189], loss=76.6187
	step [100/189], loss=70.6149
	step [101/189], loss=76.2892
	step [102/189], loss=91.0918
	step [103/189], loss=82.2651
	step [104/189], loss=68.6638
	step [105/189], loss=80.5624
	step [106/189], loss=69.8169
	step [107/189], loss=69.8176
	step [108/189], loss=87.8175
	step [109/189], loss=59.0262
	step [110/189], loss=82.7983
	step [111/189], loss=72.5436
	step [112/189], loss=68.7018
	step [113/189], loss=83.0910
	step [114/189], loss=69.6802
	step [115/189], loss=70.5879
	step [116/189], loss=75.4597
	step [117/189], loss=80.3659
	step [118/189], loss=72.6476
	step [119/189], loss=87.5954
	step [120/189], loss=83.0846
	step [121/189], loss=69.5043
	step [122/189], loss=91.5501
	step [123/189], loss=88.1229
	step [124/189], loss=72.1676
	step [125/189], loss=93.7411
	step [126/189], loss=90.8875
	step [127/189], loss=77.0028
	step [128/189], loss=70.1651
	step [129/189], loss=79.8869
	step [130/189], loss=72.0223
	step [131/189], loss=78.4582
	step [132/189], loss=86.5182
	step [133/189], loss=77.0719
	step [134/189], loss=63.3193
	step [135/189], loss=60.3853
	step [136/189], loss=57.4425
	step [137/189], loss=80.3845
	step [138/189], loss=71.9550
	step [139/189], loss=80.6815
	step [140/189], loss=74.5446
	step [141/189], loss=77.0983
	step [142/189], loss=74.7952
	step [143/189], loss=68.5989
	step [144/189], loss=80.2625
	step [145/189], loss=78.3006
	step [146/189], loss=78.6073
	step [147/189], loss=76.3168
	step [148/189], loss=90.5164
	step [149/189], loss=74.7608
	step [150/189], loss=89.0119
	step [151/189], loss=86.1920
	step [152/189], loss=70.5543
	step [153/189], loss=81.2957
	step [154/189], loss=86.0320
	step [155/189], loss=81.2832
	step [156/189], loss=80.8759
	step [157/189], loss=81.6945
	step [158/189], loss=90.8350
	step [159/189], loss=78.5123
	step [160/189], loss=90.9052
	step [161/189], loss=77.0755
	step [162/189], loss=77.8905
	step [163/189], loss=69.7181
	step [164/189], loss=83.6083
	step [165/189], loss=80.3769
	step [166/189], loss=81.8496
	step [167/189], loss=72.3613
	step [168/189], loss=70.4580
	step [169/189], loss=75.9469
	step [170/189], loss=73.7726
	step [171/189], loss=84.3226
	step [172/189], loss=69.7802
	step [173/189], loss=72.9061
	step [174/189], loss=78.8289
	step [175/189], loss=72.2849
	step [176/189], loss=76.6017
	step [177/189], loss=63.2030
	step [178/189], loss=82.3555
	step [179/189], loss=74.1844
	step [180/189], loss=79.7505
	step [181/189], loss=74.2007
	step [182/189], loss=81.7253
	step [183/189], loss=77.5456
	step [184/189], loss=80.0887
	step [185/189], loss=86.8534
	step [186/189], loss=89.2956
	step [187/189], loss=82.8508
	step [188/189], loss=70.3051
	step [189/189], loss=22.2776
	Evaluating
	loss=0.0112, precision=0.3315, recall=0.8958, f1=0.4839
Training epoch 67
	step [1/189], loss=96.2542
	step [2/189], loss=70.8508
	step [3/189], loss=81.2652
	step [4/189], loss=93.0980
	step [5/189], loss=78.0551
	step [6/189], loss=78.6479
	step [7/189], loss=85.4396
	step [8/189], loss=75.0488
	step [9/189], loss=73.4847
	step [10/189], loss=84.5681
	step [11/189], loss=70.4126
	step [12/189], loss=58.8670
	step [13/189], loss=79.6601
	step [14/189], loss=73.4790
	step [15/189], loss=73.7397
	step [16/189], loss=80.7666
	step [17/189], loss=76.0585
	step [18/189], loss=75.8966
	step [19/189], loss=78.6114
	step [20/189], loss=87.3574
	step [21/189], loss=83.8584
	step [22/189], loss=71.4113
	step [23/189], loss=77.1375
	step [24/189], loss=75.0772
	step [25/189], loss=76.8938
	step [26/189], loss=87.3785
	step [27/189], loss=82.6713
	step [28/189], loss=71.6171
	step [29/189], loss=65.6151
	step [30/189], loss=85.0920
	step [31/189], loss=76.5177
	step [32/189], loss=75.2841
	step [33/189], loss=72.3735
	step [34/189], loss=77.6168
	step [35/189], loss=79.4861
	step [36/189], loss=85.8045
	step [37/189], loss=65.8112
	step [38/189], loss=68.4582
	step [39/189], loss=77.9469
	step [40/189], loss=82.2682
	step [41/189], loss=84.7742
	step [42/189], loss=78.0323
	step [43/189], loss=91.0278
	step [44/189], loss=80.6026
	step [45/189], loss=89.0093
	step [46/189], loss=86.2344
	step [47/189], loss=78.5361
	step [48/189], loss=70.0878
	step [49/189], loss=94.6749
	step [50/189], loss=61.4363
	step [51/189], loss=71.6960
	step [52/189], loss=71.2246
	step [53/189], loss=67.0453
	step [54/189], loss=93.8593
	step [55/189], loss=91.2112
	step [56/189], loss=52.5605
	step [57/189], loss=86.7964
	step [58/189], loss=72.8298
	step [59/189], loss=64.7491
	step [60/189], loss=71.5282
	step [61/189], loss=57.3135
	step [62/189], loss=72.3194
	step [63/189], loss=71.4502
	step [64/189], loss=71.9951
	step [65/189], loss=69.3001
	step [66/189], loss=74.8051
	step [67/189], loss=74.1764
	step [68/189], loss=81.7519
	step [69/189], loss=74.0879
	step [70/189], loss=71.9892
	step [71/189], loss=74.4563
	step [72/189], loss=79.3135
	step [73/189], loss=72.6825
	step [74/189], loss=74.7048
	step [75/189], loss=81.0825
	step [76/189], loss=73.9747
	step [77/189], loss=74.0249
	step [78/189], loss=81.0165
	step [79/189], loss=66.9128
	step [80/189], loss=66.5580
	step [81/189], loss=83.1555
	step [82/189], loss=71.3168
	step [83/189], loss=89.3228
	step [84/189], loss=91.4366
	step [85/189], loss=80.5360
	step [86/189], loss=76.3533
	step [87/189], loss=63.9237
	step [88/189], loss=78.0618
	step [89/189], loss=77.1712
	step [90/189], loss=68.3222
	step [91/189], loss=66.7874
	step [92/189], loss=84.5884
	step [93/189], loss=78.6306
	step [94/189], loss=86.5664
	step [95/189], loss=71.3033
	step [96/189], loss=77.4357
	step [97/189], loss=74.6959
	step [98/189], loss=77.1165
	step [99/189], loss=74.5759
	step [100/189], loss=70.0128
	step [101/189], loss=90.0245
	step [102/189], loss=83.3258
	step [103/189], loss=68.7831
	step [104/189], loss=67.3540
	step [105/189], loss=68.9051
	step [106/189], loss=87.2867
	step [107/189], loss=76.7775
	step [108/189], loss=65.9313
	step [109/189], loss=75.2268
	step [110/189], loss=81.1779
	step [111/189], loss=88.7636
	step [112/189], loss=62.5114
	step [113/189], loss=82.7121
	step [114/189], loss=67.5169
	step [115/189], loss=70.5604
	step [116/189], loss=79.8580
	step [117/189], loss=77.2685
	step [118/189], loss=64.6107
	step [119/189], loss=86.1019
	step [120/189], loss=73.1337
	step [121/189], loss=61.1386
	step [122/189], loss=88.4197
	step [123/189], loss=71.6061
	step [124/189], loss=76.4765
	step [125/189], loss=86.9992
	step [126/189], loss=73.5599
	step [127/189], loss=84.1978
	step [128/189], loss=82.4001
	step [129/189], loss=83.2451
	step [130/189], loss=77.5627
	step [131/189], loss=70.7507
	step [132/189], loss=67.7681
	step [133/189], loss=85.7042
	step [134/189], loss=75.1435
	step [135/189], loss=83.5487
	step [136/189], loss=72.5848
	step [137/189], loss=80.1546
	step [138/189], loss=86.7718
	step [139/189], loss=66.6672
	step [140/189], loss=85.2134
	step [141/189], loss=73.9410
	step [142/189], loss=75.8671
	step [143/189], loss=68.1774
	step [144/189], loss=88.4555
	step [145/189], loss=67.0994
	step [146/189], loss=87.3650
	step [147/189], loss=80.0566
	step [148/189], loss=77.4745
	step [149/189], loss=66.2597
	step [150/189], loss=72.9398
	step [151/189], loss=87.8299
	step [152/189], loss=70.5013
	step [153/189], loss=74.9027
	step [154/189], loss=83.5946
	step [155/189], loss=71.0702
	step [156/189], loss=86.7694
	step [157/189], loss=66.5326
	step [158/189], loss=67.4552
	step [159/189], loss=78.6610
	step [160/189], loss=77.9952
	step [161/189], loss=64.6322
	step [162/189], loss=68.7562
	step [163/189], loss=75.6802
	step [164/189], loss=77.3133
	step [165/189], loss=89.4210
	step [166/189], loss=65.0604
	step [167/189], loss=86.0543
	step [168/189], loss=80.4655
	step [169/189], loss=66.0370
	step [170/189], loss=79.8201
	step [171/189], loss=69.9268
	step [172/189], loss=81.5766
	step [173/189], loss=66.0095
	step [174/189], loss=78.3444
	step [175/189], loss=73.5850
	step [176/189], loss=77.0845
	step [177/189], loss=78.0341
	step [178/189], loss=68.6085
	step [179/189], loss=80.7150
	step [180/189], loss=69.2491
	step [181/189], loss=70.9393
	step [182/189], loss=84.3506
	step [183/189], loss=71.2498
	step [184/189], loss=74.3110
	step [185/189], loss=64.5788
	step [186/189], loss=76.8371
	step [187/189], loss=73.8277
	step [188/189], loss=76.2984
	step [189/189], loss=35.1466
	Evaluating
	loss=0.0105, precision=0.3437, recall=0.8862, f1=0.4953
Training epoch 68
	step [1/189], loss=80.9160
	step [2/189], loss=81.8742
	step [3/189], loss=72.4124
	step [4/189], loss=74.6878
	step [5/189], loss=77.7801
	step [6/189], loss=85.4177
	step [7/189], loss=84.0069
	step [8/189], loss=78.1833
	step [9/189], loss=67.2416
	step [10/189], loss=69.0645
	step [11/189], loss=66.0416
	step [12/189], loss=84.7259
	step [13/189], loss=74.0179
	step [14/189], loss=62.2056
	step [15/189], loss=64.3323
	step [16/189], loss=85.6333
	step [17/189], loss=72.7845
	step [18/189], loss=85.6745
	step [19/189], loss=84.7703
	step [20/189], loss=75.5159
	step [21/189], loss=72.4621
	step [22/189], loss=71.2133
	step [23/189], loss=62.4513
	step [24/189], loss=75.6701
	step [25/189], loss=74.2471
	step [26/189], loss=87.1733
	step [27/189], loss=67.8482
	step [28/189], loss=66.9628
	step [29/189], loss=70.9727
	step [30/189], loss=74.4149
	step [31/189], loss=96.5843
	step [32/189], loss=72.4238
	step [33/189], loss=59.2034
	step [34/189], loss=67.8735
	step [35/189], loss=72.6467
	step [36/189], loss=63.5611
	step [37/189], loss=81.4753
	step [38/189], loss=64.3021
	step [39/189], loss=78.0545
	step [40/189], loss=70.3306
	step [41/189], loss=70.0621
	step [42/189], loss=68.1352
	step [43/189], loss=62.8444
	step [44/189], loss=73.1876
	step [45/189], loss=89.1104
	step [46/189], loss=66.8966
	step [47/189], loss=84.6724
	step [48/189], loss=92.3269
	step [49/189], loss=82.1237
	step [50/189], loss=74.6442
	step [51/189], loss=81.0296
	step [52/189], loss=92.7379
	step [53/189], loss=78.9173
	step [54/189], loss=65.5634
	step [55/189], loss=63.8572
	step [56/189], loss=91.9084
	step [57/189], loss=78.6102
	step [58/189], loss=74.2748
	step [59/189], loss=72.7168
	step [60/189], loss=61.4505
	step [61/189], loss=72.0864
	step [62/189], loss=73.1220
	step [63/189], loss=74.3657
	step [64/189], loss=73.9224
	step [65/189], loss=65.4660
	step [66/189], loss=80.9277
	step [67/189], loss=69.5229
	step [68/189], loss=78.5162
	step [69/189], loss=74.8853
	step [70/189], loss=80.5411
	step [71/189], loss=59.9411
	step [72/189], loss=80.9957
	step [73/189], loss=80.5784
	step [74/189], loss=72.0786
	step [75/189], loss=63.4868
	step [76/189], loss=64.7952
	step [77/189], loss=90.2125
	step [78/189], loss=75.5784
	step [79/189], loss=71.8913
	step [80/189], loss=73.5991
	step [81/189], loss=80.8715
	step [82/189], loss=63.3850
	step [83/189], loss=83.8197
	step [84/189], loss=84.5542
	step [85/189], loss=88.3265
	step [86/189], loss=84.1956
	step [87/189], loss=78.5659
	step [88/189], loss=80.3976
	step [89/189], loss=70.6008
	step [90/189], loss=86.0125
	step [91/189], loss=82.2218
	step [92/189], loss=69.9165
	step [93/189], loss=74.9364
	step [94/189], loss=80.3078
	step [95/189], loss=69.7807
	step [96/189], loss=75.5506
	step [97/189], loss=78.2321
	step [98/189], loss=82.8738
	step [99/189], loss=81.4636
	step [100/189], loss=77.4185
	step [101/189], loss=79.0634
	step [102/189], loss=70.7400
	step [103/189], loss=79.6455
	step [104/189], loss=76.7756
	step [105/189], loss=86.4100
	step [106/189], loss=66.2714
	step [107/189], loss=82.2447
	step [108/189], loss=79.2760
	step [109/189], loss=69.1822
	step [110/189], loss=59.9556
	step [111/189], loss=81.3473
	step [112/189], loss=78.7980
	step [113/189], loss=72.8135
	step [114/189], loss=72.1359
	step [115/189], loss=65.0837
	step [116/189], loss=81.2366
	step [117/189], loss=79.2732
	step [118/189], loss=70.0963
	step [119/189], loss=75.1336
	step [120/189], loss=82.4588
	step [121/189], loss=69.7697
	step [122/189], loss=73.4520
	step [123/189], loss=80.9410
	step [124/189], loss=91.5572
	step [125/189], loss=81.7579
	step [126/189], loss=70.5660
	step [127/189], loss=68.9929
	step [128/189], loss=76.8301
	step [129/189], loss=81.5667
	step [130/189], loss=62.2181
	step [131/189], loss=89.8695
	step [132/189], loss=84.1691
	step [133/189], loss=68.0790
	step [134/189], loss=85.8140
	step [135/189], loss=87.2490
	step [136/189], loss=87.2588
	step [137/189], loss=94.7953
	step [138/189], loss=74.5511
	step [139/189], loss=71.0690
	step [140/189], loss=83.4737
	step [141/189], loss=82.1860
	step [142/189], loss=77.0430
	step [143/189], loss=67.2703
	step [144/189], loss=67.8675
	step [145/189], loss=75.4273
	step [146/189], loss=80.4295
	step [147/189], loss=83.2959
	step [148/189], loss=71.8156
	step [149/189], loss=71.8963
	step [150/189], loss=71.3462
	step [151/189], loss=83.3450
	step [152/189], loss=74.5382
	step [153/189], loss=72.0979
	step [154/189], loss=77.7628
	step [155/189], loss=85.4190
	step [156/189], loss=76.0303
	step [157/189], loss=79.3336
	step [158/189], loss=78.0551
	step [159/189], loss=78.4087
	step [160/189], loss=72.3960
	step [161/189], loss=75.6278
	step [162/189], loss=63.8451
	step [163/189], loss=87.7822
	step [164/189], loss=69.7413
	step [165/189], loss=59.9252
	step [166/189], loss=83.8599
	step [167/189], loss=70.4398
	step [168/189], loss=68.0495
	step [169/189], loss=77.7102
	step [170/189], loss=72.1058
	step [171/189], loss=62.1034
	step [172/189], loss=69.2679
	step [173/189], loss=88.6633
	step [174/189], loss=63.8475
	step [175/189], loss=80.3937
	step [176/189], loss=74.8865
	step [177/189], loss=77.6599
	step [178/189], loss=78.8626
	step [179/189], loss=74.0510
	step [180/189], loss=82.5322
	step [181/189], loss=84.8306
	step [182/189], loss=93.8773
	step [183/189], loss=63.9133
	step [184/189], loss=71.2620
	step [185/189], loss=80.6860
	step [186/189], loss=84.1899
	step [187/189], loss=90.6664
	step [188/189], loss=87.3078
	step [189/189], loss=22.9615
	Evaluating
	loss=0.0089, precision=0.3855, recall=0.8845, f1=0.5369
saving model as: 2_saved_model.pth
Training epoch 69
	step [1/189], loss=84.4518
	step [2/189], loss=74.0140
	step [3/189], loss=75.4047
	step [4/189], loss=75.2767
	step [5/189], loss=77.7586
	step [6/189], loss=73.4241
	step [7/189], loss=62.5437
	step [8/189], loss=84.6941
	step [9/189], loss=75.9031
	step [10/189], loss=73.2031
	step [11/189], loss=80.4109
	step [12/189], loss=80.0461
	step [13/189], loss=65.7689
	step [14/189], loss=75.1500
	step [15/189], loss=97.3495
	step [16/189], loss=87.8351
	step [17/189], loss=83.6336
	step [18/189], loss=81.4303
	step [19/189], loss=84.6810
	step [20/189], loss=67.3326
	step [21/189], loss=64.0462
	step [22/189], loss=70.0570
	step [23/189], loss=88.1277
	step [24/189], loss=76.0885
	step [25/189], loss=71.4858
	step [26/189], loss=87.1804
	step [27/189], loss=77.4592
	step [28/189], loss=68.5782
	step [29/189], loss=75.5004
	step [30/189], loss=90.8784
	step [31/189], loss=76.5649
	step [32/189], loss=71.8092
	step [33/189], loss=82.8895
	step [34/189], loss=71.3297
	step [35/189], loss=82.0227
	step [36/189], loss=68.6294
	step [37/189], loss=91.5983
	step [38/189], loss=75.1208
	step [39/189], loss=82.3289
	step [40/189], loss=78.6258
	step [41/189], loss=69.6408
	step [42/189], loss=67.6436
	step [43/189], loss=67.0611
	step [44/189], loss=72.0426
	step [45/189], loss=74.9388
	step [46/189], loss=78.7311
	step [47/189], loss=70.7160
	step [48/189], loss=79.7852
	step [49/189], loss=63.4168
	step [50/189], loss=69.9850
	step [51/189], loss=79.5968
	step [52/189], loss=85.4414
	step [53/189], loss=86.1395
	step [54/189], loss=76.7137
	step [55/189], loss=81.2512
	step [56/189], loss=81.2827
	step [57/189], loss=75.5738
	step [58/189], loss=71.1769
	step [59/189], loss=75.7515
	step [60/189], loss=75.9534
	step [61/189], loss=85.0890
	step [62/189], loss=75.5877
	step [63/189], loss=76.0751
	step [64/189], loss=84.2261
	step [65/189], loss=65.0413
	step [66/189], loss=76.4439
	step [67/189], loss=69.6816
	step [68/189], loss=76.8028
	step [69/189], loss=68.0262
	step [70/189], loss=65.8073
	step [71/189], loss=85.1593
	step [72/189], loss=71.1310
	step [73/189], loss=66.2619
	step [74/189], loss=87.1969
	step [75/189], loss=77.4264
	step [76/189], loss=89.3626
	step [77/189], loss=76.0030
	step [78/189], loss=80.1494
	step [79/189], loss=62.7413
	step [80/189], loss=70.6706
	step [81/189], loss=77.6786
	step [82/189], loss=90.6611
	step [83/189], loss=74.1772
	step [84/189], loss=65.9140
	step [85/189], loss=64.9930
	step [86/189], loss=81.8495
	step [87/189], loss=71.2319
	step [88/189], loss=81.6866
	step [89/189], loss=69.5912
	step [90/189], loss=62.1701
	step [91/189], loss=86.2990
	step [92/189], loss=63.3109
	step [93/189], loss=84.1227
	step [94/189], loss=68.9135
	step [95/189], loss=71.3570
	step [96/189], loss=68.5463
	step [97/189], loss=76.4746
	step [98/189], loss=79.8931
	step [99/189], loss=66.2548
	step [100/189], loss=83.6455
	step [101/189], loss=83.7228
	step [102/189], loss=74.2831
	step [103/189], loss=66.7258
	step [104/189], loss=74.1139
	step [105/189], loss=81.3543
	step [106/189], loss=76.9086
	step [107/189], loss=81.4203
	step [108/189], loss=85.0057
	step [109/189], loss=71.1438
	step [110/189], loss=75.4120
	step [111/189], loss=71.3086
	step [112/189], loss=82.1925
	step [113/189], loss=76.4521
	step [114/189], loss=86.7246
	step [115/189], loss=73.6439
	step [116/189], loss=73.9354
	step [117/189], loss=78.6199
	step [118/189], loss=90.7361
	step [119/189], loss=79.8715
	step [120/189], loss=76.2257
	step [121/189], loss=75.4298
	step [122/189], loss=76.4919
	step [123/189], loss=82.5809
	step [124/189], loss=68.2611
	step [125/189], loss=80.7318
	step [126/189], loss=87.1014
	step [127/189], loss=78.5372
	step [128/189], loss=72.3373
	step [129/189], loss=81.9548
	step [130/189], loss=92.1253
	step [131/189], loss=69.6255
	step [132/189], loss=74.0587
	step [133/189], loss=68.1956
	step [134/189], loss=71.4719
	step [135/189], loss=69.1865
	step [136/189], loss=79.7789
	step [137/189], loss=76.7281
	step [138/189], loss=76.0138
	step [139/189], loss=73.7610
	step [140/189], loss=79.3848
	step [141/189], loss=71.7925
	step [142/189], loss=60.1470
	step [143/189], loss=71.0225
	step [144/189], loss=63.6090
	step [145/189], loss=79.0806
	step [146/189], loss=81.3643
	step [147/189], loss=78.2135
	step [148/189], loss=78.9865
	step [149/189], loss=72.2272
	step [150/189], loss=79.0000
	step [151/189], loss=65.4468
	step [152/189], loss=78.0615
	step [153/189], loss=81.1515
	step [154/189], loss=71.7430
	step [155/189], loss=80.7048
	step [156/189], loss=85.6373
	step [157/189], loss=80.0521
	step [158/189], loss=81.9829
	step [159/189], loss=71.0012
	step [160/189], loss=72.8063
	step [161/189], loss=80.0612
	step [162/189], loss=68.7185
	step [163/189], loss=81.7722
	step [164/189], loss=62.7318
	step [165/189], loss=79.2267
	step [166/189], loss=56.2434
	step [167/189], loss=69.9202
	step [168/189], loss=75.0946
	step [169/189], loss=83.7579
	step [170/189], loss=82.3404
	step [171/189], loss=86.9509
	step [172/189], loss=77.5563
	step [173/189], loss=74.1316
	step [174/189], loss=69.4077
	step [175/189], loss=72.2084
	step [176/189], loss=66.2100
	step [177/189], loss=66.0201
	step [178/189], loss=80.1760
	step [179/189], loss=67.9727
	step [180/189], loss=67.9117
	step [181/189], loss=90.4292
	step [182/189], loss=76.2455
	step [183/189], loss=78.6738
	step [184/189], loss=69.1916
	step [185/189], loss=64.5126
	step [186/189], loss=85.3991
	step [187/189], loss=67.7507
	step [188/189], loss=79.0082
	step [189/189], loss=30.7284
	Evaluating
	loss=0.0113, precision=0.3215, recall=0.8957, f1=0.4732
Training epoch 70
	step [1/189], loss=77.3595
	step [2/189], loss=75.5786
	step [3/189], loss=69.2968
	step [4/189], loss=80.3014
	step [5/189], loss=64.8543
	step [6/189], loss=73.7706
	step [7/189], loss=67.5440
	step [8/189], loss=72.3427
	step [9/189], loss=81.4258
	step [10/189], loss=72.8215
	step [11/189], loss=73.5079
	step [12/189], loss=81.8627
	step [13/189], loss=71.8044
	step [14/189], loss=75.5541
	step [15/189], loss=85.7772
	step [16/189], loss=77.8887
	step [17/189], loss=78.2902
	step [18/189], loss=89.3798
	step [19/189], loss=73.9276
	step [20/189], loss=83.9067
	step [21/189], loss=83.2664
	step [22/189], loss=73.0217
	step [23/189], loss=68.8387
	step [24/189], loss=74.5433
	step [25/189], loss=77.8501
	step [26/189], loss=71.7920
	step [27/189], loss=74.5572
	step [28/189], loss=75.7538
	step [29/189], loss=74.2647
	step [30/189], loss=84.5524
	step [31/189], loss=61.7171
	step [32/189], loss=67.5751
	step [33/189], loss=70.7818
	step [34/189], loss=79.3948
	step [35/189], loss=82.9142
	step [36/189], loss=62.8838
	step [37/189], loss=94.6869
	step [38/189], loss=69.8018
	step [39/189], loss=81.6981
	step [40/189], loss=75.7503
	step [41/189], loss=78.0316
	step [42/189], loss=79.3635
	step [43/189], loss=74.2505
	step [44/189], loss=82.0304
	step [45/189], loss=80.0334
	step [46/189], loss=79.6023
	step [47/189], loss=76.9447
	step [48/189], loss=84.4761
	step [49/189], loss=66.6494
	step [50/189], loss=75.9446
	step [51/189], loss=84.8815
	step [52/189], loss=76.7179
	step [53/189], loss=74.4314
	step [54/189], loss=62.5638
	step [55/189], loss=70.9574
	step [56/189], loss=80.9767
	step [57/189], loss=71.1377
	step [58/189], loss=74.4543
	step [59/189], loss=78.1321
	step [60/189], loss=59.1350
	step [61/189], loss=86.0178
	step [62/189], loss=80.5421
	step [63/189], loss=70.5907
	step [64/189], loss=88.7190
	step [65/189], loss=78.4727
	step [66/189], loss=81.1104
	step [67/189], loss=87.1682
	step [68/189], loss=70.9837
	step [69/189], loss=82.2528
	step [70/189], loss=74.1463
	step [71/189], loss=71.3031
	step [72/189], loss=99.2172
	step [73/189], loss=76.7323
	step [74/189], loss=83.3541
	step [75/189], loss=65.8009
	step [76/189], loss=88.5663
	step [77/189], loss=86.0034
	step [78/189], loss=88.1397
	step [79/189], loss=71.8720
	step [80/189], loss=87.3608
	step [81/189], loss=74.6682
	step [82/189], loss=62.5442
	step [83/189], loss=82.4356
	step [84/189], loss=77.9001
	step [85/189], loss=74.8317
	step [86/189], loss=69.6602
	step [87/189], loss=74.4966
	step [88/189], loss=82.3802
	step [89/189], loss=76.7138
	step [90/189], loss=76.3032
	step [91/189], loss=68.8684
	step [92/189], loss=91.5812
	step [93/189], loss=80.4979
	step [94/189], loss=65.9688
	step [95/189], loss=78.7531
	step [96/189], loss=63.4280
	step [97/189], loss=91.5237
	step [98/189], loss=69.8307
	step [99/189], loss=79.5421
	step [100/189], loss=77.4670
	step [101/189], loss=71.9419
	step [102/189], loss=73.0841
	step [103/189], loss=68.6057
	step [104/189], loss=85.5441
	step [105/189], loss=60.9089
	step [106/189], loss=75.5808
	step [107/189], loss=66.5897
	step [108/189], loss=73.3568
	step [109/189], loss=76.5487
	step [110/189], loss=78.9291
	step [111/189], loss=81.0893
	step [112/189], loss=66.2478
	step [113/189], loss=62.1655
	step [114/189], loss=73.6032
	step [115/189], loss=78.9113
	step [116/189], loss=82.6799
	step [117/189], loss=73.3058
	step [118/189], loss=75.8893
	step [119/189], loss=63.5554
	step [120/189], loss=76.1284
	step [121/189], loss=64.8945
	step [122/189], loss=68.3422
	step [123/189], loss=75.6708
	step [124/189], loss=84.0884
	step [125/189], loss=74.4619
	step [126/189], loss=76.4463
	step [127/189], loss=61.0340
	step [128/189], loss=88.7091
	step [129/189], loss=78.7736
	step [130/189], loss=78.4316
	step [131/189], loss=68.6492
	step [132/189], loss=67.8367
	step [133/189], loss=61.4454
	step [134/189], loss=80.0442
	step [135/189], loss=70.1430
	step [136/189], loss=89.9340
	step [137/189], loss=79.6727
	step [138/189], loss=79.6132
	step [139/189], loss=71.4324
	step [140/189], loss=79.1946
	step [141/189], loss=81.2123
	step [142/189], loss=79.4865
	step [143/189], loss=77.4316
	step [144/189], loss=67.6468
	step [145/189], loss=85.0506
	step [146/189], loss=73.4956
	step [147/189], loss=77.2924
	step [148/189], loss=82.2947
	step [149/189], loss=81.5119
	step [150/189], loss=69.9665
	step [151/189], loss=66.3690
	step [152/189], loss=73.9043
	step [153/189], loss=78.6562
	step [154/189], loss=64.0012
	step [155/189], loss=72.6496
	step [156/189], loss=66.5822
	step [157/189], loss=78.6552
	step [158/189], loss=71.0037
	step [159/189], loss=69.6565
	step [160/189], loss=72.5004
	step [161/189], loss=72.6988
	step [162/189], loss=75.1124
	step [163/189], loss=85.2947
	step [164/189], loss=86.8068
	step [165/189], loss=79.6732
	step [166/189], loss=85.8378
	step [167/189], loss=64.4017
	step [168/189], loss=76.4769
	step [169/189], loss=61.0375
	step [170/189], loss=86.7358
	step [171/189], loss=82.9102
	step [172/189], loss=62.6462
	step [173/189], loss=81.0040
	step [174/189], loss=67.7456
	step [175/189], loss=72.7022
	step [176/189], loss=88.6994
	step [177/189], loss=77.9917
	step [178/189], loss=70.4012
	step [179/189], loss=66.0760
	step [180/189], loss=67.3014
	step [181/189], loss=75.4987
	step [182/189], loss=87.3045
	step [183/189], loss=77.0032
	step [184/189], loss=71.1550
	step [185/189], loss=78.0307
	step [186/189], loss=68.8571
	step [187/189], loss=71.5052
	step [188/189], loss=63.2337
	step [189/189], loss=21.5304
	Evaluating
	loss=0.0097, precision=0.3692, recall=0.8954, f1=0.5228
Training epoch 71
	step [1/189], loss=81.8699
	step [2/189], loss=63.7145
	step [3/189], loss=80.5435
	step [4/189], loss=77.7008
	step [5/189], loss=75.7295
	step [6/189], loss=82.5959
	step [7/189], loss=69.4735
	step [8/189], loss=78.6804
	step [9/189], loss=76.2456
	step [10/189], loss=73.5165
	step [11/189], loss=92.3930
	step [12/189], loss=79.5334
	step [13/189], loss=79.8258
	step [14/189], loss=82.4378
	step [15/189], loss=67.6386
	step [16/189], loss=78.6446
	step [17/189], loss=67.2653
	step [18/189], loss=85.6189
	step [19/189], loss=79.6776
	step [20/189], loss=80.2866
	step [21/189], loss=78.5170
	step [22/189], loss=63.7516
	step [23/189], loss=82.8272
	step [24/189], loss=79.2542
	step [25/189], loss=69.9956
	step [26/189], loss=74.6597
	step [27/189], loss=93.5365
	step [28/189], loss=62.7831
	step [29/189], loss=77.9979
	step [30/189], loss=78.5591
	step [31/189], loss=82.3201
	step [32/189], loss=73.0966
	step [33/189], loss=79.1105
	step [34/189], loss=65.0073
	step [35/189], loss=68.7185
	step [36/189], loss=75.1280
	step [37/189], loss=75.1589
	step [38/189], loss=109.4314
	step [39/189], loss=66.9469
	step [40/189], loss=69.3106
	step [41/189], loss=75.2848
	step [42/189], loss=82.4778
	step [43/189], loss=93.3628
	step [44/189], loss=67.2153
	step [45/189], loss=72.3485
	step [46/189], loss=75.3983
	step [47/189], loss=67.6740
	step [48/189], loss=68.0923
	step [49/189], loss=86.5528
	step [50/189], loss=79.5301
	step [51/189], loss=73.1461
	step [52/189], loss=74.5815
	step [53/189], loss=85.4968
	step [54/189], loss=67.0459
	step [55/189], loss=72.9441
	step [56/189], loss=78.9317
	step [57/189], loss=60.7690
	step [58/189], loss=77.6890
	step [59/189], loss=72.9779
	step [60/189], loss=76.5024
	step [61/189], loss=73.8309
	step [62/189], loss=84.8707
	step [63/189], loss=86.3756
	step [64/189], loss=65.1544
	step [65/189], loss=68.0914
	step [66/189], loss=59.8056
	step [67/189], loss=65.6546
	step [68/189], loss=82.2565
	step [69/189], loss=63.8730
	step [70/189], loss=63.3446
	step [71/189], loss=80.9240
	step [72/189], loss=92.8216
	step [73/189], loss=75.0045
	step [74/189], loss=84.3070
	step [75/189], loss=74.9582
	step [76/189], loss=61.4744
	step [77/189], loss=81.4687
	step [78/189], loss=66.4033
	step [79/189], loss=80.3548
	step [80/189], loss=80.9096
	step [81/189], loss=67.7464
	step [82/189], loss=79.2556
	step [83/189], loss=78.4483
	step [84/189], loss=76.3242
	step [85/189], loss=68.5015
	step [86/189], loss=67.0458
	step [87/189], loss=71.2419
	step [88/189], loss=74.8020
	step [89/189], loss=66.8003
	step [90/189], loss=59.1808
	step [91/189], loss=77.3771
	step [92/189], loss=61.4884
	step [93/189], loss=82.1130
	step [94/189], loss=66.9382
	step [95/189], loss=72.3264
	step [96/189], loss=81.7115
	step [97/189], loss=81.2790
	step [98/189], loss=69.6889
	step [99/189], loss=62.2740
	step [100/189], loss=63.6579
	step [101/189], loss=83.7268
	step [102/189], loss=63.6173
	step [103/189], loss=71.3062
	step [104/189], loss=72.3205
	step [105/189], loss=81.1876
	step [106/189], loss=73.2112
	step [107/189], loss=72.6710
	step [108/189], loss=80.1608
	step [109/189], loss=77.5873
	step [110/189], loss=66.8756
	step [111/189], loss=65.2016
	step [112/189], loss=68.8943
	step [113/189], loss=95.3062
	step [114/189], loss=83.0017
	step [115/189], loss=77.8957
	step [116/189], loss=67.7763
	step [117/189], loss=78.1245
	step [118/189], loss=85.4313
	step [119/189], loss=76.4762
	step [120/189], loss=73.7483
	step [121/189], loss=63.6502
	step [122/189], loss=73.5825
	step [123/189], loss=71.7792
	step [124/189], loss=78.4947
	step [125/189], loss=69.0407
	step [126/189], loss=72.6282
	step [127/189], loss=71.3503
	step [128/189], loss=81.5255
	step [129/189], loss=75.7510
	step [130/189], loss=82.1242
	step [131/189], loss=79.6577
	step [132/189], loss=75.0402
	step [133/189], loss=81.9707
	step [134/189], loss=75.4335
	step [135/189], loss=80.3330
	step [136/189], loss=71.0181
	step [137/189], loss=66.4637
	step [138/189], loss=75.0482
	step [139/189], loss=88.6385
	step [140/189], loss=80.6134
	step [141/189], loss=69.3391
	step [142/189], loss=76.0259
	step [143/189], loss=77.2287
	step [144/189], loss=77.2439
	step [145/189], loss=69.5342
	step [146/189], loss=57.4693
	step [147/189], loss=76.7171
	step [148/189], loss=85.6941
	step [149/189], loss=76.9562
	step [150/189], loss=84.1490
	step [151/189], loss=100.8818
	step [152/189], loss=66.1552
	step [153/189], loss=74.2220
	step [154/189], loss=71.5614
	step [155/189], loss=81.9803
	step [156/189], loss=91.2152
	step [157/189], loss=81.5048
	step [158/189], loss=73.8027
	step [159/189], loss=75.3916
	step [160/189], loss=71.0020
	step [161/189], loss=75.4710
	step [162/189], loss=79.1546
	step [163/189], loss=69.6261
	step [164/189], loss=68.0956
	step [165/189], loss=67.9986
	step [166/189], loss=70.3299
	step [167/189], loss=67.4496
	step [168/189], loss=70.1433
	step [169/189], loss=67.3660
	step [170/189], loss=84.7310
	step [171/189], loss=74.2651
	step [172/189], loss=86.3085
	step [173/189], loss=77.0147
	step [174/189], loss=73.0920
	step [175/189], loss=74.8395
	step [176/189], loss=74.7333
	step [177/189], loss=78.3192
	step [178/189], loss=76.0596
	step [179/189], loss=73.7391
	step [180/189], loss=78.0880
	step [181/189], loss=56.6798
	step [182/189], loss=84.0174
	step [183/189], loss=77.8893
	step [184/189], loss=79.3625
	step [185/189], loss=80.5145
	step [186/189], loss=69.7563
	step [187/189], loss=74.2974
	step [188/189], loss=73.0145
	step [189/189], loss=31.2039
	Evaluating
	loss=0.0090, precision=0.3961, recall=0.9024, f1=0.5505
saving model as: 2_saved_model.pth
Training epoch 72
	step [1/189], loss=82.1865
	step [2/189], loss=71.8298
	step [3/189], loss=70.6904
	step [4/189], loss=61.5540
	step [5/189], loss=65.6260
	step [6/189], loss=89.9525
	step [7/189], loss=72.6861
	step [8/189], loss=93.4719
	step [9/189], loss=68.2345
	step [10/189], loss=78.4169
	step [11/189], loss=86.8056
	step [12/189], loss=74.9873
	step [13/189], loss=67.1581
	step [14/189], loss=63.4827
	step [15/189], loss=79.7034
	step [16/189], loss=65.4382
	step [17/189], loss=80.6855
	step [18/189], loss=78.8438
	step [19/189], loss=79.0524
	step [20/189], loss=58.8130
	step [21/189], loss=66.9059
	step [22/189], loss=80.2593
	step [23/189], loss=94.0652
	step [24/189], loss=70.5759
	step [25/189], loss=69.5922
	step [26/189], loss=79.7404
	step [27/189], loss=70.2048
	step [28/189], loss=73.7335
	step [29/189], loss=72.7961
	step [30/189], loss=62.5082
	step [31/189], loss=67.6213
	step [32/189], loss=71.9753
	step [33/189], loss=73.3206
	step [34/189], loss=69.6727
	step [35/189], loss=66.1727
	step [36/189], loss=72.6857
	step [37/189], loss=79.7760
	step [38/189], loss=73.4947
	step [39/189], loss=88.7882
	step [40/189], loss=78.6826
	step [41/189], loss=79.2162
	step [42/189], loss=68.0348
	step [43/189], loss=82.5806
	step [44/189], loss=88.9928
	step [45/189], loss=73.7423
	step [46/189], loss=71.5923
	step [47/189], loss=72.4826
	step [48/189], loss=85.7333
	step [49/189], loss=82.1485
	step [50/189], loss=70.6809
	step [51/189], loss=66.9628
	step [52/189], loss=66.6787
	step [53/189], loss=65.1743
	step [54/189], loss=84.5023
	step [55/189], loss=84.2188
	step [56/189], loss=71.7704
	step [57/189], loss=77.2487
	step [58/189], loss=63.2269
	step [59/189], loss=75.8010
	step [60/189], loss=79.1709
	step [61/189], loss=73.4780
	step [62/189], loss=70.5805
	step [63/189], loss=73.1620
	step [64/189], loss=71.3219
	step [65/189], loss=72.7711
	step [66/189], loss=69.9083
	step [67/189], loss=70.9462
	step [68/189], loss=70.9275
	step [69/189], loss=78.7148
	step [70/189], loss=76.2404
	step [71/189], loss=71.9723
	step [72/189], loss=71.0716
	step [73/189], loss=78.0747
	step [74/189], loss=72.3520
	step [75/189], loss=76.1138
	step [76/189], loss=71.5516
	step [77/189], loss=72.9102
	step [78/189], loss=64.9429
	step [79/189], loss=71.5592
	step [80/189], loss=67.0974
	step [81/189], loss=69.4384
	step [82/189], loss=71.7230
	step [83/189], loss=85.2139
	step [84/189], loss=81.6054
	step [85/189], loss=64.8751
	step [86/189], loss=81.0571
	step [87/189], loss=83.3762
	step [88/189], loss=68.6083
	step [89/189], loss=85.7482
	step [90/189], loss=80.5693
	step [91/189], loss=83.3567
	step [92/189], loss=72.8974
	step [93/189], loss=73.6939
	step [94/189], loss=74.6981
	step [95/189], loss=67.2175
	step [96/189], loss=79.4524
	step [97/189], loss=74.6553
	step [98/189], loss=75.1464
	step [99/189], loss=75.8518
	step [100/189], loss=78.8474
	step [101/189], loss=85.9052
	step [102/189], loss=67.5414
	step [103/189], loss=70.9792
	step [104/189], loss=65.3188
	step [105/189], loss=79.9453
	step [106/189], loss=73.3840
	step [107/189], loss=81.4194
	step [108/189], loss=82.6277
	step [109/189], loss=73.4083
	step [110/189], loss=79.1019
	step [111/189], loss=69.1751
	step [112/189], loss=70.1251
	step [113/189], loss=80.2972
	step [114/189], loss=82.1524
	step [115/189], loss=88.8874
	step [116/189], loss=95.9708
	step [117/189], loss=84.1891
	step [118/189], loss=68.6504
	step [119/189], loss=78.2869
	step [120/189], loss=75.4068
	step [121/189], loss=77.5926
	step [122/189], loss=61.0878
	step [123/189], loss=86.1271
	step [124/189], loss=73.7128
	step [125/189], loss=67.7231
	step [126/189], loss=73.7612
	step [127/189], loss=87.1436
	step [128/189], loss=62.5624
	step [129/189], loss=83.2440
	step [130/189], loss=73.7656
	step [131/189], loss=60.7183
	step [132/189], loss=82.0414
	step [133/189], loss=85.8131
	step [134/189], loss=84.0794
	step [135/189], loss=63.9604
	step [136/189], loss=75.4093
	step [137/189], loss=74.7041
	step [138/189], loss=72.8836
	step [139/189], loss=74.2871
	step [140/189], loss=82.3889
	step [141/189], loss=71.9736
	step [142/189], loss=63.8743
	step [143/189], loss=67.8606
	step [144/189], loss=79.2637
	step [145/189], loss=83.3980
	step [146/189], loss=70.4616
	step [147/189], loss=76.2975
	step [148/189], loss=79.7337
	step [149/189], loss=75.4241
	step [150/189], loss=79.6599
	step [151/189], loss=81.8051
	step [152/189], loss=64.0811
	step [153/189], loss=72.3736
	step [154/189], loss=81.6684
	step [155/189], loss=78.1524
	step [156/189], loss=88.7218
	step [157/189], loss=75.0799
	step [158/189], loss=78.0340
	step [159/189], loss=71.5609
	step [160/189], loss=74.1178
	step [161/189], loss=79.4378
	step [162/189], loss=77.8941
	step [163/189], loss=76.9904
	step [164/189], loss=72.4541
	step [165/189], loss=73.2245
	step [166/189], loss=70.3783
	step [167/189], loss=65.1963
	step [168/189], loss=73.9450
	step [169/189], loss=93.6036
	step [170/189], loss=74.2361
	step [171/189], loss=66.6522
	step [172/189], loss=86.6590
	step [173/189], loss=61.5189
	step [174/189], loss=72.5753
	step [175/189], loss=85.4451
	step [176/189], loss=91.8648
	step [177/189], loss=68.8714
	step [178/189], loss=70.9943
	step [179/189], loss=72.1926
	step [180/189], loss=76.1181
	step [181/189], loss=70.2612
	step [182/189], loss=77.8963
	step [183/189], loss=86.5209
	step [184/189], loss=67.3064
	step [185/189], loss=88.4567
	step [186/189], loss=71.8324
	step [187/189], loss=82.0450
	step [188/189], loss=88.7740
	step [189/189], loss=22.4113
	Evaluating
	loss=0.0102, precision=0.3542, recall=0.8886, f1=0.5066
Training epoch 73
	step [1/189], loss=74.2566
	step [2/189], loss=85.9497
	step [3/189], loss=84.5863
	step [4/189], loss=97.9290
	step [5/189], loss=79.6140
	step [6/189], loss=73.1285
	step [7/189], loss=81.0615
	step [8/189], loss=73.1501
	step [9/189], loss=77.5854
	step [10/189], loss=80.6611
	step [11/189], loss=82.7638
	step [12/189], loss=69.2468
	step [13/189], loss=78.0106
	step [14/189], loss=71.8016
	step [15/189], loss=82.9988
	step [16/189], loss=81.2152
	step [17/189], loss=68.9990
	step [18/189], loss=66.7419
	step [19/189], loss=65.6027
	step [20/189], loss=85.0810
	step [21/189], loss=69.9967
	step [22/189], loss=73.6450
	step [23/189], loss=87.2517
	step [24/189], loss=68.1237
	step [25/189], loss=66.2236
	step [26/189], loss=77.7391
	step [27/189], loss=83.3465
	step [28/189], loss=93.3305
	step [29/189], loss=57.9824
	step [30/189], loss=74.7592
	step [31/189], loss=74.0517
	step [32/189], loss=81.8591
	step [33/189], loss=71.3145
	step [34/189], loss=71.1203
	step [35/189], loss=67.5626
	step [36/189], loss=75.4484
	step [37/189], loss=85.3619
	step [38/189], loss=67.7082
	step [39/189], loss=76.7623
	step [40/189], loss=67.9305
	step [41/189], loss=59.2748
	step [42/189], loss=81.0838
	step [43/189], loss=76.7668
	step [44/189], loss=76.1558
	step [45/189], loss=64.9721
	step [46/189], loss=87.2523
	step [47/189], loss=71.1454
	step [48/189], loss=68.9457
	step [49/189], loss=67.9159
	step [50/189], loss=70.1474
	step [51/189], loss=78.0401
	step [52/189], loss=80.7549
	step [53/189], loss=87.8784
	step [54/189], loss=70.4435
	step [55/189], loss=71.9162
	step [56/189], loss=82.1308
	step [57/189], loss=70.0318
	step [58/189], loss=68.3167
	step [59/189], loss=67.6842
	step [60/189], loss=72.1489
	step [61/189], loss=74.6003
	step [62/189], loss=75.8013
	step [63/189], loss=79.1433
	step [64/189], loss=64.4371
	step [65/189], loss=74.3588
	step [66/189], loss=70.1166
	step [67/189], loss=70.9391
	step [68/189], loss=62.7430
	step [69/189], loss=81.2721
	step [70/189], loss=81.1879
	step [71/189], loss=67.1294
	step [72/189], loss=68.7710
	step [73/189], loss=88.2593
	step [74/189], loss=91.5606
	step [75/189], loss=77.4252
	step [76/189], loss=73.7165
	step [77/189], loss=71.3892
	step [78/189], loss=77.6532
	step [79/189], loss=84.9834
	step [80/189], loss=85.1007
	step [81/189], loss=79.8707
	step [82/189], loss=70.2365
	step [83/189], loss=73.8632
	step [84/189], loss=74.2756
	step [85/189], loss=69.8112
	step [86/189], loss=65.1999
	step [87/189], loss=69.5775
	step [88/189], loss=72.6867
	step [89/189], loss=67.0755
	step [90/189], loss=85.2498
	step [91/189], loss=88.4405
	step [92/189], loss=77.5061
	step [93/189], loss=85.1785
	step [94/189], loss=63.7877
	step [95/189], loss=60.8584
	step [96/189], loss=79.1550
	step [97/189], loss=69.5708
	step [98/189], loss=72.3505
	step [99/189], loss=66.6843
	step [100/189], loss=90.2595
	step [101/189], loss=79.1073
	step [102/189], loss=80.5888
	step [103/189], loss=74.8076
	step [104/189], loss=79.5776
	step [105/189], loss=73.3979
	step [106/189], loss=73.4632
	step [107/189], loss=70.0743
	step [108/189], loss=63.1100
	step [109/189], loss=85.4924
	step [110/189], loss=72.8546
	step [111/189], loss=73.0504
	step [112/189], loss=74.1070
	step [113/189], loss=74.1557
	step [114/189], loss=65.4074
	step [115/189], loss=73.1327
	step [116/189], loss=69.0466
	step [117/189], loss=76.4447
	step [118/189], loss=73.9443
	step [119/189], loss=82.7911
	step [120/189], loss=69.3015
	step [121/189], loss=78.7183
	step [122/189], loss=69.0105
	step [123/189], loss=82.4043
	step [124/189], loss=78.7342
	step [125/189], loss=65.6945
	step [126/189], loss=77.0930
	step [127/189], loss=70.7543
	step [128/189], loss=67.7858
	step [129/189], loss=76.1462
	step [130/189], loss=85.6926
	step [131/189], loss=53.8756
	step [132/189], loss=67.7991
	step [133/189], loss=68.9629
	step [134/189], loss=83.4283
	step [135/189], loss=90.9703
	step [136/189], loss=75.9005
	step [137/189], loss=70.7420
	step [138/189], loss=70.8025
	step [139/189], loss=73.5302
	step [140/189], loss=68.4293
	step [141/189], loss=66.8121
	step [142/189], loss=84.7031
	step [143/189], loss=74.8172
	step [144/189], loss=75.5124
	step [145/189], loss=79.4747
	step [146/189], loss=64.4925
	step [147/189], loss=65.7754
	step [148/189], loss=81.8691
	step [149/189], loss=64.4628
	step [150/189], loss=71.5300
	step [151/189], loss=69.4027
	step [152/189], loss=77.6958
	step [153/189], loss=73.8020
	step [154/189], loss=66.5764
	step [155/189], loss=80.7330
	step [156/189], loss=72.8740
	step [157/189], loss=75.4398
	step [158/189], loss=60.6303
	step [159/189], loss=86.8746
	step [160/189], loss=74.4111
	step [161/189], loss=84.8873
	step [162/189], loss=84.9677
	step [163/189], loss=81.5338
	step [164/189], loss=71.5626
	step [165/189], loss=73.8519
	step [166/189], loss=76.9856
	step [167/189], loss=78.4447
	step [168/189], loss=77.6477
	step [169/189], loss=67.9260
	step [170/189], loss=82.9482
	step [171/189], loss=71.4991
	step [172/189], loss=68.2479
	step [173/189], loss=75.8118
	step [174/189], loss=98.8598
	step [175/189], loss=66.6928
	step [176/189], loss=78.8696
	step [177/189], loss=71.7031
	step [178/189], loss=69.8138
	step [179/189], loss=85.5626
	step [180/189], loss=74.8467
	step [181/189], loss=74.4902
	step [182/189], loss=72.9555
	step [183/189], loss=75.9707
	step [184/189], loss=77.7280
	step [185/189], loss=75.4245
	step [186/189], loss=85.1802
	step [187/189], loss=75.2769
	step [188/189], loss=72.4159
	step [189/189], loss=33.5109
	Evaluating
	loss=0.0092, precision=0.3815, recall=0.8954, f1=0.5350
Training epoch 74
	step [1/189], loss=70.4144
	step [2/189], loss=78.6747
	step [3/189], loss=57.4306
	step [4/189], loss=96.4380
	step [5/189], loss=86.3317
	step [6/189], loss=63.6023
	step [7/189], loss=79.0017
	step [8/189], loss=88.1693
	step [9/189], loss=71.3803
	step [10/189], loss=66.1315
	step [11/189], loss=72.5104
	step [12/189], loss=69.4035
	step [13/189], loss=61.2495
	step [14/189], loss=73.7845
	step [15/189], loss=72.6314
	step [16/189], loss=66.9382
	step [17/189], loss=72.4556
	step [18/189], loss=83.0696
	step [19/189], loss=75.5592
	step [20/189], loss=68.8835
	step [21/189], loss=67.5375
	step [22/189], loss=67.6233
	step [23/189], loss=70.2610
	step [24/189], loss=75.5094
	step [25/189], loss=77.5952
	step [26/189], loss=72.3149
	step [27/189], loss=70.0953
	step [28/189], loss=71.9774
	step [29/189], loss=80.2466
	step [30/189], loss=84.1457
	step [31/189], loss=78.2237
	step [32/189], loss=76.9296
	step [33/189], loss=78.6315
	step [34/189], loss=83.6982
	step [35/189], loss=73.6258
	step [36/189], loss=74.5694
	step [37/189], loss=75.2529
	step [38/189], loss=80.8524
	step [39/189], loss=74.0970
	step [40/189], loss=75.2812
	step [41/189], loss=66.4552
	step [42/189], loss=80.5917
	step [43/189], loss=80.7654
	step [44/189], loss=70.2484
	step [45/189], loss=75.7615
	step [46/189], loss=70.3847
	step [47/189], loss=74.3585
	step [48/189], loss=77.7506
	step [49/189], loss=73.3488
	step [50/189], loss=69.0152
	step [51/189], loss=78.3462
	step [52/189], loss=73.3441
	step [53/189], loss=72.2749
	step [54/189], loss=83.9419
	step [55/189], loss=82.7348
	step [56/189], loss=60.2582
	step [57/189], loss=83.6522
	step [58/189], loss=73.3544
	step [59/189], loss=80.1603
	step [60/189], loss=85.1830
	step [61/189], loss=83.2603
	step [62/189], loss=73.3963
	step [63/189], loss=72.1828
	step [64/189], loss=86.9975
	step [65/189], loss=80.9820
	step [66/189], loss=74.3645
	step [67/189], loss=80.2845
	step [68/189], loss=70.9937
	step [69/189], loss=74.5074
	step [70/189], loss=72.7414
	step [71/189], loss=84.8202
	step [72/189], loss=78.1336
	step [73/189], loss=69.3071
	step [74/189], loss=80.1587
	step [75/189], loss=84.8318
	step [76/189], loss=83.1168
	step [77/189], loss=67.8641
	step [78/189], loss=85.9187
	step [79/189], loss=84.1978
	step [80/189], loss=70.5260
	step [81/189], loss=69.2409
	step [82/189], loss=79.1929
	step [83/189], loss=66.8769
	step [84/189], loss=58.8727
	step [85/189], loss=80.3548
	step [86/189], loss=76.0044
	step [87/189], loss=79.2173
	step [88/189], loss=70.7899
	step [89/189], loss=88.6164
	step [90/189], loss=73.6411
	step [91/189], loss=75.8677
	step [92/189], loss=69.2539
	step [93/189], loss=63.1916
	step [94/189], loss=74.4049
	step [95/189], loss=79.2974
	step [96/189], loss=73.2290
	step [97/189], loss=69.2708
	step [98/189], loss=63.5997
	step [99/189], loss=79.9681
	step [100/189], loss=78.6830
	step [101/189], loss=81.4466
	step [102/189], loss=85.5118
	step [103/189], loss=77.4419
	step [104/189], loss=76.3049
	step [105/189], loss=60.5821
	step [106/189], loss=87.5336
	step [107/189], loss=72.9190
	step [108/189], loss=70.4110
	step [109/189], loss=68.5374
	step [110/189], loss=65.8010
	step [111/189], loss=81.3374
	step [112/189], loss=69.5571
	step [113/189], loss=76.9571
	step [114/189], loss=75.4219
	step [115/189], loss=87.9669
	step [116/189], loss=75.8527
	step [117/189], loss=63.6913
	step [118/189], loss=74.9610
	step [119/189], loss=74.4333
	step [120/189], loss=65.6955
	step [121/189], loss=61.7134
	step [122/189], loss=93.6902
	step [123/189], loss=81.5439
	step [124/189], loss=79.7730
	step [125/189], loss=71.3522
	step [126/189], loss=77.6344
	step [127/189], loss=67.9807
	step [128/189], loss=79.0063
	step [129/189], loss=74.5079
	step [130/189], loss=73.5018
	step [131/189], loss=72.4907
	step [132/189], loss=66.7030
	step [133/189], loss=95.1803
	step [134/189], loss=86.4865
	step [135/189], loss=68.0747
	step [136/189], loss=69.3504
	step [137/189], loss=66.7897
	step [138/189], loss=72.6336
	step [139/189], loss=76.2542
	step [140/189], loss=71.7990
	step [141/189], loss=84.5035
	step [142/189], loss=72.9367
	step [143/189], loss=77.4662
	step [144/189], loss=75.0555
	step [145/189], loss=72.7987
	step [146/189], loss=78.1540
	step [147/189], loss=71.9721
	step [148/189], loss=71.3705
	step [149/189], loss=74.9202
	step [150/189], loss=74.5818
	step [151/189], loss=72.7849
	step [152/189], loss=74.3213
	step [153/189], loss=69.0799
	step [154/189], loss=75.0078
	step [155/189], loss=67.0102
	step [156/189], loss=69.4923
	step [157/189], loss=74.4584
	step [158/189], loss=59.8707
	step [159/189], loss=77.2879
	step [160/189], loss=77.8456
	step [161/189], loss=72.3823
	step [162/189], loss=66.2869
	step [163/189], loss=87.1880
	step [164/189], loss=76.1400
	step [165/189], loss=59.6967
	step [166/189], loss=67.2980
	step [167/189], loss=66.0399
	step [168/189], loss=71.4507
	step [169/189], loss=78.3242
	step [170/189], loss=65.3441
	step [171/189], loss=79.1519
	step [172/189], loss=80.1128
	step [173/189], loss=65.5615
	step [174/189], loss=81.2970
	step [175/189], loss=74.6193
	step [176/189], loss=76.8204
	step [177/189], loss=76.9259
	step [178/189], loss=74.5012
	step [179/189], loss=81.6701
	step [180/189], loss=83.1843
	step [181/189], loss=74.2353
	step [182/189], loss=62.2751
	step [183/189], loss=76.8806
	step [184/189], loss=61.5582
	step [185/189], loss=78.6050
	step [186/189], loss=77.7965
	step [187/189], loss=69.5744
	step [188/189], loss=77.8464
	step [189/189], loss=32.3935
	Evaluating
	loss=0.0116, precision=0.3000, recall=0.8991, f1=0.4499
Training epoch 75
	step [1/189], loss=72.6242
	step [2/189], loss=69.3867
	step [3/189], loss=69.1322
	step [4/189], loss=81.0580
	step [5/189], loss=82.9920
	step [6/189], loss=78.5586
	step [7/189], loss=75.4128
	step [8/189], loss=78.2147
	step [9/189], loss=70.9103
	step [10/189], loss=67.5508
	step [11/189], loss=76.5761
	step [12/189], loss=71.6195
	step [13/189], loss=77.4861
	step [14/189], loss=75.4236
	step [15/189], loss=75.7210
	step [16/189], loss=68.8564
	step [17/189], loss=62.7551
	step [18/189], loss=69.0622
	step [19/189], loss=74.7660
	step [20/189], loss=67.8821
	step [21/189], loss=75.1052
	step [22/189], loss=71.0293
	step [23/189], loss=68.7497
	step [24/189], loss=65.8431
	step [25/189], loss=78.5462
	step [26/189], loss=92.6555
	step [27/189], loss=85.1729
	step [28/189], loss=71.8567
	step [29/189], loss=83.1384
	step [30/189], loss=68.0667
	step [31/189], loss=66.5933
	step [32/189], loss=65.4758
	step [33/189], loss=65.4384
	step [34/189], loss=75.1908
	step [35/189], loss=82.2516
	step [36/189], loss=71.5139
	step [37/189], loss=82.0814
	step [38/189], loss=64.1087
	step [39/189], loss=69.4229
	step [40/189], loss=64.0731
	step [41/189], loss=79.6790
	step [42/189], loss=78.2140
	step [43/189], loss=82.0834
	step [44/189], loss=71.8157
	step [45/189], loss=71.3536
	step [46/189], loss=80.1199
	step [47/189], loss=77.1244
	step [48/189], loss=62.8481
	step [49/189], loss=70.9478
	step [50/189], loss=69.2487
	step [51/189], loss=61.6249
	step [52/189], loss=71.3929
	step [53/189], loss=93.5831
	step [54/189], loss=69.5356
	step [55/189], loss=73.8278
	step [56/189], loss=62.2332
	step [57/189], loss=78.5352
	step [58/189], loss=73.1398
	step [59/189], loss=62.4033
	step [60/189], loss=69.6788
	step [61/189], loss=72.4038
	step [62/189], loss=73.1180
	step [63/189], loss=84.2996
	step [64/189], loss=78.6619
	step [65/189], loss=68.4095
	step [66/189], loss=79.6278
	step [67/189], loss=82.3596
	step [68/189], loss=64.3123
	step [69/189], loss=60.1031
	step [70/189], loss=81.6642
	step [71/189], loss=88.2547
	step [72/189], loss=68.3318
	step [73/189], loss=68.6501
	step [74/189], loss=82.3415
	step [75/189], loss=78.2524
	step [76/189], loss=78.3839
	step [77/189], loss=70.0715
	step [78/189], loss=84.1359
	step [79/189], loss=72.8727
	step [80/189], loss=70.4713
	step [81/189], loss=75.8985
	step [82/189], loss=80.1439
	step [83/189], loss=76.9308
	step [84/189], loss=77.3823
	step [85/189], loss=79.5423
	step [86/189], loss=80.1944
	step [87/189], loss=75.1656
	step [88/189], loss=65.1226
	step [89/189], loss=73.8251
	step [90/189], loss=72.1216
	step [91/189], loss=79.0981
	step [92/189], loss=61.3729
	step [93/189], loss=70.5456
	step [94/189], loss=73.1714
	step [95/189], loss=80.4343
	step [96/189], loss=89.8134
	step [97/189], loss=71.5176
	step [98/189], loss=74.9096
	step [99/189], loss=73.8361
	step [100/189], loss=71.4859
	step [101/189], loss=83.1720
	step [102/189], loss=79.6077
	step [103/189], loss=66.6668
	step [104/189], loss=77.6282
	step [105/189], loss=75.6276
	step [106/189], loss=88.5506
	step [107/189], loss=66.4212
	step [108/189], loss=75.7754
	step [109/189], loss=69.7560
	step [110/189], loss=71.5378
	step [111/189], loss=68.0166
	step [112/189], loss=77.3223
	step [113/189], loss=70.6014
	step [114/189], loss=71.4290
	step [115/189], loss=67.6951
	step [116/189], loss=66.8989
	step [117/189], loss=79.6087
	step [118/189], loss=63.8494
	step [119/189], loss=76.9218
	step [120/189], loss=73.5415
	step [121/189], loss=65.7176
	step [122/189], loss=71.9849
	step [123/189], loss=76.5517
	step [124/189], loss=85.3754
	step [125/189], loss=78.1119
	step [126/189], loss=66.8412
	step [127/189], loss=67.7372
	step [128/189], loss=81.2861
	step [129/189], loss=68.6725
	step [130/189], loss=66.1881
	step [131/189], loss=70.4423
	step [132/189], loss=71.1961
	step [133/189], loss=74.4164
	step [134/189], loss=75.2186
	step [135/189], loss=77.8762
	step [136/189], loss=84.4455
	step [137/189], loss=71.6343
	step [138/189], loss=97.6668
	step [139/189], loss=65.0992
	step [140/189], loss=73.6384
	step [141/189], loss=73.8317
	step [142/189], loss=64.8937
	step [143/189], loss=79.7992
	step [144/189], loss=77.0263
	step [145/189], loss=69.1602
	step [146/189], loss=78.4130
	step [147/189], loss=77.9076
	step [148/189], loss=77.6494
	step [149/189], loss=73.2269
	step [150/189], loss=64.0682
	step [151/189], loss=78.4688
	step [152/189], loss=68.0938
	step [153/189], loss=67.0230
	step [154/189], loss=81.9644
	step [155/189], loss=72.8943
	step [156/189], loss=66.6605
	step [157/189], loss=81.9230
	step [158/189], loss=73.8365
	step [159/189], loss=78.2807
	step [160/189], loss=69.4646
	step [161/189], loss=78.8932
	step [162/189], loss=63.3975
	step [163/189], loss=68.3284
	step [164/189], loss=69.2152
	step [165/189], loss=83.5264
	step [166/189], loss=75.1783
	step [167/189], loss=90.2618
	step [168/189], loss=71.8928
	step [169/189], loss=77.8563
	step [170/189], loss=82.2235
	step [171/189], loss=66.3209
	step [172/189], loss=85.0261
	step [173/189], loss=81.9502
	step [174/189], loss=77.7040
	step [175/189], loss=75.4440
	step [176/189], loss=78.9182
	step [177/189], loss=75.2644
	step [178/189], loss=81.9253
	step [179/189], loss=75.5040
	step [180/189], loss=69.5517
	step [181/189], loss=79.9893
	step [182/189], loss=58.6288
	step [183/189], loss=74.6816
	step [184/189], loss=83.2681
	step [185/189], loss=82.5665
	step [186/189], loss=79.5443
	step [187/189], loss=63.5752
	step [188/189], loss=78.0997
	step [189/189], loss=32.7736
	Evaluating
	loss=0.0086, precision=0.4047, recall=0.8868, f1=0.5558
saving model as: 2_saved_model.pth
Training epoch 76
	step [1/189], loss=73.4296
	step [2/189], loss=80.8707
	step [3/189], loss=81.9608
	step [4/189], loss=69.7891
	step [5/189], loss=64.5087
	step [6/189], loss=73.8355
	step [7/189], loss=66.2486
	step [8/189], loss=68.9756
	step [9/189], loss=73.5157
	step [10/189], loss=76.5517
	step [11/189], loss=66.8027
	step [12/189], loss=70.7312
	step [13/189], loss=79.2449
	step [14/189], loss=70.8209
	step [15/189], loss=83.0335
	step [16/189], loss=70.9925
	step [17/189], loss=84.5112
	step [18/189], loss=85.5568
	step [19/189], loss=75.6069
	step [20/189], loss=75.4080
	step [21/189], loss=82.3346
	step [22/189], loss=69.3741
	step [23/189], loss=84.3208
	step [24/189], loss=76.4265
	step [25/189], loss=69.5423
	step [26/189], loss=70.4452
	step [27/189], loss=61.8122
	step [28/189], loss=86.8307
	step [29/189], loss=77.4582
	step [30/189], loss=77.6026
	step [31/189], loss=63.1315
	step [32/189], loss=76.7972
	step [33/189], loss=68.5281
	step [34/189], loss=79.7856
	step [35/189], loss=66.8434
	step [36/189], loss=66.5180
	step [37/189], loss=77.2962
	step [38/189], loss=76.6274
	step [39/189], loss=75.4745
	step [40/189], loss=83.7313
	step [41/189], loss=79.5644
	step [42/189], loss=70.1416
	step [43/189], loss=71.9961
	step [44/189], loss=72.1612
	step [45/189], loss=73.2552
	step [46/189], loss=69.7398
	step [47/189], loss=74.6499
	step [48/189], loss=82.6084
	step [49/189], loss=87.1651
	step [50/189], loss=73.1968
	step [51/189], loss=72.7561
	step [52/189], loss=95.1472
	step [53/189], loss=82.1884
	step [54/189], loss=81.2372
	step [55/189], loss=63.1330
	step [56/189], loss=71.0230
	step [57/189], loss=77.1308
	step [58/189], loss=74.5559
	step [59/189], loss=73.1410
	step [60/189], loss=73.0277
	step [61/189], loss=75.8205
	step [62/189], loss=76.4997
	step [63/189], loss=89.2655
	step [64/189], loss=62.0321
	step [65/189], loss=79.8501
	step [66/189], loss=74.6888
	step [67/189], loss=64.3433
	step [68/189], loss=76.5831
	step [69/189], loss=66.2889
	step [70/189], loss=81.9653
	step [71/189], loss=84.7480
	step [72/189], loss=74.3144
	step [73/189], loss=71.2395
	step [74/189], loss=70.1219
	step [75/189], loss=79.7484
	step [76/189], loss=58.7358
	step [77/189], loss=60.5048
	step [78/189], loss=68.5960
	step [79/189], loss=71.9698
	step [80/189], loss=73.4000
	step [81/189], loss=80.3938
	step [82/189], loss=77.8017
	step [83/189], loss=89.7398
	step [84/189], loss=77.6231
	step [85/189], loss=64.3823
	step [86/189], loss=80.1828
	step [87/189], loss=68.1485
	step [88/189], loss=72.5052
	step [89/189], loss=69.1037
	step [90/189], loss=72.0839
	step [91/189], loss=74.8041
	step [92/189], loss=79.7104
	step [93/189], loss=68.1433
	step [94/189], loss=68.1521
	step [95/189], loss=76.4337
	step [96/189], loss=73.2187
	step [97/189], loss=66.4982
	step [98/189], loss=61.5571
	step [99/189], loss=59.5725
	step [100/189], loss=61.6390
	step [101/189], loss=83.8068
	step [102/189], loss=76.7691
	step [103/189], loss=84.6066
	step [104/189], loss=67.0213
	step [105/189], loss=89.6214
	step [106/189], loss=74.7096
	step [107/189], loss=77.1171
	step [108/189], loss=65.5067
	step [109/189], loss=84.0944
	step [110/189], loss=77.5826
	step [111/189], loss=86.6614
	step [112/189], loss=78.8844
	step [113/189], loss=68.3500
	step [114/189], loss=81.5938
	step [115/189], loss=79.4988
	step [116/189], loss=74.4048
	step [117/189], loss=71.3825
	step [118/189], loss=71.3985
	step [119/189], loss=76.6424
	step [120/189], loss=67.3590
	step [121/189], loss=76.9819
	step [122/189], loss=65.6336
	step [123/189], loss=93.3864
	step [124/189], loss=73.2908
	step [125/189], loss=66.4833
	step [126/189], loss=79.4073
	step [127/189], loss=81.8497
	step [128/189], loss=61.1366
	step [129/189], loss=68.7079
	step [130/189], loss=67.2013
	step [131/189], loss=84.2038
	step [132/189], loss=88.7901
	step [133/189], loss=70.3542
	step [134/189], loss=68.3040
	step [135/189], loss=67.1368
	step [136/189], loss=63.0006
	step [137/189], loss=71.4145
	step [138/189], loss=70.3139
	step [139/189], loss=68.1192
	step [140/189], loss=78.8667
	step [141/189], loss=69.4544
	step [142/189], loss=80.9050
	step [143/189], loss=73.7070
	step [144/189], loss=81.7084
	step [145/189], loss=84.2912
	step [146/189], loss=66.9469
	step [147/189], loss=66.2621
	step [148/189], loss=86.4459
	step [149/189], loss=55.9564
	step [150/189], loss=73.7412
	step [151/189], loss=72.7255
	step [152/189], loss=79.0120
	step [153/189], loss=76.7223
	step [154/189], loss=73.3875
	step [155/189], loss=67.7883
	step [156/189], loss=64.6316
	step [157/189], loss=75.1013
	step [158/189], loss=67.4621
	step [159/189], loss=85.4135
	step [160/189], loss=70.4714
	step [161/189], loss=75.4241
	step [162/189], loss=64.5533
	step [163/189], loss=82.3313
	step [164/189], loss=74.8578
	step [165/189], loss=65.5466
	step [166/189], loss=67.8104
	step [167/189], loss=75.3934
	step [168/189], loss=70.7767
	step [169/189], loss=78.8746
	step [170/189], loss=85.9501
	step [171/189], loss=67.4286
	step [172/189], loss=85.3650
	step [173/189], loss=65.8266
	step [174/189], loss=72.3606
	step [175/189], loss=77.0953
	step [176/189], loss=91.4026
	step [177/189], loss=68.6487
	step [178/189], loss=71.9491
	step [179/189], loss=76.4803
	step [180/189], loss=71.7877
	step [181/189], loss=62.4219
	step [182/189], loss=69.9344
	step [183/189], loss=93.0904
	step [184/189], loss=86.7854
	step [185/189], loss=63.1251
	step [186/189], loss=55.5087
	step [187/189], loss=69.6766
	step [188/189], loss=72.0123
	step [189/189], loss=31.4975
	Evaluating
	loss=0.0085, precision=0.4110, recall=0.8894, f1=0.5622
saving model as: 2_saved_model.pth
Training epoch 77
	step [1/189], loss=51.0133
	step [2/189], loss=71.5570
	step [3/189], loss=72.5111
	step [4/189], loss=76.6809
	step [5/189], loss=60.1173
	step [6/189], loss=88.6435
	step [7/189], loss=82.8331
	step [8/189], loss=80.3804
	step [9/189], loss=65.5925
	step [10/189], loss=72.5769
	step [11/189], loss=82.6995
	step [12/189], loss=80.3336
	step [13/189], loss=77.7937
	step [14/189], loss=71.5293
	step [15/189], loss=78.0115
	step [16/189], loss=66.0213
	step [17/189], loss=78.7724
	step [18/189], loss=62.8562
	step [19/189], loss=79.0933
	step [20/189], loss=75.2962
	step [21/189], loss=66.5701
	step [22/189], loss=65.6945
	step [23/189], loss=82.3368
	step [24/189], loss=81.5496
	step [25/189], loss=62.7542
	step [26/189], loss=74.1942
	step [27/189], loss=88.0219
	step [28/189], loss=75.3358
	step [29/189], loss=75.5024
	step [30/189], loss=68.2334
	step [31/189], loss=76.9635
	step [32/189], loss=68.7218
	step [33/189], loss=80.0278
	step [34/189], loss=75.7569
	step [35/189], loss=64.7801
	step [36/189], loss=64.6632
	step [37/189], loss=83.7194
	step [38/189], loss=82.4722
	step [39/189], loss=73.1926
	step [40/189], loss=63.2908
	step [41/189], loss=64.4544
	step [42/189], loss=66.1630
	step [43/189], loss=92.9248
	step [44/189], loss=68.5389
	step [45/189], loss=72.3505
	step [46/189], loss=73.3651
	step [47/189], loss=85.4269
	step [48/189], loss=67.6634
	step [49/189], loss=51.8419
	step [50/189], loss=67.0396
	step [51/189], loss=66.3900
	step [52/189], loss=75.4863
	step [53/189], loss=75.4871
	step [54/189], loss=75.6724
	step [55/189], loss=82.4894
	step [56/189], loss=77.0196
	step [57/189], loss=66.5281
	step [58/189], loss=71.7187
	step [59/189], loss=63.0996
	step [60/189], loss=76.3099
	step [61/189], loss=90.0192
	step [62/189], loss=66.2039
	step [63/189], loss=76.9695
	step [64/189], loss=74.5987
	step [65/189], loss=70.9228
	step [66/189], loss=80.0898
	step [67/189], loss=65.8947
	step [68/189], loss=72.4681
	step [69/189], loss=78.6756
	step [70/189], loss=83.3929
	step [71/189], loss=69.2764
	step [72/189], loss=64.2317
	step [73/189], loss=62.0161
	step [74/189], loss=79.2144
	step [75/189], loss=70.8594
	step [76/189], loss=88.3915
	step [77/189], loss=86.1937
	step [78/189], loss=75.1934
	step [79/189], loss=72.5327
	step [80/189], loss=69.6977
	step [81/189], loss=79.4617
	step [82/189], loss=81.9792
	step [83/189], loss=73.7935
	step [84/189], loss=82.2405
	step [85/189], loss=72.4203
	step [86/189], loss=70.1453
	step [87/189], loss=73.5281
	step [88/189], loss=76.9816
	step [89/189], loss=67.4007
	step [90/189], loss=75.0849
	step [91/189], loss=61.3072
	step [92/189], loss=74.0067
	step [93/189], loss=65.9956
	step [94/189], loss=66.9938
	step [95/189], loss=78.8368
	step [96/189], loss=79.5426
	step [97/189], loss=77.5278
	step [98/189], loss=82.0251
	step [99/189], loss=79.3477
	step [100/189], loss=81.4614
	step [101/189], loss=80.7266
	step [102/189], loss=75.2424
	step [103/189], loss=67.2074
	step [104/189], loss=63.5394
	step [105/189], loss=77.4825
	step [106/189], loss=71.9193
	step [107/189], loss=77.7194
	step [108/189], loss=64.2022
	step [109/189], loss=81.1156
	step [110/189], loss=67.0029
	step [111/189], loss=81.6074
	step [112/189], loss=68.9419
	step [113/189], loss=77.7137
	step [114/189], loss=71.7284
	step [115/189], loss=92.9478
	step [116/189], loss=85.9698
	step [117/189], loss=76.5155
	step [118/189], loss=71.6014
	step [119/189], loss=82.1473
	step [120/189], loss=69.4455
	step [121/189], loss=85.5331
	step [122/189], loss=80.5605
	step [123/189], loss=73.5649
	step [124/189], loss=71.3700
	step [125/189], loss=64.0707
	step [126/189], loss=85.5459
	step [127/189], loss=89.9194
	step [128/189], loss=75.3766
	step [129/189], loss=74.5827
	step [130/189], loss=71.7678
	step [131/189], loss=82.0039
	step [132/189], loss=63.3317
	step [133/189], loss=70.5251
	step [134/189], loss=73.8775
	step [135/189], loss=74.6227
	step [136/189], loss=83.1216
	step [137/189], loss=70.2464
	step [138/189], loss=69.4039
	step [139/189], loss=77.6212
	step [140/189], loss=70.5634
	step [141/189], loss=76.4906
	step [142/189], loss=85.9085
	step [143/189], loss=71.0789
	step [144/189], loss=92.9454
	step [145/189], loss=69.9108
	step [146/189], loss=85.1789
	step [147/189], loss=65.2550
	step [148/189], loss=61.5161
	step [149/189], loss=77.7633
	step [150/189], loss=58.4801
	step [151/189], loss=74.5260
	step [152/189], loss=81.7987
	step [153/189], loss=71.6023
	step [154/189], loss=60.6900
	step [155/189], loss=87.6179
	step [156/189], loss=70.3408
	step [157/189], loss=87.3136
	step [158/189], loss=70.7953
	step [159/189], loss=69.5336
	step [160/189], loss=62.1605
	step [161/189], loss=65.0455
	step [162/189], loss=79.5617
	step [163/189], loss=67.3955
	step [164/189], loss=67.6056
	step [165/189], loss=58.6889
	step [166/189], loss=74.0730
	step [167/189], loss=78.7533
	step [168/189], loss=75.2577
	step [169/189], loss=67.9379
	step [170/189], loss=84.7904
	step [171/189], loss=67.4224
	step [172/189], loss=76.0831
	step [173/189], loss=63.2730
	step [174/189], loss=81.1139
	step [175/189], loss=66.7851
	step [176/189], loss=72.5684
	step [177/189], loss=75.0859
	step [178/189], loss=74.7936
	step [179/189], loss=61.1850
	step [180/189], loss=85.8980
	step [181/189], loss=71.1270
	step [182/189], loss=75.9053
	step [183/189], loss=79.2132
	step [184/189], loss=66.3616
	step [185/189], loss=73.4553
	step [186/189], loss=73.6762
	step [187/189], loss=70.2079
	step [188/189], loss=75.9896
	step [189/189], loss=22.2456
	Evaluating
	loss=0.0091, precision=0.3649, recall=0.8854, f1=0.5168
Training epoch 78
	step [1/189], loss=65.0398
	step [2/189], loss=66.8819
	step [3/189], loss=64.2224
	step [4/189], loss=73.7878
	step [5/189], loss=68.0711
	step [6/189], loss=71.1423
	step [7/189], loss=70.9569
	step [8/189], loss=69.4592
	step [9/189], loss=77.1352
	step [10/189], loss=63.6195
	step [11/189], loss=83.4681
	step [12/189], loss=79.3714
	step [13/189], loss=64.1382
	step [14/189], loss=80.2441
	step [15/189], loss=76.4668
	step [16/189], loss=60.4128
	step [17/189], loss=81.6758
	step [18/189], loss=69.4257
	step [19/189], loss=81.2602
	step [20/189], loss=82.1217
	step [21/189], loss=82.5661
	step [22/189], loss=69.5286
	step [23/189], loss=63.1195
	step [24/189], loss=81.4311
	step [25/189], loss=89.0161
	step [26/189], loss=67.8953
	step [27/189], loss=74.2881
	step [28/189], loss=73.4152
	step [29/189], loss=59.4324
	step [30/189], loss=70.8497
	step [31/189], loss=74.8276
	step [32/189], loss=84.1358
	step [33/189], loss=64.9689
	step [34/189], loss=78.2340
	step [35/189], loss=88.8826
	step [36/189], loss=78.7327
	step [37/189], loss=76.6409
	step [38/189], loss=75.1891
	step [39/189], loss=75.9652
	step [40/189], loss=69.6836
	step [41/189], loss=70.1472
	step [42/189], loss=68.8544
	step [43/189], loss=70.3987
	step [44/189], loss=66.8003
	step [45/189], loss=67.4216
	step [46/189], loss=71.6911
	step [47/189], loss=74.3770
	step [48/189], loss=84.2568
	step [49/189], loss=68.6178
	step [50/189], loss=76.1451
	step [51/189], loss=72.7186
	step [52/189], loss=75.4414
	step [53/189], loss=77.0238
	step [54/189], loss=76.9082
	step [55/189], loss=61.2403
	step [56/189], loss=80.0670
	step [57/189], loss=80.6395
	step [58/189], loss=60.6842
	step [59/189], loss=65.7090
	step [60/189], loss=83.0374
	step [61/189], loss=75.0103
	step [62/189], loss=76.4015
	step [63/189], loss=71.8922
	step [64/189], loss=79.4026
	step [65/189], loss=68.4051
	step [66/189], loss=62.8974
	step [67/189], loss=77.3508
	step [68/189], loss=85.2936
	step [69/189], loss=75.9185
	step [70/189], loss=67.1882
	step [71/189], loss=82.3725
	step [72/189], loss=76.6404
	step [73/189], loss=72.8537
	step [74/189], loss=64.2261
	step [75/189], loss=80.0977
	step [76/189], loss=59.9524
	step [77/189], loss=80.7510
	step [78/189], loss=62.0629
	step [79/189], loss=63.7568
	step [80/189], loss=72.3642
	step [81/189], loss=82.0045
	step [82/189], loss=69.3151
	step [83/189], loss=71.5367
	step [84/189], loss=69.4738
	step [85/189], loss=74.2032
	step [86/189], loss=73.8878
	step [87/189], loss=92.3665
	step [88/189], loss=79.7431
	step [89/189], loss=66.8493
	step [90/189], loss=69.3891
	step [91/189], loss=69.8553
	step [92/189], loss=71.3274
	step [93/189], loss=75.0061
	step [94/189], loss=81.2867
	step [95/189], loss=81.6085
	step [96/189], loss=79.6708
	step [97/189], loss=83.8284
	step [98/189], loss=86.8748
	step [99/189], loss=83.7440
	step [100/189], loss=61.9128
	step [101/189], loss=72.3732
	step [102/189], loss=59.8295
	step [103/189], loss=83.0571
	step [104/189], loss=72.4435
	step [105/189], loss=78.3957
	step [106/189], loss=81.2411
	step [107/189], loss=68.7300
	step [108/189], loss=72.8248
	step [109/189], loss=69.1356
	step [110/189], loss=61.5641
	step [111/189], loss=81.2323
	step [112/189], loss=80.8923
	step [113/189], loss=57.5138
	step [114/189], loss=67.8995
	step [115/189], loss=66.7758
	step [116/189], loss=75.8404
	step [117/189], loss=80.5960
	step [118/189], loss=82.1699
	step [119/189], loss=78.2823
	step [120/189], loss=63.0170
	step [121/189], loss=71.4336
	step [122/189], loss=84.7343
	step [123/189], loss=77.2493
	step [124/189], loss=73.9090
	step [125/189], loss=83.2952
	step [126/189], loss=73.2772
	step [127/189], loss=62.1734
	step [128/189], loss=69.9649
	step [129/189], loss=76.2105
	step [130/189], loss=77.2562
	step [131/189], loss=74.0259
	step [132/189], loss=77.5164
	step [133/189], loss=77.7553
	step [134/189], loss=69.5913
	step [135/189], loss=74.5826
	step [136/189], loss=69.1897
	step [137/189], loss=84.7523
	step [138/189], loss=76.4811
	step [139/189], loss=71.0728
	step [140/189], loss=79.8810
	step [141/189], loss=73.0153
	step [142/189], loss=75.0179
	step [143/189], loss=78.8312
	step [144/189], loss=69.9484
	step [145/189], loss=82.1143
	step [146/189], loss=74.0547
	step [147/189], loss=63.0539
	step [148/189], loss=68.1847
	step [149/189], loss=72.1642
	step [150/189], loss=94.4749
	step [151/189], loss=92.9232
	step [152/189], loss=73.1854
	step [153/189], loss=71.8960
	step [154/189], loss=71.9998
	step [155/189], loss=74.5414
	step [156/189], loss=70.9417
	step [157/189], loss=86.0691
	step [158/189], loss=63.4353
	step [159/189], loss=69.7894
	step [160/189], loss=85.7257
	step [161/189], loss=67.1016
	step [162/189], loss=68.7453
	step [163/189], loss=67.0642
	step [164/189], loss=59.4433
	step [165/189], loss=73.2054
	step [166/189], loss=71.3459
	step [167/189], loss=63.8041
	step [168/189], loss=79.7660
	step [169/189], loss=67.5518
	step [170/189], loss=82.8434
	step [171/189], loss=77.5332
	step [172/189], loss=72.2943
	step [173/189], loss=79.8106
	step [174/189], loss=79.5308
	step [175/189], loss=60.6006
	step [176/189], loss=79.6356
	step [177/189], loss=75.1454
	step [178/189], loss=75.5972
	step [179/189], loss=75.7383
	step [180/189], loss=74.5524
	step [181/189], loss=74.9827
	step [182/189], loss=71.2249
	step [183/189], loss=79.5320
	step [184/189], loss=64.8390
	step [185/189], loss=73.6342
	step [186/189], loss=75.3142
	step [187/189], loss=72.0884
	step [188/189], loss=69.4490
	step [189/189], loss=25.2235
	Evaluating
	loss=0.0086, precision=0.3962, recall=0.8932, f1=0.5489
Training epoch 79
	step [1/189], loss=63.7431
	step [2/189], loss=75.9591
	step [3/189], loss=77.5062
	step [4/189], loss=83.4528
	step [5/189], loss=73.1688
	step [6/189], loss=75.8076
	step [7/189], loss=71.9662
	step [8/189], loss=75.4971
	step [9/189], loss=86.4395
	step [10/189], loss=75.4033
	step [11/189], loss=76.9982
	step [12/189], loss=85.4114
	step [13/189], loss=54.7717
	step [14/189], loss=67.1809
	step [15/189], loss=70.4871
	step [16/189], loss=76.5933
	step [17/189], loss=65.6164
	step [18/189], loss=62.1045
	step [19/189], loss=73.5871
	step [20/189], loss=63.8471
	step [21/189], loss=64.2771
	step [22/189], loss=76.6960
	step [23/189], loss=72.6448
	step [24/189], loss=69.9484
	step [25/189], loss=72.0373
	step [26/189], loss=72.9125
	step [27/189], loss=60.2267
	step [28/189], loss=71.3560
	step [29/189], loss=76.2182
	step [30/189], loss=77.3943
	step [31/189], loss=66.0318
	step [32/189], loss=77.7390
	step [33/189], loss=67.2739
	step [34/189], loss=69.9149
	step [35/189], loss=74.0053
	step [36/189], loss=67.0093
	step [37/189], loss=80.9030
	step [38/189], loss=73.1440
	step [39/189], loss=81.0069
	step [40/189], loss=70.1061
	step [41/189], loss=83.5238
	step [42/189], loss=76.1225
	step [43/189], loss=65.7722
	step [44/189], loss=77.1092
	step [45/189], loss=71.4954
	step [46/189], loss=73.5543
	step [47/189], loss=59.7943
	step [48/189], loss=66.7639
	step [49/189], loss=81.1632
	step [50/189], loss=75.3936
	step [51/189], loss=72.1714
	step [52/189], loss=72.7098
	step [53/189], loss=78.8600
	step [54/189], loss=73.8723
	step [55/189], loss=76.0681
	step [56/189], loss=78.2513
	step [57/189], loss=76.6296
	step [58/189], loss=74.8240
	step [59/189], loss=64.1437
	step [60/189], loss=96.9289
	step [61/189], loss=67.8469
	step [62/189], loss=76.9350
	step [63/189], loss=75.2866
	step [64/189], loss=72.1496
	step [65/189], loss=71.5841
	step [66/189], loss=71.2962
	step [67/189], loss=82.3053
	step [68/189], loss=72.3433
	step [69/189], loss=78.1177
	step [70/189], loss=70.2591
	step [71/189], loss=71.7939
	step [72/189], loss=80.2545
	step [73/189], loss=63.5748
	step [74/189], loss=70.4286
	step [75/189], loss=88.1070
	step [76/189], loss=81.6615
	step [77/189], loss=73.8352
	step [78/189], loss=61.1088
	step [79/189], loss=69.5384
	step [80/189], loss=65.9481
	step [81/189], loss=68.4794
	step [82/189], loss=70.9656
	step [83/189], loss=66.2040
	step [84/189], loss=72.4579
	step [85/189], loss=76.3731
	step [86/189], loss=68.5674
	step [87/189], loss=69.3826
	step [88/189], loss=79.1395
	step [89/189], loss=72.1634
	step [90/189], loss=67.7923
	step [91/189], loss=74.0006
	step [92/189], loss=75.8460
	step [93/189], loss=66.0419
	step [94/189], loss=74.6647
	step [95/189], loss=81.1145
	step [96/189], loss=75.4158
	step [97/189], loss=75.5086
	step [98/189], loss=72.8969
	step [99/189], loss=61.6328
	step [100/189], loss=81.7725
	step [101/189], loss=65.8388
	step [102/189], loss=68.8413
	step [103/189], loss=72.6342
	step [104/189], loss=67.2075
	step [105/189], loss=74.8649
	step [106/189], loss=59.8855
	step [107/189], loss=74.5147
	step [108/189], loss=75.1375
	step [109/189], loss=65.3147
	step [110/189], loss=90.7874
	step [111/189], loss=81.2138
	step [112/189], loss=70.9469
	step [113/189], loss=69.3529
	step [114/189], loss=75.3512
	step [115/189], loss=72.2575
	step [116/189], loss=81.5015
	step [117/189], loss=65.9989
	step [118/189], loss=69.3703
	step [119/189], loss=86.5168
	step [120/189], loss=80.3323
	step [121/189], loss=69.1899
	step [122/189], loss=74.6580
	step [123/189], loss=70.3215
	step [124/189], loss=77.7912
	step [125/189], loss=69.9507
	step [126/189], loss=78.5188
	step [127/189], loss=73.9143
	step [128/189], loss=71.9398
	step [129/189], loss=72.7649
	step [130/189], loss=74.6473
	step [131/189], loss=71.0971
	step [132/189], loss=81.5451
	step [133/189], loss=69.9584
	step [134/189], loss=78.0547
	step [135/189], loss=72.5807
	step [136/189], loss=66.1807
	step [137/189], loss=73.4572
	step [138/189], loss=72.5434
	step [139/189], loss=66.2247
	step [140/189], loss=59.9653
	step [141/189], loss=71.6660
	step [142/189], loss=69.4844
	step [143/189], loss=75.0248
	step [144/189], loss=66.3859
	step [145/189], loss=65.9112
	step [146/189], loss=88.4831
	step [147/189], loss=66.6723
	step [148/189], loss=95.5464
	step [149/189], loss=70.4926
	step [150/189], loss=83.5383
	step [151/189], loss=65.1991
	step [152/189], loss=82.0874
	step [153/189], loss=81.0786
	step [154/189], loss=76.6022
	step [155/189], loss=79.0238
	step [156/189], loss=59.0546
	step [157/189], loss=62.4122
	step [158/189], loss=79.2641
	step [159/189], loss=82.8929
	step [160/189], loss=77.7259
	step [161/189], loss=64.5122
	step [162/189], loss=86.1049
	step [163/189], loss=87.5794
	step [164/189], loss=78.1302
	step [165/189], loss=71.9323
	step [166/189], loss=88.3771
	step [167/189], loss=97.8324
	step [168/189], loss=65.8994
	step [169/189], loss=80.4920
	step [170/189], loss=76.7476
	step [171/189], loss=76.5696
	step [172/189], loss=80.4209
	step [173/189], loss=74.4820
	step [174/189], loss=76.0044
	step [175/189], loss=62.3071
	step [176/189], loss=70.4236
	step [177/189], loss=62.4868
	step [178/189], loss=76.8301
	step [179/189], loss=74.6633
	step [180/189], loss=64.4895
	step [181/189], loss=71.3062
	step [182/189], loss=76.8213
	step [183/189], loss=77.3600
	step [184/189], loss=58.7897
	step [185/189], loss=79.0458
	step [186/189], loss=79.5486
	step [187/189], loss=73.5353
	step [188/189], loss=65.1409
	step [189/189], loss=31.5623
	Evaluating
	loss=0.0076, precision=0.4429, recall=0.8923, f1=0.5920
saving model as: 2_saved_model.pth
Training epoch 80
	step [1/189], loss=80.1101
	step [2/189], loss=68.3631
	step [3/189], loss=86.8546
	step [4/189], loss=86.5039
	step [5/189], loss=71.6080
	step [6/189], loss=73.0570
	step [7/189], loss=92.9240
	step [8/189], loss=77.0693
	step [9/189], loss=56.8913
	step [10/189], loss=76.4618
	step [11/189], loss=75.8889
	step [12/189], loss=68.4427
	step [13/189], loss=69.5352
	step [14/189], loss=87.3335
	step [15/189], loss=64.3935
	step [16/189], loss=78.5328
	step [17/189], loss=68.8968
	step [18/189], loss=79.1580
	step [19/189], loss=58.6038
	step [20/189], loss=78.2467
	step [21/189], loss=82.2725
	step [22/189], loss=54.2763
	step [23/189], loss=70.3642
	step [24/189], loss=72.7413
	step [25/189], loss=63.1311
	step [26/189], loss=78.4839
	step [27/189], loss=74.1333
	step [28/189], loss=83.3532
	step [29/189], loss=76.3856
	step [30/189], loss=69.6757
	step [31/189], loss=67.6469
	step [32/189], loss=86.2679
	step [33/189], loss=62.3797
	step [34/189], loss=70.4370
	step [35/189], loss=77.1249
	step [36/189], loss=66.0068
	step [37/189], loss=76.6592
	step [38/189], loss=62.3980
	step [39/189], loss=83.5301
	step [40/189], loss=71.4666
	step [41/189], loss=78.8149
	step [42/189], loss=75.8260
	step [43/189], loss=64.5757
	step [44/189], loss=79.5442
	step [45/189], loss=64.6137
	step [46/189], loss=82.4769
	step [47/189], loss=83.6782
	step [48/189], loss=75.0415
	step [49/189], loss=74.0868
	step [50/189], loss=59.8690
	step [51/189], loss=68.6528
	step [52/189], loss=69.6040
	step [53/189], loss=69.9449
	step [54/189], loss=64.6289
	step [55/189], loss=73.1326
	step [56/189], loss=77.9504
	step [57/189], loss=84.7084
	step [58/189], loss=64.6697
	step [59/189], loss=79.6822
	step [60/189], loss=75.7148
	step [61/189], loss=70.6304
	step [62/189], loss=58.8346
	step [63/189], loss=77.4914
	step [64/189], loss=81.3315
	step [65/189], loss=69.4711
	step [66/189], loss=69.3235
	step [67/189], loss=75.2587
	step [68/189], loss=70.3351
	step [69/189], loss=59.3594
	step [70/189], loss=69.5627
	step [71/189], loss=84.1520
	step [72/189], loss=60.2250
	step [73/189], loss=77.5309
	step [74/189], loss=71.8130
	step [75/189], loss=71.4563
	step [76/189], loss=73.1650
	step [77/189], loss=69.5471
	step [78/189], loss=57.0398
	step [79/189], loss=72.5759
	step [80/189], loss=70.6157
	step [81/189], loss=75.6875
	step [82/189], loss=77.4103
	step [83/189], loss=71.2951
	step [84/189], loss=74.7163
	step [85/189], loss=66.0755
	step [86/189], loss=81.3727
	step [87/189], loss=79.2102
	step [88/189], loss=69.8062
	step [89/189], loss=67.8262
	step [90/189], loss=66.3716
	step [91/189], loss=83.6980
	step [92/189], loss=64.0943
	step [93/189], loss=66.0407
	step [94/189], loss=68.8258
	step [95/189], loss=76.7508
	step [96/189], loss=71.7606
	step [97/189], loss=74.3404
	step [98/189], loss=85.3482
	step [99/189], loss=70.7674
	step [100/189], loss=77.4805
	step [101/189], loss=77.5378
	step [102/189], loss=76.9692
	step [103/189], loss=75.1248
	step [104/189], loss=78.3717
	step [105/189], loss=69.4445
	step [106/189], loss=97.0654
	step [107/189], loss=74.8820
	step [108/189], loss=71.0823
	step [109/189], loss=64.0767
	step [110/189], loss=75.5716
	step [111/189], loss=72.2273
	step [112/189], loss=64.8236
	step [113/189], loss=78.1285
	step [114/189], loss=77.9796
	step [115/189], loss=59.6439
	step [116/189], loss=61.9480
	step [117/189], loss=81.9203
	step [118/189], loss=82.9716
	step [119/189], loss=80.2135
	step [120/189], loss=77.9690
	step [121/189], loss=67.9528
	step [122/189], loss=86.4414
	step [123/189], loss=78.5384
	step [124/189], loss=79.6047
	step [125/189], loss=85.6864
	step [126/189], loss=80.6831
	step [127/189], loss=73.8339
	step [128/189], loss=68.7638
	step [129/189], loss=58.9365
	step [130/189], loss=82.5387
	step [131/189], loss=64.6037
	step [132/189], loss=72.7440
	step [133/189], loss=72.8464
	step [134/189], loss=79.6678
	step [135/189], loss=66.8949
	step [136/189], loss=71.6614
	step [137/189], loss=67.9797
	step [138/189], loss=91.4169
	step [139/189], loss=69.3173
	step [140/189], loss=75.3181
	step [141/189], loss=79.0733
	step [142/189], loss=81.6768
	step [143/189], loss=82.0998
	step [144/189], loss=65.1121
	step [145/189], loss=70.1607
	step [146/189], loss=78.6699
	step [147/189], loss=71.8144
	step [148/189], loss=68.4326
	step [149/189], loss=75.4225
	step [150/189], loss=70.7354
	step [151/189], loss=65.4688
	step [152/189], loss=65.8907
	step [153/189], loss=75.6251
	step [154/189], loss=71.9246
	step [155/189], loss=81.4198
	step [156/189], loss=69.7853
	step [157/189], loss=79.2455
	step [158/189], loss=76.1406
	step [159/189], loss=61.2736
	step [160/189], loss=84.1896
	step [161/189], loss=78.0296
	step [162/189], loss=68.4878
	step [163/189], loss=72.6291
	step [164/189], loss=70.9547
	step [165/189], loss=73.9936
	step [166/189], loss=65.7451
	step [167/189], loss=81.1403
	step [168/189], loss=75.8405
	step [169/189], loss=76.9425
	step [170/189], loss=73.4601
	step [171/189], loss=75.3721
	step [172/189], loss=76.2917
	step [173/189], loss=88.4140
	step [174/189], loss=68.5972
	step [175/189], loss=69.6605
	step [176/189], loss=77.4718
	step [177/189], loss=63.9981
	step [178/189], loss=80.2339
	step [179/189], loss=84.3566
	step [180/189], loss=65.4251
	step [181/189], loss=92.4472
	step [182/189], loss=82.2993
	step [183/189], loss=59.0500
	step [184/189], loss=66.8196
	step [185/189], loss=81.5949
	step [186/189], loss=67.2436
	step [187/189], loss=68.6811
	step [188/189], loss=62.3106
	step [189/189], loss=21.8080
	Evaluating
	loss=0.0079, precision=0.4361, recall=0.8907, f1=0.5856
Training epoch 81
	step [1/189], loss=74.4775
	step [2/189], loss=78.8362
	step [3/189], loss=69.9383
	step [4/189], loss=74.2715
	step [5/189], loss=55.8759
	step [6/189], loss=81.0531
	step [7/189], loss=76.2710
	step [8/189], loss=61.6483
	step [9/189], loss=72.9743
	step [10/189], loss=71.8110
	step [11/189], loss=71.7912
	step [12/189], loss=80.3261
	step [13/189], loss=78.9713
	step [14/189], loss=69.9128
	step [15/189], loss=70.5675
	step [16/189], loss=63.7313
	step [17/189], loss=62.9334
	step [18/189], loss=64.7800
	step [19/189], loss=63.1559
	step [20/189], loss=65.7666
	step [21/189], loss=65.7412
	step [22/189], loss=76.2779
	step [23/189], loss=70.4836
	step [24/189], loss=90.2641
	step [25/189], loss=72.3586
	step [26/189], loss=90.1418
	step [27/189], loss=69.2597
	step [28/189], loss=79.2916
	step [29/189], loss=65.2364
	step [30/189], loss=67.1749
	step [31/189], loss=65.8519
	step [32/189], loss=67.2094
	step [33/189], loss=81.2815
	step [34/189], loss=76.0230
	step [35/189], loss=75.9792
	step [36/189], loss=69.1994
	step [37/189], loss=74.3693
	step [38/189], loss=70.8059
	step [39/189], loss=71.4481
	step [40/189], loss=89.0579
	step [41/189], loss=69.9873
	step [42/189], loss=61.0640
	step [43/189], loss=67.8728
	step [44/189], loss=71.7943
	step [45/189], loss=77.7299
	step [46/189], loss=70.2917
	step [47/189], loss=80.0596
	step [48/189], loss=89.3144
	step [49/189], loss=78.4330
	step [50/189], loss=84.1195
	step [51/189], loss=74.4866
	step [52/189], loss=74.7052
	step [53/189], loss=67.4855
	step [54/189], loss=79.5659
	step [55/189], loss=74.0473
	step [56/189], loss=63.8538
	step [57/189], loss=66.5758
	step [58/189], loss=55.0032
	step [59/189], loss=67.5544
	step [60/189], loss=84.9139
	step [61/189], loss=69.0290
	step [62/189], loss=80.4895
	step [63/189], loss=65.8765
	step [64/189], loss=71.6946
	step [65/189], loss=67.9110
	step [66/189], loss=83.2462
	step [67/189], loss=71.2329
	step [68/189], loss=82.1809
	step [69/189], loss=76.5307
	step [70/189], loss=54.1479
	step [71/189], loss=61.7426
	step [72/189], loss=62.6956
	step [73/189], loss=73.6540
	step [74/189], loss=64.4702
	step [75/189], loss=73.6041
	step [76/189], loss=67.8158
	step [77/189], loss=66.1771
	step [78/189], loss=70.3435
	step [79/189], loss=80.4948
	step [80/189], loss=72.1059
	step [81/189], loss=75.7721
	step [82/189], loss=86.0254
	step [83/189], loss=66.7240
	step [84/189], loss=72.5828
	step [85/189], loss=77.2155
	step [86/189], loss=75.0051
	step [87/189], loss=84.8474
	step [88/189], loss=77.3866
	step [89/189], loss=90.2117
	step [90/189], loss=66.3907
	step [91/189], loss=70.3591
	step [92/189], loss=68.6431
	step [93/189], loss=77.9035
	step [94/189], loss=76.8368
	step [95/189], loss=71.7020
	step [96/189], loss=89.6491
	step [97/189], loss=75.3874
	step [98/189], loss=68.4873
	step [99/189], loss=92.7565
	step [100/189], loss=74.8357
	step [101/189], loss=82.9414
	step [102/189], loss=59.3941
	step [103/189], loss=70.8781
	step [104/189], loss=64.0387
	step [105/189], loss=81.8075
	step [106/189], loss=66.1937
	step [107/189], loss=66.9087
	step [108/189], loss=80.6559
	step [109/189], loss=78.6533
	step [110/189], loss=80.3468
	step [111/189], loss=85.2229
	step [112/189], loss=69.5617
	step [113/189], loss=74.0547
	step [114/189], loss=64.6501
	step [115/189], loss=68.2258
	step [116/189], loss=70.9733
	step [117/189], loss=83.5012
	step [118/189], loss=64.7746
	step [119/189], loss=64.2881
	step [120/189], loss=87.6119
	step [121/189], loss=66.8345
	step [122/189], loss=76.1160
	step [123/189], loss=86.0577
	step [124/189], loss=76.6430
	step [125/189], loss=68.4317
	step [126/189], loss=88.8754
	step [127/189], loss=63.3279
	step [128/189], loss=80.5869
	step [129/189], loss=90.3296
	step [130/189], loss=84.9382
	step [131/189], loss=81.3940
	step [132/189], loss=65.5377
	step [133/189], loss=71.2449
	step [134/189], loss=80.2890
	step [135/189], loss=82.6583
	step [136/189], loss=80.8149
	step [137/189], loss=66.6257
	step [138/189], loss=58.7717
	step [139/189], loss=64.3501
	step [140/189], loss=59.9466
	step [141/189], loss=59.7313
	step [142/189], loss=80.4091
	step [143/189], loss=69.6999
	step [144/189], loss=69.0266
	step [145/189], loss=78.7550
	step [146/189], loss=65.4775
	step [147/189], loss=68.1389
	step [148/189], loss=68.1531
	step [149/189], loss=75.3469
	step [150/189], loss=68.3065
	step [151/189], loss=89.9870
	step [152/189], loss=67.2996
	step [153/189], loss=70.4276
	step [154/189], loss=70.1236
	step [155/189], loss=76.4421
	step [156/189], loss=75.0019
	step [157/189], loss=62.2618
	step [158/189], loss=66.3877
	step [159/189], loss=70.0819
	step [160/189], loss=89.8974
	step [161/189], loss=77.1194
	step [162/189], loss=91.9983
	step [163/189], loss=76.8389
	step [164/189], loss=66.7769
	step [165/189], loss=61.6718
	step [166/189], loss=75.3677
	step [167/189], loss=79.7205
	step [168/189], loss=77.0415
	step [169/189], loss=81.8618
	step [170/189], loss=79.0317
	step [171/189], loss=68.8784
	step [172/189], loss=85.3218
	step [173/189], loss=88.3151
	step [174/189], loss=74.1118
	step [175/189], loss=75.9840
	step [176/189], loss=74.4670
	step [177/189], loss=79.5215
	step [178/189], loss=78.1557
	step [179/189], loss=82.7113
	step [180/189], loss=60.4126
	step [181/189], loss=67.8497
	step [182/189], loss=65.8987
	step [183/189], loss=85.4762
	step [184/189], loss=78.2136
	step [185/189], loss=68.4274
	step [186/189], loss=73.8895
	step [187/189], loss=58.5114
	step [188/189], loss=85.5842
	step [189/189], loss=24.5013
	Evaluating
	loss=0.0107, precision=0.3288, recall=0.8936, f1=0.4807
Training epoch 82
	step [1/189], loss=57.5116
	step [2/189], loss=77.3270
	step [3/189], loss=72.3122
	step [4/189], loss=71.7746
	step [5/189], loss=57.6553
	step [6/189], loss=80.6973
	step [7/189], loss=69.3974
	step [8/189], loss=67.7137
	step [9/189], loss=63.0369
	step [10/189], loss=62.8547
	step [11/189], loss=77.4862
	step [12/189], loss=68.7181
	step [13/189], loss=62.4443
	step [14/189], loss=73.0212
	step [15/189], loss=83.5402
	step [16/189], loss=70.5686
	step [17/189], loss=81.4288
	step [18/189], loss=72.6409
	step [19/189], loss=69.4210
	step [20/189], loss=74.9215
	step [21/189], loss=65.3061
	step [22/189], loss=70.5175
	step [23/189], loss=55.4661
	step [24/189], loss=75.4026
	step [25/189], loss=74.0983
	step [26/189], loss=71.4906
	step [27/189], loss=67.6693
	step [28/189], loss=80.9230
	step [29/189], loss=73.5117
	step [30/189], loss=80.0053
	step [31/189], loss=68.9874
	step [32/189], loss=74.3325
	step [33/189], loss=71.9845
	step [34/189], loss=68.6103
	step [35/189], loss=74.5250
	step [36/189], loss=65.6156
	step [37/189], loss=70.9428
	step [38/189], loss=69.9304
	step [39/189], loss=84.1440
	step [40/189], loss=64.7041
	step [41/189], loss=71.5363
	step [42/189], loss=76.1355
	step [43/189], loss=76.4399
	step [44/189], loss=60.7415
	step [45/189], loss=72.4936
	step [46/189], loss=72.4120
	step [47/189], loss=69.8406
	step [48/189], loss=76.3064
	step [49/189], loss=78.0204
	step [50/189], loss=59.6809
	step [51/189], loss=81.7527
	step [52/189], loss=84.9675
	step [53/189], loss=69.1562
	step [54/189], loss=78.9872
	step [55/189], loss=55.1381
	step [56/189], loss=92.2972
	step [57/189], loss=71.3309
	step [58/189], loss=81.1270
	step [59/189], loss=72.7048
	step [60/189], loss=67.3281
	step [61/189], loss=70.2621
	step [62/189], loss=75.7107
	step [63/189], loss=78.2081
	step [64/189], loss=77.2996
	step [65/189], loss=63.2877
	step [66/189], loss=65.3513
	step [67/189], loss=56.3137
	step [68/189], loss=71.2928
	step [69/189], loss=69.3396
	step [70/189], loss=66.9981
	step [71/189], loss=78.7302
	step [72/189], loss=70.3688
	step [73/189], loss=81.5190
	step [74/189], loss=68.7006
	step [75/189], loss=75.5425
	step [76/189], loss=68.4538
	step [77/189], loss=79.5143
	step [78/189], loss=71.0593
	step [79/189], loss=67.0027
	step [80/189], loss=79.0244
	step [81/189], loss=72.4649
	step [82/189], loss=68.5395
	step [83/189], loss=79.8426
	step [84/189], loss=61.6345
	step [85/189], loss=81.3283
	step [86/189], loss=72.9765
	step [87/189], loss=74.9002
	step [88/189], loss=68.6283
	step [89/189], loss=85.5661
	step [90/189], loss=72.4022
	step [91/189], loss=71.7231
	step [92/189], loss=83.0502
	step [93/189], loss=69.1152
	step [94/189], loss=70.7528
	step [95/189], loss=75.8303
	step [96/189], loss=77.0767
	step [97/189], loss=81.3780
	step [98/189], loss=76.7850
	step [99/189], loss=79.8812
	step [100/189], loss=72.7016
	step [101/189], loss=64.7047
	step [102/189], loss=72.1199
	step [103/189], loss=69.1717
	step [104/189], loss=71.7419
	step [105/189], loss=74.6139
	step [106/189], loss=75.8319
	step [107/189], loss=69.2946
	step [108/189], loss=68.9877
	step [109/189], loss=82.1699
	step [110/189], loss=60.2687
	step [111/189], loss=62.9900
	step [112/189], loss=77.1073
	step [113/189], loss=64.1131
	step [114/189], loss=82.7836
	step [115/189], loss=80.5480
	step [116/189], loss=74.5482
	step [117/189], loss=68.9638
	step [118/189], loss=85.0349
	step [119/189], loss=72.6717
	step [120/189], loss=74.9908
	step [121/189], loss=73.9902
	step [122/189], loss=72.1674
	step [123/189], loss=64.7809
	step [124/189], loss=80.0226
	step [125/189], loss=77.3967
	step [126/189], loss=78.8967
	step [127/189], loss=73.8769
	step [128/189], loss=77.3926
	step [129/189], loss=74.4817
	step [130/189], loss=67.1727
	step [131/189], loss=64.7113
	step [132/189], loss=80.4536
	step [133/189], loss=65.8303
	step [134/189], loss=67.8189
	step [135/189], loss=60.3473
	step [136/189], loss=89.0513
	step [137/189], loss=66.6959
	step [138/189], loss=76.2445
	step [139/189], loss=79.9841
	step [140/189], loss=81.9510
	step [141/189], loss=78.4075
	step [142/189], loss=79.0297
	step [143/189], loss=79.6444
	step [144/189], loss=64.6885
	step [145/189], loss=61.9380
	step [146/189], loss=70.1673
	step [147/189], loss=78.3573
	step [148/189], loss=65.6978
	step [149/189], loss=66.8135
	step [150/189], loss=71.0263
	step [151/189], loss=79.5968
	step [152/189], loss=61.0082
	step [153/189], loss=69.7038
	step [154/189], loss=73.1920
	step [155/189], loss=79.0649
	step [156/189], loss=69.6825
	step [157/189], loss=76.3243
	step [158/189], loss=72.5499
	step [159/189], loss=73.6992
	step [160/189], loss=65.4965
	step [161/189], loss=63.6302
	step [162/189], loss=70.2467
	step [163/189], loss=69.9952
	step [164/189], loss=89.2298
	step [165/189], loss=58.6558
	step [166/189], loss=65.2271
	step [167/189], loss=72.8787
	step [168/189], loss=60.7954
	step [169/189], loss=88.3391
	step [170/189], loss=83.0385
	step [171/189], loss=64.7900
	step [172/189], loss=75.1631
	step [173/189], loss=88.8355
	step [174/189], loss=75.0197
	step [175/189], loss=87.7820
	step [176/189], loss=70.5890
	step [177/189], loss=70.3800
	step [178/189], loss=75.1073
	step [179/189], loss=69.1321
	step [180/189], loss=79.2997
	step [181/189], loss=78.6662
	step [182/189], loss=70.8706
	step [183/189], loss=69.6423
	step [184/189], loss=63.9971
	step [185/189], loss=93.7281
	step [186/189], loss=76.7824
	step [187/189], loss=75.8515
	step [188/189], loss=64.7876
	step [189/189], loss=29.0468
	Evaluating
	loss=0.0072, precision=0.4514, recall=0.8867, f1=0.5982
saving model as: 2_saved_model.pth
Training epoch 83
	step [1/189], loss=83.3052
	step [2/189], loss=67.1020
	step [3/189], loss=70.0317
	step [4/189], loss=71.8219
	step [5/189], loss=79.2402
	step [6/189], loss=75.5341
	step [7/189], loss=67.3132
	step [8/189], loss=72.8865
	step [9/189], loss=74.2127
	step [10/189], loss=55.1830
	step [11/189], loss=74.7944
	step [12/189], loss=76.2643
	step [13/189], loss=78.4160
	step [14/189], loss=68.3122
	step [15/189], loss=72.2418
	step [16/189], loss=61.5994
	step [17/189], loss=62.3936
	step [18/189], loss=74.7364
	step [19/189], loss=73.7421
	step [20/189], loss=75.0089
	step [21/189], loss=61.4888
	step [22/189], loss=66.4673
	step [23/189], loss=73.7473
	step [24/189], loss=78.5925
	step [25/189], loss=70.4919
	step [26/189], loss=72.9912
	step [27/189], loss=72.3631
	step [28/189], loss=61.6103
	step [29/189], loss=77.5987
	step [30/189], loss=73.8594
	step [31/189], loss=57.2775
	step [32/189], loss=86.2233
	step [33/189], loss=71.4193
	step [34/189], loss=82.5867
	step [35/189], loss=62.9095
	step [36/189], loss=73.6642
	step [37/189], loss=71.8242
	step [38/189], loss=77.4384
	step [39/189], loss=84.5051
	step [40/189], loss=64.7055
	step [41/189], loss=64.1168
	step [42/189], loss=72.6338
	step [43/189], loss=79.1920
	step [44/189], loss=60.1801
	step [45/189], loss=74.7768
	step [46/189], loss=78.6896
	step [47/189], loss=77.5939
	step [48/189], loss=72.2350
	step [49/189], loss=74.6000
	step [50/189], loss=74.4982
	step [51/189], loss=64.2537
	step [52/189], loss=73.5402
	step [53/189], loss=68.3731
	step [54/189], loss=66.8427
	step [55/189], loss=64.6220
	step [56/189], loss=76.2259
	step [57/189], loss=70.3676
	step [58/189], loss=72.1595
	step [59/189], loss=65.9321
	step [60/189], loss=63.8530
	step [61/189], loss=80.3694
	step [62/189], loss=66.1137
	step [63/189], loss=84.1733
	step [64/189], loss=67.0402
	step [65/189], loss=78.6906
	step [66/189], loss=71.7687
	step [67/189], loss=69.9726
	step [68/189], loss=75.9965
	step [69/189], loss=79.2856
	step [70/189], loss=76.7590
	step [71/189], loss=63.8106
	step [72/189], loss=63.8265
	step [73/189], loss=72.4534
	step [74/189], loss=80.6095
	step [75/189], loss=94.7948
	step [76/189], loss=85.4070
	step [77/189], loss=70.5920
	step [78/189], loss=65.4092
	step [79/189], loss=73.2680
	step [80/189], loss=62.7132
	step [81/189], loss=72.1250
	step [82/189], loss=75.7464
	step [83/189], loss=67.5691
	step [84/189], loss=78.8150
	step [85/189], loss=82.6958
	step [86/189], loss=72.1532
	step [87/189], loss=77.0583
	step [88/189], loss=66.6968
	step [89/189], loss=70.8791
	step [90/189], loss=81.8866
	step [91/189], loss=68.8974
	step [92/189], loss=74.7243
	step [93/189], loss=74.5900
	step [94/189], loss=58.4834
	step [95/189], loss=71.0776
	step [96/189], loss=72.7425
	step [97/189], loss=79.3110
	step [98/189], loss=97.1774
	step [99/189], loss=71.0573
	step [100/189], loss=68.2991
	step [101/189], loss=81.8484
	step [102/189], loss=72.8444
	step [103/189], loss=80.9701
	step [104/189], loss=80.7760
	step [105/189], loss=79.4912
	step [106/189], loss=88.0049
	step [107/189], loss=68.9765
	step [108/189], loss=76.0702
	step [109/189], loss=70.8425
	step [110/189], loss=66.4451
	step [111/189], loss=66.5805
	step [112/189], loss=79.0599
	step [113/189], loss=59.1956
	step [114/189], loss=67.1183
	step [115/189], loss=67.6327
	step [116/189], loss=67.8266
	step [117/189], loss=68.9820
	step [118/189], loss=88.5829
	step [119/189], loss=65.9689
	step [120/189], loss=61.9158
	step [121/189], loss=79.0970
	step [122/189], loss=74.0811
	step [123/189], loss=50.9116
	step [124/189], loss=69.7346
	step [125/189], loss=84.6370
	step [126/189], loss=74.3249
	step [127/189], loss=90.9702
	step [128/189], loss=69.6352
	step [129/189], loss=71.7011
	step [130/189], loss=75.8815
	step [131/189], loss=73.8080
	step [132/189], loss=63.6790
	step [133/189], loss=73.0961
	step [134/189], loss=67.4774
	step [135/189], loss=68.7433
	step [136/189], loss=66.6551
	step [137/189], loss=71.4933
	step [138/189], loss=74.8969
	step [139/189], loss=70.7227
	step [140/189], loss=57.7180
	step [141/189], loss=60.8521
	step [142/189], loss=61.1300
	step [143/189], loss=71.3876
	step [144/189], loss=61.1592
	step [145/189], loss=85.1072
	step [146/189], loss=68.9510
	step [147/189], loss=81.5112
	step [148/189], loss=78.1095
	step [149/189], loss=72.6406
	step [150/189], loss=71.8064
	step [151/189], loss=67.4743
	step [152/189], loss=67.1503
	step [153/189], loss=74.5279
	step [154/189], loss=76.6933
	step [155/189], loss=76.7625
	step [156/189], loss=85.0097
	step [157/189], loss=74.4328
	step [158/189], loss=89.9719
	step [159/189], loss=74.0633
	step [160/189], loss=69.2383
	step [161/189], loss=82.7656
	step [162/189], loss=83.8787
	step [163/189], loss=78.1656
	step [164/189], loss=75.6723
	step [165/189], loss=77.9142
	step [166/189], loss=72.7000
	step [167/189], loss=69.3131
	step [168/189], loss=71.7095
	step [169/189], loss=84.4837
	step [170/189], loss=84.1635
	step [171/189], loss=74.4510
	step [172/189], loss=74.0135
	step [173/189], loss=75.1786
	step [174/189], loss=68.1518
	step [175/189], loss=60.0712
	step [176/189], loss=73.1404
	step [177/189], loss=76.7399
	step [178/189], loss=80.4400
	step [179/189], loss=61.4728
	step [180/189], loss=63.9968
	step [181/189], loss=73.2180
	step [182/189], loss=77.6756
	step [183/189], loss=59.9345
	step [184/189], loss=74.8501
	step [185/189], loss=77.3306
	step [186/189], loss=70.4408
	step [187/189], loss=62.4666
	step [188/189], loss=87.5450
	step [189/189], loss=30.8340
	Evaluating
	loss=0.0083, precision=0.3888, recall=0.8772, f1=0.5388
Training epoch 84
	step [1/189], loss=70.1993
	step [2/189], loss=68.1521
	step [3/189], loss=74.3382
	step [4/189], loss=68.9708
	step [5/189], loss=69.5706
	step [6/189], loss=74.8571
	step [7/189], loss=71.1132
	step [8/189], loss=63.9435
	step [9/189], loss=73.0157
	step [10/189], loss=73.7106
	step [11/189], loss=76.3617
	step [12/189], loss=62.2628
	step [13/189], loss=81.3361
	step [14/189], loss=66.4118
	step [15/189], loss=74.8813
	step [16/189], loss=65.3778
	step [17/189], loss=93.7052
	step [18/189], loss=74.1710
	step [19/189], loss=70.3279
	step [20/189], loss=72.2219
	step [21/189], loss=66.2937
	step [22/189], loss=65.0555
	step [23/189], loss=69.5702
	step [24/189], loss=84.8090
	step [25/189], loss=77.4217
	step [26/189], loss=77.0871
	step [27/189], loss=56.5471
	step [28/189], loss=76.4602
	step [29/189], loss=73.0405
	step [30/189], loss=66.6247
	step [31/189], loss=74.2969
	step [32/189], loss=70.1105
	step [33/189], loss=66.1042
	step [34/189], loss=80.6320
	step [35/189], loss=75.8810
	step [36/189], loss=78.8256
	step [37/189], loss=70.3668
	step [38/189], loss=94.3646
	step [39/189], loss=69.8627
	step [40/189], loss=62.0284
	step [41/189], loss=75.6371
	step [42/189], loss=70.0607
	step [43/189], loss=74.6382
	step [44/189], loss=86.1296
	step [45/189], loss=75.3329
	step [46/189], loss=75.5251
	step [47/189], loss=58.2026
	step [48/189], loss=61.7136
	step [49/189], loss=69.3197
	step [50/189], loss=79.1146
	step [51/189], loss=66.9751
	step [52/189], loss=65.0250
	step [53/189], loss=77.9210
	step [54/189], loss=70.2645
	step [55/189], loss=70.4299
	step [56/189], loss=66.2995
	step [57/189], loss=77.2645
	step [58/189], loss=81.8715
	step [59/189], loss=83.9511
	step [60/189], loss=62.9192
	step [61/189], loss=68.1741
	step [62/189], loss=70.4322
	step [63/189], loss=65.2530
	step [64/189], loss=68.3271
	step [65/189], loss=82.8081
	step [66/189], loss=72.5731
	step [67/189], loss=80.2714
	step [68/189], loss=83.7460
	step [69/189], loss=72.6389
	step [70/189], loss=80.6377
	step [71/189], loss=82.7607
	step [72/189], loss=77.8829
	step [73/189], loss=77.4683
	step [74/189], loss=77.3425
	step [75/189], loss=66.1102
	step [76/189], loss=70.7595
	step [77/189], loss=79.2030
	step [78/189], loss=67.6508
	step [79/189], loss=76.6002
	step [80/189], loss=70.5519
	step [81/189], loss=71.0929
	step [82/189], loss=76.5390
	step [83/189], loss=86.8359
	step [84/189], loss=74.4491
	step [85/189], loss=73.6383
	step [86/189], loss=70.5747
	step [87/189], loss=66.1523
	step [88/189], loss=67.0886
	step [89/189], loss=70.2059
	step [90/189], loss=54.4281
	step [91/189], loss=67.7801
	step [92/189], loss=61.6207
	step [93/189], loss=76.6252
	step [94/189], loss=65.5644
	step [95/189], loss=77.0488
	step [96/189], loss=65.1579
	step [97/189], loss=60.0362
	step [98/189], loss=72.8822
	step [99/189], loss=61.8948
	step [100/189], loss=81.3453
	step [101/189], loss=68.6753
	step [102/189], loss=73.1417
	step [103/189], loss=59.0451
	step [104/189], loss=84.8613
	step [105/189], loss=73.6596
	step [106/189], loss=78.1661
	step [107/189], loss=77.4867
	step [108/189], loss=68.9262
	step [109/189], loss=75.1538
	step [110/189], loss=63.6623
	step [111/189], loss=75.9454
	step [112/189], loss=69.4670
	step [113/189], loss=64.9390
	step [114/189], loss=76.1862
	step [115/189], loss=84.6601
	step [116/189], loss=71.6645
	step [117/189], loss=67.2853
	step [118/189], loss=77.9882
	step [119/189], loss=80.6170
	step [120/189], loss=71.3801
	step [121/189], loss=73.2202
	step [122/189], loss=65.1894
	step [123/189], loss=58.4097
	step [124/189], loss=63.8913
	step [125/189], loss=75.4505
	step [126/189], loss=63.9745
	step [127/189], loss=72.8533
	step [128/189], loss=68.3693
	step [129/189], loss=71.9699
	step [130/189], loss=61.8488
	step [131/189], loss=82.3493
	step [132/189], loss=74.5429
	step [133/189], loss=70.7771
	step [134/189], loss=85.6459
	step [135/189], loss=80.7628
	step [136/189], loss=83.3390
	step [137/189], loss=71.0959
	step [138/189], loss=74.5458
	step [139/189], loss=45.5741
	step [140/189], loss=73.3091
	step [141/189], loss=69.9060
	step [142/189], loss=71.9360
	step [143/189], loss=66.8474
	step [144/189], loss=71.5025
	step [145/189], loss=83.1791
	step [146/189], loss=72.4281
	step [147/189], loss=93.0797
	step [148/189], loss=86.2879
	step [149/189], loss=65.1992
	step [150/189], loss=87.8164
	step [151/189], loss=61.9524
	step [152/189], loss=74.4719
	step [153/189], loss=77.5137
	step [154/189], loss=80.8504
	step [155/189], loss=70.4983
	step [156/189], loss=75.9070
	step [157/189], loss=75.0197
	step [158/189], loss=71.5560
	step [159/189], loss=68.4274
	step [160/189], loss=58.7272
	step [161/189], loss=77.9151
	step [162/189], loss=74.5825
	step [163/189], loss=79.0645
	step [164/189], loss=67.7263
	step [165/189], loss=74.0169
	step [166/189], loss=78.5651
	step [167/189], loss=73.5113
	step [168/189], loss=61.4512
	step [169/189], loss=72.9681
	step [170/189], loss=64.7150
	step [171/189], loss=57.3730
	step [172/189], loss=69.6837
	step [173/189], loss=73.9711
	step [174/189], loss=76.7041
	step [175/189], loss=73.6618
	step [176/189], loss=75.2612
	step [177/189], loss=79.1716
	step [178/189], loss=73.7213
	step [179/189], loss=79.2207
	step [180/189], loss=77.3884
	step [181/189], loss=70.5763
	step [182/189], loss=80.5015
	step [183/189], loss=83.1147
	step [184/189], loss=65.0594
	step [185/189], loss=59.7473
	step [186/189], loss=68.6461
	step [187/189], loss=67.7235
	step [188/189], loss=58.2254
	step [189/189], loss=27.6797
	Evaluating
	loss=0.0098, precision=0.3554, recall=0.8903, f1=0.5080
Training epoch 85
	step [1/189], loss=77.8912
	step [2/189], loss=66.3898
	step [3/189], loss=68.2333
	step [4/189], loss=75.4545
	step [5/189], loss=68.5119
	step [6/189], loss=72.2940
	step [7/189], loss=79.8334
	step [8/189], loss=69.2383
	step [9/189], loss=74.6026
	step [10/189], loss=65.4936
	step [11/189], loss=65.0512
	step [12/189], loss=68.1026
	step [13/189], loss=71.4272
	step [14/189], loss=63.5405
	step [15/189], loss=70.5921
	step [16/189], loss=67.4883
	step [17/189], loss=65.1493
	step [18/189], loss=62.3109
	step [19/189], loss=67.0612
	step [20/189], loss=72.0467
	step [21/189], loss=69.3526
	step [22/189], loss=73.3888
	step [23/189], loss=91.0541
	step [24/189], loss=76.5247
	step [25/189], loss=66.5696
	step [26/189], loss=69.2094
	step [27/189], loss=78.2459
	step [28/189], loss=66.4286
	step [29/189], loss=66.5594
	step [30/189], loss=56.7201
	step [31/189], loss=85.7256
	step [32/189], loss=79.0342
	step [33/189], loss=76.0876
	step [34/189], loss=72.8283
	step [35/189], loss=74.5276
	step [36/189], loss=64.7640
	step [37/189], loss=77.9389
	step [38/189], loss=70.0374
	step [39/189], loss=78.1563
	step [40/189], loss=73.4747
	step [41/189], loss=72.7441
	step [42/189], loss=80.5113
	step [43/189], loss=71.9055
	step [44/189], loss=60.3017
	step [45/189], loss=72.1637
	step [46/189], loss=81.2293
	step [47/189], loss=83.6043
	step [48/189], loss=71.6668
	step [49/189], loss=66.8744
	step [50/189], loss=67.9712
	step [51/189], loss=73.3722
	step [52/189], loss=63.8153
	step [53/189], loss=64.2696
	step [54/189], loss=79.0051
	step [55/189], loss=66.5914
	step [56/189], loss=71.1609
	step [57/189], loss=70.0209
	step [58/189], loss=69.3905
	step [59/189], loss=75.2465
	step [60/189], loss=73.9226
	step [61/189], loss=82.6205
	step [62/189], loss=63.9162
	step [63/189], loss=79.1410
	step [64/189], loss=79.5831
	step [65/189], loss=74.4861
	step [66/189], loss=72.2415
	step [67/189], loss=77.1814
	step [68/189], loss=77.6514
	step [69/189], loss=61.2536
	step [70/189], loss=80.5181
	step [71/189], loss=79.1293
	step [72/189], loss=68.0990
	step [73/189], loss=59.7013
	step [74/189], loss=58.9472
	step [75/189], loss=65.8486
	step [76/189], loss=58.7376
	step [77/189], loss=71.5927
	step [78/189], loss=70.8772
	step [79/189], loss=73.2423
	step [80/189], loss=75.1619
	step [81/189], loss=63.1341
	step [82/189], loss=68.4214
	step [83/189], loss=78.6575
	step [84/189], loss=66.4116
	step [85/189], loss=74.7891
	step [86/189], loss=69.0537
	step [87/189], loss=74.2254
	step [88/189], loss=80.2813
	step [89/189], loss=71.8209
	step [90/189], loss=73.3102
	step [91/189], loss=75.8651
	step [92/189], loss=62.1682
	step [93/189], loss=65.4970
	step [94/189], loss=73.7180
	step [95/189], loss=71.3006
	step [96/189], loss=71.5435
	step [97/189], loss=68.7845
	step [98/189], loss=70.8091
	step [99/189], loss=77.2319
	step [100/189], loss=63.1004
	step [101/189], loss=66.4324
	step [102/189], loss=78.5593
	step [103/189], loss=68.1726
	step [104/189], loss=76.8190
	step [105/189], loss=52.3241
	step [106/189], loss=77.2930
	step [107/189], loss=63.9972
	step [108/189], loss=75.3182
	step [109/189], loss=70.1085
	step [110/189], loss=55.7610
	step [111/189], loss=79.6043
	step [112/189], loss=73.8978
	step [113/189], loss=64.7104
	step [114/189], loss=66.2390
	step [115/189], loss=66.5442
	step [116/189], loss=66.8477
	step [117/189], loss=67.5410
	step [118/189], loss=80.8536
	step [119/189], loss=80.1164
	step [120/189], loss=88.3644
	step [121/189], loss=93.4546
	step [122/189], loss=67.3378
	step [123/189], loss=70.7907
	step [124/189], loss=69.8101
	step [125/189], loss=85.6861
	step [126/189], loss=74.2980
	step [127/189], loss=68.7003
	step [128/189], loss=82.7758
	step [129/189], loss=81.7207
	step [130/189], loss=64.6419
	step [131/189], loss=74.3038
	step [132/189], loss=72.4208
	step [133/189], loss=68.0170
	step [134/189], loss=76.5144
	step [135/189], loss=87.0712
	step [136/189], loss=83.5707
	step [137/189], loss=75.8806
	step [138/189], loss=75.7320
	step [139/189], loss=72.9702
	step [140/189], loss=77.7596
	step [141/189], loss=84.2444
	step [142/189], loss=76.5550
	step [143/189], loss=76.3559
	step [144/189], loss=66.6509
	step [145/189], loss=70.6615
	step [146/189], loss=69.4596
	step [147/189], loss=68.3372
	step [148/189], loss=69.9156
	step [149/189], loss=68.7997
	step [150/189], loss=76.0296
	step [151/189], loss=81.0174
	step [152/189], loss=66.9873
	step [153/189], loss=79.4266
	step [154/189], loss=82.2204
	step [155/189], loss=85.8625
	step [156/189], loss=73.0761
	step [157/189], loss=65.1395
	step [158/189], loss=77.6216
	step [159/189], loss=67.3479
	step [160/189], loss=76.9304
	step [161/189], loss=72.1906
	step [162/189], loss=72.1111
	step [163/189], loss=75.4140
	step [164/189], loss=81.5052
	step [165/189], loss=77.2314
	step [166/189], loss=80.2469
	step [167/189], loss=57.5588
	step [168/189], loss=76.2295
	step [169/189], loss=81.7491
	step [170/189], loss=66.2996
	step [171/189], loss=59.5346
	step [172/189], loss=76.8787
	step [173/189], loss=65.6319
	step [174/189], loss=76.7746
	step [175/189], loss=78.4031
	step [176/189], loss=74.9349
	step [177/189], loss=67.5057
	step [178/189], loss=63.6101
	step [179/189], loss=82.4210
	step [180/189], loss=67.5677
	step [181/189], loss=70.5429
	step [182/189], loss=67.4564
	step [183/189], loss=70.5560
	step [184/189], loss=85.7395
	step [185/189], loss=62.0859
	step [186/189], loss=70.2392
	step [187/189], loss=78.0101
	step [188/189], loss=64.5618
	step [189/189], loss=34.6032
	Evaluating
	loss=0.0082, precision=0.3973, recall=0.8832, f1=0.5481
Training epoch 86
	step [1/189], loss=67.3669
	step [2/189], loss=79.3389
	step [3/189], loss=61.6820
	step [4/189], loss=72.6980
	step [5/189], loss=72.9992
	step [6/189], loss=64.5279
	step [7/189], loss=76.0571
	step [8/189], loss=58.2919
	step [9/189], loss=74.7435
	step [10/189], loss=72.9842
	step [11/189], loss=78.2118
	step [12/189], loss=79.3992
	step [13/189], loss=70.5359
	step [14/189], loss=73.1905
	step [15/189], loss=60.7845
	step [16/189], loss=75.9492
	step [17/189], loss=70.9834
	step [18/189], loss=67.0810
	step [19/189], loss=74.6736
	step [20/189], loss=83.9186
	step [21/189], loss=76.1828
	step [22/189], loss=78.5917
	step [23/189], loss=84.7702
	step [24/189], loss=64.7872
	step [25/189], loss=69.9611
	step [26/189], loss=63.8718
	step [27/189], loss=60.7959
	step [28/189], loss=83.4057
	step [29/189], loss=72.3797
	step [30/189], loss=72.7884
	step [31/189], loss=60.6091
	step [32/189], loss=67.4205
	step [33/189], loss=70.7719
	step [34/189], loss=79.7331
	step [35/189], loss=66.6833
	step [36/189], loss=63.9973
	step [37/189], loss=72.3759
	step [38/189], loss=77.6280
	step [39/189], loss=74.3636
	step [40/189], loss=71.0418
	step [41/189], loss=62.2543
	step [42/189], loss=73.6818
	step [43/189], loss=73.4993
	step [44/189], loss=72.6499
	step [45/189], loss=73.7913
	step [46/189], loss=73.6457
	step [47/189], loss=72.9077
	step [48/189], loss=79.0914
	step [49/189], loss=60.7376
	step [50/189], loss=78.3568
	step [51/189], loss=61.8124
	step [52/189], loss=64.1209
	step [53/189], loss=78.0518
	step [54/189], loss=61.0862
	step [55/189], loss=69.1049
	step [56/189], loss=83.3966
	step [57/189], loss=79.8188
	step [58/189], loss=69.4458
	step [59/189], loss=65.8183
	step [60/189], loss=76.3280
	step [61/189], loss=71.2081
	step [62/189], loss=74.4387
	step [63/189], loss=68.2084
	step [64/189], loss=79.6478
	step [65/189], loss=67.8406
	step [66/189], loss=65.2243
	step [67/189], loss=74.3352
	step [68/189], loss=92.8743
	step [69/189], loss=67.6911
	step [70/189], loss=80.0307
	step [71/189], loss=68.8108
	step [72/189], loss=80.3117
	step [73/189], loss=91.9812
	step [74/189], loss=73.3445
	step [75/189], loss=63.0337
	step [76/189], loss=71.5324
	step [77/189], loss=67.9624
	step [78/189], loss=70.1421
	step [79/189], loss=65.9063
	step [80/189], loss=68.4640
	step [81/189], loss=78.1456
	step [82/189], loss=68.0303
	step [83/189], loss=76.7591
	step [84/189], loss=60.3372
	step [85/189], loss=73.7301
	step [86/189], loss=72.2937
	step [87/189], loss=66.9216
	step [88/189], loss=65.6571
	step [89/189], loss=82.9384
	step [90/189], loss=67.3089
	step [91/189], loss=66.1783
	step [92/189], loss=75.3600
	step [93/189], loss=76.2940
	step [94/189], loss=69.6287
	step [95/189], loss=71.8995
	step [96/189], loss=80.0229
	step [97/189], loss=93.6566
	step [98/189], loss=77.6517
	step [99/189], loss=64.4055
	step [100/189], loss=66.5351
	step [101/189], loss=61.3053
	step [102/189], loss=71.4106
	step [103/189], loss=65.4677
	step [104/189], loss=80.7334
	step [105/189], loss=72.6323
	step [106/189], loss=65.3910
	step [107/189], loss=78.6921
	step [108/189], loss=62.3824
	step [109/189], loss=63.8552
	step [110/189], loss=81.9484
	step [111/189], loss=77.2180
	step [112/189], loss=74.9885
	step [113/189], loss=77.0649
	step [114/189], loss=71.0015
	step [115/189], loss=70.8428
	step [116/189], loss=59.0082
	step [117/189], loss=75.1303
	step [118/189], loss=66.2239
	step [119/189], loss=66.1211
	step [120/189], loss=70.6726
	step [121/189], loss=78.7812
	step [122/189], loss=75.3234
	step [123/189], loss=73.9689
	step [124/189], loss=62.7058
	step [125/189], loss=77.3408
	step [126/189], loss=64.6845
	step [127/189], loss=65.4901
	step [128/189], loss=69.7197
	step [129/189], loss=75.2505
	step [130/189], loss=79.3797
	step [131/189], loss=63.6772
	step [132/189], loss=72.1431
	step [133/189], loss=73.9989
	step [134/189], loss=86.8135
	step [135/189], loss=60.6980
	step [136/189], loss=66.2912
	step [137/189], loss=68.6175
	step [138/189], loss=72.6843
	step [139/189], loss=65.4368
	step [140/189], loss=67.4811
	step [141/189], loss=54.5239
	step [142/189], loss=70.2241
	step [143/189], loss=84.3741
	step [144/189], loss=67.9650
	step [145/189], loss=83.2531
	step [146/189], loss=64.3767
	step [147/189], loss=75.0347
	step [148/189], loss=71.6154
	step [149/189], loss=83.9544
	step [150/189], loss=72.5045
	step [151/189], loss=77.2290
	step [152/189], loss=73.0786
	step [153/189], loss=77.8809
	step [154/189], loss=74.1930
	step [155/189], loss=77.9529
	step [156/189], loss=66.3481
	step [157/189], loss=66.8803
	step [158/189], loss=66.7366
	step [159/189], loss=73.7356
	step [160/189], loss=70.5111
	step [161/189], loss=69.4114
	step [162/189], loss=78.7102
	step [163/189], loss=68.8485
	step [164/189], loss=86.6627
	step [165/189], loss=83.1144
	step [166/189], loss=83.7363
	step [167/189], loss=74.2178
	step [168/189], loss=88.4078
	step [169/189], loss=70.6964
	step [170/189], loss=76.6871
	step [171/189], loss=79.3941
	step [172/189], loss=69.5814
	step [173/189], loss=76.3248
	step [174/189], loss=70.5292
	step [175/189], loss=66.2888
	step [176/189], loss=61.8421
	step [177/189], loss=71.3709
	step [178/189], loss=63.5997
	step [179/189], loss=61.6128
	step [180/189], loss=69.4500
	step [181/189], loss=75.2224
	step [182/189], loss=72.5227
	step [183/189], loss=71.6791
	step [184/189], loss=76.9427
	step [185/189], loss=86.3116
	step [186/189], loss=72.4413
	step [187/189], loss=71.5114
	step [188/189], loss=68.6422
	step [189/189], loss=32.5055
	Evaluating
	loss=0.0075, precision=0.4372, recall=0.8775, f1=0.5836
Training epoch 87
	step [1/189], loss=83.3797
	step [2/189], loss=65.3247
	step [3/189], loss=62.2952
	step [4/189], loss=78.8713
	step [5/189], loss=64.5068
	step [6/189], loss=64.4639
	step [7/189], loss=67.1406
	step [8/189], loss=76.0271
	step [9/189], loss=60.8297
	step [10/189], loss=60.6418
	step [11/189], loss=69.3046
	step [12/189], loss=60.1235
	step [13/189], loss=75.0655
	step [14/189], loss=82.4725
	step [15/189], loss=78.8329
	step [16/189], loss=65.5828
	step [17/189], loss=71.8312
	step [18/189], loss=70.0167
	step [19/189], loss=65.9072
	step [20/189], loss=72.5739
	step [21/189], loss=69.5570
	step [22/189], loss=67.9483
	step [23/189], loss=72.3732
	step [24/189], loss=68.0230
	step [25/189], loss=80.4601
	step [26/189], loss=67.2587
	step [27/189], loss=77.5386
	step [28/189], loss=64.0108
	step [29/189], loss=79.2888
	step [30/189], loss=72.3602
	step [31/189], loss=59.9338
	step [32/189], loss=73.3981
	step [33/189], loss=72.6279
	step [34/189], loss=68.6955
	step [35/189], loss=64.6167
	step [36/189], loss=62.0298
	step [37/189], loss=75.5424
	step [38/189], loss=70.5751
	step [39/189], loss=60.7040
	step [40/189], loss=66.2701
	step [41/189], loss=64.8694
	step [42/189], loss=81.0298
	step [43/189], loss=103.9965
	step [44/189], loss=65.5964
	step [45/189], loss=80.3705
	step [46/189], loss=64.5498
	step [47/189], loss=70.8930
	step [48/189], loss=72.3146
	step [49/189], loss=68.3965
	step [50/189], loss=57.7701
	step [51/189], loss=73.5132
	step [52/189], loss=86.4185
	step [53/189], loss=76.8176
	step [54/189], loss=76.8121
	step [55/189], loss=72.1922
	step [56/189], loss=72.2516
	step [57/189], loss=71.9824
	step [58/189], loss=77.8823
	step [59/189], loss=79.2804
	step [60/189], loss=67.5594
	step [61/189], loss=84.0895
	step [62/189], loss=69.0661
	step [63/189], loss=73.6754
	step [64/189], loss=63.7276
	step [65/189], loss=65.5377
	step [66/189], loss=71.5915
	step [67/189], loss=77.9302
	step [68/189], loss=77.6168
	step [69/189], loss=67.1687
	step [70/189], loss=61.8758
	step [71/189], loss=71.5652
	step [72/189], loss=85.5600
	step [73/189], loss=78.3953
	step [74/189], loss=77.2824
	step [75/189], loss=83.0484
	step [76/189], loss=70.3331
	step [77/189], loss=68.7902
	step [78/189], loss=66.8802
	step [79/189], loss=72.0244
	step [80/189], loss=65.1570
	step [81/189], loss=71.8869
	step [82/189], loss=63.5363
	step [83/189], loss=68.4403
	step [84/189], loss=78.7161
	step [85/189], loss=77.4015
	step [86/189], loss=66.3042
	step [87/189], loss=70.0012
	step [88/189], loss=62.6916
	step [89/189], loss=64.5797
	step [90/189], loss=62.5074
	step [91/189], loss=77.2620
	step [92/189], loss=66.4052
	step [93/189], loss=71.8637
	step [94/189], loss=67.8655
	step [95/189], loss=68.7984
	step [96/189], loss=81.8271
	step [97/189], loss=70.4728
	step [98/189], loss=80.2346
	step [99/189], loss=79.7914
	step [100/189], loss=64.9826
	step [101/189], loss=65.8785
	step [102/189], loss=74.4311
	step [103/189], loss=77.6401
	step [104/189], loss=69.7258
	step [105/189], loss=76.5069
	step [106/189], loss=73.2225
	step [107/189], loss=68.9806
	step [108/189], loss=71.3762
	step [109/189], loss=82.2875
	step [110/189], loss=69.5140
	step [111/189], loss=80.8285
	step [112/189], loss=77.8096
	step [113/189], loss=76.6322
	step [114/189], loss=74.4754
	step [115/189], loss=67.0830
	step [116/189], loss=66.8618
	step [117/189], loss=75.3930
	step [118/189], loss=60.9735
	step [119/189], loss=58.8525
	step [120/189], loss=64.1272
	step [121/189], loss=80.2200
	step [122/189], loss=69.3998
	step [123/189], loss=71.7506
	step [124/189], loss=65.0560
	step [125/189], loss=77.5587
	step [126/189], loss=71.0925
	step [127/189], loss=84.2820
	step [128/189], loss=78.0442
	step [129/189], loss=67.8208
	step [130/189], loss=72.6284
	step [131/189], loss=80.5394
	step [132/189], loss=75.1929
	step [133/189], loss=71.8055
	step [134/189], loss=78.0861
	step [135/189], loss=69.8183
	step [136/189], loss=68.3623
	step [137/189], loss=88.3600
	step [138/189], loss=86.9574
	step [139/189], loss=81.4437
	step [140/189], loss=69.1366
	step [141/189], loss=69.6151
	step [142/189], loss=76.0352
	step [143/189], loss=77.5666
	step [144/189], loss=65.4313
	step [145/189], loss=61.5347
	step [146/189], loss=75.3644
	step [147/189], loss=69.8056
	step [148/189], loss=67.7671
	step [149/189], loss=62.6666
	step [150/189], loss=80.5841
	step [151/189], loss=78.0734
	step [152/189], loss=70.7833
	step [153/189], loss=67.9412
	step [154/189], loss=71.2672
	step [155/189], loss=66.2733
	step [156/189], loss=74.8204
	step [157/189], loss=80.7093
	step [158/189], loss=71.5970
	step [159/189], loss=66.8942
	step [160/189], loss=76.6532
	step [161/189], loss=84.3291
	step [162/189], loss=64.1952
	step [163/189], loss=65.1097
	step [164/189], loss=72.0465
	step [165/189], loss=66.6652
	step [166/189], loss=80.0118
	step [167/189], loss=72.6803
	step [168/189], loss=79.3501
	step [169/189], loss=59.3332
	step [170/189], loss=75.7832
	step [171/189], loss=82.6864
	step [172/189], loss=57.3656
	step [173/189], loss=57.2821
	step [174/189], loss=69.0803
	step [175/189], loss=70.9697
	step [176/189], loss=67.5543
	step [177/189], loss=78.5074
	step [178/189], loss=80.1406
	step [179/189], loss=70.5182
	step [180/189], loss=67.6538
	step [181/189], loss=76.6622
	step [182/189], loss=68.0743
	step [183/189], loss=52.4659
	step [184/189], loss=86.6175
	step [185/189], loss=82.5200
	step [186/189], loss=64.6051
	step [187/189], loss=68.6562
	step [188/189], loss=73.3199
	step [189/189], loss=35.9649
	Evaluating
	loss=0.0090, precision=0.3949, recall=0.8816, f1=0.5455
Training epoch 88
	step [1/189], loss=67.2855
	step [2/189], loss=76.5447
	step [3/189], loss=84.4291
	step [4/189], loss=72.8654
	step [5/189], loss=74.7463
	step [6/189], loss=79.1461
	step [7/189], loss=85.7274
	step [8/189], loss=68.5445
	step [9/189], loss=71.8620
	step [10/189], loss=72.6763
	step [11/189], loss=65.7444
	step [12/189], loss=66.4834
	step [13/189], loss=66.3416
	step [14/189], loss=71.8575
	step [15/189], loss=79.2234
	step [16/189], loss=79.1422
	step [17/189], loss=73.6576
	step [18/189], loss=67.5407
	step [19/189], loss=85.6166
	step [20/189], loss=77.4967
	step [21/189], loss=88.9726
	step [22/189], loss=80.4586
	step [23/189], loss=80.5670
	step [24/189], loss=72.7662
	step [25/189], loss=65.4027
	step [26/189], loss=72.5352
	step [27/189], loss=67.5416
	step [28/189], loss=73.8516
	step [29/189], loss=69.8712
	step [30/189], loss=66.7840
	step [31/189], loss=64.5617
	step [32/189], loss=75.1897
	step [33/189], loss=75.5237
	step [34/189], loss=70.9510
	step [35/189], loss=80.4960
	step [36/189], loss=74.7787
	step [37/189], loss=58.9411
	step [38/189], loss=69.7269
	step [39/189], loss=81.2101
	step [40/189], loss=61.4206
	step [41/189], loss=66.2619
	step [42/189], loss=65.4565
	step [43/189], loss=81.2832
	step [44/189], loss=71.3036
	step [45/189], loss=79.2099
	step [46/189], loss=67.4888
	step [47/189], loss=80.7326
	step [48/189], loss=71.2689
	step [49/189], loss=58.7048
	step [50/189], loss=82.4473
	step [51/189], loss=69.1607
	step [52/189], loss=58.0577
	step [53/189], loss=76.3251
	step [54/189], loss=54.1888
	step [55/189], loss=83.7196
	step [56/189], loss=73.3437
	step [57/189], loss=72.1708
	step [58/189], loss=83.7303
	step [59/189], loss=81.4692
	step [60/189], loss=76.3104
	step [61/189], loss=78.6121
	step [62/189], loss=86.0647
	step [63/189], loss=62.9694
	step [64/189], loss=66.7057
	step [65/189], loss=67.9306
	step [66/189], loss=80.9232
	step [67/189], loss=69.5197
	step [68/189], loss=67.6507
	step [69/189], loss=76.7019
	step [70/189], loss=69.4628
	step [71/189], loss=76.0491
	step [72/189], loss=67.5556
	step [73/189], loss=70.8991
	step [74/189], loss=72.1017
	step [75/189], loss=55.5419
	step [76/189], loss=74.1589
	step [77/189], loss=78.7804
	step [78/189], loss=71.3407
	step [79/189], loss=73.0012
	step [80/189], loss=75.5402
	step [81/189], loss=60.9918
	step [82/189], loss=67.2417
	step [83/189], loss=59.3497
	step [84/189], loss=62.9142
	step [85/189], loss=64.8273
	step [86/189], loss=71.1909
	step [87/189], loss=77.4656
	step [88/189], loss=72.5051
	step [89/189], loss=69.5660
	step [90/189], loss=69.3341
	step [91/189], loss=75.3553
	step [92/189], loss=63.9077
	step [93/189], loss=68.5071
	step [94/189], loss=76.3046
	step [95/189], loss=75.4742
	step [96/189], loss=80.3308
	step [97/189], loss=72.9108
	step [98/189], loss=79.2182
	step [99/189], loss=84.8544
	step [100/189], loss=68.8028
	step [101/189], loss=64.2926
	step [102/189], loss=76.0788
	step [103/189], loss=76.5101
	step [104/189], loss=64.0592
	step [105/189], loss=67.2816
	step [106/189], loss=78.6374
	step [107/189], loss=72.4996
	step [108/189], loss=76.3568
	step [109/189], loss=79.0859
	step [110/189], loss=74.1431
	step [111/189], loss=75.4811
	step [112/189], loss=74.4070
	step [113/189], loss=65.2247
	step [114/189], loss=51.5341
	step [115/189], loss=81.2912
	step [116/189], loss=92.2750
	step [117/189], loss=89.6795
	step [118/189], loss=65.7007
	step [119/189], loss=77.5951
	step [120/189], loss=83.8829
	step [121/189], loss=68.8702
	step [122/189], loss=70.3540
	step [123/189], loss=88.2977
	step [124/189], loss=84.9935
	step [125/189], loss=62.7545
	step [126/189], loss=83.4929
	step [127/189], loss=73.9886
	step [128/189], loss=68.4634
	step [129/189], loss=59.8766
	step [130/189], loss=61.3577
	step [131/189], loss=59.8915
	step [132/189], loss=72.9859
	step [133/189], loss=79.4224
	step [134/189], loss=77.6455
	step [135/189], loss=85.1119
	step [136/189], loss=67.0376
	step [137/189], loss=72.1402
	step [138/189], loss=68.2672
	step [139/189], loss=78.0856
	step [140/189], loss=76.4646
	step [141/189], loss=71.5510
	step [142/189], loss=71.2347
	step [143/189], loss=75.7720
	step [144/189], loss=68.6941
	step [145/189], loss=69.4921
	step [146/189], loss=65.8544
	step [147/189], loss=83.0424
	step [148/189], loss=83.3829
	step [149/189], loss=69.6768
	step [150/189], loss=71.3220
	step [151/189], loss=67.5560
	step [152/189], loss=63.8608
	step [153/189], loss=80.1103
	step [154/189], loss=78.3995
	step [155/189], loss=62.5060
	step [156/189], loss=68.4994
	step [157/189], loss=66.6942
	step [158/189], loss=74.3200
	step [159/189], loss=67.0001
	step [160/189], loss=64.5992
	step [161/189], loss=69.1747
	step [162/189], loss=65.5267
	step [163/189], loss=70.1873
	step [164/189], loss=66.8835
	step [165/189], loss=78.9243
	step [166/189], loss=76.2089
	step [167/189], loss=82.5834
	step [168/189], loss=76.9146
	step [169/189], loss=72.0404
	step [170/189], loss=75.4854
	step [171/189], loss=76.3835
	step [172/189], loss=83.2648
	step [173/189], loss=77.2290
	step [174/189], loss=72.1415
	step [175/189], loss=66.9702
	step [176/189], loss=82.0058
	step [177/189], loss=78.0050
	step [178/189], loss=75.0116
	step [179/189], loss=57.8003
	step [180/189], loss=61.5056
	step [181/189], loss=64.3212
	step [182/189], loss=69.9241
	step [183/189], loss=72.6587
	step [184/189], loss=66.4478
	step [185/189], loss=63.9606
	step [186/189], loss=58.9490
	step [187/189], loss=70.7352
	step [188/189], loss=69.3688
	step [189/189], loss=26.4316
	Evaluating
	loss=0.0080, precision=0.4114, recall=0.8874, f1=0.5622
Training epoch 89
	step [1/189], loss=85.6237
	step [2/189], loss=76.9290
	step [3/189], loss=67.6640
	step [4/189], loss=76.5394
	step [5/189], loss=63.9455
	step [6/189], loss=68.0089
	step [7/189], loss=73.6426
	step [8/189], loss=58.7588
	step [9/189], loss=81.0457
	step [10/189], loss=76.7407
	step [11/189], loss=79.0469
	step [12/189], loss=62.2658
	step [13/189], loss=73.6775
	step [14/189], loss=54.7583
	step [15/189], loss=80.0259
	step [16/189], loss=72.4973
	step [17/189], loss=70.3532
	step [18/189], loss=72.9505
	step [19/189], loss=74.2107
	step [20/189], loss=84.7554
	step [21/189], loss=76.9705
	step [22/189], loss=70.4172
	step [23/189], loss=72.0892
	step [24/189], loss=80.3902
	step [25/189], loss=73.5868
	step [26/189], loss=57.9963
	step [27/189], loss=73.6600
	step [28/189], loss=61.6446
	step [29/189], loss=74.2364
	step [30/189], loss=77.5748
	step [31/189], loss=82.0170
	step [32/189], loss=90.4128
	step [33/189], loss=78.1595
	step [34/189], loss=71.4868
	step [35/189], loss=71.3169
	step [36/189], loss=65.7021
	step [37/189], loss=78.4125
	step [38/189], loss=60.4815
	step [39/189], loss=82.2485
	step [40/189], loss=75.4201
	step [41/189], loss=71.5995
	step [42/189], loss=70.0722
	step [43/189], loss=64.4122
	step [44/189], loss=83.8833
	step [45/189], loss=73.8493
	step [46/189], loss=72.2382
	step [47/189], loss=72.5608
	step [48/189], loss=73.5398
	step [49/189], loss=76.3571
	step [50/189], loss=69.9763
	step [51/189], loss=59.0819
	step [52/189], loss=74.2767
	step [53/189], loss=63.1157
	step [54/189], loss=67.4829
	step [55/189], loss=78.0263
	step [56/189], loss=60.8814
	step [57/189], loss=69.6307
	step [58/189], loss=69.0093
	step [59/189], loss=73.8690
	step [60/189], loss=64.6530
	step [61/189], loss=67.3536
	step [62/189], loss=70.2103
	step [63/189], loss=68.4308
	step [64/189], loss=67.3196
	step [65/189], loss=77.2283
	step [66/189], loss=70.0087
	step [67/189], loss=79.4746
	step [68/189], loss=76.6575
	step [69/189], loss=81.5922
	step [70/189], loss=67.1222
	step [71/189], loss=71.8545
	step [72/189], loss=70.0923
	step [73/189], loss=81.2105
	step [74/189], loss=64.0997
	step [75/189], loss=79.4367
	step [76/189], loss=74.1511
	step [77/189], loss=72.3908
	step [78/189], loss=72.0655
	step [79/189], loss=75.8593
	step [80/189], loss=78.0368
	step [81/189], loss=75.8258
	step [82/189], loss=74.5989
	step [83/189], loss=60.8973
	step [84/189], loss=74.9060
	step [85/189], loss=66.2489
	step [86/189], loss=80.5287
	step [87/189], loss=70.4807
	step [88/189], loss=68.5175
	step [89/189], loss=67.6466
	step [90/189], loss=76.3466
	step [91/189], loss=77.2014
	step [92/189], loss=76.8509
	step [93/189], loss=57.4409
	step [94/189], loss=86.0782
	step [95/189], loss=70.9745
	step [96/189], loss=76.3496
	step [97/189], loss=67.2136
	step [98/189], loss=62.1523
	step [99/189], loss=65.8639
	step [100/189], loss=77.7367
	step [101/189], loss=68.9076
	step [102/189], loss=70.6667
	step [103/189], loss=68.5548
	step [104/189], loss=66.2737
	step [105/189], loss=65.3106
	step [106/189], loss=68.4239
	step [107/189], loss=58.3461
	step [108/189], loss=75.4942
	step [109/189], loss=68.0103
	step [110/189], loss=82.2569
	step [111/189], loss=79.4002
	step [112/189], loss=69.9869
	step [113/189], loss=65.7482
	step [114/189], loss=63.9823
	step [115/189], loss=71.7748
	step [116/189], loss=80.3541
	step [117/189], loss=62.1811
	step [118/189], loss=63.2070
	step [119/189], loss=84.1258
	step [120/189], loss=63.5540
	step [121/189], loss=76.2749
	step [122/189], loss=62.8457
	step [123/189], loss=82.6070
	step [124/189], loss=58.1568
	step [125/189], loss=65.1865
	step [126/189], loss=72.2020
	step [127/189], loss=67.1973
	step [128/189], loss=61.5219
	step [129/189], loss=67.7605
	step [130/189], loss=75.8647
	step [131/189], loss=73.6876
	step [132/189], loss=70.7527
	step [133/189], loss=76.2470
	step [134/189], loss=69.7335
	step [135/189], loss=67.9215
	step [136/189], loss=65.1819
	step [137/189], loss=66.8113
	step [138/189], loss=82.0854
	step [139/189], loss=63.9033
	step [140/189], loss=64.4165
	step [141/189], loss=78.3193
	step [142/189], loss=82.3013
	step [143/189], loss=56.9361
	step [144/189], loss=70.4654
	step [145/189], loss=69.5267
	step [146/189], loss=75.5976
	step [147/189], loss=80.0049
	step [148/189], loss=68.1047
	step [149/189], loss=79.1940
	step [150/189], loss=65.0600
	step [151/189], loss=74.7436
	step [152/189], loss=75.9551
	step [153/189], loss=69.5108
	step [154/189], loss=79.9391
	step [155/189], loss=70.9552
	step [156/189], loss=69.7988
	step [157/189], loss=78.1767
	step [158/189], loss=78.0004
	step [159/189], loss=75.4483
	step [160/189], loss=55.1970
	step [161/189], loss=70.8660
	step [162/189], loss=61.0068
	step [163/189], loss=78.2475
	step [164/189], loss=67.9045
	step [165/189], loss=67.3559
	step [166/189], loss=77.0245
	step [167/189], loss=76.6670
	step [168/189], loss=61.4396
	step [169/189], loss=78.2981
	step [170/189], loss=76.6540
	step [171/189], loss=65.5197
	step [172/189], loss=70.1132
	step [173/189], loss=65.7700
	step [174/189], loss=57.2266
	step [175/189], loss=72.0419
	step [176/189], loss=70.2960
	step [177/189], loss=77.9696
	step [178/189], loss=65.5486
	step [179/189], loss=75.3458
	step [180/189], loss=73.6702
	step [181/189], loss=64.7340
	step [182/189], loss=81.6810
	step [183/189], loss=94.1606
	step [184/189], loss=62.5642
	step [185/189], loss=71.7184
	step [186/189], loss=68.6657
	step [187/189], loss=63.1870
	step [188/189], loss=68.0670
	step [189/189], loss=33.3005
	Evaluating
	loss=0.0084, precision=0.3898, recall=0.8897, f1=0.5421
Training epoch 90
	step [1/189], loss=73.4479
	step [2/189], loss=82.3231
	step [3/189], loss=73.9626
	step [4/189], loss=69.6843
	step [5/189], loss=84.5268
	step [6/189], loss=73.3278
	step [7/189], loss=65.2626
	step [8/189], loss=67.2647
	step [9/189], loss=73.5733
	step [10/189], loss=73.6879
	step [11/189], loss=72.9296
	step [12/189], loss=62.2498
	step [13/189], loss=57.4831
	step [14/189], loss=82.8596
	step [15/189], loss=66.3532
	step [16/189], loss=83.5297
	step [17/189], loss=71.5443
	step [18/189], loss=81.1929
	step [19/189], loss=57.6571
	step [20/189], loss=73.1820
	step [21/189], loss=68.4564
	step [22/189], loss=76.8165
	step [23/189], loss=64.9821
	step [24/189], loss=59.6421
	step [25/189], loss=63.0729
	step [26/189], loss=68.4915
	step [27/189], loss=66.2976
	step [28/189], loss=64.7087
	step [29/189], loss=67.6149
	step [30/189], loss=57.1547
	step [31/189], loss=70.8959
	step [32/189], loss=65.7563
	step [33/189], loss=78.3398
	step [34/189], loss=68.7322
	step [35/189], loss=72.9616
	step [36/189], loss=83.1800
	step [37/189], loss=64.9846
	step [38/189], loss=68.4687
	step [39/189], loss=70.0704
	step [40/189], loss=71.3833
	step [41/189], loss=72.0880
	step [42/189], loss=79.9538
	step [43/189], loss=74.4286
	step [44/189], loss=67.5222
	step [45/189], loss=78.2895
	step [46/189], loss=69.2477
	step [47/189], loss=67.6541
	step [48/189], loss=69.7221
	step [49/189], loss=81.0398
	step [50/189], loss=66.4405
	step [51/189], loss=78.9215
	step [52/189], loss=72.5213
	step [53/189], loss=77.1610
	step [54/189], loss=68.7600
	step [55/189], loss=77.6964
	step [56/189], loss=52.9757
	step [57/189], loss=77.1710
	step [58/189], loss=71.5862
	step [59/189], loss=72.0098
	step [60/189], loss=77.0880
	step [61/189], loss=62.5115
	step [62/189], loss=68.1755
	step [63/189], loss=65.1999
	step [64/189], loss=76.9484
	step [65/189], loss=80.8698
	step [66/189], loss=70.5432
	step [67/189], loss=65.3113
	step [68/189], loss=66.0644
	step [69/189], loss=70.2435
	step [70/189], loss=62.0961
	step [71/189], loss=67.4710
	step [72/189], loss=64.7235
	step [73/189], loss=70.9979
	step [74/189], loss=73.9224
	step [75/189], loss=67.7424
	step [76/189], loss=54.4378
	step [77/189], loss=73.6189
	step [78/189], loss=65.8713
	step [79/189], loss=59.5904
	step [80/189], loss=69.6125
	step [81/189], loss=69.6818
	step [82/189], loss=83.2478
	step [83/189], loss=75.5031
	step [84/189], loss=77.8316
	step [85/189], loss=71.3816
	step [86/189], loss=69.6206
	step [87/189], loss=70.5859
	step [88/189], loss=67.6431
	step [89/189], loss=71.8714
	step [90/189], loss=75.5101
	step [91/189], loss=80.6015
	step [92/189], loss=70.2797
	step [93/189], loss=71.5277
	step [94/189], loss=63.5451
	step [95/189], loss=63.8056
	step [96/189], loss=79.7013
	step [97/189], loss=70.4861
	step [98/189], loss=68.8721
	step [99/189], loss=73.6929
	step [100/189], loss=67.0131
	step [101/189], loss=69.3769
	step [102/189], loss=69.3115
	step [103/189], loss=74.2498
	step [104/189], loss=58.8625
	step [105/189], loss=84.2840
	step [106/189], loss=83.6009
	step [107/189], loss=74.6354
	step [108/189], loss=79.5049
	step [109/189], loss=67.3195
	step [110/189], loss=74.3991
	step [111/189], loss=68.8959
	step [112/189], loss=72.7279
	step [113/189], loss=89.2587
	step [114/189], loss=69.4648
	step [115/189], loss=70.3996
	step [116/189], loss=73.0821
	step [117/189], loss=84.1686
	step [118/189], loss=66.9496
	step [119/189], loss=64.5637
	step [120/189], loss=62.0253
	step [121/189], loss=81.7622
	step [122/189], loss=79.8225
	step [123/189], loss=72.4924
	step [124/189], loss=71.1103
	step [125/189], loss=67.3405
	step [126/189], loss=72.0402
	step [127/189], loss=60.1011
	step [128/189], loss=68.6643
	step [129/189], loss=70.1496
	step [130/189], loss=62.3366
	step [131/189], loss=63.3077
	step [132/189], loss=83.5053
	step [133/189], loss=71.7080
	step [134/189], loss=68.3818
	step [135/189], loss=80.8239
	step [136/189], loss=67.3803
	step [137/189], loss=66.6258
	step [138/189], loss=85.0434
	step [139/189], loss=78.2076
	step [140/189], loss=71.7821
	step [141/189], loss=83.1109
	step [142/189], loss=81.8432
	step [143/189], loss=78.7089
	step [144/189], loss=71.5323
	step [145/189], loss=72.0527
	step [146/189], loss=65.6752
	step [147/189], loss=71.2711
	step [148/189], loss=71.0472
	step [149/189], loss=74.9333
	step [150/189], loss=66.8176
	step [151/189], loss=56.1017
	step [152/189], loss=72.4658
	step [153/189], loss=73.1055
	step [154/189], loss=73.0309
	step [155/189], loss=75.8650
	step [156/189], loss=64.8912
	step [157/189], loss=78.6197
	step [158/189], loss=79.7383
	step [159/189], loss=88.0178
	step [160/189], loss=67.3045
	step [161/189], loss=76.3553
	step [162/189], loss=66.6639
	step [163/189], loss=66.5782
	step [164/189], loss=81.1814
	step [165/189], loss=60.1494
	step [166/189], loss=73.4217
	step [167/189], loss=58.0748
	step [168/189], loss=57.7324
	step [169/189], loss=83.1692
	step [170/189], loss=89.9752
	step [171/189], loss=67.6528
	step [172/189], loss=89.3104
	step [173/189], loss=76.2716
	step [174/189], loss=57.8504
	step [175/189], loss=61.8914
	step [176/189], loss=66.1044
	step [177/189], loss=78.4629
	step [178/189], loss=65.2895
	step [179/189], loss=66.2031
	step [180/189], loss=66.4196
	step [181/189], loss=70.3234
	step [182/189], loss=70.4876
	step [183/189], loss=55.1046
	step [184/189], loss=81.0341
	step [185/189], loss=68.8886
	step [186/189], loss=64.2332
	step [187/189], loss=68.8477
	step [188/189], loss=61.9820
	step [189/189], loss=27.5036
	Evaluating
	loss=0.0088, precision=0.3829, recall=0.8933, f1=0.5361
Training epoch 91
	step [1/189], loss=70.1312
	step [2/189], loss=71.6674
	step [3/189], loss=69.4951
	step [4/189], loss=66.0974
	step [5/189], loss=74.8104
	step [6/189], loss=66.8498
	step [7/189], loss=54.1589
	step [8/189], loss=55.8920
	step [9/189], loss=78.4081
	step [10/189], loss=68.5072
	step [11/189], loss=78.1513
	step [12/189], loss=66.9104
	step [13/189], loss=66.2205
	step [14/189], loss=65.6676
	step [15/189], loss=71.4141
	step [16/189], loss=69.4371
	step [17/189], loss=63.9388
	step [18/189], loss=73.8664
	step [19/189], loss=73.8203
	step [20/189], loss=74.7100
	step [21/189], loss=60.1108
	step [22/189], loss=56.4488
	step [23/189], loss=64.5190
	step [24/189], loss=66.0182
	step [25/189], loss=77.3358
	step [26/189], loss=74.0661
	step [27/189], loss=54.3721
	step [28/189], loss=69.0042
	step [29/189], loss=70.0284
	step [30/189], loss=87.4896
	step [31/189], loss=64.9275
	step [32/189], loss=71.2443
	step [33/189], loss=78.3835
	step [34/189], loss=71.2772
	step [35/189], loss=69.2944
	step [36/189], loss=65.9738
	step [37/189], loss=74.7874
	step [38/189], loss=67.7040
	step [39/189], loss=63.2879
	step [40/189], loss=73.0814
	step [41/189], loss=59.4953
	step [42/189], loss=77.0695
	step [43/189], loss=68.2638
	step [44/189], loss=63.6568
	step [45/189], loss=76.2854
	step [46/189], loss=78.8086
	step [47/189], loss=64.3454
	step [48/189], loss=72.9437
	step [49/189], loss=68.5641
	step [50/189], loss=64.4508
	step [51/189], loss=64.4462
	step [52/189], loss=62.0400
	step [53/189], loss=75.9864
	step [54/189], loss=70.0314
	step [55/189], loss=80.5312
	step [56/189], loss=74.1854
	step [57/189], loss=84.3004
	step [58/189], loss=66.6026
	step [59/189], loss=74.0380
	step [60/189], loss=78.9580
	step [61/189], loss=71.3170
	step [62/189], loss=61.6859
	step [63/189], loss=65.8348
	step [64/189], loss=85.7789
	step [65/189], loss=72.9336
	step [66/189], loss=86.0902
	step [67/189], loss=79.3853
	step [68/189], loss=72.4807
	step [69/189], loss=67.9214
	step [70/189], loss=63.4757
	step [71/189], loss=76.2071
	step [72/189], loss=68.5998
	step [73/189], loss=73.8062
	step [74/189], loss=68.5563
	step [75/189], loss=62.3547
	step [76/189], loss=85.9986
	step [77/189], loss=79.9235
	step [78/189], loss=71.9116
	step [79/189], loss=64.2821
	step [80/189], loss=78.2214
	step [81/189], loss=65.5486
	step [82/189], loss=65.9699
	step [83/189], loss=66.0715
	step [84/189], loss=84.9096
	step [85/189], loss=79.1874
	step [86/189], loss=85.1810
	step [87/189], loss=75.1062
	step [88/189], loss=73.9816
	step [89/189], loss=84.9801
	step [90/189], loss=66.1696
	step [91/189], loss=78.7119
	step [92/189], loss=59.6104
	step [93/189], loss=64.4063
	step [94/189], loss=75.1202
	step [95/189], loss=71.5581
	step [96/189], loss=72.4615
	step [97/189], loss=88.6048
	step [98/189], loss=74.3144
	step [99/189], loss=62.7828
	step [100/189], loss=67.3796
	step [101/189], loss=71.4591
	step [102/189], loss=68.1555
	step [103/189], loss=63.8121
	step [104/189], loss=80.6034
	step [105/189], loss=66.4930
	step [106/189], loss=52.2179
	step [107/189], loss=78.9478
	step [108/189], loss=63.5287
	step [109/189], loss=75.1516
	step [110/189], loss=66.1338
	step [111/189], loss=69.6964
	step [112/189], loss=59.4607
	step [113/189], loss=80.3760
	step [114/189], loss=61.1076
	step [115/189], loss=75.4630
	step [116/189], loss=79.1657
	step [117/189], loss=73.3357
	step [118/189], loss=75.7611
	step [119/189], loss=59.3240
	step [120/189], loss=70.5284
	step [121/189], loss=74.1390
	step [122/189], loss=75.6481
	step [123/189], loss=65.3214
	step [124/189], loss=85.3671
	step [125/189], loss=73.9809
	step [126/189], loss=63.7232
	step [127/189], loss=67.5501
	step [128/189], loss=82.9181
	step [129/189], loss=79.2451
	step [130/189], loss=74.5236
	step [131/189], loss=67.3992
	step [132/189], loss=74.7041
	step [133/189], loss=74.0387
	step [134/189], loss=75.7162
	step [135/189], loss=66.8722
	step [136/189], loss=75.4010
	step [137/189], loss=74.7664
	step [138/189], loss=75.3093
	step [139/189], loss=71.1565
	step [140/189], loss=70.2265
	step [141/189], loss=66.2255
	step [142/189], loss=72.8080
	step [143/189], loss=81.9338
	step [144/189], loss=79.1141
	step [145/189], loss=64.1958
	step [146/189], loss=78.1472
	step [147/189], loss=64.3997
	step [148/189], loss=70.2626
	step [149/189], loss=62.7637
	step [150/189], loss=70.5940
	step [151/189], loss=75.5338
	step [152/189], loss=74.8140
	step [153/189], loss=70.4767
	step [154/189], loss=73.1607
	step [155/189], loss=72.8944
	step [156/189], loss=69.5174
	step [157/189], loss=69.3543
	step [158/189], loss=72.8811
	step [159/189], loss=68.1586
	step [160/189], loss=72.6078
	step [161/189], loss=70.4423
	step [162/189], loss=66.2925
	step [163/189], loss=68.6491
	step [164/189], loss=68.0457
	step [165/189], loss=75.5603
	step [166/189], loss=50.3265
	step [167/189], loss=62.3263
	step [168/189], loss=78.7548
	step [169/189], loss=74.7862
	step [170/189], loss=67.9037
	step [171/189], loss=67.2140
	step [172/189], loss=74.6461
	step [173/189], loss=80.9064
	step [174/189], loss=74.2739
	step [175/189], loss=85.2857
	step [176/189], loss=76.0635
	step [177/189], loss=80.8077
	step [178/189], loss=58.2563
	step [179/189], loss=63.0347
	step [180/189], loss=61.6814
	step [181/189], loss=66.9772
	step [182/189], loss=72.0208
	step [183/189], loss=70.4175
	step [184/189], loss=63.3152
	step [185/189], loss=64.1065
	step [186/189], loss=79.6519
	step [187/189], loss=79.4837
	step [188/189], loss=81.2584
	step [189/189], loss=21.5660
	Evaluating
	loss=0.0091, precision=0.3615, recall=0.8975, f1=0.5154
Training epoch 92
	step [1/189], loss=69.9420
	step [2/189], loss=75.2680
	step [3/189], loss=70.7442
	step [4/189], loss=68.2957
	step [5/189], loss=72.1755
	step [6/189], loss=64.4148
	step [7/189], loss=72.2572
	step [8/189], loss=70.6971
	step [9/189], loss=67.7585
	step [10/189], loss=66.9820
	step [11/189], loss=57.7849
	step [12/189], loss=54.6282
	step [13/189], loss=71.6192
	step [14/189], loss=71.9039
	step [15/189], loss=65.0392
	step [16/189], loss=74.7994
	step [17/189], loss=67.4638
	step [18/189], loss=74.7810
	step [19/189], loss=53.8365
	step [20/189], loss=78.3638
	step [21/189], loss=65.2122
	step [22/189], loss=69.5682
	step [23/189], loss=68.8230
	step [24/189], loss=72.0950
	step [25/189], loss=58.8935
	step [26/189], loss=75.9010
	step [27/189], loss=65.2700
	step [28/189], loss=72.8138
	step [29/189], loss=64.7327
	step [30/189], loss=60.5130
	step [31/189], loss=79.5611
	step [32/189], loss=89.2454
	step [33/189], loss=66.8930
	step [34/189], loss=61.7023
	step [35/189], loss=81.5050
	step [36/189], loss=79.5265
	step [37/189], loss=65.9613
	step [38/189], loss=82.1442
	step [39/189], loss=68.9041
	step [40/189], loss=64.7061
	step [41/189], loss=73.9178
	step [42/189], loss=71.5394
	step [43/189], loss=74.3149
	step [44/189], loss=82.3075
	step [45/189], loss=63.5343
	step [46/189], loss=78.8389
	step [47/189], loss=66.1839
	step [48/189], loss=62.9938
	step [49/189], loss=64.7808
	step [50/189], loss=74.8543
	step [51/189], loss=65.7083
	step [52/189], loss=85.5314
	step [53/189], loss=72.0204
	step [54/189], loss=65.5564
	step [55/189], loss=73.1553
	step [56/189], loss=64.5103
	step [57/189], loss=65.9361
	step [58/189], loss=69.7805
	step [59/189], loss=74.8586
	step [60/189], loss=66.4851
	step [61/189], loss=79.0103
	step [62/189], loss=61.3566
	step [63/189], loss=65.7029
	step [64/189], loss=75.9259
	step [65/189], loss=57.9170
	step [66/189], loss=68.5078
	step [67/189], loss=82.8319
	step [68/189], loss=81.9601
	step [69/189], loss=72.5536
	step [70/189], loss=76.4542
	step [71/189], loss=67.4688
	step [72/189], loss=80.4886
	step [73/189], loss=62.4720
	step [74/189], loss=73.4781
	step [75/189], loss=65.6775
	step [76/189], loss=72.3542
	step [77/189], loss=73.4559
	step [78/189], loss=70.8828
	step [79/189], loss=77.2832
	step [80/189], loss=73.8881
	step [81/189], loss=82.1857
	step [82/189], loss=59.7069
	step [83/189], loss=74.2594
	step [84/189], loss=61.6071
	step [85/189], loss=57.6272
	step [86/189], loss=58.1041
	step [87/189], loss=74.5627
	step [88/189], loss=84.5024
	step [89/189], loss=78.4563
	step [90/189], loss=75.0828
	step [91/189], loss=70.5298
	step [92/189], loss=68.2748
	step [93/189], loss=70.6579
	step [94/189], loss=69.2541
	step [95/189], loss=59.6641
	step [96/189], loss=53.1414
	step [97/189], loss=72.5639
	step [98/189], loss=65.4592
	step [99/189], loss=60.2778
	step [100/189], loss=76.5484
	step [101/189], loss=73.0989
	step [102/189], loss=68.6244
	step [103/189], loss=78.1194
	step [104/189], loss=68.5185
	step [105/189], loss=93.0127
	step [106/189], loss=68.0182
	step [107/189], loss=72.9437
	step [108/189], loss=76.0930
	step [109/189], loss=80.8145
	step [110/189], loss=77.3701
	step [111/189], loss=74.6109
	step [112/189], loss=65.8806
	step [113/189], loss=74.8340
	step [114/189], loss=87.9129
	step [115/189], loss=71.2150
	step [116/189], loss=75.1127
	step [117/189], loss=67.4319
	step [118/189], loss=80.1340
	step [119/189], loss=57.5997
	step [120/189], loss=71.5384
	step [121/189], loss=64.7405
	step [122/189], loss=78.2855
	step [123/189], loss=73.0782
	step [124/189], loss=85.8775
	step [125/189], loss=70.0167
	step [126/189], loss=74.0545
	step [127/189], loss=66.2318
	step [128/189], loss=76.1906
	step [129/189], loss=60.8592
	step [130/189], loss=63.4921
	step [131/189], loss=68.4526
	step [132/189], loss=65.3755
	step [133/189], loss=69.9268
	step [134/189], loss=68.6391
	step [135/189], loss=65.0480
	step [136/189], loss=62.5441
	step [137/189], loss=72.6856
	step [138/189], loss=53.2827
	step [139/189], loss=61.3346
	step [140/189], loss=73.0652
	step [141/189], loss=74.3269
	step [142/189], loss=69.0207
	step [143/189], loss=72.6937
	step [144/189], loss=71.3515
	step [145/189], loss=78.9993
	step [146/189], loss=60.2669
	step [147/189], loss=72.1087
	step [148/189], loss=67.3422
	step [149/189], loss=76.8973
	step [150/189], loss=56.1976
	step [151/189], loss=81.8764
	step [152/189], loss=75.9642
	step [153/189], loss=74.2033
	step [154/189], loss=80.7299
	step [155/189], loss=71.3547
	step [156/189], loss=81.5262
	step [157/189], loss=76.6846
	step [158/189], loss=58.8588
	step [159/189], loss=84.8321
	step [160/189], loss=73.4928
	step [161/189], loss=72.2044
	step [162/189], loss=64.3712
	step [163/189], loss=67.6284
	step [164/189], loss=76.2970
	step [165/189], loss=65.2384
	step [166/189], loss=70.7800
	step [167/189], loss=80.4405
	step [168/189], loss=80.7459
	step [169/189], loss=61.3571
	step [170/189], loss=72.2893
	step [171/189], loss=74.1345
	step [172/189], loss=77.3822
	step [173/189], loss=71.5366
	step [174/189], loss=70.5017
	step [175/189], loss=62.5025
	step [176/189], loss=75.3561
	step [177/189], loss=70.6908
	step [178/189], loss=65.7770
	step [179/189], loss=69.2157
	step [180/189], loss=80.9754
	step [181/189], loss=65.0166
	step [182/189], loss=69.0040
	step [183/189], loss=58.7313
	step [184/189], loss=67.9344
	step [185/189], loss=75.3194
	step [186/189], loss=71.1684
	step [187/189], loss=67.7275
	step [188/189], loss=68.5208
	step [189/189], loss=27.6771
	Evaluating
	loss=0.0075, precision=0.4241, recall=0.8869, f1=0.5738
Training epoch 93
	step [1/189], loss=60.7773
	step [2/189], loss=61.7949
	step [3/189], loss=64.5673
	step [4/189], loss=70.6685
	step [5/189], loss=69.1974
	step [6/189], loss=74.3190
	step [7/189], loss=87.2100
	step [8/189], loss=65.5906
	step [9/189], loss=62.6400
	step [10/189], loss=67.9581
	step [11/189], loss=63.5127
	step [12/189], loss=76.2173
	step [13/189], loss=65.7805
	step [14/189], loss=64.1047
	step [15/189], loss=71.7350
	step [16/189], loss=69.9585
	step [17/189], loss=59.4167
	step [18/189], loss=62.6533
	step [19/189], loss=70.5642
	step [20/189], loss=70.6909
	step [21/189], loss=84.0719
	step [22/189], loss=76.2899
	step [23/189], loss=58.0713
	step [24/189], loss=76.3654
	step [25/189], loss=71.8031
	step [26/189], loss=70.4689
	step [27/189], loss=75.8867
	step [28/189], loss=81.9576
	step [29/189], loss=71.3815
	step [30/189], loss=66.0158
	step [31/189], loss=75.0838
	step [32/189], loss=70.4969
	step [33/189], loss=73.7797
	step [34/189], loss=63.9521
	step [35/189], loss=71.2012
	step [36/189], loss=62.4113
	step [37/189], loss=96.6939
	step [38/189], loss=71.2068
	step [39/189], loss=77.1174
	step [40/189], loss=67.8115
	step [41/189], loss=67.3851
	step [42/189], loss=70.2704
	step [43/189], loss=67.8452
	step [44/189], loss=70.0090
	step [45/189], loss=79.8743
	step [46/189], loss=67.5532
	step [47/189], loss=71.2755
	step [48/189], loss=65.7034
	step [49/189], loss=78.5429
	step [50/189], loss=75.3575
	step [51/189], loss=68.8159
	step [52/189], loss=71.7119
	step [53/189], loss=75.9056
	step [54/189], loss=60.6588
	step [55/189], loss=76.6413
	step [56/189], loss=71.3800
	step [57/189], loss=56.9675
	step [58/189], loss=75.9107
	step [59/189], loss=69.7689
	step [60/189], loss=67.4376
	step [61/189], loss=57.8607
	step [62/189], loss=72.5809
	step [63/189], loss=69.7679
	step [64/189], loss=85.1985
	step [65/189], loss=60.1781
	step [66/189], loss=63.9611
	step [67/189], loss=79.9845
	step [68/189], loss=64.1240
	step [69/189], loss=74.1687
	step [70/189], loss=66.8790
	step [71/189], loss=75.7075
	step [72/189], loss=73.4166
	step [73/189], loss=69.4740
	step [74/189], loss=73.9636
	step [75/189], loss=65.3833
	step [76/189], loss=66.3445
	step [77/189], loss=85.3483
	step [78/189], loss=73.8419
	step [79/189], loss=70.8953
	step [80/189], loss=71.0843
	step [81/189], loss=61.0072
	step [82/189], loss=76.9112
	step [83/189], loss=74.8433
	step [84/189], loss=77.3121
	step [85/189], loss=70.2121
	step [86/189], loss=65.8023
	step [87/189], loss=73.5717
	step [88/189], loss=66.3451
	step [89/189], loss=85.1099
	step [90/189], loss=75.6871
	step [91/189], loss=69.1427
	step [92/189], loss=68.5064
	step [93/189], loss=78.1795
	step [94/189], loss=66.8716
	step [95/189], loss=67.2691
	step [96/189], loss=61.8103
	step [97/189], loss=72.6693
	step [98/189], loss=69.7395
	step [99/189], loss=72.3468
	step [100/189], loss=68.2294
	step [101/189], loss=68.9916
	step [102/189], loss=67.2893
	step [103/189], loss=75.3706
	step [104/189], loss=65.8477
	step [105/189], loss=74.8458
	step [106/189], loss=51.8719
	step [107/189], loss=67.4755
	step [108/189], loss=92.4165
	step [109/189], loss=85.0407
	step [110/189], loss=70.0638
	step [111/189], loss=76.5877
	step [112/189], loss=88.4734
	step [113/189], loss=75.9914
	step [114/189], loss=73.7344
	step [115/189], loss=73.8046
	step [116/189], loss=60.9269
	step [117/189], loss=62.5851
	step [118/189], loss=72.5312
	step [119/189], loss=71.3334
	step [120/189], loss=63.6183
	step [121/189], loss=74.5267
	step [122/189], loss=66.4891
	step [123/189], loss=64.9879
	step [124/189], loss=66.0077
	step [125/189], loss=70.6894
	step [126/189], loss=65.4752
	step [127/189], loss=67.3014
	step [128/189], loss=68.4536
	step [129/189], loss=72.3978
	step [130/189], loss=69.2204
	step [131/189], loss=82.3833
	step [132/189], loss=70.6557
	step [133/189], loss=64.6701
	step [134/189], loss=75.0133
	step [135/189], loss=75.1473
	step [136/189], loss=69.9076
	step [137/189], loss=67.3937
	step [138/189], loss=62.7359
	step [139/189], loss=88.1852
	step [140/189], loss=76.0092
	step [141/189], loss=82.7928
	step [142/189], loss=97.8776
	step [143/189], loss=74.2712
	step [144/189], loss=68.9949
	step [145/189], loss=68.8539
	step [146/189], loss=54.7979
	step [147/189], loss=71.3977
	step [148/189], loss=81.1239
	step [149/189], loss=70.9658
	step [150/189], loss=64.4346
	step [151/189], loss=78.3970
	step [152/189], loss=78.7414
	step [153/189], loss=82.6681
	step [154/189], loss=69.9521
	step [155/189], loss=80.5837
	step [156/189], loss=66.8135
	step [157/189], loss=58.8545
	step [158/189], loss=67.4525
	step [159/189], loss=85.2343
	step [160/189], loss=61.7319
	step [161/189], loss=63.9120
	step [162/189], loss=68.2105
	step [163/189], loss=80.6157
	step [164/189], loss=68.9480
	step [165/189], loss=64.3344
	step [166/189], loss=76.8217
	step [167/189], loss=81.2594
	step [168/189], loss=67.3970
	step [169/189], loss=75.0026
	step [170/189], loss=77.8372
	step [171/189], loss=69.5562
	step [172/189], loss=61.3379
	step [173/189], loss=67.7256
	step [174/189], loss=63.5151
	step [175/189], loss=72.9585
	step [176/189], loss=64.7376
	step [177/189], loss=67.7999
	step [178/189], loss=68.3760
	step [179/189], loss=66.2868
	step [180/189], loss=71.2845
	step [181/189], loss=72.9933
	step [182/189], loss=68.1506
	step [183/189], loss=68.1265
	step [184/189], loss=70.5989
	step [185/189], loss=79.2870
	step [186/189], loss=81.3167
	step [187/189], loss=70.9753
	step [188/189], loss=67.0854
	step [189/189], loss=27.6993
	Evaluating
	loss=0.0082, precision=0.3950, recall=0.8777, f1=0.5448
Training epoch 94
	step [1/189], loss=73.0038
	step [2/189], loss=78.9260
	step [3/189], loss=67.7179
	step [4/189], loss=69.0406
	step [5/189], loss=73.9272
	step [6/189], loss=66.7637
	step [7/189], loss=62.7037
	step [8/189], loss=65.5766
	step [9/189], loss=70.3909
	step [10/189], loss=58.5976
	step [11/189], loss=71.5644
	step [12/189], loss=65.3978
	step [13/189], loss=79.4176
	step [14/189], loss=78.9747
	step [15/189], loss=67.1643
	step [16/189], loss=65.8571
	step [17/189], loss=60.5726
	step [18/189], loss=62.7217
	step [19/189], loss=73.8929
	step [20/189], loss=72.8740
	step [21/189], loss=56.3705
	step [22/189], loss=74.9681
	step [23/189], loss=70.7368
	step [24/189], loss=69.3503
	step [25/189], loss=75.6862
	step [26/189], loss=74.5631
	step [27/189], loss=65.5735
	step [28/189], loss=60.4947
	step [29/189], loss=60.5393
	step [30/189], loss=53.6694
	step [31/189], loss=62.2840
	step [32/189], loss=83.3553
	step [33/189], loss=65.6584
	step [34/189], loss=70.7532
	step [35/189], loss=71.3134
	step [36/189], loss=67.2864
	step [37/189], loss=69.9419
	step [38/189], loss=64.7904
	step [39/189], loss=78.3231
	step [40/189], loss=71.0591
	step [41/189], loss=83.4892
	step [42/189], loss=74.5559
	step [43/189], loss=68.6194
	step [44/189], loss=71.2572
	step [45/189], loss=76.5531
	step [46/189], loss=62.5006
	step [47/189], loss=74.3218
	step [48/189], loss=84.9243
	step [49/189], loss=71.7029
	step [50/189], loss=56.4759
	step [51/189], loss=78.4204
	step [52/189], loss=85.2833
	step [53/189], loss=64.4745
	step [54/189], loss=74.6094
	step [55/189], loss=72.0666
	step [56/189], loss=68.6709
	step [57/189], loss=73.8147
	step [58/189], loss=66.8171
	step [59/189], loss=61.4434
	step [60/189], loss=63.2727
	step [61/189], loss=62.1071
	step [62/189], loss=72.0037
	step [63/189], loss=76.9158
	step [64/189], loss=54.6949
	step [65/189], loss=71.6414
	step [66/189], loss=66.6476
	step [67/189], loss=66.0702
	step [68/189], loss=78.7764
	step [69/189], loss=62.2077
	step [70/189], loss=64.8896
	step [71/189], loss=64.9045
	step [72/189], loss=78.5461
	step [73/189], loss=85.1482
	step [74/189], loss=62.3100
	step [75/189], loss=54.8569
	step [76/189], loss=74.2465
	step [77/189], loss=62.9800
	step [78/189], loss=73.0190
	step [79/189], loss=82.7748
	step [80/189], loss=70.1502
	step [81/189], loss=73.6373
	step [82/189], loss=70.1942
	step [83/189], loss=55.3370
	step [84/189], loss=56.6444
	step [85/189], loss=68.9261
	step [86/189], loss=64.8905
	step [87/189], loss=71.3095
	step [88/189], loss=65.2236
	step [89/189], loss=80.9367
	step [90/189], loss=79.2766
	step [91/189], loss=76.5987
	step [92/189], loss=77.8816
	step [93/189], loss=70.2411
	step [94/189], loss=72.7226
	step [95/189], loss=66.6487
	step [96/189], loss=85.9777
	step [97/189], loss=67.5900
	step [98/189], loss=72.1676
	step [99/189], loss=58.0528
	step [100/189], loss=47.9610
	step [101/189], loss=70.4796
	step [102/189], loss=57.3217
	step [103/189], loss=76.1215
	step [104/189], loss=83.5220
	step [105/189], loss=77.2098
	step [106/189], loss=71.3597
	step [107/189], loss=62.5045
	step [108/189], loss=72.3846
	step [109/189], loss=75.9303
	step [110/189], loss=66.2693
	step [111/189], loss=82.5283
	step [112/189], loss=67.5256
	step [113/189], loss=78.3602
	step [114/189], loss=79.2183
	step [115/189], loss=87.0008
	step [116/189], loss=76.7995
	step [117/189], loss=64.8212
	step [118/189], loss=71.1465
	step [119/189], loss=75.1418
	step [120/189], loss=71.1974
	step [121/189], loss=60.7353
	step [122/189], loss=65.9539
	step [123/189], loss=55.7493
	step [124/189], loss=62.1691
	step [125/189], loss=71.7290
	step [126/189], loss=77.9708
	step [127/189], loss=76.7351
	step [128/189], loss=77.2684
	step [129/189], loss=70.5517
	step [130/189], loss=76.1597
	step [131/189], loss=63.9434
	step [132/189], loss=57.3071
	step [133/189], loss=73.2225
	step [134/189], loss=63.5939
	step [135/189], loss=69.4016
	step [136/189], loss=71.4284
	step [137/189], loss=77.5081
	step [138/189], loss=89.1969
	step [139/189], loss=67.3774
	step [140/189], loss=84.5510
	step [141/189], loss=58.1895
	step [142/189], loss=69.3694
	step [143/189], loss=70.4494
	step [144/189], loss=69.5280
	step [145/189], loss=71.4850
	step [146/189], loss=64.5177
	step [147/189], loss=67.5606
	step [148/189], loss=72.1747
	step [149/189], loss=73.6730
	step [150/189], loss=57.7964
	step [151/189], loss=75.1694
	step [152/189], loss=63.5227
	step [153/189], loss=64.0208
	step [154/189], loss=77.9601
	step [155/189], loss=70.6830
	step [156/189], loss=75.7389
	step [157/189], loss=66.0943
	step [158/189], loss=70.2823
	step [159/189], loss=83.3749
	step [160/189], loss=68.8209
	step [161/189], loss=73.6761
	step [162/189], loss=77.2671
	step [163/189], loss=61.5265
	step [164/189], loss=74.1029
	step [165/189], loss=78.3987
	step [166/189], loss=65.9831
	step [167/189], loss=72.8951
	step [168/189], loss=73.7659
	step [169/189], loss=82.0361
	step [170/189], loss=64.4758
	step [171/189], loss=71.0499
	step [172/189], loss=67.8630
	step [173/189], loss=69.4807
	step [174/189], loss=72.3996
	step [175/189], loss=63.5412
	step [176/189], loss=68.7053
	step [177/189], loss=68.9961
	step [178/189], loss=66.2719
	step [179/189], loss=82.3698
	step [180/189], loss=79.8631
	step [181/189], loss=56.6660
	step [182/189], loss=74.2296
	step [183/189], loss=77.9967
	step [184/189], loss=79.1599
	step [185/189], loss=70.9046
	step [186/189], loss=73.1338
	step [187/189], loss=73.9701
	step [188/189], loss=79.6931
	step [189/189], loss=24.2033
	Evaluating
	loss=0.0092, precision=0.3614, recall=0.8953, f1=0.5150
Training epoch 95
	step [1/189], loss=63.2557
	step [2/189], loss=65.9469
	step [3/189], loss=63.7574
	step [4/189], loss=75.2768
	step [5/189], loss=83.8933
	step [6/189], loss=71.1836
	step [7/189], loss=70.9556
	step [8/189], loss=73.1091
	step [9/189], loss=77.6082
	step [10/189], loss=74.0623
	step [11/189], loss=75.4064
	step [12/189], loss=68.2059
	step [13/189], loss=88.0117
	step [14/189], loss=72.8732
	step [15/189], loss=63.5912
	step [16/189], loss=70.0409
	step [17/189], loss=69.6041
	step [18/189], loss=80.7187
	step [19/189], loss=77.0341
	step [20/189], loss=58.9650
	step [21/189], loss=75.2367
	step [22/189], loss=62.8436
	step [23/189], loss=62.9768
	step [24/189], loss=83.4770
	step [25/189], loss=62.7655
	step [26/189], loss=63.7702
	step [27/189], loss=71.2670
	step [28/189], loss=58.2806
	step [29/189], loss=68.9276
	step [30/189], loss=72.4625
	step [31/189], loss=61.8818
	step [32/189], loss=73.9890
	step [33/189], loss=80.4659
	step [34/189], loss=56.6670
	step [35/189], loss=83.7582
	step [36/189], loss=65.7939
	step [37/189], loss=74.3368
	step [38/189], loss=85.3022
	step [39/189], loss=66.2561
	step [40/189], loss=70.8832
	step [41/189], loss=61.0115
	step [42/189], loss=73.5598
	step [43/189], loss=72.1667
	step [44/189], loss=64.2784
	step [45/189], loss=87.6649
	step [46/189], loss=78.3877
	step [47/189], loss=81.0119
	step [48/189], loss=65.4061
	step [49/189], loss=81.0264
	step [50/189], loss=79.8184
	step [51/189], loss=69.0313
	step [52/189], loss=73.4720
	step [53/189], loss=67.4986
	step [54/189], loss=61.7839
	step [55/189], loss=74.8529
	step [56/189], loss=78.4100
	step [57/189], loss=80.3160
	step [58/189], loss=61.4135
	step [59/189], loss=73.9932
	step [60/189], loss=58.7733
	step [61/189], loss=70.4166
	step [62/189], loss=64.1166
	step [63/189], loss=68.6857
	step [64/189], loss=58.3665
	step [65/189], loss=63.2834
	step [66/189], loss=66.9246
	step [67/189], loss=53.7868
	step [68/189], loss=72.3340
	step [69/189], loss=82.0948
	step [70/189], loss=71.4191
	step [71/189], loss=64.2943
	step [72/189], loss=63.1268
	step [73/189], loss=71.6951
	step [74/189], loss=64.1963
	step [75/189], loss=55.7084
	step [76/189], loss=82.6103
	step [77/189], loss=79.0379
	step [78/189], loss=62.0989
	step [79/189], loss=77.5840
	step [80/189], loss=75.0649
	step [81/189], loss=64.0186
	step [82/189], loss=80.0061
	step [83/189], loss=70.1701
	step [84/189], loss=69.7940
	step [85/189], loss=70.0414
	step [86/189], loss=68.1237
	step [87/189], loss=82.0575
	step [88/189], loss=65.4433
	step [89/189], loss=77.9116
	step [90/189], loss=64.5495
	step [91/189], loss=67.2719
	step [92/189], loss=79.6143
	step [93/189], loss=76.3044
	step [94/189], loss=68.0386
	step [95/189], loss=65.4186
	step [96/189], loss=64.4685
	step [97/189], loss=74.6433
	step [98/189], loss=71.9286
	step [99/189], loss=59.9324
	step [100/189], loss=74.4523
	step [101/189], loss=68.0985
	step [102/189], loss=74.9025
	step [103/189], loss=73.8340
	step [104/189], loss=68.2041
	step [105/189], loss=66.2946
	step [106/189], loss=57.5002
	step [107/189], loss=72.8409
	step [108/189], loss=68.2804
	step [109/189], loss=72.8221
	step [110/189], loss=61.1776
	step [111/189], loss=70.7265
	step [112/189], loss=74.1348
	step [113/189], loss=78.1636
	step [114/189], loss=61.6269
	step [115/189], loss=83.5367
	step [116/189], loss=72.2637
	step [117/189], loss=64.9352
	step [118/189], loss=74.3130
	step [119/189], loss=65.3286
	step [120/189], loss=75.5715
	step [121/189], loss=88.5369
	step [122/189], loss=64.7443
	step [123/189], loss=59.3374
	step [124/189], loss=70.7537
	step [125/189], loss=67.7857
	step [126/189], loss=65.9884
	step [127/189], loss=70.5701
	step [128/189], loss=75.7541
	step [129/189], loss=76.7771
	step [130/189], loss=60.6005
	step [131/189], loss=63.6019
	step [132/189], loss=66.7213
	step [133/189], loss=58.7167
	step [134/189], loss=74.9829
	step [135/189], loss=77.3799
	step [136/189], loss=81.7799
	step [137/189], loss=81.3520
	step [138/189], loss=72.7170
	step [139/189], loss=87.3656
	step [140/189], loss=69.5768
	step [141/189], loss=59.3581
	step [142/189], loss=71.6840
	step [143/189], loss=73.5510
	step [144/189], loss=67.9000
	step [145/189], loss=64.0019
	step [146/189], loss=74.0472
	step [147/189], loss=54.7143
	step [148/189], loss=69.9245
	step [149/189], loss=67.6245
	step [150/189], loss=60.6289
	step [151/189], loss=61.6405
	step [152/189], loss=89.9935
	step [153/189], loss=65.9743
	step [154/189], loss=64.5833
	step [155/189], loss=63.7540
	step [156/189], loss=63.7178
	step [157/189], loss=77.0225
	step [158/189], loss=65.2800
	step [159/189], loss=68.6297
	step [160/189], loss=69.6676
	step [161/189], loss=70.6539
	step [162/189], loss=72.7339
	step [163/189], loss=59.2460
	step [164/189], loss=79.8122
	step [165/189], loss=71.6567
	step [166/189], loss=73.1600
	step [167/189], loss=72.8154
	step [168/189], loss=76.9226
	step [169/189], loss=73.2196
	step [170/189], loss=69.7455
	step [171/189], loss=67.1493
	step [172/189], loss=53.3486
	step [173/189], loss=59.1081
	step [174/189], loss=62.2121
	step [175/189], loss=74.0551
	step [176/189], loss=67.3199
	step [177/189], loss=70.3746
	step [178/189], loss=66.0613
	step [179/189], loss=68.4243
	step [180/189], loss=77.1691
	step [181/189], loss=70.8728
	step [182/189], loss=73.9493
	step [183/189], loss=70.6901
	step [184/189], loss=63.9495
	step [185/189], loss=72.0496
	step [186/189], loss=73.2193
	step [187/189], loss=64.2777
	step [188/189], loss=71.8232
	step [189/189], loss=24.6611
	Evaluating
	loss=0.0087, precision=0.3787, recall=0.8808, f1=0.5296
Training epoch 96
	step [1/189], loss=61.4316
	step [2/189], loss=76.7082
	step [3/189], loss=70.2823
	step [4/189], loss=71.6411
	step [5/189], loss=68.2506
	step [6/189], loss=71.7437
	step [7/189], loss=67.7763
	step [8/189], loss=73.4181
	step [9/189], loss=69.1258
	step [10/189], loss=75.9462
	step [11/189], loss=75.5612
	step [12/189], loss=75.9343
	step [13/189], loss=75.5188
	step [14/189], loss=61.2431
	step [15/189], loss=69.4595
	step [16/189], loss=75.5722
	step [17/189], loss=70.9916
	step [18/189], loss=57.1143
	step [19/189], loss=77.9967
	step [20/189], loss=59.4381
	step [21/189], loss=66.3883
	step [22/189], loss=62.3059
	step [23/189], loss=52.6640
	step [24/189], loss=67.7463
	step [25/189], loss=56.1865
	step [26/189], loss=61.1845
	step [27/189], loss=78.6692
	step [28/189], loss=64.4293
	step [29/189], loss=69.3080
	step [30/189], loss=53.9954
	step [31/189], loss=67.6315
	step [32/189], loss=75.9686
	step [33/189], loss=68.9318
	step [34/189], loss=77.2857
	step [35/189], loss=61.8202
	step [36/189], loss=70.6687
	step [37/189], loss=70.0024
	step [38/189], loss=77.5769
	step [39/189], loss=64.0311
	step [40/189], loss=78.5368
	step [41/189], loss=67.9570
	step [42/189], loss=69.0784
	step [43/189], loss=71.1823
	step [44/189], loss=56.5061
	step [45/189], loss=77.9446
	step [46/189], loss=73.1719
	step [47/189], loss=58.1413
	step [48/189], loss=71.2520
	step [49/189], loss=81.3421
	step [50/189], loss=82.9675
	step [51/189], loss=85.5406
	step [52/189], loss=62.4191
	step [53/189], loss=75.5745
	step [54/189], loss=61.1040
	step [55/189], loss=75.7776
	step [56/189], loss=67.6151
	step [57/189], loss=81.1983
	step [58/189], loss=75.1110
	step [59/189], loss=65.9459
	step [60/189], loss=68.2918
	step [61/189], loss=73.3187
	step [62/189], loss=67.8773
	step [63/189], loss=65.3347
	step [64/189], loss=82.6667
	step [65/189], loss=59.8557
	step [66/189], loss=63.4633
	step [67/189], loss=80.5831
	step [68/189], loss=76.2675
	step [69/189], loss=72.2089
	step [70/189], loss=62.4490
	step [71/189], loss=65.6518
	step [72/189], loss=74.5806
	step [73/189], loss=54.8275
	step [74/189], loss=63.7819
	step [75/189], loss=75.3927
	step [76/189], loss=67.0611
	step [77/189], loss=75.6413
	step [78/189], loss=81.0545
	step [79/189], loss=67.1178
	step [80/189], loss=74.3571
	step [81/189], loss=82.8120
	step [82/189], loss=72.9642
	step [83/189], loss=79.2237
	step [84/189], loss=74.3770
	step [85/189], loss=66.7167
	step [86/189], loss=73.6338
	step [87/189], loss=72.8336
	step [88/189], loss=60.9823
	step [89/189], loss=67.6240
	step [90/189], loss=79.2149
	step [91/189], loss=73.6481
	step [92/189], loss=71.2268
	step [93/189], loss=79.4112
	step [94/189], loss=78.1843
	step [95/189], loss=63.0290
	step [96/189], loss=76.6714
	step [97/189], loss=67.6770
	step [98/189], loss=55.7845
	step [99/189], loss=60.8299
	step [100/189], loss=64.9206
	step [101/189], loss=81.1266
	step [102/189], loss=65.9056
	step [103/189], loss=77.2375
	step [104/189], loss=60.3641
	step [105/189], loss=89.4998
	step [106/189], loss=65.6406
	step [107/189], loss=69.1311
	step [108/189], loss=58.4605
	step [109/189], loss=77.3345
	step [110/189], loss=76.4661
	step [111/189], loss=61.2061
	step [112/189], loss=74.8004
	step [113/189], loss=71.5755
	step [114/189], loss=71.5407
	step [115/189], loss=80.0493
	step [116/189], loss=70.4762
	step [117/189], loss=72.9115
	step [118/189], loss=82.1554
	step [119/189], loss=74.6382
	step [120/189], loss=61.8364
	step [121/189], loss=76.0629
	step [122/189], loss=71.5247
	step [123/189], loss=70.3377
	step [124/189], loss=61.9056
	step [125/189], loss=65.3156
	step [126/189], loss=70.8689
	step [127/189], loss=74.3539
	step [128/189], loss=74.1613
	step [129/189], loss=82.5299
	step [130/189], loss=69.9595
	step [131/189], loss=73.4731
	step [132/189], loss=71.9960
	step [133/189], loss=66.8791
	step [134/189], loss=62.6000
	step [135/189], loss=60.8704
	step [136/189], loss=70.0968
	step [137/189], loss=62.8313
	step [138/189], loss=60.9164
	step [139/189], loss=68.9186
	step [140/189], loss=81.3639
	step [141/189], loss=72.2502
	step [142/189], loss=75.3915
	step [143/189], loss=71.1489
	step [144/189], loss=71.0191
	step [145/189], loss=64.0038
	step [146/189], loss=75.3145
	step [147/189], loss=75.2719
	step [148/189], loss=74.1653
	step [149/189], loss=57.6817
	step [150/189], loss=71.6357
	step [151/189], loss=75.4409
	step [152/189], loss=64.7319
	step [153/189], loss=72.4870
	step [154/189], loss=72.3108
	step [155/189], loss=74.9931
	step [156/189], loss=68.8599
	step [157/189], loss=71.7144
	step [158/189], loss=74.0010
	step [159/189], loss=80.7296
	step [160/189], loss=62.7286
	step [161/189], loss=63.7594
	step [162/189], loss=71.3949
	step [163/189], loss=69.5433
	step [164/189], loss=71.5947
	step [165/189], loss=61.1016
	step [166/189], loss=65.3827
	step [167/189], loss=78.9927
	step [168/189], loss=95.1885
	step [169/189], loss=64.7928
	step [170/189], loss=69.7767
	step [171/189], loss=67.1429
	step [172/189], loss=78.9492
	step [173/189], loss=63.5617
	step [174/189], loss=75.3815
	step [175/189], loss=77.2441
	step [176/189], loss=66.1781
	step [177/189], loss=58.1008
	step [178/189], loss=71.0184
	step [179/189], loss=65.9196
	step [180/189], loss=78.8615
	step [181/189], loss=64.9341
	step [182/189], loss=71.2553
	step [183/189], loss=66.1494
	step [184/189], loss=57.3735
	step [185/189], loss=65.3385
	step [186/189], loss=76.3808
	step [187/189], loss=75.0099
	step [188/189], loss=74.0286
	step [189/189], loss=28.5551
	Evaluating
	loss=0.0072, precision=0.4341, recall=0.8819, f1=0.5818
Training epoch 97
	step [1/189], loss=71.9070
	step [2/189], loss=72.9298
	step [3/189], loss=72.3628
	step [4/189], loss=72.0840
	step [5/189], loss=68.3859
	step [6/189], loss=74.1263
	step [7/189], loss=64.0317
	step [8/189], loss=73.1891
	step [9/189], loss=65.9209
	step [10/189], loss=68.8567
	step [11/189], loss=74.6727
	step [12/189], loss=61.6763
	step [13/189], loss=62.5066
	step [14/189], loss=70.4766
	step [15/189], loss=71.0114
	step [16/189], loss=64.1040
	step [17/189], loss=70.4850
	step [18/189], loss=67.7480
	step [19/189], loss=76.5267
	step [20/189], loss=72.5718
	step [21/189], loss=70.5551
	step [22/189], loss=71.7227
	step [23/189], loss=72.3800
	step [24/189], loss=69.9804
	step [25/189], loss=79.0384
	step [26/189], loss=88.4750
	step [27/189], loss=75.2499
	step [28/189], loss=68.7094
	step [29/189], loss=74.2803
	step [30/189], loss=70.9760
	step [31/189], loss=62.1107
	step [32/189], loss=68.9790
	step [33/189], loss=74.4519
	step [34/189], loss=65.1745
	step [35/189], loss=69.4278
	step [36/189], loss=74.3097
	step [37/189], loss=61.3543
	step [38/189], loss=73.6275
	step [39/189], loss=67.2119
	step [40/189], loss=72.6371
	step [41/189], loss=68.4520
	step [42/189], loss=68.5336
	step [43/189], loss=77.5853
	step [44/189], loss=66.3613
	step [45/189], loss=75.4368
	step [46/189], loss=66.7068
	step [47/189], loss=73.3971
	step [48/189], loss=59.7650
	step [49/189], loss=83.9326
	step [50/189], loss=84.3624
	step [51/189], loss=69.0210
	step [52/189], loss=64.6365
	step [53/189], loss=72.6673
	step [54/189], loss=65.8961
	step [55/189], loss=67.2863
	step [56/189], loss=70.5442
	step [57/189], loss=83.0784
	step [58/189], loss=59.2874
	step [59/189], loss=74.0238
	step [60/189], loss=63.2285
	step [61/189], loss=75.0154
	step [62/189], loss=62.0708
	step [63/189], loss=66.6764
	step [64/189], loss=65.5763
	step [65/189], loss=74.0121
	step [66/189], loss=66.2206
	step [67/189], loss=65.8248
	step [68/189], loss=59.2595
	step [69/189], loss=70.3615
	step [70/189], loss=74.7207
	step [71/189], loss=70.5385
	step [72/189], loss=67.6490
	step [73/189], loss=77.0723
	step [74/189], loss=93.2476
	step [75/189], loss=66.4555
	step [76/189], loss=60.5761
	step [77/189], loss=71.3779
	step [78/189], loss=64.8386
	step [79/189], loss=59.8297
	step [80/189], loss=57.5273
	step [81/189], loss=63.1537
	step [82/189], loss=66.7940
	step [83/189], loss=79.2292
	step [84/189], loss=69.6504
	step [85/189], loss=75.3489
	step [86/189], loss=71.0893
	step [87/189], loss=69.0983
	step [88/189], loss=72.2116
	step [89/189], loss=76.5232
	step [90/189], loss=66.3230
	step [91/189], loss=75.2824
	step [92/189], loss=70.8765
	step [93/189], loss=68.3211
	step [94/189], loss=74.0176
	step [95/189], loss=61.8478
	step [96/189], loss=68.0049
	step [97/189], loss=83.2198
	step [98/189], loss=72.9315
	step [99/189], loss=68.1349
	step [100/189], loss=71.7893
	step [101/189], loss=68.7730
	step [102/189], loss=70.0983
	step [103/189], loss=65.2707
	step [104/189], loss=61.2099
	step [105/189], loss=69.8793
	step [106/189], loss=78.9763
	step [107/189], loss=64.4012
	step [108/189], loss=63.1511
	step [109/189], loss=78.7814
	step [110/189], loss=68.7152
	step [111/189], loss=72.5243
	step [112/189], loss=82.4991
	step [113/189], loss=53.8790
	step [114/189], loss=61.3579
	step [115/189], loss=76.9698
	step [116/189], loss=73.7893
	step [117/189], loss=62.0116
	step [118/189], loss=69.1328
	step [119/189], loss=74.7963
	step [120/189], loss=67.7298
	step [121/189], loss=66.2030
	step [122/189], loss=66.3312
	step [123/189], loss=81.7226
	step [124/189], loss=75.1277
	step [125/189], loss=57.4235
	step [126/189], loss=58.8583
	step [127/189], loss=65.3257
	step [128/189], loss=65.8260
	step [129/189], loss=67.1941
	step [130/189], loss=64.0413
	step [131/189], loss=65.3540
	step [132/189], loss=67.1927
	step [133/189], loss=70.6562
	step [134/189], loss=69.9641
	step [135/189], loss=67.6998
	step [136/189], loss=68.5654
	step [137/189], loss=79.9670
	step [138/189], loss=72.4212
	step [139/189], loss=73.5985
	step [140/189], loss=67.4536
	step [141/189], loss=79.7240
	step [142/189], loss=72.6161
	step [143/189], loss=77.9884
	step [144/189], loss=66.3876
	step [145/189], loss=81.9092
	step [146/189], loss=67.2960
	step [147/189], loss=65.7558
	step [148/189], loss=80.1283
	step [149/189], loss=63.9613
	step [150/189], loss=84.0968
	step [151/189], loss=62.7673
	step [152/189], loss=68.6813
	step [153/189], loss=71.7935
	step [154/189], loss=65.1911
	step [155/189], loss=83.2100
	step [156/189], loss=68.4190
	step [157/189], loss=56.0635
	step [158/189], loss=91.9882
	step [159/189], loss=79.2170
	step [160/189], loss=63.4887
	step [161/189], loss=73.0911
	step [162/189], loss=63.0542
	step [163/189], loss=63.9230
	step [164/189], loss=73.7093
	step [165/189], loss=71.1617
	step [166/189], loss=62.3688
	step [167/189], loss=76.6146
	step [168/189], loss=71.3953
	step [169/189], loss=68.3940
	step [170/189], loss=69.3702
	step [171/189], loss=74.1277
	step [172/189], loss=82.8044
	step [173/189], loss=61.2803
	step [174/189], loss=69.7379
	step [175/189], loss=65.8998
	step [176/189], loss=71.0513
	step [177/189], loss=62.3701
	step [178/189], loss=70.3885
	step [179/189], loss=74.0287
	step [180/189], loss=78.1627
	step [181/189], loss=60.0554
	step [182/189], loss=71.8068
	step [183/189], loss=59.5694
	step [184/189], loss=69.0214
	step [185/189], loss=62.1427
	step [186/189], loss=60.2943
	step [187/189], loss=75.3231
	step [188/189], loss=64.8942
	step [189/189], loss=26.0373
	Evaluating
	loss=0.0080, precision=0.3993, recall=0.8768, f1=0.5487
Training epoch 98
	step [1/189], loss=67.1948
	step [2/189], loss=66.7701
	step [3/189], loss=66.6379
	step [4/189], loss=69.4454
	step [5/189], loss=78.0647
	step [6/189], loss=70.0589
	step [7/189], loss=72.0294
	step [8/189], loss=62.6921
	step [9/189], loss=76.9417
	step [10/189], loss=64.2338
	step [11/189], loss=89.0679
	step [12/189], loss=82.4863
	step [13/189], loss=60.8565
	step [14/189], loss=74.5263
	step [15/189], loss=65.8139
	step [16/189], loss=61.8667
	step [17/189], loss=64.7506
	step [18/189], loss=68.2255
	step [19/189], loss=71.1765
	step [20/189], loss=77.8142
	step [21/189], loss=69.3360
	step [22/189], loss=71.4215
	step [23/189], loss=70.2870
	step [24/189], loss=52.1968
	step [25/189], loss=63.7191
	step [26/189], loss=67.8275
	step [27/189], loss=63.2091
	step [28/189], loss=69.7379
	step [29/189], loss=73.9488
	step [30/189], loss=67.4692
	step [31/189], loss=63.4025
	step [32/189], loss=77.0337
	step [33/189], loss=69.8403
	step [34/189], loss=85.7348
	step [35/189], loss=77.8676
	step [36/189], loss=64.8714
	step [37/189], loss=64.5714
	step [38/189], loss=54.4056
	step [39/189], loss=61.7228
	step [40/189], loss=69.2441
	step [41/189], loss=70.9694
	step [42/189], loss=66.7207
	step [43/189], loss=62.0662
	step [44/189], loss=70.1187
	step [45/189], loss=68.2183
	step [46/189], loss=74.6618
	step [47/189], loss=78.0936
	step [48/189], loss=65.7290
	step [49/189], loss=58.5077
	step [50/189], loss=65.6439
	step [51/189], loss=72.0661
	step [52/189], loss=69.1720
	step [53/189], loss=77.3707
	step [54/189], loss=74.3843
	step [55/189], loss=76.7808
	step [56/189], loss=68.6269
	step [57/189], loss=66.6277
	step [58/189], loss=66.2467
	step [59/189], loss=77.0227
	step [60/189], loss=67.2745
	step [61/189], loss=71.3918
	step [62/189], loss=76.5239
	step [63/189], loss=84.2794
	step [64/189], loss=73.3954
	step [65/189], loss=69.6604
	step [66/189], loss=76.3016
	step [67/189], loss=95.5949
	step [68/189], loss=64.2656
	step [69/189], loss=65.2563
	step [70/189], loss=61.6348
	step [71/189], loss=67.3236
	step [72/189], loss=79.9387
	step [73/189], loss=63.0763
	step [74/189], loss=69.4985
	step [75/189], loss=57.5779
	step [76/189], loss=68.2170
	step [77/189], loss=73.1143
	step [78/189], loss=70.9010
	step [79/189], loss=74.1199
	step [80/189], loss=59.2970
	step [81/189], loss=74.0857
	step [82/189], loss=71.9485
	step [83/189], loss=65.9378
	step [84/189], loss=73.2528
	step [85/189], loss=68.1266
	step [86/189], loss=60.4256
	step [87/189], loss=71.5597
	step [88/189], loss=75.0140
	step [89/189], loss=64.1092
	step [90/189], loss=65.9491
	step [91/189], loss=81.3749
	step [92/189], loss=69.1471
	step [93/189], loss=77.1118
	step [94/189], loss=67.7936
	step [95/189], loss=66.2793
	step [96/189], loss=63.9782
	step [97/189], loss=72.7978
	step [98/189], loss=72.1460
	step [99/189], loss=64.0768
	step [100/189], loss=78.4308
	step [101/189], loss=87.6080
	step [102/189], loss=78.9132
	step [103/189], loss=70.4913
	step [104/189], loss=66.2876
	step [105/189], loss=64.4713
	step [106/189], loss=62.8416
	step [107/189], loss=70.8422
	step [108/189], loss=75.9794
	step [109/189], loss=61.0110
	step [110/189], loss=72.2852
	step [111/189], loss=73.8540
	step [112/189], loss=68.4845
	step [113/189], loss=61.4318
	step [114/189], loss=77.0000
	step [115/189], loss=63.7401
	step [116/189], loss=74.1008
	step [117/189], loss=66.7827
	step [118/189], loss=66.7167
	step [119/189], loss=71.0020
	step [120/189], loss=76.9745
	step [121/189], loss=76.1061
	step [122/189], loss=72.6822
	step [123/189], loss=67.0335
	step [124/189], loss=70.0467
	step [125/189], loss=65.2470
	step [126/189], loss=67.7947
	step [127/189], loss=68.8675
	step [128/189], loss=62.2021
	step [129/189], loss=59.7075
	step [130/189], loss=73.6554
	step [131/189], loss=80.9304
	step [132/189], loss=62.0763
	step [133/189], loss=70.5310
	step [134/189], loss=64.5689
	step [135/189], loss=70.1000
	step [136/189], loss=62.4626
	step [137/189], loss=72.6303
	step [138/189], loss=77.9961
	step [139/189], loss=69.8436
	step [140/189], loss=80.4414
	step [141/189], loss=61.7874
	step [142/189], loss=79.4853
	step [143/189], loss=68.7885
	step [144/189], loss=70.6396
	step [145/189], loss=78.1079
	step [146/189], loss=71.5062
	step [147/189], loss=71.8915
	step [148/189], loss=68.9552
	step [149/189], loss=69.9970
	step [150/189], loss=78.0149
	step [151/189], loss=74.8709
	step [152/189], loss=73.4397
	step [153/189], loss=82.9354
	step [154/189], loss=73.6532
	step [155/189], loss=63.0947
	step [156/189], loss=62.4613
	step [157/189], loss=59.3989
	step [158/189], loss=68.9055
	step [159/189], loss=71.1970
	step [160/189], loss=76.7592
	step [161/189], loss=64.9278
	step [162/189], loss=66.1205
	step [163/189], loss=61.7873
	step [164/189], loss=74.0111
	step [165/189], loss=76.2253
	step [166/189], loss=77.0173
	step [167/189], loss=58.9590
	step [168/189], loss=83.1881
	step [169/189], loss=66.4880
	step [170/189], loss=59.7561
	step [171/189], loss=69.8285
	step [172/189], loss=73.5548
	step [173/189], loss=58.4024
	step [174/189], loss=72.9279
	step [175/189], loss=81.2476
	step [176/189], loss=68.4746
	step [177/189], loss=69.8359
	step [178/189], loss=64.8598
	step [179/189], loss=81.6795
	step [180/189], loss=70.1645
	step [181/189], loss=56.0994
	step [182/189], loss=62.6630
	step [183/189], loss=67.5946
	step [184/189], loss=58.1019
	step [185/189], loss=82.5618
	step [186/189], loss=63.2611
	step [187/189], loss=59.3421
	step [188/189], loss=65.7419
	step [189/189], loss=30.5508
	Evaluating
	loss=0.0071, precision=0.4442, recall=0.8860, f1=0.5917
Training epoch 99
	step [1/189], loss=62.4516
	step [2/189], loss=77.1103
	step [3/189], loss=68.3386
	step [4/189], loss=69.6476
	step [5/189], loss=65.4807
	step [6/189], loss=75.9562
	step [7/189], loss=92.4048
	step [8/189], loss=74.6423
	step [9/189], loss=69.9211
	step [10/189], loss=61.1363
	step [11/189], loss=73.7353
	step [12/189], loss=72.4204
	step [13/189], loss=57.3090
	step [14/189], loss=62.2825
	step [15/189], loss=74.4334
	step [16/189], loss=67.8422
	step [17/189], loss=57.0361
	step [18/189], loss=69.7025
	step [19/189], loss=73.1103
	step [20/189], loss=69.4268
	step [21/189], loss=77.6618
	step [22/189], loss=77.8043
	step [23/189], loss=80.3583
	step [24/189], loss=73.9053
	step [25/189], loss=59.9480
	step [26/189], loss=66.0748
	step [27/189], loss=71.1885
	step [28/189], loss=75.6026
	step [29/189], loss=80.5153
	step [30/189], loss=70.5846
	step [31/189], loss=58.8408
	step [32/189], loss=68.4730
	step [33/189], loss=69.5580
	step [34/189], loss=65.4492
	step [35/189], loss=68.6417
	step [36/189], loss=61.9490
	step [37/189], loss=58.5498
	step [38/189], loss=78.4377
	step [39/189], loss=70.4908
	step [40/189], loss=58.4908
	step [41/189], loss=69.6148
	step [42/189], loss=74.1800
	step [43/189], loss=65.0691
	step [44/189], loss=78.4486
	step [45/189], loss=75.8413
	step [46/189], loss=65.5947
	step [47/189], loss=65.1392
	step [48/189], loss=66.7112
	step [49/189], loss=70.7736
	step [50/189], loss=65.0524
	step [51/189], loss=58.2656
	step [52/189], loss=59.7398
	step [53/189], loss=68.1536
	step [54/189], loss=69.0126
	step [55/189], loss=75.8592
	step [56/189], loss=75.1059
	step [57/189], loss=69.2832
	step [58/189], loss=69.1301
	step [59/189], loss=62.0975
	step [60/189], loss=68.2415
	step [61/189], loss=71.1714
	step [62/189], loss=48.0937
	step [63/189], loss=74.9793
	step [64/189], loss=54.5031
	step [65/189], loss=70.5948
	step [66/189], loss=66.4866
	step [67/189], loss=69.1036
	step [68/189], loss=72.3171
	step [69/189], loss=61.2409
	step [70/189], loss=74.9834
	step [71/189], loss=64.8290
	step [72/189], loss=67.9736
	step [73/189], loss=74.4833
	step [74/189], loss=77.8413
	step [75/189], loss=73.2371
	step [76/189], loss=67.4020
	step [77/189], loss=76.3871
	step [78/189], loss=67.8040
	step [79/189], loss=65.5369
	step [80/189], loss=71.5879
	step [81/189], loss=64.4517
	step [82/189], loss=62.6862
	step [83/189], loss=73.4306
	step [84/189], loss=63.0192
	step [85/189], loss=67.8604
	step [86/189], loss=66.9313
	step [87/189], loss=70.4936
	step [88/189], loss=73.1437
	step [89/189], loss=73.4442
	step [90/189], loss=77.1872
	step [91/189], loss=64.1745
	step [92/189], loss=58.5053
	step [93/189], loss=69.9175
	step [94/189], loss=69.2340
	step [95/189], loss=72.3626
	step [96/189], loss=70.7361
	step [97/189], loss=64.7588
	step [98/189], loss=60.3274
	step [99/189], loss=87.6641
	step [100/189], loss=79.8094
	step [101/189], loss=60.0040
	step [102/189], loss=56.4407
	step [103/189], loss=86.0523
	step [104/189], loss=74.0137
	step [105/189], loss=59.4682
	step [106/189], loss=57.5843
	step [107/189], loss=65.4265
	step [108/189], loss=73.3970
	step [109/189], loss=74.3562
	step [110/189], loss=68.8030
	step [111/189], loss=61.1838
	step [112/189], loss=87.3820
	step [113/189], loss=71.4081
	step [114/189], loss=66.9487
	step [115/189], loss=87.1191
	step [116/189], loss=55.6538
	step [117/189], loss=69.4926
	step [118/189], loss=68.5136
	step [119/189], loss=70.3095
	step [120/189], loss=73.5576
	step [121/189], loss=78.3473
	step [122/189], loss=69.8560
	step [123/189], loss=61.7654
	step [124/189], loss=63.8544
	step [125/189], loss=59.0367
	step [126/189], loss=60.5594
	step [127/189], loss=63.7970
	step [128/189], loss=81.7870
	step [129/189], loss=74.1893
	step [130/189], loss=85.0245
	step [131/189], loss=62.4000
	step [132/189], loss=71.0277
	step [133/189], loss=72.8197
	step [134/189], loss=73.2593
	step [135/189], loss=47.4371
	step [136/189], loss=76.3966
	step [137/189], loss=68.4809
	step [138/189], loss=72.6241
	step [139/189], loss=80.9317
	step [140/189], loss=63.0473
	step [141/189], loss=72.3864
	step [142/189], loss=83.8255
	step [143/189], loss=63.2970
	step [144/189], loss=78.3362
	step [145/189], loss=63.7237
	step [146/189], loss=63.1814
	step [147/189], loss=72.8161
	step [148/189], loss=70.1450
	step [149/189], loss=66.5441
	step [150/189], loss=55.8343
	step [151/189], loss=83.3301
	step [152/189], loss=71.7934
	step [153/189], loss=84.2880
	step [154/189], loss=75.5089
	step [155/189], loss=70.1490
	step [156/189], loss=71.9309
	step [157/189], loss=68.2748
	step [158/189], loss=60.3776
	step [159/189], loss=75.4298
	step [160/189], loss=59.0743
	step [161/189], loss=73.0938
	step [162/189], loss=66.1122
	step [163/189], loss=83.4629
	step [164/189], loss=76.6798
	step [165/189], loss=68.2923
	step [166/189], loss=70.6991
	step [167/189], loss=65.9223
	step [168/189], loss=86.1484
	step [169/189], loss=69.5911
	step [170/189], loss=61.7902
	step [171/189], loss=62.5385
	step [172/189], loss=70.4533
	step [173/189], loss=73.2616
	step [174/189], loss=71.8070
	step [175/189], loss=82.2504
	step [176/189], loss=68.4714
	step [177/189], loss=58.3008
	step [178/189], loss=69.7241
	step [179/189], loss=68.0195
	step [180/189], loss=67.4316
	step [181/189], loss=64.5264
	step [182/189], loss=62.3531
	step [183/189], loss=82.0439
	step [184/189], loss=78.5010
	step [185/189], loss=59.9973
	step [186/189], loss=59.8031
	step [187/189], loss=71.9752
	step [188/189], loss=74.7159
	step [189/189], loss=30.4042
	Evaluating
	loss=0.0064, precision=0.4776, recall=0.8798, f1=0.6191
saving model as: 2_saved_model.pth
Training epoch 100
	step [1/189], loss=73.1036
	step [2/189], loss=62.4037
	step [3/189], loss=68.6980
	step [4/189], loss=70.2048
	step [5/189], loss=64.9289
	step [6/189], loss=69.7144
	step [7/189], loss=65.1011
	step [8/189], loss=72.6471
	step [9/189], loss=73.7117
	step [10/189], loss=72.2382
	step [11/189], loss=75.5936
	step [12/189], loss=57.0917
	step [13/189], loss=65.5414
	step [14/189], loss=60.2984
	step [15/189], loss=76.0125
	step [16/189], loss=65.0165
	step [17/189], loss=76.7721
	step [18/189], loss=80.5977
	step [19/189], loss=68.8263
	step [20/189], loss=73.2920
	step [21/189], loss=69.2245
	step [22/189], loss=64.4501
	step [23/189], loss=61.1927
	step [24/189], loss=75.3556
	step [25/189], loss=76.5645
	step [26/189], loss=75.4630
	step [27/189], loss=55.2750
	step [28/189], loss=69.1293
	step [29/189], loss=79.8291
	step [30/189], loss=79.6529
	step [31/189], loss=71.9355
	step [32/189], loss=60.6467
	step [33/189], loss=69.8888
	step [34/189], loss=72.5750
	step [35/189], loss=75.8994
	step [36/189], loss=61.2882
	step [37/189], loss=78.3842
	step [38/189], loss=86.6502
	step [39/189], loss=64.4132
	step [40/189], loss=63.4071
	step [41/189], loss=71.4477
	step [42/189], loss=64.6695
	step [43/189], loss=52.7923
	step [44/189], loss=81.7049
	step [45/189], loss=74.6634
	step [46/189], loss=70.8957
	step [47/189], loss=68.3269
	step [48/189], loss=78.8568
	step [49/189], loss=59.9723
	step [50/189], loss=74.8153
	step [51/189], loss=64.6425
	step [52/189], loss=67.2465
	step [53/189], loss=59.3914
	step [54/189], loss=55.8952
	step [55/189], loss=61.4856
	step [56/189], loss=79.6736
	step [57/189], loss=66.6915
	step [58/189], loss=64.4355
	step [59/189], loss=65.7496
	step [60/189], loss=87.7248
	step [61/189], loss=63.1177
	step [62/189], loss=67.7341
	step [63/189], loss=61.4433
	step [64/189], loss=62.6149
	step [65/189], loss=69.4565
	step [66/189], loss=72.3601
	step [67/189], loss=83.8042
	step [68/189], loss=77.2531
	step [69/189], loss=76.6292
	step [70/189], loss=72.2026
	step [71/189], loss=69.7810
	step [72/189], loss=73.9889
	step [73/189], loss=74.7733
	step [74/189], loss=72.9080
	step [75/189], loss=62.6561
	step [76/189], loss=59.3436
	step [77/189], loss=76.0840
	step [78/189], loss=66.5960
	step [79/189], loss=75.4202
	step [80/189], loss=64.7684
	step [81/189], loss=75.7539
	step [82/189], loss=72.7475
	step [83/189], loss=57.0648
	step [84/189], loss=66.6573
	step [85/189], loss=63.6800
	step [86/189], loss=53.6282
	step [87/189], loss=71.6697
	step [88/189], loss=70.8627
	step [89/189], loss=82.8090
	step [90/189], loss=67.7653
	step [91/189], loss=77.4936
	step [92/189], loss=71.6861
	step [93/189], loss=68.9041
	step [94/189], loss=66.0975
	step [95/189], loss=73.1443
	step [96/189], loss=75.1940
	step [97/189], loss=65.6322
	step [98/189], loss=73.1218
	step [99/189], loss=81.3332
	step [100/189], loss=59.4170
	step [101/189], loss=82.4286
	step [102/189], loss=74.6918
	step [103/189], loss=65.5515
	step [104/189], loss=62.4456
	step [105/189], loss=60.4458
	step [106/189], loss=67.5252
	step [107/189], loss=67.8139
	step [108/189], loss=79.8486
	step [109/189], loss=63.4707
	step [110/189], loss=68.8206
	step [111/189], loss=62.3723
	step [112/189], loss=72.3778
	step [113/189], loss=73.2969
	step [114/189], loss=65.1777
	step [115/189], loss=68.0636
	step [116/189], loss=71.4848
	step [117/189], loss=60.0981
	step [118/189], loss=60.9781
	step [119/189], loss=66.4351
	step [120/189], loss=64.6968
	step [121/189], loss=63.7052
	step [122/189], loss=76.1502
	step [123/189], loss=69.8814
	step [124/189], loss=79.8830
	step [125/189], loss=71.9370
	step [126/189], loss=70.1149
	step [127/189], loss=65.4248
	step [128/189], loss=74.5882
	step [129/189], loss=75.4320
	step [130/189], loss=70.9389
	step [131/189], loss=62.6602
	step [132/189], loss=63.3342
	step [133/189], loss=68.6563
	step [134/189], loss=62.2918
	step [135/189], loss=71.7976
	step [136/189], loss=66.2618
	step [137/189], loss=80.8906
	step [138/189], loss=72.7769
	step [139/189], loss=77.5996
	step [140/189], loss=65.5106
	step [141/189], loss=55.6551
	step [142/189], loss=65.5245
	step [143/189], loss=56.4477
	step [144/189], loss=75.7319
	step [145/189], loss=72.8510
	step [146/189], loss=79.4798
	step [147/189], loss=69.3048
	step [148/189], loss=82.4631
	step [149/189], loss=71.9300
	step [150/189], loss=73.7750
	step [151/189], loss=61.8527
	step [152/189], loss=69.4026
	step [153/189], loss=66.6956
	step [154/189], loss=64.9312
	step [155/189], loss=78.4296
	step [156/189], loss=62.8366
	step [157/189], loss=64.3695
	step [158/189], loss=78.2004
	step [159/189], loss=74.6170
	step [160/189], loss=67.5396
	step [161/189], loss=57.2383
	step [162/189], loss=64.2802
	step [163/189], loss=62.9894
	step [164/189], loss=71.9055
	step [165/189], loss=60.6417
	step [166/189], loss=65.7135
	step [167/189], loss=69.6375
	step [168/189], loss=66.3548
	step [169/189], loss=74.0113
	step [170/189], loss=59.0614
	step [171/189], loss=76.3482
	step [172/189], loss=74.9771
	step [173/189], loss=55.9395
	step [174/189], loss=75.0097
	step [175/189], loss=71.3320
	step [176/189], loss=61.2891
	step [177/189], loss=60.9252
	step [178/189], loss=80.1628
	step [179/189], loss=86.8449
	step [180/189], loss=68.5123
	step [181/189], loss=68.7619
	step [182/189], loss=49.8090
	step [183/189], loss=59.8939
	step [184/189], loss=76.3005
	step [185/189], loss=76.3666
	step [186/189], loss=58.7982
	step [187/189], loss=76.0921
	step [188/189], loss=75.0156
	step [189/189], loss=28.4758
	Evaluating
	loss=0.0071, precision=0.4356, recall=0.8797, f1=0.5827
Training epoch 101
	step [1/189], loss=64.1836
	step [2/189], loss=78.5991
	step [3/189], loss=62.8909
	step [4/189], loss=75.1384
	step [5/189], loss=71.8543
	step [6/189], loss=65.5492
	step [7/189], loss=60.3823
	step [8/189], loss=66.3460
	step [9/189], loss=65.5106
	step [10/189], loss=75.6786
	step [11/189], loss=51.8745
	step [12/189], loss=63.8588
	step [13/189], loss=76.0550
	step [14/189], loss=72.6113
	step [15/189], loss=66.5679
	step [16/189], loss=84.8960
	step [17/189], loss=76.5743
	step [18/189], loss=50.8923
	step [19/189], loss=65.2563
	step [20/189], loss=65.5636
	step [21/189], loss=68.4305
	step [22/189], loss=64.9214
	step [23/189], loss=75.5214
	step [24/189], loss=68.0319
	step [25/189], loss=65.6343
	step [26/189], loss=58.2931
	step [27/189], loss=73.5750
	step [28/189], loss=61.6190
	step [29/189], loss=71.2551
	step [30/189], loss=76.1722
	step [31/189], loss=57.1336
	step [32/189], loss=57.0538
	step [33/189], loss=72.3952
	step [34/189], loss=71.2315
	step [35/189], loss=69.8699
	step [36/189], loss=70.8047
	step [37/189], loss=76.0971
	step [38/189], loss=64.3989
	step [39/189], loss=64.2230
	step [40/189], loss=60.3082
	step [41/189], loss=69.8641
	step [42/189], loss=82.9970
	step [43/189], loss=71.7516
	step [44/189], loss=63.6399
	step [45/189], loss=65.0036
	step [46/189], loss=70.7834
	step [47/189], loss=64.7747
	step [48/189], loss=81.0675
	step [49/189], loss=76.3017
	step [50/189], loss=71.8051
	step [51/189], loss=64.7736
	step [52/189], loss=72.5144
	step [53/189], loss=72.2397
	step [54/189], loss=62.2665
	step [55/189], loss=61.8065
	step [56/189], loss=68.1567
	step [57/189], loss=78.0608
	step [58/189], loss=65.3210
	step [59/189], loss=71.5416
	step [60/189], loss=70.4363
	step [61/189], loss=73.3681
	step [62/189], loss=76.3100
	step [63/189], loss=73.3785
	step [64/189], loss=73.8916
	step [65/189], loss=67.7880
	step [66/189], loss=67.9191
	step [67/189], loss=69.8350
	step [68/189], loss=91.9739
	step [69/189], loss=64.6810
	step [70/189], loss=71.9438
	step [71/189], loss=80.2072
	step [72/189], loss=62.8736
	step [73/189], loss=68.0172
	step [74/189], loss=55.0666
	step [75/189], loss=67.4948
	step [76/189], loss=77.9759
	step [77/189], loss=70.7144
	step [78/189], loss=60.6546
	step [79/189], loss=69.9742
	step [80/189], loss=69.9113
	step [81/189], loss=65.2899
	step [82/189], loss=84.2291
	step [83/189], loss=89.9765
	step [84/189], loss=76.8573
	step [85/189], loss=71.6178
	step [86/189], loss=63.6243
	step [87/189], loss=63.0996
	step [88/189], loss=63.5409
	step [89/189], loss=65.4508
	step [90/189], loss=65.4125
	step [91/189], loss=77.5761
	step [92/189], loss=72.5714
	step [93/189], loss=81.3928
	step [94/189], loss=62.5441
	step [95/189], loss=72.2469
	step [96/189], loss=71.7130
	step [97/189], loss=59.1525
	step [98/189], loss=79.4543
	step [99/189], loss=64.2966
	step [100/189], loss=69.9544
	step [101/189], loss=69.0440
	step [102/189], loss=70.7690
	step [103/189], loss=82.6406
	step [104/189], loss=69.3497
	step [105/189], loss=74.3576
	step [106/189], loss=74.4362
	step [107/189], loss=59.9180
	step [108/189], loss=56.8290
	step [109/189], loss=54.5827
	step [110/189], loss=65.7170
	step [111/189], loss=78.7064
	step [112/189], loss=71.2946
	step [113/189], loss=70.3312
	step [114/189], loss=67.6159
	step [115/189], loss=60.2973
	step [116/189], loss=74.1245
	step [117/189], loss=69.6005
	step [118/189], loss=65.5019
	step [119/189], loss=59.5180
	step [120/189], loss=72.9364
	step [121/189], loss=66.3925
	step [122/189], loss=73.3059
	step [123/189], loss=69.8623
	step [124/189], loss=67.5565
	step [125/189], loss=70.3143
	step [126/189], loss=79.5145
	step [127/189], loss=69.6739
	step [128/189], loss=63.6466
	step [129/189], loss=74.6707
	step [130/189], loss=79.4482
	step [131/189], loss=60.6940
	step [132/189], loss=76.0519
	step [133/189], loss=68.6697
	step [134/189], loss=70.3602
	step [135/189], loss=65.1422
	step [136/189], loss=61.5183
	step [137/189], loss=72.9692
	step [138/189], loss=78.6236
	step [139/189], loss=66.2323
	step [140/189], loss=65.4221
	step [141/189], loss=70.7795
	step [142/189], loss=70.5398
	step [143/189], loss=78.3853
	step [144/189], loss=62.8289
	step [145/189], loss=54.1966
	step [146/189], loss=69.7384
	step [147/189], loss=60.9702
	step [148/189], loss=73.0736
	step [149/189], loss=70.7723
	step [150/189], loss=56.0738
	step [151/189], loss=81.2373
	step [152/189], loss=66.9355
	step [153/189], loss=82.8959
	step [154/189], loss=52.8688
	step [155/189], loss=67.6147
	step [156/189], loss=72.7483
	step [157/189], loss=75.0336
	step [158/189], loss=67.9554
	step [159/189], loss=89.1044
	step [160/189], loss=70.6604
	step [161/189], loss=76.9134
	step [162/189], loss=63.5364
	step [163/189], loss=72.5695
	step [164/189], loss=54.8462
	step [165/189], loss=71.2293
	step [166/189], loss=79.8153
	step [167/189], loss=77.7476
	step [168/189], loss=65.7592
	step [169/189], loss=67.2316
	step [170/189], loss=67.8115
	step [171/189], loss=75.0285
	step [172/189], loss=59.9083
	step [173/189], loss=73.6865
	step [174/189], loss=67.6537
	step [175/189], loss=59.8904
	step [176/189], loss=71.6407
	step [177/189], loss=79.9646
	step [178/189], loss=71.1790
	step [179/189], loss=65.1411
	step [180/189], loss=58.2537
	step [181/189], loss=60.0583
	step [182/189], loss=72.6433
	step [183/189], loss=71.9466
	step [184/189], loss=75.4279
	step [185/189], loss=60.2031
	step [186/189], loss=70.8559
	step [187/189], loss=71.7507
	step [188/189], loss=68.2117
	step [189/189], loss=24.3139
	Evaluating
	loss=0.0082, precision=0.3932, recall=0.8971, f1=0.5467
Training epoch 102
	step [1/189], loss=65.2534
	step [2/189], loss=76.8978
	step [3/189], loss=65.5918
	step [4/189], loss=76.6466
	step [5/189], loss=77.1088
	step [6/189], loss=63.7133
	step [7/189], loss=67.4381
	step [8/189], loss=70.6108
	step [9/189], loss=83.5099
	step [10/189], loss=74.1999
	step [11/189], loss=71.8666
	step [12/189], loss=70.2559
	step [13/189], loss=71.3643
	step [14/189], loss=62.2375
	step [15/189], loss=69.3810
	step [16/189], loss=73.5583
	step [17/189], loss=58.9218
	step [18/189], loss=68.1758
	step [19/189], loss=67.9313
	step [20/189], loss=73.7845
	step [21/189], loss=77.1544
	step [22/189], loss=58.2495
	step [23/189], loss=58.4024
	step [24/189], loss=75.4613
	step [25/189], loss=71.8427
	step [26/189], loss=62.0844
	step [27/189], loss=68.5345
	step [28/189], loss=70.8912
	step [29/189], loss=73.3993
	step [30/189], loss=60.5200
	step [31/189], loss=65.7309
	step [32/189], loss=65.0898
	step [33/189], loss=65.6854
	step [34/189], loss=65.7819
	step [35/189], loss=76.6835
	step [36/189], loss=77.4388
	step [37/189], loss=75.1495
	step [38/189], loss=65.4043
	step [39/189], loss=69.3597
	step [40/189], loss=65.2659
	step [41/189], loss=58.9360
	step [42/189], loss=81.5256
	step [43/189], loss=73.7164
	step [44/189], loss=58.2782
	step [45/189], loss=81.4421
	step [46/189], loss=77.3033
	step [47/189], loss=62.4283
	step [48/189], loss=62.6377
	step [49/189], loss=70.2736
	step [50/189], loss=59.0762
	step [51/189], loss=59.8596
	step [52/189], loss=64.1417
	step [53/189], loss=69.0379
	step [54/189], loss=70.4934
	step [55/189], loss=54.0651
	step [56/189], loss=73.6539
	step [57/189], loss=67.7683
	step [58/189], loss=77.2476
	step [59/189], loss=72.1247
	step [60/189], loss=67.9914
	step [61/189], loss=60.6369
	step [62/189], loss=72.2581
	step [63/189], loss=69.6448
	step [64/189], loss=67.9825
	step [65/189], loss=69.9316
	step [66/189], loss=72.4335
	step [67/189], loss=69.6838
	step [68/189], loss=61.2030
	step [69/189], loss=89.6947
	step [70/189], loss=70.7805
	step [71/189], loss=61.9187
	step [72/189], loss=73.8425
	step [73/189], loss=65.3067
	step [74/189], loss=70.5994
	step [75/189], loss=65.1730
	step [76/189], loss=72.0708
	step [77/189], loss=76.9646
	step [78/189], loss=79.6736
	step [79/189], loss=64.1309
	step [80/189], loss=81.1706
	step [81/189], loss=64.6454
	step [82/189], loss=82.2665
	step [83/189], loss=76.5214
	step [84/189], loss=64.8652
	step [85/189], loss=76.1129
	step [86/189], loss=68.5698
	step [87/189], loss=75.6418
	step [88/189], loss=53.8413
	step [89/189], loss=68.2857
	step [90/189], loss=76.4363
	step [91/189], loss=72.6207
	step [92/189], loss=60.4230
	step [93/189], loss=71.3346
	step [94/189], loss=70.5624
	step [95/189], loss=72.7138
	step [96/189], loss=63.6651
	step [97/189], loss=54.5754
	step [98/189], loss=63.4495
	step [99/189], loss=62.8697
	step [100/189], loss=73.5569
	step [101/189], loss=76.4298
	step [102/189], loss=69.7177
	step [103/189], loss=73.4282
	step [104/189], loss=72.7887
	step [105/189], loss=74.5475
	step [106/189], loss=74.0554
	step [107/189], loss=75.5001
	step [108/189], loss=67.7956
	step [109/189], loss=75.5329
	step [110/189], loss=77.1334
	step [111/189], loss=71.0161
	step [112/189], loss=73.6673
	step [113/189], loss=72.8299
	step [114/189], loss=71.1717
	step [115/189], loss=83.4158
	step [116/189], loss=60.0873
	step [117/189], loss=72.1822
	step [118/189], loss=77.1114
	step [119/189], loss=67.0724
	step [120/189], loss=70.5883
	step [121/189], loss=68.4104
	step [122/189], loss=66.9819
	step [123/189], loss=63.6305
	step [124/189], loss=66.0114
	step [125/189], loss=69.2726
	step [126/189], loss=75.5543
	step [127/189], loss=65.4638
	step [128/189], loss=60.2149
	step [129/189], loss=76.6303
	step [130/189], loss=71.4081
	step [131/189], loss=76.1148
	step [132/189], loss=75.0686
	step [133/189], loss=66.5861
	step [134/189], loss=65.8411
	step [135/189], loss=68.0316
	step [136/189], loss=75.0105
	step [137/189], loss=66.4365
	step [138/189], loss=64.8177
	step [139/189], loss=69.4785
	step [140/189], loss=67.4070
	step [141/189], loss=68.8828
	step [142/189], loss=62.1770
	step [143/189], loss=53.7142
	step [144/189], loss=63.3524
	step [145/189], loss=69.6896
	step [146/189], loss=58.2549
	step [147/189], loss=76.8647
	step [148/189], loss=60.6493
	step [149/189], loss=68.1039
	step [150/189], loss=81.5494
	step [151/189], loss=72.2920
	step [152/189], loss=60.0914
	step [153/189], loss=64.0685
	step [154/189], loss=71.8263
	step [155/189], loss=73.8714
	step [156/189], loss=62.7861
	step [157/189], loss=73.3715
	step [158/189], loss=59.0370
	step [159/189], loss=66.0443
	step [160/189], loss=66.0556
	step [161/189], loss=71.7189
	step [162/189], loss=72.9569
	step [163/189], loss=67.5175
	step [164/189], loss=73.6408
	step [165/189], loss=78.3960
	step [166/189], loss=69.9403
	step [167/189], loss=61.5304
	step [168/189], loss=73.7416
	step [169/189], loss=74.1703
	step [170/189], loss=62.4687
	step [171/189], loss=66.8326
	step [172/189], loss=69.9795
	step [173/189], loss=79.7969
	step [174/189], loss=69.9385
	step [175/189], loss=77.0558
	step [176/189], loss=78.1138
	step [177/189], loss=70.0916
	step [178/189], loss=62.2294
	step [179/189], loss=75.6948
	step [180/189], loss=60.1635
	step [181/189], loss=67.7953
	step [182/189], loss=53.8785
	step [183/189], loss=72.5837
	step [184/189], loss=66.8544
	step [185/189], loss=63.8773
	step [186/189], loss=67.5628
	step [187/189], loss=52.4947
	step [188/189], loss=71.4111
	step [189/189], loss=29.3464
	Evaluating
	loss=0.0077, precision=0.4148, recall=0.8780, f1=0.5634
Training epoch 103
	step [1/189], loss=62.6109
	step [2/189], loss=67.6368
	step [3/189], loss=63.0774
	step [4/189], loss=56.2162
	step [5/189], loss=57.4732
	step [6/189], loss=65.1393
	step [7/189], loss=65.7642
	step [8/189], loss=61.3806
	step [9/189], loss=71.8390
	step [10/189], loss=71.1538
	step [11/189], loss=63.4911
	step [12/189], loss=68.3063
	step [13/189], loss=54.7901
	step [14/189], loss=82.1392
	step [15/189], loss=67.4502
	step [16/189], loss=67.3123
	step [17/189], loss=76.7265
	step [18/189], loss=68.3099
	step [19/189], loss=69.2216
	step [20/189], loss=70.5767
	step [21/189], loss=70.1458
	step [22/189], loss=89.6597
	step [23/189], loss=63.4572
	step [24/189], loss=70.3988
	step [25/189], loss=61.1230
	step [26/189], loss=62.1586
	step [27/189], loss=81.4721
	step [28/189], loss=65.8585
	step [29/189], loss=77.4298
	step [30/189], loss=72.1919
	step [31/189], loss=61.7377
	step [32/189], loss=57.8529
	step [33/189], loss=72.8301
	step [34/189], loss=72.9856
	step [35/189], loss=61.4649
	step [36/189], loss=66.2580
	step [37/189], loss=72.1195
	step [38/189], loss=60.5579
	step [39/189], loss=61.4633
	step [40/189], loss=66.1224
	step [41/189], loss=75.3992
	step [42/189], loss=59.7066
	step [43/189], loss=77.0490
	step [44/189], loss=72.5850
	step [45/189], loss=72.3303
	step [46/189], loss=65.4303
	step [47/189], loss=73.2727
	step [48/189], loss=68.0919
	step [49/189], loss=70.6285
	step [50/189], loss=58.8926
	step [51/189], loss=70.0795
	step [52/189], loss=85.4230
	step [53/189], loss=61.9666
	step [54/189], loss=73.8621
	step [55/189], loss=68.7558
	step [56/189], loss=71.8613
	step [57/189], loss=67.0801
	step [58/189], loss=75.7267
	step [59/189], loss=80.4119
	step [60/189], loss=65.1510
	step [61/189], loss=60.0757
	step [62/189], loss=77.7275
	step [63/189], loss=57.6767
	step [64/189], loss=59.6895
	step [65/189], loss=56.4591
	step [66/189], loss=81.0085
	step [67/189], loss=74.1889
	step [68/189], loss=67.3628
	step [69/189], loss=71.8909
	step [70/189], loss=67.3043
	step [71/189], loss=54.9534
	step [72/189], loss=77.8390
	step [73/189], loss=56.2190
	step [74/189], loss=63.2329
	step [75/189], loss=65.8794
	step [76/189], loss=67.9366
	step [77/189], loss=59.9677
	step [78/189], loss=83.2391
	step [79/189], loss=71.6196
	step [80/189], loss=66.5916
	step [81/189], loss=69.2275
	step [82/189], loss=75.9198
	step [83/189], loss=74.2134
	step [84/189], loss=74.9836
	step [85/189], loss=71.4833
	step [86/189], loss=81.4994
	step [87/189], loss=70.6501
	step [88/189], loss=56.5775
	step [89/189], loss=77.9144
	step [90/189], loss=76.7143
	step [91/189], loss=77.4292
	step [92/189], loss=55.9314
	step [93/189], loss=74.2150
	step [94/189], loss=62.3141
	step [95/189], loss=76.4029
	step [96/189], loss=62.6915
	step [97/189], loss=75.6728
	step [98/189], loss=73.7122
	step [99/189], loss=78.5197
	step [100/189], loss=49.5385
	step [101/189], loss=66.3745
	step [102/189], loss=69.5043
	step [103/189], loss=67.2089
	step [104/189], loss=69.1585
	step [105/189], loss=67.4786
	step [106/189], loss=65.8608
	step [107/189], loss=70.2492
	step [108/189], loss=71.4345
	step [109/189], loss=76.8490
	step [110/189], loss=78.2532
	step [111/189], loss=72.6327
	step [112/189], loss=71.2363
	step [113/189], loss=60.0477
	step [114/189], loss=62.5744
	step [115/189], loss=80.5490
	step [116/189], loss=71.7111
	step [117/189], loss=66.7118
	step [118/189], loss=62.9120
	step [119/189], loss=68.6591
	step [120/189], loss=66.3083
	step [121/189], loss=75.9972
	step [122/189], loss=66.4947
	step [123/189], loss=78.4443
	step [124/189], loss=72.0406
	step [125/189], loss=67.8657
	step [126/189], loss=73.6323
	step [127/189], loss=70.7921
	step [128/189], loss=72.1508
	step [129/189], loss=63.1595
	step [130/189], loss=57.2784
	step [131/189], loss=68.5265
	step [132/189], loss=76.0496
	step [133/189], loss=63.3624
	step [134/189], loss=65.1791
	step [135/189], loss=58.4999
	step [136/189], loss=64.9240
	step [137/189], loss=71.9372
	step [138/189], loss=76.7537
	step [139/189], loss=63.1866
	step [140/189], loss=59.2741
	step [141/189], loss=67.2950
	step [142/189], loss=80.7574
	step [143/189], loss=77.8804
	step [144/189], loss=66.4150
	step [145/189], loss=75.0156
	step [146/189], loss=70.8820
	step [147/189], loss=70.1157
	step [148/189], loss=69.0922
	step [149/189], loss=70.4267
	step [150/189], loss=77.3201
	step [151/189], loss=75.1976
	step [152/189], loss=71.0651
	step [153/189], loss=59.6866
	step [154/189], loss=62.9346
	step [155/189], loss=70.1579
	step [156/189], loss=78.0354
	step [157/189], loss=67.8412
	step [158/189], loss=74.2688
	step [159/189], loss=74.8389
	step [160/189], loss=86.0711
	step [161/189], loss=68.1762
	step [162/189], loss=74.1462
	step [163/189], loss=63.3329
	step [164/189], loss=77.0632
	step [165/189], loss=69.9427
	step [166/189], loss=78.3902
	step [167/189], loss=66.7582
	step [168/189], loss=65.5758
	step [169/189], loss=57.2262
	step [170/189], loss=79.2579
	step [171/189], loss=60.6238
	step [172/189], loss=69.8676
	step [173/189], loss=68.5612
	step [174/189], loss=61.3622
	step [175/189], loss=65.9031
	step [176/189], loss=70.4857
	step [177/189], loss=63.3347
	step [178/189], loss=63.3862
	step [179/189], loss=59.9180
	step [180/189], loss=47.6848
	step [181/189], loss=74.5283
	step [182/189], loss=65.7150
	step [183/189], loss=58.7550
	step [184/189], loss=67.9116
	step [185/189], loss=73.7364
	step [186/189], loss=80.6421
	step [187/189], loss=73.1616
	step [188/189], loss=82.0029
	step [189/189], loss=23.0098
	Evaluating
	loss=0.0087, precision=0.3734, recall=0.8799, f1=0.5243
Training epoch 104
	step [1/189], loss=74.5011
	step [2/189], loss=56.5327
	step [3/189], loss=72.8766
	step [4/189], loss=57.9829
	step [5/189], loss=68.1916
	step [6/189], loss=75.9102
	step [7/189], loss=73.8932
	step [8/189], loss=81.7653
	step [9/189], loss=73.6575
	step [10/189], loss=91.3456
	step [11/189], loss=80.6931
	step [12/189], loss=73.2444
	step [13/189], loss=58.4781
	step [14/189], loss=61.8493
	step [15/189], loss=67.9165
	step [16/189], loss=66.4443
	step [17/189], loss=77.5284
	step [18/189], loss=63.2654
	step [19/189], loss=67.9882
	step [20/189], loss=66.8079
	step [21/189], loss=69.7845
	step [22/189], loss=71.7855
	step [23/189], loss=57.0285
	step [24/189], loss=60.5204
	step [25/189], loss=62.9081
	step [26/189], loss=68.0016
	step [27/189], loss=64.8012
	step [28/189], loss=64.5484
	step [29/189], loss=67.5893
	step [30/189], loss=68.3257
	step [31/189], loss=88.6389
	step [32/189], loss=82.7167
	step [33/189], loss=64.0622
	step [34/189], loss=69.3263
	step [35/189], loss=63.6068
	step [36/189], loss=59.9544
	step [37/189], loss=55.9813
	step [38/189], loss=70.3210
	step [39/189], loss=67.2462
	step [40/189], loss=66.2994
	step [41/189], loss=60.3399
	step [42/189], loss=61.7010
	step [43/189], loss=60.7371
	step [44/189], loss=71.9946
	step [45/189], loss=62.4473
	step [46/189], loss=71.0637
	step [47/189], loss=70.4180
	step [48/189], loss=70.6610
	step [49/189], loss=67.6721
	step [50/189], loss=58.9708
	step [51/189], loss=57.4965
	step [52/189], loss=74.4737
	step [53/189], loss=64.2842
	step [54/189], loss=73.0766
	step [55/189], loss=69.2593
	step [56/189], loss=71.1720
	step [57/189], loss=83.5901
	step [58/189], loss=87.8822
	step [59/189], loss=69.6254
	step [60/189], loss=72.1417
	step [61/189], loss=76.5788
	step [62/189], loss=62.8848
	step [63/189], loss=59.6893
	step [64/189], loss=69.1453
	step [65/189], loss=66.0436
	step [66/189], loss=83.3266
	step [67/189], loss=63.9068
	step [68/189], loss=62.0200
	step [69/189], loss=72.2383
	step [70/189], loss=67.7798
	step [71/189], loss=60.6958
	step [72/189], loss=78.4851
	step [73/189], loss=67.7226
	step [74/189], loss=72.4935
	step [75/189], loss=81.9362
	step [76/189], loss=76.8211
	step [77/189], loss=67.5824
	step [78/189], loss=71.0946
	step [79/189], loss=69.8122
	step [80/189], loss=74.7635
	step [81/189], loss=61.0196
	step [82/189], loss=69.6172
	step [83/189], loss=64.2897
	step [84/189], loss=61.9744
	step [85/189], loss=74.0301
	step [86/189], loss=72.6486
	step [87/189], loss=72.9603
	step [88/189], loss=68.4230
	step [89/189], loss=63.9843
	step [90/189], loss=60.8567
	step [91/189], loss=74.5679
	step [92/189], loss=68.0127
	step [93/189], loss=68.1275
	step [94/189], loss=62.8273
	step [95/189], loss=60.2231
	step [96/189], loss=67.8954
	step [97/189], loss=59.5608
	step [98/189], loss=67.0668
	step [99/189], loss=71.8120
	step [100/189], loss=81.2848
	step [101/189], loss=62.0827
	step [102/189], loss=93.4111
	step [103/189], loss=70.0139
	step [104/189], loss=60.3865
	step [105/189], loss=55.1629
	step [106/189], loss=60.7892
	step [107/189], loss=63.6090
	step [108/189], loss=71.5822
	step [109/189], loss=63.7695
	step [110/189], loss=68.4397
	step [111/189], loss=62.7334
	step [112/189], loss=61.0956
	step [113/189], loss=68.6538
	step [114/189], loss=75.9694
	step [115/189], loss=70.7907
	step [116/189], loss=61.3508
	step [117/189], loss=68.4229
	step [118/189], loss=61.2409
	step [119/189], loss=54.3629
	step [120/189], loss=53.3333
	step [121/189], loss=66.3605
	step [122/189], loss=64.6242
	step [123/189], loss=50.5024
	step [124/189], loss=73.2372
	step [125/189], loss=65.5744
	step [126/189], loss=64.9194
	step [127/189], loss=77.7167
	step [128/189], loss=76.4527
	step [129/189], loss=66.4232
	step [130/189], loss=74.0391
	step [131/189], loss=70.1712
	step [132/189], loss=74.2400
	step [133/189], loss=68.0129
	step [134/189], loss=66.0274
	step [135/189], loss=53.1419
	step [136/189], loss=81.5912
	step [137/189], loss=67.3678
	step [138/189], loss=76.8157
	step [139/189], loss=76.3481
	step [140/189], loss=64.8065
	step [141/189], loss=59.2943
	step [142/189], loss=63.6518
	step [143/189], loss=69.3010
	step [144/189], loss=78.7589
	step [145/189], loss=79.9023
	step [146/189], loss=64.2362
	step [147/189], loss=70.2018
	step [148/189], loss=79.5607
	step [149/189], loss=68.7096
	step [150/189], loss=74.2087
	step [151/189], loss=62.9897
	step [152/189], loss=71.3534
	step [153/189], loss=67.8081
	step [154/189], loss=66.7645
	step [155/189], loss=79.9668
	step [156/189], loss=60.6030
	step [157/189], loss=51.5620
	step [158/189], loss=65.0903
	step [159/189], loss=71.7195
	step [160/189], loss=77.4092
	step [161/189], loss=66.5566
	step [162/189], loss=58.0469
	step [163/189], loss=75.8848
	step [164/189], loss=81.4802
	step [165/189], loss=76.4284
	step [166/189], loss=71.7128
	step [167/189], loss=55.4554
	step [168/189], loss=60.8343
	step [169/189], loss=69.6600
	step [170/189], loss=70.6485
	step [171/189], loss=69.0938
	step [172/189], loss=69.8716
	step [173/189], loss=71.8807
	step [174/189], loss=68.4016
	step [175/189], loss=69.0822
	step [176/189], loss=63.1441
	step [177/189], loss=71.3189
	step [178/189], loss=74.4872
	step [179/189], loss=74.2261
	step [180/189], loss=66.2862
	step [181/189], loss=68.5574
	step [182/189], loss=73.7265
	step [183/189], loss=54.2725
	step [184/189], loss=80.0193
	step [185/189], loss=75.4854
	step [186/189], loss=67.4033
	step [187/189], loss=60.9004
	step [188/189], loss=63.2380
	step [189/189], loss=34.6409
	Evaluating
	loss=0.0074, precision=0.4303, recall=0.8828, f1=0.5786
Training epoch 105
	step [1/189], loss=61.5604
	step [2/189], loss=69.9103
	step [3/189], loss=77.4733
	step [4/189], loss=71.9186
	step [5/189], loss=66.7014
	step [6/189], loss=81.5606
	step [7/189], loss=80.0888
	step [8/189], loss=75.4581
	step [9/189], loss=75.4516
	step [10/189], loss=74.2335
	step [11/189], loss=58.8529
	step [12/189], loss=59.8212
	step [13/189], loss=73.4711
	step [14/189], loss=76.7712
	step [15/189], loss=66.0430
	step [16/189], loss=64.2904
	step [17/189], loss=67.5685
	step [18/189], loss=81.8029
	step [19/189], loss=63.6088
	step [20/189], loss=72.9890
	step [21/189], loss=71.3611
	step [22/189], loss=76.9322
	step [23/189], loss=63.3610
	step [24/189], loss=62.6148
	step [25/189], loss=57.9700
	step [26/189], loss=75.0390
	step [27/189], loss=82.0958
	step [28/189], loss=81.2456
	step [29/189], loss=70.4517
	step [30/189], loss=61.1015
	step [31/189], loss=76.7656
	step [32/189], loss=68.1775
	step [33/189], loss=76.9943
	step [34/189], loss=67.3591
	step [35/189], loss=63.8328
	step [36/189], loss=72.4475
	step [37/189], loss=71.5180
	step [38/189], loss=77.8721
	step [39/189], loss=58.9487
	step [40/189], loss=65.9873
	step [41/189], loss=67.9679
	step [42/189], loss=73.1591
	step [43/189], loss=60.5370
	step [44/189], loss=58.8288
	step [45/189], loss=67.2800
	step [46/189], loss=57.9182
	step [47/189], loss=59.8013
	step [48/189], loss=76.5356
	step [49/189], loss=56.2668
	step [50/189], loss=69.4737
	step [51/189], loss=60.2853
	step [52/189], loss=66.9036
	step [53/189], loss=52.4489
	step [54/189], loss=74.0562
	step [55/189], loss=69.6733
	step [56/189], loss=71.5808
	step [57/189], loss=75.5315
	step [58/189], loss=70.6190
	step [59/189], loss=60.2864
	step [60/189], loss=81.6217
	step [61/189], loss=86.6870
	step [62/189], loss=82.7011
	step [63/189], loss=71.4633
	step [64/189], loss=65.8384
	step [65/189], loss=60.4824
	step [66/189], loss=67.8068
	step [67/189], loss=67.4488
	step [68/189], loss=62.8599
	step [69/189], loss=62.2801
	step [70/189], loss=82.6714
	step [71/189], loss=73.7459
	step [72/189], loss=59.2823
	step [73/189], loss=79.4433
	step [74/189], loss=66.5642
	step [75/189], loss=72.1521
	step [76/189], loss=71.3444
	step [77/189], loss=58.8889
	step [78/189], loss=65.3615
	step [79/189], loss=66.9235
	step [80/189], loss=59.5994
	step [81/189], loss=69.9749
	step [82/189], loss=76.2028
	step [83/189], loss=64.7895
	step [84/189], loss=62.3730
	step [85/189], loss=74.4372
	step [86/189], loss=71.0137
	step [87/189], loss=64.3387
	step [88/189], loss=69.6326
	step [89/189], loss=62.5944
	step [90/189], loss=68.5670
	step [91/189], loss=68.9125
	step [92/189], loss=62.2104
	step [93/189], loss=70.1626
	step [94/189], loss=69.3813
	step [95/189], loss=56.2160
	step [96/189], loss=61.0607
	step [97/189], loss=77.5549
	step [98/189], loss=69.6558
	step [99/189], loss=66.7462
	step [100/189], loss=65.7807
	step [101/189], loss=63.8791
	step [102/189], loss=68.6623
	step [103/189], loss=60.9201
	step [104/189], loss=62.7109
	step [105/189], loss=66.8403
	step [106/189], loss=69.4252
	step [107/189], loss=73.7660
	step [108/189], loss=68.1498
	step [109/189], loss=59.6245
	step [110/189], loss=67.7660
	step [111/189], loss=81.5024
	step [112/189], loss=78.6403
	step [113/189], loss=75.8147
	step [114/189], loss=71.8061
	step [115/189], loss=76.1782
	step [116/189], loss=75.8414
	step [117/189], loss=63.1365
	step [118/189], loss=83.1606
	step [119/189], loss=56.4808
	step [120/189], loss=73.2623
	step [121/189], loss=71.1143
	step [122/189], loss=70.1392
	step [123/189], loss=58.3284
	step [124/189], loss=62.4055
	step [125/189], loss=73.3903
	step [126/189], loss=60.4690
	step [127/189], loss=75.0740
	step [128/189], loss=66.9537
	step [129/189], loss=67.9942
	step [130/189], loss=61.0004
	step [131/189], loss=50.3455
	step [132/189], loss=66.5636
	step [133/189], loss=60.5796
	step [134/189], loss=77.2537
	step [135/189], loss=80.9635
	step [136/189], loss=81.6717
	step [137/189], loss=67.8438
	step [138/189], loss=68.6973
	step [139/189], loss=65.6158
	step [140/189], loss=67.2907
	step [141/189], loss=72.2875
	step [142/189], loss=73.9127
	step [143/189], loss=79.7848
	step [144/189], loss=58.0283
	step [145/189], loss=77.1965
	step [146/189], loss=75.0714
	step [147/189], loss=67.0744
	step [148/189], loss=67.5018
	step [149/189], loss=73.4737
	step [150/189], loss=68.6394
	step [151/189], loss=76.3754
	step [152/189], loss=72.5860
	step [153/189], loss=71.0812
	step [154/189], loss=65.6347
	step [155/189], loss=61.3772
	step [156/189], loss=73.5593
	step [157/189], loss=63.6196
	step [158/189], loss=73.4325
	step [159/189], loss=66.7718
	step [160/189], loss=62.0990
	step [161/189], loss=83.5161
	step [162/189], loss=71.1647
	step [163/189], loss=63.2001
	step [164/189], loss=71.4999
	step [165/189], loss=59.2222
	step [166/189], loss=70.5983
	step [167/189], loss=81.7976
	step [168/189], loss=62.9720
	step [169/189], loss=72.5190
	step [170/189], loss=52.9980
	step [171/189], loss=68.7684
	step [172/189], loss=70.3064
	step [173/189], loss=61.7264
	step [174/189], loss=54.2304
	step [175/189], loss=67.7940
	step [176/189], loss=73.2514
	step [177/189], loss=74.1530
	step [178/189], loss=55.1007
	step [179/189], loss=62.2809
	step [180/189], loss=59.5557
	step [181/189], loss=61.1421
	step [182/189], loss=56.5652
	step [183/189], loss=65.9430
	step [184/189], loss=78.6035
	step [185/189], loss=67.4721
	step [186/189], loss=59.2059
	step [187/189], loss=83.1103
	step [188/189], loss=74.9599
	step [189/189], loss=30.3177
	Evaluating
	loss=0.0077, precision=0.4070, recall=0.8910, f1=0.5588
Training epoch 106
	step [1/189], loss=67.7393
	step [2/189], loss=62.6995
	step [3/189], loss=66.3270
	step [4/189], loss=80.2226
	step [5/189], loss=60.7418
	step [6/189], loss=75.5084
	step [7/189], loss=73.8237
	step [8/189], loss=54.6250
	step [9/189], loss=68.3759
	step [10/189], loss=71.1435
	step [11/189], loss=65.4110
	step [12/189], loss=78.2687
	step [13/189], loss=69.5430
	step [14/189], loss=73.2919
	step [15/189], loss=69.0728
	step [16/189], loss=70.9694
	step [17/189], loss=75.1639
	step [18/189], loss=76.8601
	step [19/189], loss=72.8158
	step [20/189], loss=60.8510
	step [21/189], loss=76.1210
	step [22/189], loss=66.7099
	step [23/189], loss=78.4135
	step [24/189], loss=59.9747
	step [25/189], loss=74.3624
	step [26/189], loss=64.7014
	step [27/189], loss=71.8255
	step [28/189], loss=60.0216
	step [29/189], loss=53.9519
	step [30/189], loss=67.3024
	step [31/189], loss=69.2480
	step [32/189], loss=61.7367
	step [33/189], loss=72.4710
	step [34/189], loss=83.6947
	step [35/189], loss=64.3936
	step [36/189], loss=62.3769
	step [37/189], loss=70.2894
	step [38/189], loss=78.3286
	step [39/189], loss=65.4848
	step [40/189], loss=67.0013
	step [41/189], loss=66.9916
	step [42/189], loss=70.4491
	step [43/189], loss=72.2574
	step [44/189], loss=73.8292
	step [45/189], loss=75.3892
	step [46/189], loss=65.9142
	step [47/189], loss=72.2759
	step [48/189], loss=71.4196
	step [49/189], loss=64.3697
	step [50/189], loss=65.7551
	step [51/189], loss=66.3946
	step [52/189], loss=58.8893
	step [53/189], loss=73.9266
	step [54/189], loss=72.9074
	step [55/189], loss=57.1976
	step [56/189], loss=70.1634
	step [57/189], loss=66.9800
	step [58/189], loss=75.9115
	step [59/189], loss=66.0113
	step [60/189], loss=71.8403
	step [61/189], loss=62.5324
	step [62/189], loss=66.3404
	step [63/189], loss=58.7992
	step [64/189], loss=61.6310
	step [65/189], loss=73.1198
	step [66/189], loss=70.1886
	step [67/189], loss=81.1882
	step [68/189], loss=73.5291
	step [69/189], loss=68.5453
	step [70/189], loss=69.2592
	step [71/189], loss=58.1631
	step [72/189], loss=68.5199
	step [73/189], loss=62.8400
	step [74/189], loss=57.6137
	step [75/189], loss=71.6395
	step [76/189], loss=75.2360
	step [77/189], loss=66.5924
	step [78/189], loss=69.2089
	step [79/189], loss=74.1972
	step [80/189], loss=60.7033
	step [81/189], loss=72.2920
	step [82/189], loss=71.6891
	step [83/189], loss=58.0890
	step [84/189], loss=69.2777
	step [85/189], loss=71.2182
	step [86/189], loss=60.0913
	step [87/189], loss=61.2876
	step [88/189], loss=62.2341
	step [89/189], loss=68.6327
	step [90/189], loss=72.9660
	step [91/189], loss=70.9072
	step [92/189], loss=63.8875
	step [93/189], loss=78.3935
	step [94/189], loss=69.7396
	step [95/189], loss=75.9586
	step [96/189], loss=62.1817
	step [97/189], loss=67.2247
	step [98/189], loss=83.0768
	step [99/189], loss=64.3131
	step [100/189], loss=79.2556
	step [101/189], loss=55.5204
	step [102/189], loss=69.8109
	step [103/189], loss=58.2101
	step [104/189], loss=71.1464
	step [105/189], loss=66.1454
	step [106/189], loss=76.7782
	step [107/189], loss=68.7809
	step [108/189], loss=56.5500
	step [109/189], loss=61.5689
	step [110/189], loss=74.9964
	step [111/189], loss=72.5438
	step [112/189], loss=67.6469
	step [113/189], loss=73.3325
	step [114/189], loss=66.0581
	step [115/189], loss=67.5102
	step [116/189], loss=67.9891
	step [117/189], loss=64.3304
	step [118/189], loss=62.0407
	step [119/189], loss=58.4836
	step [120/189], loss=71.6022
	step [121/189], loss=76.9927
	step [122/189], loss=63.8033
	step [123/189], loss=73.3562
	step [124/189], loss=78.1656
	step [125/189], loss=70.0098
	step [126/189], loss=73.0153
	step [127/189], loss=63.3148
	step [128/189], loss=85.5370
	step [129/189], loss=70.2955
	step [130/189], loss=63.8778
	step [131/189], loss=51.2411
	step [132/189], loss=67.0914
	step [133/189], loss=74.7996
	step [134/189], loss=71.7200
	step [135/189], loss=68.6919
	step [136/189], loss=67.6574
	step [137/189], loss=79.4960
	step [138/189], loss=66.5444
	step [139/189], loss=72.0810
	step [140/189], loss=69.9900
	step [141/189], loss=73.4804
	step [142/189], loss=88.8094
	step [143/189], loss=61.2512
	step [144/189], loss=68.2070
	step [145/189], loss=66.6513
	step [146/189], loss=65.0342
	step [147/189], loss=72.3987
	step [148/189], loss=67.1947
	step [149/189], loss=68.3419
	step [150/189], loss=61.4093
	step [151/189], loss=64.8683
	step [152/189], loss=76.0018
	step [153/189], loss=70.0502
	step [154/189], loss=74.0257
	step [155/189], loss=66.2368
	step [156/189], loss=71.4291
	step [157/189], loss=62.3253
	step [158/189], loss=58.3613
	step [159/189], loss=72.7564
	step [160/189], loss=66.8451
	step [161/189], loss=72.5420
	step [162/189], loss=68.2103
	step [163/189], loss=62.6677
	step [164/189], loss=57.6288
	step [165/189], loss=76.0697
	step [166/189], loss=61.7301
	step [167/189], loss=70.0205
	step [168/189], loss=51.9516
	step [169/189], loss=69.2527
	step [170/189], loss=67.1927
	step [171/189], loss=57.0896
	step [172/189], loss=74.8358
	step [173/189], loss=70.3412
	step [174/189], loss=64.3878
	step [175/189], loss=84.6500
	step [176/189], loss=61.4505
	step [177/189], loss=71.2702
	step [178/189], loss=64.9274
	step [179/189], loss=67.0544
	step [180/189], loss=74.6860
	step [181/189], loss=69.2437
	step [182/189], loss=74.0063
	step [183/189], loss=60.8556
	step [184/189], loss=70.5844
	step [185/189], loss=66.2828
	step [186/189], loss=75.9000
	step [187/189], loss=70.9545
	step [188/189], loss=77.3744
	step [189/189], loss=24.2741
	Evaluating
	loss=0.0073, precision=0.4254, recall=0.8808, f1=0.5737
Training epoch 107
	step [1/189], loss=67.7927
	step [2/189], loss=81.0714
	step [3/189], loss=65.1729
	step [4/189], loss=59.3680
	step [5/189], loss=76.0208
	step [6/189], loss=64.0407
	step [7/189], loss=60.7874
	step [8/189], loss=63.5590
	step [9/189], loss=60.8721
	step [10/189], loss=62.9883
	step [11/189], loss=75.7515
	step [12/189], loss=52.9426
	step [13/189], loss=66.7927
	step [14/189], loss=70.8859
	step [15/189], loss=74.2212
	step [16/189], loss=69.5406
	step [17/189], loss=56.1157
	step [18/189], loss=75.7728
	step [19/189], loss=67.9749
	step [20/189], loss=67.1224
	step [21/189], loss=59.6684
	step [22/189], loss=64.9051
	step [23/189], loss=74.3457
	step [24/189], loss=63.4452
	step [25/189], loss=72.4398
	step [26/189], loss=58.5687
	step [27/189], loss=66.4378
	step [28/189], loss=69.0420
	step [29/189], loss=81.9065
	step [30/189], loss=66.0099
	step [31/189], loss=75.3579
	step [32/189], loss=64.4995
	step [33/189], loss=75.2154
	step [34/189], loss=57.2243
	step [35/189], loss=74.6811
	step [36/189], loss=77.7279
	step [37/189], loss=65.1822
	step [38/189], loss=73.9181
	step [39/189], loss=63.4464
	step [40/189], loss=70.9453
	step [41/189], loss=72.4272
	step [42/189], loss=58.2198
	step [43/189], loss=73.7125
	step [44/189], loss=73.3436
	step [45/189], loss=69.0243
	step [46/189], loss=56.0089
	step [47/189], loss=66.0619
	step [48/189], loss=66.5588
	step [49/189], loss=65.2538
	step [50/189], loss=61.1378
	step [51/189], loss=66.3435
	step [52/189], loss=66.5767
	step [53/189], loss=78.2310
	step [54/189], loss=69.0770
	step [55/189], loss=69.1189
	step [56/189], loss=62.1522
	step [57/189], loss=69.0475
	step [58/189], loss=73.5561
	step [59/189], loss=62.7207
	step [60/189], loss=67.8407
	step [61/189], loss=75.1514
	step [62/189], loss=75.1993
	step [63/189], loss=76.7014
	step [64/189], loss=57.3950
	step [65/189], loss=61.4327
	step [66/189], loss=80.1761
	step [67/189], loss=66.5677
	step [68/189], loss=69.3397
	step [69/189], loss=67.0457
	step [70/189], loss=54.5748
	step [71/189], loss=68.1416
	step [72/189], loss=66.5049
	step [73/189], loss=62.3583
	step [74/189], loss=53.5254
	step [75/189], loss=63.8606
	step [76/189], loss=66.0997
	step [77/189], loss=65.9484
	step [78/189], loss=73.3277
	step [79/189], loss=66.6373
	step [80/189], loss=65.6685
	step [81/189], loss=64.5159
	step [82/189], loss=63.5784
	step [83/189], loss=77.4282
	step [84/189], loss=66.8632
	step [85/189], loss=60.8822
	step [86/189], loss=79.1158
	step [87/189], loss=81.2616
	step [88/189], loss=66.0511
	step [89/189], loss=62.0145
	step [90/189], loss=75.9951
	step [91/189], loss=68.0906
	step [92/189], loss=72.8688
	step [93/189], loss=72.3320
	step [94/189], loss=70.9256
	step [95/189], loss=82.6827
	step [96/189], loss=68.0428
	step [97/189], loss=68.8604
	step [98/189], loss=66.7627
	step [99/189], loss=77.7003
	step [100/189], loss=73.9920
	step [101/189], loss=66.2681
	step [102/189], loss=65.6460
	step [103/189], loss=67.8381
	step [104/189], loss=64.1117
	step [105/189], loss=69.8195
	step [106/189], loss=70.7599
	step [107/189], loss=65.6470
	step [108/189], loss=56.3319
	step [109/189], loss=64.3117
	step [110/189], loss=66.1812
	step [111/189], loss=66.1876
	step [112/189], loss=58.5277
	step [113/189], loss=67.6619
	step [114/189], loss=78.9250
	step [115/189], loss=71.0975
	step [116/189], loss=65.8858
	step [117/189], loss=59.6324
	step [118/189], loss=79.3315
	step [119/189], loss=76.4726
	step [120/189], loss=69.3589
	step [121/189], loss=65.8340
	step [122/189], loss=75.0629
	step [123/189], loss=73.7615
	step [124/189], loss=66.6189
	step [125/189], loss=69.0157
	step [126/189], loss=72.8297
	step [127/189], loss=60.2640
	step [128/189], loss=73.8547
	step [129/189], loss=61.7173
	step [130/189], loss=76.3480
	step [131/189], loss=60.9372
	step [132/189], loss=63.5534
	step [133/189], loss=79.2235
	step [134/189], loss=73.5368
	step [135/189], loss=77.7170
	step [136/189], loss=78.5732
	step [137/189], loss=71.7991
	step [138/189], loss=73.6736
	step [139/189], loss=77.2580
	step [140/189], loss=71.5873
	step [141/189], loss=68.1146
	step [142/189], loss=77.4030
	step [143/189], loss=63.9475
	step [144/189], loss=83.1580
	step [145/189], loss=65.8880
	step [146/189], loss=63.9281
	step [147/189], loss=79.8338
	step [148/189], loss=66.3178
	step [149/189], loss=65.1533
	step [150/189], loss=59.9844
	step [151/189], loss=61.2450
	step [152/189], loss=61.3509
	step [153/189], loss=81.1743
	step [154/189], loss=76.5289
	step [155/189], loss=63.4328
	step [156/189], loss=77.4919
	step [157/189], loss=65.0422
	step [158/189], loss=61.7315
	step [159/189], loss=73.7508
	step [160/189], loss=62.8370
	step [161/189], loss=71.5424
	step [162/189], loss=73.7859
	step [163/189], loss=66.0540
	step [164/189], loss=68.0876
	step [165/189], loss=52.1486
	step [166/189], loss=84.1716
	step [167/189], loss=67.6797
	step [168/189], loss=67.8221
	step [169/189], loss=63.3149
	step [170/189], loss=75.8511
	step [171/189], loss=54.0927
	step [172/189], loss=70.0014
	step [173/189], loss=58.9812
	step [174/189], loss=63.1164
	step [175/189], loss=75.1234
	step [176/189], loss=67.1250
	step [177/189], loss=69.7094
	step [178/189], loss=72.1068
	step [179/189], loss=55.1642
	step [180/189], loss=65.7858
	step [181/189], loss=75.2999
	step [182/189], loss=72.7655
	step [183/189], loss=70.5045
	step [184/189], loss=55.7203
	step [185/189], loss=66.1185
	step [186/189], loss=85.5931
	step [187/189], loss=59.4005
	step [188/189], loss=79.4049
	step [189/189], loss=24.0713
	Evaluating
	loss=0.0084, precision=0.3867, recall=0.8869, f1=0.5386
Training epoch 108
	step [1/189], loss=73.6764
	step [2/189], loss=67.9946
	step [3/189], loss=83.9525
	step [4/189], loss=70.7841
	step [5/189], loss=71.4048
	step [6/189], loss=78.6934
	step [7/189], loss=67.0371
	step [8/189], loss=71.3776
	step [9/189], loss=74.4572
	step [10/189], loss=75.3883
	step [11/189], loss=65.5242
	step [12/189], loss=64.9245
	step [13/189], loss=62.2502
	step [14/189], loss=79.2797
	step [15/189], loss=58.5859
	step [16/189], loss=63.5928
	step [17/189], loss=67.0836
	step [18/189], loss=64.3661
	step [19/189], loss=72.9734
	step [20/189], loss=68.2516
	step [21/189], loss=77.8865
	step [22/189], loss=70.7240
	step [23/189], loss=70.3839
	step [24/189], loss=67.3535
	step [25/189], loss=73.5190
	step [26/189], loss=62.9899
	step [27/189], loss=68.1828
	step [28/189], loss=61.9304
	step [29/189], loss=65.4344
	step [30/189], loss=61.6866
	step [31/189], loss=67.2059
	step [32/189], loss=91.8643
	step [33/189], loss=69.0506
	step [34/189], loss=63.5925
	step [35/189], loss=62.4783
	step [36/189], loss=70.1351
	step [37/189], loss=62.5488
	step [38/189], loss=61.4920
	step [39/189], loss=87.5096
	step [40/189], loss=59.1183
	step [41/189], loss=67.4302
	step [42/189], loss=75.8246
	step [43/189], loss=72.4267
	step [44/189], loss=63.3747
	step [45/189], loss=74.2389
	step [46/189], loss=50.6974
	step [47/189], loss=71.8257
	step [48/189], loss=58.5277
	step [49/189], loss=63.3883
	step [50/189], loss=64.6689
	step [51/189], loss=67.0278
	step [52/189], loss=61.5219
	step [53/189], loss=68.5481
	step [54/189], loss=66.2786
	step [55/189], loss=55.2079
	step [56/189], loss=81.2248
	step [57/189], loss=68.2428
	step [58/189], loss=55.3606
	step [59/189], loss=65.2129
	step [60/189], loss=72.2297
	step [61/189], loss=71.1135
	step [62/189], loss=86.5088
	step [63/189], loss=67.6561
	step [64/189], loss=74.3218
	step [65/189], loss=75.1236
	step [66/189], loss=67.2813
	step [67/189], loss=66.4427
	step [68/189], loss=61.0962
	step [69/189], loss=79.8244
	step [70/189], loss=70.6651
	step [71/189], loss=68.5778
	step [72/189], loss=67.0168
	step [73/189], loss=64.7119
	step [74/189], loss=61.7285
	step [75/189], loss=71.2008
	step [76/189], loss=68.5899
	step [77/189], loss=67.3908
	step [78/189], loss=56.0580
	step [79/189], loss=58.6192
	step [80/189], loss=54.8899
	step [81/189], loss=72.6189
	step [82/189], loss=61.5776
	step [83/189], loss=68.9582
	step [84/189], loss=61.5228
	step [85/189], loss=68.5615
	step [86/189], loss=79.1267
	step [87/189], loss=75.5132
	step [88/189], loss=71.4379
	step [89/189], loss=65.4383
	step [90/189], loss=61.3248
	step [91/189], loss=67.7300
	step [92/189], loss=68.5922
	step [93/189], loss=66.4073
	step [94/189], loss=85.6649
	step [95/189], loss=66.0833
	step [96/189], loss=70.0199
	step [97/189], loss=61.9786
	step [98/189], loss=69.1056
	step [99/189], loss=76.8456
	step [100/189], loss=77.1546
	step [101/189], loss=53.4616
	step [102/189], loss=68.4115
	step [103/189], loss=69.4542
	step [104/189], loss=70.4785
	step [105/189], loss=61.2129
	step [106/189], loss=60.8976
	step [107/189], loss=59.4839
	step [108/189], loss=65.0806
	step [109/189], loss=57.9306
	step [110/189], loss=65.1164
	step [111/189], loss=71.1154
	step [112/189], loss=71.0289
	step [113/189], loss=70.6475
	step [114/189], loss=66.0348
	step [115/189], loss=70.6509
	step [116/189], loss=74.0403
	step [117/189], loss=65.8743
	step [118/189], loss=63.6983
	step [119/189], loss=70.3231
	step [120/189], loss=56.1385
	step [121/189], loss=65.7761
	step [122/189], loss=69.9881
	step [123/189], loss=70.5901
	step [124/189], loss=84.9975
	step [125/189], loss=66.2126
	step [126/189], loss=65.3317
	step [127/189], loss=67.6264
	step [128/189], loss=74.0640
	step [129/189], loss=71.8070
	step [130/189], loss=55.9615
	step [131/189], loss=60.8297
	step [132/189], loss=65.9841
	step [133/189], loss=75.9066
	step [134/189], loss=75.9237
	step [135/189], loss=66.9413
	step [136/189], loss=74.5252
	step [137/189], loss=55.1382
	step [138/189], loss=58.8373
	step [139/189], loss=59.3166
	step [140/189], loss=66.3039
	step [141/189], loss=71.8373
	step [142/189], loss=76.9482
	step [143/189], loss=66.4131
	step [144/189], loss=78.3997
	step [145/189], loss=67.4737
	step [146/189], loss=60.6052
	step [147/189], loss=71.7946
	step [148/189], loss=77.4487
	step [149/189], loss=71.9340
	step [150/189], loss=60.7324
	step [151/189], loss=71.4527
	step [152/189], loss=58.3467
	step [153/189], loss=64.6442
	step [154/189], loss=65.6306
	step [155/189], loss=76.9982
	step [156/189], loss=73.3916
	step [157/189], loss=78.0792
	step [158/189], loss=64.8425
	step [159/189], loss=70.3935
	step [160/189], loss=61.7220
	step [161/189], loss=70.1500
	step [162/189], loss=61.2814
	step [163/189], loss=62.9511
	step [164/189], loss=66.0065
	step [165/189], loss=75.4753
	step [166/189], loss=68.7530
	step [167/189], loss=63.2076
	step [168/189], loss=63.6930
	step [169/189], loss=78.9036
	step [170/189], loss=69.9177
	step [171/189], loss=68.5388
	step [172/189], loss=77.1165
	step [173/189], loss=67.0816
	step [174/189], loss=74.6271
	step [175/189], loss=67.1201
	step [176/189], loss=61.3259
	step [177/189], loss=66.0507
	step [178/189], loss=76.6637
	step [179/189], loss=67.8016
	step [180/189], loss=62.0955
	step [181/189], loss=65.7933
	step [182/189], loss=70.2199
	step [183/189], loss=68.1184
	step [184/189], loss=71.3453
	step [185/189], loss=60.2926
	step [186/189], loss=78.1057
	step [187/189], loss=71.0702
	step [188/189], loss=62.6363
	step [189/189], loss=24.9222
	Evaluating
	loss=0.0073, precision=0.4264, recall=0.8845, f1=0.5754
Training epoch 109
	step [1/189], loss=58.2575
	step [2/189], loss=59.0403
	step [3/189], loss=58.1281
	step [4/189], loss=63.0091
	step [5/189], loss=63.2535
	step [6/189], loss=63.8501
	step [7/189], loss=68.1307
	step [8/189], loss=64.6492
	step [9/189], loss=61.0388
	step [10/189], loss=61.2134
	step [11/189], loss=63.7186
	step [12/189], loss=61.4753
	step [13/189], loss=67.7670
	step [14/189], loss=77.4463
	step [15/189], loss=62.6259
	step [16/189], loss=81.4868
	step [17/189], loss=72.2646
	step [18/189], loss=70.0610
	step [19/189], loss=70.0499
	step [20/189], loss=55.8619
	step [21/189], loss=62.6215
	step [22/189], loss=68.1542
	step [23/189], loss=57.1922
	step [24/189], loss=74.5267
	step [25/189], loss=70.7185
	step [26/189], loss=78.1875
	step [27/189], loss=59.3172
	step [28/189], loss=69.8112
	step [29/189], loss=67.7766
	step [30/189], loss=66.2799
	step [31/189], loss=71.6062
	step [32/189], loss=65.7039
	step [33/189], loss=73.5200
	step [34/189], loss=61.4034
	step [35/189], loss=58.3309
	step [36/189], loss=71.4445
	step [37/189], loss=62.0820
	step [38/189], loss=72.9087
	step [39/189], loss=64.0497
	step [40/189], loss=63.2303
	step [41/189], loss=86.0602
	step [42/189], loss=77.8874
	step [43/189], loss=69.4720
	step [44/189], loss=67.1384
	step [45/189], loss=64.2265
	step [46/189], loss=62.1109
	step [47/189], loss=62.8157
	step [48/189], loss=70.5937
	step [49/189], loss=56.9274
	step [50/189], loss=78.2548
	step [51/189], loss=69.3693
	step [52/189], loss=68.9769
	step [53/189], loss=67.2103
	step [54/189], loss=70.5468
	step [55/189], loss=75.4345
	step [56/189], loss=56.5218
	step [57/189], loss=74.9293
	step [58/189], loss=56.3741
	step [59/189], loss=63.5196
	step [60/189], loss=77.1581
	step [61/189], loss=68.2492
	step [62/189], loss=63.1269
	step [63/189], loss=71.5212
	step [64/189], loss=71.1668
	step [65/189], loss=71.0571
	step [66/189], loss=68.5782
	step [67/189], loss=71.1942
	step [68/189], loss=66.3512
	step [69/189], loss=72.1709
	step [70/189], loss=64.6586
	step [71/189], loss=59.8739
	step [72/189], loss=71.6727
	step [73/189], loss=62.7677
	step [74/189], loss=65.1169
	step [75/189], loss=66.1298
	step [76/189], loss=54.8434
	step [77/189], loss=71.1338
	step [78/189], loss=81.6969
	step [79/189], loss=61.4780
	step [80/189], loss=68.4115
	step [81/189], loss=62.6706
	step [82/189], loss=65.3660
	step [83/189], loss=63.7083
	step [84/189], loss=64.3640
	step [85/189], loss=79.4075
	step [86/189], loss=70.3618
	step [87/189], loss=73.8161
	step [88/189], loss=77.7443
	step [89/189], loss=56.9877
	step [90/189], loss=64.0158
	step [91/189], loss=65.0110
	step [92/189], loss=63.8161
	step [93/189], loss=63.3546
	step [94/189], loss=68.8269
	step [95/189], loss=66.5477
	step [96/189], loss=76.5394
	step [97/189], loss=75.2986
	step [98/189], loss=71.1715
	step [99/189], loss=80.1687
	step [100/189], loss=73.5364
	step [101/189], loss=69.5559
	step [102/189], loss=66.2950
	step [103/189], loss=73.7072
	step [104/189], loss=71.1963
	step [105/189], loss=71.0550
	step [106/189], loss=61.1026
	step [107/189], loss=73.1194
	step [108/189], loss=65.3231
	step [109/189], loss=85.8566
	step [110/189], loss=74.8707
	step [111/189], loss=65.7382
	step [112/189], loss=67.3670
	step [113/189], loss=79.2659
	step [114/189], loss=59.4707
	step [115/189], loss=75.9270
	step [116/189], loss=64.8665
	step [117/189], loss=60.5465
	step [118/189], loss=62.5027
	step [119/189], loss=78.0512
	step [120/189], loss=68.4901
	step [121/189], loss=73.6893
	step [122/189], loss=68.3806
	step [123/189], loss=70.8011
	step [124/189], loss=54.2711
	step [125/189], loss=70.8997
	step [126/189], loss=58.5549
	step [127/189], loss=73.7874
	step [128/189], loss=70.3054
	step [129/189], loss=62.9056
	step [130/189], loss=71.5003
	step [131/189], loss=73.1598
	step [132/189], loss=72.7673
	step [133/189], loss=67.8405
	step [134/189], loss=72.6995
	step [135/189], loss=62.3112
	step [136/189], loss=67.0811
	step [137/189], loss=74.3879
	step [138/189], loss=66.4400
	step [139/189], loss=69.9332
	step [140/189], loss=58.3536
	step [141/189], loss=68.5470
	step [142/189], loss=67.2228
	step [143/189], loss=56.5831
	step [144/189], loss=65.1822
	step [145/189], loss=68.2263
	step [146/189], loss=72.3616
	step [147/189], loss=62.5691
	step [148/189], loss=58.2731
	step [149/189], loss=71.3692
	step [150/189], loss=75.2707
	step [151/189], loss=65.2279
	step [152/189], loss=75.2562
	step [153/189], loss=62.3635
	step [154/189], loss=85.0172
	step [155/189], loss=61.0261
	step [156/189], loss=69.8613
	step [157/189], loss=60.3673
	step [158/189], loss=64.4562
	step [159/189], loss=65.9064
	step [160/189], loss=67.9867
	step [161/189], loss=72.5947
	step [162/189], loss=78.7032
	step [163/189], loss=63.8234
	step [164/189], loss=74.8894
	step [165/189], loss=61.0242
	step [166/189], loss=67.0766
	step [167/189], loss=74.2211
	step [168/189], loss=72.0111
	step [169/189], loss=68.8471
	step [170/189], loss=63.9900
	step [171/189], loss=63.5094
	step [172/189], loss=66.5382
	step [173/189], loss=73.1435
	step [174/189], loss=87.8855
	step [175/189], loss=58.4918
	step [176/189], loss=59.8514
	step [177/189], loss=81.0158
	step [178/189], loss=70.2712
	step [179/189], loss=62.5357
	step [180/189], loss=70.6184
	step [181/189], loss=65.8891
	step [182/189], loss=73.8383
	step [183/189], loss=60.1999
	step [184/189], loss=62.8594
	step [185/189], loss=72.7920
	step [186/189], loss=85.8070
	step [187/189], loss=64.5788
	step [188/189], loss=66.2611
	step [189/189], loss=23.5392
	Evaluating
	loss=0.0072, precision=0.4324, recall=0.8809, f1=0.5800
Training finished
best_f1: 0.6191201802781295
directing: Z rim_enhanced: True test_id 3
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 12484 # image files with weight 12484
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 3060 # image files with weight 3060
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 12484
Using 4 GPUs
Going to train epochs [47-96]
Training epoch 47
	step [1/196], loss=66.9767
	step [2/196], loss=80.6212
	step [3/196], loss=92.2193
	step [4/196], loss=76.3404
	step [5/196], loss=74.4343
	step [6/196], loss=76.8607
	step [7/196], loss=72.8970
	step [8/196], loss=74.3573
	step [9/196], loss=83.2647
	step [10/196], loss=81.9498
	step [11/196], loss=72.6770
	step [12/196], loss=87.8886
	step [13/196], loss=94.4918
	step [14/196], loss=76.9540
	step [15/196], loss=79.9601
	step [16/196], loss=71.0035
	step [17/196], loss=88.4351
	step [18/196], loss=82.2439
	step [19/196], loss=70.5086
	step [20/196], loss=81.9706
	step [21/196], loss=81.4861
	step [22/196], loss=70.1645
	step [23/196], loss=67.8568
	step [24/196], loss=86.0611
	step [25/196], loss=93.2103
	step [26/196], loss=84.6801
	step [27/196], loss=91.7352
	step [28/196], loss=78.2269
	step [29/196], loss=81.5175
	step [30/196], loss=78.2931
	step [31/196], loss=74.6230
	step [32/196], loss=77.0328
	step [33/196], loss=94.2699
	step [34/196], loss=76.5182
	step [35/196], loss=90.6838
	step [36/196], loss=83.8446
	step [37/196], loss=85.3084
	step [38/196], loss=80.5536
	step [39/196], loss=83.6758
	step [40/196], loss=84.8392
	step [41/196], loss=88.3602
	step [42/196], loss=77.6273
	step [43/196], loss=73.7227
	step [44/196], loss=92.0351
	step [45/196], loss=75.2794
	step [46/196], loss=84.6341
	step [47/196], loss=91.3117
	step [48/196], loss=73.3177
	step [49/196], loss=76.9129
	step [50/196], loss=68.8499
	step [51/196], loss=72.1951
	step [52/196], loss=88.9367
	step [53/196], loss=83.6240
	step [54/196], loss=68.9719
	step [55/196], loss=90.8736
	step [56/196], loss=80.5799
	step [57/196], loss=83.4866
	step [58/196], loss=86.7572
	step [59/196], loss=77.3156
	step [60/196], loss=85.0867
	step [61/196], loss=73.3142
	step [62/196], loss=74.0252
	step [63/196], loss=73.2477
	step [64/196], loss=89.9176
	step [65/196], loss=81.9455
	step [66/196], loss=69.0317
	step [67/196], loss=84.0405
	step [68/196], loss=85.5855
	step [69/196], loss=85.4838
	step [70/196], loss=74.6523
	step [71/196], loss=91.2153
	step [72/196], loss=86.1604
	step [73/196], loss=79.8448
	step [74/196], loss=82.6088
	step [75/196], loss=96.3426
	step [76/196], loss=71.6010
	step [77/196], loss=85.1514
	step [78/196], loss=78.1500
	step [79/196], loss=92.8250
	step [80/196], loss=72.1683
	step [81/196], loss=88.7485
	step [82/196], loss=81.8985
	step [83/196], loss=67.0417
	step [84/196], loss=69.4339
	step [85/196], loss=85.8084
	step [86/196], loss=65.1451
	step [87/196], loss=87.7422
	step [88/196], loss=65.3491
	step [89/196], loss=81.5088
	step [90/196], loss=74.2432
	step [91/196], loss=69.3314
	step [92/196], loss=82.7949
	step [93/196], loss=64.5249
	step [94/196], loss=73.3259
	step [95/196], loss=71.7296
	step [96/196], loss=90.9125
	step [97/196], loss=82.2572
	step [98/196], loss=75.6997
	step [99/196], loss=84.2387
	step [100/196], loss=92.4461
	step [101/196], loss=83.6427
	step [102/196], loss=72.8236
	step [103/196], loss=76.9534
	step [104/196], loss=81.1511
	step [105/196], loss=87.9022
	step [106/196], loss=64.1117
	step [107/196], loss=75.9552
	step [108/196], loss=99.3886
	step [109/196], loss=102.6236
	step [110/196], loss=90.3284
	step [111/196], loss=94.9984
	step [112/196], loss=74.3128
	step [113/196], loss=88.2096
	step [114/196], loss=77.3075
	step [115/196], loss=84.4323
	step [116/196], loss=75.5906
	step [117/196], loss=93.9860
	step [118/196], loss=77.4876
	step [119/196], loss=84.9237
	step [120/196], loss=78.8834
	step [121/196], loss=76.4012
	step [122/196], loss=79.5156
	step [123/196], loss=90.4773
	step [124/196], loss=83.0482
	step [125/196], loss=97.7957
	step [126/196], loss=77.3910
	step [127/196], loss=88.3437
	step [128/196], loss=80.9753
	step [129/196], loss=79.1804
	step [130/196], loss=84.7754
	step [131/196], loss=73.0844
	step [132/196], loss=79.5275
	step [133/196], loss=75.8822
	step [134/196], loss=67.8258
	step [135/196], loss=85.3488
	step [136/196], loss=81.3614
	step [137/196], loss=76.1210
	step [138/196], loss=73.2209
	step [139/196], loss=80.3566
	step [140/196], loss=80.7085
	step [141/196], loss=88.5954
	step [142/196], loss=83.9035
	step [143/196], loss=70.1170
	step [144/196], loss=85.9023
	step [145/196], loss=74.9466
	step [146/196], loss=80.7586
	step [147/196], loss=68.9044
	step [148/196], loss=73.5278
	step [149/196], loss=69.9179
	step [150/196], loss=84.7281
	step [151/196], loss=86.3947
	step [152/196], loss=80.1706
	step [153/196], loss=78.2615
	step [154/196], loss=72.8091
	step [155/196], loss=64.0905
	step [156/196], loss=74.9724
	step [157/196], loss=93.5156
	step [158/196], loss=85.0295
	step [159/196], loss=84.3184
	step [160/196], loss=82.9935
	step [161/196], loss=73.9414
	step [162/196], loss=71.3125
	step [163/196], loss=70.9277
	step [164/196], loss=76.9455
	step [165/196], loss=89.0948
	step [166/196], loss=83.3985
	step [167/196], loss=83.1406
	step [168/196], loss=83.3332
	step [169/196], loss=93.5585
	step [170/196], loss=68.3236
	step [171/196], loss=77.6558
	step [172/196], loss=70.4090
	step [173/196], loss=84.9393
	step [174/196], loss=72.6281
	step [175/196], loss=81.3915
	step [176/196], loss=84.6257
	step [177/196], loss=82.6346
	step [178/196], loss=80.2432
	step [179/196], loss=93.4880
	step [180/196], loss=84.3570
	step [181/196], loss=79.3846
	step [182/196], loss=81.7467
	step [183/196], loss=92.9236
	step [184/196], loss=82.4941
	step [185/196], loss=92.9906
	step [186/196], loss=96.5703
	step [187/196], loss=79.0851
	step [188/196], loss=80.5028
	step [189/196], loss=72.9479
	step [190/196], loss=88.6551
	step [191/196], loss=82.1589
	step [192/196], loss=96.9954
	step [193/196], loss=83.0562
	step [194/196], loss=70.8158
	step [195/196], loss=94.7160
	step [196/196], loss=3.6584
	Evaluating
	loss=0.0097, precision=0.3644, recall=0.8674, f1=0.5132
saving model as: 3_saved_model.pth
Training epoch 48
	step [1/196], loss=79.6960
	step [2/196], loss=80.7315
	step [3/196], loss=68.7378
	step [4/196], loss=73.8640
	step [5/196], loss=78.4300
	step [6/196], loss=81.0009
	step [7/196], loss=76.0353
	step [8/196], loss=61.4539
	step [9/196], loss=70.7915
	step [10/196], loss=72.0904
	step [11/196], loss=95.6915
	step [12/196], loss=75.5402
	step [13/196], loss=74.9729
	step [14/196], loss=72.1481
	step [15/196], loss=81.8465
	step [16/196], loss=80.4512
	step [17/196], loss=75.2908
	step [18/196], loss=85.8140
	step [19/196], loss=86.4656
	step [20/196], loss=78.1941
	step [21/196], loss=70.9060
	step [22/196], loss=83.3470
	step [23/196], loss=68.9777
	step [24/196], loss=76.9791
	step [25/196], loss=80.6533
	step [26/196], loss=84.3802
	step [27/196], loss=86.5224
	step [28/196], loss=86.3767
	step [29/196], loss=90.5206
	step [30/196], loss=99.5345
	step [31/196], loss=77.6038
	step [32/196], loss=79.1811
	step [33/196], loss=88.3111
	step [34/196], loss=72.9054
	step [35/196], loss=69.4402
	step [36/196], loss=96.8029
	step [37/196], loss=87.7766
	step [38/196], loss=95.9192
	step [39/196], loss=69.8661
	step [40/196], loss=88.7056
	step [41/196], loss=75.5692
	step [42/196], loss=76.2730
	step [43/196], loss=77.0731
	step [44/196], loss=89.3857
	step [45/196], loss=79.1878
	step [46/196], loss=84.1988
	step [47/196], loss=81.0909
	step [48/196], loss=69.2117
	step [49/196], loss=100.2836
	step [50/196], loss=63.1413
	step [51/196], loss=89.0332
	step [52/196], loss=91.4978
	step [53/196], loss=86.2566
	step [54/196], loss=68.2906
	step [55/196], loss=81.6055
	step [56/196], loss=85.5692
	step [57/196], loss=97.1061
	step [58/196], loss=80.0174
	step [59/196], loss=82.3554
	step [60/196], loss=75.3773
	step [61/196], loss=88.0499
	step [62/196], loss=75.2061
	step [63/196], loss=85.7228
	step [64/196], loss=67.9565
	step [65/196], loss=85.0109
	step [66/196], loss=67.3115
	step [67/196], loss=78.4093
	step [68/196], loss=73.5599
	step [69/196], loss=68.8761
	step [70/196], loss=78.1972
	step [71/196], loss=85.8784
	step [72/196], loss=83.5710
	step [73/196], loss=74.1492
	step [74/196], loss=79.7294
	step [75/196], loss=83.8282
	step [76/196], loss=70.7156
	step [77/196], loss=99.1791
	step [78/196], loss=79.7557
	step [79/196], loss=90.8139
	step [80/196], loss=84.2868
	step [81/196], loss=67.6059
	step [82/196], loss=81.1380
	step [83/196], loss=65.6510
	step [84/196], loss=88.0486
	step [85/196], loss=77.5143
	step [86/196], loss=96.9171
	step [87/196], loss=85.5563
	step [88/196], loss=89.9357
	step [89/196], loss=89.7374
	step [90/196], loss=79.7534
	step [91/196], loss=70.2087
	step [92/196], loss=58.0115
	step [93/196], loss=79.5244
	step [94/196], loss=83.3007
	step [95/196], loss=88.6983
	step [96/196], loss=70.1344
	step [97/196], loss=89.0135
	step [98/196], loss=68.1012
	step [99/196], loss=81.5542
	step [100/196], loss=75.8593
	step [101/196], loss=84.7663
	step [102/196], loss=88.8778
	step [103/196], loss=85.9092
	step [104/196], loss=82.8001
	step [105/196], loss=79.5201
	step [106/196], loss=83.0090
	step [107/196], loss=86.1196
	step [108/196], loss=74.5314
	step [109/196], loss=77.6400
	step [110/196], loss=69.1499
	step [111/196], loss=86.9199
	step [112/196], loss=102.1481
	step [113/196], loss=89.1404
	step [114/196], loss=87.1656
	step [115/196], loss=88.0399
	step [116/196], loss=75.9880
	step [117/196], loss=88.6881
	step [118/196], loss=71.3810
	step [119/196], loss=75.9639
	step [120/196], loss=73.1839
	step [121/196], loss=98.4693
	step [122/196], loss=81.4138
	step [123/196], loss=78.6070
	step [124/196], loss=84.9749
	step [125/196], loss=77.3407
	step [126/196], loss=87.2182
	step [127/196], loss=78.4172
	step [128/196], loss=107.4335
	step [129/196], loss=73.0727
	step [130/196], loss=71.3315
	step [131/196], loss=76.3562
	step [132/196], loss=79.4184
	step [133/196], loss=90.1446
	step [134/196], loss=62.5741
	step [135/196], loss=68.7046
	step [136/196], loss=88.6138
	step [137/196], loss=88.7668
	step [138/196], loss=82.1307
	step [139/196], loss=66.3731
	step [140/196], loss=88.5236
	step [141/196], loss=69.3137
	step [142/196], loss=73.2153
	step [143/196], loss=82.6630
	step [144/196], loss=69.8070
	step [145/196], loss=73.4912
	step [146/196], loss=75.6489
	step [147/196], loss=77.8924
	step [148/196], loss=72.3855
	step [149/196], loss=75.1082
	step [150/196], loss=80.7360
	step [151/196], loss=68.6404
	step [152/196], loss=84.0486
	step [153/196], loss=57.1180
	step [154/196], loss=72.5101
	step [155/196], loss=72.7341
	step [156/196], loss=83.4381
	step [157/196], loss=78.2493
	step [158/196], loss=72.8494
	step [159/196], loss=90.6447
	step [160/196], loss=92.9843
	step [161/196], loss=65.5744
	step [162/196], loss=74.0572
	step [163/196], loss=80.8473
	step [164/196], loss=99.9632
	step [165/196], loss=76.4058
	step [166/196], loss=92.8831
	step [167/196], loss=88.1400
	step [168/196], loss=83.5055
	step [169/196], loss=78.1750
	step [170/196], loss=84.5285
	step [171/196], loss=91.3828
	step [172/196], loss=73.9942
	step [173/196], loss=79.2281
	step [174/196], loss=70.0470
	step [175/196], loss=85.6734
	step [176/196], loss=66.1061
	step [177/196], loss=77.7261
	step [178/196], loss=89.5992
	step [179/196], loss=99.2069
	step [180/196], loss=68.5659
	step [181/196], loss=97.5408
	step [182/196], loss=76.5126
	step [183/196], loss=77.7249
	step [184/196], loss=71.8424
	step [185/196], loss=78.8136
	step [186/196], loss=84.0378
	step [187/196], loss=64.0023
	step [188/196], loss=67.2007
	step [189/196], loss=84.5219
	step [190/196], loss=76.7746
	step [191/196], loss=80.0712
	step [192/196], loss=79.7920
	step [193/196], loss=75.8395
	step [194/196], loss=86.0519
	step [195/196], loss=82.5608
	step [196/196], loss=13.5230
	Evaluating
	loss=0.0076, precision=0.4688, recall=0.8470, f1=0.6036
saving model as: 3_saved_model.pth
Training epoch 49
	step [1/196], loss=83.8417
	step [2/196], loss=81.2329
	step [3/196], loss=83.6561
	step [4/196], loss=84.4078
	step [5/196], loss=79.0022
	step [6/196], loss=85.3302
	step [7/196], loss=74.4501
	step [8/196], loss=78.1427
	step [9/196], loss=83.4985
	step [10/196], loss=83.2242
	step [11/196], loss=71.0902
	step [12/196], loss=73.6086
	step [13/196], loss=93.8077
	step [14/196], loss=72.2818
	step [15/196], loss=84.5518
	step [16/196], loss=69.3651
	step [17/196], loss=71.5331
	step [18/196], loss=83.3129
	step [19/196], loss=83.5645
	step [20/196], loss=81.3527
	step [21/196], loss=75.9155
	step [22/196], loss=81.2867
	step [23/196], loss=83.6108
	step [24/196], loss=75.8612
	step [25/196], loss=83.3906
	step [26/196], loss=78.2054
	step [27/196], loss=76.5457
	step [28/196], loss=87.9984
	step [29/196], loss=102.7149
	step [30/196], loss=73.9517
	step [31/196], loss=68.6816
	step [32/196], loss=75.3591
	step [33/196], loss=91.2131
	step [34/196], loss=93.9848
	step [35/196], loss=92.1960
	step [36/196], loss=82.1206
	step [37/196], loss=77.1384
	step [38/196], loss=87.5903
	step [39/196], loss=77.6077
	step [40/196], loss=83.4840
	step [41/196], loss=69.6650
	step [42/196], loss=87.0276
	step [43/196], loss=73.7312
	step [44/196], loss=86.9181
	step [45/196], loss=70.4350
	step [46/196], loss=89.4073
	step [47/196], loss=86.2761
	step [48/196], loss=80.4369
	step [49/196], loss=77.6423
	step [50/196], loss=66.5047
	step [51/196], loss=71.5910
	step [52/196], loss=82.9887
	step [53/196], loss=75.1664
	step [54/196], loss=87.7328
	step [55/196], loss=79.2848
	step [56/196], loss=73.4485
	step [57/196], loss=85.0004
	step [58/196], loss=87.1551
	step [59/196], loss=76.3925
	step [60/196], loss=86.0473
	step [61/196], loss=90.8879
	step [62/196], loss=79.2351
	step [63/196], loss=72.0338
	step [64/196], loss=73.4932
	step [65/196], loss=81.9013
	step [66/196], loss=78.3386
	step [67/196], loss=71.8855
	step [68/196], loss=63.4911
	step [69/196], loss=85.0570
	step [70/196], loss=85.8439
	step [71/196], loss=60.1855
	step [72/196], loss=87.7502
	step [73/196], loss=74.7243
	step [74/196], loss=79.7253
	step [75/196], loss=69.3447
	step [76/196], loss=77.3798
	step [77/196], loss=79.9759
	step [78/196], loss=68.7135
	step [79/196], loss=81.6637
	step [80/196], loss=78.5096
	step [81/196], loss=76.8838
	step [82/196], loss=89.0359
	step [83/196], loss=91.5635
	step [84/196], loss=68.9633
	step [85/196], loss=74.2075
	step [86/196], loss=84.0460
	step [87/196], loss=82.9287
	step [88/196], loss=80.6437
	step [89/196], loss=73.9201
	step [90/196], loss=83.2719
	step [91/196], loss=69.4055
	step [92/196], loss=71.9828
	step [93/196], loss=84.1104
	step [94/196], loss=85.3450
	step [95/196], loss=72.5806
	step [96/196], loss=67.9358
	step [97/196], loss=81.5442
	step [98/196], loss=67.5256
	step [99/196], loss=82.7429
	step [100/196], loss=79.6796
	step [101/196], loss=86.4570
	step [102/196], loss=70.5463
	step [103/196], loss=79.7018
	step [104/196], loss=75.1129
	step [105/196], loss=89.7598
	step [106/196], loss=83.4048
	step [107/196], loss=81.3847
	step [108/196], loss=69.5288
	step [109/196], loss=74.4166
	step [110/196], loss=78.9369
	step [111/196], loss=65.6408
	step [112/196], loss=81.4050
	step [113/196], loss=78.2803
	step [114/196], loss=74.0509
	step [115/196], loss=79.3548
	step [116/196], loss=92.0030
	step [117/196], loss=78.4006
	step [118/196], loss=73.8242
	step [119/196], loss=75.5093
	step [120/196], loss=75.0798
	step [121/196], loss=101.3892
	step [122/196], loss=82.3497
	step [123/196], loss=83.1816
	step [124/196], loss=68.6319
	step [125/196], loss=77.9060
	step [126/196], loss=93.6031
	step [127/196], loss=79.7292
	step [128/196], loss=79.7326
	step [129/196], loss=85.3383
	step [130/196], loss=67.2413
	step [131/196], loss=76.9219
	step [132/196], loss=78.4227
	step [133/196], loss=89.5189
	step [134/196], loss=70.3692
	step [135/196], loss=78.0956
	step [136/196], loss=83.0485
	step [137/196], loss=72.9031
	step [138/196], loss=73.2356
	step [139/196], loss=88.9972
	step [140/196], loss=65.2516
	step [141/196], loss=69.0100
	step [142/196], loss=101.0102
	step [143/196], loss=89.3387
	step [144/196], loss=88.2251
	step [145/196], loss=81.5882
	step [146/196], loss=81.5644
	step [147/196], loss=88.3153
	step [148/196], loss=80.8656
	step [149/196], loss=93.0130
	step [150/196], loss=81.5289
	step [151/196], loss=77.9539
	step [152/196], loss=77.5744
	step [153/196], loss=81.8983
	step [154/196], loss=90.4523
	step [155/196], loss=79.0457
	step [156/196], loss=78.4722
	step [157/196], loss=73.8827
	step [158/196], loss=75.0092
	step [159/196], loss=80.5284
	step [160/196], loss=108.7037
	step [161/196], loss=81.4405
	step [162/196], loss=85.5506
	step [163/196], loss=79.5488
	step [164/196], loss=75.1617
	step [165/196], loss=73.1397
	step [166/196], loss=85.2426
	step [167/196], loss=81.6496
	step [168/196], loss=84.2043
	step [169/196], loss=80.9830
	step [170/196], loss=73.0574
	step [171/196], loss=90.4665
	step [172/196], loss=87.0124
	step [173/196], loss=89.1601
	step [174/196], loss=81.5705
	step [175/196], loss=100.0698
	step [176/196], loss=68.1982
	step [177/196], loss=80.3827
	step [178/196], loss=82.5432
	step [179/196], loss=80.2328
	step [180/196], loss=88.9826
	step [181/196], loss=70.2267
	step [182/196], loss=83.2745
	step [183/196], loss=72.5527
	step [184/196], loss=69.5066
	step [185/196], loss=80.2046
	step [186/196], loss=84.4214
	step [187/196], loss=75.8625
	step [188/196], loss=72.0180
	step [189/196], loss=75.4416
	step [190/196], loss=68.1251
	step [191/196], loss=76.3614
	step [192/196], loss=69.6307
	step [193/196], loss=77.9371
	step [194/196], loss=66.1741
	step [195/196], loss=86.7771
	step [196/196], loss=6.2218
	Evaluating
	loss=0.0085, precision=0.3954, recall=0.8614, f1=0.5420
Training epoch 50
	step [1/196], loss=100.3954
	step [2/196], loss=78.0579
	step [3/196], loss=67.4005
	step [4/196], loss=74.8270
	step [5/196], loss=81.5973
	step [6/196], loss=77.7168
	step [7/196], loss=75.2483
	step [8/196], loss=92.6535
	step [9/196], loss=79.6417
	step [10/196], loss=82.5031
	step [11/196], loss=75.6264
	step [12/196], loss=73.0542
	step [13/196], loss=79.8729
	step [14/196], loss=69.1665
	step [15/196], loss=89.7560
	step [16/196], loss=76.8242
	step [17/196], loss=86.3682
	step [18/196], loss=85.2193
	step [19/196], loss=86.9070
	step [20/196], loss=79.8797
	step [21/196], loss=76.6386
	step [22/196], loss=78.9310
	step [23/196], loss=73.2338
	step [24/196], loss=76.7623
	step [25/196], loss=90.5112
	step [26/196], loss=90.9890
	step [27/196], loss=71.0404
	step [28/196], loss=83.2489
	step [29/196], loss=91.3859
	step [30/196], loss=96.8985
	step [31/196], loss=80.6168
	step [32/196], loss=66.7006
	step [33/196], loss=88.0677
	step [34/196], loss=80.5210
	step [35/196], loss=82.6548
	step [36/196], loss=84.4229
	step [37/196], loss=73.1560
	step [38/196], loss=78.8671
	step [39/196], loss=70.5744
	step [40/196], loss=72.4420
	step [41/196], loss=75.4209
	step [42/196], loss=75.1776
	step [43/196], loss=66.2458
	step [44/196], loss=86.2500
	step [45/196], loss=76.2308
	step [46/196], loss=86.6528
	step [47/196], loss=67.9929
	step [48/196], loss=70.0994
	step [49/196], loss=90.9154
	step [50/196], loss=79.5573
	step [51/196], loss=84.0241
	step [52/196], loss=79.5422
	step [53/196], loss=67.8775
	step [54/196], loss=90.2260
	step [55/196], loss=82.1764
	step [56/196], loss=78.3887
	step [57/196], loss=89.3945
	step [58/196], loss=79.8322
	step [59/196], loss=73.3404
	step [60/196], loss=76.1506
	step [61/196], loss=88.4758
	step [62/196], loss=85.5264
	step [63/196], loss=73.7088
	step [64/196], loss=57.8873
	step [65/196], loss=78.8034
	step [66/196], loss=75.0043
	step [67/196], loss=68.3550
	step [68/196], loss=77.5391
	step [69/196], loss=81.8790
	step [70/196], loss=83.1460
	step [71/196], loss=61.4778
	step [72/196], loss=88.1262
	step [73/196], loss=95.0783
	step [74/196], loss=75.7626
	step [75/196], loss=92.9052
	step [76/196], loss=84.2579
	step [77/196], loss=74.2842
	step [78/196], loss=77.3751
	step [79/196], loss=91.1990
	step [80/196], loss=77.4663
	step [81/196], loss=68.1239
	step [82/196], loss=87.4128
	step [83/196], loss=76.8472
	step [84/196], loss=87.0582
	step [85/196], loss=80.3562
	step [86/196], loss=89.9693
	step [87/196], loss=85.2496
	step [88/196], loss=75.0141
	step [89/196], loss=68.5656
	step [90/196], loss=74.3387
	step [91/196], loss=79.9778
	step [92/196], loss=93.2856
	step [93/196], loss=72.9152
	step [94/196], loss=77.9067
	step [95/196], loss=73.4946
	step [96/196], loss=82.2382
	step [97/196], loss=75.1096
	step [98/196], loss=73.9902
	step [99/196], loss=83.6731
	step [100/196], loss=84.5641
	step [101/196], loss=76.5369
	step [102/196], loss=78.5536
	step [103/196], loss=90.1487
	step [104/196], loss=93.2511
	step [105/196], loss=76.5507
	step [106/196], loss=73.0476
	step [107/196], loss=93.5500
	step [108/196], loss=77.0070
	step [109/196], loss=75.1945
	step [110/196], loss=87.8699
	step [111/196], loss=72.2060
	step [112/196], loss=97.0288
	step [113/196], loss=74.5779
	step [114/196], loss=71.8820
	step [115/196], loss=62.5677
	step [116/196], loss=76.2702
	step [117/196], loss=75.4762
	step [118/196], loss=78.3650
	step [119/196], loss=88.5717
	step [120/196], loss=79.8006
	step [121/196], loss=72.4857
	step [122/196], loss=80.0561
	step [123/196], loss=78.3006
	step [124/196], loss=76.2206
	step [125/196], loss=94.4771
	step [126/196], loss=76.2180
	step [127/196], loss=76.1403
	step [128/196], loss=73.7350
	step [129/196], loss=75.9756
	step [130/196], loss=85.7935
	step [131/196], loss=80.2622
	step [132/196], loss=86.0239
	step [133/196], loss=94.5229
	step [134/196], loss=78.1707
	step [135/196], loss=85.3322
	step [136/196], loss=86.1411
	step [137/196], loss=72.3418
	step [138/196], loss=83.3070
	step [139/196], loss=74.6179
	step [140/196], loss=76.8512
	step [141/196], loss=70.2191
	step [142/196], loss=87.8832
	step [143/196], loss=82.1527
	step [144/196], loss=77.8387
	step [145/196], loss=71.9507
	step [146/196], loss=77.2793
	step [147/196], loss=61.4849
	step [148/196], loss=86.2178
	step [149/196], loss=75.8145
	step [150/196], loss=65.5893
	step [151/196], loss=79.8649
	step [152/196], loss=78.9161
	step [153/196], loss=96.1305
	step [154/196], loss=73.6973
	step [155/196], loss=72.0736
	step [156/196], loss=71.3866
	step [157/196], loss=94.3533
	step [158/196], loss=73.5091
	step [159/196], loss=71.3162
	step [160/196], loss=89.4592
	step [161/196], loss=75.0126
	step [162/196], loss=81.2616
	step [163/196], loss=81.1033
	step [164/196], loss=86.8607
	step [165/196], loss=93.4297
	step [166/196], loss=71.0030
	step [167/196], loss=87.9753
	step [168/196], loss=81.6283
	step [169/196], loss=92.9522
	step [170/196], loss=80.1822
	step [171/196], loss=64.3693
	step [172/196], loss=70.1003
	step [173/196], loss=86.2617
	step [174/196], loss=72.2821
	step [175/196], loss=80.9526
	step [176/196], loss=80.8326
	step [177/196], loss=73.6779
	step [178/196], loss=70.6124
	step [179/196], loss=81.0145
	step [180/196], loss=77.6776
	step [181/196], loss=72.0253
	step [182/196], loss=69.0956
	step [183/196], loss=80.6035
	step [184/196], loss=84.0038
	step [185/196], loss=79.5657
	step [186/196], loss=71.2667
	step [187/196], loss=80.5126
	step [188/196], loss=78.0496
	step [189/196], loss=88.6824
	step [190/196], loss=88.6075
	step [191/196], loss=77.8685
	step [192/196], loss=70.3641
	step [193/196], loss=70.6407
	step [194/196], loss=89.5318
	step [195/196], loss=68.4957
	step [196/196], loss=3.6349
	Evaluating
	loss=0.0078, precision=0.4089, recall=0.8504, f1=0.5523
Training epoch 51
	step [1/196], loss=71.3301
	step [2/196], loss=87.7251
	step [3/196], loss=83.1244
	step [4/196], loss=85.7644
	step [5/196], loss=87.3813
	step [6/196], loss=80.1863
	step [7/196], loss=83.1209
	step [8/196], loss=80.3882
	step [9/196], loss=65.0966
	step [10/196], loss=77.1664
	step [11/196], loss=77.9776
	step [12/196], loss=80.3334
	step [13/196], loss=85.1575
	step [14/196], loss=79.1109
	step [15/196], loss=77.1288
	step [16/196], loss=86.6843
	step [17/196], loss=89.6163
	step [18/196], loss=84.2722
	step [19/196], loss=70.6842
	step [20/196], loss=81.7356
	step [21/196], loss=66.7521
	step [22/196], loss=79.9600
	step [23/196], loss=65.9789
	step [24/196], loss=78.9161
	step [25/196], loss=71.3579
	step [26/196], loss=79.2362
	step [27/196], loss=82.4452
	step [28/196], loss=84.4190
	step [29/196], loss=98.6082
	step [30/196], loss=74.9924
	step [31/196], loss=69.2912
	step [32/196], loss=80.2986
	step [33/196], loss=80.7330
	step [34/196], loss=85.8505
	step [35/196], loss=72.2632
	step [36/196], loss=91.3274
	step [37/196], loss=83.0762
	step [38/196], loss=88.4878
	step [39/196], loss=70.2016
	step [40/196], loss=76.2747
	step [41/196], loss=72.5835
	step [42/196], loss=70.1295
	step [43/196], loss=63.7779
	step [44/196], loss=86.3354
	step [45/196], loss=76.9598
	step [46/196], loss=75.2630
	step [47/196], loss=88.3230
	step [48/196], loss=84.2946
	step [49/196], loss=75.9401
	step [50/196], loss=95.7051
	step [51/196], loss=78.6280
	step [52/196], loss=81.3010
	step [53/196], loss=74.6578
	step [54/196], loss=82.6162
	step [55/196], loss=67.5576
	step [56/196], loss=76.8439
	step [57/196], loss=84.9525
	step [58/196], loss=92.7179
	step [59/196], loss=67.4012
	step [60/196], loss=82.8373
	step [61/196], loss=65.9253
	step [62/196], loss=78.6049
	step [63/196], loss=92.8140
	step [64/196], loss=98.7449
	step [65/196], loss=80.5678
	step [66/196], loss=96.1073
	step [67/196], loss=79.9870
	step [68/196], loss=78.7598
	step [69/196], loss=80.3409
	step [70/196], loss=90.6355
	step [71/196], loss=88.0383
	step [72/196], loss=71.1128
	step [73/196], loss=70.9600
	step [74/196], loss=70.6983
	step [75/196], loss=84.4222
	step [76/196], loss=76.2278
	step [77/196], loss=80.4814
	step [78/196], loss=77.9370
	step [79/196], loss=79.2869
	step [80/196], loss=62.0769
	step [81/196], loss=74.8846
	step [82/196], loss=75.7883
	step [83/196], loss=85.6431
	step [84/196], loss=82.8880
	step [85/196], loss=69.4577
	step [86/196], loss=80.4217
	step [87/196], loss=84.7499
	step [88/196], loss=79.6934
	step [89/196], loss=71.3357
	step [90/196], loss=65.8237
	step [91/196], loss=87.3600
	step [92/196], loss=87.2800
	step [93/196], loss=65.2207
	step [94/196], loss=79.4329
	step [95/196], loss=72.3242
	step [96/196], loss=70.7404
	step [97/196], loss=101.2276
	step [98/196], loss=86.1316
	step [99/196], loss=71.1254
	step [100/196], loss=84.1641
	step [101/196], loss=78.7272
	step [102/196], loss=79.6815
	step [103/196], loss=63.5796
	step [104/196], loss=81.8942
	step [105/196], loss=79.0640
	step [106/196], loss=84.2187
	step [107/196], loss=70.6463
	step [108/196], loss=78.8087
	step [109/196], loss=92.1525
	step [110/196], loss=76.1170
	step [111/196], loss=75.1700
	step [112/196], loss=62.7592
	step [113/196], loss=70.4499
	step [114/196], loss=79.4743
	step [115/196], loss=77.8890
	step [116/196], loss=68.5606
	step [117/196], loss=76.7233
	step [118/196], loss=86.7591
	step [119/196], loss=84.6087
	step [120/196], loss=64.2440
	step [121/196], loss=87.3393
	step [122/196], loss=73.8601
	step [123/196], loss=75.1800
	step [124/196], loss=86.3560
	step [125/196], loss=71.0797
	step [126/196], loss=80.7697
	step [127/196], loss=74.6812
	step [128/196], loss=81.3149
	step [129/196], loss=92.4127
	step [130/196], loss=71.2542
	step [131/196], loss=84.2053
	step [132/196], loss=76.3696
	step [133/196], loss=70.4158
	step [134/196], loss=80.7121
	step [135/196], loss=90.7433
	step [136/196], loss=76.2461
	step [137/196], loss=80.2539
	step [138/196], loss=85.3383
	step [139/196], loss=90.8668
	step [140/196], loss=76.3203
	step [141/196], loss=82.8690
	step [142/196], loss=72.3267
	step [143/196], loss=89.1465
	step [144/196], loss=78.4929
	step [145/196], loss=78.9576
	step [146/196], loss=80.8037
	step [147/196], loss=74.1597
	step [148/196], loss=72.4339
	step [149/196], loss=67.4021
	step [150/196], loss=90.5803
	step [151/196], loss=76.7720
	step [152/196], loss=75.5419
	step [153/196], loss=78.4374
	step [154/196], loss=71.9145
	step [155/196], loss=82.2843
	step [156/196], loss=80.7523
	step [157/196], loss=76.2587
	step [158/196], loss=82.6769
	step [159/196], loss=75.7130
	step [160/196], loss=77.9491
	step [161/196], loss=65.2418
	step [162/196], loss=88.0718
	step [163/196], loss=72.7950
	step [164/196], loss=73.7580
	step [165/196], loss=75.9692
	step [166/196], loss=77.4733
	step [167/196], loss=90.4366
	step [168/196], loss=89.3490
	step [169/196], loss=72.2051
	step [170/196], loss=83.2133
	step [171/196], loss=82.4969
	step [172/196], loss=69.2672
	step [173/196], loss=90.4378
	step [174/196], loss=71.6737
	step [175/196], loss=85.1490
	step [176/196], loss=78.8717
	step [177/196], loss=73.1025
	step [178/196], loss=102.3528
	step [179/196], loss=85.9557
	step [180/196], loss=79.1943
	step [181/196], loss=76.4600
	step [182/196], loss=71.9440
	step [183/196], loss=72.1812
	step [184/196], loss=86.8877
	step [185/196], loss=75.5320
	step [186/196], loss=75.7271
	step [187/196], loss=80.7134
	step [188/196], loss=95.1232
	step [189/196], loss=99.0780
	step [190/196], loss=72.5631
	step [191/196], loss=86.1996
	step [192/196], loss=75.5598
	step [193/196], loss=76.2046
	step [194/196], loss=73.7587
	step [195/196], loss=94.0285
	step [196/196], loss=3.2557
	Evaluating
	loss=0.0083, precision=0.4152, recall=0.8625, f1=0.5605
Training epoch 52
	step [1/196], loss=73.4303
	step [2/196], loss=86.5068
	step [3/196], loss=93.4428
	step [4/196], loss=82.1544
	step [5/196], loss=79.2754
	step [6/196], loss=74.4946
	step [7/196], loss=78.7164
	step [8/196], loss=93.4440
	step [9/196], loss=75.9903
	step [10/196], loss=62.3451
	step [11/196], loss=74.1475
	step [12/196], loss=64.9393
	step [13/196], loss=73.8433
	step [14/196], loss=80.0161
	step [15/196], loss=90.6262
	step [16/196], loss=68.2462
	step [17/196], loss=94.6639
	step [18/196], loss=74.7152
	step [19/196], loss=83.3008
	step [20/196], loss=74.4507
	step [21/196], loss=71.4423
	step [22/196], loss=81.0271
	step [23/196], loss=84.3818
	step [24/196], loss=85.4666
	step [25/196], loss=83.2827
	step [26/196], loss=85.5251
	step [27/196], loss=73.5580
	step [28/196], loss=93.2279
	step [29/196], loss=72.2389
	step [30/196], loss=82.4491
	step [31/196], loss=78.5530
	step [32/196], loss=63.7106
	step [33/196], loss=72.6137
	step [34/196], loss=71.0111
	step [35/196], loss=85.0897
	step [36/196], loss=86.8484
	step [37/196], loss=79.0701
	step [38/196], loss=90.5486
	step [39/196], loss=89.0423
	step [40/196], loss=82.6983
	step [41/196], loss=68.6132
	step [42/196], loss=75.0183
	step [43/196], loss=81.1156
	step [44/196], loss=92.3198
	step [45/196], loss=82.4510
	step [46/196], loss=79.4452
	step [47/196], loss=74.9813
	step [48/196], loss=72.1833
	step [49/196], loss=74.5638
	step [50/196], loss=67.7564
	step [51/196], loss=74.1056
	step [52/196], loss=77.1025
	step [53/196], loss=79.8258
	step [54/196], loss=78.4742
	step [55/196], loss=69.9037
	step [56/196], loss=73.6974
	step [57/196], loss=76.9491
	step [58/196], loss=79.0075
	step [59/196], loss=75.8167
	step [60/196], loss=75.1867
	step [61/196], loss=84.6172
	step [62/196], loss=67.8918
	step [63/196], loss=71.7844
	step [64/196], loss=75.8935
	step [65/196], loss=84.0444
	step [66/196], loss=81.8410
	step [67/196], loss=78.7153
	step [68/196], loss=73.1898
	step [69/196], loss=76.1025
	step [70/196], loss=86.6979
	step [71/196], loss=67.1521
	step [72/196], loss=78.6052
	step [73/196], loss=87.4383
	step [74/196], loss=91.3933
	step [75/196], loss=76.1897
	step [76/196], loss=86.3692
	step [77/196], loss=101.0255
	step [78/196], loss=73.4024
	step [79/196], loss=78.4157
	step [80/196], loss=85.4080
	step [81/196], loss=77.2625
	step [82/196], loss=75.2010
	step [83/196], loss=68.5326
	step [84/196], loss=85.6836
	step [85/196], loss=84.6161
	step [86/196], loss=79.0901
	step [87/196], loss=83.7311
	step [88/196], loss=81.4083
	step [89/196], loss=67.8154
	step [90/196], loss=94.5683
	step [91/196], loss=64.5181
	step [92/196], loss=88.0027
	step [93/196], loss=83.0851
	step [94/196], loss=84.2244
	step [95/196], loss=85.7714
	step [96/196], loss=85.8599
	step [97/196], loss=78.0507
	step [98/196], loss=75.5200
	step [99/196], loss=69.6004
	step [100/196], loss=69.2089
	step [101/196], loss=78.5746
	step [102/196], loss=100.9086
	step [103/196], loss=68.0811
	step [104/196], loss=87.4780
	step [105/196], loss=89.1276
	step [106/196], loss=58.5155
	step [107/196], loss=68.8659
	step [108/196], loss=68.3932
	step [109/196], loss=77.7033
	step [110/196], loss=78.7056
	step [111/196], loss=82.1061
	step [112/196], loss=83.0293
	step [113/196], loss=80.0602
	step [114/196], loss=88.7308
	step [115/196], loss=69.8259
	step [116/196], loss=64.0008
	step [117/196], loss=76.2792
	step [118/196], loss=87.9709
	step [119/196], loss=84.2506
	step [120/196], loss=87.9017
	step [121/196], loss=80.0689
	step [122/196], loss=91.6587
	step [123/196], loss=85.8678
	step [124/196], loss=85.1658
	step [125/196], loss=71.4709
	step [126/196], loss=97.0601
	step [127/196], loss=85.5166
	step [128/196], loss=96.9653
	step [129/196], loss=75.6629
	step [130/196], loss=87.9516
	step [131/196], loss=73.8165
	step [132/196], loss=78.8435
	step [133/196], loss=89.6225
	step [134/196], loss=76.2011
	step [135/196], loss=71.6936
	step [136/196], loss=74.8937
	step [137/196], loss=76.2477
	step [138/196], loss=90.2151
	step [139/196], loss=70.7037
	step [140/196], loss=77.6251
	step [141/196], loss=83.3584
	step [142/196], loss=88.7466
	step [143/196], loss=93.9668
	step [144/196], loss=76.7052
	step [145/196], loss=69.1313
	step [146/196], loss=64.2220
	step [147/196], loss=80.1686
	step [148/196], loss=72.4043
	step [149/196], loss=78.7367
	step [150/196], loss=85.0564
	step [151/196], loss=72.6231
	step [152/196], loss=73.9774
	step [153/196], loss=74.1030
	step [154/196], loss=72.3591
	step [155/196], loss=79.6682
	step [156/196], loss=66.9252
	step [157/196], loss=67.0548
	step [158/196], loss=87.1727
	step [159/196], loss=69.7152
	step [160/196], loss=72.4628
	step [161/196], loss=71.1352
	step [162/196], loss=88.2424
	step [163/196], loss=73.2729
	step [164/196], loss=92.9222
	step [165/196], loss=87.6142
	step [166/196], loss=80.4936
	step [167/196], loss=73.0588
	step [168/196], loss=74.9514
	step [169/196], loss=83.2700
	step [170/196], loss=77.7990
	step [171/196], loss=79.1570
	step [172/196], loss=88.7945
	step [173/196], loss=83.5176
	step [174/196], loss=85.5759
	step [175/196], loss=70.4372
	step [176/196], loss=83.6902
	step [177/196], loss=79.0717
	step [178/196], loss=70.0285
	step [179/196], loss=70.6591
	step [180/196], loss=72.2560
	step [181/196], loss=83.3881
	step [182/196], loss=78.1676
	step [183/196], loss=78.8040
	step [184/196], loss=70.6975
	step [185/196], loss=85.0595
	step [186/196], loss=76.8146
	step [187/196], loss=76.3936
	step [188/196], loss=81.5479
	step [189/196], loss=82.5027
	step [190/196], loss=84.2587
	step [191/196], loss=65.9744
	step [192/196], loss=66.4828
	step [193/196], loss=71.4886
	step [194/196], loss=72.0473
	step [195/196], loss=92.6880
	step [196/196], loss=4.6550
	Evaluating
	loss=0.0085, precision=0.3989, recall=0.8500, f1=0.5430
Training epoch 53
	step [1/196], loss=73.1235
	step [2/196], loss=81.2517
	step [3/196], loss=94.3948
	step [4/196], loss=97.8661
	step [5/196], loss=81.1816
	step [6/196], loss=83.9519
	step [7/196], loss=81.4763
	step [8/196], loss=79.8657
	step [9/196], loss=76.8780
	step [10/196], loss=84.9373
	step [11/196], loss=63.8974
	step [12/196], loss=86.0617
	step [13/196], loss=60.5145
	step [14/196], loss=77.8218
	step [15/196], loss=79.8436
	step [16/196], loss=90.9985
	step [17/196], loss=74.2771
	step [18/196], loss=68.9059
	step [19/196], loss=68.7715
	step [20/196], loss=88.5597
	step [21/196], loss=79.9237
	step [22/196], loss=72.7011
	step [23/196], loss=85.2987
	step [24/196], loss=101.6457
	step [25/196], loss=76.1833
	step [26/196], loss=74.8222
	step [27/196], loss=75.7459
	step [28/196], loss=73.2621
	step [29/196], loss=83.8356
	step [30/196], loss=98.9108
	step [31/196], loss=73.7861
	step [32/196], loss=87.5311
	step [33/196], loss=82.3013
	step [34/196], loss=77.8376
	step [35/196], loss=74.8829
	step [36/196], loss=100.8596
	step [37/196], loss=83.8261
	step [38/196], loss=85.7322
	step [39/196], loss=86.5291
	step [40/196], loss=80.6042
	step [41/196], loss=84.9778
	step [42/196], loss=86.4165
	step [43/196], loss=76.0452
	step [44/196], loss=82.6139
	step [45/196], loss=62.0845
	step [46/196], loss=80.2466
	step [47/196], loss=90.4866
	step [48/196], loss=73.2845
	step [49/196], loss=65.5556
	step [50/196], loss=79.0024
	step [51/196], loss=76.2469
	step [52/196], loss=91.6938
	step [53/196], loss=75.9549
	step [54/196], loss=68.1765
	step [55/196], loss=66.9488
	step [56/196], loss=74.1818
	step [57/196], loss=85.9064
	step [58/196], loss=82.5387
	step [59/196], loss=98.5484
	step [60/196], loss=69.4024
	step [61/196], loss=76.4259
	step [62/196], loss=78.5859
	step [63/196], loss=93.4565
	step [64/196], loss=69.3834
	step [65/196], loss=71.1435
	step [66/196], loss=75.6243
	step [67/196], loss=68.5061
	step [68/196], loss=78.6303
	step [69/196], loss=80.8693
	step [70/196], loss=74.8234
	step [71/196], loss=86.9604
	step [72/196], loss=79.6768
	step [73/196], loss=70.0839
	step [74/196], loss=84.5902
	step [75/196], loss=62.4131
	step [76/196], loss=74.7167
	step [77/196], loss=87.0812
	step [78/196], loss=79.7298
	step [79/196], loss=61.6288
	step [80/196], loss=80.3363
	step [81/196], loss=89.2114
	step [82/196], loss=70.3081
	step [83/196], loss=70.5748
	step [84/196], loss=74.7753
	step [85/196], loss=79.9497
	step [86/196], loss=81.4607
	step [87/196], loss=76.4985
	step [88/196], loss=76.0625
	step [89/196], loss=93.7637
	step [90/196], loss=77.4294
	step [91/196], loss=69.2244
	step [92/196], loss=85.0173
	step [93/196], loss=77.6434
	step [94/196], loss=83.7884
	step [95/196], loss=89.4027
	step [96/196], loss=72.1788
	step [97/196], loss=71.9143
	step [98/196], loss=67.6060
	step [99/196], loss=83.5344
	step [100/196], loss=65.3531
	step [101/196], loss=75.5074
	step [102/196], loss=66.9126
	step [103/196], loss=75.6098
	step [104/196], loss=82.4315
	step [105/196], loss=83.9504
	step [106/196], loss=82.0814
	step [107/196], loss=78.3682
	step [108/196], loss=76.0719
	step [109/196], loss=88.4154
	step [110/196], loss=88.1021
	step [111/196], loss=80.9256
	step [112/196], loss=71.0702
	step [113/196], loss=63.7779
	step [114/196], loss=76.6273
	step [115/196], loss=63.7467
	step [116/196], loss=81.6125
	step [117/196], loss=83.1307
	step [118/196], loss=69.5890
	step [119/196], loss=77.6581
	step [120/196], loss=91.1766
	step [121/196], loss=81.6511
	step [122/196], loss=86.9621
	step [123/196], loss=78.3158
	step [124/196], loss=66.3430
	step [125/196], loss=63.6208
	step [126/196], loss=70.3075
	step [127/196], loss=84.7756
	step [128/196], loss=71.2785
	step [129/196], loss=72.7862
	step [130/196], loss=87.8349
	step [131/196], loss=81.7814
	step [132/196], loss=75.0091
	step [133/196], loss=71.6577
	step [134/196], loss=76.9231
	step [135/196], loss=74.6883
	step [136/196], loss=66.7668
	step [137/196], loss=80.9403
	step [138/196], loss=62.3901
	step [139/196], loss=83.6035
	step [140/196], loss=89.2541
	step [141/196], loss=76.1178
	step [142/196], loss=85.4704
	step [143/196], loss=70.3892
	step [144/196], loss=77.6489
	step [145/196], loss=85.2805
	step [146/196], loss=80.8761
	step [147/196], loss=68.2796
	step [148/196], loss=88.4594
	step [149/196], loss=68.7853
	step [150/196], loss=83.8654
	step [151/196], loss=82.0304
	step [152/196], loss=72.3502
	step [153/196], loss=87.8375
	step [154/196], loss=80.3971
	step [155/196], loss=82.5173
	step [156/196], loss=73.1015
	step [157/196], loss=76.6461
	step [158/196], loss=76.5663
	step [159/196], loss=82.3902
	step [160/196], loss=68.7403
	step [161/196], loss=79.6126
	step [162/196], loss=76.9756
	step [163/196], loss=69.5627
	step [164/196], loss=76.0930
	step [165/196], loss=79.3824
	step [166/196], loss=73.5930
	step [167/196], loss=75.6771
	step [168/196], loss=78.2915
	step [169/196], loss=80.0649
	step [170/196], loss=78.1901
	step [171/196], loss=77.6296
	step [172/196], loss=68.6576
	step [173/196], loss=91.2079
	step [174/196], loss=81.4850
	step [175/196], loss=71.2225
	step [176/196], loss=74.2861
	step [177/196], loss=85.7414
	step [178/196], loss=80.8890
	step [179/196], loss=84.7272
	step [180/196], loss=75.9870
	step [181/196], loss=65.7022
	step [182/196], loss=75.0736
	step [183/196], loss=78.1719
	step [184/196], loss=82.4067
	step [185/196], loss=72.9036
	step [186/196], loss=65.5928
	step [187/196], loss=70.4275
	step [188/196], loss=88.6764
	step [189/196], loss=77.3842
	step [190/196], loss=82.5711
	step [191/196], loss=73.5602
	step [192/196], loss=78.7331
	step [193/196], loss=84.1693
	step [194/196], loss=70.0159
	step [195/196], loss=85.5425
	step [196/196], loss=14.3643
	Evaluating
	loss=0.0079, precision=0.4200, recall=0.8551, f1=0.5634
Training epoch 54
	step [1/196], loss=87.0518
	step [2/196], loss=91.3965
	step [3/196], loss=81.3425
	step [4/196], loss=72.3197
	step [5/196], loss=67.5239
	step [6/196], loss=68.8280
	step [7/196], loss=74.9908
	step [8/196], loss=86.6141
	step [9/196], loss=79.4276
	step [10/196], loss=84.8375
	step [11/196], loss=66.5153
	step [12/196], loss=83.5090
	step [13/196], loss=73.4232
	step [14/196], loss=64.9249
	step [15/196], loss=78.4929
	step [16/196], loss=75.3345
	step [17/196], loss=71.1116
	step [18/196], loss=85.5495
	step [19/196], loss=76.3494
	step [20/196], loss=90.6336
	step [21/196], loss=79.0899
	step [22/196], loss=88.1802
	step [23/196], loss=86.5101
	step [24/196], loss=80.3896
	step [25/196], loss=88.6499
	step [26/196], loss=74.2177
	step [27/196], loss=75.2812
	step [28/196], loss=84.7198
	step [29/196], loss=89.4718
	step [30/196], loss=75.0889
	step [31/196], loss=87.2930
	step [32/196], loss=80.2459
	step [33/196], loss=70.7676
	step [34/196], loss=75.6295
	step [35/196], loss=75.8688
	step [36/196], loss=73.2147
	step [37/196], loss=86.9955
	step [38/196], loss=68.2322
	step [39/196], loss=80.9304
	step [40/196], loss=71.4658
	step [41/196], loss=72.5748
	step [42/196], loss=70.9801
	step [43/196], loss=80.0429
	step [44/196], loss=76.4613
	step [45/196], loss=73.7034
	step [46/196], loss=83.9842
	step [47/196], loss=90.8990
	step [48/196], loss=106.2133
	step [49/196], loss=90.3496
	step [50/196], loss=88.2644
	step [51/196], loss=81.6488
	step [52/196], loss=75.7395
	step [53/196], loss=69.8656
	step [54/196], loss=78.1811
	step [55/196], loss=59.5417
	step [56/196], loss=71.0378
	step [57/196], loss=82.4524
	step [58/196], loss=85.4316
	step [59/196], loss=71.4331
	step [60/196], loss=69.2440
	step [61/196], loss=72.7400
	step [62/196], loss=81.3803
	step [63/196], loss=80.2092
	step [64/196], loss=83.5920
	step [65/196], loss=74.2002
	step [66/196], loss=72.2232
	step [67/196], loss=81.9107
	step [68/196], loss=72.8860
	step [69/196], loss=67.6487
	step [70/196], loss=86.5833
	step [71/196], loss=80.4890
	step [72/196], loss=85.2357
	step [73/196], loss=65.7080
	step [74/196], loss=83.8134
	step [75/196], loss=66.3697
	step [76/196], loss=84.9032
	step [77/196], loss=75.4771
	step [78/196], loss=81.9730
	step [79/196], loss=79.0898
	step [80/196], loss=89.1060
	step [81/196], loss=74.6928
	step [82/196], loss=81.6556
	step [83/196], loss=75.8242
	step [84/196], loss=75.3318
	step [85/196], loss=80.1796
	step [86/196], loss=73.1971
	step [87/196], loss=79.1618
	step [88/196], loss=73.6702
	step [89/196], loss=72.6961
	step [90/196], loss=73.9114
	step [91/196], loss=81.4051
	step [92/196], loss=71.7025
	step [93/196], loss=82.1654
	step [94/196], loss=74.6743
	step [95/196], loss=76.8678
	step [96/196], loss=81.3719
	step [97/196], loss=81.5804
	step [98/196], loss=82.6377
	step [99/196], loss=86.0292
	step [100/196], loss=69.0028
	step [101/196], loss=70.2170
	step [102/196], loss=61.5839
	step [103/196], loss=78.9150
	step [104/196], loss=77.6913
	step [105/196], loss=70.2778
	step [106/196], loss=70.9502
	step [107/196], loss=72.8076
	step [108/196], loss=75.6814
	step [109/196], loss=91.9015
	step [110/196], loss=78.5949
	step [111/196], loss=73.0455
	step [112/196], loss=71.3493
	step [113/196], loss=96.9942
	step [114/196], loss=85.4001
	step [115/196], loss=81.4222
	step [116/196], loss=63.9312
	step [117/196], loss=82.5282
	step [118/196], loss=71.8705
	step [119/196], loss=84.2502
	step [120/196], loss=63.3692
	step [121/196], loss=84.3418
	step [122/196], loss=76.8436
	step [123/196], loss=71.4918
	step [124/196], loss=95.3383
	step [125/196], loss=75.6173
	step [126/196], loss=78.1947
	step [127/196], loss=83.7180
	step [128/196], loss=83.9689
	step [129/196], loss=75.9384
	step [130/196], loss=88.5428
	step [131/196], loss=77.0187
	step [132/196], loss=75.7007
	step [133/196], loss=78.0260
	step [134/196], loss=78.4665
	step [135/196], loss=77.2502
	step [136/196], loss=88.4562
	step [137/196], loss=86.4105
	step [138/196], loss=69.9249
	step [139/196], loss=69.3128
	step [140/196], loss=62.6225
	step [141/196], loss=84.1933
	step [142/196], loss=76.8846
	step [143/196], loss=72.2919
	step [144/196], loss=74.1050
	step [145/196], loss=85.9670
	step [146/196], loss=72.3274
	step [147/196], loss=91.4763
	step [148/196], loss=82.3662
	step [149/196], loss=70.1713
	step [150/196], loss=93.3890
	step [151/196], loss=83.3913
	step [152/196], loss=81.5850
	step [153/196], loss=90.1566
	step [154/196], loss=71.4000
	step [155/196], loss=75.3707
	step [156/196], loss=93.0626
	step [157/196], loss=75.2695
	step [158/196], loss=78.1443
	step [159/196], loss=78.8297
	step [160/196], loss=74.5974
	step [161/196], loss=78.9842
	step [162/196], loss=71.8528
	step [163/196], loss=72.9872
	step [164/196], loss=77.9476
	step [165/196], loss=80.4159
	step [166/196], loss=88.5286
	step [167/196], loss=79.3706
	step [168/196], loss=92.2007
	step [169/196], loss=74.2739
	step [170/196], loss=58.4833
	step [171/196], loss=82.8791
	step [172/196], loss=80.6428
	step [173/196], loss=69.0548
	step [174/196], loss=76.2809
	step [175/196], loss=77.5829
	step [176/196], loss=83.4952
	step [177/196], loss=86.0009
	step [178/196], loss=73.5536
	step [179/196], loss=86.7278
	step [180/196], loss=83.8210
	step [181/196], loss=74.7184
	step [182/196], loss=85.6305
	step [183/196], loss=73.6812
	step [184/196], loss=75.3551
	step [185/196], loss=79.3474
	step [186/196], loss=77.5703
	step [187/196], loss=69.8335
	step [188/196], loss=92.5798
	step [189/196], loss=80.4942
	step [190/196], loss=75.7500
	step [191/196], loss=76.0480
	step [192/196], loss=76.0091
	step [193/196], loss=77.7859
	step [194/196], loss=57.8856
	step [195/196], loss=89.2910
	step [196/196], loss=5.4182
	Evaluating
	loss=0.0119, precision=0.2895, recall=0.8656, f1=0.4339
Training epoch 55
	step [1/196], loss=80.0933
	step [2/196], loss=83.9934
	step [3/196], loss=76.6351
	step [4/196], loss=74.6536
	step [5/196], loss=82.6178
	step [6/196], loss=80.5276
	step [7/196], loss=80.5362
	step [8/196], loss=86.2982
	step [9/196], loss=68.1697
	step [10/196], loss=92.8283
	step [11/196], loss=78.1223
	step [12/196], loss=81.3810
	step [13/196], loss=69.2339
	step [14/196], loss=68.0866
	step [15/196], loss=95.6961
	step [16/196], loss=71.1514
	step [17/196], loss=73.7307
	step [18/196], loss=78.6945
	step [19/196], loss=78.9174
	step [20/196], loss=71.6685
	step [21/196], loss=77.2084
	step [22/196], loss=75.5190
	step [23/196], loss=69.6905
	step [24/196], loss=60.6723
	step [25/196], loss=72.3213
	step [26/196], loss=78.9825
	step [27/196], loss=78.4135
	step [28/196], loss=75.9629
	step [29/196], loss=89.6131
	step [30/196], loss=73.5940
	step [31/196], loss=88.2989
	step [32/196], loss=67.9555
	step [33/196], loss=64.8847
	step [34/196], loss=86.9802
	step [35/196], loss=87.8318
	step [36/196], loss=64.4636
	step [37/196], loss=76.4367
	step [38/196], loss=85.8571
	step [39/196], loss=76.5936
	step [40/196], loss=80.4672
	step [41/196], loss=80.2734
	step [42/196], loss=74.6304
	step [43/196], loss=72.4342
	step [44/196], loss=96.9169
	step [45/196], loss=81.0924
	step [46/196], loss=72.0237
	step [47/196], loss=81.0184
	step [48/196], loss=79.4460
	step [49/196], loss=82.4226
	step [50/196], loss=69.6568
	step [51/196], loss=63.0017
	step [52/196], loss=80.6003
	step [53/196], loss=95.4870
	step [54/196], loss=68.4218
	step [55/196], loss=71.3700
	step [56/196], loss=66.9777
	step [57/196], loss=64.7837
	step [58/196], loss=82.9484
	step [59/196], loss=88.3793
	step [60/196], loss=82.7676
	step [61/196], loss=76.5319
	step [62/196], loss=81.9718
	step [63/196], loss=93.1137
	step [64/196], loss=80.6955
	step [65/196], loss=65.2226
	step [66/196], loss=78.8479
	step [67/196], loss=64.0293
	step [68/196], loss=77.8577
	step [69/196], loss=62.6097
	step [70/196], loss=71.7644
	step [71/196], loss=65.4713
	step [72/196], loss=79.3000
	step [73/196], loss=68.9647
	step [74/196], loss=78.8633
	step [75/196], loss=80.3095
	step [76/196], loss=84.2819
	step [77/196], loss=71.8646
	step [78/196], loss=84.9320
	step [79/196], loss=84.1976
	step [80/196], loss=66.0337
	step [81/196], loss=87.6182
	step [82/196], loss=81.3601
	step [83/196], loss=80.6409
	step [84/196], loss=80.5379
	step [85/196], loss=87.7523
	step [86/196], loss=80.6327
	step [87/196], loss=82.3096
	step [88/196], loss=84.3375
	step [89/196], loss=72.3954
	step [90/196], loss=84.9163
	step [91/196], loss=78.2696
	step [92/196], loss=82.5686
	step [93/196], loss=87.2806
	step [94/196], loss=72.9267
	step [95/196], loss=76.0205
	step [96/196], loss=88.6023
	step [97/196], loss=82.8785
	step [98/196], loss=77.9774
	step [99/196], loss=77.3420
	step [100/196], loss=71.3005
	step [101/196], loss=81.2396
	step [102/196], loss=64.9843
	step [103/196], loss=72.4807
	step [104/196], loss=63.3344
	step [105/196], loss=74.0680
	step [106/196], loss=87.1420
	step [107/196], loss=67.3649
	step [108/196], loss=73.1707
	step [109/196], loss=79.5751
	step [110/196], loss=67.9783
	step [111/196], loss=80.8798
	step [112/196], loss=82.0271
	step [113/196], loss=82.7403
	step [114/196], loss=66.3473
	step [115/196], loss=82.7016
	step [116/196], loss=85.0134
	step [117/196], loss=75.3751
	step [118/196], loss=79.2090
	step [119/196], loss=81.2114
	step [120/196], loss=77.3343
	step [121/196], loss=78.1938
	step [122/196], loss=82.5206
	step [123/196], loss=65.8914
	step [124/196], loss=93.0995
	step [125/196], loss=68.5956
	step [126/196], loss=84.1936
	step [127/196], loss=84.6728
	step [128/196], loss=72.5656
	step [129/196], loss=84.1203
	step [130/196], loss=70.4464
	step [131/196], loss=86.2240
	step [132/196], loss=81.4414
	step [133/196], loss=88.0133
	step [134/196], loss=76.1568
	step [135/196], loss=68.8447
	step [136/196], loss=83.7249
	step [137/196], loss=79.7788
	step [138/196], loss=72.1763
	step [139/196], loss=85.8228
	step [140/196], loss=77.8334
	step [141/196], loss=72.8446
	step [142/196], loss=83.5557
	step [143/196], loss=77.8097
	step [144/196], loss=81.4120
	step [145/196], loss=68.3614
	step [146/196], loss=76.3986
	step [147/196], loss=66.9568
	step [148/196], loss=74.4858
	step [149/196], loss=85.1743
	step [150/196], loss=80.7618
	step [151/196], loss=92.0284
	step [152/196], loss=88.1617
	step [153/196], loss=78.1597
	step [154/196], loss=90.6380
	step [155/196], loss=61.4315
	step [156/196], loss=78.8209
	step [157/196], loss=76.1839
	step [158/196], loss=84.8304
	step [159/196], loss=74.7756
	step [160/196], loss=76.9111
	step [161/196], loss=75.8723
	step [162/196], loss=77.8666
	step [163/196], loss=85.0684
	step [164/196], loss=87.0423
	step [165/196], loss=88.6224
	step [166/196], loss=76.8967
	step [167/196], loss=71.6856
	step [168/196], loss=74.8290
	step [169/196], loss=73.9390
	step [170/196], loss=65.0922
	step [171/196], loss=77.5499
	step [172/196], loss=71.8140
	step [173/196], loss=70.0645
	step [174/196], loss=89.9526
	step [175/196], loss=80.6900
	step [176/196], loss=87.1769
	step [177/196], loss=83.3606
	step [178/196], loss=73.8586
	step [179/196], loss=59.8860
	step [180/196], loss=75.1600
	step [181/196], loss=70.1515
	step [182/196], loss=79.1819
	step [183/196], loss=74.9935
	step [184/196], loss=82.7564
	step [185/196], loss=82.6880
	step [186/196], loss=73.0905
	step [187/196], loss=77.2608
	step [188/196], loss=77.5818
	step [189/196], loss=70.8885
	step [190/196], loss=78.7581
	step [191/196], loss=74.1357
	step [192/196], loss=86.8552
	step [193/196], loss=76.7802
	step [194/196], loss=87.6347
	step [195/196], loss=82.7298
	step [196/196], loss=4.7116
	Evaluating
	loss=0.0107, precision=0.3123, recall=0.8591, f1=0.4581
Training epoch 56
	step [1/196], loss=87.4794
	step [2/196], loss=83.8022
	step [3/196], loss=87.3378
	step [4/196], loss=78.4747
	step [5/196], loss=73.5928
	step [6/196], loss=92.0461
	step [7/196], loss=75.9371
	step [8/196], loss=88.1225
	step [9/196], loss=70.8394
	step [10/196], loss=91.5148
	step [11/196], loss=76.4391
	step [12/196], loss=78.9914
	step [13/196], loss=73.6519
	step [14/196], loss=65.2936
	step [15/196], loss=89.8783
	step [16/196], loss=79.8359
	step [17/196], loss=87.9564
	step [18/196], loss=84.4445
	step [19/196], loss=74.3063
	step [20/196], loss=56.5892
	step [21/196], loss=80.4736
	step [22/196], loss=74.7482
	step [23/196], loss=74.2998
	step [24/196], loss=61.6526
	step [25/196], loss=81.1909
	step [26/196], loss=65.8884
	step [27/196], loss=72.1540
	step [28/196], loss=77.3718
	step [29/196], loss=64.3082
	step [30/196], loss=73.3061
	step [31/196], loss=77.4078
	step [32/196], loss=91.2091
	step [33/196], loss=82.5234
	step [34/196], loss=89.5449
	step [35/196], loss=80.3267
	step [36/196], loss=73.7209
	step [37/196], loss=61.5415
	step [38/196], loss=90.3172
	step [39/196], loss=76.7710
	step [40/196], loss=66.7411
	step [41/196], loss=67.7969
	step [42/196], loss=79.5774
	step [43/196], loss=79.0859
	step [44/196], loss=76.4733
	step [45/196], loss=91.7789
	step [46/196], loss=78.9351
	step [47/196], loss=87.7867
	step [48/196], loss=84.5667
	step [49/196], loss=82.2308
	step [50/196], loss=89.2244
	step [51/196], loss=79.7389
	step [52/196], loss=95.0712
	step [53/196], loss=83.6429
	step [54/196], loss=71.1319
	step [55/196], loss=74.3566
	step [56/196], loss=77.7628
	step [57/196], loss=76.3227
	step [58/196], loss=78.1518
	step [59/196], loss=71.7106
	step [60/196], loss=78.7961
	step [61/196], loss=62.9352
	step [62/196], loss=73.7811
	step [63/196], loss=72.5449
	step [64/196], loss=76.2902
	step [65/196], loss=72.7887
	step [66/196], loss=77.9078
	step [67/196], loss=77.8553
	step [68/196], loss=90.5192
	step [69/196], loss=94.7528
	step [70/196], loss=66.7274
	step [71/196], loss=90.3423
	step [72/196], loss=81.8284
	step [73/196], loss=75.1502
	step [74/196], loss=75.0486
	step [75/196], loss=73.3025
	step [76/196], loss=88.2568
	step [77/196], loss=72.3581
	step [78/196], loss=74.8696
	step [79/196], loss=69.3947
	step [80/196], loss=86.5337
	step [81/196], loss=68.2298
	step [82/196], loss=75.6580
	step [83/196], loss=97.0492
	step [84/196], loss=72.3204
	step [85/196], loss=69.0085
	step [86/196], loss=79.7915
	step [87/196], loss=73.9986
	step [88/196], loss=71.1611
	step [89/196], loss=77.0892
	step [90/196], loss=95.9520
	step [91/196], loss=74.7608
	step [92/196], loss=66.8499
	step [93/196], loss=69.8710
	step [94/196], loss=75.1054
	step [95/196], loss=78.7145
	step [96/196], loss=77.6428
	step [97/196], loss=77.1404
	step [98/196], loss=77.0741
	step [99/196], loss=99.0657
	step [100/196], loss=87.4140
	step [101/196], loss=85.0924
	step [102/196], loss=69.7575
	step [103/196], loss=92.7146
	step [104/196], loss=57.4770
	step [105/196], loss=69.3094
	step [106/196], loss=85.7721
	step [107/196], loss=79.5841
	step [108/196], loss=67.8226
	step [109/196], loss=83.1223
	step [110/196], loss=72.1674
	step [111/196], loss=69.6077
	step [112/196], loss=88.9546
	step [113/196], loss=90.5130
	step [114/196], loss=93.8953
	step [115/196], loss=65.9790
	step [116/196], loss=80.8346
	step [117/196], loss=77.7955
	step [118/196], loss=82.9931
	step [119/196], loss=76.9366
	step [120/196], loss=73.5831
	step [121/196], loss=83.0293
	step [122/196], loss=76.9481
	step [123/196], loss=81.3427
	step [124/196], loss=70.0001
	step [125/196], loss=79.3302
	step [126/196], loss=68.7173
	step [127/196], loss=94.9509
	step [128/196], loss=84.8532
	step [129/196], loss=61.9396
	step [130/196], loss=81.2106
	step [131/196], loss=72.7001
	step [132/196], loss=83.6695
	step [133/196], loss=76.4025
	step [134/196], loss=78.1840
	step [135/196], loss=65.0762
	step [136/196], loss=78.0040
	step [137/196], loss=84.9291
	step [138/196], loss=86.5414
	step [139/196], loss=61.5558
	step [140/196], loss=71.2069
	step [141/196], loss=83.6067
	step [142/196], loss=79.2321
	step [143/196], loss=64.9077
	step [144/196], loss=74.8394
	step [145/196], loss=71.6118
	step [146/196], loss=90.2648
	step [147/196], loss=92.5064
	step [148/196], loss=73.5705
	step [149/196], loss=77.5633
	step [150/196], loss=88.4338
	step [151/196], loss=94.0418
	step [152/196], loss=65.5010
	step [153/196], loss=69.0798
	step [154/196], loss=78.9456
	step [155/196], loss=74.6130
	step [156/196], loss=91.7470
	step [157/196], loss=85.8089
	step [158/196], loss=85.2430
	step [159/196], loss=67.4968
	step [160/196], loss=76.6030
	step [161/196], loss=76.0867
	step [162/196], loss=79.1671
	step [163/196], loss=100.2963
	step [164/196], loss=75.1071
	step [165/196], loss=79.9195
	step [166/196], loss=80.4020
	step [167/196], loss=72.8767
	step [168/196], loss=60.3653
	step [169/196], loss=81.8403
	step [170/196], loss=77.2625
	step [171/196], loss=79.1255
	step [172/196], loss=78.3952
	step [173/196], loss=75.2135
	step [174/196], loss=61.5161
	step [175/196], loss=85.4396
	step [176/196], loss=55.5823
	step [177/196], loss=88.6809
	step [178/196], loss=77.9452
	step [179/196], loss=59.7847
	step [180/196], loss=74.0538
	step [181/196], loss=85.0930
	step [182/196], loss=79.1405
	step [183/196], loss=87.8128
	step [184/196], loss=72.0076
	step [185/196], loss=81.1282
	step [186/196], loss=84.8812
	step [187/196], loss=72.2359
	step [188/196], loss=73.5562
	step [189/196], loss=73.1377
	step [190/196], loss=79.7637
	step [191/196], loss=79.3232
	step [192/196], loss=93.5263
	step [193/196], loss=78.3159
	step [194/196], loss=96.4734
	step [195/196], loss=64.9069
	step [196/196], loss=5.8017
	Evaluating
	loss=0.0072, precision=0.4275, recall=0.8572, f1=0.5704
Training epoch 57
	step [1/196], loss=73.2622
	step [2/196], loss=71.9167
	step [3/196], loss=75.0019
	step [4/196], loss=68.8387
	step [5/196], loss=89.6506
	step [6/196], loss=69.2442
	step [7/196], loss=76.0260
	step [8/196], loss=76.8004
	step [9/196], loss=83.8702
	step [10/196], loss=77.4553
	step [11/196], loss=80.3209
	step [12/196], loss=86.8971
	step [13/196], loss=69.2115
	step [14/196], loss=75.4216
	step [15/196], loss=84.3530
	step [16/196], loss=78.6060
	step [17/196], loss=76.6671
	step [18/196], loss=71.8025
	step [19/196], loss=87.1720
	step [20/196], loss=71.7788
	step [21/196], loss=65.1832
	step [22/196], loss=93.9557
	step [23/196], loss=96.6990
	step [24/196], loss=82.5946
	step [25/196], loss=67.0930
	step [26/196], loss=80.1576
	step [27/196], loss=82.6257
	step [28/196], loss=72.0276
	step [29/196], loss=86.2776
	step [30/196], loss=74.1304
	step [31/196], loss=78.4281
	step [32/196], loss=91.2007
	step [33/196], loss=68.3187
	step [34/196], loss=85.1943
	step [35/196], loss=76.7929
	step [36/196], loss=74.1922
	step [37/196], loss=81.9525
	step [38/196], loss=85.1533
	step [39/196], loss=86.0731
	step [40/196], loss=82.0463
	step [41/196], loss=65.9355
	step [42/196], loss=75.4031
	step [43/196], loss=82.5281
	step [44/196], loss=64.2243
	step [45/196], loss=86.2696
	step [46/196], loss=79.4400
	step [47/196], loss=80.5592
	step [48/196], loss=82.3278
	step [49/196], loss=74.3868
	step [50/196], loss=79.7758
	step [51/196], loss=69.9678
	step [52/196], loss=83.8556
	step [53/196], loss=71.7564
	step [54/196], loss=80.4279
	step [55/196], loss=65.0877
	step [56/196], loss=76.2400
	step [57/196], loss=80.4013
	step [58/196], loss=76.1569
	step [59/196], loss=68.4321
	step [60/196], loss=74.6823
	step [61/196], loss=78.4362
	step [62/196], loss=81.1510
	step [63/196], loss=74.0343
	step [64/196], loss=84.4762
	step [65/196], loss=92.7039
	step [66/196], loss=68.2377
	step [67/196], loss=62.0334
	step [68/196], loss=79.5212
	step [69/196], loss=77.6043
	step [70/196], loss=75.6899
	step [71/196], loss=85.5332
	step [72/196], loss=63.8282
	step [73/196], loss=71.0051
	step [74/196], loss=73.7872
	step [75/196], loss=84.0535
	step [76/196], loss=81.9052
	step [77/196], loss=73.9298
	step [78/196], loss=84.2095
	step [79/196], loss=84.0140
	step [80/196], loss=61.3662
	step [81/196], loss=75.0339
	step [82/196], loss=81.8782
	step [83/196], loss=89.4134
	step [84/196], loss=80.8225
	step [85/196], loss=63.5004
	step [86/196], loss=85.2582
	step [87/196], loss=74.3129
	step [88/196], loss=85.0999
	step [89/196], loss=63.9204
	step [90/196], loss=75.3610
	step [91/196], loss=67.6161
	step [92/196], loss=72.7854
	step [93/196], loss=97.4464
	step [94/196], loss=76.4013
	step [95/196], loss=74.3081
	step [96/196], loss=67.6497
	step [97/196], loss=91.3289
	step [98/196], loss=75.4022
	step [99/196], loss=71.3646
	step [100/196], loss=71.6306
	step [101/196], loss=87.5942
	step [102/196], loss=81.3871
	step [103/196], loss=89.9076
	step [104/196], loss=83.7230
	step [105/196], loss=75.4604
	step [106/196], loss=74.6709
	step [107/196], loss=81.4353
	step [108/196], loss=85.3901
	step [109/196], loss=68.8588
	step [110/196], loss=80.1402
	step [111/196], loss=63.2065
	step [112/196], loss=71.6771
	step [113/196], loss=88.7081
	step [114/196], loss=85.8202
	step [115/196], loss=90.0475
	step [116/196], loss=78.7024
	step [117/196], loss=73.9009
	step [118/196], loss=82.2639
	step [119/196], loss=72.1479
	step [120/196], loss=93.8325
	step [121/196], loss=93.0260
	step [122/196], loss=84.5882
	step [123/196], loss=74.9832
	step [124/196], loss=87.1962
	step [125/196], loss=63.7771
	step [126/196], loss=93.3443
	step [127/196], loss=79.6491
	step [128/196], loss=72.4011
	step [129/196], loss=81.4866
	step [130/196], loss=78.3067
	step [131/196], loss=81.1490
	step [132/196], loss=70.4984
	step [133/196], loss=93.8708
	step [134/196], loss=79.8223
	step [135/196], loss=70.8526
	step [136/196], loss=83.3840
	step [137/196], loss=61.8749
	step [138/196], loss=75.9293
	step [139/196], loss=72.7940
	step [140/196], loss=64.2051
	step [141/196], loss=83.0363
	step [142/196], loss=76.6397
	step [143/196], loss=64.5120
	step [144/196], loss=77.6255
	step [145/196], loss=80.9314
	step [146/196], loss=74.5898
	step [147/196], loss=78.6169
	step [148/196], loss=85.6986
	step [149/196], loss=64.3456
	step [150/196], loss=72.1044
	step [151/196], loss=93.2168
	step [152/196], loss=66.9238
	step [153/196], loss=69.5197
	step [154/196], loss=72.3276
	step [155/196], loss=76.1205
	step [156/196], loss=84.7558
	step [157/196], loss=77.9316
	step [158/196], loss=75.5943
	step [159/196], loss=71.7621
	step [160/196], loss=83.7535
	step [161/196], loss=73.9675
	step [162/196], loss=68.6444
	step [163/196], loss=102.6618
	step [164/196], loss=90.2156
	step [165/196], loss=84.0300
	step [166/196], loss=62.0939
	step [167/196], loss=76.1440
	step [168/196], loss=70.8013
	step [169/196], loss=69.9387
	step [170/196], loss=86.4829
	step [171/196], loss=87.0523
	step [172/196], loss=87.8623
	step [173/196], loss=74.4192
	step [174/196], loss=66.3064
	step [175/196], loss=68.5905
	step [176/196], loss=84.0783
	step [177/196], loss=70.8239
	step [178/196], loss=79.0467
	step [179/196], loss=83.2988
	step [180/196], loss=76.0959
	step [181/196], loss=70.6307
	step [182/196], loss=61.9918
	step [183/196], loss=78.0115
	step [184/196], loss=79.4830
	step [185/196], loss=75.4449
	step [186/196], loss=74.3333
	step [187/196], loss=58.4659
	step [188/196], loss=72.2293
	step [189/196], loss=83.3187
	step [190/196], loss=75.8911
	step [191/196], loss=67.5164
	step [192/196], loss=51.4665
	step [193/196], loss=75.3971
	step [194/196], loss=68.1252
	step [195/196], loss=69.2383
	step [196/196], loss=4.6667
	Evaluating
	loss=0.0065, precision=0.4855, recall=0.8505, f1=0.6181
saving model as: 3_saved_model.pth
Training epoch 58
	step [1/196], loss=68.2829
	step [2/196], loss=93.5107
	step [3/196], loss=65.6037
	step [4/196], loss=68.5020
	step [5/196], loss=77.3713
	step [6/196], loss=79.0474
	step [7/196], loss=80.2628
	step [8/196], loss=78.3156
	step [9/196], loss=83.3570
	step [10/196], loss=87.7260
	step [11/196], loss=72.4805
	step [12/196], loss=68.2309
	step [13/196], loss=75.4007
	step [14/196], loss=76.8611
	step [15/196], loss=73.8937
	step [16/196], loss=79.2505
	step [17/196], loss=71.5378
	step [18/196], loss=82.0139
	step [19/196], loss=89.0758
	step [20/196], loss=71.7373
	step [21/196], loss=84.0478
	step [22/196], loss=67.6385
	step [23/196], loss=80.0352
	step [24/196], loss=69.6141
	step [25/196], loss=63.8171
	step [26/196], loss=69.4391
	step [27/196], loss=80.8315
	step [28/196], loss=77.3134
	step [29/196], loss=75.1389
	step [30/196], loss=90.4298
	step [31/196], loss=75.6329
	step [32/196], loss=82.3058
	step [33/196], loss=90.1982
	step [34/196], loss=83.1843
	step [35/196], loss=71.1766
	step [36/196], loss=80.8715
	step [37/196], loss=77.9153
	step [38/196], loss=67.2091
	step [39/196], loss=75.5174
	step [40/196], loss=83.6284
	step [41/196], loss=68.6017
	step [42/196], loss=87.9223
	step [43/196], loss=81.9787
	step [44/196], loss=88.3008
	step [45/196], loss=67.7492
	step [46/196], loss=67.7706
	step [47/196], loss=89.4975
	step [48/196], loss=76.8024
	step [49/196], loss=73.0295
	step [50/196], loss=77.8004
	step [51/196], loss=71.5676
	step [52/196], loss=94.5645
	step [53/196], loss=75.4013
	step [54/196], loss=68.9339
	step [55/196], loss=90.6704
	step [56/196], loss=79.5559
	step [57/196], loss=79.6080
	step [58/196], loss=78.5450
	step [59/196], loss=79.3567
	step [60/196], loss=71.2294
	step [61/196], loss=74.3465
	step [62/196], loss=69.3822
	step [63/196], loss=83.5978
	step [64/196], loss=95.0029
	step [65/196], loss=84.7932
	step [66/196], loss=66.0781
	step [67/196], loss=71.6518
	step [68/196], loss=74.0868
	step [69/196], loss=75.0717
	step [70/196], loss=69.8983
	step [71/196], loss=84.7409
	step [72/196], loss=58.4200
	step [73/196], loss=72.1072
	step [74/196], loss=87.2025
	step [75/196], loss=67.9758
	step [76/196], loss=72.1579
	step [77/196], loss=81.2368
	step [78/196], loss=70.6966
	step [79/196], loss=74.7430
	step [80/196], loss=76.2518
	step [81/196], loss=88.7307
	step [82/196], loss=67.0312
	step [83/196], loss=75.6964
	step [84/196], loss=72.7755
	step [85/196], loss=72.7319
	step [86/196], loss=65.8957
	step [87/196], loss=74.4393
	step [88/196], loss=93.4345
	step [89/196], loss=77.7486
	step [90/196], loss=64.6817
	step [91/196], loss=76.0999
	step [92/196], loss=97.7799
	step [93/196], loss=76.9107
	step [94/196], loss=82.8664
	step [95/196], loss=75.5018
	step [96/196], loss=86.7331
	step [97/196], loss=99.0714
	step [98/196], loss=83.4370
	step [99/196], loss=69.4231
	step [100/196], loss=66.1736
	step [101/196], loss=81.6456
	step [102/196], loss=76.8072
	step [103/196], loss=90.9152
	step [104/196], loss=59.2350
	step [105/196], loss=68.1630
	step [106/196], loss=71.6253
	step [107/196], loss=65.4000
	step [108/196], loss=74.7585
	step [109/196], loss=85.7074
	step [110/196], loss=80.8079
	step [111/196], loss=75.4272
	step [112/196], loss=75.7074
	step [113/196], loss=73.8995
	step [114/196], loss=80.0307
	step [115/196], loss=73.4547
	step [116/196], loss=67.3857
	step [117/196], loss=72.5534
	step [118/196], loss=78.6285
	step [119/196], loss=73.6896
	step [120/196], loss=76.9895
	step [121/196], loss=76.8269
	step [122/196], loss=78.2472
	step [123/196], loss=89.5680
	step [124/196], loss=66.7848
	step [125/196], loss=77.7283
	step [126/196], loss=87.6206
	step [127/196], loss=71.8916
	step [128/196], loss=83.1231
	step [129/196], loss=69.6572
	step [130/196], loss=78.2879
	step [131/196], loss=76.8384
	step [132/196], loss=84.7487
	step [133/196], loss=81.5294
	step [134/196], loss=82.2154
	step [135/196], loss=78.2096
	step [136/196], loss=70.3754
	step [137/196], loss=70.9632
	step [138/196], loss=78.5676
	step [139/196], loss=74.3578
	step [140/196], loss=75.9878
	step [141/196], loss=81.5742
	step [142/196], loss=85.9861
	step [143/196], loss=77.3501
	step [144/196], loss=77.7634
	step [145/196], loss=92.1852
	step [146/196], loss=87.1540
	step [147/196], loss=62.9599
	step [148/196], loss=67.3477
	step [149/196], loss=71.9357
	step [150/196], loss=89.1694
	step [151/196], loss=87.4838
	step [152/196], loss=71.2990
	step [153/196], loss=87.0655
	step [154/196], loss=68.8809
	step [155/196], loss=90.3145
	step [156/196], loss=67.3562
	step [157/196], loss=68.0286
	step [158/196], loss=74.0949
	step [159/196], loss=65.4794
	step [160/196], loss=74.3981
	step [161/196], loss=86.7426
	step [162/196], loss=93.5042
	step [163/196], loss=63.2513
	step [164/196], loss=82.4601
	step [165/196], loss=76.0202
	step [166/196], loss=76.0344
	step [167/196], loss=88.1161
	step [168/196], loss=68.4660
	step [169/196], loss=80.2312
	step [170/196], loss=77.0032
	step [171/196], loss=81.9737
	step [172/196], loss=74.4422
	step [173/196], loss=80.4716
	step [174/196], loss=61.1345
	step [175/196], loss=73.4099
	step [176/196], loss=79.4992
	step [177/196], loss=75.6714
	step [178/196], loss=80.2852
	step [179/196], loss=69.1546
	step [180/196], loss=71.2857
	step [181/196], loss=65.1155
	step [182/196], loss=72.0057
	step [183/196], loss=61.8412
	step [184/196], loss=76.1210
	step [185/196], loss=80.0134
	step [186/196], loss=86.0242
	step [187/196], loss=77.5279
	step [188/196], loss=80.9722
	step [189/196], loss=75.5822
	step [190/196], loss=76.4122
	step [191/196], loss=70.1340
	step [192/196], loss=67.2638
	step [193/196], loss=99.7094
	step [194/196], loss=69.2838
	step [195/196], loss=76.8144
	step [196/196], loss=6.9405
	Evaluating
	loss=0.0070, precision=0.4468, recall=0.8452, f1=0.5846
Training epoch 59
	step [1/196], loss=79.0736
	step [2/196], loss=77.9703
	step [3/196], loss=83.6672
	step [4/196], loss=72.7335
	step [5/196], loss=68.5915
	step [6/196], loss=70.4778
	step [7/196], loss=57.4939
	step [8/196], loss=75.9599
	step [9/196], loss=68.9505
	step [10/196], loss=81.7712
	step [11/196], loss=78.6885
	step [12/196], loss=82.8134
	step [13/196], loss=75.7363
	step [14/196], loss=75.5134
	step [15/196], loss=74.9076
	step [16/196], loss=81.8225
	step [17/196], loss=92.8068
	step [18/196], loss=64.1908
	step [19/196], loss=73.8411
	step [20/196], loss=78.6622
	step [21/196], loss=67.9242
	step [22/196], loss=62.3089
	step [23/196], loss=69.5174
	step [24/196], loss=101.6671
	step [25/196], loss=77.1115
	step [26/196], loss=81.4552
	step [27/196], loss=72.1523
	step [28/196], loss=67.7674
	step [29/196], loss=66.8881
	step [30/196], loss=79.2338
	step [31/196], loss=70.8343
	step [32/196], loss=70.8756
	step [33/196], loss=81.6713
	step [34/196], loss=80.3598
	step [35/196], loss=79.1716
	step [36/196], loss=77.0598
	step [37/196], loss=72.8163
	step [38/196], loss=75.9858
	step [39/196], loss=89.6843
	step [40/196], loss=76.9471
	step [41/196], loss=82.2053
	step [42/196], loss=77.4530
	step [43/196], loss=77.9203
	step [44/196], loss=77.3498
	step [45/196], loss=83.1966
	step [46/196], loss=71.3532
	step [47/196], loss=71.2161
	step [48/196], loss=76.5814
	step [49/196], loss=69.9023
	step [50/196], loss=80.2570
	step [51/196], loss=83.2107
	step [52/196], loss=71.5047
	step [53/196], loss=76.1543
	step [54/196], loss=72.9932
	step [55/196], loss=85.6806
	step [56/196], loss=64.1843
	step [57/196], loss=73.6652
	step [58/196], loss=76.1172
	step [59/196], loss=79.8844
	step [60/196], loss=82.0243
	step [61/196], loss=82.8675
	step [62/196], loss=81.2162
	step [63/196], loss=81.8892
	step [64/196], loss=82.7703
	step [65/196], loss=75.9740
	step [66/196], loss=72.5331
	step [67/196], loss=86.0712
	step [68/196], loss=65.4355
	step [69/196], loss=74.5130
	step [70/196], loss=72.2798
	step [71/196], loss=64.4333
	step [72/196], loss=75.3800
	step [73/196], loss=78.7075
	step [74/196], loss=76.9514
	step [75/196], loss=75.0308
	step [76/196], loss=76.1154
	step [77/196], loss=80.8850
	step [78/196], loss=82.1132
	step [79/196], loss=84.0485
	step [80/196], loss=73.2457
	step [81/196], loss=83.8379
	step [82/196], loss=63.6699
	step [83/196], loss=59.6068
	step [84/196], loss=64.1438
	step [85/196], loss=102.9387
	step [86/196], loss=79.5740
	step [87/196], loss=66.8331
	step [88/196], loss=92.3682
	step [89/196], loss=74.1330
	step [90/196], loss=70.0909
	step [91/196], loss=65.1780
	step [92/196], loss=84.1704
	step [93/196], loss=74.9644
	step [94/196], loss=77.6116
	step [95/196], loss=81.5683
	step [96/196], loss=76.4003
	step [97/196], loss=64.4579
	step [98/196], loss=67.5456
	step [99/196], loss=74.7449
	step [100/196], loss=81.1031
	step [101/196], loss=72.3036
	step [102/196], loss=90.1890
	step [103/196], loss=76.6534
	step [104/196], loss=73.1088
	step [105/196], loss=67.3811
	step [106/196], loss=103.8754
	step [107/196], loss=83.5092
	step [108/196], loss=84.8674
	step [109/196], loss=71.9354
	step [110/196], loss=75.8218
	step [111/196], loss=74.8902
	step [112/196], loss=87.3278
	step [113/196], loss=62.1738
	step [114/196], loss=76.5415
	step [115/196], loss=94.7651
	step [116/196], loss=89.3894
	step [117/196], loss=82.0867
	step [118/196], loss=63.5784
	step [119/196], loss=91.5439
	step [120/196], loss=93.1539
	step [121/196], loss=80.6815
	step [122/196], loss=69.4718
	step [123/196], loss=71.5274
	step [124/196], loss=74.8183
	step [125/196], loss=74.4771
	step [126/196], loss=69.2126
	step [127/196], loss=75.7346
	step [128/196], loss=77.2328
	step [129/196], loss=81.8396
	step [130/196], loss=72.8400
	step [131/196], loss=70.9862
	step [132/196], loss=62.1844
	step [133/196], loss=84.2942
	step [134/196], loss=74.0959
	step [135/196], loss=76.7383
	step [136/196], loss=78.0750
	step [137/196], loss=88.2076
	step [138/196], loss=76.4232
	step [139/196], loss=74.0832
	step [140/196], loss=70.9363
	step [141/196], loss=79.0149
	step [142/196], loss=69.9846
	step [143/196], loss=80.7762
	step [144/196], loss=80.6442
	step [145/196], loss=67.1231
	step [146/196], loss=92.6489
	step [147/196], loss=63.6726
	step [148/196], loss=76.0175
	step [149/196], loss=79.2639
	step [150/196], loss=74.7260
	step [151/196], loss=70.2669
	step [152/196], loss=82.5375
	step [153/196], loss=76.7952
	step [154/196], loss=76.2955
	step [155/196], loss=82.8085
	step [156/196], loss=62.3359
	step [157/196], loss=77.9559
	step [158/196], loss=79.3434
	step [159/196], loss=106.5793
	step [160/196], loss=83.8826
	step [161/196], loss=81.7186
	step [162/196], loss=83.4683
	step [163/196], loss=76.7719
	step [164/196], loss=76.1819
	step [165/196], loss=60.8801
	step [166/196], loss=61.6261
	step [167/196], loss=74.9301
	step [168/196], loss=75.2583
	step [169/196], loss=90.5044
	step [170/196], loss=69.5762
	step [171/196], loss=71.9049
	step [172/196], loss=89.2159
	step [173/196], loss=87.2293
	step [174/196], loss=80.4245
	step [175/196], loss=59.2241
	step [176/196], loss=90.4253
	step [177/196], loss=67.6009
	step [178/196], loss=78.9359
	step [179/196], loss=84.1313
	step [180/196], loss=82.8436
	step [181/196], loss=83.4671
	step [182/196], loss=73.7587
	step [183/196], loss=85.2399
	step [184/196], loss=69.5252
	step [185/196], loss=85.5835
	step [186/196], loss=76.3248
	step [187/196], loss=76.2589
	step [188/196], loss=91.6723
	step [189/196], loss=86.4769
	step [190/196], loss=91.4545
	step [191/196], loss=83.6259
	step [192/196], loss=70.6823
	step [193/196], loss=83.2989
	step [194/196], loss=80.5584
	step [195/196], loss=69.5338
	step [196/196], loss=4.7664
	Evaluating
	loss=0.0087, precision=0.3650, recall=0.8565, f1=0.5119
Training epoch 60
	step [1/196], loss=81.3004
	step [2/196], loss=69.3496
	step [3/196], loss=72.0917
	step [4/196], loss=67.5464
	step [5/196], loss=82.2025
	step [6/196], loss=78.8904
	step [7/196], loss=85.6581
	step [8/196], loss=78.6103
	step [9/196], loss=78.1469
	step [10/196], loss=77.7593
	step [11/196], loss=62.7970
	step [12/196], loss=80.6405
	step [13/196], loss=82.9310
	step [14/196], loss=82.3625
	step [15/196], loss=86.4930
	step [16/196], loss=83.2502
	step [17/196], loss=81.4264
	step [18/196], loss=60.4621
	step [19/196], loss=73.8259
	step [20/196], loss=78.0465
	step [21/196], loss=70.8298
	step [22/196], loss=79.7142
	step [23/196], loss=86.7035
	step [24/196], loss=67.7162
	step [25/196], loss=73.4647
	step [26/196], loss=71.6927
	step [27/196], loss=68.1602
	step [28/196], loss=83.2051
	step [29/196], loss=66.5470
	step [30/196], loss=70.5548
	step [31/196], loss=70.4976
	step [32/196], loss=77.9750
	step [33/196], loss=69.4942
	step [34/196], loss=84.3067
	step [35/196], loss=66.9551
	step [36/196], loss=71.0965
	step [37/196], loss=79.9413
	step [38/196], loss=74.4242
	step [39/196], loss=74.3233
	step [40/196], loss=85.4725
	step [41/196], loss=63.4704
	step [42/196], loss=87.1894
	step [43/196], loss=82.7665
	step [44/196], loss=72.3259
	step [45/196], loss=64.7429
	step [46/196], loss=88.5130
	step [47/196], loss=81.5973
	step [48/196], loss=62.9288
	step [49/196], loss=75.3424
	step [50/196], loss=75.7819
	step [51/196], loss=65.8649
	step [52/196], loss=79.2263
	step [53/196], loss=76.8575
	step [54/196], loss=64.6497
	step [55/196], loss=81.0201
	step [56/196], loss=90.3553
	step [57/196], loss=80.0872
	step [58/196], loss=68.7850
	step [59/196], loss=75.2488
	step [60/196], loss=73.0163
	step [61/196], loss=70.7351
	step [62/196], loss=69.7995
	step [63/196], loss=68.7815
	step [64/196], loss=76.8820
	step [65/196], loss=83.8886
	step [66/196], loss=86.1015
	step [67/196], loss=70.2127
	step [68/196], loss=88.3152
	step [69/196], loss=75.4671
	step [70/196], loss=69.7403
	step [71/196], loss=84.1837
	step [72/196], loss=72.0017
	step [73/196], loss=71.5414
	step [74/196], loss=74.5195
	step [75/196], loss=89.4574
	step [76/196], loss=94.2480
	step [77/196], loss=87.8459
	step [78/196], loss=76.8434
	step [79/196], loss=71.3719
	step [80/196], loss=81.0668
	step [81/196], loss=65.7811
	step [82/196], loss=76.3568
	step [83/196], loss=93.1154
	step [84/196], loss=85.4716
	step [85/196], loss=64.3267
	step [86/196], loss=75.8728
	step [87/196], loss=68.1693
	step [88/196], loss=69.6179
	step [89/196], loss=80.4050
	step [90/196], loss=52.7364
	step [91/196], loss=67.3586
	step [92/196], loss=73.0593
	step [93/196], loss=75.6239
	step [94/196], loss=88.8624
	step [95/196], loss=75.0320
	step [96/196], loss=73.5350
	step [97/196], loss=71.5581
	step [98/196], loss=77.1224
	step [99/196], loss=64.7887
	step [100/196], loss=83.3143
	step [101/196], loss=76.6802
	step [102/196], loss=83.8348
	step [103/196], loss=83.8205
	step [104/196], loss=66.6533
	step [105/196], loss=73.4299
	step [106/196], loss=70.0368
	step [107/196], loss=83.5936
	step [108/196], loss=72.7480
	step [109/196], loss=73.5484
	step [110/196], loss=68.0338
	step [111/196], loss=79.5398
	step [112/196], loss=86.6730
	step [113/196], loss=82.3918
	step [114/196], loss=77.8444
	step [115/196], loss=75.8241
	step [116/196], loss=82.7092
	step [117/196], loss=78.5639
	step [118/196], loss=88.1069
	step [119/196], loss=81.1296
	step [120/196], loss=76.7656
	step [121/196], loss=85.2679
	step [122/196], loss=79.1890
	step [123/196], loss=84.2635
	step [124/196], loss=60.6036
	step [125/196], loss=78.9547
	step [126/196], loss=85.5235
	step [127/196], loss=71.0962
	step [128/196], loss=77.6098
	step [129/196], loss=88.2187
	step [130/196], loss=65.7040
	step [131/196], loss=81.1304
	step [132/196], loss=86.8206
	step [133/196], loss=68.3022
	step [134/196], loss=68.3114
	step [135/196], loss=77.2874
	step [136/196], loss=86.0634
	step [137/196], loss=71.2371
	step [138/196], loss=68.6061
	step [139/196], loss=78.2823
	step [140/196], loss=86.1284
	step [141/196], loss=63.0134
	step [142/196], loss=87.0663
	step [143/196], loss=79.2059
	step [144/196], loss=96.4936
	step [145/196], loss=62.6239
	step [146/196], loss=81.1043
	step [147/196], loss=83.8515
	step [148/196], loss=66.9552
	step [149/196], loss=79.0200
	step [150/196], loss=78.9241
	step [151/196], loss=75.5006
	step [152/196], loss=69.2344
	step [153/196], loss=67.2755
	step [154/196], loss=63.4042
	step [155/196], loss=86.1193
	step [156/196], loss=82.2183
	step [157/196], loss=78.1697
	step [158/196], loss=83.8909
	step [159/196], loss=78.3729
	step [160/196], loss=76.7837
	step [161/196], loss=59.4807
	step [162/196], loss=81.4189
	step [163/196], loss=89.2933
	step [164/196], loss=77.0208
	step [165/196], loss=67.2616
	step [166/196], loss=84.0780
	step [167/196], loss=64.4050
	step [168/196], loss=77.7568
	step [169/196], loss=78.7679
	step [170/196], loss=71.7537
	step [171/196], loss=74.7966
	step [172/196], loss=79.1228
	step [173/196], loss=78.4931
	step [174/196], loss=94.5188
	step [175/196], loss=75.6913
	step [176/196], loss=78.9597
	step [177/196], loss=65.4125
	step [178/196], loss=94.5308
	step [179/196], loss=68.0976
	step [180/196], loss=78.2460
	step [181/196], loss=79.2690
	step [182/196], loss=70.6973
	step [183/196], loss=81.7695
	step [184/196], loss=73.2497
	step [185/196], loss=77.1307
	step [186/196], loss=72.1872
	step [187/196], loss=60.5123
	step [188/196], loss=75.1231
	step [189/196], loss=89.5032
	step [190/196], loss=67.6856
	step [191/196], loss=71.9719
	step [192/196], loss=71.9786
	step [193/196], loss=71.1439
	step [194/196], loss=83.6848
	step [195/196], loss=75.9794
	step [196/196], loss=20.4393
	Evaluating
	loss=0.0075, precision=0.4393, recall=0.7841, f1=0.5631
Training epoch 61
	step [1/196], loss=85.3840
	step [2/196], loss=68.4294
	step [3/196], loss=86.4748
	step [4/196], loss=58.1164
	step [5/196], loss=73.5176
	step [6/196], loss=80.5014
	step [7/196], loss=73.3560
	step [8/196], loss=91.9468
	step [9/196], loss=82.0305
	step [10/196], loss=67.6837
	step [11/196], loss=69.6003
	step [12/196], loss=65.4041
	step [13/196], loss=78.9468
	step [14/196], loss=82.8362
	step [15/196], loss=69.6448
	step [16/196], loss=72.3408
	step [17/196], loss=73.3844
	step [18/196], loss=88.8594
	step [19/196], loss=72.3346
	step [20/196], loss=78.4051
	step [21/196], loss=92.6476
	step [22/196], loss=69.6847
	step [23/196], loss=71.0412
	step [24/196], loss=84.1127
	step [25/196], loss=88.2354
	step [26/196], loss=71.0702
	step [27/196], loss=91.1714
	step [28/196], loss=78.3364
	step [29/196], loss=95.1684
	step [30/196], loss=75.9123
	step [31/196], loss=83.6634
	step [32/196], loss=73.6433
	step [33/196], loss=72.6383
	step [34/196], loss=73.9402
	step [35/196], loss=85.6118
	step [36/196], loss=74.4347
	step [37/196], loss=66.7118
	step [38/196], loss=77.1560
	step [39/196], loss=75.5306
	step [40/196], loss=90.5622
	step [41/196], loss=66.9761
	step [42/196], loss=83.9923
	step [43/196], loss=62.0264
	step [44/196], loss=79.7141
	step [45/196], loss=86.2223
	step [46/196], loss=88.2933
	step [47/196], loss=86.6211
	step [48/196], loss=80.0578
	step [49/196], loss=92.3841
	step [50/196], loss=83.6204
	step [51/196], loss=69.7830
	step [52/196], loss=67.6930
	step [53/196], loss=64.1207
	step [54/196], loss=78.1356
	step [55/196], loss=68.4929
	step [56/196], loss=79.1882
	step [57/196], loss=80.2556
	step [58/196], loss=82.5428
	step [59/196], loss=76.6078
	step [60/196], loss=79.9267
	step [61/196], loss=73.5738
	step [62/196], loss=74.9754
	step [63/196], loss=74.8204
	step [64/196], loss=76.7759
	step [65/196], loss=66.9677
	step [66/196], loss=73.9545
	step [67/196], loss=77.5325
	step [68/196], loss=74.7463
	step [69/196], loss=85.6033
	step [70/196], loss=85.5601
	step [71/196], loss=74.4963
	step [72/196], loss=62.8530
	step [73/196], loss=63.1280
	step [74/196], loss=86.6720
	step [75/196], loss=85.9531
	step [76/196], loss=70.2193
	step [77/196], loss=77.5145
	step [78/196], loss=71.4290
	step [79/196], loss=65.5723
	step [80/196], loss=67.0820
	step [81/196], loss=75.9825
	step [82/196], loss=76.3802
	step [83/196], loss=71.7439
	step [84/196], loss=67.7544
	step [85/196], loss=71.0569
	step [86/196], loss=78.1609
	step [87/196], loss=81.7483
	step [88/196], loss=84.2841
	step [89/196], loss=73.1757
	step [90/196], loss=72.5222
	step [91/196], loss=84.1104
	step [92/196], loss=74.5614
	step [93/196], loss=76.9106
	step [94/196], loss=87.4477
	step [95/196], loss=69.8458
	step [96/196], loss=69.3863
	step [97/196], loss=59.1815
	step [98/196], loss=86.6994
	step [99/196], loss=67.4530
	step [100/196], loss=66.8419
	step [101/196], loss=90.3962
	step [102/196], loss=84.9367
	step [103/196], loss=74.2446
	step [104/196], loss=72.2391
	step [105/196], loss=75.1633
	step [106/196], loss=75.6767
	step [107/196], loss=81.4116
	step [108/196], loss=62.3565
	step [109/196], loss=71.1431
	step [110/196], loss=77.0771
	step [111/196], loss=77.3640
	step [112/196], loss=86.5744
	step [113/196], loss=85.6869
	step [114/196], loss=74.8296
	step [115/196], loss=66.3408
	step [116/196], loss=68.8973
	step [117/196], loss=73.1823
	step [118/196], loss=72.3711
	step [119/196], loss=90.7710
	step [120/196], loss=84.1702
	step [121/196], loss=79.9865
	step [122/196], loss=78.8844
	step [123/196], loss=87.8402
	step [124/196], loss=67.8659
	step [125/196], loss=92.9993
	step [126/196], loss=71.7832
	step [127/196], loss=72.3302
	step [128/196], loss=75.0274
	step [129/196], loss=65.1445
	step [130/196], loss=74.3073
	step [131/196], loss=69.2660
	step [132/196], loss=77.1559
	step [133/196], loss=68.8154
	step [134/196], loss=78.3037
	step [135/196], loss=76.3068
	step [136/196], loss=76.2630
	step [137/196], loss=76.3787
	step [138/196], loss=84.5413
	step [139/196], loss=86.8736
	step [140/196], loss=88.2884
	step [141/196], loss=75.7428
	step [142/196], loss=63.5751
	step [143/196], loss=68.1381
	step [144/196], loss=78.5432
	step [145/196], loss=75.5147
	step [146/196], loss=70.8024
	step [147/196], loss=71.8733
	step [148/196], loss=76.0437
	step [149/196], loss=72.9621
	step [150/196], loss=61.3683
	step [151/196], loss=71.6503
	step [152/196], loss=71.6536
	step [153/196], loss=72.8213
	step [154/196], loss=89.1991
	step [155/196], loss=71.8779
	step [156/196], loss=61.5954
	step [157/196], loss=67.3241
	step [158/196], loss=80.1997
	step [159/196], loss=66.6965
	step [160/196], loss=74.2477
	step [161/196], loss=65.8604
	step [162/196], loss=78.4269
	step [163/196], loss=62.3866
	step [164/196], loss=90.3478
	step [165/196], loss=85.6675
	step [166/196], loss=80.1728
	step [167/196], loss=65.7655
	step [168/196], loss=70.8153
	step [169/196], loss=79.8632
	step [170/196], loss=72.4570
	step [171/196], loss=73.2707
	step [172/196], loss=74.1837
	step [173/196], loss=76.4983
	step [174/196], loss=96.0468
	step [175/196], loss=88.1326
	step [176/196], loss=79.7164
	step [177/196], loss=75.9218
	step [178/196], loss=71.5841
	step [179/196], loss=85.9900
	step [180/196], loss=76.6809
	step [181/196], loss=61.4537
	step [182/196], loss=72.5732
	step [183/196], loss=78.9173
	step [184/196], loss=79.6137
	step [185/196], loss=68.0208
	step [186/196], loss=79.9436
	step [187/196], loss=67.1056
	step [188/196], loss=66.1715
	step [189/196], loss=86.2113
	step [190/196], loss=77.1000
	step [191/196], loss=74.1866
	step [192/196], loss=75.8083
	step [193/196], loss=77.1090
	step [194/196], loss=92.6660
	step [195/196], loss=64.7789
	step [196/196], loss=6.2056
	Evaluating
	loss=0.0072, precision=0.4164, recall=0.8516, f1=0.5593
Training epoch 62
	step [1/196], loss=88.8280
	step [2/196], loss=78.4650
	step [3/196], loss=93.2074
	step [4/196], loss=66.9499
	step [5/196], loss=71.9035
	step [6/196], loss=82.0010
	step [7/196], loss=66.2472
	step [8/196], loss=76.2818
	step [9/196], loss=73.7595
	step [10/196], loss=64.5786
	step [11/196], loss=77.1227
	step [12/196], loss=71.2672
	step [13/196], loss=83.9148
	step [14/196], loss=69.5491
	step [15/196], loss=77.2790
	step [16/196], loss=71.2691
	step [17/196], loss=76.1685
	step [18/196], loss=70.0504
	step [19/196], loss=79.3738
	step [20/196], loss=65.8795
	step [21/196], loss=91.4520
	step [22/196], loss=78.1660
	step [23/196], loss=78.5201
	step [24/196], loss=87.8874
	step [25/196], loss=61.3706
	step [26/196], loss=85.3632
	step [27/196], loss=78.5969
	step [28/196], loss=83.7477
	step [29/196], loss=74.3745
	step [30/196], loss=83.7845
	step [31/196], loss=72.2319
	step [32/196], loss=80.1223
	step [33/196], loss=79.8194
	step [34/196], loss=71.9102
	step [35/196], loss=80.9053
	step [36/196], loss=72.6114
	step [37/196], loss=84.9389
	step [38/196], loss=80.5185
	step [39/196], loss=82.1933
	step [40/196], loss=66.4935
	step [41/196], loss=68.1406
	step [42/196], loss=73.6989
	step [43/196], loss=78.1337
	step [44/196], loss=75.3036
	step [45/196], loss=68.4779
	step [46/196], loss=64.6351
	step [47/196], loss=77.3057
	step [48/196], loss=82.3741
	step [49/196], loss=67.3430
	step [50/196], loss=81.9855
	step [51/196], loss=73.4281
	step [52/196], loss=66.0107
	step [53/196], loss=78.3069
	step [54/196], loss=76.9615
	step [55/196], loss=71.0400
	step [56/196], loss=80.4027
	step [57/196], loss=75.4882
	step [58/196], loss=78.4612
	step [59/196], loss=76.7785
	step [60/196], loss=83.6745
	step [61/196], loss=65.5622
	step [62/196], loss=84.2652
	step [63/196], loss=73.5571
	step [64/196], loss=72.8537
	step [65/196], loss=66.3428
	step [66/196], loss=80.1149
	step [67/196], loss=81.4341
	step [68/196], loss=66.6985
	step [69/196], loss=73.6603
	step [70/196], loss=77.3547
	step [71/196], loss=94.7135
	step [72/196], loss=75.9350
	step [73/196], loss=71.7983
	step [74/196], loss=73.8591
	step [75/196], loss=73.9085
	step [76/196], loss=66.0649
	step [77/196], loss=76.8494
	step [78/196], loss=68.5580
	step [79/196], loss=83.5035
	step [80/196], loss=75.6209
	step [81/196], loss=77.9189
	step [82/196], loss=70.5730
	step [83/196], loss=81.2170
	step [84/196], loss=85.3473
	step [85/196], loss=76.7377
	step [86/196], loss=83.2888
	step [87/196], loss=68.7996
	step [88/196], loss=80.6793
	step [89/196], loss=81.8085
	step [90/196], loss=77.1760
	step [91/196], loss=77.0414
	step [92/196], loss=85.2960
	step [93/196], loss=92.2249
	step [94/196], loss=68.1382
	step [95/196], loss=67.8883
	step [96/196], loss=83.2749
	step [97/196], loss=65.3945
	step [98/196], loss=80.7454
	step [99/196], loss=73.6230
	step [100/196], loss=77.0138
	step [101/196], loss=74.5815
	step [102/196], loss=83.9475
	step [103/196], loss=73.4789
	step [104/196], loss=79.6274
	step [105/196], loss=73.2872
	step [106/196], loss=84.5982
	step [107/196], loss=72.1528
	step [108/196], loss=73.0399
	step [109/196], loss=66.7076
	step [110/196], loss=76.2471
	step [111/196], loss=74.4647
	step [112/196], loss=64.3742
	step [113/196], loss=70.4527
	step [114/196], loss=67.8771
	step [115/196], loss=72.6691
	step [116/196], loss=78.5516
	step [117/196], loss=77.4275
	step [118/196], loss=76.8038
	step [119/196], loss=74.7369
	step [120/196], loss=76.4060
	step [121/196], loss=74.8124
	step [122/196], loss=78.0979
	step [123/196], loss=81.0354
	step [124/196], loss=61.0058
	step [125/196], loss=74.5157
	step [126/196], loss=85.5327
	step [127/196], loss=93.4920
	step [128/196], loss=65.3377
	step [129/196], loss=72.6904
	step [130/196], loss=86.0571
	step [131/196], loss=86.9796
	step [132/196], loss=74.8099
	step [133/196], loss=70.5024
	step [134/196], loss=57.3031
	step [135/196], loss=72.0817
	step [136/196], loss=87.0439
	step [137/196], loss=65.5854
	step [138/196], loss=67.4493
	step [139/196], loss=86.1519
	step [140/196], loss=75.0547
	step [141/196], loss=72.1770
	step [142/196], loss=69.5022
	step [143/196], loss=77.3073
	step [144/196], loss=72.4767
	step [145/196], loss=92.2093
	step [146/196], loss=78.1717
	step [147/196], loss=76.1409
	step [148/196], loss=80.6862
	step [149/196], loss=71.8943
	step [150/196], loss=87.5549
	step [151/196], loss=80.7524
	step [152/196], loss=82.4126
	step [153/196], loss=70.6689
	step [154/196], loss=81.4764
	step [155/196], loss=82.5476
	step [156/196], loss=66.7191
	step [157/196], loss=73.5309
	step [158/196], loss=74.9354
	step [159/196], loss=66.7178
	step [160/196], loss=71.0444
	step [161/196], loss=73.6519
	step [162/196], loss=72.0528
	step [163/196], loss=72.7103
	step [164/196], loss=90.5866
	step [165/196], loss=69.2607
	step [166/196], loss=78.9306
	step [167/196], loss=72.5203
	step [168/196], loss=86.2684
	step [169/196], loss=76.8507
	step [170/196], loss=82.1863
	step [171/196], loss=71.2445
	step [172/196], loss=72.4106
	step [173/196], loss=81.9205
	step [174/196], loss=86.1809
	step [175/196], loss=72.1919
	step [176/196], loss=83.4133
	step [177/196], loss=69.7689
	step [178/196], loss=74.1792
	step [179/196], loss=65.7935
	step [180/196], loss=63.6936
	step [181/196], loss=78.0599
	step [182/196], loss=89.8240
	step [183/196], loss=75.6981
	step [184/196], loss=70.7513
	step [185/196], loss=63.0830
	step [186/196], loss=78.9001
	step [187/196], loss=65.1132
	step [188/196], loss=78.4576
	step [189/196], loss=70.6372
	step [190/196], loss=68.8284
	step [191/196], loss=71.4512
	step [192/196], loss=69.5649
	step [193/196], loss=82.8242
	step [194/196], loss=81.2664
	step [195/196], loss=66.1848
	step [196/196], loss=5.7502
	Evaluating
	loss=0.0072, precision=0.4379, recall=0.8510, f1=0.5783
Training epoch 63
	step [1/196], loss=75.5785
	step [2/196], loss=82.4424
	step [3/196], loss=82.6512
	step [4/196], loss=93.6198
	step [5/196], loss=62.7440
	step [6/196], loss=61.8191
	step [7/196], loss=73.6021
	step [8/196], loss=73.1681
	step [9/196], loss=77.7809
	step [10/196], loss=77.9910
	step [11/196], loss=76.1107
	step [12/196], loss=74.1412
	step [13/196], loss=71.8287
	step [14/196], loss=85.3394
	step [15/196], loss=74.0776
	step [16/196], loss=65.1168
	step [17/196], loss=70.5465
	step [18/196], loss=79.1686
	step [19/196], loss=78.9800
	step [20/196], loss=65.5548
	step [21/196], loss=69.6809
	step [22/196], loss=86.6820
	step [23/196], loss=71.4400
	step [24/196], loss=82.7878
	step [25/196], loss=63.6063
	step [26/196], loss=86.1529
	step [27/196], loss=83.2462
	step [28/196], loss=78.8170
	step [29/196], loss=67.5001
	step [30/196], loss=77.3206
	step [31/196], loss=54.7889
	step [32/196], loss=71.0993
	step [33/196], loss=85.5446
	step [34/196], loss=72.1705
	step [35/196], loss=75.5715
	step [36/196], loss=60.9125
	step [37/196], loss=74.8180
	step [38/196], loss=75.7719
	step [39/196], loss=77.8194
	step [40/196], loss=52.6700
	step [41/196], loss=84.7946
	step [42/196], loss=81.6048
	step [43/196], loss=67.6555
	step [44/196], loss=76.0595
	step [45/196], loss=67.4240
	step [46/196], loss=95.0829
	step [47/196], loss=78.9854
	step [48/196], loss=87.3414
	step [49/196], loss=72.9664
	step [50/196], loss=63.9198
	step [51/196], loss=84.1573
	step [52/196], loss=78.8013
	step [53/196], loss=90.9408
	step [54/196], loss=78.1176
	step [55/196], loss=62.5714
	step [56/196], loss=88.1030
	step [57/196], loss=68.4440
	step [58/196], loss=78.5062
	step [59/196], loss=86.6642
	step [60/196], loss=81.1497
	step [61/196], loss=66.4041
	step [62/196], loss=69.2980
	step [63/196], loss=61.2661
	step [64/196], loss=84.8152
	step [65/196], loss=77.5355
	step [66/196], loss=81.1236
	step [67/196], loss=75.2107
	step [68/196], loss=69.1493
	step [69/196], loss=82.9487
	step [70/196], loss=70.1257
	step [71/196], loss=58.5887
	step [72/196], loss=93.9002
	step [73/196], loss=77.2370
	step [74/196], loss=70.8653
	step [75/196], loss=60.0523
	step [76/196], loss=63.0520
	step [77/196], loss=73.1569
	step [78/196], loss=66.3481
	step [79/196], loss=68.2289
	step [80/196], loss=87.1409
	step [81/196], loss=75.3251
	step [82/196], loss=89.5600
	step [83/196], loss=82.8479
	step [84/196], loss=82.9503
	step [85/196], loss=81.7750
	step [86/196], loss=68.2365
	step [87/196], loss=66.4487
	step [88/196], loss=77.8086
	step [89/196], loss=77.0737
	step [90/196], loss=79.7618
	step [91/196], loss=82.0457
	step [92/196], loss=70.1542
	step [93/196], loss=72.9887
	step [94/196], loss=82.3269
	step [95/196], loss=85.9468
	step [96/196], loss=81.9332
	step [97/196], loss=89.2803
	step [98/196], loss=81.2037
	step [99/196], loss=73.8630
	step [100/196], loss=71.8359
	step [101/196], loss=76.5339
	step [102/196], loss=63.5871
	step [103/196], loss=79.5312
	step [104/196], loss=80.3773
	step [105/196], loss=67.4247
	step [106/196], loss=63.9864
	step [107/196], loss=81.1409
	step [108/196], loss=62.9664
	step [109/196], loss=71.3163
	step [110/196], loss=67.7296
	step [111/196], loss=93.2626
	step [112/196], loss=71.3966
	step [113/196], loss=81.8494
	step [114/196], loss=63.4861
	step [115/196], loss=64.9777
	step [116/196], loss=80.1806
	step [117/196], loss=78.1288
	step [118/196], loss=79.7234
	step [119/196], loss=95.1299
	step [120/196], loss=74.5957
	step [121/196], loss=75.0608
	step [122/196], loss=84.8323
	step [123/196], loss=72.8210
	step [124/196], loss=73.6883
	step [125/196], loss=89.4582
	step [126/196], loss=55.1226
	step [127/196], loss=80.2344
	step [128/196], loss=78.4205
	step [129/196], loss=60.1117
	step [130/196], loss=86.3086
	step [131/196], loss=71.3159
	step [132/196], loss=80.0694
	step [133/196], loss=78.6873
	step [134/196], loss=78.3392
	step [135/196], loss=85.5303
	step [136/196], loss=73.6029
	step [137/196], loss=69.1969
	step [138/196], loss=79.6021
	step [139/196], loss=105.2485
	step [140/196], loss=82.5249
	step [141/196], loss=82.9174
	step [142/196], loss=69.4126
	step [143/196], loss=82.3907
	step [144/196], loss=72.0970
	step [145/196], loss=67.1137
	step [146/196], loss=70.7798
	step [147/196], loss=72.1414
	step [148/196], loss=62.2088
	step [149/196], loss=73.7896
	step [150/196], loss=80.8857
	step [151/196], loss=70.7405
	step [152/196], loss=68.6605
	step [153/196], loss=93.3253
	step [154/196], loss=71.3690
	step [155/196], loss=61.5245
	step [156/196], loss=88.5544
	step [157/196], loss=81.1719
	step [158/196], loss=73.1887
	step [159/196], loss=71.0031
	step [160/196], loss=91.0781
	step [161/196], loss=79.2924
	step [162/196], loss=83.3906
	step [163/196], loss=76.7532
	step [164/196], loss=82.3813
	step [165/196], loss=79.8901
	step [166/196], loss=90.1937
	step [167/196], loss=73.1708
	step [168/196], loss=81.1319
	step [169/196], loss=73.7938
	step [170/196], loss=77.0568
	step [171/196], loss=85.7339
	step [172/196], loss=73.5107
	step [173/196], loss=64.7187
	step [174/196], loss=75.9711
	step [175/196], loss=61.4880
	step [176/196], loss=66.1488
	step [177/196], loss=74.9194
	step [178/196], loss=83.5703
	step [179/196], loss=80.3279
	step [180/196], loss=70.9271
	step [181/196], loss=67.4096
	step [182/196], loss=73.6725
	step [183/196], loss=77.8005
	step [184/196], loss=86.7322
	step [185/196], loss=79.5637
	step [186/196], loss=69.6969
	step [187/196], loss=79.3416
	step [188/196], loss=77.2398
	step [189/196], loss=71.9413
	step [190/196], loss=78.2981
	step [191/196], loss=77.1463
	step [192/196], loss=81.4019
	step [193/196], loss=92.2041
	step [194/196], loss=63.3621
	step [195/196], loss=70.6315
	step [196/196], loss=19.4319
	Evaluating
	loss=0.0095, precision=0.3255, recall=0.7061, f1=0.4455
Training epoch 64
	step [1/196], loss=66.3452
	step [2/196], loss=75.1518
	step [3/196], loss=82.3179
	step [4/196], loss=81.5144
	step [5/196], loss=66.9715
	step [6/196], loss=71.8245
	step [7/196], loss=65.8595
	step [8/196], loss=76.6507
	step [9/196], loss=74.3542
	step [10/196], loss=69.1206
	step [11/196], loss=94.1893
	step [12/196], loss=70.0977
	step [13/196], loss=72.4753
	step [14/196], loss=77.9700
	step [15/196], loss=80.0311
	step [16/196], loss=75.9044
	step [17/196], loss=67.3330
	step [18/196], loss=67.1066
	step [19/196], loss=72.5556
	step [20/196], loss=74.6183
	step [21/196], loss=86.1280
	step [22/196], loss=82.8462
	step [23/196], loss=79.8746
	step [24/196], loss=77.1374
	step [25/196], loss=81.0577
	step [26/196], loss=88.8931
	step [27/196], loss=88.3397
	step [28/196], loss=76.8061
	step [29/196], loss=62.2445
	step [30/196], loss=74.2012
	step [31/196], loss=52.9228
	step [32/196], loss=96.5882
	step [33/196], loss=88.3558
	step [34/196], loss=70.1927
	step [35/196], loss=76.0789
	step [36/196], loss=79.5356
	step [37/196], loss=80.4598
	step [38/196], loss=79.9360
	step [39/196], loss=61.8385
	step [40/196], loss=83.3691
	step [41/196], loss=75.1826
	step [42/196], loss=65.8318
	step [43/196], loss=64.8688
	step [44/196], loss=87.5259
	step [45/196], loss=82.1370
	step [46/196], loss=71.1803
	step [47/196], loss=73.2639
	step [48/196], loss=70.0482
	step [49/196], loss=81.2770
	step [50/196], loss=63.9198
	step [51/196], loss=88.6217
	step [52/196], loss=88.0520
	step [53/196], loss=75.4615
	step [54/196], loss=61.9361
	step [55/196], loss=84.6479
	step [56/196], loss=72.4571
	step [57/196], loss=75.8858
	step [58/196], loss=67.2720
	step [59/196], loss=74.9251
	step [60/196], loss=78.8534
	step [61/196], loss=88.6093
	step [62/196], loss=84.3688
	step [63/196], loss=74.9455
	step [64/196], loss=73.1722
	step [65/196], loss=74.8655
	step [66/196], loss=82.8633
	step [67/196], loss=66.7667
	step [68/196], loss=71.0083
	step [69/196], loss=76.3418
	step [70/196], loss=75.2396
	step [71/196], loss=70.6225
	step [72/196], loss=64.6286
	step [73/196], loss=70.1864
	step [74/196], loss=83.5335
	step [75/196], loss=85.1742
	step [76/196], loss=69.4387
	step [77/196], loss=64.6406
	step [78/196], loss=77.8560
	step [79/196], loss=72.5174
	step [80/196], loss=69.4003
	step [81/196], loss=64.3565
	step [82/196], loss=90.9410
	step [83/196], loss=71.3225
	step [84/196], loss=64.4341
	step [85/196], loss=75.5214
	step [86/196], loss=83.5079
	step [87/196], loss=63.6865
	step [88/196], loss=77.9885
	step [89/196], loss=66.8853
	step [90/196], loss=80.5919
	step [91/196], loss=69.7485
	step [92/196], loss=76.6191
	step [93/196], loss=66.4464
	step [94/196], loss=71.2664
	step [95/196], loss=70.3254
	step [96/196], loss=66.6330
	step [97/196], loss=78.3487
	step [98/196], loss=75.1079
	step [99/196], loss=76.1512
	step [100/196], loss=74.7062
	step [101/196], loss=80.3494
	step [102/196], loss=74.5085
	step [103/196], loss=84.1438
	step [104/196], loss=78.0545
	step [105/196], loss=69.6272
	step [106/196], loss=65.9091
	step [107/196], loss=71.1826
	step [108/196], loss=68.5851
	step [109/196], loss=86.3938
	step [110/196], loss=68.6685
	step [111/196], loss=66.5934
	step [112/196], loss=85.0462
	step [113/196], loss=73.8713
	step [114/196], loss=78.3053
	step [115/196], loss=80.4584
	step [116/196], loss=86.8013
	step [117/196], loss=74.4509
	step [118/196], loss=75.8512
	step [119/196], loss=83.9988
	step [120/196], loss=77.0592
	step [121/196], loss=73.9481
	step [122/196], loss=83.4512
	step [123/196], loss=88.2032
	step [124/196], loss=83.3964
	step [125/196], loss=70.4212
	step [126/196], loss=69.3993
	step [127/196], loss=80.8494
	step [128/196], loss=92.1295
	step [129/196], loss=75.7799
	step [130/196], loss=82.2146
	step [131/196], loss=82.0058
	step [132/196], loss=75.4174
	step [133/196], loss=86.9604
	step [134/196], loss=71.4231
	step [135/196], loss=73.5501
	step [136/196], loss=78.9443
	step [137/196], loss=56.4905
	step [138/196], loss=67.8065
	step [139/196], loss=74.4576
	step [140/196], loss=69.8521
	step [141/196], loss=69.2835
	step [142/196], loss=74.3704
	step [143/196], loss=73.4388
	step [144/196], loss=73.1210
	step [145/196], loss=74.5995
	step [146/196], loss=73.9860
	step [147/196], loss=60.4817
	step [148/196], loss=81.3334
	step [149/196], loss=84.6463
	step [150/196], loss=75.4288
	step [151/196], loss=66.4226
	step [152/196], loss=70.8871
	step [153/196], loss=76.0893
	step [154/196], loss=73.8930
	step [155/196], loss=68.5636
	step [156/196], loss=75.3168
	step [157/196], loss=65.3622
	step [158/196], loss=65.2592
	step [159/196], loss=67.1475
	step [160/196], loss=76.0654
	step [161/196], loss=82.0283
	step [162/196], loss=81.3601
	step [163/196], loss=79.6375
	step [164/196], loss=76.7210
	step [165/196], loss=63.4639
	step [166/196], loss=68.0477
	step [167/196], loss=94.5799
	step [168/196], loss=65.8505
	step [169/196], loss=78.9057
	step [170/196], loss=78.7264
	step [171/196], loss=73.5349
	step [172/196], loss=77.9038
	step [173/196], loss=74.2848
	step [174/196], loss=77.4108
	step [175/196], loss=56.3698
	step [176/196], loss=85.9880
	step [177/196], loss=68.3202
	step [178/196], loss=76.3125
	step [179/196], loss=83.5139
	step [180/196], loss=79.3538
	step [181/196], loss=66.3068
	step [182/196], loss=88.0881
	step [183/196], loss=64.3799
	step [184/196], loss=71.9821
	step [185/196], loss=88.8497
	step [186/196], loss=71.0803
	step [187/196], loss=79.7645
	step [188/196], loss=87.1178
	step [189/196], loss=76.8297
	step [190/196], loss=72.0601
	step [191/196], loss=67.1487
	step [192/196], loss=72.4670
	step [193/196], loss=72.5384
	step [194/196], loss=77.1102
	step [195/196], loss=73.9557
	step [196/196], loss=3.9804
	Evaluating
	loss=0.0079, precision=0.3883, recall=0.8487, f1=0.5328
Training epoch 65
	step [1/196], loss=75.5772
	step [2/196], loss=63.0447
	step [3/196], loss=69.0851
	step [4/196], loss=75.3384
	step [5/196], loss=76.7875
	step [6/196], loss=79.3349
	step [7/196], loss=56.3861
	step [8/196], loss=74.2247
	step [9/196], loss=80.2957
	step [10/196], loss=84.6113
	step [11/196], loss=66.8797
	step [12/196], loss=64.6040
	step [13/196], loss=69.8164
	step [14/196], loss=67.9644
	step [15/196], loss=86.5088
	step [16/196], loss=78.8213
	step [17/196], loss=68.2306
	step [18/196], loss=60.1460
	step [19/196], loss=75.8306
	step [20/196], loss=72.7495
	step [21/196], loss=85.1919
	step [22/196], loss=83.8723
	step [23/196], loss=73.3152
	step [24/196], loss=86.2583
	step [25/196], loss=75.9225
	step [26/196], loss=77.0325
	step [27/196], loss=83.2028
	step [28/196], loss=73.1871
	step [29/196], loss=72.7787
	step [30/196], loss=65.3274
	step [31/196], loss=76.2113
	step [32/196], loss=66.9167
	step [33/196], loss=76.2557
	step [34/196], loss=83.4947
	step [35/196], loss=73.2786
	step [36/196], loss=89.3740
	step [37/196], loss=76.8457
	step [38/196], loss=82.6179
	step [39/196], loss=69.4257
	step [40/196], loss=77.6288
	step [41/196], loss=86.6644
	step [42/196], loss=81.4324
	step [43/196], loss=79.6639
	step [44/196], loss=83.1556
	step [45/196], loss=81.4303
	step [46/196], loss=68.9741
	step [47/196], loss=80.8864
	step [48/196], loss=55.8986
	step [49/196], loss=83.8703
	step [50/196], loss=80.9951
	step [51/196], loss=81.9334
	step [52/196], loss=68.8653
	step [53/196], loss=58.9754
	step [54/196], loss=78.6748
	step [55/196], loss=66.9720
	step [56/196], loss=86.8792
	step [57/196], loss=75.0664
	step [58/196], loss=88.2603
	step [59/196], loss=85.5633
	step [60/196], loss=85.0901
	step [61/196], loss=74.1907
	step [62/196], loss=74.9250
	step [63/196], loss=77.4918
	step [64/196], loss=69.3338
	step [65/196], loss=79.6019
	step [66/196], loss=82.0305
	step [67/196], loss=72.3555
	step [68/196], loss=80.5177
	step [69/196], loss=74.8202
	step [70/196], loss=74.3871
	step [71/196], loss=74.2460
	step [72/196], loss=65.2759
	step [73/196], loss=72.8003
	step [74/196], loss=76.3549
	step [75/196], loss=67.7499
	step [76/196], loss=73.0746
	step [77/196], loss=76.0346
	step [78/196], loss=78.8269
	step [79/196], loss=70.0474
	step [80/196], loss=83.9651
	step [81/196], loss=71.1218
	step [82/196], loss=62.9311
	step [83/196], loss=60.5818
	step [84/196], loss=76.7821
	step [85/196], loss=71.9888
	step [86/196], loss=66.9809
	step [87/196], loss=59.5852
	step [88/196], loss=80.0815
	step [89/196], loss=90.2351
	step [90/196], loss=68.7457
	step [91/196], loss=77.8984
	step [92/196], loss=85.3670
	step [93/196], loss=75.1877
	step [94/196], loss=70.8166
	step [95/196], loss=63.6899
	step [96/196], loss=79.4619
	step [97/196], loss=70.8482
	step [98/196], loss=67.4530
	step [99/196], loss=78.7462
	step [100/196], loss=81.4545
	step [101/196], loss=71.8842
	step [102/196], loss=71.4529
	step [103/196], loss=73.4804
	step [104/196], loss=78.2949
	step [105/196], loss=63.7928
	step [106/196], loss=75.3220
	step [107/196], loss=79.3600
	step [108/196], loss=75.9658
	step [109/196], loss=79.0713
	step [110/196], loss=82.2510
	step [111/196], loss=78.1650
	step [112/196], loss=71.0809
	step [113/196], loss=75.2257
	step [114/196], loss=70.0159
	step [115/196], loss=76.0378
	step [116/196], loss=82.4419
	step [117/196], loss=87.3579
	step [118/196], loss=79.7304
	step [119/196], loss=70.2813
	step [120/196], loss=71.7167
	step [121/196], loss=90.5219
	step [122/196], loss=80.1069
	step [123/196], loss=68.3923
	step [124/196], loss=82.4417
	step [125/196], loss=50.8266
	step [126/196], loss=63.5946
	step [127/196], loss=81.9868
	step [128/196], loss=69.9142
	step [129/196], loss=78.6733
	step [130/196], loss=68.2864
	step [131/196], loss=75.9780
	step [132/196], loss=64.9698
	step [133/196], loss=71.4947
	step [134/196], loss=61.8161
	step [135/196], loss=77.4100
	step [136/196], loss=90.1675
	step [137/196], loss=65.3461
	step [138/196], loss=62.7881
	step [139/196], loss=92.9070
	step [140/196], loss=79.2775
	step [141/196], loss=87.4565
	step [142/196], loss=83.9247
	step [143/196], loss=79.3514
	step [144/196], loss=70.8650
	step [145/196], loss=71.6863
	step [146/196], loss=80.7874
	step [147/196], loss=93.5677
	step [148/196], loss=63.8175
	step [149/196], loss=66.2238
	step [150/196], loss=64.6427
	step [151/196], loss=62.1389
	step [152/196], loss=80.5130
	step [153/196], loss=73.9973
	step [154/196], loss=76.5234
	step [155/196], loss=68.1692
	step [156/196], loss=62.1150
	step [157/196], loss=81.0588
	step [158/196], loss=74.3537
	step [159/196], loss=75.8907
	step [160/196], loss=71.8670
	step [161/196], loss=81.7332
	step [162/196], loss=77.8342
	step [163/196], loss=67.2244
	step [164/196], loss=75.3647
	step [165/196], loss=75.1420
	step [166/196], loss=73.1371
	step [167/196], loss=84.4219
	step [168/196], loss=75.0987
	step [169/196], loss=70.6041
	step [170/196], loss=74.9999
	step [171/196], loss=74.7245
	step [172/196], loss=63.3984
	step [173/196], loss=72.0345
	step [174/196], loss=75.6613
	step [175/196], loss=76.3770
	step [176/196], loss=73.1714
	step [177/196], loss=73.1100
	step [178/196], loss=68.7895
	step [179/196], loss=88.2257
	step [180/196], loss=73.3618
	step [181/196], loss=74.3269
	step [182/196], loss=78.0411
	step [183/196], loss=69.7301
	step [184/196], loss=73.7078
	step [185/196], loss=84.5645
	step [186/196], loss=74.5091
	step [187/196], loss=65.6402
	step [188/196], loss=70.4022
	step [189/196], loss=74.5965
	step [190/196], loss=81.6109
	step [191/196], loss=81.3590
	step [192/196], loss=69.2010
	step [193/196], loss=80.8786
	step [194/196], loss=78.7259
	step [195/196], loss=76.9491
	step [196/196], loss=9.1735
	Evaluating
	loss=0.0075, precision=0.4058, recall=0.8746, f1=0.5544
Training epoch 66
	step [1/196], loss=74.8385
	step [2/196], loss=71.3020
	step [3/196], loss=69.1526
	step [4/196], loss=72.8769
	step [5/196], loss=71.1578
	step [6/196], loss=67.2865
	step [7/196], loss=72.1358
	step [8/196], loss=75.3036
	step [9/196], loss=78.2096
	step [10/196], loss=67.8640
	step [11/196], loss=76.4054
	step [12/196], loss=75.6338
	step [13/196], loss=69.8162
	step [14/196], loss=67.0748
	step [15/196], loss=64.5114
	step [16/196], loss=60.2897
	step [17/196], loss=75.5691
	step [18/196], loss=70.9307
	step [19/196], loss=70.8609
	step [20/196], loss=69.1614
	step [21/196], loss=85.9025
	step [22/196], loss=73.4616
	step [23/196], loss=66.0996
	step [24/196], loss=68.1900
	step [25/196], loss=68.7367
	step [26/196], loss=69.3862
	step [27/196], loss=83.8552
	step [28/196], loss=79.2377
	step [29/196], loss=80.3128
	step [30/196], loss=87.9864
	step [31/196], loss=79.3858
	step [32/196], loss=88.9115
	step [33/196], loss=72.7281
	step [34/196], loss=86.1945
	step [35/196], loss=65.4574
	step [36/196], loss=80.2708
	step [37/196], loss=74.3657
	step [38/196], loss=68.5737
	step [39/196], loss=72.0042
	step [40/196], loss=77.7666
	step [41/196], loss=70.4401
	step [42/196], loss=75.3690
	step [43/196], loss=86.5300
	step [44/196], loss=77.0252
	step [45/196], loss=71.5287
	step [46/196], loss=66.1973
	step [47/196], loss=78.4454
	step [48/196], loss=78.0346
	step [49/196], loss=63.7191
	step [50/196], loss=81.3861
	step [51/196], loss=72.9408
	step [52/196], loss=79.7991
	step [53/196], loss=63.9623
	step [54/196], loss=75.5446
	step [55/196], loss=80.4456
	step [56/196], loss=71.8968
	step [57/196], loss=73.4200
	step [58/196], loss=68.9548
	step [59/196], loss=77.2170
	step [60/196], loss=69.8487
	step [61/196], loss=64.4459
	step [62/196], loss=91.6123
	step [63/196], loss=78.5866
	step [64/196], loss=71.7357
	step [65/196], loss=76.8735
	step [66/196], loss=66.7204
	step [67/196], loss=71.4366
	step [68/196], loss=88.8163
	step [69/196], loss=78.2004
	step [70/196], loss=81.7385
	step [71/196], loss=59.3991
	step [72/196], loss=73.2409
	step [73/196], loss=73.1530
	step [74/196], loss=70.7524
	step [75/196], loss=66.7995
	step [76/196], loss=83.9241
	step [77/196], loss=71.5406
	step [78/196], loss=74.7728
	step [79/196], loss=88.4541
	step [80/196], loss=79.7536
	step [81/196], loss=69.6801
	step [82/196], loss=76.0437
	step [83/196], loss=67.4882
	step [84/196], loss=75.9604
	step [85/196], loss=73.2922
	step [86/196], loss=73.4734
	step [87/196], loss=72.3965
	step [88/196], loss=72.4891
	step [89/196], loss=87.7540
	step [90/196], loss=62.2055
	step [91/196], loss=71.1632
	step [92/196], loss=77.6973
	step [93/196], loss=68.7568
	step [94/196], loss=81.4692
	step [95/196], loss=76.3484
	step [96/196], loss=65.6442
	step [97/196], loss=72.2807
	step [98/196], loss=76.7209
	step [99/196], loss=68.9274
	step [100/196], loss=78.8129
	step [101/196], loss=60.8168
	step [102/196], loss=67.2086
	step [103/196], loss=77.1655
	step [104/196], loss=74.2829
	step [105/196], loss=79.1766
	step [106/196], loss=67.6282
	step [107/196], loss=75.6222
	step [108/196], loss=77.1647
	step [109/196], loss=78.1435
	step [110/196], loss=90.5638
	step [111/196], loss=64.1096
	step [112/196], loss=85.3210
	step [113/196], loss=74.5307
	step [114/196], loss=75.0093
	step [115/196], loss=61.2958
	step [116/196], loss=81.9778
	step [117/196], loss=72.3155
	step [118/196], loss=70.9690
	step [119/196], loss=75.6270
	step [120/196], loss=68.7126
	step [121/196], loss=78.9409
	step [122/196], loss=85.8151
	step [123/196], loss=87.9943
	step [124/196], loss=63.5136
	step [125/196], loss=95.2792
	step [126/196], loss=69.3059
	step [127/196], loss=82.9264
	step [128/196], loss=76.5823
	step [129/196], loss=69.3916
	step [130/196], loss=73.0050
	step [131/196], loss=70.1969
	step [132/196], loss=74.3912
	step [133/196], loss=62.9330
	step [134/196], loss=78.6371
	step [135/196], loss=71.3871
	step [136/196], loss=73.8746
	step [137/196], loss=68.4041
	step [138/196], loss=85.8390
	step [139/196], loss=77.8280
	step [140/196], loss=73.1247
	step [141/196], loss=79.1200
	step [142/196], loss=76.5598
	step [143/196], loss=74.0332
	step [144/196], loss=77.2859
	step [145/196], loss=80.7357
	step [146/196], loss=76.2943
	step [147/196], loss=74.2064
	step [148/196], loss=68.8700
	step [149/196], loss=80.7326
	step [150/196], loss=77.1603
	step [151/196], loss=73.4463
	step [152/196], loss=80.6941
	step [153/196], loss=76.7129
	step [154/196], loss=68.1746
	step [155/196], loss=88.0996
	step [156/196], loss=70.5710
	step [157/196], loss=90.1068
	step [158/196], loss=61.8633
	step [159/196], loss=81.7753
	step [160/196], loss=66.6535
	step [161/196], loss=91.7640
	step [162/196], loss=67.2991
	step [163/196], loss=70.5305
	step [164/196], loss=72.6000
	step [165/196], loss=79.4508
	step [166/196], loss=75.5828
	step [167/196], loss=77.5485
	step [168/196], loss=77.7107
	step [169/196], loss=83.6242
	step [170/196], loss=88.8831
	step [171/196], loss=80.4152
	step [172/196], loss=81.7861
	step [173/196], loss=78.6402
	step [174/196], loss=78.3829
	step [175/196], loss=75.0769
	step [176/196], loss=80.5947
	step [177/196], loss=74.5170
	step [178/196], loss=83.1822
	step [179/196], loss=74.4419
	step [180/196], loss=75.7229
	step [181/196], loss=68.4933
	step [182/196], loss=62.2035
	step [183/196], loss=74.7960
	step [184/196], loss=58.6017
	step [185/196], loss=79.8359
	step [186/196], loss=61.6839
	step [187/196], loss=75.8949
	step [188/196], loss=72.7080
	step [189/196], loss=60.1749
	step [190/196], loss=80.7903
	step [191/196], loss=62.9050
	step [192/196], loss=81.7994
	step [193/196], loss=75.6522
	step [194/196], loss=68.3562
	step [195/196], loss=82.3192
	step [196/196], loss=6.3553
	Evaluating
	loss=0.0067, precision=0.4397, recall=0.8530, f1=0.5803
Training epoch 67
	step [1/196], loss=70.6013
	step [2/196], loss=79.9287
	step [3/196], loss=75.2715
	step [4/196], loss=67.4606
	step [5/196], loss=80.8618
	step [6/196], loss=66.6200
	step [7/196], loss=68.9765
	step [8/196], loss=89.7121
	step [9/196], loss=66.9605
	step [10/196], loss=58.6368
	step [11/196], loss=87.2499
	step [12/196], loss=74.8990
	step [13/196], loss=82.4545
	step [14/196], loss=71.7903
	step [15/196], loss=73.3691
	step [16/196], loss=78.0842
	step [17/196], loss=79.2078
	step [18/196], loss=69.5582
	step [19/196], loss=61.7526
	step [20/196], loss=89.1673
	step [21/196], loss=87.4959
	step [22/196], loss=82.9164
	step [23/196], loss=81.2343
	step [24/196], loss=74.2654
	step [25/196], loss=62.0508
	step [26/196], loss=61.7397
	step [27/196], loss=89.3044
	step [28/196], loss=81.9320
	step [29/196], loss=71.2669
	step [30/196], loss=76.7673
	step [31/196], loss=75.6982
	step [32/196], loss=76.3703
	step [33/196], loss=67.2704
	step [34/196], loss=77.8714
	step [35/196], loss=76.1268
	step [36/196], loss=79.6393
	step [37/196], loss=69.6692
	step [38/196], loss=70.0414
	step [39/196], loss=83.4802
	step [40/196], loss=84.7230
	step [41/196], loss=85.4700
	step [42/196], loss=75.7507
	step [43/196], loss=60.6088
	step [44/196], loss=92.5333
	step [45/196], loss=69.2287
	step [46/196], loss=77.6992
	step [47/196], loss=70.8675
	step [48/196], loss=66.3638
	step [49/196], loss=67.8353
	step [50/196], loss=91.9376
	step [51/196], loss=73.9351
	step [52/196], loss=69.7725
	step [53/196], loss=79.2917
	step [54/196], loss=66.1797
	step [55/196], loss=69.2655
	step [56/196], loss=78.0223
	step [57/196], loss=74.9563
	step [58/196], loss=77.8701
	step [59/196], loss=88.9519
	step [60/196], loss=78.3949
	step [61/196], loss=81.3570
	step [62/196], loss=79.9871
	step [63/196], loss=81.7441
	step [64/196], loss=74.9056
	step [65/196], loss=99.2125
	step [66/196], loss=74.4648
	step [67/196], loss=62.6362
	step [68/196], loss=82.9753
	step [69/196], loss=76.1035
	step [70/196], loss=72.3068
	step [71/196], loss=67.6081
	step [72/196], loss=76.1607
	step [73/196], loss=76.6866
	step [74/196], loss=65.1402
	step [75/196], loss=80.7279
	step [76/196], loss=66.3833
	step [77/196], loss=66.2227
	step [78/196], loss=64.8547
	step [79/196], loss=85.5976
	step [80/196], loss=73.3743
	step [81/196], loss=90.1757
	step [82/196], loss=59.0054
	step [83/196], loss=70.0102
	step [84/196], loss=78.4290
	step [85/196], loss=79.3700
	step [86/196], loss=81.5959
	step [87/196], loss=66.6420
	step [88/196], loss=72.8936
	step [89/196], loss=80.7959
	step [90/196], loss=67.0660
	step [91/196], loss=87.9840
	step [92/196], loss=71.8529
	step [93/196], loss=68.9648
	step [94/196], loss=75.8787
	step [95/196], loss=62.4620
	step [96/196], loss=76.4686
	step [97/196], loss=78.2979
	step [98/196], loss=65.8133
	step [99/196], loss=69.7297
	step [100/196], loss=88.9557
	step [101/196], loss=70.4941
	step [102/196], loss=92.1845
	step [103/196], loss=72.8291
	step [104/196], loss=68.9049
	step [105/196], loss=82.9765
	step [106/196], loss=82.2416
	step [107/196], loss=84.1905
	step [108/196], loss=70.7278
	step [109/196], loss=75.3505
	step [110/196], loss=81.2385
	step [111/196], loss=73.6474
	step [112/196], loss=66.7867
	step [113/196], loss=80.2991
	step [114/196], loss=78.5318
	step [115/196], loss=89.9387
	step [116/196], loss=85.1558
	step [117/196], loss=72.5202
	step [118/196], loss=70.0217
	step [119/196], loss=76.1961
	step [120/196], loss=91.5946
	step [121/196], loss=63.6592
	step [122/196], loss=73.5504
	step [123/196], loss=77.9989
	step [124/196], loss=71.3336
	step [125/196], loss=73.7297
	step [126/196], loss=66.9024
	step [127/196], loss=78.0257
	step [128/196], loss=73.0990
	step [129/196], loss=67.2113
	step [130/196], loss=75.7459
	step [131/196], loss=78.2374
	step [132/196], loss=82.0565
	step [133/196], loss=76.6788
	step [134/196], loss=89.1416
	step [135/196], loss=82.6551
	step [136/196], loss=85.9356
	step [137/196], loss=74.4938
	step [138/196], loss=65.5226
	step [139/196], loss=75.2425
	step [140/196], loss=85.7010
	step [141/196], loss=60.6995
	step [142/196], loss=77.5080
	step [143/196], loss=74.5198
	step [144/196], loss=74.5930
	step [145/196], loss=75.2089
	step [146/196], loss=69.8304
	step [147/196], loss=81.6993
	step [148/196], loss=75.3736
	step [149/196], loss=60.9009
	step [150/196], loss=76.6698
	step [151/196], loss=66.8551
	step [152/196], loss=94.3689
	step [153/196], loss=66.4335
	step [154/196], loss=85.9527
	step [155/196], loss=59.3637
	step [156/196], loss=82.1293
	step [157/196], loss=73.9012
	step [158/196], loss=80.1989
	step [159/196], loss=81.1766
	step [160/196], loss=68.4177
	step [161/196], loss=68.8803
	step [162/196], loss=75.4957
	step [163/196], loss=67.5208
	step [164/196], loss=71.7399
	step [165/196], loss=76.1803
	step [166/196], loss=65.6142
	step [167/196], loss=64.6581
	step [168/196], loss=73.5327
	step [169/196], loss=76.9153
	step [170/196], loss=65.9874
	step [171/196], loss=78.6988
	step [172/196], loss=59.0474
	step [173/196], loss=71.4906
	step [174/196], loss=69.5987
	step [175/196], loss=72.2619
	step [176/196], loss=67.8350
	step [177/196], loss=78.7867
	step [178/196], loss=62.2244
	step [179/196], loss=77.8318
	step [180/196], loss=69.0291
	step [181/196], loss=81.1364
	step [182/196], loss=78.3140
	step [183/196], loss=54.7294
	step [184/196], loss=82.9847
	step [185/196], loss=84.9204
	step [186/196], loss=68.0233
	step [187/196], loss=65.6059
	step [188/196], loss=89.1412
	step [189/196], loss=74.3131
	step [190/196], loss=76.3800
	step [191/196], loss=77.1232
	step [192/196], loss=75.6620
	step [193/196], loss=82.8210
	step [194/196], loss=73.0237
	step [195/196], loss=81.1355
	step [196/196], loss=4.9266
	Evaluating
	loss=0.0058, precision=0.5014, recall=0.8495, f1=0.6306
saving model as: 3_saved_model.pth
Training epoch 68
	step [1/196], loss=69.0855
	step [2/196], loss=80.8812
	step [3/196], loss=84.1320
	step [4/196], loss=68.2595
	step [5/196], loss=80.3671
	step [6/196], loss=70.4597
	step [7/196], loss=74.8377
	step [8/196], loss=65.1441
	step [9/196], loss=70.5143
	step [10/196], loss=79.5477
	step [11/196], loss=80.9641
	step [12/196], loss=73.8730
	step [13/196], loss=75.9781
	step [14/196], loss=69.3764
	step [15/196], loss=75.9389
	step [16/196], loss=74.2105
	step [17/196], loss=87.7850
	step [18/196], loss=70.0265
	step [19/196], loss=73.6311
	step [20/196], loss=77.8504
	step [21/196], loss=86.5921
	step [22/196], loss=73.0395
	step [23/196], loss=70.6869
	step [24/196], loss=74.3731
	step [25/196], loss=66.1919
	step [26/196], loss=88.8038
	step [27/196], loss=91.5380
	step [28/196], loss=62.8935
	step [29/196], loss=75.3218
	step [30/196], loss=70.8922
	step [31/196], loss=99.8487
	step [32/196], loss=62.0789
	step [33/196], loss=77.7273
	step [34/196], loss=75.9802
	step [35/196], loss=82.1259
	step [36/196], loss=79.4869
	step [37/196], loss=73.2593
	step [38/196], loss=72.3035
	step [39/196], loss=66.3081
	step [40/196], loss=76.7562
	step [41/196], loss=69.3511
	step [42/196], loss=70.4872
	step [43/196], loss=84.7969
	step [44/196], loss=90.0752
	step [45/196], loss=73.2619
	step [46/196], loss=74.0317
	step [47/196], loss=74.8751
	step [48/196], loss=75.1780
	step [49/196], loss=90.6539
	step [50/196], loss=84.2649
	step [51/196], loss=61.6743
	step [52/196], loss=74.4164
	step [53/196], loss=67.2460
	step [54/196], loss=67.1294
	step [55/196], loss=88.5161
	step [56/196], loss=68.6011
	step [57/196], loss=64.7050
	step [58/196], loss=71.8114
	step [59/196], loss=82.1054
	step [60/196], loss=56.3269
	step [61/196], loss=78.4803
	step [62/196], loss=72.0605
	step [63/196], loss=85.0536
	step [64/196], loss=72.3041
	step [65/196], loss=70.7795
	step [66/196], loss=63.9640
	step [67/196], loss=65.9517
	step [68/196], loss=82.6384
	step [69/196], loss=92.1502
	step [70/196], loss=72.7612
	step [71/196], loss=77.4486
	step [72/196], loss=73.8702
	step [73/196], loss=80.0424
	step [74/196], loss=65.2109
	step [75/196], loss=77.3432
	step [76/196], loss=62.8615
	step [77/196], loss=74.3142
	step [78/196], loss=73.4887
	step [79/196], loss=75.0509
	step [80/196], loss=80.8970
	step [81/196], loss=72.5105
	step [82/196], loss=67.8610
	step [83/196], loss=77.5959
	step [84/196], loss=69.2699
	step [85/196], loss=64.3943
	step [86/196], loss=69.6467
	step [87/196], loss=83.4819
	step [88/196], loss=73.9624
	step [89/196], loss=70.2739
	step [90/196], loss=67.5786
	step [91/196], loss=69.7417
	step [92/196], loss=69.9669
	step [93/196], loss=72.5985
	step [94/196], loss=77.9400
	step [95/196], loss=70.3983
	step [96/196], loss=62.7264
	step [97/196], loss=70.1458
	step [98/196], loss=83.5924
	step [99/196], loss=81.0809
	step [100/196], loss=75.9104
	step [101/196], loss=70.8563
	step [102/196], loss=78.5700
	step [103/196], loss=76.3857
	step [104/196], loss=62.6831
	step [105/196], loss=69.0359
	step [106/196], loss=78.1784
	step [107/196], loss=79.6178
	step [108/196], loss=62.0259
	step [109/196], loss=63.3115
	step [110/196], loss=70.8385
	step [111/196], loss=89.2611
	step [112/196], loss=74.3847
	step [113/196], loss=76.7374
	step [114/196], loss=71.9780
	step [115/196], loss=69.8045
	step [116/196], loss=86.0378
	step [117/196], loss=82.6109
	step [118/196], loss=69.0662
	step [119/196], loss=85.0594
	step [120/196], loss=88.5493
	step [121/196], loss=70.9251
	step [122/196], loss=79.7557
	step [123/196], loss=58.1711
	step [124/196], loss=83.8680
	step [125/196], loss=82.5911
	step [126/196], loss=62.2317
	step [127/196], loss=77.2731
	step [128/196], loss=68.8471
	step [129/196], loss=63.4826
	step [130/196], loss=82.8584
	step [131/196], loss=85.6260
	step [132/196], loss=66.4585
	step [133/196], loss=71.1588
	step [134/196], loss=76.0552
	step [135/196], loss=54.5169
	step [136/196], loss=80.6764
	step [137/196], loss=74.6590
	step [138/196], loss=76.5221
	step [139/196], loss=75.0500
	step [140/196], loss=75.6821
	step [141/196], loss=85.4910
	step [142/196], loss=78.6005
	step [143/196], loss=66.7758
	step [144/196], loss=81.3351
	step [145/196], loss=65.9026
	step [146/196], loss=67.5049
	step [147/196], loss=84.2721
	step [148/196], loss=74.7371
	step [149/196], loss=50.1766
	step [150/196], loss=76.5401
	step [151/196], loss=74.4925
	step [152/196], loss=58.8982
	step [153/196], loss=79.9317
	step [154/196], loss=76.2172
	step [155/196], loss=68.8985
	step [156/196], loss=69.8195
	step [157/196], loss=78.8815
	step [158/196], loss=82.5475
	step [159/196], loss=71.0103
	step [160/196], loss=68.6363
	step [161/196], loss=78.0427
	step [162/196], loss=88.7243
	step [163/196], loss=87.0953
	step [164/196], loss=59.3904
	step [165/196], loss=84.1113
	step [166/196], loss=67.8556
	step [167/196], loss=76.8960
	step [168/196], loss=76.0583
	step [169/196], loss=68.3613
	step [170/196], loss=79.1787
	step [171/196], loss=69.3889
	step [172/196], loss=60.5704
	step [173/196], loss=72.4490
	step [174/196], loss=63.0129
	step [175/196], loss=82.3567
	step [176/196], loss=60.6099
	step [177/196], loss=69.9053
	step [178/196], loss=78.3957
	step [179/196], loss=73.5903
	step [180/196], loss=79.7531
	step [181/196], loss=80.9705
	step [182/196], loss=81.8270
	step [183/196], loss=65.4565
	step [184/196], loss=67.0608
	step [185/196], loss=69.5098
	step [186/196], loss=71.1725
	step [187/196], loss=70.7402
	step [188/196], loss=78.3540
	step [189/196], loss=82.9828
	step [190/196], loss=76.6756
	step [191/196], loss=72.9731
	step [192/196], loss=77.6878
	step [193/196], loss=83.3773
	step [194/196], loss=78.1485
	step [195/196], loss=87.7096
	step [196/196], loss=20.8322
	Evaluating
	loss=0.0066, precision=0.4386, recall=0.8546, f1=0.5797
Training epoch 69
	step [1/196], loss=63.9961
	step [2/196], loss=71.2367
	step [3/196], loss=74.8183
	step [4/196], loss=76.0966
	step [5/196], loss=76.6335
	step [6/196], loss=73.5695
	step [7/196], loss=79.4036
	step [8/196], loss=74.0302
	step [9/196], loss=75.8520
	step [10/196], loss=97.2301
	step [11/196], loss=68.1546
	step [12/196], loss=80.0616
	step [13/196], loss=70.4127
	step [14/196], loss=74.9159
	step [15/196], loss=85.2679
	step [16/196], loss=72.4875
	step [17/196], loss=77.0802
	step [18/196], loss=72.8259
	step [19/196], loss=77.6019
	step [20/196], loss=63.6589
	step [21/196], loss=60.6368
	step [22/196], loss=73.1143
	step [23/196], loss=87.2940
	step [24/196], loss=76.9554
	step [25/196], loss=72.6953
	step [26/196], loss=82.7091
	step [27/196], loss=78.9526
	step [28/196], loss=77.9220
	step [29/196], loss=68.2908
	step [30/196], loss=65.8298
	step [31/196], loss=87.2411
	step [32/196], loss=74.2458
	step [33/196], loss=84.6935
	step [34/196], loss=74.5362
	step [35/196], loss=76.0511
	step [36/196], loss=61.9155
	step [37/196], loss=71.5351
	step [38/196], loss=68.4402
	step [39/196], loss=78.4330
	step [40/196], loss=67.0380
	step [41/196], loss=70.6447
	step [42/196], loss=64.8838
	step [43/196], loss=74.0215
	step [44/196], loss=61.4426
	step [45/196], loss=79.0235
	step [46/196], loss=81.0490
	step [47/196], loss=74.2098
	step [48/196], loss=70.6018
	step [49/196], loss=87.2878
	step [50/196], loss=69.7299
	step [51/196], loss=62.4645
	step [52/196], loss=54.2634
	step [53/196], loss=68.4063
	step [54/196], loss=77.4561
	step [55/196], loss=83.7936
	step [56/196], loss=83.1306
	step [57/196], loss=75.6122
	step [58/196], loss=76.5018
	step [59/196], loss=68.8463
	step [60/196], loss=55.9710
	step [61/196], loss=59.8099
	step [62/196], loss=84.4653
	step [63/196], loss=75.1704
	step [64/196], loss=95.4657
	step [65/196], loss=88.8237
	step [66/196], loss=70.9580
	step [67/196], loss=66.1684
	step [68/196], loss=80.1085
	step [69/196], loss=63.7328
	step [70/196], loss=71.9623
	step [71/196], loss=78.3069
	step [72/196], loss=75.0691
	step [73/196], loss=83.0515
	step [74/196], loss=73.3584
	step [75/196], loss=82.5727
	step [76/196], loss=89.3711
	step [77/196], loss=67.5232
	step [78/196], loss=69.0373
	step [79/196], loss=72.2013
	step [80/196], loss=74.4964
	step [81/196], loss=65.8520
	step [82/196], loss=81.0420
	step [83/196], loss=72.2014
	step [84/196], loss=88.8988
	step [85/196], loss=71.6208
	step [86/196], loss=92.8008
	step [87/196], loss=70.8711
	step [88/196], loss=80.6368
	step [89/196], loss=69.0577
	step [90/196], loss=77.1371
	step [91/196], loss=68.2111
	step [92/196], loss=67.5238
	step [93/196], loss=82.9580
	step [94/196], loss=73.2180
	step [95/196], loss=74.4658
	step [96/196], loss=68.7614
	step [97/196], loss=79.2727
	step [98/196], loss=70.2021
	step [99/196], loss=64.9849
	step [100/196], loss=72.2183
	step [101/196], loss=75.6313
	step [102/196], loss=89.6801
	step [103/196], loss=76.1788
	step [104/196], loss=76.7317
	step [105/196], loss=76.0628
	step [106/196], loss=73.4944
	step [107/196], loss=79.7145
	step [108/196], loss=87.2107
	step [109/196], loss=66.6087
	step [110/196], loss=72.9894
	step [111/196], loss=77.9813
	step [112/196], loss=57.3573
	step [113/196], loss=83.9260
	step [114/196], loss=78.1742
	step [115/196], loss=77.0448
	step [116/196], loss=62.0327
	step [117/196], loss=64.2110
	step [118/196], loss=62.6017
	step [119/196], loss=79.5112
	step [120/196], loss=72.3372
	step [121/196], loss=64.0035
	step [122/196], loss=68.4314
	step [123/196], loss=74.4371
	step [124/196], loss=66.7195
	step [125/196], loss=56.7868
	step [126/196], loss=66.8251
	step [127/196], loss=76.9900
	step [128/196], loss=80.0859
	step [129/196], loss=67.4107
	step [130/196], loss=88.8523
	step [131/196], loss=79.5859
	step [132/196], loss=64.6498
	step [133/196], loss=63.7426
	step [134/196], loss=80.6205
	step [135/196], loss=73.4271
	step [136/196], loss=75.2719
	step [137/196], loss=72.5420
	step [138/196], loss=74.8587
	step [139/196], loss=78.2287
	step [140/196], loss=89.9757
	step [141/196], loss=77.3022
	step [142/196], loss=66.5099
	step [143/196], loss=69.0039
	step [144/196], loss=76.3180
	step [145/196], loss=74.2573
	step [146/196], loss=84.1103
	step [147/196], loss=66.8540
	step [148/196], loss=77.6883
	step [149/196], loss=82.1334
	step [150/196], loss=72.1102
	step [151/196], loss=70.2937
	step [152/196], loss=75.6624
	step [153/196], loss=54.9951
	step [154/196], loss=68.8191
	step [155/196], loss=63.0134
	step [156/196], loss=71.7603
	step [157/196], loss=63.0482
	step [158/196], loss=65.9362
	step [159/196], loss=74.4691
	step [160/196], loss=68.5424
	step [161/196], loss=80.8450
	step [162/196], loss=78.3064
	step [163/196], loss=81.1230
	step [164/196], loss=69.8035
	step [165/196], loss=78.5698
	step [166/196], loss=77.2423
	step [167/196], loss=70.7154
	step [168/196], loss=75.9087
	step [169/196], loss=65.5161
	step [170/196], loss=70.8099
	step [171/196], loss=79.0057
	step [172/196], loss=60.5862
	step [173/196], loss=72.4295
	step [174/196], loss=61.8655
	step [175/196], loss=73.1880
	step [176/196], loss=80.1635
	step [177/196], loss=71.5564
	step [178/196], loss=80.6096
	step [179/196], loss=76.5676
	step [180/196], loss=75.7712
	step [181/196], loss=76.9646
	step [182/196], loss=80.1669
	step [183/196], loss=78.1956
	step [184/196], loss=95.4967
	step [185/196], loss=70.8663
	step [186/196], loss=78.7348
	step [187/196], loss=84.8865
	step [188/196], loss=82.6223
	step [189/196], loss=80.1308
	step [190/196], loss=78.1407
	step [191/196], loss=78.4479
	step [192/196], loss=60.0364
	step [193/196], loss=90.4138
	step [194/196], loss=76.7439
	step [195/196], loss=88.4982
	step [196/196], loss=3.7172
	Evaluating
	loss=0.0063, precision=0.4577, recall=0.8706, f1=0.6000
Training epoch 70
	step [1/196], loss=85.2074
	step [2/196], loss=57.5496
	step [3/196], loss=63.0160
	step [4/196], loss=64.7108
	step [5/196], loss=72.8783
	step [6/196], loss=62.3565
	step [7/196], loss=64.2504
	step [8/196], loss=88.5413
	step [9/196], loss=77.9819
	step [10/196], loss=84.1778
	step [11/196], loss=71.0849
	step [12/196], loss=86.4897
	step [13/196], loss=67.6365
	step [14/196], loss=69.3527
	step [15/196], loss=71.9310
	step [16/196], loss=89.5465
	step [17/196], loss=69.4282
	step [18/196], loss=80.1349
	step [19/196], loss=71.2162
	step [20/196], loss=61.4796
	step [21/196], loss=75.0176
	step [22/196], loss=69.4412
	step [23/196], loss=85.6405
	step [24/196], loss=80.1125
	step [25/196], loss=75.0831
	step [26/196], loss=89.7081
	step [27/196], loss=86.5332
	step [28/196], loss=65.7024
	step [29/196], loss=85.8627
	step [30/196], loss=80.1999
	step [31/196], loss=79.2771
	step [32/196], loss=73.8028
	step [33/196], loss=94.4290
	step [34/196], loss=80.1581
	step [35/196], loss=81.9042
	step [36/196], loss=72.0115
	step [37/196], loss=74.5443
	step [38/196], loss=74.7390
	step [39/196], loss=76.9331
	step [40/196], loss=71.5923
	step [41/196], loss=68.0280
	step [42/196], loss=76.9122
	step [43/196], loss=73.2926
	step [44/196], loss=66.8555
	step [45/196], loss=70.4676
	step [46/196], loss=66.8554
	step [47/196], loss=94.2319
	step [48/196], loss=67.8064
	step [49/196], loss=76.3329
	step [50/196], loss=70.3573
	step [51/196], loss=73.7977
	step [52/196], loss=74.3123
	step [53/196], loss=72.7620
	step [54/196], loss=69.9501
	step [55/196], loss=69.8682
	step [56/196], loss=61.8584
	step [57/196], loss=64.8639
	step [58/196], loss=61.4933
	step [59/196], loss=72.6350
	step [60/196], loss=70.6744
	step [61/196], loss=68.2603
	step [62/196], loss=65.0207
	step [63/196], loss=75.3679
	step [64/196], loss=76.6602
	step [65/196], loss=63.8786
	step [66/196], loss=75.7188
	step [67/196], loss=76.8963
	step [68/196], loss=49.2544
	step [69/196], loss=84.6155
	step [70/196], loss=86.5544
	step [71/196], loss=69.4934
	step [72/196], loss=59.3120
	step [73/196], loss=78.7327
	step [74/196], loss=69.6668
	step [75/196], loss=70.3251
	step [76/196], loss=69.0212
	step [77/196], loss=65.2116
	step [78/196], loss=72.9622
	step [79/196], loss=72.3141
	step [80/196], loss=72.8945
	step [81/196], loss=83.4029
	step [82/196], loss=67.9185
	step [83/196], loss=67.9219
	step [84/196], loss=70.1783
	step [85/196], loss=73.7579
	step [86/196], loss=61.2912
	step [87/196], loss=81.5916
	step [88/196], loss=77.5886
	step [89/196], loss=77.8104
	step [90/196], loss=68.6703
	step [91/196], loss=74.8929
	step [92/196], loss=75.0065
	step [93/196], loss=80.5287
	step [94/196], loss=73.3751
	step [95/196], loss=68.8994
	step [96/196], loss=65.7879
	step [97/196], loss=60.5046
	step [98/196], loss=72.3556
	step [99/196], loss=76.0991
	step [100/196], loss=75.2253
	step [101/196], loss=66.9511
	step [102/196], loss=56.1850
	step [103/196], loss=85.9624
	step [104/196], loss=68.9937
	step [105/196], loss=74.3218
	step [106/196], loss=58.6746
	step [107/196], loss=73.4057
	step [108/196], loss=80.5872
	step [109/196], loss=82.2817
	step [110/196], loss=75.3353
	step [111/196], loss=80.2966
	step [112/196], loss=76.4449
	step [113/196], loss=73.7930
	step [114/196], loss=69.4481
	step [115/196], loss=84.8961
	step [116/196], loss=69.4785
	step [117/196], loss=77.3901
	step [118/196], loss=78.7602
	step [119/196], loss=87.7390
	step [120/196], loss=66.0979
	step [121/196], loss=58.1560
	step [122/196], loss=68.9905
	step [123/196], loss=69.2524
	step [124/196], loss=74.1958
	step [125/196], loss=77.4629
	step [126/196], loss=70.7668
	step [127/196], loss=69.0174
	step [128/196], loss=71.2496
	step [129/196], loss=64.5795
	step [130/196], loss=72.4475
	step [131/196], loss=66.7822
	step [132/196], loss=72.6818
	step [133/196], loss=78.6069
	step [134/196], loss=83.6410
	step [135/196], loss=76.7583
	step [136/196], loss=86.6677
	step [137/196], loss=73.1324
	step [138/196], loss=71.7119
	step [139/196], loss=74.1435
	step [140/196], loss=84.0295
	step [141/196], loss=71.8500
	step [142/196], loss=79.3726
	step [143/196], loss=68.7027
	step [144/196], loss=84.9551
	step [145/196], loss=72.7538
	step [146/196], loss=64.2772
	step [147/196], loss=76.7043
	step [148/196], loss=80.8648
	step [149/196], loss=80.1087
	step [150/196], loss=62.0119
	step [151/196], loss=67.2975
	step [152/196], loss=68.1410
	step [153/196], loss=89.7226
	step [154/196], loss=90.3192
	step [155/196], loss=72.5372
	step [156/196], loss=70.9094
	step [157/196], loss=69.9337
	step [158/196], loss=67.9367
	step [159/196], loss=82.4689
	step [160/196], loss=88.2590
	step [161/196], loss=72.7215
	step [162/196], loss=68.0663
	step [163/196], loss=78.6965
	step [164/196], loss=65.5280
	step [165/196], loss=65.1188
	step [166/196], loss=78.0921
	step [167/196], loss=79.5624
	step [168/196], loss=80.3025
	step [169/196], loss=77.2221
	step [170/196], loss=68.9275
	step [171/196], loss=77.2452
	step [172/196], loss=72.6907
	step [173/196], loss=73.2127
	step [174/196], loss=79.6533
	step [175/196], loss=79.3265
	step [176/196], loss=70.5738
	step [177/196], loss=72.0525
	step [178/196], loss=61.3004
	step [179/196], loss=83.3990
	step [180/196], loss=63.6181
	step [181/196], loss=75.9879
	step [182/196], loss=71.2531
	step [183/196], loss=59.2389
	step [184/196], loss=73.1433
	step [185/196], loss=70.8215
	step [186/196], loss=74.7760
	step [187/196], loss=69.8939
	step [188/196], loss=79.2705
	step [189/196], loss=74.3145
	step [190/196], loss=85.2755
	step [191/196], loss=72.6607
	step [192/196], loss=82.6140
	step [193/196], loss=63.8394
	step [194/196], loss=76.4666
	step [195/196], loss=70.5148
	step [196/196], loss=4.8237
	Evaluating
	loss=0.0063, precision=0.4507, recall=0.8368, f1=0.5858
Training epoch 71
	step [1/196], loss=73.3720
	step [2/196], loss=62.1719
	step [3/196], loss=81.8637
	step [4/196], loss=66.4067
	step [5/196], loss=86.9821
	step [6/196], loss=76.3690
	step [7/196], loss=77.1351
	step [8/196], loss=85.3192
	step [9/196], loss=76.8043
	step [10/196], loss=73.7621
	step [11/196], loss=84.7298
	step [12/196], loss=80.8212
	step [13/196], loss=79.9891
	step [14/196], loss=68.9959
	step [15/196], loss=74.2233
	step [16/196], loss=80.3387
	step [17/196], loss=75.1517
	step [18/196], loss=80.1131
	step [19/196], loss=77.4360
	step [20/196], loss=74.4463
	step [21/196], loss=75.6867
	step [22/196], loss=74.4720
	step [23/196], loss=59.4205
	step [24/196], loss=73.7005
	step [25/196], loss=86.8233
	step [26/196], loss=67.8483
	step [27/196], loss=76.9185
	step [28/196], loss=72.3966
	step [29/196], loss=71.4503
	step [30/196], loss=80.5084
	step [31/196], loss=69.3159
	step [32/196], loss=67.6875
	step [33/196], loss=68.5013
	step [34/196], loss=69.4123
	step [35/196], loss=83.3027
	step [36/196], loss=61.7808
	step [37/196], loss=60.7298
	step [38/196], loss=72.5476
	step [39/196], loss=68.9467
	step [40/196], loss=77.8076
	step [41/196], loss=75.5329
	step [42/196], loss=63.3583
	step [43/196], loss=75.4551
	step [44/196], loss=66.5841
	step [45/196], loss=78.1804
	step [46/196], loss=67.9088
	step [47/196], loss=66.4036
	step [48/196], loss=87.1530
	step [49/196], loss=81.1761
	step [50/196], loss=72.5571
	step [51/196], loss=76.4678
	step [52/196], loss=76.9892
	step [53/196], loss=82.8109
	step [54/196], loss=74.2741
	step [55/196], loss=68.6048
	step [56/196], loss=68.0807
	step [57/196], loss=75.4232
	step [58/196], loss=70.1794
	step [59/196], loss=82.2980
	step [60/196], loss=87.7131
	step [61/196], loss=64.6870
	step [62/196], loss=63.5215
	step [63/196], loss=66.0829
	step [64/196], loss=80.1055
	step [65/196], loss=82.5265
	step [66/196], loss=66.2074
	step [67/196], loss=76.4482
	step [68/196], loss=63.2633
	step [69/196], loss=82.0301
	step [70/196], loss=74.5666
	step [71/196], loss=73.9268
	step [72/196], loss=71.3137
	step [73/196], loss=73.8275
	step [74/196], loss=65.8351
	step [75/196], loss=70.4763
	step [76/196], loss=78.3949
	step [77/196], loss=78.7289
	step [78/196], loss=70.8303
	step [79/196], loss=79.3793
	step [80/196], loss=88.8893
	step [81/196], loss=88.5081
	step [82/196], loss=72.8064
	step [83/196], loss=87.1745
	step [84/196], loss=85.1204
	step [85/196], loss=67.4960
	step [86/196], loss=71.5364
	step [87/196], loss=70.5790
	step [88/196], loss=62.2488
	step [89/196], loss=77.5637
	step [90/196], loss=80.9768
	step [91/196], loss=62.6764
	step [92/196], loss=86.7132
	step [93/196], loss=66.6254
	step [94/196], loss=85.5481
	step [95/196], loss=73.0422
	step [96/196], loss=76.6346
	step [97/196], loss=72.0054
	step [98/196], loss=72.7930
	step [99/196], loss=75.6943
	step [100/196], loss=78.8944
	step [101/196], loss=87.6319
	step [102/196], loss=71.9097
	step [103/196], loss=66.8163
	step [104/196], loss=78.1165
	step [105/196], loss=63.3514
	step [106/196], loss=62.6977
	step [107/196], loss=68.0173
	step [108/196], loss=74.5953
	step [109/196], loss=75.1938
	step [110/196], loss=56.6726
	step [111/196], loss=77.3106
	step [112/196], loss=77.2597
	step [113/196], loss=72.7358
	step [114/196], loss=65.1906
	step [115/196], loss=76.0923
	step [116/196], loss=64.7671
	step [117/196], loss=69.6758
	step [118/196], loss=73.1593
	step [119/196], loss=87.9904
	step [120/196], loss=73.6696
	step [121/196], loss=73.2588
	step [122/196], loss=76.2123
	step [123/196], loss=68.7830
	step [124/196], loss=73.6726
	step [125/196], loss=78.6148
	step [126/196], loss=67.1181
	step [127/196], loss=76.5680
	step [128/196], loss=63.0081
	step [129/196], loss=70.7675
	step [130/196], loss=61.5463
	step [131/196], loss=83.0135
	step [132/196], loss=63.6177
	step [133/196], loss=66.0387
	step [134/196], loss=65.8536
	step [135/196], loss=57.4740
	step [136/196], loss=62.6629
	step [137/196], loss=78.4757
	step [138/196], loss=72.6870
	step [139/196], loss=65.7026
	step [140/196], loss=76.3190
	step [141/196], loss=65.3755
	step [142/196], loss=77.9356
	step [143/196], loss=72.4388
	step [144/196], loss=63.4234
	step [145/196], loss=67.0168
	step [146/196], loss=72.9705
	step [147/196], loss=88.9675
	step [148/196], loss=74.5945
	step [149/196], loss=69.6831
	step [150/196], loss=71.9946
	step [151/196], loss=75.5950
	step [152/196], loss=72.3732
	step [153/196], loss=73.2063
	step [154/196], loss=71.2669
	step [155/196], loss=68.0352
	step [156/196], loss=59.1135
	step [157/196], loss=77.5086
	step [158/196], loss=73.9239
	step [159/196], loss=81.1973
	step [160/196], loss=88.5723
	step [161/196], loss=68.3833
	step [162/196], loss=80.1867
	step [163/196], loss=77.0096
	step [164/196], loss=69.2467
	step [165/196], loss=83.2083
	step [166/196], loss=65.4941
	step [167/196], loss=72.1925
	step [168/196], loss=86.1605
	step [169/196], loss=61.4340
	step [170/196], loss=82.9890
	step [171/196], loss=83.1784
	step [172/196], loss=70.0214
	step [173/196], loss=61.2807
	step [174/196], loss=93.0001
	step [175/196], loss=87.8857
	step [176/196], loss=65.0817
	step [177/196], loss=73.2039
	step [178/196], loss=71.4902
	step [179/196], loss=83.4841
	step [180/196], loss=67.5688
	step [181/196], loss=73.4266
	step [182/196], loss=80.7441
	step [183/196], loss=73.1372
	step [184/196], loss=84.4687
	step [185/196], loss=79.0543
	step [186/196], loss=63.3459
	step [187/196], loss=72.5343
	step [188/196], loss=66.4273
	step [189/196], loss=76.9981
	step [190/196], loss=60.9337
	step [191/196], loss=79.7072
	step [192/196], loss=63.8114
	step [193/196], loss=86.3685
	step [194/196], loss=74.4438
	step [195/196], loss=57.5802
	step [196/196], loss=6.3527
	Evaluating
	loss=0.0076, precision=0.3952, recall=0.8580, f1=0.5411
Training epoch 72
	step [1/196], loss=74.8623
	step [2/196], loss=83.0608
	step [3/196], loss=78.9899
	step [4/196], loss=78.3267
	step [5/196], loss=83.0074
	step [6/196], loss=66.0978
	step [7/196], loss=71.8055
	step [8/196], loss=76.2706
	step [9/196], loss=66.0594
	step [10/196], loss=75.1430
	step [11/196], loss=73.3568
	step [12/196], loss=86.5361
	step [13/196], loss=65.3044
	step [14/196], loss=67.1178
	step [15/196], loss=61.5473
	step [16/196], loss=64.5263
	step [17/196], loss=82.5155
	step [18/196], loss=78.4604
	step [19/196], loss=60.1708
	step [20/196], loss=62.0228
	step [21/196], loss=85.8782
	step [22/196], loss=83.9975
	step [23/196], loss=55.0593
	step [24/196], loss=71.1897
	step [25/196], loss=70.6891
	step [26/196], loss=86.6307
	step [27/196], loss=73.4212
	step [28/196], loss=80.8691
	step [29/196], loss=63.9346
	step [30/196], loss=77.8707
	step [31/196], loss=69.6206
	step [32/196], loss=74.2943
	step [33/196], loss=64.5127
	step [34/196], loss=75.9547
	step [35/196], loss=70.7602
	step [36/196], loss=81.4047
	step [37/196], loss=63.0148
	step [38/196], loss=70.7636
	step [39/196], loss=81.2177
	step [40/196], loss=62.4118
	step [41/196], loss=65.4756
	step [42/196], loss=77.6497
	step [43/196], loss=67.6586
	step [44/196], loss=60.5652
	step [45/196], loss=86.6277
	step [46/196], loss=66.4669
	step [47/196], loss=78.8102
	step [48/196], loss=70.6188
	step [49/196], loss=73.0144
	step [50/196], loss=79.1132
	step [51/196], loss=83.8885
	step [52/196], loss=62.9948
	step [53/196], loss=70.2829
	step [54/196], loss=72.4132
	step [55/196], loss=85.5659
	step [56/196], loss=75.8382
	step [57/196], loss=70.5674
	step [58/196], loss=89.2541
	step [59/196], loss=59.1266
	step [60/196], loss=68.2494
	step [61/196], loss=77.6678
	step [62/196], loss=90.2487
	step [63/196], loss=72.8603
	step [64/196], loss=70.8120
	step [65/196], loss=76.1226
	step [66/196], loss=71.6409
	step [67/196], loss=64.2496
	step [68/196], loss=68.1489
	step [69/196], loss=77.3917
	step [70/196], loss=77.6628
	step [71/196], loss=68.0340
	step [72/196], loss=76.5844
	step [73/196], loss=65.5972
	step [74/196], loss=72.7265
	step [75/196], loss=74.7532
	step [76/196], loss=54.6761
	step [77/196], loss=81.7415
	step [78/196], loss=67.9191
	step [79/196], loss=71.5538
	step [80/196], loss=69.9166
	step [81/196], loss=80.3178
	step [82/196], loss=63.7256
	step [83/196], loss=81.0867
	step [84/196], loss=81.7688
	step [85/196], loss=80.6508
	step [86/196], loss=74.8698
	step [87/196], loss=81.3514
	step [88/196], loss=62.7281
	step [89/196], loss=67.9028
	step [90/196], loss=69.2772
	step [91/196], loss=70.3693
	step [92/196], loss=69.7619
	step [93/196], loss=84.8417
	step [94/196], loss=69.7108
	step [95/196], loss=77.6964
	step [96/196], loss=80.8793
	step [97/196], loss=80.0116
	step [98/196], loss=79.6544
	step [99/196], loss=72.6725
	step [100/196], loss=70.4381
	step [101/196], loss=95.0242
	step [102/196], loss=73.1858
	step [103/196], loss=83.7653
	step [104/196], loss=76.7439
	step [105/196], loss=67.9856
	step [106/196], loss=73.4411
	step [107/196], loss=68.6826
	step [108/196], loss=59.9154
	step [109/196], loss=60.6304
	step [110/196], loss=65.8755
	step [111/196], loss=58.9076
	step [112/196], loss=71.8364
	step [113/196], loss=86.5024
	step [114/196], loss=82.0200
	step [115/196], loss=75.8156
	step [116/196], loss=76.2599
	step [117/196], loss=70.6832
	step [118/196], loss=83.2193
	step [119/196], loss=73.7523
	step [120/196], loss=87.3379
	step [121/196], loss=77.2402
	step [122/196], loss=75.6541
	step [123/196], loss=65.6754
	step [124/196], loss=65.4488
	step [125/196], loss=73.5042
	step [126/196], loss=74.7503
	step [127/196], loss=67.9590
	step [128/196], loss=81.8483
	step [129/196], loss=77.8414
	step [130/196], loss=80.5355
	step [131/196], loss=84.3492
	step [132/196], loss=80.0625
	step [133/196], loss=77.8688
	step [134/196], loss=79.0163
	step [135/196], loss=85.5931
	step [136/196], loss=63.0886
	step [137/196], loss=69.1453
	step [138/196], loss=62.4344
	step [139/196], loss=73.9480
	step [140/196], loss=70.0556
	step [141/196], loss=86.3498
	step [142/196], loss=78.5159
	step [143/196], loss=76.8201
	step [144/196], loss=89.2219
	step [145/196], loss=65.4000
	step [146/196], loss=69.4998
	step [147/196], loss=70.0502
	step [148/196], loss=68.6546
	step [149/196], loss=84.8563
	step [150/196], loss=77.4840
	step [151/196], loss=67.2836
	step [152/196], loss=76.7878
	step [153/196], loss=79.7999
	step [154/196], loss=67.1473
	step [155/196], loss=75.7158
	step [156/196], loss=74.4419
	step [157/196], loss=82.5684
	step [158/196], loss=64.5512
	step [159/196], loss=73.5861
	step [160/196], loss=66.6665
	step [161/196], loss=66.4249
	step [162/196], loss=74.9168
	step [163/196], loss=72.8218
	step [164/196], loss=68.2494
	step [165/196], loss=80.3873
	step [166/196], loss=86.6329
	step [167/196], loss=71.1174
	step [168/196], loss=78.9054
	step [169/196], loss=79.6800
	step [170/196], loss=74.4035
	step [171/196], loss=73.5916
	step [172/196], loss=79.4546
	step [173/196], loss=74.8105
	step [174/196], loss=73.2487
	step [175/196], loss=76.4196
	step [176/196], loss=70.1113
	step [177/196], loss=67.2257
	step [178/196], loss=87.7679
	step [179/196], loss=69.7212
	step [180/196], loss=77.1985
	step [181/196], loss=76.2465
	step [182/196], loss=65.9123
	step [183/196], loss=78.2935
	step [184/196], loss=66.0518
	step [185/196], loss=79.3970
	step [186/196], loss=76.3597
	step [187/196], loss=76.2368
	step [188/196], loss=74.9547
	step [189/196], loss=73.6139
	step [190/196], loss=75.1556
	step [191/196], loss=78.3777
	step [192/196], loss=93.3471
	step [193/196], loss=83.0670
	step [194/196], loss=74.1768
	step [195/196], loss=68.0473
	step [196/196], loss=9.8520
	Evaluating
	loss=0.0076, precision=0.3840, recall=0.8615, f1=0.5313
Training epoch 73
	step [1/196], loss=68.1560
	step [2/196], loss=64.1868
	step [3/196], loss=72.1091
	step [4/196], loss=81.6756
	step [5/196], loss=86.0016
	step [6/196], loss=69.0990
	step [7/196], loss=74.8326
	step [8/196], loss=74.1817
	step [9/196], loss=63.8817
	step [10/196], loss=82.1516
	step [11/196], loss=70.9538
	step [12/196], loss=72.7270
	step [13/196], loss=80.1227
	step [14/196], loss=69.7538
	step [15/196], loss=72.0635
	step [16/196], loss=69.4858
	step [17/196], loss=84.0713
	step [18/196], loss=68.6461
	step [19/196], loss=75.8486
	step [20/196], loss=62.7821
	step [21/196], loss=74.5883
	step [22/196], loss=68.2957
	step [23/196], loss=68.6374
	step [24/196], loss=66.3650
	step [25/196], loss=67.1224
	step [26/196], loss=70.9756
	step [27/196], loss=64.1594
	step [28/196], loss=58.2932
	step [29/196], loss=86.3745
	step [30/196], loss=68.9153
	step [31/196], loss=86.9736
	step [32/196], loss=76.6160
	step [33/196], loss=71.1652
	step [34/196], loss=82.7757
	step [35/196], loss=72.0786
	step [36/196], loss=73.1132
	step [37/196], loss=66.3495
	step [38/196], loss=68.7022
	step [39/196], loss=67.4249
	step [40/196], loss=76.5175
	step [41/196], loss=75.6814
	step [42/196], loss=67.5699
	step [43/196], loss=86.8589
	step [44/196], loss=72.2475
	step [45/196], loss=75.1729
	step [46/196], loss=74.3549
	step [47/196], loss=78.4481
	step [48/196], loss=69.2945
	step [49/196], loss=73.3682
	step [50/196], loss=72.4295
	step [51/196], loss=78.9129
	step [52/196], loss=74.8857
	step [53/196], loss=81.7945
	step [54/196], loss=83.6115
	step [55/196], loss=71.4783
	step [56/196], loss=65.2415
	step [57/196], loss=65.7687
	step [58/196], loss=72.7246
	step [59/196], loss=68.4531
	step [60/196], loss=73.7072
	step [61/196], loss=69.9453
	step [62/196], loss=81.3897
	step [63/196], loss=55.9694
	step [64/196], loss=70.1974
	step [65/196], loss=59.0769
	step [66/196], loss=83.5371
	step [67/196], loss=77.4267
	step [68/196], loss=81.7500
	step [69/196], loss=79.3785
	step [70/196], loss=67.5069
	step [71/196], loss=76.1361
	step [72/196], loss=68.1696
	step [73/196], loss=83.2550
	step [74/196], loss=85.4566
	step [75/196], loss=85.0380
	step [76/196], loss=73.6425
	step [77/196], loss=79.0245
	step [78/196], loss=78.0973
	step [79/196], loss=77.1984
	step [80/196], loss=76.8654
	step [81/196], loss=83.9637
	step [82/196], loss=65.1397
	step [83/196], loss=70.1709
	step [84/196], loss=72.9671
	step [85/196], loss=72.8325
	step [86/196], loss=78.9835
	step [87/196], loss=72.2152
	step [88/196], loss=81.5774
	step [89/196], loss=69.0775
	step [90/196], loss=77.0936
	step [91/196], loss=72.7914
	step [92/196], loss=74.4668
	step [93/196], loss=87.6598
	step [94/196], loss=67.2027
	step [95/196], loss=78.5336
	step [96/196], loss=68.1815
	step [97/196], loss=76.5071
	step [98/196], loss=68.4181
	step [99/196], loss=83.8884
	step [100/196], loss=71.4678
	step [101/196], loss=86.7948
	step [102/196], loss=69.4662
	step [103/196], loss=69.1295
	step [104/196], loss=76.1443
	step [105/196], loss=70.2501
	step [106/196], loss=65.2203
	step [107/196], loss=72.2243
	step [108/196], loss=72.6849
	step [109/196], loss=72.4423
	step [110/196], loss=63.1112
	step [111/196], loss=57.6364
	step [112/196], loss=76.3970
	step [113/196], loss=76.2011
	step [114/196], loss=79.2215
	step [115/196], loss=63.2421
	step [116/196], loss=70.1570
	step [117/196], loss=81.0256
	step [118/196], loss=79.7111
	step [119/196], loss=56.9693
	step [120/196], loss=73.6248
	step [121/196], loss=78.7656
	step [122/196], loss=60.9795
	step [123/196], loss=67.7636
	step [124/196], loss=72.9182
	step [125/196], loss=69.5187
	step [126/196], loss=86.0468
	step [127/196], loss=85.0643
	step [128/196], loss=63.4566
	step [129/196], loss=78.8121
	step [130/196], loss=78.1263
	step [131/196], loss=83.9079
	step [132/196], loss=90.5129
	step [133/196], loss=74.5327
	step [134/196], loss=68.6676
	step [135/196], loss=74.4101
	step [136/196], loss=65.7422
	step [137/196], loss=63.9846
	step [138/196], loss=96.3041
	step [139/196], loss=81.4619
	step [140/196], loss=64.4821
	step [141/196], loss=85.7311
	step [142/196], loss=67.9973
	step [143/196], loss=83.1895
	step [144/196], loss=66.9830
	step [145/196], loss=58.2674
	step [146/196], loss=62.0802
	step [147/196], loss=67.9074
	step [148/196], loss=69.7566
	step [149/196], loss=75.8120
	step [150/196], loss=67.0231
	step [151/196], loss=70.4666
	step [152/196], loss=76.6479
	step [153/196], loss=71.3077
	step [154/196], loss=75.3842
	step [155/196], loss=62.1730
	step [156/196], loss=84.4849
	step [157/196], loss=71.5040
	step [158/196], loss=71.3062
	step [159/196], loss=60.7498
	step [160/196], loss=67.2249
	step [161/196], loss=83.4041
	step [162/196], loss=80.3411
	step [163/196], loss=61.5589
	step [164/196], loss=77.4996
	step [165/196], loss=63.5441
	step [166/196], loss=83.0701
	step [167/196], loss=77.6073
	step [168/196], loss=70.4415
	step [169/196], loss=68.9384
	step [170/196], loss=65.7902
	step [171/196], loss=76.1823
	step [172/196], loss=55.3037
	step [173/196], loss=84.7965
	step [174/196], loss=66.8673
	step [175/196], loss=77.8010
	step [176/196], loss=76.0067
	step [177/196], loss=72.9288
	step [178/196], loss=84.8759
	step [179/196], loss=66.7545
	step [180/196], loss=86.4026
	step [181/196], loss=76.4003
	step [182/196], loss=66.1458
	step [183/196], loss=64.1863
	step [184/196], loss=72.9460
	step [185/196], loss=74.0300
	step [186/196], loss=62.1394
	step [187/196], loss=69.7429
	step [188/196], loss=70.2193
	step [189/196], loss=85.3785
	step [190/196], loss=78.0236
	step [191/196], loss=66.0850
	step [192/196], loss=80.5090
	step [193/196], loss=69.4176
	step [194/196], loss=76.6137
	step [195/196], loss=75.8107
	step [196/196], loss=6.3696
	Evaluating
	loss=0.0076, precision=0.4002, recall=0.8656, f1=0.5473
Training epoch 74
	step [1/196], loss=72.2861
	step [2/196], loss=64.4951
	step [3/196], loss=73.9315
	step [4/196], loss=76.7043
	step [5/196], loss=65.6022
	step [6/196], loss=80.4551
	step [7/196], loss=68.4677
	step [8/196], loss=72.1887
	step [9/196], loss=93.6021
	step [10/196], loss=73.2157
	step [11/196], loss=67.2203
	step [12/196], loss=82.8423
	step [13/196], loss=74.1785
	step [14/196], loss=75.0225
	step [15/196], loss=63.8161
	step [16/196], loss=79.9856
	step [17/196], loss=74.3083
	step [18/196], loss=89.1358
	step [19/196], loss=76.9346
	step [20/196], loss=80.8559
	step [21/196], loss=63.0384
	step [22/196], loss=82.7396
	step [23/196], loss=89.0243
	step [24/196], loss=86.0864
	step [25/196], loss=67.3838
	step [26/196], loss=66.2864
	step [27/196], loss=76.9365
	step [28/196], loss=73.6999
	step [29/196], loss=75.4140
	step [30/196], loss=84.3717
	step [31/196], loss=77.8732
	step [32/196], loss=69.9113
	step [33/196], loss=85.0972
	step [34/196], loss=79.7233
	step [35/196], loss=73.1119
	step [36/196], loss=78.6687
	step [37/196], loss=75.1863
	step [38/196], loss=65.9346
	step [39/196], loss=64.5360
	step [40/196], loss=85.0159
	step [41/196], loss=55.7210
	step [42/196], loss=78.3311
	step [43/196], loss=76.9807
	step [44/196], loss=70.3833
	step [45/196], loss=73.0172
	step [46/196], loss=73.2147
	step [47/196], loss=70.6566
	step [48/196], loss=67.9964
	step [49/196], loss=76.6841
	step [50/196], loss=69.4731
	step [51/196], loss=77.2159
	step [52/196], loss=70.1090
	step [53/196], loss=69.1679
	step [54/196], loss=68.1303
	step [55/196], loss=65.5424
	step [56/196], loss=71.6179
	step [57/196], loss=80.9854
	step [58/196], loss=67.5384
	step [59/196], loss=69.4122
	step [60/196], loss=82.7135
	step [61/196], loss=81.7744
	step [62/196], loss=61.8776
	step [63/196], loss=66.1559
	step [64/196], loss=76.1766
	step [65/196], loss=67.2865
	step [66/196], loss=74.8627
	step [67/196], loss=78.5638
	step [68/196], loss=66.9132
	step [69/196], loss=67.7930
	step [70/196], loss=66.8755
	step [71/196], loss=61.2841
	step [72/196], loss=78.6643
	step [73/196], loss=73.0762
	step [74/196], loss=65.9660
	step [75/196], loss=64.5349
	step [76/196], loss=68.5269
	step [77/196], loss=68.2032
	step [78/196], loss=77.2746
	step [79/196], loss=72.7366
	step [80/196], loss=72.0603
	step [81/196], loss=62.6607
	step [82/196], loss=65.6623
	step [83/196], loss=69.3166
	step [84/196], loss=73.7968
	step [85/196], loss=79.1104
	step [86/196], loss=63.9719
	step [87/196], loss=77.7240
	step [88/196], loss=85.7474
	step [89/196], loss=83.5905
	step [90/196], loss=64.5415
	step [91/196], loss=72.2414
	step [92/196], loss=78.8807
	step [93/196], loss=74.5891
	step [94/196], loss=65.4088
	step [95/196], loss=68.6464
	step [96/196], loss=71.1802
	step [97/196], loss=57.7340
	step [98/196], loss=65.1598
	step [99/196], loss=91.0378
	step [100/196], loss=64.7517
	step [101/196], loss=72.0840
	step [102/196], loss=64.1928
	step [103/196], loss=68.6292
	step [104/196], loss=68.5648
	step [105/196], loss=70.3047
	step [106/196], loss=80.0335
	step [107/196], loss=62.6810
	step [108/196], loss=73.1724
	step [109/196], loss=69.6661
	step [110/196], loss=65.9694
	step [111/196], loss=65.3787
	step [112/196], loss=84.6311
	step [113/196], loss=65.5019
	step [114/196], loss=80.4459
	step [115/196], loss=66.3852
	step [116/196], loss=72.3400
	step [117/196], loss=93.8045
	step [118/196], loss=88.6218
	step [119/196], loss=72.3751
	step [120/196], loss=83.6476
	step [121/196], loss=68.9017
	step [122/196], loss=70.3960
	step [123/196], loss=70.4640
	step [124/196], loss=79.6277
	step [125/196], loss=85.3684
	step [126/196], loss=73.2062
	step [127/196], loss=67.1940
	step [128/196], loss=57.5585
	step [129/196], loss=77.4058
	step [130/196], loss=75.8643
	step [131/196], loss=78.8658
	step [132/196], loss=61.9161
	step [133/196], loss=75.6354
	step [134/196], loss=72.6322
	step [135/196], loss=55.3967
	step [136/196], loss=74.4981
	step [137/196], loss=68.7672
	step [138/196], loss=77.7585
	step [139/196], loss=78.0820
	step [140/196], loss=73.1147
	step [141/196], loss=68.1825
	step [142/196], loss=80.0271
	step [143/196], loss=58.0321
	step [144/196], loss=71.9282
	step [145/196], loss=75.9247
	step [146/196], loss=66.2487
	step [147/196], loss=85.5356
	step [148/196], loss=78.5081
	step [149/196], loss=53.4961
	step [150/196], loss=71.5639
	step [151/196], loss=82.4396
	step [152/196], loss=71.4442
	step [153/196], loss=65.9242
	step [154/196], loss=79.1992
	step [155/196], loss=83.5087
	step [156/196], loss=63.1108
	step [157/196], loss=73.4932
	step [158/196], loss=83.1044
	step [159/196], loss=71.2141
	step [160/196], loss=69.4009
	step [161/196], loss=75.7799
	step [162/196], loss=67.9346
	step [163/196], loss=83.8997
	step [164/196], loss=66.6432
	step [165/196], loss=64.9904
	step [166/196], loss=80.8348
	step [167/196], loss=83.3759
	step [168/196], loss=76.7917
	step [169/196], loss=81.2380
	step [170/196], loss=71.5370
	step [171/196], loss=75.9905
	step [172/196], loss=68.8645
	step [173/196], loss=77.0587
	step [174/196], loss=63.6035
	step [175/196], loss=82.1052
	step [176/196], loss=81.2289
	step [177/196], loss=71.0799
	step [178/196], loss=65.4360
	step [179/196], loss=69.1497
	step [180/196], loss=62.5543
	step [181/196], loss=65.4009
	step [182/196], loss=72.6320
	step [183/196], loss=80.8737
	step [184/196], loss=77.9577
	step [185/196], loss=76.2342
	step [186/196], loss=70.5872
	step [187/196], loss=88.0698
	step [188/196], loss=64.2886
	step [189/196], loss=70.0557
	step [190/196], loss=70.7839
	step [191/196], loss=68.9526
	step [192/196], loss=64.3865
	step [193/196], loss=69.4775
	step [194/196], loss=74.6646
	step [195/196], loss=72.5397
	step [196/196], loss=8.5344
	Evaluating
	loss=0.0119, precision=0.2793, recall=0.7765, f1=0.4108
Training epoch 75
	step [1/196], loss=74.5222
	step [2/196], loss=84.4663
	step [3/196], loss=58.2008
	step [4/196], loss=64.1241
	step [5/196], loss=79.3027
	step [6/196], loss=66.5539
	step [7/196], loss=67.0103
	step [8/196], loss=77.3381
	step [9/196], loss=75.4761
	step [10/196], loss=68.7277
	step [11/196], loss=67.5094
	step [12/196], loss=78.1809
	step [13/196], loss=64.4612
	step [14/196], loss=91.2483
	step [15/196], loss=66.1049
	step [16/196], loss=79.7394
	step [17/196], loss=77.4985
	step [18/196], loss=72.8424
	step [19/196], loss=96.2816
	step [20/196], loss=80.5316
	step [21/196], loss=73.2504
	step [22/196], loss=81.4780
	step [23/196], loss=77.0495
	step [24/196], loss=81.5898
	step [25/196], loss=61.8884
	step [26/196], loss=78.2041
	step [27/196], loss=67.9222
	step [28/196], loss=67.6802
	step [29/196], loss=82.4377
	step [30/196], loss=65.7935
	step [31/196], loss=83.4753
	step [32/196], loss=74.0832
	step [33/196], loss=57.5118
	step [34/196], loss=83.4024
	step [35/196], loss=66.3738
	step [36/196], loss=84.3834
	step [37/196], loss=60.7439
	step [38/196], loss=64.7911
	step [39/196], loss=84.8709
	step [40/196], loss=80.7817
	step [41/196], loss=60.2913
	step [42/196], loss=83.4715
	step [43/196], loss=81.5289
	step [44/196], loss=79.0839
	step [45/196], loss=83.7781
	step [46/196], loss=80.2241
	step [47/196], loss=76.1384
	step [48/196], loss=60.4599
	step [49/196], loss=74.4342
	step [50/196], loss=87.3616
	step [51/196], loss=66.3056
	step [52/196], loss=77.4816
	step [53/196], loss=80.1057
	step [54/196], loss=51.2116
	step [55/196], loss=72.2587
	step [56/196], loss=90.0978
	step [57/196], loss=70.6688
	step [58/196], loss=65.3551
	step [59/196], loss=68.2396
	step [60/196], loss=71.1485
	step [61/196], loss=85.0273
	step [62/196], loss=79.9381
	step [63/196], loss=74.1010
	step [64/196], loss=76.3717
	step [65/196], loss=61.2611
	step [66/196], loss=69.0734
	step [67/196], loss=71.1780
	step [68/196], loss=61.6305
	step [69/196], loss=67.2102
	step [70/196], loss=66.0859
	step [71/196], loss=56.1719
	step [72/196], loss=61.2061
	step [73/196], loss=58.0958
	step [74/196], loss=75.9055
	step [75/196], loss=82.2866
	step [76/196], loss=68.4897
	step [77/196], loss=82.2708
	step [78/196], loss=85.0909
	step [79/196], loss=81.1066
	step [80/196], loss=70.1299
	step [81/196], loss=75.0006
	step [82/196], loss=66.4299
	step [83/196], loss=72.0798
	step [84/196], loss=71.1102
	step [85/196], loss=65.1915
	step [86/196], loss=79.4981
	step [87/196], loss=82.0594
	step [88/196], loss=73.9183
	step [89/196], loss=67.9460
	step [90/196], loss=69.4290
	step [91/196], loss=72.9818
	step [92/196], loss=65.7854
	step [93/196], loss=81.1924
	step [94/196], loss=67.3916
	step [95/196], loss=69.1671
	step [96/196], loss=67.4268
	step [97/196], loss=73.0933
	step [98/196], loss=68.2683
	step [99/196], loss=75.2544
	step [100/196], loss=67.5838
	step [101/196], loss=72.3317
	step [102/196], loss=73.7724
	step [103/196], loss=85.6177
	step [104/196], loss=72.6419
	step [105/196], loss=77.6768
	step [106/196], loss=69.9816
	step [107/196], loss=84.6341
	step [108/196], loss=82.0394
	step [109/196], loss=69.5554
	step [110/196], loss=70.3598
	step [111/196], loss=65.8055
	step [112/196], loss=79.6785
	step [113/196], loss=68.4427
	step [114/196], loss=75.1676
	step [115/196], loss=74.1426
	step [116/196], loss=79.7790
	step [117/196], loss=67.8201
	step [118/196], loss=75.1878
	step [119/196], loss=57.1455
	step [120/196], loss=73.2664
	step [121/196], loss=81.4539
	step [122/196], loss=72.1673
	step [123/196], loss=61.1701
	step [124/196], loss=59.5511
	step [125/196], loss=63.1301
	step [126/196], loss=62.6708
	step [127/196], loss=71.2260
	step [128/196], loss=65.0843
	step [129/196], loss=69.4803
	step [130/196], loss=68.4929
	step [131/196], loss=78.8940
	step [132/196], loss=59.0706
	step [133/196], loss=65.7311
	step [134/196], loss=65.9311
	step [135/196], loss=59.9904
	step [136/196], loss=74.9270
	step [137/196], loss=82.0114
	step [138/196], loss=71.9966
	step [139/196], loss=81.3508
	step [140/196], loss=72.4124
	step [141/196], loss=62.2579
	step [142/196], loss=86.9152
	step [143/196], loss=76.4858
	step [144/196], loss=70.0889
	step [145/196], loss=83.8657
	step [146/196], loss=75.8361
	step [147/196], loss=75.5144
	step [148/196], loss=71.7805
	step [149/196], loss=72.6576
	step [150/196], loss=70.9217
	step [151/196], loss=67.3731
	step [152/196], loss=67.7018
	step [153/196], loss=75.3559
	step [154/196], loss=58.0386
	step [155/196], loss=80.3239
	step [156/196], loss=68.9089
	step [157/196], loss=69.5131
	step [158/196], loss=75.2600
	step [159/196], loss=93.2424
	step [160/196], loss=79.1305
	step [161/196], loss=71.4525
	step [162/196], loss=83.1433
	step [163/196], loss=69.4217
	step [164/196], loss=77.2804
	step [165/196], loss=69.7483
	step [166/196], loss=67.4100
	step [167/196], loss=65.2944
	step [168/196], loss=71.5947
	step [169/196], loss=78.6352
	step [170/196], loss=79.8286
	step [171/196], loss=82.4240
	step [172/196], loss=80.1362
	step [173/196], loss=71.3836
	step [174/196], loss=69.1030
	step [175/196], loss=78.1986
	step [176/196], loss=71.8952
	step [177/196], loss=78.6989
	step [178/196], loss=63.6277
	step [179/196], loss=74.1934
	step [180/196], loss=77.6280
	step [181/196], loss=74.9816
	step [182/196], loss=74.8859
	step [183/196], loss=65.5952
	step [184/196], loss=66.8025
	step [185/196], loss=75.5051
	step [186/196], loss=87.6736
	step [187/196], loss=70.6023
	step [188/196], loss=75.8919
	step [189/196], loss=66.6617
	step [190/196], loss=73.3035
	step [191/196], loss=78.0578
	step [192/196], loss=67.4380
	step [193/196], loss=67.8592
	step [194/196], loss=62.0144
	step [195/196], loss=83.8280
	step [196/196], loss=2.2893
	Evaluating
	loss=0.0069, precision=0.4172, recall=0.8389, f1=0.5572
Training epoch 76
	step [1/196], loss=68.1205
	step [2/196], loss=62.4081
	step [3/196], loss=66.4703
	step [4/196], loss=62.4909
	step [5/196], loss=72.9714
	step [6/196], loss=74.8346
	step [7/196], loss=60.0730
	step [8/196], loss=73.0007
	step [9/196], loss=73.4848
	step [10/196], loss=57.3644
	step [11/196], loss=69.9648
	step [12/196], loss=60.4853
	step [13/196], loss=73.1461
	step [14/196], loss=69.9807
	step [15/196], loss=73.9592
	step [16/196], loss=74.1867
	step [17/196], loss=73.3962
	step [18/196], loss=63.5198
	step [19/196], loss=58.9790
	step [20/196], loss=68.5406
	step [21/196], loss=66.6147
	step [22/196], loss=74.2965
	step [23/196], loss=76.3465
	step [24/196], loss=79.2701
	step [25/196], loss=80.8029
	step [26/196], loss=82.6094
	step [27/196], loss=68.3121
	step [28/196], loss=81.2090
	step [29/196], loss=58.5681
	step [30/196], loss=66.1771
	step [31/196], loss=68.4604
	step [32/196], loss=72.8692
	step [33/196], loss=85.7273
	step [34/196], loss=68.2233
	step [35/196], loss=74.5098
	step [36/196], loss=67.4048
	step [37/196], loss=76.5200
	step [38/196], loss=80.4996
	step [39/196], loss=72.2436
	step [40/196], loss=71.1277
	step [41/196], loss=65.0342
	step [42/196], loss=73.1228
	step [43/196], loss=73.5882
	step [44/196], loss=73.3666
	step [45/196], loss=73.9685
	step [46/196], loss=65.9919
	step [47/196], loss=76.0348
	step [48/196], loss=69.5181
	step [49/196], loss=69.5680
	step [50/196], loss=57.9324
	step [51/196], loss=69.8095
	step [52/196], loss=72.5869
	step [53/196], loss=65.1944
	step [54/196], loss=61.4984
	step [55/196], loss=79.7874
	step [56/196], loss=73.7559
	step [57/196], loss=90.7467
	step [58/196], loss=73.1189
	step [59/196], loss=65.1808
	step [60/196], loss=86.0732
	step [61/196], loss=74.0534
	step [62/196], loss=68.5259
	step [63/196], loss=68.4043
	step [64/196], loss=71.9069
	step [65/196], loss=65.9084
	step [66/196], loss=73.5442
	step [67/196], loss=66.8190
	step [68/196], loss=66.8426
	step [69/196], loss=70.1197
	step [70/196], loss=83.2448
	step [71/196], loss=75.8052
	step [72/196], loss=72.1157
	step [73/196], loss=82.0353
	step [74/196], loss=69.8748
	step [75/196], loss=71.1955
	step [76/196], loss=63.2000
	step [77/196], loss=61.3130
	step [78/196], loss=73.7105
	step [79/196], loss=68.6938
	step [80/196], loss=91.3122
	step [81/196], loss=69.4752
	step [82/196], loss=69.8728
	step [83/196], loss=78.5840
	step [84/196], loss=73.2307
	step [85/196], loss=82.3672
	step [86/196], loss=66.6423
	step [87/196], loss=66.8847
	step [88/196], loss=75.2532
	step [89/196], loss=72.2691
	step [90/196], loss=65.7623
	step [91/196], loss=79.4255
	step [92/196], loss=68.2520
	step [93/196], loss=83.9709
	step [94/196], loss=71.2331
	step [95/196], loss=78.5303
	step [96/196], loss=75.1545
	step [97/196], loss=70.8435
	step [98/196], loss=70.5563
	step [99/196], loss=80.3207
	step [100/196], loss=77.7725
	step [101/196], loss=72.9136
	step [102/196], loss=71.2852
	step [103/196], loss=72.5768
	step [104/196], loss=81.3965
	step [105/196], loss=69.0917
	step [106/196], loss=69.2013
	step [107/196], loss=70.0319
	step [108/196], loss=70.5900
	step [109/196], loss=67.0464
	step [110/196], loss=66.2194
	step [111/196], loss=79.2849
	step [112/196], loss=65.8670
	step [113/196], loss=75.3831
	step [114/196], loss=72.3064
	step [115/196], loss=62.9872
	step [116/196], loss=77.6716
	step [117/196], loss=73.1700
	step [118/196], loss=71.9208
	step [119/196], loss=62.0972
	step [120/196], loss=73.5756
	step [121/196], loss=86.5657
	step [122/196], loss=58.3492
	step [123/196], loss=76.8344
	step [124/196], loss=86.4781
	step [125/196], loss=72.5647
	step [126/196], loss=69.3486
	step [127/196], loss=82.6046
	step [128/196], loss=75.7562
	step [129/196], loss=65.7011
	step [130/196], loss=73.9279
	step [131/196], loss=73.2654
	step [132/196], loss=79.4243
	step [133/196], loss=74.3543
	step [134/196], loss=75.4646
	step [135/196], loss=70.3595
	step [136/196], loss=77.7849
	step [137/196], loss=69.6731
	step [138/196], loss=66.3870
	step [139/196], loss=70.5826
	step [140/196], loss=79.7742
	step [141/196], loss=64.7474
	step [142/196], loss=67.9207
	step [143/196], loss=67.5206
	step [144/196], loss=67.9376
	step [145/196], loss=77.7812
	step [146/196], loss=68.9536
	step [147/196], loss=70.3084
	step [148/196], loss=76.6309
	step [149/196], loss=81.4298
	step [150/196], loss=76.1964
	step [151/196], loss=72.9857
	step [152/196], loss=81.8997
	step [153/196], loss=87.6688
	step [154/196], loss=66.8783
	step [155/196], loss=80.0292
	step [156/196], loss=82.4576
	step [157/196], loss=67.6302
	step [158/196], loss=70.6649
	step [159/196], loss=49.4195
	step [160/196], loss=74.2676
	step [161/196], loss=77.8367
	step [162/196], loss=66.6570
	step [163/196], loss=61.7854
	step [164/196], loss=80.4735
	step [165/196], loss=79.4499
	step [166/196], loss=68.8870
	step [167/196], loss=69.2321
	step [168/196], loss=73.0063
	step [169/196], loss=67.1117
	step [170/196], loss=69.7780
	step [171/196], loss=78.4368
	step [172/196], loss=82.4073
	step [173/196], loss=69.1562
	step [174/196], loss=67.2851
	step [175/196], loss=65.2881
	step [176/196], loss=77.8360
	step [177/196], loss=77.6241
	step [178/196], loss=72.2482
	step [179/196], loss=70.7243
	step [180/196], loss=80.4438
	step [181/196], loss=68.5779
	step [182/196], loss=66.5604
	step [183/196], loss=76.2706
	step [184/196], loss=71.6299
	step [185/196], loss=72.5531
	step [186/196], loss=82.0726
	step [187/196], loss=71.7504
	step [188/196], loss=69.9075
	step [189/196], loss=72.8659
	step [190/196], loss=68.9161
	step [191/196], loss=70.6958
	step [192/196], loss=70.5107
	step [193/196], loss=78.9076
	step [194/196], loss=87.5612
	step [195/196], loss=67.0895
	step [196/196], loss=5.8355
	Evaluating
	loss=0.0065, precision=0.4503, recall=0.8562, f1=0.5902
Training epoch 77
	step [1/196], loss=84.0951
	step [2/196], loss=74.6703
	step [3/196], loss=61.9726
	step [4/196], loss=77.7295
	step [5/196], loss=72.7776
	step [6/196], loss=60.1340
	step [7/196], loss=59.0097
	step [8/196], loss=68.6142
	step [9/196], loss=62.3106
	step [10/196], loss=73.3845
	step [11/196], loss=69.2625
	step [12/196], loss=75.1578
	step [13/196], loss=78.3840
	step [14/196], loss=74.6896
	step [15/196], loss=82.0193
	step [16/196], loss=71.6350
	step [17/196], loss=68.7088
	step [18/196], loss=72.1189
	step [19/196], loss=58.9946
	step [20/196], loss=81.7534
	step [21/196], loss=80.7118
	step [22/196], loss=76.8606
	step [23/196], loss=67.6593
	step [24/196], loss=74.9647
	step [25/196], loss=66.0453
	step [26/196], loss=70.3283
	step [27/196], loss=64.6887
	step [28/196], loss=77.1564
	step [29/196], loss=62.8810
	step [30/196], loss=76.6256
	step [31/196], loss=68.9873
	step [32/196], loss=65.9530
	step [33/196], loss=76.9307
	step [34/196], loss=51.6513
	step [35/196], loss=69.9302
	step [36/196], loss=73.8459
	step [37/196], loss=76.0089
	step [38/196], loss=70.5157
	step [39/196], loss=69.6398
	step [40/196], loss=61.7850
	step [41/196], loss=66.6523
	step [42/196], loss=61.8123
	step [43/196], loss=60.2811
	step [44/196], loss=58.8181
	step [45/196], loss=65.6311
	step [46/196], loss=84.3689
	step [47/196], loss=84.9201
	step [48/196], loss=73.6630
	step [49/196], loss=79.0167
	step [50/196], loss=62.5685
	step [51/196], loss=71.8223
	step [52/196], loss=80.0221
	step [53/196], loss=69.3903
	step [54/196], loss=66.5011
	step [55/196], loss=74.3874
	step [56/196], loss=59.5599
	step [57/196], loss=73.9851
	step [58/196], loss=64.7570
	step [59/196], loss=72.5590
	step [60/196], loss=78.7292
	step [61/196], loss=80.1351
	step [62/196], loss=76.6617
	step [63/196], loss=74.8018
	step [64/196], loss=65.8722
	step [65/196], loss=58.5030
	step [66/196], loss=79.8899
	step [67/196], loss=53.6140
	step [68/196], loss=73.3473
	step [69/196], loss=73.3310
	step [70/196], loss=89.1635
	step [71/196], loss=62.0031
	step [72/196], loss=78.7929
	step [73/196], loss=74.5454
	step [74/196], loss=54.6818
	step [75/196], loss=92.0336
	step [76/196], loss=68.8586
	step [77/196], loss=79.1644
	step [78/196], loss=88.3343
	step [79/196], loss=74.2095
	step [80/196], loss=79.7629
	step [81/196], loss=87.7381
	step [82/196], loss=71.3905
	step [83/196], loss=68.6027
	step [84/196], loss=73.6718
	step [85/196], loss=86.2550
	step [86/196], loss=73.9842
	step [87/196], loss=57.9055
	step [88/196], loss=71.0730
	step [89/196], loss=76.7464
	step [90/196], loss=66.2001
	step [91/196], loss=78.1239
	step [92/196], loss=67.9793
	step [93/196], loss=75.5633
	step [94/196], loss=81.1895
	step [95/196], loss=78.0962
	step [96/196], loss=74.4040
	step [97/196], loss=60.4503
	step [98/196], loss=62.0372
	step [99/196], loss=66.9956
	step [100/196], loss=74.3242
	step [101/196], loss=81.6768
	step [102/196], loss=77.5908
	step [103/196], loss=69.7708
	step [104/196], loss=57.2773
	step [105/196], loss=66.5658
	step [106/196], loss=87.1097
	step [107/196], loss=62.8224
	step [108/196], loss=70.8786
	step [109/196], loss=68.7239
	step [110/196], loss=65.1875
	step [111/196], loss=71.5191
	step [112/196], loss=80.8691
	step [113/196], loss=79.4576
	step [114/196], loss=58.5866
	step [115/196], loss=87.0253
	step [116/196], loss=63.4565
	step [117/196], loss=76.5896
	step [118/196], loss=73.8304
	step [119/196], loss=80.3417
	step [120/196], loss=74.0827
	step [121/196], loss=77.0830
	step [122/196], loss=71.3405
	step [123/196], loss=78.9856
	step [124/196], loss=65.8752
	step [125/196], loss=83.3751
	step [126/196], loss=66.4513
	step [127/196], loss=84.2635
	step [128/196], loss=62.0815
	step [129/196], loss=73.2272
	step [130/196], loss=72.6132
	step [131/196], loss=66.6024
	step [132/196], loss=69.0238
	step [133/196], loss=61.8382
	step [134/196], loss=73.7895
	step [135/196], loss=70.1442
	step [136/196], loss=76.4383
	step [137/196], loss=64.4216
	step [138/196], loss=75.4510
	step [139/196], loss=68.0581
	step [140/196], loss=56.9466
	step [141/196], loss=66.0052
	step [142/196], loss=64.5325
	step [143/196], loss=72.3758
	step [144/196], loss=69.5628
	step [145/196], loss=84.6286
	step [146/196], loss=64.2103
	step [147/196], loss=86.0266
	step [148/196], loss=71.1239
	step [149/196], loss=76.5614
	step [150/196], loss=67.3369
	step [151/196], loss=78.1510
	step [152/196], loss=88.8641
	step [153/196], loss=64.8539
	step [154/196], loss=65.1985
	step [155/196], loss=62.9262
	step [156/196], loss=72.2006
	step [157/196], loss=73.2300
	step [158/196], loss=70.8392
	step [159/196], loss=66.7411
	step [160/196], loss=65.7307
	step [161/196], loss=80.3162
	step [162/196], loss=67.0810
	step [163/196], loss=67.0629
	step [164/196], loss=74.8749
	step [165/196], loss=80.9862
	step [166/196], loss=72.8005
	step [167/196], loss=80.2672
	step [168/196], loss=58.0050
	step [169/196], loss=71.0290
	step [170/196], loss=71.2622
	step [171/196], loss=74.1346
	step [172/196], loss=76.8016
	step [173/196], loss=83.0741
	step [174/196], loss=71.8904
	step [175/196], loss=75.6877
	step [176/196], loss=72.2840
	step [177/196], loss=74.4844
	step [178/196], loss=79.7122
	step [179/196], loss=66.7439
	step [180/196], loss=62.6656
	step [181/196], loss=72.5490
	step [182/196], loss=78.7956
	step [183/196], loss=60.2596
	step [184/196], loss=77.0583
	step [185/196], loss=63.8544
	step [186/196], loss=75.9869
	step [187/196], loss=72.5711
	step [188/196], loss=67.0375
	step [189/196], loss=69.7761
	step [190/196], loss=74.0569
	step [191/196], loss=75.8901
	step [192/196], loss=73.3981
	step [193/196], loss=76.9539
	step [194/196], loss=90.4377
	step [195/196], loss=76.2582
	step [196/196], loss=8.4168
	Evaluating
	loss=0.0070, precision=0.4200, recall=0.8550, f1=0.5633
Training epoch 78
	step [1/196], loss=85.5987
	step [2/196], loss=62.7900
	step [3/196], loss=57.6839
	step [4/196], loss=85.3963
	step [5/196], loss=83.2663
	step [6/196], loss=79.2791
	step [7/196], loss=74.2002
	step [8/196], loss=75.8135
	step [9/196], loss=76.4240
	step [10/196], loss=70.2348
	step [11/196], loss=66.2526
	step [12/196], loss=91.3551
	step [13/196], loss=73.8532
	step [14/196], loss=57.0247
	step [15/196], loss=75.6850
	step [16/196], loss=66.3835
	step [17/196], loss=60.3180
	step [18/196], loss=83.0749
	step [19/196], loss=70.9760
	step [20/196], loss=78.1314
	step [21/196], loss=67.5912
	step [22/196], loss=65.4856
	step [23/196], loss=73.2410
	step [24/196], loss=73.3007
	step [25/196], loss=67.3253
	step [26/196], loss=83.4077
	step [27/196], loss=73.1747
	step [28/196], loss=73.2394
	step [29/196], loss=68.0584
	step [30/196], loss=67.4638
	step [31/196], loss=73.6517
	step [32/196], loss=75.1922
	step [33/196], loss=60.5043
	step [34/196], loss=67.4033
	step [35/196], loss=81.4393
	step [36/196], loss=78.4609
	step [37/196], loss=69.0848
	step [38/196], loss=70.0305
	step [39/196], loss=83.1476
	step [40/196], loss=71.0815
	step [41/196], loss=87.0939
	step [42/196], loss=65.2885
	step [43/196], loss=74.2162
	step [44/196], loss=79.3674
	step [45/196], loss=79.5727
	step [46/196], loss=75.7415
	step [47/196], loss=77.1349
	step [48/196], loss=61.6202
	step [49/196], loss=74.9422
	step [50/196], loss=72.3687
	step [51/196], loss=87.6241
	step [52/196], loss=68.5242
	step [53/196], loss=69.3882
	step [54/196], loss=77.8383
	step [55/196], loss=70.1598
	step [56/196], loss=55.3096
	step [57/196], loss=73.8814
	step [58/196], loss=54.2114
	step [59/196], loss=79.2926
	step [60/196], loss=74.5791
	step [61/196], loss=71.4587
	step [62/196], loss=72.0720
	step [63/196], loss=62.7366
	step [64/196], loss=72.5754
	step [65/196], loss=69.7908
	step [66/196], loss=71.2217
	step [67/196], loss=66.5100
	step [68/196], loss=72.5409
	step [69/196], loss=67.5068
	step [70/196], loss=67.3431
	step [71/196], loss=86.8299
	step [72/196], loss=69.1873
	step [73/196], loss=73.9045
	step [74/196], loss=54.4785
	step [75/196], loss=75.1966
	step [76/196], loss=77.2620
	step [77/196], loss=66.4579
	step [78/196], loss=74.5656
	step [79/196], loss=71.7767
	step [80/196], loss=85.4693
	step [81/196], loss=67.7896
	step [82/196], loss=65.3336
	step [83/196], loss=77.0033
	step [84/196], loss=70.2471
	step [85/196], loss=71.8909
	step [86/196], loss=72.5786
	step [87/196], loss=69.5858
	step [88/196], loss=81.1870
	step [89/196], loss=68.1747
	step [90/196], loss=71.6546
	step [91/196], loss=63.9912
	step [92/196], loss=89.8153
	step [93/196], loss=64.3864
	step [94/196], loss=82.0361
	step [95/196], loss=78.9932
	step [96/196], loss=52.0654
	step [97/196], loss=74.4836
	step [98/196], loss=64.9105
	step [99/196], loss=65.1217
	step [100/196], loss=67.2724
	step [101/196], loss=65.2507
	step [102/196], loss=68.5177
	step [103/196], loss=85.0119
	step [104/196], loss=57.0061
	step [105/196], loss=73.4115
	step [106/196], loss=79.6987
	step [107/196], loss=68.2978
	step [108/196], loss=85.3989
	step [109/196], loss=72.9472
	step [110/196], loss=61.2574
	step [111/196], loss=80.5564
	step [112/196], loss=69.4518
	step [113/196], loss=75.7531
	step [114/196], loss=78.2836
	step [115/196], loss=66.6673
	step [116/196], loss=79.4872
	step [117/196], loss=68.1889
	step [118/196], loss=66.4478
	step [119/196], loss=69.1371
	step [120/196], loss=77.7726
	step [121/196], loss=83.3990
	step [122/196], loss=68.3653
	step [123/196], loss=65.6360
	step [124/196], loss=71.7226
	step [125/196], loss=66.1470
	step [126/196], loss=89.6486
	step [127/196], loss=80.9510
	step [128/196], loss=71.7259
	step [129/196], loss=82.7193
	step [130/196], loss=78.4631
	step [131/196], loss=67.5297
	step [132/196], loss=72.5118
	step [133/196], loss=70.1902
	step [134/196], loss=81.9879
	step [135/196], loss=76.0300
	step [136/196], loss=82.5900
	step [137/196], loss=67.4924
	step [138/196], loss=51.4328
	step [139/196], loss=78.0128
	step [140/196], loss=64.8669
	step [141/196], loss=68.5740
	step [142/196], loss=70.6618
	step [143/196], loss=71.9701
	step [144/196], loss=71.0584
	step [145/196], loss=69.1971
	step [146/196], loss=83.1765
	step [147/196], loss=63.7926
	step [148/196], loss=78.2102
	step [149/196], loss=67.8374
	step [150/196], loss=84.8983
	step [151/196], loss=79.7630
	step [152/196], loss=75.5900
	step [153/196], loss=64.3798
	step [154/196], loss=65.3177
	step [155/196], loss=77.7387
	step [156/196], loss=61.7047
	step [157/196], loss=69.2435
	step [158/196], loss=76.3832
	step [159/196], loss=73.7313
	step [160/196], loss=51.9736
	step [161/196], loss=75.0571
	step [162/196], loss=80.0724
	step [163/196], loss=75.8283
	step [164/196], loss=68.7387
	step [165/196], loss=72.5002
	step [166/196], loss=61.6909
	step [167/196], loss=68.5731
	step [168/196], loss=57.1334
	step [169/196], loss=81.5534
	step [170/196], loss=71.1339
	step [171/196], loss=82.7791
	step [172/196], loss=64.2441
	step [173/196], loss=65.6501
	step [174/196], loss=73.1245
	step [175/196], loss=62.8171
	step [176/196], loss=66.2645
	step [177/196], loss=83.1102
	step [178/196], loss=67.8066
	step [179/196], loss=75.1919
	step [180/196], loss=67.9184
	step [181/196], loss=63.3754
	step [182/196], loss=66.4906
	step [183/196], loss=63.2884
	step [184/196], loss=75.8860
	step [185/196], loss=64.4325
	step [186/196], loss=58.4396
	step [187/196], loss=68.0419
	step [188/196], loss=76.3501
	step [189/196], loss=77.0545
	step [190/196], loss=68.7028
	step [191/196], loss=64.5941
	step [192/196], loss=74.9578
	step [193/196], loss=67.5723
	step [194/196], loss=68.3661
	step [195/196], loss=69.0671
	step [196/196], loss=3.7391
	Evaluating
	loss=0.0065, precision=0.4479, recall=0.8613, f1=0.5893
Training epoch 79
	step [1/196], loss=74.2179
	step [2/196], loss=77.9330
	step [3/196], loss=77.8784
	step [4/196], loss=67.6292
	step [5/196], loss=81.8437
	step [6/196], loss=55.3976
	step [7/196], loss=67.2701
	step [8/196], loss=79.8929
	step [9/196], loss=63.0476
	step [10/196], loss=74.0970
	step [11/196], loss=77.5238
	step [12/196], loss=77.1489
	step [13/196], loss=68.2956
	step [14/196], loss=73.6129
	step [15/196], loss=76.5108
	step [16/196], loss=75.5789
	step [17/196], loss=67.6706
	step [18/196], loss=62.7526
	step [19/196], loss=60.3885
	step [20/196], loss=73.5759
	step [21/196], loss=63.7203
	step [22/196], loss=59.2468
	step [23/196], loss=69.5757
	step [24/196], loss=73.1037
	step [25/196], loss=72.6999
	step [26/196], loss=74.0233
	step [27/196], loss=65.3033
	step [28/196], loss=62.1117
	step [29/196], loss=78.1468
	step [30/196], loss=72.7293
	step [31/196], loss=63.3974
	step [32/196], loss=71.3241
	step [33/196], loss=85.4632
	step [34/196], loss=66.9512
	step [35/196], loss=68.1511
	step [36/196], loss=63.6253
	step [37/196], loss=83.9031
	step [38/196], loss=83.3375
	step [39/196], loss=84.5076
	step [40/196], loss=82.6033
	step [41/196], loss=72.7071
	step [42/196], loss=79.8732
	step [43/196], loss=71.7418
	step [44/196], loss=75.0293
	step [45/196], loss=68.2575
	step [46/196], loss=77.4854
	step [47/196], loss=62.3087
	step [48/196], loss=74.6157
	step [49/196], loss=79.7483
	step [50/196], loss=84.7359
	step [51/196], loss=79.4976
	step [52/196], loss=62.0996
	step [53/196], loss=69.3838
	step [54/196], loss=71.1984
	step [55/196], loss=64.8968
	step [56/196], loss=65.1010
	step [57/196], loss=66.2936
	step [58/196], loss=62.0027
	step [59/196], loss=78.5193
	step [60/196], loss=65.5836
	step [61/196], loss=68.6770
	step [62/196], loss=62.4582
	step [63/196], loss=57.1306
	step [64/196], loss=71.5772
	step [65/196], loss=73.4940
	step [66/196], loss=88.2712
	step [67/196], loss=74.9652
	step [68/196], loss=66.9045
	step [69/196], loss=83.0603
	step [70/196], loss=74.3186
	step [71/196], loss=66.3856
	step [72/196], loss=66.5156
	step [73/196], loss=83.1451
	step [74/196], loss=54.6100
	step [75/196], loss=62.6751
	step [76/196], loss=80.9454
	step [77/196], loss=85.0586
	step [78/196], loss=63.9647
	step [79/196], loss=78.7948
	step [80/196], loss=74.6539
	step [81/196], loss=72.1732
	step [82/196], loss=65.2557
	step [83/196], loss=67.2092
	step [84/196], loss=72.2417
	step [85/196], loss=86.8477
	step [86/196], loss=72.9425
	step [87/196], loss=77.1714
	step [88/196], loss=79.1694
	step [89/196], loss=62.7201
	step [90/196], loss=65.3557
	step [91/196], loss=76.5420
	step [92/196], loss=69.0530
	step [93/196], loss=75.1780
	step [94/196], loss=65.0142
	step [95/196], loss=66.8169
	step [96/196], loss=71.1996
	step [97/196], loss=56.8319
	step [98/196], loss=75.7814
	step [99/196], loss=63.8055
	step [100/196], loss=76.7141
	step [101/196], loss=70.1127
	step [102/196], loss=71.2526
	step [103/196], loss=75.3749
	step [104/196], loss=70.4814
	step [105/196], loss=72.0572
	step [106/196], loss=72.1694
	step [107/196], loss=78.3850
	step [108/196], loss=71.7015
	step [109/196], loss=63.5159
	step [110/196], loss=74.4073
	step [111/196], loss=57.5803
	step [112/196], loss=90.7479
	step [113/196], loss=71.2254
	step [114/196], loss=69.1255
	step [115/196], loss=65.7438
	step [116/196], loss=71.1532
	step [117/196], loss=85.7320
	step [118/196], loss=73.8908
	step [119/196], loss=69.6730
	step [120/196], loss=73.8514
	step [121/196], loss=82.5701
	step [122/196], loss=63.5462
	step [123/196], loss=81.2836
	step [124/196], loss=67.7964
	step [125/196], loss=79.7220
	step [126/196], loss=76.1679
	step [127/196], loss=72.4802
	step [128/196], loss=73.1975
	step [129/196], loss=82.1704
	step [130/196], loss=79.4145
	step [131/196], loss=72.9675
	step [132/196], loss=69.4755
	step [133/196], loss=64.2730
	step [134/196], loss=74.1941
	step [135/196], loss=76.9698
	step [136/196], loss=74.0763
	step [137/196], loss=89.4348
	step [138/196], loss=71.7263
	step [139/196], loss=72.9517
	step [140/196], loss=70.1600
	step [141/196], loss=67.6043
	step [142/196], loss=58.7173
	step [143/196], loss=72.3769
	step [144/196], loss=71.1384
	step [145/196], loss=62.8178
	step [146/196], loss=69.5683
	step [147/196], loss=64.8192
	step [148/196], loss=74.2776
	step [149/196], loss=64.9871
	step [150/196], loss=79.1955
	step [151/196], loss=75.7422
	step [152/196], loss=78.2033
	step [153/196], loss=57.7741
	step [154/196], loss=74.5505
	step [155/196], loss=71.9067
	step [156/196], loss=79.0746
	step [157/196], loss=57.9821
	step [158/196], loss=77.8821
	step [159/196], loss=69.7822
	step [160/196], loss=65.8955
	step [161/196], loss=63.2408
	step [162/196], loss=74.9479
	step [163/196], loss=69.7834
	step [164/196], loss=76.7813
	step [165/196], loss=71.3776
	step [166/196], loss=77.2980
	step [167/196], loss=61.6502
	step [168/196], loss=65.6382
	step [169/196], loss=76.2105
	step [170/196], loss=64.9240
	step [171/196], loss=71.5431
	step [172/196], loss=61.2766
	step [173/196], loss=80.2496
	step [174/196], loss=62.7471
	step [175/196], loss=64.0445
	step [176/196], loss=68.4509
	step [177/196], loss=67.2453
	step [178/196], loss=65.6275
	step [179/196], loss=85.6120
	step [180/196], loss=64.2094
	step [181/196], loss=80.9543
	step [182/196], loss=69.8991
	step [183/196], loss=74.2495
	step [184/196], loss=79.6068
	step [185/196], loss=81.9032
	step [186/196], loss=69.3727
	step [187/196], loss=82.3125
	step [188/196], loss=72.4960
	step [189/196], loss=76.9404
	step [190/196], loss=73.8711
	step [191/196], loss=74.5564
	step [192/196], loss=71.8512
	step [193/196], loss=68.4121
	step [194/196], loss=71.8964
	step [195/196], loss=72.2084
	step [196/196], loss=2.9455
	Evaluating
	loss=0.0067, precision=0.4293, recall=0.8660, f1=0.5740
Training epoch 80
	step [1/196], loss=81.4295
	step [2/196], loss=74.3770
	step [3/196], loss=59.1225
	step [4/196], loss=75.0667
	step [5/196], loss=72.1877
	step [6/196], loss=89.3688
	step [7/196], loss=76.1183
	step [8/196], loss=80.0249
	step [9/196], loss=61.4948
	step [10/196], loss=72.1456
	step [11/196], loss=65.8799
	step [12/196], loss=72.8732
	step [13/196], loss=68.4862
	step [14/196], loss=63.4685
	step [15/196], loss=77.2066
	step [16/196], loss=67.1188
	step [17/196], loss=68.4914
	step [18/196], loss=68.4323
	step [19/196], loss=70.8365
	step [20/196], loss=61.9795
	step [21/196], loss=82.5431
	step [22/196], loss=63.2898
	step [23/196], loss=55.6798
	step [24/196], loss=68.5079
	step [25/196], loss=76.5407
	step [26/196], loss=77.6894
	step [27/196], loss=70.5146
	step [28/196], loss=73.0005
	step [29/196], loss=70.2200
	step [30/196], loss=63.2858
	step [31/196], loss=62.1947
	step [32/196], loss=66.5527
	step [33/196], loss=70.8372
	step [34/196], loss=66.7237
	step [35/196], loss=67.3509
	step [36/196], loss=73.7638
	step [37/196], loss=68.3567
	step [38/196], loss=69.0060
	step [39/196], loss=76.5159
	step [40/196], loss=69.4183
	step [41/196], loss=63.8979
	step [42/196], loss=68.8646
	step [43/196], loss=59.1871
	step [44/196], loss=75.7682
	step [45/196], loss=74.6456
	step [46/196], loss=64.6481
	step [47/196], loss=70.9515
	step [48/196], loss=79.3527
	step [49/196], loss=71.3870
	step [50/196], loss=65.1502
	step [51/196], loss=70.9815
	step [52/196], loss=74.7061
	step [53/196], loss=82.6969
	step [54/196], loss=60.9842
	step [55/196], loss=72.0410
	step [56/196], loss=78.7694
	step [57/196], loss=72.1606
	step [58/196], loss=69.5707
	step [59/196], loss=74.3863
	step [60/196], loss=66.3300
	step [61/196], loss=71.4045
	step [62/196], loss=66.9936
	step [63/196], loss=72.4080
	step [64/196], loss=62.9713
	step [65/196], loss=72.6592
	step [66/196], loss=65.1961
	step [67/196], loss=77.2624
	step [68/196], loss=68.9343
	step [69/196], loss=71.9123
	step [70/196], loss=82.4125
	step [71/196], loss=86.3605
	step [72/196], loss=64.5826
	step [73/196], loss=63.2835
	step [74/196], loss=72.1951
	step [75/196], loss=67.8190
	step [76/196], loss=78.2951
	step [77/196], loss=74.4847
	step [78/196], loss=69.4742
	step [79/196], loss=68.3805
	step [80/196], loss=77.4259
	step [81/196], loss=78.5652
	step [82/196], loss=68.8611
	step [83/196], loss=66.8127
	step [84/196], loss=66.9959
	step [85/196], loss=74.8396
	step [86/196], loss=68.8916
	step [87/196], loss=74.0166
	step [88/196], loss=69.1010
	step [89/196], loss=71.3376
	step [90/196], loss=66.9689
	step [91/196], loss=82.3243
	step [92/196], loss=70.7644
	step [93/196], loss=75.7276
	step [94/196], loss=73.6248
	step [95/196], loss=75.1817
	step [96/196], loss=84.5450
	step [97/196], loss=66.1267
	step [98/196], loss=62.4105
	step [99/196], loss=67.0190
	step [100/196], loss=76.9206
	step [101/196], loss=82.5824
	step [102/196], loss=62.8874
	step [103/196], loss=75.8336
	step [104/196], loss=74.6841
	step [105/196], loss=57.9193
	step [106/196], loss=88.5142
	step [107/196], loss=93.2008
	step [108/196], loss=72.9290
	step [109/196], loss=64.9070
	step [110/196], loss=64.0283
	step [111/196], loss=77.5083
	step [112/196], loss=64.3033
	step [113/196], loss=69.2110
	step [114/196], loss=69.2901
	step [115/196], loss=63.9259
	step [116/196], loss=77.6710
	step [117/196], loss=91.1590
	step [118/196], loss=58.7853
	step [119/196], loss=76.2378
	step [120/196], loss=83.0703
	step [121/196], loss=59.2627
	step [122/196], loss=73.5204
	step [123/196], loss=74.2061
	step [124/196], loss=66.7206
	step [125/196], loss=66.0641
	step [126/196], loss=68.2199
	step [127/196], loss=72.7262
	step [128/196], loss=75.8807
	step [129/196], loss=78.2639
	step [130/196], loss=73.9073
	step [131/196], loss=59.5697
	step [132/196], loss=67.2957
	step [133/196], loss=67.9766
	step [134/196], loss=68.2043
	step [135/196], loss=72.9427
	step [136/196], loss=71.1725
	step [137/196], loss=77.3940
	step [138/196], loss=62.1422
	step [139/196], loss=74.1937
	step [140/196], loss=77.1719
	step [141/196], loss=65.2709
	step [142/196], loss=74.4381
	step [143/196], loss=76.9041
	step [144/196], loss=73.0691
	step [145/196], loss=78.3077
	step [146/196], loss=64.6595
	step [147/196], loss=70.3330
	step [148/196], loss=75.8924
	step [149/196], loss=68.5240
	step [150/196], loss=76.1664
	step [151/196], loss=75.8457
	step [152/196], loss=65.6786
	step [153/196], loss=69.7145
	step [154/196], loss=66.8398
	step [155/196], loss=82.6097
	step [156/196], loss=63.1650
	step [157/196], loss=65.7869
	step [158/196], loss=64.2348
	step [159/196], loss=73.3654
	step [160/196], loss=79.2006
	step [161/196], loss=75.8083
	step [162/196], loss=75.0438
	step [163/196], loss=75.2547
	step [164/196], loss=76.1244
	step [165/196], loss=70.8168
	step [166/196], loss=77.4948
	step [167/196], loss=71.9011
	step [168/196], loss=75.1075
	step [169/196], loss=61.1285
	step [170/196], loss=56.8246
	step [171/196], loss=70.8296
	step [172/196], loss=64.1438
	step [173/196], loss=73.2826
	step [174/196], loss=74.9077
	step [175/196], loss=73.7012
	step [176/196], loss=85.0803
	step [177/196], loss=64.0261
	step [178/196], loss=87.5097
	step [179/196], loss=63.6552
	step [180/196], loss=72.6607
	step [181/196], loss=84.5632
	step [182/196], loss=80.9872
	step [183/196], loss=75.4068
	step [184/196], loss=74.5289
	step [185/196], loss=77.9633
	step [186/196], loss=70.9303
	step [187/196], loss=64.2929
	step [188/196], loss=73.9612
	step [189/196], loss=66.2834
	step [190/196], loss=80.0838
	step [191/196], loss=62.6757
	step [192/196], loss=78.5803
	step [193/196], loss=66.8320
	step [194/196], loss=76.4029
	step [195/196], loss=71.2244
	step [196/196], loss=7.4958
	Evaluating
	loss=0.0069, precision=0.4231, recall=0.8534, f1=0.5657
Training epoch 81
	step [1/196], loss=66.1457
	step [2/196], loss=67.2687
	step [3/196], loss=68.3669
	step [4/196], loss=74.4160
	step [5/196], loss=61.8891
	step [6/196], loss=72.7411
	step [7/196], loss=80.9794
	step [8/196], loss=60.5617
	step [9/196], loss=63.3530
	step [10/196], loss=65.7617
	step [11/196], loss=63.8629
	step [12/196], loss=77.2547
	step [13/196], loss=84.8815
	step [14/196], loss=69.5148
	step [15/196], loss=64.6678
	step [16/196], loss=73.6056
	step [17/196], loss=71.2517
	step [18/196], loss=73.2677
	step [19/196], loss=79.4374
	step [20/196], loss=70.2830
	step [21/196], loss=66.2348
	step [22/196], loss=67.7589
	step [23/196], loss=69.7111
	step [24/196], loss=63.1391
	step [25/196], loss=65.9497
	step [26/196], loss=65.6615
	step [27/196], loss=75.7378
	step [28/196], loss=79.0107
	step [29/196], loss=70.5682
	step [30/196], loss=68.2537
	step [31/196], loss=61.6124
	step [32/196], loss=66.2341
	step [33/196], loss=70.6491
	step [34/196], loss=68.7247
	step [35/196], loss=72.5588
	step [36/196], loss=71.0071
	step [37/196], loss=69.6811
	step [38/196], loss=67.0359
	step [39/196], loss=66.8419
	step [40/196], loss=70.7683
	step [41/196], loss=70.9802
	step [42/196], loss=64.9500
	step [43/196], loss=75.0073
	step [44/196], loss=65.0316
	step [45/196], loss=87.0391
	step [46/196], loss=71.4135
	step [47/196], loss=66.1538
	step [48/196], loss=69.2780
	step [49/196], loss=91.0319
	step [50/196], loss=68.3103
	step [51/196], loss=65.5949
	step [52/196], loss=52.1039
	step [53/196], loss=88.2638
	step [54/196], loss=71.7980
	step [55/196], loss=73.8140
	step [56/196], loss=61.7693
	step [57/196], loss=69.1007
	step [58/196], loss=65.2774
	step [59/196], loss=85.0959
	step [60/196], loss=52.2329
	step [61/196], loss=65.0621
	step [62/196], loss=71.5396
	step [63/196], loss=75.6355
	step [64/196], loss=59.3882
	step [65/196], loss=61.7748
	step [66/196], loss=77.9443
	step [67/196], loss=78.6750
	step [68/196], loss=62.7217
	step [69/196], loss=80.2079
	step [70/196], loss=76.1205
	step [71/196], loss=67.3381
	step [72/196], loss=68.9972
	step [73/196], loss=64.2116
	step [74/196], loss=80.6394
	step [75/196], loss=73.9927
	step [76/196], loss=71.1445
	step [77/196], loss=60.8710
	step [78/196], loss=72.9492
	step [79/196], loss=69.5948
	step [80/196], loss=76.6640
	step [81/196], loss=72.9641
	step [82/196], loss=73.0698
	step [83/196], loss=70.1128
	step [84/196], loss=71.5196
	step [85/196], loss=62.1994
	step [86/196], loss=80.6754
	step [87/196], loss=75.6608
	step [88/196], loss=77.9662
	step [89/196], loss=82.2707
	step [90/196], loss=74.3360
	step [91/196], loss=64.0818
	step [92/196], loss=69.9577
	step [93/196], loss=77.4687
	step [94/196], loss=77.2679
	step [95/196], loss=74.6829
	step [96/196], loss=67.6619
	step [97/196], loss=74.2806
	step [98/196], loss=63.1508
	step [99/196], loss=64.7796
	step [100/196], loss=77.9380
	step [101/196], loss=62.9784
	step [102/196], loss=76.4359
	step [103/196], loss=72.4725
	step [104/196], loss=68.3292
	step [105/196], loss=70.9216
	step [106/196], loss=73.9364
	step [107/196], loss=81.1888
	step [108/196], loss=70.6773
	step [109/196], loss=66.2878
	step [110/196], loss=70.2690
	step [111/196], loss=63.6215
	step [112/196], loss=69.2355
	step [113/196], loss=63.7467
	step [114/196], loss=67.6354
	step [115/196], loss=86.9154
	step [116/196], loss=59.6660
	step [117/196], loss=71.3904
	step [118/196], loss=68.2024
	step [119/196], loss=75.8875
	step [120/196], loss=79.1529
	step [121/196], loss=70.0892
	step [122/196], loss=74.0403
	step [123/196], loss=82.4672
	step [124/196], loss=75.6765
	step [125/196], loss=81.0700
	step [126/196], loss=73.8489
	step [127/196], loss=69.6730
	step [128/196], loss=77.8583
	step [129/196], loss=71.1601
	step [130/196], loss=74.4860
	step [131/196], loss=73.4493
	step [132/196], loss=75.9435
	step [133/196], loss=75.0105
	step [134/196], loss=59.6207
	step [135/196], loss=68.5066
	step [136/196], loss=84.3445
	step [137/196], loss=66.0802
	step [138/196], loss=73.2289
	step [139/196], loss=67.9694
	step [140/196], loss=83.7529
	step [141/196], loss=77.5714
	step [142/196], loss=79.9068
	step [143/196], loss=63.2227
	step [144/196], loss=67.5176
	step [145/196], loss=56.2992
	step [146/196], loss=84.3037
	step [147/196], loss=79.6670
	step [148/196], loss=72.7912
	step [149/196], loss=60.8687
	step [150/196], loss=72.3304
	step [151/196], loss=73.0540
	step [152/196], loss=58.3336
	step [153/196], loss=83.0412
	step [154/196], loss=79.3722
	step [155/196], loss=71.0264
	step [156/196], loss=63.6323
	step [157/196], loss=60.0759
	step [158/196], loss=70.7986
	step [159/196], loss=73.6775
	step [160/196], loss=72.4885
	step [161/196], loss=73.1537
	step [162/196], loss=73.1073
	step [163/196], loss=72.7745
	step [164/196], loss=67.4671
	step [165/196], loss=73.0192
	step [166/196], loss=79.5050
	step [167/196], loss=73.1098
	step [168/196], loss=66.3927
	step [169/196], loss=73.0297
	step [170/196], loss=71.5588
	step [171/196], loss=62.3910
	step [172/196], loss=63.7454
	step [173/196], loss=68.4332
	step [174/196], loss=86.4691
	step [175/196], loss=73.3667
	step [176/196], loss=88.3508
	step [177/196], loss=87.7271
	step [178/196], loss=75.3574
	step [179/196], loss=75.9508
	step [180/196], loss=74.0039
	step [181/196], loss=73.6933
	step [182/196], loss=66.1871
	step [183/196], loss=72.7674
	step [184/196], loss=80.2894
	step [185/196], loss=55.5431
	step [186/196], loss=86.2871
	step [187/196], loss=64.5619
	step [188/196], loss=71.9855
	step [189/196], loss=81.2583
	step [190/196], loss=65.3421
	step [191/196], loss=68.4330
	step [192/196], loss=82.0237
	step [193/196], loss=59.9922
	step [194/196], loss=58.8444
	step [195/196], loss=78.4029
	step [196/196], loss=7.2090
	Evaluating
	loss=0.0083, precision=0.3694, recall=0.7998, f1=0.5054
Training epoch 82
	step [1/196], loss=63.5313
	step [2/196], loss=60.8148
	step [3/196], loss=69.5780
	step [4/196], loss=79.5796
	step [5/196], loss=59.8239
	step [6/196], loss=75.8041
	step [7/196], loss=80.6135
	step [8/196], loss=63.0618
	step [9/196], loss=67.9194
	step [10/196], loss=63.9098
	step [11/196], loss=61.5143
	step [12/196], loss=80.4289
	step [13/196], loss=80.5758
	step [14/196], loss=59.9986
	step [15/196], loss=53.6983
	step [16/196], loss=85.2903
	step [17/196], loss=76.2679
	step [18/196], loss=68.9620
	step [19/196], loss=76.0582
	step [20/196], loss=77.7744
	step [21/196], loss=78.8975
	step [22/196], loss=86.6408
	step [23/196], loss=79.2988
	step [24/196], loss=80.8056
	step [25/196], loss=62.8614
	step [26/196], loss=78.0300
	step [27/196], loss=75.5772
	step [28/196], loss=64.8537
	step [29/196], loss=74.3331
	step [30/196], loss=61.8204
	step [31/196], loss=59.9419
	step [32/196], loss=58.4275
	step [33/196], loss=76.8569
	step [34/196], loss=70.0072
	step [35/196], loss=79.4816
	step [36/196], loss=67.6464
	step [37/196], loss=81.2037
	step [38/196], loss=75.8473
	step [39/196], loss=67.5896
	step [40/196], loss=75.4649
	step [41/196], loss=85.1330
	step [42/196], loss=73.1770
	step [43/196], loss=83.9664
	step [44/196], loss=73.7760
	step [45/196], loss=62.1485
	step [46/196], loss=62.8021
	step [47/196], loss=58.1403
	step [48/196], loss=76.5000
	step [49/196], loss=68.8665
	step [50/196], loss=68.9104
	step [51/196], loss=67.6376
	step [52/196], loss=64.5977
	step [53/196], loss=83.8226
	step [54/196], loss=57.4394
	step [55/196], loss=78.1105
	step [56/196], loss=72.5365
	step [57/196], loss=68.9219
	step [58/196], loss=75.5619
	step [59/196], loss=65.1205
	step [60/196], loss=69.7029
	step [61/196], loss=65.7591
	step [62/196], loss=82.4242
	step [63/196], loss=70.7501
	step [64/196], loss=65.7567
	step [65/196], loss=65.6939
	step [66/196], loss=67.9726
	step [67/196], loss=64.1927
	step [68/196], loss=74.7111
	step [69/196], loss=71.4165
	step [70/196], loss=72.7754
	step [71/196], loss=67.3916
	step [72/196], loss=54.9928
	step [73/196], loss=80.6526
	step [74/196], loss=59.6629
	step [75/196], loss=70.7206
	step [76/196], loss=70.9969
	step [77/196], loss=85.3764
	step [78/196], loss=62.1259
	step [79/196], loss=77.2616
	step [80/196], loss=67.5341
	step [81/196], loss=82.4715
	step [82/196], loss=71.5421
	step [83/196], loss=59.0890
	step [84/196], loss=72.7973
	step [85/196], loss=70.9879
	step [86/196], loss=72.8615
	step [87/196], loss=68.7369
	step [88/196], loss=72.6474
	step [89/196], loss=71.5866
	step [90/196], loss=73.5548
	step [91/196], loss=72.3966
	step [92/196], loss=80.0319
	step [93/196], loss=82.8423
	step [94/196], loss=62.1773
	step [95/196], loss=65.3159
	step [96/196], loss=69.7052
	step [97/196], loss=72.8126
	step [98/196], loss=79.1174
	step [99/196], loss=71.9908
	step [100/196], loss=73.2095
	step [101/196], loss=69.5800
	step [102/196], loss=63.5948
	step [103/196], loss=70.2401
	step [104/196], loss=75.9038
	step [105/196], loss=67.3027
	step [106/196], loss=74.5598
	step [107/196], loss=70.4311
	step [108/196], loss=48.9331
	step [109/196], loss=75.0562
	step [110/196], loss=63.9271
	step [111/196], loss=75.1048
	step [112/196], loss=66.9051
	step [113/196], loss=66.6530
	step [114/196], loss=66.8651
	step [115/196], loss=60.9543
	step [116/196], loss=72.1961
	step [117/196], loss=76.0900
	step [118/196], loss=72.7403
	step [119/196], loss=66.7375
	step [120/196], loss=88.3983
	step [121/196], loss=63.6566
	step [122/196], loss=75.8265
	step [123/196], loss=87.8114
	step [124/196], loss=66.1668
	step [125/196], loss=67.5591
	step [126/196], loss=80.3041
	step [127/196], loss=67.6205
	step [128/196], loss=67.3654
	step [129/196], loss=88.2838
	step [130/196], loss=73.8756
	step [131/196], loss=78.2397
	step [132/196], loss=73.3072
	step [133/196], loss=67.1205
	step [134/196], loss=71.4040
	step [135/196], loss=70.2081
	step [136/196], loss=76.0737
	step [137/196], loss=61.8690
	step [138/196], loss=71.1248
	step [139/196], loss=74.6541
	step [140/196], loss=79.6504
	step [141/196], loss=59.8067
	step [142/196], loss=73.5165
	step [143/196], loss=68.1379
	step [144/196], loss=66.2229
	step [145/196], loss=74.8904
	step [146/196], loss=70.2685
	step [147/196], loss=84.1475
	step [148/196], loss=72.6498
	step [149/196], loss=61.0147
	step [150/196], loss=64.1296
	step [151/196], loss=69.7427
	step [152/196], loss=67.5742
	step [153/196], loss=74.4323
	step [154/196], loss=69.5949
	step [155/196], loss=74.7461
	step [156/196], loss=69.0959
	step [157/196], loss=76.8411
	step [158/196], loss=61.3313
	step [159/196], loss=81.3213
	step [160/196], loss=68.8314
	step [161/196], loss=75.8962
	step [162/196], loss=68.7228
	step [163/196], loss=78.0773
	step [164/196], loss=74.5229
	step [165/196], loss=74.7219
	step [166/196], loss=78.6119
	step [167/196], loss=73.1523
	step [168/196], loss=68.2837
	step [169/196], loss=78.0055
	step [170/196], loss=69.9731
	step [171/196], loss=71.6557
	step [172/196], loss=66.7893
	step [173/196], loss=78.3617
	step [174/196], loss=82.5442
	step [175/196], loss=67.8551
	step [176/196], loss=64.6077
	step [177/196], loss=69.3382
	step [178/196], loss=64.1725
	step [179/196], loss=68.3534
	step [180/196], loss=70.1047
	step [181/196], loss=83.8149
	step [182/196], loss=68.9775
	step [183/196], loss=71.6999
	step [184/196], loss=79.0038
	step [185/196], loss=68.8733
	step [186/196], loss=72.4135
	step [187/196], loss=81.2327
	step [188/196], loss=70.6673
	step [189/196], loss=56.8832
	step [190/196], loss=59.3705
	step [191/196], loss=87.8408
	step [192/196], loss=68.3889
	step [193/196], loss=76.9976
	step [194/196], loss=60.9166
	step [195/196], loss=79.5492
	step [196/196], loss=6.7572
	Evaluating
	loss=0.0065, precision=0.4377, recall=0.8375, f1=0.5750
Training epoch 83
	step [1/196], loss=72.7227
	step [2/196], loss=78.2259
	step [3/196], loss=70.0289
	step [4/196], loss=73.1746
	step [5/196], loss=61.7563
	step [6/196], loss=76.7788
	step [7/196], loss=66.2317
	step [8/196], loss=67.8694
	step [9/196], loss=73.3057
	step [10/196], loss=61.1652
	step [11/196], loss=66.7429
	step [12/196], loss=59.2500
	step [13/196], loss=71.3595
	step [14/196], loss=73.3620
	step [15/196], loss=81.6386
	step [16/196], loss=74.3644
	step [17/196], loss=81.8520
	step [18/196], loss=69.6942
	step [19/196], loss=78.6109
	step [20/196], loss=68.5776
	step [21/196], loss=66.4462
	step [22/196], loss=64.5811
	step [23/196], loss=81.7578
	step [24/196], loss=80.2960
	step [25/196], loss=71.1937
	step [26/196], loss=63.8831
	step [27/196], loss=61.7504
	step [28/196], loss=62.0988
	step [29/196], loss=54.1259
	step [30/196], loss=68.9268
	step [31/196], loss=74.4839
	step [32/196], loss=69.2515
	step [33/196], loss=78.0113
	step [34/196], loss=73.0360
	step [35/196], loss=71.2066
	step [36/196], loss=75.5072
	step [37/196], loss=77.9156
	step [38/196], loss=74.0513
	step [39/196], loss=78.2799
	step [40/196], loss=64.4332
	step [41/196], loss=54.0409
	step [42/196], loss=63.8853
	step [43/196], loss=73.6777
	step [44/196], loss=72.3515
	step [45/196], loss=68.3271
	step [46/196], loss=73.9395
	step [47/196], loss=62.1460
	step [48/196], loss=82.7609
	step [49/196], loss=86.5906
	step [50/196], loss=79.3485
	step [51/196], loss=77.1034
	step [52/196], loss=80.7071
	step [53/196], loss=65.5470
	step [54/196], loss=65.6413
	step [55/196], loss=63.6266
	step [56/196], loss=68.7632
	step [57/196], loss=62.6151
	step [58/196], loss=77.4465
	step [59/196], loss=83.8775
	step [60/196], loss=65.3257
	step [61/196], loss=62.0857
	step [62/196], loss=56.5581
	step [63/196], loss=76.4754
	step [64/196], loss=71.4072
	step [65/196], loss=82.8645
	step [66/196], loss=64.4980
	step [67/196], loss=73.6197
	step [68/196], loss=64.1738
	step [69/196], loss=75.2792
	step [70/196], loss=70.6822
	step [71/196], loss=83.3580
	step [72/196], loss=68.6167
	step [73/196], loss=72.2969
	step [74/196], loss=65.6872
	step [75/196], loss=58.6465
	step [76/196], loss=73.3834
	step [77/196], loss=89.3104
	step [78/196], loss=72.6850
	step [79/196], loss=74.7354
	step [80/196], loss=60.5085
	step [81/196], loss=59.0777
	step [82/196], loss=73.1885
	step [83/196], loss=73.4430
	step [84/196], loss=72.7328
	step [85/196], loss=83.1024
	step [86/196], loss=69.1000
	step [87/196], loss=70.4661
	step [88/196], loss=67.8362
	step [89/196], loss=58.5749
	step [90/196], loss=64.2188
	step [91/196], loss=76.9071
	step [92/196], loss=70.6717
	step [93/196], loss=72.6784
	step [94/196], loss=66.5913
	step [95/196], loss=80.6171
	step [96/196], loss=70.7748
	step [97/196], loss=94.2020
	step [98/196], loss=73.1140
	step [99/196], loss=69.8289
	step [100/196], loss=85.2128
	step [101/196], loss=65.5120
	step [102/196], loss=72.6047
	step [103/196], loss=83.1295
	step [104/196], loss=81.8274
	step [105/196], loss=78.1966
	step [106/196], loss=66.1737
	step [107/196], loss=57.1299
	step [108/196], loss=66.0546
	step [109/196], loss=68.4909
	step [110/196], loss=76.1098
	step [111/196], loss=70.6624
	step [112/196], loss=71.9424
	step [113/196], loss=74.4478
	step [114/196], loss=82.1642
	step [115/196], loss=67.2548
	step [116/196], loss=64.1079
	step [117/196], loss=75.0115
	step [118/196], loss=76.7779
	step [119/196], loss=70.8777
	step [120/196], loss=85.2788
	step [121/196], loss=58.9786
	step [122/196], loss=73.2117
	step [123/196], loss=65.1676
	step [124/196], loss=68.9069
	step [125/196], loss=65.5763
	step [126/196], loss=57.6620
	step [127/196], loss=73.8567
	step [128/196], loss=66.7256
	step [129/196], loss=73.9325
	step [130/196], loss=71.8780
	step [131/196], loss=82.5762
	step [132/196], loss=67.7606
	step [133/196], loss=68.3468
	step [134/196], loss=62.4677
	step [135/196], loss=76.9526
	step [136/196], loss=77.4613
	step [137/196], loss=70.9697
	step [138/196], loss=77.0407
	step [139/196], loss=61.6872
	step [140/196], loss=87.9733
	step [141/196], loss=80.1720
	step [142/196], loss=78.6208
	step [143/196], loss=71.7971
	step [144/196], loss=54.1235
	step [145/196], loss=62.8681
	step [146/196], loss=71.1263
	step [147/196], loss=73.6065
	step [148/196], loss=56.4296
	step [149/196], loss=68.7648
	step [150/196], loss=73.0419
	step [151/196], loss=87.2602
	step [152/196], loss=71.6242
	step [153/196], loss=73.1068
	step [154/196], loss=75.2254
	step [155/196], loss=75.7849
	step [156/196], loss=72.2964
	step [157/196], loss=79.7007
	step [158/196], loss=77.1395
	step [159/196], loss=67.3953
	step [160/196], loss=56.3314
	step [161/196], loss=68.8721
	step [162/196], loss=72.7890
	step [163/196], loss=75.7374
	step [164/196], loss=69.3213
	step [165/196], loss=78.7325
	step [166/196], loss=71.1700
	step [167/196], loss=62.2169
	step [168/196], loss=68.9703
	step [169/196], loss=59.2897
	step [170/196], loss=74.2988
	step [171/196], loss=70.8609
	step [172/196], loss=74.3966
	step [173/196], loss=67.2526
	step [174/196], loss=72.7587
	step [175/196], loss=62.6440
	step [176/196], loss=69.0089
	step [177/196], loss=67.1848
	step [178/196], loss=63.4840
	step [179/196], loss=65.1848
	step [180/196], loss=65.2803
	step [181/196], loss=71.4824
	step [182/196], loss=70.2838
	step [183/196], loss=55.5828
	step [184/196], loss=78.5401
	step [185/196], loss=68.2987
	step [186/196], loss=74.6468
	step [187/196], loss=63.8996
	step [188/196], loss=72.2790
	step [189/196], loss=70.9337
	step [190/196], loss=67.0095
	step [191/196], loss=78.6822
	step [192/196], loss=60.9579
	step [193/196], loss=72.1773
	step [194/196], loss=58.7342
	step [195/196], loss=60.1476
	step [196/196], loss=5.0438
	Evaluating
	loss=0.0066, precision=0.4323, recall=0.8577, f1=0.5748
Training epoch 84
	step [1/196], loss=72.6571
	step [2/196], loss=73.6896
	step [3/196], loss=67.9140
	step [4/196], loss=73.7740
	step [5/196], loss=74.6512
	step [6/196], loss=67.7287
	step [7/196], loss=65.5950
	step [8/196], loss=63.0606
	step [9/196], loss=65.0726
	step [10/196], loss=55.6217
	step [11/196], loss=79.2309
	step [12/196], loss=68.0552
	step [13/196], loss=77.9203
	step [14/196], loss=64.1474
	step [15/196], loss=82.5305
	step [16/196], loss=88.1014
	step [17/196], loss=70.4582
	step [18/196], loss=64.8361
	step [19/196], loss=65.2911
	step [20/196], loss=80.6029
	step [21/196], loss=66.3775
	step [22/196], loss=66.1571
	step [23/196], loss=67.5714
	step [24/196], loss=73.4806
	step [25/196], loss=78.2436
	step [26/196], loss=66.6597
	step [27/196], loss=73.2225
	step [28/196], loss=69.9012
	step [29/196], loss=72.2864
	step [30/196], loss=79.8239
	step [31/196], loss=73.4035
	step [32/196], loss=65.5865
	step [33/196], loss=69.3781
	step [34/196], loss=60.0376
	step [35/196], loss=70.3484
	step [36/196], loss=78.9893
	step [37/196], loss=69.6565
	step [38/196], loss=75.1122
	step [39/196], loss=59.3376
	step [40/196], loss=77.2884
	step [41/196], loss=69.8293
	step [42/196], loss=59.8355
	step [43/196], loss=76.8946
	step [44/196], loss=68.2167
	step [45/196], loss=55.5982
	step [46/196], loss=62.6046
	step [47/196], loss=70.7479
	step [48/196], loss=75.2059
	step [49/196], loss=63.5258
	step [50/196], loss=65.7634
	step [51/196], loss=77.3673
	step [52/196], loss=79.3210
	step [53/196], loss=80.0644
	step [54/196], loss=71.0069
	step [55/196], loss=57.9201
	step [56/196], loss=68.8901
	step [57/196], loss=68.6952
	step [58/196], loss=73.4988
	step [59/196], loss=72.3414
	step [60/196], loss=61.7562
	step [61/196], loss=69.1795
	step [62/196], loss=76.2818
	step [63/196], loss=68.9194
	step [64/196], loss=72.0123
	step [65/196], loss=58.6288
	step [66/196], loss=73.8673
	step [67/196], loss=74.6675
	step [68/196], loss=87.4008
	step [69/196], loss=70.1037
	step [70/196], loss=65.9045
	step [71/196], loss=68.9876
	step [72/196], loss=81.2682
	step [73/196], loss=74.7753
	step [74/196], loss=82.9839
	step [75/196], loss=66.0498
	step [76/196], loss=72.3581
	step [77/196], loss=76.7550
	step [78/196], loss=64.7272
	step [79/196], loss=75.4874
	step [80/196], loss=79.7922
	step [81/196], loss=68.9871
	step [82/196], loss=67.9499
	step [83/196], loss=83.5886
	step [84/196], loss=63.2356
	step [85/196], loss=85.4040
	step [86/196], loss=66.0355
	step [87/196], loss=61.3966
	step [88/196], loss=61.3361
	step [89/196], loss=77.5830
	step [90/196], loss=72.7542
	step [91/196], loss=76.2015
	step [92/196], loss=85.5345
	step [93/196], loss=64.5576
	step [94/196], loss=69.0851
	step [95/196], loss=63.5750
	step [96/196], loss=75.0402
	step [97/196], loss=71.1268
	step [98/196], loss=69.6487
	step [99/196], loss=61.7711
	step [100/196], loss=79.1082
	step [101/196], loss=75.7968
	step [102/196], loss=69.6688
	step [103/196], loss=65.9121
	step [104/196], loss=75.0819
	step [105/196], loss=59.9943
	step [106/196], loss=64.9751
	step [107/196], loss=64.7043
	step [108/196], loss=67.5270
	step [109/196], loss=69.7675
	step [110/196], loss=60.9281
	step [111/196], loss=84.4420
	step [112/196], loss=70.9784
	step [113/196], loss=76.0238
	step [114/196], loss=65.3291
	step [115/196], loss=69.8752
	step [116/196], loss=67.5187
	step [117/196], loss=73.0422
	step [118/196], loss=81.9348
	step [119/196], loss=83.1515
	step [120/196], loss=78.4114
	step [121/196], loss=64.9598
	step [122/196], loss=63.4373
	step [123/196], loss=64.1335
	step [124/196], loss=68.3326
	step [125/196], loss=69.5082
	step [126/196], loss=74.8293
	step [127/196], loss=83.2925
	step [128/196], loss=79.6627
	step [129/196], loss=62.9441
	step [130/196], loss=68.1304
	step [131/196], loss=60.9763
	step [132/196], loss=65.5997
	step [133/196], loss=71.6497
	step [134/196], loss=56.1922
	step [135/196], loss=75.8253
	step [136/196], loss=70.4143
	step [137/196], loss=66.0636
	step [138/196], loss=75.3567
	step [139/196], loss=72.4316
	step [140/196], loss=55.6634
	step [141/196], loss=69.6410
	step [142/196], loss=69.0706
	step [143/196], loss=62.2361
	step [144/196], loss=77.7366
	step [145/196], loss=77.6937
	step [146/196], loss=66.6275
	step [147/196], loss=70.0879
	step [148/196], loss=79.7169
	step [149/196], loss=70.7192
	step [150/196], loss=70.7447
	step [151/196], loss=65.0003
	step [152/196], loss=79.9809
	step [153/196], loss=72.5087
	step [154/196], loss=76.6143
	step [155/196], loss=67.5191
	step [156/196], loss=70.5855
	step [157/196], loss=60.0414
	step [158/196], loss=66.5135
	step [159/196], loss=77.5878
	step [160/196], loss=70.6241
	step [161/196], loss=66.1289
	step [162/196], loss=76.7082
	step [163/196], loss=67.9682
	step [164/196], loss=64.7134
	step [165/196], loss=72.3720
	step [166/196], loss=62.5846
	step [167/196], loss=68.8833
	step [168/196], loss=70.5683
	step [169/196], loss=71.5266
	step [170/196], loss=78.2850
	step [171/196], loss=66.5120
	step [172/196], loss=74.1309
	step [173/196], loss=76.0891
	step [174/196], loss=65.9894
	step [175/196], loss=68.7374
	step [176/196], loss=91.5173
	step [177/196], loss=72.9061
	step [178/196], loss=75.9001
	step [179/196], loss=70.9004
	step [180/196], loss=64.5179
	step [181/196], loss=54.0303
	step [182/196], loss=75.5363
	step [183/196], loss=66.0180
	step [184/196], loss=70.4484
	step [185/196], loss=67.4714
	step [186/196], loss=58.0434
	step [187/196], loss=64.7087
	step [188/196], loss=72.8470
	step [189/196], loss=81.4809
	step [190/196], loss=76.2220
	step [191/196], loss=69.9012
	step [192/196], loss=73.5113
	step [193/196], loss=69.5038
	step [194/196], loss=64.7362
	step [195/196], loss=84.7608
	step [196/196], loss=4.0477
	Evaluating
	loss=0.0067, precision=0.4283, recall=0.8488, f1=0.5693
Training epoch 85
	step [1/196], loss=73.9045
	step [2/196], loss=71.5634
	step [3/196], loss=81.7815
	step [4/196], loss=67.7648
	step [5/196], loss=70.3854
	step [6/196], loss=59.0640
	step [7/196], loss=67.1494
	step [8/196], loss=69.3194
	step [9/196], loss=66.6910
	step [10/196], loss=75.2285
	step [11/196], loss=64.6794
	step [12/196], loss=84.9722
	step [13/196], loss=61.9921
	step [14/196], loss=65.7769
	step [15/196], loss=68.9824
	step [16/196], loss=67.6306
	step [17/196], loss=74.6260
	step [18/196], loss=62.3745
	step [19/196], loss=83.5533
	step [20/196], loss=64.1706
	step [21/196], loss=70.2138
	step [22/196], loss=78.3764
	step [23/196], loss=73.6075
	step [24/196], loss=73.3601
	step [25/196], loss=54.7454
	step [26/196], loss=58.1849
	step [27/196], loss=62.1489
	step [28/196], loss=65.3690
	step [29/196], loss=70.4018
	step [30/196], loss=87.8560
	step [31/196], loss=74.5972
	step [32/196], loss=72.9424
	step [33/196], loss=71.3981
	step [34/196], loss=66.3110
	step [35/196], loss=74.4944
	step [36/196], loss=78.4550
	step [37/196], loss=63.8671
	step [38/196], loss=64.0469
	step [39/196], loss=72.3590
	step [40/196], loss=77.8113
	step [41/196], loss=72.3413
	step [42/196], loss=70.1414
	step [43/196], loss=62.4898
	step [44/196], loss=59.2699
	step [45/196], loss=79.7058
	step [46/196], loss=63.1917
	step [47/196], loss=65.7566
	step [48/196], loss=63.5695
	step [49/196], loss=73.6619
	step [50/196], loss=70.5842
	step [51/196], loss=74.6704
	step [52/196], loss=56.6445
	step [53/196], loss=75.1155
	step [54/196], loss=64.7304
	step [55/196], loss=67.8424
	step [56/196], loss=73.1076
	step [57/196], loss=69.2418
	step [58/196], loss=70.6592
	step [59/196], loss=63.3153
	step [60/196], loss=64.6849
	step [61/196], loss=73.1739
	step [62/196], loss=63.0913
	step [63/196], loss=84.4964
	step [64/196], loss=66.7920
	step [65/196], loss=73.0991
	step [66/196], loss=68.0800
	step [67/196], loss=69.0112
	step [68/196], loss=67.4589
	step [69/196], loss=81.2146
	step [70/196], loss=73.6493
	step [71/196], loss=84.6611
	step [72/196], loss=84.7498
	step [73/196], loss=71.9516
	step [74/196], loss=83.8375
	step [75/196], loss=71.0274
	step [76/196], loss=63.4315
	step [77/196], loss=72.9902
	step [78/196], loss=64.0071
	step [79/196], loss=65.3744
	step [80/196], loss=77.0779
	step [81/196], loss=69.1503
	step [82/196], loss=69.7101
	step [83/196], loss=66.9646
	step [84/196], loss=75.8111
	step [85/196], loss=63.1412
	step [86/196], loss=85.8987
	step [87/196], loss=66.0242
	step [88/196], loss=71.2176
	step [89/196], loss=82.4164
	step [90/196], loss=70.1603
	step [91/196], loss=65.1081
	step [92/196], loss=69.2894
	step [93/196], loss=66.7711
	step [94/196], loss=64.0078
	step [95/196], loss=71.7505
	step [96/196], loss=61.4045
	step [97/196], loss=71.1623
	step [98/196], loss=66.7790
	step [99/196], loss=66.8035
	step [100/196], loss=73.7077
	step [101/196], loss=77.4600
	step [102/196], loss=80.1440
	step [103/196], loss=67.4884
	step [104/196], loss=80.1350
	step [105/196], loss=83.2495
	step [106/196], loss=77.7682
	step [107/196], loss=72.1640
	step [108/196], loss=70.6132
	step [109/196], loss=71.3717
	step [110/196], loss=73.7019
	step [111/196], loss=75.8511
	step [112/196], loss=65.3295
	step [113/196], loss=77.6222
	step [114/196], loss=68.0465
	step [115/196], loss=73.4439
	step [116/196], loss=56.1615
	step [117/196], loss=63.8315
	step [118/196], loss=62.9153
	step [119/196], loss=71.0639
	step [120/196], loss=63.3408
	step [121/196], loss=68.3004
	step [122/196], loss=73.3375
	step [123/196], loss=60.7100
	step [124/196], loss=74.6626
	step [125/196], loss=70.7446
	step [126/196], loss=73.4307
	step [127/196], loss=55.8570
	step [128/196], loss=63.5213
	step [129/196], loss=87.0319
	step [130/196], loss=69.2113
	step [131/196], loss=82.0001
	step [132/196], loss=71.6120
	step [133/196], loss=77.3923
	step [134/196], loss=60.9064
	step [135/196], loss=71.0825
	step [136/196], loss=63.2658
	step [137/196], loss=82.5119
	step [138/196], loss=62.5683
	step [139/196], loss=71.9221
	step [140/196], loss=69.1106
	step [141/196], loss=74.1610
	step [142/196], loss=58.2776
	step [143/196], loss=85.7949
	step [144/196], loss=67.5992
	step [145/196], loss=70.7224
	step [146/196], loss=64.5462
	step [147/196], loss=74.7830
	step [148/196], loss=74.5106
	step [149/196], loss=78.5211
	step [150/196], loss=71.8490
	step [151/196], loss=76.0124
	step [152/196], loss=60.8058
	step [153/196], loss=78.3027
	step [154/196], loss=72.0268
	step [155/196], loss=76.8629
	step [156/196], loss=59.1230
	step [157/196], loss=61.6986
	step [158/196], loss=69.3526
	step [159/196], loss=52.5353
	step [160/196], loss=62.5913
	step [161/196], loss=72.0253
	step [162/196], loss=57.9136
	step [163/196], loss=65.9800
	step [164/196], loss=72.8817
	step [165/196], loss=64.5181
	step [166/196], loss=65.9870
	step [167/196], loss=76.6657
	step [168/196], loss=73.3851
	step [169/196], loss=76.9913
	step [170/196], loss=83.4722
	step [171/196], loss=58.9802
	step [172/196], loss=69.4869
	step [173/196], loss=74.8853
	step [174/196], loss=78.5210
	step [175/196], loss=75.7302
	step [176/196], loss=69.7316
	step [177/196], loss=66.2909
	step [178/196], loss=66.1618
	step [179/196], loss=75.0964
	step [180/196], loss=68.3336
	step [181/196], loss=77.0967
	step [182/196], loss=59.3519
	step [183/196], loss=78.2546
	step [184/196], loss=61.2493
	step [185/196], loss=65.9637
	step [186/196], loss=78.3277
	step [187/196], loss=63.9097
	step [188/196], loss=80.8222
	step [189/196], loss=87.1965
	step [190/196], loss=75.5971
	step [191/196], loss=71.2310
	step [192/196], loss=74.7965
	step [193/196], loss=59.6046
	step [194/196], loss=66.5753
	step [195/196], loss=67.0688
	step [196/196], loss=4.2598
	Evaluating
	loss=0.0062, precision=0.4376, recall=0.8552, f1=0.5790
Training epoch 86
	step [1/196], loss=81.2408
	step [2/196], loss=76.8872
	step [3/196], loss=78.4185
	step [4/196], loss=79.2202
	step [5/196], loss=69.0338
	step [6/196], loss=71.2611
	step [7/196], loss=60.8951
	step [8/196], loss=68.4861
	step [9/196], loss=76.6207
	step [10/196], loss=77.8707
	step [11/196], loss=68.5323
	step [12/196], loss=66.6789
	step [13/196], loss=69.1075
	step [14/196], loss=72.8403
	step [15/196], loss=68.2850
	step [16/196], loss=64.4937
	step [17/196], loss=72.5642
	step [18/196], loss=66.0679
	step [19/196], loss=65.3672
	step [20/196], loss=72.9310
	step [21/196], loss=67.3064
	step [22/196], loss=64.8604
	step [23/196], loss=54.4827
	step [24/196], loss=71.2678
	step [25/196], loss=78.2818
	step [26/196], loss=70.9054
	step [27/196], loss=81.9051
	step [28/196], loss=69.2979
	step [29/196], loss=63.0972
	step [30/196], loss=71.6804
	step [31/196], loss=61.5953
	step [32/196], loss=73.9118
	step [33/196], loss=58.0950
	step [34/196], loss=79.9988
	step [35/196], loss=68.0948
	step [36/196], loss=65.8281
	step [37/196], loss=67.5374
	step [38/196], loss=69.3321
	step [39/196], loss=72.9789
	step [40/196], loss=70.4337
	step [41/196], loss=72.9806
	step [42/196], loss=74.8222
	step [43/196], loss=80.3486
	step [44/196], loss=62.6543
	step [45/196], loss=72.6679
	step [46/196], loss=69.5252
	step [47/196], loss=63.2700
	step [48/196], loss=71.4214
	step [49/196], loss=64.2891
	step [50/196], loss=65.4171
	step [51/196], loss=57.7205
	step [52/196], loss=68.5205
	step [53/196], loss=72.1423
	step [54/196], loss=65.8110
	step [55/196], loss=73.7269
	step [56/196], loss=59.4073
	step [57/196], loss=73.4024
	step [58/196], loss=74.0306
	step [59/196], loss=75.6270
	step [60/196], loss=61.9842
	step [61/196], loss=66.5406
	step [62/196], loss=64.8807
	step [63/196], loss=68.2982
	step [64/196], loss=62.5726
	step [65/196], loss=68.8637
	step [66/196], loss=64.9127
	step [67/196], loss=80.1904
	step [68/196], loss=64.1784
	step [69/196], loss=77.2227
	step [70/196], loss=63.7170
	step [71/196], loss=63.9401
	step [72/196], loss=72.3963
	step [73/196], loss=62.8180
	step [74/196], loss=71.2556
	step [75/196], loss=68.9275
	step [76/196], loss=58.3198
	step [77/196], loss=74.8933
	step [78/196], loss=53.3501
	step [79/196], loss=78.2091
	step [80/196], loss=60.2208
	step [81/196], loss=59.7485
	step [82/196], loss=87.2277
	step [83/196], loss=75.6723
	step [84/196], loss=72.5492
	step [85/196], loss=73.1393
	step [86/196], loss=69.1532
	step [87/196], loss=85.1749
	step [88/196], loss=68.0146
	step [89/196], loss=79.9892
	step [90/196], loss=62.7288
	step [91/196], loss=73.2187
	step [92/196], loss=77.6491
	step [93/196], loss=76.0436
	step [94/196], loss=62.8238
	step [95/196], loss=73.1812
	step [96/196], loss=71.3972
	step [97/196], loss=59.2243
	step [98/196], loss=70.3615
	step [99/196], loss=69.9187
	step [100/196], loss=73.3960
	step [101/196], loss=69.0184
	step [102/196], loss=62.5028
	step [103/196], loss=70.9915
	step [104/196], loss=68.1929
	step [105/196], loss=67.7811
	step [106/196], loss=79.2286
	step [107/196], loss=68.0500
	step [108/196], loss=67.4778
	step [109/196], loss=65.8471
	step [110/196], loss=58.6627
	step [111/196], loss=77.2408
	step [112/196], loss=73.6878
	step [113/196], loss=59.9322
	step [114/196], loss=73.4037
	step [115/196], loss=70.7251
	step [116/196], loss=59.4577
	step [117/196], loss=69.2196
	step [118/196], loss=69.4538
	step [119/196], loss=67.2556
	step [120/196], loss=60.5384
	step [121/196], loss=75.5853
	step [122/196], loss=76.0770
	step [123/196], loss=87.0755
	step [124/196], loss=81.8951
	step [125/196], loss=75.4929
	step [126/196], loss=64.2442
	step [127/196], loss=68.6309
	step [128/196], loss=70.0408
	step [129/196], loss=86.6915
	step [130/196], loss=80.7563
	step [131/196], loss=77.7349
	step [132/196], loss=88.3776
	step [133/196], loss=77.1741
	step [134/196], loss=85.2296
	step [135/196], loss=86.2868
	step [136/196], loss=77.2900
	step [137/196], loss=70.1424
	step [138/196], loss=85.9599
	step [139/196], loss=85.1932
	step [140/196], loss=92.9386
	step [141/196], loss=78.2385
	step [142/196], loss=67.2269
	step [143/196], loss=63.7470
	step [144/196], loss=69.6940
	step [145/196], loss=75.8578
	step [146/196], loss=71.2960
	step [147/196], loss=69.0815
	step [148/196], loss=88.0318
	step [149/196], loss=78.8293
	step [150/196], loss=87.5135
	step [151/196], loss=65.4836
	step [152/196], loss=73.9258
	step [153/196], loss=72.9585
	step [154/196], loss=65.3266
	step [155/196], loss=73.6675
	step [156/196], loss=70.9130
	step [157/196], loss=74.0004
	step [158/196], loss=85.7170
	step [159/196], loss=79.3701
	step [160/196], loss=71.3088
	step [161/196], loss=84.9656
	step [162/196], loss=64.6492
	step [163/196], loss=78.1761
	step [164/196], loss=88.7957
	step [165/196], loss=65.4857
	step [166/196], loss=70.3292
	step [167/196], loss=72.5972
	step [168/196], loss=77.0153
	step [169/196], loss=84.8994
	step [170/196], loss=71.5637
	step [171/196], loss=77.0984
	step [172/196], loss=72.5506
	step [173/196], loss=65.8298
	step [174/196], loss=83.8829
	step [175/196], loss=76.2733
	step [176/196], loss=70.5568
	step [177/196], loss=76.9199
	step [178/196], loss=85.5091
	step [179/196], loss=68.7138
	step [180/196], loss=70.4809
	step [181/196], loss=78.9329
	step [182/196], loss=74.3969
	step [183/196], loss=61.2222
	step [184/196], loss=70.6484
	step [185/196], loss=75.1560
	step [186/196], loss=70.6865
	step [187/196], loss=81.1070
	step [188/196], loss=67.7453
	step [189/196], loss=71.2669
	step [190/196], loss=74.6304
	step [191/196], loss=74.4254
	step [192/196], loss=66.2179
	step [193/196], loss=67.4260
	step [194/196], loss=77.8756
	step [195/196], loss=62.6808
	step [196/196], loss=5.7528
	Evaluating
	loss=0.0089, precision=0.3623, recall=0.8578, f1=0.5095
Training epoch 87
	step [1/196], loss=63.4747
	step [2/196], loss=55.6774
	step [3/196], loss=64.5303
	step [4/196], loss=70.7708
	step [5/196], loss=83.8595
	step [6/196], loss=76.0668
	step [7/196], loss=75.2052
	step [8/196], loss=60.5352
	step [9/196], loss=66.2724
	step [10/196], loss=62.1189
	step [11/196], loss=91.9289
	step [12/196], loss=67.0648
	step [13/196], loss=73.4896
	step [14/196], loss=64.4464
	step [15/196], loss=67.9408
	step [16/196], loss=74.3666
	step [17/196], loss=57.8548
	step [18/196], loss=71.9941
	step [19/196], loss=76.4944
	step [20/196], loss=70.9699
	step [21/196], loss=73.0788
	step [22/196], loss=65.7601
	step [23/196], loss=77.4279
	step [24/196], loss=58.4059
	step [25/196], loss=75.0564
	step [26/196], loss=77.4994
	step [27/196], loss=74.6939
	step [28/196], loss=82.4093
	step [29/196], loss=67.1089
	step [30/196], loss=76.6399
	step [31/196], loss=63.8548
	step [32/196], loss=77.1488
	step [33/196], loss=74.1525
	step [34/196], loss=74.1491
	step [35/196], loss=62.5449
	step [36/196], loss=63.9599
	step [37/196], loss=76.7241
	step [38/196], loss=63.1997
	step [39/196], loss=82.2462
	step [40/196], loss=68.5265
	step [41/196], loss=73.5079
	step [42/196], loss=72.3685
	step [43/196], loss=65.6782
	step [44/196], loss=63.1838
	step [45/196], loss=69.7512
	step [46/196], loss=77.3436
	step [47/196], loss=73.6605
	step [48/196], loss=59.6613
	step [49/196], loss=62.4538
	step [50/196], loss=69.6161
	step [51/196], loss=70.4842
	step [52/196], loss=80.4382
	step [53/196], loss=64.0303
	step [54/196], loss=81.3466
	step [55/196], loss=64.5935
	step [56/196], loss=62.6893
	step [57/196], loss=77.8035
	step [58/196], loss=63.3559
	step [59/196], loss=70.2442
	step [60/196], loss=79.4866
	step [61/196], loss=79.6492
	step [62/196], loss=67.3959
	step [63/196], loss=64.8392
	step [64/196], loss=72.1087
	step [65/196], loss=70.5907
	step [66/196], loss=64.6345
	step [67/196], loss=69.4366
	step [68/196], loss=71.7301
	step [69/196], loss=60.7528
	step [70/196], loss=66.5740
	step [71/196], loss=78.7213
	step [72/196], loss=60.3855
	step [73/196], loss=66.7778
	step [74/196], loss=67.9933
	step [75/196], loss=73.9683
	step [76/196], loss=67.7625
	step [77/196], loss=69.9730
	step [78/196], loss=61.1896
	step [79/196], loss=80.1941
	step [80/196], loss=76.7735
	step [81/196], loss=66.9720
	step [82/196], loss=84.5567
	step [83/196], loss=75.8210
	step [84/196], loss=71.8409
	step [85/196], loss=86.3195
	step [86/196], loss=76.8433
	step [87/196], loss=64.6164
	step [88/196], loss=72.1124
	step [89/196], loss=55.6599
	step [90/196], loss=59.2071
	step [91/196], loss=63.0636
	step [92/196], loss=68.7613
	step [93/196], loss=61.0807
	step [94/196], loss=72.0677
	step [95/196], loss=84.4005
	step [96/196], loss=67.5489
	step [97/196], loss=82.0306
	step [98/196], loss=75.9777
	step [99/196], loss=74.9758
	step [100/196], loss=77.5167
	step [101/196], loss=65.0203
	step [102/196], loss=71.7963
	step [103/196], loss=79.7041
	step [104/196], loss=63.6262
	step [105/196], loss=65.8029
	step [106/196], loss=70.5375
	step [107/196], loss=72.5969
	step [108/196], loss=82.8310
	step [109/196], loss=74.2601
	step [110/196], loss=58.7410
	step [111/196], loss=70.2138
	step [112/196], loss=74.8218
	step [113/196], loss=70.1779
	step [114/196], loss=65.7983
	step [115/196], loss=73.7471
	step [116/196], loss=71.8555
	step [117/196], loss=88.6707
	step [118/196], loss=74.8301
	step [119/196], loss=69.0079
	step [120/196], loss=66.6039
	step [121/196], loss=68.4134
	step [122/196], loss=74.8690
	step [123/196], loss=67.7759
	step [124/196], loss=62.1671
	step [125/196], loss=73.9764
	step [126/196], loss=77.3245
	step [127/196], loss=62.9318
	step [128/196], loss=66.6377
	step [129/196], loss=80.3497
	step [130/196], loss=71.6921
	step [131/196], loss=73.3043
	step [132/196], loss=68.2804
	step [133/196], loss=52.4296
	step [134/196], loss=71.1094
	step [135/196], loss=65.8978
	step [136/196], loss=76.9230
	step [137/196], loss=65.7453
	step [138/196], loss=76.5036
	step [139/196], loss=70.1297
	step [140/196], loss=69.6921
	step [141/196], loss=83.6314
	step [142/196], loss=72.4571
	step [143/196], loss=61.0243
	step [144/196], loss=69.3138
	step [145/196], loss=67.2529
	step [146/196], loss=67.4467
	step [147/196], loss=69.8421
	step [148/196], loss=69.3701
	step [149/196], loss=63.3251
	step [150/196], loss=85.2926
	step [151/196], loss=68.3367
	step [152/196], loss=72.1104
	step [153/196], loss=65.6068
	step [154/196], loss=65.9163
	step [155/196], loss=85.3771
	step [156/196], loss=57.9689
	step [157/196], loss=72.4979
	step [158/196], loss=83.5636
	step [159/196], loss=74.8757
	step [160/196], loss=67.5891
	step [161/196], loss=68.4280
	step [162/196], loss=72.2748
	step [163/196], loss=67.3129
	step [164/196], loss=65.3487
	step [165/196], loss=60.7896
	step [166/196], loss=64.4704
	step [167/196], loss=59.7033
	step [168/196], loss=71.2417
	step [169/196], loss=78.7165
	step [170/196], loss=79.1752
	step [171/196], loss=66.2088
	step [172/196], loss=77.1192
	step [173/196], loss=69.9456
	step [174/196], loss=70.0267
	step [175/196], loss=64.2782
	step [176/196], loss=68.3686
	step [177/196], loss=55.4209
	step [178/196], loss=70.8739
	step [179/196], loss=66.8210
	step [180/196], loss=58.4985
	step [181/196], loss=68.4015
	step [182/196], loss=67.1570
	step [183/196], loss=67.2689
	step [184/196], loss=69.9586
	step [185/196], loss=66.6769
	step [186/196], loss=71.9133
	step [187/196], loss=56.8439
	step [188/196], loss=88.4405
	step [189/196], loss=68.9344
	step [190/196], loss=74.6617
	step [191/196], loss=84.4750
	step [192/196], loss=75.2594
	step [193/196], loss=73.2894
	step [194/196], loss=67.1678
	step [195/196], loss=58.0417
	step [196/196], loss=7.4137
	Evaluating
	loss=0.0062, precision=0.4476, recall=0.8542, f1=0.5874
Training epoch 88
	step [1/196], loss=73.3697
	step [2/196], loss=61.4699
	step [3/196], loss=67.3797
	step [4/196], loss=55.1926
	step [5/196], loss=69.0647
	step [6/196], loss=79.4432
	step [7/196], loss=69.2150
	step [8/196], loss=64.1326
	step [9/196], loss=71.7867
	step [10/196], loss=61.5281
	step [11/196], loss=68.4172
	step [12/196], loss=60.0417
	step [13/196], loss=75.5861
	step [14/196], loss=69.4187
	step [15/196], loss=66.5137
	step [16/196], loss=69.7100
	step [17/196], loss=74.5944
	step [18/196], loss=66.6912
	step [19/196], loss=78.8825
	step [20/196], loss=67.1188
	step [21/196], loss=68.9360
	step [22/196], loss=60.5264
	step [23/196], loss=72.2568
	step [24/196], loss=72.6625
	step [25/196], loss=63.1621
	step [26/196], loss=60.9440
	step [27/196], loss=76.0060
	step [28/196], loss=66.5710
	step [29/196], loss=65.9964
	step [30/196], loss=72.0781
	step [31/196], loss=66.0731
	step [32/196], loss=68.2014
	step [33/196], loss=75.3002
	step [34/196], loss=61.0958
	step [35/196], loss=69.7150
	step [36/196], loss=67.6789
	step [37/196], loss=67.8429
	step [38/196], loss=79.5571
	step [39/196], loss=79.7527
	step [40/196], loss=66.5380
	step [41/196], loss=73.7621
	step [42/196], loss=66.5483
	step [43/196], loss=68.0992
	step [44/196], loss=66.8207
	step [45/196], loss=74.8667
	step [46/196], loss=72.6550
	step [47/196], loss=70.7196
	step [48/196], loss=75.0196
	step [49/196], loss=75.2661
	step [50/196], loss=67.7332
	step [51/196], loss=71.9290
	step [52/196], loss=60.6921
	step [53/196], loss=67.1807
	step [54/196], loss=62.9173
	step [55/196], loss=70.4135
	step [56/196], loss=62.4756
	step [57/196], loss=71.9225
	step [58/196], loss=67.9109
	step [59/196], loss=73.2365
	step [60/196], loss=73.4727
	step [61/196], loss=65.3274
	step [62/196], loss=70.3049
	step [63/196], loss=70.2442
	step [64/196], loss=66.9408
	step [65/196], loss=56.9182
	step [66/196], loss=72.7373
	step [67/196], loss=75.0290
	step [68/196], loss=69.0324
	step [69/196], loss=68.2950
	step [70/196], loss=70.1730
	step [71/196], loss=76.2220
	step [72/196], loss=72.5849
	step [73/196], loss=76.2261
	step [74/196], loss=72.1640
	step [75/196], loss=72.2240
	step [76/196], loss=66.7813
	step [77/196], loss=61.7576
	step [78/196], loss=73.6389
	step [79/196], loss=71.0665
	step [80/196], loss=77.4606
	step [81/196], loss=68.3726
	step [82/196], loss=66.2505
	step [83/196], loss=81.1567
	step [84/196], loss=72.6639
	step [85/196], loss=80.2957
	step [86/196], loss=74.2473
	step [87/196], loss=67.2438
	step [88/196], loss=68.3342
	step [89/196], loss=74.6145
	step [90/196], loss=60.3585
	step [91/196], loss=62.8871
	step [92/196], loss=71.4022
	step [93/196], loss=64.6486
	step [94/196], loss=61.7451
	step [95/196], loss=79.5160
	step [96/196], loss=70.0516
	step [97/196], loss=77.0614
	step [98/196], loss=61.7028
	step [99/196], loss=91.0796
	step [100/196], loss=55.0359
	step [101/196], loss=71.5223
	step [102/196], loss=63.1928
	step [103/196], loss=57.9616
	step [104/196], loss=66.1387
	step [105/196], loss=63.4703
	step [106/196], loss=66.1993
	step [107/196], loss=59.8830
	step [108/196], loss=79.9268
	step [109/196], loss=71.1204
	step [110/196], loss=77.1295
	step [111/196], loss=74.2385
	step [112/196], loss=75.2497
	step [113/196], loss=73.4483
	step [114/196], loss=65.8753
	step [115/196], loss=77.0713
	step [116/196], loss=81.5552
	step [117/196], loss=85.5820
	step [118/196], loss=66.3154
	step [119/196], loss=70.7483
	step [120/196], loss=76.6755
	step [121/196], loss=74.0162
	step [122/196], loss=77.3706
	step [123/196], loss=68.3352
	step [124/196], loss=66.1738
	step [125/196], loss=67.3842
	step [126/196], loss=63.4647
	step [127/196], loss=66.2176
	step [128/196], loss=80.8763
	step [129/196], loss=83.8087
	step [130/196], loss=56.9423
	step [131/196], loss=71.0089
	step [132/196], loss=75.2867
	step [133/196], loss=71.5741
	step [134/196], loss=69.5967
	step [135/196], loss=64.2896
	step [136/196], loss=77.8494
	step [137/196], loss=68.7911
	step [138/196], loss=72.0338
	step [139/196], loss=73.2272
	step [140/196], loss=73.7479
	step [141/196], loss=68.2454
	step [142/196], loss=78.0782
	step [143/196], loss=61.2089
	step [144/196], loss=70.0219
	step [145/196], loss=63.7569
	step [146/196], loss=61.6246
	step [147/196], loss=73.1665
	step [148/196], loss=72.7615
	step [149/196], loss=64.8446
	step [150/196], loss=77.6428
	step [151/196], loss=62.8529
	step [152/196], loss=62.9211
	step [153/196], loss=57.5921
	step [154/196], loss=65.3926
	step [155/196], loss=77.1858
	step [156/196], loss=93.3736
	step [157/196], loss=75.5420
	step [158/196], loss=76.0009
	step [159/196], loss=72.1208
	step [160/196], loss=77.1966
	step [161/196], loss=68.8562
	step [162/196], loss=69.9435
	step [163/196], loss=60.9262
	step [164/196], loss=78.7904
	step [165/196], loss=83.8880
	step [166/196], loss=64.3179
	step [167/196], loss=63.7094
	step [168/196], loss=69.6823
	step [169/196], loss=76.6842
	step [170/196], loss=72.0199
	step [171/196], loss=68.0520
	step [172/196], loss=80.0461
	step [173/196], loss=58.3324
	step [174/196], loss=78.1955
	step [175/196], loss=70.8078
	step [176/196], loss=59.9867
	step [177/196], loss=67.0836
	step [178/196], loss=76.1861
	step [179/196], loss=69.5324
	step [180/196], loss=74.2794
	step [181/196], loss=63.8771
	step [182/196], loss=65.9648
	step [183/196], loss=90.8417
	step [184/196], loss=69.0215
	step [185/196], loss=73.0877
	step [186/196], loss=57.1246
	step [187/196], loss=70.5925
	step [188/196], loss=78.5908
	step [189/196], loss=66.7385
	step [190/196], loss=66.2769
	step [191/196], loss=63.2126
	step [192/196], loss=70.3995
	step [193/196], loss=68.4310
	step [194/196], loss=73.7735
	step [195/196], loss=84.0980
	step [196/196], loss=12.1210
	Evaluating
	loss=0.0050, precision=0.5238, recall=0.8411, f1=0.6456
saving model as: 3_saved_model.pth
Training epoch 89
	step [1/196], loss=70.2091
	step [2/196], loss=70.7843
	step [3/196], loss=71.1035
	step [4/196], loss=63.9217
	step [5/196], loss=56.3366
	step [6/196], loss=72.8097
	step [7/196], loss=82.2876
	step [8/196], loss=85.5521
	step [9/196], loss=68.3438
	step [10/196], loss=71.1297
	step [11/196], loss=68.5611
	step [12/196], loss=75.0353
	step [13/196], loss=70.8461
	step [14/196], loss=79.6196
	step [15/196], loss=86.4368
	step [16/196], loss=72.5707
	step [17/196], loss=68.7798
	step [18/196], loss=57.4546
	step [19/196], loss=73.2824
	step [20/196], loss=93.3915
	step [21/196], loss=87.1120
	step [22/196], loss=65.2053
	step [23/196], loss=61.8927
	step [24/196], loss=60.9704
	step [25/196], loss=57.0063
	step [26/196], loss=53.6656
	step [27/196], loss=83.3977
	step [28/196], loss=74.2483
	step [29/196], loss=64.3951
	step [30/196], loss=76.2878
	step [31/196], loss=48.4981
	step [32/196], loss=83.2521
	step [33/196], loss=72.3135
	step [34/196], loss=64.5719
	step [35/196], loss=61.0930
	step [36/196], loss=75.9277
	step [37/196], loss=73.4654
	step [38/196], loss=70.9308
	step [39/196], loss=66.0837
	step [40/196], loss=72.1805
	step [41/196], loss=76.2336
	step [42/196], loss=58.7456
	step [43/196], loss=69.6794
	step [44/196], loss=66.7154
	step [45/196], loss=74.7616
	step [46/196], loss=65.6174
	step [47/196], loss=76.0675
	step [48/196], loss=56.2454
	step [49/196], loss=69.6945
	step [50/196], loss=65.9364
	step [51/196], loss=67.5497
	step [52/196], loss=69.1710
	step [53/196], loss=61.4801
	step [54/196], loss=64.5737
	step [55/196], loss=61.3910
	step [56/196], loss=78.5577
	step [57/196], loss=74.8102
	step [58/196], loss=69.0427
	step [59/196], loss=75.5954
	step [60/196], loss=71.3119
	step [61/196], loss=76.5723
	step [62/196], loss=63.0909
	step [63/196], loss=74.8828
	step [64/196], loss=81.2700
	step [65/196], loss=67.7200
	step [66/196], loss=64.6939
	step [67/196], loss=82.7216
	step [68/196], loss=72.8475
	step [69/196], loss=64.8839
	step [70/196], loss=59.8687
	step [71/196], loss=72.7378
	step [72/196], loss=76.8210
	step [73/196], loss=73.3295
	step [74/196], loss=77.8084
	step [75/196], loss=68.1335
	step [76/196], loss=81.3608
	step [77/196], loss=69.1423
	step [78/196], loss=62.1219
	step [79/196], loss=75.7762
	step [80/196], loss=59.3570
	step [81/196], loss=69.0292
	step [82/196], loss=65.7127
	step [83/196], loss=71.2029
	step [84/196], loss=87.1402
	step [85/196], loss=84.7274
	step [86/196], loss=65.0089
	step [87/196], loss=74.1550
	step [88/196], loss=68.0436
	step [89/196], loss=56.1076
	step [90/196], loss=75.3276
	step [91/196], loss=64.2717
	step [92/196], loss=67.6559
	step [93/196], loss=67.7426
	step [94/196], loss=65.9212
	step [95/196], loss=65.1248
	step [96/196], loss=63.6219
	step [97/196], loss=63.3861
	step [98/196], loss=68.3418
	step [99/196], loss=63.9888
	step [100/196], loss=71.2265
	step [101/196], loss=68.4266
	step [102/196], loss=72.0892
	step [103/196], loss=65.8092
	step [104/196], loss=79.3074
	step [105/196], loss=74.4052
	step [106/196], loss=78.5325
	step [107/196], loss=60.8563
	step [108/196], loss=67.6644
	step [109/196], loss=67.4288
	step [110/196], loss=72.2244
	step [111/196], loss=67.9067
	step [112/196], loss=65.3667
	step [113/196], loss=66.5898
	step [114/196], loss=58.9796
	step [115/196], loss=68.5354
	step [116/196], loss=81.7291
	step [117/196], loss=60.4007
	step [118/196], loss=70.7277
	step [119/196], loss=67.2741
	step [120/196], loss=59.7866
	step [121/196], loss=70.0452
	step [122/196], loss=69.1558
	step [123/196], loss=74.2292
	step [124/196], loss=61.1989
	step [125/196], loss=64.7469
	step [126/196], loss=73.4984
	step [127/196], loss=73.1139
	step [128/196], loss=73.5383
	step [129/196], loss=88.1830
	step [130/196], loss=69.5395
	step [131/196], loss=69.2031
	step [132/196], loss=74.3289
	step [133/196], loss=70.2002
	step [134/196], loss=68.8408
	step [135/196], loss=69.8632
	step [136/196], loss=67.2070
	step [137/196], loss=71.1191
	step [138/196], loss=79.0960
	step [139/196], loss=58.8811
	step [140/196], loss=79.5842
	step [141/196], loss=77.7448
	step [142/196], loss=70.7548
	step [143/196], loss=75.1611
	step [144/196], loss=61.0619
	step [145/196], loss=62.2369
	step [146/196], loss=78.3859
	step [147/196], loss=74.9039
	step [148/196], loss=73.2733
	step [149/196], loss=51.9220
	step [150/196], loss=70.7203
	step [151/196], loss=64.2753
	step [152/196], loss=68.9615
	step [153/196], loss=60.0325
	step [154/196], loss=69.0127
	step [155/196], loss=66.5555
	step [156/196], loss=65.8984
	step [157/196], loss=74.0322
	step [158/196], loss=77.4879
	step [159/196], loss=81.9263
	step [160/196], loss=62.9470
	step [161/196], loss=63.5630
	step [162/196], loss=74.0003
	step [163/196], loss=66.8802
	step [164/196], loss=72.4196
	step [165/196], loss=63.7019
	step [166/196], loss=75.2746
	step [167/196], loss=72.8543
	step [168/196], loss=63.1090
	step [169/196], loss=74.2003
	step [170/196], loss=66.4132
	step [171/196], loss=91.0447
	step [172/196], loss=73.6210
	step [173/196], loss=66.6169
	step [174/196], loss=69.1773
	step [175/196], loss=77.1699
	step [176/196], loss=75.3071
	step [177/196], loss=67.2558
	step [178/196], loss=63.2304
	step [179/196], loss=61.6189
	step [180/196], loss=66.7775
	step [181/196], loss=60.6292
	step [182/196], loss=57.8112
	step [183/196], loss=74.6760
	step [184/196], loss=62.7087
	step [185/196], loss=75.6786
	step [186/196], loss=61.0003
	step [187/196], loss=63.4889
	step [188/196], loss=58.4021
	step [189/196], loss=70.2805
	step [190/196], loss=70.3402
	step [191/196], loss=63.7689
	step [192/196], loss=65.5111
	step [193/196], loss=67.0159
	step [194/196], loss=62.2072
	step [195/196], loss=86.3258
	step [196/196], loss=5.5762
	Evaluating
	loss=0.0057, precision=0.4747, recall=0.8462, f1=0.6082
Training epoch 90
	step [1/196], loss=71.9105
	step [2/196], loss=63.0005
	step [3/196], loss=70.0585
	step [4/196], loss=58.2351
	step [5/196], loss=79.4580
	step [6/196], loss=85.5902
	step [7/196], loss=76.9244
	step [8/196], loss=73.0444
	step [9/196], loss=75.4690
	step [10/196], loss=68.2981
	step [11/196], loss=70.0424
	step [12/196], loss=65.2382
	step [13/196], loss=73.5404
	step [14/196], loss=77.1679
	step [15/196], loss=73.0995
	step [16/196], loss=69.1823
	step [17/196], loss=77.3067
	step [18/196], loss=58.2025
	step [19/196], loss=73.4132
	step [20/196], loss=75.8787
	step [21/196], loss=80.4629
	step [22/196], loss=73.5127
	step [23/196], loss=68.9157
	step [24/196], loss=80.6721
	step [25/196], loss=70.1892
	step [26/196], loss=73.2119
	step [27/196], loss=71.9335
	step [28/196], loss=70.6943
	step [29/196], loss=82.0469
	step [30/196], loss=70.0856
	step [31/196], loss=63.1504
	step [32/196], loss=72.7116
	step [33/196], loss=80.5817
	step [34/196], loss=76.7020
	step [35/196], loss=76.2481
	step [36/196], loss=62.3150
	step [37/196], loss=72.9452
	step [38/196], loss=67.9269
	step [39/196], loss=74.3671
	step [40/196], loss=60.9256
	step [41/196], loss=77.2620
	step [42/196], loss=67.5175
	step [43/196], loss=70.0139
	step [44/196], loss=70.4899
	step [45/196], loss=73.6982
	step [46/196], loss=60.9557
	step [47/196], loss=57.3098
	step [48/196], loss=60.2477
	step [49/196], loss=65.1967
	step [50/196], loss=73.9404
	step [51/196], loss=69.4112
	step [52/196], loss=66.3366
	step [53/196], loss=62.0140
	step [54/196], loss=63.5077
	step [55/196], loss=66.0762
	step [56/196], loss=60.8103
	step [57/196], loss=68.0525
	step [58/196], loss=65.2777
	step [59/196], loss=65.5135
	step [60/196], loss=60.3497
	step [61/196], loss=73.8792
	step [62/196], loss=80.2429
	step [63/196], loss=71.3298
	step [64/196], loss=69.9521
	step [65/196], loss=63.0304
	step [66/196], loss=62.5481
	step [67/196], loss=70.9490
	step [68/196], loss=69.7414
	step [69/196], loss=70.7059
	step [70/196], loss=67.9530
	step [71/196], loss=57.2158
	step [72/196], loss=73.6117
	step [73/196], loss=73.4560
	step [74/196], loss=66.0036
	step [75/196], loss=68.0142
	step [76/196], loss=77.6635
	step [77/196], loss=70.5568
	step [78/196], loss=69.9765
	step [79/196], loss=82.2956
	step [80/196], loss=66.0735
	step [81/196], loss=63.8538
	step [82/196], loss=67.0157
	step [83/196], loss=77.7534
	step [84/196], loss=67.2982
	step [85/196], loss=72.7896
	step [86/196], loss=70.6701
	step [87/196], loss=70.7613
	step [88/196], loss=79.3298
	step [89/196], loss=72.5692
	step [90/196], loss=74.5387
	step [91/196], loss=66.1303
	step [92/196], loss=62.2042
	step [93/196], loss=75.3973
	step [94/196], loss=72.4924
	step [95/196], loss=68.0834
	step [96/196], loss=69.4445
	step [97/196], loss=63.1360
	step [98/196], loss=63.7935
	step [99/196], loss=76.2724
	step [100/196], loss=73.2836
	step [101/196], loss=67.3177
	step [102/196], loss=66.7568
	step [103/196], loss=59.6674
	step [104/196], loss=62.0781
	step [105/196], loss=75.5757
	step [106/196], loss=70.4136
	step [107/196], loss=59.3902
	step [108/196], loss=82.8615
	step [109/196], loss=69.3797
	step [110/196], loss=63.6030
	step [111/196], loss=66.2853
	step [112/196], loss=69.6252
	step [113/196], loss=76.2190
	step [114/196], loss=70.5110
	step [115/196], loss=72.3281
	step [116/196], loss=69.1140
	step [117/196], loss=68.3726
	step [118/196], loss=64.6340
	step [119/196], loss=73.5114
	step [120/196], loss=78.7819
	step [121/196], loss=60.1419
	step [122/196], loss=72.9667
	step [123/196], loss=73.0621
	step [124/196], loss=68.9175
	step [125/196], loss=52.9812
	step [126/196], loss=72.9244
	step [127/196], loss=62.0070
	step [128/196], loss=88.0805
	step [129/196], loss=70.9420
	step [130/196], loss=59.9190
	step [131/196], loss=81.9635
	step [132/196], loss=55.7749
	step [133/196], loss=60.0235
	step [134/196], loss=65.5905
	step [135/196], loss=72.9095
	step [136/196], loss=61.9659
	step [137/196], loss=65.0748
	step [138/196], loss=73.0541
	step [139/196], loss=66.7251
	step [140/196], loss=76.4803
	step [141/196], loss=77.0686
	step [142/196], loss=70.2160
	step [143/196], loss=70.6014
	step [144/196], loss=59.3341
	step [145/196], loss=70.5335
	step [146/196], loss=69.0215
	step [147/196], loss=74.8796
	step [148/196], loss=56.8009
	step [149/196], loss=79.2773
	step [150/196], loss=66.8229
	step [151/196], loss=73.9112
	step [152/196], loss=56.1899
	step [153/196], loss=73.9347
	step [154/196], loss=59.5771
	step [155/196], loss=84.4324
	step [156/196], loss=63.0098
	step [157/196], loss=74.4327
	step [158/196], loss=83.6872
	step [159/196], loss=72.4355
	step [160/196], loss=76.0334
	step [161/196], loss=56.9759
	step [162/196], loss=70.0468
	step [163/196], loss=66.1322
	step [164/196], loss=72.7354
	step [165/196], loss=76.2008
	step [166/196], loss=66.8425
	step [167/196], loss=61.6306
	step [168/196], loss=66.3912
	step [169/196], loss=91.2344
	step [170/196], loss=77.4962
	step [171/196], loss=82.9407
	step [172/196], loss=72.7644
	step [173/196], loss=60.5977
	step [174/196], loss=77.6111
	step [175/196], loss=57.2440
	step [176/196], loss=65.2316
	step [177/196], loss=57.5764
	step [178/196], loss=75.7434
	step [179/196], loss=67.6510
	step [180/196], loss=67.9794
	step [181/196], loss=68.4612
	step [182/196], loss=66.0806
	step [183/196], loss=72.6057
	step [184/196], loss=68.6701
	step [185/196], loss=75.3025
	step [186/196], loss=80.8263
	step [187/196], loss=74.0027
	step [188/196], loss=68.7215
	step [189/196], loss=61.8448
	step [190/196], loss=60.3334
	step [191/196], loss=64.3312
	step [192/196], loss=64.6449
	step [193/196], loss=59.5767
	step [194/196], loss=84.2275
	step [195/196], loss=63.4661
	step [196/196], loss=8.5032
	Evaluating
	loss=0.0059, precision=0.4697, recall=0.8470, f1=0.6043
Training epoch 91
	step [1/196], loss=78.3799
	step [2/196], loss=67.3417
	step [3/196], loss=70.7376
	step [4/196], loss=80.0308
	step [5/196], loss=77.0200
	step [6/196], loss=73.8810
	step [7/196], loss=71.1506
	step [8/196], loss=63.9865
	step [9/196], loss=74.1466
	step [10/196], loss=53.4767
	step [11/196], loss=74.9762
	step [12/196], loss=70.3744
	step [13/196], loss=71.1521
	step [14/196], loss=70.3487
	step [15/196], loss=60.0109
	step [16/196], loss=64.4659
	step [17/196], loss=69.4674
	step [18/196], loss=77.7221
	step [19/196], loss=74.2478
	step [20/196], loss=72.3956
	step [21/196], loss=90.3581
	step [22/196], loss=68.3571
	step [23/196], loss=71.9201
	step [24/196], loss=70.4703
	step [25/196], loss=79.3136
	step [26/196], loss=63.7897
	step [27/196], loss=70.3989
	step [28/196], loss=64.9453
	step [29/196], loss=66.2332
	step [30/196], loss=74.3651
	step [31/196], loss=71.7939
	step [32/196], loss=69.3072
	step [33/196], loss=61.3952
	step [34/196], loss=69.1231
	step [35/196], loss=77.4118
	step [36/196], loss=72.9101
	step [37/196], loss=70.8900
	step [38/196], loss=75.9016
	step [39/196], loss=77.2613
	step [40/196], loss=73.8789
	step [41/196], loss=57.2879
	step [42/196], loss=70.1953
	step [43/196], loss=61.0583
	step [44/196], loss=63.8486
	step [45/196], loss=71.9743
	step [46/196], loss=57.8948
	step [47/196], loss=70.8604
	step [48/196], loss=67.2752
	step [49/196], loss=59.1097
	step [50/196], loss=56.4080
	step [51/196], loss=65.1365
	step [52/196], loss=77.0767
	step [53/196], loss=75.9827
	step [54/196], loss=58.0390
	step [55/196], loss=62.3117
	step [56/196], loss=73.1955
	step [57/196], loss=79.7213
	step [58/196], loss=73.2490
	step [59/196], loss=63.9834
	step [60/196], loss=76.0159
	step [61/196], loss=69.0860
	step [62/196], loss=78.7983
	step [63/196], loss=81.7699
	step [64/196], loss=71.4703
	step [65/196], loss=61.6570
	step [66/196], loss=70.2129
	step [67/196], loss=81.0999
	step [68/196], loss=68.5291
	step [69/196], loss=62.6465
	step [70/196], loss=75.9873
	step [71/196], loss=67.5735
	step [72/196], loss=55.5331
	step [73/196], loss=62.9684
	step [74/196], loss=59.1495
	step [75/196], loss=62.5824
	step [76/196], loss=63.3657
	step [77/196], loss=66.6552
	step [78/196], loss=66.9365
	step [79/196], loss=68.2053
	step [80/196], loss=63.6560
	step [81/196], loss=65.2759
	step [82/196], loss=60.2260
	step [83/196], loss=76.4063
	step [84/196], loss=69.3873
	step [85/196], loss=66.6321
	step [86/196], loss=66.9939
	step [87/196], loss=63.4870
	step [88/196], loss=73.2250
	step [89/196], loss=60.9905
	step [90/196], loss=63.1509
	step [91/196], loss=80.6012
	step [92/196], loss=73.9492
	step [93/196], loss=69.6000
	step [94/196], loss=62.6027
	step [95/196], loss=60.8872
	step [96/196], loss=70.3416
	step [97/196], loss=69.3790
	step [98/196], loss=81.2035
	step [99/196], loss=61.5902
	step [100/196], loss=72.8125
	step [101/196], loss=64.5329
	step [102/196], loss=75.3920
	step [103/196], loss=74.7346
	step [104/196], loss=67.3115
	step [105/196], loss=74.3006
	step [106/196], loss=71.3673
	step [107/196], loss=67.7641
	step [108/196], loss=67.0850
	step [109/196], loss=73.8411
	step [110/196], loss=67.7671
	step [111/196], loss=77.6502
	step [112/196], loss=84.3509
	step [113/196], loss=74.1464
	step [114/196], loss=70.1828
	step [115/196], loss=62.0516
	step [116/196], loss=59.4113
	step [117/196], loss=76.1420
	step [118/196], loss=71.1066
	step [119/196], loss=71.6565
	step [120/196], loss=79.4943
	step [121/196], loss=73.9615
	step [122/196], loss=74.2577
	step [123/196], loss=59.1722
	step [124/196], loss=71.0893
	step [125/196], loss=58.9500
	step [126/196], loss=78.9199
	step [127/196], loss=64.4976
	step [128/196], loss=57.5590
	step [129/196], loss=65.1922
	step [130/196], loss=49.8325
	step [131/196], loss=67.7253
	step [132/196], loss=55.1782
	step [133/196], loss=70.8701
	step [134/196], loss=78.0313
	step [135/196], loss=74.9934
	step [136/196], loss=64.9487
	step [137/196], loss=73.9929
	step [138/196], loss=64.2558
	step [139/196], loss=69.2599
	step [140/196], loss=70.0674
	step [141/196], loss=66.2989
	step [142/196], loss=63.3926
	step [143/196], loss=70.6162
	step [144/196], loss=63.7723
	step [145/196], loss=65.7489
	step [146/196], loss=70.2887
	step [147/196], loss=69.6399
	step [148/196], loss=80.1103
	step [149/196], loss=73.2491
	step [150/196], loss=84.8657
	step [151/196], loss=69.0235
	step [152/196], loss=60.5230
	step [153/196], loss=74.1221
	step [154/196], loss=70.0567
	step [155/196], loss=64.2459
	step [156/196], loss=77.6931
	step [157/196], loss=57.8598
	step [158/196], loss=65.9631
	step [159/196], loss=68.7074
	step [160/196], loss=72.8712
	step [161/196], loss=82.1243
	step [162/196], loss=64.4312
	step [163/196], loss=69.2943
	step [164/196], loss=74.0687
	step [165/196], loss=77.2297
	step [166/196], loss=85.1828
	step [167/196], loss=61.9663
	step [168/196], loss=76.4047
	step [169/196], loss=70.5517
	step [170/196], loss=70.2339
	step [171/196], loss=63.4154
	step [172/196], loss=56.5683
	step [173/196], loss=64.2926
	step [174/196], loss=78.7718
	step [175/196], loss=69.7803
	step [176/196], loss=64.3967
	step [177/196], loss=68.7911
	step [178/196], loss=56.8234
	step [179/196], loss=78.9790
	step [180/196], loss=68.6754
	step [181/196], loss=63.2859
	step [182/196], loss=78.7583
	step [183/196], loss=75.0936
	step [184/196], loss=70.6324
	step [185/196], loss=77.1138
	step [186/196], loss=65.5803
	step [187/196], loss=72.4301
	step [188/196], loss=72.2978
	step [189/196], loss=66.5790
	step [190/196], loss=72.6295
	step [191/196], loss=65.1692
	step [192/196], loss=68.5666
	step [193/196], loss=67.2073
	step [194/196], loss=62.7278
	step [195/196], loss=71.0396
	step [196/196], loss=2.9932
	Evaluating
	loss=0.0068, precision=0.4227, recall=0.8461, f1=0.5637
Training epoch 92
	step [1/196], loss=70.3402
	step [2/196], loss=70.9500
	step [3/196], loss=68.7452
	step [4/196], loss=64.9760
	step [5/196], loss=68.1855
	step [6/196], loss=61.0143
	step [7/196], loss=58.1997
	step [8/196], loss=75.5373
	step [9/196], loss=58.2454
	step [10/196], loss=64.5072
	step [11/196], loss=66.3528
	step [12/196], loss=61.0535
	step [13/196], loss=63.5666
	step [14/196], loss=88.9950
	step [15/196], loss=72.8833
	step [16/196], loss=64.7065
	step [17/196], loss=59.5321
	step [18/196], loss=78.0193
	step [19/196], loss=75.2550
	step [20/196], loss=59.1936
	step [21/196], loss=71.0599
	step [22/196], loss=73.1700
	step [23/196], loss=70.2508
	step [24/196], loss=74.3532
	step [25/196], loss=65.6155
	step [26/196], loss=64.5089
	step [27/196], loss=82.2393
	step [28/196], loss=59.0197
	step [29/196], loss=61.8500
	step [30/196], loss=75.7499
	step [31/196], loss=72.6827
	step [32/196], loss=76.4896
	step [33/196], loss=63.2018
	step [34/196], loss=65.1372
	step [35/196], loss=65.4626
	step [36/196], loss=58.8396
	step [37/196], loss=60.9357
	step [38/196], loss=67.3694
	step [39/196], loss=69.4879
	step [40/196], loss=69.6366
	step [41/196], loss=62.6775
	step [42/196], loss=68.3649
	step [43/196], loss=59.7534
	step [44/196], loss=61.6281
	step [45/196], loss=63.8594
	step [46/196], loss=69.3018
	step [47/196], loss=50.0890
	step [48/196], loss=74.4484
	step [49/196], loss=80.0259
	step [50/196], loss=75.3806
	step [51/196], loss=74.1723
	step [52/196], loss=62.2151
	step [53/196], loss=72.4041
	step [54/196], loss=66.2662
	step [55/196], loss=79.1149
	step [56/196], loss=61.4063
	step [57/196], loss=70.0901
	step [58/196], loss=78.2384
	step [59/196], loss=63.0241
	step [60/196], loss=69.8694
	step [61/196], loss=66.9572
	step [62/196], loss=70.9337
	step [63/196], loss=68.0197
	step [64/196], loss=68.2310
	step [65/196], loss=77.1492
	step [66/196], loss=77.8828
	step [67/196], loss=66.3579
	step [68/196], loss=64.6688
	step [69/196], loss=72.6351
	step [70/196], loss=61.1194
	step [71/196], loss=69.2582
	step [72/196], loss=79.7807
	step [73/196], loss=69.8801
	step [74/196], loss=69.9370
	step [75/196], loss=71.9770
	step [76/196], loss=59.3444
	step [77/196], loss=56.8068
	step [78/196], loss=78.0093
	step [79/196], loss=56.0161
	step [80/196], loss=77.9804
	step [81/196], loss=66.0851
	step [82/196], loss=76.1300
	step [83/196], loss=70.4577
	step [84/196], loss=66.2309
	step [85/196], loss=71.9781
	step [86/196], loss=81.0654
	step [87/196], loss=72.6670
	step [88/196], loss=71.8754
	step [89/196], loss=73.5308
	step [90/196], loss=70.7416
	step [91/196], loss=63.3997
	step [92/196], loss=56.4269
	step [93/196], loss=80.5192
	step [94/196], loss=77.4157
	step [95/196], loss=67.3659
	step [96/196], loss=66.4398
	step [97/196], loss=69.8939
	step [98/196], loss=64.9420
	step [99/196], loss=67.8047
	step [100/196], loss=66.5116
	step [101/196], loss=64.4874
	step [102/196], loss=68.3867
	step [103/196], loss=66.3685
	step [104/196], loss=65.9368
	step [105/196], loss=83.7451
	step [106/196], loss=70.9009
	step [107/196], loss=58.1653
	step [108/196], loss=63.8281
	step [109/196], loss=56.2617
	step [110/196], loss=67.6315
	step [111/196], loss=78.2366
	step [112/196], loss=65.0974
	step [113/196], loss=69.9937
	step [114/196], loss=67.5294
	step [115/196], loss=69.3642
	step [116/196], loss=73.0640
	step [117/196], loss=68.8006
	step [118/196], loss=80.7265
	step [119/196], loss=74.4712
	step [120/196], loss=71.2960
	step [121/196], loss=65.9256
	step [122/196], loss=69.5103
	step [123/196], loss=69.7481
	step [124/196], loss=73.9248
	step [125/196], loss=76.6170
	step [126/196], loss=85.7802
	step [127/196], loss=64.6659
	step [128/196], loss=73.5463
	step [129/196], loss=55.3480
	step [130/196], loss=58.7057
	step [131/196], loss=52.5371
	step [132/196], loss=64.7405
	step [133/196], loss=74.5187
	step [134/196], loss=76.3360
	step [135/196], loss=67.9422
	step [136/196], loss=63.0581
	step [137/196], loss=77.0742
	step [138/196], loss=76.9514
	step [139/196], loss=66.7825
	step [140/196], loss=69.2896
	step [141/196], loss=74.1021
	step [142/196], loss=66.5680
	step [143/196], loss=79.5600
	step [144/196], loss=74.8453
	step [145/196], loss=72.3813
	step [146/196], loss=58.6462
	step [147/196], loss=69.8274
	step [148/196], loss=80.7427
	step [149/196], loss=66.3880
	step [150/196], loss=71.0840
	step [151/196], loss=72.5965
	step [152/196], loss=70.2613
	step [153/196], loss=70.8130
	step [154/196], loss=72.6045
	step [155/196], loss=63.7872
	step [156/196], loss=63.0725
	step [157/196], loss=59.5055
	step [158/196], loss=84.9310
	step [159/196], loss=81.8477
	step [160/196], loss=66.1967
	step [161/196], loss=63.5016
	step [162/196], loss=71.2116
	step [163/196], loss=72.8605
	step [164/196], loss=67.9005
	step [165/196], loss=67.9072
	step [166/196], loss=71.7581
	step [167/196], loss=78.5734
	step [168/196], loss=65.4787
	step [169/196], loss=63.2013
	step [170/196], loss=65.0474
	step [171/196], loss=71.9803
	step [172/196], loss=72.9219
	step [173/196], loss=66.0529
	step [174/196], loss=86.3985
	step [175/196], loss=69.1312
	step [176/196], loss=64.5201
	step [177/196], loss=68.8996
	step [178/196], loss=74.2969
	step [179/196], loss=68.4945
	step [180/196], loss=71.3250
	step [181/196], loss=70.4575
	step [182/196], loss=65.0745
	step [183/196], loss=65.1340
	step [184/196], loss=76.6230
	step [185/196], loss=65.2267
	step [186/196], loss=62.4847
	step [187/196], loss=75.0805
	step [188/196], loss=75.0854
	step [189/196], loss=72.1911
	step [190/196], loss=63.6575
	step [191/196], loss=63.4312
	step [192/196], loss=76.8432
	step [193/196], loss=86.2216
	step [194/196], loss=65.8238
	step [195/196], loss=78.0681
	step [196/196], loss=5.5264
	Evaluating
	loss=0.0059, precision=0.4579, recall=0.8456, f1=0.5941
Training epoch 93
	step [1/196], loss=71.2983
	step [2/196], loss=64.1494
	step [3/196], loss=70.5520
	step [4/196], loss=89.0765
	step [5/196], loss=76.5484
	step [6/196], loss=64.0227
	step [7/196], loss=74.9929
	step [8/196], loss=71.4529
	step [9/196], loss=71.6897
	step [10/196], loss=65.5403
	step [11/196], loss=68.8778
	step [12/196], loss=70.4966
	step [13/196], loss=63.0280
	step [14/196], loss=73.7992
	step [15/196], loss=51.7850
	step [16/196], loss=65.4663
	step [17/196], loss=69.5019
	step [18/196], loss=67.8791
	step [19/196], loss=71.8658
	step [20/196], loss=66.0306
	step [21/196], loss=73.7998
	step [22/196], loss=66.4289
	step [23/196], loss=71.3742
	step [24/196], loss=70.8797
	step [25/196], loss=71.2006
	step [26/196], loss=60.5442
	step [27/196], loss=68.4396
	step [28/196], loss=54.0464
	step [29/196], loss=70.1236
	step [30/196], loss=77.0082
	step [31/196], loss=70.7062
	step [32/196], loss=71.8023
	step [33/196], loss=66.6940
	step [34/196], loss=72.6924
	step [35/196], loss=55.3944
	step [36/196], loss=78.3278
	step [37/196], loss=71.6913
	step [38/196], loss=53.8932
	step [39/196], loss=76.6733
	step [40/196], loss=52.7706
	step [41/196], loss=64.0022
	step [42/196], loss=67.6379
	step [43/196], loss=85.3051
	step [44/196], loss=69.4824
	step [45/196], loss=70.7869
	step [46/196], loss=75.5435
	step [47/196], loss=65.5508
	step [48/196], loss=72.3961
	step [49/196], loss=64.8437
	step [50/196], loss=68.9578
	step [51/196], loss=65.6714
	step [52/196], loss=66.8433
	step [53/196], loss=68.4115
	step [54/196], loss=60.4194
	step [55/196], loss=70.9874
	step [56/196], loss=60.0299
	step [57/196], loss=72.9189
	step [58/196], loss=66.3918
	step [59/196], loss=59.2532
	step [60/196], loss=66.8860
	step [61/196], loss=86.1254
	step [62/196], loss=69.9476
	step [63/196], loss=73.8635
	step [64/196], loss=64.4690
	step [65/196], loss=70.2078
	step [66/196], loss=65.7419
	step [67/196], loss=76.2245
	step [68/196], loss=66.3436
	step [69/196], loss=58.9718
	step [70/196], loss=64.4136
	step [71/196], loss=70.7272
	step [72/196], loss=64.7890
	step [73/196], loss=75.0264
	step [74/196], loss=73.1929
	step [75/196], loss=68.8784
	step [76/196], loss=60.0585
	step [77/196], loss=67.3031
	step [78/196], loss=57.9717
	step [79/196], loss=58.6317
	step [80/196], loss=57.7042
	step [81/196], loss=68.1786
	step [82/196], loss=71.3257
	step [83/196], loss=57.1929
	step [84/196], loss=65.5333
	step [85/196], loss=85.1570
	step [86/196], loss=73.8134
	step [87/196], loss=60.4142
	step [88/196], loss=67.4746
	step [89/196], loss=59.5667
	step [90/196], loss=65.2437
	step [91/196], loss=73.9895
	step [92/196], loss=68.5284
	step [93/196], loss=75.9839
	step [94/196], loss=69.6896
	step [95/196], loss=62.9045
	step [96/196], loss=51.3324
	step [97/196], loss=64.9284
	step [98/196], loss=80.5906
	step [99/196], loss=67.2552
	step [100/196], loss=82.3563
	step [101/196], loss=61.8923
	step [102/196], loss=87.3837
	step [103/196], loss=86.9239
	step [104/196], loss=72.3282
	step [105/196], loss=70.3066
	step [106/196], loss=73.6462
	step [107/196], loss=84.1919
	step [108/196], loss=72.4806
	step [109/196], loss=72.1944
	step [110/196], loss=66.1460
	step [111/196], loss=71.9048
	step [112/196], loss=64.4276
	step [113/196], loss=74.5598
	step [114/196], loss=77.6924
	step [115/196], loss=64.4162
	step [116/196], loss=61.6028
	step [117/196], loss=72.9349
	step [118/196], loss=73.6622
	step [119/196], loss=61.7411
	step [120/196], loss=77.3600
	step [121/196], loss=75.6248
	step [122/196], loss=71.2838
	step [123/196], loss=78.4077
	step [124/196], loss=65.7029
	step [125/196], loss=76.3724
	step [126/196], loss=53.5312
	step [127/196], loss=55.3310
	step [128/196], loss=55.8550
	step [129/196], loss=75.8058
	step [130/196], loss=72.2745
	step [131/196], loss=67.1350
	step [132/196], loss=60.7840
	step [133/196], loss=65.4808
	step [134/196], loss=71.7111
	step [135/196], loss=62.3258
	step [136/196], loss=74.2915
	step [137/196], loss=69.4033
	step [138/196], loss=77.1056
	step [139/196], loss=73.4072
	step [140/196], loss=72.2282
	step [141/196], loss=68.5025
	step [142/196], loss=68.4863
	step [143/196], loss=75.1115
	step [144/196], loss=60.2792
	step [145/196], loss=63.7910
	step [146/196], loss=63.4319
	step [147/196], loss=72.9090
	step [148/196], loss=59.6426
	step [149/196], loss=68.9317
	step [150/196], loss=62.6773
	step [151/196], loss=68.0244
	step [152/196], loss=72.1362
	step [153/196], loss=68.7237
	step [154/196], loss=70.4291
	step [155/196], loss=69.2021
	step [156/196], loss=71.6993
	step [157/196], loss=78.2999
	step [158/196], loss=73.1795
	step [159/196], loss=81.9998
	step [160/196], loss=70.6276
	step [161/196], loss=84.0871
	step [162/196], loss=69.0985
	step [163/196], loss=82.8392
	step [164/196], loss=84.6131
	step [165/196], loss=66.4106
	step [166/196], loss=65.0840
	step [167/196], loss=66.9564
	step [168/196], loss=72.0710
	step [169/196], loss=64.9660
	step [170/196], loss=74.3572
	step [171/196], loss=60.8621
	step [172/196], loss=68.4042
	step [173/196], loss=67.1962
	step [174/196], loss=75.0678
	step [175/196], loss=67.3304
	step [176/196], loss=63.8746
	step [177/196], loss=63.4405
	step [178/196], loss=69.8857
	step [179/196], loss=72.9735
	step [180/196], loss=77.1647
	step [181/196], loss=78.2202
	step [182/196], loss=64.0456
	step [183/196], loss=55.9785
	step [184/196], loss=68.4751
	step [185/196], loss=68.2697
	step [186/196], loss=66.0001
	step [187/196], loss=64.1849
	step [188/196], loss=69.5412
	step [189/196], loss=61.5903
	step [190/196], loss=61.9235
	step [191/196], loss=75.3258
	step [192/196], loss=72.2083
	step [193/196], loss=67.3967
	step [194/196], loss=79.4952
	step [195/196], loss=53.8745
	step [196/196], loss=3.5614
	Evaluating
	loss=0.0059, precision=0.4594, recall=0.8485, f1=0.5961
Training epoch 94
	step [1/196], loss=80.1699
	step [2/196], loss=71.5441
	step [3/196], loss=64.1933
	step [4/196], loss=76.3151
	step [5/196], loss=63.0405
	step [6/196], loss=62.2817
	step [7/196], loss=73.0092
	step [8/196], loss=75.2818
	step [9/196], loss=68.9769
	step [10/196], loss=63.6409
	step [11/196], loss=71.0641
	step [12/196], loss=63.5063
	step [13/196], loss=57.9920
	step [14/196], loss=60.7761
	step [15/196], loss=64.9544
	step [16/196], loss=68.4324
	step [17/196], loss=57.9669
	step [18/196], loss=70.4202
	step [19/196], loss=62.9641
	step [20/196], loss=68.9788
	step [21/196], loss=58.7323
	step [22/196], loss=71.2452
	step [23/196], loss=65.2787
	step [24/196], loss=69.7846
	step [25/196], loss=66.0545
	step [26/196], loss=63.8847
	step [27/196], loss=69.9439
	step [28/196], loss=76.1280
	step [29/196], loss=75.9709
	step [30/196], loss=66.1119
	step [31/196], loss=77.8027
	step [32/196], loss=66.9831
	step [33/196], loss=60.0086
	step [34/196], loss=69.3644
	step [35/196], loss=75.5820
	step [36/196], loss=78.3716
	step [37/196], loss=72.0197
	step [38/196], loss=72.3398
	step [39/196], loss=58.6830
	step [40/196], loss=75.6747
	step [41/196], loss=70.9178
	step [42/196], loss=72.6547
	step [43/196], loss=64.7626
	step [44/196], loss=61.1127
	step [45/196], loss=71.0594
	step [46/196], loss=73.7873
	step [47/196], loss=62.6151
	step [48/196], loss=74.4126
	step [49/196], loss=68.2306
	step [50/196], loss=66.6200
	step [51/196], loss=64.6145
	step [52/196], loss=81.6448
	step [53/196], loss=73.0264
	step [54/196], loss=71.6485
	step [55/196], loss=67.2046
	step [56/196], loss=62.7978
	step [57/196], loss=59.9323
	step [58/196], loss=68.2817
	step [59/196], loss=74.5078
	step [60/196], loss=66.2379
	step [61/196], loss=68.4717
	step [62/196], loss=68.0727
	step [63/196], loss=73.2835
	step [64/196], loss=67.4823
	step [65/196], loss=60.5649
	step [66/196], loss=69.9138
	step [67/196], loss=58.5878
	step [68/196], loss=63.6896
	step [69/196], loss=65.9941
	step [70/196], loss=63.9879
	step [71/196], loss=57.5300
	step [72/196], loss=61.5426
	step [73/196], loss=62.7255
	step [74/196], loss=71.0694
	step [75/196], loss=73.6773
	step [76/196], loss=58.8944
	step [77/196], loss=65.4823
	step [78/196], loss=81.4681
	step [79/196], loss=71.6927
	step [80/196], loss=66.3623
	step [81/196], loss=68.8976
	step [82/196], loss=73.1689
	step [83/196], loss=73.9002
	step [84/196], loss=62.9042
	step [85/196], loss=66.0467
	step [86/196], loss=64.3647
	step [87/196], loss=65.5568
	step [88/196], loss=70.9123
	step [89/196], loss=66.6715
	step [90/196], loss=80.7513
	step [91/196], loss=68.1340
	step [92/196], loss=68.7257
	step [93/196], loss=70.3785
	step [94/196], loss=69.9548
	step [95/196], loss=87.8076
	step [96/196], loss=76.8266
	step [97/196], loss=64.9205
	step [98/196], loss=62.4700
	step [99/196], loss=66.6267
	step [100/196], loss=69.9539
	step [101/196], loss=68.1557
	step [102/196], loss=59.4585
	step [103/196], loss=68.3249
	step [104/196], loss=66.1118
	step [105/196], loss=82.0182
	step [106/196], loss=75.0070
	step [107/196], loss=70.9521
	step [108/196], loss=77.2130
	step [109/196], loss=77.2199
	step [110/196], loss=59.7905
	step [111/196], loss=57.6265
	step [112/196], loss=66.0996
	step [113/196], loss=68.2522
	step [114/196], loss=72.7118
	step [115/196], loss=77.6534
	step [116/196], loss=69.0671
	step [117/196], loss=63.0748
	step [118/196], loss=72.6526
	step [119/196], loss=62.1383
	step [120/196], loss=55.9882
	step [121/196], loss=72.7872
	step [122/196], loss=71.9913
	step [123/196], loss=75.0793
	step [124/196], loss=74.9691
	step [125/196], loss=72.6497
	step [126/196], loss=66.6934
	step [127/196], loss=66.0896
	step [128/196], loss=77.7040
	step [129/196], loss=74.6350
	step [130/196], loss=70.8101
	step [131/196], loss=65.2346
	step [132/196], loss=85.2550
	step [133/196], loss=64.1191
	step [134/196], loss=69.9130
	step [135/196], loss=57.8932
	step [136/196], loss=87.6717
	step [137/196], loss=70.8144
	step [138/196], loss=64.7377
	step [139/196], loss=66.3913
	step [140/196], loss=73.7454
	step [141/196], loss=72.0953
	step [142/196], loss=73.9956
	step [143/196], loss=68.3733
	step [144/196], loss=65.4471
	step [145/196], loss=59.1857
	step [146/196], loss=79.1608
	step [147/196], loss=69.3499
	step [148/196], loss=67.2549
	step [149/196], loss=68.7480
	step [150/196], loss=84.0498
	step [151/196], loss=66.9461
	step [152/196], loss=64.7168
	step [153/196], loss=65.1507
	step [154/196], loss=59.3183
	step [155/196], loss=61.5019
	step [156/196], loss=56.7984
	step [157/196], loss=57.1383
	step [158/196], loss=76.6174
	step [159/196], loss=79.0572
	step [160/196], loss=74.7302
	step [161/196], loss=66.7640
	step [162/196], loss=64.8418
	step [163/196], loss=58.7072
	step [164/196], loss=62.1283
	step [165/196], loss=68.3163
	step [166/196], loss=63.7210
	step [167/196], loss=60.5653
	step [168/196], loss=74.2000
	step [169/196], loss=54.7873
	step [170/196], loss=67.7758
	step [171/196], loss=58.2834
	step [172/196], loss=67.9379
	step [173/196], loss=75.7679
	step [174/196], loss=66.4904
	step [175/196], loss=69.1119
	step [176/196], loss=75.0955
	step [177/196], loss=68.4435
	step [178/196], loss=66.4424
	step [179/196], loss=66.6257
	step [180/196], loss=64.2315
	step [181/196], loss=61.5746
	step [182/196], loss=70.9526
	step [183/196], loss=63.7103
	step [184/196], loss=81.9313
	step [185/196], loss=80.7201
	step [186/196], loss=66.3094
	step [187/196], loss=79.3947
	step [188/196], loss=68.0206
	step [189/196], loss=75.6148
	step [190/196], loss=88.7710
	step [191/196], loss=68.0907
	step [192/196], loss=72.3836
	step [193/196], loss=76.5008
	step [194/196], loss=58.1210
	step [195/196], loss=69.2507
	step [196/196], loss=5.9386
	Evaluating
	loss=0.0056, precision=0.4966, recall=0.8465, f1=0.6260
Training epoch 95
	step [1/196], loss=72.1929
	step [2/196], loss=76.8236
	step [3/196], loss=69.9733
	step [4/196], loss=58.8297
	step [5/196], loss=64.7770
	step [6/196], loss=78.4468
	step [7/196], loss=67.7086
	step [8/196], loss=73.0978
	step [9/196], loss=57.1211
	step [10/196], loss=67.8289
	step [11/196], loss=64.8116
	step [12/196], loss=74.5787
	step [13/196], loss=72.2836
	step [14/196], loss=67.2737
	step [15/196], loss=68.2418
	step [16/196], loss=79.4816
	step [17/196], loss=55.5187
	step [18/196], loss=57.1003
	step [19/196], loss=79.2423
	step [20/196], loss=67.6542
	step [21/196], loss=67.0877
	step [22/196], loss=61.5693
	step [23/196], loss=71.9693
	step [24/196], loss=61.8124
	step [25/196], loss=61.7763
	step [26/196], loss=80.6793
	step [27/196], loss=81.5693
	step [28/196], loss=74.8847
	step [29/196], loss=59.2216
	step [30/196], loss=72.7614
	step [31/196], loss=82.0763
	step [32/196], loss=65.3037
	step [33/196], loss=72.4463
	step [34/196], loss=56.1102
	step [35/196], loss=67.3842
	step [36/196], loss=60.1295
	step [37/196], loss=55.1920
	step [38/196], loss=79.2989
	step [39/196], loss=72.4467
	step [40/196], loss=76.3445
	step [41/196], loss=63.3355
	step [42/196], loss=68.6163
	step [43/196], loss=82.5373
	step [44/196], loss=70.0270
	step [45/196], loss=60.0836
	step [46/196], loss=71.3299
	step [47/196], loss=56.7166
	step [48/196], loss=65.4079
	step [49/196], loss=77.7040
	step [50/196], loss=56.5966
	step [51/196], loss=74.8317
	step [52/196], loss=65.1259
	step [53/196], loss=73.8041
	step [54/196], loss=69.1098
	step [55/196], loss=57.6137
	step [56/196], loss=72.3570
	step [57/196], loss=75.3182
	step [58/196], loss=62.7848
	step [59/196], loss=79.0046
	step [60/196], loss=64.4282
	step [61/196], loss=54.6587
	step [62/196], loss=83.0902
	step [63/196], loss=79.9695
	step [64/196], loss=60.4384
	step [65/196], loss=67.3976
	step [66/196], loss=64.3959
	step [67/196], loss=63.2237
	step [68/196], loss=65.0988
	step [69/196], loss=59.2512
	step [70/196], loss=63.6720
	step [71/196], loss=69.0223
	step [72/196], loss=66.4160
	step [73/196], loss=75.5577
	step [74/196], loss=78.1903
	step [75/196], loss=60.1411
	step [76/196], loss=67.7077
	step [77/196], loss=66.6236
	step [78/196], loss=64.4992
	step [79/196], loss=67.3103
	step [80/196], loss=62.8278
	step [81/196], loss=63.5314
	step [82/196], loss=72.4558
	step [83/196], loss=59.5114
	step [84/196], loss=69.3731
	step [85/196], loss=80.8758
	step [86/196], loss=79.0432
	step [87/196], loss=72.7913
	step [88/196], loss=70.8059
	step [89/196], loss=72.6009
	step [90/196], loss=72.0664
	step [91/196], loss=61.8317
	step [92/196], loss=72.4662
	step [93/196], loss=68.2548
	step [94/196], loss=73.0747
	step [95/196], loss=59.7378
	step [96/196], loss=51.3206
	step [97/196], loss=66.4069
	step [98/196], loss=55.3097
	step [99/196], loss=64.7618
	step [100/196], loss=78.3167
	step [101/196], loss=64.6203
	step [102/196], loss=74.1538
	step [103/196], loss=70.3902
	step [104/196], loss=55.3298
	step [105/196], loss=72.5550
	step [106/196], loss=67.1744
	step [107/196], loss=59.2272
	step [108/196], loss=65.2943
	step [109/196], loss=83.2899
	step [110/196], loss=61.7404
	step [111/196], loss=76.9301
	step [112/196], loss=72.5580
	step [113/196], loss=62.9408
	step [114/196], loss=71.1064
	step [115/196], loss=78.9234
	step [116/196], loss=64.9931
	step [117/196], loss=68.5899
	step [118/196], loss=66.6319
	step [119/196], loss=61.2938
	step [120/196], loss=79.1035
	step [121/196], loss=72.6898
	step [122/196], loss=59.1589
	step [123/196], loss=67.9157
	step [124/196], loss=64.2377
	step [125/196], loss=71.4847
	step [126/196], loss=76.4251
	step [127/196], loss=74.3297
	step [128/196], loss=68.2330
	step [129/196], loss=68.7808
	step [130/196], loss=61.9077
	step [131/196], loss=63.6887
	step [132/196], loss=75.7460
	step [133/196], loss=56.9978
	step [134/196], loss=59.7604
	step [135/196], loss=72.3473
	step [136/196], loss=60.7108
	step [137/196], loss=64.9582
	step [138/196], loss=66.6733
	step [139/196], loss=66.5140
	step [140/196], loss=68.6500
	step [141/196], loss=73.5409
	step [142/196], loss=72.5183
	step [143/196], loss=80.9410
	step [144/196], loss=74.8540
	step [145/196], loss=61.5443
	step [146/196], loss=71.5060
	step [147/196], loss=69.8587
	step [148/196], loss=60.2318
	step [149/196], loss=74.3138
	step [150/196], loss=69.4924
	step [151/196], loss=73.8813
	step [152/196], loss=69.1871
	step [153/196], loss=72.2515
	step [154/196], loss=62.3487
	step [155/196], loss=71.6325
	step [156/196], loss=60.2131
	step [157/196], loss=69.7738
	step [158/196], loss=71.4917
	step [159/196], loss=74.8232
	step [160/196], loss=75.4487
	step [161/196], loss=70.2479
	step [162/196], loss=61.8949
	step [163/196], loss=75.8270
	step [164/196], loss=73.0602
	step [165/196], loss=55.4130
	step [166/196], loss=73.6661
	step [167/196], loss=71.9681
	step [168/196], loss=64.7379
	step [169/196], loss=71.7671
	step [170/196], loss=74.9502
	step [171/196], loss=70.9426
	step [172/196], loss=73.2343
	step [173/196], loss=70.9713
	step [174/196], loss=60.9726
	step [175/196], loss=77.9552
	step [176/196], loss=67.4801
	step [177/196], loss=60.5008
	step [178/196], loss=67.7579
	step [179/196], loss=68.8074
	step [180/196], loss=80.3446
	step [181/196], loss=75.4230
	step [182/196], loss=64.2626
	step [183/196], loss=74.3868
	step [184/196], loss=74.9464
	step [185/196], loss=64.0992
	step [186/196], loss=61.5190
	step [187/196], loss=66.1762
	step [188/196], loss=61.1783
	step [189/196], loss=72.9412
	step [190/196], loss=65.3484
	step [191/196], loss=79.6150
	step [192/196], loss=74.0818
	step [193/196], loss=63.5585
	step [194/196], loss=63.8704
	step [195/196], loss=63.7748
	step [196/196], loss=2.2992
	Evaluating
	loss=0.0050, precision=0.5249, recall=0.8374, f1=0.6453
Training epoch 96
	step [1/196], loss=68.9726
	step [2/196], loss=67.2098
	step [3/196], loss=68.9631
	step [4/196], loss=74.4936
	step [5/196], loss=68.0990
	step [6/196], loss=70.0098
	step [7/196], loss=52.5759
	step [8/196], loss=57.4432
	step [9/196], loss=66.4847
	step [10/196], loss=58.0552
	step [11/196], loss=65.4174
	step [12/196], loss=62.6636
	step [13/196], loss=73.6576
	step [14/196], loss=71.7341
	step [15/196], loss=60.8880
	step [16/196], loss=72.9834
	step [17/196], loss=69.6512
	step [18/196], loss=65.2333
	step [19/196], loss=58.1816
	step [20/196], loss=62.3253
	step [21/196], loss=72.8758
	step [22/196], loss=60.7455
	step [23/196], loss=82.0336
	step [24/196], loss=67.8688
	step [25/196], loss=55.8150
	step [26/196], loss=67.4177
	step [27/196], loss=69.7557
	step [28/196], loss=60.8424
	step [29/196], loss=64.0210
	step [30/196], loss=75.2308
	step [31/196], loss=62.6922
	step [32/196], loss=64.6611
	step [33/196], loss=72.8340
	step [34/196], loss=79.1635
	step [35/196], loss=73.2982
	step [36/196], loss=75.2401
	step [37/196], loss=66.8766
	step [38/196], loss=65.7972
	step [39/196], loss=54.8967
	step [40/196], loss=71.0607
	step [41/196], loss=76.0588
	step [42/196], loss=61.6085
	step [43/196], loss=68.1237
	step [44/196], loss=77.2114
	step [45/196], loss=60.4760
	step [46/196], loss=65.4124
	step [47/196], loss=67.7133
	step [48/196], loss=60.7023
	step [49/196], loss=69.4853
	step [50/196], loss=68.2582
	step [51/196], loss=64.4202
	step [52/196], loss=70.8703
	step [53/196], loss=73.0746
	step [54/196], loss=64.8994
	step [55/196], loss=80.0351
	step [56/196], loss=79.1087
	step [57/196], loss=68.2352
	step [58/196], loss=63.7580
	step [59/196], loss=64.2085
	step [60/196], loss=56.3132
	step [61/196], loss=63.4627
	step [62/196], loss=63.6346
	step [63/196], loss=64.9865
	step [64/196], loss=69.3903
	step [65/196], loss=67.6259
	step [66/196], loss=66.5990
	step [67/196], loss=82.0883
	step [68/196], loss=70.5946
	step [69/196], loss=90.7673
	step [70/196], loss=73.2400
	step [71/196], loss=52.8903
	step [72/196], loss=64.3316
	step [73/196], loss=86.7763
	step [74/196], loss=66.6251
	step [75/196], loss=72.2594
	step [76/196], loss=64.3500
	step [77/196], loss=66.8965
	step [78/196], loss=70.1297
	step [79/196], loss=81.9982
	step [80/196], loss=64.2175
	step [81/196], loss=61.0718
	step [82/196], loss=67.9853
	step [83/196], loss=64.7686
	step [84/196], loss=80.5933
	step [85/196], loss=63.5287
	step [86/196], loss=69.2652
	step [87/196], loss=67.3144
	step [88/196], loss=67.9802
	step [89/196], loss=62.4815
	step [90/196], loss=63.3431
	step [91/196], loss=78.7116
	step [92/196], loss=81.8935
	step [93/196], loss=69.3176
	step [94/196], loss=67.6440
	step [95/196], loss=60.1834
	step [96/196], loss=58.8619
	step [97/196], loss=63.2939
	step [98/196], loss=69.2598
	step [99/196], loss=71.1266
	step [100/196], loss=64.8903
	step [101/196], loss=63.9033
	step [102/196], loss=77.1705
	step [103/196], loss=78.9781
	step [104/196], loss=78.1794
	step [105/196], loss=61.9582
	step [106/196], loss=71.1628
	step [107/196], loss=66.8414
	step [108/196], loss=72.1458
	step [109/196], loss=68.9091
	step [110/196], loss=71.2350
	step [111/196], loss=73.4206
	step [112/196], loss=58.5549
	step [113/196], loss=62.3653
	step [114/196], loss=72.8892
	step [115/196], loss=61.1686
	step [116/196], loss=71.3237
	step [117/196], loss=65.1359
	step [118/196], loss=71.4551
	step [119/196], loss=72.1273
	step [120/196], loss=67.5534
	step [121/196], loss=69.8362
	step [122/196], loss=66.0594
	step [123/196], loss=76.8281
	step [124/196], loss=72.6068
	step [125/196], loss=59.6161
	step [126/196], loss=72.0374
	step [127/196], loss=62.6572
	step [128/196], loss=70.5147
	step [129/196], loss=78.0745
	step [130/196], loss=74.4860
	step [131/196], loss=58.1015
	step [132/196], loss=60.3384
	step [133/196], loss=56.8362
	step [134/196], loss=49.5833
	step [135/196], loss=72.4732
	step [136/196], loss=75.5751
	step [137/196], loss=73.7936
	step [138/196], loss=66.7529
	step [139/196], loss=75.1762
	step [140/196], loss=83.6967
	step [141/196], loss=63.6526
	step [142/196], loss=71.9161
	step [143/196], loss=58.9491
	step [144/196], loss=78.1398
	step [145/196], loss=59.9552
	step [146/196], loss=73.6451
	step [147/196], loss=67.0043
	step [148/196], loss=72.8062
	step [149/196], loss=76.6156
	step [150/196], loss=74.5555
	step [151/196], loss=57.9233
	step [152/196], loss=68.7293
	step [153/196], loss=64.2939
	step [154/196], loss=72.5533
	step [155/196], loss=73.2115
	step [156/196], loss=64.6285
	step [157/196], loss=80.3273
	step [158/196], loss=70.4674
	step [159/196], loss=71.4919
	step [160/196], loss=70.8199
	step [161/196], loss=60.7461
	step [162/196], loss=64.0061
	step [163/196], loss=67.0504
	step [164/196], loss=87.1824
	step [165/196], loss=74.4158
	step [166/196], loss=63.6802
	step [167/196], loss=66.8366
	step [168/196], loss=79.6207
	step [169/196], loss=67.2293
	step [170/196], loss=81.8062
	step [171/196], loss=75.7016
	step [172/196], loss=67.2012
	step [173/196], loss=70.5617
	step [174/196], loss=58.2003
	step [175/196], loss=79.7598
	step [176/196], loss=62.7636
	step [177/196], loss=75.4938
	step [178/196], loss=77.8931
	step [179/196], loss=67.7243
	step [180/196], loss=57.3750
	step [181/196], loss=65.6505
	step [182/196], loss=60.8604
	step [183/196], loss=66.2813
	step [184/196], loss=69.2988
	step [185/196], loss=76.4670
	step [186/196], loss=78.4644
	step [187/196], loss=64.4823
	step [188/196], loss=66.6806
	step [189/196], loss=67.9226
	step [190/196], loss=67.8792
	step [191/196], loss=77.0409
	step [192/196], loss=68.8819
	step [193/196], loss=66.7661
	step [194/196], loss=64.7583
	step [195/196], loss=71.2214
	step [196/196], loss=3.3001
	Evaluating
	loss=0.0053, precision=0.4876, recall=0.8576, f1=0.6217
Training finished
best_f1: 0.6455630281435408
directing: Z rim_enhanced: True test_id 4
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 13163 # image files with weight 13163
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_221_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_296_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_243_xwqg-A00121_2019-05-15.npy
# all image files: 16489 # all weight files in weight_dir: 2381 # image files with weight 2381
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/Z 13163
Using 4 GPUs
Going to train epochs [31-80]
Training epoch 31
	step [1/206], loss=82.4147
	step [2/206], loss=84.8501
	step [3/206], loss=96.7191
	step [4/206], loss=77.1249
	step [5/206], loss=89.9972
	step [6/206], loss=79.3309
	step [7/206], loss=85.1261
	step [8/206], loss=78.6462
	step [9/206], loss=105.9883
	step [10/206], loss=93.2659
	step [11/206], loss=95.3128
	step [12/206], loss=83.3023
	step [13/206], loss=95.4834
	step [14/206], loss=86.3719
	step [15/206], loss=100.4536
	step [16/206], loss=82.1924
	step [17/206], loss=85.6374
	step [18/206], loss=99.7499
	step [19/206], loss=102.0522
	step [20/206], loss=93.4478
	step [21/206], loss=103.1570
	step [22/206], loss=71.4109
	step [23/206], loss=95.6885
	step [24/206], loss=84.9188
	step [25/206], loss=93.4730
	step [26/206], loss=88.1226
	step [27/206], loss=95.7266
	step [28/206], loss=86.4617
	step [29/206], loss=82.3578
	step [30/206], loss=83.8358
	step [31/206], loss=84.9578
	step [32/206], loss=83.7830
	step [33/206], loss=106.7223
	step [34/206], loss=93.8115
	step [35/206], loss=85.6596
	step [36/206], loss=80.1411
	step [37/206], loss=87.8013
	step [38/206], loss=98.2104
	step [39/206], loss=89.6778
	step [40/206], loss=102.0594
	step [41/206], loss=86.8047
	step [42/206], loss=97.0492
	step [43/206], loss=86.7160
	step [44/206], loss=82.4127
	step [45/206], loss=99.5187
	step [46/206], loss=98.6035
	step [47/206], loss=92.9052
	step [48/206], loss=83.7711
	step [49/206], loss=87.7745
	step [50/206], loss=92.0913
	step [51/206], loss=96.0476
	step [52/206], loss=78.6141
	step [53/206], loss=89.0766
	step [54/206], loss=93.4839
	step [55/206], loss=84.2412
	step [56/206], loss=99.1855
	step [57/206], loss=94.9293
	step [58/206], loss=91.5968
	step [59/206], loss=89.2576
	step [60/206], loss=78.0815
	step [61/206], loss=96.4112
	step [62/206], loss=87.5093
	step [63/206], loss=85.3301
	step [64/206], loss=92.3724
	step [65/206], loss=85.0548
	step [66/206], loss=89.3909
	step [67/206], loss=91.9177
	step [68/206], loss=79.6334
	step [69/206], loss=94.7160
	step [70/206], loss=86.3228
	step [71/206], loss=83.1326
	step [72/206], loss=81.3153
	step [73/206], loss=95.2566
	step [74/206], loss=86.6738
	step [75/206], loss=95.2263
	step [76/206], loss=98.3957
	step [77/206], loss=90.8217
	step [78/206], loss=90.2086
	step [79/206], loss=93.3848
	step [80/206], loss=90.9248
	step [81/206], loss=89.7162
	step [82/206], loss=78.6632
	step [83/206], loss=80.4000
	step [84/206], loss=89.2855
	step [85/206], loss=95.9478
	step [86/206], loss=90.2164
	step [87/206], loss=82.8437
	step [88/206], loss=89.9550
	step [89/206], loss=100.1956
	step [90/206], loss=77.4327
	step [91/206], loss=90.2851
	step [92/206], loss=90.6844
	step [93/206], loss=92.7978
	step [94/206], loss=79.5928
	step [95/206], loss=102.1429
	step [96/206], loss=99.4292
	step [97/206], loss=87.2307
	step [98/206], loss=97.2897
	step [99/206], loss=90.0985
	step [100/206], loss=102.7157
	step [101/206], loss=88.2386
	step [102/206], loss=96.1353
	step [103/206], loss=93.2424
	step [104/206], loss=85.0317
	step [105/206], loss=85.1221
	step [106/206], loss=86.5601
	step [107/206], loss=86.9485
	step [108/206], loss=99.1573
	step [109/206], loss=87.5815
	step [110/206], loss=84.0484
	step [111/206], loss=80.3064
	step [112/206], loss=92.4119
	step [113/206], loss=92.4021
	step [114/206], loss=70.9470
	step [115/206], loss=82.7652
	step [116/206], loss=82.6624
	step [117/206], loss=79.0183
	step [118/206], loss=76.6763
	step [119/206], loss=90.9011
	step [120/206], loss=98.5642
	step [121/206], loss=100.9899
	step [122/206], loss=79.8199
	step [123/206], loss=76.9714
	step [124/206], loss=73.9771
	step [125/206], loss=94.0280
	step [126/206], loss=108.4779
	step [127/206], loss=78.6269
	step [128/206], loss=98.1216
	step [129/206], loss=90.6278
	step [130/206], loss=90.0130
	step [131/206], loss=85.8918
	step [132/206], loss=92.9822
	step [133/206], loss=95.8866
	step [134/206], loss=84.0783
	step [135/206], loss=93.6534
	step [136/206], loss=96.4246
	step [137/206], loss=90.1574
	step [138/206], loss=94.7315
	step [139/206], loss=102.2367
	step [140/206], loss=90.6110
	step [141/206], loss=99.9580
	step [142/206], loss=98.0161
	step [143/206], loss=87.6288
	step [144/206], loss=106.1067
	step [145/206], loss=92.6760
	step [146/206], loss=85.4106
	step [147/206], loss=90.2109
	step [148/206], loss=69.7164
	step [149/206], loss=80.8319
	step [150/206], loss=83.7925
	step [151/206], loss=78.4175
	step [152/206], loss=104.9097
	step [153/206], loss=85.4336
	step [154/206], loss=82.5799
	step [155/206], loss=84.7045
	step [156/206], loss=86.6271
	step [157/206], loss=85.3358
	step [158/206], loss=92.1917
	step [159/206], loss=80.3428
	step [160/206], loss=94.2164
	step [161/206], loss=72.4144
	step [162/206], loss=105.9014
	step [163/206], loss=81.7442
	step [164/206], loss=79.2106
	step [165/206], loss=78.9608
	step [166/206], loss=91.8172
	step [167/206], loss=100.8793
	step [168/206], loss=86.6146
	step [169/206], loss=91.0034
	step [170/206], loss=94.5619
	step [171/206], loss=82.6473
	step [172/206], loss=99.7228
	step [173/206], loss=95.9371
	step [174/206], loss=106.9308
	step [175/206], loss=80.3221
	step [176/206], loss=87.5868
	step [177/206], loss=79.2510
	step [178/206], loss=91.5757
	step [179/206], loss=101.0732
	step [180/206], loss=103.5776
	step [181/206], loss=88.8090
	step [182/206], loss=97.1098
	step [183/206], loss=87.3690
	step [184/206], loss=70.8706
	step [185/206], loss=90.7742
	step [186/206], loss=79.6549
	step [187/206], loss=77.2102
	step [188/206], loss=77.4618
	step [189/206], loss=99.3425
	step [190/206], loss=98.6965
	step [191/206], loss=83.3373
	step [192/206], loss=89.8880
	step [193/206], loss=79.1049
	step [194/206], loss=79.3365
	step [195/206], loss=91.3649
	step [196/206], loss=96.0479
	step [197/206], loss=70.6469
	step [198/206], loss=99.6498
	step [199/206], loss=100.5429
	step [200/206], loss=93.9441
	step [201/206], loss=96.9711
	step [202/206], loss=83.2583
	step [203/206], loss=85.7558
	step [204/206], loss=91.0195
	step [205/206], loss=75.0469
	step [206/206], loss=79.2821
	Evaluating
	loss=0.0130, precision=0.3318, recall=0.9148, f1=0.4869
saving model as: 4_saved_model.pth
Training epoch 32
	step [1/206], loss=69.8904
	step [2/206], loss=86.5677
	step [3/206], loss=104.1885
	step [4/206], loss=106.0830
	step [5/206], loss=90.2045
	step [6/206], loss=77.1238
	step [7/206], loss=85.0729
	step [8/206], loss=75.9635
	step [9/206], loss=69.3211
	step [10/206], loss=89.7973
	step [11/206], loss=93.6718
	step [12/206], loss=91.8767
	step [13/206], loss=98.2864
	step [14/206], loss=97.5081
	step [15/206], loss=90.3887
	step [16/206], loss=85.1748
	step [17/206], loss=101.8635
	step [18/206], loss=88.7747
	step [19/206], loss=84.7098
	step [20/206], loss=92.8338
	step [21/206], loss=88.7612
	step [22/206], loss=97.8375
	step [23/206], loss=93.8902
	step [24/206], loss=99.9838
	step [25/206], loss=80.9354
	step [26/206], loss=102.8240
	step [27/206], loss=85.6444
	step [28/206], loss=86.3528
	step [29/206], loss=82.4090
	step [30/206], loss=87.3262
	step [31/206], loss=82.8643
	step [32/206], loss=88.0645
	step [33/206], loss=81.7122
	step [34/206], loss=84.5186
	step [35/206], loss=98.0165
	step [36/206], loss=84.0426
	step [37/206], loss=80.3268
	step [38/206], loss=88.4923
	step [39/206], loss=87.1858
	step [40/206], loss=87.5596
	step [41/206], loss=89.9528
	step [42/206], loss=84.0775
	step [43/206], loss=83.6397
	step [44/206], loss=81.9182
	step [45/206], loss=73.0426
	step [46/206], loss=85.1985
	step [47/206], loss=89.5040
	step [48/206], loss=100.8136
	step [49/206], loss=90.5232
	step [50/206], loss=95.2259
	step [51/206], loss=84.3744
	step [52/206], loss=87.1820
	step [53/206], loss=89.2212
	step [54/206], loss=91.5939
	step [55/206], loss=81.1513
	step [56/206], loss=93.9507
	step [57/206], loss=96.2843
	step [58/206], loss=73.9761
	step [59/206], loss=79.2371
	step [60/206], loss=92.9900
	step [61/206], loss=87.0533
	step [62/206], loss=86.5941
	step [63/206], loss=97.4182
	step [64/206], loss=81.5187
	step [65/206], loss=77.4408
	step [66/206], loss=94.0821
	step [67/206], loss=84.6999
	step [68/206], loss=92.4226
	step [69/206], loss=100.7612
	step [70/206], loss=83.9655
	step [71/206], loss=89.9026
	step [72/206], loss=84.6514
	step [73/206], loss=91.5129
	step [74/206], loss=84.2745
	step [75/206], loss=76.6406
	step [76/206], loss=86.5286
	step [77/206], loss=89.2034
	step [78/206], loss=73.1588
	step [79/206], loss=90.8678
	step [80/206], loss=83.4778
	step [81/206], loss=87.6655
	step [82/206], loss=92.9645
	step [83/206], loss=81.5120
	step [84/206], loss=77.9714
	step [85/206], loss=97.7199
	step [86/206], loss=78.9102
	step [87/206], loss=93.6159
	step [88/206], loss=89.9346
	step [89/206], loss=70.0820
	step [90/206], loss=85.6768
	step [91/206], loss=86.8573
	step [92/206], loss=86.9845
	step [93/206], loss=75.4475
	step [94/206], loss=85.9912
	step [95/206], loss=88.6889
	step [96/206], loss=92.9525
	step [97/206], loss=108.3628
	step [98/206], loss=85.8943
	step [99/206], loss=86.8354
	step [100/206], loss=80.4116
	step [101/206], loss=82.1286
	step [102/206], loss=100.7423
	step [103/206], loss=79.0049
	step [104/206], loss=81.1556
	step [105/206], loss=88.6366
	step [106/206], loss=88.3896
	step [107/206], loss=89.4242
	step [108/206], loss=81.1880
	step [109/206], loss=78.5103
	step [110/206], loss=75.7861
	step [111/206], loss=89.4970
	step [112/206], loss=82.8498
	step [113/206], loss=93.3490
	step [114/206], loss=104.9577
	step [115/206], loss=94.7940
	step [116/206], loss=95.5862
	step [117/206], loss=95.1350
	step [118/206], loss=68.9741
	step [119/206], loss=91.5750
	step [120/206], loss=86.8619
	step [121/206], loss=89.2982
	step [122/206], loss=87.7716
	step [123/206], loss=83.3981
	step [124/206], loss=107.7190
	step [125/206], loss=100.8177
	step [126/206], loss=76.1012
	step [127/206], loss=78.7659
	step [128/206], loss=102.3612
	step [129/206], loss=83.8975
	step [130/206], loss=88.0408
	step [131/206], loss=101.4241
	step [132/206], loss=102.6891
	step [133/206], loss=85.4118
	step [134/206], loss=91.7973
	step [135/206], loss=81.0044
	step [136/206], loss=87.0632
	step [137/206], loss=88.3512
	step [138/206], loss=94.8972
	step [139/206], loss=90.2951
	step [140/206], loss=94.2229
	step [141/206], loss=95.1820
	step [142/206], loss=83.6856
	step [143/206], loss=79.9857
	step [144/206], loss=102.6122
	step [145/206], loss=90.5091
	step [146/206], loss=92.4409
	step [147/206], loss=90.6366
	step [148/206], loss=90.6062
	step [149/206], loss=102.6123
	step [150/206], loss=85.0115
	step [151/206], loss=81.4593
	step [152/206], loss=87.8843
	step [153/206], loss=93.1633
	step [154/206], loss=85.3171
	step [155/206], loss=90.4336
	step [156/206], loss=86.9060
	step [157/206], loss=89.8862
	step [158/206], loss=80.3113
	step [159/206], loss=88.7718
	step [160/206], loss=96.4644
	step [161/206], loss=106.6307
	step [162/206], loss=90.8666
	step [163/206], loss=98.5504
	step [164/206], loss=92.8536
	step [165/206], loss=88.8276
	step [166/206], loss=84.1319
	step [167/206], loss=98.7631
	step [168/206], loss=83.5160
	step [169/206], loss=83.4545
	step [170/206], loss=97.2268
	step [171/206], loss=93.3224
	step [172/206], loss=108.4828
	step [173/206], loss=74.6088
	step [174/206], loss=113.4031
	step [175/206], loss=96.4276
	step [176/206], loss=87.5087
	step [177/206], loss=77.6760
	step [178/206], loss=95.2966
	step [179/206], loss=93.9206
	step [180/206], loss=100.4028
	step [181/206], loss=93.7758
	step [182/206], loss=87.4490
	step [183/206], loss=84.9853
	step [184/206], loss=92.5346
	step [185/206], loss=83.9246
	step [186/206], loss=99.9699
	step [187/206], loss=88.5230
	step [188/206], loss=84.3981
	step [189/206], loss=94.1124
	step [190/206], loss=91.4654
	step [191/206], loss=92.9373
	step [192/206], loss=85.7696
	step [193/206], loss=74.6243
	step [194/206], loss=92.4852
	step [195/206], loss=90.9693
	step [196/206], loss=80.8119
	step [197/206], loss=98.2445
	step [198/206], loss=87.3116
	step [199/206], loss=93.1154
	step [200/206], loss=79.7093
	step [201/206], loss=89.4991
	step [202/206], loss=97.5719
	step [203/206], loss=82.9595
	step [204/206], loss=94.2668
	step [205/206], loss=79.5038
	step [206/206], loss=56.3499
	Evaluating
	loss=0.0124, precision=0.3593, recall=0.9103, f1=0.5152
saving model as: 4_saved_model.pth
Training epoch 33
	step [1/206], loss=78.6988
	step [2/206], loss=100.8753
	step [3/206], loss=96.3386
	step [4/206], loss=96.1263
	step [5/206], loss=78.0492
	step [6/206], loss=92.7070
	step [7/206], loss=89.5218
	step [8/206], loss=80.7666
	step [9/206], loss=87.7678
	step [10/206], loss=73.8391
	step [11/206], loss=93.6912
	step [12/206], loss=72.9117
	step [13/206], loss=87.8155
	step [14/206], loss=90.2494
	step [15/206], loss=93.2413
	step [16/206], loss=82.9099
	step [17/206], loss=97.7380
	step [18/206], loss=91.9546
	step [19/206], loss=84.5710
	step [20/206], loss=108.7678
	step [21/206], loss=97.0572
	step [22/206], loss=83.4910
	step [23/206], loss=95.1412
	step [24/206], loss=83.8232
	step [25/206], loss=73.7903
	step [26/206], loss=93.5907
	step [27/206], loss=88.4380
	step [28/206], loss=81.5518
	step [29/206], loss=84.5418
	step [30/206], loss=97.8863
	step [31/206], loss=84.9811
	step [32/206], loss=77.7572
	step [33/206], loss=94.2935
	step [34/206], loss=99.8872
	step [35/206], loss=99.2663
	step [36/206], loss=106.0319
	step [37/206], loss=97.3887
	step [38/206], loss=85.7475
	step [39/206], loss=84.5269
	step [40/206], loss=87.8049
	step [41/206], loss=85.5020
	step [42/206], loss=105.6769
	step [43/206], loss=85.4538
	step [44/206], loss=89.2881
	step [45/206], loss=86.4099
	step [46/206], loss=92.3048
	step [47/206], loss=95.2340
	step [48/206], loss=82.4234
	step [49/206], loss=102.2080
	step [50/206], loss=99.1424
	step [51/206], loss=94.7213
	step [52/206], loss=105.5018
	step [53/206], loss=92.0929
	step [54/206], loss=96.0836
	step [55/206], loss=95.8090
	step [56/206], loss=86.3776
	step [57/206], loss=84.9051
	step [58/206], loss=86.7986
	step [59/206], loss=94.0346
	step [60/206], loss=79.1294
	step [61/206], loss=70.5177
	step [62/206], loss=80.6681
	step [63/206], loss=88.0600
	step [64/206], loss=94.2349
	step [65/206], loss=94.9329
	step [66/206], loss=85.6949
	step [67/206], loss=92.8065
	step [68/206], loss=94.3617
	step [69/206], loss=87.2571
	step [70/206], loss=91.0097
	step [71/206], loss=85.2297
	step [72/206], loss=109.6214
	step [73/206], loss=74.2557
	step [74/206], loss=88.9429
	step [75/206], loss=79.9189
	step [76/206], loss=80.8340
	step [77/206], loss=82.1198
	step [78/206], loss=88.7852
	step [79/206], loss=83.0626
	step [80/206], loss=103.5241
	step [81/206], loss=93.7584
	step [82/206], loss=65.1249
	step [83/206], loss=85.9928
	step [84/206], loss=80.9242
	step [85/206], loss=80.3595
	step [86/206], loss=73.3850
	step [87/206], loss=86.1874
	step [88/206], loss=89.1768
	step [89/206], loss=86.5582
	step [90/206], loss=81.1739
	step [91/206], loss=88.3292
	step [92/206], loss=70.9256
	step [93/206], loss=91.8056
	step [94/206], loss=88.5419
	step [95/206], loss=72.2506
	step [96/206], loss=85.1493
	step [97/206], loss=83.7029
	step [98/206], loss=89.6174
	step [99/206], loss=97.7558
	step [100/206], loss=83.0373
	step [101/206], loss=80.9181
	step [102/206], loss=74.3844
	step [103/206], loss=96.4645
	step [104/206], loss=84.8067
	step [105/206], loss=86.3133
	step [106/206], loss=99.3383
	step [107/206], loss=98.3225
	step [108/206], loss=85.7301
	step [109/206], loss=105.6700
	step [110/206], loss=106.8287
	step [111/206], loss=82.2435
	step [112/206], loss=104.0101
	step [113/206], loss=90.9147
	step [114/206], loss=87.4895
	step [115/206], loss=80.5666
	step [116/206], loss=84.6433
	step [117/206], loss=81.7782
	step [118/206], loss=86.8950
	step [119/206], loss=81.6012
	step [120/206], loss=92.6069
	step [121/206], loss=87.3571
	step [122/206], loss=91.3083
	step [123/206], loss=91.5412
	step [124/206], loss=96.3504
	step [125/206], loss=96.3234
	step [126/206], loss=99.0914
	step [127/206], loss=85.3946
	step [128/206], loss=90.8503
	step [129/206], loss=92.7790
	step [130/206], loss=87.7397
	step [131/206], loss=90.6370
	step [132/206], loss=82.4751
	step [133/206], loss=107.1790
	step [134/206], loss=91.2814
	step [135/206], loss=69.5090
	step [136/206], loss=86.5295
	step [137/206], loss=98.0790
	step [138/206], loss=93.9551
	step [139/206], loss=88.4046
	step [140/206], loss=86.3806
	step [141/206], loss=79.9885
	step [142/206], loss=73.8773
	step [143/206], loss=88.6683
	step [144/206], loss=83.9811
	step [145/206], loss=79.1682
	step [146/206], loss=94.4717
	step [147/206], loss=85.7897
	step [148/206], loss=84.3670
	step [149/206], loss=105.8986
	step [150/206], loss=99.4187
	step [151/206], loss=79.6545
	step [152/206], loss=66.0790
	step [153/206], loss=105.2351
	step [154/206], loss=82.3965
	step [155/206], loss=83.2909
	step [156/206], loss=83.7481
	step [157/206], loss=80.1048
	step [158/206], loss=91.8842
	step [159/206], loss=81.8214
	step [160/206], loss=73.1638
	step [161/206], loss=83.6139
	step [162/206], loss=81.6378
	step [163/206], loss=77.8107
	step [164/206], loss=78.1592
	step [165/206], loss=81.4272
	step [166/206], loss=90.9471
	step [167/206], loss=87.4256
	step [168/206], loss=99.1176
	step [169/206], loss=87.4884
	step [170/206], loss=84.1407
	step [171/206], loss=80.4639
	step [172/206], loss=98.9331
	step [173/206], loss=93.5651
	step [174/206], loss=83.7648
	step [175/206], loss=80.4963
	step [176/206], loss=92.5824
	step [177/206], loss=85.3859
	step [178/206], loss=71.5569
	step [179/206], loss=83.3722
	step [180/206], loss=78.4357
	step [181/206], loss=83.3407
	step [182/206], loss=81.5485
	step [183/206], loss=92.2142
	step [184/206], loss=93.2988
	step [185/206], loss=69.9232
	step [186/206], loss=95.1805
	step [187/206], loss=85.2025
	step [188/206], loss=102.7123
	step [189/206], loss=81.5203
	step [190/206], loss=79.5688
	step [191/206], loss=90.8735
	step [192/206], loss=88.3091
	step [193/206], loss=92.3118
	step [194/206], loss=93.2685
	step [195/206], loss=75.4762
	step [196/206], loss=99.6550
	step [197/206], loss=90.3453
	step [198/206], loss=88.7057
	step [199/206], loss=84.2756
	step [200/206], loss=85.3316
	step [201/206], loss=74.8108
	step [202/206], loss=88.0417
	step [203/206], loss=96.9121
	step [204/206], loss=97.2074
	step [205/206], loss=84.4588
	step [206/206], loss=54.1093
	Evaluating
	loss=0.0136, precision=0.3221, recall=0.9162, f1=0.4767
Training epoch 34
	step [1/206], loss=84.1163
	step [2/206], loss=78.5691
	step [3/206], loss=87.1613
	step [4/206], loss=88.3712
	step [5/206], loss=85.2901
	step [6/206], loss=79.4544
	step [7/206], loss=77.7899
	step [8/206], loss=92.6899
	step [9/206], loss=87.2327
	step [10/206], loss=82.0748
	step [11/206], loss=97.4737
	step [12/206], loss=104.1535
	step [13/206], loss=81.5355
	step [14/206], loss=90.9047
	step [15/206], loss=90.2858
	step [16/206], loss=91.9290
	step [17/206], loss=88.2572
	step [18/206], loss=80.9929
	step [19/206], loss=89.9419
	step [20/206], loss=78.4998
	step [21/206], loss=91.9551
	step [22/206], loss=88.1634
	step [23/206], loss=86.8759
	step [24/206], loss=83.5618
	step [25/206], loss=102.9025
	step [26/206], loss=96.8556
	step [27/206], loss=84.1832
	step [28/206], loss=96.7423
	step [29/206], loss=83.1695
	step [30/206], loss=86.6783
	step [31/206], loss=86.2642
	step [32/206], loss=84.3689
	step [33/206], loss=94.2389
	step [34/206], loss=83.6414
	step [35/206], loss=92.2689
	step [36/206], loss=106.9498
	step [37/206], loss=88.1027
	step [38/206], loss=97.1657
	step [39/206], loss=97.8832
	step [40/206], loss=91.8546
	step [41/206], loss=89.3038
	step [42/206], loss=85.2065
	step [43/206], loss=90.3695
	step [44/206], loss=82.5280
	step [45/206], loss=80.0940
	step [46/206], loss=90.9194
	step [47/206], loss=70.2922
	step [48/206], loss=92.1731
	step [49/206], loss=80.3988
	step [50/206], loss=85.8367
	step [51/206], loss=91.3140
	step [52/206], loss=88.0907
	step [53/206], loss=89.6294
	step [54/206], loss=85.5378
	step [55/206], loss=96.1672
	step [56/206], loss=75.6247
	step [57/206], loss=82.4120
	step [58/206], loss=102.9252
	step [59/206], loss=94.0060
	step [60/206], loss=75.5280
	step [61/206], loss=86.9807
	step [62/206], loss=103.7551
	step [63/206], loss=95.1240
	step [64/206], loss=77.7985
	step [65/206], loss=93.7100
	step [66/206], loss=77.4783
	step [67/206], loss=88.6121
	step [68/206], loss=94.4018
	step [69/206], loss=84.6999
	step [70/206], loss=95.5872
	step [71/206], loss=85.2812
	step [72/206], loss=79.7545
	step [73/206], loss=75.8534
	step [74/206], loss=94.3053
	step [75/206], loss=78.5449
	step [76/206], loss=94.0520
	step [77/206], loss=74.6776
	step [78/206], loss=81.6305
	step [79/206], loss=83.5027
	step [80/206], loss=93.5971
	step [81/206], loss=85.1921
	step [82/206], loss=85.3749
	step [83/206], loss=76.0481
	step [84/206], loss=74.4305
	step [85/206], loss=82.9036
	step [86/206], loss=90.6737
	step [87/206], loss=80.8327
	step [88/206], loss=80.4643
	step [89/206], loss=79.7553
	step [90/206], loss=87.7649
	step [91/206], loss=74.5030
	step [92/206], loss=99.0659
	step [93/206], loss=84.9083
	step [94/206], loss=93.9899
	step [95/206], loss=91.5796
	step [96/206], loss=73.4960
	step [97/206], loss=89.5218
	step [98/206], loss=93.0949
	step [99/206], loss=70.8490
	step [100/206], loss=99.6566
	step [101/206], loss=97.9024
	step [102/206], loss=73.6440
	step [103/206], loss=80.3924
	step [104/206], loss=87.4063
	step [105/206], loss=82.0547
	step [106/206], loss=92.3686
	step [107/206], loss=100.6317
	step [108/206], loss=94.6818
	step [109/206], loss=78.0766
	step [110/206], loss=87.0686
	step [111/206], loss=83.2547
	step [112/206], loss=76.0261
	step [113/206], loss=86.1484
	step [114/206], loss=69.4250
	step [115/206], loss=104.3930
	step [116/206], loss=88.8146
	step [117/206], loss=81.8561
	step [118/206], loss=87.9303
	step [119/206], loss=73.8102
	step [120/206], loss=81.0560
	step [121/206], loss=91.6039
	step [122/206], loss=88.5434
	step [123/206], loss=105.8875
	step [124/206], loss=86.8375
	step [125/206], loss=79.2098
	step [126/206], loss=79.2764
	step [127/206], loss=89.9557
	step [128/206], loss=84.5537
	step [129/206], loss=94.5301
	step [130/206], loss=81.9105
	step [131/206], loss=106.3033
	step [132/206], loss=96.9066
	step [133/206], loss=85.1392
	step [134/206], loss=81.7560
	step [135/206], loss=80.3857
	step [136/206], loss=79.2082
	step [137/206], loss=88.5188
	step [138/206], loss=105.4737
	step [139/206], loss=94.8077
	step [140/206], loss=80.6052
	step [141/206], loss=96.5613
	step [142/206], loss=100.1649
	step [143/206], loss=74.7380
	step [144/206], loss=85.8722
	step [145/206], loss=92.0072
	step [146/206], loss=91.9054
	step [147/206], loss=99.4408
	step [148/206], loss=91.8152
	step [149/206], loss=82.6601
	step [150/206], loss=80.8966
	step [151/206], loss=69.2450
	step [152/206], loss=104.9878
	step [153/206], loss=81.2081
	step [154/206], loss=99.4599
	step [155/206], loss=89.8485
	step [156/206], loss=104.9829
	step [157/206], loss=80.2775
	step [158/206], loss=83.7408
	step [159/206], loss=99.1481
	step [160/206], loss=96.0317
	step [161/206], loss=92.7466
	step [162/206], loss=89.4845
	step [163/206], loss=91.5055
	step [164/206], loss=98.4335
	step [165/206], loss=82.0614
	step [166/206], loss=79.6715
	step [167/206], loss=95.5536
	step [168/206], loss=78.1304
	step [169/206], loss=92.1697
	step [170/206], loss=78.4116
	step [171/206], loss=78.9024
	step [172/206], loss=76.8373
	step [173/206], loss=86.6834
	step [174/206], loss=81.5733
	step [175/206], loss=69.9989
	step [176/206], loss=71.3479
	step [177/206], loss=85.0306
	step [178/206], loss=100.4518
	step [179/206], loss=74.8743
	step [180/206], loss=91.1889
	step [181/206], loss=93.5291
	step [182/206], loss=95.4653
	step [183/206], loss=88.7323
	step [184/206], loss=74.8285
	step [185/206], loss=95.5677
	step [186/206], loss=88.4464
	step [187/206], loss=97.1145
	step [188/206], loss=83.6947
	step [189/206], loss=95.1113
	step [190/206], loss=85.2511
	step [191/206], loss=97.4250
	step [192/206], loss=87.0472
	step [193/206], loss=89.2521
	step [194/206], loss=92.2738
	step [195/206], loss=106.8773
	step [196/206], loss=81.7720
	step [197/206], loss=68.3237
	step [198/206], loss=82.8599
	step [199/206], loss=86.4782
	step [200/206], loss=86.7484
	step [201/206], loss=84.2267
	step [202/206], loss=84.9734
	step [203/206], loss=79.3853
	step [204/206], loss=89.1987
	step [205/206], loss=94.4217
	step [206/206], loss=60.7647
	Evaluating
	loss=0.0115, precision=0.3493, recall=0.9130, f1=0.5053
Training epoch 35
	step [1/206], loss=92.7029
	step [2/206], loss=72.3618
	step [3/206], loss=87.5080
	step [4/206], loss=83.7187
	step [5/206], loss=92.1096
	step [6/206], loss=90.0772
	step [7/206], loss=102.2572
	step [8/206], loss=91.8807
	step [9/206], loss=88.1254
	step [10/206], loss=78.5898
	step [11/206], loss=84.5584
	step [12/206], loss=82.6186
	step [13/206], loss=98.8357
	step [14/206], loss=91.4364
	step [15/206], loss=88.2160
	step [16/206], loss=101.4104
	step [17/206], loss=91.2179
	step [18/206], loss=94.8498
	step [19/206], loss=80.3451
	step [20/206], loss=85.6241
	step [21/206], loss=87.0602
	step [22/206], loss=79.0684
	step [23/206], loss=96.5889
	step [24/206], loss=87.5172
	step [25/206], loss=82.0195
	step [26/206], loss=86.5662
	step [27/206], loss=86.6721
	step [28/206], loss=93.8698
	step [29/206], loss=71.7276
	step [30/206], loss=84.5854
	step [31/206], loss=82.4702
	step [32/206], loss=102.3419
	step [33/206], loss=99.0525
	step [34/206], loss=92.5034
	step [35/206], loss=92.8217
	step [36/206], loss=92.9097
	step [37/206], loss=92.3739
	step [38/206], loss=99.2837
	step [39/206], loss=82.2673
	step [40/206], loss=96.4275
	step [41/206], loss=96.7457
	step [42/206], loss=92.9710
	step [43/206], loss=97.8284
	step [44/206], loss=81.2622
	step [45/206], loss=96.7661
	step [46/206], loss=103.2401
	step [47/206], loss=85.8218
	step [48/206], loss=88.5101
	step [49/206], loss=81.6250
	step [50/206], loss=98.7707
	step [51/206], loss=92.7442
	step [52/206], loss=72.7195
	step [53/206], loss=88.4575
	step [54/206], loss=88.3641
	step [55/206], loss=91.7508
	step [56/206], loss=82.3138
	step [57/206], loss=93.1039
	step [58/206], loss=96.7519
	step [59/206], loss=80.1080
	step [60/206], loss=91.9438
	step [61/206], loss=92.9550
	step [62/206], loss=89.8141
	step [63/206], loss=94.0763
	step [64/206], loss=86.7628
	step [65/206], loss=79.1033
	step [66/206], loss=84.9351
	step [67/206], loss=90.5739
	step [68/206], loss=73.6329
	step [69/206], loss=87.2934
	step [70/206], loss=76.4449
	step [71/206], loss=80.9153
	step [72/206], loss=74.4228
	step [73/206], loss=105.4821
	step [74/206], loss=89.7524
	step [75/206], loss=74.7881
	step [76/206], loss=82.0360
	step [77/206], loss=79.6606
	step [78/206], loss=91.7741
	step [79/206], loss=85.0067
	step [80/206], loss=86.8867
	step [81/206], loss=84.4948
	step [82/206], loss=88.7170
	step [83/206], loss=84.8195
	step [84/206], loss=94.5395
	step [85/206], loss=90.5371
	step [86/206], loss=85.9916
	step [87/206], loss=90.0596
	step [88/206], loss=91.7398
	step [89/206], loss=82.7188
	step [90/206], loss=87.2915
	step [91/206], loss=69.0821
	step [92/206], loss=88.1239
	step [93/206], loss=87.8860
	step [94/206], loss=80.7936
	step [95/206], loss=76.1323
	step [96/206], loss=106.6596
	step [97/206], loss=84.7902
	step [98/206], loss=78.3785
	step [99/206], loss=73.4683
	step [100/206], loss=103.2625
	step [101/206], loss=93.4067
	step [102/206], loss=87.8971
	step [103/206], loss=76.5276
	step [104/206], loss=92.5366
	step [105/206], loss=103.0258
	step [106/206], loss=81.6677
	step [107/206], loss=81.5763
	step [108/206], loss=94.9373
	step [109/206], loss=78.4577
	step [110/206], loss=84.5168
	step [111/206], loss=91.2956
	step [112/206], loss=78.2047
	step [113/206], loss=97.0527
	step [114/206], loss=94.7901
	step [115/206], loss=98.1886
	step [116/206], loss=89.9259
	step [117/206], loss=85.5519
	step [118/206], loss=83.3772
	step [119/206], loss=80.1532
	step [120/206], loss=89.0138
	step [121/206], loss=90.1368
	step [122/206], loss=80.3381
	step [123/206], loss=86.8386
	step [124/206], loss=86.5798
	step [125/206], loss=87.7889
	step [126/206], loss=100.3784
	step [127/206], loss=90.5227
	step [128/206], loss=82.9713
	step [129/206], loss=110.3704
	step [130/206], loss=87.9706
	step [131/206], loss=100.8136
	step [132/206], loss=103.3868
	step [133/206], loss=92.6912
	step [134/206], loss=88.3068
	step [135/206], loss=83.5817
	step [136/206], loss=92.0799
	step [137/206], loss=76.9091
	step [138/206], loss=83.0651
	step [139/206], loss=94.4465
	step [140/206], loss=71.1186
	step [141/206], loss=87.3216
	step [142/206], loss=76.2101
	step [143/206], loss=108.0496
	step [144/206], loss=82.1404
	step [145/206], loss=87.1387
	step [146/206], loss=90.9473
	step [147/206], loss=75.8142
	step [148/206], loss=73.9762
	step [149/206], loss=82.1578
	step [150/206], loss=83.6734
	step [151/206], loss=96.8226
	step [152/206], loss=79.8170
	step [153/206], loss=84.2188
	step [154/206], loss=92.0674
	step [155/206], loss=85.9123
	step [156/206], loss=70.3999
	step [157/206], loss=81.1785
	step [158/206], loss=83.7305
	step [159/206], loss=66.9924
	step [160/206], loss=114.3440
	step [161/206], loss=77.3634
	step [162/206], loss=93.0352
	step [163/206], loss=91.7006
	step [164/206], loss=83.2526
	step [165/206], loss=83.5837
	step [166/206], loss=90.4847
	step [167/206], loss=100.5412
	step [168/206], loss=83.1220
	step [169/206], loss=72.6839
	step [170/206], loss=80.1083
	step [171/206], loss=88.1921
	step [172/206], loss=72.8469
	step [173/206], loss=95.6380
	step [174/206], loss=85.0238
	step [175/206], loss=95.5583
	step [176/206], loss=78.1238
	step [177/206], loss=81.5389
	step [178/206], loss=95.8889
	step [179/206], loss=84.0883
	step [180/206], loss=93.9142
	step [181/206], loss=79.3554
	step [182/206], loss=81.0627
	step [183/206], loss=89.3906
	step [184/206], loss=78.7092
	step [185/206], loss=78.6468
	step [186/206], loss=85.1301
	step [187/206], loss=79.0880
	step [188/206], loss=94.6207
	step [189/206], loss=88.5939
	step [190/206], loss=86.9106
	step [191/206], loss=74.4060
	step [192/206], loss=91.0666
	step [193/206], loss=71.8229
	step [194/206], loss=74.6302
	step [195/206], loss=91.1995
	step [196/206], loss=72.8315
	step [197/206], loss=77.6467
	step [198/206], loss=80.9651
	step [199/206], loss=92.3651
	step [200/206], loss=79.0637
	step [201/206], loss=104.2504
	step [202/206], loss=97.4212
	step [203/206], loss=91.7872
	step [204/206], loss=77.1426
	step [205/206], loss=88.5200
	step [206/206], loss=54.3416
	Evaluating
	loss=0.0118, precision=0.3351, recall=0.9035, f1=0.4889
Training epoch 36
	step [1/206], loss=91.1807
	step [2/206], loss=87.2575
	step [3/206], loss=97.1658
	step [4/206], loss=78.6848
	step [5/206], loss=80.1091
	step [6/206], loss=87.3735
	step [7/206], loss=91.9177
	step [8/206], loss=86.7767
	step [9/206], loss=98.8545
	step [10/206], loss=87.8432
	step [11/206], loss=92.4585
	step [12/206], loss=85.3763
	step [13/206], loss=93.0879
	step [14/206], loss=90.5483
	step [15/206], loss=83.8946
	step [16/206], loss=82.7975
	step [17/206], loss=86.9078
	step [18/206], loss=97.6712
	step [19/206], loss=86.4688
	step [20/206], loss=86.2411
	step [21/206], loss=103.2846
	step [22/206], loss=86.3351
	step [23/206], loss=104.2312
	step [24/206], loss=78.9446
	step [25/206], loss=92.2721
	step [26/206], loss=86.6174
	step [27/206], loss=92.5516
	step [28/206], loss=78.4795
	step [29/206], loss=93.6747
	step [30/206], loss=79.7930
	step [31/206], loss=80.7757
	step [32/206], loss=83.1205
	step [33/206], loss=87.5584
	step [34/206], loss=90.7998
	step [35/206], loss=83.6857
	step [36/206], loss=84.6448
	step [37/206], loss=79.5831
	step [38/206], loss=83.4705
	step [39/206], loss=91.7217
	step [40/206], loss=84.2876
	step [41/206], loss=71.5266
	step [42/206], loss=93.8088
	step [43/206], loss=80.3668
	step [44/206], loss=83.7164
	step [45/206], loss=82.9332
	step [46/206], loss=84.3636
	step [47/206], loss=91.8792
	step [48/206], loss=79.8216
	step [49/206], loss=85.7248
	step [50/206], loss=86.2296
	step [51/206], loss=79.1201
	step [52/206], loss=96.3423
	step [53/206], loss=93.9495
	step [54/206], loss=93.8787
	step [55/206], loss=86.9090
	step [56/206], loss=84.0880
	step [57/206], loss=81.4667
	step [58/206], loss=84.2493
	step [59/206], loss=79.9054
	step [60/206], loss=85.8306
	step [61/206], loss=90.5094
	step [62/206], loss=67.4533
	step [63/206], loss=85.6573
	step [64/206], loss=96.5228
	step [65/206], loss=82.9663
	step [66/206], loss=88.4604
	step [67/206], loss=86.7676
	step [68/206], loss=91.5599
	step [69/206], loss=84.1881
	step [70/206], loss=76.8780
	step [71/206], loss=85.2547
	step [72/206], loss=90.0594
	step [73/206], loss=84.5132
	step [74/206], loss=96.8223
	step [75/206], loss=92.6779
	step [76/206], loss=78.9704
	step [77/206], loss=85.5860
	step [78/206], loss=87.1089
	step [79/206], loss=89.5321
	step [80/206], loss=82.4916
	step [81/206], loss=87.5750
	step [82/206], loss=83.4315
	step [83/206], loss=95.0157
	step [84/206], loss=97.6817
	step [85/206], loss=95.2084
	step [86/206], loss=92.2294
	step [87/206], loss=77.2980
	step [88/206], loss=82.6180
	step [89/206], loss=92.9698
	step [90/206], loss=77.0306
	step [91/206], loss=93.2186
	step [92/206], loss=95.0481
	step [93/206], loss=79.7053
	step [94/206], loss=78.7118
	step [95/206], loss=78.2215
	step [96/206], loss=97.7348
	step [97/206], loss=81.4172
	step [98/206], loss=82.8752
	step [99/206], loss=79.2949
	step [100/206], loss=97.2063
	step [101/206], loss=78.7919
	step [102/206], loss=85.1947
	step [103/206], loss=91.3108
	step [104/206], loss=83.0316
	step [105/206], loss=80.3292
	step [106/206], loss=96.7930
	step [107/206], loss=74.4678
	step [108/206], loss=88.3055
	step [109/206], loss=75.1452
	step [110/206], loss=72.1224
	step [111/206], loss=91.4159
	step [112/206], loss=64.8245
	step [113/206], loss=95.4148
	step [114/206], loss=96.6758
	step [115/206], loss=83.7844
	step [116/206], loss=102.0515
	step [117/206], loss=82.6586
	step [118/206], loss=69.5598
	step [119/206], loss=83.2149
	step [120/206], loss=86.9429
	step [121/206], loss=93.7278
	step [122/206], loss=81.7146
	step [123/206], loss=88.6183
	step [124/206], loss=92.7125
	step [125/206], loss=99.1506
	step [126/206], loss=89.3627
	step [127/206], loss=71.3583
	step [128/206], loss=89.8402
	step [129/206], loss=77.6352
	step [130/206], loss=68.0895
	step [131/206], loss=75.6142
	step [132/206], loss=96.0086
	step [133/206], loss=86.2795
	step [134/206], loss=84.6259
	step [135/206], loss=88.2293
	step [136/206], loss=95.3325
	step [137/206], loss=90.2440
	step [138/206], loss=81.2842
	step [139/206], loss=79.8198
	step [140/206], loss=79.5341
	step [141/206], loss=90.6067
	step [142/206], loss=87.2952
	step [143/206], loss=87.3597
	step [144/206], loss=86.3005
	step [145/206], loss=90.3512
	step [146/206], loss=91.3863
	step [147/206], loss=85.7823
	step [148/206], loss=97.8725
	step [149/206], loss=81.1292
	step [150/206], loss=93.2065
	step [151/206], loss=88.1296
	step [152/206], loss=82.3745
	step [153/206], loss=77.2627
	step [154/206], loss=91.4265
	step [155/206], loss=79.2033
	step [156/206], loss=99.0981
	step [157/206], loss=85.5972
	step [158/206], loss=91.9971
	step [159/206], loss=87.5792
	step [160/206], loss=80.6972
	step [161/206], loss=92.5862
	step [162/206], loss=70.5281
	step [163/206], loss=77.7710
	step [164/206], loss=103.2561
	step [165/206], loss=93.9924
	step [166/206], loss=83.1504
	step [167/206], loss=67.0835
	step [168/206], loss=102.1855
	step [169/206], loss=104.4654
	step [170/206], loss=96.4807
	step [171/206], loss=93.7784
	step [172/206], loss=75.3869
	step [173/206], loss=86.1749
	step [174/206], loss=97.4706
	step [175/206], loss=102.7422
	step [176/206], loss=75.3279
	step [177/206], loss=80.8767
	step [178/206], loss=84.0926
	step [179/206], loss=82.0206
	step [180/206], loss=81.8743
	step [181/206], loss=96.7790
	step [182/206], loss=72.7059
	step [183/206], loss=91.2963
	step [184/206], loss=91.1626
	step [185/206], loss=67.7262
	step [186/206], loss=88.4413
	step [187/206], loss=83.2025
	step [188/206], loss=84.8860
	step [189/206], loss=92.7057
	step [190/206], loss=87.6919
	step [191/206], loss=86.2441
	step [192/206], loss=90.2136
	step [193/206], loss=91.8695
	step [194/206], loss=99.6890
	step [195/206], loss=81.0168
	step [196/206], loss=74.8775
	step [197/206], loss=68.2992
	step [198/206], loss=96.7824
	step [199/206], loss=93.1689
	step [200/206], loss=95.5668
	step [201/206], loss=83.7058
	step [202/206], loss=84.4435
	step [203/206], loss=79.1242
	step [204/206], loss=80.8558
	step [205/206], loss=93.2900
	step [206/206], loss=73.3188
	Evaluating
	loss=0.0109, precision=0.3737, recall=0.9070, f1=0.5294
saving model as: 4_saved_model.pth
Training epoch 37
	step [1/206], loss=101.2725
	step [2/206], loss=75.0054
	step [3/206], loss=77.3201
	step [4/206], loss=86.5844
	step [5/206], loss=94.6661
	step [6/206], loss=70.4853
	step [7/206], loss=81.2247
	step [8/206], loss=83.3354
	step [9/206], loss=87.4677
	step [10/206], loss=99.9592
	step [11/206], loss=87.6211
	step [12/206], loss=93.8355
	step [13/206], loss=84.3528
	step [14/206], loss=85.7665
	step [15/206], loss=106.2285
	step [16/206], loss=79.5137
	step [17/206], loss=85.7183
	step [18/206], loss=97.2516
	step [19/206], loss=89.0131
	step [20/206], loss=74.9812
	step [21/206], loss=90.4956
	step [22/206], loss=85.0421
	step [23/206], loss=82.4326
	step [24/206], loss=92.2406
	step [25/206], loss=81.6563
	step [26/206], loss=90.4479
	step [27/206], loss=86.7680
	step [28/206], loss=95.8096
	step [29/206], loss=76.4357
	step [30/206], loss=96.7455
	step [31/206], loss=90.4571
	step [32/206], loss=78.7144
	step [33/206], loss=93.3014
	step [34/206], loss=84.8853
	step [35/206], loss=87.9832
	step [36/206], loss=66.7954
	step [37/206], loss=79.6350
	step [38/206], loss=78.7241
	step [39/206], loss=85.1988
	step [40/206], loss=93.3638
	step [41/206], loss=85.1733
	step [42/206], loss=99.5870
	step [43/206], loss=91.2277
	step [44/206], loss=78.3219
	step [45/206], loss=101.4725
	step [46/206], loss=95.0033
	step [47/206], loss=87.7471
	step [48/206], loss=79.2936
	step [49/206], loss=95.4543
	step [50/206], loss=91.6075
	step [51/206], loss=94.1384
	step [52/206], loss=84.8422
	step [53/206], loss=87.4391
	step [54/206], loss=80.4280
	step [55/206], loss=90.9388
	step [56/206], loss=80.9010
	step [57/206], loss=73.4239
	step [58/206], loss=87.2642
	step [59/206], loss=86.7151
	step [60/206], loss=95.6450
	step [61/206], loss=87.1384
	step [62/206], loss=95.0030
	step [63/206], loss=88.8267
	step [64/206], loss=71.0522
	step [65/206], loss=75.1945
	step [66/206], loss=90.7872
	step [67/206], loss=86.0743
	step [68/206], loss=84.9193
	step [69/206], loss=84.6679
	step [70/206], loss=91.5455
	step [71/206], loss=76.8253
	step [72/206], loss=98.4182
	step [73/206], loss=92.4241
	step [74/206], loss=86.6527
	step [75/206], loss=81.9185
	step [76/206], loss=85.1793
	step [77/206], loss=100.8416
	step [78/206], loss=88.9851
	step [79/206], loss=92.8310
	step [80/206], loss=76.8843
	step [81/206], loss=95.1756
	step [82/206], loss=80.0909
	step [83/206], loss=95.7238
	step [84/206], loss=63.9759
	step [85/206], loss=84.0559
	step [86/206], loss=78.7254
	step [87/206], loss=98.1962
	step [88/206], loss=94.5825
	step [89/206], loss=79.8517
	step [90/206], loss=94.9811
	step [91/206], loss=83.6623
	step [92/206], loss=91.5890
	step [93/206], loss=83.7157
	step [94/206], loss=90.8584
	step [95/206], loss=97.5393
	step [96/206], loss=83.3415
	step [97/206], loss=96.0947
	step [98/206], loss=90.7610
	step [99/206], loss=84.7826
	step [100/206], loss=84.5027
	step [101/206], loss=86.4005
	step [102/206], loss=82.1633
	step [103/206], loss=79.1785
	step [104/206], loss=78.1845
	step [105/206], loss=107.2919
	step [106/206], loss=93.1286
	step [107/206], loss=98.2425
	step [108/206], loss=85.1306
	step [109/206], loss=81.0552
	step [110/206], loss=78.7797
	step [111/206], loss=89.2676
	step [112/206], loss=86.5622
	step [113/206], loss=88.8591
	step [114/206], loss=87.4531
	step [115/206], loss=72.2354
	step [116/206], loss=73.2804
	step [117/206], loss=88.9236
	step [118/206], loss=89.6333
	step [119/206], loss=92.2218
	step [120/206], loss=82.7987
	step [121/206], loss=92.6552
	step [122/206], loss=98.1637
	step [123/206], loss=95.4001
	step [124/206], loss=85.4567
	step [125/206], loss=105.3066
	step [126/206], loss=78.6105
	step [127/206], loss=80.4636
	step [128/206], loss=89.4265
	step [129/206], loss=81.6597
	step [130/206], loss=79.1871
	step [131/206], loss=93.4792
	step [132/206], loss=99.3037
	step [133/206], loss=72.3679
	step [134/206], loss=79.5832
	step [135/206], loss=71.3563
	step [136/206], loss=96.2301
	step [137/206], loss=79.1290
	step [138/206], loss=87.8883
	step [139/206], loss=84.4935
	step [140/206], loss=91.1763
	step [141/206], loss=92.1904
	step [142/206], loss=75.7589
	step [143/206], loss=83.9585
	step [144/206], loss=77.8011
	step [145/206], loss=89.7909
	step [146/206], loss=85.8372
	step [147/206], loss=88.2167
	step [148/206], loss=80.1677
	step [149/206], loss=69.9335
	step [150/206], loss=82.1595
	step [151/206], loss=90.5710
	step [152/206], loss=79.8792
	step [153/206], loss=93.4149
	step [154/206], loss=66.6775
	step [155/206], loss=98.7338
	step [156/206], loss=105.8275
	step [157/206], loss=75.0604
	step [158/206], loss=91.3125
	step [159/206], loss=81.7445
	step [160/206], loss=85.8971
	step [161/206], loss=85.9467
	step [162/206], loss=86.3823
	step [163/206], loss=90.1255
	step [164/206], loss=79.8103
	step [165/206], loss=90.9775
	step [166/206], loss=76.2598
	step [167/206], loss=86.0217
	step [168/206], loss=96.2124
	step [169/206], loss=75.1944
	step [170/206], loss=100.5593
	step [171/206], loss=84.5941
	step [172/206], loss=79.0510
	step [173/206], loss=76.2752
	step [174/206], loss=92.0481
	step [175/206], loss=84.9108
	step [176/206], loss=85.8714
	step [177/206], loss=77.6887
	step [178/206], loss=75.2227
	step [179/206], loss=79.5163
	step [180/206], loss=81.5050
	step [181/206], loss=72.1967
	step [182/206], loss=92.5823
	step [183/206], loss=91.0068
	step [184/206], loss=98.9986
	step [185/206], loss=77.2355
	step [186/206], loss=84.6126
	step [187/206], loss=96.8644
	step [188/206], loss=91.2854
	step [189/206], loss=90.9898
	step [190/206], loss=84.1710
	step [191/206], loss=79.0800
	step [192/206], loss=75.0237
	step [193/206], loss=96.7936
	step [194/206], loss=78.1387
	step [195/206], loss=82.6152
	step [196/206], loss=94.1679
	step [197/206], loss=82.7005
	step [198/206], loss=83.2321
	step [199/206], loss=80.8782
	step [200/206], loss=86.2768
	step [201/206], loss=96.7576
	step [202/206], loss=63.1710
	step [203/206], loss=73.6524
	step [204/206], loss=92.7743
	step [205/206], loss=93.3959
	step [206/206], loss=48.3606
	Evaluating
	loss=0.0110, precision=0.3629, recall=0.9082, f1=0.5186
Training epoch 38
	step [1/206], loss=82.4618
	step [2/206], loss=77.6964
	step [3/206], loss=90.6260
	step [4/206], loss=81.9327
	step [5/206], loss=77.5953
	step [6/206], loss=76.9453
	step [7/206], loss=80.5451
	step [8/206], loss=89.5845
	step [9/206], loss=103.2439
	step [10/206], loss=73.5985
	step [11/206], loss=76.6481
	step [12/206], loss=92.2319
	step [13/206], loss=96.7192
	step [14/206], loss=81.5667
	step [15/206], loss=83.1136
	step [16/206], loss=85.4713
	step [17/206], loss=85.3154
	step [18/206], loss=71.4409
	step [19/206], loss=89.7310
	step [20/206], loss=85.0421
	step [21/206], loss=89.3261
	step [22/206], loss=96.3512
	step [23/206], loss=98.4300
	step [24/206], loss=90.4020
	step [25/206], loss=84.7826
	step [26/206], loss=92.5109
	step [27/206], loss=96.6852
	step [28/206], loss=87.6499
	step [29/206], loss=81.3358
	step [30/206], loss=79.9330
	step [31/206], loss=92.5021
	step [32/206], loss=78.5229
	step [33/206], loss=79.0245
	step [34/206], loss=101.9103
	step [35/206], loss=78.1665
	step [36/206], loss=91.4536
	step [37/206], loss=82.1622
	step [38/206], loss=84.4704
	step [39/206], loss=84.0113
	step [40/206], loss=95.5857
	step [41/206], loss=97.6623
	step [42/206], loss=88.6398
	step [43/206], loss=78.1509
	step [44/206], loss=82.1873
	step [45/206], loss=78.8584
	step [46/206], loss=87.8942
	step [47/206], loss=81.5889
	step [48/206], loss=80.6486
	step [49/206], loss=94.1922
	step [50/206], loss=76.4490
	step [51/206], loss=94.8539
	step [52/206], loss=88.9285
	step [53/206], loss=78.5520
	step [54/206], loss=82.1325
	step [55/206], loss=87.8577
	step [56/206], loss=84.4731
	step [57/206], loss=82.7419
	step [58/206], loss=79.6330
	step [59/206], loss=85.7534
	step [60/206], loss=80.1527
	step [61/206], loss=93.9365
	step [62/206], loss=93.9393
	step [63/206], loss=88.7843
	step [64/206], loss=81.9484
	step [65/206], loss=87.4719
	step [66/206], loss=81.6631
	step [67/206], loss=82.8497
	step [68/206], loss=87.4900
	step [69/206], loss=95.9921
	step [70/206], loss=77.4046
	step [71/206], loss=79.0412
	step [72/206], loss=90.4669
	step [73/206], loss=83.2100
	step [74/206], loss=79.6292
	step [75/206], loss=94.8556
	step [76/206], loss=89.8970
	step [77/206], loss=91.2991
	step [78/206], loss=88.6138
	step [79/206], loss=81.1867
	step [80/206], loss=66.4753
	step [81/206], loss=94.6710
	step [82/206], loss=79.7381
	step [83/206], loss=79.4760
	step [84/206], loss=92.1164
	step [85/206], loss=93.9431
	step [86/206], loss=82.5796
	step [87/206], loss=79.1161
	step [88/206], loss=94.9923
	step [89/206], loss=72.3049
	step [90/206], loss=71.5256
	step [91/206], loss=88.1055
	step [92/206], loss=85.6293
	step [93/206], loss=104.4582
	step [94/206], loss=85.4681
	step [95/206], loss=89.4480
	step [96/206], loss=93.5659
	step [97/206], loss=84.0042
	step [98/206], loss=91.4465
	step [99/206], loss=94.3744
	step [100/206], loss=71.5881
	step [101/206], loss=84.1234
	step [102/206], loss=76.8535
	step [103/206], loss=87.9688
	step [104/206], loss=77.5451
	step [105/206], loss=81.1904
	step [106/206], loss=94.9661
	step [107/206], loss=89.3853
	step [108/206], loss=88.8665
	step [109/206], loss=93.4259
	step [110/206], loss=84.8739
	step [111/206], loss=88.9026
	step [112/206], loss=90.9140
	step [113/206], loss=77.5396
	step [114/206], loss=91.5799
	step [115/206], loss=73.5691
	step [116/206], loss=87.3615
	step [117/206], loss=74.3347
	step [118/206], loss=87.8665
	step [119/206], loss=81.5387
	step [120/206], loss=77.6920
	step [121/206], loss=111.3830
	step [122/206], loss=83.5112
	step [123/206], loss=89.5369
	step [124/206], loss=90.5502
	step [125/206], loss=66.8405
	step [126/206], loss=80.2728
	step [127/206], loss=89.3469
	step [128/206], loss=105.9165
	step [129/206], loss=84.6841
	step [130/206], loss=78.1631
	step [131/206], loss=90.3745
	step [132/206], loss=84.0933
	step [133/206], loss=78.0778
	step [134/206], loss=70.1335
	step [135/206], loss=85.1646
	step [136/206], loss=84.3201
	step [137/206], loss=95.0701
	step [138/206], loss=99.4397
	step [139/206], loss=85.1877
	step [140/206], loss=85.6168
	step [141/206], loss=77.3445
	step [142/206], loss=97.6249
	step [143/206], loss=74.9045
	step [144/206], loss=94.3290
	step [145/206], loss=80.7674
	step [146/206], loss=80.8439
	step [147/206], loss=81.6698
	step [148/206], loss=94.8297
	step [149/206], loss=80.1677
	step [150/206], loss=86.1904
	step [151/206], loss=79.9014
	step [152/206], loss=80.9950
	step [153/206], loss=75.1147
	step [154/206], loss=97.8285
	step [155/206], loss=100.6787
	step [156/206], loss=82.4857
	step [157/206], loss=82.5008
	step [158/206], loss=82.6936
	step [159/206], loss=93.9324
	step [160/206], loss=87.4920
	step [161/206], loss=77.9413
	step [162/206], loss=92.2739
	step [163/206], loss=86.6580
	step [164/206], loss=74.3649
	step [165/206], loss=79.2124
	step [166/206], loss=85.7350
	step [167/206], loss=93.2979
	step [168/206], loss=83.6645
	step [169/206], loss=87.7108
	step [170/206], loss=72.9302
	step [171/206], loss=96.0127
	step [172/206], loss=83.8584
	step [173/206], loss=76.1664
	step [174/206], loss=89.6760
	step [175/206], loss=86.0523
	step [176/206], loss=99.8432
	step [177/206], loss=80.7995
	step [178/206], loss=80.0705
	step [179/206], loss=95.9943
	step [180/206], loss=86.6637
	step [181/206], loss=73.0044
	step [182/206], loss=82.9605
	step [183/206], loss=89.6411
	step [184/206], loss=91.1700
	step [185/206], loss=82.5547
	step [186/206], loss=88.2927
	step [187/206], loss=71.2007
	step [188/206], loss=85.5785
	step [189/206], loss=93.6445
	step [190/206], loss=89.2554
	step [191/206], loss=86.7889
	step [192/206], loss=91.6864
	step [193/206], loss=89.0310
	step [194/206], loss=82.6579
	step [195/206], loss=102.0266
	step [196/206], loss=79.3853
	step [197/206], loss=82.2040
	step [198/206], loss=94.9234
	step [199/206], loss=87.5876
	step [200/206], loss=92.3086
	step [201/206], loss=72.9020
	step [202/206], loss=82.0424
	step [203/206], loss=75.2794
	step [204/206], loss=74.3683
	step [205/206], loss=79.2673
	step [206/206], loss=49.6171
	Evaluating
	loss=0.0103, precision=0.3648, recall=0.9173, f1=0.5220
Training epoch 39
	step [1/206], loss=97.1386
	step [2/206], loss=91.8556
	step [3/206], loss=91.4227
	step [4/206], loss=89.6747
	step [5/206], loss=81.8760
	step [6/206], loss=78.8197
	step [7/206], loss=85.5477
	step [8/206], loss=81.3672
	step [9/206], loss=71.4431
	step [10/206], loss=97.1728
	step [11/206], loss=82.0842
	step [12/206], loss=94.2141
	step [13/206], loss=80.9996
	step [14/206], loss=95.1115
	step [15/206], loss=83.5645
	step [16/206], loss=79.1357
	step [17/206], loss=85.6529
	step [18/206], loss=89.6955
	step [19/206], loss=80.8981
	step [20/206], loss=84.8395
	step [21/206], loss=87.6475
	step [22/206], loss=100.3468
	step [23/206], loss=82.5739
	step [24/206], loss=84.1594
	step [25/206], loss=90.2543
	step [26/206], loss=77.8295
	step [27/206], loss=101.1254
	step [28/206], loss=90.3470
	step [29/206], loss=80.6528
	step [30/206], loss=70.6070
	step [31/206], loss=72.4753
	step [32/206], loss=78.1735
	step [33/206], loss=83.1882
	step [34/206], loss=100.9677
	step [35/206], loss=79.7891
	step [36/206], loss=86.8318
	step [37/206], loss=90.0512
	step [38/206], loss=74.5766
	step [39/206], loss=89.9612
	step [40/206], loss=79.3988
	step [41/206], loss=86.4342
	step [42/206], loss=107.3596
	step [43/206], loss=100.3811
	step [44/206], loss=78.0934
	step [45/206], loss=95.2842
	step [46/206], loss=100.8142
	step [47/206], loss=77.6160
	step [48/206], loss=75.9225
	step [49/206], loss=77.8577
	step [50/206], loss=73.0142
	step [51/206], loss=85.1360
	step [52/206], loss=92.3003
	step [53/206], loss=76.8413
	step [54/206], loss=87.3384
	step [55/206], loss=95.3242
	step [56/206], loss=71.5044
	step [57/206], loss=77.1898
	step [58/206], loss=92.7451
	step [59/206], loss=89.8683
	step [60/206], loss=86.2062
	step [61/206], loss=84.2977
	step [62/206], loss=84.7466
	step [63/206], loss=73.7135
	step [64/206], loss=97.1911
	step [65/206], loss=106.4390
	step [66/206], loss=69.6893
	step [67/206], loss=88.5064
	step [68/206], loss=91.8736
	step [69/206], loss=79.2013
	step [70/206], loss=95.1579
	step [71/206], loss=73.0531
	step [72/206], loss=81.4979
	step [73/206], loss=92.8492
	step [74/206], loss=79.0743
	step [75/206], loss=83.5870
	step [76/206], loss=83.2568
	step [77/206], loss=91.6712
	step [78/206], loss=85.2537
	step [79/206], loss=95.2415
	step [80/206], loss=93.9621
	step [81/206], loss=76.3360
	step [82/206], loss=95.7177
	step [83/206], loss=79.8086
	step [84/206], loss=78.8980
	step [85/206], loss=76.4858
	step [86/206], loss=90.4419
	step [87/206], loss=80.3981
	step [88/206], loss=100.3755
	step [89/206], loss=74.7693
	step [90/206], loss=85.0215
	step [91/206], loss=75.9934
	step [92/206], loss=82.5097
	step [93/206], loss=74.8815
	step [94/206], loss=82.9950
	step [95/206], loss=92.1236
	step [96/206], loss=90.9493
	step [97/206], loss=91.1995
	step [98/206], loss=81.3659
	step [99/206], loss=79.5549
	step [100/206], loss=78.7712
	step [101/206], loss=74.4009
	step [102/206], loss=77.3901
	step [103/206], loss=81.1961
	step [104/206], loss=76.0165
	step [105/206], loss=85.8314
	step [106/206], loss=84.8234
	step [107/206], loss=82.2876
	step [108/206], loss=84.1900
	step [109/206], loss=90.2699
	step [110/206], loss=88.4092
	step [111/206], loss=90.6848
	step [112/206], loss=94.9901
	step [113/206], loss=83.0004
	step [114/206], loss=91.5820
	step [115/206], loss=88.5757
	step [116/206], loss=98.6986
	step [117/206], loss=93.4898
	step [118/206], loss=77.3195
	step [119/206], loss=65.7731
	step [120/206], loss=74.1594
	step [121/206], loss=83.5931
	step [122/206], loss=70.6466
	step [123/206], loss=74.7789
	step [124/206], loss=97.2375
	step [125/206], loss=68.7526
	step [126/206], loss=92.8445
	step [127/206], loss=86.8270
	step [128/206], loss=81.8360
	step [129/206], loss=82.8335
	step [130/206], loss=82.8805
	step [131/206], loss=78.8894
	step [132/206], loss=102.6746
	step [133/206], loss=82.5248
	step [134/206], loss=79.4127
	step [135/206], loss=100.7836
	step [136/206], loss=80.9012
	step [137/206], loss=89.6149
	step [138/206], loss=97.5266
	step [139/206], loss=78.8694
	step [140/206], loss=89.7225
	step [141/206], loss=91.4201
	step [142/206], loss=68.8828
	step [143/206], loss=90.7623
	step [144/206], loss=82.3356
	step [145/206], loss=89.5966
	step [146/206], loss=93.7148
	step [147/206], loss=86.8576
	step [148/206], loss=98.3271
	step [149/206], loss=78.7731
	step [150/206], loss=77.7092
	step [151/206], loss=69.7989
	step [152/206], loss=83.2954
	step [153/206], loss=93.8260
	step [154/206], loss=82.3491
	step [155/206], loss=65.8753
	step [156/206], loss=78.3496
	step [157/206], loss=75.1243
	step [158/206], loss=89.6905
	step [159/206], loss=66.7020
	step [160/206], loss=80.9334
	step [161/206], loss=81.4823
	step [162/206], loss=82.7155
	step [163/206], loss=79.8449
	step [164/206], loss=88.9510
	step [165/206], loss=96.9906
	step [166/206], loss=88.2117
	step [167/206], loss=92.0157
	step [168/206], loss=75.0863
	step [169/206], loss=92.0417
	step [170/206], loss=83.7775
	step [171/206], loss=84.4025
	step [172/206], loss=81.3247
	step [173/206], loss=80.0167
	step [174/206], loss=82.0036
	step [175/206], loss=86.5339
	step [176/206], loss=89.5018
	step [177/206], loss=102.9596
	step [178/206], loss=76.2345
	step [179/206], loss=87.3513
	step [180/206], loss=86.0738
	step [181/206], loss=76.5305
	step [182/206], loss=82.2157
	step [183/206], loss=86.5223
	step [184/206], loss=78.5654
	step [185/206], loss=82.9434
	step [186/206], loss=84.6632
	step [187/206], loss=95.9961
	step [188/206], loss=90.6324
	step [189/206], loss=75.0707
	step [190/206], loss=84.6398
	step [191/206], loss=93.5983
	step [192/206], loss=103.9384
	step [193/206], loss=74.6683
	step [194/206], loss=80.5246
	step [195/206], loss=77.1770
	step [196/206], loss=86.1125
	step [197/206], loss=99.7216
	step [198/206], loss=84.8331
	step [199/206], loss=96.5960
	step [200/206], loss=109.7372
	step [201/206], loss=95.3030
	step [202/206], loss=73.3170
	step [203/206], loss=82.5294
	step [204/206], loss=87.2077
	step [205/206], loss=92.0950
	step [206/206], loss=64.5544
	Evaluating
	loss=0.0098, precision=0.3701, recall=0.8979, f1=0.5242
Training epoch 40
	step [1/206], loss=99.9949
	step [2/206], loss=66.6421
	step [3/206], loss=81.0889
	step [4/206], loss=94.2532
	step [5/206], loss=84.3734
	step [6/206], loss=71.9452
	step [7/206], loss=102.7849
	step [8/206], loss=88.1870
	step [9/206], loss=87.5525
	step [10/206], loss=82.9769
	step [11/206], loss=82.5639
	step [12/206], loss=80.3124
	step [13/206], loss=85.0036
	step [14/206], loss=95.3162
	step [15/206], loss=93.3281
	step [16/206], loss=91.1788
	step [17/206], loss=78.2888
	step [18/206], loss=89.8448
	step [19/206], loss=81.4231
	step [20/206], loss=94.8010
	step [21/206], loss=97.4068
	step [22/206], loss=88.8408
	step [23/206], loss=77.3246
	step [24/206], loss=78.8291
	step [25/206], loss=94.3292
	step [26/206], loss=85.2665
	step [27/206], loss=74.4404
	step [28/206], loss=79.8026
	step [29/206], loss=84.8853
	step [30/206], loss=91.1580
	step [31/206], loss=88.8596
	step [32/206], loss=68.0904
	step [33/206], loss=103.9604
	step [34/206], loss=89.4330
	step [35/206], loss=91.0793
	step [36/206], loss=84.5640
	step [37/206], loss=77.6201
	step [38/206], loss=83.2450
	step [39/206], loss=63.0659
	step [40/206], loss=95.5657
	step [41/206], loss=80.6684
	step [42/206], loss=74.6031
	step [43/206], loss=85.3025
	step [44/206], loss=77.5832
	step [45/206], loss=93.8448
	step [46/206], loss=81.2400
	step [47/206], loss=86.9645
	step [48/206], loss=93.6067
	step [49/206], loss=85.1489
	step [50/206], loss=83.2557
	step [51/206], loss=90.5978
	step [52/206], loss=82.8232
	step [53/206], loss=90.7336
	step [54/206], loss=79.6742
	step [55/206], loss=86.5145
	step [56/206], loss=100.4474
	step [57/206], loss=81.5281
	step [58/206], loss=66.6740
	step [59/206], loss=82.0693
	step [60/206], loss=89.2060
	step [61/206], loss=94.2736
	step [62/206], loss=91.0659
	step [63/206], loss=82.3808
	step [64/206], loss=93.4847
	step [65/206], loss=78.1623
	step [66/206], loss=95.2468
	step [67/206], loss=76.3794
	step [68/206], loss=84.2657
	step [69/206], loss=84.5063
	step [70/206], loss=92.6534
	step [71/206], loss=106.1686
	step [72/206], loss=78.9669
	step [73/206], loss=102.1442
	step [74/206], loss=84.1409
	step [75/206], loss=80.2372
	step [76/206], loss=91.6123
	step [77/206], loss=76.3580
	step [78/206], loss=93.4560
	step [79/206], loss=71.8870
	step [80/206], loss=80.4092
	step [81/206], loss=86.4882
	step [82/206], loss=85.5595
	step [83/206], loss=83.4565
	step [84/206], loss=84.5596
	step [85/206], loss=93.6591
	step [86/206], loss=102.4811
	step [87/206], loss=89.2989
	step [88/206], loss=73.5484
	step [89/206], loss=90.8507
	step [90/206], loss=83.5765
	step [91/206], loss=76.9274
	step [92/206], loss=86.7116
	step [93/206], loss=81.9261
	step [94/206], loss=87.3927
	step [95/206], loss=75.5975
	step [96/206], loss=76.1823
	step [97/206], loss=84.6400
	step [98/206], loss=76.2341
	step [99/206], loss=81.1855
	step [100/206], loss=81.0805
	step [101/206], loss=88.5719
	step [102/206], loss=74.9806
	step [103/206], loss=80.8718
	step [104/206], loss=80.5045
	step [105/206], loss=86.1921
	step [106/206], loss=80.3951
	step [107/206], loss=78.8770
	step [108/206], loss=81.5752
	step [109/206], loss=76.4223
	step [110/206], loss=86.3183
	step [111/206], loss=82.5361
	step [112/206], loss=80.1077
	step [113/206], loss=77.4121
	step [114/206], loss=72.9074
	step [115/206], loss=87.3271
	step [116/206], loss=87.7829
	step [117/206], loss=97.8692
	step [118/206], loss=86.7015
	step [119/206], loss=87.5505
	step [120/206], loss=80.5812
	step [121/206], loss=80.2711
	step [122/206], loss=76.8359
	step [123/206], loss=81.5165
	step [124/206], loss=93.1898
	step [125/206], loss=83.5916
	step [126/206], loss=83.1220
	step [127/206], loss=93.9042
	step [128/206], loss=92.3963
	step [129/206], loss=92.4385
	step [130/206], loss=79.1404
	step [131/206], loss=93.4305
	step [132/206], loss=85.0015
	step [133/206], loss=79.5269
	step [134/206], loss=83.9092
	step [135/206], loss=71.6119
	step [136/206], loss=90.4623
	step [137/206], loss=73.9059
	step [138/206], loss=95.4122
	step [139/206], loss=72.3844
	step [140/206], loss=64.9034
	step [141/206], loss=83.8315
	step [142/206], loss=80.2953
	step [143/206], loss=80.5238
	step [144/206], loss=79.8881
	step [145/206], loss=82.9142
	step [146/206], loss=81.5624
	step [147/206], loss=83.9557
	step [148/206], loss=99.5475
	step [149/206], loss=90.1548
	step [150/206], loss=71.9298
	step [151/206], loss=86.3158
	step [152/206], loss=90.8712
	step [153/206], loss=88.0707
	step [154/206], loss=82.5073
	step [155/206], loss=87.4891
	step [156/206], loss=87.7941
	step [157/206], loss=83.9344
	step [158/206], loss=84.9750
	step [159/206], loss=80.1104
	step [160/206], loss=88.5685
	step [161/206], loss=88.7276
	step [162/206], loss=63.6749
	step [163/206], loss=81.3184
	step [164/206], loss=84.1238
	step [165/206], loss=93.2633
	step [166/206], loss=91.0583
	step [167/206], loss=93.7390
	step [168/206], loss=94.8772
	step [169/206], loss=91.9254
	step [170/206], loss=85.1236
	step [171/206], loss=91.2146
	step [172/206], loss=87.9587
	step [173/206], loss=93.5865
	step [174/206], loss=94.2380
	step [175/206], loss=74.7947
	step [176/206], loss=93.6401
	step [177/206], loss=90.2576
	step [178/206], loss=91.8147
	step [179/206], loss=84.0694
	step [180/206], loss=88.4224
	step [181/206], loss=70.6369
	step [182/206], loss=82.3894
	step [183/206], loss=78.4758
	step [184/206], loss=83.3662
	step [185/206], loss=81.4733
	step [186/206], loss=72.4635
	step [187/206], loss=83.4630
	step [188/206], loss=92.9931
	step [189/206], loss=79.8421
	step [190/206], loss=84.1266
	step [191/206], loss=88.4108
	step [192/206], loss=93.5282
	step [193/206], loss=86.0626
	step [194/206], loss=80.6846
	step [195/206], loss=81.5893
	step [196/206], loss=88.7172
	step [197/206], loss=74.4369
	step [198/206], loss=80.2861
	step [199/206], loss=98.5183
	step [200/206], loss=91.4531
	step [201/206], loss=78.8867
	step [202/206], loss=88.8168
	step [203/206], loss=93.1957
	step [204/206], loss=73.2126
	step [205/206], loss=80.8113
	step [206/206], loss=48.6147
	Evaluating
	loss=0.0108, precision=0.3397, recall=0.9126, f1=0.4951
Training epoch 41
	step [1/206], loss=80.7948
	step [2/206], loss=98.3473
	step [3/206], loss=81.5856
	step [4/206], loss=73.7361
	step [5/206], loss=69.9565
	step [6/206], loss=75.6890
	step [7/206], loss=81.9080
	step [8/206], loss=83.0252
	step [9/206], loss=85.9342
	step [10/206], loss=86.8199
	step [11/206], loss=79.8820
	step [12/206], loss=86.2443
	step [13/206], loss=82.0124
	step [14/206], loss=86.9532
	step [15/206], loss=89.6675
	step [16/206], loss=92.9638
	step [17/206], loss=103.6277
	step [18/206], loss=92.3107
	step [19/206], loss=69.5500
	step [20/206], loss=84.8323
	step [21/206], loss=78.5604
	step [22/206], loss=88.1639
	step [23/206], loss=91.2110
	step [24/206], loss=76.5534
	step [25/206], loss=77.5491
	step [26/206], loss=88.1544
	step [27/206], loss=75.0895
	step [28/206], loss=93.7926
	step [29/206], loss=78.5160
	step [30/206], loss=88.0434
	step [31/206], loss=78.2654
	step [32/206], loss=76.7312
	step [33/206], loss=87.7468
	step [34/206], loss=81.3087
	step [35/206], loss=74.3052
	step [36/206], loss=88.5530
	step [37/206], loss=72.3956
	step [38/206], loss=77.8901
	step [39/206], loss=77.9795
	step [40/206], loss=85.3496
	step [41/206], loss=89.2670
	step [42/206], loss=82.8160
	step [43/206], loss=80.0330
	step [44/206], loss=80.6595
	step [45/206], loss=96.2492
	step [46/206], loss=86.2970
	step [47/206], loss=72.5270
	step [48/206], loss=84.3967
	step [49/206], loss=86.2191
	step [50/206], loss=80.3696
	step [51/206], loss=82.6077
	step [52/206], loss=80.3705
	step [53/206], loss=92.9725
	step [54/206], loss=89.9798
	step [55/206], loss=77.9220
	step [56/206], loss=89.7408
	step [57/206], loss=84.4490
	step [58/206], loss=87.9964
	step [59/206], loss=78.6886
	step [60/206], loss=82.7361
	step [61/206], loss=77.4238
	step [62/206], loss=89.8048
	step [63/206], loss=81.9654
	step [64/206], loss=77.1865
	step [65/206], loss=85.5688
	step [66/206], loss=89.1232
	step [67/206], loss=70.2110
	step [68/206], loss=80.4357
	step [69/206], loss=65.6085
	step [70/206], loss=71.7666
	step [71/206], loss=90.1461
	step [72/206], loss=83.9869
	step [73/206], loss=88.2318
	step [74/206], loss=72.4925
	step [75/206], loss=92.2922
	step [76/206], loss=83.5298
	step [77/206], loss=82.6133
	step [78/206], loss=81.1900
	step [79/206], loss=66.9301
	step [80/206], loss=76.4678
	step [81/206], loss=90.6492
	step [82/206], loss=81.7827
	step [83/206], loss=77.7075
	step [84/206], loss=85.0575
	step [85/206], loss=97.1820
	step [86/206], loss=75.6673
	step [87/206], loss=92.9634
	step [88/206], loss=89.4551
	step [89/206], loss=86.6126
	step [90/206], loss=87.1851
	step [91/206], loss=84.8555
	step [92/206], loss=85.8495
	step [93/206], loss=72.8629
	step [94/206], loss=86.2019
	step [95/206], loss=81.6192
	step [96/206], loss=77.5065
	step [97/206], loss=79.9492
	step [98/206], loss=100.8374
	step [99/206], loss=89.7954
	step [100/206], loss=84.7587
	step [101/206], loss=75.2426
	step [102/206], loss=78.1687
	step [103/206], loss=96.7949
	step [104/206], loss=89.1836
	step [105/206], loss=76.7227
	step [106/206], loss=96.1721
	step [107/206], loss=82.6206
	step [108/206], loss=86.2584
	step [109/206], loss=79.5553
	step [110/206], loss=87.4356
	step [111/206], loss=80.1816
	step [112/206], loss=74.3730
	step [113/206], loss=81.0072
	step [114/206], loss=90.3971
	step [115/206], loss=85.1942
	step [116/206], loss=85.0085
	step [117/206], loss=101.6560
	step [118/206], loss=80.6238
	step [119/206], loss=72.3504
	step [120/206], loss=89.0759
	step [121/206], loss=79.5097
	step [122/206], loss=90.7834
	step [123/206], loss=75.4057
	step [124/206], loss=76.4925
	step [125/206], loss=82.3170
	step [126/206], loss=102.8123
	step [127/206], loss=71.5001
	step [128/206], loss=79.4616
	step [129/206], loss=90.0001
	step [130/206], loss=90.0727
	step [131/206], loss=81.2573
	step [132/206], loss=72.4841
	step [133/206], loss=97.9064
	step [134/206], loss=80.5606
	step [135/206], loss=97.5955
	step [136/206], loss=78.2853
	step [137/206], loss=95.2117
	step [138/206], loss=77.1799
	step [139/206], loss=85.9736
	step [140/206], loss=77.8972
	step [141/206], loss=96.6116
	step [142/206], loss=91.6611
	step [143/206], loss=81.7232
	step [144/206], loss=103.9443
	step [145/206], loss=89.0769
	step [146/206], loss=83.9587
	step [147/206], loss=81.3338
	step [148/206], loss=84.0512
	step [149/206], loss=79.7223
	step [150/206], loss=83.1331
	step [151/206], loss=87.8418
	step [152/206], loss=69.0997
	step [153/206], loss=92.4810
	step [154/206], loss=98.5807
	step [155/206], loss=77.9876
	step [156/206], loss=101.2353
	step [157/206], loss=72.2747
	step [158/206], loss=105.1696
	step [159/206], loss=77.7684
	step [160/206], loss=75.8856
	step [161/206], loss=85.1870
	step [162/206], loss=101.5138
	step [163/206], loss=76.3808
	step [164/206], loss=98.4453
	step [165/206], loss=90.1772
	step [166/206], loss=78.2278
	step [167/206], loss=91.1054
	step [168/206], loss=75.2695
	step [169/206], loss=86.6491
	step [170/206], loss=76.4919
	step [171/206], loss=81.0395
	step [172/206], loss=94.1223
	step [173/206], loss=77.4704
	step [174/206], loss=84.1492
	step [175/206], loss=91.1942
	step [176/206], loss=90.7019
	step [177/206], loss=78.3759
	step [178/206], loss=82.6137
	step [179/206], loss=80.5734
	step [180/206], loss=68.7520
	step [181/206], loss=70.9079
	step [182/206], loss=88.7582
	step [183/206], loss=91.4487
	step [184/206], loss=90.0968
	step [185/206], loss=87.8851
	step [186/206], loss=90.0200
	step [187/206], loss=86.1623
	step [188/206], loss=86.9267
	step [189/206], loss=92.8858
	step [190/206], loss=89.1906
	step [191/206], loss=72.9564
	step [192/206], loss=84.5150
	step [193/206], loss=93.0263
	step [194/206], loss=78.5137
	step [195/206], loss=79.4374
	step [196/206], loss=85.1561
	step [197/206], loss=96.1027
	step [198/206], loss=79.1871
	step [199/206], loss=86.8632
	step [200/206], loss=83.8530
	step [201/206], loss=97.1001
	step [202/206], loss=85.5581
	step [203/206], loss=96.8104
	step [204/206], loss=87.5511
	step [205/206], loss=80.9072
	step [206/206], loss=49.6659
	Evaluating
	loss=0.0110, precision=0.3275, recall=0.9134, f1=0.4821
Training epoch 42
	step [1/206], loss=87.7898
	step [2/206], loss=97.5333
	step [3/206], loss=70.3637
	step [4/206], loss=84.7572
	step [5/206], loss=97.8411
	step [6/206], loss=93.4729
	step [7/206], loss=96.6945
	step [8/206], loss=93.7853
	step [9/206], loss=94.7590
	step [10/206], loss=85.3562
	step [11/206], loss=73.0615
	step [12/206], loss=94.8886
	step [13/206], loss=97.6798
	step [14/206], loss=71.6302
	step [15/206], loss=92.9185
	step [16/206], loss=88.9192
	step [17/206], loss=96.7767
	step [18/206], loss=83.6057
	step [19/206], loss=79.1250
	step [20/206], loss=97.7178
	step [21/206], loss=85.2472
	step [22/206], loss=85.3748
	step [23/206], loss=78.2298
	step [24/206], loss=74.4586
	step [25/206], loss=79.9685
	step [26/206], loss=79.6259
	step [27/206], loss=77.0795
	step [28/206], loss=80.9298
	step [29/206], loss=79.0957
	step [30/206], loss=85.5980
	step [31/206], loss=73.9358
	step [32/206], loss=71.1641
	step [33/206], loss=88.4684
	step [34/206], loss=95.5760
	step [35/206], loss=75.5824
	step [36/206], loss=81.4393
	step [37/206], loss=89.8889
	step [38/206], loss=87.9503
	step [39/206], loss=83.0191
	step [40/206], loss=89.4227
	step [41/206], loss=80.4984
	step [42/206], loss=85.6593
	step [43/206], loss=91.0231
	step [44/206], loss=87.6283
	step [45/206], loss=78.0244
	step [46/206], loss=84.8517
	step [47/206], loss=82.1760
	step [48/206], loss=75.4224
	step [49/206], loss=90.0907
	step [50/206], loss=74.7762
	step [51/206], loss=110.1779
	step [52/206], loss=77.3311
	step [53/206], loss=75.0193
	step [54/206], loss=67.5750
	step [55/206], loss=81.8556
	step [56/206], loss=77.0640
	step [57/206], loss=87.9940
	step [58/206], loss=81.1222
	step [59/206], loss=76.2128
	step [60/206], loss=85.2383
	step [61/206], loss=85.7236
	step [62/206], loss=79.8739
	step [63/206], loss=78.6890
	step [64/206], loss=80.7000
	step [65/206], loss=88.8224
	step [66/206], loss=82.0178
	step [67/206], loss=89.5419
	step [68/206], loss=76.6033
	step [69/206], loss=76.4774
	step [70/206], loss=71.4360
	step [71/206], loss=93.1028
	step [72/206], loss=94.8299
	step [73/206], loss=84.6816
	step [74/206], loss=94.6118
	step [75/206], loss=98.0245
	step [76/206], loss=95.8068
	step [77/206], loss=82.6751
	step [78/206], loss=88.4286
	step [79/206], loss=81.4318
	step [80/206], loss=72.5586
	step [81/206], loss=86.0711
	step [82/206], loss=86.3965
	step [83/206], loss=70.4100
	step [84/206], loss=101.2984
	step [85/206], loss=75.1392
	step [86/206], loss=76.5208
	step [87/206], loss=93.4887
	step [88/206], loss=67.0897
	step [89/206], loss=89.1026
	step [90/206], loss=92.8583
	step [91/206], loss=83.6953
	step [92/206], loss=98.8965
	step [93/206], loss=75.9991
	step [94/206], loss=89.7607
	step [95/206], loss=79.1551
	step [96/206], loss=89.3126
	step [97/206], loss=91.1301
	step [98/206], loss=90.1372
	step [99/206], loss=81.3847
	step [100/206], loss=86.0918
	step [101/206], loss=99.2470
	step [102/206], loss=69.0463
	step [103/206], loss=80.1085
	step [104/206], loss=86.1252
	step [105/206], loss=84.6560
	step [106/206], loss=83.7845
	step [107/206], loss=69.2502
	step [108/206], loss=87.3895
	step [109/206], loss=86.6980
	step [110/206], loss=92.6778
	step [111/206], loss=96.1171
	step [112/206], loss=91.8938
	step [113/206], loss=80.9285
	step [114/206], loss=78.5417
	step [115/206], loss=86.1723
	step [116/206], loss=92.1041
	step [117/206], loss=80.9133
	step [118/206], loss=83.0263
	step [119/206], loss=79.2067
	step [120/206], loss=79.8560
	step [121/206], loss=75.7340
	step [122/206], loss=102.2826
	step [123/206], loss=87.3762
	step [124/206], loss=90.3491
	step [125/206], loss=91.6720
	step [126/206], loss=69.4049
	step [127/206], loss=94.1249
	step [128/206], loss=70.0745
	step [129/206], loss=88.1524
	step [130/206], loss=95.8361
	step [131/206], loss=84.2022
	step [132/206], loss=82.5652
	step [133/206], loss=71.8968
	step [134/206], loss=97.7183
	step [135/206], loss=76.0510
	step [136/206], loss=85.0863
	step [137/206], loss=67.0708
	step [138/206], loss=87.6362
	step [139/206], loss=83.7588
	step [140/206], loss=87.9048
	step [141/206], loss=94.3746
	step [142/206], loss=75.4719
	step [143/206], loss=86.6291
	step [144/206], loss=73.6753
	step [145/206], loss=84.0176
	step [146/206], loss=64.2142
	step [147/206], loss=104.5364
	step [148/206], loss=93.0901
	step [149/206], loss=82.4929
	step [150/206], loss=77.6046
	step [151/206], loss=76.8945
	step [152/206], loss=84.7179
	step [153/206], loss=86.4292
	step [154/206], loss=92.1294
	step [155/206], loss=72.1246
	step [156/206], loss=86.1819
	step [157/206], loss=86.0168
	step [158/206], loss=93.9945
	step [159/206], loss=75.8624
	step [160/206], loss=82.0312
	step [161/206], loss=74.4958
	step [162/206], loss=76.8054
	step [163/206], loss=80.2514
	step [164/206], loss=74.5052
	step [165/206], loss=79.8450
	step [166/206], loss=84.3815
	step [167/206], loss=89.4593
	step [168/206], loss=94.3546
	step [169/206], loss=82.8452
	step [170/206], loss=88.5853
	step [171/206], loss=84.1721
	step [172/206], loss=85.3308
	step [173/206], loss=85.2977
	step [174/206], loss=87.2644
	step [175/206], loss=81.7753
	step [176/206], loss=75.1810
	step [177/206], loss=79.3001
	step [178/206], loss=78.9230
	step [179/206], loss=80.8193
	step [180/206], loss=76.3734
	step [181/206], loss=100.4533
	step [182/206], loss=89.2690
	step [183/206], loss=85.7599
	step [184/206], loss=91.8913
	step [185/206], loss=78.1124
	step [186/206], loss=86.9275
	step [187/206], loss=94.5549
	step [188/206], loss=87.7690
	step [189/206], loss=77.2226
	step [190/206], loss=77.9245
	step [191/206], loss=84.1132
	step [192/206], loss=77.4704
	step [193/206], loss=88.0726
	step [194/206], loss=97.4018
	step [195/206], loss=82.7271
	step [196/206], loss=73.9904
	step [197/206], loss=85.9400
	step [198/206], loss=70.9743
	step [199/206], loss=70.1188
	step [200/206], loss=73.9821
	step [201/206], loss=84.2107
	step [202/206], loss=81.1756
	step [203/206], loss=82.3386
	step [204/206], loss=78.4343
	step [205/206], loss=100.3874
	step [206/206], loss=54.1744
	Evaluating
	loss=0.0102, precision=0.3546, recall=0.9218, f1=0.5121
Training epoch 43
	step [1/206], loss=73.0057
	step [2/206], loss=90.6394
	step [3/206], loss=82.3514
	step [4/206], loss=79.6324
	step [5/206], loss=90.7202
	step [6/206], loss=87.5592
	step [7/206], loss=94.4336
	step [8/206], loss=78.1760
	step [9/206], loss=79.1501
	step [10/206], loss=96.2325
	step [11/206], loss=64.9485
	step [12/206], loss=96.5223
	step [13/206], loss=81.3014
	step [14/206], loss=86.4930
	step [15/206], loss=84.1057
	step [16/206], loss=81.3539
	step [17/206], loss=86.6991
	step [18/206], loss=99.2583
	step [19/206], loss=84.3395
	step [20/206], loss=84.7924
	step [21/206], loss=91.9765
	step [22/206], loss=80.2759
	step [23/206], loss=89.2537
	step [24/206], loss=79.3604
	step [25/206], loss=83.0285
	step [26/206], loss=78.6844
	step [27/206], loss=85.3449
	step [28/206], loss=86.1367
	step [29/206], loss=114.2394
	step [30/206], loss=83.0300
	step [31/206], loss=85.6308
	step [32/206], loss=78.6040
	step [33/206], loss=82.5998
	step [34/206], loss=97.5494
	step [35/206], loss=76.8178
	step [36/206], loss=84.8885
	step [37/206], loss=79.5017
	step [38/206], loss=84.5463
	step [39/206], loss=93.7092
	step [40/206], loss=79.3088
	step [41/206], loss=90.2025
	step [42/206], loss=87.2685
	step [43/206], loss=76.8694
	step [44/206], loss=97.4191
	step [45/206], loss=77.3544
	step [46/206], loss=75.1855
	step [47/206], loss=91.3594
	step [48/206], loss=99.1465
	step [49/206], loss=79.1796
	step [50/206], loss=69.6899
	step [51/206], loss=89.4489
	step [52/206], loss=83.3148
	step [53/206], loss=85.5788
	step [54/206], loss=80.2583
	step [55/206], loss=82.5034
	step [56/206], loss=79.6256
	step [57/206], loss=77.4958
	step [58/206], loss=99.5762
	step [59/206], loss=79.4325
	step [60/206], loss=78.5072
	step [61/206], loss=97.4918
	step [62/206], loss=83.8202
	step [63/206], loss=73.8567
	step [64/206], loss=78.0661
	step [65/206], loss=84.4480
	step [66/206], loss=100.7551
	step [67/206], loss=90.0722
	step [68/206], loss=87.6568
	step [69/206], loss=83.3397
	step [70/206], loss=88.6293
	step [71/206], loss=78.5514
	step [72/206], loss=78.0439
	step [73/206], loss=76.0531
	step [74/206], loss=85.6454
	step [75/206], loss=80.8962
	step [76/206], loss=91.9209
	step [77/206], loss=86.6275
	step [78/206], loss=93.1733
	step [79/206], loss=78.0260
	step [80/206], loss=76.1603
	step [81/206], loss=83.8590
	step [82/206], loss=66.4344
	step [83/206], loss=83.0586
	step [84/206], loss=96.9954
	step [85/206], loss=76.8883
	step [86/206], loss=77.8994
	step [87/206], loss=85.2558
	step [88/206], loss=97.8615
	step [89/206], loss=82.4222
	step [90/206], loss=72.8021
	step [91/206], loss=87.5848
	step [92/206], loss=72.5740
	step [93/206], loss=87.7154
	step [94/206], loss=66.6920
	step [95/206], loss=85.6399
	step [96/206], loss=78.1075
	step [97/206], loss=74.4539
	step [98/206], loss=95.4679
	step [99/206], loss=80.9415
	step [100/206], loss=90.5698
	step [101/206], loss=61.8970
	step [102/206], loss=91.2977
	step [103/206], loss=93.0799
	step [104/206], loss=71.5200
	step [105/206], loss=78.6461
	step [106/206], loss=67.9738
	step [107/206], loss=88.3383
	step [108/206], loss=78.7934
	step [109/206], loss=91.5115
	step [110/206], loss=75.5425
	step [111/206], loss=71.2878
	step [112/206], loss=72.5347
	step [113/206], loss=72.2772
	step [114/206], loss=70.8459
	step [115/206], loss=91.3191
	step [116/206], loss=61.0948
	step [117/206], loss=95.2072
	step [118/206], loss=93.8577
	step [119/206], loss=79.8627
	step [120/206], loss=81.0375
	step [121/206], loss=90.7248
	step [122/206], loss=74.8069
	step [123/206], loss=87.4861
	step [124/206], loss=84.7240
	step [125/206], loss=87.9682
	step [126/206], loss=83.5280
	step [127/206], loss=82.7122
	step [128/206], loss=84.3546
	step [129/206], loss=81.6644
	step [130/206], loss=75.8416
	step [131/206], loss=89.5336
	step [132/206], loss=79.2475
	step [133/206], loss=79.6014
	step [134/206], loss=93.3373
	step [135/206], loss=91.1098
	step [136/206], loss=75.2529
	step [137/206], loss=70.7815
	step [138/206], loss=84.6791
	step [139/206], loss=89.7968
	step [140/206], loss=85.3305
	step [141/206], loss=85.1987
	step [142/206], loss=89.6798
	step [143/206], loss=82.6448
	step [144/206], loss=80.7906
	step [145/206], loss=90.0886
	step [146/206], loss=80.4652
	step [147/206], loss=91.5198
	step [148/206], loss=72.7802
	step [149/206], loss=70.1758
	step [150/206], loss=78.3698
	step [151/206], loss=93.2079
	step [152/206], loss=71.7701
	step [153/206], loss=89.0834
	step [154/206], loss=85.0741
	step [155/206], loss=81.2688
	step [156/206], loss=90.0016
	step [157/206], loss=82.7772
	step [158/206], loss=75.7102
	step [159/206], loss=77.6389
	step [160/206], loss=82.3516
	step [161/206], loss=87.0676
	step [162/206], loss=78.3602
	step [163/206], loss=66.3865
	step [164/206], loss=92.0974
	step [165/206], loss=78.9855
	step [166/206], loss=86.7254
	step [167/206], loss=83.8848
	step [168/206], loss=89.5618
	step [169/206], loss=79.5867
	step [170/206], loss=84.9454
	step [171/206], loss=92.9707
	step [172/206], loss=94.1379
	step [173/206], loss=81.3049
	step [174/206], loss=86.0239
	step [175/206], loss=90.7246
	step [176/206], loss=88.4942
	step [177/206], loss=78.7218
	step [178/206], loss=84.5367
	step [179/206], loss=81.7596
	step [180/206], loss=79.2852
	step [181/206], loss=96.6832
	step [182/206], loss=78.9022
	step [183/206], loss=81.1123
	step [184/206], loss=66.8120
	step [185/206], loss=81.9691
	step [186/206], loss=91.5379
	step [187/206], loss=73.8922
	step [188/206], loss=81.7875
	step [189/206], loss=83.0677
	step [190/206], loss=82.9472
	step [191/206], loss=89.5792
	step [192/206], loss=84.9921
	step [193/206], loss=89.7485
	step [194/206], loss=81.6653
	step [195/206], loss=95.5566
	step [196/206], loss=82.6666
	step [197/206], loss=93.8931
	step [198/206], loss=78.3890
	step [199/206], loss=75.0275
	step [200/206], loss=92.1856
	step [201/206], loss=77.7968
	step [202/206], loss=72.6930
	step [203/206], loss=81.7433
	step [204/206], loss=95.7464
	step [205/206], loss=75.7647
	step [206/206], loss=52.7019
	Evaluating
	loss=0.0110, precision=0.3206, recall=0.9163, f1=0.4749
Training epoch 44
	step [1/206], loss=77.9511
	step [2/206], loss=91.8006
	step [3/206], loss=73.3610
	step [4/206], loss=85.6128
	step [5/206], loss=86.6805
	step [6/206], loss=85.8172
	step [7/206], loss=75.5789
	step [8/206], loss=73.1131
	step [9/206], loss=90.4155
	step [10/206], loss=81.9761
	step [11/206], loss=82.7500
	step [12/206], loss=84.8900
	step [13/206], loss=107.7653
	step [14/206], loss=98.3278
	step [15/206], loss=89.4160
	step [16/206], loss=82.4140
	step [17/206], loss=90.3653
	step [18/206], loss=88.2007
	step [19/206], loss=85.1549
	step [20/206], loss=81.6568
	step [21/206], loss=101.0278
	step [22/206], loss=90.1607
	step [23/206], loss=91.5196
	step [24/206], loss=83.2096
	step [25/206], loss=71.2216
	step [26/206], loss=88.2313
	step [27/206], loss=78.1808
	step [28/206], loss=80.2021
	step [29/206], loss=72.0045
	step [30/206], loss=82.9025
	step [31/206], loss=71.7269
	step [32/206], loss=72.4897
	step [33/206], loss=78.6459
	step [34/206], loss=66.7241
	step [35/206], loss=95.7124
	step [36/206], loss=80.2259
	step [37/206], loss=79.4976
	step [38/206], loss=107.1561
	step [39/206], loss=68.0463
	step [40/206], loss=78.2692
	step [41/206], loss=85.2678
	step [42/206], loss=94.5372
	step [43/206], loss=76.3121
	step [44/206], loss=84.3318
	step [45/206], loss=88.9580
	step [46/206], loss=79.7466
	step [47/206], loss=82.9234
	step [48/206], loss=102.2792
	step [49/206], loss=97.5822
	step [50/206], loss=77.3490
	step [51/206], loss=76.7941
	step [52/206], loss=80.5684
	step [53/206], loss=68.7544
	step [54/206], loss=87.0022
	step [55/206], loss=89.1639
	step [56/206], loss=85.5612
	step [57/206], loss=94.2871
	step [58/206], loss=80.1590
	step [59/206], loss=86.3551
	step [60/206], loss=74.4188
	step [61/206], loss=81.2789
	step [62/206], loss=81.3666
	step [63/206], loss=80.1482
	step [64/206], loss=76.5096
	step [65/206], loss=86.2573
	step [66/206], loss=84.4194
	step [67/206], loss=81.7389
	step [68/206], loss=81.7224
	step [69/206], loss=76.7841
	step [70/206], loss=72.6988
	step [71/206], loss=89.0579
	step [72/206], loss=79.6030
	step [73/206], loss=94.1078
	step [74/206], loss=68.6683
	step [75/206], loss=85.6798
	step [76/206], loss=78.9683
	step [77/206], loss=97.3780
	step [78/206], loss=81.5002
	step [79/206], loss=78.7156
	step [80/206], loss=87.7231
	step [81/206], loss=85.5193
	step [82/206], loss=84.9281
	step [83/206], loss=84.2743
	step [84/206], loss=71.3794
	step [85/206], loss=101.2691
	step [86/206], loss=76.9981
	step [87/206], loss=74.2573
	step [88/206], loss=87.8332
	step [89/206], loss=87.1076
	step [90/206], loss=79.4265
	step [91/206], loss=65.8262
	step [92/206], loss=75.2460
	step [93/206], loss=81.9959
	step [94/206], loss=80.6798
	step [95/206], loss=81.3546
	step [96/206], loss=79.6154
	step [97/206], loss=86.3305
	step [98/206], loss=78.1904
	step [99/206], loss=83.2747
	step [100/206], loss=70.7543
	step [101/206], loss=87.8897
	step [102/206], loss=97.4353
	step [103/206], loss=77.5059
	step [104/206], loss=85.5618
	step [105/206], loss=86.2952
	step [106/206], loss=81.6358
	step [107/206], loss=86.9220
	step [108/206], loss=94.5312
	step [109/206], loss=88.2577
	step [110/206], loss=82.0859
	step [111/206], loss=71.9709
	step [112/206], loss=89.6760
	step [113/206], loss=80.9036
	step [114/206], loss=82.0203
	step [115/206], loss=84.4852
	step [116/206], loss=80.2249
	step [117/206], loss=68.9230
	step [118/206], loss=82.0060
	step [119/206], loss=78.1156
	step [120/206], loss=81.8054
	step [121/206], loss=76.9802
	step [122/206], loss=87.4715
	step [123/206], loss=83.7954
	step [124/206], loss=79.0375
	step [125/206], loss=95.4222
	step [126/206], loss=85.2285
	step [127/206], loss=90.7008
	step [128/206], loss=99.4316
	step [129/206], loss=74.5918
	step [130/206], loss=83.5432
	step [131/206], loss=78.1967
	step [132/206], loss=87.3369
	step [133/206], loss=78.6259
	step [134/206], loss=86.3779
	step [135/206], loss=78.9991
	step [136/206], loss=78.4328
	step [137/206], loss=89.4453
	step [138/206], loss=75.0493
	step [139/206], loss=85.1806
	step [140/206], loss=73.2858
	step [141/206], loss=82.4977
	step [142/206], loss=92.8296
	step [143/206], loss=89.3635
	step [144/206], loss=85.9707
	step [145/206], loss=82.8389
	step [146/206], loss=84.6192
	step [147/206], loss=75.9557
	step [148/206], loss=77.5666
	step [149/206], loss=75.1308
	step [150/206], loss=74.6277
	step [151/206], loss=83.8814
	step [152/206], loss=86.8945
	step [153/206], loss=82.5337
	step [154/206], loss=73.5648
	step [155/206], loss=81.6812
	step [156/206], loss=85.2468
	step [157/206], loss=87.0377
	step [158/206], loss=85.8199
	step [159/206], loss=88.0692
	step [160/206], loss=97.3440
	step [161/206], loss=92.7372
	step [162/206], loss=78.3048
	step [163/206], loss=90.2393
	step [164/206], loss=89.3162
	step [165/206], loss=79.7171
	step [166/206], loss=82.9999
	step [167/206], loss=82.7540
	step [168/206], loss=87.6162
	step [169/206], loss=100.8906
	step [170/206], loss=92.6331
	step [171/206], loss=80.6941
	step [172/206], loss=79.0080
	step [173/206], loss=85.7438
	step [174/206], loss=82.5692
	step [175/206], loss=85.5860
	step [176/206], loss=89.9394
	step [177/206], loss=80.3995
	step [178/206], loss=83.2574
	step [179/206], loss=89.9502
	step [180/206], loss=82.9161
	step [181/206], loss=69.9708
	step [182/206], loss=91.0243
	step [183/206], loss=98.1608
	step [184/206], loss=90.7556
	step [185/206], loss=74.2883
	step [186/206], loss=77.1767
	step [187/206], loss=73.1435
	step [188/206], loss=77.1741
	step [189/206], loss=101.9641
	step [190/206], loss=78.8130
	step [191/206], loss=77.2372
	step [192/206], loss=70.3253
	step [193/206], loss=80.0051
	step [194/206], loss=75.4233
	step [195/206], loss=73.0462
	step [196/206], loss=75.0626
	step [197/206], loss=79.8416
	step [198/206], loss=73.2982
	step [199/206], loss=80.6563
	step [200/206], loss=68.1628
	step [201/206], loss=84.3359
	step [202/206], loss=92.4871
	step [203/206], loss=80.8066
	step [204/206], loss=81.9077
	step [205/206], loss=90.1356
	step [206/206], loss=57.7737
	Evaluating
	loss=0.0100, precision=0.3459, recall=0.9153, f1=0.5020
Training epoch 45
	step [1/206], loss=86.2010
	step [2/206], loss=80.3408
	step [3/206], loss=81.2803
	step [4/206], loss=74.5276
	step [5/206], loss=94.3280
	step [6/206], loss=75.9295
	step [7/206], loss=73.3532
	step [8/206], loss=84.0124
	step [9/206], loss=80.9938
	step [10/206], loss=91.2118
	step [11/206], loss=76.8658
	step [12/206], loss=85.7085
	step [13/206], loss=94.5686
	step [14/206], loss=90.0102
	step [15/206], loss=83.9476
	step [16/206], loss=82.1266
	step [17/206], loss=97.1072
	step [18/206], loss=69.3524
	step [19/206], loss=80.9234
	step [20/206], loss=94.1128
	step [21/206], loss=75.8000
	step [22/206], loss=74.4896
	step [23/206], loss=83.2651
	step [24/206], loss=75.6403
	step [25/206], loss=74.4959
	step [26/206], loss=93.0332
	step [27/206], loss=81.2781
	step [28/206], loss=80.9503
	step [29/206], loss=78.2589
	step [30/206], loss=83.8139
	step [31/206], loss=83.4585
	step [32/206], loss=81.2953
	step [33/206], loss=76.7488
	step [34/206], loss=92.7695
	step [35/206], loss=85.7613
	step [36/206], loss=73.5381
	step [37/206], loss=115.3625
	step [38/206], loss=74.9530
	step [39/206], loss=78.2151
	step [40/206], loss=96.1345
	step [41/206], loss=96.0157
	step [42/206], loss=81.2126
	step [43/206], loss=87.1294
	step [44/206], loss=92.1688
	step [45/206], loss=94.6862
	step [46/206], loss=58.9514
	step [47/206], loss=84.0970
	step [48/206], loss=98.5188
	step [49/206], loss=77.0745
	step [50/206], loss=86.7961
	step [51/206], loss=82.6588
	step [52/206], loss=80.3214
	step [53/206], loss=72.5259
	step [54/206], loss=81.6311
	step [55/206], loss=83.9177
	step [56/206], loss=83.2728
	step [57/206], loss=80.3049
	step [58/206], loss=79.2025
	step [59/206], loss=80.2072
	step [60/206], loss=79.6830
	step [61/206], loss=77.3046
	step [62/206], loss=85.9608
	step [63/206], loss=84.0957
	step [64/206], loss=69.8404
	step [65/206], loss=84.3734
	step [66/206], loss=78.3084
	step [67/206], loss=69.6434
	step [68/206], loss=86.9798
	step [69/206], loss=89.2067
	step [70/206], loss=69.3106
	step [71/206], loss=70.7222
	step [72/206], loss=82.9836
	step [73/206], loss=78.6897
	step [74/206], loss=78.0594
	step [75/206], loss=91.2146
	step [76/206], loss=73.0672
	step [77/206], loss=84.9602
	step [78/206], loss=95.0236
	step [79/206], loss=78.5470
	step [80/206], loss=95.9410
	step [81/206], loss=94.6145
	step [82/206], loss=79.8376
	step [83/206], loss=81.3534
	step [84/206], loss=88.9485
	step [85/206], loss=89.8474
	step [86/206], loss=89.6754
	step [87/206], loss=94.1420
	step [88/206], loss=69.1069
	step [89/206], loss=88.7312
	step [90/206], loss=90.9212
	step [91/206], loss=77.8877
	step [92/206], loss=88.6657
	step [93/206], loss=74.0444
	step [94/206], loss=85.0953
	step [95/206], loss=84.6477
	step [96/206], loss=94.6891
	step [97/206], loss=93.6707
	step [98/206], loss=92.5860
	step [99/206], loss=81.1714
	step [100/206], loss=85.7435
	step [101/206], loss=84.5429
	step [102/206], loss=75.1096
	step [103/206], loss=81.3111
	step [104/206], loss=85.2709
	step [105/206], loss=94.9217
	step [106/206], loss=81.7157
	step [107/206], loss=72.1009
	step [108/206], loss=86.8320
	step [109/206], loss=81.3689
	step [110/206], loss=80.8590
	step [111/206], loss=84.7239
	step [112/206], loss=71.4687
	step [113/206], loss=96.9445
	step [114/206], loss=80.1252
	step [115/206], loss=87.2758
	step [116/206], loss=71.9671
	step [117/206], loss=84.4368
	step [118/206], loss=65.4117
	step [119/206], loss=73.8337
	step [120/206], loss=91.0577
	step [121/206], loss=82.7493
	step [122/206], loss=90.6721
	step [123/206], loss=88.6216
	step [124/206], loss=75.1947
	step [125/206], loss=75.6161
	step [126/206], loss=73.8931
	step [127/206], loss=97.7501
	step [128/206], loss=83.3567
	step [129/206], loss=83.1228
	step [130/206], loss=80.8311
	step [131/206], loss=69.1573
	step [132/206], loss=93.6541
	step [133/206], loss=82.7453
	step [134/206], loss=92.3665
	step [135/206], loss=64.6692
	step [136/206], loss=94.6834
	step [137/206], loss=74.8731
	step [138/206], loss=88.7834
	step [139/206], loss=70.7461
	step [140/206], loss=77.0717
	step [141/206], loss=90.1632
	step [142/206], loss=84.7771
	step [143/206], loss=79.6861
	step [144/206], loss=71.2655
	step [145/206], loss=87.7476
	step [146/206], loss=77.6035
	step [147/206], loss=89.9678
	step [148/206], loss=84.6814
	step [149/206], loss=89.8714
	step [150/206], loss=82.8998
	step [151/206], loss=89.2474
	step [152/206], loss=89.9969
	step [153/206], loss=88.7551
	step [154/206], loss=84.6230
	step [155/206], loss=76.2886
	step [156/206], loss=84.7006
	step [157/206], loss=77.5911
	step [158/206], loss=75.7292
	step [159/206], loss=75.1467
	step [160/206], loss=91.1604
	step [161/206], loss=102.0016
	step [162/206], loss=75.6449
	step [163/206], loss=93.4833
	step [164/206], loss=85.5739
	step [165/206], loss=86.4033
	step [166/206], loss=71.8950
	step [167/206], loss=81.6372
	step [168/206], loss=100.8194
	step [169/206], loss=74.2223
	step [170/206], loss=77.6643
	step [171/206], loss=72.5702
	step [172/206], loss=78.4615
	step [173/206], loss=76.4870
	step [174/206], loss=85.1389
	step [175/206], loss=61.4639
	step [176/206], loss=84.7626
	step [177/206], loss=78.0178
	step [178/206], loss=75.1067
	step [179/206], loss=77.0365
	step [180/206], loss=80.8237
	step [181/206], loss=82.8350
	step [182/206], loss=85.9800
	step [183/206], loss=75.5088
	step [184/206], loss=79.5041
	step [185/206], loss=85.2826
	step [186/206], loss=85.0863
	step [187/206], loss=84.9142
	step [188/206], loss=82.7270
	step [189/206], loss=94.1848
	step [190/206], loss=83.5410
	step [191/206], loss=73.9263
	step [192/206], loss=83.6093
	step [193/206], loss=85.3732
	step [194/206], loss=91.9656
	step [195/206], loss=89.5462
	step [196/206], loss=73.3160
	step [197/206], loss=82.3765
	step [198/206], loss=81.0657
	step [199/206], loss=72.6393
	step [200/206], loss=96.5603
	step [201/206], loss=91.6395
	step [202/206], loss=79.8957
	step [203/206], loss=83.8997
	step [204/206], loss=70.3674
	step [205/206], loss=94.7594
	step [206/206], loss=53.4282
	Evaluating
	loss=0.0101, precision=0.3451, recall=0.9128, f1=0.5008
Training epoch 46
	step [1/206], loss=71.3971
	step [2/206], loss=84.1435
	step [3/206], loss=91.7870
	step [4/206], loss=83.1166
	step [5/206], loss=82.1149
	step [6/206], loss=78.8632
	step [7/206], loss=82.5902
	step [8/206], loss=75.7189
	step [9/206], loss=75.6827
	step [10/206], loss=72.4398
	step [11/206], loss=72.3713
	step [12/206], loss=73.1810
	step [13/206], loss=78.9783
	step [14/206], loss=73.3310
	step [15/206], loss=91.2536
	step [16/206], loss=87.9154
	step [17/206], loss=89.3371
	step [18/206], loss=97.6254
	step [19/206], loss=71.3956
	step [20/206], loss=85.9272
	step [21/206], loss=78.0118
	step [22/206], loss=96.1882
	step [23/206], loss=79.1655
	step [24/206], loss=84.4944
	step [25/206], loss=81.3661
	step [26/206], loss=92.6063
	step [27/206], loss=76.5159
	step [28/206], loss=84.9720
	step [29/206], loss=89.9361
	step [30/206], loss=83.7865
	step [31/206], loss=77.0696
	step [32/206], loss=73.4425
	step [33/206], loss=85.5603
	step [34/206], loss=69.8358
	step [35/206], loss=78.8465
	step [36/206], loss=87.2093
	step [37/206], loss=79.8482
	step [38/206], loss=79.1173
	step [39/206], loss=88.9091
	step [40/206], loss=75.8428
	step [41/206], loss=81.8655
	step [42/206], loss=92.7900
	step [43/206], loss=74.1309
	step [44/206], loss=99.7948
	step [45/206], loss=82.7816
	step [46/206], loss=91.1966
	step [47/206], loss=81.1259
	step [48/206], loss=87.7133
	step [49/206], loss=93.2043
	step [50/206], loss=98.0633
	step [51/206], loss=69.2295
	step [52/206], loss=86.0741
	step [53/206], loss=103.0842
	step [54/206], loss=87.6200
	step [55/206], loss=81.4081
	step [56/206], loss=77.7292
	step [57/206], loss=83.1321
	step [58/206], loss=67.8477
	step [59/206], loss=80.9083
	step [60/206], loss=84.3107
	step [61/206], loss=90.5266
	step [62/206], loss=91.7445
	step [63/206], loss=83.3518
	step [64/206], loss=83.6744
	step [65/206], loss=86.4382
	step [66/206], loss=75.6650
	step [67/206], loss=81.1927
	step [68/206], loss=88.1007
	step [69/206], loss=96.5415
	step [70/206], loss=81.1311
	step [71/206], loss=69.3237
	step [72/206], loss=94.2524
	step [73/206], loss=89.7650
	step [74/206], loss=80.5498
	step [75/206], loss=78.8428
	step [76/206], loss=78.4435
	step [77/206], loss=78.9258
	step [78/206], loss=77.6161
	step [79/206], loss=79.5734
	step [80/206], loss=70.7728
	step [81/206], loss=80.8287
	step [82/206], loss=69.6586
	step [83/206], loss=67.9851
	step [84/206], loss=93.4779
	step [85/206], loss=75.1631
	step [86/206], loss=91.5110
	step [87/206], loss=83.9230
	step [88/206], loss=65.9973
	step [89/206], loss=90.8493
	step [90/206], loss=73.5870
	step [91/206], loss=84.9108
	step [92/206], loss=82.8368
	step [93/206], loss=73.9049
	step [94/206], loss=96.3151
	step [95/206], loss=92.4956
	step [96/206], loss=87.1342
	step [97/206], loss=72.6686
	step [98/206], loss=86.0044
	step [99/206], loss=90.3498
	step [100/206], loss=71.6266
	step [101/206], loss=91.0190
	step [102/206], loss=78.7544
	step [103/206], loss=85.3220
	step [104/206], loss=90.7538
	step [105/206], loss=67.4312
	step [106/206], loss=89.9427
	step [107/206], loss=78.4271
	step [108/206], loss=83.6106
	step [109/206], loss=82.5624
	step [110/206], loss=84.8168
	step [111/206], loss=81.0476
	step [112/206], loss=81.9075
	step [113/206], loss=67.2371
	step [114/206], loss=70.9110
	step [115/206], loss=78.7855
	step [116/206], loss=89.1913
	step [117/206], loss=67.6540
	step [118/206], loss=88.4490
	step [119/206], loss=77.1835
	step [120/206], loss=74.8488
	step [121/206], loss=86.0130
	step [122/206], loss=83.2583
	step [123/206], loss=93.5075
	step [124/206], loss=70.0691
	step [125/206], loss=70.7148
	step [126/206], loss=73.6823
	step [127/206], loss=77.7536
	step [128/206], loss=81.3503
	step [129/206], loss=79.8071
	step [130/206], loss=77.7795
	step [131/206], loss=76.1901
	step [132/206], loss=67.4941
	step [133/206], loss=78.2085
	step [134/206], loss=77.4356
	step [135/206], loss=99.9387
	step [136/206], loss=74.2546
	step [137/206], loss=69.0613
	step [138/206], loss=91.3407
	step [139/206], loss=82.9205
	step [140/206], loss=98.2210
	step [141/206], loss=83.4426
	step [142/206], loss=91.8716
	step [143/206], loss=68.3602
	step [144/206], loss=90.4517
	step [145/206], loss=82.4273
	step [146/206], loss=75.9634
	step [147/206], loss=92.4142
	step [148/206], loss=73.7716
	step [149/206], loss=72.1246
	step [150/206], loss=77.5681
	step [151/206], loss=84.3313
	step [152/206], loss=86.4442
	step [153/206], loss=84.9888
	step [154/206], loss=77.9128
	step [155/206], loss=95.2396
	step [156/206], loss=75.4934
	step [157/206], loss=84.6747
	step [158/206], loss=67.3990
	step [159/206], loss=76.9556
	step [160/206], loss=87.7935
	step [161/206], loss=85.6617
	step [162/206], loss=78.4944
	step [163/206], loss=84.3225
	step [164/206], loss=65.3433
	step [165/206], loss=76.6582
	step [166/206], loss=90.6979
	step [167/206], loss=85.9477
	step [168/206], loss=86.8111
	step [169/206], loss=78.5115
	step [170/206], loss=91.2124
	step [171/206], loss=77.4817
	step [172/206], loss=82.0703
	step [173/206], loss=80.4788
	step [174/206], loss=89.6662
	step [175/206], loss=76.8634
	step [176/206], loss=70.0903
	step [177/206], loss=81.5417
	step [178/206], loss=94.0274
	step [179/206], loss=91.2086
	step [180/206], loss=79.2147
	step [181/206], loss=93.9893
	step [182/206], loss=93.4830
	step [183/206], loss=75.8955
	step [184/206], loss=83.6492
	step [185/206], loss=76.9981
	step [186/206], loss=80.3950
	step [187/206], loss=98.8411
	step [188/206], loss=80.7584
	step [189/206], loss=78.3716
	step [190/206], loss=88.4401
	step [191/206], loss=79.4579
	step [192/206], loss=81.0550
	step [193/206], loss=90.2125
	step [194/206], loss=93.7481
	step [195/206], loss=81.7011
	step [196/206], loss=79.6407
	step [197/206], loss=79.5940
	step [198/206], loss=83.6423
	step [199/206], loss=97.9938
	step [200/206], loss=74.2364
	step [201/206], loss=91.2494
	step [202/206], loss=81.9786
	step [203/206], loss=71.0516
	step [204/206], loss=85.4086
	step [205/206], loss=91.1135
	step [206/206], loss=55.8795
	Evaluating
	loss=0.0095, precision=0.3569, recall=0.9031, f1=0.5116
Training epoch 47
	step [1/206], loss=70.4078
	step [2/206], loss=73.6482
	step [3/206], loss=82.0579
	step [4/206], loss=81.2997
	step [5/206], loss=81.5855
	step [6/206], loss=70.5865
	step [7/206], loss=87.3422
	step [8/206], loss=89.9365
	step [9/206], loss=78.5608
	step [10/206], loss=77.7241
	step [11/206], loss=89.8252
	step [12/206], loss=95.8134
	step [13/206], loss=82.0589
	step [14/206], loss=80.8745
	step [15/206], loss=71.0906
	step [16/206], loss=93.5542
	step [17/206], loss=77.9843
	step [18/206], loss=88.6098
	step [19/206], loss=94.6949
	step [20/206], loss=79.6283
	step [21/206], loss=84.1890
	step [22/206], loss=75.2620
	step [23/206], loss=97.0634
	step [24/206], loss=90.4346
	step [25/206], loss=72.1612
	step [26/206], loss=106.9058
	step [27/206], loss=82.5565
	step [28/206], loss=81.0450
	step [29/206], loss=89.2589
	step [30/206], loss=83.6535
	step [31/206], loss=77.7083
	step [32/206], loss=86.6658
	step [33/206], loss=75.4355
	step [34/206], loss=95.5867
	step [35/206], loss=84.0171
	step [36/206], loss=77.9788
	step [37/206], loss=96.6757
	step [38/206], loss=96.5678
	step [39/206], loss=74.6744
	step [40/206], loss=78.8127
	step [41/206], loss=82.3712
	step [42/206], loss=85.8031
	step [43/206], loss=71.2611
	step [44/206], loss=91.0587
	step [45/206], loss=79.4309
	step [46/206], loss=72.8432
	step [47/206], loss=78.3710
	step [48/206], loss=62.5093
	step [49/206], loss=84.9756
	step [50/206], loss=81.9449
	step [51/206], loss=67.6633
	step [52/206], loss=75.0556
	step [53/206], loss=89.4047
	step [54/206], loss=72.9429
	step [55/206], loss=94.4650
	step [56/206], loss=85.7750
	step [57/206], loss=82.7561
	step [58/206], loss=85.0085
	step [59/206], loss=79.9876
	step [60/206], loss=84.6226
	step [61/206], loss=83.5309
	step [62/206], loss=88.9286
	step [63/206], loss=86.9282
	step [64/206], loss=83.8675
	step [65/206], loss=84.6194
	step [66/206], loss=74.8619
	step [67/206], loss=80.2704
	step [68/206], loss=72.5678
	step [69/206], loss=79.3565
	step [70/206], loss=90.5152
	step [71/206], loss=80.4523
	step [72/206], loss=84.8042
	step [73/206], loss=77.2847
	step [74/206], loss=80.7487
	step [75/206], loss=91.1576
	step [76/206], loss=100.2254
	step [77/206], loss=90.0859
	step [78/206], loss=78.9899
	step [79/206], loss=87.8846
	step [80/206], loss=86.1426
	step [81/206], loss=73.8109
	step [82/206], loss=91.6888
	step [83/206], loss=78.4728
	step [84/206], loss=79.0615
	step [85/206], loss=84.6066
	step [86/206], loss=88.0047
	step [87/206], loss=59.5311
	step [88/206], loss=91.8430
	step [89/206], loss=83.2356
	step [90/206], loss=72.3813
	step [91/206], loss=73.3601
	step [92/206], loss=83.7148
	step [93/206], loss=80.1806
	step [94/206], loss=72.0159
	step [95/206], loss=72.6810
	step [96/206], loss=88.3001
	step [97/206], loss=76.9221
	step [98/206], loss=75.5928
	step [99/206], loss=79.9826
	step [100/206], loss=74.2804
	step [101/206], loss=91.9877
	step [102/206], loss=73.4557
	step [103/206], loss=80.7107
	step [104/206], loss=84.8524
	step [105/206], loss=76.1980
	step [106/206], loss=85.7815
	step [107/206], loss=66.3370
	step [108/206], loss=69.4353
	step [109/206], loss=75.6158
	step [110/206], loss=78.8226
	step [111/206], loss=85.9799
	step [112/206], loss=83.4915
	step [113/206], loss=77.7353
	step [114/206], loss=84.8307
	step [115/206], loss=83.0740
	step [116/206], loss=79.2487
	step [117/206], loss=78.2741
	step [118/206], loss=88.6471
	step [119/206], loss=63.3335
	step [120/206], loss=76.8297
	step [121/206], loss=77.9834
	step [122/206], loss=80.5548
	step [123/206], loss=80.7366
	step [124/206], loss=92.2120
	step [125/206], loss=75.5678
	step [126/206], loss=84.4420
	step [127/206], loss=82.2491
	step [128/206], loss=97.2635
	step [129/206], loss=86.4112
	step [130/206], loss=71.7004
	step [131/206], loss=77.6887
	step [132/206], loss=98.3694
	step [133/206], loss=94.6478
	step [134/206], loss=77.8187
	step [135/206], loss=82.5342
	step [136/206], loss=76.4121
	step [137/206], loss=91.8846
	step [138/206], loss=81.7751
	step [139/206], loss=86.1151
	step [140/206], loss=68.2558
	step [141/206], loss=80.7686
	step [142/206], loss=85.8233
	step [143/206], loss=91.4123
	step [144/206], loss=82.8562
	step [145/206], loss=70.8737
	step [146/206], loss=74.5943
	step [147/206], loss=81.7488
	step [148/206], loss=80.4738
	step [149/206], loss=92.9111
	step [150/206], loss=79.8412
	step [151/206], loss=86.2837
	step [152/206], loss=77.9368
	step [153/206], loss=89.0628
	step [154/206], loss=90.1645
	step [155/206], loss=92.6382
	step [156/206], loss=85.6367
	step [157/206], loss=90.2246
	step [158/206], loss=78.6703
	step [159/206], loss=78.3895
	step [160/206], loss=82.5203
	step [161/206], loss=83.9246
	step [162/206], loss=72.5518
	step [163/206], loss=70.8296
	step [164/206], loss=88.2588
	step [165/206], loss=91.5204
	step [166/206], loss=80.5243
	step [167/206], loss=85.1349
	step [168/206], loss=85.3123
	step [169/206], loss=83.4721
	step [170/206], loss=87.0535
	step [171/206], loss=74.9073
	step [172/206], loss=86.9970
	step [173/206], loss=74.8863
	step [174/206], loss=90.5358
	step [175/206], loss=87.3380
	step [176/206], loss=77.9273
	step [177/206], loss=74.5199
	step [178/206], loss=93.6125
	step [179/206], loss=83.8967
	step [180/206], loss=71.4047
	step [181/206], loss=69.8904
	step [182/206], loss=78.9117
	step [183/206], loss=80.5045
	step [184/206], loss=94.2383
	step [185/206], loss=83.9225
	step [186/206], loss=66.2432
	step [187/206], loss=72.3725
	step [188/206], loss=91.9063
	step [189/206], loss=90.9997
	step [190/206], loss=91.5091
	step [191/206], loss=81.0855
	step [192/206], loss=75.6906
	step [193/206], loss=81.6186
	step [194/206], loss=76.4904
	step [195/206], loss=76.7706
	step [196/206], loss=89.5123
	step [197/206], loss=95.3402
	step [198/206], loss=83.0533
	step [199/206], loss=78.3752
	step [200/206], loss=79.5004
	step [201/206], loss=75.8826
	step [202/206], loss=84.6538
	step [203/206], loss=74.3516
	step [204/206], loss=92.2015
	step [205/206], loss=73.5131
	step [206/206], loss=45.1181
	Evaluating
	loss=0.0080, precision=0.4063, recall=0.9043, f1=0.5607
saving model as: 4_saved_model.pth
Training epoch 48
	step [1/206], loss=84.4596
	step [2/206], loss=88.3284
	step [3/206], loss=75.0546
	step [4/206], loss=94.9215
	step [5/206], loss=81.0950
	step [6/206], loss=82.0644
	step [7/206], loss=82.7194
	step [8/206], loss=84.0198
	step [9/206], loss=77.9673
	step [10/206], loss=76.5690
	step [11/206], loss=85.8913
	step [12/206], loss=89.3341
	step [13/206], loss=76.3228
	step [14/206], loss=84.3655
	step [15/206], loss=76.3910
	step [16/206], loss=90.1057
	step [17/206], loss=69.8919
	step [18/206], loss=78.6817
	step [19/206], loss=83.9735
	step [20/206], loss=81.8042
	step [21/206], loss=72.4239
	step [22/206], loss=74.7263
	step [23/206], loss=89.8513
	step [24/206], loss=74.9015
	step [25/206], loss=83.7977
	step [26/206], loss=100.1620
	step [27/206], loss=81.7063
	step [28/206], loss=102.4507
	step [29/206], loss=76.7859
	step [30/206], loss=73.1394
	step [31/206], loss=88.0053
	step [32/206], loss=86.9223
	step [33/206], loss=82.9271
	step [34/206], loss=67.3973
	step [35/206], loss=87.1539
	step [36/206], loss=82.5178
	step [37/206], loss=71.4179
	step [38/206], loss=89.5721
	step [39/206], loss=70.2868
	step [40/206], loss=87.9240
	step [41/206], loss=79.9767
	step [42/206], loss=74.0057
	step [43/206], loss=76.8979
	step [44/206], loss=90.9855
	step [45/206], loss=91.6854
	step [46/206], loss=75.4867
	step [47/206], loss=61.8070
	step [48/206], loss=86.9797
	step [49/206], loss=94.6697
	step [50/206], loss=76.4920
	step [51/206], loss=75.9445
	step [52/206], loss=70.5810
	step [53/206], loss=80.4679
	step [54/206], loss=83.2261
	step [55/206], loss=93.5756
	step [56/206], loss=76.6184
	step [57/206], loss=98.3096
	step [58/206], loss=79.1230
	step [59/206], loss=81.4609
	step [60/206], loss=89.7190
	step [61/206], loss=86.3179
	step [62/206], loss=72.4469
	step [63/206], loss=100.1448
	step [64/206], loss=85.3910
	step [65/206], loss=80.1984
	step [66/206], loss=78.4703
	step [67/206], loss=97.2683
	step [68/206], loss=83.6855
	step [69/206], loss=88.3934
	step [70/206], loss=87.8919
	step [71/206], loss=80.0959
	step [72/206], loss=78.4291
	step [73/206], loss=89.7105
	step [74/206], loss=86.9004
	step [75/206], loss=86.7894
	step [76/206], loss=85.4903
	step [77/206], loss=95.2801
	step [78/206], loss=80.1472
	step [79/206], loss=88.4444
	step [80/206], loss=75.7128
	step [81/206], loss=84.5853
	step [82/206], loss=78.7514
	step [83/206], loss=89.0804
	step [84/206], loss=75.3137
	step [85/206], loss=80.1323
	step [86/206], loss=64.8841
	step [87/206], loss=74.9172
	step [88/206], loss=88.8601
	step [89/206], loss=91.9437
	step [90/206], loss=78.9706
	step [91/206], loss=80.5665
	step [92/206], loss=83.8767
	step [93/206], loss=84.2901
	step [94/206], loss=79.6255
	step [95/206], loss=72.6069
	step [96/206], loss=84.9672
	step [97/206], loss=85.8763
	step [98/206], loss=78.9202
	step [99/206], loss=75.8206
	step [100/206], loss=76.5076
	step [101/206], loss=86.9629
	step [102/206], loss=75.2903
	step [103/206], loss=83.8185
	step [104/206], loss=72.9555
	step [105/206], loss=75.4981
	step [106/206], loss=75.4189
	step [107/206], loss=97.3221
	step [108/206], loss=90.1988
	step [109/206], loss=90.6229
	step [110/206], loss=84.3456
	step [111/206], loss=86.8959
	step [112/206], loss=82.2787
	step [113/206], loss=84.6037
	step [114/206], loss=75.9419
	step [115/206], loss=87.0470
	step [116/206], loss=82.1877
	step [117/206], loss=80.0071
	step [118/206], loss=85.8883
	step [119/206], loss=71.2673
	step [120/206], loss=73.2578
	step [121/206], loss=87.3797
	step [122/206], loss=80.2299
	step [123/206], loss=83.9494
	step [124/206], loss=75.9955
	step [125/206], loss=77.1511
	step [126/206], loss=102.6625
	step [127/206], loss=75.2850
	step [128/206], loss=78.4430
	step [129/206], loss=84.6152
	step [130/206], loss=66.4160
	step [131/206], loss=94.4674
	step [132/206], loss=89.7193
	step [133/206], loss=66.3988
	step [134/206], loss=73.8946
	step [135/206], loss=85.4623
	step [136/206], loss=87.1524
	step [137/206], loss=77.6124
	step [138/206], loss=82.0521
	step [139/206], loss=103.0881
	step [140/206], loss=87.2801
	step [141/206], loss=80.1897
	step [142/206], loss=68.6329
	step [143/206], loss=76.0522
	step [144/206], loss=74.6231
	step [145/206], loss=73.7955
	step [146/206], loss=86.3620
	step [147/206], loss=69.2763
	step [148/206], loss=85.1506
	step [149/206], loss=89.2981
	step [150/206], loss=88.1113
	step [151/206], loss=77.1297
	step [152/206], loss=79.7811
	step [153/206], loss=91.3161
	step [154/206], loss=64.0190
	step [155/206], loss=75.2910
	step [156/206], loss=73.2786
	step [157/206], loss=83.3827
	step [158/206], loss=73.2146
	step [159/206], loss=87.0794
	step [160/206], loss=85.9363
	step [161/206], loss=87.8188
	step [162/206], loss=75.8546
	step [163/206], loss=98.3102
	step [164/206], loss=80.5166
	step [165/206], loss=88.3767
	step [166/206], loss=75.3293
	step [167/206], loss=80.1995
	step [168/206], loss=77.9718
	step [169/206], loss=73.1724
	step [170/206], loss=73.1116
	step [171/206], loss=75.5446
	step [172/206], loss=68.4684
	step [173/206], loss=77.9841
	step [174/206], loss=78.0985
	step [175/206], loss=77.0122
	step [176/206], loss=92.6336
	step [177/206], loss=74.5734
	step [178/206], loss=80.8693
	step [179/206], loss=74.6628
	step [180/206], loss=79.1710
	step [181/206], loss=61.4218
	step [182/206], loss=83.3645
	step [183/206], loss=73.6719
	step [184/206], loss=80.2244
	step [185/206], loss=90.3985
	step [186/206], loss=73.3343
	step [187/206], loss=73.9811
	step [188/206], loss=97.4187
	step [189/206], loss=69.9632
	step [190/206], loss=83.2885
	step [191/206], loss=74.9268
	step [192/206], loss=82.9713
	step [193/206], loss=78.5145
	step [194/206], loss=81.7482
	step [195/206], loss=95.8540
	step [196/206], loss=88.6932
	step [197/206], loss=90.9708
	step [198/206], loss=72.1031
	step [199/206], loss=90.7122
	step [200/206], loss=71.3866
	step [201/206], loss=87.8399
	step [202/206], loss=74.4646
	step [203/206], loss=83.1643
	step [204/206], loss=83.2180
	step [205/206], loss=90.8239
	step [206/206], loss=43.5847
	Evaluating
	loss=0.0082, precision=0.4106, recall=0.9002, f1=0.5640
saving model as: 4_saved_model.pth
Training epoch 49
	step [1/206], loss=68.1395
	step [2/206], loss=92.8702
	step [3/206], loss=73.8654
	step [4/206], loss=86.1705
	step [5/206], loss=85.8961
	step [6/206], loss=86.7373
	step [7/206], loss=83.2429
	step [8/206], loss=87.7556
	step [9/206], loss=72.2967
	step [10/206], loss=89.6986
	step [11/206], loss=94.0816
	step [12/206], loss=68.0174
	step [13/206], loss=82.7509
	step [14/206], loss=72.4151
	step [15/206], loss=96.2831
	step [16/206], loss=88.1221
	step [17/206], loss=85.0912
	step [18/206], loss=66.6416
	step [19/206], loss=89.8943
	step [20/206], loss=94.4646
	step [21/206], loss=76.6674
	step [22/206], loss=88.1488
	step [23/206], loss=93.8274
	step [24/206], loss=83.3572
	step [25/206], loss=79.7899
	step [26/206], loss=85.8282
	step [27/206], loss=82.2474
	step [28/206], loss=85.2630
	step [29/206], loss=71.0823
	step [30/206], loss=78.2262
	step [31/206], loss=84.5011
	step [32/206], loss=76.0038
	step [33/206], loss=81.6750
	step [34/206], loss=89.7162
	step [35/206], loss=83.1394
	step [36/206], loss=86.8049
	step [37/206], loss=96.9582
	step [38/206], loss=75.0146
	step [39/206], loss=79.8335
	step [40/206], loss=78.0933
	step [41/206], loss=65.8634
	step [42/206], loss=97.8472
	step [43/206], loss=92.7005
	step [44/206], loss=88.1828
	step [45/206], loss=81.6623
	step [46/206], loss=71.1921
	step [47/206], loss=68.7982
	step [48/206], loss=89.7227
	step [49/206], loss=66.6516
	step [50/206], loss=95.0259
	step [51/206], loss=81.3418
	step [52/206], loss=93.4853
	step [53/206], loss=83.4067
	step [54/206], loss=85.6631
	step [55/206], loss=80.3908
	step [56/206], loss=87.7759
	step [57/206], loss=79.9223
	step [58/206], loss=64.1105
	step [59/206], loss=70.5724
	step [60/206], loss=71.2654
	step [61/206], loss=81.2218
	step [62/206], loss=81.0010
	step [63/206], loss=76.7181
	step [64/206], loss=79.8493
	step [65/206], loss=73.9562
	step [66/206], loss=75.6199
	step [67/206], loss=92.1863
	step [68/206], loss=81.6620
	step [69/206], loss=71.7262
	step [70/206], loss=81.8612
	step [71/206], loss=78.2441
	step [72/206], loss=68.2900
	step [73/206], loss=75.7838
	step [74/206], loss=85.9960
	step [75/206], loss=82.2505
	step [76/206], loss=58.4979
	step [77/206], loss=72.5729
	step [78/206], loss=85.2205
	step [79/206], loss=70.7345
	step [80/206], loss=76.2774
	step [81/206], loss=87.0264
	step [82/206], loss=79.0334
	step [83/206], loss=77.8334
	step [84/206], loss=82.2535
	step [85/206], loss=71.3392
	step [86/206], loss=86.4085
	step [87/206], loss=90.1724
	step [88/206], loss=80.5703
	step [89/206], loss=81.6871
	step [90/206], loss=76.6320
	step [91/206], loss=76.6196
	step [92/206], loss=82.7264
	step [93/206], loss=75.1189
	step [94/206], loss=75.5412
	step [95/206], loss=74.0821
	step [96/206], loss=88.8011
	step [97/206], loss=80.2669
	step [98/206], loss=80.3908
	step [99/206], loss=80.9140
	step [100/206], loss=87.8927
	step [101/206], loss=73.0526
	step [102/206], loss=77.9827
	step [103/206], loss=93.1685
	step [104/206], loss=87.6622
	step [105/206], loss=73.5854
	step [106/206], loss=101.2686
	step [107/206], loss=82.3594
	step [108/206], loss=64.1336
	step [109/206], loss=75.6117
	step [110/206], loss=87.2140
	step [111/206], loss=71.8732
	step [112/206], loss=87.6332
	step [113/206], loss=95.1635
	step [114/206], loss=79.2064
	step [115/206], loss=94.4930
	step [116/206], loss=90.3710
	step [117/206], loss=62.3347
	step [118/206], loss=100.2047
	step [119/206], loss=77.3398
	step [120/206], loss=92.5011
	step [121/206], loss=75.9706
	step [122/206], loss=90.7272
	step [123/206], loss=87.8341
	step [124/206], loss=83.7527
	step [125/206], loss=68.3009
	step [126/206], loss=80.1655
	step [127/206], loss=70.4948
	step [128/206], loss=72.0475
	step [129/206], loss=77.2744
	step [130/206], loss=84.3534
	step [131/206], loss=70.8029
	step [132/206], loss=60.0858
	step [133/206], loss=90.1614
	step [134/206], loss=88.0858
	step [135/206], loss=68.0384
	step [136/206], loss=78.6340
	step [137/206], loss=87.5387
	step [138/206], loss=86.2189
	step [139/206], loss=88.7600
	step [140/206], loss=90.7144
	step [141/206], loss=92.8579
	step [142/206], loss=84.0253
	step [143/206], loss=75.7821
	step [144/206], loss=96.0681
	step [145/206], loss=84.7092
	step [146/206], loss=87.2332
	step [147/206], loss=70.9968
	step [148/206], loss=81.5611
	step [149/206], loss=88.9021
	step [150/206], loss=86.4136
	step [151/206], loss=64.7383
	step [152/206], loss=83.4522
	step [153/206], loss=73.1450
	step [154/206], loss=86.9956
	step [155/206], loss=100.5991
	step [156/206], loss=85.3210
	step [157/206], loss=86.4527
	step [158/206], loss=92.1404
	step [159/206], loss=78.9690
	step [160/206], loss=83.7393
	step [161/206], loss=87.6426
	step [162/206], loss=76.7624
	step [163/206], loss=81.2404
	step [164/206], loss=92.8424
	step [165/206], loss=81.6031
	step [166/206], loss=83.4939
	step [167/206], loss=77.1875
	step [168/206], loss=77.7614
	step [169/206], loss=70.9338
	step [170/206], loss=83.9773
	step [171/206], loss=87.2822
	step [172/206], loss=71.7559
	step [173/206], loss=85.5868
	step [174/206], loss=75.0306
	step [175/206], loss=80.0927
	step [176/206], loss=97.6513
	step [177/206], loss=76.1155
	step [178/206], loss=65.7642
	step [179/206], loss=94.6863
	step [180/206], loss=85.5332
	step [181/206], loss=94.4520
	step [182/206], loss=80.7514
	step [183/206], loss=74.3769
	step [184/206], loss=76.6593
	step [185/206], loss=83.5743
	step [186/206], loss=77.2703
	step [187/206], loss=81.7051
	step [188/206], loss=83.1599
	step [189/206], loss=82.1436
	step [190/206], loss=77.6873
	step [191/206], loss=65.0541
	step [192/206], loss=86.8904
	step [193/206], loss=80.5507
	step [194/206], loss=79.5576
	step [195/206], loss=83.0409
	step [196/206], loss=78.3763
	step [197/206], loss=76.0332
	step [198/206], loss=78.9450
	step [199/206], loss=84.8211
	step [200/206], loss=77.8467
	step [201/206], loss=86.7347
	step [202/206], loss=76.3605
	step [203/206], loss=66.9232
	step [204/206], loss=73.3118
	step [205/206], loss=97.3635
	step [206/206], loss=49.0527
	Evaluating
	loss=0.0084, precision=0.3862, recall=0.9043, f1=0.5412
Training epoch 50
	step [1/206], loss=78.4397
	step [2/206], loss=81.5677
	step [3/206], loss=86.1146
	step [4/206], loss=81.9024
	step [5/206], loss=75.1667
	step [6/206], loss=80.6438
	step [7/206], loss=76.2446
	step [8/206], loss=95.7789
	step [9/206], loss=67.7710
	step [10/206], loss=86.1859
	step [11/206], loss=82.0038
	step [12/206], loss=76.3831
	step [13/206], loss=76.0834
	step [14/206], loss=81.6653
	step [15/206], loss=74.3307
	step [16/206], loss=83.3934
	step [17/206], loss=89.0093
	step [18/206], loss=72.4356
	step [19/206], loss=82.6925
	step [20/206], loss=85.8678
	step [21/206], loss=86.6693
	step [22/206], loss=84.0977
	step [23/206], loss=92.6133
	step [24/206], loss=83.4561
	step [25/206], loss=70.9674
	step [26/206], loss=71.0738
	step [27/206], loss=81.4156
	step [28/206], loss=89.5384
	step [29/206], loss=73.6320
	step [30/206], loss=92.6856
	step [31/206], loss=76.6449
	step [32/206], loss=75.8809
	step [33/206], loss=88.1655
	step [34/206], loss=92.3292
	step [35/206], loss=75.8994
	step [36/206], loss=85.5211
	step [37/206], loss=74.6174
	step [38/206], loss=86.8096
	step [39/206], loss=80.0506
	step [40/206], loss=82.7353
	step [41/206], loss=77.4110
	step [42/206], loss=87.4572
	step [43/206], loss=81.1964
	step [44/206], loss=82.4576
	step [45/206], loss=76.3729
	step [46/206], loss=78.8938
	step [47/206], loss=76.6953
	step [48/206], loss=80.4506
	step [49/206], loss=90.9650
	step [50/206], loss=82.0515
	step [51/206], loss=78.3570
	step [52/206], loss=61.6909
	step [53/206], loss=78.2791
	step [54/206], loss=76.5257
	step [55/206], loss=85.2976
	step [56/206], loss=77.2886
	step [57/206], loss=94.2288
	step [58/206], loss=85.4957
	step [59/206], loss=87.2362
	step [60/206], loss=85.5062
	step [61/206], loss=84.0503
	step [62/206], loss=85.6819
	step [63/206], loss=95.0309
	step [64/206], loss=73.0940
	step [65/206], loss=81.4481
	step [66/206], loss=79.1776
	step [67/206], loss=96.8736
	step [68/206], loss=83.2183
	step [69/206], loss=72.8596
	step [70/206], loss=88.0456
	step [71/206], loss=77.4493
	step [72/206], loss=82.0567
	step [73/206], loss=71.9318
	step [74/206], loss=75.2783
	step [75/206], loss=67.3263
	step [76/206], loss=79.3083
	step [77/206], loss=79.9508
	step [78/206], loss=89.1937
	step [79/206], loss=74.0161
	step [80/206], loss=72.7227
	step [81/206], loss=67.7997
	step [82/206], loss=65.9823
	step [83/206], loss=76.2887
	step [84/206], loss=92.8768
	step [85/206], loss=77.6645
	step [86/206], loss=70.4878
	step [87/206], loss=87.1019
	step [88/206], loss=86.5805
	step [89/206], loss=95.8579
	step [90/206], loss=72.4860
	step [91/206], loss=87.2417
	step [92/206], loss=86.2951
	step [93/206], loss=70.8162
	step [94/206], loss=87.4643
	step [95/206], loss=81.9872
	step [96/206], loss=94.8789
	step [97/206], loss=83.9925
	step [98/206], loss=78.7165
	step [99/206], loss=83.9353
	step [100/206], loss=91.9586
	step [101/206], loss=96.2440
	step [102/206], loss=80.0605
	step [103/206], loss=82.5504
	step [104/206], loss=77.1821
	step [105/206], loss=72.1772
	step [106/206], loss=72.8574
	step [107/206], loss=76.9827
	step [108/206], loss=73.9490
	step [109/206], loss=91.9889
	step [110/206], loss=76.8429
	step [111/206], loss=78.6372
	step [112/206], loss=78.8046
	step [113/206], loss=78.2025
	step [114/206], loss=72.1959
	step [115/206], loss=82.0993
	step [116/206], loss=78.6674
	step [117/206], loss=93.6312
	step [118/206], loss=81.7036
	step [119/206], loss=76.5202
	step [120/206], loss=95.4924
	step [121/206], loss=80.6362
	step [122/206], loss=74.1052
	step [123/206], loss=75.5657
	step [124/206], loss=90.3572
	step [125/206], loss=85.4204
	step [126/206], loss=82.8735
	step [127/206], loss=73.9742
	step [128/206], loss=101.9824
	step [129/206], loss=79.2124
	step [130/206], loss=78.7908
	step [131/206], loss=82.0212
	step [132/206], loss=90.0701
	step [133/206], loss=71.3305
	step [134/206], loss=77.2983
	step [135/206], loss=79.7733
	step [136/206], loss=88.8696
	step [137/206], loss=71.3777
	step [138/206], loss=75.8372
	step [139/206], loss=94.8363
	step [140/206], loss=83.7588
	step [141/206], loss=80.9480
	step [142/206], loss=88.3210
	step [143/206], loss=72.2973
	step [144/206], loss=93.9474
	step [145/206], loss=80.9971
	step [146/206], loss=78.7934
	step [147/206], loss=80.7158
	step [148/206], loss=86.7836
	step [149/206], loss=83.5774
	step [150/206], loss=69.5489
	step [151/206], loss=79.0177
	step [152/206], loss=93.3961
	step [153/206], loss=74.4253
	step [154/206], loss=87.2533
	step [155/206], loss=94.8821
	step [156/206], loss=90.2924
	step [157/206], loss=86.4030
	step [158/206], loss=72.5778
	step [159/206], loss=88.0455
	step [160/206], loss=81.4906
	step [161/206], loss=89.0992
	step [162/206], loss=91.6694
	step [163/206], loss=85.2822
	step [164/206], loss=81.4184
	step [165/206], loss=74.7423
	step [166/206], loss=62.9359
	step [167/206], loss=87.7567
	step [168/206], loss=86.6568
	step [169/206], loss=70.2272
	step [170/206], loss=75.0160
	step [171/206], loss=67.5569
	step [172/206], loss=80.0791
	step [173/206], loss=80.8098
	step [174/206], loss=82.0881
	step [175/206], loss=90.8048
	step [176/206], loss=72.8569
	step [177/206], loss=82.6775
	step [178/206], loss=80.4688
	step [179/206], loss=81.5609
	step [180/206], loss=90.0429
	step [181/206], loss=88.9481
	step [182/206], loss=88.3202
	step [183/206], loss=79.1580
	step [184/206], loss=82.3222
	step [185/206], loss=87.7711
	step [186/206], loss=76.6624
	step [187/206], loss=79.5679
	step [188/206], loss=78.4291
	step [189/206], loss=70.1810
	step [190/206], loss=86.9612
	step [191/206], loss=86.6612
	step [192/206], loss=74.9422
	step [193/206], loss=88.2459
	step [194/206], loss=83.3000
	step [195/206], loss=78.2850
	step [196/206], loss=67.7774
	step [197/206], loss=72.6362
	step [198/206], loss=79.3372
	step [199/206], loss=85.3291
	step [200/206], loss=93.3913
	step [201/206], loss=64.7716
	step [202/206], loss=94.5647
	step [203/206], loss=81.6340
	step [204/206], loss=84.1329
	step [205/206], loss=85.6353
	step [206/206], loss=51.8736
	Evaluating
	loss=0.0082, precision=0.3898, recall=0.9115, f1=0.5461
Training epoch 51
	step [1/206], loss=100.8055
	step [2/206], loss=81.0691
	step [3/206], loss=77.3939
	step [4/206], loss=92.7872
	step [5/206], loss=76.7871
	step [6/206], loss=81.8962
	step [7/206], loss=78.3051
	step [8/206], loss=86.7190
	step [9/206], loss=97.7989
	step [10/206], loss=74.6542
	step [11/206], loss=69.1366
	step [12/206], loss=74.8055
	step [13/206], loss=89.4117
	step [14/206], loss=72.9891
	step [15/206], loss=73.7789
	step [16/206], loss=79.8050
	step [17/206], loss=77.3465
	step [18/206], loss=68.1310
	step [19/206], loss=78.3586
	step [20/206], loss=87.6430
	step [21/206], loss=86.5162
	step [22/206], loss=79.5260
	step [23/206], loss=85.3197
	step [24/206], loss=74.0088
	step [25/206], loss=72.6690
	step [26/206], loss=87.8352
	step [27/206], loss=71.4268
	step [28/206], loss=70.1157
	step [29/206], loss=73.9410
	step [30/206], loss=91.4878
	step [31/206], loss=82.1554
	step [32/206], loss=77.3680
	step [33/206], loss=82.0403
	step [34/206], loss=90.1416
	step [35/206], loss=74.7396
	step [36/206], loss=78.8356
	step [37/206], loss=76.9469
	step [38/206], loss=83.1955
	step [39/206], loss=84.1615
	step [40/206], loss=74.0057
	step [41/206], loss=82.0582
	step [42/206], loss=88.8208
	step [43/206], loss=76.2755
	step [44/206], loss=78.4325
	step [45/206], loss=70.7622
	step [46/206], loss=77.2157
	step [47/206], loss=71.5405
	step [48/206], loss=74.0222
	step [49/206], loss=89.0381
	step [50/206], loss=81.9963
	step [51/206], loss=72.7951
	step [52/206], loss=74.8571
	step [53/206], loss=84.5182
	step [54/206], loss=85.0746
	step [55/206], loss=80.3836
	step [56/206], loss=63.9840
	step [57/206], loss=85.2510
	step [58/206], loss=74.1723
	step [59/206], loss=79.4460
	step [60/206], loss=86.0553
	step [61/206], loss=94.8891
	step [62/206], loss=95.5643
	step [63/206], loss=74.7830
	step [64/206], loss=98.5164
	step [65/206], loss=82.7663
	step [66/206], loss=72.4997
	step [67/206], loss=77.7668
	step [68/206], loss=97.8231
	step [69/206], loss=74.0385
	step [70/206], loss=84.2288
	step [71/206], loss=88.0103
	step [72/206], loss=72.1446
	step [73/206], loss=68.2985
	step [74/206], loss=87.0215
	step [75/206], loss=81.4372
	step [76/206], loss=81.6053
	step [77/206], loss=86.4139
	step [78/206], loss=78.5364
	step [79/206], loss=86.2649
	step [80/206], loss=86.4601
	step [81/206], loss=84.9035
	step [82/206], loss=80.6265
	step [83/206], loss=75.8659
	step [84/206], loss=90.1340
	step [85/206], loss=85.0900
	step [86/206], loss=77.9082
	step [87/206], loss=84.3886
	step [88/206], loss=77.4081
	step [89/206], loss=65.8128
	step [90/206], loss=74.7782
	step [91/206], loss=75.0385
	step [92/206], loss=75.3232
	step [93/206], loss=61.4874
	step [94/206], loss=75.0644
	step [95/206], loss=79.2811
	step [96/206], loss=88.4703
	step [97/206], loss=72.3991
	step [98/206], loss=78.5143
	step [99/206], loss=81.5867
	step [100/206], loss=79.4204
	step [101/206], loss=78.5631
	step [102/206], loss=88.2729
	step [103/206], loss=72.1582
	step [104/206], loss=79.9247
	step [105/206], loss=67.2688
	step [106/206], loss=86.7594
	step [107/206], loss=83.7531
	step [108/206], loss=80.3569
	step [109/206], loss=82.0126
	step [110/206], loss=76.1084
	step [111/206], loss=66.0866
	step [112/206], loss=72.0664
	step [113/206], loss=76.0459
	step [114/206], loss=82.8685
	step [115/206], loss=103.3785
	step [116/206], loss=80.1475
	step [117/206], loss=77.5804
	step [118/206], loss=84.9929
	step [119/206], loss=79.7809
	step [120/206], loss=83.3831
	step [121/206], loss=87.1729
	step [122/206], loss=83.2958
	step [123/206], loss=77.4598
	step [124/206], loss=79.1971
	step [125/206], loss=82.2256
	step [126/206], loss=81.8649
	step [127/206], loss=91.7392
	step [128/206], loss=83.8777
	step [129/206], loss=73.0197
	step [130/206], loss=95.1683
	step [131/206], loss=85.2973
	step [132/206], loss=74.1128
	step [133/206], loss=71.2909
	step [134/206], loss=82.6159
	step [135/206], loss=73.9618
	step [136/206], loss=82.0715
	step [137/206], loss=84.2781
	step [138/206], loss=63.3593
	step [139/206], loss=89.2638
	step [140/206], loss=75.3558
	step [141/206], loss=85.2928
	step [142/206], loss=72.8663
	step [143/206], loss=81.6443
	step [144/206], loss=78.4439
	step [145/206], loss=77.8059
	step [146/206], loss=76.0570
	step [147/206], loss=84.7524
	step [148/206], loss=79.6141
	step [149/206], loss=77.2051
	step [150/206], loss=68.2548
	step [151/206], loss=80.1655
	step [152/206], loss=69.0887
	step [153/206], loss=76.7363
	step [154/206], loss=83.2075
	step [155/206], loss=81.4206
	step [156/206], loss=85.0749
	step [157/206], loss=88.5092
	step [158/206], loss=88.6028
	step [159/206], loss=71.4696
	step [160/206], loss=92.4791
	step [161/206], loss=75.4483
	step [162/206], loss=75.6860
	step [163/206], loss=83.2382
	step [164/206], loss=70.4517
	step [165/206], loss=78.7358
	step [166/206], loss=80.7283
	step [167/206], loss=84.4352
	step [168/206], loss=93.7891
	step [169/206], loss=82.7029
	step [170/206], loss=85.6196
	step [171/206], loss=66.0067
	step [172/206], loss=85.8384
	step [173/206], loss=68.4957
	step [174/206], loss=91.2658
	step [175/206], loss=70.8265
	step [176/206], loss=84.4683
	step [177/206], loss=75.7485
	step [178/206], loss=71.7907
	step [179/206], loss=72.9702
	step [180/206], loss=83.9003
	step [181/206], loss=84.9489
	step [182/206], loss=90.1489
	step [183/206], loss=73.9289
	step [184/206], loss=85.4815
	step [185/206], loss=82.6086
	step [186/206], loss=80.1916
	step [187/206], loss=81.1260
	step [188/206], loss=82.9354
	step [189/206], loss=79.2650
	step [190/206], loss=88.2572
	step [191/206], loss=77.6667
	step [192/206], loss=94.1443
	step [193/206], loss=90.6439
	step [194/206], loss=80.3689
	step [195/206], loss=86.2650
	step [196/206], loss=68.4006
	step [197/206], loss=79.9975
	step [198/206], loss=94.3039
	step [199/206], loss=70.9382
	step [200/206], loss=83.5168
	step [201/206], loss=88.1222
	step [202/206], loss=83.6271
	step [203/206], loss=81.1583
	step [204/206], loss=85.6639
	step [205/206], loss=83.0347
	step [206/206], loss=56.0317
	Evaluating
	loss=0.0077, precision=0.4262, recall=0.9020, f1=0.5788
saving model as: 4_saved_model.pth
Training epoch 52
	step [1/206], loss=76.2991
	step [2/206], loss=86.9608
	step [3/206], loss=68.7528
	step [4/206], loss=78.0925
	step [5/206], loss=94.5406
	step [6/206], loss=68.3085
	step [7/206], loss=79.3607
	step [8/206], loss=81.3065
	step [9/206], loss=72.9911
	step [10/206], loss=73.3353
	step [11/206], loss=79.4589
	step [12/206], loss=89.0911
	step [13/206], loss=76.1247
	step [14/206], loss=85.9259
	step [15/206], loss=76.9095
	step [16/206], loss=85.6228
	step [17/206], loss=87.3257
	step [18/206], loss=77.5638
	step [19/206], loss=74.4700
	step [20/206], loss=81.3122
	step [21/206], loss=82.6586
	step [22/206], loss=77.3685
	step [23/206], loss=83.1859
	step [24/206], loss=71.0581
	step [25/206], loss=77.2169
	step [26/206], loss=78.2410
	step [27/206], loss=87.1720
	step [28/206], loss=70.1473
	step [29/206], loss=68.9627
	step [30/206], loss=73.6548
	step [31/206], loss=84.1599
	step [32/206], loss=78.7778
	step [33/206], loss=88.3372
	step [34/206], loss=73.0979
	step [35/206], loss=108.5994
	step [36/206], loss=85.4968
	step [37/206], loss=94.7750
	step [38/206], loss=75.7001
	step [39/206], loss=77.0451
	step [40/206], loss=69.6841
	step [41/206], loss=84.7251
	step [42/206], loss=72.3179
	step [43/206], loss=73.0824
	step [44/206], loss=72.2714
	step [45/206], loss=92.3316
	step [46/206], loss=68.3160
	step [47/206], loss=65.7002
	step [48/206], loss=70.6844
	step [49/206], loss=77.0671
	step [50/206], loss=74.0247
	step [51/206], loss=85.2968
	step [52/206], loss=87.3303
	step [53/206], loss=66.8366
	step [54/206], loss=90.6548
	step [55/206], loss=73.6784
	step [56/206], loss=72.9932
	step [57/206], loss=78.5202
	step [58/206], loss=82.1190
	step [59/206], loss=79.5784
	step [60/206], loss=84.6595
	step [61/206], loss=75.0825
	step [62/206], loss=82.3299
	step [63/206], loss=78.2774
	step [64/206], loss=79.6443
	step [65/206], loss=78.3029
	step [66/206], loss=89.4562
	step [67/206], loss=81.2133
	step [68/206], loss=82.5374
	step [69/206], loss=73.2738
	step [70/206], loss=81.6428
	step [71/206], loss=100.3336
	step [72/206], loss=75.9500
	step [73/206], loss=83.1225
	step [74/206], loss=76.7077
	step [75/206], loss=71.4555
	step [76/206], loss=95.7029
	step [77/206], loss=89.3177
	step [78/206], loss=82.0837
	step [79/206], loss=88.8038
	step [80/206], loss=79.0132
	step [81/206], loss=69.1320
	step [82/206], loss=92.0677
	step [83/206], loss=82.6996
	step [84/206], loss=70.6887
	step [85/206], loss=76.1764
	step [86/206], loss=83.0424
	step [87/206], loss=92.0323
	step [88/206], loss=72.2683
	step [89/206], loss=78.1735
	step [90/206], loss=77.7719
	step [91/206], loss=85.0131
	step [92/206], loss=95.6561
	step [93/206], loss=70.1161
	step [94/206], loss=66.6841
	step [95/206], loss=76.4199
	step [96/206], loss=74.9861
	step [97/206], loss=76.3178
	step [98/206], loss=92.8893
	step [99/206], loss=89.2384
	step [100/206], loss=78.9525
	step [101/206], loss=80.0763
	step [102/206], loss=77.5429
	step [103/206], loss=77.5346
	step [104/206], loss=66.6190
	step [105/206], loss=102.7309
	step [106/206], loss=72.9543
	step [107/206], loss=84.5532
	step [108/206], loss=83.5436
	step [109/206], loss=79.1164
	step [110/206], loss=70.7635
	step [111/206], loss=69.3335
	step [112/206], loss=76.0962
	step [113/206], loss=74.2124
	step [114/206], loss=83.0904
	step [115/206], loss=83.8627
	step [116/206], loss=82.6228
	step [117/206], loss=79.6528
	step [118/206], loss=85.1691
	step [119/206], loss=84.7341
	step [120/206], loss=79.7106
	step [121/206], loss=96.4188
	step [122/206], loss=76.2033
	step [123/206], loss=84.3432
	step [124/206], loss=75.9723
	step [125/206], loss=83.1657
	step [126/206], loss=79.4195
	step [127/206], loss=88.8217
	step [128/206], loss=73.5580
	step [129/206], loss=85.5266
	step [130/206], loss=73.9897
	step [131/206], loss=89.2517
	step [132/206], loss=82.0301
	step [133/206], loss=83.4144
	step [134/206], loss=73.5822
	step [135/206], loss=78.2422
	step [136/206], loss=83.7003
	step [137/206], loss=92.3547
	step [138/206], loss=80.5256
	step [139/206], loss=75.5296
	step [140/206], loss=78.6158
	step [141/206], loss=97.1859
	step [142/206], loss=69.5878
	step [143/206], loss=75.6285
	step [144/206], loss=78.1725
	step [145/206], loss=83.4101
	step [146/206], loss=74.5479
	step [147/206], loss=88.4319
	step [148/206], loss=73.7837
	step [149/206], loss=90.1290
	step [150/206], loss=72.5034
	step [151/206], loss=78.3119
	step [152/206], loss=94.3825
	step [153/206], loss=88.3788
	step [154/206], loss=75.7662
	step [155/206], loss=77.9059
	step [156/206], loss=87.6562
	step [157/206], loss=93.5446
	step [158/206], loss=71.5584
	step [159/206], loss=83.3440
	step [160/206], loss=86.2883
	step [161/206], loss=78.2475
	step [162/206], loss=83.1228
	step [163/206], loss=92.8922
	step [164/206], loss=81.5471
	step [165/206], loss=74.9104
	step [166/206], loss=90.5911
	step [167/206], loss=74.4971
	step [168/206], loss=92.2934
	step [169/206], loss=87.1986
	step [170/206], loss=70.3472
	step [171/206], loss=94.1213
	step [172/206], loss=84.3681
	step [173/206], loss=79.6552
	step [174/206], loss=87.1861
	step [175/206], loss=77.9732
	step [176/206], loss=83.2906
	step [177/206], loss=81.8969
	step [178/206], loss=73.4503
	step [179/206], loss=79.4522
	step [180/206], loss=84.4503
	step [181/206], loss=85.0280
	step [182/206], loss=73.3280
	step [183/206], loss=88.1611
	step [184/206], loss=73.3567
	step [185/206], loss=72.9106
	step [186/206], loss=90.7219
	step [187/206], loss=79.7551
	step [188/206], loss=74.4863
	step [189/206], loss=66.4853
	step [190/206], loss=107.2012
	step [191/206], loss=78.2699
	step [192/206], loss=74.2545
	step [193/206], loss=66.0967
	step [194/206], loss=74.0232
	step [195/206], loss=70.9344
	step [196/206], loss=97.2243
	step [197/206], loss=72.5916
	step [198/206], loss=74.4105
	step [199/206], loss=95.7473
	step [200/206], loss=80.6659
	step [201/206], loss=74.6164
	step [202/206], loss=79.5543
	step [203/206], loss=75.7963
	step [204/206], loss=89.3908
	step [205/206], loss=85.6648
	step [206/206], loss=50.1529
	Evaluating
	loss=0.0086, precision=0.3754, recall=0.9095, f1=0.5315
Training epoch 53
	step [1/206], loss=97.0963
	step [2/206], loss=76.6926
	step [3/206], loss=82.6409
	step [4/206], loss=91.9386
	step [5/206], loss=78.2558
	step [6/206], loss=77.9234
	step [7/206], loss=71.5020
	step [8/206], loss=84.0974
	step [9/206], loss=93.1230
	step [10/206], loss=84.8983
	step [11/206], loss=98.3170
	step [12/206], loss=81.4614
	step [13/206], loss=93.3709
	step [14/206], loss=69.5188
	step [15/206], loss=76.6516
	step [16/206], loss=73.6909
	step [17/206], loss=81.1356
	step [18/206], loss=73.8758
	step [19/206], loss=83.3634
	step [20/206], loss=81.2694
	step [21/206], loss=77.2871
	step [22/206], loss=83.5359
	step [23/206], loss=88.6471
	step [24/206], loss=85.9500
	step [25/206], loss=87.6576
	step [26/206], loss=76.9857
	step [27/206], loss=95.3015
	step [28/206], loss=76.3213
	step [29/206], loss=85.3407
	step [30/206], loss=77.8680
	step [31/206], loss=85.5976
	step [32/206], loss=84.6770
	step [33/206], loss=78.4840
	step [34/206], loss=69.8700
	step [35/206], loss=75.4072
	step [36/206], loss=60.0944
	step [37/206], loss=76.8933
	step [38/206], loss=80.5313
	step [39/206], loss=98.1517
	step [40/206], loss=82.5518
	step [41/206], loss=78.5895
	step [42/206], loss=100.5337
	step [43/206], loss=67.5225
	step [44/206], loss=86.8295
	step [45/206], loss=79.8313
	step [46/206], loss=101.0428
	step [47/206], loss=76.0109
	step [48/206], loss=71.0907
	step [49/206], loss=69.5879
	step [50/206], loss=93.4072
	step [51/206], loss=69.6241
	step [52/206], loss=86.7188
	step [53/206], loss=77.0456
	step [54/206], loss=86.2567
	step [55/206], loss=66.6537
	step [56/206], loss=88.0633
	step [57/206], loss=82.7930
	step [58/206], loss=83.9158
	step [59/206], loss=72.1339
	step [60/206], loss=72.9551
	step [61/206], loss=92.6751
	step [62/206], loss=79.0933
	step [63/206], loss=92.1753
	step [64/206], loss=91.4548
	step [65/206], loss=68.3399
	step [66/206], loss=72.1386
	step [67/206], loss=78.5286
	step [68/206], loss=98.4211
	step [69/206], loss=69.5510
	step [70/206], loss=71.6093
	step [71/206], loss=70.9633
	step [72/206], loss=69.9888
	step [73/206], loss=79.6517
	step [74/206], loss=57.2396
	step [75/206], loss=72.8706
	step [76/206], loss=91.4816
	step [77/206], loss=86.7934
	step [78/206], loss=74.9915
	step [79/206], loss=77.0012
	step [80/206], loss=81.0101
	step [81/206], loss=73.5419
	step [82/206], loss=86.2743
	step [83/206], loss=69.1838
	step [84/206], loss=84.4773
	step [85/206], loss=67.3793
	step [86/206], loss=97.6584
	step [87/206], loss=79.7103
	step [88/206], loss=69.0247
	step [89/206], loss=74.4818
	step [90/206], loss=84.0028
	step [91/206], loss=81.1382
	step [92/206], loss=77.8434
	step [93/206], loss=74.4030
	step [94/206], loss=77.5170
	step [95/206], loss=79.3533
	step [96/206], loss=87.3849
	step [97/206], loss=88.2237
	step [98/206], loss=88.1026
	step [99/206], loss=83.8947
	step [100/206], loss=81.7890
	step [101/206], loss=76.1095
	step [102/206], loss=87.0455
	step [103/206], loss=86.1305
	step [104/206], loss=74.3511
	step [105/206], loss=69.2355
	step [106/206], loss=71.9833
	step [107/206], loss=81.0128
	step [108/206], loss=81.2392
	step [109/206], loss=83.8086
	step [110/206], loss=85.9544
	step [111/206], loss=80.2364
	step [112/206], loss=78.7493
	step [113/206], loss=81.6924
	step [114/206], loss=77.7277
	step [115/206], loss=64.5259
	step [116/206], loss=89.5362
	step [117/206], loss=79.3517
	step [118/206], loss=81.7053
	step [119/206], loss=88.6706
	step [120/206], loss=74.0724
	step [121/206], loss=73.8298
	step [122/206], loss=90.3398
	step [123/206], loss=92.3631
	step [124/206], loss=78.9968
	step [125/206], loss=87.1102
	step [126/206], loss=70.6584
	step [127/206], loss=87.1966
	step [128/206], loss=94.9680
	step [129/206], loss=76.8302
	step [130/206], loss=64.9676
	step [131/206], loss=91.9338
	step [132/206], loss=76.7938
	step [133/206], loss=79.4552
	step [134/206], loss=71.5335
	step [135/206], loss=78.4534
	step [136/206], loss=71.6654
	step [137/206], loss=71.6074
	step [138/206], loss=72.5929
	step [139/206], loss=73.9840
	step [140/206], loss=72.7749
	step [141/206], loss=91.4406
	step [142/206], loss=78.7119
	step [143/206], loss=74.6970
	step [144/206], loss=82.9244
	step [145/206], loss=75.2777
	step [146/206], loss=86.4566
	step [147/206], loss=86.7454
	step [148/206], loss=77.6119
	step [149/206], loss=80.4445
	step [150/206], loss=85.7200
	step [151/206], loss=80.9289
	step [152/206], loss=72.6496
	step [153/206], loss=80.8365
	step [154/206], loss=85.9490
	step [155/206], loss=75.9282
	step [156/206], loss=68.5205
	step [157/206], loss=78.9835
	step [158/206], loss=80.7065
	step [159/206], loss=89.4780
	step [160/206], loss=81.4846
	step [161/206], loss=75.1632
	step [162/206], loss=75.6730
	step [163/206], loss=80.3543
	step [164/206], loss=72.1346
	step [165/206], loss=66.6830
	step [166/206], loss=86.7316
	step [167/206], loss=73.1769
	step [168/206], loss=79.5240
	step [169/206], loss=79.8742
	step [170/206], loss=83.1636
	step [171/206], loss=81.6836
	step [172/206], loss=93.7545
	step [173/206], loss=75.1747
	step [174/206], loss=101.5775
	step [175/206], loss=76.7084
	step [176/206], loss=86.6607
	step [177/206], loss=85.9002
	step [178/206], loss=79.5237
	step [179/206], loss=88.7704
	step [180/206], loss=100.9191
	step [181/206], loss=81.2333
	step [182/206], loss=83.5452
	step [183/206], loss=71.0194
	step [184/206], loss=92.8838
	step [185/206], loss=76.2898
	step [186/206], loss=67.0380
	step [187/206], loss=79.8598
	step [188/206], loss=70.4708
	step [189/206], loss=85.1784
	step [190/206], loss=71.8557
	step [191/206], loss=88.3087
	step [192/206], loss=66.3663
	step [193/206], loss=80.8918
	step [194/206], loss=77.8081
	step [195/206], loss=76.1438
	step [196/206], loss=84.0161
	step [197/206], loss=91.7049
	step [198/206], loss=89.6185
	step [199/206], loss=86.6102
	step [200/206], loss=80.6307
	step [201/206], loss=83.7233
	step [202/206], loss=76.9250
	step [203/206], loss=78.2348
	step [204/206], loss=77.8366
	step [205/206], loss=92.8139
	step [206/206], loss=57.4103
	Evaluating
	loss=0.0085, precision=0.3872, recall=0.9182, f1=0.5447
Training epoch 54
	step [1/206], loss=70.8734
	step [2/206], loss=73.2317
	step [3/206], loss=89.1013
	step [4/206], loss=72.8814
	step [5/206], loss=84.4145
	step [6/206], loss=74.1475
	step [7/206], loss=81.4012
	step [8/206], loss=83.7219
	step [9/206], loss=107.1209
	step [10/206], loss=94.6688
	step [11/206], loss=87.3549
	step [12/206], loss=98.2382
	step [13/206], loss=82.7670
	step [14/206], loss=79.8775
	step [15/206], loss=72.3065
	step [16/206], loss=66.9944
	step [17/206], loss=92.3233
	step [18/206], loss=70.4252
	step [19/206], loss=82.4631
	step [20/206], loss=82.8960
	step [21/206], loss=80.5866
	step [22/206], loss=89.7122
	step [23/206], loss=78.1654
	step [24/206], loss=93.0332
	step [25/206], loss=69.1536
	step [26/206], loss=84.5238
	step [27/206], loss=81.7637
	step [28/206], loss=73.3162
	step [29/206], loss=75.4302
	step [30/206], loss=81.9076
	step [31/206], loss=98.4281
	step [32/206], loss=81.0225
	step [33/206], loss=64.4472
	step [34/206], loss=75.1774
	step [35/206], loss=74.4906
	step [36/206], loss=80.5789
	step [37/206], loss=86.4017
	step [38/206], loss=96.4129
	step [39/206], loss=91.7058
	step [40/206], loss=72.4361
	step [41/206], loss=86.7894
	step [42/206], loss=92.9821
	step [43/206], loss=87.9053
	step [44/206], loss=65.1741
	step [45/206], loss=80.5509
	step [46/206], loss=80.0378
	step [47/206], loss=69.9434
	step [48/206], loss=78.9181
	step [49/206], loss=82.1749
	step [50/206], loss=70.6630
	step [51/206], loss=85.7418
	step [52/206], loss=61.9558
	step [53/206], loss=84.3946
	step [54/206], loss=92.8156
	step [55/206], loss=66.9165
	step [56/206], loss=82.2542
	step [57/206], loss=78.7435
	step [58/206], loss=87.2046
	step [59/206], loss=78.5970
	step [60/206], loss=73.3864
	step [61/206], loss=65.2028
	step [62/206], loss=85.4667
	step [63/206], loss=90.3261
	step [64/206], loss=73.6572
	step [65/206], loss=70.4027
	step [66/206], loss=89.0057
	step [67/206], loss=74.7525
	step [68/206], loss=86.9909
	step [69/206], loss=72.4864
	step [70/206], loss=69.4346
	step [71/206], loss=88.1829
	step [72/206], loss=88.3002
	step [73/206], loss=79.7320
	step [74/206], loss=88.9647
	step [75/206], loss=75.2947
	step [76/206], loss=87.1690
	step [77/206], loss=61.1987
	step [78/206], loss=82.3734
	step [79/206], loss=76.8863
	step [80/206], loss=76.1962
	step [81/206], loss=58.6355
	step [82/206], loss=74.2846
	step [83/206], loss=56.9073
	step [84/206], loss=79.5528
	step [85/206], loss=71.9408
	step [86/206], loss=79.5340
	step [87/206], loss=79.8563
	step [88/206], loss=84.6597
	step [89/206], loss=72.6313
	step [90/206], loss=73.3915
	step [91/206], loss=63.9139
	step [92/206], loss=90.6471
	step [93/206], loss=84.7160
	step [94/206], loss=64.7256
	step [95/206], loss=88.7698
	step [96/206], loss=91.0338
	step [97/206], loss=94.1647
	step [98/206], loss=73.0488
	step [99/206], loss=91.4908
	step [100/206], loss=89.9723
	step [101/206], loss=77.9301
	step [102/206], loss=74.9586
	step [103/206], loss=86.1777
	step [104/206], loss=65.5026
	step [105/206], loss=75.3374
	step [106/206], loss=85.6827
	step [107/206], loss=72.7601
	step [108/206], loss=74.5682
	step [109/206], loss=86.8394
	step [110/206], loss=69.5483
	step [111/206], loss=68.1686
	step [112/206], loss=73.8939
	step [113/206], loss=89.6131
	step [114/206], loss=79.3082
	step [115/206], loss=78.4292
	step [116/206], loss=81.2943
	step [117/206], loss=71.6716
	step [118/206], loss=60.3891
	step [119/206], loss=84.7803
	step [120/206], loss=85.9956
	step [121/206], loss=78.6581
	step [122/206], loss=77.2071
	step [123/206], loss=74.9693
	step [124/206], loss=87.7731
	step [125/206], loss=73.3548
	step [126/206], loss=86.6540
	step [127/206], loss=87.5023
	step [128/206], loss=70.7767
	step [129/206], loss=92.2929
	step [130/206], loss=80.2281
	step [131/206], loss=86.1387
	step [132/206], loss=72.6434
	step [133/206], loss=80.5269
	step [134/206], loss=76.7612
	step [135/206], loss=94.3916
	step [136/206], loss=74.6174
	step [137/206], loss=81.1107
	step [138/206], loss=73.9421
	step [139/206], loss=89.9530
	step [140/206], loss=87.3100
	step [141/206], loss=82.8030
	step [142/206], loss=79.8517
	step [143/206], loss=84.1442
	step [144/206], loss=72.5011
	step [145/206], loss=83.7520
	step [146/206], loss=83.0805
	step [147/206], loss=91.1353
	step [148/206], loss=89.8341
	step [149/206], loss=73.2179
	step [150/206], loss=92.4039
	step [151/206], loss=72.9045
	step [152/206], loss=71.8936
	step [153/206], loss=81.6188
	step [154/206], loss=94.4799
	step [155/206], loss=83.6059
	step [156/206], loss=77.4432
	step [157/206], loss=88.2892
	step [158/206], loss=81.0648
	step [159/206], loss=74.0340
	step [160/206], loss=76.5705
	step [161/206], loss=67.2869
	step [162/206], loss=70.6163
	step [163/206], loss=84.3765
	step [164/206], loss=63.8310
	step [165/206], loss=80.2989
	step [166/206], loss=62.6683
	step [167/206], loss=80.6327
	step [168/206], loss=73.8605
	step [169/206], loss=66.3919
	step [170/206], loss=84.0426
	step [171/206], loss=83.1203
	step [172/206], loss=77.7641
	step [173/206], loss=81.3851
	step [174/206], loss=82.8603
	step [175/206], loss=76.9685
	step [176/206], loss=76.5546
	step [177/206], loss=84.1110
	step [178/206], loss=95.1025
	step [179/206], loss=81.9650
	step [180/206], loss=71.0943
	step [181/206], loss=75.8595
	step [182/206], loss=61.3089
	step [183/206], loss=99.5645
	step [184/206], loss=77.0826
	step [185/206], loss=80.8439
	step [186/206], loss=87.5191
	step [187/206], loss=80.6515
	step [188/206], loss=74.5560
	step [189/206], loss=68.7395
	step [190/206], loss=90.8201
	step [191/206], loss=77.3885
	step [192/206], loss=86.2012
	step [193/206], loss=75.7420
	step [194/206], loss=69.5354
	step [195/206], loss=79.6214
	step [196/206], loss=92.5806
	step [197/206], loss=82.6612
	step [198/206], loss=87.4124
	step [199/206], loss=95.0991
	step [200/206], loss=80.9325
	step [201/206], loss=68.6230
	step [202/206], loss=83.2028
	step [203/206], loss=75.9666
	step [204/206], loss=86.2369
	step [205/206], loss=72.5291
	step [206/206], loss=50.7631
	Evaluating
	loss=0.0078, precision=0.4083, recall=0.9036, f1=0.5624
Training epoch 55
	step [1/206], loss=82.9073
	step [2/206], loss=79.1874
	step [3/206], loss=86.7983
	step [4/206], loss=69.2298
	step [5/206], loss=83.7479
	step [6/206], loss=74.1377
	step [7/206], loss=75.0765
	step [8/206], loss=72.0783
	step [9/206], loss=74.6937
	step [10/206], loss=87.3623
	step [11/206], loss=87.3400
	step [12/206], loss=71.8885
	step [13/206], loss=97.3251
	step [14/206], loss=73.6679
	step [15/206], loss=73.3099
	step [16/206], loss=79.6472
	step [17/206], loss=77.5954
	step [18/206], loss=80.3482
	step [19/206], loss=79.4969
	step [20/206], loss=79.2569
	step [21/206], loss=87.6265
	step [22/206], loss=84.4438
	step [23/206], loss=83.1248
	step [24/206], loss=81.6461
	step [25/206], loss=81.8226
	step [26/206], loss=80.2278
	step [27/206], loss=79.5358
	step [28/206], loss=71.1613
	step [29/206], loss=77.6354
	step [30/206], loss=79.4271
	step [31/206], loss=89.5242
	step [32/206], loss=71.2936
	step [33/206], loss=83.5219
	step [34/206], loss=86.6931
	step [35/206], loss=71.9477
	step [36/206], loss=67.4977
	step [37/206], loss=87.3804
	step [38/206], loss=82.2640
	step [39/206], loss=66.9647
	step [40/206], loss=67.5926
	step [41/206], loss=80.0386
	step [42/206], loss=75.7144
	step [43/206], loss=81.4682
	step [44/206], loss=69.7511
	step [45/206], loss=62.4569
	step [46/206], loss=85.2406
	step [47/206], loss=74.3123
	step [48/206], loss=90.2020
	step [49/206], loss=62.9029
	step [50/206], loss=77.8297
	step [51/206], loss=97.4240
	step [52/206], loss=72.6588
	step [53/206], loss=79.5727
	step [54/206], loss=81.1345
	step [55/206], loss=86.4538
	step [56/206], loss=79.6751
	step [57/206], loss=89.7400
	step [58/206], loss=80.2220
	step [59/206], loss=71.4774
	step [60/206], loss=56.5589
	step [61/206], loss=91.4376
	step [62/206], loss=83.2982
	step [63/206], loss=87.5179
	step [64/206], loss=83.3280
	step [65/206], loss=72.1492
	step [66/206], loss=84.8436
	step [67/206], loss=84.5825
	step [68/206], loss=80.7458
	step [69/206], loss=76.8394
	step [70/206], loss=77.6419
	step [71/206], loss=85.7239
	step [72/206], loss=68.7952
	step [73/206], loss=77.0173
	step [74/206], loss=81.1372
	step [75/206], loss=82.2627
	step [76/206], loss=84.0025
	step [77/206], loss=90.0903
	step [78/206], loss=70.4412
	step [79/206], loss=69.7420
	step [80/206], loss=66.5647
	step [81/206], loss=74.1625
	step [82/206], loss=76.4582
	step [83/206], loss=75.3715
	step [84/206], loss=82.6168
	step [85/206], loss=76.1207
	step [86/206], loss=89.6338
	step [87/206], loss=79.0352
	step [88/206], loss=76.6190
	step [89/206], loss=78.2144
	step [90/206], loss=76.9471
	step [91/206], loss=83.8653
	step [92/206], loss=60.5026
	step [93/206], loss=60.3585
	step [94/206], loss=92.4487
	step [95/206], loss=77.7824
	step [96/206], loss=73.0786
	step [97/206], loss=70.1733
	step [98/206], loss=80.8438
	step [99/206], loss=85.6861
	step [100/206], loss=78.9217
	step [101/206], loss=66.3455
	step [102/206], loss=72.2323
	step [103/206], loss=78.9589
	step [104/206], loss=78.7133
	step [105/206], loss=82.8689
	step [106/206], loss=71.0900
	step [107/206], loss=76.5193
	step [108/206], loss=72.7389
	step [109/206], loss=107.1313
	step [110/206], loss=79.4478
	step [111/206], loss=84.9052
	step [112/206], loss=80.6934
	step [113/206], loss=84.4464
	step [114/206], loss=62.8624
	step [115/206], loss=77.4998
	step [116/206], loss=77.2906
	step [117/206], loss=78.8998
	step [118/206], loss=90.0955
	step [119/206], loss=70.1529
	step [120/206], loss=76.4403
	step [121/206], loss=87.0121
	step [122/206], loss=79.6377
	step [123/206], loss=88.5368
	step [124/206], loss=85.1714
	step [125/206], loss=85.7062
	step [126/206], loss=80.1415
	step [127/206], loss=81.7655
	step [128/206], loss=83.9161
	step [129/206], loss=79.6290
	step [130/206], loss=70.3294
	step [131/206], loss=73.8774
	step [132/206], loss=70.1751
	step [133/206], loss=65.0255
	step [134/206], loss=75.0594
	step [135/206], loss=98.2894
	step [136/206], loss=82.9654
	step [137/206], loss=92.8143
	step [138/206], loss=73.9912
	step [139/206], loss=85.2320
	step [140/206], loss=93.1816
	step [141/206], loss=78.7587
	step [142/206], loss=80.6171
	step [143/206], loss=78.9971
	step [144/206], loss=70.2805
	step [145/206], loss=73.7992
	step [146/206], loss=81.9773
	step [147/206], loss=88.2561
	step [148/206], loss=71.7227
	step [149/206], loss=92.1197
	step [150/206], loss=83.5163
	step [151/206], loss=73.1396
	step [152/206], loss=81.8648
	step [153/206], loss=76.5153
	step [154/206], loss=77.4448
	step [155/206], loss=82.1165
	step [156/206], loss=89.8625
	step [157/206], loss=92.2001
	step [158/206], loss=75.1434
	step [159/206], loss=79.0101
	step [160/206], loss=70.2810
	step [161/206], loss=81.8286
	step [162/206], loss=60.1527
	step [163/206], loss=94.4212
	step [164/206], loss=89.2596
	step [165/206], loss=78.8979
	step [166/206], loss=69.2330
	step [167/206], loss=88.9551
	step [168/206], loss=86.4502
	step [169/206], loss=73.6022
	step [170/206], loss=78.0700
	step [171/206], loss=77.4725
	step [172/206], loss=92.8065
	step [173/206], loss=65.0176
	step [174/206], loss=69.6307
	step [175/206], loss=92.8606
	step [176/206], loss=68.0991
	step [177/206], loss=76.9568
	step [178/206], loss=64.4314
	step [179/206], loss=73.3661
	step [180/206], loss=89.0566
	step [181/206], loss=73.9344
	step [182/206], loss=87.0117
	step [183/206], loss=72.5485
	step [184/206], loss=75.4879
	step [185/206], loss=73.0440
	step [186/206], loss=82.0477
	step [187/206], loss=84.0412
	step [188/206], loss=96.4843
	step [189/206], loss=77.0376
	step [190/206], loss=82.2188
	step [191/206], loss=88.7612
	step [192/206], loss=77.5258
	step [193/206], loss=87.6653
	step [194/206], loss=78.1030
	step [195/206], loss=71.2792
	step [196/206], loss=85.2633
	step [197/206], loss=75.0855
	step [198/206], loss=83.5833
	step [199/206], loss=75.7973
	step [200/206], loss=88.1599
	step [201/206], loss=74.4165
	step [202/206], loss=93.2426
	step [203/206], loss=81.9778
	step [204/206], loss=81.0284
	step [205/206], loss=81.8678
	step [206/206], loss=52.8873
	Evaluating
	loss=0.0090, precision=0.3404, recall=0.9062, f1=0.4949
Training epoch 56
	step [1/206], loss=81.0512
	step [2/206], loss=81.9657
	step [3/206], loss=84.5610
	step [4/206], loss=85.3616
	step [5/206], loss=74.9358
	step [6/206], loss=86.8129
	step [7/206], loss=86.5752
	step [8/206], loss=81.3030
	step [9/206], loss=81.9841
	step [10/206], loss=74.2857
	step [11/206], loss=84.5994
	step [12/206], loss=73.3812
	step [13/206], loss=68.2359
	step [14/206], loss=73.0371
	step [15/206], loss=92.0839
	step [16/206], loss=81.7159
	step [17/206], loss=80.3259
	step [18/206], loss=88.8504
	step [19/206], loss=83.1052
	step [20/206], loss=70.2797
	step [21/206], loss=71.5591
	step [22/206], loss=82.8826
	step [23/206], loss=68.6770
	step [24/206], loss=76.5662
	step [25/206], loss=74.9087
	step [26/206], loss=60.6973
	step [27/206], loss=87.5841
	step [28/206], loss=82.5195
	step [29/206], loss=79.1185
	step [30/206], loss=71.5125
	step [31/206], loss=76.1054
	step [32/206], loss=75.8266
	step [33/206], loss=62.3580
	step [34/206], loss=75.6253
	step [35/206], loss=80.8767
	step [36/206], loss=83.3701
	step [37/206], loss=76.8170
	step [38/206], loss=74.1996
	step [39/206], loss=87.0390
	step [40/206], loss=75.9980
	step [41/206], loss=74.4846
	step [42/206], loss=80.2809
	step [43/206], loss=81.5111
	step [44/206], loss=71.7139
	step [45/206], loss=68.4405
	step [46/206], loss=85.0106
	step [47/206], loss=73.4344
	step [48/206], loss=86.7957
	step [49/206], loss=88.9495
	step [50/206], loss=74.4428
	step [51/206], loss=68.3671
	step [52/206], loss=76.1612
	step [53/206], loss=68.6777
	step [54/206], loss=89.8885
	step [55/206], loss=74.7401
	step [56/206], loss=85.2470
	step [57/206], loss=69.5224
	step [58/206], loss=69.9419
	step [59/206], loss=79.8029
	step [60/206], loss=81.6605
	step [61/206], loss=65.8039
	step [62/206], loss=83.7936
	step [63/206], loss=74.9123
	step [64/206], loss=80.1026
	step [65/206], loss=81.4370
	step [66/206], loss=80.5843
	step [67/206], loss=81.4775
	step [68/206], loss=81.7337
	step [69/206], loss=89.0412
	step [70/206], loss=90.7261
	step [71/206], loss=74.9647
	step [72/206], loss=76.4546
	step [73/206], loss=94.6574
	step [74/206], loss=65.0409
	step [75/206], loss=67.5185
	step [76/206], loss=88.0390
	step [77/206], loss=65.0476
	step [78/206], loss=87.9392
	step [79/206], loss=71.2370
	step [80/206], loss=70.9276
	step [81/206], loss=70.4433
	step [82/206], loss=73.1072
	step [83/206], loss=84.0498
	step [84/206], loss=61.4046
	step [85/206], loss=79.5859
	step [86/206], loss=94.2022
	step [87/206], loss=78.8868
	step [88/206], loss=78.1645
	step [89/206], loss=75.2297
	step [90/206], loss=78.9189
	step [91/206], loss=78.1105
	step [92/206], loss=81.7462
	step [93/206], loss=84.7671
	step [94/206], loss=72.4236
	step [95/206], loss=75.6646
	step [96/206], loss=83.2781
	step [97/206], loss=57.7372
	step [98/206], loss=77.5069
	step [99/206], loss=72.2405
	step [100/206], loss=81.5433
	step [101/206], loss=72.9367
	step [102/206], loss=86.0738
	step [103/206], loss=67.5645
	step [104/206], loss=71.5959
	step [105/206], loss=73.8410
	step [106/206], loss=87.6018
	step [107/206], loss=81.6873
	step [108/206], loss=80.3582
	step [109/206], loss=81.9498
	step [110/206], loss=75.3405
	step [111/206], loss=79.4990
	step [112/206], loss=93.1538
	step [113/206], loss=88.3016
	step [114/206], loss=82.7630
	step [115/206], loss=82.4880
	step [116/206], loss=77.9339
	step [117/206], loss=80.1469
	step [118/206], loss=71.5762
	step [119/206], loss=83.7085
	step [120/206], loss=84.0273
	step [121/206], loss=91.1031
	step [122/206], loss=78.6134
	step [123/206], loss=74.4434
	step [124/206], loss=84.2666
	step [125/206], loss=86.1956
	step [126/206], loss=76.4749
	step [127/206], loss=74.7678
	step [128/206], loss=83.3648
	step [129/206], loss=78.0758
	step [130/206], loss=75.4371
	step [131/206], loss=67.1663
	step [132/206], loss=63.1978
	step [133/206], loss=77.8576
	step [134/206], loss=76.7980
	step [135/206], loss=77.0212
	step [136/206], loss=79.0967
	step [137/206], loss=77.1008
	step [138/206], loss=88.3437
	step [139/206], loss=69.1239
	step [140/206], loss=74.1304
	step [141/206], loss=72.9973
	step [142/206], loss=91.9481
	step [143/206], loss=86.1671
	step [144/206], loss=77.0691
	step [145/206], loss=74.4160
	step [146/206], loss=79.4578
	step [147/206], loss=80.9550
	step [148/206], loss=92.1082
	step [149/206], loss=79.8651
	step [150/206], loss=80.2751
	step [151/206], loss=86.0904
	step [152/206], loss=86.0994
	step [153/206], loss=78.4941
	step [154/206], loss=78.8587
	step [155/206], loss=61.2659
	step [156/206], loss=77.4667
	step [157/206], loss=87.4542
	step [158/206], loss=79.0344
	step [159/206], loss=70.9760
	step [160/206], loss=78.4194
	step [161/206], loss=79.2323
	step [162/206], loss=81.8341
	step [163/206], loss=76.8002
	step [164/206], loss=66.0497
	step [165/206], loss=72.9837
	step [166/206], loss=72.4741
	step [167/206], loss=81.0701
	step [168/206], loss=87.1915
	step [169/206], loss=82.3373
	step [170/206], loss=81.1774
	step [171/206], loss=79.6299
	step [172/206], loss=79.1302
	step [173/206], loss=74.7915
	step [174/206], loss=90.8890
	step [175/206], loss=65.6220
	step [176/206], loss=78.4896
	step [177/206], loss=74.7311
	step [178/206], loss=83.4017
	step [179/206], loss=62.8708
	step [180/206], loss=78.9188
	step [181/206], loss=87.6324
	step [182/206], loss=83.9437
	step [183/206], loss=78.9361
	step [184/206], loss=78.1369
	step [185/206], loss=86.4592
	step [186/206], loss=71.6440
	step [187/206], loss=68.0280
	step [188/206], loss=96.9063
	step [189/206], loss=83.3069
	step [190/206], loss=70.1730
	step [191/206], loss=84.5841
	step [192/206], loss=93.7061
	step [193/206], loss=86.0794
	step [194/206], loss=86.8647
	step [195/206], loss=90.2186
	step [196/206], loss=77.2705
	step [197/206], loss=69.7707
	step [198/206], loss=77.0150
	step [199/206], loss=79.9026
	step [200/206], loss=102.7589
	step [201/206], loss=76.7134
	step [202/206], loss=100.3037
	step [203/206], loss=70.7793
	step [204/206], loss=84.6637
	step [205/206], loss=86.5416
	step [206/206], loss=62.5552
	Evaluating
	loss=0.0075, precision=0.4083, recall=0.9101, f1=0.5637
Training epoch 57
	step [1/206], loss=85.6123
	step [2/206], loss=64.9720
	step [3/206], loss=83.4066
	step [4/206], loss=86.0953
	step [5/206], loss=81.2296
	step [6/206], loss=80.8328
	step [7/206], loss=95.3886
	step [8/206], loss=82.8201
	step [9/206], loss=88.0886
	step [10/206], loss=85.1189
	step [11/206], loss=82.1694
	step [12/206], loss=64.8875
	step [13/206], loss=74.3272
	step [14/206], loss=86.1901
	step [15/206], loss=92.6690
	step [16/206], loss=89.4181
	step [17/206], loss=77.6828
	step [18/206], loss=72.8785
	step [19/206], loss=87.8009
	step [20/206], loss=81.7750
	step [21/206], loss=77.6565
	step [22/206], loss=84.5061
	step [23/206], loss=87.0385
	step [24/206], loss=73.3630
	step [25/206], loss=84.3702
	step [26/206], loss=82.4755
	step [27/206], loss=83.8818
	step [28/206], loss=77.1105
	step [29/206], loss=68.1752
	step [30/206], loss=87.9126
	step [31/206], loss=56.8527
	step [32/206], loss=81.8941
	step [33/206], loss=85.9546
	step [34/206], loss=83.4438
	step [35/206], loss=68.4811
	step [36/206], loss=65.8906
	step [37/206], loss=53.9763
	step [38/206], loss=78.1766
	step [39/206], loss=71.9862
	step [40/206], loss=95.4610
	step [41/206], loss=71.0303
	step [42/206], loss=87.0630
	step [43/206], loss=87.3739
	step [44/206], loss=76.9567
	step [45/206], loss=77.3703
	step [46/206], loss=77.4556
	step [47/206], loss=71.6187
	step [48/206], loss=85.2946
	step [49/206], loss=70.4016
	step [50/206], loss=86.0464
	step [51/206], loss=78.0711
	step [52/206], loss=85.1518
	step [53/206], loss=65.5894
	step [54/206], loss=82.4683
	step [55/206], loss=66.0847
	step [56/206], loss=78.0257
	step [57/206], loss=76.5118
	step [58/206], loss=65.5070
	step [59/206], loss=66.2507
	step [60/206], loss=77.4994
	step [61/206], loss=62.5756
	step [62/206], loss=87.5273
	step [63/206], loss=82.5218
	step [64/206], loss=81.3201
	step [65/206], loss=75.1054
	step [66/206], loss=79.2087
	step [67/206], loss=75.5287
	step [68/206], loss=82.2458
	step [69/206], loss=85.9490
	step [70/206], loss=73.3738
	step [71/206], loss=73.0617
	step [72/206], loss=73.5308
	step [73/206], loss=84.9570
	step [74/206], loss=80.5992
	step [75/206], loss=88.9358
	step [76/206], loss=87.4481
	step [77/206], loss=76.5519
	step [78/206], loss=81.6055
	step [79/206], loss=62.4157
	step [80/206], loss=72.6157
	step [81/206], loss=87.4055
	step [82/206], loss=77.4422
	step [83/206], loss=71.9613
	step [84/206], loss=85.1750
	step [85/206], loss=86.9602
	step [86/206], loss=73.7444
	step [87/206], loss=78.0452
	step [88/206], loss=78.4736
	step [89/206], loss=75.6271
	step [90/206], loss=80.0024
	step [91/206], loss=74.2455
	step [92/206], loss=75.4424
	step [93/206], loss=91.5881
	step [94/206], loss=79.9138
	step [95/206], loss=71.5843
	step [96/206], loss=77.5685
	step [97/206], loss=81.1373
	step [98/206], loss=105.3613
	step [99/206], loss=86.4170
	step [100/206], loss=88.1424
	step [101/206], loss=79.5730
	step [102/206], loss=77.7278
	step [103/206], loss=90.4302
	step [104/206], loss=76.6337
	step [105/206], loss=89.6079
	step [106/206], loss=79.6925
	step [107/206], loss=76.2726
	step [108/206], loss=79.6899
	step [109/206], loss=79.7508
	step [110/206], loss=82.7298
	step [111/206], loss=88.7796
	step [112/206], loss=81.0988
	step [113/206], loss=62.7715
	step [114/206], loss=87.6102
	step [115/206], loss=70.3168
	step [116/206], loss=80.8547
	step [117/206], loss=81.7896
	step [118/206], loss=84.8996
	step [119/206], loss=64.0935
	step [120/206], loss=84.0391
	step [121/206], loss=92.4783
	step [122/206], loss=80.8414
	step [123/206], loss=94.7857
	step [124/206], loss=102.4090
	step [125/206], loss=75.3704
	step [126/206], loss=74.2411
	step [127/206], loss=84.1280
	step [128/206], loss=79.2214
	step [129/206], loss=72.6009
	step [130/206], loss=71.2722
	step [131/206], loss=80.4595
	step [132/206], loss=84.0399
	step [133/206], loss=77.9111
	step [134/206], loss=87.9070
	step [135/206], loss=79.0985
	step [136/206], loss=85.7912
	step [137/206], loss=71.2229
	step [138/206], loss=65.3492
	step [139/206], loss=84.7018
	step [140/206], loss=71.9925
	step [141/206], loss=72.9225
	step [142/206], loss=82.7803
	step [143/206], loss=78.7856
	step [144/206], loss=91.4562
	step [145/206], loss=60.9087
	step [146/206], loss=76.7543
	step [147/206], loss=75.5210
	step [148/206], loss=85.3011
	step [149/206], loss=64.9074
	step [150/206], loss=76.1779
	step [151/206], loss=86.0306
	step [152/206], loss=76.3245
	step [153/206], loss=87.0369
	step [154/206], loss=82.2541
	step [155/206], loss=80.8052
	step [156/206], loss=78.5354
	step [157/206], loss=72.2978
	step [158/206], loss=79.7435
	step [159/206], loss=89.6357
	step [160/206], loss=78.8311
	step [161/206], loss=77.3386
	step [162/206], loss=84.2587
	step [163/206], loss=75.3767
	step [164/206], loss=71.4253
	step [165/206], loss=71.1961
	step [166/206], loss=65.5212
	step [167/206], loss=80.7670
	step [168/206], loss=70.5587
	step [169/206], loss=81.6163
	step [170/206], loss=94.1520
	step [171/206], loss=72.5189
	step [172/206], loss=66.8976
	step [173/206], loss=91.8008
	step [174/206], loss=67.6361
	step [175/206], loss=80.0516
	step [176/206], loss=66.8238
	step [177/206], loss=87.0036
	step [178/206], loss=75.5183
	step [179/206], loss=75.4604
	step [180/206], loss=68.2290
	step [181/206], loss=68.4965
	step [182/206], loss=67.5524
	step [183/206], loss=88.0411
	step [184/206], loss=88.5400
	step [185/206], loss=72.4701
	step [186/206], loss=81.3312
	step [187/206], loss=68.7029
	step [188/206], loss=79.0784
	step [189/206], loss=84.1120
	step [190/206], loss=78.7949
	step [191/206], loss=81.4191
	step [192/206], loss=80.3281
	step [193/206], loss=89.4535
	step [194/206], loss=72.8773
	step [195/206], loss=85.7579
	step [196/206], loss=72.0876
	step [197/206], loss=85.3148
	step [198/206], loss=78.4913
	step [199/206], loss=82.7843
	step [200/206], loss=67.5137
	step [201/206], loss=80.5509
	step [202/206], loss=71.0430
	step [203/206], loss=70.6275
	step [204/206], loss=83.1650
	step [205/206], loss=73.9979
	step [206/206], loss=49.2899
	Evaluating
	loss=0.0082, precision=0.3819, recall=0.9082, f1=0.5377
Training epoch 58
	step [1/206], loss=89.3604
	step [2/206], loss=69.7458
	step [3/206], loss=77.1908
	step [4/206], loss=83.6005
	step [5/206], loss=71.7392
	step [6/206], loss=64.2176
	step [7/206], loss=91.1255
	step [8/206], loss=62.9421
	step [9/206], loss=76.0026
	step [10/206], loss=64.1378
	step [11/206], loss=84.3893
	step [12/206], loss=74.3486
	step [13/206], loss=93.3498
	step [14/206], loss=76.3027
	step [15/206], loss=68.8176
	step [16/206], loss=80.7908
	step [17/206], loss=73.7561
	step [18/206], loss=69.6069
	step [19/206], loss=75.3686
	step [20/206], loss=69.0018
	step [21/206], loss=63.1603
	step [22/206], loss=66.8630
	step [23/206], loss=76.9459
	step [24/206], loss=74.8627
	step [25/206], loss=85.2500
	step [26/206], loss=91.7465
	step [27/206], loss=70.1069
	step [28/206], loss=67.2152
	step [29/206], loss=79.6629
	step [30/206], loss=72.9941
	step [31/206], loss=83.2280
	step [32/206], loss=81.6499
	step [33/206], loss=75.8327
	step [34/206], loss=78.3466
	step [35/206], loss=69.4635
	step [36/206], loss=87.4592
	step [37/206], loss=61.6910
	step [38/206], loss=71.2294
	step [39/206], loss=84.7747
	step [40/206], loss=75.0847
	step [41/206], loss=78.2566
	step [42/206], loss=76.4215
	step [43/206], loss=68.4846
	step [44/206], loss=85.7616
	step [45/206], loss=67.8909
	step [46/206], loss=80.0862
	step [47/206], loss=64.2168
	step [48/206], loss=80.3022
	step [49/206], loss=76.7436
	step [50/206], loss=82.2393
	step [51/206], loss=73.4279
	step [52/206], loss=87.0046
	step [53/206], loss=69.3426
	step [54/206], loss=82.0239
	step [55/206], loss=71.9378
	step [56/206], loss=76.6587
	step [57/206], loss=72.0560
	step [58/206], loss=83.0673
	step [59/206], loss=81.1658
	step [60/206], loss=89.0937
	step [61/206], loss=87.4946
	step [62/206], loss=76.2993
	step [63/206], loss=72.6167
	step [64/206], loss=76.5652
	step [65/206], loss=83.1027
	step [66/206], loss=76.4967
	step [67/206], loss=80.2728
	step [68/206], loss=86.3783
	step [69/206], loss=79.9456
	step [70/206], loss=71.9603
	step [71/206], loss=73.4541
	step [72/206], loss=71.7740
	step [73/206], loss=78.5196
	step [74/206], loss=71.7020
	step [75/206], loss=90.2067
	step [76/206], loss=84.0431
	step [77/206], loss=69.9707
	step [78/206], loss=81.2852
	step [79/206], loss=77.4037
	step [80/206], loss=87.2973
	step [81/206], loss=82.1458
	step [82/206], loss=66.0549
	step [83/206], loss=90.4395
	step [84/206], loss=85.3426
	step [85/206], loss=82.5111
	step [86/206], loss=75.0551
	step [87/206], loss=79.1202
	step [88/206], loss=72.7141
	step [89/206], loss=66.4121
	step [90/206], loss=75.5292
	step [91/206], loss=83.8264
	step [92/206], loss=87.7041
	step [93/206], loss=83.7143
	step [94/206], loss=71.7318
	step [95/206], loss=94.1428
	step [96/206], loss=71.3736
	step [97/206], loss=83.8335
	step [98/206], loss=75.1770
	step [99/206], loss=82.2959
	step [100/206], loss=69.0111
	step [101/206], loss=87.0216
	step [102/206], loss=71.4113
	step [103/206], loss=84.1479
	step [104/206], loss=71.1286
	step [105/206], loss=82.8692
	step [106/206], loss=83.1986
	step [107/206], loss=72.4418
	step [108/206], loss=97.4538
	step [109/206], loss=77.8892
	step [110/206], loss=89.0637
	step [111/206], loss=73.5754
	step [112/206], loss=78.8462
	step [113/206], loss=80.6750
	step [114/206], loss=80.2871
	step [115/206], loss=83.7094
	step [116/206], loss=70.8660
	step [117/206], loss=81.3750
	step [118/206], loss=78.3112
	step [119/206], loss=68.3232
	step [120/206], loss=98.2636
	step [121/206], loss=83.4132
	step [122/206], loss=81.4566
	step [123/206], loss=85.3546
	step [124/206], loss=78.3781
	step [125/206], loss=87.8435
	step [126/206], loss=71.5285
	step [127/206], loss=85.4749
	step [128/206], loss=90.5539
	step [129/206], loss=70.7646
	step [130/206], loss=78.9391
	step [131/206], loss=79.5986
	step [132/206], loss=83.6301
	step [133/206], loss=89.4122
	step [134/206], loss=78.4625
	step [135/206], loss=72.8170
	step [136/206], loss=80.8822
	step [137/206], loss=77.7461
	step [138/206], loss=65.7380
	step [139/206], loss=77.1984
	step [140/206], loss=92.2912
	step [141/206], loss=72.1301
	step [142/206], loss=85.5557
	step [143/206], loss=79.3924
	step [144/206], loss=76.7024
	step [145/206], loss=81.3679
	step [146/206], loss=69.0407
	step [147/206], loss=77.9836
	step [148/206], loss=68.6347
	step [149/206], loss=90.9432
	step [150/206], loss=78.5943
	step [151/206], loss=82.2743
	step [152/206], loss=80.3554
	step [153/206], loss=81.3850
	step [154/206], loss=82.3712
	step [155/206], loss=89.9996
	step [156/206], loss=70.5507
	step [157/206], loss=72.9017
	step [158/206], loss=79.2324
	step [159/206], loss=81.1529
	step [160/206], loss=72.2263
	step [161/206], loss=80.5656
	step [162/206], loss=83.0043
	step [163/206], loss=68.3513
	step [164/206], loss=100.3865
	step [165/206], loss=71.9654
	step [166/206], loss=74.4628
	step [167/206], loss=66.5546
	step [168/206], loss=72.6104
	step [169/206], loss=84.5951
	step [170/206], loss=73.0295
	step [171/206], loss=82.0086
	step [172/206], loss=72.6836
	step [173/206], loss=94.7202
	step [174/206], loss=72.2010
	step [175/206], loss=90.0726
	step [176/206], loss=70.8854
	step [177/206], loss=62.5024
	step [178/206], loss=88.9191
	step [179/206], loss=75.4967
	step [180/206], loss=85.7060
	step [181/206], loss=94.4600
	step [182/206], loss=85.0466
	step [183/206], loss=95.8548
	step [184/206], loss=79.7962
	step [185/206], loss=76.9683
	step [186/206], loss=77.7961
	step [187/206], loss=78.6231
	step [188/206], loss=79.3300
	step [189/206], loss=63.6239
	step [190/206], loss=77.9994
	step [191/206], loss=86.6277
	step [192/206], loss=69.8401
	step [193/206], loss=88.0299
	step [194/206], loss=75.6700
	step [195/206], loss=88.6898
	step [196/206], loss=69.1629
	step [197/206], loss=78.6921
	step [198/206], loss=74.5303
	step [199/206], loss=85.3464
	step [200/206], loss=69.6143
	step [201/206], loss=86.2480
	step [202/206], loss=80.7146
	step [203/206], loss=83.9565
	step [204/206], loss=86.6667
	step [205/206], loss=76.2767
	step [206/206], loss=61.0603
	Evaluating
	loss=0.0071, precision=0.4149, recall=0.8990, f1=0.5677
Training epoch 59
	step [1/206], loss=67.1620
	step [2/206], loss=87.0506
	step [3/206], loss=60.1525
	step [4/206], loss=99.2281
	step [5/206], loss=85.8052
	step [6/206], loss=79.0039
	step [7/206], loss=80.2630
	step [8/206], loss=72.4310
	step [9/206], loss=73.3756
	step [10/206], loss=92.6957
	step [11/206], loss=81.8504
	step [12/206], loss=93.3309
	step [13/206], loss=78.6375
	step [14/206], loss=81.7643
	step [15/206], loss=70.5282
	step [16/206], loss=88.9327
	step [17/206], loss=69.8279
	step [18/206], loss=86.6070
	step [19/206], loss=83.0682
	step [20/206], loss=92.5577
	step [21/206], loss=78.4374
	step [22/206], loss=75.6307
	step [23/206], loss=72.4087
	step [24/206], loss=73.4121
	step [25/206], loss=66.4974
	step [26/206], loss=79.6548
	step [27/206], loss=79.5095
	step [28/206], loss=79.2844
	step [29/206], loss=79.5524
	step [30/206], loss=80.4019
	step [31/206], loss=80.8531
	step [32/206], loss=73.7514
	step [33/206], loss=76.2687
	step [34/206], loss=87.9758
	step [35/206], loss=100.4007
	step [36/206], loss=74.9563
	step [37/206], loss=86.1283
	step [38/206], loss=76.2338
	step [39/206], loss=75.9831
	step [40/206], loss=74.1332
	step [41/206], loss=72.9978
	step [42/206], loss=72.7031
	step [43/206], loss=78.8832
	step [44/206], loss=77.7463
	step [45/206], loss=66.7124
	step [46/206], loss=71.6210
	step [47/206], loss=75.5340
	step [48/206], loss=73.4593
	step [49/206], loss=84.8743
	step [50/206], loss=74.2673
	step [51/206], loss=74.7824
	step [52/206], loss=80.9662
	step [53/206], loss=80.8941
	step [54/206], loss=68.7135
	step [55/206], loss=75.9812
	step [56/206], loss=80.5623
	step [57/206], loss=72.3951
	step [58/206], loss=75.8243
	step [59/206], loss=68.9764
	step [60/206], loss=81.0932
	step [61/206], loss=79.9458
	step [62/206], loss=68.8969
	step [63/206], loss=79.7602
	step [64/206], loss=68.7537
	step [65/206], loss=73.9490
	step [66/206], loss=72.6307
	step [67/206], loss=78.1781
	step [68/206], loss=71.3058
	step [69/206], loss=84.5957
	step [70/206], loss=72.5203
	step [71/206], loss=76.8750
	step [72/206], loss=77.1323
	step [73/206], loss=70.7338
	step [74/206], loss=75.8359
	step [75/206], loss=84.9383
	step [76/206], loss=82.1057
	step [77/206], loss=89.4292
	step [78/206], loss=78.1389
	step [79/206], loss=69.0268
	step [80/206], loss=72.6828
	step [81/206], loss=74.7305
	step [82/206], loss=76.8435
	step [83/206], loss=82.5785
	step [84/206], loss=88.7532
	step [85/206], loss=81.4304
	step [86/206], loss=81.0597
	step [87/206], loss=90.0477
	step [88/206], loss=90.1352
	step [89/206], loss=81.8799
	step [90/206], loss=77.9344
	step [91/206], loss=83.8974
	step [92/206], loss=87.3575
	step [93/206], loss=71.2992
	step [94/206], loss=82.4572
	step [95/206], loss=63.8638
	step [96/206], loss=78.3532
	step [97/206], loss=74.8613
	step [98/206], loss=73.3709
	step [99/206], loss=74.0560
	step [100/206], loss=82.0585
	step [101/206], loss=85.1334
	step [102/206], loss=70.4459
	step [103/206], loss=83.9641
	step [104/206], loss=84.1098
	step [105/206], loss=70.8187
	step [106/206], loss=75.2730
	step [107/206], loss=76.2273
	step [108/206], loss=81.7797
	step [109/206], loss=77.0259
	step [110/206], loss=93.3358
	step [111/206], loss=83.5084
	step [112/206], loss=67.9325
	step [113/206], loss=86.4030
	step [114/206], loss=70.0308
	step [115/206], loss=67.0627
	step [116/206], loss=91.0197
	step [117/206], loss=82.2240
	step [118/206], loss=77.6003
	step [119/206], loss=70.4391
	step [120/206], loss=69.2858
	step [121/206], loss=71.4962
	step [122/206], loss=76.4042
	step [123/206], loss=87.9356
	step [124/206], loss=82.0377
	step [125/206], loss=76.9974
	step [126/206], loss=72.1555
	step [127/206], loss=78.1923
	step [128/206], loss=80.5397
	step [129/206], loss=69.3665
	step [130/206], loss=85.4512
	step [131/206], loss=79.9042
	step [132/206], loss=72.4217
	step [133/206], loss=90.9642
	step [134/206], loss=58.0694
	step [135/206], loss=81.6605
	step [136/206], loss=79.2722
	step [137/206], loss=73.0521
	step [138/206], loss=77.6384
	step [139/206], loss=82.1865
	step [140/206], loss=88.2625
	step [141/206], loss=72.6139
	step [142/206], loss=76.3073
	step [143/206], loss=89.7460
	step [144/206], loss=69.9689
	step [145/206], loss=76.0802
	step [146/206], loss=76.9026
	step [147/206], loss=84.3806
	step [148/206], loss=68.5729
	step [149/206], loss=73.8038
	step [150/206], loss=74.0528
	step [151/206], loss=83.1890
	step [152/206], loss=73.8805
	step [153/206], loss=81.6286
	step [154/206], loss=73.2025
	step [155/206], loss=73.7968
	step [156/206], loss=76.6275
	step [157/206], loss=76.3157
	step [158/206], loss=81.8411
	step [159/206], loss=66.2817
	step [160/206], loss=81.1509
	step [161/206], loss=79.5614
	step [162/206], loss=77.8657
	step [163/206], loss=81.3174
	step [164/206], loss=75.5114
	step [165/206], loss=74.5490
	step [166/206], loss=85.0871
	step [167/206], loss=73.4666
	step [168/206], loss=78.6115
	step [169/206], loss=83.9299
	step [170/206], loss=85.6523
	step [171/206], loss=68.7762
	step [172/206], loss=78.1059
	step [173/206], loss=83.3627
	step [174/206], loss=91.5880
	step [175/206], loss=77.9545
	step [176/206], loss=80.8151
	step [177/206], loss=94.5738
	step [178/206], loss=71.8079
	step [179/206], loss=75.3472
	step [180/206], loss=77.5170
	step [181/206], loss=86.4694
	step [182/206], loss=87.4190
	step [183/206], loss=85.1621
	step [184/206], loss=82.6188
	step [185/206], loss=86.8134
	step [186/206], loss=83.9901
	step [187/206], loss=83.6973
	step [188/206], loss=74.8158
	step [189/206], loss=78.7238
	step [190/206], loss=73.2695
	step [191/206], loss=70.8115
	step [192/206], loss=76.3918
	step [193/206], loss=65.0034
	step [194/206], loss=69.7103
	step [195/206], loss=90.3290
	step [196/206], loss=84.0246
	step [197/206], loss=75.7101
	step [198/206], loss=74.8023
	step [199/206], loss=95.3495
	step [200/206], loss=69.1805
	step [201/206], loss=79.6803
	step [202/206], loss=58.8845
	step [203/206], loss=84.5314
	step [204/206], loss=75.1801
	step [205/206], loss=66.2710
	step [206/206], loss=59.9099
	Evaluating
	loss=0.0072, precision=0.4304, recall=0.9027, f1=0.5829
saving model as: 4_saved_model.pth
Training epoch 60
	step [1/206], loss=80.6614
	step [2/206], loss=79.3449
	step [3/206], loss=78.3821
	step [4/206], loss=85.0374
	step [5/206], loss=71.1471
	step [6/206], loss=90.4481
	step [7/206], loss=72.6643
	step [8/206], loss=59.0893
	step [9/206], loss=83.3770
	step [10/206], loss=74.3377
	step [11/206], loss=86.3757
	step [12/206], loss=81.9343
	step [13/206], loss=89.5671
	step [14/206], loss=74.1367
	step [15/206], loss=74.1300
	step [16/206], loss=73.4185
	step [17/206], loss=82.5688
	step [18/206], loss=72.9236
	step [19/206], loss=70.6724
	step [20/206], loss=79.8696
	step [21/206], loss=78.3039
	step [22/206], loss=78.2593
	step [23/206], loss=79.6855
	step [24/206], loss=85.8402
	step [25/206], loss=72.6509
	step [26/206], loss=83.6780
	step [27/206], loss=65.3487
	step [28/206], loss=81.7234
	step [29/206], loss=76.0889
	step [30/206], loss=88.3734
	step [31/206], loss=84.6010
	step [32/206], loss=84.0136
	step [33/206], loss=78.7004
	step [34/206], loss=72.3768
	step [35/206], loss=79.6548
	step [36/206], loss=86.2233
	step [37/206], loss=87.3858
	step [38/206], loss=78.9060
	step [39/206], loss=76.0663
	step [40/206], loss=84.0045
	step [41/206], loss=84.7297
	step [42/206], loss=85.9218
	step [43/206], loss=81.4859
	step [44/206], loss=75.5356
	step [45/206], loss=83.0887
	step [46/206], loss=73.8700
	step [47/206], loss=75.1252
	step [48/206], loss=78.4541
	step [49/206], loss=78.8732
	step [50/206], loss=84.8528
	step [51/206], loss=79.9631
	step [52/206], loss=80.9296
	step [53/206], loss=67.5809
	step [54/206], loss=74.2065
	step [55/206], loss=85.5113
	step [56/206], loss=76.9826
	step [57/206], loss=80.5591
	step [58/206], loss=65.1417
	step [59/206], loss=83.7705
	step [60/206], loss=75.1491
	step [61/206], loss=70.2406
	step [62/206], loss=69.2553
	step [63/206], loss=81.5777
	step [64/206], loss=79.2906
	step [65/206], loss=88.5029
	step [66/206], loss=86.5379
	step [67/206], loss=77.3769
	step [68/206], loss=94.6280
	step [69/206], loss=94.2986
	step [70/206], loss=66.2397
	step [71/206], loss=75.2273
	step [72/206], loss=78.2226
	step [73/206], loss=73.9890
	step [74/206], loss=90.2496
	step [75/206], loss=65.7026
	step [76/206], loss=80.1717
	step [77/206], loss=65.9153
	step [78/206], loss=75.3645
	step [79/206], loss=76.6566
	step [80/206], loss=70.3545
	step [81/206], loss=73.1921
	step [82/206], loss=63.6652
	step [83/206], loss=74.7444
	step [84/206], loss=75.6531
	step [85/206], loss=89.9349
	step [86/206], loss=85.3043
	step [87/206], loss=77.7104
	step [88/206], loss=75.1732
	step [89/206], loss=97.3824
	step [90/206], loss=82.4445
	step [91/206], loss=69.5006
	step [92/206], loss=103.6478
	step [93/206], loss=75.1099
	step [94/206], loss=77.4014
	step [95/206], loss=82.6039
	step [96/206], loss=67.6457
	step [97/206], loss=63.0120
	step [98/206], loss=70.2996
	step [99/206], loss=73.9062
	step [100/206], loss=70.2030
	step [101/206], loss=72.4485
	step [102/206], loss=67.0696
	step [103/206], loss=83.2498
	step [104/206], loss=79.4016
	step [105/206], loss=74.1614
	step [106/206], loss=70.8633
	step [107/206], loss=78.1649
	step [108/206], loss=90.4295
	step [109/206], loss=75.5307
	step [110/206], loss=84.5419
	step [111/206], loss=78.1086
	step [112/206], loss=79.9799
	step [113/206], loss=84.7976
	step [114/206], loss=80.4472
	step [115/206], loss=73.5302
	step [116/206], loss=68.1180
	step [117/206], loss=69.4850
	step [118/206], loss=80.2622
	step [119/206], loss=86.6068
	step [120/206], loss=76.9199
	step [121/206], loss=79.1341
	step [122/206], loss=83.3295
	step [123/206], loss=83.3572
	step [124/206], loss=76.8200
	step [125/206], loss=84.1310
	step [126/206], loss=59.4562
	step [127/206], loss=79.4042
	step [128/206], loss=71.6387
	step [129/206], loss=92.1073
	step [130/206], loss=77.8599
	step [131/206], loss=87.6892
	step [132/206], loss=77.6480
	step [133/206], loss=81.6511
	step [134/206], loss=69.5120
	step [135/206], loss=79.9156
	step [136/206], loss=85.2989
	step [137/206], loss=83.8658
	step [138/206], loss=76.3683
	step [139/206], loss=75.1257
	step [140/206], loss=73.9794
	step [141/206], loss=71.0159
	step [142/206], loss=60.4797
	step [143/206], loss=72.9447
	step [144/206], loss=83.8507
	step [145/206], loss=75.8602
	step [146/206], loss=77.4434
	step [147/206], loss=71.6352
	step [148/206], loss=73.1452
	step [149/206], loss=73.8318
	step [150/206], loss=69.4961
	step [151/206], loss=70.5322
	step [152/206], loss=81.8946
	step [153/206], loss=63.1483
	step [154/206], loss=87.0536
	step [155/206], loss=81.5944
	step [156/206], loss=91.3423
	step [157/206], loss=73.6448
	step [158/206], loss=65.7139
	step [159/206], loss=74.3630
	step [160/206], loss=79.7026
	step [161/206], loss=85.8898
	step [162/206], loss=74.9821
	step [163/206], loss=71.4629
	step [164/206], loss=87.1741
	step [165/206], loss=84.1348
	step [166/206], loss=60.2980
	step [167/206], loss=59.2767
	step [168/206], loss=67.3290
	step [169/206], loss=80.8974
	step [170/206], loss=85.0488
	step [171/206], loss=89.3898
	step [172/206], loss=89.0813
	step [173/206], loss=88.1712
	step [174/206], loss=76.5600
	step [175/206], loss=81.2274
	step [176/206], loss=70.3459
	step [177/206], loss=72.4976
	step [178/206], loss=78.0234
	step [179/206], loss=85.3786
	step [180/206], loss=70.6349
	step [181/206], loss=77.1949
	step [182/206], loss=77.5644
	step [183/206], loss=77.8253
	step [184/206], loss=77.5911
	step [185/206], loss=71.6094
	step [186/206], loss=78.3760
	step [187/206], loss=68.4165
	step [188/206], loss=79.9515
	step [189/206], loss=76.7630
	step [190/206], loss=66.5736
	step [191/206], loss=71.2501
	step [192/206], loss=80.3734
	step [193/206], loss=73.5455
	step [194/206], loss=95.4159
	step [195/206], loss=85.1453
	step [196/206], loss=74.7542
	step [197/206], loss=72.9101
	step [198/206], loss=75.3411
	step [199/206], loss=81.4661
	step [200/206], loss=67.6869
	step [201/206], loss=75.6050
	step [202/206], loss=77.4086
	step [203/206], loss=78.1500
	step [204/206], loss=90.9542
	step [205/206], loss=85.0197
	step [206/206], loss=52.0970
	Evaluating
	loss=0.0068, precision=0.4386, recall=0.8993, f1=0.5896
saving model as: 4_saved_model.pth
Training epoch 61
	step [1/206], loss=61.1360
	step [2/206], loss=82.0955
	step [3/206], loss=77.0344
	step [4/206], loss=84.6718
	step [5/206], loss=78.8056
	step [6/206], loss=87.4185
	step [7/206], loss=75.8032
	step [8/206], loss=67.6724
	step [9/206], loss=73.9627
	step [10/206], loss=76.3672
	step [11/206], loss=75.5775
	step [12/206], loss=70.0754
	step [13/206], loss=81.0522
	step [14/206], loss=74.7431
	step [15/206], loss=86.3394
	step [16/206], loss=83.8788
	step [17/206], loss=83.1088
	step [18/206], loss=71.7342
	step [19/206], loss=79.3702
	step [20/206], loss=91.7434
	step [21/206], loss=75.7879
	step [22/206], loss=73.1102
	step [23/206], loss=85.2428
	step [24/206], loss=60.3426
	step [25/206], loss=73.9976
	step [26/206], loss=75.8838
	step [27/206], loss=72.4811
	step [28/206], loss=73.4954
	step [29/206], loss=75.8876
	step [30/206], loss=72.1269
	step [31/206], loss=69.0889
	step [32/206], loss=69.9833
	step [33/206], loss=79.2830
	step [34/206], loss=88.1527
	step [35/206], loss=82.2929
	step [36/206], loss=70.1047
	step [37/206], loss=75.4961
	step [38/206], loss=76.8098
	step [39/206], loss=82.7143
	step [40/206], loss=84.0114
	step [41/206], loss=67.7015
	step [42/206], loss=75.3754
	step [43/206], loss=68.2640
	step [44/206], loss=74.3550
	step [45/206], loss=71.1868
	step [46/206], loss=84.6010
	step [47/206], loss=82.0057
	step [48/206], loss=74.5221
	step [49/206], loss=73.0207
	step [50/206], loss=74.5615
	step [51/206], loss=88.0237
	step [52/206], loss=77.9086
	step [53/206], loss=75.0307
	step [54/206], loss=62.3724
	step [55/206], loss=81.0582
	step [56/206], loss=80.6558
	step [57/206], loss=84.4282
	step [58/206], loss=93.6235
	step [59/206], loss=80.7856
	step [60/206], loss=66.9489
	step [61/206], loss=83.4487
	step [62/206], loss=75.4533
	step [63/206], loss=87.9106
	step [64/206], loss=86.5574
	step [65/206], loss=86.4458
	step [66/206], loss=78.9813
	step [67/206], loss=78.6449
	step [68/206], loss=75.1695
	step [69/206], loss=74.2491
	step [70/206], loss=77.7368
	step [71/206], loss=89.0856
	step [72/206], loss=76.9889
	step [73/206], loss=61.0805
	step [74/206], loss=71.4779
	step [75/206], loss=81.6495
	step [76/206], loss=71.5331
	step [77/206], loss=78.5881
	step [78/206], loss=63.6334
	step [79/206], loss=82.9045
	step [80/206], loss=82.3291
	step [81/206], loss=87.3815
	step [82/206], loss=81.7374
	step [83/206], loss=83.2726
	step [84/206], loss=78.0312
	step [85/206], loss=74.3880
	step [86/206], loss=87.2051
	step [87/206], loss=87.3799
	step [88/206], loss=85.9859
	step [89/206], loss=74.9702
	step [90/206], loss=88.5130
	step [91/206], loss=70.3584
	step [92/206], loss=77.7327
	step [93/206], loss=80.8082
	step [94/206], loss=65.0947
	step [95/206], loss=69.7465
	step [96/206], loss=60.9321
	step [97/206], loss=84.7510
	step [98/206], loss=80.2935
	step [99/206], loss=78.1222
	step [100/206], loss=68.4638
	step [101/206], loss=81.5150
	step [102/206], loss=79.8312
	step [103/206], loss=71.1159
	step [104/206], loss=79.1255
	step [105/206], loss=74.8240
	step [106/206], loss=77.7466
	step [107/206], loss=84.2906
	step [108/206], loss=77.4981
	step [109/206], loss=81.4171
	step [110/206], loss=77.1988
	step [111/206], loss=80.7591
	step [112/206], loss=76.6978
	step [113/206], loss=84.4753
	step [114/206], loss=93.6195
	step [115/206], loss=68.1344
	step [116/206], loss=85.9219
	step [117/206], loss=90.9244
	step [118/206], loss=83.1933
	step [119/206], loss=72.1206
	step [120/206], loss=79.7464
	step [121/206], loss=89.9641
	step [122/206], loss=82.3729
	step [123/206], loss=73.9452
	step [124/206], loss=77.6035
	step [125/206], loss=78.1929
	step [126/206], loss=69.2816
	step [127/206], loss=74.2545
	step [128/206], loss=76.4643
	step [129/206], loss=63.3197
	step [130/206], loss=73.4720
	step [131/206], loss=79.5836
	step [132/206], loss=68.0094
	step [133/206], loss=79.2363
	step [134/206], loss=77.6331
	step [135/206], loss=75.4588
	step [136/206], loss=74.4897
	step [137/206], loss=85.6056
	step [138/206], loss=76.9874
	step [139/206], loss=70.7471
	step [140/206], loss=78.9912
	step [141/206], loss=77.6995
	step [142/206], loss=98.5368
	step [143/206], loss=81.4848
	step [144/206], loss=71.4742
	step [145/206], loss=76.2829
	step [146/206], loss=85.9192
	step [147/206], loss=79.5468
	step [148/206], loss=69.8663
	step [149/206], loss=68.2254
	step [150/206], loss=74.9240
	step [151/206], loss=65.1640
	step [152/206], loss=69.8946
	step [153/206], loss=89.9427
	step [154/206], loss=93.7315
	step [155/206], loss=79.7356
	step [156/206], loss=71.4913
	step [157/206], loss=72.6258
	step [158/206], loss=77.4013
	step [159/206], loss=76.3857
	step [160/206], loss=77.7684
	step [161/206], loss=83.9839
	step [162/206], loss=79.4177
	step [163/206], loss=90.6064
	step [164/206], loss=77.2205
	step [165/206], loss=88.1376
	step [166/206], loss=79.9477
	step [167/206], loss=73.8261
	step [168/206], loss=81.0891
	step [169/206], loss=64.6814
	step [170/206], loss=85.8769
	step [171/206], loss=75.8601
	step [172/206], loss=85.4324
	step [173/206], loss=72.5296
	step [174/206], loss=71.1196
	step [175/206], loss=75.3161
	step [176/206], loss=81.9188
	step [177/206], loss=71.1792
	step [178/206], loss=70.6290
	step [179/206], loss=80.7490
	step [180/206], loss=70.6157
	step [181/206], loss=74.0474
	step [182/206], loss=86.3326
	step [183/206], loss=64.7266
	step [184/206], loss=74.5755
	step [185/206], loss=85.3359
	step [186/206], loss=68.6871
	step [187/206], loss=73.8464
	step [188/206], loss=84.6454
	step [189/206], loss=88.4867
	step [190/206], loss=71.3257
	step [191/206], loss=75.4152
	step [192/206], loss=75.2007
	step [193/206], loss=84.7501
	step [194/206], loss=79.9251
	step [195/206], loss=87.7888
	step [196/206], loss=81.6568
	step [197/206], loss=75.0021
	step [198/206], loss=76.3424
	step [199/206], loss=84.9333
	step [200/206], loss=75.9121
	step [201/206], loss=70.1602
	step [202/206], loss=77.2122
	step [203/206], loss=86.4576
	step [204/206], loss=85.2095
	step [205/206], loss=82.3809
	step [206/206], loss=50.4296
	Evaluating
	loss=0.0088, precision=0.3543, recall=0.9035, f1=0.5089
Training epoch 62
	step [1/206], loss=64.2059
	step [2/206], loss=84.1281
	step [3/206], loss=73.3801
	step [4/206], loss=66.2986
	step [5/206], loss=88.0597
	step [6/206], loss=76.1010
	step [7/206], loss=80.2307
	step [8/206], loss=73.7626
	step [9/206], loss=64.8684
	step [10/206], loss=74.2016
	step [11/206], loss=76.7747
	step [12/206], loss=81.1622
	step [13/206], loss=75.8309
	step [14/206], loss=73.0075
	step [15/206], loss=78.6148
	step [16/206], loss=83.4627
	step [17/206], loss=78.4941
	step [18/206], loss=75.7019
	step [19/206], loss=84.9026
	step [20/206], loss=77.5802
	step [21/206], loss=76.9815
	step [22/206], loss=67.1418
	step [23/206], loss=85.8530
	step [24/206], loss=78.4578
	step [25/206], loss=85.4992
	step [26/206], loss=82.9620
	step [27/206], loss=70.4646
	step [28/206], loss=81.5718
	step [29/206], loss=86.4993
	step [30/206], loss=83.9236
	step [31/206], loss=72.7936
	step [32/206], loss=84.7175
	step [33/206], loss=71.7233
	step [34/206], loss=70.7204
	step [35/206], loss=69.4045
	step [36/206], loss=60.0117
	step [37/206], loss=85.4991
	step [38/206], loss=65.8103
	step [39/206], loss=71.9668
	step [40/206], loss=82.1410
	step [41/206], loss=83.7678
	step [42/206], loss=66.1933
	step [43/206], loss=84.3804
	step [44/206], loss=84.7928
	step [45/206], loss=74.0115
	step [46/206], loss=88.2105
	step [47/206], loss=70.8112
	step [48/206], loss=99.7696
	step [49/206], loss=78.2582
	step [50/206], loss=99.9096
	step [51/206], loss=87.5201
	step [52/206], loss=77.6322
	step [53/206], loss=90.3013
	step [54/206], loss=74.6964
	step [55/206], loss=74.6375
	step [56/206], loss=76.1843
	step [57/206], loss=77.6776
	step [58/206], loss=91.0312
	step [59/206], loss=68.5426
	step [60/206], loss=75.1090
	step [61/206], loss=82.6868
	step [62/206], loss=64.5062
	step [63/206], loss=61.0779
	step [64/206], loss=89.5647
	step [65/206], loss=92.4441
	step [66/206], loss=82.9396
	step [67/206], loss=79.4267
	step [68/206], loss=59.3859
	step [69/206], loss=64.5974
	step [70/206], loss=82.9274
	step [71/206], loss=88.8045
	step [72/206], loss=79.5539
	step [73/206], loss=77.4647
	step [74/206], loss=89.0540
	step [75/206], loss=78.9900
	step [76/206], loss=75.8998
	step [77/206], loss=82.2193
	step [78/206], loss=88.6495
	step [79/206], loss=83.7978
	step [80/206], loss=92.2746
	step [81/206], loss=85.7120
	step [82/206], loss=85.7279
	step [83/206], loss=75.0669
	step [84/206], loss=82.5229
	step [85/206], loss=69.3084
	step [86/206], loss=72.5818
	step [87/206], loss=79.5352
	step [88/206], loss=84.8934
	step [89/206], loss=91.4307
	step [90/206], loss=89.2106
	step [91/206], loss=76.2829
	step [92/206], loss=82.5678
	step [93/206], loss=81.0291
	step [94/206], loss=79.0669
	step [95/206], loss=87.0184
	step [96/206], loss=83.3070
	step [97/206], loss=74.0717
	step [98/206], loss=78.5691
	step [99/206], loss=62.4880
	step [100/206], loss=84.2193
	step [101/206], loss=77.7674
	step [102/206], loss=73.6572
	step [103/206], loss=81.6524
	step [104/206], loss=79.4280
	step [105/206], loss=67.2709
	step [106/206], loss=75.9700
	step [107/206], loss=72.7914
	step [108/206], loss=68.4256
	step [109/206], loss=78.8971
	step [110/206], loss=73.4457
	step [111/206], loss=63.4171
	step [112/206], loss=76.2741
	step [113/206], loss=79.5546
	step [114/206], loss=84.5488
	step [115/206], loss=81.0991
	step [116/206], loss=69.3403
	step [117/206], loss=73.7245
	step [118/206], loss=79.5622
	step [119/206], loss=55.7033
	step [120/206], loss=86.8565
	step [121/206], loss=86.1172
	step [122/206], loss=78.4651
	step [123/206], loss=74.1791
	step [124/206], loss=75.1796
	step [125/206], loss=66.8371
	step [126/206], loss=91.7679
	step [127/206], loss=75.9445
	step [128/206], loss=70.9789
	step [129/206], loss=70.7771
	step [130/206], loss=67.0983
	step [131/206], loss=71.1162
	step [132/206], loss=88.7862
	step [133/206], loss=77.6732
	step [134/206], loss=80.5733
	step [135/206], loss=81.8234
	step [136/206], loss=66.1144
	step [137/206], loss=66.3139
	step [138/206], loss=64.4885
	step [139/206], loss=83.6893
	step [140/206], loss=69.6415
	step [141/206], loss=75.7342
	step [142/206], loss=69.7184
	step [143/206], loss=66.6376
	step [144/206], loss=68.4998
	step [145/206], loss=70.9773
	step [146/206], loss=86.8524
	step [147/206], loss=74.6220
	step [148/206], loss=69.8452
	step [149/206], loss=75.2046
	step [150/206], loss=78.8855
	step [151/206], loss=75.9794
	step [152/206], loss=82.1085
	step [153/206], loss=58.9897
	step [154/206], loss=68.4100
	step [155/206], loss=73.1773
	step [156/206], loss=77.2172
	step [157/206], loss=86.6086
	step [158/206], loss=70.4576
	step [159/206], loss=87.0961
	step [160/206], loss=74.6875
	step [161/206], loss=78.3811
	step [162/206], loss=74.5377
	step [163/206], loss=79.5514
	step [164/206], loss=84.4735
	step [165/206], loss=74.0886
	step [166/206], loss=75.0422
	step [167/206], loss=73.6990
	step [168/206], loss=85.7011
	step [169/206], loss=69.0255
	step [170/206], loss=82.2788
	step [171/206], loss=79.7758
	step [172/206], loss=78.3817
	step [173/206], loss=84.4293
	step [174/206], loss=68.1243
	step [175/206], loss=65.9719
	step [176/206], loss=68.9366
	step [177/206], loss=64.9957
	step [178/206], loss=77.3995
	step [179/206], loss=77.6272
	step [180/206], loss=73.3293
	step [181/206], loss=81.1344
	step [182/206], loss=83.8683
	step [183/206], loss=83.0941
	step [184/206], loss=76.2065
	step [185/206], loss=67.4592
	step [186/206], loss=84.2188
	step [187/206], loss=75.3480
	step [188/206], loss=81.0915
	step [189/206], loss=91.8779
	step [190/206], loss=66.9715
	step [191/206], loss=90.7820
	step [192/206], loss=83.1096
	step [193/206], loss=73.0492
	step [194/206], loss=75.6769
	step [195/206], loss=76.4753
	step [196/206], loss=85.2395
	step [197/206], loss=88.7045
	step [198/206], loss=69.0135
	step [199/206], loss=80.9640
	step [200/206], loss=68.9312
	step [201/206], loss=75.0701
	step [202/206], loss=80.6193
	step [203/206], loss=75.9646
	step [204/206], loss=74.7460
	step [205/206], loss=82.3648
	step [206/206], loss=53.0002
	Evaluating
	loss=0.0076, precision=0.3970, recall=0.9056, f1=0.5520
Training epoch 63
	step [1/206], loss=73.3244
	step [2/206], loss=100.0788
	step [3/206], loss=61.4248
	step [4/206], loss=67.1404
	step [5/206], loss=81.5214
	step [6/206], loss=56.3169
	step [7/206], loss=90.8715
	step [8/206], loss=85.6588
	step [9/206], loss=68.1513
	step [10/206], loss=73.7458
	step [11/206], loss=80.5182
	step [12/206], loss=81.0879
	step [13/206], loss=78.8401
	step [14/206], loss=78.5423
	step [15/206], loss=78.7944
	step [16/206], loss=69.0484
	step [17/206], loss=75.8617
	step [18/206], loss=73.6799
	step [19/206], loss=76.2417
	step [20/206], loss=85.0543
	step [21/206], loss=69.0247
	step [22/206], loss=67.5353
	step [23/206], loss=81.0442
	step [24/206], loss=68.7284
	step [25/206], loss=76.3342
	step [26/206], loss=61.6110
	step [27/206], loss=81.4833
	step [28/206], loss=68.6431
	step [29/206], loss=72.1095
	step [30/206], loss=80.7141
	step [31/206], loss=68.2914
	step [32/206], loss=84.7838
	step [33/206], loss=75.0976
	step [34/206], loss=78.3001
	step [35/206], loss=75.4473
	step [36/206], loss=100.8350
	step [37/206], loss=65.0252
	step [38/206], loss=74.9053
	step [39/206], loss=81.6170
	step [40/206], loss=82.1953
	step [41/206], loss=87.6868
	step [42/206], loss=81.0445
	step [43/206], loss=84.4527
	step [44/206], loss=67.8350
	step [45/206], loss=66.9456
	step [46/206], loss=68.4821
	step [47/206], loss=87.6574
	step [48/206], loss=74.0589
	step [49/206], loss=81.9590
	step [50/206], loss=77.1819
	step [51/206], loss=66.9505
	step [52/206], loss=81.1709
	step [53/206], loss=81.9560
	step [54/206], loss=70.4396
	step [55/206], loss=72.2851
	step [56/206], loss=86.3875
	step [57/206], loss=85.8605
	step [58/206], loss=77.1455
	step [59/206], loss=70.2003
	step [60/206], loss=69.8188
	step [61/206], loss=66.8466
	step [62/206], loss=79.2847
	step [63/206], loss=76.3016
	step [64/206], loss=76.7629
	step [65/206], loss=83.1714
	step [66/206], loss=79.1422
	step [67/206], loss=85.5930
	step [68/206], loss=72.0983
	step [69/206], loss=70.4403
	step [70/206], loss=67.4806
	step [71/206], loss=60.8875
	step [72/206], loss=71.9794
	step [73/206], loss=74.3224
	step [74/206], loss=86.7385
	step [75/206], loss=61.1027
	step [76/206], loss=80.9941
	step [77/206], loss=71.4508
	step [78/206], loss=75.8685
	step [79/206], loss=73.6597
	step [80/206], loss=83.3292
	step [81/206], loss=83.0085
	step [82/206], loss=68.2399
	step [83/206], loss=67.0863
	step [84/206], loss=55.5700
	step [85/206], loss=90.0888
	step [86/206], loss=83.8907
	step [87/206], loss=80.6901
	step [88/206], loss=79.7373
	step [89/206], loss=82.3915
	step [90/206], loss=89.6514
	step [91/206], loss=79.6677
	step [92/206], loss=73.0084
	step [93/206], loss=87.5565
	step [94/206], loss=83.4149
	step [95/206], loss=70.5804
	step [96/206], loss=74.1937
	step [97/206], loss=83.8622
	step [98/206], loss=71.4083
	step [99/206], loss=84.2534
	step [100/206], loss=97.0794
	step [101/206], loss=81.7649
	step [102/206], loss=82.3471
	step [103/206], loss=65.3442
	step [104/206], loss=86.9619
	step [105/206], loss=60.1843
	step [106/206], loss=65.2434
	step [107/206], loss=66.8509
	step [108/206], loss=79.4182
	step [109/206], loss=80.1252
	step [110/206], loss=85.6750
	step [111/206], loss=91.2288
	step [112/206], loss=76.5785
	step [113/206], loss=82.6868
	step [114/206], loss=73.2050
	step [115/206], loss=72.5446
	step [116/206], loss=75.5721
	step [117/206], loss=72.6712
	step [118/206], loss=74.7829
	step [119/206], loss=86.0358
	step [120/206], loss=105.5165
	step [121/206], loss=88.5563
	step [122/206], loss=78.2605
	step [123/206], loss=76.4177
	step [124/206], loss=68.2071
	step [125/206], loss=80.6579
	step [126/206], loss=69.1739
	step [127/206], loss=95.2043
	step [128/206], loss=80.8700
	step [129/206], loss=84.8140
	step [130/206], loss=87.6298
	step [131/206], loss=67.0405
	step [132/206], loss=71.8962
	step [133/206], loss=79.1144
	step [134/206], loss=77.2363
	step [135/206], loss=94.8637
	step [136/206], loss=75.4604
	step [137/206], loss=58.7416
	step [138/206], loss=79.1931
	step [139/206], loss=72.7217
	step [140/206], loss=74.6628
	step [141/206], loss=65.6282
	step [142/206], loss=82.9879
	step [143/206], loss=74.9259
	step [144/206], loss=77.0400
	step [145/206], loss=83.9376
	step [146/206], loss=66.0421
	step [147/206], loss=72.1064
	step [148/206], loss=74.6611
	step [149/206], loss=84.3311
	step [150/206], loss=71.6606
	step [151/206], loss=88.9620
	step [152/206], loss=74.9413
	step [153/206], loss=82.5151
	step [154/206], loss=86.5559
	step [155/206], loss=75.1006
	step [156/206], loss=93.9378
	step [157/206], loss=82.5329
	step [158/206], loss=76.0385
	step [159/206], loss=71.9830
	step [160/206], loss=55.9224
	step [161/206], loss=77.0896
	step [162/206], loss=56.4575
	step [163/206], loss=84.8083
	step [164/206], loss=64.3684
	step [165/206], loss=69.3250
	step [166/206], loss=61.8656
	step [167/206], loss=77.7656
	step [168/206], loss=80.2098
	step [169/206], loss=69.2773
	step [170/206], loss=74.2327
	step [171/206], loss=75.9020
	step [172/206], loss=78.5289
	step [173/206], loss=64.8990
	step [174/206], loss=83.0215
	step [175/206], loss=85.9980
	step [176/206], loss=74.1169
	step [177/206], loss=78.6444
	step [178/206], loss=73.7513
	step [179/206], loss=83.1640
	step [180/206], loss=84.1750
	step [181/206], loss=87.0820
	step [182/206], loss=83.8146
	step [183/206], loss=87.3230
	step [184/206], loss=73.2271
	step [185/206], loss=74.4095
	step [186/206], loss=69.7953
	step [187/206], loss=71.2665
	step [188/206], loss=75.0131
	step [189/206], loss=81.1588
	step [190/206], loss=84.5513
	step [191/206], loss=80.8104
	step [192/206], loss=69.2005
	step [193/206], loss=84.2680
	step [194/206], loss=85.2317
	step [195/206], loss=75.0387
	step [196/206], loss=64.9924
	step [197/206], loss=76.0856
	step [198/206], loss=84.5411
	step [199/206], loss=85.9746
	step [200/206], loss=74.1161
	step [201/206], loss=80.2792
	step [202/206], loss=76.5757
	step [203/206], loss=86.1934
	step [204/206], loss=84.6857
	step [205/206], loss=94.1660
	step [206/206], loss=49.2223
	Evaluating
	loss=0.0069, precision=0.4265, recall=0.9066, f1=0.5801
Training epoch 64
	step [1/206], loss=78.8832
	step [2/206], loss=71.6170
	step [3/206], loss=72.0609
	step [4/206], loss=76.0821
	step [5/206], loss=79.7308
	step [6/206], loss=79.4671
	step [7/206], loss=75.3972
	step [8/206], loss=67.8709
	step [9/206], loss=65.8382
	step [10/206], loss=82.9274
	step [11/206], loss=70.0022
	step [12/206], loss=82.9392
	step [13/206], loss=70.9458
	step [14/206], loss=83.9318
	step [15/206], loss=77.5009
	step [16/206], loss=75.1874
	step [17/206], loss=64.4671
	step [18/206], loss=71.7454
	step [19/206], loss=95.1177
	step [20/206], loss=88.3468
	step [21/206], loss=68.7044
	step [22/206], loss=68.8641
	step [23/206], loss=80.5339
	step [24/206], loss=75.0986
	step [25/206], loss=88.2506
	step [26/206], loss=73.5863
	step [27/206], loss=77.6136
	step [28/206], loss=61.8533
	step [29/206], loss=89.1296
	step [30/206], loss=65.0701
	step [31/206], loss=71.1834
	step [32/206], loss=70.6269
	step [33/206], loss=86.2130
	step [34/206], loss=68.5631
	step [35/206], loss=85.2676
	step [36/206], loss=78.2721
	step [37/206], loss=76.7178
	step [38/206], loss=75.8693
	step [39/206], loss=69.7604
	step [40/206], loss=85.1412
	step [41/206], loss=72.2324
	step [42/206], loss=80.3473
	step [43/206], loss=70.1987
	step [44/206], loss=80.7476
	step [45/206], loss=85.9863
	step [46/206], loss=76.1145
	step [47/206], loss=67.1713
	step [48/206], loss=74.5896
	step [49/206], loss=80.4463
	step [50/206], loss=90.3120
	step [51/206], loss=79.0340
	step [52/206], loss=82.3145
	step [53/206], loss=86.1824
	step [54/206], loss=88.1383
	step [55/206], loss=73.6081
	step [56/206], loss=75.4138
	step [57/206], loss=78.2205
	step [58/206], loss=83.5610
	step [59/206], loss=82.8756
	step [60/206], loss=72.7938
	step [61/206], loss=78.9679
	step [62/206], loss=73.9152
	step [63/206], loss=83.4382
	step [64/206], loss=70.5933
	step [65/206], loss=70.0540
	step [66/206], loss=77.4485
	step [67/206], loss=97.0981
	step [68/206], loss=83.0193
	step [69/206], loss=71.3968
	step [70/206], loss=88.5415
	step [71/206], loss=73.1903
	step [72/206], loss=79.1064
	step [73/206], loss=74.7154
	step [74/206], loss=83.1921
	step [75/206], loss=68.7688
	step [76/206], loss=74.8931
	step [77/206], loss=70.8688
	step [78/206], loss=81.3726
	step [79/206], loss=94.9541
	step [80/206], loss=79.2599
	step [81/206], loss=75.8329
	step [82/206], loss=70.6941
	step [83/206], loss=84.9785
	step [84/206], loss=80.5574
	step [85/206], loss=83.8000
	step [86/206], loss=71.5292
	step [87/206], loss=87.5335
	step [88/206], loss=72.1874
	step [89/206], loss=60.4334
	step [90/206], loss=76.5682
	step [91/206], loss=75.1024
	step [92/206], loss=80.0407
	step [93/206], loss=78.1752
	step [94/206], loss=83.8315
	step [95/206], loss=62.7971
	step [96/206], loss=76.6056
	step [97/206], loss=75.7370
	step [98/206], loss=73.6892
	step [99/206], loss=73.4311
	step [100/206], loss=71.9491
	step [101/206], loss=71.8052
	step [102/206], loss=73.6388
	step [103/206], loss=82.0201
	step [104/206], loss=65.7727
	step [105/206], loss=70.6526
	step [106/206], loss=74.2506
	step [107/206], loss=73.4751
	step [108/206], loss=76.7579
	step [109/206], loss=79.8932
	step [110/206], loss=75.8390
	step [111/206], loss=78.3603
	step [112/206], loss=85.0544
	step [113/206], loss=70.4543
	step [114/206], loss=82.2359
	step [115/206], loss=73.3546
	step [116/206], loss=79.6778
	step [117/206], loss=80.2224
	step [118/206], loss=86.0363
	step [119/206], loss=67.2461
	step [120/206], loss=76.0105
	step [121/206], loss=72.6990
	step [122/206], loss=82.4347
	step [123/206], loss=76.2438
	step [124/206], loss=83.5871
	step [125/206], loss=89.8802
	step [126/206], loss=79.3345
	step [127/206], loss=73.0283
	step [128/206], loss=86.0564
	step [129/206], loss=84.3939
	step [130/206], loss=67.1797
	step [131/206], loss=74.7148
	step [132/206], loss=76.6331
	step [133/206], loss=81.4134
	step [134/206], loss=87.6232
	step [135/206], loss=68.0045
	step [136/206], loss=83.2244
	step [137/206], loss=80.5984
	step [138/206], loss=81.7834
	step [139/206], loss=72.2180
	step [140/206], loss=82.6167
	step [141/206], loss=72.4289
	step [142/206], loss=75.5006
	step [143/206], loss=77.0482
	step [144/206], loss=77.4818
	step [145/206], loss=76.8934
	step [146/206], loss=77.8204
	step [147/206], loss=81.8937
	step [148/206], loss=84.4315
	step [149/206], loss=71.2618
	step [150/206], loss=76.0872
	step [151/206], loss=54.2078
	step [152/206], loss=82.9668
	step [153/206], loss=91.0986
	step [154/206], loss=73.7609
	step [155/206], loss=78.3312
	step [156/206], loss=73.1113
	step [157/206], loss=66.2213
	step [158/206], loss=78.8364
	step [159/206], loss=75.6351
	step [160/206], loss=70.9853
	step [161/206], loss=79.6716
	step [162/206], loss=69.1856
	step [163/206], loss=80.9364
	step [164/206], loss=78.2470
	step [165/206], loss=68.8713
	step [166/206], loss=73.3976
	step [167/206], loss=78.5024
	step [168/206], loss=83.4522
	step [169/206], loss=93.6660
	step [170/206], loss=78.7314
	step [171/206], loss=65.5419
	step [172/206], loss=81.3571
	step [173/206], loss=87.5746
	step [174/206], loss=73.5892
	step [175/206], loss=81.4347
	step [176/206], loss=75.7051
	step [177/206], loss=83.7680
	step [178/206], loss=75.3131
	step [179/206], loss=90.7554
	step [180/206], loss=71.1029
	step [181/206], loss=82.3862
	step [182/206], loss=73.7154
	step [183/206], loss=86.0127
	step [184/206], loss=83.6186
	step [185/206], loss=71.1262
	step [186/206], loss=69.6412
	step [187/206], loss=77.4464
	step [188/206], loss=70.9793
	step [189/206], loss=74.0258
	step [190/206], loss=61.3485
	step [191/206], loss=92.1143
	step [192/206], loss=56.5074
	step [193/206], loss=75.5300
	step [194/206], loss=72.1922
	step [195/206], loss=73.1197
	step [196/206], loss=59.5889
	step [197/206], loss=73.3306
	step [198/206], loss=75.2599
	step [199/206], loss=82.5714
	step [200/206], loss=76.7763
	step [201/206], loss=70.6408
	step [202/206], loss=86.6451
	step [203/206], loss=82.5405
	step [204/206], loss=78.9319
	step [205/206], loss=71.3632
	step [206/206], loss=45.3748
	Evaluating
	loss=0.0074, precision=0.4138, recall=0.9100, f1=0.5689
Training epoch 65
	step [1/206], loss=76.6759
	step [2/206], loss=78.0635
	step [3/206], loss=73.1327
	step [4/206], loss=69.5934
	step [5/206], loss=74.9366
	step [6/206], loss=97.4900
	step [7/206], loss=76.8412
	step [8/206], loss=83.8027
	step [9/206], loss=75.6924
	step [10/206], loss=75.1455
	step [11/206], loss=73.5850
	step [12/206], loss=76.0336
	step [13/206], loss=90.8535
	step [14/206], loss=78.3966
	step [15/206], loss=73.7387
	step [16/206], loss=72.7913
	step [17/206], loss=70.6161
	step [18/206], loss=84.6239
	step [19/206], loss=69.7626
	step [20/206], loss=76.5895
	step [21/206], loss=80.1103
	step [22/206], loss=72.8656
	step [23/206], loss=75.0879
	step [24/206], loss=63.8073
	step [25/206], loss=66.0317
	step [26/206], loss=59.5431
	step [27/206], loss=93.1316
	step [28/206], loss=70.6391
	step [29/206], loss=73.1534
	step [30/206], loss=77.5173
	step [31/206], loss=70.4532
	step [32/206], loss=88.1699
	step [33/206], loss=73.5849
	step [34/206], loss=68.9404
	step [35/206], loss=77.5761
	step [36/206], loss=76.5238
	step [37/206], loss=68.6232
	step [38/206], loss=80.0742
	step [39/206], loss=77.3411
	step [40/206], loss=74.9303
	step [41/206], loss=70.9486
	step [42/206], loss=78.4065
	step [43/206], loss=85.2672
	step [44/206], loss=86.0475
	step [45/206], loss=67.7484
	step [46/206], loss=71.5091
	step [47/206], loss=83.3526
	step [48/206], loss=72.1732
	step [49/206], loss=72.9180
	step [50/206], loss=69.8848
	step [51/206], loss=89.7387
	step [52/206], loss=73.7471
	step [53/206], loss=86.7676
	step [54/206], loss=77.3906
	step [55/206], loss=80.9787
	step [56/206], loss=88.9780
	step [57/206], loss=77.9483
	step [58/206], loss=80.3893
	step [59/206], loss=76.1540
	step [60/206], loss=66.1170
	step [61/206], loss=85.8341
	step [62/206], loss=75.1341
	step [63/206], loss=71.3124
	step [64/206], loss=75.5116
	step [65/206], loss=76.2219
	step [66/206], loss=84.9414
	step [67/206], loss=70.5033
	step [68/206], loss=80.5672
	step [69/206], loss=76.7844
	step [70/206], loss=79.2911
	step [71/206], loss=80.8361
	step [72/206], loss=78.3165
	step [73/206], loss=74.7301
	step [74/206], loss=71.5783
	step [75/206], loss=91.8265
	step [76/206], loss=63.5402
	step [77/206], loss=68.7504
	step [78/206], loss=73.0325
	step [79/206], loss=83.2737
	step [80/206], loss=69.1171
	step [81/206], loss=78.7598
	step [82/206], loss=82.2710
	step [83/206], loss=80.5343
	step [84/206], loss=78.8854
	step [85/206], loss=94.6141
	step [86/206], loss=77.0981
	step [87/206], loss=78.0021
	step [88/206], loss=83.2027
	step [89/206], loss=66.2681
	step [90/206], loss=66.0672
	step [91/206], loss=69.4953
	step [92/206], loss=64.3834
	step [93/206], loss=64.5722
	step [94/206], loss=71.0843
	step [95/206], loss=83.0058
	step [96/206], loss=80.4918
	step [97/206], loss=83.9651
	step [98/206], loss=69.1036
	step [99/206], loss=70.3648
	step [100/206], loss=72.3937
	step [101/206], loss=67.9837
	step [102/206], loss=68.8898
	step [103/206], loss=88.3959
	step [104/206], loss=69.0053
	step [105/206], loss=75.4613
	step [106/206], loss=66.6555
	step [107/206], loss=84.4006
	step [108/206], loss=76.2310
	step [109/206], loss=83.4504
	step [110/206], loss=78.1631
	step [111/206], loss=85.9830
	step [112/206], loss=67.4327
	step [113/206], loss=60.4560
	step [114/206], loss=87.9963
	step [115/206], loss=74.8068
	step [116/206], loss=80.3930
	step [117/206], loss=84.0696
	step [118/206], loss=66.5329
	step [119/206], loss=75.2152
	step [120/206], loss=90.0229
	step [121/206], loss=73.7087
	step [122/206], loss=75.6763
	step [123/206], loss=77.3512
	step [124/206], loss=75.4501
	step [125/206], loss=79.8644
	step [126/206], loss=67.0014
	step [127/206], loss=73.5679
	step [128/206], loss=69.3813
	step [129/206], loss=73.4548
	step [130/206], loss=75.3179
	step [131/206], loss=77.8137
	step [132/206], loss=84.5730
	step [133/206], loss=72.8606
	step [134/206], loss=71.1463
	step [135/206], loss=61.1140
	step [136/206], loss=61.5748
	step [137/206], loss=84.7258
	step [138/206], loss=101.6739
	step [139/206], loss=76.7851
	step [140/206], loss=73.1013
	step [141/206], loss=96.2966
	step [142/206], loss=72.6585
	step [143/206], loss=87.8354
	step [144/206], loss=68.4830
	step [145/206], loss=73.7878
	step [146/206], loss=82.2986
	step [147/206], loss=65.4482
	step [148/206], loss=57.5037
	step [149/206], loss=81.2543
	step [150/206], loss=81.3325
	step [151/206], loss=78.0332
	step [152/206], loss=61.9730
	step [153/206], loss=83.7903
	step [154/206], loss=90.6505
	step [155/206], loss=71.7081
	step [156/206], loss=65.7710
	step [157/206], loss=76.7913
	step [158/206], loss=71.0761
	step [159/206], loss=73.2100
	step [160/206], loss=59.1881
	step [161/206], loss=84.3051
	step [162/206], loss=69.2408
	step [163/206], loss=68.9753
	step [164/206], loss=89.3387
	step [165/206], loss=77.7571
	step [166/206], loss=84.9635
	step [167/206], loss=79.8993
	step [168/206], loss=68.6186
	step [169/206], loss=81.3409
	step [170/206], loss=69.8036
	step [171/206], loss=71.5686
	step [172/206], loss=91.6350
	step [173/206], loss=74.4028
	step [174/206], loss=83.3785
	step [175/206], loss=85.2968
	step [176/206], loss=78.1538
	step [177/206], loss=91.9230
	step [178/206], loss=78.3297
	step [179/206], loss=76.7601
	step [180/206], loss=78.2458
	step [181/206], loss=75.1079
	step [182/206], loss=84.1810
	step [183/206], loss=69.6320
	step [184/206], loss=89.3538
	step [185/206], loss=84.8979
	step [186/206], loss=76.5737
	step [187/206], loss=77.6296
	step [188/206], loss=84.0364
	step [189/206], loss=78.3059
	step [190/206], loss=78.4969
	step [191/206], loss=69.6500
	step [192/206], loss=72.0416
	step [193/206], loss=70.8484
	step [194/206], loss=65.0683
	step [195/206], loss=83.7334
	step [196/206], loss=88.4079
	step [197/206], loss=73.6704
	step [198/206], loss=78.6901
	step [199/206], loss=82.6080
	step [200/206], loss=74.5826
	step [201/206], loss=68.8145
	step [202/206], loss=85.9540
	step [203/206], loss=77.4461
	step [204/206], loss=73.8264
	step [205/206], loss=71.2936
	step [206/206], loss=60.8005
	Evaluating
	loss=0.0073, precision=0.4061, recall=0.8992, f1=0.5595
Training epoch 66
	step [1/206], loss=79.7759
	step [2/206], loss=74.5867
	step [3/206], loss=80.1676
	step [4/206], loss=73.4680
	step [5/206], loss=76.7425
	step [6/206], loss=77.2906
	step [7/206], loss=86.6319
	step [8/206], loss=69.8267
	step [9/206], loss=84.0836
	step [10/206], loss=71.1565
	step [11/206], loss=81.0151
	step [12/206], loss=91.1645
	step [13/206], loss=82.1686
	step [14/206], loss=72.7788
	step [15/206], loss=81.3422
	step [16/206], loss=70.2075
	step [17/206], loss=77.5752
	step [18/206], loss=77.9369
	step [19/206], loss=76.5745
	step [20/206], loss=80.6878
	step [21/206], loss=76.1348
	step [22/206], loss=95.3801
	step [23/206], loss=75.7549
	step [24/206], loss=76.5643
	step [25/206], loss=71.3103
	step [26/206], loss=75.4053
	step [27/206], loss=84.7051
	step [28/206], loss=72.4809
	step [29/206], loss=83.4869
	step [30/206], loss=84.5245
	step [31/206], loss=72.1066
	step [32/206], loss=72.2541
	step [33/206], loss=76.7790
	step [34/206], loss=79.3445
	step [35/206], loss=75.4384
	step [36/206], loss=75.5199
	step [37/206], loss=73.7220
	step [38/206], loss=73.9557
	step [39/206], loss=73.6730
	step [40/206], loss=65.3113
	step [41/206], loss=73.3160
	step [42/206], loss=85.8177
	step [43/206], loss=64.8420
	step [44/206], loss=66.7671
	step [45/206], loss=74.7387
	step [46/206], loss=57.9127
	step [47/206], loss=72.3382
	step [48/206], loss=67.0095
	step [49/206], loss=76.4258
	step [50/206], loss=66.5029
	step [51/206], loss=77.4081
	step [52/206], loss=81.0321
	step [53/206], loss=81.7140
	step [54/206], loss=71.0990
	step [55/206], loss=71.8360
	step [56/206], loss=73.0086
	step [57/206], loss=76.8472
	step [58/206], loss=80.6805
	step [59/206], loss=72.5550
	step [60/206], loss=75.3493
	step [61/206], loss=70.6164
	step [62/206], loss=81.0573
	step [63/206], loss=66.2119
	step [64/206], loss=73.0574
	step [65/206], loss=85.4322
	step [66/206], loss=64.7226
	step [67/206], loss=74.2027
	step [68/206], loss=90.1702
	step [69/206], loss=74.0987
	step [70/206], loss=81.3398
	step [71/206], loss=75.8712
	step [72/206], loss=67.4618
	step [73/206], loss=78.9851
	step [74/206], loss=88.2591
	step [75/206], loss=74.0416
	step [76/206], loss=73.0150
	step [77/206], loss=86.2362
	step [78/206], loss=63.1984
	step [79/206], loss=74.6909
	step [80/206], loss=72.7824
	step [81/206], loss=89.7870
	step [82/206], loss=70.3985
	step [83/206], loss=65.4305
	step [84/206], loss=72.9820
	step [85/206], loss=77.8905
	step [86/206], loss=65.4408
	step [87/206], loss=73.5935
	step [88/206], loss=88.6218
	step [89/206], loss=69.5554
	step [90/206], loss=56.2597
	step [91/206], loss=70.1983
	step [92/206], loss=68.1877
	step [93/206], loss=79.2531
	step [94/206], loss=72.4069
	step [95/206], loss=79.5958
	step [96/206], loss=69.4682
	step [97/206], loss=79.0126
	step [98/206], loss=77.8077
	step [99/206], loss=75.9882
	step [100/206], loss=79.3719
	step [101/206], loss=72.8696
	step [102/206], loss=77.5895
	step [103/206], loss=79.6687
	step [104/206], loss=72.6580
	step [105/206], loss=69.4169
	step [106/206], loss=62.9961
	step [107/206], loss=68.8352
	step [108/206], loss=79.0760
	step [109/206], loss=82.9731
	step [110/206], loss=86.5865
	step [111/206], loss=69.9288
	step [112/206], loss=86.2172
	step [113/206], loss=80.7757
	step [114/206], loss=77.4591
	step [115/206], loss=75.7095
	step [116/206], loss=71.4652
	step [117/206], loss=85.7610
	step [118/206], loss=81.6840
	step [119/206], loss=78.7236
	step [120/206], loss=69.6209
	step [121/206], loss=86.5491
	step [122/206], loss=75.2269
	step [123/206], loss=77.8327
	step [124/206], loss=60.6627
	step [125/206], loss=82.3007
	step [126/206], loss=75.0264
	step [127/206], loss=69.7730
	step [128/206], loss=85.6676
	step [129/206], loss=63.8165
	step [130/206], loss=91.3970
	step [131/206], loss=69.2240
	step [132/206], loss=64.8721
	step [133/206], loss=87.6206
	step [134/206], loss=85.7632
	step [135/206], loss=67.9383
	step [136/206], loss=86.4808
	step [137/206], loss=68.6903
	step [138/206], loss=78.3063
	step [139/206], loss=82.3697
	step [140/206], loss=69.6360
	step [141/206], loss=79.6605
	step [142/206], loss=78.3302
	step [143/206], loss=69.4792
	step [144/206], loss=82.4671
	step [145/206], loss=82.8667
	step [146/206], loss=71.6499
	step [147/206], loss=73.1338
	step [148/206], loss=64.3249
	step [149/206], loss=70.4324
	step [150/206], loss=66.9919
	step [151/206], loss=75.6591
	step [152/206], loss=92.2793
	step [153/206], loss=75.5511
	step [154/206], loss=79.5037
	step [155/206], loss=63.2047
	step [156/206], loss=71.6728
	step [157/206], loss=70.0500
	step [158/206], loss=67.2152
	step [159/206], loss=81.7250
	step [160/206], loss=94.0132
	step [161/206], loss=81.0892
	step [162/206], loss=85.5087
	step [163/206], loss=80.2778
	step [164/206], loss=77.7229
	step [165/206], loss=73.5640
	step [166/206], loss=69.1686
	step [167/206], loss=73.4621
	step [168/206], loss=75.7856
	step [169/206], loss=69.8284
	step [170/206], loss=85.8126
	step [171/206], loss=78.7175
	step [172/206], loss=79.8353
	step [173/206], loss=78.8099
	step [174/206], loss=84.3533
	step [175/206], loss=77.7285
	step [176/206], loss=72.0767
	step [177/206], loss=66.0545
	step [178/206], loss=76.9375
	step [179/206], loss=80.8194
	step [180/206], loss=78.4211
	step [181/206], loss=85.0470
	step [182/206], loss=71.2503
	step [183/206], loss=68.5667
	step [184/206], loss=65.5017
	step [185/206], loss=82.2372
	step [186/206], loss=85.1372
	step [187/206], loss=76.6703
	step [188/206], loss=73.2593
	step [189/206], loss=73.8642
	step [190/206], loss=81.5354
	step [191/206], loss=67.6395
	step [192/206], loss=80.5307
	step [193/206], loss=66.0434
	step [194/206], loss=84.5077
	step [195/206], loss=80.2164
	step [196/206], loss=75.6674
	step [197/206], loss=67.4073
	step [198/206], loss=73.9285
	step [199/206], loss=77.8506
	step [200/206], loss=78.5516
	step [201/206], loss=94.2025
	step [202/206], loss=95.3569
	step [203/206], loss=70.5508
	step [204/206], loss=93.1522
	step [205/206], loss=80.1334
	step [206/206], loss=56.5400
	Evaluating
	loss=0.0069, precision=0.4118, recall=0.8968, f1=0.5644
Training epoch 67
	step [1/206], loss=80.4461
	step [2/206], loss=81.9001
	step [3/206], loss=81.1135
	step [4/206], loss=63.8077
	step [5/206], loss=73.6679
	step [6/206], loss=73.8185
	step [7/206], loss=68.6201
	step [8/206], loss=72.4883
	step [9/206], loss=71.7720
	step [10/206], loss=83.3465
	step [11/206], loss=72.1914
	step [12/206], loss=61.0353
	step [13/206], loss=67.3953
	step [14/206], loss=82.4162
	step [15/206], loss=80.9996
	step [16/206], loss=67.5229
	step [17/206], loss=72.0035
	step [18/206], loss=71.1209
	step [19/206], loss=74.3016
	step [20/206], loss=80.2454
	step [21/206], loss=72.3152
	step [22/206], loss=87.2738
	step [23/206], loss=85.2861
	step [24/206], loss=78.4525
	step [25/206], loss=70.7658
	step [26/206], loss=87.2668
	step [27/206], loss=83.5354
	step [28/206], loss=77.5084
	step [29/206], loss=72.8069
	step [30/206], loss=79.6122
	step [31/206], loss=91.2500
	step [32/206], loss=79.2737
	step [33/206], loss=80.0457
	step [34/206], loss=64.1273
	step [35/206], loss=76.8167
	step [36/206], loss=74.2101
	step [37/206], loss=71.0786
	step [38/206], loss=70.3868
	step [39/206], loss=60.9921
	step [40/206], loss=72.9282
	step [41/206], loss=63.4730
	step [42/206], loss=68.9458
	step [43/206], loss=87.6001
	step [44/206], loss=77.4780
	step [45/206], loss=70.2197
	step [46/206], loss=79.2030
	step [47/206], loss=63.0700
	step [48/206], loss=84.6718
	step [49/206], loss=87.2835
	step [50/206], loss=61.2660
	step [51/206], loss=73.8238
	step [52/206], loss=72.7595
	step [53/206], loss=67.2840
	step [54/206], loss=84.4738
	step [55/206], loss=77.8221
	step [56/206], loss=81.7639
	step [57/206], loss=65.6739
	step [58/206], loss=90.8783
	step [59/206], loss=97.8349
	step [60/206], loss=69.2555
	step [61/206], loss=79.1244
	step [62/206], loss=80.5763
	step [63/206], loss=93.3479
	step [64/206], loss=81.6213
	step [65/206], loss=83.1334
	step [66/206], loss=76.8249
	step [67/206], loss=71.9580
	step [68/206], loss=74.8330
	step [69/206], loss=79.6999
	step [70/206], loss=80.4058
	step [71/206], loss=83.8216
	step [72/206], loss=76.2199
	step [73/206], loss=77.6688
	step [74/206], loss=77.6524
	step [75/206], loss=67.6484
	step [76/206], loss=78.4996
	step [77/206], loss=73.2681
	step [78/206], loss=73.9952
	step [79/206], loss=70.4512
	step [80/206], loss=76.8313
	step [81/206], loss=75.1205
	step [82/206], loss=73.0928
	step [83/206], loss=69.9106
	step [84/206], loss=89.8396
	step [85/206], loss=74.2308
	step [86/206], loss=78.0744
	step [87/206], loss=64.6191
	step [88/206], loss=84.0979
	step [89/206], loss=77.1637
	step [90/206], loss=77.1220
	step [91/206], loss=77.1749
	step [92/206], loss=69.0387
	step [93/206], loss=74.5394
	step [94/206], loss=75.3317
	step [95/206], loss=72.8302
	step [96/206], loss=70.3346
	step [97/206], loss=89.6162
	step [98/206], loss=69.3308
	step [99/206], loss=75.4413
	step [100/206], loss=79.7987
	step [101/206], loss=77.4257
	step [102/206], loss=88.2981
	step [103/206], loss=71.2090
	step [104/206], loss=81.3616
	step [105/206], loss=80.5648
	step [106/206], loss=81.9591
	step [107/206], loss=71.7296
	step [108/206], loss=65.1003
	step [109/206], loss=68.2888
	step [110/206], loss=90.4259
	step [111/206], loss=69.5187
	step [112/206], loss=68.1042
	step [113/206], loss=83.2344
	step [114/206], loss=87.2075
	step [115/206], loss=77.7289
	step [116/206], loss=72.0394
	step [117/206], loss=75.6806
	step [118/206], loss=77.3908
	step [119/206], loss=84.0404
	step [120/206], loss=76.9000
	step [121/206], loss=82.7574
	step [122/206], loss=77.6177
	step [123/206], loss=68.5322
	step [124/206], loss=68.6492
	step [125/206], loss=69.2936
	step [126/206], loss=67.9076
	step [127/206], loss=77.4690
	step [128/206], loss=72.6046
	step [129/206], loss=82.3484
	step [130/206], loss=81.2286
	step [131/206], loss=78.3510
	step [132/206], loss=67.9350
	step [133/206], loss=83.4637
	step [134/206], loss=63.5954
	step [135/206], loss=64.3442
	step [136/206], loss=85.1073
	step [137/206], loss=79.4022
	step [138/206], loss=68.2134
	step [139/206], loss=74.6497
	step [140/206], loss=82.4511
	step [141/206], loss=74.1292
	step [142/206], loss=65.6403
	step [143/206], loss=75.7960
	step [144/206], loss=72.5938
	step [145/206], loss=65.0532
	step [146/206], loss=65.5724
	step [147/206], loss=66.1847
	step [148/206], loss=86.1688
	step [149/206], loss=76.0737
	step [150/206], loss=69.0815
	step [151/206], loss=75.0151
	step [152/206], loss=73.6083
	step [153/206], loss=84.4166
	step [154/206], loss=79.6035
	step [155/206], loss=83.3256
	step [156/206], loss=88.8245
	step [157/206], loss=66.2102
	step [158/206], loss=74.2486
	step [159/206], loss=71.7606
	step [160/206], loss=69.6489
	step [161/206], loss=70.4990
	step [162/206], loss=88.0633
	step [163/206], loss=72.8640
	step [164/206], loss=98.4025
	step [165/206], loss=72.2701
	step [166/206], loss=74.0437
	step [167/206], loss=91.6691
	step [168/206], loss=76.7966
	step [169/206], loss=83.8556
	step [170/206], loss=66.2202
	step [171/206], loss=68.9333
	step [172/206], loss=76.4999
	step [173/206], loss=76.3967
	step [174/206], loss=87.5698
	step [175/206], loss=71.5088
	step [176/206], loss=63.7566
	step [177/206], loss=90.1737
	step [178/206], loss=80.2490
	step [179/206], loss=75.5462
	step [180/206], loss=74.7221
	step [181/206], loss=69.5174
	step [182/206], loss=69.7100
	step [183/206], loss=84.1821
	step [184/206], loss=79.9737
	step [185/206], loss=80.5857
	step [186/206], loss=68.3433
	step [187/206], loss=60.6744
	step [188/206], loss=76.0606
	step [189/206], loss=83.8174
	step [190/206], loss=75.9501
	step [191/206], loss=74.6045
	step [192/206], loss=85.2608
	step [193/206], loss=74.0326
	step [194/206], loss=80.9862
	step [195/206], loss=68.0609
	step [196/206], loss=86.6502
	step [197/206], loss=71.8467
	step [198/206], loss=70.3131
	step [199/206], loss=75.3086
	step [200/206], loss=80.7590
	step [201/206], loss=99.9241
	step [202/206], loss=77.9164
	step [203/206], loss=73.0179
	step [204/206], loss=74.3381
	step [205/206], loss=83.9181
	step [206/206], loss=61.9721
	Evaluating
	loss=0.0063, precision=0.4454, recall=0.9016, f1=0.5962
saving model as: 4_saved_model.pth
Training epoch 68
	step [1/206], loss=80.8994
	step [2/206], loss=70.9210
	step [3/206], loss=66.8326
	step [4/206], loss=77.7177
	step [5/206], loss=70.1577
	step [6/206], loss=79.8326
	step [7/206], loss=64.3320
	step [8/206], loss=74.4508
	step [9/206], loss=71.2261
	step [10/206], loss=82.2517
	step [11/206], loss=70.8179
	step [12/206], loss=83.7683
	step [13/206], loss=78.3617
	step [14/206], loss=87.4735
	step [15/206], loss=71.1663
	step [16/206], loss=72.3143
	step [17/206], loss=82.1252
	step [18/206], loss=72.3773
	step [19/206], loss=91.5241
	step [20/206], loss=81.6320
	step [21/206], loss=73.8818
	step [22/206], loss=90.0992
	step [23/206], loss=67.2857
	step [24/206], loss=79.3585
	step [25/206], loss=87.7823
	step [26/206], loss=64.5704
	step [27/206], loss=64.2006
	step [28/206], loss=76.3180
	step [29/206], loss=82.3129
	step [30/206], loss=85.5977
	step [31/206], loss=87.0200
	step [32/206], loss=83.0695
	step [33/206], loss=67.4730
	step [34/206], loss=64.6158
	step [35/206], loss=82.6413
	step [36/206], loss=67.4598
	step [37/206], loss=55.9537
	step [38/206], loss=75.5607
	step [39/206], loss=78.8143
	step [40/206], loss=62.1201
	step [41/206], loss=66.6313
	step [42/206], loss=79.7348
	step [43/206], loss=62.4880
	step [44/206], loss=82.8353
	step [45/206], loss=73.2444
	step [46/206], loss=75.7058
	step [47/206], loss=79.7015
	step [48/206], loss=64.0717
	step [49/206], loss=66.3782
	step [50/206], loss=54.3559
	step [51/206], loss=76.2082
	step [52/206], loss=81.7426
	step [53/206], loss=77.1143
	step [54/206], loss=78.7800
	step [55/206], loss=75.9538
	step [56/206], loss=76.2619
	step [57/206], loss=75.9286
	step [58/206], loss=79.9773
	step [59/206], loss=68.2086
	step [60/206], loss=69.5110
	step [61/206], loss=78.5571
	step [62/206], loss=78.6459
	step [63/206], loss=76.6593
	step [64/206], loss=81.7793
	step [65/206], loss=85.3707
	step [66/206], loss=80.0032
	step [67/206], loss=70.0334
	step [68/206], loss=70.0461
	step [69/206], loss=58.7820
	step [70/206], loss=80.8456
	step [71/206], loss=65.0826
	step [72/206], loss=75.7100
	step [73/206], loss=85.4318
	step [74/206], loss=73.7420
	step [75/206], loss=72.7932
	step [76/206], loss=84.1481
	step [77/206], loss=79.1082
	step [78/206], loss=73.3887
	step [79/206], loss=65.9528
	step [80/206], loss=81.7975
	step [81/206], loss=72.3266
	step [82/206], loss=70.9032
	step [83/206], loss=88.5774
	step [84/206], loss=74.1111
	step [85/206], loss=89.5043
	step [86/206], loss=81.4917
	step [87/206], loss=90.6401
	step [88/206], loss=79.2757
	step [89/206], loss=83.6942
	step [90/206], loss=69.3793
	step [91/206], loss=81.6805
	step [92/206], loss=70.4361
	step [93/206], loss=67.3481
	step [94/206], loss=88.4100
	step [95/206], loss=79.8691
	step [96/206], loss=82.3860
	step [97/206], loss=75.9982
	step [98/206], loss=79.9485
	step [99/206], loss=84.7462
	step [100/206], loss=71.9781
	step [101/206], loss=78.5481
	step [102/206], loss=75.0752
	step [103/206], loss=77.9144
	step [104/206], loss=81.0546
	step [105/206], loss=77.3211
	step [106/206], loss=79.1775
	step [107/206], loss=88.0488
	step [108/206], loss=61.9841
	step [109/206], loss=78.4834
	step [110/206], loss=83.1383
	step [111/206], loss=58.0254
	step [112/206], loss=76.1920
	step [113/206], loss=75.1738
	step [114/206], loss=67.5326
	step [115/206], loss=83.1275
	step [116/206], loss=84.7152
	step [117/206], loss=79.4341
	step [118/206], loss=76.7732
	step [119/206], loss=89.0704
	step [120/206], loss=76.9042
	step [121/206], loss=73.5174
	step [122/206], loss=76.7770
	step [123/206], loss=69.3604
	step [124/206], loss=64.1218
	step [125/206], loss=77.9405
	step [126/206], loss=75.0602
	step [127/206], loss=72.8721
	step [128/206], loss=79.1860
	step [129/206], loss=74.0209
	step [130/206], loss=71.5437
	step [131/206], loss=81.8299
	step [132/206], loss=71.3666
	step [133/206], loss=69.1163
	step [134/206], loss=86.8857
	step [135/206], loss=71.7702
	step [136/206], loss=63.0066
	step [137/206], loss=79.5843
	step [138/206], loss=75.1820
	step [139/206], loss=68.6337
	step [140/206], loss=83.1196
	step [141/206], loss=72.7523
	step [142/206], loss=73.0656
	step [143/206], loss=88.6354
	step [144/206], loss=66.4635
	step [145/206], loss=78.7069
	step [146/206], loss=64.6271
	step [147/206], loss=66.2415
	step [148/206], loss=81.1077
	step [149/206], loss=84.6327
	step [150/206], loss=69.2646
	step [151/206], loss=61.4312
	step [152/206], loss=66.2000
	step [153/206], loss=68.5693
	step [154/206], loss=75.2491
	step [155/206], loss=79.8631
	step [156/206], loss=79.1831
	step [157/206], loss=75.0528
	step [158/206], loss=71.8237
	step [159/206], loss=80.5265
	step [160/206], loss=93.1965
	step [161/206], loss=80.8673
	step [162/206], loss=68.0366
	step [163/206], loss=85.9888
	step [164/206], loss=88.8409
	step [165/206], loss=79.8714
	step [166/206], loss=65.4034
	step [167/206], loss=79.2309
	step [168/206], loss=90.9133
	step [169/206], loss=78.3401
	step [170/206], loss=64.1437
	step [171/206], loss=91.0164
	step [172/206], loss=73.3966
	step [173/206], loss=84.9201
	step [174/206], loss=69.0455
	step [175/206], loss=80.5220
	step [176/206], loss=63.0199
	step [177/206], loss=61.4301
	step [178/206], loss=81.4225
	step [179/206], loss=83.5095
	step [180/206], loss=72.8446
	step [181/206], loss=73.8228
	step [182/206], loss=73.6016
	step [183/206], loss=73.3739
	step [184/206], loss=76.5997
	step [185/206], loss=84.1731
	step [186/206], loss=74.7346
	step [187/206], loss=79.3359
	step [188/206], loss=84.3397
	step [189/206], loss=73.7701
	step [190/206], loss=89.7763
	step [191/206], loss=77.8647
	step [192/206], loss=81.8986
	step [193/206], loss=71.0041
	step [194/206], loss=67.3312
	step [195/206], loss=77.5396
	step [196/206], loss=74.8624
	step [197/206], loss=83.3440
	step [198/206], loss=62.0567
	step [199/206], loss=82.5729
	step [200/206], loss=75.0122
	step [201/206], loss=59.1727
	step [202/206], loss=75.3983
	step [203/206], loss=76.9444
	step [204/206], loss=72.6375
	step [205/206], loss=83.0316
	step [206/206], loss=41.8494
	Evaluating
	loss=0.0071, precision=0.4231, recall=0.9046, f1=0.5765
Training epoch 69
	step [1/206], loss=75.4571
	step [2/206], loss=82.4336
	step [3/206], loss=90.0153
	step [4/206], loss=75.6885
	step [5/206], loss=63.2790
	step [6/206], loss=75.6433
	step [7/206], loss=88.4849
	step [8/206], loss=74.9940
	step [9/206], loss=77.8582
	step [10/206], loss=71.8985
	step [11/206], loss=67.7283
	step [12/206], loss=77.4137
	step [13/206], loss=72.0184
	step [14/206], loss=68.3388
	step [15/206], loss=86.7745
	step [16/206], loss=73.0944
	step [17/206], loss=74.8529
	step [18/206], loss=81.2107
	step [19/206], loss=67.9131
	step [20/206], loss=73.1777
	step [21/206], loss=74.0714
	step [22/206], loss=81.2761
	step [23/206], loss=69.2160
	step [24/206], loss=60.7172
	step [25/206], loss=83.6280
	step [26/206], loss=74.7155
	step [27/206], loss=83.4118
	step [28/206], loss=68.1436
	step [29/206], loss=80.7891
	step [30/206], loss=69.1031
	step [31/206], loss=66.0931
	step [32/206], loss=62.7663
	step [33/206], loss=65.8056
	step [34/206], loss=81.9584
	step [35/206], loss=74.6344
	step [36/206], loss=67.3258
	step [37/206], loss=82.2629
	step [38/206], loss=83.6453
	step [39/206], loss=89.3796
	step [40/206], loss=84.1405
	step [41/206], loss=78.2131
	step [42/206], loss=76.0019
	step [43/206], loss=80.0197
	step [44/206], loss=66.8920
	step [45/206], loss=73.4357
	step [46/206], loss=83.0438
	step [47/206], loss=70.8157
	step [48/206], loss=74.2868
	step [49/206], loss=66.6962
	step [50/206], loss=72.9041
	step [51/206], loss=66.0668
	step [52/206], loss=63.2512
	step [53/206], loss=77.8182
	step [54/206], loss=74.8325
	step [55/206], loss=66.6714
	step [56/206], loss=88.8440
	step [57/206], loss=95.1294
	step [58/206], loss=68.6316
	step [59/206], loss=84.5399
	step [60/206], loss=66.0319
	step [61/206], loss=86.6575
	step [62/206], loss=72.7946
	step [63/206], loss=70.3311
	step [64/206], loss=75.8257
	step [65/206], loss=87.4299
	step [66/206], loss=82.5830
	step [67/206], loss=83.8638
	step [68/206], loss=80.1997
	step [69/206], loss=80.6598
	step [70/206], loss=85.8792
	step [71/206], loss=70.1517
	step [72/206], loss=75.2851
	step [73/206], loss=61.4555
	step [74/206], loss=67.6132
	step [75/206], loss=79.8804
	step [76/206], loss=85.4578
	step [77/206], loss=78.8462
	step [78/206], loss=78.9319
	step [79/206], loss=72.2514
	step [80/206], loss=77.3326
	step [81/206], loss=73.7401
	step [82/206], loss=60.6284
	step [83/206], loss=67.2995
	step [84/206], loss=68.8790
	step [85/206], loss=85.3445
	step [86/206], loss=70.4932
	step [87/206], loss=84.3172
	step [88/206], loss=68.3418
	step [89/206], loss=81.8418
	step [90/206], loss=78.0329
	step [91/206], loss=61.1935
	step [92/206], loss=69.0476
	step [93/206], loss=77.1264
	step [94/206], loss=69.1379
	step [95/206], loss=70.7240
	step [96/206], loss=73.5120
	step [97/206], loss=71.6652
	step [98/206], loss=77.1631
	step [99/206], loss=86.1714
	step [100/206], loss=66.2357
	step [101/206], loss=63.2154
	step [102/206], loss=72.3357
	step [103/206], loss=72.0126
	step [104/206], loss=76.3026
	step [105/206], loss=70.8297
	step [106/206], loss=70.0536
	step [107/206], loss=88.7492
	step [108/206], loss=65.1301
	step [109/206], loss=73.5414
	step [110/206], loss=69.6232
	step [111/206], loss=78.5621
	step [112/206], loss=70.6944
	step [113/206], loss=81.4574
	step [114/206], loss=89.1757
	step [115/206], loss=74.0607
	step [116/206], loss=78.1830
	step [117/206], loss=64.3713
	step [118/206], loss=79.7461
	step [119/206], loss=70.5994
	step [120/206], loss=74.2137
	step [121/206], loss=71.4524
	step [122/206], loss=71.6656
	step [123/206], loss=86.8812
	step [124/206], loss=86.3976
	step [125/206], loss=77.5867
	step [126/206], loss=77.2219
	step [127/206], loss=88.6218
	step [128/206], loss=63.0911
	step [129/206], loss=75.7486
	step [130/206], loss=78.9847
	step [131/206], loss=68.2150
	step [132/206], loss=70.6357
	step [133/206], loss=75.0208
	step [134/206], loss=76.9389
	step [135/206], loss=77.6368
	step [136/206], loss=79.4668
	step [137/206], loss=70.4131
	step [138/206], loss=77.6916
	step [139/206], loss=60.4938
	step [140/206], loss=67.1537
	step [141/206], loss=86.4789
	step [142/206], loss=84.2850
	step [143/206], loss=69.1873
	step [144/206], loss=76.6067
	step [145/206], loss=74.1564
	step [146/206], loss=77.3812
	step [147/206], loss=75.9482
	step [148/206], loss=77.7686
	step [149/206], loss=80.5644
	step [150/206], loss=69.9445
	step [151/206], loss=71.7854
	step [152/206], loss=92.4867
	step [153/206], loss=83.8307
	step [154/206], loss=90.0598
	step [155/206], loss=74.0524
	step [156/206], loss=89.8571
	step [157/206], loss=84.2097
	step [158/206], loss=67.5504
	step [159/206], loss=71.1267
	step [160/206], loss=78.3048
	step [161/206], loss=85.4209
	step [162/206], loss=72.7885
	step [163/206], loss=78.4595
	step [164/206], loss=76.7783
	step [165/206], loss=87.7866
	step [166/206], loss=70.8035
	step [167/206], loss=66.9420
	step [168/206], loss=75.1377
	step [169/206], loss=74.9117
	step [170/206], loss=78.8189
	step [171/206], loss=85.7153
	step [172/206], loss=75.2103
	step [173/206], loss=69.4062
	step [174/206], loss=58.6720
	step [175/206], loss=76.5663
	step [176/206], loss=71.9202
	step [177/206], loss=78.9895
	step [178/206], loss=81.9503
	step [179/206], loss=89.7451
	step [180/206], loss=77.7828
	step [181/206], loss=74.6951
	step [182/206], loss=88.2086
	step [183/206], loss=66.2011
	step [184/206], loss=76.6442
	step [185/206], loss=71.9221
	step [186/206], loss=76.8744
	step [187/206], loss=76.0158
	step [188/206], loss=75.6142
	step [189/206], loss=72.0518
	step [190/206], loss=67.3099
	step [191/206], loss=81.8131
	step [192/206], loss=77.9423
	step [193/206], loss=83.3536
	step [194/206], loss=83.2471
	step [195/206], loss=63.6756
	step [196/206], loss=80.3447
	step [197/206], loss=69.5556
	step [198/206], loss=69.3262
	step [199/206], loss=80.9915
	step [200/206], loss=81.1304
	step [201/206], loss=82.8423
	step [202/206], loss=74.5115
	step [203/206], loss=86.3038
	step [204/206], loss=77.1743
	step [205/206], loss=76.9066
	step [206/206], loss=56.8633
	Evaluating
	loss=0.0071, precision=0.4056, recall=0.9013, f1=0.5594
Training epoch 70
	step [1/206], loss=75.1632
	step [2/206], loss=81.4607
	step [3/206], loss=68.2415
	step [4/206], loss=80.5117
	step [5/206], loss=75.0762
	step [6/206], loss=79.3234
	step [7/206], loss=80.9879
	step [8/206], loss=70.8776
	step [9/206], loss=79.9058
	step [10/206], loss=74.0328
	step [11/206], loss=76.8562
	step [12/206], loss=70.1230
	step [13/206], loss=69.2585
	step [14/206], loss=67.1910
	step [15/206], loss=75.3409
	step [16/206], loss=86.4429
	step [17/206], loss=82.9670
	step [18/206], loss=84.8051
	step [19/206], loss=81.1897
	step [20/206], loss=79.0917
	step [21/206], loss=80.3107
	step [22/206], loss=81.5658
	step [23/206], loss=71.9273
	step [24/206], loss=87.1345
	step [25/206], loss=76.7819
	step [26/206], loss=74.1614
	step [27/206], loss=77.3576
	step [28/206], loss=66.1923
	step [29/206], loss=88.7350
	step [30/206], loss=82.6553
	step [31/206], loss=76.3222
	step [32/206], loss=83.3434
	step [33/206], loss=77.6096
	step [34/206], loss=78.5369
	step [35/206], loss=88.6300
	step [36/206], loss=79.4818
	step [37/206], loss=73.0532
	step [38/206], loss=60.4897
	step [39/206], loss=74.7677
	step [40/206], loss=69.4429
	step [41/206], loss=68.2044
	step [42/206], loss=84.3709
	step [43/206], loss=68.4376
	step [44/206], loss=94.8722
	step [45/206], loss=65.6473
	step [46/206], loss=71.1414
	step [47/206], loss=79.1711
	step [48/206], loss=74.0924
	step [49/206], loss=76.2671
	step [50/206], loss=82.4839
	step [51/206], loss=78.2307
	step [52/206], loss=69.7695
	step [53/206], loss=72.2423
	step [54/206], loss=66.0598
	step [55/206], loss=76.9967
	step [56/206], loss=82.1839
	step [57/206], loss=69.7244
	step [58/206], loss=84.4578
	step [59/206], loss=86.4134
	step [60/206], loss=80.4090
	step [61/206], loss=72.0176
	step [62/206], loss=88.5570
	step [63/206], loss=82.2476
	step [64/206], loss=92.5783
	step [65/206], loss=65.7286
	step [66/206], loss=86.8813
	step [67/206], loss=65.1195
	step [68/206], loss=71.6504
	step [69/206], loss=69.3730
	step [70/206], loss=67.8647
	step [71/206], loss=69.0679
	step [72/206], loss=67.0107
	step [73/206], loss=83.7352
	step [74/206], loss=62.2949
	step [75/206], loss=67.3647
	step [76/206], loss=72.3758
	step [77/206], loss=71.1997
	step [78/206], loss=73.4259
	step [79/206], loss=83.0485
	step [80/206], loss=76.2109
	step [81/206], loss=71.9360
	step [82/206], loss=68.9847
	step [83/206], loss=68.7696
	step [84/206], loss=82.8682
	step [85/206], loss=83.7875
	step [86/206], loss=98.1087
	step [87/206], loss=70.3955
	step [88/206], loss=52.1875
	step [89/206], loss=77.8965
	step [90/206], loss=64.7074
	step [91/206], loss=66.2417
	step [92/206], loss=75.3377
	step [93/206], loss=73.9910
	step [94/206], loss=75.9535
	step [95/206], loss=80.8268
	step [96/206], loss=81.6379
	step [97/206], loss=80.2905
	step [98/206], loss=71.4452
	step [99/206], loss=71.0769
	step [100/206], loss=72.2089
	step [101/206], loss=65.5232
	step [102/206], loss=88.4775
	step [103/206], loss=71.8675
	step [104/206], loss=77.9630
	step [105/206], loss=73.7737
	step [106/206], loss=73.5162
	step [107/206], loss=79.2313
	step [108/206], loss=82.8955
	step [109/206], loss=84.5342
	step [110/206], loss=81.3662
	step [111/206], loss=68.1608
	step [112/206], loss=79.4154
	step [113/206], loss=70.9757
	step [114/206], loss=79.1336
	step [115/206], loss=65.6447
	step [116/206], loss=78.2082
	step [117/206], loss=76.9669
	step [118/206], loss=74.4730
	step [119/206], loss=71.8970
	step [120/206], loss=71.6084
	step [121/206], loss=73.8505
	step [122/206], loss=95.9483
	step [123/206], loss=65.9805
	step [124/206], loss=73.8564
	step [125/206], loss=75.9838
	step [126/206], loss=72.8706
	step [127/206], loss=70.8482
	step [128/206], loss=72.8372
	step [129/206], loss=66.1326
	step [130/206], loss=69.6161
	step [131/206], loss=64.1434
	step [132/206], loss=79.9390
	step [133/206], loss=98.3275
	step [134/206], loss=73.7916
	step [135/206], loss=84.6363
	step [136/206], loss=75.8647
	step [137/206], loss=77.5990
	step [138/206], loss=64.0148
	step [139/206], loss=77.3372
	step [140/206], loss=80.8012
	step [141/206], loss=75.6159
	step [142/206], loss=59.9896
	step [143/206], loss=78.5758
	step [144/206], loss=78.5241
	step [145/206], loss=66.6182
	step [146/206], loss=71.1643
	step [147/206], loss=77.2954
	step [148/206], loss=77.6325
	step [149/206], loss=70.7084
	step [150/206], loss=68.7155
	step [151/206], loss=65.1935
	step [152/206], loss=70.9857
	step [153/206], loss=68.4702
	step [154/206], loss=72.0930
	step [155/206], loss=61.5693
	step [156/206], loss=85.6414
	step [157/206], loss=83.2277
	step [158/206], loss=73.2847
	step [159/206], loss=72.8405
	step [160/206], loss=67.7586
	step [161/206], loss=69.9211
	step [162/206], loss=67.1663
	step [163/206], loss=64.2799
	step [164/206], loss=72.9823
	step [165/206], loss=75.2614
	step [166/206], loss=86.6341
	step [167/206], loss=89.7406
	step [168/206], loss=67.6561
	step [169/206], loss=65.1353
	step [170/206], loss=76.2484
	step [171/206], loss=66.5299
	step [172/206], loss=68.3111
	step [173/206], loss=69.4645
	step [174/206], loss=73.1394
	step [175/206], loss=85.4006
	step [176/206], loss=75.9409
	step [177/206], loss=65.9444
	step [178/206], loss=73.1123
	step [179/206], loss=66.8790
	step [180/206], loss=83.0646
	step [181/206], loss=81.6830
	step [182/206], loss=73.2949
	step [183/206], loss=62.2623
	step [184/206], loss=75.1366
	step [185/206], loss=75.9319
	step [186/206], loss=73.1241
	step [187/206], loss=84.4583
	step [188/206], loss=83.0365
	step [189/206], loss=82.4468
	step [190/206], loss=70.0001
	step [191/206], loss=82.4150
	step [192/206], loss=75.6402
	step [193/206], loss=79.8794
	step [194/206], loss=89.9331
	step [195/206], loss=67.3678
	step [196/206], loss=70.7352
	step [197/206], loss=78.6777
	step [198/206], loss=68.6059
	step [199/206], loss=73.9205
	step [200/206], loss=86.3671
	step [201/206], loss=66.6760
	step [202/206], loss=86.9671
	step [203/206], loss=70.0626
	step [204/206], loss=82.1672
	step [205/206], loss=79.3333
	step [206/206], loss=69.6672
	Evaluating
	loss=0.0070, precision=0.4261, recall=0.9068, f1=0.5798
Training epoch 71
	step [1/206], loss=82.6967
	step [2/206], loss=85.2107
	step [3/206], loss=67.2394
	step [4/206], loss=77.3795
	step [5/206], loss=76.3951
	step [6/206], loss=81.6928
	step [7/206], loss=70.3162
	step [8/206], loss=80.2176
	step [9/206], loss=69.9817
	step [10/206], loss=73.2651
	step [11/206], loss=84.8562
	step [12/206], loss=69.7814
	step [13/206], loss=74.4627
	step [14/206], loss=80.2120
	step [15/206], loss=78.1768
	step [16/206], loss=92.3838
	step [17/206], loss=84.8762
	step [18/206], loss=86.7305
	step [19/206], loss=82.0607
	step [20/206], loss=89.8771
	step [21/206], loss=68.9438
	step [22/206], loss=84.3477
	step [23/206], loss=68.9470
	step [24/206], loss=65.2211
	step [25/206], loss=59.3525
	step [26/206], loss=69.3345
	step [27/206], loss=63.6581
	step [28/206], loss=69.7209
	step [29/206], loss=77.8699
	step [30/206], loss=71.6615
	step [31/206], loss=63.6577
	step [32/206], loss=77.8513
	step [33/206], loss=87.0051
	step [34/206], loss=76.2173
	step [35/206], loss=80.4689
	step [36/206], loss=72.7801
	step [37/206], loss=72.9961
	step [38/206], loss=59.7504
	step [39/206], loss=85.3303
	step [40/206], loss=82.5192
	step [41/206], loss=76.8408
	step [42/206], loss=75.3308
	step [43/206], loss=71.8717
	step [44/206], loss=62.3499
	step [45/206], loss=81.0389
	step [46/206], loss=88.9562
	step [47/206], loss=90.3820
	step [48/206], loss=67.4287
	step [49/206], loss=80.4504
	step [50/206], loss=73.7384
	step [51/206], loss=69.5546
	step [52/206], loss=75.5610
	step [53/206], loss=64.5538
	step [54/206], loss=69.5765
	step [55/206], loss=54.9528
	step [56/206], loss=73.9097
	step [57/206], loss=78.6792
	step [58/206], loss=84.9603
	step [59/206], loss=79.9837
	step [60/206], loss=74.6004
	step [61/206], loss=73.4800
	step [62/206], loss=59.5048
	step [63/206], loss=81.3752
	step [64/206], loss=74.5834
	step [65/206], loss=74.4232
	step [66/206], loss=68.4912
	step [67/206], loss=66.7943
	step [68/206], loss=67.0198
	step [69/206], loss=68.7683
	step [70/206], loss=83.8189
	step [71/206], loss=74.7942
	step [72/206], loss=74.1349
	step [73/206], loss=76.9698
	step [74/206], loss=69.1408
	step [75/206], loss=71.9605
	step [76/206], loss=74.4069
	step [77/206], loss=69.3587
	step [78/206], loss=76.9442
	step [79/206], loss=73.1633
	step [80/206], loss=64.1262
	step [81/206], loss=82.2560
	step [82/206], loss=73.3090
	step [83/206], loss=79.7001
	step [84/206], loss=71.0654
	step [85/206], loss=69.6785
	step [86/206], loss=71.6356
	step [87/206], loss=63.2807
	step [88/206], loss=80.6184
	step [89/206], loss=83.7474
	step [90/206], loss=73.5727
	step [91/206], loss=74.0245
	step [92/206], loss=88.3971
	step [93/206], loss=77.0744
	step [94/206], loss=72.7654
	step [95/206], loss=68.2010
	step [96/206], loss=71.0995
	step [97/206], loss=72.7758
	step [98/206], loss=68.6355
	step [99/206], loss=72.5965
	step [100/206], loss=68.0194
	step [101/206], loss=88.3820
	step [102/206], loss=75.3807
	step [103/206], loss=65.7746
	step [104/206], loss=80.8409
	step [105/206], loss=73.0334
	step [106/206], loss=78.8136
	step [107/206], loss=74.1008
	step [108/206], loss=74.3425
	step [109/206], loss=78.8753
	step [110/206], loss=75.2359
	step [111/206], loss=64.2663
	step [112/206], loss=69.4244
	step [113/206], loss=77.9161
	step [114/206], loss=72.6566
	step [115/206], loss=81.3787
	step [116/206], loss=68.4687
	step [117/206], loss=60.5109
	step [118/206], loss=69.0858
	step [119/206], loss=81.5824
	step [120/206], loss=62.9049
	step [121/206], loss=70.8308
	step [122/206], loss=72.5581
	step [123/206], loss=56.3866
	step [124/206], loss=87.6850
	step [125/206], loss=67.0507
	step [126/206], loss=80.1517
	step [127/206], loss=75.6155
	step [128/206], loss=77.1824
	step [129/206], loss=78.4191
	step [130/206], loss=70.8013
	step [131/206], loss=75.1959
	step [132/206], loss=85.8163
	step [133/206], loss=73.8732
	step [134/206], loss=74.5360
	step [135/206], loss=63.4650
	step [136/206], loss=80.2868
	step [137/206], loss=87.7732
	step [138/206], loss=90.6183
	step [139/206], loss=74.7721
	step [140/206], loss=55.7031
	step [141/206], loss=66.0564
	step [142/206], loss=75.3587
	step [143/206], loss=70.7273
	step [144/206], loss=79.8969
	step [145/206], loss=70.6974
	step [146/206], loss=76.7151
	step [147/206], loss=75.2751
	step [148/206], loss=81.5072
	step [149/206], loss=85.1628
	step [150/206], loss=75.2651
	step [151/206], loss=75.2888
	step [152/206], loss=80.9339
	step [153/206], loss=90.2826
	step [154/206], loss=78.8666
	step [155/206], loss=67.3172
	step [156/206], loss=68.7755
	step [157/206], loss=71.6517
	step [158/206], loss=74.9095
	step [159/206], loss=89.8309
	step [160/206], loss=75.3611
	step [161/206], loss=77.1469
	step [162/206], loss=85.5776
	step [163/206], loss=75.6348
	step [164/206], loss=76.7854
	step [165/206], loss=79.4906
	step [166/206], loss=65.9616
	step [167/206], loss=69.7807
	step [168/206], loss=72.4222
	step [169/206], loss=73.3840
	step [170/206], loss=84.3108
	step [171/206], loss=69.5004
	step [172/206], loss=76.5966
	step [173/206], loss=78.3066
	step [174/206], loss=68.8783
	step [175/206], loss=64.4299
	step [176/206], loss=75.0131
	step [177/206], loss=82.4672
	step [178/206], loss=82.7897
	step [179/206], loss=76.1493
	step [180/206], loss=78.7376
	step [181/206], loss=65.8959
	step [182/206], loss=69.1476
	step [183/206], loss=88.6195
	step [184/206], loss=74.2271
	step [185/206], loss=80.6703
	step [186/206], loss=72.5942
	step [187/206], loss=74.4258
	step [188/206], loss=66.6641
	step [189/206], loss=71.6617
	step [190/206], loss=74.4099
	step [191/206], loss=77.4526
	step [192/206], loss=70.0003
	step [193/206], loss=73.5019
	step [194/206], loss=94.6306
	step [195/206], loss=68.1016
	step [196/206], loss=90.0603
	step [197/206], loss=71.1709
	step [198/206], loss=68.4067
	step [199/206], loss=62.6021
	step [200/206], loss=67.6842
	step [201/206], loss=90.7793
	step [202/206], loss=88.6817
	step [203/206], loss=86.0941
	step [204/206], loss=81.7232
	step [205/206], loss=71.2749
	step [206/206], loss=48.2443
	Evaluating
	loss=0.0056, precision=0.4742, recall=0.9017, f1=0.6215
saving model as: 4_saved_model.pth
Training epoch 72
	step [1/206], loss=75.6673
	step [2/206], loss=65.4841
	step [3/206], loss=80.8651
	step [4/206], loss=69.4641
	step [5/206], loss=89.7007
	step [6/206], loss=75.4307
	step [7/206], loss=75.5955
	step [8/206], loss=57.6307
	step [9/206], loss=85.0346
	step [10/206], loss=68.8132
	step [11/206], loss=62.7046
	step [12/206], loss=74.4506
	step [13/206], loss=68.3311
	step [14/206], loss=73.8820
	step [15/206], loss=75.8999
	step [16/206], loss=82.0192
	step [17/206], loss=77.7578
	step [18/206], loss=62.5984
	step [19/206], loss=75.8706
	step [20/206], loss=64.2470
	step [21/206], loss=60.8890
	step [22/206], loss=91.1949
	step [23/206], loss=89.6581
	step [24/206], loss=69.3082
	step [25/206], loss=72.7684
	step [26/206], loss=86.3989
	step [27/206], loss=80.5271
	step [28/206], loss=65.0276
	step [29/206], loss=74.3107
	step [30/206], loss=90.5339
	step [31/206], loss=79.7613
	step [32/206], loss=74.7551
	step [33/206], loss=90.0178
	step [34/206], loss=60.1143
	step [35/206], loss=73.0716
	step [36/206], loss=84.7137
	step [37/206], loss=64.8370
	step [38/206], loss=87.2137
	step [39/206], loss=84.3496
	step [40/206], loss=67.2987
	step [41/206], loss=76.5303
	step [42/206], loss=64.4623
	step [43/206], loss=71.0846
	step [44/206], loss=75.0297
	step [45/206], loss=84.8609
	step [46/206], loss=75.3284
	step [47/206], loss=70.5274
	step [48/206], loss=72.2485
	step [49/206], loss=69.5397
	step [50/206], loss=76.6608
	step [51/206], loss=84.6426
	step [52/206], loss=70.4779
	step [53/206], loss=87.5244
	step [54/206], loss=70.4465
	step [55/206], loss=70.1651
	step [56/206], loss=85.3730
	step [57/206], loss=81.1040
	step [58/206], loss=63.1945
	step [59/206], loss=60.7913
	step [60/206], loss=71.1860
	step [61/206], loss=80.3965
	step [62/206], loss=66.7020
	step [63/206], loss=79.4834
	step [64/206], loss=78.7842
	step [65/206], loss=85.5606
	step [66/206], loss=74.0850
	step [67/206], loss=63.4612
	step [68/206], loss=86.3291
	step [69/206], loss=65.3553
	step [70/206], loss=71.4821
	step [71/206], loss=77.1389
	step [72/206], loss=72.7774
	step [73/206], loss=77.7951
	step [74/206], loss=79.6468
	step [75/206], loss=77.0173
	step [76/206], loss=73.4357
	step [77/206], loss=74.8806
	step [78/206], loss=81.8254
	step [79/206], loss=73.0191
	step [80/206], loss=84.8242
	step [81/206], loss=73.3566
	step [82/206], loss=82.3783
	step [83/206], loss=67.8370
	step [84/206], loss=78.3878
	step [85/206], loss=80.2243
	step [86/206], loss=57.2404
	step [87/206], loss=79.8225
	step [88/206], loss=63.7671
	step [89/206], loss=72.3275
	step [90/206], loss=70.2359
	step [91/206], loss=73.9870
	step [92/206], loss=83.9655
	step [93/206], loss=82.5143
	step [94/206], loss=73.1265
	step [95/206], loss=79.6145
	step [96/206], loss=67.6486
	step [97/206], loss=65.1945
	step [98/206], loss=75.4927
	step [99/206], loss=70.8264
	step [100/206], loss=81.2597
	step [101/206], loss=73.3166
	step [102/206], loss=74.6047
	step [103/206], loss=63.6078
	step [104/206], loss=87.5782
	step [105/206], loss=75.9654
	step [106/206], loss=86.7902
	step [107/206], loss=72.8619
	step [108/206], loss=75.9081
	step [109/206], loss=81.9160
	step [110/206], loss=72.0993
	step [111/206], loss=81.7248
	step [112/206], loss=81.9274
	step [113/206], loss=69.5423
	step [114/206], loss=75.1227
	step [115/206], loss=85.7603
	step [116/206], loss=75.9363
	step [117/206], loss=76.2480
	step [118/206], loss=70.6680
	step [119/206], loss=70.1361
	step [120/206], loss=75.8030
	step [121/206], loss=71.3676
	step [122/206], loss=73.1149
	step [123/206], loss=78.5111
	step [124/206], loss=68.2045
	step [125/206], loss=77.6469
	step [126/206], loss=79.2654
	step [127/206], loss=69.7462
	step [128/206], loss=85.7976
	step [129/206], loss=70.3453
	step [130/206], loss=74.8520
	step [131/206], loss=78.5554
	step [132/206], loss=82.3770
	step [133/206], loss=71.3131
	step [134/206], loss=83.8033
	step [135/206], loss=80.2662
	step [136/206], loss=63.6794
	step [137/206], loss=81.4678
	step [138/206], loss=76.5878
	step [139/206], loss=73.5995
	step [140/206], loss=74.8227
	step [141/206], loss=75.1643
	step [142/206], loss=72.0107
	step [143/206], loss=69.6171
	step [144/206], loss=74.9504
	step [145/206], loss=74.1328
	step [146/206], loss=73.8380
	step [147/206], loss=82.1013
	step [148/206], loss=70.8760
	step [149/206], loss=70.0645
	step [150/206], loss=67.9406
	step [151/206], loss=83.1322
	step [152/206], loss=81.3655
	step [153/206], loss=92.3827
	step [154/206], loss=99.8810
	step [155/206], loss=65.7017
	step [156/206], loss=65.7255
	step [157/206], loss=70.5176
	step [158/206], loss=77.7845
	step [159/206], loss=68.5097
	step [160/206], loss=61.5052
	step [161/206], loss=58.9064
	step [162/206], loss=80.0624
	step [163/206], loss=86.6175
	step [164/206], loss=72.1681
	step [165/206], loss=94.6544
	step [166/206], loss=74.8716
	step [167/206], loss=68.1926
	step [168/206], loss=68.0140
	step [169/206], loss=75.7316
	step [170/206], loss=80.9061
	step [171/206], loss=66.2227
	step [172/206], loss=82.8565
	step [173/206], loss=78.0905
	step [174/206], loss=78.6204
	step [175/206], loss=73.0842
	step [176/206], loss=72.0144
	step [177/206], loss=62.5616
	step [178/206], loss=62.4135
	step [179/206], loss=92.2521
	step [180/206], loss=73.6715
	step [181/206], loss=79.3856
	step [182/206], loss=76.6705
	step [183/206], loss=72.0939
	step [184/206], loss=78.3805
	step [185/206], loss=80.6737
	step [186/206], loss=76.7405
	step [187/206], loss=68.4852
	step [188/206], loss=81.7190
	step [189/206], loss=68.0007
	step [190/206], loss=69.8437
	step [191/206], loss=77.5042
	step [192/206], loss=80.4893
	step [193/206], loss=68.0575
	step [194/206], loss=82.4686
	step [195/206], loss=74.3647
	step [196/206], loss=71.2702
	step [197/206], loss=89.4232
	step [198/206], loss=76.5898
	step [199/206], loss=64.9402
	step [200/206], loss=75.7462
	step [201/206], loss=77.3914
	step [202/206], loss=72.1364
	step [203/206], loss=63.3899
	step [204/206], loss=57.6524
	step [205/206], loss=65.7673
	step [206/206], loss=55.3290
	Evaluating
	loss=0.0062, precision=0.4456, recall=0.9079, f1=0.5978
Training epoch 73
	step [1/206], loss=68.8600
	step [2/206], loss=73.9452
	step [3/206], loss=74.8501
	step [4/206], loss=79.8609
	step [5/206], loss=71.8402
	step [6/206], loss=68.7138
	step [7/206], loss=75.8211
	step [8/206], loss=71.1288
	step [9/206], loss=64.2269
	step [10/206], loss=58.3096
	step [11/206], loss=88.6526
	step [12/206], loss=70.7605
	step [13/206], loss=77.5901
	step [14/206], loss=76.2334
	step [15/206], loss=84.6918
	step [16/206], loss=86.7918
	step [17/206], loss=75.4191
	step [18/206], loss=81.6658
	step [19/206], loss=74.9555
	step [20/206], loss=73.3428
	step [21/206], loss=73.4279
	step [22/206], loss=76.0016
	step [23/206], loss=69.5437
	step [24/206], loss=74.2947
	step [25/206], loss=59.0668
	step [26/206], loss=60.3528
	step [27/206], loss=79.7188
	step [28/206], loss=73.2898
	step [29/206], loss=78.2461
	step [30/206], loss=87.2955
	step [31/206], loss=90.2003
	step [32/206], loss=74.1917
	step [33/206], loss=73.8258
	step [34/206], loss=76.4172
	step [35/206], loss=77.1068
	step [36/206], loss=69.6933
	step [37/206], loss=66.9582
	step [38/206], loss=82.7093
	step [39/206], loss=69.1169
	step [40/206], loss=73.3313
	step [41/206], loss=71.3663
	step [42/206], loss=63.0598
	step [43/206], loss=68.8201
	step [44/206], loss=71.8872
	step [45/206], loss=64.2826
	step [46/206], loss=69.1483
	step [47/206], loss=66.1864
	step [48/206], loss=73.0115
	step [49/206], loss=74.6471
	step [50/206], loss=80.8839
	step [51/206], loss=74.2907
	step [52/206], loss=71.2678
	step [53/206], loss=81.8671
	step [54/206], loss=73.4368
	step [55/206], loss=70.6127
	step [56/206], loss=80.2887
	step [57/206], loss=82.1000
	step [58/206], loss=68.8826
	step [59/206], loss=63.2229
	step [60/206], loss=68.6853
	step [61/206], loss=74.0887
	step [62/206], loss=78.4579
	step [63/206], loss=73.1680
	step [64/206], loss=70.5239
	step [65/206], loss=71.0002
	step [66/206], loss=79.3179
	step [67/206], loss=78.0156
	step [68/206], loss=87.3729
	step [69/206], loss=75.9909
	step [70/206], loss=74.6621
	step [71/206], loss=67.2408
	step [72/206], loss=71.9874
	step [73/206], loss=77.4980
	step [74/206], loss=85.9812
	step [75/206], loss=83.0220
	step [76/206], loss=72.1941
	step [77/206], loss=61.1234
	step [78/206], loss=63.7186
	step [79/206], loss=79.4390
	step [80/206], loss=79.1711
	step [81/206], loss=70.9275
	step [82/206], loss=75.4281
	step [83/206], loss=82.2904
	step [84/206], loss=84.0642
	step [85/206], loss=86.0061
	step [86/206], loss=89.0164
	step [87/206], loss=79.6020
	step [88/206], loss=72.8576
	step [89/206], loss=76.4247
	step [90/206], loss=73.6043
	step [91/206], loss=86.1601
	step [92/206], loss=89.9978
	step [93/206], loss=74.5676
	step [94/206], loss=73.5723
	step [95/206], loss=75.7911
	step [96/206], loss=74.3793
	step [97/206], loss=83.5291
	step [98/206], loss=70.9883
	step [99/206], loss=76.4625
	step [100/206], loss=86.8111
	step [101/206], loss=74.6851
	step [102/206], loss=78.2215
	step [103/206], loss=81.7572
	step [104/206], loss=62.7838
	step [105/206], loss=73.1380
	step [106/206], loss=86.7748
	step [107/206], loss=77.4948
	step [108/206], loss=83.5454
	step [109/206], loss=66.5632
	step [110/206], loss=81.9974
	step [111/206], loss=63.0572
	step [112/206], loss=81.4786
	step [113/206], loss=64.6717
	step [114/206], loss=77.8642
	step [115/206], loss=75.5905
	step [116/206], loss=62.3665
	step [117/206], loss=79.4853
	step [118/206], loss=72.6235
	step [119/206], loss=64.9434
	step [120/206], loss=77.0100
	step [121/206], loss=89.3830
	step [122/206], loss=63.8326
	step [123/206], loss=79.1588
	step [124/206], loss=75.0900
	step [125/206], loss=72.9427
	step [126/206], loss=76.9280
	step [127/206], loss=73.3812
	step [128/206], loss=73.4217
	step [129/206], loss=79.5374
	step [130/206], loss=69.3697
	step [131/206], loss=83.9102
	step [132/206], loss=78.5622
	step [133/206], loss=71.8868
	step [134/206], loss=78.8009
	step [135/206], loss=71.7330
	step [136/206], loss=70.8048
	step [137/206], loss=83.2050
	step [138/206], loss=69.7552
	step [139/206], loss=78.1907
	step [140/206], loss=69.3266
	step [141/206], loss=83.2228
	step [142/206], loss=62.5993
	step [143/206], loss=74.1897
	step [144/206], loss=70.7582
	step [145/206], loss=73.3193
	step [146/206], loss=70.8639
	step [147/206], loss=85.5503
	step [148/206], loss=79.9320
	step [149/206], loss=68.2457
	step [150/206], loss=79.9435
	step [151/206], loss=64.0690
	step [152/206], loss=72.1452
	step [153/206], loss=60.8024
	step [154/206], loss=79.7476
	step [155/206], loss=85.2451
	step [156/206], loss=80.5855
	step [157/206], loss=80.9115
	step [158/206], loss=79.0570
	step [159/206], loss=63.0928
	step [160/206], loss=64.9364
	step [161/206], loss=61.7317
	step [162/206], loss=78.3554
	step [163/206], loss=87.7274
	step [164/206], loss=64.0112
	step [165/206], loss=78.5504
	step [166/206], loss=70.4701
	step [167/206], loss=68.8333
	step [168/206], loss=76.6379
	step [169/206], loss=64.6905
	step [170/206], loss=67.9561
	step [171/206], loss=69.5720
	step [172/206], loss=68.3434
	step [173/206], loss=72.8307
	step [174/206], loss=80.0645
	step [175/206], loss=76.1673
	step [176/206], loss=76.6182
	step [177/206], loss=69.7335
	step [178/206], loss=68.3970
	step [179/206], loss=81.7326
	step [180/206], loss=70.0596
	step [181/206], loss=84.8213
	step [182/206], loss=81.6427
	step [183/206], loss=64.6337
	step [184/206], loss=65.9701
	step [185/206], loss=79.6640
	step [186/206], loss=87.4614
	step [187/206], loss=68.4007
	step [188/206], loss=63.6475
	step [189/206], loss=79.6964
	step [190/206], loss=71.4977
	step [191/206], loss=72.6982
	step [192/206], loss=68.2574
	step [193/206], loss=79.7123
	step [194/206], loss=78.2373
	step [195/206], loss=79.4130
	step [196/206], loss=76.5184
	step [197/206], loss=62.0455
	step [198/206], loss=75.0555
	step [199/206], loss=85.9477
	step [200/206], loss=73.1039
	step [201/206], loss=77.3089
	step [202/206], loss=86.1844
	step [203/206], loss=85.0527
	step [204/206], loss=66.1003
	step [205/206], loss=83.5745
	step [206/206], loss=46.8354
	Evaluating
	loss=0.0066, precision=0.4250, recall=0.9093, f1=0.5793
Training epoch 74
	step [1/206], loss=77.6656
	step [2/206], loss=79.4964
	step [3/206], loss=74.1138
	step [4/206], loss=76.5440
	step [5/206], loss=80.4150
	step [6/206], loss=67.4411
	step [7/206], loss=64.6538
	step [8/206], loss=72.9414
	step [9/206], loss=74.5123
	step [10/206], loss=75.1331
	step [11/206], loss=75.6239
	step [12/206], loss=77.4411
	step [13/206], loss=82.2128
	step [14/206], loss=77.0195
	step [15/206], loss=69.3296
	step [16/206], loss=65.9998
	step [17/206], loss=66.1030
	step [18/206], loss=65.2776
	step [19/206], loss=71.3170
	step [20/206], loss=64.5768
	step [21/206], loss=69.1091
	step [22/206], loss=66.9424
	step [23/206], loss=99.0769
	step [24/206], loss=66.0733
	step [25/206], loss=81.0807
	step [26/206], loss=76.8644
	step [27/206], loss=73.4740
	step [28/206], loss=73.8821
	step [29/206], loss=88.0833
	step [30/206], loss=77.9512
	step [31/206], loss=74.4168
	step [32/206], loss=71.0675
	step [33/206], loss=70.7368
	step [34/206], loss=77.6081
	step [35/206], loss=78.9381
	step [36/206], loss=73.0522
	step [37/206], loss=65.7157
	step [38/206], loss=74.3269
	step [39/206], loss=76.6631
	step [40/206], loss=79.8238
	step [41/206], loss=74.8705
	step [42/206], loss=71.7289
	step [43/206], loss=79.8929
	step [44/206], loss=79.1938
	step [45/206], loss=84.6962
	step [46/206], loss=79.9364
	step [47/206], loss=87.3148
	step [48/206], loss=80.8574
	step [49/206], loss=68.1603
	step [50/206], loss=72.3456
	step [51/206], loss=71.0432
	step [52/206], loss=75.2750
	step [53/206], loss=70.3344
	step [54/206], loss=81.0868
	step [55/206], loss=66.3335
	step [56/206], loss=85.8173
	step [57/206], loss=85.9044
	step [58/206], loss=74.3657
	step [59/206], loss=79.0609
	step [60/206], loss=74.8700
	step [61/206], loss=83.8097
	step [62/206], loss=76.7614
	step [63/206], loss=76.5207
	step [64/206], loss=69.8917
	step [65/206], loss=75.2813
	step [66/206], loss=74.5436
	step [67/206], loss=63.4059
	step [68/206], loss=67.0561
	step [69/206], loss=66.7506
	step [70/206], loss=68.9529
	step [71/206], loss=72.1385
	step [72/206], loss=79.5497
	step [73/206], loss=60.6118
	step [74/206], loss=85.5392
	step [75/206], loss=73.2942
	step [76/206], loss=81.5237
	step [77/206], loss=76.3438
	step [78/206], loss=76.0554
	step [79/206], loss=87.4237
	step [80/206], loss=82.9743
	step [81/206], loss=86.3663
	step [82/206], loss=76.2070
	step [83/206], loss=76.0391
	step [84/206], loss=65.6582
	step [85/206], loss=80.2105
	step [86/206], loss=75.6354
	step [87/206], loss=69.1906
	step [88/206], loss=80.3895
	step [89/206], loss=84.7845
	step [90/206], loss=85.2672
	step [91/206], loss=87.3372
	step [92/206], loss=75.7264
	step [93/206], loss=73.4792
	step [94/206], loss=83.2535
	step [95/206], loss=67.9402
	step [96/206], loss=71.0733
	step [97/206], loss=77.4193
	step [98/206], loss=67.6055
	step [99/206], loss=69.4161
	step [100/206], loss=71.5900
	step [101/206], loss=83.9717
	step [102/206], loss=71.6293
	step [103/206], loss=66.4760
	step [104/206], loss=79.1973
	step [105/206], loss=72.9175
	step [106/206], loss=66.9310
	step [107/206], loss=72.0341
	step [108/206], loss=66.3105
	step [109/206], loss=90.7117
	step [110/206], loss=59.7696
	step [111/206], loss=86.3993
	step [112/206], loss=72.8647
	step [113/206], loss=64.7222
	step [114/206], loss=68.6999
	step [115/206], loss=84.0871
	step [116/206], loss=63.2751
	step [117/206], loss=68.5128
	step [118/206], loss=74.5134
	step [119/206], loss=60.6267
	step [120/206], loss=67.2113
	step [121/206], loss=77.3821
	step [122/206], loss=78.4137
	step [123/206], loss=78.1534
	step [124/206], loss=70.3190
	step [125/206], loss=91.7739
	step [126/206], loss=67.9051
	step [127/206], loss=62.3106
	step [128/206], loss=71.2168
	step [129/206], loss=82.9368
	step [130/206], loss=80.1563
	step [131/206], loss=73.4115
	step [132/206], loss=71.3656
	step [133/206], loss=68.3339
	step [134/206], loss=71.9707
	step [135/206], loss=82.5567
	step [136/206], loss=67.9706
	step [137/206], loss=69.0977
	step [138/206], loss=86.8769
	step [139/206], loss=66.5113
	step [140/206], loss=75.2986
	step [141/206], loss=66.8335
	step [142/206], loss=72.3662
	step [143/206], loss=74.5499
	step [144/206], loss=80.1212
	step [145/206], loss=74.4204
	step [146/206], loss=74.3819
	step [147/206], loss=69.2612
	step [148/206], loss=75.5196
	step [149/206], loss=76.9524
	step [150/206], loss=67.5115
	step [151/206], loss=68.8702
	step [152/206], loss=82.8454
	step [153/206], loss=74.0192
	step [154/206], loss=63.6725
	step [155/206], loss=92.3164
	step [156/206], loss=58.6499
	step [157/206], loss=78.3294
	step [158/206], loss=67.9141
	step [159/206], loss=82.2232
	step [160/206], loss=66.3978
	step [161/206], loss=83.6701
	step [162/206], loss=68.5369
	step [163/206], loss=72.6166
	step [164/206], loss=75.7403
	step [165/206], loss=56.1186
	step [166/206], loss=74.1332
	step [167/206], loss=84.2865
	step [168/206], loss=63.3356
	step [169/206], loss=63.1121
	step [170/206], loss=83.9737
	step [171/206], loss=80.0182
	step [172/206], loss=67.2705
	step [173/206], loss=79.3500
	step [174/206], loss=62.7832
	step [175/206], loss=69.2772
	step [176/206], loss=66.2162
	step [177/206], loss=62.5916
	step [178/206], loss=71.3930
	step [179/206], loss=71.7442
	step [180/206], loss=75.8082
	step [181/206], loss=67.3873
	step [182/206], loss=69.9866
	step [183/206], loss=79.9585
	step [184/206], loss=72.9147
	step [185/206], loss=78.1556
	step [186/206], loss=71.2071
	step [187/206], loss=72.6675
	step [188/206], loss=70.4057
	step [189/206], loss=79.5610
	step [190/206], loss=75.4182
	step [191/206], loss=67.2136
	step [192/206], loss=89.3361
	step [193/206], loss=66.0955
	step [194/206], loss=72.2768
	step [195/206], loss=88.7086
	step [196/206], loss=74.6427
	step [197/206], loss=68.4537
	step [198/206], loss=77.4304
	step [199/206], loss=66.5219
	step [200/206], loss=75.5783
	step [201/206], loss=67.4594
	step [202/206], loss=79.3097
	step [203/206], loss=65.0339
	step [204/206], loss=80.2668
	step [205/206], loss=86.8872
	step [206/206], loss=37.2346
	Evaluating
	loss=0.0059, precision=0.4554, recall=0.8955, f1=0.6038
Training epoch 75
	step [1/206], loss=72.9376
	step [2/206], loss=79.6592
	step [3/206], loss=67.7053
	step [4/206], loss=55.5512
	step [5/206], loss=70.1643
	step [6/206], loss=80.7873
	step [7/206], loss=79.1974
	step [8/206], loss=78.0491
	step [9/206], loss=84.3457
	step [10/206], loss=60.7876
	step [11/206], loss=75.0821
	step [12/206], loss=79.7076
	step [13/206], loss=78.5513
	step [14/206], loss=65.2287
	step [15/206], loss=71.5829
	step [16/206], loss=70.6236
	step [17/206], loss=70.7693
	step [18/206], loss=69.0965
	step [19/206], loss=79.6586
	step [20/206], loss=66.5799
	step [21/206], loss=72.7310
	step [22/206], loss=78.8995
	step [23/206], loss=89.0952
	step [24/206], loss=72.7382
	step [25/206], loss=72.7927
	step [26/206], loss=71.8803
	step [27/206], loss=65.9351
	step [28/206], loss=62.7935
	step [29/206], loss=70.5104
	step [30/206], loss=64.6701
	step [31/206], loss=77.2108
	step [32/206], loss=66.5294
	step [33/206], loss=72.5048
	step [34/206], loss=65.4430
	step [35/206], loss=82.9865
	step [36/206], loss=74.8754
	step [37/206], loss=81.7504
	step [38/206], loss=88.0692
	step [39/206], loss=78.3284
	step [40/206], loss=75.3208
	step [41/206], loss=81.6192
	step [42/206], loss=70.4903
	step [43/206], loss=76.3771
	step [44/206], loss=73.9071
	step [45/206], loss=81.1725
	step [46/206], loss=71.5547
	step [47/206], loss=64.5811
	step [48/206], loss=64.6585
	step [49/206], loss=69.2470
	step [50/206], loss=84.2813
	step [51/206], loss=71.3156
	step [52/206], loss=76.8114
	step [53/206], loss=63.6151
	step [54/206], loss=73.4454
	step [55/206], loss=84.2304
	step [56/206], loss=70.0493
	step [57/206], loss=74.9254
	step [58/206], loss=77.3584
	step [59/206], loss=76.3834
	step [60/206], loss=84.1928
	step [61/206], loss=75.5766
	step [62/206], loss=72.3646
	step [63/206], loss=74.5834
	step [64/206], loss=87.0012
	step [65/206], loss=75.2621
	step [66/206], loss=70.0097
	step [67/206], loss=72.1468
	step [68/206], loss=83.2824
	step [69/206], loss=76.6144
	step [70/206], loss=85.8653
	step [71/206], loss=75.0888
	step [72/206], loss=62.7838
	step [73/206], loss=79.2005
	step [74/206], loss=88.0677
	step [75/206], loss=78.9955
	step [76/206], loss=69.1588
	step [77/206], loss=77.8800
	step [78/206], loss=74.9451
	step [79/206], loss=62.8236
	step [80/206], loss=75.7582
	step [81/206], loss=73.0243
	step [82/206], loss=62.3398
	step [83/206], loss=68.3447
	step [84/206], loss=58.0000
	step [85/206], loss=80.1792
	step [86/206], loss=70.2577
	step [87/206], loss=65.5242
	step [88/206], loss=74.0907
	step [89/206], loss=64.9100
	step [90/206], loss=78.2723
	step [91/206], loss=76.3454
	step [92/206], loss=72.4278
	step [93/206], loss=77.0312
	step [94/206], loss=61.5990
	step [95/206], loss=77.3905
	step [96/206], loss=82.5229
	step [97/206], loss=63.2610
	step [98/206], loss=67.0359
	step [99/206], loss=84.0819
	step [100/206], loss=68.5185
	step [101/206], loss=71.5513
	step [102/206], loss=73.9574
	step [103/206], loss=85.3550
	step [104/206], loss=72.0408
	step [105/206], loss=83.7689
	step [106/206], loss=65.2075
	step [107/206], loss=75.2057
	step [108/206], loss=61.5679
	step [109/206], loss=82.0216
	step [110/206], loss=60.9963
	step [111/206], loss=77.4922
	step [112/206], loss=82.5782
	step [113/206], loss=74.4021
	step [114/206], loss=71.8387
	step [115/206], loss=75.7693
	step [116/206], loss=73.4770
	step [117/206], loss=70.4983
	step [118/206], loss=74.4579
	step [119/206], loss=74.9825
	step [120/206], loss=81.4390
	step [121/206], loss=76.5034
	step [122/206], loss=75.8401
	step [123/206], loss=90.0291
	step [124/206], loss=74.3941
	step [125/206], loss=71.9897
	step [126/206], loss=89.4562
	step [127/206], loss=92.3656
	step [128/206], loss=73.1901
	step [129/206], loss=69.1821
	step [130/206], loss=77.3311
	step [131/206], loss=89.0469
	step [132/206], loss=73.8331
	step [133/206], loss=66.8905
	step [134/206], loss=73.4397
	step [135/206], loss=74.8046
	step [136/206], loss=72.8080
	step [137/206], loss=72.8580
	step [138/206], loss=68.4395
	step [139/206], loss=63.7319
	step [140/206], loss=79.0450
	step [141/206], loss=71.3426
	step [142/206], loss=59.7865
	step [143/206], loss=73.6824
	step [144/206], loss=78.4014
	step [145/206], loss=79.1514
	step [146/206], loss=59.4526
	step [147/206], loss=61.6473
	step [148/206], loss=80.5311
	step [149/206], loss=67.4023
	step [150/206], loss=70.8149
	step [151/206], loss=67.4764
	step [152/206], loss=75.6255
	step [153/206], loss=68.4222
	step [154/206], loss=76.6070
	step [155/206], loss=83.9462
	step [156/206], loss=84.2440
	step [157/206], loss=91.5909
	step [158/206], loss=85.9723
	step [159/206], loss=72.7900
	step [160/206], loss=81.1304
	step [161/206], loss=72.9295
	step [162/206], loss=80.8133
	step [163/206], loss=77.2744
	step [164/206], loss=77.2213
	step [165/206], loss=80.2559
	step [166/206], loss=65.5209
	step [167/206], loss=71.8584
	step [168/206], loss=65.1157
	step [169/206], loss=80.5791
	step [170/206], loss=73.6015
	step [171/206], loss=68.2646
	step [172/206], loss=60.6422
	step [173/206], loss=83.8861
	step [174/206], loss=79.4617
	step [175/206], loss=74.8862
	step [176/206], loss=71.8108
	step [177/206], loss=87.9384
	step [178/206], loss=69.2566
	step [179/206], loss=79.2557
	step [180/206], loss=71.5184
	step [181/206], loss=81.2419
	step [182/206], loss=79.8355
	step [183/206], loss=80.0237
	step [184/206], loss=60.7303
	step [185/206], loss=81.0308
	step [186/206], loss=68.5103
	step [187/206], loss=80.0660
	step [188/206], loss=65.7077
	step [189/206], loss=68.6791
	step [190/206], loss=78.5823
	step [191/206], loss=65.9054
	step [192/206], loss=71.2674
	step [193/206], loss=74.8050
	step [194/206], loss=76.1655
	step [195/206], loss=74.0438
	step [196/206], loss=65.8230
	step [197/206], loss=71.0177
	step [198/206], loss=73.0586
	step [199/206], loss=70.5466
	step [200/206], loss=83.2307
	step [201/206], loss=79.6043
	step [202/206], loss=89.9704
	step [203/206], loss=79.5109
	step [204/206], loss=75.0424
	step [205/206], loss=66.9537
	step [206/206], loss=51.1199
	Evaluating
	loss=0.0061, precision=0.4477, recall=0.8968, f1=0.5973
Training epoch 76
	step [1/206], loss=65.5161
	step [2/206], loss=67.3152
	step [3/206], loss=74.8433
	step [4/206], loss=80.3111
	step [5/206], loss=74.7072
	step [6/206], loss=80.2443
	step [7/206], loss=71.6615
	step [8/206], loss=79.1349
	step [9/206], loss=73.0555
	step [10/206], loss=80.9371
	step [11/206], loss=82.3711
	step [12/206], loss=78.3948
	step [13/206], loss=81.1058
	step [14/206], loss=66.3311
	step [15/206], loss=74.3919
	step [16/206], loss=81.1945
	step [17/206], loss=65.2285
	step [18/206], loss=69.2701
	step [19/206], loss=69.5493
	step [20/206], loss=79.1693
	step [21/206], loss=71.8459
	step [22/206], loss=71.4233
	step [23/206], loss=84.2246
	step [24/206], loss=81.8942
	step [25/206], loss=61.9301
	step [26/206], loss=68.0023
	step [27/206], loss=71.0769
	step [28/206], loss=69.0489
	step [29/206], loss=72.5317
	step [30/206], loss=78.3711
	step [31/206], loss=82.1423
	step [32/206], loss=64.7273
	step [33/206], loss=73.9747
	step [34/206], loss=71.1629
	step [35/206], loss=88.6278
	step [36/206], loss=89.7488
	step [37/206], loss=70.8469
	step [38/206], loss=60.3060
	step [39/206], loss=78.7278
	step [40/206], loss=70.6108
	step [41/206], loss=66.3452
	step [42/206], loss=66.4017
	step [43/206], loss=65.8475
	step [44/206], loss=71.8720
	step [45/206], loss=61.7409
	step [46/206], loss=72.1478
	step [47/206], loss=70.2437
	step [48/206], loss=71.0768
	step [49/206], loss=70.0667
	step [50/206], loss=71.3964
	step [51/206], loss=69.5363
	step [52/206], loss=59.9036
	step [53/206], loss=74.1514
	step [54/206], loss=74.2800
	step [55/206], loss=75.0478
	step [56/206], loss=76.6442
	step [57/206], loss=77.8873
	step [58/206], loss=81.7982
	step [59/206], loss=83.6844
	step [60/206], loss=75.7548
	step [61/206], loss=68.5189
	step [62/206], loss=79.2090
	step [63/206], loss=86.3960
	step [64/206], loss=66.3643
	step [65/206], loss=79.6567
	step [66/206], loss=85.4342
	step [67/206], loss=70.6019
	step [68/206], loss=71.0524
	step [69/206], loss=61.9848
	step [70/206], loss=78.8486
	step [71/206], loss=61.5735
	step [72/206], loss=68.5397
	step [73/206], loss=77.0801
	step [74/206], loss=67.6908
	step [75/206], loss=80.4954
	step [76/206], loss=70.9192
	step [77/206], loss=75.6643
	step [78/206], loss=75.8143
	step [79/206], loss=64.9218
	step [80/206], loss=75.3461
	step [81/206], loss=74.3737
	step [82/206], loss=74.1665
	step [83/206], loss=73.6235
	step [84/206], loss=79.8361
	step [85/206], loss=75.9293
	step [86/206], loss=73.2536
	step [87/206], loss=74.8174
	step [88/206], loss=66.0091
	step [89/206], loss=73.7984
	step [90/206], loss=78.3816
	step [91/206], loss=81.4313
	step [92/206], loss=77.5316
	step [93/206], loss=75.8455
	step [94/206], loss=71.3424
	step [95/206], loss=78.4319
	step [96/206], loss=78.4179
	step [97/206], loss=68.6971
	step [98/206], loss=61.4283
	step [99/206], loss=68.8533
	step [100/206], loss=77.4443
	step [101/206], loss=63.6560
	step [102/206], loss=74.3362
	step [103/206], loss=72.5133
	step [104/206], loss=79.0260
	step [105/206], loss=72.9249
	step [106/206], loss=75.2844
	step [107/206], loss=73.7768
	step [108/206], loss=69.2121
	step [109/206], loss=81.5067
	step [110/206], loss=69.5620
	step [111/206], loss=63.6209
	step [112/206], loss=76.4702
	step [113/206], loss=71.3821
	step [114/206], loss=75.3895
	step [115/206], loss=62.0497
	step [116/206], loss=84.1697
	step [117/206], loss=66.9525
	step [118/206], loss=75.7638
	step [119/206], loss=79.2270
	step [120/206], loss=80.3431
	step [121/206], loss=71.7324
	step [122/206], loss=71.5036
	step [123/206], loss=80.4322
	step [124/206], loss=65.9707
	step [125/206], loss=72.7195
	step [126/206], loss=76.2262
	step [127/206], loss=89.5808
	step [128/206], loss=62.4342
	step [129/206], loss=74.3875
	step [130/206], loss=80.9927
	step [131/206], loss=73.8367
	step [132/206], loss=71.7405
	step [133/206], loss=77.3604
	step [134/206], loss=76.5946
	step [135/206], loss=84.6613
	step [136/206], loss=79.0829
	step [137/206], loss=79.6975
	step [138/206], loss=78.0511
	step [139/206], loss=63.7675
	step [140/206], loss=70.1415
	step [141/206], loss=82.0403
	step [142/206], loss=79.6903
	step [143/206], loss=63.5379
	step [144/206], loss=70.4268
	step [145/206], loss=87.4833
	step [146/206], loss=71.5226
	step [147/206], loss=80.8059
	step [148/206], loss=69.6250
	step [149/206], loss=73.4644
	step [150/206], loss=78.1265
	step [151/206], loss=83.6779
	step [152/206], loss=61.3676
	step [153/206], loss=75.3941
	step [154/206], loss=80.1018
	step [155/206], loss=65.9820
	step [156/206], loss=73.4476
	step [157/206], loss=75.1320
	step [158/206], loss=71.1517
	step [159/206], loss=68.1452
	step [160/206], loss=83.2853
	step [161/206], loss=71.9677
	step [162/206], loss=75.0908
	step [163/206], loss=62.9389
	step [164/206], loss=90.8379
	step [165/206], loss=70.9890
	step [166/206], loss=77.6302
	step [167/206], loss=67.3506
	step [168/206], loss=76.9839
	step [169/206], loss=67.9667
	step [170/206], loss=98.3788
	step [171/206], loss=80.2597
	step [172/206], loss=79.0068
	step [173/206], loss=76.4887
	step [174/206], loss=73.3473
	step [175/206], loss=70.2998
	step [176/206], loss=84.5697
	step [177/206], loss=84.9131
	step [178/206], loss=66.8127
	step [179/206], loss=77.8604
	step [180/206], loss=78.1240
	step [181/206], loss=87.9538
	step [182/206], loss=73.1047
	step [183/206], loss=86.0174
	step [184/206], loss=75.8314
	step [185/206], loss=71.3875
	step [186/206], loss=78.1228
	step [187/206], loss=68.7247
	step [188/206], loss=58.5812
	step [189/206], loss=75.9008
	step [190/206], loss=74.2231
	step [191/206], loss=82.6899
	step [192/206], loss=68.8718
	step [193/206], loss=87.1552
	step [194/206], loss=68.0296
	step [195/206], loss=65.8947
	step [196/206], loss=71.4308
	step [197/206], loss=67.1593
	step [198/206], loss=67.2761
	step [199/206], loss=86.1117
	step [200/206], loss=73.1733
	step [201/206], loss=70.3053
	step [202/206], loss=63.3336
	step [203/206], loss=84.3994
	step [204/206], loss=66.3352
	step [205/206], loss=66.5449
	step [206/206], loss=40.5239
	Evaluating
	loss=0.0062, precision=0.4425, recall=0.8981, f1=0.5928
Training epoch 77
	step [1/206], loss=80.5304
	step [2/206], loss=70.8762
	step [3/206], loss=76.2458
	step [4/206], loss=73.1758
	step [5/206], loss=65.8421
	step [6/206], loss=82.1090
	step [7/206], loss=70.2577
	step [8/206], loss=79.6807
	step [9/206], loss=83.6445
	step [10/206], loss=66.5321
	step [11/206], loss=80.6176
	step [12/206], loss=80.0449
	step [13/206], loss=79.9928
	step [14/206], loss=87.8353
	step [15/206], loss=74.0705
	step [16/206], loss=76.8776
	step [17/206], loss=74.6155
	step [18/206], loss=71.2357
	step [19/206], loss=70.9441
	step [20/206], loss=69.1406
	step [21/206], loss=75.9223
	step [22/206], loss=60.3077
	step [23/206], loss=87.2214
	step [24/206], loss=79.4600
	step [25/206], loss=85.5722
	step [26/206], loss=76.4175
	step [27/206], loss=82.5720
	step [28/206], loss=60.9940
	step [29/206], loss=81.8349
	step [30/206], loss=72.5010
	step [31/206], loss=73.9492
	step [32/206], loss=62.3631
	step [33/206], loss=70.5046
	step [34/206], loss=61.8825
	step [35/206], loss=71.2264
	step [36/206], loss=83.5534
	step [37/206], loss=82.8953
	step [38/206], loss=67.8047
	step [39/206], loss=61.7861
	step [40/206], loss=69.5132
	step [41/206], loss=73.3134
	step [42/206], loss=81.7046
	step [43/206], loss=82.0329
	step [44/206], loss=72.2284
	step [45/206], loss=74.8160
	step [46/206], loss=73.6863
	step [47/206], loss=74.6787
	step [48/206], loss=80.3429
	step [49/206], loss=69.7607
	step [50/206], loss=77.4591
	step [51/206], loss=81.3159
	step [52/206], loss=69.4694
	step [53/206], loss=68.3174
	step [54/206], loss=85.6929
	step [55/206], loss=68.9996
	step [56/206], loss=66.6657
	step [57/206], loss=78.0770
	step [58/206], loss=73.8888
	step [59/206], loss=75.9023
	step [60/206], loss=62.0781
	step [61/206], loss=80.4515
	step [62/206], loss=79.5276
	step [63/206], loss=65.5506
	step [64/206], loss=72.7516
	step [65/206], loss=64.9240
	step [66/206], loss=72.3928
	step [67/206], loss=73.7533
	step [68/206], loss=81.5793
	step [69/206], loss=54.7134
	step [70/206], loss=79.3757
	step [71/206], loss=71.0431
	step [72/206], loss=68.9340
	step [73/206], loss=71.8257
	step [74/206], loss=59.5526
	step [75/206], loss=87.9409
	step [76/206], loss=75.1865
	step [77/206], loss=68.5424
	step [78/206], loss=74.5866
	step [79/206], loss=73.6633
	step [80/206], loss=61.2609
	step [81/206], loss=71.7512
	step [82/206], loss=80.0274
	step [83/206], loss=73.2071
	step [84/206], loss=82.5691
	step [85/206], loss=68.2328
	step [86/206], loss=81.5731
	step [87/206], loss=86.4737
	step [88/206], loss=64.1494
	step [89/206], loss=67.2573
	step [90/206], loss=73.3267
	step [91/206], loss=74.9477
	step [92/206], loss=69.9151
	step [93/206], loss=74.7048
	step [94/206], loss=71.6203
	step [95/206], loss=65.2038
	step [96/206], loss=81.1222
	step [97/206], loss=65.0287
	step [98/206], loss=80.9256
	step [99/206], loss=67.1501
	step [100/206], loss=74.5657
	step [101/206], loss=82.7114
	step [102/206], loss=83.0779
	step [103/206], loss=77.6836
	step [104/206], loss=66.8763
	step [105/206], loss=68.1512
	step [106/206], loss=70.2726
	step [107/206], loss=65.5230
	step [108/206], loss=75.5368
	step [109/206], loss=76.4474
	step [110/206], loss=76.8598
	step [111/206], loss=75.6903
	step [112/206], loss=68.9815
	step [113/206], loss=79.2651
	step [114/206], loss=84.3662
	step [115/206], loss=75.5177
	step [116/206], loss=71.3846
	step [117/206], loss=64.9972
	step [118/206], loss=81.9970
	step [119/206], loss=84.8909
	step [120/206], loss=79.4805
	step [121/206], loss=71.9783
	step [122/206], loss=65.3107
	step [123/206], loss=78.0440
	step [124/206], loss=83.7000
	step [125/206], loss=75.2893
	step [126/206], loss=73.0264
	step [127/206], loss=90.9421
	step [128/206], loss=75.1714
	step [129/206], loss=72.5663
	step [130/206], loss=88.7813
	step [131/206], loss=72.7564
	step [132/206], loss=71.7351
	step [133/206], loss=78.5844
	step [134/206], loss=65.2849
	step [135/206], loss=64.4771
	step [136/206], loss=70.8385
	step [137/206], loss=61.2420
	step [138/206], loss=67.2837
	step [139/206], loss=77.6648
	step [140/206], loss=71.8825
	step [141/206], loss=67.5837
	step [142/206], loss=65.7730
	step [143/206], loss=84.7489
	step [144/206], loss=75.6653
	step [145/206], loss=87.0097
	step [146/206], loss=84.1612
	step [147/206], loss=73.3421
	step [148/206], loss=81.2418
	step [149/206], loss=82.6572
	step [150/206], loss=65.8833
	step [151/206], loss=64.6652
	step [152/206], loss=67.2216
	step [153/206], loss=68.0154
	step [154/206], loss=60.9264
	step [155/206], loss=76.3533
	step [156/206], loss=79.5603
	step [157/206], loss=69.2411
	step [158/206], loss=76.1775
	step [159/206], loss=72.6044
	step [160/206], loss=71.9214
	step [161/206], loss=83.7831
	step [162/206], loss=76.8444
	step [163/206], loss=67.8233
	step [164/206], loss=73.3195
	step [165/206], loss=66.3266
	step [166/206], loss=74.6269
	step [167/206], loss=67.6237
	step [168/206], loss=63.9550
	step [169/206], loss=81.0693
	step [170/206], loss=65.7163
	step [171/206], loss=79.3437
	step [172/206], loss=86.5767
	step [173/206], loss=69.4999
	step [174/206], loss=70.4020
	step [175/206], loss=90.8984
	step [176/206], loss=80.3375
	step [177/206], loss=88.8850
	step [178/206], loss=74.1475
	step [179/206], loss=97.0479
	step [180/206], loss=69.9133
	step [181/206], loss=66.6758
	step [182/206], loss=70.9397
	step [183/206], loss=78.8652
	step [184/206], loss=74.3125
	step [185/206], loss=81.7254
	step [186/206], loss=69.0385
	step [187/206], loss=82.0454
	step [188/206], loss=69.9378
	step [189/206], loss=74.5125
	step [190/206], loss=76.2901
	step [191/206], loss=67.2808
	step [192/206], loss=70.0121
	step [193/206], loss=75.0417
	step [194/206], loss=70.5512
	step [195/206], loss=59.4634
	step [196/206], loss=65.6667
	step [197/206], loss=69.7465
	step [198/206], loss=60.5448
	step [199/206], loss=61.3853
	step [200/206], loss=56.8137
	step [201/206], loss=73.0460
	step [202/206], loss=69.0065
	step [203/206], loss=77.9497
	step [204/206], loss=71.3367
	step [205/206], loss=58.9301
	step [206/206], loss=42.7406
	Evaluating
	loss=0.0061, precision=0.4506, recall=0.9054, f1=0.6017
Training epoch 78
	step [1/206], loss=72.7726
	step [2/206], loss=80.3528
	step [3/206], loss=59.5833
	step [4/206], loss=79.4394
	step [5/206], loss=62.2994
	step [6/206], loss=68.7862
	step [7/206], loss=79.0381
	step [8/206], loss=88.3479
	step [9/206], loss=74.1475
	step [10/206], loss=71.3754
	step [11/206], loss=82.0758
	step [12/206], loss=73.8821
	step [13/206], loss=65.9919
	step [14/206], loss=73.7180
	step [15/206], loss=77.5058
	step [16/206], loss=82.8754
	step [17/206], loss=59.1659
	step [18/206], loss=69.0799
	step [19/206], loss=92.2973
	step [20/206], loss=67.3147
	step [21/206], loss=74.2663
	step [22/206], loss=60.2249
	step [23/206], loss=78.1229
	step [24/206], loss=77.1509
	step [25/206], loss=62.2867
	step [26/206], loss=73.1029
	step [27/206], loss=69.5613
	step [28/206], loss=60.0641
	step [29/206], loss=70.3529
	step [30/206], loss=74.9400
	step [31/206], loss=83.6681
	step [32/206], loss=76.7591
	step [33/206], loss=78.1330
	step [34/206], loss=64.3124
	step [35/206], loss=73.5695
	step [36/206], loss=75.0213
	step [37/206], loss=62.8560
	step [38/206], loss=71.9121
	step [39/206], loss=70.0686
	step [40/206], loss=82.4181
	step [41/206], loss=81.0473
	step [42/206], loss=66.5890
	step [43/206], loss=64.0853
	step [44/206], loss=76.6384
	step [45/206], loss=72.7366
	step [46/206], loss=71.0559
	step [47/206], loss=78.2158
	step [48/206], loss=70.4944
	step [49/206], loss=77.8853
	step [50/206], loss=68.5660
	step [51/206], loss=76.5290
	step [52/206], loss=71.2975
	step [53/206], loss=79.2476
	step [54/206], loss=75.2634
	step [55/206], loss=79.0106
	step [56/206], loss=69.3116
	step [57/206], loss=86.0599
	step [58/206], loss=72.5193
	step [59/206], loss=77.6281
	step [60/206], loss=84.0156
	step [61/206], loss=75.9449
	step [62/206], loss=68.2022
	step [63/206], loss=68.5004
	step [64/206], loss=67.3740
	step [65/206], loss=78.8654
	step [66/206], loss=82.8097
	step [67/206], loss=75.1028
	step [68/206], loss=80.6880
	step [69/206], loss=85.3737
	step [70/206], loss=79.1240
	step [71/206], loss=76.4345
	step [72/206], loss=73.3077
	step [73/206], loss=72.7646
	step [74/206], loss=78.4445
	step [75/206], loss=67.4151
	step [76/206], loss=78.3526
	step [77/206], loss=72.1973
	step [78/206], loss=83.0826
	step [79/206], loss=86.3936
	step [80/206], loss=76.0630
	step [81/206], loss=88.4902
	step [82/206], loss=83.2518
	step [83/206], loss=80.6089
	step [84/206], loss=72.1256
	step [85/206], loss=69.1434
	step [86/206], loss=76.6083
	step [87/206], loss=84.5601
	step [88/206], loss=75.3660
	step [89/206], loss=83.9004
	step [90/206], loss=75.9152
	step [91/206], loss=71.6175
	step [92/206], loss=68.3464
	step [93/206], loss=70.1364
	step [94/206], loss=82.3787
	step [95/206], loss=71.8134
	step [96/206], loss=61.3419
	step [97/206], loss=70.1033
	step [98/206], loss=90.0358
	step [99/206], loss=70.1620
	step [100/206], loss=82.5317
	step [101/206], loss=73.2087
	step [102/206], loss=88.1508
	step [103/206], loss=76.6424
	step [104/206], loss=82.6452
	step [105/206], loss=77.1575
	step [106/206], loss=65.5013
	step [107/206], loss=79.8406
	step [108/206], loss=75.5086
	step [109/206], loss=67.8258
	step [110/206], loss=71.6222
	step [111/206], loss=64.5979
	step [112/206], loss=72.9227
	step [113/206], loss=68.0728
	step [114/206], loss=71.1480
	step [115/206], loss=72.1359
	step [116/206], loss=79.4653
	step [117/206], loss=79.0078
	step [118/206], loss=67.1488
	step [119/206], loss=77.0583
	step [120/206], loss=61.7964
	step [121/206], loss=95.2629
	step [122/206], loss=75.0837
	step [123/206], loss=70.8197
	step [124/206], loss=70.1544
	step [125/206], loss=65.0734
	step [126/206], loss=61.0179
	step [127/206], loss=70.0344
	step [128/206], loss=76.4885
	step [129/206], loss=82.8008
	step [130/206], loss=82.3783
	step [131/206], loss=79.1970
	step [132/206], loss=71.1655
	step [133/206], loss=94.7769
	step [134/206], loss=72.4175
	step [135/206], loss=72.6925
	step [136/206], loss=83.3221
	step [137/206], loss=67.4027
	step [138/206], loss=83.0248
	step [139/206], loss=78.9056
	step [140/206], loss=65.7424
	step [141/206], loss=70.4201
	step [142/206], loss=74.7301
	step [143/206], loss=84.3491
	step [144/206], loss=63.4755
	step [145/206], loss=77.6785
	step [146/206], loss=76.3833
	step [147/206], loss=72.0890
	step [148/206], loss=61.7178
	step [149/206], loss=78.7219
	step [150/206], loss=67.3251
	step [151/206], loss=64.6324
	step [152/206], loss=79.6958
	step [153/206], loss=79.5496
	step [154/206], loss=68.4326
	step [155/206], loss=62.9131
	step [156/206], loss=79.2000
	step [157/206], loss=78.8992
	step [158/206], loss=74.3460
	step [159/206], loss=86.8066
	step [160/206], loss=76.9198
	step [161/206], loss=64.6131
	step [162/206], loss=70.7317
	step [163/206], loss=69.4912
	step [164/206], loss=76.4172
	step [165/206], loss=79.5666
	step [166/206], loss=77.6489
	step [167/206], loss=63.0316
	step [168/206], loss=76.1566
	step [169/206], loss=79.4220
	step [170/206], loss=82.4289
	step [171/206], loss=69.4551
	step [172/206], loss=68.4590
	step [173/206], loss=71.0616
	step [174/206], loss=71.5777
	step [175/206], loss=73.9402
	step [176/206], loss=79.2440
	step [177/206], loss=72.0672
	step [178/206], loss=68.7659
	step [179/206], loss=79.8059
	step [180/206], loss=61.7865
	step [181/206], loss=80.4028
	step [182/206], loss=64.8972
	step [183/206], loss=62.9320
	step [184/206], loss=88.6779
	step [185/206], loss=68.3352
	step [186/206], loss=72.9511
	step [187/206], loss=81.0553
	step [188/206], loss=70.0829
	step [189/206], loss=65.3881
	step [190/206], loss=67.6459
	step [191/206], loss=69.9845
	step [192/206], loss=73.6062
	step [193/206], loss=60.4589
	step [194/206], loss=77.2448
	step [195/206], loss=58.0372
	step [196/206], loss=66.6906
	step [197/206], loss=63.6493
	step [198/206], loss=65.5038
	step [199/206], loss=77.0561
	step [200/206], loss=74.7091
	step [201/206], loss=70.1199
	step [202/206], loss=69.2614
	step [203/206], loss=67.7545
	step [204/206], loss=75.0159
	step [205/206], loss=87.2218
	step [206/206], loss=54.2937
	Evaluating
	loss=0.0056, precision=0.4690, recall=0.8934, f1=0.6151
Training epoch 79
	step [1/206], loss=75.1999
	step [2/206], loss=73.5653
	step [3/206], loss=76.1465
	step [4/206], loss=59.3819
	step [5/206], loss=56.6519
	step [6/206], loss=74.8570
	step [7/206], loss=77.1122
	step [8/206], loss=74.7343
	step [9/206], loss=64.9651
	step [10/206], loss=83.2350
	step [11/206], loss=79.7525
	step [12/206], loss=67.9690
	step [13/206], loss=84.4831
	step [14/206], loss=76.2224
	step [15/206], loss=67.3381
	step [16/206], loss=80.6343
	step [17/206], loss=72.0883
	step [18/206], loss=79.2129
	step [19/206], loss=67.9165
	step [20/206], loss=71.6699
	step [21/206], loss=73.9722
	step [22/206], loss=64.1795
	step [23/206], loss=82.2510
	step [24/206], loss=69.6856
	step [25/206], loss=68.1800
	step [26/206], loss=70.6105
	step [27/206], loss=64.4406
	step [28/206], loss=68.1435
	step [29/206], loss=60.7584
	step [30/206], loss=64.3404
	step [31/206], loss=69.2599
	step [32/206], loss=63.5459
	step [33/206], loss=94.3145
	step [34/206], loss=79.8575
	step [35/206], loss=68.1400
	step [36/206], loss=68.4038
	step [37/206], loss=88.1204
	step [38/206], loss=70.6582
	step [39/206], loss=56.1635
	step [40/206], loss=71.6056
	step [41/206], loss=76.8413
	step [42/206], loss=77.5317
	step [43/206], loss=64.9016
	step [44/206], loss=84.4404
	step [45/206], loss=71.6539
	step [46/206], loss=76.2185
	step [47/206], loss=67.7555
	step [48/206], loss=70.8214
	step [49/206], loss=70.2456
	step [50/206], loss=64.5053
	step [51/206], loss=71.7863
	step [52/206], loss=61.3663
	step [53/206], loss=82.2836
	step [54/206], loss=72.6458
	step [55/206], loss=74.7856
	step [56/206], loss=67.0752
	step [57/206], loss=77.9680
	step [58/206], loss=61.3914
	step [59/206], loss=76.6653
	step [60/206], loss=67.7943
	step [61/206], loss=76.3954
	step [62/206], loss=84.6639
	step [63/206], loss=67.5728
	step [64/206], loss=80.3662
	step [65/206], loss=59.1223
	step [66/206], loss=73.7318
	step [67/206], loss=73.2356
	step [68/206], loss=58.6222
	step [69/206], loss=69.1204
	step [70/206], loss=71.2811
	step [71/206], loss=84.2519
	step [72/206], loss=81.0412
	step [73/206], loss=69.7863
	step [74/206], loss=66.3590
	step [75/206], loss=69.0106
	step [76/206], loss=67.4835
	step [77/206], loss=71.3907
	step [78/206], loss=72.4257
	step [79/206], loss=73.0074
	step [80/206], loss=79.3176
	step [81/206], loss=69.9089
	step [82/206], loss=64.2876
	step [83/206], loss=78.7845
	step [84/206], loss=73.2289
	step [85/206], loss=69.4929
	step [86/206], loss=58.2875
	step [87/206], loss=74.6612
	step [88/206], loss=79.5934
	step [89/206], loss=61.9552
	step [90/206], loss=80.4789
	step [91/206], loss=71.5410
	step [92/206], loss=61.0611
	step [93/206], loss=78.5355
	step [94/206], loss=66.7797
	step [95/206], loss=74.9925
	step [96/206], loss=77.8820
	step [97/206], loss=81.0611
	step [98/206], loss=81.1960
	step [99/206], loss=79.9425
	step [100/206], loss=72.6899
	step [101/206], loss=76.5116
	step [102/206], loss=81.8738
	step [103/206], loss=91.2805
	step [104/206], loss=80.7849
	step [105/206], loss=76.9320
	step [106/206], loss=77.3209
	step [107/206], loss=82.2479
	step [108/206], loss=67.3415
	step [109/206], loss=75.0852
	step [110/206], loss=72.9045
	step [111/206], loss=68.4302
	step [112/206], loss=86.3919
	step [113/206], loss=81.8086
	step [114/206], loss=64.8563
	step [115/206], loss=71.6325
	step [116/206], loss=81.4337
	step [117/206], loss=64.9822
	step [118/206], loss=71.1327
	step [119/206], loss=84.8856
	step [120/206], loss=84.1508
	step [121/206], loss=74.8770
	step [122/206], loss=70.3824
	step [123/206], loss=75.9896
	step [124/206], loss=70.7556
	step [125/206], loss=70.7315
	step [126/206], loss=69.2056
	step [127/206], loss=69.4937
	step [128/206], loss=73.4896
	step [129/206], loss=73.5322
	step [130/206], loss=74.7161
	step [131/206], loss=78.2095
	step [132/206], loss=72.7182
	step [133/206], loss=71.0445
	step [134/206], loss=65.5365
	step [135/206], loss=63.0138
	step [136/206], loss=67.0423
	step [137/206], loss=78.4776
	step [138/206], loss=72.2187
	step [139/206], loss=67.8319
	step [140/206], loss=72.9321
	step [141/206], loss=68.3793
	step [142/206], loss=69.1320
	step [143/206], loss=81.5702
	step [144/206], loss=75.0888
	step [145/206], loss=77.2809
	step [146/206], loss=68.1790
	step [147/206], loss=72.7589
	step [148/206], loss=85.3947
	step [149/206], loss=65.2990
	step [150/206], loss=84.0864
	step [151/206], loss=70.1568
	step [152/206], loss=72.8943
	step [153/206], loss=78.5166
	step [154/206], loss=69.7942
	step [155/206], loss=92.7134
	step [156/206], loss=71.4754
	step [157/206], loss=79.1657
	step [158/206], loss=57.3723
	step [159/206], loss=73.6728
	step [160/206], loss=66.7367
	step [161/206], loss=74.8009
	step [162/206], loss=80.0191
	step [163/206], loss=94.2274
	step [164/206], loss=83.1240
	step [165/206], loss=69.4501
	step [166/206], loss=73.8212
	step [167/206], loss=69.9805
	step [168/206], loss=77.7082
	step [169/206], loss=72.8303
	step [170/206], loss=76.1588
	step [171/206], loss=72.5925
	step [172/206], loss=86.9869
	step [173/206], loss=72.9635
	step [174/206], loss=69.9937
	step [175/206], loss=59.3892
	step [176/206], loss=73.3197
	step [177/206], loss=77.1193
	step [178/206], loss=77.6763
	step [179/206], loss=66.0104
	step [180/206], loss=68.4145
	step [181/206], loss=72.8639
	step [182/206], loss=79.9573
	step [183/206], loss=72.6079
	step [184/206], loss=77.4750
	step [185/206], loss=72.8231
	step [186/206], loss=81.6030
	step [187/206], loss=71.6262
	step [188/206], loss=77.9767
	step [189/206], loss=76.5148
	step [190/206], loss=80.7670
	step [191/206], loss=73.0183
	step [192/206], loss=65.7809
	step [193/206], loss=76.2527
	step [194/206], loss=76.3496
	step [195/206], loss=66.9379
	step [196/206], loss=75.1820
	step [197/206], loss=64.3239
	step [198/206], loss=77.5244
	step [199/206], loss=85.1435
	step [200/206], loss=68.6113
	step [201/206], loss=60.3683
	step [202/206], loss=71.6746
	step [203/206], loss=83.0750
	step [204/206], loss=77.9365
	step [205/206], loss=79.0632
	step [206/206], loss=52.0966
	Evaluating
	loss=0.0059, precision=0.4560, recall=0.9005, f1=0.6054
Training epoch 80
	step [1/206], loss=77.1605
	step [2/206], loss=69.0154
	step [3/206], loss=74.1829
	step [4/206], loss=74.5224
	step [5/206], loss=69.7788
	step [6/206], loss=71.8810
	step [7/206], loss=65.1020
	step [8/206], loss=78.7203
	step [9/206], loss=70.8878
	step [10/206], loss=66.6150
	step [11/206], loss=79.7596
	step [12/206], loss=66.8538
	step [13/206], loss=69.5951
	step [14/206], loss=66.8259
	step [15/206], loss=56.7712
	step [16/206], loss=70.8553
	step [17/206], loss=83.3040
	step [18/206], loss=61.7632
	step [19/206], loss=71.5382
	step [20/206], loss=76.5088
	step [21/206], loss=67.7341
	step [22/206], loss=81.9623
	step [23/206], loss=75.3657
	step [24/206], loss=72.2598
	step [25/206], loss=71.0686
	step [26/206], loss=69.7592
	step [27/206], loss=69.6790
	step [28/206], loss=66.6897
	step [29/206], loss=73.4747
	step [30/206], loss=79.9126
	step [31/206], loss=65.7771
	step [32/206], loss=75.5031
	step [33/206], loss=65.9538
	step [34/206], loss=80.7423
	step [35/206], loss=75.6098
	step [36/206], loss=67.6397
	step [37/206], loss=62.9142
	step [38/206], loss=84.1624
	step [39/206], loss=86.9582
	step [40/206], loss=88.9362
	step [41/206], loss=78.0233
	step [42/206], loss=77.7857
	step [43/206], loss=78.2382
	step [44/206], loss=68.4897
	step [45/206], loss=79.5701
	step [46/206], loss=77.9108
	step [47/206], loss=74.5182
	step [48/206], loss=73.3896
	step [49/206], loss=78.6608
	step [50/206], loss=78.9589
	step [51/206], loss=77.0391
	step [52/206], loss=73.9442
	step [53/206], loss=51.1470
	step [54/206], loss=69.3443
	step [55/206], loss=64.9223
	step [56/206], loss=57.1014
	step [57/206], loss=78.6939
	step [58/206], loss=75.3008
	step [59/206], loss=64.0333
	step [60/206], loss=69.8223
	step [61/206], loss=77.4947
	step [62/206], loss=81.5345
	step [63/206], loss=72.7974
	step [64/206], loss=72.9150
	step [65/206], loss=77.7476
	step [66/206], loss=69.3431
	step [67/206], loss=75.9799
	step [68/206], loss=71.8167
	step [69/206], loss=79.0301
	step [70/206], loss=62.0164
	step [71/206], loss=82.1958
	step [72/206], loss=62.0457
	step [73/206], loss=70.0213
	step [74/206], loss=76.0259
	step [75/206], loss=64.8496
	step [76/206], loss=78.5264
	step [77/206], loss=69.8564
	step [78/206], loss=66.1035
	step [79/206], loss=66.2191
	step [80/206], loss=76.2715
	step [81/206], loss=68.3480
	step [82/206], loss=62.8693
	step [83/206], loss=79.5994
	step [84/206], loss=60.8780
	step [85/206], loss=72.4857
	step [86/206], loss=81.2334
	step [87/206], loss=73.4835
	step [88/206], loss=85.6492
	step [89/206], loss=69.8234
	step [90/206], loss=73.7588
	step [91/206], loss=72.6267
	step [92/206], loss=87.4438
	step [93/206], loss=65.5725
	step [94/206], loss=67.0821
	step [95/206], loss=68.8647
	step [96/206], loss=71.4099
	step [97/206], loss=80.4250
	step [98/206], loss=70.1872
	step [99/206], loss=73.9625
	step [100/206], loss=81.6144
	step [101/206], loss=68.6940
	step [102/206], loss=66.8083
	step [103/206], loss=79.2324
	step [104/206], loss=64.2648
	step [105/206], loss=78.9294
	step [106/206], loss=71.8736
	step [107/206], loss=72.5474
	step [108/206], loss=76.2694
	step [109/206], loss=79.1185
	step [110/206], loss=68.5741
	step [111/206], loss=75.6453
	step [112/206], loss=62.3495
	step [113/206], loss=77.0899
	step [114/206], loss=75.3662
	step [115/206], loss=75.6357
	step [116/206], loss=70.9139
	step [117/206], loss=79.9792
	step [118/206], loss=65.3222
	step [119/206], loss=61.7806
	step [120/206], loss=67.3687
	step [121/206], loss=80.6016
	step [122/206], loss=71.0453
	step [123/206], loss=65.3280
	step [124/206], loss=75.3975
	step [125/206], loss=75.4120
	step [126/206], loss=77.1745
	step [127/206], loss=80.0208
	step [128/206], loss=75.3642
	step [129/206], loss=71.8931
	step [130/206], loss=65.9783
	step [131/206], loss=67.5843
	step [132/206], loss=73.4109
	step [133/206], loss=64.4473
	step [134/206], loss=68.3812
	step [135/206], loss=68.3402
	step [136/206], loss=76.8901
	step [137/206], loss=61.2553
	step [138/206], loss=68.1606
	step [139/206], loss=79.0630
	step [140/206], loss=76.6343
	step [141/206], loss=83.3215
	step [142/206], loss=72.1991
	step [143/206], loss=65.0502
	step [144/206], loss=82.9658
	step [145/206], loss=80.7417
	step [146/206], loss=87.2624
	step [147/206], loss=72.0423
	step [148/206], loss=68.2714
	step [149/206], loss=69.1494
	step [150/206], loss=69.2135
	step [151/206], loss=83.8953
	step [152/206], loss=72.8540
	step [153/206], loss=65.2724
	step [154/206], loss=77.8124
	step [155/206], loss=73.4430
	step [156/206], loss=85.5970
	step [157/206], loss=75.2008
	step [158/206], loss=76.8286
	step [159/206], loss=87.4598
	step [160/206], loss=59.5075
	step [161/206], loss=69.4119
	step [162/206], loss=67.7879
	step [163/206], loss=69.0113
	step [164/206], loss=77.0393
	step [165/206], loss=68.3102
	step [166/206], loss=75.1492
	step [167/206], loss=68.7188
	step [168/206], loss=81.7688
	step [169/206], loss=69.9380
	step [170/206], loss=79.0498
	step [171/206], loss=66.0345
	step [172/206], loss=62.6736
	step [173/206], loss=67.1403
	step [174/206], loss=72.0731
	step [175/206], loss=72.8757
	step [176/206], loss=69.1623
	step [177/206], loss=78.5582
	step [178/206], loss=78.4795
	step [179/206], loss=76.0179
	step [180/206], loss=80.8917
	step [181/206], loss=77.2340
	step [182/206], loss=79.3800
	step [183/206], loss=61.1945
	step [184/206], loss=71.4891
	step [185/206], loss=81.3864
	step [186/206], loss=65.5407
	step [187/206], loss=78.9475
	step [188/206], loss=76.2579
	step [189/206], loss=86.4731
	step [190/206], loss=73.0877
	step [191/206], loss=72.6213
	step [192/206], loss=72.5303
	step [193/206], loss=74.5647
	step [194/206], loss=63.2552
	step [195/206], loss=75.8807
	step [196/206], loss=73.9331
	step [197/206], loss=59.6943
	step [198/206], loss=77.0699
	step [199/206], loss=81.7993
	step [200/206], loss=79.8889
	step [201/206], loss=66.0278
	step [202/206], loss=74.0021
	step [203/206], loss=78.7875
	step [204/206], loss=79.2176
	step [205/206], loss=75.9937
	step [206/206], loss=61.0871
	Evaluating
	loss=0.0059, precision=0.4541, recall=0.9012, f1=0.6039
Training finished
best_f1: 0.621517789609649
finish!
...training function Done
