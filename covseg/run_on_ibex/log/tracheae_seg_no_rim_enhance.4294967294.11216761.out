Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: True test_id 0
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 9412 # image files with weight 9412
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 2471 # image files with weight 2471
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/X 9412
Using 4 GPUs
Going to train epochs [58-107]
Training epoch 58
	step [1/148], loss=78.3980
	step [2/148], loss=105.6599
	step [3/148], loss=86.1737
	step [4/148], loss=97.7800
	step [5/148], loss=89.5301
	step [6/148], loss=107.6337
	step [7/148], loss=89.9890
	step [8/148], loss=107.9293
	step [9/148], loss=96.4623
	step [10/148], loss=92.4155
	step [11/148], loss=97.3648
	step [12/148], loss=86.9457
	step [13/148], loss=76.1789
	step [14/148], loss=80.6051
	step [15/148], loss=94.4117
	step [16/148], loss=94.7949
	step [17/148], loss=90.8129
	step [18/148], loss=91.2582
	step [19/148], loss=97.4288
	step [20/148], loss=96.6479
	step [21/148], loss=106.3298
	step [22/148], loss=94.1798
	step [23/148], loss=91.0107
	step [24/148], loss=87.5012
	step [25/148], loss=107.0222
	step [26/148], loss=92.5919
	step [27/148], loss=103.3698
	step [28/148], loss=90.7776
	step [29/148], loss=98.6215
	step [30/148], loss=87.5044
	step [31/148], loss=88.8730
	step [32/148], loss=111.9054
	step [33/148], loss=113.1693
	step [34/148], loss=89.5592
	step [35/148], loss=94.8105
	step [36/148], loss=82.8848
	step [37/148], loss=99.4695
	step [38/148], loss=99.7396
	step [39/148], loss=98.8331
	step [40/148], loss=95.2021
	step [41/148], loss=95.6568
	step [42/148], loss=98.3180
	step [43/148], loss=97.9382
	step [44/148], loss=96.0377
	step [45/148], loss=88.0676
	step [46/148], loss=99.4033
	step [47/148], loss=82.5311
	step [48/148], loss=93.7469
	step [49/148], loss=93.7397
	step [50/148], loss=102.9709
	step [51/148], loss=93.8500
	step [52/148], loss=108.4562
	step [53/148], loss=88.7817
	step [54/148], loss=104.7482
	step [55/148], loss=78.4067
	step [56/148], loss=80.4063
	step [57/148], loss=103.3903
	step [58/148], loss=80.1283
	step [59/148], loss=83.8532
	step [60/148], loss=80.1561
	step [61/148], loss=102.3780
	step [62/148], loss=90.7040
	step [63/148], loss=112.9005
	step [64/148], loss=93.5796
	step [65/148], loss=81.2305
	step [66/148], loss=106.1856
	step [67/148], loss=89.7183
	step [68/148], loss=96.2369
	step [69/148], loss=87.9209
	step [70/148], loss=96.7431
	step [71/148], loss=104.9346
	step [72/148], loss=89.9825
	step [73/148], loss=107.7796
	step [74/148], loss=73.9684
	step [75/148], loss=78.9050
	step [76/148], loss=92.0609
	step [77/148], loss=119.9631
	step [78/148], loss=83.8318
	step [79/148], loss=92.1293
	step [80/148], loss=85.1780
	step [81/148], loss=93.5073
	step [82/148], loss=99.4479
	step [83/148], loss=86.3531
	step [84/148], loss=102.1962
	step [85/148], loss=100.1570
	step [86/148], loss=88.2707
	step [87/148], loss=104.8833
	step [88/148], loss=95.5438
	step [89/148], loss=85.0720
	step [90/148], loss=92.9914
	step [91/148], loss=80.3898
	step [92/148], loss=94.7775
	step [93/148], loss=81.9569
	step [94/148], loss=81.9006
	step [95/148], loss=82.2016
	step [96/148], loss=85.5534
	step [97/148], loss=100.3319
	step [98/148], loss=98.6148
	step [99/148], loss=74.3648
	step [100/148], loss=85.8806
	step [101/148], loss=98.4965
	step [102/148], loss=91.1705
	step [103/148], loss=77.9738
	step [104/148], loss=93.6772
	step [105/148], loss=100.6743
	step [106/148], loss=105.4679
	step [107/148], loss=106.1373
	step [108/148], loss=104.0740
	step [109/148], loss=106.5763
	step [110/148], loss=103.3172
	step [111/148], loss=93.3866
	step [112/148], loss=83.5126
	step [113/148], loss=103.9483
	step [114/148], loss=103.5408
	step [115/148], loss=79.5269
	step [116/148], loss=84.7151
	step [117/148], loss=81.3671
	step [118/148], loss=76.0741
	step [119/148], loss=87.5449
	step [120/148], loss=91.2356
	step [121/148], loss=87.2782
	step [122/148], loss=88.7824
	step [123/148], loss=98.3018
	step [124/148], loss=86.6763
	step [125/148], loss=89.7290
	step [126/148], loss=92.5206
	step [127/148], loss=90.7577
	step [128/148], loss=96.9506
	step [129/148], loss=100.8513
	step [130/148], loss=103.0216
	step [131/148], loss=81.9166
	step [132/148], loss=83.0577
	step [133/148], loss=94.2931
	step [134/148], loss=88.0993
	step [135/148], loss=102.9737
	step [136/148], loss=97.3505
	step [137/148], loss=83.0502
	step [138/148], loss=92.8994
	step [139/148], loss=101.7151
	step [140/148], loss=90.4531
	step [141/148], loss=91.0617
	step [142/148], loss=99.0158
	step [143/148], loss=97.7317
	step [144/148], loss=100.7714
	step [145/148], loss=111.1340
	step [146/148], loss=97.2531
	step [147/148], loss=103.7457
	step [148/148], loss=6.0913
	Evaluating
	loss=0.0161, precision=0.2528, recall=0.8392, f1=0.3886
saving model as: 0_saved_model.pth
Training epoch 59
	step [1/148], loss=100.1673
	step [2/148], loss=127.8591
	step [3/148], loss=84.0870
	step [4/148], loss=87.7249
	step [5/148], loss=78.6045
	step [6/148], loss=100.0910
	step [7/148], loss=86.0753
	step [8/148], loss=113.1732
	step [9/148], loss=90.9415
	step [10/148], loss=82.9492
	step [11/148], loss=96.5248
	step [12/148], loss=94.9156
	step [13/148], loss=95.4464
	step [14/148], loss=92.2424
	step [15/148], loss=107.4885
	step [16/148], loss=101.0266
	step [17/148], loss=108.2168
	step [18/148], loss=85.3346
	step [19/148], loss=86.4215
	step [20/148], loss=94.6494
	step [21/148], loss=68.9057
	step [22/148], loss=101.0132
	step [23/148], loss=95.4236
	step [24/148], loss=93.6409
	step [25/148], loss=97.6100
	step [26/148], loss=110.8213
	step [27/148], loss=100.7566
	step [28/148], loss=93.0364
	step [29/148], loss=96.1556
	step [30/148], loss=91.5065
	step [31/148], loss=111.5602
	step [32/148], loss=101.8405
	step [33/148], loss=90.4699
	step [34/148], loss=87.8727
	step [35/148], loss=86.1344
	step [36/148], loss=86.1006
	step [37/148], loss=89.7400
	step [38/148], loss=85.0888
	step [39/148], loss=101.1929
	step [40/148], loss=88.6115
	step [41/148], loss=85.8812
	step [42/148], loss=105.2083
	step [43/148], loss=84.9191
	step [44/148], loss=94.9666
	step [45/148], loss=100.1702
	step [46/148], loss=97.5553
	step [47/148], loss=94.2420
	step [48/148], loss=93.8880
	step [49/148], loss=91.3496
	step [50/148], loss=89.3399
	step [51/148], loss=84.7837
	step [52/148], loss=80.7182
	step [53/148], loss=93.6187
	step [54/148], loss=99.1151
	step [55/148], loss=86.9448
	step [56/148], loss=95.7234
	step [57/148], loss=105.1298
	step [58/148], loss=93.3747
	step [59/148], loss=86.2626
	step [60/148], loss=103.0937
	step [61/148], loss=93.0984
	step [62/148], loss=86.0711
	step [63/148], loss=86.2972
	step [64/148], loss=96.6783
	step [65/148], loss=89.6472
	step [66/148], loss=76.9500
	step [67/148], loss=95.5762
	step [68/148], loss=83.7992
	step [69/148], loss=98.0071
	step [70/148], loss=98.9896
	step [71/148], loss=94.2455
	step [72/148], loss=93.3087
	step [73/148], loss=104.1286
	step [74/148], loss=91.6872
	step [75/148], loss=91.6690
	step [76/148], loss=96.9132
	step [77/148], loss=97.9221
	step [78/148], loss=95.9944
	step [79/148], loss=85.7413
	step [80/148], loss=85.8792
	step [81/148], loss=72.4533
	step [82/148], loss=91.9196
	step [83/148], loss=81.5342
	step [84/148], loss=75.7404
	step [85/148], loss=87.4769
	step [86/148], loss=97.3512
	step [87/148], loss=102.5455
	step [88/148], loss=78.4407
	step [89/148], loss=101.7187
	step [90/148], loss=91.0659
	step [91/148], loss=97.5016
	step [92/148], loss=82.8608
	step [93/148], loss=87.7931
	step [94/148], loss=92.4983
	step [95/148], loss=94.3870
	step [96/148], loss=83.5684
	step [97/148], loss=85.3284
	step [98/148], loss=93.1481
	step [99/148], loss=111.8846
	step [100/148], loss=97.3099
	step [101/148], loss=93.4321
	step [102/148], loss=90.9374
	step [103/148], loss=93.5349
	step [104/148], loss=88.8127
	step [105/148], loss=98.0751
	step [106/148], loss=101.8252
	step [107/148], loss=108.4239
	step [108/148], loss=93.3930
	step [109/148], loss=102.2057
	step [110/148], loss=85.7567
	step [111/148], loss=90.1262
	step [112/148], loss=101.7691
	step [113/148], loss=95.9177
	step [114/148], loss=86.2860
	step [115/148], loss=90.8422
	step [116/148], loss=89.5557
	step [117/148], loss=86.1356
	step [118/148], loss=92.1224
	step [119/148], loss=99.9925
	step [120/148], loss=98.6823
	step [121/148], loss=87.8077
	step [122/148], loss=99.5567
	step [123/148], loss=96.4587
	step [124/148], loss=83.8026
	step [125/148], loss=82.3027
	step [126/148], loss=93.6252
	step [127/148], loss=90.1678
	step [128/148], loss=73.8223
	step [129/148], loss=99.0123
	step [130/148], loss=84.5371
	step [131/148], loss=97.5711
	step [132/148], loss=109.8282
	step [133/148], loss=97.5800
	step [134/148], loss=91.1131
	step [135/148], loss=89.5753
	step [136/148], loss=86.1877
	step [137/148], loss=113.2317
	step [138/148], loss=98.0132
	step [139/148], loss=122.2764
	step [140/148], loss=89.5092
	step [141/148], loss=96.7121
	step [142/148], loss=100.7363
	step [143/148], loss=112.2018
	step [144/148], loss=94.5603
	step [145/148], loss=96.8960
	step [146/148], loss=84.1232
	step [147/148], loss=82.8929
	step [148/148], loss=8.4723
	Evaluating
	loss=0.0153, precision=0.2608, recall=0.8602, f1=0.4003
saving model as: 0_saved_model.pth
Training epoch 60
	step [1/148], loss=102.0302
	step [2/148], loss=99.5982
	step [3/148], loss=71.0360
	step [4/148], loss=101.8965
	step [5/148], loss=90.2547
	step [6/148], loss=83.8265
	step [7/148], loss=99.9835
	step [8/148], loss=89.7957
	step [9/148], loss=88.9356
	step [10/148], loss=91.3557
	step [11/148], loss=86.2166
	step [12/148], loss=94.1541
	step [13/148], loss=99.0781
	step [14/148], loss=97.6486
	step [15/148], loss=96.9432
	step [16/148], loss=90.7311
	step [17/148], loss=98.0278
	step [18/148], loss=89.7240
	step [19/148], loss=90.4069
	step [20/148], loss=100.2758
	step [21/148], loss=101.1405
	step [22/148], loss=91.5270
	step [23/148], loss=87.3872
	step [24/148], loss=86.6689
	step [25/148], loss=100.6424
	step [26/148], loss=96.4392
	step [27/148], loss=91.8558
	step [28/148], loss=87.1976
	step [29/148], loss=96.5115
	step [30/148], loss=101.7102
	step [31/148], loss=92.5331
	step [32/148], loss=93.6547
	step [33/148], loss=103.4770
	step [34/148], loss=87.7348
	step [35/148], loss=112.8319
	step [36/148], loss=74.6253
	step [37/148], loss=80.7921
	step [38/148], loss=87.8360
	step [39/148], loss=101.9883
	step [40/148], loss=89.4495
	step [41/148], loss=95.7746
	step [42/148], loss=78.3667
	step [43/148], loss=71.1471
	step [44/148], loss=103.6427
	step [45/148], loss=85.6949
	step [46/148], loss=88.6636
	step [47/148], loss=75.7324
	step [48/148], loss=98.9962
	step [49/148], loss=75.6613
	step [50/148], loss=107.4591
	step [51/148], loss=106.4624
	step [52/148], loss=92.2133
	step [53/148], loss=108.3853
	step [54/148], loss=91.2091
	step [55/148], loss=92.7472
	step [56/148], loss=107.0451
	step [57/148], loss=87.8294
	step [58/148], loss=112.4602
	step [59/148], loss=80.1442
	step [60/148], loss=70.8049
	step [61/148], loss=105.8885
	step [62/148], loss=87.3628
	step [63/148], loss=84.9119
	step [64/148], loss=96.8264
	step [65/148], loss=97.8411
	step [66/148], loss=109.9849
	step [67/148], loss=91.8792
	step [68/148], loss=92.7758
	step [69/148], loss=76.1528
	step [70/148], loss=83.5328
	step [71/148], loss=95.7958
	step [72/148], loss=79.0857
	step [73/148], loss=118.8923
	step [74/148], loss=91.7175
	step [75/148], loss=79.4904
	step [76/148], loss=101.5890
	step [77/148], loss=79.7843
	step [78/148], loss=80.6102
	step [79/148], loss=77.3677
	step [80/148], loss=85.4673
	step [81/148], loss=90.1443
	step [82/148], loss=88.1451
	step [83/148], loss=95.9524
	step [84/148], loss=85.3090
	step [85/148], loss=72.2975
	step [86/148], loss=93.8959
	step [87/148], loss=100.8839
	step [88/148], loss=94.3440
	step [89/148], loss=94.4948
	step [90/148], loss=104.0576
	step [91/148], loss=98.4960
	step [92/148], loss=83.0476
	step [93/148], loss=87.7638
	step [94/148], loss=89.9399
	step [95/148], loss=97.0561
	step [96/148], loss=89.1978
	step [97/148], loss=106.6702
	step [98/148], loss=88.0827
	step [99/148], loss=87.7343
	step [100/148], loss=87.7557
	step [101/148], loss=95.8772
	step [102/148], loss=85.2073
	step [103/148], loss=75.1927
	step [104/148], loss=108.8276
	step [105/148], loss=108.5495
	step [106/148], loss=100.3086
	step [107/148], loss=98.4970
	step [108/148], loss=99.8291
	step [109/148], loss=106.3804
	step [110/148], loss=84.6716
	step [111/148], loss=93.1909
	step [112/148], loss=86.9263
	step [113/148], loss=91.9605
	step [114/148], loss=86.8391
	step [115/148], loss=113.2462
	step [116/148], loss=91.8714
	step [117/148], loss=78.3697
	step [118/148], loss=94.5894
	step [119/148], loss=87.5960
	step [120/148], loss=98.6671
	step [121/148], loss=98.1724
	step [122/148], loss=92.3842
	step [123/148], loss=75.4518
	step [124/148], loss=105.7913
	step [125/148], loss=77.6449
	step [126/148], loss=109.0153
	step [127/148], loss=90.0200
	step [128/148], loss=90.2327
	step [129/148], loss=81.2222
	step [130/148], loss=85.6447
	step [131/148], loss=91.9658
	step [132/148], loss=97.6762
	step [133/148], loss=102.2906
	step [134/148], loss=91.7071
	step [135/148], loss=105.4936
	step [136/148], loss=98.9995
	step [137/148], loss=79.0281
	step [138/148], loss=104.0461
	step [139/148], loss=87.1259
	step [140/148], loss=87.2489
	step [141/148], loss=100.8470
	step [142/148], loss=81.4225
	step [143/148], loss=101.6615
	step [144/148], loss=109.7622
	step [145/148], loss=102.8649
	step [146/148], loss=90.9966
	step [147/148], loss=77.2823
	step [148/148], loss=8.3859
	Evaluating
	loss=0.0162, precision=0.2530, recall=0.8578, f1=0.3907
Training epoch 61
	step [1/148], loss=96.3679
	step [2/148], loss=84.9989
	step [3/148], loss=90.7093
	step [4/148], loss=99.2304
	step [5/148], loss=95.5358
	step [6/148], loss=80.4714
	step [7/148], loss=100.3174
	step [8/148], loss=80.0016
	step [9/148], loss=105.8237
	step [10/148], loss=114.2451
	step [11/148], loss=80.6723
	step [12/148], loss=95.7389
	step [13/148], loss=97.9234
	step [14/148], loss=91.5704
	step [15/148], loss=97.1465
	step [16/148], loss=87.6569
	step [17/148], loss=82.5679
	step [18/148], loss=88.9946
	step [19/148], loss=96.5854
	step [20/148], loss=85.7428
	step [21/148], loss=84.6168
	step [22/148], loss=99.1091
	step [23/148], loss=104.3468
	step [24/148], loss=90.3753
	step [25/148], loss=96.8547
	step [26/148], loss=96.8918
	step [27/148], loss=90.8228
	step [28/148], loss=86.4364
	step [29/148], loss=89.7010
	step [30/148], loss=91.9920
	step [31/148], loss=101.4535
	step [32/148], loss=97.2620
	step [33/148], loss=100.6901
	step [34/148], loss=93.5334
	step [35/148], loss=93.9300
	step [36/148], loss=89.3784
	step [37/148], loss=88.1060
	step [38/148], loss=102.5736
	step [39/148], loss=98.9542
	step [40/148], loss=79.5943
	step [41/148], loss=88.5474
	step [42/148], loss=96.1308
	step [43/148], loss=87.2685
	step [44/148], loss=79.2774
	step [45/148], loss=97.3195
	step [46/148], loss=77.1402
	step [47/148], loss=91.8175
	step [48/148], loss=104.4368
	step [49/148], loss=80.2470
	step [50/148], loss=92.8352
	step [51/148], loss=81.6333
	step [52/148], loss=99.3309
	step [53/148], loss=98.7040
	step [54/148], loss=87.3467
	step [55/148], loss=94.1053
	step [56/148], loss=94.7854
	step [57/148], loss=71.8276
	step [58/148], loss=92.9707
	step [59/148], loss=96.8128
	step [60/148], loss=76.3674
	step [61/148], loss=84.8644
	step [62/148], loss=114.5295
	step [63/148], loss=97.8559
	step [64/148], loss=100.7942
	step [65/148], loss=94.5049
	step [66/148], loss=80.9128
	step [67/148], loss=111.1173
	step [68/148], loss=96.4574
	step [69/148], loss=92.5478
	step [70/148], loss=71.7111
	step [71/148], loss=91.0370
	step [72/148], loss=91.9224
	step [73/148], loss=92.3353
	step [74/148], loss=109.2351
	step [75/148], loss=82.5915
	step [76/148], loss=96.0937
	step [77/148], loss=97.9570
	step [78/148], loss=89.2131
	step [79/148], loss=94.1314
	step [80/148], loss=94.6508
	step [81/148], loss=82.5133
	step [82/148], loss=84.1391
	step [83/148], loss=108.5286
	step [84/148], loss=84.5840
	step [85/148], loss=105.5102
	step [86/148], loss=89.5785
	step [87/148], loss=103.3227
	step [88/148], loss=91.6519
	step [89/148], loss=96.4329
	step [90/148], loss=96.0918
	step [91/148], loss=89.1501
	step [92/148], loss=92.2084
	step [93/148], loss=100.8012
	step [94/148], loss=94.2804
	step [95/148], loss=93.8124
	step [96/148], loss=100.3612
	step [97/148], loss=80.6786
	step [98/148], loss=92.2735
	step [99/148], loss=100.3157
	step [100/148], loss=76.5891
	step [101/148], loss=102.0918
	step [102/148], loss=88.1077
	step [103/148], loss=90.0183
	step [104/148], loss=86.3554
	step [105/148], loss=98.1600
	step [106/148], loss=89.5250
	step [107/148], loss=90.4051
	step [108/148], loss=105.7075
	step [109/148], loss=85.0950
	step [110/148], loss=90.4380
	step [111/148], loss=100.1105
	step [112/148], loss=80.0668
	step [113/148], loss=86.7514
	step [114/148], loss=91.6362
	step [115/148], loss=120.0386
	step [116/148], loss=94.4390
	step [117/148], loss=91.7690
	step [118/148], loss=91.6853
	step [119/148], loss=100.8023
	step [120/148], loss=91.6701
	step [121/148], loss=80.8695
	step [122/148], loss=91.7243
	step [123/148], loss=86.1141
	step [124/148], loss=104.4219
	step [125/148], loss=82.5710
	step [126/148], loss=90.2411
	step [127/148], loss=95.0780
	step [128/148], loss=83.0469
	step [129/148], loss=72.1690
	step [130/148], loss=91.2140
	step [131/148], loss=91.7411
	step [132/148], loss=77.7575
	step [133/148], loss=88.1148
	step [134/148], loss=88.6535
	step [135/148], loss=86.0596
	step [136/148], loss=95.7313
	step [137/148], loss=82.3858
	step [138/148], loss=91.7298
	step [139/148], loss=90.3532
	step [140/148], loss=91.7346
	step [141/148], loss=98.1303
	step [142/148], loss=98.7473
	step [143/148], loss=86.7602
	step [144/148], loss=107.4724
	step [145/148], loss=85.5853
	step [146/148], loss=92.4940
	step [147/148], loss=95.7548
	step [148/148], loss=5.3456
	Evaluating
	loss=0.0151, precision=0.2636, recall=0.8679, f1=0.4044
saving model as: 0_saved_model.pth
Training epoch 62
	step [1/148], loss=89.8600
	step [2/148], loss=79.0136
	step [3/148], loss=100.0112
	step [4/148], loss=87.5297
	step [5/148], loss=93.6118
	step [6/148], loss=94.3181
	step [7/148], loss=77.7461
	step [8/148], loss=101.4322
	step [9/148], loss=84.6817
	step [10/148], loss=110.7291
	step [11/148], loss=87.0529
	step [12/148], loss=96.7331
	step [13/148], loss=81.0876
	step [14/148], loss=95.5676
	step [15/148], loss=87.3919
	step [16/148], loss=111.5089
	step [17/148], loss=96.5960
	step [18/148], loss=93.7787
	step [19/148], loss=85.4785
	step [20/148], loss=96.4991
	step [21/148], loss=98.1214
	step [22/148], loss=79.8115
	step [23/148], loss=95.7159
	step [24/148], loss=78.7167
	step [25/148], loss=104.3345
	step [26/148], loss=94.1212
	step [27/148], loss=96.8637
	step [28/148], loss=98.1681
	step [29/148], loss=93.3313
	step [30/148], loss=78.1442
	step [31/148], loss=75.3827
	step [32/148], loss=100.7868
	step [33/148], loss=84.2832
	step [34/148], loss=94.6870
	step [35/148], loss=97.4002
	step [36/148], loss=103.1530
	step [37/148], loss=84.1052
	step [38/148], loss=88.1845
	step [39/148], loss=94.9548
	step [40/148], loss=94.7965
	step [41/148], loss=88.5973
	step [42/148], loss=95.1463
	step [43/148], loss=88.6738
	step [44/148], loss=97.7385
	step [45/148], loss=95.7882
	step [46/148], loss=97.8013
	step [47/148], loss=87.3669
	step [48/148], loss=104.6803
	step [49/148], loss=102.4940
	step [50/148], loss=80.5831
	step [51/148], loss=101.9690
	step [52/148], loss=103.8237
	step [53/148], loss=69.0371
	step [54/148], loss=95.9249
	step [55/148], loss=89.0117
	step [56/148], loss=90.6885
	step [57/148], loss=82.2957
	step [58/148], loss=86.7932
	step [59/148], loss=78.5651
	step [60/148], loss=103.8803
	step [61/148], loss=97.0344
	step [62/148], loss=90.7466
	step [63/148], loss=86.9840
	step [64/148], loss=91.8335
	step [65/148], loss=95.1431
	step [66/148], loss=87.0712
	step [67/148], loss=89.9518
	step [68/148], loss=88.8876
	step [69/148], loss=82.9500
	step [70/148], loss=92.9686
	step [71/148], loss=111.4781
	step [72/148], loss=94.8136
	step [73/148], loss=95.4795
	step [74/148], loss=77.6210
	step [75/148], loss=91.2995
	step [76/148], loss=85.8725
	step [77/148], loss=72.2081
	step [78/148], loss=90.5653
	step [79/148], loss=109.4873
	step [80/148], loss=96.0793
	step [81/148], loss=85.7074
	step [82/148], loss=82.2010
	step [83/148], loss=91.0873
	step [84/148], loss=95.6747
	step [85/148], loss=84.3795
	step [86/148], loss=95.1414
	step [87/148], loss=95.7327
	step [88/148], loss=93.0496
	step [89/148], loss=84.4652
	step [90/148], loss=102.0554
	step [91/148], loss=83.2287
	step [92/148], loss=94.6996
	step [93/148], loss=89.0135
	step [94/148], loss=91.0414
	step [95/148], loss=93.8430
	step [96/148], loss=95.5317
	step [97/148], loss=85.3405
	step [98/148], loss=94.6850
	step [99/148], loss=95.9568
	step [100/148], loss=89.5232
	step [101/148], loss=106.0584
	step [102/148], loss=90.8254
	step [103/148], loss=97.8197
	step [104/148], loss=105.3311
	step [105/148], loss=78.8236
	step [106/148], loss=101.8096
	step [107/148], loss=106.8119
	step [108/148], loss=95.9990
	step [109/148], loss=95.2847
	step [110/148], loss=90.5953
	step [111/148], loss=98.2157
	step [112/148], loss=90.4397
	step [113/148], loss=93.6971
	step [114/148], loss=88.5934
	step [115/148], loss=76.6058
	step [116/148], loss=104.2323
	step [117/148], loss=83.8812
	step [118/148], loss=101.4159
	step [119/148], loss=95.8477
	step [120/148], loss=101.7625
	step [121/148], loss=98.7805
	step [122/148], loss=84.5273
	step [123/148], loss=89.6827
	step [124/148], loss=82.6705
	step [125/148], loss=92.8196
	step [126/148], loss=78.4722
	step [127/148], loss=80.3479
	step [128/148], loss=94.2201
	step [129/148], loss=95.9466
	step [130/148], loss=94.3633
	step [131/148], loss=98.6733
	step [132/148], loss=93.9684
	step [133/148], loss=96.8505
	step [134/148], loss=81.6803
	step [135/148], loss=96.2662
	step [136/148], loss=78.4966
	step [137/148], loss=93.4154
	step [138/148], loss=74.4541
	step [139/148], loss=88.6912
	step [140/148], loss=85.4877
	step [141/148], loss=101.3080
	step [142/148], loss=96.1863
	step [143/148], loss=81.6362
	step [144/148], loss=100.9807
	step [145/148], loss=82.6418
	step [146/148], loss=78.9199
	step [147/148], loss=95.7139
	step [148/148], loss=9.8912
	Evaluating
	loss=0.0151, precision=0.2619, recall=0.8610, f1=0.4016
Training epoch 63
	step [1/148], loss=96.0391
	step [2/148], loss=95.4027
	step [3/148], loss=78.4956
	step [4/148], loss=79.2206
	step [5/148], loss=98.9035
	step [6/148], loss=89.5108
	step [7/148], loss=119.3796
	step [8/148], loss=95.6101
	step [9/148], loss=101.1345
	step [10/148], loss=93.2616
	step [11/148], loss=92.9261
	step [12/148], loss=87.8488
	step [13/148], loss=87.6048
	step [14/148], loss=90.0087
	step [15/148], loss=78.9456
	step [16/148], loss=75.8156
	step [17/148], loss=105.7094
	step [18/148], loss=92.2100
	step [19/148], loss=92.2391
	step [20/148], loss=96.1509
	step [21/148], loss=92.0846
	step [22/148], loss=82.4483
	step [23/148], loss=87.8418
	step [24/148], loss=112.1215
	step [25/148], loss=88.1060
	step [26/148], loss=103.5580
	step [27/148], loss=72.2028
	step [28/148], loss=96.0881
	step [29/148], loss=99.8786
	step [30/148], loss=76.0941
	step [31/148], loss=94.9872
	step [32/148], loss=93.9725
	step [33/148], loss=74.6328
	step [34/148], loss=91.4026
	step [35/148], loss=88.6846
	step [36/148], loss=96.4537
	step [37/148], loss=98.0690
	step [38/148], loss=92.9994
	step [39/148], loss=85.3126
	step [40/148], loss=87.7935
	step [41/148], loss=92.8285
	step [42/148], loss=87.4924
	step [43/148], loss=83.8080
	step [44/148], loss=92.2102
	step [45/148], loss=94.8212
	step [46/148], loss=101.8269
	step [47/148], loss=97.1759
	step [48/148], loss=102.9218
	step [49/148], loss=107.3566
	step [50/148], loss=77.8413
	step [51/148], loss=95.2382
	step [52/148], loss=110.2803
	step [53/148], loss=86.8050
	step [54/148], loss=104.4772
	step [55/148], loss=92.5083
	step [56/148], loss=109.2542
	step [57/148], loss=75.6912
	step [58/148], loss=91.4174
	step [59/148], loss=102.8335
	step [60/148], loss=97.7402
	step [61/148], loss=103.4453
	step [62/148], loss=81.1289
	step [63/148], loss=97.1750
	step [64/148], loss=75.2477
	step [65/148], loss=82.8826
	step [66/148], loss=83.7057
	step [67/148], loss=95.1268
	step [68/148], loss=86.4835
	step [69/148], loss=91.6625
	step [70/148], loss=90.6842
	step [71/148], loss=89.0614
	step [72/148], loss=102.5118
	step [73/148], loss=108.3251
	step [74/148], loss=91.4137
	step [75/148], loss=91.4928
	step [76/148], loss=69.0311
	step [77/148], loss=106.8417
	step [78/148], loss=109.3334
	step [79/148], loss=97.5798
	step [80/148], loss=84.3699
	step [81/148], loss=89.8614
	step [82/148], loss=94.8932
	step [83/148], loss=90.9354
	step [84/148], loss=96.6985
	step [85/148], loss=81.4175
	step [86/148], loss=89.8995
	step [87/148], loss=83.3578
	step [88/148], loss=103.6248
	step [89/148], loss=90.1231
	step [90/148], loss=106.9593
	step [91/148], loss=73.9951
	step [92/148], loss=100.8250
	step [93/148], loss=80.3135
	step [94/148], loss=74.1996
	step [95/148], loss=100.9525
	step [96/148], loss=79.3770
	step [97/148], loss=96.4784
	step [98/148], loss=89.0505
	step [99/148], loss=80.1595
	step [100/148], loss=97.8918
	step [101/148], loss=96.4999
	step [102/148], loss=90.5476
	step [103/148], loss=97.8700
	step [104/148], loss=82.6091
	step [105/148], loss=99.0773
	step [106/148], loss=91.2425
	step [107/148], loss=98.1461
	step [108/148], loss=83.9996
	step [109/148], loss=82.0657
	step [110/148], loss=82.3602
	step [111/148], loss=86.9777
	step [112/148], loss=77.1881
	step [113/148], loss=89.4620
	step [114/148], loss=80.9162
	step [115/148], loss=94.0766
	step [116/148], loss=93.2113
	step [117/148], loss=97.0957
	step [118/148], loss=78.5598
	step [119/148], loss=93.0013
	step [120/148], loss=77.7543
	step [121/148], loss=112.8216
	step [122/148], loss=94.4102
	step [123/148], loss=108.1578
	step [124/148], loss=88.6436
	step [125/148], loss=79.1435
	step [126/148], loss=91.7493
	step [127/148], loss=67.4615
	step [128/148], loss=91.0048
	step [129/148], loss=86.8204
	step [130/148], loss=106.3188
	step [131/148], loss=90.3278
	step [132/148], loss=97.0162
	step [133/148], loss=92.4039
	step [134/148], loss=95.3031
	step [135/148], loss=93.9196
	step [136/148], loss=105.0791
	step [137/148], loss=104.0907
	step [138/148], loss=87.4566
	step [139/148], loss=95.4318
	step [140/148], loss=80.2154
	step [141/148], loss=93.0037
	step [142/148], loss=77.5182
	step [143/148], loss=95.6918
	step [144/148], loss=99.1617
	step [145/148], loss=98.6895
	step [146/148], loss=76.6689
	step [147/148], loss=101.5358
	step [148/148], loss=4.0725
	Evaluating
	loss=0.0153, precision=0.2702, recall=0.8520, f1=0.4103
saving model as: 0_saved_model.pth
Training epoch 64
	step [1/148], loss=85.2943
	step [2/148], loss=97.2960
	step [3/148], loss=85.2046
	step [4/148], loss=104.2927
	step [5/148], loss=89.8292
	step [6/148], loss=98.0315
	step [7/148], loss=79.3197
	step [8/148], loss=88.6169
	step [9/148], loss=98.5353
	step [10/148], loss=90.6693
	step [11/148], loss=80.5646
	step [12/148], loss=85.2002
	step [13/148], loss=70.7589
	step [14/148], loss=119.8876
	step [15/148], loss=103.0912
	step [16/148], loss=110.4717
	step [17/148], loss=86.4235
	step [18/148], loss=100.3597
	step [19/148], loss=72.2138
	step [20/148], loss=95.2670
	step [21/148], loss=79.0084
	step [22/148], loss=85.3350
	step [23/148], loss=88.7492
	step [24/148], loss=94.1019
	step [25/148], loss=78.5819
	step [26/148], loss=88.0732
	step [27/148], loss=90.7004
	step [28/148], loss=84.7176
	step [29/148], loss=83.1603
	step [30/148], loss=101.1652
	step [31/148], loss=115.2734
	step [32/148], loss=83.3945
	step [33/148], loss=83.0719
	step [34/148], loss=82.3268
	step [35/148], loss=101.9072
	step [36/148], loss=100.7372
	step [37/148], loss=74.2687
	step [38/148], loss=87.9635
	step [39/148], loss=96.6948
	step [40/148], loss=90.6648
	step [41/148], loss=78.2732
	step [42/148], loss=85.6788
	step [43/148], loss=102.6625
	step [44/148], loss=109.4951
	step [45/148], loss=89.6703
	step [46/148], loss=94.1057
	step [47/148], loss=88.9334
	step [48/148], loss=94.0634
	step [49/148], loss=83.0686
	step [50/148], loss=107.2793
	step [51/148], loss=82.7245
	step [52/148], loss=94.9910
	step [53/148], loss=97.5414
	step [54/148], loss=84.9360
	step [55/148], loss=99.2479
	step [56/148], loss=84.4406
	step [57/148], loss=90.5103
	step [58/148], loss=96.1034
	step [59/148], loss=78.5218
	step [60/148], loss=80.7933
	step [61/148], loss=105.0181
	step [62/148], loss=83.8020
	step [63/148], loss=88.5892
	step [64/148], loss=93.7650
	step [65/148], loss=90.6679
	step [66/148], loss=76.3037
	step [67/148], loss=79.3949
	step [68/148], loss=83.8917
	step [69/148], loss=84.7055
	step [70/148], loss=90.3714
	step [71/148], loss=98.4539
	step [72/148], loss=105.3622
	step [73/148], loss=84.5146
	step [74/148], loss=108.8132
	step [75/148], loss=81.9902
	step [76/148], loss=95.8139
	step [77/148], loss=92.3891
	step [78/148], loss=82.8011
	step [79/148], loss=89.3761
	step [80/148], loss=79.3419
	step [81/148], loss=98.2433
	step [82/148], loss=100.5871
	step [83/148], loss=98.6764
	step [84/148], loss=97.9258
	step [85/148], loss=75.4060
	step [86/148], loss=73.5512
	step [87/148], loss=99.1330
	step [88/148], loss=93.0267
	step [89/148], loss=82.2646
	step [90/148], loss=91.1733
	step [91/148], loss=88.0530
	step [92/148], loss=103.6210
	step [93/148], loss=96.0283
	step [94/148], loss=101.0512
	step [95/148], loss=90.9898
	step [96/148], loss=95.2660
	step [97/148], loss=90.3504
	step [98/148], loss=94.8360
	step [99/148], loss=79.4652
	step [100/148], loss=93.2415
	step [101/148], loss=93.7865
	step [102/148], loss=80.4680
	step [103/148], loss=118.2718
	step [104/148], loss=100.2153
	step [105/148], loss=100.2789
	step [106/148], loss=92.8299
	step [107/148], loss=72.2895
	step [108/148], loss=72.3263
	step [109/148], loss=72.4552
	step [110/148], loss=83.1831
	step [111/148], loss=87.3336
	step [112/148], loss=101.0235
	step [113/148], loss=91.6373
	step [114/148], loss=92.1305
	step [115/148], loss=84.7449
	step [116/148], loss=89.3433
	step [117/148], loss=89.2028
	step [118/148], loss=105.4180
	step [119/148], loss=103.1928
	step [120/148], loss=99.0663
	step [121/148], loss=93.8577
	step [122/148], loss=93.3926
	step [123/148], loss=108.9598
	step [124/148], loss=91.0276
	step [125/148], loss=81.9618
	step [126/148], loss=76.5816
	step [127/148], loss=93.8643
	step [128/148], loss=83.5220
	step [129/148], loss=74.9225
	step [130/148], loss=98.6659
	step [131/148], loss=84.8814
	step [132/148], loss=77.7336
	step [133/148], loss=103.6952
	step [134/148], loss=96.7498
	step [135/148], loss=101.3889
	step [136/148], loss=121.0618
	step [137/148], loss=95.1172
	step [138/148], loss=79.9732
	step [139/148], loss=88.8916
	step [140/148], loss=91.3664
	step [141/148], loss=93.8390
	step [142/148], loss=78.0509
	step [143/148], loss=94.5019
	step [144/148], loss=93.7584
	step [145/148], loss=84.9771
	step [146/148], loss=86.9302
	step [147/148], loss=86.7449
	step [148/148], loss=5.0639
	Evaluating
	loss=0.0150, precision=0.2723, recall=0.8662, f1=0.4144
saving model as: 0_saved_model.pth
Training epoch 65
	step [1/148], loss=87.2054
	step [2/148], loss=101.5434
	step [3/148], loss=91.1341
	step [4/148], loss=89.1620
	step [5/148], loss=83.9626
	step [6/148], loss=93.7818
	step [7/148], loss=95.0251
	step [8/148], loss=99.6664
	step [9/148], loss=93.0349
	step [10/148], loss=85.3647
	step [11/148], loss=104.0401
	step [12/148], loss=103.6612
	step [13/148], loss=79.7471
	step [14/148], loss=95.0389
	step [15/148], loss=93.8752
	step [16/148], loss=95.7099
	step [17/148], loss=91.9531
	step [18/148], loss=90.5671
	step [19/148], loss=80.2552
	step [20/148], loss=74.4269
	step [21/148], loss=91.3264
	step [22/148], loss=107.0211
	step [23/148], loss=90.7773
	step [24/148], loss=100.8941
	step [25/148], loss=78.2970
	step [26/148], loss=95.6652
	step [27/148], loss=90.3927
	step [28/148], loss=89.9978
	step [29/148], loss=94.9741
	step [30/148], loss=98.1455
	step [31/148], loss=85.9949
	step [32/148], loss=78.2441
	step [33/148], loss=81.9043
	step [34/148], loss=82.7210
	step [35/148], loss=86.7726
	step [36/148], loss=91.4515
	step [37/148], loss=95.3196
	step [38/148], loss=80.4625
	step [39/148], loss=88.9644
	step [40/148], loss=94.4137
	step [41/148], loss=89.5156
	step [42/148], loss=86.6342
	step [43/148], loss=90.2430
	step [44/148], loss=85.0814
	step [45/148], loss=92.2835
	step [46/148], loss=71.5212
	step [47/148], loss=80.6386
	step [48/148], loss=96.9776
	step [49/148], loss=105.5190
	step [50/148], loss=93.2778
	step [51/148], loss=80.9392
	step [52/148], loss=81.4204
	step [53/148], loss=106.0587
	step [54/148], loss=103.7874
	step [55/148], loss=69.4056
	step [56/148], loss=102.3221
	step [57/148], loss=76.4409
	step [58/148], loss=99.2781
	step [59/148], loss=97.9329
	step [60/148], loss=76.4045
	step [61/148], loss=106.9605
	step [62/148], loss=93.3453
	step [63/148], loss=94.4672
	step [64/148], loss=81.9532
	step [65/148], loss=90.2975
	step [66/148], loss=81.3570
	step [67/148], loss=111.0609
	step [68/148], loss=92.7016
	step [69/148], loss=76.5856
	step [70/148], loss=111.2272
	step [71/148], loss=84.7402
	step [72/148], loss=87.9414
	step [73/148], loss=73.5637
	step [74/148], loss=104.8130
	step [75/148], loss=92.6437
	step [76/148], loss=111.4549
	step [77/148], loss=84.4830
	step [78/148], loss=93.6169
	step [79/148], loss=90.9378
	step [80/148], loss=90.9171
	step [81/148], loss=89.7883
	step [82/148], loss=80.6228
	step [83/148], loss=98.8225
	step [84/148], loss=106.9127
	step [85/148], loss=85.2626
	step [86/148], loss=93.9973
	step [87/148], loss=104.6451
	step [88/148], loss=89.1118
	step [89/148], loss=78.8929
	step [90/148], loss=99.7088
	step [91/148], loss=102.8351
	step [92/148], loss=84.5001
	step [93/148], loss=101.8230
	step [94/148], loss=97.1624
	step [95/148], loss=94.3427
	step [96/148], loss=85.2953
	step [97/148], loss=88.1606
	step [98/148], loss=78.0735
	step [99/148], loss=89.1661
	step [100/148], loss=92.9453
	step [101/148], loss=90.5210
	step [102/148], loss=96.4215
	step [103/148], loss=78.8310
	step [104/148], loss=94.8903
	step [105/148], loss=86.3310
	step [106/148], loss=116.0838
	step [107/148], loss=94.6846
	step [108/148], loss=97.6873
	step [109/148], loss=90.9984
	step [110/148], loss=87.2287
	step [111/148], loss=84.9321
	step [112/148], loss=98.2553
	step [113/148], loss=93.0016
	step [114/148], loss=84.3072
	step [115/148], loss=90.0274
	step [116/148], loss=87.6962
	step [117/148], loss=87.2027
	step [118/148], loss=85.1216
	step [119/148], loss=82.4849
	step [120/148], loss=78.8875
	step [121/148], loss=81.1300
	step [122/148], loss=88.1263
	step [123/148], loss=90.9666
	step [124/148], loss=81.8549
	step [125/148], loss=77.0132
	step [126/148], loss=107.9676
	step [127/148], loss=100.5029
	step [128/148], loss=98.1549
	step [129/148], loss=71.5239
	step [130/148], loss=100.8700
	step [131/148], loss=91.8237
	step [132/148], loss=94.9099
	step [133/148], loss=85.6850
	step [134/148], loss=87.1959
	step [135/148], loss=85.7411
	step [136/148], loss=96.6135
	step [137/148], loss=88.4631
	step [138/148], loss=96.2828
	step [139/148], loss=80.6539
	step [140/148], loss=69.9928
	step [141/148], loss=93.3741
	step [142/148], loss=84.1429
	step [143/148], loss=102.5774
	step [144/148], loss=83.0594
	step [145/148], loss=77.2918
	step [146/148], loss=111.2473
	step [147/148], loss=84.1432
	step [148/148], loss=7.1414
	Evaluating
	loss=0.0155, precision=0.2549, recall=0.8699, f1=0.3942
Training epoch 66
	step [1/148], loss=94.6719
	step [2/148], loss=102.8855
	step [3/148], loss=80.2610
	step [4/148], loss=112.6357
	step [5/148], loss=104.4529
	step [6/148], loss=79.9210
	step [7/148], loss=103.4624
	step [8/148], loss=88.4917
	step [9/148], loss=86.3598
	step [10/148], loss=103.7785
	step [11/148], loss=100.1439
	step [12/148], loss=95.0907
	step [13/148], loss=90.8440
	step [14/148], loss=78.4425
	step [15/148], loss=78.0350
	step [16/148], loss=88.7819
	step [17/148], loss=90.4409
	step [18/148], loss=80.8084
	step [19/148], loss=92.7705
	step [20/148], loss=71.3201
	step [21/148], loss=91.5313
	step [22/148], loss=118.5056
	step [23/148], loss=95.0979
	step [24/148], loss=85.0634
	step [25/148], loss=100.6487
	step [26/148], loss=100.5032
	step [27/148], loss=90.8701
	step [28/148], loss=88.8590
	step [29/148], loss=76.9787
	step [30/148], loss=102.1962
	step [31/148], loss=74.1720
	step [32/148], loss=104.1945
	step [33/148], loss=83.0485
	step [34/148], loss=85.9985
	step [35/148], loss=99.3682
	step [36/148], loss=93.0479
	step [37/148], loss=78.9494
	step [38/148], loss=70.8410
	step [39/148], loss=90.4876
	step [40/148], loss=97.6360
	step [41/148], loss=76.9853
	step [42/148], loss=78.0849
	step [43/148], loss=88.9190
	step [44/148], loss=87.5627
	step [45/148], loss=87.0245
	step [46/148], loss=89.0157
	step [47/148], loss=97.0407
	step [48/148], loss=81.5870
	step [49/148], loss=115.5504
	step [50/148], loss=83.5118
	step [51/148], loss=83.8022
	step [52/148], loss=93.6843
	step [53/148], loss=89.5127
	step [54/148], loss=88.0360
	step [55/148], loss=79.3107
	step [56/148], loss=91.3174
	step [57/148], loss=90.1210
	step [58/148], loss=90.0794
	step [59/148], loss=102.0814
	step [60/148], loss=85.7397
	step [61/148], loss=107.9138
	step [62/148], loss=103.2619
	step [63/148], loss=100.4993
	step [64/148], loss=100.3644
	step [65/148], loss=94.1372
	step [66/148], loss=89.9755
	step [67/148], loss=86.7946
	step [68/148], loss=83.7202
	step [69/148], loss=95.4365
	step [70/148], loss=87.8563
	step [71/148], loss=77.7926
	step [72/148], loss=76.6597
	step [73/148], loss=90.9604
	step [74/148], loss=78.5510
	step [75/148], loss=101.3253
	step [76/148], loss=87.0887
	step [77/148], loss=76.1101
	step [78/148], loss=82.1566
	step [79/148], loss=84.2430
	step [80/148], loss=102.4277
	step [81/148], loss=104.1006
	step [82/148], loss=105.7685
	step [83/148], loss=93.9497
	step [84/148], loss=92.5409
	step [85/148], loss=81.5691
	step [86/148], loss=85.1598
	step [87/148], loss=92.3562
	step [88/148], loss=87.0047
	step [89/148], loss=95.1615
	step [90/148], loss=82.2686
	step [91/148], loss=114.2832
	step [92/148], loss=95.7403
	step [93/148], loss=83.1386
	step [94/148], loss=100.2585
	step [95/148], loss=95.0847
	step [96/148], loss=97.5025
	step [97/148], loss=96.2996
	step [98/148], loss=92.7495
	step [99/148], loss=81.4491
	step [100/148], loss=87.8544
	step [101/148], loss=94.7672
	step [102/148], loss=78.7070
	step [103/148], loss=81.3858
	step [104/148], loss=90.9638
	step [105/148], loss=87.2155
	step [106/148], loss=96.5297
	step [107/148], loss=95.3124
	step [108/148], loss=86.2797
	step [109/148], loss=70.8875
	step [110/148], loss=92.9118
	step [111/148], loss=84.0201
	step [112/148], loss=77.3501
	step [113/148], loss=92.6206
	step [114/148], loss=75.8465
	step [115/148], loss=88.4408
	step [116/148], loss=83.3497
	step [117/148], loss=88.8489
	step [118/148], loss=90.2721
	step [119/148], loss=61.6464
	step [120/148], loss=85.9199
	step [121/148], loss=102.8119
	step [122/148], loss=97.9709
	step [123/148], loss=83.8931
	step [124/148], loss=101.2891
	step [125/148], loss=88.3163
	step [126/148], loss=88.2904
	step [127/148], loss=92.5139
	step [128/148], loss=78.1801
	step [129/148], loss=98.0403
	step [130/148], loss=95.7138
	step [131/148], loss=91.6258
	step [132/148], loss=102.8872
	step [133/148], loss=106.4414
	step [134/148], loss=83.9953
	step [135/148], loss=85.2843
	step [136/148], loss=94.9407
	step [137/148], loss=90.3413
	step [138/148], loss=85.7383
	step [139/148], loss=77.8203
	step [140/148], loss=94.3556
	step [141/148], loss=106.8558
	step [142/148], loss=103.3067
	step [143/148], loss=93.9168
	step [144/148], loss=87.6549
	step [145/148], loss=85.5533
	step [146/148], loss=97.1956
	step [147/148], loss=70.1952
	step [148/148], loss=12.0636
	Evaluating
	loss=0.0141, precision=0.2889, recall=0.8562, f1=0.4320
saving model as: 0_saved_model.pth
Training epoch 67
	step [1/148], loss=101.0793
	step [2/148], loss=75.1414
	step [3/148], loss=77.4111
	step [4/148], loss=77.4045
	step [5/148], loss=99.1000
	step [6/148], loss=91.2538
	step [7/148], loss=84.4982
	step [8/148], loss=98.6283
	step [9/148], loss=93.3269
	step [10/148], loss=90.9094
	step [11/148], loss=77.4723
	step [12/148], loss=91.2765
	step [13/148], loss=103.5602
	step [14/148], loss=87.2208
	step [15/148], loss=78.2908
	step [16/148], loss=81.4298
	step [17/148], loss=88.1373
	step [18/148], loss=82.6510
	step [19/148], loss=92.5238
	step [20/148], loss=97.7756
	step [21/148], loss=95.3702
	step [22/148], loss=95.7411
	step [23/148], loss=86.5074
	step [24/148], loss=91.1451
	step [25/148], loss=91.8325
	step [26/148], loss=103.9747
	step [27/148], loss=91.9492
	step [28/148], loss=83.2666
	step [29/148], loss=80.5010
	step [30/148], loss=94.3114
	step [31/148], loss=95.0074
	step [32/148], loss=98.1960
	step [33/148], loss=92.1568
	step [34/148], loss=86.7507
	step [35/148], loss=74.3066
	step [36/148], loss=87.9685
	step [37/148], loss=90.2245
	step [38/148], loss=81.8015
	step [39/148], loss=93.6420
	step [40/148], loss=95.6920
	step [41/148], loss=80.1560
	step [42/148], loss=104.1239
	step [43/148], loss=90.9369
	step [44/148], loss=87.4812
	step [45/148], loss=82.0319
	step [46/148], loss=102.4235
	step [47/148], loss=86.8429
	step [48/148], loss=89.7042
	step [49/148], loss=75.6374
	step [50/148], loss=85.7704
	step [51/148], loss=96.8309
	step [52/148], loss=87.4176
	step [53/148], loss=74.9486
	step [54/148], loss=84.9005
	step [55/148], loss=92.0429
	step [56/148], loss=80.7846
	step [57/148], loss=106.3006
	step [58/148], loss=81.6478
	step [59/148], loss=97.2652
	step [60/148], loss=90.6829
	step [61/148], loss=98.4418
	step [62/148], loss=74.3446
	step [63/148], loss=97.3375
	step [64/148], loss=89.7351
	step [65/148], loss=100.4480
	step [66/148], loss=87.9488
	step [67/148], loss=98.8804
	step [68/148], loss=99.6210
	step [69/148], loss=69.5293
	step [70/148], loss=80.1930
	step [71/148], loss=94.2807
	step [72/148], loss=91.4123
	step [73/148], loss=80.7048
	step [74/148], loss=98.9490
	step [75/148], loss=85.5954
	step [76/148], loss=96.7766
	step [77/148], loss=94.1198
	step [78/148], loss=77.7389
	step [79/148], loss=86.7598
	step [80/148], loss=105.8410
	step [81/148], loss=93.0710
	step [82/148], loss=83.1715
	step [83/148], loss=88.9447
	step [84/148], loss=93.3297
	step [85/148], loss=89.6540
	step [86/148], loss=90.9710
	step [87/148], loss=87.9815
	step [88/148], loss=94.1481
	step [89/148], loss=74.7923
	step [90/148], loss=92.9862
	step [91/148], loss=92.6564
	step [92/148], loss=104.8315
	step [93/148], loss=92.0721
	step [94/148], loss=99.7058
	step [95/148], loss=92.9492
	step [96/148], loss=87.1993
	step [97/148], loss=86.7032
	step [98/148], loss=94.4387
	step [99/148], loss=96.5190
	step [100/148], loss=81.4056
	step [101/148], loss=91.7029
	step [102/148], loss=86.6965
	step [103/148], loss=108.0546
	step [104/148], loss=88.1280
	step [105/148], loss=99.1229
	step [106/148], loss=97.1429
	step [107/148], loss=79.8867
	step [108/148], loss=73.2984
	step [109/148], loss=99.9296
	step [110/148], loss=88.8958
	step [111/148], loss=88.6764
	step [112/148], loss=100.2323
	step [113/148], loss=106.5401
	step [114/148], loss=98.7491
	step [115/148], loss=82.5381
	step [116/148], loss=80.3326
	step [117/148], loss=87.5860
	step [118/148], loss=97.4863
	step [119/148], loss=102.7787
	step [120/148], loss=79.3683
	step [121/148], loss=109.5490
	step [122/148], loss=100.1790
	step [123/148], loss=97.0238
	step [124/148], loss=86.4674
	step [125/148], loss=97.8384
	step [126/148], loss=94.7806
	step [127/148], loss=96.5299
	step [128/148], loss=92.8004
	step [129/148], loss=101.2573
	step [130/148], loss=88.3487
	step [131/148], loss=93.6970
	step [132/148], loss=102.9509
	step [133/148], loss=89.4365
	step [134/148], loss=98.4986
	step [135/148], loss=102.6832
	step [136/148], loss=82.5476
	step [137/148], loss=82.9586
	step [138/148], loss=86.6635
	step [139/148], loss=82.9226
	step [140/148], loss=89.0682
	step [141/148], loss=84.8576
	step [142/148], loss=88.1053
	step [143/148], loss=83.4167
	step [144/148], loss=90.1946
	step [145/148], loss=77.4600
	step [146/148], loss=73.5649
	step [147/148], loss=97.8219
	step [148/148], loss=5.3646
	Evaluating
	loss=0.0156, precision=0.2517, recall=0.8463, f1=0.3880
Training epoch 68
	step [1/148], loss=89.6740
	step [2/148], loss=82.3448
	step [3/148], loss=71.9699
	step [4/148], loss=96.8686
	step [5/148], loss=99.0941
	step [6/148], loss=97.6708
	step [7/148], loss=73.7384
	step [8/148], loss=97.3304
	step [9/148], loss=95.7087
	step [10/148], loss=91.1080
	step [11/148], loss=75.3011
	step [12/148], loss=86.2756
	step [13/148], loss=82.6184
	step [14/148], loss=88.5384
	step [15/148], loss=65.5883
	step [16/148], loss=97.1343
	step [17/148], loss=86.9130
	step [18/148], loss=80.5974
	step [19/148], loss=97.5964
	step [20/148], loss=100.0184
	step [21/148], loss=104.5303
	step [22/148], loss=92.4636
	step [23/148], loss=91.1264
	step [24/148], loss=85.3864
	step [25/148], loss=90.7468
	step [26/148], loss=88.9352
	step [27/148], loss=84.6783
	step [28/148], loss=89.1345
	step [29/148], loss=88.0347
	step [30/148], loss=102.9585
	step [31/148], loss=77.6011
	step [32/148], loss=87.3444
	step [33/148], loss=82.8038
	step [34/148], loss=81.2918
	step [35/148], loss=80.0013
	step [36/148], loss=89.8715
	step [37/148], loss=92.2063
	step [38/148], loss=88.5937
	step [39/148], loss=116.7255
	step [40/148], loss=91.9632
	step [41/148], loss=88.1371
	step [42/148], loss=74.4013
	step [43/148], loss=85.9177
	step [44/148], loss=76.5067
	step [45/148], loss=106.7078
	step [46/148], loss=85.0333
	step [47/148], loss=98.5945
	step [48/148], loss=88.3940
	step [49/148], loss=93.5607
	step [50/148], loss=87.4961
	step [51/148], loss=76.7582
	step [52/148], loss=78.7794
	step [53/148], loss=98.7061
	step [54/148], loss=90.3749
	step [55/148], loss=100.0959
	step [56/148], loss=88.8949
	step [57/148], loss=90.8970
	step [58/148], loss=101.5917
	step [59/148], loss=81.9405
	step [60/148], loss=86.1968
	step [61/148], loss=86.3765
	step [62/148], loss=76.3723
	step [63/148], loss=92.3780
	step [64/148], loss=103.9758
	step [65/148], loss=73.3139
	step [66/148], loss=76.5417
	step [67/148], loss=80.0518
	step [68/148], loss=89.2334
	step [69/148], loss=92.8299
	step [70/148], loss=91.9832
	step [71/148], loss=94.7835
	step [72/148], loss=85.1501
	step [73/148], loss=93.6630
	step [74/148], loss=98.7114
	step [75/148], loss=84.6731
	step [76/148], loss=87.6867
	step [77/148], loss=112.2436
	step [78/148], loss=83.0768
	step [79/148], loss=97.0434
	step [80/148], loss=71.6368
	step [81/148], loss=89.6976
	step [82/148], loss=87.2648
	step [83/148], loss=86.6624
	step [84/148], loss=88.8102
	step [85/148], loss=103.2199
	step [86/148], loss=96.5935
	step [87/148], loss=91.6463
	step [88/148], loss=87.5353
	step [89/148], loss=84.9450
	step [90/148], loss=89.5552
	step [91/148], loss=79.7819
	step [92/148], loss=80.1249
	step [93/148], loss=92.5985
	step [94/148], loss=93.9435
	step [95/148], loss=75.8390
	step [96/148], loss=90.6747
	step [97/148], loss=93.4210
	step [98/148], loss=96.5069
	step [99/148], loss=79.9958
	step [100/148], loss=102.5147
	step [101/148], loss=96.6782
	step [102/148], loss=87.9023
	step [103/148], loss=82.4066
	step [104/148], loss=94.8155
	step [105/148], loss=95.8318
	step [106/148], loss=83.3497
	step [107/148], loss=106.7186
	step [108/148], loss=103.5312
	step [109/148], loss=85.6783
	step [110/148], loss=87.4092
	step [111/148], loss=80.4666
	step [112/148], loss=71.3729
	step [113/148], loss=84.5333
	step [114/148], loss=94.1019
	step [115/148], loss=108.3917
	step [116/148], loss=102.2977
	step [117/148], loss=82.4770
	step [118/148], loss=103.1336
	step [119/148], loss=95.8413
	step [120/148], loss=75.1060
	step [121/148], loss=79.1373
	step [122/148], loss=103.8980
	step [123/148], loss=91.3113
	step [124/148], loss=85.3375
	step [125/148], loss=101.6398
	step [126/148], loss=109.8882
	step [127/148], loss=90.5062
	step [128/148], loss=77.3146
	step [129/148], loss=87.3933
	step [130/148], loss=87.9376
	step [131/148], loss=90.4280
	step [132/148], loss=107.8168
	step [133/148], loss=83.4175
	step [134/148], loss=94.4004
	step [135/148], loss=90.1040
	step [136/148], loss=94.9636
	step [137/148], loss=97.3368
	step [138/148], loss=77.6446
	step [139/148], loss=73.0033
	step [140/148], loss=94.3437
	step [141/148], loss=92.4783
	step [142/148], loss=85.7704
	step [143/148], loss=71.6274
	step [144/148], loss=102.5063
	step [145/148], loss=74.8716
	step [146/148], loss=98.1959
	step [147/148], loss=82.7544
	step [148/148], loss=10.7392
	Evaluating
	loss=0.0136, precision=0.2929, recall=0.8595, f1=0.4369
saving model as: 0_saved_model.pth
Training epoch 69
	step [1/148], loss=94.9870
	step [2/148], loss=76.8783
	step [3/148], loss=81.7127
	step [4/148], loss=73.7674
	step [5/148], loss=87.6250
	step [6/148], loss=76.9252
	step [7/148], loss=94.2585
	step [8/148], loss=89.3039
	step [9/148], loss=93.2299
	step [10/148], loss=71.1732
	step [11/148], loss=104.6525
	step [12/148], loss=93.9485
	step [13/148], loss=88.3833
	step [14/148], loss=76.9561
	step [15/148], loss=82.1178
	step [16/148], loss=86.5251
	step [17/148], loss=81.8778
	step [18/148], loss=83.2065
	step [19/148], loss=83.8299
	step [20/148], loss=82.7199
	step [21/148], loss=90.4583
	step [22/148], loss=83.6106
	step [23/148], loss=96.2682
	step [24/148], loss=107.5669
	step [25/148], loss=97.7171
	step [26/148], loss=75.7257
	step [27/148], loss=90.3014
	step [28/148], loss=94.1113
	step [29/148], loss=89.3399
	step [30/148], loss=95.9995
	step [31/148], loss=86.2939
	step [32/148], loss=89.1450
	step [33/148], loss=84.8534
	step [34/148], loss=77.3825
	step [35/148], loss=94.9210
	step [36/148], loss=93.9721
	step [37/148], loss=83.2150
	step [38/148], loss=79.8391
	step [39/148], loss=84.2247
	step [40/148], loss=105.6810
	step [41/148], loss=84.3401
	step [42/148], loss=94.1473
	step [43/148], loss=84.7397
	step [44/148], loss=75.4804
	step [45/148], loss=104.0333
	step [46/148], loss=91.4071
	step [47/148], loss=70.9567
	step [48/148], loss=81.9563
	step [49/148], loss=113.2703
	step [50/148], loss=88.4848
	step [51/148], loss=92.9237
	step [52/148], loss=90.8327
	step [53/148], loss=89.5965
	step [54/148], loss=111.4652
	step [55/148], loss=89.5719
	step [56/148], loss=89.1624
	step [57/148], loss=92.4272
	step [58/148], loss=87.1146
	step [59/148], loss=79.7999
	step [60/148], loss=97.9159
	step [61/148], loss=92.3460
	step [62/148], loss=86.8803
	step [63/148], loss=90.5841
	step [64/148], loss=97.4553
	step [65/148], loss=88.4565
	step [66/148], loss=76.9523
	step [67/148], loss=85.4778
	step [68/148], loss=94.4915
	step [69/148], loss=85.3411
	step [70/148], loss=83.3102
	step [71/148], loss=91.3406
	step [72/148], loss=78.1379
	step [73/148], loss=90.2791
	step [74/148], loss=76.1397
	step [75/148], loss=95.6615
	step [76/148], loss=91.1572
	step [77/148], loss=67.4755
	step [78/148], loss=89.2417
	step [79/148], loss=84.7512
	step [80/148], loss=76.6265
	step [81/148], loss=94.7155
	step [82/148], loss=85.5723
	step [83/148], loss=92.5396
	step [84/148], loss=92.1211
	step [85/148], loss=97.6918
	step [86/148], loss=84.0138
	step [87/148], loss=88.4164
	step [88/148], loss=90.8385
	step [89/148], loss=95.6062
	step [90/148], loss=83.3483
	step [91/148], loss=96.6290
	step [92/148], loss=87.4167
	step [93/148], loss=98.8925
	step [94/148], loss=81.6908
	step [95/148], loss=87.2354
	step [96/148], loss=90.3248
	step [97/148], loss=85.8145
	step [98/148], loss=94.1059
	step [99/148], loss=81.4799
	step [100/148], loss=91.5724
	step [101/148], loss=92.4925
	step [102/148], loss=97.8415
	step [103/148], loss=85.8686
	step [104/148], loss=86.2791
	step [105/148], loss=86.0608
	step [106/148], loss=93.9031
	step [107/148], loss=91.3722
	step [108/148], loss=79.0305
	step [109/148], loss=89.0536
	step [110/148], loss=89.7534
	step [111/148], loss=86.2630
	step [112/148], loss=100.5641
	step [113/148], loss=85.3053
	step [114/148], loss=96.2758
	step [115/148], loss=106.2836
	step [116/148], loss=76.6111
	step [117/148], loss=96.9999
	step [118/148], loss=102.7359
	step [119/148], loss=82.9044
	step [120/148], loss=76.3756
	step [121/148], loss=86.1248
	step [122/148], loss=90.7503
	step [123/148], loss=93.6135
	step [124/148], loss=83.3021
	step [125/148], loss=95.6800
	step [126/148], loss=95.7367
	step [127/148], loss=101.1298
	step [128/148], loss=102.0392
	step [129/148], loss=82.8147
	step [130/148], loss=81.5536
	step [131/148], loss=83.7866
	step [132/148], loss=77.4405
	step [133/148], loss=83.7274
	step [134/148], loss=88.6042
	step [135/148], loss=94.7753
	step [136/148], loss=92.1710
	step [137/148], loss=92.9638
	step [138/148], loss=91.5977
	step [139/148], loss=95.5259
	step [140/148], loss=91.5275
	step [141/148], loss=85.3016
	step [142/148], loss=105.1656
	step [143/148], loss=97.2260
	step [144/148], loss=99.8756
	step [145/148], loss=93.3985
	step [146/148], loss=91.1531
	step [147/148], loss=80.0384
	step [148/148], loss=8.3179
	Evaluating
	loss=0.0128, precision=0.3099, recall=0.8583, f1=0.4554
saving model as: 0_saved_model.pth
Training epoch 70
	step [1/148], loss=104.2115
	step [2/148], loss=87.8556
	step [3/148], loss=92.1982
	step [4/148], loss=87.2671
	step [5/148], loss=102.8605
	step [6/148], loss=99.5846
	step [7/148], loss=93.6945
	step [8/148], loss=85.9260
	step [9/148], loss=93.5670
	step [10/148], loss=94.9271
	step [11/148], loss=67.7845
	step [12/148], loss=83.3772
	step [13/148], loss=102.6516
	step [14/148], loss=93.0147
	step [15/148], loss=91.4759
	step [16/148], loss=94.3639
	step [17/148], loss=97.9322
	step [18/148], loss=84.2420
	step [19/148], loss=93.4412
	step [20/148], loss=77.5204
	step [21/148], loss=81.8199
	step [22/148], loss=94.4422
	step [23/148], loss=86.6500
	step [24/148], loss=91.1052
	step [25/148], loss=82.8030
	step [26/148], loss=93.7667
	step [27/148], loss=91.2426
	step [28/148], loss=93.4903
	step [29/148], loss=87.6278
	step [30/148], loss=81.6793
	step [31/148], loss=79.4095
	step [32/148], loss=85.4047
	step [33/148], loss=83.6633
	step [34/148], loss=70.9912
	step [35/148], loss=92.7455
	step [36/148], loss=84.6527
	step [37/148], loss=71.6381
	step [38/148], loss=99.6997
	step [39/148], loss=97.8986
	step [40/148], loss=79.4709
	step [41/148], loss=84.1220
	step [42/148], loss=90.4820
	step [43/148], loss=101.9791
	step [44/148], loss=79.7292
	step [45/148], loss=93.5040
	step [46/148], loss=84.2805
	step [47/148], loss=105.7930
	step [48/148], loss=94.5475
	step [49/148], loss=89.3745
	step [50/148], loss=88.5427
	step [51/148], loss=85.7194
	step [52/148], loss=77.9243
	step [53/148], loss=86.1018
	step [54/148], loss=104.9308
	step [55/148], loss=100.6002
	step [56/148], loss=86.8825
	step [57/148], loss=91.7180
	step [58/148], loss=105.3241
	step [59/148], loss=90.6205
	step [60/148], loss=82.7901
	step [61/148], loss=94.0302
	step [62/148], loss=87.1128
	step [63/148], loss=80.7601
	step [64/148], loss=100.0102
	step [65/148], loss=86.3408
	step [66/148], loss=78.6229
	step [67/148], loss=82.3789
	step [68/148], loss=84.0167
	step [69/148], loss=86.7862
	step [70/148], loss=85.4491
	step [71/148], loss=105.8885
	step [72/148], loss=83.0166
	step [73/148], loss=94.7469
	step [74/148], loss=74.9388
	step [75/148], loss=97.4375
	step [76/148], loss=85.3077
	step [77/148], loss=98.1546
	step [78/148], loss=92.6750
	step [79/148], loss=72.7960
	step [80/148], loss=88.2634
	step [81/148], loss=94.1775
	step [82/148], loss=85.2404
	step [83/148], loss=80.8020
	step [84/148], loss=101.1915
	step [85/148], loss=82.2249
	step [86/148], loss=92.5414
	step [87/148], loss=76.2515
	step [88/148], loss=90.1970
	step [89/148], loss=82.0056
	step [90/148], loss=108.0229
	step [91/148], loss=90.4282
	step [92/148], loss=88.8476
	step [93/148], loss=71.8062
	step [94/148], loss=89.9743
	step [95/148], loss=72.6517
	step [96/148], loss=90.9841
	step [97/148], loss=83.2607
	step [98/148], loss=114.7685
	step [99/148], loss=84.6375
	step [100/148], loss=96.6768
	step [101/148], loss=79.1097
	step [102/148], loss=91.6999
	step [103/148], loss=97.5846
	step [104/148], loss=88.0564
	step [105/148], loss=100.4250
	step [106/148], loss=97.5974
	step [107/148], loss=84.4286
	step [108/148], loss=85.4536
	step [109/148], loss=91.7928
	step [110/148], loss=80.7589
	step [111/148], loss=108.1910
	step [112/148], loss=86.1603
	step [113/148], loss=94.2699
	step [114/148], loss=93.7617
	step [115/148], loss=100.7169
	step [116/148], loss=85.5590
	step [117/148], loss=77.9701
	step [118/148], loss=74.6593
	step [119/148], loss=72.8530
	step [120/148], loss=81.2117
	step [121/148], loss=89.4213
	step [122/148], loss=91.9873
	step [123/148], loss=97.9862
	step [124/148], loss=97.7001
	step [125/148], loss=85.8688
	step [126/148], loss=88.0483
	step [127/148], loss=86.7680
	step [128/148], loss=107.2335
	step [129/148], loss=83.6220
	step [130/148], loss=90.1626
	step [131/148], loss=70.7479
	step [132/148], loss=71.0841
	step [133/148], loss=78.1908
	step [134/148], loss=103.4087
	step [135/148], loss=95.5629
	step [136/148], loss=90.8855
	step [137/148], loss=97.4042
	step [138/148], loss=75.0564
	step [139/148], loss=83.0889
	step [140/148], loss=93.5514
	step [141/148], loss=87.0933
	step [142/148], loss=98.2129
	step [143/148], loss=76.8338
	step [144/148], loss=91.2602
	step [145/148], loss=85.1054
	step [146/148], loss=99.6340
	step [147/148], loss=93.6644
	step [148/148], loss=2.3206
	Evaluating
	loss=0.0177, precision=0.2015, recall=0.8790, f1=0.3278
Training epoch 71
	step [1/148], loss=72.9478
	step [2/148], loss=86.8838
	step [3/148], loss=89.0455
	step [4/148], loss=103.3050
	step [5/148], loss=85.0840
	step [6/148], loss=89.1209
	step [7/148], loss=89.6328
	step [8/148], loss=76.0777
	step [9/148], loss=84.4196
	step [10/148], loss=99.3894
	step [11/148], loss=88.2418
	step [12/148], loss=89.2665
	step [13/148], loss=79.7938
	step [14/148], loss=103.6824
	step [15/148], loss=86.1951
	step [16/148], loss=107.2835
	step [17/148], loss=99.7215
	step [18/148], loss=95.0397
	step [19/148], loss=79.5405
	step [20/148], loss=88.3483
	step [21/148], loss=99.0746
	step [22/148], loss=88.5029
	step [23/148], loss=96.0817
	step [24/148], loss=69.6618
	step [25/148], loss=97.6714
	step [26/148], loss=91.6448
	step [27/148], loss=94.2875
	step [28/148], loss=100.0831
	step [29/148], loss=96.1871
	step [30/148], loss=95.2764
	step [31/148], loss=89.1060
	step [32/148], loss=99.9024
	step [33/148], loss=75.4923
	step [34/148], loss=102.6369
	step [35/148], loss=78.5418
	step [36/148], loss=98.3989
	step [37/148], loss=80.4461
	step [38/148], loss=90.7678
	step [39/148], loss=94.9584
	step [40/148], loss=64.3970
	step [41/148], loss=86.3594
	step [42/148], loss=79.2179
	step [43/148], loss=87.0555
	step [44/148], loss=76.4882
	step [45/148], loss=93.4861
	step [46/148], loss=109.6530
	step [47/148], loss=87.7079
	step [48/148], loss=96.3679
	step [49/148], loss=75.7124
	step [50/148], loss=104.7901
	step [51/148], loss=94.8062
	step [52/148], loss=97.5284
	step [53/148], loss=83.5639
	step [54/148], loss=82.5471
	step [55/148], loss=99.9402
	step [56/148], loss=89.1169
	step [57/148], loss=81.7673
	step [58/148], loss=85.2789
	step [59/148], loss=95.3111
	step [60/148], loss=87.2660
	step [61/148], loss=82.4866
	step [62/148], loss=87.1164
	step [63/148], loss=101.2928
	step [64/148], loss=91.5416
	step [65/148], loss=74.1183
	step [66/148], loss=86.4737
	step [67/148], loss=100.2406
	step [68/148], loss=84.8042
	step [69/148], loss=86.4820
	step [70/148], loss=72.9438
	step [71/148], loss=97.5571
	step [72/148], loss=87.5840
	step [73/148], loss=78.5769
	step [74/148], loss=95.4314
	step [75/148], loss=89.6379
	step [76/148], loss=94.7592
	step [77/148], loss=87.6530
	step [78/148], loss=73.9921
	step [79/148], loss=88.7827
	step [80/148], loss=95.0745
	step [81/148], loss=84.9523
	step [82/148], loss=85.4180
	step [83/148], loss=105.6395
	step [84/148], loss=85.6744
	step [85/148], loss=83.7017
	step [86/148], loss=94.7999
	step [87/148], loss=91.1144
	step [88/148], loss=78.5534
	step [89/148], loss=82.7991
	step [90/148], loss=90.2790
	step [91/148], loss=91.9664
	step [92/148], loss=80.7959
	step [93/148], loss=90.3593
	step [94/148], loss=98.4278
	step [95/148], loss=89.2036
	step [96/148], loss=93.6151
	step [97/148], loss=97.3225
	step [98/148], loss=74.0050
	step [99/148], loss=89.2869
	step [100/148], loss=73.2768
	step [101/148], loss=77.4114
	step [102/148], loss=85.9930
	step [103/148], loss=97.7668
	step [104/148], loss=74.9766
	step [105/148], loss=93.7342
	step [106/148], loss=90.5495
	step [107/148], loss=84.7918
	step [108/148], loss=84.2839
	step [109/148], loss=72.2160
	step [110/148], loss=76.7152
	step [111/148], loss=80.9921
	step [112/148], loss=92.7955
	step [113/148], loss=94.5517
	step [114/148], loss=80.8286
	step [115/148], loss=73.5402
	step [116/148], loss=97.0548
	step [117/148], loss=67.4651
	step [118/148], loss=87.1800
	step [119/148], loss=81.3679
	step [120/148], loss=87.2261
	step [121/148], loss=95.0332
	step [122/148], loss=87.9310
	step [123/148], loss=93.7765
	step [124/148], loss=81.3579
	step [125/148], loss=98.4889
	step [126/148], loss=87.9659
	step [127/148], loss=95.5193
	step [128/148], loss=86.8910
	step [129/148], loss=80.3683
	step [130/148], loss=75.2266
	step [131/148], loss=80.0088
	step [132/148], loss=63.9413
	step [133/148], loss=95.5873
	step [134/148], loss=103.8881
	step [135/148], loss=87.2545
	step [136/148], loss=81.4081
	step [137/148], loss=91.2162
	step [138/148], loss=86.8730
	step [139/148], loss=88.5342
	step [140/148], loss=87.6929
	step [141/148], loss=91.4437
	step [142/148], loss=90.5833
	step [143/148], loss=110.9120
	step [144/148], loss=92.1041
	step [145/148], loss=87.6213
	step [146/148], loss=98.4238
	step [147/148], loss=86.3285
	step [148/148], loss=4.6267
	Evaluating
	loss=0.0143, precision=0.2838, recall=0.8582, f1=0.4265
Training epoch 72
	step [1/148], loss=96.2366
	step [2/148], loss=92.2693
	step [3/148], loss=75.8154
	step [4/148], loss=74.8855
	step [5/148], loss=91.0948
	step [6/148], loss=92.5584
	step [7/148], loss=108.1428
	step [8/148], loss=79.7650
	step [9/148], loss=90.7401
	step [10/148], loss=97.0737
	step [11/148], loss=79.5593
	step [12/148], loss=90.3029
	step [13/148], loss=92.6949
	step [14/148], loss=80.0154
	step [15/148], loss=84.5192
	step [16/148], loss=84.5369
	step [17/148], loss=87.3382
	step [18/148], loss=87.1056
	step [19/148], loss=98.2309
	step [20/148], loss=94.2326
	step [21/148], loss=90.2848
	step [22/148], loss=84.8761
	step [23/148], loss=86.3723
	step [24/148], loss=97.3516
	step [25/148], loss=87.6541
	step [26/148], loss=100.6899
	step [27/148], loss=81.8806
	step [28/148], loss=89.1226
	step [29/148], loss=85.1805
	step [30/148], loss=84.7890
	step [31/148], loss=87.2733
	step [32/148], loss=90.2922
	step [33/148], loss=96.8278
	step [34/148], loss=91.2915
	step [35/148], loss=92.4975
	step [36/148], loss=86.0418
	step [37/148], loss=74.4659
	step [38/148], loss=84.9223
	step [39/148], loss=85.0254
	step [40/148], loss=93.2094
	step [41/148], loss=105.7263
	step [42/148], loss=94.9425
	step [43/148], loss=81.9321
	step [44/148], loss=88.7966
	step [45/148], loss=86.5658
	step [46/148], loss=91.8290
	step [47/148], loss=87.6773
	step [48/148], loss=89.1322
	step [49/148], loss=94.8658
	step [50/148], loss=100.0285
	step [51/148], loss=103.0147
	step [52/148], loss=82.7490
	step [53/148], loss=78.5848
	step [54/148], loss=92.3605
	step [55/148], loss=85.1275
	step [56/148], loss=80.2671
	step [57/148], loss=80.6526
	step [58/148], loss=83.2136
	step [59/148], loss=88.9868
	step [60/148], loss=102.0011
	step [61/148], loss=93.0963
	step [62/148], loss=76.1900
	step [63/148], loss=82.8024
	step [64/148], loss=86.0139
	step [65/148], loss=91.9931
	step [66/148], loss=85.3820
	step [67/148], loss=84.2936
	step [68/148], loss=97.8796
	step [69/148], loss=109.1154
	step [70/148], loss=80.0752
	step [71/148], loss=79.4821
	step [72/148], loss=81.3845
	step [73/148], loss=83.0822
	step [74/148], loss=84.4552
	step [75/148], loss=82.3601
	step [76/148], loss=81.7139
	step [77/148], loss=90.2999
	step [78/148], loss=77.2557
	step [79/148], loss=78.8406
	step [80/148], loss=92.4592
	step [81/148], loss=97.5983
	step [82/148], loss=96.7977
	step [83/148], loss=87.1696
	step [84/148], loss=84.8158
	step [85/148], loss=91.5861
	step [86/148], loss=109.3682
	step [87/148], loss=69.8944
	step [88/148], loss=83.9691
	step [89/148], loss=83.6915
	step [90/148], loss=78.8693
	step [91/148], loss=80.0005
	step [92/148], loss=74.4254
	step [93/148], loss=84.1747
	step [94/148], loss=76.5982
	step [95/148], loss=96.1025
	step [96/148], loss=108.1325
	step [97/148], loss=85.2540
	step [98/148], loss=78.1531
	step [99/148], loss=88.0248
	step [100/148], loss=87.0372
	step [101/148], loss=92.9736
	step [102/148], loss=81.5015
	step [103/148], loss=93.3031
	step [104/148], loss=85.7211
	step [105/148], loss=88.6904
	step [106/148], loss=78.3444
	step [107/148], loss=98.4865
	step [108/148], loss=92.4107
	step [109/148], loss=87.2793
	step [110/148], loss=102.3701
	step [111/148], loss=74.7551
	step [112/148], loss=87.6099
	step [113/148], loss=83.6508
	step [114/148], loss=81.3876
	step [115/148], loss=81.4560
	step [116/148], loss=88.6221
	step [117/148], loss=102.0306
	step [118/148], loss=81.9151
	step [119/148], loss=96.1905
	step [120/148], loss=93.9772
	step [121/148], loss=91.1215
	step [122/148], loss=98.6210
	step [123/148], loss=92.8048
	step [124/148], loss=89.2961
	step [125/148], loss=99.1938
	step [126/148], loss=75.2882
	step [127/148], loss=94.7803
	step [128/148], loss=88.8085
	step [129/148], loss=99.9541
	step [130/148], loss=75.2888
	step [131/148], loss=81.4926
	step [132/148], loss=71.0265
	step [133/148], loss=95.1889
	step [134/148], loss=84.4330
	step [135/148], loss=78.0468
	step [136/148], loss=95.3267
	step [137/148], loss=85.6551
	step [138/148], loss=95.3227
	step [139/148], loss=90.8280
	step [140/148], loss=91.5797
	step [141/148], loss=84.2175
	step [142/148], loss=83.9116
	step [143/148], loss=71.0455
	step [144/148], loss=86.2978
	step [145/148], loss=92.8360
	step [146/148], loss=81.0459
	step [147/148], loss=98.2822
	step [148/148], loss=9.1613
	Evaluating
	loss=0.0133, precision=0.2749, recall=0.8575, f1=0.4164
Training epoch 73
	step [1/148], loss=85.8460
	step [2/148], loss=85.5124
	step [3/148], loss=85.4284
	step [4/148], loss=95.0096
	step [5/148], loss=88.6408
	step [6/148], loss=110.0377
	step [7/148], loss=96.8707
	step [8/148], loss=103.3093
	step [9/148], loss=99.6147
	step [10/148], loss=78.5028
	step [11/148], loss=84.0721
	step [12/148], loss=83.5722
	step [13/148], loss=67.2665
	step [14/148], loss=84.4234
	step [15/148], loss=69.9383
	step [16/148], loss=86.8548
	step [17/148], loss=98.8500
	step [18/148], loss=89.5002
	step [19/148], loss=76.8401
	step [20/148], loss=86.1742
	step [21/148], loss=81.1122
	step [22/148], loss=89.5452
	step [23/148], loss=94.1384
	step [24/148], loss=74.6761
	step [25/148], loss=85.5347
	step [26/148], loss=103.4198
	step [27/148], loss=77.3773
	step [28/148], loss=83.5812
	step [29/148], loss=100.9839
	step [30/148], loss=81.5399
	step [31/148], loss=93.4437
	step [32/148], loss=91.3506
	step [33/148], loss=81.7027
	step [34/148], loss=82.4865
	step [35/148], loss=86.2980
	step [36/148], loss=71.6093
	step [37/148], loss=83.4508
	step [38/148], loss=93.7480
	step [39/148], loss=84.5793
	step [40/148], loss=83.5022
	step [41/148], loss=83.5397
	step [42/148], loss=83.3648
	step [43/148], loss=83.1307
	step [44/148], loss=86.3988
	step [45/148], loss=96.3629
	step [46/148], loss=84.5302
	step [47/148], loss=94.2422
	step [48/148], loss=88.5609
	step [49/148], loss=80.9583
	step [50/148], loss=96.5040
	step [51/148], loss=91.7880
	step [52/148], loss=87.2686
	step [53/148], loss=91.3126
	step [54/148], loss=83.6063
	step [55/148], loss=84.4890
	step [56/148], loss=84.1971
	step [57/148], loss=81.3913
	step [58/148], loss=86.6089
	step [59/148], loss=85.6420
	step [60/148], loss=82.5982
	step [61/148], loss=82.1207
	step [62/148], loss=73.4440
	step [63/148], loss=87.1569
	step [64/148], loss=92.3072
	step [65/148], loss=75.7685
	step [66/148], loss=86.1947
	step [67/148], loss=104.9723
	step [68/148], loss=92.8432
	step [69/148], loss=90.1236
	step [70/148], loss=86.0548
	step [71/148], loss=87.6827
	step [72/148], loss=74.2947
	step [73/148], loss=89.1725
	step [74/148], loss=88.8684
	step [75/148], loss=85.3729
	step [76/148], loss=91.3118
	step [77/148], loss=80.6583
	step [78/148], loss=83.9837
	step [79/148], loss=99.8521
	step [80/148], loss=80.1979
	step [81/148], loss=93.1550
	step [82/148], loss=93.3602
	step [83/148], loss=91.7404
	step [84/148], loss=88.8267
	step [85/148], loss=100.4368
	step [86/148], loss=79.1644
	step [87/148], loss=87.6399
	step [88/148], loss=84.3642
	step [89/148], loss=84.1410
	step [90/148], loss=90.6557
	step [91/148], loss=80.9492
	step [92/148], loss=80.2384
	step [93/148], loss=96.0705
	step [94/148], loss=89.2577
	step [95/148], loss=84.8947
	step [96/148], loss=85.1512
	step [97/148], loss=89.7256
	step [98/148], loss=82.0663
	step [99/148], loss=78.3608
	step [100/148], loss=86.8721
	step [101/148], loss=82.9696
	step [102/148], loss=76.8729
	step [103/148], loss=91.6026
	step [104/148], loss=98.5422
	step [105/148], loss=98.0373
	step [106/148], loss=73.1603
	step [107/148], loss=90.4518
	step [108/148], loss=81.2392
	step [109/148], loss=81.9350
	step [110/148], loss=85.6698
	step [111/148], loss=111.2028
	step [112/148], loss=79.6157
	step [113/148], loss=89.4403
	step [114/148], loss=101.2348
	step [115/148], loss=82.1908
	step [116/148], loss=102.2322
	step [117/148], loss=96.2914
	step [118/148], loss=86.9139
	step [119/148], loss=82.4231
	step [120/148], loss=85.1883
	step [121/148], loss=72.0188
	step [122/148], loss=91.5609
	step [123/148], loss=77.5529
	step [124/148], loss=103.9750
	step [125/148], loss=79.7228
	step [126/148], loss=77.5239
	step [127/148], loss=74.0850
	step [128/148], loss=107.6483
	step [129/148], loss=85.0447
	step [130/148], loss=71.4145
	step [131/148], loss=81.4347
	step [132/148], loss=106.1605
	step [133/148], loss=83.9344
	step [134/148], loss=97.8825
	step [135/148], loss=85.7921
	step [136/148], loss=92.8476
	step [137/148], loss=106.6928
	step [138/148], loss=89.5827
	step [139/148], loss=85.8598
	step [140/148], loss=104.2510
	step [141/148], loss=96.9084
	step [142/148], loss=90.2886
	step [143/148], loss=98.7874
	step [144/148], loss=91.1745
	step [145/148], loss=100.9667
	step [146/148], loss=81.6201
	step [147/148], loss=84.3348
	step [148/148], loss=9.2389
	Evaluating
	loss=0.0129, precision=0.2970, recall=0.8416, f1=0.4390
Training epoch 74
	step [1/148], loss=95.5254
	step [2/148], loss=94.7087
	step [3/148], loss=82.0316
	step [4/148], loss=96.6605
	step [5/148], loss=83.1443
	step [6/148], loss=94.2157
	step [7/148], loss=71.2729
	step [8/148], loss=81.1085
	step [9/148], loss=104.2259
	step [10/148], loss=80.1896
	step [11/148], loss=97.1131
	step [12/148], loss=76.4917
	step [13/148], loss=80.9543
	step [14/148], loss=68.3094
	step [15/148], loss=94.2541
	step [16/148], loss=91.1545
	step [17/148], loss=96.0253
	step [18/148], loss=79.8369
	step [19/148], loss=90.1659
	step [20/148], loss=95.3455
	step [21/148], loss=80.4511
	step [22/148], loss=97.7385
	step [23/148], loss=96.0874
	step [24/148], loss=93.8617
	step [25/148], loss=92.7591
	step [26/148], loss=97.0887
	step [27/148], loss=78.4776
	step [28/148], loss=110.8340
	step [29/148], loss=88.6026
	step [30/148], loss=104.1233
	step [31/148], loss=97.7172
	step [32/148], loss=92.5089
	step [33/148], loss=91.6661
	step [34/148], loss=101.1895
	step [35/148], loss=87.2952
	step [36/148], loss=82.5781
	step [37/148], loss=78.7751
	step [38/148], loss=101.1252
	step [39/148], loss=93.2368
	step [40/148], loss=93.9443
	step [41/148], loss=92.7949
	step [42/148], loss=78.8056
	step [43/148], loss=93.8365
	step [44/148], loss=105.4217
	step [45/148], loss=98.8190
	step [46/148], loss=77.8596
	step [47/148], loss=78.1571
	step [48/148], loss=95.4305
	step [49/148], loss=100.7175
	step [50/148], loss=89.1247
	step [51/148], loss=97.6322
	step [52/148], loss=104.4372
	step [53/148], loss=99.1911
	step [54/148], loss=92.3421
	step [55/148], loss=74.0761
	step [56/148], loss=86.3818
	step [57/148], loss=86.3007
	step [58/148], loss=85.5124
	step [59/148], loss=88.5442
	step [60/148], loss=63.0127
	step [61/148], loss=77.5412
	step [62/148], loss=85.2964
	step [63/148], loss=84.6686
	step [64/148], loss=81.8767
	step [65/148], loss=82.4503
	step [66/148], loss=97.4036
	step [67/148], loss=90.5180
	step [68/148], loss=86.3698
	step [69/148], loss=83.6931
	step [70/148], loss=87.2396
	step [71/148], loss=108.0626
	step [72/148], loss=93.1756
	step [73/148], loss=98.7048
	step [74/148], loss=87.5343
	step [75/148], loss=104.4054
	step [76/148], loss=85.4074
	step [77/148], loss=85.1257
	step [78/148], loss=86.2633
	step [79/148], loss=94.1759
	step [80/148], loss=81.1069
	step [81/148], loss=89.3905
	step [82/148], loss=85.5767
	step [83/148], loss=100.8372
	step [84/148], loss=75.1125
	step [85/148], loss=81.2699
	step [86/148], loss=70.9802
	step [87/148], loss=83.6345
	step [88/148], loss=98.8420
	step [89/148], loss=94.0682
	step [90/148], loss=84.1225
	step [91/148], loss=88.9710
	step [92/148], loss=89.5086
	step [93/148], loss=82.6770
	step [94/148], loss=83.9959
	step [95/148], loss=74.2979
	step [96/148], loss=85.6888
	step [97/148], loss=85.7603
	step [98/148], loss=92.3865
	step [99/148], loss=99.6076
	step [100/148], loss=86.9335
	step [101/148], loss=70.4766
	step [102/148], loss=67.8905
	step [103/148], loss=91.1973
	step [104/148], loss=88.6697
	step [105/148], loss=87.7806
	step [106/148], loss=90.5764
	step [107/148], loss=75.9778
	step [108/148], loss=87.2115
	step [109/148], loss=75.5424
	step [110/148], loss=76.6089
	step [111/148], loss=68.8163
	step [112/148], loss=95.8000
	step [113/148], loss=80.6503
	step [114/148], loss=76.0114
	step [115/148], loss=94.0984
	step [116/148], loss=74.5355
	step [117/148], loss=90.0844
	step [118/148], loss=78.2950
	step [119/148], loss=67.8236
	step [120/148], loss=81.6313
	step [121/148], loss=104.0244
	step [122/148], loss=89.3507
	step [123/148], loss=74.6148
	step [124/148], loss=93.6862
	step [125/148], loss=76.4073
	step [126/148], loss=91.9265
	step [127/148], loss=81.0136
	step [128/148], loss=79.1563
	step [129/148], loss=94.6767
	step [130/148], loss=81.8578
	step [131/148], loss=91.3767
	step [132/148], loss=78.2688
	step [133/148], loss=79.0051
	step [134/148], loss=90.8204
	step [135/148], loss=80.4299
	step [136/148], loss=88.2606
	step [137/148], loss=95.0135
	step [138/148], loss=75.5498
	step [139/148], loss=83.4858
	step [140/148], loss=74.6872
	step [141/148], loss=91.8821
	step [142/148], loss=82.2705
	step [143/148], loss=107.6733
	step [144/148], loss=87.5037
	step [145/148], loss=101.3233
	step [146/148], loss=82.5470
	step [147/148], loss=83.3533
	step [148/148], loss=5.1653
	Evaluating
	loss=0.0140, precision=0.2757, recall=0.8563, f1=0.4171
Training epoch 75
	step [1/148], loss=91.8467
	step [2/148], loss=88.2200
	step [3/148], loss=84.8867
	step [4/148], loss=94.8091
	step [5/148], loss=83.0450
	step [6/148], loss=93.2076
	step [7/148], loss=87.5027
	step [8/148], loss=81.6742
	step [9/148], loss=75.5388
	step [10/148], loss=82.8029
	step [11/148], loss=82.4796
	step [12/148], loss=81.8253
	step [13/148], loss=92.4701
	step [14/148], loss=94.1706
	step [15/148], loss=112.0235
	step [16/148], loss=95.6643
	step [17/148], loss=97.6071
	step [18/148], loss=101.3604
	step [19/148], loss=80.3926
	step [20/148], loss=74.0873
	step [21/148], loss=85.4724
	step [22/148], loss=94.6356
	step [23/148], loss=83.2065
	step [24/148], loss=78.6237
	step [25/148], loss=84.3721
	step [26/148], loss=97.1587
	step [27/148], loss=70.1517
	step [28/148], loss=80.8022
	step [29/148], loss=93.7478
	step [30/148], loss=74.5298
	step [31/148], loss=106.7527
	step [32/148], loss=79.8860
	step [33/148], loss=86.0579
	step [34/148], loss=109.4234
	step [35/148], loss=74.9848
	step [36/148], loss=95.5920
	step [37/148], loss=88.9995
	step [38/148], loss=82.5713
	step [39/148], loss=88.4753
	step [40/148], loss=73.3182
	step [41/148], loss=95.4996
	step [42/148], loss=84.0169
	step [43/148], loss=87.2896
	step [44/148], loss=81.4823
	step [45/148], loss=88.8673
	step [46/148], loss=88.5504
	step [47/148], loss=104.6145
	step [48/148], loss=79.3042
	step [49/148], loss=73.3605
	step [50/148], loss=87.2391
	step [51/148], loss=87.6424
	step [52/148], loss=74.2404
	step [53/148], loss=92.3978
	step [54/148], loss=89.7172
	step [55/148], loss=89.3866
	step [56/148], loss=90.6094
	step [57/148], loss=86.1588
	step [58/148], loss=80.7864
	step [59/148], loss=87.5204
	step [60/148], loss=91.7596
	step [61/148], loss=73.9459
	step [62/148], loss=87.9973
	step [63/148], loss=80.2018
	step [64/148], loss=96.8800
	step [65/148], loss=103.5901
	step [66/148], loss=89.8728
	step [67/148], loss=93.5464
	step [68/148], loss=91.0438
	step [69/148], loss=86.7949
	step [70/148], loss=80.8600
	step [71/148], loss=89.4716
	step [72/148], loss=77.8096
	step [73/148], loss=91.4062
	step [74/148], loss=68.6735
	step [75/148], loss=92.2503
	step [76/148], loss=101.1766
	step [77/148], loss=81.8424
	step [78/148], loss=72.6179
	step [79/148], loss=97.4505
	step [80/148], loss=94.2507
	step [81/148], loss=97.9526
	step [82/148], loss=107.7622
	step [83/148], loss=76.7763
	step [84/148], loss=89.0757
	step [85/148], loss=94.6039
	step [86/148], loss=94.9990
	step [87/148], loss=74.2305
	step [88/148], loss=89.8274
	step [89/148], loss=111.0898
	step [90/148], loss=73.5630
	step [91/148], loss=86.6888
	step [92/148], loss=82.1624
	step [93/148], loss=93.3950
	step [94/148], loss=72.0152
	step [95/148], loss=92.5774
	step [96/148], loss=85.9658
	step [97/148], loss=94.8346
	step [98/148], loss=77.4431
	step [99/148], loss=90.1121
	step [100/148], loss=81.4329
	step [101/148], loss=85.5759
	step [102/148], loss=99.0647
	step [103/148], loss=100.4888
	step [104/148], loss=89.0480
	step [105/148], loss=85.2863
	step [106/148], loss=76.4769
	step [107/148], loss=75.0830
	step [108/148], loss=96.0870
	step [109/148], loss=92.3876
	step [110/148], loss=84.1035
	step [111/148], loss=79.4861
	step [112/148], loss=71.2541
	step [113/148], loss=81.5419
	step [114/148], loss=71.0393
	step [115/148], loss=83.9396
	step [116/148], loss=84.2971
	step [117/148], loss=89.8605
	step [118/148], loss=85.4822
	step [119/148], loss=98.3776
	step [120/148], loss=81.1752
	step [121/148], loss=98.5873
	step [122/148], loss=92.0403
	step [123/148], loss=79.9772
	step [124/148], loss=77.4269
	step [125/148], loss=83.3792
	step [126/148], loss=85.4492
	step [127/148], loss=89.7004
	step [128/148], loss=96.4757
	step [129/148], loss=85.8355
	step [130/148], loss=74.7998
	step [131/148], loss=95.8640
	step [132/148], loss=82.8716
	step [133/148], loss=97.7815
	step [134/148], loss=64.5398
	step [135/148], loss=78.6516
	step [136/148], loss=85.5823
	step [137/148], loss=90.9389
	step [138/148], loss=77.8385
	step [139/148], loss=82.5078
	step [140/148], loss=86.6528
	step [141/148], loss=91.5487
	step [142/148], loss=67.8941
	step [143/148], loss=85.4518
	step [144/148], loss=87.2312
	step [145/148], loss=88.5790
	step [146/148], loss=79.4979
	step [147/148], loss=96.8472
	step [148/148], loss=9.0462
	Evaluating
	loss=0.0137, precision=0.2666, recall=0.8490, f1=0.4057
Training epoch 76
	step [1/148], loss=82.8486
	step [2/148], loss=97.9816
	step [3/148], loss=98.2546
	step [4/148], loss=82.8629
	step [5/148], loss=92.7671
	step [6/148], loss=89.3565
	step [7/148], loss=101.0055
	step [8/148], loss=73.9615
	step [9/148], loss=69.3644
	step [10/148], loss=65.8892
	step [11/148], loss=96.9315
	step [12/148], loss=99.4166
	step [13/148], loss=87.0988
	step [14/148], loss=89.6555
	step [15/148], loss=83.5045
	step [16/148], loss=87.1981
	step [17/148], loss=92.0440
	step [18/148], loss=75.7095
	step [19/148], loss=92.8319
	step [20/148], loss=97.2699
	step [21/148], loss=86.0042
	step [22/148], loss=87.3560
	step [23/148], loss=75.7813
	step [24/148], loss=93.1565
	step [25/148], loss=86.3513
	step [26/148], loss=86.1560
	step [27/148], loss=73.7826
	step [28/148], loss=75.3440
	step [29/148], loss=79.7530
	step [30/148], loss=92.1338
	step [31/148], loss=83.6486
	step [32/148], loss=93.8670
	step [33/148], loss=78.7229
	step [34/148], loss=77.1767
	step [35/148], loss=88.9259
	step [36/148], loss=95.7107
	step [37/148], loss=81.4424
	step [38/148], loss=78.1457
	step [39/148], loss=92.0572
	step [40/148], loss=75.7900
	step [41/148], loss=72.9887
	step [42/148], loss=102.5393
	step [43/148], loss=95.1429
	step [44/148], loss=78.3199
	step [45/148], loss=88.8146
	step [46/148], loss=109.9753
	step [47/148], loss=96.7852
	step [48/148], loss=108.5969
	step [49/148], loss=88.9880
	step [50/148], loss=90.6955
	step [51/148], loss=78.3428
	step [52/148], loss=74.5644
	step [53/148], loss=95.5501
	step [54/148], loss=84.5383
	step [55/148], loss=75.9434
	step [56/148], loss=73.1782
	step [57/148], loss=85.1882
	step [58/148], loss=100.4562
	step [59/148], loss=89.2824
	step [60/148], loss=84.2995
	step [61/148], loss=92.9416
	step [62/148], loss=76.8418
	step [63/148], loss=83.8297
	step [64/148], loss=93.4816
	step [65/148], loss=91.5664
	step [66/148], loss=75.9091
	step [67/148], loss=87.5391
	step [68/148], loss=93.4454
	step [69/148], loss=86.9768
	step [70/148], loss=99.0160
	step [71/148], loss=88.1831
	step [72/148], loss=63.9353
	step [73/148], loss=81.2429
	step [74/148], loss=103.2342
	step [75/148], loss=94.1273
	step [76/148], loss=81.7344
	step [77/148], loss=97.0100
	step [78/148], loss=93.1247
	step [79/148], loss=93.6564
	step [80/148], loss=93.0270
	step [81/148], loss=88.5500
	step [82/148], loss=94.6946
	step [83/148], loss=85.9763
	step [84/148], loss=83.6553
	step [85/148], loss=92.7701
	step [86/148], loss=89.2237
	step [87/148], loss=88.9753
	step [88/148], loss=84.1246
	step [89/148], loss=81.2739
	step [90/148], loss=92.9245
	step [91/148], loss=81.0266
	step [92/148], loss=77.5134
	step [93/148], loss=82.6621
	step [94/148], loss=96.3455
	step [95/148], loss=72.7567
	step [96/148], loss=106.6670
	step [97/148], loss=74.8114
	step [98/148], loss=87.2314
	step [99/148], loss=81.4663
	step [100/148], loss=81.4258
	step [101/148], loss=78.8980
	step [102/148], loss=73.7634
	step [103/148], loss=81.3384
	step [104/148], loss=93.6666
	step [105/148], loss=81.0506
	step [106/148], loss=81.6644
	step [107/148], loss=89.9909
	step [108/148], loss=93.0415
	step [109/148], loss=76.5089
	step [110/148], loss=83.0572
	step [111/148], loss=84.2102
	step [112/148], loss=89.2765
	step [113/148], loss=89.2722
	step [114/148], loss=93.7050
	step [115/148], loss=94.8768
	step [116/148], loss=85.5594
	step [117/148], loss=80.4972
	step [118/148], loss=79.7086
	step [119/148], loss=94.0777
	step [120/148], loss=84.9317
	step [121/148], loss=72.8960
	step [122/148], loss=91.3997
	step [123/148], loss=89.8042
	step [124/148], loss=89.4953
	step [125/148], loss=90.9689
	step [126/148], loss=82.0197
	step [127/148], loss=82.6251
	step [128/148], loss=93.3332
	step [129/148], loss=84.4058
	step [130/148], loss=92.9899
	step [131/148], loss=84.5125
	step [132/148], loss=81.5026
	step [133/148], loss=94.0499
	step [134/148], loss=93.5652
	step [135/148], loss=78.1175
	step [136/148], loss=91.1409
	step [137/148], loss=95.6208
	step [138/148], loss=87.9634
	step [139/148], loss=79.9152
	step [140/148], loss=78.8785
	step [141/148], loss=73.6524
	step [142/148], loss=90.1544
	step [143/148], loss=71.5181
	step [144/148], loss=90.8090
	step [145/148], loss=91.6369
	step [146/148], loss=102.0195
	step [147/148], loss=78.3270
	step [148/148], loss=6.8935
	Evaluating
	loss=0.0138, precision=0.2566, recall=0.8433, f1=0.3935
Training epoch 77
	step [1/148], loss=92.9233
	step [2/148], loss=92.7946
	step [3/148], loss=86.5242
	step [4/148], loss=95.0041
	step [5/148], loss=79.4233
	step [6/148], loss=86.9472
	step [7/148], loss=95.4651
	step [8/148], loss=91.9358
	step [9/148], loss=88.4313
	step [10/148], loss=86.7333
	step [11/148], loss=93.0733
	step [12/148], loss=88.4386
	step [13/148], loss=88.6744
	step [14/148], loss=78.8038
	step [15/148], loss=93.5372
	step [16/148], loss=78.2752
	step [17/148], loss=86.2076
	step [18/148], loss=87.2522
	step [19/148], loss=84.0879
	step [20/148], loss=90.0793
	step [21/148], loss=85.1139
	step [22/148], loss=89.7500
	step [23/148], loss=102.7264
	step [24/148], loss=82.3020
	step [25/148], loss=82.4370
	step [26/148], loss=107.9172
	step [27/148], loss=89.5644
	step [28/148], loss=78.7517
	step [29/148], loss=94.4080
	step [30/148], loss=79.5676
	step [31/148], loss=78.7023
	step [32/148], loss=94.7554
	step [33/148], loss=88.0907
	step [34/148], loss=85.8881
	step [35/148], loss=89.0508
	step [36/148], loss=90.5579
	step [37/148], loss=77.7953
	step [38/148], loss=84.0430
	step [39/148], loss=87.4929
	step [40/148], loss=83.7090
	step [41/148], loss=82.3304
	step [42/148], loss=79.8646
	step [43/148], loss=93.2539
	step [44/148], loss=97.1310
	step [45/148], loss=77.4965
	step [46/148], loss=91.4740
	step [47/148], loss=85.1764
	step [48/148], loss=84.8797
	step [49/148], loss=87.8090
	step [50/148], loss=93.2612
	step [51/148], loss=78.1003
	step [52/148], loss=92.5286
	step [53/148], loss=93.6293
	step [54/148], loss=98.1711
	step [55/148], loss=88.5477
	step [56/148], loss=64.8256
	step [57/148], loss=69.8756
	step [58/148], loss=102.4175
	step [59/148], loss=90.5618
	step [60/148], loss=90.9371
	step [61/148], loss=88.9821
	step [62/148], loss=94.2128
	step [63/148], loss=82.1225
	step [64/148], loss=70.8670
	step [65/148], loss=72.4539
	step [66/148], loss=91.1464
	step [67/148], loss=84.8783
	step [68/148], loss=79.6195
	step [69/148], loss=96.7929
	step [70/148], loss=94.2464
	step [71/148], loss=93.2978
	step [72/148], loss=85.2157
	step [73/148], loss=88.1639
	step [74/148], loss=81.8328
	step [75/148], loss=100.8064
	step [76/148], loss=87.2465
	step [77/148], loss=81.7739
	step [78/148], loss=81.3660
	step [79/148], loss=82.5482
	step [80/148], loss=78.2026
	step [81/148], loss=85.2661
	step [82/148], loss=93.6784
	step [83/148], loss=104.4248
	step [84/148], loss=82.9496
	step [85/148], loss=85.8532
	step [86/148], loss=79.3449
	step [87/148], loss=74.3716
	step [88/148], loss=87.4758
	step [89/148], loss=79.6634
	step [90/148], loss=89.2093
	step [91/148], loss=84.7249
	step [92/148], loss=79.3466
	step [93/148], loss=86.3783
	step [94/148], loss=72.1799
	step [95/148], loss=83.5817
	step [96/148], loss=80.7948
	step [97/148], loss=83.2986
	step [98/148], loss=83.1910
	step [99/148], loss=96.3637
	step [100/148], loss=91.8757
	step [101/148], loss=76.4617
	step [102/148], loss=102.2126
	step [103/148], loss=100.7980
	step [104/148], loss=89.8833
	step [105/148], loss=80.7227
	step [106/148], loss=86.1508
	step [107/148], loss=96.9827
	step [108/148], loss=103.8933
	step [109/148], loss=75.6214
	step [110/148], loss=97.9407
	step [111/148], loss=95.6863
	step [112/148], loss=81.5928
	step [113/148], loss=72.1846
	step [114/148], loss=89.6798
	step [115/148], loss=94.9405
	step [116/148], loss=82.0208
	step [117/148], loss=88.9306
	step [118/148], loss=75.6437
	step [119/148], loss=77.9186
	step [120/148], loss=82.6097
	step [121/148], loss=79.6008
	step [122/148], loss=96.6086
	step [123/148], loss=90.1684
	step [124/148], loss=98.0425
	step [125/148], loss=91.2915
	step [126/148], loss=77.3745
	step [127/148], loss=91.7915
	step [128/148], loss=79.9110
	step [129/148], loss=75.1207
	step [130/148], loss=81.0959
	step [131/148], loss=80.8039
	step [132/148], loss=100.7783
	step [133/148], loss=82.2490
	step [134/148], loss=69.3349
	step [135/148], loss=78.3900
	step [136/148], loss=83.1616
	step [137/148], loss=86.8696
	step [138/148], loss=67.8074
	step [139/148], loss=84.2730
	step [140/148], loss=72.7241
	step [141/148], loss=75.4726
	step [142/148], loss=82.0975
	step [143/148], loss=87.8045
	step [144/148], loss=79.1919
	step [145/148], loss=100.3257
	step [146/148], loss=85.6245
	step [147/148], loss=97.7713
	step [148/148], loss=5.8896
	Evaluating
	loss=0.0135, precision=0.2775, recall=0.8548, f1=0.4190
Training epoch 78
	step [1/148], loss=77.3862
	step [2/148], loss=90.7179
	step [3/148], loss=81.1562
	step [4/148], loss=79.0583
	step [5/148], loss=88.6188
	step [6/148], loss=82.5704
	step [7/148], loss=80.0471
	step [8/148], loss=90.6634
	step [9/148], loss=81.0934
	step [10/148], loss=86.8943
	step [11/148], loss=93.4534
	step [12/148], loss=74.9035
	step [13/148], loss=77.5659
	step [14/148], loss=76.4484
	step [15/148], loss=96.4644
	step [16/148], loss=77.9656
	step [17/148], loss=96.0865
	step [18/148], loss=84.7747
	step [19/148], loss=100.7702
	step [20/148], loss=84.5981
	step [21/148], loss=99.6751
	step [22/148], loss=92.9227
	step [23/148], loss=82.1017
	step [24/148], loss=67.5626
	step [25/148], loss=87.4007
	step [26/148], loss=80.6384
	step [27/148], loss=89.7042
	step [28/148], loss=81.4730
	step [29/148], loss=84.9094
	step [30/148], loss=84.7063
	step [31/148], loss=81.2711
	step [32/148], loss=82.7854
	step [33/148], loss=89.6065
	step [34/148], loss=86.3869
	step [35/148], loss=81.6647
	step [36/148], loss=86.6425
	step [37/148], loss=95.1385
	step [38/148], loss=90.7777
	step [39/148], loss=91.7181
	step [40/148], loss=90.7071
	step [41/148], loss=93.3344
	step [42/148], loss=96.4225
	step [43/148], loss=93.0679
	step [44/148], loss=94.2870
	step [45/148], loss=68.3555
	step [46/148], loss=89.8148
	step [47/148], loss=91.2076
	step [48/148], loss=77.6008
	step [49/148], loss=97.3888
	step [50/148], loss=84.3360
	step [51/148], loss=104.8847
	step [52/148], loss=82.1013
	step [53/148], loss=79.6132
	step [54/148], loss=88.7686
	step [55/148], loss=106.3130
	step [56/148], loss=77.8883
	step [57/148], loss=76.5247
	step [58/148], loss=87.8308
	step [59/148], loss=89.4950
	step [60/148], loss=89.8464
	step [61/148], loss=92.3374
	step [62/148], loss=92.2037
	step [63/148], loss=85.4265
	step [64/148], loss=81.2525
	step [65/148], loss=73.8542
	step [66/148], loss=67.6380
	step [67/148], loss=72.8690
	step [68/148], loss=85.6564
	step [69/148], loss=82.0753
	step [70/148], loss=81.6951
	step [71/148], loss=87.7570
	step [72/148], loss=87.3815
	step [73/148], loss=79.9467
	step [74/148], loss=82.0860
	step [75/148], loss=80.0274
	step [76/148], loss=75.7026
	step [77/148], loss=87.0758
	step [78/148], loss=83.6854
	step [79/148], loss=84.9665
	step [80/148], loss=70.9243
	step [81/148], loss=96.6071
	step [82/148], loss=87.6626
	step [83/148], loss=89.5227
	step [84/148], loss=81.5832
	step [85/148], loss=92.9550
	step [86/148], loss=92.8618
	step [87/148], loss=91.7993
	step [88/148], loss=81.2691
	step [89/148], loss=75.3006
	step [90/148], loss=98.4287
	step [91/148], loss=87.4373
	step [92/148], loss=92.7085
	step [93/148], loss=86.8769
	step [94/148], loss=69.1375
	step [95/148], loss=99.0015
	step [96/148], loss=83.0316
	step [97/148], loss=89.5938
	step [98/148], loss=81.4246
	step [99/148], loss=75.6275
	step [100/148], loss=75.2661
	step [101/148], loss=88.3745
	step [102/148], loss=71.0762
	step [103/148], loss=88.3248
	step [104/148], loss=82.8670
	step [105/148], loss=99.3155
	step [106/148], loss=88.1597
	step [107/148], loss=95.7365
	step [108/148], loss=72.4669
	step [109/148], loss=98.4907
	step [110/148], loss=92.6222
	step [111/148], loss=82.3471
	step [112/148], loss=85.0757
	step [113/148], loss=94.0879
	step [114/148], loss=92.8042
	step [115/148], loss=88.3768
	step [116/148], loss=88.4686
	step [117/148], loss=77.2291
	step [118/148], loss=111.3197
	step [119/148], loss=98.0939
	step [120/148], loss=92.5871
	step [121/148], loss=105.9266
	step [122/148], loss=87.1149
	step [123/148], loss=85.5501
	step [124/148], loss=82.3626
	step [125/148], loss=83.6506
	step [126/148], loss=81.1331
	step [127/148], loss=70.0133
	step [128/148], loss=80.3265
	step [129/148], loss=92.5849
	step [130/148], loss=83.1393
	step [131/148], loss=82.0780
	step [132/148], loss=78.8407
	step [133/148], loss=77.4147
	step [134/148], loss=93.7281
	step [135/148], loss=96.5528
	step [136/148], loss=75.9617
	step [137/148], loss=77.0914
	step [138/148], loss=99.4539
	step [139/148], loss=79.5709
	step [140/148], loss=70.3016
	step [141/148], loss=88.7845
	step [142/148], loss=84.3496
	step [143/148], loss=77.1270
	step [144/148], loss=92.8125
	step [145/148], loss=104.1670
	step [146/148], loss=82.3434
	step [147/148], loss=95.6280
	step [148/148], loss=9.9075
	Evaluating
	loss=0.0119, precision=0.3133, recall=0.8495, f1=0.4578
saving model as: 0_saved_model.pth
Training epoch 79
	step [1/148], loss=89.9675
	step [2/148], loss=82.8515
	step [3/148], loss=92.2347
	step [4/148], loss=82.0567
	step [5/148], loss=100.3890
	step [6/148], loss=58.0692
	step [7/148], loss=86.0731
	step [8/148], loss=91.9034
	step [9/148], loss=80.4207
	step [10/148], loss=90.4215
	step [11/148], loss=75.0815
	step [12/148], loss=86.4310
	step [13/148], loss=65.6177
	step [14/148], loss=76.2927
	step [15/148], loss=91.4989
	step [16/148], loss=85.1466
	step [17/148], loss=84.8121
	step [18/148], loss=79.3607
	step [19/148], loss=84.5376
	step [20/148], loss=83.1580
	step [21/148], loss=76.8562
	step [22/148], loss=73.0206
	step [23/148], loss=89.9859
	step [24/148], loss=92.9668
	step [25/148], loss=93.3999
	step [26/148], loss=83.7551
	step [27/148], loss=81.3951
	step [28/148], loss=82.1105
	step [29/148], loss=89.4459
	step [30/148], loss=89.2036
	step [31/148], loss=89.8057
	step [32/148], loss=82.6639
	step [33/148], loss=96.2529
	step [34/148], loss=81.4025
	step [35/148], loss=94.4675
	step [36/148], loss=82.3078
	step [37/148], loss=92.1117
	step [38/148], loss=83.6646
	step [39/148], loss=80.8598
	step [40/148], loss=91.3314
	step [41/148], loss=87.2442
	step [42/148], loss=98.3654
	step [43/148], loss=83.3498
	step [44/148], loss=84.0103
	step [45/148], loss=94.1472
	step [46/148], loss=97.7337
	step [47/148], loss=74.5836
	step [48/148], loss=85.1650
	step [49/148], loss=85.0509
	step [50/148], loss=77.4268
	step [51/148], loss=76.2742
	step [52/148], loss=90.7451
	step [53/148], loss=92.9667
	step [54/148], loss=82.3106
	step [55/148], loss=85.4515
	step [56/148], loss=90.0770
	step [57/148], loss=79.1531
	step [58/148], loss=86.1496
	step [59/148], loss=93.1686
	step [60/148], loss=93.1125
	step [61/148], loss=103.5290
	step [62/148], loss=83.4136
	step [63/148], loss=71.5986
	step [64/148], loss=111.7928
	step [65/148], loss=88.8993
	step [66/148], loss=90.9928
	step [67/148], loss=85.6649
	step [68/148], loss=100.1770
	step [69/148], loss=86.8870
	step [70/148], loss=86.1143
	step [71/148], loss=80.2077
	step [72/148], loss=70.1775
	step [73/148], loss=85.4604
	step [74/148], loss=79.0155
	step [75/148], loss=71.5240
	step [76/148], loss=88.1298
	step [77/148], loss=79.1648
	step [78/148], loss=71.3495
	step [79/148], loss=75.1652
	step [80/148], loss=96.9038
	step [81/148], loss=94.3681
	step [82/148], loss=94.2484
	step [83/148], loss=71.6519
	step [84/148], loss=76.7546
	step [85/148], loss=91.3038
	step [86/148], loss=74.6422
	step [87/148], loss=88.4407
	step [88/148], loss=98.7790
	step [89/148], loss=88.8380
	step [90/148], loss=84.9131
	step [91/148], loss=89.9977
	step [92/148], loss=76.5977
	step [93/148], loss=86.2295
	step [94/148], loss=86.5138
	step [95/148], loss=83.4596
	step [96/148], loss=67.7222
	step [97/148], loss=69.7070
	step [98/148], loss=99.4607
	step [99/148], loss=81.0656
	step [100/148], loss=82.2723
	step [101/148], loss=69.4365
	step [102/148], loss=88.0828
	step [103/148], loss=86.4379
	step [104/148], loss=86.3345
	step [105/148], loss=87.4033
	step [106/148], loss=71.6667
	step [107/148], loss=87.9885
	step [108/148], loss=88.5820
	step [109/148], loss=93.7267
	step [110/148], loss=78.7821
	step [111/148], loss=89.3111
	step [112/148], loss=100.3740
	step [113/148], loss=78.7100
	step [114/148], loss=88.6249
	step [115/148], loss=97.4772
	step [116/148], loss=86.2776
	step [117/148], loss=105.0039
	step [118/148], loss=89.6861
	step [119/148], loss=86.0934
	step [120/148], loss=85.3896
	step [121/148], loss=79.7479
	step [122/148], loss=78.2252
	step [123/148], loss=84.5515
	step [124/148], loss=88.9885
	step [125/148], loss=84.3630
	step [126/148], loss=87.0135
	step [127/148], loss=79.5901
	step [128/148], loss=84.2679
	step [129/148], loss=86.6068
	step [130/148], loss=90.5754
	step [131/148], loss=82.0424
	step [132/148], loss=84.5717
	step [133/148], loss=88.0479
	step [134/148], loss=79.5596
	step [135/148], loss=96.2559
	step [136/148], loss=92.1268
	step [137/148], loss=87.2981
	step [138/148], loss=82.5506
	step [139/148], loss=95.0577
	step [140/148], loss=81.7897
	step [141/148], loss=93.5221
	step [142/148], loss=84.3591
	step [143/148], loss=101.6968
	step [144/148], loss=74.3640
	step [145/148], loss=104.4769
	step [146/148], loss=68.7290
	step [147/148], loss=84.5607
	step [148/148], loss=5.8993
	Evaluating
	loss=0.0134, precision=0.2745, recall=0.8440, f1=0.4143
Training epoch 80
	step [1/148], loss=89.5301
	step [2/148], loss=82.0071
	step [3/148], loss=101.3292
	step [4/148], loss=94.2605
	step [5/148], loss=80.3035
	step [6/148], loss=81.9380
	step [7/148], loss=91.1416
	step [8/148], loss=78.7789
	step [9/148], loss=89.0855
	step [10/148], loss=92.3714
	step [11/148], loss=87.4923
	step [12/148], loss=81.3154
	step [13/148], loss=82.5543
	step [14/148], loss=77.5855
	step [15/148], loss=82.6539
	step [16/148], loss=75.1279
	step [17/148], loss=82.9796
	step [18/148], loss=89.0976
	step [19/148], loss=79.0024
	step [20/148], loss=80.8566
	step [21/148], loss=89.7389
	step [22/148], loss=97.1746
	step [23/148], loss=84.9388
	step [24/148], loss=92.3530
	step [25/148], loss=101.7940
	step [26/148], loss=95.7535
	step [27/148], loss=76.9832
	step [28/148], loss=90.1156
	step [29/148], loss=97.3970
	step [30/148], loss=91.5745
	step [31/148], loss=87.2573
	step [32/148], loss=73.7066
	step [33/148], loss=89.4222
	step [34/148], loss=71.9427
	step [35/148], loss=85.9821
	step [36/148], loss=95.3895
	step [37/148], loss=95.9453
	step [38/148], loss=73.7230
	step [39/148], loss=74.2716
	step [40/148], loss=68.8304
	step [41/148], loss=87.3643
	step [42/148], loss=74.7740
	step [43/148], loss=74.5919
	step [44/148], loss=81.6373
	step [45/148], loss=94.0470
	step [46/148], loss=95.1295
	step [47/148], loss=74.9140
	step [48/148], loss=84.6052
	step [49/148], loss=84.7622
	step [50/148], loss=87.8957
	step [51/148], loss=72.8717
	step [52/148], loss=83.9233
	step [53/148], loss=91.9259
	step [54/148], loss=80.9673
	step [55/148], loss=85.3121
	step [56/148], loss=92.0388
	step [57/148], loss=106.9948
	step [58/148], loss=77.1749
	step [59/148], loss=95.1518
	step [60/148], loss=86.1078
	step [61/148], loss=96.5383
	step [62/148], loss=85.7948
	step [63/148], loss=68.0876
	step [64/148], loss=93.3443
	step [65/148], loss=91.0138
	step [66/148], loss=88.8342
	step [67/148], loss=97.0402
	step [68/148], loss=81.6613
	step [69/148], loss=69.9213
	step [70/148], loss=95.3829
	step [71/148], loss=90.2088
	step [72/148], loss=83.4828
	step [73/148], loss=91.6566
	step [74/148], loss=80.1297
	step [75/148], loss=84.1351
	step [76/148], loss=82.7774
	step [77/148], loss=91.0152
	step [78/148], loss=117.3250
	step [79/148], loss=81.0607
	step [80/148], loss=100.4210
	step [81/148], loss=81.3722
	step [82/148], loss=72.5775
	step [83/148], loss=75.5227
	step [84/148], loss=92.5618
	step [85/148], loss=85.7306
	step [86/148], loss=87.6475
	step [87/148], loss=86.4663
	step [88/148], loss=85.0537
	step [89/148], loss=90.2083
	step [90/148], loss=97.4437
	step [91/148], loss=88.5524
	step [92/148], loss=82.6665
	step [93/148], loss=87.9029
	step [94/148], loss=85.4671
	step [95/148], loss=69.0261
	step [96/148], loss=99.4257
	step [97/148], loss=74.3472
	step [98/148], loss=71.9497
	step [99/148], loss=85.0014
	step [100/148], loss=87.6330
	step [101/148], loss=67.7225
	step [102/148], loss=80.6411
	step [103/148], loss=88.7336
	step [104/148], loss=78.9221
	step [105/148], loss=79.5559
	step [106/148], loss=83.3799
	step [107/148], loss=90.1043
	step [108/148], loss=84.1560
	step [109/148], loss=67.3647
	step [110/148], loss=77.6968
	step [111/148], loss=87.5542
	step [112/148], loss=97.8988
	step [113/148], loss=89.6016
	step [114/148], loss=84.2830
	step [115/148], loss=75.8581
	step [116/148], loss=89.6976
	step [117/148], loss=91.9515
	step [118/148], loss=75.7650
	step [119/148], loss=94.6946
	step [120/148], loss=80.0090
	step [121/148], loss=74.2416
	step [122/148], loss=69.2217
	step [123/148], loss=64.7073
	step [124/148], loss=80.0300
	step [125/148], loss=84.9835
	step [126/148], loss=87.3621
	step [127/148], loss=81.0386
	step [128/148], loss=79.8795
	step [129/148], loss=93.9498
	step [130/148], loss=76.6578
	step [131/148], loss=69.4632
	step [132/148], loss=91.6877
	step [133/148], loss=87.4185
	step [134/148], loss=80.0181
	step [135/148], loss=91.6979
	step [136/148], loss=98.3160
	step [137/148], loss=97.7075
	step [138/148], loss=90.0914
	step [139/148], loss=106.7319
	step [140/148], loss=91.8275
	step [141/148], loss=78.7038
	step [142/148], loss=85.7782
	step [143/148], loss=78.7515
	step [144/148], loss=85.9196
	step [145/148], loss=76.8050
	step [146/148], loss=94.0471
	step [147/148], loss=87.2201
	step [148/148], loss=7.2720
	Evaluating
	loss=0.0128, precision=0.2926, recall=0.8517, f1=0.4355
Training epoch 81
	step [1/148], loss=81.4931
	step [2/148], loss=89.3601
	step [3/148], loss=85.7461
	step [4/148], loss=83.8916
	step [5/148], loss=84.9077
	step [6/148], loss=91.3770
	step [7/148], loss=91.6849
	step [8/148], loss=106.8851
	step [9/148], loss=92.2773
	step [10/148], loss=71.9302
	step [11/148], loss=91.5650
	step [12/148], loss=86.0431
	step [13/148], loss=96.0298
	step [14/148], loss=86.0320
	step [15/148], loss=88.8100
	step [16/148], loss=84.3125
	step [17/148], loss=72.7845
	step [18/148], loss=94.4923
	step [19/148], loss=83.2385
	step [20/148], loss=89.8292
	step [21/148], loss=78.3359
	step [22/148], loss=88.7631
	step [23/148], loss=83.8404
	step [24/148], loss=81.2185
	step [25/148], loss=97.9669
	step [26/148], loss=84.8538
	step [27/148], loss=77.2395
	step [28/148], loss=69.8434
	step [29/148], loss=78.5095
	step [30/148], loss=83.1125
	step [31/148], loss=77.1972
	step [32/148], loss=93.0841
	step [33/148], loss=80.5482
	step [34/148], loss=83.6393
	step [35/148], loss=91.8154
	step [36/148], loss=72.9912
	step [37/148], loss=84.5180
	step [38/148], loss=77.6060
	step [39/148], loss=85.1678
	step [40/148], loss=82.7203
	step [41/148], loss=81.7127
	step [42/148], loss=85.5702
	step [43/148], loss=96.7037
	step [44/148], loss=97.0486
	step [45/148], loss=98.5862
	step [46/148], loss=71.3401
	step [47/148], loss=83.9135
	step [48/148], loss=84.5832
	step [49/148], loss=78.2530
	step [50/148], loss=80.1497
	step [51/148], loss=87.8192
	step [52/148], loss=79.7574
	step [53/148], loss=77.6891
	step [54/148], loss=74.3956
	step [55/148], loss=78.2959
	step [56/148], loss=76.5236
	step [57/148], loss=99.7430
	step [58/148], loss=86.9851
	step [59/148], loss=86.9974
	step [60/148], loss=90.8848
	step [61/148], loss=87.7670
	step [62/148], loss=90.6548
	step [63/148], loss=76.4649
	step [64/148], loss=85.4794
	step [65/148], loss=79.7494
	step [66/148], loss=78.5617
	step [67/148], loss=100.9828
	step [68/148], loss=80.8979
	step [69/148], loss=82.8614
	step [70/148], loss=91.1373
	step [71/148], loss=87.6548
	step [72/148], loss=88.0313
	step [73/148], loss=83.0672
	step [74/148], loss=86.1908
	step [75/148], loss=77.0194
	step [76/148], loss=87.7013
	step [77/148], loss=80.5483
	step [78/148], loss=83.2098
	step [79/148], loss=79.2889
	step [80/148], loss=100.3963
	step [81/148], loss=88.5648
	step [82/148], loss=79.1487
	step [83/148], loss=85.6236
	step [84/148], loss=90.9537
	step [85/148], loss=84.3055
	step [86/148], loss=85.8655
	step [87/148], loss=68.4517
	step [88/148], loss=91.1832
	step [89/148], loss=92.3001
	step [90/148], loss=99.1329
	step [91/148], loss=78.3226
	step [92/148], loss=90.4206
	step [93/148], loss=84.8461
	step [94/148], loss=80.3704
	step [95/148], loss=85.5812
	step [96/148], loss=80.5874
	step [97/148], loss=84.0718
	step [98/148], loss=84.7632
	step [99/148], loss=87.7659
	step [100/148], loss=91.0392
	step [101/148], loss=73.3682
	step [102/148], loss=84.8785
	step [103/148], loss=90.5832
	step [104/148], loss=89.9609
	step [105/148], loss=77.1910
	step [106/148], loss=92.5881
	step [107/148], loss=82.8480
	step [108/148], loss=80.5947
	step [109/148], loss=93.5078
	step [110/148], loss=80.8601
	step [111/148], loss=92.8241
	step [112/148], loss=74.8608
	step [113/148], loss=84.2783
	step [114/148], loss=92.1728
	step [115/148], loss=95.4846
	step [116/148], loss=88.4205
	step [117/148], loss=92.5701
	step [118/148], loss=85.7766
	step [119/148], loss=78.0690
	step [120/148], loss=87.7280
	step [121/148], loss=84.0650
	step [122/148], loss=91.8645
	step [123/148], loss=69.7366
	step [124/148], loss=81.7497
	step [125/148], loss=81.7132
	step [126/148], loss=76.8788
	step [127/148], loss=94.1125
	step [128/148], loss=83.0211
	step [129/148], loss=87.0494
	step [130/148], loss=99.1416
	step [131/148], loss=93.1140
	step [132/148], loss=92.0580
	step [133/148], loss=90.8570
	step [134/148], loss=76.6183
	step [135/148], loss=89.3806
	step [136/148], loss=84.3161
	step [137/148], loss=73.0880
	step [138/148], loss=81.7740
	step [139/148], loss=67.9110
	step [140/148], loss=96.6417
	step [141/148], loss=81.0360
	step [142/148], loss=92.3704
	step [143/148], loss=91.6622
	step [144/148], loss=98.8746
	step [145/148], loss=90.0077
	step [146/148], loss=83.1494
	step [147/148], loss=87.2455
	step [148/148], loss=8.6927
	Evaluating
	loss=0.0137, precision=0.2451, recall=0.8494, f1=0.3804
Training epoch 82
	step [1/148], loss=93.9463
	step [2/148], loss=72.5365
	step [3/148], loss=87.1263
	step [4/148], loss=75.6510
	step [5/148], loss=82.2641
	step [6/148], loss=70.9521
	step [7/148], loss=73.8098
	step [8/148], loss=99.3814
	step [9/148], loss=85.6316
	step [10/148], loss=82.2855
	step [11/148], loss=84.9887
	step [12/148], loss=81.0868
	step [13/148], loss=80.8737
	step [14/148], loss=88.9449
	step [15/148], loss=84.8093
	step [16/148], loss=99.7767
	step [17/148], loss=82.4370
	step [18/148], loss=92.9236
	step [19/148], loss=87.0654
	step [20/148], loss=91.2997
	step [21/148], loss=81.1788
	step [22/148], loss=82.0984
	step [23/148], loss=87.6616
	step [24/148], loss=93.6874
	step [25/148], loss=74.8593
	step [26/148], loss=76.0030
	step [27/148], loss=79.1845
	step [28/148], loss=84.3497
	step [29/148], loss=78.0456
	step [30/148], loss=84.1467
	step [31/148], loss=87.5581
	step [32/148], loss=74.6946
	step [33/148], loss=74.8518
	step [34/148], loss=74.0521
	step [35/148], loss=90.8130
	step [36/148], loss=94.2101
	step [37/148], loss=92.7645
	step [38/148], loss=93.3565
	step [39/148], loss=93.5500
	step [40/148], loss=90.4856
	step [41/148], loss=80.7534
	step [42/148], loss=93.9914
	step [43/148], loss=83.5737
	step [44/148], loss=93.5491
	step [45/148], loss=93.7120
	step [46/148], loss=88.0628
	step [47/148], loss=80.0521
	step [48/148], loss=73.3619
	step [49/148], loss=104.6684
	step [50/148], loss=82.2395
	step [51/148], loss=83.0184
	step [52/148], loss=77.3604
	step [53/148], loss=91.7626
	step [54/148], loss=90.5307
	step [55/148], loss=99.5015
	step [56/148], loss=81.7878
	step [57/148], loss=95.7495
	step [58/148], loss=91.6246
	step [59/148], loss=87.8687
	step [60/148], loss=106.0529
	step [61/148], loss=88.7736
	step [62/148], loss=87.4922
	step [63/148], loss=71.9014
	step [64/148], loss=86.3289
	step [65/148], loss=77.3092
	step [66/148], loss=80.4761
	step [67/148], loss=79.4578
	step [68/148], loss=101.4437
	step [69/148], loss=85.2532
	step [70/148], loss=66.0807
	step [71/148], loss=89.7416
	step [72/148], loss=72.5442
	step [73/148], loss=80.6655
	step [74/148], loss=84.0805
	step [75/148], loss=86.7547
	step [76/148], loss=77.0225
	step [77/148], loss=89.2226
	step [78/148], loss=92.1752
	step [79/148], loss=85.9353
	step [80/148], loss=94.6036
	step [81/148], loss=76.8281
	step [82/148], loss=67.7539
	step [83/148], loss=81.5990
	step [84/148], loss=84.3754
	step [85/148], loss=75.2254
	step [86/148], loss=78.8703
	step [87/148], loss=74.2580
	step [88/148], loss=92.6466
	step [89/148], loss=84.7750
	step [90/148], loss=73.9460
	step [91/148], loss=98.8947
	step [92/148], loss=80.4353
	step [93/148], loss=82.8525
	step [94/148], loss=90.2056
	step [95/148], loss=89.6871
	step [96/148], loss=76.6939
	step [97/148], loss=81.8347
	step [98/148], loss=72.6912
	step [99/148], loss=91.1087
	step [100/148], loss=83.7592
	step [101/148], loss=76.1363
	step [102/148], loss=95.6885
	step [103/148], loss=79.1941
	step [104/148], loss=76.3988
	step [105/148], loss=97.9784
	step [106/148], loss=77.9559
	step [107/148], loss=75.2723
	step [108/148], loss=73.9481
	step [109/148], loss=79.8102
	step [110/148], loss=85.8271
	step [111/148], loss=82.5622
	step [112/148], loss=82.2963
	step [113/148], loss=83.1460
	step [114/148], loss=100.7513
	step [115/148], loss=80.7658
	step [116/148], loss=78.2739
	step [117/148], loss=77.1532
	step [118/148], loss=86.6613
	step [119/148], loss=91.7303
	step [120/148], loss=95.9148
	step [121/148], loss=84.4156
	step [122/148], loss=86.5379
	step [123/148], loss=91.2715
	step [124/148], loss=76.6743
	step [125/148], loss=75.0628
	step [126/148], loss=89.0549
	step [127/148], loss=73.8436
	step [128/148], loss=68.8959
	step [129/148], loss=81.5007
	step [130/148], loss=98.9246
	step [131/148], loss=89.2886
	step [132/148], loss=62.2634
	step [133/148], loss=77.4834
	step [134/148], loss=97.2341
	step [135/148], loss=89.9452
	step [136/148], loss=97.5399
	step [137/148], loss=99.1219
	step [138/148], loss=93.1857
	step [139/148], loss=79.1956
	step [140/148], loss=97.3202
	step [141/148], loss=78.7601
	step [142/148], loss=95.9647
	step [143/148], loss=90.3549
	step [144/148], loss=93.1682
	step [145/148], loss=79.7859
	step [146/148], loss=87.0890
	step [147/148], loss=79.6291
	step [148/148], loss=7.5361
	Evaluating
	loss=0.0149, precision=0.2457, recall=0.8502, f1=0.3813
Training epoch 83
	step [1/148], loss=57.0683
	step [2/148], loss=80.2352
	step [3/148], loss=73.3515
	step [4/148], loss=101.9305
	step [5/148], loss=82.8538
	step [6/148], loss=71.4646
	step [7/148], loss=86.3326
	step [8/148], loss=79.9248
	step [9/148], loss=79.3572
	step [10/148], loss=70.1177
	step [11/148], loss=77.1592
	step [12/148], loss=90.0514
	step [13/148], loss=87.8316
	step [14/148], loss=80.1523
	step [15/148], loss=81.5771
	step [16/148], loss=90.1900
	step [17/148], loss=70.4310
	step [18/148], loss=86.4603
	step [19/148], loss=84.4723
	step [20/148], loss=76.4862
	step [21/148], loss=72.2952
	step [22/148], loss=87.2332
	step [23/148], loss=94.5462
	step [24/148], loss=87.0801
	step [25/148], loss=84.7473
	step [26/148], loss=91.7643
	step [27/148], loss=72.8324
	step [28/148], loss=83.0828
	step [29/148], loss=105.4771
	step [30/148], loss=73.8606
	step [31/148], loss=80.0638
	step [32/148], loss=82.5602
	step [33/148], loss=82.7035
	step [34/148], loss=90.5259
	step [35/148], loss=82.4696
	step [36/148], loss=89.2389
	step [37/148], loss=94.8087
	step [38/148], loss=79.3872
	step [39/148], loss=85.6412
	step [40/148], loss=87.1404
	step [41/148], loss=92.6019
	step [42/148], loss=70.0940
	step [43/148], loss=82.6557
	step [44/148], loss=78.0051
	step [45/148], loss=78.8735
	step [46/148], loss=81.6800
	step [47/148], loss=98.7894
	step [48/148], loss=86.0372
	step [49/148], loss=87.3287
	step [50/148], loss=74.9010
	step [51/148], loss=87.1316
	step [52/148], loss=96.4015
	step [53/148], loss=101.9253
	step [54/148], loss=74.8084
	step [55/148], loss=86.6784
	step [56/148], loss=85.1255
	step [57/148], loss=79.7109
	step [58/148], loss=76.3315
	step [59/148], loss=80.8089
	step [60/148], loss=92.3157
	step [61/148], loss=71.3299
	step [62/148], loss=88.2860
	step [63/148], loss=62.2742
	step [64/148], loss=85.3173
	step [65/148], loss=84.3621
	step [66/148], loss=80.8395
	step [67/148], loss=78.1219
	step [68/148], loss=79.9035
	step [69/148], loss=69.9814
	step [70/148], loss=97.1801
	step [71/148], loss=80.0434
	step [72/148], loss=92.4908
	step [73/148], loss=68.3150
	step [74/148], loss=88.6677
	step [75/148], loss=79.6554
	step [76/148], loss=97.4742
	step [77/148], loss=95.2742
	step [78/148], loss=87.3438
	step [79/148], loss=77.9493
	step [80/148], loss=87.9310
	step [81/148], loss=99.4473
	step [82/148], loss=96.6851
	step [83/148], loss=92.9184
	step [84/148], loss=81.5103
	step [85/148], loss=77.3061
	step [86/148], loss=88.0561
	step [87/148], loss=90.1581
	step [88/148], loss=87.0297
	step [89/148], loss=88.8961
	step [90/148], loss=76.6714
	step [91/148], loss=81.7824
	step [92/148], loss=93.6320
	step [93/148], loss=80.0236
	step [94/148], loss=95.3887
	step [95/148], loss=98.9363
	step [96/148], loss=88.1884
	step [97/148], loss=94.6705
	step [98/148], loss=81.7718
	step [99/148], loss=80.7570
	step [100/148], loss=78.5464
	step [101/148], loss=85.6797
	step [102/148], loss=87.4227
	step [103/148], loss=75.4384
	step [104/148], loss=84.6599
	step [105/148], loss=92.6529
	step [106/148], loss=87.9560
	step [107/148], loss=83.4801
	step [108/148], loss=92.0664
	step [109/148], loss=81.9694
	step [110/148], loss=90.4835
	step [111/148], loss=90.7171
	step [112/148], loss=88.5454
	step [113/148], loss=85.4371
	step [114/148], loss=77.7359
	step [115/148], loss=87.4449
	step [116/148], loss=92.0670
	step [117/148], loss=71.3467
	step [118/148], loss=85.4820
	step [119/148], loss=107.1780
	step [120/148], loss=79.6103
	step [121/148], loss=85.2722
	step [122/148], loss=71.4653
	step [123/148], loss=95.0771
	step [124/148], loss=90.2621
	step [125/148], loss=70.7365
	step [126/148], loss=92.5383
	step [127/148], loss=95.2019
	step [128/148], loss=87.4192
	step [129/148], loss=85.4499
	step [130/148], loss=86.2278
	step [131/148], loss=88.3231
	step [132/148], loss=80.5625
	step [133/148], loss=84.1123
	step [134/148], loss=83.8507
	step [135/148], loss=74.1562
	step [136/148], loss=83.5184
	step [137/148], loss=78.9170
	step [138/148], loss=79.5123
	step [139/148], loss=98.5319
	step [140/148], loss=97.6661
	step [141/148], loss=97.4466
	step [142/148], loss=72.2950
	step [143/148], loss=85.3660
	step [144/148], loss=71.2815
	step [145/148], loss=95.7182
	step [146/148], loss=79.8039
	step [147/148], loss=86.6475
	step [148/148], loss=6.4419
	Evaluating
	loss=0.0119, precision=0.3204, recall=0.8587, f1=0.4666
saving model as: 0_saved_model.pth
Training epoch 84
	step [1/148], loss=83.3030
	step [2/148], loss=102.2883
	step [3/148], loss=90.2041
	step [4/148], loss=61.3410
	step [5/148], loss=80.0365
	step [6/148], loss=77.1050
	step [7/148], loss=77.0412
	step [8/148], loss=85.3471
	step [9/148], loss=79.4814
	step [10/148], loss=89.6407
	step [11/148], loss=97.7491
	step [12/148], loss=92.0163
	step [13/148], loss=77.3804
	step [14/148], loss=90.6336
	step [15/148], loss=89.0489
	step [16/148], loss=86.7903
	step [17/148], loss=67.0012
	step [18/148], loss=103.8112
	step [19/148], loss=83.8158
	step [20/148], loss=88.5743
	step [21/148], loss=83.7558
	step [22/148], loss=94.1256
	step [23/148], loss=98.2891
	step [24/148], loss=77.4687
	step [25/148], loss=88.2856
	step [26/148], loss=91.3266
	step [27/148], loss=86.4409
	step [28/148], loss=98.1331
	step [29/148], loss=81.4896
	step [30/148], loss=78.7566
	step [31/148], loss=89.2583
	step [32/148], loss=81.6882
	step [33/148], loss=93.5024
	step [34/148], loss=73.3771
	step [35/148], loss=80.5002
	step [36/148], loss=76.3902
	step [37/148], loss=84.9909
	step [38/148], loss=78.5817
	step [39/148], loss=76.6306
	step [40/148], loss=79.2078
	step [41/148], loss=91.3016
	step [42/148], loss=93.7164
	step [43/148], loss=85.8079
	step [44/148], loss=86.4694
	step [45/148], loss=79.4051
	step [46/148], loss=84.5431
	step [47/148], loss=86.8394
	step [48/148], loss=85.4477
	step [49/148], loss=75.3701
	step [50/148], loss=63.3668
	step [51/148], loss=96.3506
	step [52/148], loss=99.8854
	step [53/148], loss=73.1546
	step [54/148], loss=87.4323
	step [55/148], loss=81.5233
	step [56/148], loss=77.1590
	step [57/148], loss=73.2716
	step [58/148], loss=98.9363
	step [59/148], loss=84.0582
	step [60/148], loss=78.9212
	step [61/148], loss=88.3708
	step [62/148], loss=89.4894
	step [63/148], loss=79.3400
	step [64/148], loss=76.6745
	step [65/148], loss=69.7227
	step [66/148], loss=75.5004
	step [67/148], loss=85.2978
	step [68/148], loss=83.0287
	step [69/148], loss=105.6267
	step [70/148], loss=93.2614
	step [71/148], loss=87.5943
	step [72/148], loss=82.6273
	step [73/148], loss=77.8095
	step [74/148], loss=76.2602
	step [75/148], loss=78.6086
	step [76/148], loss=83.3449
	step [77/148], loss=84.7752
	step [78/148], loss=93.1469
	step [79/148], loss=70.5287
	step [80/148], loss=69.3604
	step [81/148], loss=94.5176
	step [82/148], loss=92.8177
	step [83/148], loss=78.2041
	step [84/148], loss=73.6050
	step [85/148], loss=85.3987
	step [86/148], loss=102.4295
	step [87/148], loss=82.6544
	step [88/148], loss=86.9658
	step [89/148], loss=90.3378
	step [90/148], loss=80.7027
	step [91/148], loss=72.6926
	step [92/148], loss=91.2751
	step [93/148], loss=89.5239
	step [94/148], loss=79.1979
	step [95/148], loss=99.1062
	step [96/148], loss=81.0713
	step [97/148], loss=88.8266
	step [98/148], loss=88.0050
	step [99/148], loss=78.9489
	step [100/148], loss=104.7666
	step [101/148], loss=83.0976
	step [102/148], loss=93.6629
	step [103/148], loss=71.0464
	step [104/148], loss=94.5881
	step [105/148], loss=77.2508
	step [106/148], loss=87.5568
	step [107/148], loss=76.2768
	step [108/148], loss=73.4875
	step [109/148], loss=83.2737
	step [110/148], loss=78.5804
	step [111/148], loss=72.0823
	step [112/148], loss=82.3527
	step [113/148], loss=84.7005
	step [114/148], loss=95.6297
	step [115/148], loss=85.8344
	step [116/148], loss=72.9775
	step [117/148], loss=67.9935
	step [118/148], loss=98.2394
	step [119/148], loss=76.6283
	step [120/148], loss=81.5207
	step [121/148], loss=77.6853
	step [122/148], loss=95.9619
	step [123/148], loss=81.3964
	step [124/148], loss=77.0954
	step [125/148], loss=83.8800
	step [126/148], loss=65.1520
	step [127/148], loss=90.5122
	step [128/148], loss=98.9802
	step [129/148], loss=97.7697
	step [130/148], loss=83.9329
	step [131/148], loss=81.4807
	step [132/148], loss=84.5548
	step [133/148], loss=66.5392
	step [134/148], loss=87.2599
	step [135/148], loss=85.2973
	step [136/148], loss=93.4947
	step [137/148], loss=79.1608
	step [138/148], loss=85.7250
	step [139/148], loss=77.7943
	step [140/148], loss=81.7620
	step [141/148], loss=82.2767
	step [142/148], loss=94.6357
	step [143/148], loss=72.9764
	step [144/148], loss=77.2581
	step [145/148], loss=102.8609
	step [146/148], loss=84.0150
	step [147/148], loss=83.8109
	step [148/148], loss=8.2999
	Evaluating
	loss=0.0130, precision=0.2944, recall=0.8585, f1=0.4384
Training epoch 85
	step [1/148], loss=73.1720
	step [2/148], loss=68.4220
	step [3/148], loss=85.4712
	step [4/148], loss=78.4904
	step [5/148], loss=69.4524
	step [6/148], loss=80.1861
	step [7/148], loss=89.2073
	step [8/148], loss=81.1477
	step [9/148], loss=84.6957
	step [10/148], loss=85.1969
	step [11/148], loss=90.0038
	step [12/148], loss=70.8337
	step [13/148], loss=87.7417
	step [14/148], loss=91.7956
	step [15/148], loss=70.9294
	step [16/148], loss=79.2601
	step [17/148], loss=90.5836
	step [18/148], loss=85.7626
	step [19/148], loss=85.9424
	step [20/148], loss=90.8395
	step [21/148], loss=67.1075
	step [22/148], loss=72.0690
	step [23/148], loss=79.9442
	step [24/148], loss=80.0129
	step [25/148], loss=83.1992
	step [26/148], loss=73.8884
	step [27/148], loss=77.6785
	step [28/148], loss=82.8285
	step [29/148], loss=88.0386
	step [30/148], loss=90.6976
	step [31/148], loss=78.9405
	step [32/148], loss=90.0885
	step [33/148], loss=84.8782
	step [34/148], loss=73.8913
	step [35/148], loss=81.9038
	step [36/148], loss=74.5817
	step [37/148], loss=72.6298
	step [38/148], loss=100.3122
	step [39/148], loss=84.7978
	step [40/148], loss=78.6477
	step [41/148], loss=90.2349
	step [42/148], loss=90.6986
	step [43/148], loss=78.1405
	step [44/148], loss=81.5691
	step [45/148], loss=90.2290
	step [46/148], loss=78.3463
	step [47/148], loss=63.4189
	step [48/148], loss=71.7371
	step [49/148], loss=74.7999
	step [50/148], loss=62.4123
	step [51/148], loss=67.0060
	step [52/148], loss=96.3654
	step [53/148], loss=78.4521
	step [54/148], loss=81.9991
	step [55/148], loss=90.8470
	step [56/148], loss=74.4591
	step [57/148], loss=86.1062
	step [58/148], loss=82.2518
	step [59/148], loss=82.8003
	step [60/148], loss=92.1043
	step [61/148], loss=83.4841
	step [62/148], loss=78.1738
	step [63/148], loss=71.3378
	step [64/148], loss=77.8483
	step [65/148], loss=93.3578
	step [66/148], loss=87.9337
	step [67/148], loss=68.6635
	step [68/148], loss=105.7649
	step [69/148], loss=88.2557
	step [70/148], loss=87.2270
	step [71/148], loss=85.1740
	step [72/148], loss=102.4048
	step [73/148], loss=94.6301
	step [74/148], loss=81.8968
	step [75/148], loss=79.9085
	step [76/148], loss=87.1138
	step [77/148], loss=87.8895
	step [78/148], loss=78.1520
	step [79/148], loss=92.4500
	step [80/148], loss=78.2997
	step [81/148], loss=95.6676
	step [82/148], loss=81.0967
	step [83/148], loss=100.0323
	step [84/148], loss=73.9800
	step [85/148], loss=93.5722
	step [86/148], loss=91.1465
	step [87/148], loss=89.0690
	step [88/148], loss=73.3495
	step [89/148], loss=112.0127
	step [90/148], loss=84.1168
	step [91/148], loss=88.3495
	step [92/148], loss=84.3307
	step [93/148], loss=80.8916
	step [94/148], loss=86.9958
	step [95/148], loss=68.6633
	step [96/148], loss=101.3888
	step [97/148], loss=86.2841
	step [98/148], loss=79.6095
	step [99/148], loss=76.9852
	step [100/148], loss=87.9509
	step [101/148], loss=87.4331
	step [102/148], loss=84.1530
	step [103/148], loss=84.2503
	step [104/148], loss=73.4413
	step [105/148], loss=80.2770
	step [106/148], loss=95.4472
	step [107/148], loss=78.6903
	step [108/148], loss=84.6315
	step [109/148], loss=85.8741
	step [110/148], loss=75.3932
	step [111/148], loss=77.0256
	step [112/148], loss=76.0094
	step [113/148], loss=88.9402
	step [114/148], loss=76.0620
	step [115/148], loss=91.2325
	step [116/148], loss=100.4645
	step [117/148], loss=108.9363
	step [118/148], loss=71.3567
	step [119/148], loss=79.2135
	step [120/148], loss=86.5378
	step [121/148], loss=87.0787
	step [122/148], loss=81.0501
	step [123/148], loss=100.2216
	step [124/148], loss=97.4316
	step [125/148], loss=71.0399
	step [126/148], loss=74.0305
	step [127/148], loss=88.7408
	step [128/148], loss=85.6640
	step [129/148], loss=87.4745
	step [130/148], loss=102.0943
	step [131/148], loss=72.7865
	step [132/148], loss=86.6207
	step [133/148], loss=99.6531
	step [134/148], loss=74.2588
	step [135/148], loss=94.6002
	step [136/148], loss=86.1522
	step [137/148], loss=90.3038
	step [138/148], loss=83.6072
	step [139/148], loss=95.2522
	step [140/148], loss=107.4103
	step [141/148], loss=83.8100
	step [142/148], loss=88.4815
	step [143/148], loss=78.8489
	step [144/148], loss=101.4484
	step [145/148], loss=73.0276
	step [146/148], loss=78.7619
	step [147/148], loss=76.7582
	step [148/148], loss=7.3802
	Evaluating
	loss=0.0128, precision=0.2963, recall=0.8526, f1=0.4397
Training epoch 86
	step [1/148], loss=74.0567
	step [2/148], loss=94.0774
	step [3/148], loss=85.4204
	step [4/148], loss=77.1154
	step [5/148], loss=104.1118
	step [6/148], loss=75.7121
	step [7/148], loss=84.4538
	step [8/148], loss=67.6100
	step [9/148], loss=83.2468
	step [10/148], loss=89.4369
	step [11/148], loss=99.1371
	step [12/148], loss=101.9965
	step [13/148], loss=91.9140
	step [14/148], loss=90.5743
	step [15/148], loss=82.0950
	step [16/148], loss=91.4754
	step [17/148], loss=97.3274
	step [18/148], loss=81.1533
	step [19/148], loss=96.0183
	step [20/148], loss=79.0285
	step [21/148], loss=93.8680
	step [22/148], loss=69.0645
	step [23/148], loss=84.7453
	step [24/148], loss=97.7354
	step [25/148], loss=83.7140
	step [26/148], loss=85.3569
	step [27/148], loss=86.2563
	step [28/148], loss=96.1710
	step [29/148], loss=96.4388
	step [30/148], loss=88.5570
	step [31/148], loss=91.1482
	step [32/148], loss=89.9110
	step [33/148], loss=76.0220
	step [34/148], loss=93.1801
	step [35/148], loss=74.2235
	step [36/148], loss=77.3397
	step [37/148], loss=104.5024
	step [38/148], loss=77.9643
	step [39/148], loss=93.9855
	step [40/148], loss=66.5317
	step [41/148], loss=93.7267
	step [42/148], loss=96.8533
	step [43/148], loss=94.2268
	step [44/148], loss=86.0619
	step [45/148], loss=87.3354
	step [46/148], loss=90.1629
	step [47/148], loss=67.8421
	step [48/148], loss=83.5613
	step [49/148], loss=78.3239
	step [50/148], loss=81.8518
	step [51/148], loss=85.9031
	step [52/148], loss=92.8755
	step [53/148], loss=93.9036
	step [54/148], loss=75.4947
	step [55/148], loss=64.7449
	step [56/148], loss=98.8732
	step [57/148], loss=75.7445
	step [58/148], loss=95.2223
	step [59/148], loss=83.8429
	step [60/148], loss=96.5249
	step [61/148], loss=76.7077
	step [62/148], loss=85.1148
	step [63/148], loss=100.5689
	step [64/148], loss=69.5693
	step [65/148], loss=66.5332
	step [66/148], loss=84.1168
	step [67/148], loss=89.5430
	step [68/148], loss=81.7776
	step [69/148], loss=69.4222
	step [70/148], loss=74.9260
	step [71/148], loss=76.1977
	step [72/148], loss=71.8792
	step [73/148], loss=90.9254
	step [74/148], loss=83.7721
	step [75/148], loss=87.9094
	step [76/148], loss=77.4146
	step [77/148], loss=85.0737
	step [78/148], loss=87.3887
	step [79/148], loss=78.0114
	step [80/148], loss=94.8442
	step [81/148], loss=82.9410
	step [82/148], loss=105.9492
	step [83/148], loss=67.8438
	step [84/148], loss=78.6115
	step [85/148], loss=72.5704
	step [86/148], loss=82.8788
	step [87/148], loss=79.0405
	step [88/148], loss=88.3482
	step [89/148], loss=78.3957
	step [90/148], loss=78.9610
	step [91/148], loss=85.6987
	step [92/148], loss=94.2842
	step [93/148], loss=80.3046
	step [94/148], loss=80.7753
	step [95/148], loss=84.9414
	step [96/148], loss=77.3179
	step [97/148], loss=64.4125
	step [98/148], loss=72.1539
	step [99/148], loss=88.9254
	step [100/148], loss=86.9915
	step [101/148], loss=79.8609
	step [102/148], loss=84.1099
	step [103/148], loss=103.6235
	step [104/148], loss=76.6443
	step [105/148], loss=83.2681
	step [106/148], loss=75.8574
	step [107/148], loss=85.2288
	step [108/148], loss=85.4029
	step [109/148], loss=85.7372
	step [110/148], loss=91.5017
	step [111/148], loss=87.7971
	step [112/148], loss=82.0904
	step [113/148], loss=82.4271
	step [114/148], loss=76.4051
	step [115/148], loss=78.8500
	step [116/148], loss=77.3655
	step [117/148], loss=75.1523
	step [118/148], loss=104.2287
	step [119/148], loss=83.1709
	step [120/148], loss=92.4977
	step [121/148], loss=95.2431
	step [122/148], loss=83.0715
	step [123/148], loss=89.1123
	step [124/148], loss=87.8463
	step [125/148], loss=91.1515
	step [126/148], loss=84.2234
	step [127/148], loss=73.4803
	step [128/148], loss=79.0640
	step [129/148], loss=83.7747
	step [130/148], loss=73.3243
	step [131/148], loss=71.6609
	step [132/148], loss=65.1238
	step [133/148], loss=85.2826
	step [134/148], loss=83.6122
	step [135/148], loss=72.9904
	step [136/148], loss=86.2259
	step [137/148], loss=75.9417
	step [138/148], loss=97.6170
	step [139/148], loss=78.4972
	step [140/148], loss=77.4875
	step [141/148], loss=78.6552
	step [142/148], loss=65.2527
	step [143/148], loss=77.6974
	step [144/148], loss=88.1608
	step [145/148], loss=63.0360
	step [146/148], loss=87.1205
	step [147/148], loss=82.8340
	step [148/148], loss=6.9073
	Evaluating
	loss=0.0110, precision=0.3285, recall=0.8584, f1=0.4752
saving model as: 0_saved_model.pth
Training epoch 87
	step [1/148], loss=81.5857
	step [2/148], loss=76.8930
	step [3/148], loss=83.4645
	step [4/148], loss=66.0257
	step [5/148], loss=78.8228
	step [6/148], loss=80.6314
	step [7/148], loss=84.5661
	step [8/148], loss=99.5947
	step [9/148], loss=89.8696
	step [10/148], loss=69.2819
	step [11/148], loss=79.2303
	step [12/148], loss=79.9645
	step [13/148], loss=90.8604
	step [14/148], loss=76.6171
	step [15/148], loss=96.3002
	step [16/148], loss=89.8314
	step [17/148], loss=102.2828
	step [18/148], loss=89.9544
	step [19/148], loss=73.2717
	step [20/148], loss=76.3930
	step [21/148], loss=72.2563
	step [22/148], loss=93.6031
	step [23/148], loss=84.7310
	step [24/148], loss=90.1329
	step [25/148], loss=84.4238
	step [26/148], loss=78.4990
	step [27/148], loss=71.7623
	step [28/148], loss=64.1685
	step [29/148], loss=83.7094
	step [30/148], loss=87.7395
	step [31/148], loss=81.0574
	step [32/148], loss=99.0654
	step [33/148], loss=71.3065
	step [34/148], loss=90.6777
	step [35/148], loss=75.9514
	step [36/148], loss=84.4754
	step [37/148], loss=90.2980
	step [38/148], loss=89.3603
	step [39/148], loss=92.4291
	step [40/148], loss=89.6222
	step [41/148], loss=82.4060
	step [42/148], loss=85.2885
	step [43/148], loss=78.3528
	step [44/148], loss=52.8744
	step [45/148], loss=85.0171
	step [46/148], loss=97.4260
	step [47/148], loss=76.9866
	step [48/148], loss=70.5999
	step [49/148], loss=87.0617
	step [50/148], loss=74.3928
	step [51/148], loss=73.0733
	step [52/148], loss=85.9926
	step [53/148], loss=97.4542
	step [54/148], loss=97.0783
	step [55/148], loss=82.3563
	step [56/148], loss=77.6007
	step [57/148], loss=73.6422
	step [58/148], loss=77.9866
	step [59/148], loss=84.6066
	step [60/148], loss=91.0589
	step [61/148], loss=72.6791
	step [62/148], loss=81.3088
	step [63/148], loss=79.7674
	step [64/148], loss=84.7821
	step [65/148], loss=105.2489
	step [66/148], loss=98.3691
	step [67/148], loss=88.0628
	step [68/148], loss=85.8021
	step [69/148], loss=100.3880
	step [70/148], loss=74.6226
	step [71/148], loss=85.5582
	step [72/148], loss=91.3589
	step [73/148], loss=64.3969
	step [74/148], loss=73.9759
	step [75/148], loss=89.5166
	step [76/148], loss=74.9818
	step [77/148], loss=73.5818
	step [78/148], loss=80.7223
	step [79/148], loss=75.1017
	step [80/148], loss=93.5453
	step [81/148], loss=76.4185
	step [82/148], loss=92.5463
	step [83/148], loss=74.9235
	step [84/148], loss=87.9777
	step [85/148], loss=80.0343
	step [86/148], loss=67.7874
	step [87/148], loss=92.3071
	step [88/148], loss=86.3488
	step [89/148], loss=81.5049
	step [90/148], loss=97.5201
	step [91/148], loss=83.6136
	step [92/148], loss=86.1960
	step [93/148], loss=76.3086
	step [94/148], loss=94.4243
	step [95/148], loss=76.6696
	step [96/148], loss=72.1925
	step [97/148], loss=89.5411
	step [98/148], loss=82.1355
	step [99/148], loss=78.1919
	step [100/148], loss=79.4541
	step [101/148], loss=83.0769
	step [102/148], loss=98.1237
	step [103/148], loss=77.7207
	step [104/148], loss=77.2376
	step [105/148], loss=97.4644
	step [106/148], loss=61.3314
	step [107/148], loss=90.1710
	step [108/148], loss=92.6789
	step [109/148], loss=83.6736
	step [110/148], loss=75.5336
	step [111/148], loss=82.2405
	step [112/148], loss=88.5377
	step [113/148], loss=91.0013
	step [114/148], loss=74.0776
	step [115/148], loss=80.6281
	step [116/148], loss=80.2684
	step [117/148], loss=82.3486
	step [118/148], loss=96.1145
	step [119/148], loss=84.1680
	step [120/148], loss=79.8140
	step [121/148], loss=81.7924
	step [122/148], loss=73.6918
	step [123/148], loss=89.9973
	step [124/148], loss=74.9082
	step [125/148], loss=94.7585
	step [126/148], loss=77.7271
	step [127/148], loss=86.8200
	step [128/148], loss=89.1036
	step [129/148], loss=85.1822
	step [130/148], loss=91.4074
	step [131/148], loss=97.5047
	step [132/148], loss=85.5703
	step [133/148], loss=86.7540
	step [134/148], loss=90.8944
	step [135/148], loss=95.2989
	step [136/148], loss=79.5787
	step [137/148], loss=80.7269
	step [138/148], loss=80.4531
	step [139/148], loss=90.2989
	step [140/148], loss=81.7873
	step [141/148], loss=82.4503
	step [142/148], loss=79.5842
	step [143/148], loss=86.1442
	step [144/148], loss=89.6289
	step [145/148], loss=81.3784
	step [146/148], loss=77.3162
	step [147/148], loss=74.7077
	step [148/148], loss=4.0033
	Evaluating
	loss=0.0110, precision=0.3411, recall=0.8473, f1=0.4863
saving model as: 0_saved_model.pth
Training epoch 88
	step [1/148], loss=78.2439
	step [2/148], loss=76.0374
	step [3/148], loss=98.1451
	step [4/148], loss=88.6760
	step [5/148], loss=83.7608
	step [6/148], loss=72.8548
	step [7/148], loss=89.8931
	step [8/148], loss=84.5187
	step [9/148], loss=83.7585
	step [10/148], loss=85.2221
	step [11/148], loss=82.4449
	step [12/148], loss=84.1918
	step [13/148], loss=84.4770
	step [14/148], loss=82.7260
	step [15/148], loss=76.9631
	step [16/148], loss=74.2097
	step [17/148], loss=80.3108
	step [18/148], loss=89.6822
	step [19/148], loss=84.0765
	step [20/148], loss=75.5461
	step [21/148], loss=81.4817
	step [22/148], loss=83.8404
	step [23/148], loss=63.8823
	step [24/148], loss=68.8559
	step [25/148], loss=77.7256
	step [26/148], loss=76.1546
	step [27/148], loss=98.7725
	step [28/148], loss=75.3506
	step [29/148], loss=89.2157
	step [30/148], loss=75.9863
	step [31/148], loss=80.4316
	step [32/148], loss=69.9530
	step [33/148], loss=89.1511
	step [34/148], loss=73.3707
	step [35/148], loss=91.8539
	step [36/148], loss=86.7550
	step [37/148], loss=81.8364
	step [38/148], loss=68.6425
	step [39/148], loss=92.9515
	step [40/148], loss=82.0618
	step [41/148], loss=94.3756
	step [42/148], loss=77.9862
	step [43/148], loss=75.0107
	step [44/148], loss=82.1656
	step [45/148], loss=75.0431
	step [46/148], loss=94.7489
	step [47/148], loss=79.8635
	step [48/148], loss=64.3699
	step [49/148], loss=93.0051
	step [50/148], loss=80.8720
	step [51/148], loss=87.5240
	step [52/148], loss=84.1941
	step [53/148], loss=83.9572
	step [54/148], loss=87.1927
	step [55/148], loss=81.7472
	step [56/148], loss=77.1161
	step [57/148], loss=90.7734
	step [58/148], loss=88.3707
	step [59/148], loss=75.6381
	step [60/148], loss=83.7361
	step [61/148], loss=86.5456
	step [62/148], loss=82.8955
	step [63/148], loss=90.0582
	step [64/148], loss=82.4129
	step [65/148], loss=77.6700
	step [66/148], loss=77.8589
	step [67/148], loss=82.9275
	step [68/148], loss=78.0031
	step [69/148], loss=84.5562
	step [70/148], loss=85.2773
	step [71/148], loss=78.6448
	step [72/148], loss=76.3709
	step [73/148], loss=76.2965
	step [74/148], loss=81.5908
	step [75/148], loss=94.8115
	step [76/148], loss=99.2382
	step [77/148], loss=87.1957
	step [78/148], loss=83.8693
	step [79/148], loss=77.0772
	step [80/148], loss=77.7973
	step [81/148], loss=86.3680
	step [82/148], loss=77.7216
	step [83/148], loss=91.4038
	step [84/148], loss=74.5935
	step [85/148], loss=78.4183
	step [86/148], loss=87.3067
	step [87/148], loss=83.4374
	step [88/148], loss=87.4222
	step [89/148], loss=78.1155
	step [90/148], loss=80.8461
	step [91/148], loss=91.6855
	step [92/148], loss=78.1044
	step [93/148], loss=84.1671
	step [94/148], loss=103.8037
	step [95/148], loss=69.5844
	step [96/148], loss=88.5005
	step [97/148], loss=75.9587
	step [98/148], loss=79.2442
	step [99/148], loss=77.1668
	step [100/148], loss=78.1598
	step [101/148], loss=94.3038
	step [102/148], loss=89.8099
	step [103/148], loss=86.9794
	step [104/148], loss=91.7879
	step [105/148], loss=72.3892
	step [106/148], loss=91.9244
	step [107/148], loss=85.0150
	step [108/148], loss=88.8702
	step [109/148], loss=75.5686
	step [110/148], loss=89.8680
	step [111/148], loss=87.5405
	step [112/148], loss=89.8401
	step [113/148], loss=81.7523
	step [114/148], loss=84.4086
	step [115/148], loss=84.4230
	step [116/148], loss=104.9872
	step [117/148], loss=104.7793
	step [118/148], loss=87.0887
	step [119/148], loss=84.2822
	step [120/148], loss=80.6411
	step [121/148], loss=84.3226
	step [122/148], loss=78.5363
	step [123/148], loss=102.2243
	step [124/148], loss=69.4412
	step [125/148], loss=77.2119
	step [126/148], loss=75.9128
	step [127/148], loss=92.8435
	step [128/148], loss=85.2734
	step [129/148], loss=88.6622
	step [130/148], loss=82.8142
	step [131/148], loss=81.7431
	step [132/148], loss=64.4701
	step [133/148], loss=76.5077
	step [134/148], loss=82.4690
	step [135/148], loss=83.1214
	step [136/148], loss=68.2531
	step [137/148], loss=94.5940
	step [138/148], loss=86.0746
	step [139/148], loss=81.0369
	step [140/148], loss=79.9114
	step [141/148], loss=91.1370
	step [142/148], loss=89.7987
	step [143/148], loss=92.2424
	step [144/148], loss=80.7002
	step [145/148], loss=89.8506
	step [146/148], loss=77.3746
	step [147/148], loss=93.0437
	step [148/148], loss=4.5691
	Evaluating
	loss=0.0106, precision=0.3490, recall=0.8441, f1=0.4939
saving model as: 0_saved_model.pth
Training epoch 89
	step [1/148], loss=92.9951
	step [2/148], loss=73.5344
	step [3/148], loss=82.2911
	step [4/148], loss=83.0472
	step [5/148], loss=80.8216
	step [6/148], loss=83.5289
	step [7/148], loss=85.5729
	step [8/148], loss=85.2778
	step [9/148], loss=86.2746
	step [10/148], loss=76.3810
	step [11/148], loss=78.4104
	step [12/148], loss=77.8331
	step [13/148], loss=79.8118
	step [14/148], loss=78.8118
	step [15/148], loss=81.4098
	step [16/148], loss=94.6325
	step [17/148], loss=96.8990
	step [18/148], loss=92.0101
	step [19/148], loss=80.8277
	step [20/148], loss=71.0998
	step [21/148], loss=76.9791
	step [22/148], loss=74.8727
	step [23/148], loss=94.6519
	step [24/148], loss=88.9875
	step [25/148], loss=92.0440
	step [26/148], loss=68.0774
	step [27/148], loss=79.7432
	step [28/148], loss=89.6640
	step [29/148], loss=73.4925
	step [30/148], loss=83.2263
	step [31/148], loss=74.4133
	step [32/148], loss=73.3984
	step [33/148], loss=79.1237
	step [34/148], loss=79.8165
	step [35/148], loss=85.9658
	step [36/148], loss=90.5585
	step [37/148], loss=83.1087
	step [38/148], loss=85.8342
	step [39/148], loss=81.0183
	step [40/148], loss=96.1012
	step [41/148], loss=87.4556
	step [42/148], loss=90.1071
	step [43/148], loss=77.8167
	step [44/148], loss=89.1296
	step [45/148], loss=78.1790
	step [46/148], loss=79.1808
	step [47/148], loss=75.5131
	step [48/148], loss=81.9921
	step [49/148], loss=96.2139
	step [50/148], loss=92.3522
	step [51/148], loss=78.1770
	step [52/148], loss=71.9008
	step [53/148], loss=66.3929
	step [54/148], loss=79.3001
	step [55/148], loss=86.0367
	step [56/148], loss=70.7193
	step [57/148], loss=89.4910
	step [58/148], loss=77.5648
	step [59/148], loss=76.8108
	step [60/148], loss=102.1677
	step [61/148], loss=87.8689
	step [62/148], loss=71.5115
	step [63/148], loss=78.9549
	step [64/148], loss=77.8480
	step [65/148], loss=86.8384
	step [66/148], loss=89.8439
	step [67/148], loss=92.0530
	step [68/148], loss=82.9247
	step [69/148], loss=84.3340
	step [70/148], loss=90.6945
	step [71/148], loss=96.1611
	step [72/148], loss=81.6246
	step [73/148], loss=97.6317
	step [74/148], loss=82.8349
	step [75/148], loss=68.4026
	step [76/148], loss=69.7811
	step [77/148], loss=85.9321
	step [78/148], loss=67.7180
	step [79/148], loss=94.3571
	step [80/148], loss=71.8414
	step [81/148], loss=71.8350
	step [82/148], loss=81.2936
	step [83/148], loss=77.4795
	step [84/148], loss=102.6633
	step [85/148], loss=86.9207
	step [86/148], loss=85.9309
	step [87/148], loss=68.9435
	step [88/148], loss=77.4395
	step [89/148], loss=87.2556
	step [90/148], loss=76.1179
	step [91/148], loss=68.0768
	step [92/148], loss=81.8017
	step [93/148], loss=100.4900
	step [94/148], loss=93.5193
	step [95/148], loss=72.2209
	step [96/148], loss=80.4779
	step [97/148], loss=80.6085
	step [98/148], loss=87.1469
	step [99/148], loss=92.5282
	step [100/148], loss=76.5176
	step [101/148], loss=81.8294
	step [102/148], loss=83.8885
	step [103/148], loss=90.5717
	step [104/148], loss=90.9662
	step [105/148], loss=97.8660
	step [106/148], loss=76.1341
	step [107/148], loss=86.1882
	step [108/148], loss=93.7654
	step [109/148], loss=72.2728
	step [110/148], loss=85.0655
	step [111/148], loss=87.2321
	step [112/148], loss=86.7184
	step [113/148], loss=91.5372
	step [114/148], loss=86.6901
	step [115/148], loss=93.9754
	step [116/148], loss=84.8573
	step [117/148], loss=78.7265
	step [118/148], loss=82.7925
	step [119/148], loss=71.8336
	step [120/148], loss=80.5501
	step [121/148], loss=103.0976
	step [122/148], loss=80.4915
	step [123/148], loss=71.5142
	step [124/148], loss=101.5058
	step [125/148], loss=87.5164
	step [126/148], loss=80.6082
	step [127/148], loss=75.0184
	step [128/148], loss=71.7511
	step [129/148], loss=88.6420
	step [130/148], loss=82.0250
	step [131/148], loss=80.3080
	step [132/148], loss=81.3180
	step [133/148], loss=85.3330
	step [134/148], loss=100.7939
	step [135/148], loss=72.6222
	step [136/148], loss=94.0676
	step [137/148], loss=73.3079
	step [138/148], loss=79.5814
	step [139/148], loss=90.3965
	step [140/148], loss=80.6439
	step [141/148], loss=69.3656
	step [142/148], loss=93.8998
	step [143/148], loss=71.7755
	step [144/148], loss=72.9491
	step [145/148], loss=72.2657
	step [146/148], loss=66.5161
	step [147/148], loss=86.2681
	step [148/148], loss=3.4156
	Evaluating
	loss=0.0120, precision=0.3016, recall=0.8400, f1=0.4438
Training epoch 90
	step [1/148], loss=90.2356
	step [2/148], loss=76.9247
	step [3/148], loss=88.4833
	step [4/148], loss=78.8534
	step [5/148], loss=84.9799
	step [6/148], loss=64.6927
	step [7/148], loss=80.2587
	step [8/148], loss=75.8878
	step [9/148], loss=86.9924
	step [10/148], loss=80.5419
	step [11/148], loss=69.9912
	step [12/148], loss=87.4167
	step [13/148], loss=70.8542
	step [14/148], loss=98.6289
	step [15/148], loss=73.4797
	step [16/148], loss=104.3726
	step [17/148], loss=86.5349
	step [18/148], loss=94.0853
	step [19/148], loss=82.3874
	step [20/148], loss=81.3602
	step [21/148], loss=86.4600
	step [22/148], loss=63.7531
	step [23/148], loss=97.1299
	step [24/148], loss=91.9564
	step [25/148], loss=78.9543
	step [26/148], loss=70.9997
	step [27/148], loss=76.8404
	step [28/148], loss=105.2650
	step [29/148], loss=90.2468
	step [30/148], loss=80.0738
	step [31/148], loss=89.7523
	step [32/148], loss=75.8780
	step [33/148], loss=78.5329
	step [34/148], loss=106.1394
	step [35/148], loss=85.4897
	step [36/148], loss=74.0740
	step [37/148], loss=86.9343
	step [38/148], loss=80.1723
	step [39/148], loss=75.4129
	step [40/148], loss=80.3711
	step [41/148], loss=88.9378
	step [42/148], loss=83.4676
	step [43/148], loss=87.3702
	step [44/148], loss=75.5133
	step [45/148], loss=91.0549
	step [46/148], loss=82.6997
	step [47/148], loss=76.3867
	step [48/148], loss=83.8131
	step [49/148], loss=74.9336
	step [50/148], loss=90.2764
	step [51/148], loss=82.8572
	step [52/148], loss=79.4902
	step [53/148], loss=77.4506
	step [54/148], loss=76.5445
	step [55/148], loss=91.7262
	step [56/148], loss=84.3991
	step [57/148], loss=76.5938
	step [58/148], loss=80.8605
	step [59/148], loss=87.7320
	step [60/148], loss=74.3857
	step [61/148], loss=86.5295
	step [62/148], loss=98.1831
	step [63/148], loss=72.9142
	step [64/148], loss=77.1472
	step [65/148], loss=69.2920
	step [66/148], loss=60.4027
	step [67/148], loss=81.4761
	step [68/148], loss=87.6941
	step [69/148], loss=77.9806
	step [70/148], loss=90.9570
	step [71/148], loss=81.4939
	step [72/148], loss=90.8017
	step [73/148], loss=82.3987
	step [74/148], loss=95.1069
	step [75/148], loss=82.9913
	step [76/148], loss=73.4242
	step [77/148], loss=85.0470
	step [78/148], loss=85.8096
	step [79/148], loss=84.8972
	step [80/148], loss=98.1321
	step [81/148], loss=75.5493
	step [82/148], loss=76.3891
	step [83/148], loss=84.0958
	step [84/148], loss=67.4629
	step [85/148], loss=89.2812
	step [86/148], loss=89.9933
	step [87/148], loss=72.4341
	step [88/148], loss=91.6829
	step [89/148], loss=75.9080
	step [90/148], loss=87.8675
	step [91/148], loss=80.2426
	step [92/148], loss=75.9752
	step [93/148], loss=86.9386
	step [94/148], loss=76.5596
	step [95/148], loss=94.8884
	step [96/148], loss=79.4867
	step [97/148], loss=87.5334
	step [98/148], loss=90.8576
	step [99/148], loss=91.4878
	step [100/148], loss=92.5392
	step [101/148], loss=83.4892
	step [102/148], loss=80.0759
	step [103/148], loss=85.2498
	step [104/148], loss=69.2410
	step [105/148], loss=83.3135
	step [106/148], loss=84.0531
	step [107/148], loss=70.0083
	step [108/148], loss=67.9514
	step [109/148], loss=81.2512
	step [110/148], loss=63.4131
	step [111/148], loss=76.2192
	step [112/148], loss=69.5687
	step [113/148], loss=77.9283
	step [114/148], loss=84.4874
	step [115/148], loss=81.2674
	step [116/148], loss=77.1011
	step [117/148], loss=80.5374
	step [118/148], loss=73.7444
	step [119/148], loss=74.7287
	step [120/148], loss=91.7758
	step [121/148], loss=88.5711
	step [122/148], loss=85.0496
	step [123/148], loss=85.4150
	step [124/148], loss=70.9712
	step [125/148], loss=82.7055
	step [126/148], loss=82.7994
	step [127/148], loss=87.8420
	step [128/148], loss=96.0031
	step [129/148], loss=91.6444
	step [130/148], loss=101.2360
	step [131/148], loss=82.0504
	step [132/148], loss=77.9534
	step [133/148], loss=80.5878
	step [134/148], loss=85.7438
	step [135/148], loss=79.8501
	step [136/148], loss=102.2008
	step [137/148], loss=70.0524
	step [138/148], loss=79.9688
	step [139/148], loss=82.4679
	step [140/148], loss=95.5714
	step [141/148], loss=63.9280
	step [142/148], loss=87.7581
	step [143/148], loss=89.0504
	step [144/148], loss=87.2975
	step [145/148], loss=85.1830
	step [146/148], loss=75.9178
	step [147/148], loss=82.5642
	step [148/148], loss=6.5452
	Evaluating
	loss=0.0113, precision=0.3265, recall=0.8483, f1=0.4715
Training epoch 91
	step [1/148], loss=85.5592
	step [2/148], loss=86.4437
	step [3/148], loss=69.2452
	step [4/148], loss=82.7058
	step [5/148], loss=82.1840
	step [6/148], loss=77.2605
	step [7/148], loss=100.1031
	step [8/148], loss=70.9754
	step [9/148], loss=76.0783
	step [10/148], loss=88.6573
	step [11/148], loss=78.7898
	step [12/148], loss=79.1524
	step [13/148], loss=90.9642
	step [14/148], loss=84.4059
	step [15/148], loss=94.0546
	step [16/148], loss=84.7815
	step [17/148], loss=80.9262
	step [18/148], loss=78.9036
	step [19/148], loss=87.7402
	step [20/148], loss=64.6587
	step [21/148], loss=81.3869
	step [22/148], loss=66.9059
	step [23/148], loss=80.4445
	step [24/148], loss=69.9253
	step [25/148], loss=76.7181
	step [26/148], loss=85.9502
	step [27/148], loss=80.0135
	step [28/148], loss=82.1365
	step [29/148], loss=82.0483
	step [30/148], loss=92.4397
	step [31/148], loss=84.2013
	step [32/148], loss=74.4235
	step [33/148], loss=87.7829
	step [34/148], loss=75.6185
	step [35/148], loss=70.7175
	step [36/148], loss=110.2097
	step [37/148], loss=86.8948
	step [38/148], loss=82.3075
	step [39/148], loss=84.2530
	step [40/148], loss=98.5457
	step [41/148], loss=85.5099
	step [42/148], loss=80.2033
	step [43/148], loss=79.5905
	step [44/148], loss=79.2066
	step [45/148], loss=75.5108
	step [46/148], loss=75.1637
	step [47/148], loss=80.1959
	step [48/148], loss=63.5692
	step [49/148], loss=84.5503
	step [50/148], loss=85.9716
	step [51/148], loss=86.9584
	step [52/148], loss=77.5700
	step [53/148], loss=75.7576
	step [54/148], loss=79.6992
	step [55/148], loss=93.1064
	step [56/148], loss=100.0455
	step [57/148], loss=88.7055
	step [58/148], loss=78.9849
	step [59/148], loss=73.6053
	step [60/148], loss=86.3553
	step [61/148], loss=82.8182
	step [62/148], loss=82.6023
	step [63/148], loss=80.2452
	step [64/148], loss=75.3878
	step [65/148], loss=93.3608
	step [66/148], loss=81.8766
	step [67/148], loss=85.3635
	step [68/148], loss=82.6749
	step [69/148], loss=86.8299
	step [70/148], loss=80.8165
	step [71/148], loss=86.2643
	step [72/148], loss=76.2516
	step [73/148], loss=90.8368
	step [74/148], loss=88.0094
	step [75/148], loss=73.7868
	step [76/148], loss=65.2302
	step [77/148], loss=79.2515
	step [78/148], loss=94.4791
	step [79/148], loss=74.2003
	step [80/148], loss=88.2034
	step [81/148], loss=67.3486
	step [82/148], loss=85.6531
	step [83/148], loss=83.9232
	step [84/148], loss=85.9469
	step [85/148], loss=79.2964
	step [86/148], loss=84.9693
	step [87/148], loss=85.8174
	step [88/148], loss=77.7023
	step [89/148], loss=72.9008
	step [90/148], loss=87.9645
	step [91/148], loss=80.3096
	step [92/148], loss=87.1154
	step [93/148], loss=92.3699
	step [94/148], loss=79.1545
	step [95/148], loss=75.6879
	step [96/148], loss=79.0019
	step [97/148], loss=95.5609
	step [98/148], loss=87.3871
	step [99/148], loss=95.4006
	step [100/148], loss=78.2189
	step [101/148], loss=90.8472
	step [102/148], loss=84.2485
	step [103/148], loss=80.3518
	step [104/148], loss=74.3525
	step [105/148], loss=75.3697
	step [106/148], loss=91.3516
	step [107/148], loss=116.4821
	step [108/148], loss=90.8598
	step [109/148], loss=83.5783
	step [110/148], loss=80.2406
	step [111/148], loss=89.8893
	step [112/148], loss=73.7401
	step [113/148], loss=76.8192
	step [114/148], loss=94.3653
	step [115/148], loss=86.7114
	step [116/148], loss=78.7384
	step [117/148], loss=77.1232
	step [118/148], loss=83.9345
	step [119/148], loss=90.7869
	step [120/148], loss=77.4469
	step [121/148], loss=81.0328
	step [122/148], loss=71.5318
	step [123/148], loss=85.2711
	step [124/148], loss=75.3208
	step [125/148], loss=75.1591
	step [126/148], loss=62.9647
	step [127/148], loss=69.6506
	step [128/148], loss=72.3522
	step [129/148], loss=77.3690
	step [130/148], loss=83.0821
	step [131/148], loss=90.9232
	step [132/148], loss=76.1494
	step [133/148], loss=74.7415
	step [134/148], loss=81.6120
	step [135/148], loss=80.1316
	step [136/148], loss=80.6846
	step [137/148], loss=89.2048
	step [138/148], loss=73.5658
	step [139/148], loss=91.0932
	step [140/148], loss=73.8183
	step [141/148], loss=86.8488
	step [142/148], loss=66.7615
	step [143/148], loss=87.3176
	step [144/148], loss=90.5119
	step [145/148], loss=70.8131
	step [146/148], loss=92.4256
	step [147/148], loss=89.3099
	step [148/148], loss=10.5974
	Evaluating
	loss=0.0111, precision=0.3219, recall=0.8344, f1=0.4646
Training epoch 92
	step [1/148], loss=78.3563
	step [2/148], loss=90.7346
	step [3/148], loss=96.5307
	step [4/148], loss=76.8151
	step [5/148], loss=70.3439
	step [6/148], loss=80.2497
	step [7/148], loss=79.1829
	step [8/148], loss=69.3303
	step [9/148], loss=87.5219
	step [10/148], loss=73.3226
	step [11/148], loss=84.6127
	step [12/148], loss=70.6387
	step [13/148], loss=73.9969
	step [14/148], loss=82.3873
	step [15/148], loss=77.8752
	step [16/148], loss=91.5222
	step [17/148], loss=86.9462
	step [18/148], loss=100.5189
	step [19/148], loss=86.3033
	step [20/148], loss=84.4055
	step [21/148], loss=78.5710
	step [22/148], loss=72.3879
	step [23/148], loss=76.4518
	step [24/148], loss=79.2981
	step [25/148], loss=74.5251
	step [26/148], loss=79.9000
	step [27/148], loss=81.4036
	step [28/148], loss=98.7612
	step [29/148], loss=82.0927
	step [30/148], loss=96.3600
	step [31/148], loss=95.5199
	step [32/148], loss=75.3721
	step [33/148], loss=81.5278
	step [34/148], loss=97.9072
	step [35/148], loss=81.9009
	step [36/148], loss=95.0527
	step [37/148], loss=71.7296
	step [38/148], loss=84.6720
	step [39/148], loss=74.3207
	step [40/148], loss=71.9048
	step [41/148], loss=72.8641
	step [42/148], loss=105.0721
	step [43/148], loss=80.0237
	step [44/148], loss=88.7796
	step [45/148], loss=67.2787
	step [46/148], loss=78.9690
	step [47/148], loss=76.6320
	step [48/148], loss=70.4242
	step [49/148], loss=90.2749
	step [50/148], loss=88.3787
	step [51/148], loss=69.7507
	step [52/148], loss=65.1093
	step [53/148], loss=75.2645
	step [54/148], loss=83.5735
	step [55/148], loss=96.1384
	step [56/148], loss=81.8573
	step [57/148], loss=74.3040
	step [58/148], loss=76.2260
	step [59/148], loss=96.6950
	step [60/148], loss=90.8130
	step [61/148], loss=96.4372
	step [62/148], loss=96.8799
	step [63/148], loss=65.5742
	step [64/148], loss=75.9427
	step [65/148], loss=82.7140
	step [66/148], loss=81.1097
	step [67/148], loss=83.2611
	step [68/148], loss=87.9704
	step [69/148], loss=74.1865
	step [70/148], loss=82.8027
	step [71/148], loss=81.7786
	step [72/148], loss=74.2229
	step [73/148], loss=85.6791
	step [74/148], loss=66.5223
	step [75/148], loss=110.2749
	step [76/148], loss=93.4965
	step [77/148], loss=78.0556
	step [78/148], loss=68.9983
	step [79/148], loss=78.8274
	step [80/148], loss=79.8374
	step [81/148], loss=88.1376
	step [82/148], loss=97.0182
	step [83/148], loss=77.7608
	step [84/148], loss=84.9550
	step [85/148], loss=88.2776
	step [86/148], loss=79.3811
	step [87/148], loss=86.2785
	step [88/148], loss=90.3138
	step [89/148], loss=94.4727
	step [90/148], loss=78.3768
	step [91/148], loss=86.7317
	step [92/148], loss=84.3721
	step [93/148], loss=70.0653
	step [94/148], loss=88.8670
	step [95/148], loss=94.7579
	step [96/148], loss=98.6473
	step [97/148], loss=80.8745
	step [98/148], loss=64.8865
	step [99/148], loss=72.1839
	step [100/148], loss=78.8625
	step [101/148], loss=70.4437
	step [102/148], loss=84.3143
	step [103/148], loss=88.0080
	step [104/148], loss=78.4649
	step [105/148], loss=78.5963
	step [106/148], loss=95.7237
	step [107/148], loss=84.5652
	step [108/148], loss=87.1990
	step [109/148], loss=82.6446
	step [110/148], loss=71.4040
	step [111/148], loss=85.1122
	step [112/148], loss=80.5361
	step [113/148], loss=73.7658
	step [114/148], loss=71.1941
	step [115/148], loss=71.7944
	step [116/148], loss=92.6126
	step [117/148], loss=76.5137
	step [118/148], loss=76.0487
	step [119/148], loss=91.1078
	step [120/148], loss=84.4469
	step [121/148], loss=68.2619
	step [122/148], loss=74.5331
	step [123/148], loss=80.4110
	step [124/148], loss=87.4376
	step [125/148], loss=87.4129
	step [126/148], loss=83.9432
	step [127/148], loss=86.4631
	step [128/148], loss=85.1148
	step [129/148], loss=89.1628
	step [130/148], loss=88.3702
	step [131/148], loss=84.5871
	step [132/148], loss=79.5116
	step [133/148], loss=76.5254
	step [134/148], loss=74.8324
	step [135/148], loss=84.8454
	step [136/148], loss=83.7814
	step [137/148], loss=77.4600
	step [138/148], loss=96.1225
	step [139/148], loss=82.0883
	step [140/148], loss=84.5031
	step [141/148], loss=80.5452
	step [142/148], loss=62.5967
	step [143/148], loss=81.8614
	step [144/148], loss=70.4662
	step [145/148], loss=95.7607
	step [146/148], loss=84.7138
	step [147/148], loss=74.8203
	step [148/148], loss=4.8481
	Evaluating
	loss=0.0125, precision=0.2765, recall=0.8447, f1=0.4166
Training epoch 93
	step [1/148], loss=85.2444
	step [2/148], loss=80.8939
	step [3/148], loss=74.6306
	step [4/148], loss=92.1627
	step [5/148], loss=72.1643
	step [6/148], loss=78.1640
	step [7/148], loss=84.2715
	step [8/148], loss=83.4075
	step [9/148], loss=91.3467
	step [10/148], loss=78.1439
	step [11/148], loss=71.9716
	step [12/148], loss=70.4257
	step [13/148], loss=95.8067
	step [14/148], loss=80.4344
	step [15/148], loss=81.8195
	step [16/148], loss=68.4566
	step [17/148], loss=81.4600
	step [18/148], loss=77.2558
	step [19/148], loss=89.3874
	step [20/148], loss=96.7007
	step [21/148], loss=82.8737
	step [22/148], loss=65.7157
	step [23/148], loss=75.0614
	step [24/148], loss=83.9816
	step [25/148], loss=77.5047
	step [26/148], loss=88.4778
	step [27/148], loss=78.4613
	step [28/148], loss=84.1954
	step [29/148], loss=79.9716
	step [30/148], loss=88.3813
	step [31/148], loss=95.0110
	step [32/148], loss=77.1389
	step [33/148], loss=85.2190
	step [34/148], loss=83.8620
	step [35/148], loss=73.1792
	step [36/148], loss=85.5841
	step [37/148], loss=91.6580
	step [38/148], loss=78.5735
	step [39/148], loss=92.0017
	step [40/148], loss=99.6509
	step [41/148], loss=83.2382
	step [42/148], loss=82.3272
	step [43/148], loss=90.0696
	step [44/148], loss=90.6592
	step [45/148], loss=79.8305
	step [46/148], loss=77.8926
	step [47/148], loss=76.5569
	step [48/148], loss=90.9610
	step [49/148], loss=83.8114
	step [50/148], loss=84.1976
	step [51/148], loss=87.3314
	step [52/148], loss=66.6201
	step [53/148], loss=75.9922
	step [54/148], loss=80.0671
	step [55/148], loss=81.8484
	step [56/148], loss=79.1142
	step [57/148], loss=80.7792
	step [58/148], loss=79.2042
	step [59/148], loss=81.7066
	step [60/148], loss=90.0417
	step [61/148], loss=83.2389
	step [62/148], loss=99.9509
	step [63/148], loss=76.9462
	step [64/148], loss=81.3378
	step [65/148], loss=71.4192
	step [66/148], loss=76.3690
	step [67/148], loss=78.7564
	step [68/148], loss=86.2562
	step [69/148], loss=78.1817
	step [70/148], loss=72.6148
	step [71/148], loss=83.2911
	step [72/148], loss=79.6087
	step [73/148], loss=77.1267
	step [74/148], loss=78.2306
	step [75/148], loss=87.7116
	step [76/148], loss=88.0913
	step [77/148], loss=86.6591
	step [78/148], loss=80.6575
	step [79/148], loss=107.6111
	step [80/148], loss=79.5481
	step [81/148], loss=77.1554
	step [82/148], loss=78.4827
	step [83/148], loss=64.1683
	step [84/148], loss=82.4401
	step [85/148], loss=69.9710
	step [86/148], loss=65.6288
	step [87/148], loss=80.2670
	step [88/148], loss=87.7878
	step [89/148], loss=71.7513
	step [90/148], loss=72.7720
	step [91/148], loss=90.5402
	step [92/148], loss=55.8889
	step [93/148], loss=80.5944
	step [94/148], loss=84.2370
	step [95/148], loss=85.2608
	step [96/148], loss=90.6760
	step [97/148], loss=85.8508
	step [98/148], loss=77.1772
	step [99/148], loss=78.8234
	step [100/148], loss=72.6151
	step [101/148], loss=77.7242
	step [102/148], loss=77.1468
	step [103/148], loss=79.1809
	step [104/148], loss=97.8231
	step [105/148], loss=92.8640
	step [106/148], loss=73.9082
	step [107/148], loss=81.0547
	step [108/148], loss=86.2593
	step [109/148], loss=87.6539
	step [110/148], loss=87.9852
	step [111/148], loss=62.2491
	step [112/148], loss=78.0349
	step [113/148], loss=64.9265
	step [114/148], loss=98.6667
	step [115/148], loss=84.3294
	step [116/148], loss=96.0130
	step [117/148], loss=92.9781
	step [118/148], loss=73.2482
	step [119/148], loss=75.4811
	step [120/148], loss=70.3747
	step [121/148], loss=68.1016
	step [122/148], loss=76.4042
	step [123/148], loss=74.9210
	step [124/148], loss=74.9058
	step [125/148], loss=72.7199
	step [126/148], loss=76.3534
	step [127/148], loss=90.9863
	step [128/148], loss=70.3302
	step [129/148], loss=94.1667
	step [130/148], loss=66.4872
	step [131/148], loss=85.6215
	step [132/148], loss=84.4084
	step [133/148], loss=81.1327
	step [134/148], loss=86.1076
	step [135/148], loss=101.5082
	step [136/148], loss=93.1592
	step [137/148], loss=81.1910
	step [138/148], loss=88.0454
	step [139/148], loss=88.4320
	step [140/148], loss=77.2941
	step [141/148], loss=78.2874
	step [142/148], loss=76.2793
	step [143/148], loss=99.0426
	step [144/148], loss=83.9107
	step [145/148], loss=84.1244
	step [146/148], loss=87.8457
	step [147/148], loss=65.6333
	step [148/148], loss=8.9625
	Evaluating
	loss=0.0110, precision=0.3285, recall=0.8418, f1=0.4726
Training epoch 94
	step [1/148], loss=72.4873
	step [2/148], loss=102.0563
	step [3/148], loss=99.3827
	step [4/148], loss=69.9495
	step [5/148], loss=92.8007
	step [6/148], loss=82.8116
	step [7/148], loss=94.0723
	step [8/148], loss=79.9905
	step [9/148], loss=77.3410
	step [10/148], loss=84.3837
	step [11/148], loss=78.7593
	step [12/148], loss=82.7181
	step [13/148], loss=69.4064
	step [14/148], loss=71.8222
	step [15/148], loss=81.0755
	step [16/148], loss=103.4532
	step [17/148], loss=80.4886
	step [18/148], loss=75.5795
	step [19/148], loss=95.4939
	step [20/148], loss=71.6687
	step [21/148], loss=85.8932
	step [22/148], loss=68.0338
	step [23/148], loss=66.8024
	step [24/148], loss=77.1548
	step [25/148], loss=88.2298
	step [26/148], loss=70.6026
	step [27/148], loss=63.3227
	step [28/148], loss=74.6903
	step [29/148], loss=83.5272
	step [30/148], loss=86.1684
	step [31/148], loss=87.9360
	step [32/148], loss=84.2454
	step [33/148], loss=82.4153
	step [34/148], loss=67.4084
	step [35/148], loss=92.8547
	step [36/148], loss=87.1441
	step [37/148], loss=75.6519
	step [38/148], loss=78.8950
	step [39/148], loss=81.0864
	step [40/148], loss=97.2620
	step [41/148], loss=86.2645
	step [42/148], loss=82.5470
	step [43/148], loss=82.7726
	step [44/148], loss=92.5617
	step [45/148], loss=86.6753
	step [46/148], loss=85.0714
	step [47/148], loss=74.0279
	step [48/148], loss=76.3493
	step [49/148], loss=86.2865
	step [50/148], loss=86.0921
	step [51/148], loss=71.7749
	step [52/148], loss=80.3081
	step [53/148], loss=90.8590
	step [54/148], loss=96.3707
	step [55/148], loss=96.8792
	step [56/148], loss=71.4559
	step [57/148], loss=92.0375
	step [58/148], loss=69.7006
	step [59/148], loss=71.3752
	step [60/148], loss=80.3571
	step [61/148], loss=89.5750
	step [62/148], loss=86.2589
	step [63/148], loss=73.4760
	step [64/148], loss=88.8948
	step [65/148], loss=78.5397
	step [66/148], loss=74.7955
	step [67/148], loss=81.8204
	step [68/148], loss=84.8173
	step [69/148], loss=95.8371
	step [70/148], loss=78.7028
	step [71/148], loss=81.9726
	step [72/148], loss=84.6304
	step [73/148], loss=82.1097
	step [74/148], loss=74.6843
	step [75/148], loss=87.6292
	step [76/148], loss=93.9962
	step [77/148], loss=83.4387
	step [78/148], loss=71.7926
	step [79/148], loss=88.5888
	step [80/148], loss=69.7789
	step [81/148], loss=65.9719
	step [82/148], loss=91.8363
	step [83/148], loss=77.6666
	step [84/148], loss=80.5909
	step [85/148], loss=84.3022
	step [86/148], loss=81.5737
	step [87/148], loss=82.4075
	step [88/148], loss=92.0929
	step [89/148], loss=83.0546
	step [90/148], loss=69.2987
	step [91/148], loss=82.5450
	step [92/148], loss=67.5370
	step [93/148], loss=75.1618
	step [94/148], loss=94.1911
	step [95/148], loss=80.5506
	step [96/148], loss=97.2939
	step [97/148], loss=79.7951
	step [98/148], loss=99.6216
	step [99/148], loss=67.9434
	step [100/148], loss=88.4951
	step [101/148], loss=90.4982
	step [102/148], loss=76.2708
	step [103/148], loss=69.6451
	step [104/148], loss=88.5435
	step [105/148], loss=80.4155
	step [106/148], loss=86.7605
	step [107/148], loss=76.0202
	step [108/148], loss=68.4229
	step [109/148], loss=79.3859
	step [110/148], loss=74.0073
	step [111/148], loss=82.3918
	step [112/148], loss=82.7328
	step [113/148], loss=72.0294
	step [114/148], loss=107.9882
	step [115/148], loss=91.2372
	step [116/148], loss=76.1057
	step [117/148], loss=80.8642
	step [118/148], loss=81.3402
	step [119/148], loss=89.6481
	step [120/148], loss=77.7170
	step [121/148], loss=74.0282
	step [122/148], loss=76.1199
	step [123/148], loss=59.0599
	step [124/148], loss=64.4461
	step [125/148], loss=80.3866
	step [126/148], loss=72.3709
	step [127/148], loss=86.3841
	step [128/148], loss=79.6565
	step [129/148], loss=87.5666
	step [130/148], loss=75.0341
	step [131/148], loss=82.1910
	step [132/148], loss=96.7393
	step [133/148], loss=67.1217
	step [134/148], loss=66.1887
	step [135/148], loss=85.6840
	step [136/148], loss=72.3033
	step [137/148], loss=85.7020
	step [138/148], loss=76.7250
	step [139/148], loss=66.4531
	step [140/148], loss=77.8058
	step [141/148], loss=85.0139
	step [142/148], loss=79.7420
	step [143/148], loss=82.5780
	step [144/148], loss=93.9912
	step [145/148], loss=80.9596
	step [146/148], loss=86.5722
	step [147/148], loss=74.6752
	step [148/148], loss=6.3732
	Evaluating
	loss=0.0127, precision=0.2691, recall=0.8316, f1=0.4066
Training epoch 95
	step [1/148], loss=88.0960
	step [2/148], loss=76.3353
	step [3/148], loss=78.4654
	step [4/148], loss=60.8905
	step [5/148], loss=81.9331
	step [6/148], loss=78.2073
	step [7/148], loss=104.1546
	step [8/148], loss=66.5601
	step [9/148], loss=82.5292
	step [10/148], loss=70.1351
	step [11/148], loss=87.8568
	step [12/148], loss=77.6694
	step [13/148], loss=86.9401
	step [14/148], loss=61.0928
	step [15/148], loss=91.9759
	step [16/148], loss=66.7790
	step [17/148], loss=76.8908
	step [18/148], loss=79.1877
	step [19/148], loss=77.4643
	step [20/148], loss=93.5741
	step [21/148], loss=76.7703
	step [22/148], loss=80.5422
	step [23/148], loss=74.1169
	step [24/148], loss=76.2719
	step [25/148], loss=73.7737
	step [26/148], loss=82.6322
	step [27/148], loss=72.6951
	step [28/148], loss=91.1350
	step [29/148], loss=94.5638
	step [30/148], loss=86.9493
	step [31/148], loss=79.2945
	step [32/148], loss=82.0501
	step [33/148], loss=84.8886
	step [34/148], loss=90.7513
	step [35/148], loss=86.1695
	step [36/148], loss=78.3617
	step [37/148], loss=93.1181
	step [38/148], loss=84.8567
	step [39/148], loss=84.4605
	step [40/148], loss=87.9692
	step [41/148], loss=74.1345
	step [42/148], loss=66.6857
	step [43/148], loss=80.2807
	step [44/148], loss=90.1233
	step [45/148], loss=63.8057
	step [46/148], loss=79.2738
	step [47/148], loss=74.0884
	step [48/148], loss=80.8392
	step [49/148], loss=72.6711
	step [50/148], loss=83.9740
	step [51/148], loss=81.6854
	step [52/148], loss=84.7294
	step [53/148], loss=81.2416
	step [54/148], loss=67.7389
	step [55/148], loss=64.0647
	step [56/148], loss=71.3910
	step [57/148], loss=74.7934
	step [58/148], loss=76.1983
	step [59/148], loss=75.7444
	step [60/148], loss=75.9772
	step [61/148], loss=82.8519
	step [62/148], loss=69.7037
	step [63/148], loss=106.3230
	step [64/148], loss=82.5204
	step [65/148], loss=80.7026
	step [66/148], loss=83.7498
	step [67/148], loss=61.0717
	step [68/148], loss=80.3527
	step [69/148], loss=90.9051
	step [70/148], loss=70.7141
	step [71/148], loss=83.4780
	step [72/148], loss=81.6908
	step [73/148], loss=106.3838
	step [74/148], loss=84.8403
	step [75/148], loss=89.6630
	step [76/148], loss=81.2243
	step [77/148], loss=87.9161
	step [78/148], loss=81.3900
	step [79/148], loss=78.4194
	step [80/148], loss=82.3606
	step [81/148], loss=77.0300
	step [82/148], loss=107.0672
	step [83/148], loss=79.2032
	step [84/148], loss=75.4454
	step [85/148], loss=79.2760
	step [86/148], loss=75.7226
	step [87/148], loss=74.4388
	step [88/148], loss=85.2275
	step [89/148], loss=82.3266
	step [90/148], loss=103.2229
	step [91/148], loss=91.4691
	step [92/148], loss=85.0574
	step [93/148], loss=59.9080
	step [94/148], loss=77.5763
	step [95/148], loss=83.6359
	step [96/148], loss=88.5693
	step [97/148], loss=85.4912
	step [98/148], loss=73.2507
	step [99/148], loss=71.0643
	step [100/148], loss=68.0413
	step [101/148], loss=80.1748
	step [102/148], loss=63.6799
	step [103/148], loss=63.9179
	step [104/148], loss=88.9826
	step [105/148], loss=88.5248
	step [106/148], loss=80.2975
	step [107/148], loss=88.3519
	step [108/148], loss=75.8287
	step [109/148], loss=81.6024
	step [110/148], loss=98.3765
	step [111/148], loss=68.9352
	step [112/148], loss=76.4662
	step [113/148], loss=86.0101
	step [114/148], loss=81.7851
	step [115/148], loss=101.2536
	step [116/148], loss=75.8316
	step [117/148], loss=84.2936
	step [118/148], loss=77.0432
	step [119/148], loss=75.0629
	step [120/148], loss=87.8176
	step [121/148], loss=86.0771
	step [122/148], loss=76.1841
	step [123/148], loss=79.4922
	step [124/148], loss=70.6728
	step [125/148], loss=79.1351
	step [126/148], loss=87.8691
	step [127/148], loss=90.3918
	step [128/148], loss=79.3932
	step [129/148], loss=80.1142
	step [130/148], loss=81.9080
	step [131/148], loss=85.0782
	step [132/148], loss=81.7958
	step [133/148], loss=79.3949
	step [134/148], loss=93.7250
	step [135/148], loss=85.0351
	step [136/148], loss=93.8378
	step [137/148], loss=90.9443
	step [138/148], loss=74.9560
	step [139/148], loss=80.3841
	step [140/148], loss=83.3182
	step [141/148], loss=84.4328
	step [142/148], loss=97.0539
	step [143/148], loss=83.5399
	step [144/148], loss=67.4140
	step [145/148], loss=75.6746
	step [146/148], loss=92.9712
	step [147/148], loss=74.7603
	step [148/148], loss=2.6003
	Evaluating
	loss=0.0133, precision=0.2707, recall=0.8577, f1=0.4115
Training epoch 96
	step [1/148], loss=73.3722
	step [2/148], loss=87.2364
	step [3/148], loss=80.6608
	step [4/148], loss=78.8299
	step [5/148], loss=76.3545
	step [6/148], loss=79.6767
	step [7/148], loss=81.7384
	step [8/148], loss=82.9574
	step [9/148], loss=94.0950
	step [10/148], loss=83.8392
	step [11/148], loss=93.1434
	step [12/148], loss=85.8324
	step [13/148], loss=83.1277
	step [14/148], loss=69.0939
	step [15/148], loss=82.3208
	step [16/148], loss=72.9923
	step [17/148], loss=63.7401
	step [18/148], loss=71.3330
	step [19/148], loss=75.6971
	step [20/148], loss=75.1014
	step [21/148], loss=73.4371
	step [22/148], loss=77.2702
	step [23/148], loss=73.3128
	step [24/148], loss=80.9346
	step [25/148], loss=79.6874
	step [26/148], loss=77.9279
	step [27/148], loss=79.5254
	step [28/148], loss=91.4633
	step [29/148], loss=79.2869
	step [30/148], loss=72.3891
	step [31/148], loss=81.4761
	step [32/148], loss=87.3769
	step [33/148], loss=84.6910
	step [34/148], loss=80.1373
	step [35/148], loss=102.9331
	step [36/148], loss=78.1771
	step [37/148], loss=76.7903
	step [38/148], loss=80.1712
	step [39/148], loss=80.2787
	step [40/148], loss=79.1809
	step [41/148], loss=60.9948
	step [42/148], loss=73.6521
	step [43/148], loss=77.4442
	step [44/148], loss=97.7712
	step [45/148], loss=70.4793
	step [46/148], loss=90.1489
	step [47/148], loss=85.1973
	step [48/148], loss=80.4245
	step [49/148], loss=67.5890
	step [50/148], loss=85.9209
	step [51/148], loss=76.2264
	step [52/148], loss=79.5109
	step [53/148], loss=92.5192
	step [54/148], loss=85.1952
	step [55/148], loss=89.8888
	step [56/148], loss=83.3441
	step [57/148], loss=73.8515
	step [58/148], loss=75.4269
	step [59/148], loss=86.3772
	step [60/148], loss=92.6188
	step [61/148], loss=74.6680
	step [62/148], loss=88.3043
	step [63/148], loss=79.6797
	step [64/148], loss=80.3291
	step [65/148], loss=87.5228
	step [66/148], loss=92.0144
	step [67/148], loss=70.6305
	step [68/148], loss=70.1483
	step [69/148], loss=81.7726
	step [70/148], loss=72.6715
	step [71/148], loss=88.8361
	step [72/148], loss=86.9044
	step [73/148], loss=78.5007
	step [74/148], loss=81.3946
	step [75/148], loss=83.6426
	step [76/148], loss=73.4529
	step [77/148], loss=82.3220
	step [78/148], loss=84.8221
	step [79/148], loss=100.8432
	step [80/148], loss=91.9496
	step [81/148], loss=87.3016
	step [82/148], loss=73.7550
	step [83/148], loss=87.7604
	step [84/148], loss=81.3328
	step [85/148], loss=90.7496
	step [86/148], loss=71.0617
	step [87/148], loss=85.2532
	step [88/148], loss=88.9274
	step [89/148], loss=86.3572
	step [90/148], loss=74.8576
	step [91/148], loss=95.8719
	step [92/148], loss=94.2237
	step [93/148], loss=91.9126
	step [94/148], loss=88.5088
	step [95/148], loss=91.2949
	step [96/148], loss=87.8857
	step [97/148], loss=96.0918
	step [98/148], loss=95.0438
	step [99/148], loss=94.9121
	step [100/148], loss=80.6489
	step [101/148], loss=71.2520
	step [102/148], loss=73.6574
	step [103/148], loss=80.1790
	step [104/148], loss=97.0251
	step [105/148], loss=84.5098
	step [106/148], loss=78.2814
	step [107/148], loss=80.0462
	step [108/148], loss=84.9121
	step [109/148], loss=79.3464
	step [110/148], loss=95.4211
	step [111/148], loss=69.1643
	step [112/148], loss=82.3576
	step [113/148], loss=74.7053
	step [114/148], loss=75.0564
	step [115/148], loss=73.6973
	step [116/148], loss=79.8123
	step [117/148], loss=83.7827
	step [118/148], loss=85.3374
	step [119/148], loss=78.6732
	step [120/148], loss=87.9480
	step [121/148], loss=80.0911
	step [122/148], loss=96.6691
	step [123/148], loss=91.9325
	step [124/148], loss=73.3158
	step [125/148], loss=57.2101
	step [126/148], loss=63.4252
	step [127/148], loss=88.2706
	step [128/148], loss=100.9253
	step [129/148], loss=82.0569
	step [130/148], loss=77.6119
	step [131/148], loss=89.3687
	step [132/148], loss=95.5110
	step [133/148], loss=59.9293
	step [134/148], loss=66.6693
	step [135/148], loss=81.5863
	step [136/148], loss=80.7815
	step [137/148], loss=79.9439
	step [138/148], loss=81.6080
	step [139/148], loss=78.4278
	step [140/148], loss=75.0890
	step [141/148], loss=83.6327
	step [142/148], loss=82.8943
	step [143/148], loss=81.5243
	step [144/148], loss=76.0989
	step [145/148], loss=77.6102
	step [146/148], loss=78.7986
	step [147/148], loss=85.6228
	step [148/148], loss=5.8799
	Evaluating
	loss=0.0102, precision=0.3598, recall=0.8577, f1=0.5069
saving model as: 0_saved_model.pth
Training epoch 97
	step [1/148], loss=90.6300
	step [2/148], loss=85.1369
	step [3/148], loss=71.7451
	step [4/148], loss=69.4044
	step [5/148], loss=77.0415
	step [6/148], loss=75.8787
	step [7/148], loss=78.0138
	step [8/148], loss=82.0431
	step [9/148], loss=84.3628
	step [10/148], loss=81.4048
	step [11/148], loss=73.9544
	step [12/148], loss=75.6881
	step [13/148], loss=88.0285
	step [14/148], loss=83.0618
	step [15/148], loss=87.5362
	step [16/148], loss=90.4457
	step [17/148], loss=83.7408
	step [18/148], loss=84.1144
	step [19/148], loss=75.9342
	step [20/148], loss=78.3532
	step [21/148], loss=75.7668
	step [22/148], loss=95.1141
	step [23/148], loss=64.5207
	step [24/148], loss=77.7344
	step [25/148], loss=71.7606
	step [26/148], loss=89.3667
	step [27/148], loss=80.5305
	step [28/148], loss=64.1098
	step [29/148], loss=86.3732
	step [30/148], loss=86.9298
	step [31/148], loss=78.6988
	step [32/148], loss=57.3122
	step [33/148], loss=71.3494
	step [34/148], loss=86.8296
	step [35/148], loss=94.0426
	step [36/148], loss=90.2587
	step [37/148], loss=83.6760
	step [38/148], loss=81.7075
	step [39/148], loss=92.6902
	step [40/148], loss=76.0049
	step [41/148], loss=87.5203
	step [42/148], loss=87.2095
	step [43/148], loss=82.8399
	step [44/148], loss=76.6012
	step [45/148], loss=76.9142
	step [46/148], loss=63.7954
	step [47/148], loss=92.6896
	step [48/148], loss=69.9210
	step [49/148], loss=74.0214
	step [50/148], loss=91.5305
	step [51/148], loss=82.1884
	step [52/148], loss=88.4274
	step [53/148], loss=82.1724
	step [54/148], loss=78.1241
	step [55/148], loss=82.0303
	step [56/148], loss=96.3357
	step [57/148], loss=89.6034
	step [58/148], loss=77.4059
	step [59/148], loss=71.0749
	step [60/148], loss=75.3850
	step [61/148], loss=88.0617
	step [62/148], loss=78.0802
	step [63/148], loss=77.7292
	step [64/148], loss=81.4068
	step [65/148], loss=68.8662
	step [66/148], loss=88.7510
	step [67/148], loss=81.8478
	step [68/148], loss=89.0501
	step [69/148], loss=84.6383
	step [70/148], loss=81.9858
	step [71/148], loss=82.5551
	step [72/148], loss=79.6651
	step [73/148], loss=85.3691
	step [74/148], loss=79.5398
	step [75/148], loss=74.7520
	step [76/148], loss=88.8387
	step [77/148], loss=80.7981
	step [78/148], loss=89.2355
	step [79/148], loss=77.4396
	step [80/148], loss=80.2858
	step [81/148], loss=89.4377
	step [82/148], loss=74.8667
	step [83/148], loss=84.0711
	step [84/148], loss=83.9212
	step [85/148], loss=80.6168
	step [86/148], loss=88.2161
	step [87/148], loss=72.1127
	step [88/148], loss=96.4168
	step [89/148], loss=82.1806
	step [90/148], loss=68.0244
	step [91/148], loss=95.6366
	step [92/148], loss=89.4529
	step [93/148], loss=87.4427
	step [94/148], loss=74.8590
	step [95/148], loss=68.9419
	step [96/148], loss=84.2797
	step [97/148], loss=65.0389
	step [98/148], loss=82.1808
	step [99/148], loss=77.1338
	step [100/148], loss=96.8836
	step [101/148], loss=72.8883
	step [102/148], loss=78.1322
	step [103/148], loss=84.2329
	step [104/148], loss=80.7893
	step [105/148], loss=83.6119
	step [106/148], loss=80.4929
	step [107/148], loss=64.7250
	step [108/148], loss=76.9069
	step [109/148], loss=80.7321
	step [110/148], loss=64.8688
	step [111/148], loss=70.8134
	step [112/148], loss=87.8956
	step [113/148], loss=82.2896
	step [114/148], loss=79.5598
	step [115/148], loss=72.6741
	step [116/148], loss=82.0229
	step [117/148], loss=87.8144
	step [118/148], loss=92.7912
	step [119/148], loss=83.1963
	step [120/148], loss=98.1066
	step [121/148], loss=61.3058
	step [122/148], loss=87.7874
	step [123/148], loss=78.2499
	step [124/148], loss=78.1879
	step [125/148], loss=72.0766
	step [126/148], loss=77.6653
	step [127/148], loss=94.3933
	step [128/148], loss=78.9024
	step [129/148], loss=82.5699
	step [130/148], loss=66.8787
	step [131/148], loss=76.2391
	step [132/148], loss=70.9474
	step [133/148], loss=76.6268
	step [134/148], loss=78.6428
	step [135/148], loss=88.9027
	step [136/148], loss=76.5435
	step [137/148], loss=86.8592
	step [138/148], loss=79.9208
	step [139/148], loss=75.9837
	step [140/148], loss=83.3346
	step [141/148], loss=65.6226
	step [142/148], loss=90.9438
	step [143/148], loss=87.3139
	step [144/148], loss=75.1208
	step [145/148], loss=81.7292
	step [146/148], loss=76.9222
	step [147/148], loss=82.3331
	step [148/148], loss=5.4294
	Evaluating
	loss=0.0117, precision=0.3122, recall=0.8572, f1=0.4577
Training epoch 98
	step [1/148], loss=57.1099
	step [2/148], loss=75.3785
	step [3/148], loss=70.4767
	step [4/148], loss=86.8074
	step [5/148], loss=81.4309
	step [6/148], loss=78.0830
	step [7/148], loss=81.7221
	step [8/148], loss=78.7373
	step [9/148], loss=69.6681
	step [10/148], loss=80.8805
	step [11/148], loss=79.7512
	step [12/148], loss=86.6782
	step [13/148], loss=87.4284
	step [14/148], loss=77.7214
	step [15/148], loss=65.3863
	step [16/148], loss=72.5699
	step [17/148], loss=92.3902
	step [18/148], loss=86.9919
	step [19/148], loss=86.9482
	step [20/148], loss=73.3711
	step [21/148], loss=73.4352
	step [22/148], loss=76.2023
	step [23/148], loss=87.6420
	step [24/148], loss=86.6765
	step [25/148], loss=77.8614
	step [26/148], loss=87.5914
	step [27/148], loss=86.4261
	step [28/148], loss=93.3105
	step [29/148], loss=69.2056
	step [30/148], loss=78.6864
	step [31/148], loss=74.0861
	step [32/148], loss=79.9419
	step [33/148], loss=70.9830
	step [34/148], loss=66.0585
	step [35/148], loss=79.0177
	step [36/148], loss=71.3296
	step [37/148], loss=84.3847
	step [38/148], loss=81.9499
	step [39/148], loss=90.7202
	step [40/148], loss=81.1693
	step [41/148], loss=73.5706
	step [42/148], loss=69.9784
	step [43/148], loss=78.7449
	step [44/148], loss=90.4432
	step [45/148], loss=75.4851
	step [46/148], loss=68.1655
	step [47/148], loss=67.6404
	step [48/148], loss=87.7912
	step [49/148], loss=91.5324
	step [50/148], loss=81.0546
	step [51/148], loss=60.2282
	step [52/148], loss=83.9106
	step [53/148], loss=78.1249
	step [54/148], loss=83.6067
	step [55/148], loss=70.5710
	step [56/148], loss=87.6640
	step [57/148], loss=88.3566
	step [58/148], loss=73.8891
	step [59/148], loss=83.8736
	step [60/148], loss=74.0681
	step [61/148], loss=82.3846
	step [62/148], loss=63.4429
	step [63/148], loss=71.9582
	step [64/148], loss=83.4955
	step [65/148], loss=63.4511
	step [66/148], loss=86.4543
	step [67/148], loss=79.5799
	step [68/148], loss=72.9998
	step [69/148], loss=70.5278
	step [70/148], loss=86.5653
	step [71/148], loss=83.2934
	step [72/148], loss=87.2940
	step [73/148], loss=85.1789
	step [74/148], loss=72.1132
	step [75/148], loss=85.4395
	step [76/148], loss=70.7289
	step [77/148], loss=83.8174
	step [78/148], loss=83.3422
	step [79/148], loss=83.6786
	step [80/148], loss=91.5240
	step [81/148], loss=96.1679
	step [82/148], loss=84.6632
	step [83/148], loss=76.4486
	step [84/148], loss=103.1041
	step [85/148], loss=75.5950
	step [86/148], loss=74.3672
	step [87/148], loss=80.4798
	step [88/148], loss=80.3715
	step [89/148], loss=83.7272
	step [90/148], loss=70.3145
	step [91/148], loss=79.8292
	step [92/148], loss=84.9540
	step [93/148], loss=90.5283
	step [94/148], loss=86.5725
	step [95/148], loss=98.6880
	step [96/148], loss=83.3984
	step [97/148], loss=78.5368
	step [98/148], loss=73.9955
	step [99/148], loss=86.8669
	step [100/148], loss=79.1070
	step [101/148], loss=79.2853
	step [102/148], loss=66.6954
	step [103/148], loss=84.1997
	step [104/148], loss=96.2970
	step [105/148], loss=69.0972
	step [106/148], loss=103.1637
	step [107/148], loss=83.5734
	step [108/148], loss=81.1974
	step [109/148], loss=86.0646
	step [110/148], loss=89.1671
	step [111/148], loss=83.6782
	step [112/148], loss=70.6110
	step [113/148], loss=94.5564
	step [114/148], loss=78.9565
	step [115/148], loss=86.2784
	step [116/148], loss=80.9809
	step [117/148], loss=95.8299
	step [118/148], loss=85.8140
	step [119/148], loss=83.6998
	step [120/148], loss=67.5232
	step [121/148], loss=85.4320
	step [122/148], loss=81.4664
	step [123/148], loss=72.8164
	step [124/148], loss=76.2896
	step [125/148], loss=77.1558
	step [126/148], loss=74.2330
	step [127/148], loss=72.4566
	step [128/148], loss=77.6332
	step [129/148], loss=77.7029
	step [130/148], loss=73.8946
	step [131/148], loss=78.5346
	step [132/148], loss=77.1083
	step [133/148], loss=71.5632
	step [134/148], loss=88.1387
	step [135/148], loss=86.6328
	step [136/148], loss=80.7589
	step [137/148], loss=62.2564
	step [138/148], loss=76.7886
	step [139/148], loss=84.7378
	step [140/148], loss=86.1362
	step [141/148], loss=87.9487
	step [142/148], loss=88.2744
	step [143/148], loss=88.9006
	step [144/148], loss=87.9659
	step [145/148], loss=75.3258
	step [146/148], loss=92.1648
	step [147/148], loss=78.1315
	step [148/148], loss=5.3108
	Evaluating
	loss=0.0104, precision=0.3465, recall=0.8502, f1=0.4924
Training epoch 99
	step [1/148], loss=75.6779
	step [2/148], loss=75.0477
	step [3/148], loss=73.7718
	step [4/148], loss=79.6153
	step [5/148], loss=80.1454
	step [6/148], loss=71.3284
	step [7/148], loss=91.1799
	step [8/148], loss=77.0050
	step [9/148], loss=67.7712
	step [10/148], loss=70.9924
	step [11/148], loss=88.1058
	step [12/148], loss=79.3824
	step [13/148], loss=92.5533
	step [14/148], loss=89.5518
	step [15/148], loss=80.7693
	step [16/148], loss=79.3918
	step [17/148], loss=70.6317
	step [18/148], loss=77.4440
	step [19/148], loss=80.5711
	step [20/148], loss=93.8221
	step [21/148], loss=75.9528
	step [22/148], loss=80.6948
	step [23/148], loss=72.4861
	step [24/148], loss=80.3655
	step [25/148], loss=71.0797
	step [26/148], loss=97.1457
	step [27/148], loss=83.7784
	step [28/148], loss=78.4436
	step [29/148], loss=81.4661
	step [30/148], loss=66.2454
	step [31/148], loss=89.1227
	step [32/148], loss=85.9218
	step [33/148], loss=81.4611
	step [34/148], loss=92.1603
	step [35/148], loss=90.6640
	step [36/148], loss=65.4637
	step [37/148], loss=75.2175
	step [38/148], loss=74.2886
	step [39/148], loss=75.7293
	step [40/148], loss=82.3369
	step [41/148], loss=74.8398
	step [42/148], loss=73.8350
	step [43/148], loss=80.9003
	step [44/148], loss=83.7032
	step [45/148], loss=76.1817
	step [46/148], loss=93.3336
	step [47/148], loss=88.1201
	step [48/148], loss=80.5121
	step [49/148], loss=83.0905
	step [50/148], loss=89.6388
	step [51/148], loss=81.7204
	step [52/148], loss=73.9386
	step [53/148], loss=84.8018
	step [54/148], loss=69.4443
	step [55/148], loss=79.0118
	step [56/148], loss=82.3525
	step [57/148], loss=80.3514
	step [58/148], loss=78.7867
	step [59/148], loss=77.0318
	step [60/148], loss=80.2845
	step [61/148], loss=68.7062
	step [62/148], loss=89.5048
	step [63/148], loss=63.7188
	step [64/148], loss=81.4438
	step [65/148], loss=82.5829
	step [66/148], loss=92.6785
	step [67/148], loss=88.8564
	step [68/148], loss=64.9099
	step [69/148], loss=84.8308
	step [70/148], loss=85.7675
	step [71/148], loss=91.4202
	step [72/148], loss=80.0553
	step [73/148], loss=79.1464
	step [74/148], loss=71.1375
	step [75/148], loss=74.4510
	step [76/148], loss=87.7959
	step [77/148], loss=61.4083
	step [78/148], loss=76.3535
	step [79/148], loss=76.2284
	step [80/148], loss=82.3669
	step [81/148], loss=90.0403
	step [82/148], loss=75.9818
	step [83/148], loss=82.1414
	step [84/148], loss=81.6105
	step [85/148], loss=91.1296
	step [86/148], loss=95.7655
	step [87/148], loss=86.1785
	step [88/148], loss=102.3156
	step [89/148], loss=86.9750
	step [90/148], loss=84.9595
	step [91/148], loss=75.0150
	step [92/148], loss=76.2048
	step [93/148], loss=70.9019
	step [94/148], loss=71.6965
	step [95/148], loss=76.4297
	step [96/148], loss=82.8610
	step [97/148], loss=74.2518
	step [98/148], loss=72.7581
	step [99/148], loss=80.7328
	step [100/148], loss=86.7529
	step [101/148], loss=76.8109
	step [102/148], loss=60.2376
	step [103/148], loss=98.8097
	step [104/148], loss=84.4401
	step [105/148], loss=78.7247
	step [106/148], loss=66.7670
	step [107/148], loss=80.3058
	step [108/148], loss=83.2044
	step [109/148], loss=73.7159
	step [110/148], loss=87.5917
	step [111/148], loss=84.0522
	step [112/148], loss=75.5822
	step [113/148], loss=88.8895
	step [114/148], loss=75.5283
	step [115/148], loss=97.3533
	step [116/148], loss=75.2276
	step [117/148], loss=94.8500
	step [118/148], loss=90.1821
	step [119/148], loss=75.1248
	step [120/148], loss=75.2176
	step [121/148], loss=65.5324
	step [122/148], loss=82.5234
	step [123/148], loss=86.3882
	step [124/148], loss=80.0003
	step [125/148], loss=70.4447
	step [126/148], loss=72.0449
	step [127/148], loss=78.7384
	step [128/148], loss=73.9057
	step [129/148], loss=79.9837
	step [130/148], loss=68.1018
	step [131/148], loss=90.7187
	step [132/148], loss=77.8177
	step [133/148], loss=78.3507
	step [134/148], loss=83.3492
	step [135/148], loss=98.4010
	step [136/148], loss=72.9208
	step [137/148], loss=82.6782
	step [138/148], loss=67.4327
	step [139/148], loss=78.9618
	step [140/148], loss=87.7140
	step [141/148], loss=86.0256
	step [142/148], loss=77.7035
	step [143/148], loss=77.9402
	step [144/148], loss=85.6869
	step [145/148], loss=78.3674
	step [146/148], loss=67.5351
	step [147/148], loss=84.8193
	step [148/148], loss=11.0563
	Evaluating
	loss=0.0138, precision=0.2308, recall=0.8454, f1=0.3627
Training epoch 100
	step [1/148], loss=68.4195
	step [2/148], loss=94.2699
	step [3/148], loss=94.8294
	step [4/148], loss=75.1197
	step [5/148], loss=64.2835
	step [6/148], loss=75.2202
	step [7/148], loss=81.2282
	step [8/148], loss=85.0008
	step [9/148], loss=88.5530
	step [10/148], loss=78.8422
	step [11/148], loss=74.1773
	step [12/148], loss=73.9020
	step [13/148], loss=83.4455
	step [14/148], loss=85.7673
	step [15/148], loss=76.8298
	step [16/148], loss=80.0149
	step [17/148], loss=84.2340
	step [18/148], loss=67.3134
	step [19/148], loss=86.2576
	step [20/148], loss=88.8609
	step [21/148], loss=75.2807
	step [22/148], loss=91.3347
	step [23/148], loss=71.5751
	step [24/148], loss=76.4741
	step [25/148], loss=79.2496
	step [26/148], loss=78.3256
	step [27/148], loss=75.3961
	step [28/148], loss=88.1277
	step [29/148], loss=66.6329
	step [30/148], loss=80.6103
	step [31/148], loss=73.6525
	step [32/148], loss=95.4097
	step [33/148], loss=81.6360
	step [34/148], loss=63.9112
	step [35/148], loss=89.0427
	step [36/148], loss=75.1020
	step [37/148], loss=67.7038
	step [38/148], loss=78.6395
	step [39/148], loss=71.9889
	step [40/148], loss=79.5611
	step [41/148], loss=84.7727
	step [42/148], loss=81.9561
	step [43/148], loss=84.9677
	step [44/148], loss=82.2807
	step [45/148], loss=80.4697
	step [46/148], loss=81.5059
	step [47/148], loss=84.9359
	step [48/148], loss=82.4319
	step [49/148], loss=81.2207
	step [50/148], loss=89.7906
	step [51/148], loss=86.8453
	step [52/148], loss=74.5982
	step [53/148], loss=82.9377
	step [54/148], loss=85.5771
	step [55/148], loss=82.7949
	step [56/148], loss=79.1733
	step [57/148], loss=74.7600
	step [58/148], loss=97.3032
	step [59/148], loss=93.9974
	step [60/148], loss=71.9909
	step [61/148], loss=88.8320
	step [62/148], loss=87.8234
	step [63/148], loss=90.4908
	step [64/148], loss=72.6486
	step [65/148], loss=87.0414
	step [66/148], loss=78.5759
	step [67/148], loss=68.8997
	step [68/148], loss=68.1345
	step [69/148], loss=70.8762
	step [70/148], loss=74.8054
	step [71/148], loss=79.1873
	step [72/148], loss=71.7977
	step [73/148], loss=73.8951
	step [74/148], loss=76.6017
	step [75/148], loss=83.4906
	step [76/148], loss=72.9397
	step [77/148], loss=77.0969
	step [78/148], loss=66.1056
	step [79/148], loss=76.8997
	step [80/148], loss=73.9055
	step [81/148], loss=64.0498
	step [82/148], loss=90.7537
	step [83/148], loss=75.5810
	step [84/148], loss=77.8800
	step [85/148], loss=81.6851
	step [86/148], loss=81.8874
	step [87/148], loss=73.1966
	step [88/148], loss=77.5285
	step [89/148], loss=78.3211
	step [90/148], loss=91.6398
	step [91/148], loss=81.4888
	step [92/148], loss=70.4819
	step [93/148], loss=77.0925
	step [94/148], loss=78.8497
	step [95/148], loss=81.0066
	step [96/148], loss=88.8452
	step [97/148], loss=77.3073
	step [98/148], loss=75.3163
	step [99/148], loss=82.6814
	step [100/148], loss=78.6119
	step [101/148], loss=106.6320
	step [102/148], loss=78.8930
	step [103/148], loss=74.8293
	step [104/148], loss=87.3986
	step [105/148], loss=89.3844
	step [106/148], loss=74.4812
	step [107/148], loss=79.2626
	step [108/148], loss=96.1817
	step [109/148], loss=77.4679
	step [110/148], loss=67.2878
	step [111/148], loss=80.8584
	step [112/148], loss=85.2276
	step [113/148], loss=65.5229
	step [114/148], loss=90.0093
	step [115/148], loss=66.7542
	step [116/148], loss=91.4220
	step [117/148], loss=80.3645
	step [118/148], loss=70.1711
	step [119/148], loss=84.9662
	step [120/148], loss=77.8941
	step [121/148], loss=76.3786
	step [122/148], loss=71.7927
	step [123/148], loss=76.0789
	step [124/148], loss=63.8365
	step [125/148], loss=82.8587
	step [126/148], loss=86.2080
	step [127/148], loss=82.7772
	step [128/148], loss=79.6721
	step [129/148], loss=68.7317
	step [130/148], loss=82.9145
	step [131/148], loss=92.8626
	step [132/148], loss=96.3801
	step [133/148], loss=79.8419
	step [134/148], loss=82.2986
	step [135/148], loss=80.1984
	step [136/148], loss=74.4547
	step [137/148], loss=78.9546
	step [138/148], loss=67.0081
	step [139/148], loss=91.7725
	step [140/148], loss=65.3571
	step [141/148], loss=91.9314
	step [142/148], loss=85.8136
	step [143/148], loss=76.5030
	step [144/148], loss=75.5104
	step [145/148], loss=80.0532
	step [146/148], loss=90.2691
	step [147/148], loss=75.7525
	step [148/148], loss=1.5556
	Evaluating
	loss=0.0106, precision=0.3346, recall=0.8448, f1=0.4794
Training epoch 101
	step [1/148], loss=84.6228
	step [2/148], loss=65.9994
	step [3/148], loss=77.1606
	step [4/148], loss=72.8972
	step [5/148], loss=86.8422
	step [6/148], loss=89.0524
	step [7/148], loss=67.1001
	step [8/148], loss=72.2756
	step [9/148], loss=70.9359
	step [10/148], loss=69.7050
	step [11/148], loss=69.7908
	step [12/148], loss=66.7929
	step [13/148], loss=95.2889
	step [14/148], loss=81.0624
	step [15/148], loss=89.9298
	step [16/148], loss=83.2968
	step [17/148], loss=66.7146
	step [18/148], loss=90.1605
	step [19/148], loss=80.4535
	step [20/148], loss=92.4079
	step [21/148], loss=90.8152
	step [22/148], loss=81.8042
	step [23/148], loss=82.0647
	step [24/148], loss=82.5943
	step [25/148], loss=85.9564
	step [26/148], loss=68.0259
	step [27/148], loss=78.1007
	step [28/148], loss=88.7663
	step [29/148], loss=95.5058
	step [30/148], loss=81.7283
	step [31/148], loss=77.3662
	step [32/148], loss=72.1650
	step [33/148], loss=82.6291
	step [34/148], loss=74.9724
	step [35/148], loss=70.4330
	step [36/148], loss=89.7047
	step [37/148], loss=77.3274
	step [38/148], loss=75.9902
	step [39/148], loss=75.5137
	step [40/148], loss=75.8152
	step [41/148], loss=81.0803
	step [42/148], loss=88.3870
	step [43/148], loss=96.2627
	step [44/148], loss=70.5490
	step [45/148], loss=96.0842
	step [46/148], loss=71.3015
	step [47/148], loss=76.7231
	step [48/148], loss=79.9035
	step [49/148], loss=79.0003
	step [50/148], loss=73.5467
	step [51/148], loss=79.3882
	step [52/148], loss=84.5387
	step [53/148], loss=81.1441
	step [54/148], loss=78.5514
	step [55/148], loss=76.8005
	step [56/148], loss=71.2360
	step [57/148], loss=82.0660
	step [58/148], loss=74.6325
	step [59/148], loss=84.4384
	step [60/148], loss=74.0696
	step [61/148], loss=75.2102
	step [62/148], loss=82.0707
	step [63/148], loss=79.8931
	step [64/148], loss=81.5090
	step [65/148], loss=76.8871
	step [66/148], loss=74.7010
	step [67/148], loss=62.6888
	step [68/148], loss=80.0273
	step [69/148], loss=85.4861
	step [70/148], loss=80.8193
	step [71/148], loss=88.3891
	step [72/148], loss=89.6749
	step [73/148], loss=84.6029
	step [74/148], loss=75.5939
	step [75/148], loss=94.5815
	step [76/148], loss=91.9384
	step [77/148], loss=91.3015
	step [78/148], loss=82.6284
	step [79/148], loss=71.6548
	step [80/148], loss=90.2533
	step [81/148], loss=89.6668
	step [82/148], loss=77.5770
	step [83/148], loss=74.3429
	step [84/148], loss=83.7564
	step [85/148], loss=69.5074
	step [86/148], loss=93.1996
	step [87/148], loss=90.7295
	step [88/148], loss=68.7550
	step [89/148], loss=87.7216
	step [90/148], loss=81.6096
	step [91/148], loss=101.1895
	step [92/148], loss=65.5293
	step [93/148], loss=83.7273
	step [94/148], loss=100.1248
	step [95/148], loss=82.2042
	step [96/148], loss=77.8860
	step [97/148], loss=71.1409
	step [98/148], loss=79.9314
	step [99/148], loss=72.7230
	step [100/148], loss=68.6129
	step [101/148], loss=79.5660
	step [102/148], loss=71.2001
	step [103/148], loss=72.8095
	step [104/148], loss=84.5469
	step [105/148], loss=75.5146
	step [106/148], loss=70.1861
	step [107/148], loss=82.9946
	step [108/148], loss=81.1671
	step [109/148], loss=88.9151
	step [110/148], loss=76.6485
	step [111/148], loss=92.7714
	step [112/148], loss=79.9473
	step [113/148], loss=83.8622
	step [114/148], loss=70.5482
	step [115/148], loss=80.9910
	step [116/148], loss=87.4185
	step [117/148], loss=80.6256
	step [118/148], loss=76.5124
	step [119/148], loss=84.1140
	step [120/148], loss=76.5182
	step [121/148], loss=83.7392
	step [122/148], loss=89.7572
	step [123/148], loss=79.8165
	step [124/148], loss=62.9600
	step [125/148], loss=81.6128
	step [126/148], loss=66.6759
	step [127/148], loss=72.6125
	step [128/148], loss=67.9024
	step [129/148], loss=74.7891
	step [130/148], loss=71.6135
	step [131/148], loss=65.2602
	step [132/148], loss=95.0868
	step [133/148], loss=76.2186
	step [134/148], loss=57.5821
	step [135/148], loss=70.4457
	step [136/148], loss=87.8076
	step [137/148], loss=86.7587
	step [138/148], loss=88.4910
	step [139/148], loss=76.9186
	step [140/148], loss=79.0497
	step [141/148], loss=70.0817
	step [142/148], loss=88.1592
	step [143/148], loss=80.6504
	step [144/148], loss=75.4077
	step [145/148], loss=77.5542
	step [146/148], loss=70.8603
	step [147/148], loss=93.7616
	step [148/148], loss=8.6689
	Evaluating
	loss=0.0129, precision=0.2546, recall=0.8429, f1=0.3911
Training epoch 102
	step [1/148], loss=71.4012
	step [2/148], loss=70.9078
	step [3/148], loss=78.2374
	step [4/148], loss=80.0072
	step [5/148], loss=78.1744
	step [6/148], loss=81.7979
	step [7/148], loss=86.8481
	step [8/148], loss=85.0511
	step [9/148], loss=67.7182
	step [10/148], loss=91.0723
	step [11/148], loss=67.9773
	step [12/148], loss=76.5152
	step [13/148], loss=81.0620
	step [14/148], loss=88.4478
	step [15/148], loss=78.3031
	step [16/148], loss=94.2336
	step [17/148], loss=93.1454
	step [18/148], loss=75.6263
	step [19/148], loss=76.0053
	step [20/148], loss=75.4403
	step [21/148], loss=89.4919
	step [22/148], loss=78.1987
	step [23/148], loss=86.8422
	step [24/148], loss=74.1872
	step [25/148], loss=96.2303
	step [26/148], loss=66.9951
	step [27/148], loss=95.4359
	step [28/148], loss=78.6623
	step [29/148], loss=71.3987
	step [30/148], loss=50.4320
	step [31/148], loss=94.2524
	step [32/148], loss=67.8217
	step [33/148], loss=82.3465
	step [34/148], loss=74.1589
	step [35/148], loss=71.3742
	step [36/148], loss=113.7526
	step [37/148], loss=73.7951
	step [38/148], loss=79.7378
	step [39/148], loss=83.6458
	step [40/148], loss=79.7279
	step [41/148], loss=75.2380
	step [42/148], loss=65.9955
	step [43/148], loss=70.9411
	step [44/148], loss=79.0850
	step [45/148], loss=84.8182
	step [46/148], loss=84.5024
	step [47/148], loss=93.6784
	step [48/148], loss=65.2802
	step [49/148], loss=82.0125
	step [50/148], loss=79.0972
	step [51/148], loss=82.4506
	step [52/148], loss=86.7480
	step [53/148], loss=76.9188
	step [54/148], loss=86.3130
	step [55/148], loss=76.5057
	step [56/148], loss=79.8538
	step [57/148], loss=85.8794
	step [58/148], loss=68.5690
	step [59/148], loss=72.9247
	step [60/148], loss=87.0512
	step [61/148], loss=73.1366
	step [62/148], loss=84.9686
	step [63/148], loss=81.3445
	step [64/148], loss=72.1913
	step [65/148], loss=83.6616
	step [66/148], loss=77.5355
	step [67/148], loss=92.4648
	step [68/148], loss=101.5192
	step [69/148], loss=89.1104
	step [70/148], loss=95.5910
	step [71/148], loss=74.2940
	step [72/148], loss=79.2250
	step [73/148], loss=81.2582
	step [74/148], loss=84.8812
	step [75/148], loss=62.6660
	step [76/148], loss=78.5168
	step [77/148], loss=64.0863
	step [78/148], loss=71.6037
	step [79/148], loss=83.5603
	step [80/148], loss=87.3537
	step [81/148], loss=79.1610
	step [82/148], loss=93.5335
	step [83/148], loss=75.5560
	step [84/148], loss=68.3730
	step [85/148], loss=87.0999
	step [86/148], loss=94.4313
	step [87/148], loss=71.2246
	step [88/148], loss=67.0607
	step [89/148], loss=75.1839
	step [90/148], loss=73.6918
	step [91/148], loss=77.1253
	step [92/148], loss=88.9855
	step [93/148], loss=79.2762
	step [94/148], loss=74.8706
	step [95/148], loss=88.2469
	step [96/148], loss=68.1927
	step [97/148], loss=101.6288
	step [98/148], loss=78.3706
	step [99/148], loss=72.0931
	step [100/148], loss=76.8794
	step [101/148], loss=79.2532
	step [102/148], loss=74.8265
	step [103/148], loss=82.1516
	step [104/148], loss=65.7999
	step [105/148], loss=86.8244
	step [106/148], loss=72.4433
	step [107/148], loss=89.2590
	step [108/148], loss=76.1071
	step [109/148], loss=89.1192
	step [110/148], loss=80.0510
	step [111/148], loss=98.6215
	step [112/148], loss=77.0495
	step [113/148], loss=81.9495
	step [114/148], loss=64.0576
	step [115/148], loss=67.2508
	step [116/148], loss=87.4514
	step [117/148], loss=77.0533
	step [118/148], loss=76.4739
	step [119/148], loss=83.7050
	step [120/148], loss=85.2928
	step [121/148], loss=77.5630
	step [122/148], loss=90.9748
	step [123/148], loss=71.7496
	step [124/148], loss=75.7324
	step [125/148], loss=94.1207
	step [126/148], loss=97.3582
	step [127/148], loss=72.4909
	step [128/148], loss=68.8698
	step [129/148], loss=76.4721
	step [130/148], loss=74.0839
	step [131/148], loss=83.1628
	step [132/148], loss=92.1050
	step [133/148], loss=88.2359
	step [134/148], loss=71.2882
	step [135/148], loss=81.0626
	step [136/148], loss=73.0840
	step [137/148], loss=97.6409
	step [138/148], loss=83.8478
	step [139/148], loss=79.3900
	step [140/148], loss=70.7916
	step [141/148], loss=85.7172
	step [142/148], loss=77.5039
	step [143/148], loss=76.6626
	step [144/148], loss=73.9284
	step [145/148], loss=75.9530
	step [146/148], loss=65.1936
	step [147/148], loss=73.7117
	step [148/148], loss=5.3089
	Evaluating
	loss=0.0102, precision=0.3462, recall=0.8417, f1=0.4906
Training epoch 103
	step [1/148], loss=86.9568
	step [2/148], loss=94.7309
	step [3/148], loss=67.0803
	step [4/148], loss=86.6744
	step [5/148], loss=93.4675
	step [6/148], loss=71.3020
	step [7/148], loss=72.6169
	step [8/148], loss=77.4148
	step [9/148], loss=89.8298
	step [10/148], loss=74.7484
	step [11/148], loss=89.8375
	step [12/148], loss=83.2259
	step [13/148], loss=79.7656
	step [14/148], loss=87.9171
	step [15/148], loss=74.6005
	step [16/148], loss=77.7644
	step [17/148], loss=82.3610
	step [18/148], loss=80.0767
	step [19/148], loss=73.6001
	step [20/148], loss=86.8024
	step [21/148], loss=71.0183
	step [22/148], loss=84.8985
	step [23/148], loss=73.9158
	step [24/148], loss=70.4172
	step [25/148], loss=78.3949
	step [26/148], loss=78.9420
	step [27/148], loss=87.5695
	step [28/148], loss=78.1289
	step [29/148], loss=73.9485
	step [30/148], loss=68.1898
	step [31/148], loss=64.5600
	step [32/148], loss=79.5329
	step [33/148], loss=75.2450
	step [34/148], loss=79.3923
	step [35/148], loss=73.6527
	step [36/148], loss=77.8494
	step [37/148], loss=73.5354
	step [38/148], loss=67.6277
	step [39/148], loss=81.0445
	step [40/148], loss=77.3630
	step [41/148], loss=83.7530
	step [42/148], loss=74.5358
	step [43/148], loss=74.2074
	step [44/148], loss=63.4497
	step [45/148], loss=94.1759
	step [46/148], loss=77.0131
	step [47/148], loss=77.3805
	step [48/148], loss=82.8613
	step [49/148], loss=91.2065
	step [50/148], loss=79.2039
	step [51/148], loss=82.3573
	step [52/148], loss=68.3479
	step [53/148], loss=83.7535
	step [54/148], loss=70.7077
	step [55/148], loss=72.3170
	step [56/148], loss=73.3252
	step [57/148], loss=95.8310
	step [58/148], loss=81.1017
	step [59/148], loss=61.8361
	step [60/148], loss=74.3133
	step [61/148], loss=70.3792
	step [62/148], loss=71.7868
	step [63/148], loss=86.1290
	step [64/148], loss=82.2086
	step [65/148], loss=77.3089
	step [66/148], loss=70.2536
	step [67/148], loss=68.8970
	step [68/148], loss=80.9168
	step [69/148], loss=71.8759
	step [70/148], loss=93.3983
	step [71/148], loss=81.3500
	step [72/148], loss=71.0046
	step [73/148], loss=80.0612
	step [74/148], loss=90.3393
	step [75/148], loss=79.5079
	step [76/148], loss=73.4046
	step [77/148], loss=74.6695
	step [78/148], loss=79.4916
	step [79/148], loss=77.9382
	step [80/148], loss=77.1828
	step [81/148], loss=67.5680
	step [82/148], loss=75.4168
	step [83/148], loss=81.1418
	step [84/148], loss=88.7119
	step [85/148], loss=84.8290
	step [86/148], loss=72.7249
	step [87/148], loss=78.2386
	step [88/148], loss=77.3955
	step [89/148], loss=69.4654
	step [90/148], loss=82.3629
	step [91/148], loss=91.7953
	step [92/148], loss=77.1205
	step [93/148], loss=82.7808
	step [94/148], loss=69.7235
	step [95/148], loss=79.1444
	step [96/148], loss=79.4395
	step [97/148], loss=71.1211
	step [98/148], loss=78.5593
	step [99/148], loss=83.8881
	step [100/148], loss=82.7029
	step [101/148], loss=81.8695
	step [102/148], loss=87.0984
	step [103/148], loss=94.1656
	step [104/148], loss=88.1869
	step [105/148], loss=86.8964
	step [106/148], loss=67.1735
	step [107/148], loss=78.4015
	step [108/148], loss=80.8120
	step [109/148], loss=75.5263
	step [110/148], loss=74.4413
	step [111/148], loss=83.0077
	step [112/148], loss=77.5710
	step [113/148], loss=90.4892
	step [114/148], loss=104.2115
	step [115/148], loss=67.0598
	step [116/148], loss=70.6433
	step [117/148], loss=77.4565
	step [118/148], loss=81.9702
	step [119/148], loss=81.2352
	step [120/148], loss=67.6780
	step [121/148], loss=90.6500
	step [122/148], loss=63.3825
	step [123/148], loss=93.8434
	step [124/148], loss=83.1564
	step [125/148], loss=72.8030
	step [126/148], loss=80.0133
	step [127/148], loss=78.1196
	step [128/148], loss=84.1759
	step [129/148], loss=88.3136
	step [130/148], loss=74.9625
	step [131/148], loss=89.8466
	step [132/148], loss=79.9872
	step [133/148], loss=88.7645
	step [134/148], loss=87.2661
	step [135/148], loss=61.5758
	step [136/148], loss=69.5579
	step [137/148], loss=74.5144
	step [138/148], loss=77.8451
	step [139/148], loss=92.6856
	step [140/148], loss=78.1729
	step [141/148], loss=95.3746
	step [142/148], loss=81.8817
	step [143/148], loss=71.3599
	step [144/148], loss=82.8921
	step [145/148], loss=88.0954
	step [146/148], loss=74.1051
	step [147/148], loss=86.6117
	step [148/148], loss=6.1813
	Evaluating
	loss=0.0110, precision=0.3187, recall=0.8285, f1=0.4604
Training epoch 104
	step [1/148], loss=90.1818
	step [2/148], loss=73.2373
	step [3/148], loss=84.5223
	step [4/148], loss=65.5425
	step [5/148], loss=80.5758
	step [6/148], loss=83.0380
	step [7/148], loss=75.8573
	step [8/148], loss=78.6975
	step [9/148], loss=74.9842
	step [10/148], loss=82.5958
	step [11/148], loss=95.4772
	step [12/148], loss=70.5590
	step [13/148], loss=71.6340
	step [14/148], loss=73.0726
	step [15/148], loss=91.8390
	step [16/148], loss=80.9285
	step [17/148], loss=79.3346
	step [18/148], loss=81.1618
	step [19/148], loss=84.8893
	step [20/148], loss=80.1921
	step [21/148], loss=62.0554
	step [22/148], loss=77.6089
	step [23/148], loss=76.7488
	step [24/148], loss=79.5937
	step [25/148], loss=81.3841
	step [26/148], loss=80.5768
	step [27/148], loss=75.5902
	step [28/148], loss=74.5450
	step [29/148], loss=75.0917
	step [30/148], loss=78.6202
	step [31/148], loss=76.1330
	step [32/148], loss=77.1062
	step [33/148], loss=69.3514
	step [34/148], loss=80.0496
	step [35/148], loss=78.0516
	step [36/148], loss=87.5228
	step [37/148], loss=91.9370
	step [38/148], loss=63.2682
	step [39/148], loss=74.3017
	step [40/148], loss=62.0252
	step [41/148], loss=78.0420
	step [42/148], loss=68.7466
	step [43/148], loss=70.4784
	step [44/148], loss=74.9294
	step [45/148], loss=94.8790
	step [46/148], loss=77.5602
	step [47/148], loss=64.7275
	step [48/148], loss=82.1771
	step [49/148], loss=87.7622
	step [50/148], loss=73.4638
	step [51/148], loss=68.0076
	step [52/148], loss=86.2106
	step [53/148], loss=83.1989
	step [54/148], loss=80.2821
	step [55/148], loss=83.5098
	step [56/148], loss=87.5047
	step [57/148], loss=66.0198
	step [58/148], loss=72.9766
	step [59/148], loss=75.3702
	step [60/148], loss=69.1058
	step [61/148], loss=76.5752
	step [62/148], loss=83.6555
	step [63/148], loss=86.6956
	step [64/148], loss=90.3138
	step [65/148], loss=79.3489
	step [66/148], loss=78.1672
	step [67/148], loss=91.9290
	step [68/148], loss=67.1539
	step [69/148], loss=66.5920
	step [70/148], loss=84.0889
	step [71/148], loss=64.3412
	step [72/148], loss=55.9374
	step [73/148], loss=71.1353
	step [74/148], loss=87.4920
	step [75/148], loss=82.3577
	step [76/148], loss=75.7564
	step [77/148], loss=84.1380
	step [78/148], loss=78.9449
	step [79/148], loss=84.3763
	step [80/148], loss=83.5080
	step [81/148], loss=98.7430
	step [82/148], loss=92.1165
	step [83/148], loss=71.1645
	step [84/148], loss=82.9052
	step [85/148], loss=83.5204
	step [86/148], loss=80.6750
	step [87/148], loss=80.0523
	step [88/148], loss=89.9090
	step [89/148], loss=76.4289
	step [90/148], loss=92.0023
	step [91/148], loss=84.1139
	step [92/148], loss=81.4493
	step [93/148], loss=92.5294
	step [94/148], loss=68.8324
	step [95/148], loss=80.6420
	step [96/148], loss=75.4536
	step [97/148], loss=85.4747
	step [98/148], loss=79.8208
	step [99/148], loss=77.2716
	step [100/148], loss=72.7060
	step [101/148], loss=83.0367
	step [102/148], loss=74.0954
	step [103/148], loss=73.0174
	step [104/148], loss=67.5469
	step [105/148], loss=83.9796
	step [106/148], loss=97.8986
	step [107/148], loss=68.0553
	step [108/148], loss=76.9669
	step [109/148], loss=79.2782
	step [110/148], loss=85.7124
	step [111/148], loss=79.9968
	step [112/148], loss=79.1267
	step [113/148], loss=66.0425
	step [114/148], loss=76.4232
	step [115/148], loss=87.9948
	step [116/148], loss=88.3556
	step [117/148], loss=77.4557
	step [118/148], loss=93.9744
	step [119/148], loss=72.7669
	step [120/148], loss=74.4558
	step [121/148], loss=92.1768
	step [122/148], loss=79.9367
	step [123/148], loss=86.3221
	step [124/148], loss=75.0647
	step [125/148], loss=74.9527
	step [126/148], loss=75.2962
	step [127/148], loss=107.8314
	step [128/148], loss=78.5720
	step [129/148], loss=85.1796
	step [130/148], loss=86.9710
	step [131/148], loss=71.4306
	step [132/148], loss=81.1080
	step [133/148], loss=71.7124
	step [134/148], loss=79.8107
	step [135/148], loss=77.1594
	step [136/148], loss=74.8322
	step [137/148], loss=82.8129
	step [138/148], loss=85.7116
	step [139/148], loss=76.6441
	step [140/148], loss=79.2190
	step [141/148], loss=74.8944
	step [142/148], loss=77.3736
	step [143/148], loss=66.9945
	step [144/148], loss=86.3026
	step [145/148], loss=83.7664
	step [146/148], loss=82.6345
	step [147/148], loss=79.9034
	step [148/148], loss=6.1425
	Evaluating
	loss=0.0108, precision=0.2992, recall=0.8304, f1=0.4399
Training epoch 105
	step [1/148], loss=77.7153
	step [2/148], loss=70.6943
	step [3/148], loss=79.0791
	step [4/148], loss=74.6044
	step [5/148], loss=72.6978
	step [6/148], loss=79.2290
	step [7/148], loss=73.1786
	step [8/148], loss=64.9485
	step [9/148], loss=64.5056
	step [10/148], loss=83.3176
	step [11/148], loss=89.5842
	step [12/148], loss=80.6021
	step [13/148], loss=96.2988
	step [14/148], loss=94.8736
	step [15/148], loss=73.5122
	step [16/148], loss=82.0378
	step [17/148], loss=74.4424
	step [18/148], loss=83.3454
	step [19/148], loss=86.2436
	step [20/148], loss=90.8417
	step [21/148], loss=86.8272
	step [22/148], loss=77.6742
	step [23/148], loss=86.9022
	step [24/148], loss=76.9041
	step [25/148], loss=66.5314
	step [26/148], loss=85.3004
	step [27/148], loss=78.1705
	step [28/148], loss=87.3385
	step [29/148], loss=87.0939
	step [30/148], loss=68.6780
	step [31/148], loss=81.6881
	step [32/148], loss=85.9025
	step [33/148], loss=77.5233
	step [34/148], loss=87.1710
	step [35/148], loss=75.8435
	step [36/148], loss=77.5984
	step [37/148], loss=82.2184
	step [38/148], loss=80.2587
	step [39/148], loss=78.7995
	step [40/148], loss=84.1815
	step [41/148], loss=75.1643
	step [42/148], loss=76.3519
	step [43/148], loss=81.7226
	step [44/148], loss=80.9700
	step [45/148], loss=74.4104
	step [46/148], loss=86.6314
	step [47/148], loss=101.4841
	step [48/148], loss=63.4276
	step [49/148], loss=59.9972
	step [50/148], loss=82.7321
	step [51/148], loss=85.9156
	step [52/148], loss=57.6219
	step [53/148], loss=76.3059
	step [54/148], loss=89.3189
	step [55/148], loss=89.5452
	step [56/148], loss=89.4672
	step [57/148], loss=70.5408
	step [58/148], loss=65.0414
	step [59/148], loss=74.2910
	step [60/148], loss=91.4730
	step [61/148], loss=75.5367
	step [62/148], loss=71.1362
	step [63/148], loss=96.5793
	step [64/148], loss=75.5761
	step [65/148], loss=79.4103
	step [66/148], loss=67.6765
	step [67/148], loss=78.6800
	step [68/148], loss=80.5443
	step [69/148], loss=111.9232
	step [70/148], loss=77.6705
	step [71/148], loss=75.6678
	step [72/148], loss=63.9191
	step [73/148], loss=84.0306
	step [74/148], loss=82.1392
	step [75/148], loss=76.4781
	step [76/148], loss=82.6072
	step [77/148], loss=73.5667
	step [78/148], loss=84.4319
	step [79/148], loss=73.9014
	step [80/148], loss=61.5929
	step [81/148], loss=77.5374
	step [82/148], loss=70.5528
	step [83/148], loss=77.5141
	step [84/148], loss=75.6615
	step [85/148], loss=86.7714
	step [86/148], loss=79.8237
	step [87/148], loss=76.6435
	step [88/148], loss=92.0006
	step [89/148], loss=75.5277
	step [90/148], loss=94.6198
	step [91/148], loss=74.4875
	step [92/148], loss=115.3898
	step [93/148], loss=80.0768
	step [94/148], loss=85.5052
	step [95/148], loss=73.4428
	step [96/148], loss=85.9150
	step [97/148], loss=75.1509
	step [98/148], loss=78.1604
	step [99/148], loss=84.0040
	step [100/148], loss=66.9232
	step [101/148], loss=67.2096
	step [102/148], loss=75.8728
	step [103/148], loss=76.4800
	step [104/148], loss=85.5965
	step [105/148], loss=86.3044
	step [106/148], loss=76.1811
	step [107/148], loss=76.3156
	step [108/148], loss=81.3415
	step [109/148], loss=86.4897
	step [110/148], loss=81.8028
	step [111/148], loss=79.6420
	step [112/148], loss=76.7224
	step [113/148], loss=79.7668
	step [114/148], loss=91.1069
	step [115/148], loss=72.4737
	step [116/148], loss=73.4346
	step [117/148], loss=81.0079
	step [118/148], loss=80.5239
	step [119/148], loss=63.2649
	step [120/148], loss=76.2591
	step [121/148], loss=79.0945
	step [122/148], loss=63.4908
	step [123/148], loss=70.3912
	step [124/148], loss=69.3761
	step [125/148], loss=80.5143
	step [126/148], loss=82.4238
	step [127/148], loss=62.8373
	step [128/148], loss=80.6505
	step [129/148], loss=75.2161
	step [130/148], loss=79.1717
	step [131/148], loss=75.5303
	step [132/148], loss=81.9800
	step [133/148], loss=80.1397
	step [134/148], loss=82.7576
	step [135/148], loss=72.0936
	step [136/148], loss=61.1275
	step [137/148], loss=78.5023
	step [138/148], loss=81.6371
	step [139/148], loss=90.2347
	step [140/148], loss=65.8835
	step [141/148], loss=85.6853
	step [142/148], loss=67.5067
	step [143/148], loss=69.9607
	step [144/148], loss=70.5421
	step [145/148], loss=73.4662
	step [146/148], loss=73.5809
	step [147/148], loss=89.0091
	step [148/148], loss=10.0006
	Evaluating
	loss=0.0107, precision=0.3374, recall=0.8466, f1=0.4825
Training epoch 106
	step [1/148], loss=84.5150
	step [2/148], loss=82.8265
	step [3/148], loss=65.3856
	step [4/148], loss=57.5598
	step [5/148], loss=82.7571
	step [6/148], loss=91.3330
	step [7/148], loss=88.3741
	step [8/148], loss=77.3593
	step [9/148], loss=84.5294
	step [10/148], loss=71.6380
	step [11/148], loss=87.2042
	step [12/148], loss=92.6637
	step [13/148], loss=78.4590
	step [14/148], loss=78.6145
	step [15/148], loss=83.7880
	step [16/148], loss=69.4828
	step [17/148], loss=81.3248
	step [18/148], loss=73.3700
	step [19/148], loss=89.5264
	step [20/148], loss=84.7624
	step [21/148], loss=93.2718
	step [22/148], loss=85.8400
	step [23/148], loss=69.8227
	step [24/148], loss=88.5881
	step [25/148], loss=84.2457
	step [26/148], loss=81.9823
	step [27/148], loss=86.2527
	step [28/148], loss=66.0609
	step [29/148], loss=67.2909
	step [30/148], loss=71.3485
	step [31/148], loss=80.9322
	step [32/148], loss=85.2427
	step [33/148], loss=76.7926
	step [34/148], loss=71.2868
	step [35/148], loss=72.5697
	step [36/148], loss=64.1217
	step [37/148], loss=57.0049
	step [38/148], loss=79.4281
	step [39/148], loss=78.9319
	step [40/148], loss=78.6098
	step [41/148], loss=79.1699
	step [42/148], loss=85.2331
	step [43/148], loss=85.2044
	step [44/148], loss=73.5947
	step [45/148], loss=73.2867
	step [46/148], loss=71.3662
	step [47/148], loss=78.7697
	step [48/148], loss=64.6025
	step [49/148], loss=75.0966
	step [50/148], loss=77.2547
	step [51/148], loss=87.6288
	step [52/148], loss=80.3627
	step [53/148], loss=64.7819
	step [54/148], loss=77.0493
	step [55/148], loss=71.3434
	step [56/148], loss=65.6985
	step [57/148], loss=95.5898
	step [58/148], loss=75.1169
	step [59/148], loss=88.0753
	step [60/148], loss=75.9408
	step [61/148], loss=77.6702
	step [62/148], loss=77.0692
	step [63/148], loss=79.5416
	step [64/148], loss=77.5656
	step [65/148], loss=80.0208
	step [66/148], loss=75.4848
	step [67/148], loss=80.3771
	step [68/148], loss=75.9771
	step [69/148], loss=90.2125
	step [70/148], loss=92.5585
	step [71/148], loss=85.7077
	step [72/148], loss=72.4142
	step [73/148], loss=78.4223
	step [74/148], loss=78.6448
	step [75/148], loss=67.1078
	step [76/148], loss=86.1301
	step [77/148], loss=67.3688
	step [78/148], loss=80.1902
	step [79/148], loss=85.0392
	step [80/148], loss=98.4797
	step [81/148], loss=80.0914
	step [82/148], loss=67.6463
	step [83/148], loss=87.4392
	step [84/148], loss=78.6497
	step [85/148], loss=77.5096
	step [86/148], loss=81.3578
	step [87/148], loss=82.5980
	step [88/148], loss=87.4710
	step [89/148], loss=83.5592
	step [90/148], loss=74.9629
	step [91/148], loss=80.9132
	step [92/148], loss=85.2229
	step [93/148], loss=73.6499
	step [94/148], loss=67.3184
	step [95/148], loss=86.3030
	step [96/148], loss=66.8817
	step [97/148], loss=66.4961
	step [98/148], loss=84.1409
	step [99/148], loss=84.6474
	step [100/148], loss=80.6421
	step [101/148], loss=98.9243
	step [102/148], loss=66.1757
	step [103/148], loss=80.5691
	step [104/148], loss=76.4933
	step [105/148], loss=108.0730
	step [106/148], loss=60.7480
	step [107/148], loss=92.5938
	step [108/148], loss=74.5591
	step [109/148], loss=88.2041
	step [110/148], loss=81.1312
	step [111/148], loss=64.9695
	step [112/148], loss=76.5701
	step [113/148], loss=93.7454
	step [114/148], loss=93.4134
	step [115/148], loss=76.1285
	step [116/148], loss=79.5547
	step [117/148], loss=72.5227
	step [118/148], loss=85.2699
	step [119/148], loss=65.6708
	step [120/148], loss=81.2595
	step [121/148], loss=67.0659
	step [122/148], loss=90.5761
	step [123/148], loss=74.5147
	step [124/148], loss=79.5394
	step [125/148], loss=77.7505
	step [126/148], loss=72.5467
	step [127/148], loss=75.6228
	step [128/148], loss=73.4852
	step [129/148], loss=71.8200
	step [130/148], loss=76.1754
	step [131/148], loss=77.4416
	step [132/148], loss=91.5398
	step [133/148], loss=78.8145
	step [134/148], loss=71.7514
	step [135/148], loss=80.5393
	step [136/148], loss=77.3487
	step [137/148], loss=75.0735
	step [138/148], loss=79.3012
	step [139/148], loss=73.7866
	step [140/148], loss=69.4358
	step [141/148], loss=87.9419
	step [142/148], loss=78.9762
	step [143/148], loss=75.6922
	step [144/148], loss=87.1786
	step [145/148], loss=69.0407
	step [146/148], loss=70.4850
	step [147/148], loss=74.8043
	step [148/148], loss=6.0623
	Evaluating
	loss=0.0102, precision=0.3202, recall=0.8496, f1=0.4651
Training epoch 107
	step [1/148], loss=62.4245
	step [2/148], loss=75.7627
	step [3/148], loss=73.5962
	step [4/148], loss=65.3060
	step [5/148], loss=77.0502
	step [6/148], loss=77.5609
	step [7/148], loss=74.0950
	step [8/148], loss=83.4346
	step [9/148], loss=83.2981
	step [10/148], loss=85.9996
	step [11/148], loss=84.1463
	step [12/148], loss=72.8741
	step [13/148], loss=76.8778
	step [14/148], loss=63.2599
	step [15/148], loss=91.6056
	step [16/148], loss=78.1348
	step [17/148], loss=83.5590
	step [18/148], loss=78.7704
	step [19/148], loss=73.0934
	step [20/148], loss=82.2784
	step [21/148], loss=69.3407
	step [22/148], loss=77.1554
	step [23/148], loss=67.8702
	step [24/148], loss=67.0058
	step [25/148], loss=68.8740
	step [26/148], loss=82.1742
	step [27/148], loss=78.0509
	step [28/148], loss=69.2688
	step [29/148], loss=85.3572
	step [30/148], loss=75.3630
	step [31/148], loss=70.3155
	step [32/148], loss=84.7609
	step [33/148], loss=77.3132
	step [34/148], loss=86.8448
	step [35/148], loss=82.6928
	step [36/148], loss=86.1394
	step [37/148], loss=79.5086
	step [38/148], loss=76.5467
	step [39/148], loss=74.4740
	step [40/148], loss=70.9107
	step [41/148], loss=74.1293
	step [42/148], loss=80.7497
	step [43/148], loss=82.4106
	step [44/148], loss=66.3225
	step [45/148], loss=71.6086
	step [46/148], loss=76.3783
	step [47/148], loss=69.8904
	step [48/148], loss=85.5388
	step [49/148], loss=93.9597
	step [50/148], loss=75.6602
	step [51/148], loss=78.9773
	step [52/148], loss=83.7476
	step [53/148], loss=89.3590
	step [54/148], loss=83.1966
	step [55/148], loss=83.5654
	step [56/148], loss=78.7003
	step [57/148], loss=81.8889
	step [58/148], loss=69.8713
	step [59/148], loss=70.8591
	step [60/148], loss=74.3682
	step [61/148], loss=90.2496
	step [62/148], loss=86.6947
	step [63/148], loss=69.7007
	step [64/148], loss=81.1087
	step [65/148], loss=72.7607
	step [66/148], loss=78.7455
	step [67/148], loss=83.6577
	step [68/148], loss=70.3739
	step [69/148], loss=75.0239
	step [70/148], loss=83.2763
	step [71/148], loss=89.1289
	step [72/148], loss=82.6283
	step [73/148], loss=82.2571
	step [74/148], loss=87.3485
	step [75/148], loss=76.6460
	step [76/148], loss=71.7757
	step [77/148], loss=85.5465
	step [78/148], loss=77.1497
	step [79/148], loss=74.4939
	step [80/148], loss=86.7867
	step [81/148], loss=77.2604
	step [82/148], loss=79.2643
	step [83/148], loss=78.8814
	step [84/148], loss=71.5249
	step [85/148], loss=81.0086
	step [86/148], loss=90.7943
	step [87/148], loss=64.9995
	step [88/148], loss=80.1578
	step [89/148], loss=84.9421
	step [90/148], loss=83.2308
	step [91/148], loss=68.8593
	step [92/148], loss=71.6295
	step [93/148], loss=79.5333
	step [94/148], loss=81.9632
	step [95/148], loss=78.0708
	step [96/148], loss=79.4900
	step [97/148], loss=81.4651
	step [98/148], loss=71.7445
	step [99/148], loss=73.5613
	step [100/148], loss=75.8817
	step [101/148], loss=85.0495
	step [102/148], loss=89.9957
	step [103/148], loss=76.4744
	step [104/148], loss=82.5426
	step [105/148], loss=70.2603
	step [106/148], loss=73.9407
	step [107/148], loss=81.6020
	step [108/148], loss=78.0135
	step [109/148], loss=70.9103
	step [110/148], loss=83.8255
	step [111/148], loss=79.2631
	step [112/148], loss=75.9274
	step [113/148], loss=85.7963
	step [114/148], loss=73.4478
	step [115/148], loss=84.6775
	step [116/148], loss=84.4735
	step [117/148], loss=76.3076
	step [118/148], loss=82.5667
	step [119/148], loss=84.8880
	step [120/148], loss=77.7070
	step [121/148], loss=75.1829
	step [122/148], loss=75.5589
	step [123/148], loss=88.5733
	step [124/148], loss=68.4818
	step [125/148], loss=83.1968
	step [126/148], loss=89.1465
	step [127/148], loss=67.1781
	step [128/148], loss=73.7051
	step [129/148], loss=83.2718
	step [130/148], loss=80.0129
	step [131/148], loss=71.2545
	step [132/148], loss=72.3144
	step [133/148], loss=78.6128
	step [134/148], loss=73.6942
	step [135/148], loss=79.7032
	step [136/148], loss=87.2213
	step [137/148], loss=74.3944
	step [138/148], loss=75.9969
	step [139/148], loss=86.9668
	step [140/148], loss=72.5115
	step [141/148], loss=83.1823
	step [142/148], loss=77.6334
	step [143/148], loss=73.0416
	step [144/148], loss=90.9391
	step [145/148], loss=80.0260
	step [146/148], loss=70.6442
	step [147/148], loss=88.2049
	step [148/148], loss=5.1244
	Evaluating
	loss=0.0112, precision=0.3030, recall=0.8571, f1=0.4477
Training finished
best_f1: 0.5069425280470901
directing: X rim_enhanced: True test_id 1
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 9175 # image files with weight 9175
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 2708 # image files with weight 2708
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/X 9175
Using 4 GPUs
Going to train epochs [45-94]
Training epoch 45
	step [1/144], loss=106.2297
	step [2/144], loss=105.2566
	step [3/144], loss=97.2805
	step [4/144], loss=90.6821
	step [5/144], loss=77.2325
	step [6/144], loss=104.8584
	step [7/144], loss=86.9001
	step [8/144], loss=91.7399
	step [9/144], loss=101.9235
	step [10/144], loss=103.3895
	step [11/144], loss=83.5387
	step [12/144], loss=85.7817
	step [13/144], loss=94.5517
	step [14/144], loss=96.2223
	step [15/144], loss=100.8709
	step [16/144], loss=117.4811
	step [17/144], loss=93.0097
	step [18/144], loss=94.5039
	step [19/144], loss=93.7123
	step [20/144], loss=88.5624
	step [21/144], loss=82.9041
	step [22/144], loss=99.5787
	step [23/144], loss=88.9113
	step [24/144], loss=85.1967
	step [25/144], loss=84.7147
	step [26/144], loss=105.6215
	step [27/144], loss=81.3428
	step [28/144], loss=89.0103
	step [29/144], loss=93.3216
	step [30/144], loss=109.1611
	step [31/144], loss=87.6799
	step [32/144], loss=106.9508
	step [33/144], loss=104.9162
	step [34/144], loss=110.7742
	step [35/144], loss=90.5737
	step [36/144], loss=98.0602
	step [37/144], loss=97.7988
	step [38/144], loss=100.0686
	step [39/144], loss=108.5694
	step [40/144], loss=77.6894
	step [41/144], loss=92.3089
	step [42/144], loss=93.0611
	step [43/144], loss=104.4698
	step [44/144], loss=97.5236
	step [45/144], loss=89.1842
	step [46/144], loss=81.8443
	step [47/144], loss=102.8782
	step [48/144], loss=107.4323
	step [49/144], loss=94.1975
	step [50/144], loss=83.7811
	step [51/144], loss=98.0653
	step [52/144], loss=88.7635
	step [53/144], loss=90.5643
	step [54/144], loss=93.5237
	step [55/144], loss=97.1906
	step [56/144], loss=90.5504
	step [57/144], loss=98.6793
	step [58/144], loss=107.8312
	step [59/144], loss=104.3057
	step [60/144], loss=91.3702
	step [61/144], loss=86.0356
	step [62/144], loss=90.6619
	step [63/144], loss=96.4502
	step [64/144], loss=93.1383
	step [65/144], loss=86.9647
	step [66/144], loss=107.4208
	step [67/144], loss=88.0226
	step [68/144], loss=97.7532
	step [69/144], loss=98.3384
	step [70/144], loss=81.4955
	step [71/144], loss=104.0691
	step [72/144], loss=97.3673
	step [73/144], loss=104.8494
	step [74/144], loss=103.5218
	step [75/144], loss=98.4822
	step [76/144], loss=94.6143
	step [77/144], loss=99.1094
	step [78/144], loss=98.8580
	step [79/144], loss=102.4409
	step [80/144], loss=96.6997
	step [81/144], loss=98.4033
	step [82/144], loss=99.8389
	step [83/144], loss=99.2123
	step [84/144], loss=103.5167
	step [85/144], loss=106.3450
	step [86/144], loss=105.5919
	step [87/144], loss=93.0482
	step [88/144], loss=74.8176
	step [89/144], loss=93.3569
	step [90/144], loss=87.3616
	step [91/144], loss=100.7258
	step [92/144], loss=91.5345
	step [93/144], loss=93.0213
	step [94/144], loss=97.7545
	step [95/144], loss=97.6146
	step [96/144], loss=99.6973
	step [97/144], loss=85.3069
	step [98/144], loss=86.2454
	step [99/144], loss=90.4381
	step [100/144], loss=83.1068
	step [101/144], loss=91.4871
	step [102/144], loss=110.2084
	step [103/144], loss=95.9759
	step [104/144], loss=97.4927
	step [105/144], loss=86.1854
	step [106/144], loss=96.9328
	step [107/144], loss=80.6825
	step [108/144], loss=90.7086
	step [109/144], loss=111.9700
	step [110/144], loss=96.9781
	step [111/144], loss=95.5638
	step [112/144], loss=95.2673
	step [113/144], loss=90.1208
	step [114/144], loss=94.2360
	step [115/144], loss=106.4046
	step [116/144], loss=111.7528
	step [117/144], loss=76.3242
	step [118/144], loss=97.7997
	step [119/144], loss=71.4675
	step [120/144], loss=90.8574
	step [121/144], loss=80.6054
	step [122/144], loss=95.6870
	step [123/144], loss=89.3788
	step [124/144], loss=91.6867
	step [125/144], loss=98.8398
	step [126/144], loss=90.6991
	step [127/144], loss=81.8241
	step [128/144], loss=85.1946
	step [129/144], loss=97.8072
	step [130/144], loss=84.1543
	step [131/144], loss=113.8522
	step [132/144], loss=94.9529
	step [133/144], loss=104.3748
	step [134/144], loss=98.2022
	step [135/144], loss=88.0317
	step [136/144], loss=80.4110
	step [137/144], loss=82.7433
	step [138/144], loss=84.3971
	step [139/144], loss=84.8304
	step [140/144], loss=90.9339
	step [141/144], loss=81.7045
	step [142/144], loss=84.0476
	step [143/144], loss=101.6951
	step [144/144], loss=35.7692
	Evaluating
	loss=0.0174, precision=0.3248, recall=0.8408, f1=0.4686
saving model as: 1_saved_model.pth
Training epoch 46
	step [1/144], loss=97.1502
	step [2/144], loss=75.7472
	step [3/144], loss=82.4547
	step [4/144], loss=90.2338
	step [5/144], loss=89.8955
	step [6/144], loss=96.8435
	step [7/144], loss=114.4544
	step [8/144], loss=99.4758
	step [9/144], loss=90.4993
	step [10/144], loss=100.6886
	step [11/144], loss=104.3166
	step [12/144], loss=115.4389
	step [13/144], loss=115.0324
	step [14/144], loss=88.6292
	step [15/144], loss=91.5096
	step [16/144], loss=110.7611
	step [17/144], loss=109.1997
	step [18/144], loss=92.5609
	step [19/144], loss=79.1538
	step [20/144], loss=95.0114
	step [21/144], loss=96.4280
	step [22/144], loss=96.2543
	step [23/144], loss=80.2489
	step [24/144], loss=83.8217
	step [25/144], loss=89.5939
	step [26/144], loss=99.7470
	step [27/144], loss=88.5665
	step [28/144], loss=95.4240
	step [29/144], loss=101.3487
	step [30/144], loss=102.5674
	step [31/144], loss=85.5599
	step [32/144], loss=100.8579
	step [33/144], loss=87.0500
	step [34/144], loss=89.3248
	step [35/144], loss=96.6097
	step [36/144], loss=74.0641
	step [37/144], loss=94.8433
	step [38/144], loss=86.6385
	step [39/144], loss=92.2265
	step [40/144], loss=94.0309
	step [41/144], loss=85.9145
	step [42/144], loss=95.8677
	step [43/144], loss=95.3337
	step [44/144], loss=93.1976
	step [45/144], loss=77.3945
	step [46/144], loss=86.3782
	step [47/144], loss=73.3250
	step [48/144], loss=99.0016
	step [49/144], loss=97.6050
	step [50/144], loss=100.0063
	step [51/144], loss=73.3639
	step [52/144], loss=81.8845
	step [53/144], loss=105.9754
	step [54/144], loss=99.2014
	step [55/144], loss=90.2952
	step [56/144], loss=90.7963
	step [57/144], loss=88.3842
	step [58/144], loss=97.0486
	step [59/144], loss=91.7027
	step [60/144], loss=90.1327
	step [61/144], loss=81.5648
	step [62/144], loss=81.6825
	step [63/144], loss=77.8755
	step [64/144], loss=115.3799
	step [65/144], loss=79.9460
	step [66/144], loss=71.5573
	step [67/144], loss=86.5818
	step [68/144], loss=90.2368
	step [69/144], loss=101.5434
	step [70/144], loss=94.4313
	step [71/144], loss=84.1308
	step [72/144], loss=89.1419
	step [73/144], loss=93.1318
	step [74/144], loss=99.2627
	step [75/144], loss=91.9861
	step [76/144], loss=99.2324
	step [77/144], loss=83.8959
	step [78/144], loss=88.8933
	step [79/144], loss=111.1463
	step [80/144], loss=96.0116
	step [81/144], loss=113.7325
	step [82/144], loss=92.5864
	step [83/144], loss=91.1567
	step [84/144], loss=84.2769
	step [85/144], loss=91.9491
	step [86/144], loss=83.7077
	step [87/144], loss=101.6690
	step [88/144], loss=82.1348
	step [89/144], loss=97.1674
	step [90/144], loss=96.8190
	step [91/144], loss=106.8592
	step [92/144], loss=89.0614
	step [93/144], loss=88.6706
	step [94/144], loss=99.7370
	step [95/144], loss=80.2099
	step [96/144], loss=101.2146
	step [97/144], loss=98.7553
	step [98/144], loss=95.5389
	step [99/144], loss=98.6041
	step [100/144], loss=104.8537
	step [101/144], loss=98.3090
	step [102/144], loss=90.3746
	step [103/144], loss=99.8256
	step [104/144], loss=80.2224
	step [105/144], loss=106.3643
	step [106/144], loss=102.5762
	step [107/144], loss=104.3196
	step [108/144], loss=84.6380
	step [109/144], loss=88.8195
	step [110/144], loss=100.8985
	step [111/144], loss=93.6520
	step [112/144], loss=107.5808
	step [113/144], loss=97.1041
	step [114/144], loss=87.3542
	step [115/144], loss=91.9364
	step [116/144], loss=102.3495
	step [117/144], loss=97.2503
	step [118/144], loss=92.1064
	step [119/144], loss=90.5943
	step [120/144], loss=75.5205
	step [121/144], loss=95.7825
	step [122/144], loss=127.9431
	step [123/144], loss=80.8051
	step [124/144], loss=89.1622
	step [125/144], loss=93.4990
	step [126/144], loss=91.4406
	step [127/144], loss=87.4465
	step [128/144], loss=119.1855
	step [129/144], loss=99.5594
	step [130/144], loss=101.7246
	step [131/144], loss=89.5007
	step [132/144], loss=96.1603
	step [133/144], loss=88.5262
	step [134/144], loss=107.0905
	step [135/144], loss=112.1283
	step [136/144], loss=76.6651
	step [137/144], loss=94.9834
	step [138/144], loss=94.9005
	step [139/144], loss=91.7786
	step [140/144], loss=90.7188
	step [141/144], loss=116.2174
	step [142/144], loss=82.2176
	step [143/144], loss=94.9125
	step [144/144], loss=43.1218
	Evaluating
	loss=0.0222, precision=0.2601, recall=0.8649, f1=0.4000
Training epoch 47
	step [1/144], loss=101.5322
	step [2/144], loss=93.9880
	step [3/144], loss=94.5848
	step [4/144], loss=92.5565
	step [5/144], loss=87.9508
	step [6/144], loss=88.4566
	step [7/144], loss=90.0422
	step [8/144], loss=91.9115
	step [9/144], loss=94.8359
	step [10/144], loss=93.4506
	step [11/144], loss=92.9358
	step [12/144], loss=91.8628
	step [13/144], loss=91.0801
	step [14/144], loss=87.1956
	step [15/144], loss=100.7961
	step [16/144], loss=98.0126
	step [17/144], loss=97.0181
	step [18/144], loss=79.2650
	step [19/144], loss=96.2492
	step [20/144], loss=93.7715
	step [21/144], loss=87.6820
	step [22/144], loss=102.2079
	step [23/144], loss=92.9088
	step [24/144], loss=97.6463
	step [25/144], loss=89.5495
	step [26/144], loss=90.2984
	step [27/144], loss=89.2030
	step [28/144], loss=97.7643
	step [29/144], loss=94.5540
	step [30/144], loss=90.7970
	step [31/144], loss=102.8110
	step [32/144], loss=95.2203
	step [33/144], loss=92.9306
	step [34/144], loss=92.2998
	step [35/144], loss=104.4506
	step [36/144], loss=72.2078
	step [37/144], loss=91.3410
	step [38/144], loss=69.9753
	step [39/144], loss=96.8901
	step [40/144], loss=93.5379
	step [41/144], loss=97.1177
	step [42/144], loss=100.4273
	step [43/144], loss=102.5071
	step [44/144], loss=93.7615
	step [45/144], loss=91.7578
	step [46/144], loss=99.9341
	step [47/144], loss=102.6636
	step [48/144], loss=111.2811
	step [49/144], loss=89.8909
	step [50/144], loss=94.1145
	step [51/144], loss=89.9352
	step [52/144], loss=86.3483
	step [53/144], loss=90.8887
	step [54/144], loss=83.9250
	step [55/144], loss=87.1415
	step [56/144], loss=90.6091
	step [57/144], loss=89.7779
	step [58/144], loss=98.0219
	step [59/144], loss=85.5041
	step [60/144], loss=95.7843
	step [61/144], loss=104.4045
	step [62/144], loss=85.8862
	step [63/144], loss=89.3143
	step [64/144], loss=120.6581
	step [65/144], loss=100.2352
	step [66/144], loss=109.3082
	step [67/144], loss=99.9699
	step [68/144], loss=100.7645
	step [69/144], loss=91.6378
	step [70/144], loss=86.0261
	step [71/144], loss=95.4043
	step [72/144], loss=83.9333
	step [73/144], loss=95.5122
	step [74/144], loss=87.8858
	step [75/144], loss=108.9947
	step [76/144], loss=92.7591
	step [77/144], loss=81.1601
	step [78/144], loss=87.1726
	step [79/144], loss=96.7337
	step [80/144], loss=94.0789
	step [81/144], loss=93.2865
	step [82/144], loss=66.9849
	step [83/144], loss=94.9424
	step [84/144], loss=84.7214
	step [85/144], loss=100.6415
	step [86/144], loss=96.0471
	step [87/144], loss=95.8178
	step [88/144], loss=88.5748
	step [89/144], loss=110.2130
	step [90/144], loss=95.1384
	step [91/144], loss=85.1049
	step [92/144], loss=111.6660
	step [93/144], loss=85.8075
	step [94/144], loss=78.0920
	step [95/144], loss=82.7887
	step [96/144], loss=93.0235
	step [97/144], loss=114.1472
	step [98/144], loss=88.2629
	step [99/144], loss=86.5922
	step [100/144], loss=90.6860
	step [101/144], loss=99.7423
	step [102/144], loss=92.4592
	step [103/144], loss=86.7768
	step [104/144], loss=76.5936
	step [105/144], loss=102.6067
	step [106/144], loss=80.8341
	step [107/144], loss=93.4597
	step [108/144], loss=99.2175
	step [109/144], loss=97.9242
	step [110/144], loss=102.0033
	step [111/144], loss=115.4081
	step [112/144], loss=94.2345
	step [113/144], loss=86.9638
	step [114/144], loss=85.0309
	step [115/144], loss=110.3633
	step [116/144], loss=99.1034
	step [117/144], loss=90.7515
	step [118/144], loss=91.5862
	step [119/144], loss=79.8789
	step [120/144], loss=114.8782
	step [121/144], loss=99.8910
	step [122/144], loss=77.2340
	step [123/144], loss=85.4149
	step [124/144], loss=67.1459
	step [125/144], loss=104.3722
	step [126/144], loss=95.9846
	step [127/144], loss=101.6260
	step [128/144], loss=75.4707
	step [129/144], loss=85.5456
	step [130/144], loss=85.8866
	step [131/144], loss=88.4597
	step [132/144], loss=106.6684
	step [133/144], loss=105.4252
	step [134/144], loss=77.6743
	step [135/144], loss=85.5366
	step [136/144], loss=85.3887
	step [137/144], loss=85.7955
	step [138/144], loss=79.6228
	step [139/144], loss=80.4406
	step [140/144], loss=117.2258
	step [141/144], loss=118.3842
	step [142/144], loss=84.9671
	step [143/144], loss=88.0068
	step [144/144], loss=39.2233
	Evaluating
	loss=0.0207, precision=0.2699, recall=0.8611, f1=0.4110
Training epoch 48
	step [1/144], loss=85.8893
	step [2/144], loss=92.2631
	step [3/144], loss=98.9340
	step [4/144], loss=104.1840
	step [5/144], loss=96.5957
	step [6/144], loss=99.6458
	step [7/144], loss=91.6527
	step [8/144], loss=83.2586
	step [9/144], loss=100.0660
	step [10/144], loss=74.0048
	step [11/144], loss=97.8717
	step [12/144], loss=94.0513
	step [13/144], loss=100.2574
	step [14/144], loss=78.5862
	step [15/144], loss=89.0979
	step [16/144], loss=93.5040
	step [17/144], loss=87.2831
	step [18/144], loss=90.5495
	step [19/144], loss=90.7281
	step [20/144], loss=87.7325
	step [21/144], loss=94.2618
	step [22/144], loss=87.7022
	step [23/144], loss=100.1206
	step [24/144], loss=66.8569
	step [25/144], loss=78.3591
	step [26/144], loss=80.5683
	step [27/144], loss=101.5165
	step [28/144], loss=83.5087
	step [29/144], loss=93.1699
	step [30/144], loss=90.0174
	step [31/144], loss=96.3515
	step [32/144], loss=90.7257
	step [33/144], loss=96.3921
	step [34/144], loss=91.6151
	step [35/144], loss=112.0395
	step [36/144], loss=89.8743
	step [37/144], loss=81.3978
	step [38/144], loss=93.0096
	step [39/144], loss=86.9743
	step [40/144], loss=105.9099
	step [41/144], loss=92.5156
	step [42/144], loss=105.6815
	step [43/144], loss=100.1715
	step [44/144], loss=71.1494
	step [45/144], loss=86.6430
	step [46/144], loss=121.0738
	step [47/144], loss=99.6541
	step [48/144], loss=88.7896
	step [49/144], loss=83.9235
	step [50/144], loss=106.5205
	step [51/144], loss=77.8079
	step [52/144], loss=114.0194
	step [53/144], loss=101.0583
	step [54/144], loss=92.1046
	step [55/144], loss=101.3196
	step [56/144], loss=104.0649
	step [57/144], loss=117.2863
	step [58/144], loss=99.1381
	step [59/144], loss=101.4097
	step [60/144], loss=102.4861
	step [61/144], loss=94.2035
	step [62/144], loss=79.1079
	step [63/144], loss=100.8033
	step [64/144], loss=87.8954
	step [65/144], loss=94.4720
	step [66/144], loss=83.4660
	step [67/144], loss=88.9811
	step [68/144], loss=114.9945
	step [69/144], loss=101.1425
	step [70/144], loss=104.3607
	step [71/144], loss=86.8568
	step [72/144], loss=93.6178
	step [73/144], loss=80.7162
	step [74/144], loss=100.6557
	step [75/144], loss=97.0395
	step [76/144], loss=72.0620
	step [77/144], loss=89.3843
	step [78/144], loss=91.3494
	step [79/144], loss=88.4435
	step [80/144], loss=79.4447
	step [81/144], loss=97.7014
	step [82/144], loss=91.4712
	step [83/144], loss=95.0984
	step [84/144], loss=102.4821
	step [85/144], loss=97.4858
	step [86/144], loss=82.1492
	step [87/144], loss=90.2735
	step [88/144], loss=81.6995
	step [89/144], loss=97.4710
	step [90/144], loss=82.3519
	step [91/144], loss=69.8763
	step [92/144], loss=90.4566
	step [93/144], loss=96.9671
	step [94/144], loss=91.7285
	step [95/144], loss=110.5571
	step [96/144], loss=95.2624
	step [97/144], loss=98.7539
	step [98/144], loss=83.3289
	step [99/144], loss=85.4479
	step [100/144], loss=98.7222
	step [101/144], loss=101.4568
	step [102/144], loss=100.8411
	step [103/144], loss=103.8787
	step [104/144], loss=112.9405
	step [105/144], loss=98.5385
	step [106/144], loss=93.8067
	step [107/144], loss=113.2029
	step [108/144], loss=97.1163
	step [109/144], loss=79.6682
	step [110/144], loss=97.3578
	step [111/144], loss=89.0167
	step [112/144], loss=109.9400
	step [113/144], loss=104.7621
	step [114/144], loss=89.5544
	step [115/144], loss=59.8460
	step [116/144], loss=100.3326
	step [117/144], loss=89.8117
	step [118/144], loss=85.3401
	step [119/144], loss=90.7549
	step [120/144], loss=93.3169
	step [121/144], loss=95.6886
	step [122/144], loss=86.1574
	step [123/144], loss=98.2774
	step [124/144], loss=94.2744
	step [125/144], loss=90.8121
	step [126/144], loss=95.0508
	step [127/144], loss=92.9943
	step [128/144], loss=110.8490
	step [129/144], loss=101.9110
	step [130/144], loss=83.7042
	step [131/144], loss=109.1397
	step [132/144], loss=72.6465
	step [133/144], loss=103.7624
	step [134/144], loss=77.3360
	step [135/144], loss=88.0257
	step [136/144], loss=85.0508
	step [137/144], loss=92.6686
	step [138/144], loss=101.1490
	step [139/144], loss=91.6264
	step [140/144], loss=82.4958
	step [141/144], loss=99.0988
	step [142/144], loss=83.3393
	step [143/144], loss=101.6838
	step [144/144], loss=34.4108
	Evaluating
	loss=0.0197, precision=0.2791, recall=0.8645, f1=0.4220
Training epoch 49
	step [1/144], loss=91.7774
	step [2/144], loss=97.2800
	step [3/144], loss=87.2312
	step [4/144], loss=92.1177
	step [5/144], loss=93.1459
	step [6/144], loss=92.9230
	step [7/144], loss=98.9252
	step [8/144], loss=86.8711
	step [9/144], loss=87.8087
	step [10/144], loss=90.3097
	step [11/144], loss=93.6481
	step [12/144], loss=96.9433
	step [13/144], loss=86.0534
	step [14/144], loss=111.4317
	step [15/144], loss=90.7461
	step [16/144], loss=75.0953
	step [17/144], loss=82.8432
	step [18/144], loss=88.0292
	step [19/144], loss=99.7749
	step [20/144], loss=97.2998
	step [21/144], loss=93.9891
	step [22/144], loss=78.8473
	step [23/144], loss=109.2915
	step [24/144], loss=83.9870
	step [25/144], loss=108.1684
	step [26/144], loss=110.1248
	step [27/144], loss=94.2713
	step [28/144], loss=82.9974
	step [29/144], loss=95.6320
	step [30/144], loss=101.5335
	step [31/144], loss=102.2698
	step [32/144], loss=105.3195
	step [33/144], loss=91.4829
	step [34/144], loss=76.9356
	step [35/144], loss=86.2608
	step [36/144], loss=82.3584
	step [37/144], loss=87.6436
	step [38/144], loss=85.0749
	step [39/144], loss=102.9752
	step [40/144], loss=105.2961
	step [41/144], loss=96.0087
	step [42/144], loss=98.9652
	step [43/144], loss=97.1786
	step [44/144], loss=92.4134
	step [45/144], loss=86.8701
	step [46/144], loss=99.2193
	step [47/144], loss=99.6119
	step [48/144], loss=109.5570
	step [49/144], loss=94.9893
	step [50/144], loss=94.9041
	step [51/144], loss=106.0081
	step [52/144], loss=102.5165
	step [53/144], loss=80.2966
	step [54/144], loss=86.6223
	step [55/144], loss=105.8214
	step [56/144], loss=72.2495
	step [57/144], loss=85.1670
	step [58/144], loss=100.0782
	step [59/144], loss=83.4962
	step [60/144], loss=85.7030
	step [61/144], loss=82.8836
	step [62/144], loss=105.2281
	step [63/144], loss=76.4360
	step [64/144], loss=90.2965
	step [65/144], loss=96.5123
	step [66/144], loss=91.0637
	step [67/144], loss=96.7924
	step [68/144], loss=80.9462
	step [69/144], loss=83.9255
	step [70/144], loss=96.5266
	step [71/144], loss=92.6257
	step [72/144], loss=78.0149
	step [73/144], loss=106.1410
	step [74/144], loss=92.2017
	step [75/144], loss=92.1259
	step [76/144], loss=90.3626
	step [77/144], loss=102.3002
	step [78/144], loss=86.0196
	step [79/144], loss=84.1052
	step [80/144], loss=90.2938
	step [81/144], loss=84.2005
	step [82/144], loss=88.6126
	step [83/144], loss=85.6766
	step [84/144], loss=91.3664
	step [85/144], loss=108.6365
	step [86/144], loss=85.0919
	step [87/144], loss=75.7103
	step [88/144], loss=96.1494
	step [89/144], loss=82.6786
	step [90/144], loss=112.0646
	step [91/144], loss=90.4270
	step [92/144], loss=98.2652
	step [93/144], loss=89.1831
	step [94/144], loss=90.5626
	step [95/144], loss=81.1954
	step [96/144], loss=93.0950
	step [97/144], loss=95.4330
	step [98/144], loss=76.5294
	step [99/144], loss=97.3989
	step [100/144], loss=82.1120
	step [101/144], loss=93.7820
	step [102/144], loss=76.4642
	step [103/144], loss=90.5822
	step [104/144], loss=80.9263
	step [105/144], loss=98.1572
	step [106/144], loss=95.4445
	step [107/144], loss=100.5169
	step [108/144], loss=86.7135
	step [109/144], loss=87.7543
	step [110/144], loss=82.6962
	step [111/144], loss=96.4577
	step [112/144], loss=90.3063
	step [113/144], loss=93.5034
	step [114/144], loss=109.5184
	step [115/144], loss=94.9297
	step [116/144], loss=96.1741
	step [117/144], loss=84.8762
	step [118/144], loss=81.3264
	step [119/144], loss=94.6942
	step [120/144], loss=103.0753
	step [121/144], loss=89.3761
	step [122/144], loss=97.1615
	step [123/144], loss=100.6920
	step [124/144], loss=83.6693
	step [125/144], loss=90.4253
	step [126/144], loss=101.2308
	step [127/144], loss=97.4496
	step [128/144], loss=94.4511
	step [129/144], loss=91.3640
	step [130/144], loss=85.9540
	step [131/144], loss=80.3444
	step [132/144], loss=112.2529
	step [133/144], loss=86.6127
	step [134/144], loss=85.0626
	step [135/144], loss=102.4685
	step [136/144], loss=100.8936
	step [137/144], loss=107.7110
	step [138/144], loss=101.0464
	step [139/144], loss=87.5922
	step [140/144], loss=99.2482
	step [141/144], loss=102.7936
	step [142/144], loss=91.6972
	step [143/144], loss=82.9829
	step [144/144], loss=44.6928
	Evaluating
	loss=0.0160, precision=0.3551, recall=0.8495, f1=0.5008
saving model as: 1_saved_model.pth
Training epoch 50
	step [1/144], loss=69.3778
	step [2/144], loss=80.5297
	step [3/144], loss=91.1532
	step [4/144], loss=96.7380
	step [5/144], loss=97.0983
	step [6/144], loss=82.3715
	step [7/144], loss=107.2319
	step [8/144], loss=94.4928
	step [9/144], loss=99.5416
	step [10/144], loss=98.3624
	step [11/144], loss=93.3639
	step [12/144], loss=105.1594
	step [13/144], loss=90.5266
	step [14/144], loss=105.1395
	step [15/144], loss=88.2208
	step [16/144], loss=102.0683
	step [17/144], loss=89.1370
	step [18/144], loss=88.4330
	step [19/144], loss=100.1942
	step [20/144], loss=94.6579
	step [21/144], loss=72.5931
	step [22/144], loss=101.5734
	step [23/144], loss=100.1548
	step [24/144], loss=94.0956
	step [25/144], loss=90.3641
	step [26/144], loss=89.8037
	step [27/144], loss=103.6751
	step [28/144], loss=94.6006
	step [29/144], loss=82.9562
	step [30/144], loss=94.3342
	step [31/144], loss=95.7352
	step [32/144], loss=83.2056
	step [33/144], loss=88.3702
	step [34/144], loss=89.3047
	step [35/144], loss=90.9176
	step [36/144], loss=86.6316
	step [37/144], loss=93.9623
	step [38/144], loss=104.3073
	step [39/144], loss=78.5907
	step [40/144], loss=73.1982
	step [41/144], loss=109.3487
	step [42/144], loss=81.2249
	step [43/144], loss=86.3930
	step [44/144], loss=100.5570
	step [45/144], loss=86.7580
	step [46/144], loss=83.8629
	step [47/144], loss=84.5583
	step [48/144], loss=93.9102
	step [49/144], loss=106.3785
	step [50/144], loss=97.9049
	step [51/144], loss=71.4180
	step [52/144], loss=72.2171
	step [53/144], loss=83.2659
	step [54/144], loss=77.1696
	step [55/144], loss=91.9329
	step [56/144], loss=100.7592
	step [57/144], loss=94.7180
	step [58/144], loss=108.5203
	step [59/144], loss=75.1835
	step [60/144], loss=89.2856
	step [61/144], loss=102.0148
	step [62/144], loss=90.3363
	step [63/144], loss=106.2566
	step [64/144], loss=93.0269
	step [65/144], loss=99.2512
	step [66/144], loss=103.4226
	step [67/144], loss=94.0033
	step [68/144], loss=98.8075
	step [69/144], loss=94.1420
	step [70/144], loss=105.8883
	step [71/144], loss=103.1652
	step [72/144], loss=90.3868
	step [73/144], loss=90.5597
	step [74/144], loss=84.2118
	step [75/144], loss=80.3367
	step [76/144], loss=109.1635
	step [77/144], loss=80.4007
	step [78/144], loss=101.0552
	step [79/144], loss=96.2740
	step [80/144], loss=82.3301
	step [81/144], loss=89.9659
	step [82/144], loss=108.3044
	step [83/144], loss=101.8414
	step [84/144], loss=90.4821
	step [85/144], loss=90.7826
	step [86/144], loss=94.9864
	step [87/144], loss=92.3921
	step [88/144], loss=89.8049
	step [89/144], loss=90.9494
	step [90/144], loss=99.1824
	step [91/144], loss=96.7305
	step [92/144], loss=94.6680
	step [93/144], loss=91.7465
	step [94/144], loss=95.0640
	step [95/144], loss=105.9622
	step [96/144], loss=104.7001
	step [97/144], loss=100.1463
	step [98/144], loss=82.1338
	step [99/144], loss=93.0367
	step [100/144], loss=88.6983
	step [101/144], loss=89.3358
	step [102/144], loss=93.3853
	step [103/144], loss=83.2971
	step [104/144], loss=75.6361
	step [105/144], loss=93.1400
	step [106/144], loss=79.1457
	step [107/144], loss=95.2639
	step [108/144], loss=91.0835
	step [109/144], loss=105.4107
	step [110/144], loss=70.6016
	step [111/144], loss=85.3152
	step [112/144], loss=87.1458
	step [113/144], loss=88.8829
	step [114/144], loss=88.6290
	step [115/144], loss=99.7104
	step [116/144], loss=87.2808
	step [117/144], loss=93.5253
	step [118/144], loss=92.3712
	step [119/144], loss=96.4982
	step [120/144], loss=83.0484
	step [121/144], loss=83.8876
	step [122/144], loss=106.1725
	step [123/144], loss=81.8215
	step [124/144], loss=85.0861
	step [125/144], loss=107.1349
	step [126/144], loss=106.5764
	step [127/144], loss=87.4305
	step [128/144], loss=87.8809
	step [129/144], loss=87.8170
	step [130/144], loss=96.9719
	step [131/144], loss=91.4792
	step [132/144], loss=103.9849
	step [133/144], loss=92.2639
	step [134/144], loss=81.8924
	step [135/144], loss=80.8274
	step [136/144], loss=90.5225
	step [137/144], loss=84.5084
	step [138/144], loss=96.5422
	step [139/144], loss=98.2702
	step [140/144], loss=93.5435
	step [141/144], loss=76.7624
	step [142/144], loss=96.0155
	step [143/144], loss=87.3600
	step [144/144], loss=42.8812
	Evaluating
	loss=0.0173, precision=0.3206, recall=0.8542, f1=0.4662
Training epoch 51
	step [1/144], loss=89.9570
	step [2/144], loss=90.9869
	step [3/144], loss=102.0090
	step [4/144], loss=80.5332
	step [5/144], loss=91.8175
	step [6/144], loss=92.0539
	step [7/144], loss=100.6990
	step [8/144], loss=96.3018
	step [9/144], loss=92.9494
	step [10/144], loss=79.2223
	step [11/144], loss=94.1682
	step [12/144], loss=83.4503
	step [13/144], loss=87.8667
	step [14/144], loss=85.7366
	step [15/144], loss=108.3303
	step [16/144], loss=88.8375
	step [17/144], loss=98.7842
	step [18/144], loss=100.7366
	step [19/144], loss=92.6460
	step [20/144], loss=84.9276
	step [21/144], loss=83.3965
	step [22/144], loss=92.0231
	step [23/144], loss=96.8807
	step [24/144], loss=103.9629
	step [25/144], loss=86.4459
	step [26/144], loss=100.1600
	step [27/144], loss=94.6656
	step [28/144], loss=83.3417
	step [29/144], loss=90.5353
	step [30/144], loss=97.6649
	step [31/144], loss=86.7241
	step [32/144], loss=91.8099
	step [33/144], loss=87.7819
	step [34/144], loss=95.9187
	step [35/144], loss=101.7850
	step [36/144], loss=78.5560
	step [37/144], loss=98.6538
	step [38/144], loss=89.3940
	step [39/144], loss=88.1081
	step [40/144], loss=83.1293
	step [41/144], loss=82.0286
	step [42/144], loss=86.5699
	step [43/144], loss=93.4612
	step [44/144], loss=94.8734
	step [45/144], loss=107.6307
	step [46/144], loss=114.4622
	step [47/144], loss=93.0594
	step [48/144], loss=119.9702
	step [49/144], loss=93.1125
	step [50/144], loss=81.0668
	step [51/144], loss=89.1172
	step [52/144], loss=90.1528
	step [53/144], loss=94.9839
	step [54/144], loss=85.6841
	step [55/144], loss=78.3365
	step [56/144], loss=77.2133
	step [57/144], loss=96.8166
	step [58/144], loss=71.0431
	step [59/144], loss=80.2219
	step [60/144], loss=80.8325
	step [61/144], loss=90.6015
	step [62/144], loss=77.8607
	step [63/144], loss=99.9476
	step [64/144], loss=86.5580
	step [65/144], loss=103.7054
	step [66/144], loss=104.8682
	step [67/144], loss=106.7609
	step [68/144], loss=87.5298
	step [69/144], loss=82.4239
	step [70/144], loss=98.7981
	step [71/144], loss=99.5564
	step [72/144], loss=92.1193
	step [73/144], loss=85.2129
	step [74/144], loss=96.8193
	step [75/144], loss=98.4384
	step [76/144], loss=98.5539
	step [77/144], loss=113.3599
	step [78/144], loss=93.6053
	step [79/144], loss=96.9336
	step [80/144], loss=90.5146
	step [81/144], loss=93.4002
	step [82/144], loss=100.2438
	step [83/144], loss=96.8038
	step [84/144], loss=86.7417
	step [85/144], loss=95.2411
	step [86/144], loss=76.6313
	step [87/144], loss=95.7894
	step [88/144], loss=106.8318
	step [89/144], loss=102.6757
	step [90/144], loss=74.4873
	step [91/144], loss=89.1337
	step [92/144], loss=87.5426
	step [93/144], loss=99.9833
	step [94/144], loss=86.5891
	step [95/144], loss=96.5489
	step [96/144], loss=89.9703
	step [97/144], loss=81.4377
	step [98/144], loss=90.3353
	step [99/144], loss=87.9857
	step [100/144], loss=80.7736
	step [101/144], loss=88.9494
	step [102/144], loss=96.3800
	step [103/144], loss=92.1609
	step [104/144], loss=79.7084
	step [105/144], loss=87.2701
	step [106/144], loss=96.1283
	step [107/144], loss=104.7802
	step [108/144], loss=96.5074
	step [109/144], loss=90.7304
	step [110/144], loss=104.7069
	step [111/144], loss=85.8788
	step [112/144], loss=103.0135
	step [113/144], loss=89.4440
	step [114/144], loss=87.0988
	step [115/144], loss=76.8420
	step [116/144], loss=83.1133
	step [117/144], loss=97.9260
	step [118/144], loss=98.7782
	step [119/144], loss=86.4518
	step [120/144], loss=95.2385
	step [121/144], loss=82.5486
	step [122/144], loss=87.6757
	step [123/144], loss=87.8418
	step [124/144], loss=103.4984
	step [125/144], loss=75.4736
	step [126/144], loss=105.7140
	step [127/144], loss=81.3446
	step [128/144], loss=82.7978
	step [129/144], loss=97.0741
	step [130/144], loss=94.2339
	step [131/144], loss=82.6816
	step [132/144], loss=76.7816
	step [133/144], loss=60.2342
	step [134/144], loss=93.7997
	step [135/144], loss=90.7054
	step [136/144], loss=93.0768
	step [137/144], loss=103.3389
	step [138/144], loss=87.3354
	step [139/144], loss=89.6641
	step [140/144], loss=74.4420
	step [141/144], loss=83.0031
	step [142/144], loss=86.9231
	step [143/144], loss=104.5482
	step [144/144], loss=29.9581
	Evaluating
	loss=0.0170, precision=0.3167, recall=0.8566, f1=0.4625
Training epoch 52
	step [1/144], loss=94.1677
	step [2/144], loss=98.1765
	step [3/144], loss=90.0653
	step [4/144], loss=85.0984
	step [5/144], loss=108.5321
	step [6/144], loss=93.6171
	step [7/144], loss=91.9220
	step [8/144], loss=88.0605
	step [9/144], loss=83.7543
	step [10/144], loss=75.4149
	step [11/144], loss=103.5621
	step [12/144], loss=89.7206
	step [13/144], loss=103.7876
	step [14/144], loss=89.4562
	step [15/144], loss=108.0387
	step [16/144], loss=77.5497
	step [17/144], loss=91.5276
	step [18/144], loss=79.7161
	step [19/144], loss=80.8500
	step [20/144], loss=78.4484
	step [21/144], loss=89.7329
	step [22/144], loss=84.0201
	step [23/144], loss=86.2101
	step [24/144], loss=95.9024
	step [25/144], loss=82.3258
	step [26/144], loss=97.1816
	step [27/144], loss=90.4636
	step [28/144], loss=79.4567
	step [29/144], loss=101.7929
	step [30/144], loss=95.4012
	step [31/144], loss=96.8013
	step [32/144], loss=84.0960
	step [33/144], loss=93.7428
	step [34/144], loss=84.3280
	step [35/144], loss=95.8418
	step [36/144], loss=85.5519
	step [37/144], loss=92.8489
	step [38/144], loss=106.0220
	step [39/144], loss=83.6169
	step [40/144], loss=86.2733
	step [41/144], loss=81.4990
	step [42/144], loss=90.4001
	step [43/144], loss=93.1080
	step [44/144], loss=91.8660
	step [45/144], loss=98.6319
	step [46/144], loss=95.7031
	step [47/144], loss=105.2778
	step [48/144], loss=94.0199
	step [49/144], loss=88.1629
	step [50/144], loss=95.8927
	step [51/144], loss=109.0586
	step [52/144], loss=96.1140
	step [53/144], loss=89.9202
	step [54/144], loss=83.2682
	step [55/144], loss=86.1348
	step [56/144], loss=101.5801
	step [57/144], loss=97.8111
	step [58/144], loss=107.5237
	step [59/144], loss=96.8716
	step [60/144], loss=83.0172
	step [61/144], loss=105.8088
	step [62/144], loss=86.4831
	step [63/144], loss=87.1097
	step [64/144], loss=107.3313
	step [65/144], loss=76.0533
	step [66/144], loss=98.3401
	step [67/144], loss=89.4555
	step [68/144], loss=98.3975
	step [69/144], loss=86.6628
	step [70/144], loss=93.9652
	step [71/144], loss=96.1786
	step [72/144], loss=88.3146
	step [73/144], loss=93.2638
	step [74/144], loss=82.3795
	step [75/144], loss=88.1123
	step [76/144], loss=81.1563
	step [77/144], loss=99.6454
	step [78/144], loss=88.4781
	step [79/144], loss=88.9949
	step [80/144], loss=86.8088
	step [81/144], loss=97.1036
	step [82/144], loss=92.5746
	step [83/144], loss=86.8728
	step [84/144], loss=87.0469
	step [85/144], loss=87.2212
	step [86/144], loss=89.1133
	step [87/144], loss=91.7717
	step [88/144], loss=88.1333
	step [89/144], loss=88.0721
	step [90/144], loss=83.4882
	step [91/144], loss=84.4129
	step [92/144], loss=86.8594
	step [93/144], loss=105.3652
	step [94/144], loss=80.0514
	step [95/144], loss=90.6407
	step [96/144], loss=76.5859
	step [97/144], loss=97.6201
	step [98/144], loss=93.1949
	step [99/144], loss=93.7606
	step [100/144], loss=80.6277
	step [101/144], loss=86.8067
	step [102/144], loss=89.7827
	step [103/144], loss=76.6623
	step [104/144], loss=90.9949
	step [105/144], loss=82.4534
	step [106/144], loss=101.2386
	step [107/144], loss=94.9858
	step [108/144], loss=96.7174
	step [109/144], loss=78.8315
	step [110/144], loss=106.1957
	step [111/144], loss=86.3111
	step [112/144], loss=95.3655
	step [113/144], loss=98.6948
	step [114/144], loss=95.2554
	step [115/144], loss=91.9682
	step [116/144], loss=113.2814
	step [117/144], loss=84.3571
	step [118/144], loss=89.4109
	step [119/144], loss=76.1943
	step [120/144], loss=77.2057
	step [121/144], loss=95.5306
	step [122/144], loss=103.7808
	step [123/144], loss=74.8500
	step [124/144], loss=95.7482
	step [125/144], loss=94.8195
	step [126/144], loss=93.1983
	step [127/144], loss=108.1875
	step [128/144], loss=83.9650
	step [129/144], loss=92.5001
	step [130/144], loss=84.3585
	step [131/144], loss=90.9396
	step [132/144], loss=91.3667
	step [133/144], loss=91.8968
	step [134/144], loss=90.5915
	step [135/144], loss=91.6666
	step [136/144], loss=86.2810
	step [137/144], loss=98.7800
	step [138/144], loss=94.8095
	step [139/144], loss=83.7822
	step [140/144], loss=86.2474
	step [141/144], loss=89.7811
	step [142/144], loss=85.3483
	step [143/144], loss=99.0399
	step [144/144], loss=27.1013
	Evaluating
	loss=0.0172, precision=0.3163, recall=0.8608, f1=0.4626
Training epoch 53
	step [1/144], loss=102.8604
	step [2/144], loss=94.4298
	step [3/144], loss=107.3465
	step [4/144], loss=93.1513
	step [5/144], loss=81.7962
	step [6/144], loss=101.7525
	step [7/144], loss=103.7327
	step [8/144], loss=100.1860
	step [9/144], loss=97.8736
	step [10/144], loss=92.5379
	step [11/144], loss=90.9996
	step [12/144], loss=84.3765
	step [13/144], loss=130.4834
	step [14/144], loss=88.4830
	step [15/144], loss=93.0070
	step [16/144], loss=88.1153
	step [17/144], loss=83.5963
	step [18/144], loss=99.6487
	step [19/144], loss=120.6423
	step [20/144], loss=86.2670
	step [21/144], loss=81.7242
	step [22/144], loss=79.8435
	step [23/144], loss=78.8069
	step [24/144], loss=83.8424
	step [25/144], loss=68.9157
	step [26/144], loss=96.1842
	step [27/144], loss=99.8177
	step [28/144], loss=103.7340
	step [29/144], loss=93.6763
	step [30/144], loss=81.7784
	step [31/144], loss=96.4628
	step [32/144], loss=76.0528
	step [33/144], loss=83.5268
	step [34/144], loss=90.2195
	step [35/144], loss=89.0778
	step [36/144], loss=88.3750
	step [37/144], loss=101.9545
	step [38/144], loss=91.3534
	step [39/144], loss=80.3630
	step [40/144], loss=88.0696
	step [41/144], loss=76.6486
	step [42/144], loss=89.6856
	step [43/144], loss=99.4409
	step [44/144], loss=83.7453
	step [45/144], loss=83.3419
	step [46/144], loss=106.9061
	step [47/144], loss=91.7375
	step [48/144], loss=96.3461
	step [49/144], loss=86.9224
	step [50/144], loss=67.0447
	step [51/144], loss=77.3161
	step [52/144], loss=96.4568
	step [53/144], loss=87.3201
	step [54/144], loss=89.2173
	step [55/144], loss=81.8676
	step [56/144], loss=94.9038
	step [57/144], loss=88.4816
	step [58/144], loss=80.3309
	step [59/144], loss=79.1885
	step [60/144], loss=115.4650
	step [61/144], loss=80.6654
	step [62/144], loss=83.6837
	step [63/144], loss=101.2016
	step [64/144], loss=107.6939
	step [65/144], loss=99.5887
	step [66/144], loss=105.0767
	step [67/144], loss=105.2605
	step [68/144], loss=106.7303
	step [69/144], loss=97.4721
	step [70/144], loss=102.1747
	step [71/144], loss=80.2115
	step [72/144], loss=87.7059
	step [73/144], loss=79.4012
	step [74/144], loss=73.8073
	step [75/144], loss=86.8369
	step [76/144], loss=95.7460
	step [77/144], loss=90.1190
	step [78/144], loss=79.9799
	step [79/144], loss=93.9711
	step [80/144], loss=93.7299
	step [81/144], loss=85.0316
	step [82/144], loss=95.5923
	step [83/144], loss=106.1898
	step [84/144], loss=79.0911
	step [85/144], loss=90.1030
	step [86/144], loss=76.9114
	step [87/144], loss=76.5291
	step [88/144], loss=84.0355
	step [89/144], loss=92.3619
	step [90/144], loss=108.3077
	step [91/144], loss=93.1324
	step [92/144], loss=93.8812
	step [93/144], loss=82.3740
	step [94/144], loss=92.0899
	step [95/144], loss=79.7716
	step [96/144], loss=96.9262
	step [97/144], loss=80.3893
	step [98/144], loss=91.5689
	step [99/144], loss=103.3336
	step [100/144], loss=105.9721
	step [101/144], loss=97.0495
	step [102/144], loss=95.1090
	step [103/144], loss=81.1011
	step [104/144], loss=112.6778
	step [105/144], loss=96.1544
	step [106/144], loss=97.7767
	step [107/144], loss=100.7145
	step [108/144], loss=90.0161
	step [109/144], loss=85.4109
	step [110/144], loss=81.0844
	step [111/144], loss=73.0916
	step [112/144], loss=80.7257
	step [113/144], loss=86.9745
	step [114/144], loss=91.3834
	step [115/144], loss=72.7692
	step [116/144], loss=86.6340
	step [117/144], loss=76.7154
	step [118/144], loss=87.9390
	step [119/144], loss=88.1170
	step [120/144], loss=97.5483
	step [121/144], loss=83.8310
	step [122/144], loss=107.4066
	step [123/144], loss=85.5390
	step [124/144], loss=91.6428
	step [125/144], loss=97.0160
	step [126/144], loss=80.7930
	step [127/144], loss=95.5663
	step [128/144], loss=83.8007
	step [129/144], loss=103.0631
	step [130/144], loss=80.4056
	step [131/144], loss=82.1187
	step [132/144], loss=80.0960
	step [133/144], loss=102.1303
	step [134/144], loss=77.1422
	step [135/144], loss=88.4237
	step [136/144], loss=77.1888
	step [137/144], loss=82.3067
	step [138/144], loss=87.5172
	step [139/144], loss=105.4499
	step [140/144], loss=78.3593
	step [141/144], loss=90.9046
	step [142/144], loss=87.6942
	step [143/144], loss=89.9848
	step [144/144], loss=26.1629
	Evaluating
	loss=0.0132, precision=0.4055, recall=0.8351, f1=0.5460
saving model as: 1_saved_model.pth
Training epoch 54
	step [1/144], loss=94.6963
	step [2/144], loss=86.6706
	step [3/144], loss=103.6812
	step [4/144], loss=86.4892
	step [5/144], loss=90.9629
	step [6/144], loss=70.3930
	step [7/144], loss=89.2297
	step [8/144], loss=90.4509
	step [9/144], loss=105.1513
	step [10/144], loss=94.4103
	step [11/144], loss=99.4782
	step [12/144], loss=84.9470
	step [13/144], loss=96.1295
	step [14/144], loss=92.4612
	step [15/144], loss=98.5602
	step [16/144], loss=100.8089
	step [17/144], loss=77.7799
	step [18/144], loss=88.6144
	step [19/144], loss=90.2572
	step [20/144], loss=93.3221
	step [21/144], loss=99.6440
	step [22/144], loss=84.2469
	step [23/144], loss=87.7615
	step [24/144], loss=88.3743
	step [25/144], loss=80.9960
	step [26/144], loss=97.7086
	step [27/144], loss=93.2031
	step [28/144], loss=99.2154
	step [29/144], loss=92.2476
	step [30/144], loss=94.3493
	step [31/144], loss=82.6252
	step [32/144], loss=96.3423
	step [33/144], loss=96.8140
	step [34/144], loss=81.4231
	step [35/144], loss=93.6559
	step [36/144], loss=92.1622
	step [37/144], loss=90.2618
	step [38/144], loss=80.9792
	step [39/144], loss=82.5737
	step [40/144], loss=73.0808
	step [41/144], loss=92.9477
	step [42/144], loss=68.4259
	step [43/144], loss=114.2265
	step [44/144], loss=82.9992
	step [45/144], loss=91.1733
	step [46/144], loss=91.8301
	step [47/144], loss=99.5939
	step [48/144], loss=104.5077
	step [49/144], loss=82.8367
	step [50/144], loss=89.0418
	step [51/144], loss=100.0763
	step [52/144], loss=91.6554
	step [53/144], loss=93.8209
	step [54/144], loss=89.7973
	step [55/144], loss=76.1406
	step [56/144], loss=89.1111
	step [57/144], loss=87.2307
	step [58/144], loss=88.7225
	step [59/144], loss=75.2247
	step [60/144], loss=95.1159
	step [61/144], loss=93.1105
	step [62/144], loss=85.8688
	step [63/144], loss=86.0341
	step [64/144], loss=94.6854
	step [65/144], loss=89.0094
	step [66/144], loss=88.0203
	step [67/144], loss=84.3318
	step [68/144], loss=90.1605
	step [69/144], loss=79.4439
	step [70/144], loss=84.7365
	step [71/144], loss=99.1107
	step [72/144], loss=88.9685
	step [73/144], loss=92.9293
	step [74/144], loss=77.8311
	step [75/144], loss=78.6881
	step [76/144], loss=89.8993
	step [77/144], loss=92.7114
	step [78/144], loss=85.0498
	step [79/144], loss=73.6310
	step [80/144], loss=93.8082
	step [81/144], loss=88.3108
	step [82/144], loss=84.8126
	step [83/144], loss=81.1044
	step [84/144], loss=107.7298
	step [85/144], loss=83.7269
	step [86/144], loss=102.0176
	step [87/144], loss=90.2303
	step [88/144], loss=90.1568
	step [89/144], loss=78.2662
	step [90/144], loss=99.2419
	step [91/144], loss=103.7651
	step [92/144], loss=78.3985
	step [93/144], loss=92.9962
	step [94/144], loss=94.0666
	step [95/144], loss=89.3251
	step [96/144], loss=104.6764
	step [97/144], loss=94.5449
	step [98/144], loss=80.7348
	step [99/144], loss=73.6360
	step [100/144], loss=76.4946
	step [101/144], loss=82.9571
	step [102/144], loss=109.2646
	step [103/144], loss=97.1950
	step [104/144], loss=86.9577
	step [105/144], loss=82.2359
	step [106/144], loss=90.4424
	step [107/144], loss=82.2776
	step [108/144], loss=91.2639
	step [109/144], loss=86.8093
	step [110/144], loss=93.2266
	step [111/144], loss=86.2047
	step [112/144], loss=100.0453
	step [113/144], loss=96.6432
	step [114/144], loss=94.7237
	step [115/144], loss=99.3971
	step [116/144], loss=103.9150
	step [117/144], loss=80.1104
	step [118/144], loss=82.5564
	step [119/144], loss=93.4935
	step [120/144], loss=95.1334
	step [121/144], loss=83.2901
	step [122/144], loss=73.6656
	step [123/144], loss=83.9619
	step [124/144], loss=99.9563
	step [125/144], loss=90.5719
	step [126/144], loss=99.1022
	step [127/144], loss=86.0893
	step [128/144], loss=95.9052
	step [129/144], loss=94.2918
	step [130/144], loss=97.1894
	step [131/144], loss=86.1381
	step [132/144], loss=73.7252
	step [133/144], loss=103.2697
	step [134/144], loss=78.4943
	step [135/144], loss=95.9949
	step [136/144], loss=97.1623
	step [137/144], loss=91.5545
	step [138/144], loss=82.7957
	step [139/144], loss=84.1675
	step [140/144], loss=92.9453
	step [141/144], loss=92.2530
	step [142/144], loss=104.8029
	step [143/144], loss=89.1006
	step [144/144], loss=30.4967
	Evaluating
	loss=0.0161, precision=0.3382, recall=0.8487, f1=0.4837
Training epoch 55
	step [1/144], loss=79.9796
	step [2/144], loss=95.1139
	step [3/144], loss=89.2405
	step [4/144], loss=115.1007
	step [5/144], loss=103.0133
	step [6/144], loss=81.4061
	step [7/144], loss=83.8528
	step [8/144], loss=88.4810
	step [9/144], loss=79.6066
	step [10/144], loss=99.5442
	step [11/144], loss=95.3921
	step [12/144], loss=85.8793
	step [13/144], loss=102.3625
	step [14/144], loss=90.7058
	step [15/144], loss=87.3525
	step [16/144], loss=87.1278
	step [17/144], loss=90.2790
	step [18/144], loss=79.4890
	step [19/144], loss=71.8553
	step [20/144], loss=90.4149
	step [21/144], loss=88.8139
	step [22/144], loss=80.7002
	step [23/144], loss=81.3430
	step [24/144], loss=105.3134
	step [25/144], loss=97.2764
	step [26/144], loss=84.2033
	step [27/144], loss=96.9027
	step [28/144], loss=81.3906
	step [29/144], loss=89.8085
	step [30/144], loss=91.9088
	step [31/144], loss=79.1179
	step [32/144], loss=70.7285
	step [33/144], loss=77.2935
	step [34/144], loss=75.6703
	step [35/144], loss=83.5885
	step [36/144], loss=96.8919
	step [37/144], loss=88.3466
	step [38/144], loss=88.3086
	step [39/144], loss=98.4170
	step [40/144], loss=85.7089
	step [41/144], loss=83.4862
	step [42/144], loss=88.6155
	step [43/144], loss=76.0404
	step [44/144], loss=99.3401
	step [45/144], loss=87.8264
	step [46/144], loss=109.1371
	step [47/144], loss=100.4011
	step [48/144], loss=90.2127
	step [49/144], loss=80.5377
	step [50/144], loss=90.1369
	step [51/144], loss=70.7336
	step [52/144], loss=88.7756
	step [53/144], loss=82.3596
	step [54/144], loss=91.4177
	step [55/144], loss=105.0507
	step [56/144], loss=95.9888
	step [57/144], loss=88.1073
	step [58/144], loss=87.4418
	step [59/144], loss=99.6002
	step [60/144], loss=105.4061
	step [61/144], loss=104.7837
	step [62/144], loss=104.2376
	step [63/144], loss=101.5123
	step [64/144], loss=82.1464
	step [65/144], loss=85.4292
	step [66/144], loss=84.8164
	step [67/144], loss=81.7988
	step [68/144], loss=93.9772
	step [69/144], loss=81.3724
	step [70/144], loss=81.7735
	step [71/144], loss=94.4627
	step [72/144], loss=88.8914
	step [73/144], loss=83.8444
	step [74/144], loss=90.2939
	step [75/144], loss=91.2193
	step [76/144], loss=99.4653
	step [77/144], loss=100.9520
	step [78/144], loss=93.6792
	step [79/144], loss=90.9851
	step [80/144], loss=81.8431
	step [81/144], loss=83.5996
	step [82/144], loss=86.1374
	step [83/144], loss=105.0454
	step [84/144], loss=79.4582
	step [85/144], loss=96.5429
	step [86/144], loss=100.9883
	step [87/144], loss=94.8555
	step [88/144], loss=84.2802
	step [89/144], loss=98.4554
	step [90/144], loss=88.3557
	step [91/144], loss=90.3508
	step [92/144], loss=86.3696
	step [93/144], loss=85.0907
	step [94/144], loss=79.3332
	step [95/144], loss=95.5379
	step [96/144], loss=93.2059
	step [97/144], loss=76.8841
	step [98/144], loss=100.1858
	step [99/144], loss=69.4769
	step [100/144], loss=87.2781
	step [101/144], loss=102.1001
	step [102/144], loss=84.5966
	step [103/144], loss=85.3417
	step [104/144], loss=74.2779
	step [105/144], loss=91.3805
	step [106/144], loss=88.4387
	step [107/144], loss=81.4926
	step [108/144], loss=100.1545
	step [109/144], loss=93.1525
	step [110/144], loss=100.1364
	step [111/144], loss=92.1233
	step [112/144], loss=96.6639
	step [113/144], loss=87.7687
	step [114/144], loss=93.2305
	step [115/144], loss=109.2669
	step [116/144], loss=90.0796
	step [117/144], loss=90.0586
	step [118/144], loss=79.1716
	step [119/144], loss=79.8728
	step [120/144], loss=96.4492
	step [121/144], loss=75.6234
	step [122/144], loss=78.1365
	step [123/144], loss=91.5236
	step [124/144], loss=76.0883
	step [125/144], loss=94.4921
	step [126/144], loss=94.5160
	step [127/144], loss=68.9951
	step [128/144], loss=75.3291
	step [129/144], loss=85.8137
	step [130/144], loss=81.6052
	step [131/144], loss=90.9389
	step [132/144], loss=97.6817
	step [133/144], loss=91.6784
	step [134/144], loss=100.4620
	step [135/144], loss=96.5766
	step [136/144], loss=86.3949
	step [137/144], loss=85.4487
	step [138/144], loss=92.9001
	step [139/144], loss=95.4923
	step [140/144], loss=101.3954
	step [141/144], loss=88.8341
	step [142/144], loss=108.1573
	step [143/144], loss=92.5181
	step [144/144], loss=28.1705
	Evaluating
	loss=0.0132, precision=0.3916, recall=0.8376, f1=0.5337
Training epoch 56
	step [1/144], loss=86.6048
	step [2/144], loss=102.8970
	step [3/144], loss=107.6657
	step [4/144], loss=81.2638
	step [5/144], loss=91.2128
	step [6/144], loss=95.1044
	step [7/144], loss=78.7879
	step [8/144], loss=92.8317
	step [9/144], loss=96.6292
	step [10/144], loss=67.1264
	step [11/144], loss=99.2122
	step [12/144], loss=83.8855
	step [13/144], loss=88.8431
	step [14/144], loss=87.2625
	step [15/144], loss=95.2151
	step [16/144], loss=84.8190
	step [17/144], loss=101.7725
	step [18/144], loss=91.9511
	step [19/144], loss=79.6462
	step [20/144], loss=96.0298
	step [21/144], loss=89.4696
	step [22/144], loss=89.1394
	step [23/144], loss=78.3915
	step [24/144], loss=86.9245
	step [25/144], loss=86.7120
	step [26/144], loss=84.2811
	step [27/144], loss=92.8348
	step [28/144], loss=86.3620
	step [29/144], loss=93.0917
	step [30/144], loss=81.9300
	step [31/144], loss=84.8076
	step [32/144], loss=99.8627
	step [33/144], loss=99.7389
	step [34/144], loss=92.2033
	step [35/144], loss=84.9031
	step [36/144], loss=82.5715
	step [37/144], loss=78.1582
	step [38/144], loss=90.7837
	step [39/144], loss=98.8747
	step [40/144], loss=87.6044
	step [41/144], loss=85.9341
	step [42/144], loss=91.3567
	step [43/144], loss=86.1560
	step [44/144], loss=100.3978
	step [45/144], loss=80.8370
	step [46/144], loss=87.8282
	step [47/144], loss=89.9233
	step [48/144], loss=77.4228
	step [49/144], loss=100.5836
	step [50/144], loss=103.9946
	step [51/144], loss=89.2315
	step [52/144], loss=74.9829
	step [53/144], loss=86.1145
	step [54/144], loss=82.1068
	step [55/144], loss=90.7460
	step [56/144], loss=83.7111
	step [57/144], loss=93.0770
	step [58/144], loss=92.2842
	step [59/144], loss=97.3100
	step [60/144], loss=81.8061
	step [61/144], loss=76.7311
	step [62/144], loss=81.4153
	step [63/144], loss=87.7603
	step [64/144], loss=88.4523
	step [65/144], loss=93.9194
	step [66/144], loss=99.2141
	step [67/144], loss=83.1691
	step [68/144], loss=94.6143
	step [69/144], loss=84.0541
	step [70/144], loss=88.8194
	step [71/144], loss=77.5073
	step [72/144], loss=79.4007
	step [73/144], loss=93.9149
	step [74/144], loss=77.3684
	step [75/144], loss=96.7723
	step [76/144], loss=79.7379
	step [77/144], loss=91.2295
	step [78/144], loss=85.1099
	step [79/144], loss=92.3101
	step [80/144], loss=99.5282
	step [81/144], loss=81.4474
	step [82/144], loss=99.2849
	step [83/144], loss=80.6711
	step [84/144], loss=81.4406
	step [85/144], loss=87.4517
	step [86/144], loss=81.5613
	step [87/144], loss=99.8394
	step [88/144], loss=93.8483
	step [89/144], loss=86.8118
	step [90/144], loss=80.1726
	step [91/144], loss=83.9384
	step [92/144], loss=83.6786
	step [93/144], loss=90.0676
	step [94/144], loss=92.2129
	step [95/144], loss=78.4393
	step [96/144], loss=86.7729
	step [97/144], loss=94.6319
	step [98/144], loss=108.4892
	step [99/144], loss=82.0455
	step [100/144], loss=93.3475
	step [101/144], loss=97.5082
	step [102/144], loss=89.2064
	step [103/144], loss=92.7104
	step [104/144], loss=74.3200
	step [105/144], loss=87.1978
	step [106/144], loss=76.5791
	step [107/144], loss=82.4693
	step [108/144], loss=73.1744
	step [109/144], loss=96.1353
	step [110/144], loss=86.2764
	step [111/144], loss=98.7380
	step [112/144], loss=108.0451
	step [113/144], loss=79.0226
	step [114/144], loss=97.8211
	step [115/144], loss=91.4236
	step [116/144], loss=86.7658
	step [117/144], loss=79.9696
	step [118/144], loss=95.6214
	step [119/144], loss=86.8545
	step [120/144], loss=90.6192
	step [121/144], loss=91.3278
	step [122/144], loss=82.7612
	step [123/144], loss=90.8029
	step [124/144], loss=94.7115
	step [125/144], loss=96.4712
	step [126/144], loss=88.5272
	step [127/144], loss=108.6114
	step [128/144], loss=105.1764
	step [129/144], loss=95.4686
	step [130/144], loss=95.1198
	step [131/144], loss=95.6906
	step [132/144], loss=89.9259
	step [133/144], loss=96.6376
	step [134/144], loss=110.5299
	step [135/144], loss=100.0352
	step [136/144], loss=100.2378
	step [137/144], loss=83.8432
	step [138/144], loss=78.9790
	step [139/144], loss=81.7110
	step [140/144], loss=83.9464
	step [141/144], loss=75.9904
	step [142/144], loss=80.9008
	step [143/144], loss=84.5679
	step [144/144], loss=27.1100
	Evaluating
	loss=0.0131, precision=0.3776, recall=0.8342, f1=0.5199
Training epoch 57
	step [1/144], loss=98.9084
	step [2/144], loss=75.1248
	step [3/144], loss=86.8434
	step [4/144], loss=80.1048
	step [5/144], loss=92.0706
	step [6/144], loss=96.5014
	step [7/144], loss=93.3904
	step [8/144], loss=113.1174
	step [9/144], loss=96.9635
	step [10/144], loss=86.9024
	step [11/144], loss=82.4791
	step [12/144], loss=88.5378
	step [13/144], loss=87.1965
	step [14/144], loss=74.4765
	step [15/144], loss=118.5353
	step [16/144], loss=98.2761
	step [17/144], loss=90.6333
	step [18/144], loss=73.4145
	step [19/144], loss=89.7421
	step [20/144], loss=97.6140
	step [21/144], loss=93.4855
	step [22/144], loss=81.6075
	step [23/144], loss=94.3078
	step [24/144], loss=90.6629
	step [25/144], loss=97.1776
	step [26/144], loss=99.6922
	step [27/144], loss=90.4042
	step [28/144], loss=77.4568
	step [29/144], loss=76.1766
	step [30/144], loss=72.7896
	step [31/144], loss=80.3831
	step [32/144], loss=84.8138
	step [33/144], loss=92.4171
	step [34/144], loss=81.9116
	step [35/144], loss=70.8678
	step [36/144], loss=97.5583
	step [37/144], loss=101.4685
	step [38/144], loss=99.3135
	step [39/144], loss=83.3847
	step [40/144], loss=98.4585
	step [41/144], loss=82.1317
	step [42/144], loss=79.3795
	step [43/144], loss=86.2964
	step [44/144], loss=85.4665
	step [45/144], loss=71.6932
	step [46/144], loss=91.3051
	step [47/144], loss=119.9436
	step [48/144], loss=85.0888
	step [49/144], loss=79.6867
	step [50/144], loss=92.1048
	step [51/144], loss=103.9365
	step [52/144], loss=104.4191
	step [53/144], loss=81.4396
	step [54/144], loss=86.2568
	step [55/144], loss=109.9070
	step [56/144], loss=91.9857
	step [57/144], loss=87.3892
	step [58/144], loss=72.0919
	step [59/144], loss=98.0520
	step [60/144], loss=90.2084
	step [61/144], loss=92.9174
	step [62/144], loss=98.1829
	step [63/144], loss=77.3767
	step [64/144], loss=89.5526
	step [65/144], loss=96.2421
	step [66/144], loss=79.1604
	step [67/144], loss=93.7851
	step [68/144], loss=89.4559
	step [69/144], loss=94.8095
	step [70/144], loss=90.6778
	step [71/144], loss=83.1689
	step [72/144], loss=81.5201
	step [73/144], loss=87.2304
	step [74/144], loss=93.8365
	step [75/144], loss=90.4803
	step [76/144], loss=90.4944
	step [77/144], loss=90.8859
	step [78/144], loss=92.8161
	step [79/144], loss=95.0393
	step [80/144], loss=72.1745
	step [81/144], loss=98.8239
	step [82/144], loss=99.7212
	step [83/144], loss=81.7044
	step [84/144], loss=83.5973
	step [85/144], loss=85.9739
	step [86/144], loss=93.7501
	step [87/144], loss=98.1154
	step [88/144], loss=88.3054
	step [89/144], loss=67.3159
	step [90/144], loss=83.2381
	step [91/144], loss=87.2805
	step [92/144], loss=88.1933
	step [93/144], loss=106.4295
	step [94/144], loss=98.8367
	step [95/144], loss=91.5841
	step [96/144], loss=79.5992
	step [97/144], loss=92.6864
	step [98/144], loss=88.7078
	step [99/144], loss=102.1080
	step [100/144], loss=96.9779
	step [101/144], loss=71.8181
	step [102/144], loss=91.3510
	step [103/144], loss=78.9515
	step [104/144], loss=67.8066
	step [105/144], loss=75.5010
	step [106/144], loss=87.1701
	step [107/144], loss=110.2922
	step [108/144], loss=83.2838
	step [109/144], loss=72.4304
	step [110/144], loss=81.1032
	step [111/144], loss=104.3223
	step [112/144], loss=86.4484
	step [113/144], loss=92.9552
	step [114/144], loss=78.2971
	step [115/144], loss=87.0089
	step [116/144], loss=96.8467
	step [117/144], loss=90.5416
	step [118/144], loss=73.4658
	step [119/144], loss=73.0685
	step [120/144], loss=78.7954
	step [121/144], loss=106.9877
	step [122/144], loss=81.3031
	step [123/144], loss=95.3925
	step [124/144], loss=91.7086
	step [125/144], loss=89.5437
	step [126/144], loss=78.2887
	step [127/144], loss=79.7595
	step [128/144], loss=92.6444
	step [129/144], loss=80.7901
	step [130/144], loss=72.0235
	step [131/144], loss=100.8109
	step [132/144], loss=88.6922
	step [133/144], loss=90.3598
	step [134/144], loss=99.9187
	step [135/144], loss=100.0582
	step [136/144], loss=110.1129
	step [137/144], loss=83.4884
	step [138/144], loss=75.9389
	step [139/144], loss=91.6002
	step [140/144], loss=77.5159
	step [141/144], loss=100.1599
	step [142/144], loss=74.6403
	step [143/144], loss=91.0724
	step [144/144], loss=47.1334
	Evaluating
	loss=0.0138, precision=0.3680, recall=0.8481, f1=0.5133
Training epoch 58
	step [1/144], loss=87.1593
	step [2/144], loss=103.3410
	step [3/144], loss=96.0276
	step [4/144], loss=88.4048
	step [5/144], loss=76.6119
	step [6/144], loss=104.0636
	step [7/144], loss=93.6355
	step [8/144], loss=85.0572
	step [9/144], loss=79.4980
	step [10/144], loss=80.9181
	step [11/144], loss=81.2917
	step [12/144], loss=89.4375
	step [13/144], loss=90.9093
	step [14/144], loss=60.6773
	step [15/144], loss=87.1593
	step [16/144], loss=86.3631
	step [17/144], loss=92.4908
	step [18/144], loss=84.6409
	step [19/144], loss=102.3613
	step [20/144], loss=93.2695
	step [21/144], loss=78.2274
	step [22/144], loss=87.5390
	step [23/144], loss=102.1685
	step [24/144], loss=90.3761
	step [25/144], loss=104.1834
	step [26/144], loss=93.5484
	step [27/144], loss=81.5520
	step [28/144], loss=83.1255
	step [29/144], loss=108.4969
	step [30/144], loss=98.8764
	step [31/144], loss=71.5958
	step [32/144], loss=74.6985
	step [33/144], loss=78.6787
	step [34/144], loss=86.9834
	step [35/144], loss=75.7865
	step [36/144], loss=78.9071
	step [37/144], loss=70.9309
	step [38/144], loss=91.1165
	step [39/144], loss=92.8924
	step [40/144], loss=81.9381
	step [41/144], loss=96.8584
	step [42/144], loss=95.8986
	step [43/144], loss=83.6183
	step [44/144], loss=89.0836
	step [45/144], loss=98.5573
	step [46/144], loss=80.2787
	step [47/144], loss=89.4475
	step [48/144], loss=85.9669
	step [49/144], loss=91.6113
	step [50/144], loss=83.0375
	step [51/144], loss=98.6684
	step [52/144], loss=107.8043
	step [53/144], loss=84.1045
	step [54/144], loss=93.8078
	step [55/144], loss=101.1874
	step [56/144], loss=93.8372
	step [57/144], loss=82.5679
	step [58/144], loss=96.2863
	step [59/144], loss=79.4005
	step [60/144], loss=77.7193
	step [61/144], loss=87.6778
	step [62/144], loss=80.3240
	step [63/144], loss=86.6496
	step [64/144], loss=75.5483
	step [65/144], loss=81.9084
	step [66/144], loss=87.3131
	step [67/144], loss=101.8342
	step [68/144], loss=76.2007
	step [69/144], loss=90.0848
	step [70/144], loss=92.5299
	step [71/144], loss=70.4743
	step [72/144], loss=99.1104
	step [73/144], loss=77.3747
	step [74/144], loss=66.8422
	step [75/144], loss=85.0574
	step [76/144], loss=102.9290
	step [77/144], loss=83.5051
	step [78/144], loss=79.6027
	step [79/144], loss=86.3643
	step [80/144], loss=90.5089
	step [81/144], loss=83.6792
	step [82/144], loss=94.2938
	step [83/144], loss=81.5794
	step [84/144], loss=89.9544
	step [85/144], loss=87.9396
	step [86/144], loss=86.4028
	step [87/144], loss=106.9802
	step [88/144], loss=77.9031
	step [89/144], loss=104.5096
	step [90/144], loss=104.0358
	step [91/144], loss=104.3508
	step [92/144], loss=99.1899
	step [93/144], loss=89.8065
	step [94/144], loss=95.5027
	step [95/144], loss=85.3324
	step [96/144], loss=86.0075
	step [97/144], loss=86.0906
	step [98/144], loss=84.8219
	step [99/144], loss=76.5828
	step [100/144], loss=93.3232
	step [101/144], loss=70.0320
	step [102/144], loss=89.9289
	step [103/144], loss=98.3164
	step [104/144], loss=87.1996
	step [105/144], loss=75.4568
	step [106/144], loss=83.3923
	step [107/144], loss=73.4673
	step [108/144], loss=96.6943
	step [109/144], loss=75.2342
	step [110/144], loss=96.2080
	step [111/144], loss=94.9957
	step [112/144], loss=109.4225
	step [113/144], loss=86.9336
	step [114/144], loss=95.3897
	step [115/144], loss=99.3439
	step [116/144], loss=95.1260
	step [117/144], loss=89.7376
	step [118/144], loss=101.6446
	step [119/144], loss=78.9150
	step [120/144], loss=95.9524
	step [121/144], loss=91.3035
	step [122/144], loss=78.1270
	step [123/144], loss=96.9483
	step [124/144], loss=70.7145
	step [125/144], loss=83.5485
	step [126/144], loss=84.8702
	step [127/144], loss=75.5914
	step [128/144], loss=114.2799
	step [129/144], loss=80.3074
	step [130/144], loss=90.6192
	step [131/144], loss=86.5994
	step [132/144], loss=95.9516
	step [133/144], loss=83.3968
	step [134/144], loss=82.7734
	step [135/144], loss=84.6477
	step [136/144], loss=87.3526
	step [137/144], loss=78.5880
	step [138/144], loss=86.1523
	step [139/144], loss=94.6551
	step [140/144], loss=95.1507
	step [141/144], loss=76.8272
	step [142/144], loss=89.7455
	step [143/144], loss=88.8538
	step [144/144], loss=29.8339
	Evaluating
	loss=0.0155, precision=0.3335, recall=0.8485, f1=0.4788
Training epoch 59
	step [1/144], loss=86.3170
	step [2/144], loss=97.6928
	step [3/144], loss=97.3933
	step [4/144], loss=80.9344
	step [5/144], loss=90.1249
	step [6/144], loss=71.4708
	step [7/144], loss=70.2699
	step [8/144], loss=87.2751
	step [9/144], loss=87.6264
	step [10/144], loss=73.2806
	step [11/144], loss=94.7128
	step [12/144], loss=98.6613
	step [13/144], loss=88.2719
	step [14/144], loss=96.0859
	step [15/144], loss=86.5968
	step [16/144], loss=78.0670
	step [17/144], loss=88.6132
	step [18/144], loss=99.3294
	step [19/144], loss=98.2993
	step [20/144], loss=78.4189
	step [21/144], loss=95.4731
	step [22/144], loss=100.7819
	step [23/144], loss=78.8023
	step [24/144], loss=84.1991
	step [25/144], loss=98.7687
	step [26/144], loss=106.1674
	step [27/144], loss=93.5222
	step [28/144], loss=98.0610
	step [29/144], loss=87.3934
	step [30/144], loss=78.2303
	step [31/144], loss=75.4958
	step [32/144], loss=90.0859
	step [33/144], loss=93.4859
	step [34/144], loss=89.8181
	step [35/144], loss=82.7986
	step [36/144], loss=82.3816
	step [37/144], loss=70.3708
	step [38/144], loss=110.1702
	step [39/144], loss=92.2731
	step [40/144], loss=94.7522
	step [41/144], loss=90.5510
	step [42/144], loss=102.9677
	step [43/144], loss=91.3969
	step [44/144], loss=82.9737
	step [45/144], loss=95.0912
	step [46/144], loss=81.2714
	step [47/144], loss=86.2757
	step [48/144], loss=79.3084
	step [49/144], loss=91.5157
	step [50/144], loss=85.4324
	step [51/144], loss=75.5207
	step [52/144], loss=76.8854
	step [53/144], loss=102.3797
	step [54/144], loss=86.5124
	step [55/144], loss=91.7519
	step [56/144], loss=88.0656
	step [57/144], loss=96.3377
	step [58/144], loss=83.1103
	step [59/144], loss=84.8479
	step [60/144], loss=82.0460
	step [61/144], loss=100.8127
	step [62/144], loss=95.7189
	step [63/144], loss=95.1589
	step [64/144], loss=75.9417
	step [65/144], loss=85.6728
	step [66/144], loss=91.5426
	step [67/144], loss=83.4701
	step [68/144], loss=96.3376
	step [69/144], loss=94.6639
	step [70/144], loss=77.0813
	step [71/144], loss=91.2016
	step [72/144], loss=93.8100
	step [73/144], loss=91.3887
	step [74/144], loss=90.1331
	step [75/144], loss=100.5492
	step [76/144], loss=76.9450
	step [77/144], loss=80.2153
	step [78/144], loss=88.3261
	step [79/144], loss=98.8356
	step [80/144], loss=89.2529
	step [81/144], loss=93.7831
	step [82/144], loss=81.9747
	step [83/144], loss=90.9875
	step [84/144], loss=83.7001
	step [85/144], loss=93.2974
	step [86/144], loss=84.8098
	step [87/144], loss=77.4253
	step [88/144], loss=91.8438
	step [89/144], loss=75.4991
	step [90/144], loss=74.5614
	step [91/144], loss=90.7836
	step [92/144], loss=83.9948
	step [93/144], loss=73.9815
	step [94/144], loss=85.8840
	step [95/144], loss=73.7342
	step [96/144], loss=75.6272
	step [97/144], loss=90.8205
	step [98/144], loss=103.8075
	step [99/144], loss=89.5862
	step [100/144], loss=76.8504
	step [101/144], loss=93.4339
	step [102/144], loss=82.5801
	step [103/144], loss=86.8099
	step [104/144], loss=76.9725
	step [105/144], loss=93.8019
	step [106/144], loss=75.6314
	step [107/144], loss=104.8593
	step [108/144], loss=84.4704
	step [109/144], loss=78.5415
	step [110/144], loss=94.7140
	step [111/144], loss=90.6243
	step [112/144], loss=81.8725
	step [113/144], loss=91.1032
	step [114/144], loss=70.2631
	step [115/144], loss=73.5007
	step [116/144], loss=98.0302
	step [117/144], loss=82.2576
	step [118/144], loss=97.1147
	step [119/144], loss=80.3600
	step [120/144], loss=79.4734
	step [121/144], loss=83.4861
	step [122/144], loss=83.3250
	step [123/144], loss=90.1847
	step [124/144], loss=105.6417
	step [125/144], loss=76.7084
	step [126/144], loss=102.8326
	step [127/144], loss=82.6788
	step [128/144], loss=109.3509
	step [129/144], loss=81.2403
	step [130/144], loss=83.7366
	step [131/144], loss=87.0326
	step [132/144], loss=100.3572
	step [133/144], loss=82.9172
	step [134/144], loss=97.3022
	step [135/144], loss=81.3644
	step [136/144], loss=90.9884
	step [137/144], loss=93.0373
	step [138/144], loss=89.6916
	step [139/144], loss=93.2508
	step [140/144], loss=80.0940
	step [141/144], loss=98.5953
	step [142/144], loss=93.6528
	step [143/144], loss=90.5826
	step [144/144], loss=32.2765
	Evaluating
	loss=0.0126, precision=0.4062, recall=0.8413, f1=0.5478
saving model as: 1_saved_model.pth
Training epoch 60
	step [1/144], loss=85.8609
	step [2/144], loss=97.7859
	step [3/144], loss=101.9970
	step [4/144], loss=93.2464
	step [5/144], loss=87.5076
	step [6/144], loss=93.6092
	step [7/144], loss=94.6842
	step [8/144], loss=89.8386
	step [9/144], loss=89.3669
	step [10/144], loss=88.9186
	step [11/144], loss=98.6710
	step [12/144], loss=97.1175
	step [13/144], loss=102.8612
	step [14/144], loss=76.1064
	step [15/144], loss=90.1507
	step [16/144], loss=96.4885
	step [17/144], loss=86.3436
	step [18/144], loss=91.9932
	step [19/144], loss=86.5835
	step [20/144], loss=81.0047
	step [21/144], loss=83.9475
	step [22/144], loss=94.6277
	step [23/144], loss=72.0995
	step [24/144], loss=89.7997
	step [25/144], loss=76.7405
	step [26/144], loss=75.3217
	step [27/144], loss=83.3790
	step [28/144], loss=88.8302
	step [29/144], loss=93.9662
	step [30/144], loss=84.8569
	step [31/144], loss=90.8445
	step [32/144], loss=80.8714
	step [33/144], loss=73.2617
	step [34/144], loss=104.6128
	step [35/144], loss=87.8705
	step [36/144], loss=83.9604
	step [37/144], loss=84.2058
	step [38/144], loss=92.7868
	step [39/144], loss=94.4454
	step [40/144], loss=64.3195
	step [41/144], loss=81.8171
	step [42/144], loss=92.6956
	step [43/144], loss=79.8400
	step [44/144], loss=92.2767
	step [45/144], loss=100.4144
	step [46/144], loss=83.5856
	step [47/144], loss=85.8082
	step [48/144], loss=86.9777
	step [49/144], loss=77.4395
	step [50/144], loss=96.8501
	step [51/144], loss=86.0658
	step [52/144], loss=86.2622
	step [53/144], loss=81.7758
	step [54/144], loss=74.4942
	step [55/144], loss=98.4731
	step [56/144], loss=96.2861
	step [57/144], loss=81.5662
	step [58/144], loss=98.7393
	step [59/144], loss=93.4044
	step [60/144], loss=99.5346
	step [61/144], loss=82.3156
	step [62/144], loss=99.8975
	step [63/144], loss=90.5712
	step [64/144], loss=61.2384
	step [65/144], loss=82.0114
	step [66/144], loss=77.2298
	step [67/144], loss=93.2265
	step [68/144], loss=85.9446
	step [69/144], loss=78.0087
	step [70/144], loss=99.6389
	step [71/144], loss=92.1012
	step [72/144], loss=90.6991
	step [73/144], loss=65.3846
	step [74/144], loss=94.9090
	step [75/144], loss=76.0669
	step [76/144], loss=106.4996
	step [77/144], loss=96.4259
	step [78/144], loss=84.3342
	step [79/144], loss=93.7615
	step [80/144], loss=90.8833
	step [81/144], loss=101.3486
	step [82/144], loss=103.4606
	step [83/144], loss=92.0275
	step [84/144], loss=86.7510
	step [85/144], loss=87.2885
	step [86/144], loss=85.3890
	step [87/144], loss=92.1137
	step [88/144], loss=66.7556
	step [89/144], loss=81.3463
	step [90/144], loss=87.8081
	step [91/144], loss=86.0475
	step [92/144], loss=90.4075
	step [93/144], loss=101.9422
	step [94/144], loss=86.3588
	step [95/144], loss=76.6133
	step [96/144], loss=106.4875
	step [97/144], loss=78.5716
	step [98/144], loss=87.3148
	step [99/144], loss=87.6202
	step [100/144], loss=77.1970
	step [101/144], loss=83.3971
	step [102/144], loss=80.7326
	step [103/144], loss=87.6113
	step [104/144], loss=81.2203
	step [105/144], loss=78.9122
	step [106/144], loss=95.0874
	step [107/144], loss=77.9903
	step [108/144], loss=85.4047
	step [109/144], loss=89.3960
	step [110/144], loss=77.7102
	step [111/144], loss=96.3782
	step [112/144], loss=68.5959
	step [113/144], loss=98.8597
	step [114/144], loss=91.2347
	step [115/144], loss=76.5526
	step [116/144], loss=85.4982
	step [117/144], loss=79.9846
	step [118/144], loss=85.7351
	step [119/144], loss=101.5085
	step [120/144], loss=74.2104
	step [121/144], loss=74.0245
	step [122/144], loss=83.1128
	step [123/144], loss=67.2756
	step [124/144], loss=76.4311
	step [125/144], loss=92.0763
	step [126/144], loss=101.1179
	step [127/144], loss=94.4750
	step [128/144], loss=91.8963
	step [129/144], loss=95.2554
	step [130/144], loss=106.0484
	step [131/144], loss=98.2828
	step [132/144], loss=107.3010
	step [133/144], loss=69.5317
	step [134/144], loss=78.2620
	step [135/144], loss=96.2108
	step [136/144], loss=107.4960
	step [137/144], loss=80.2655
	step [138/144], loss=85.1744
	step [139/144], loss=70.7125
	step [140/144], loss=78.1973
	step [141/144], loss=105.6070
	step [142/144], loss=83.4291
	step [143/144], loss=81.8426
	step [144/144], loss=33.8460
	Evaluating
	loss=0.0138, precision=0.3720, recall=0.8592, f1=0.5192
Training epoch 61
	step [1/144], loss=90.7094
	step [2/144], loss=79.0457
	step [3/144], loss=92.0014
	step [4/144], loss=99.3190
	step [5/144], loss=92.5088
	step [6/144], loss=84.1846
	step [7/144], loss=92.2318
	step [8/144], loss=93.1443
	step [9/144], loss=88.1010
	step [10/144], loss=89.8239
	step [11/144], loss=99.8080
	step [12/144], loss=68.0601
	step [13/144], loss=76.0545
	step [14/144], loss=73.0126
	step [15/144], loss=80.5671
	step [16/144], loss=82.0650
	step [17/144], loss=86.0696
	step [18/144], loss=87.9297
	step [19/144], loss=94.0948
	step [20/144], loss=90.1129
	step [21/144], loss=95.6527
	step [22/144], loss=81.5541
	step [23/144], loss=78.9090
	step [24/144], loss=68.0636
	step [25/144], loss=82.6996
	step [26/144], loss=93.2081
	step [27/144], loss=82.3943
	step [28/144], loss=90.8133
	step [29/144], loss=71.1708
	step [30/144], loss=60.5077
	step [31/144], loss=94.1121
	step [32/144], loss=84.1076
	step [33/144], loss=73.0900
	step [34/144], loss=93.5307
	step [35/144], loss=112.3354
	step [36/144], loss=79.4172
	step [37/144], loss=77.4496
	step [38/144], loss=106.8573
	step [39/144], loss=81.1341
	step [40/144], loss=107.8415
	step [41/144], loss=89.9681
	step [42/144], loss=93.6500
	step [43/144], loss=80.4640
	step [44/144], loss=90.9148
	step [45/144], loss=86.5405
	step [46/144], loss=95.8526
	step [47/144], loss=81.7851
	step [48/144], loss=79.2034
	step [49/144], loss=84.9022
	step [50/144], loss=89.3162
	step [51/144], loss=95.7529
	step [52/144], loss=90.5525
	step [53/144], loss=85.9222
	step [54/144], loss=82.2304
	step [55/144], loss=85.0470
	step [56/144], loss=76.8377
	step [57/144], loss=106.5117
	step [58/144], loss=83.0990
	step [59/144], loss=83.6537
	step [60/144], loss=83.8021
	step [61/144], loss=97.1707
	step [62/144], loss=86.3429
	step [63/144], loss=72.9736
	step [64/144], loss=91.1887
	step [65/144], loss=102.0016
	step [66/144], loss=91.5735
	step [67/144], loss=77.7219
	step [68/144], loss=76.4862
	step [69/144], loss=89.9344
	step [70/144], loss=95.4720
	step [71/144], loss=85.3001
	step [72/144], loss=87.7859
	step [73/144], loss=89.0772
	step [74/144], loss=83.9886
	step [75/144], loss=97.2280
	step [76/144], loss=90.3470
	step [77/144], loss=84.4683
	step [78/144], loss=82.9550
	step [79/144], loss=73.2137
	step [80/144], loss=101.4631
	step [81/144], loss=77.4174
	step [82/144], loss=92.2891
	step [83/144], loss=91.4596
	step [84/144], loss=98.9514
	step [85/144], loss=85.8812
	step [86/144], loss=76.2457
	step [87/144], loss=94.3215
	step [88/144], loss=93.9419
	step [89/144], loss=80.4721
	step [90/144], loss=75.6610
	step [91/144], loss=87.7551
	step [92/144], loss=98.3996
	step [93/144], loss=98.1856
	step [94/144], loss=89.7989
	step [95/144], loss=67.5616
	step [96/144], loss=92.2404
	step [97/144], loss=83.0885
	step [98/144], loss=86.6448
	step [99/144], loss=85.2746
	step [100/144], loss=85.4000
	step [101/144], loss=83.3262
	step [102/144], loss=88.0655
	step [103/144], loss=91.8026
	step [104/144], loss=84.6226
	step [105/144], loss=84.7368
	step [106/144], loss=85.5697
	step [107/144], loss=85.9579
	step [108/144], loss=89.2689
	step [109/144], loss=84.1939
	step [110/144], loss=76.9112
	step [111/144], loss=92.1507
	step [112/144], loss=99.4300
	step [113/144], loss=82.4344
	step [114/144], loss=94.8515
	step [115/144], loss=99.1474
	step [116/144], loss=95.9180
	step [117/144], loss=70.9888
	step [118/144], loss=109.4253
	step [119/144], loss=79.0878
	step [120/144], loss=82.3116
	step [121/144], loss=85.3949
	step [122/144], loss=73.9761
	step [123/144], loss=90.7226
	step [124/144], loss=73.6019
	step [125/144], loss=74.1755
	step [126/144], loss=86.2689
	step [127/144], loss=78.4719
	step [128/144], loss=74.7420
	step [129/144], loss=95.5555
	step [130/144], loss=87.5958
	step [131/144], loss=94.1296
	step [132/144], loss=78.3083
	step [133/144], loss=92.1856
	step [134/144], loss=86.7750
	step [135/144], loss=87.4502
	step [136/144], loss=91.4152
	step [137/144], loss=103.4385
	step [138/144], loss=83.4853
	step [139/144], loss=96.7329
	step [140/144], loss=76.4086
	step [141/144], loss=88.7167
	step [142/144], loss=92.3916
	step [143/144], loss=72.0172
	step [144/144], loss=40.3066
	Evaluating
	loss=0.0114, precision=0.4301, recall=0.8505, f1=0.5713
saving model as: 1_saved_model.pth
Training epoch 62
	step [1/144], loss=79.9671
	step [2/144], loss=83.9584
	step [3/144], loss=76.1454
	step [4/144], loss=82.0043
	step [5/144], loss=88.7565
	step [6/144], loss=88.3521
	step [7/144], loss=98.5666
	step [8/144], loss=97.7910
	step [9/144], loss=78.1610
	step [10/144], loss=93.8017
	step [11/144], loss=82.2175
	step [12/144], loss=90.6308
	step [13/144], loss=103.7647
	step [14/144], loss=91.3451
	step [15/144], loss=85.2686
	step [16/144], loss=79.0304
	step [17/144], loss=93.7364
	step [18/144], loss=81.5212
	step [19/144], loss=84.3680
	step [20/144], loss=101.6063
	step [21/144], loss=87.6235
	step [22/144], loss=84.2169
	step [23/144], loss=90.7665
	step [24/144], loss=76.3419
	step [25/144], loss=82.2039
	step [26/144], loss=104.3375
	step [27/144], loss=90.1213
	step [28/144], loss=88.8148
	step [29/144], loss=83.5837
	step [30/144], loss=77.2579
	step [31/144], loss=87.0047
	step [32/144], loss=85.1661
	step [33/144], loss=99.3439
	step [34/144], loss=79.9428
	step [35/144], loss=111.9205
	step [36/144], loss=81.6084
	step [37/144], loss=85.5657
	step [38/144], loss=76.5791
	step [39/144], loss=88.4371
	step [40/144], loss=74.6319
	step [41/144], loss=82.0763
	step [42/144], loss=84.9623
	step [43/144], loss=92.9574
	step [44/144], loss=108.0712
	step [45/144], loss=96.1183
	step [46/144], loss=93.6526
	step [47/144], loss=86.4919
	step [48/144], loss=95.7481
	step [49/144], loss=78.1950
	step [50/144], loss=84.3237
	step [51/144], loss=92.7659
	step [52/144], loss=78.8508
	step [53/144], loss=79.9011
	step [54/144], loss=93.4761
	step [55/144], loss=96.1105
	step [56/144], loss=82.5310
	step [57/144], loss=86.7792
	step [58/144], loss=85.5011
	step [59/144], loss=75.8777
	step [60/144], loss=71.9085
	step [61/144], loss=89.0305
	step [62/144], loss=98.8075
	step [63/144], loss=104.9951
	step [64/144], loss=82.1646
	step [65/144], loss=93.0942
	step [66/144], loss=77.3019
	step [67/144], loss=87.0955
	step [68/144], loss=82.9236
	step [69/144], loss=87.5764
	step [70/144], loss=79.7094
	step [71/144], loss=99.1764
	step [72/144], loss=94.4716
	step [73/144], loss=58.3506
	step [74/144], loss=86.7551
	step [75/144], loss=73.5796
	step [76/144], loss=95.5723
	step [77/144], loss=69.4476
	step [78/144], loss=81.7232
	step [79/144], loss=85.8466
	step [80/144], loss=84.1795
	step [81/144], loss=93.9845
	step [82/144], loss=80.7151
	step [83/144], loss=89.1464
	step [84/144], loss=79.1317
	step [85/144], loss=92.0608
	step [86/144], loss=85.3209
	step [87/144], loss=97.7542
	step [88/144], loss=101.7126
	step [89/144], loss=97.6868
	step [90/144], loss=94.5532
	step [91/144], loss=95.1763
	step [92/144], loss=90.3925
	step [93/144], loss=93.6768
	step [94/144], loss=99.1701
	step [95/144], loss=74.7246
	step [96/144], loss=69.6578
	step [97/144], loss=104.1914
	step [98/144], loss=82.8688
	step [99/144], loss=93.2758
	step [100/144], loss=78.7480
	step [101/144], loss=93.1355
	step [102/144], loss=74.2606
	step [103/144], loss=91.6261
	step [104/144], loss=81.5374
	step [105/144], loss=91.1845
	step [106/144], loss=91.0935
	step [107/144], loss=102.5583
	step [108/144], loss=92.0824
	step [109/144], loss=76.8688
	step [110/144], loss=97.6068
	step [111/144], loss=72.1501
	step [112/144], loss=82.4280
	step [113/144], loss=97.3997
	step [114/144], loss=95.4014
	step [115/144], loss=104.7668
	step [116/144], loss=69.9433
	step [117/144], loss=82.1868
	step [118/144], loss=85.0126
	step [119/144], loss=88.9009
	step [120/144], loss=83.4629
	step [121/144], loss=93.7642
	step [122/144], loss=81.1626
	step [123/144], loss=72.9095
	step [124/144], loss=85.5055
	step [125/144], loss=93.5697
	step [126/144], loss=93.7504
	step [127/144], loss=82.3575
	step [128/144], loss=92.0948
	step [129/144], loss=93.0928
	step [130/144], loss=86.6020
	step [131/144], loss=74.0100
	step [132/144], loss=74.2251
	step [133/144], loss=67.9961
	step [134/144], loss=78.5871
	step [135/144], loss=97.6933
	step [136/144], loss=80.6481
	step [137/144], loss=98.2037
	step [138/144], loss=87.1499
	step [139/144], loss=106.3395
	step [140/144], loss=74.7658
	step [141/144], loss=81.7780
	step [142/144], loss=86.1984
	step [143/144], loss=81.4158
	step [144/144], loss=24.4297
	Evaluating
	loss=0.0128, precision=0.3731, recall=0.8653, f1=0.5214
Training epoch 63
	step [1/144], loss=81.5199
	step [2/144], loss=85.7096
	step [3/144], loss=100.5575
	step [4/144], loss=91.8062
	step [5/144], loss=80.1157
	step [6/144], loss=82.3421
	step [7/144], loss=96.7214
	step [8/144], loss=69.0066
	step [9/144], loss=84.4254
	step [10/144], loss=80.3177
	step [11/144], loss=83.9890
	step [12/144], loss=87.3309
	step [13/144], loss=109.2137
	step [14/144], loss=76.2547
	step [15/144], loss=86.3337
	step [16/144], loss=75.3534
	step [17/144], loss=89.5887
	step [18/144], loss=82.7457
	step [19/144], loss=83.0013
	step [20/144], loss=90.0772
	step [21/144], loss=84.7749
	step [22/144], loss=74.5914
	step [23/144], loss=77.0484
	step [24/144], loss=88.7845
	step [25/144], loss=80.5725
	step [26/144], loss=85.5440
	step [27/144], loss=75.5954
	step [28/144], loss=86.3696
	step [29/144], loss=85.0217
	step [30/144], loss=99.5116
	step [31/144], loss=84.2381
	step [32/144], loss=83.4619
	step [33/144], loss=82.8911
	step [34/144], loss=82.9515
	step [35/144], loss=79.8443
	step [36/144], loss=84.0030
	step [37/144], loss=93.5469
	step [38/144], loss=69.0737
	step [39/144], loss=74.1926
	step [40/144], loss=73.9099
	step [41/144], loss=77.5360
	step [42/144], loss=89.8141
	step [43/144], loss=89.4520
	step [44/144], loss=106.9797
	step [45/144], loss=95.4501
	step [46/144], loss=84.3196
	step [47/144], loss=95.1541
	step [48/144], loss=94.9579
	step [49/144], loss=102.1196
	step [50/144], loss=68.6051
	step [51/144], loss=92.8140
	step [52/144], loss=87.4963
	step [53/144], loss=88.4621
	step [54/144], loss=83.6185
	step [55/144], loss=90.0671
	step [56/144], loss=72.5872
	step [57/144], loss=65.2649
	step [58/144], loss=75.3938
	step [59/144], loss=80.1748
	step [60/144], loss=76.3933
	step [61/144], loss=88.8949
	step [62/144], loss=103.4053
	step [63/144], loss=79.8935
	step [64/144], loss=76.0405
	step [65/144], loss=77.4731
	step [66/144], loss=92.9577
	step [67/144], loss=92.3249
	step [68/144], loss=79.2164
	step [69/144], loss=89.2231
	step [70/144], loss=76.8405
	step [71/144], loss=92.7782
	step [72/144], loss=96.4837
	step [73/144], loss=76.6590
	step [74/144], loss=92.7740
	step [75/144], loss=97.6128
	step [76/144], loss=93.4836
	step [77/144], loss=99.9484
	step [78/144], loss=80.0368
	step [79/144], loss=95.6096
	step [80/144], loss=82.8961
	step [81/144], loss=93.3221
	step [82/144], loss=85.1996
	step [83/144], loss=77.7614
	step [84/144], loss=92.3124
	step [85/144], loss=87.8531
	step [86/144], loss=81.8755
	step [87/144], loss=104.6703
	step [88/144], loss=81.4015
	step [89/144], loss=86.1392
	step [90/144], loss=76.6470
	step [91/144], loss=77.6713
	step [92/144], loss=79.1663
	step [93/144], loss=80.6480
	step [94/144], loss=94.8698
	step [95/144], loss=83.7744
	step [96/144], loss=82.0986
	step [97/144], loss=88.3432
	step [98/144], loss=92.2623
	step [99/144], loss=83.4628
	step [100/144], loss=74.8246
	step [101/144], loss=86.1408
	step [102/144], loss=91.9607
	step [103/144], loss=81.6827
	step [104/144], loss=101.1676
	step [105/144], loss=72.5295
	step [106/144], loss=93.1482
	step [107/144], loss=96.8080
	step [108/144], loss=87.4081
	step [109/144], loss=90.6751
	step [110/144], loss=91.0162
	step [111/144], loss=101.1272
	step [112/144], loss=83.3022
	step [113/144], loss=95.1761
	step [114/144], loss=105.2221
	step [115/144], loss=80.1036
	step [116/144], loss=117.1188
	step [117/144], loss=85.1320
	step [118/144], loss=90.3024
	step [119/144], loss=104.5880
	step [120/144], loss=86.0476
	step [121/144], loss=90.7281
	step [122/144], loss=84.7376
	step [123/144], loss=78.7357
	step [124/144], loss=88.5224
	step [125/144], loss=88.5640
	step [126/144], loss=88.8822
	step [127/144], loss=99.0813
	step [128/144], loss=78.7663
	step [129/144], loss=88.3454
	step [130/144], loss=96.3660
	step [131/144], loss=75.6766
	step [132/144], loss=90.6152
	step [133/144], loss=86.4495
	step [134/144], loss=82.8570
	step [135/144], loss=83.3140
	step [136/144], loss=77.6834
	step [137/144], loss=86.2194
	step [138/144], loss=81.4509
	step [139/144], loss=75.7058
	step [140/144], loss=82.7889
	step [141/144], loss=102.8193
	step [142/144], loss=84.8605
	step [143/144], loss=84.0303
	step [144/144], loss=38.3488
	Evaluating
	loss=0.0134, precision=0.3626, recall=0.8432, f1=0.5071
Training epoch 64
	step [1/144], loss=87.9184
	step [2/144], loss=95.8076
	step [3/144], loss=105.8796
	step [4/144], loss=85.5310
	step [5/144], loss=91.5102
	step [6/144], loss=95.1414
	step [7/144], loss=92.2912
	step [8/144], loss=86.0165
	step [9/144], loss=105.5631
	step [10/144], loss=84.7466
	step [11/144], loss=91.4969
	step [12/144], loss=80.5817
	step [13/144], loss=93.9769
	step [14/144], loss=89.0170
	step [15/144], loss=85.0781
	step [16/144], loss=89.5503
	step [17/144], loss=85.3956
	step [18/144], loss=87.7017
	step [19/144], loss=70.4815
	step [20/144], loss=92.9923
	step [21/144], loss=89.0299
	step [22/144], loss=75.6408
	step [23/144], loss=87.4041
	step [24/144], loss=84.2663
	step [25/144], loss=80.7406
	step [26/144], loss=83.5048
	step [27/144], loss=78.1341
	step [28/144], loss=90.6889
	step [29/144], loss=71.7330
	step [30/144], loss=93.9648
	step [31/144], loss=90.0237
	step [32/144], loss=78.1589
	step [33/144], loss=92.1941
	step [34/144], loss=93.2312
	step [35/144], loss=86.8221
	step [36/144], loss=113.3348
	step [37/144], loss=81.6735
	step [38/144], loss=91.6367
	step [39/144], loss=89.3299
	step [40/144], loss=92.3220
	step [41/144], loss=78.5857
	step [42/144], loss=86.1653
	step [43/144], loss=84.2653
	step [44/144], loss=93.6980
	step [45/144], loss=89.1848
	step [46/144], loss=85.4011
	step [47/144], loss=95.9685
	step [48/144], loss=78.7264
	step [49/144], loss=78.1792
	step [50/144], loss=74.0756
	step [51/144], loss=77.7914
	step [52/144], loss=78.1232
	step [53/144], loss=92.7767
	step [54/144], loss=97.1851
	step [55/144], loss=71.1677
	step [56/144], loss=79.6054
	step [57/144], loss=89.8224
	step [58/144], loss=96.0442
	step [59/144], loss=97.7328
	step [60/144], loss=79.2150
	step [61/144], loss=81.6376
	step [62/144], loss=72.5877
	step [63/144], loss=82.0085
	step [64/144], loss=102.5233
	step [65/144], loss=72.8423
	step [66/144], loss=65.6104
	step [67/144], loss=81.6790
	step [68/144], loss=97.6109
	step [69/144], loss=90.4219
	step [70/144], loss=84.2047
	step [71/144], loss=76.7826
	step [72/144], loss=83.6040
	step [73/144], loss=86.3676
	step [74/144], loss=95.6334
	step [75/144], loss=78.9056
	step [76/144], loss=78.9112
	step [77/144], loss=87.1039
	step [78/144], loss=83.8742
	step [79/144], loss=69.2854
	step [80/144], loss=76.5100
	step [81/144], loss=83.5170
	step [82/144], loss=88.7581
	step [83/144], loss=79.0981
	step [84/144], loss=76.8286
	step [85/144], loss=101.2284
	step [86/144], loss=90.5567
	step [87/144], loss=87.3487
	step [88/144], loss=78.7321
	step [89/144], loss=97.3255
	step [90/144], loss=88.3530
	step [91/144], loss=102.7475
	step [92/144], loss=89.6352
	step [93/144], loss=87.6203
	step [94/144], loss=82.1335
	step [95/144], loss=106.8348
	step [96/144], loss=72.4333
	step [97/144], loss=101.5588
	step [98/144], loss=76.2063
	step [99/144], loss=83.2141
	step [100/144], loss=91.2512
	step [101/144], loss=80.5499
	step [102/144], loss=90.9574
	step [103/144], loss=63.7654
	step [104/144], loss=88.6003
	step [105/144], loss=91.1463
	step [106/144], loss=94.2921
	step [107/144], loss=95.0305
	step [108/144], loss=84.5634
	step [109/144], loss=76.8401
	step [110/144], loss=91.3954
	step [111/144], loss=89.1773
	step [112/144], loss=79.9713
	step [113/144], loss=95.2633
	step [114/144], loss=90.7942
	step [115/144], loss=91.0417
	step [116/144], loss=90.6966
	step [117/144], loss=73.0686
	step [118/144], loss=83.8277
	step [119/144], loss=92.1893
	step [120/144], loss=74.0949
	step [121/144], loss=68.5591
	step [122/144], loss=107.5494
	step [123/144], loss=90.0618
	step [124/144], loss=74.9074
	step [125/144], loss=72.4078
	step [126/144], loss=82.2875
	step [127/144], loss=99.8439
	step [128/144], loss=78.4811
	step [129/144], loss=83.0435
	step [130/144], loss=87.3273
	step [131/144], loss=83.9673
	step [132/144], loss=82.4623
	step [133/144], loss=87.1728
	step [134/144], loss=80.4409
	step [135/144], loss=90.7152
	step [136/144], loss=91.8326
	step [137/144], loss=94.2197
	step [138/144], loss=80.3634
	step [139/144], loss=83.5766
	step [140/144], loss=89.4384
	step [141/144], loss=90.4927
	step [142/144], loss=67.8305
	step [143/144], loss=83.4300
	step [144/144], loss=21.7874
	Evaluating
	loss=0.0132, precision=0.3566, recall=0.8535, f1=0.5030
Training epoch 65
	step [1/144], loss=83.7465
	step [2/144], loss=79.9569
	step [3/144], loss=103.0409
	step [4/144], loss=72.8920
	step [5/144], loss=103.1797
	step [6/144], loss=85.9453
	step [7/144], loss=87.0736
	step [8/144], loss=107.5729
	step [9/144], loss=86.0705
	step [10/144], loss=83.3791
	step [11/144], loss=78.0961
	step [12/144], loss=73.7323
	step [13/144], loss=82.6891
	step [14/144], loss=80.0004
	step [15/144], loss=73.7561
	step [16/144], loss=90.9371
	step [17/144], loss=74.5724
	step [18/144], loss=90.9769
	step [19/144], loss=97.4102
	step [20/144], loss=87.5016
	step [21/144], loss=104.8148
	step [22/144], loss=77.6255
	step [23/144], loss=81.9686
	step [24/144], loss=80.8259
	step [25/144], loss=84.3067
	step [26/144], loss=91.9701
	step [27/144], loss=84.6643
	step [28/144], loss=85.0330
	step [29/144], loss=82.0147
	step [30/144], loss=85.8990
	step [31/144], loss=85.2009
	step [32/144], loss=84.0711
	step [33/144], loss=79.7452
	step [34/144], loss=87.2638
	step [35/144], loss=76.0883
	step [36/144], loss=85.6148
	step [37/144], loss=89.2829
	step [38/144], loss=88.2234
	step [39/144], loss=75.8386
	step [40/144], loss=83.5677
	step [41/144], loss=73.3795
	step [42/144], loss=88.6344
	step [43/144], loss=77.4430
	step [44/144], loss=91.5756
	step [45/144], loss=69.6049
	step [46/144], loss=84.9552
	step [47/144], loss=77.4074
	step [48/144], loss=104.3335
	step [49/144], loss=85.7156
	step [50/144], loss=96.9118
	step [51/144], loss=85.4455
	step [52/144], loss=74.8486
	step [53/144], loss=66.6754
	step [54/144], loss=87.8992
	step [55/144], loss=83.3999
	step [56/144], loss=78.2909
	step [57/144], loss=96.6592
	step [58/144], loss=90.9439
	step [59/144], loss=82.6634
	step [60/144], loss=84.8552
	step [61/144], loss=86.2472
	step [62/144], loss=92.6934
	step [63/144], loss=84.8034
	step [64/144], loss=74.6646
	step [65/144], loss=93.5895
	step [66/144], loss=67.0558
	step [67/144], loss=76.5171
	step [68/144], loss=70.2884
	step [69/144], loss=91.7121
	step [70/144], loss=90.6647
	step [71/144], loss=82.0251
	step [72/144], loss=92.8494
	step [73/144], loss=94.7435
	step [74/144], loss=98.0078
	step [75/144], loss=74.6729
	step [76/144], loss=75.3725
	step [77/144], loss=88.5167
	step [78/144], loss=91.5649
	step [79/144], loss=95.5926
	step [80/144], loss=84.1441
	step [81/144], loss=91.6012
	step [82/144], loss=76.9530
	step [83/144], loss=101.5412
	step [84/144], loss=101.4014
	step [85/144], loss=95.4492
	step [86/144], loss=84.6616
	step [87/144], loss=86.2615
	step [88/144], loss=87.0951
	step [89/144], loss=100.1585
	step [90/144], loss=73.3106
	step [91/144], loss=103.5005
	step [92/144], loss=97.3893
	step [93/144], loss=84.3866
	step [94/144], loss=86.5906
	step [95/144], loss=82.6517
	step [96/144], loss=78.2613
	step [97/144], loss=72.0107
	step [98/144], loss=93.3565
	step [99/144], loss=92.6669
	step [100/144], loss=81.0411
	step [101/144], loss=76.1933
	step [102/144], loss=79.4535
	step [103/144], loss=87.1611
	step [104/144], loss=89.7269
	step [105/144], loss=87.1751
	step [106/144], loss=90.7890
	step [107/144], loss=81.3392
	step [108/144], loss=83.0423
	step [109/144], loss=107.5056
	step [110/144], loss=95.4964
	step [111/144], loss=65.9662
	step [112/144], loss=90.0334
	step [113/144], loss=76.2233
	step [114/144], loss=91.3248
	step [115/144], loss=92.9003
	step [116/144], loss=82.7722
	step [117/144], loss=97.3197
	step [118/144], loss=82.0902
	step [119/144], loss=79.5855
	step [120/144], loss=78.0506
	step [121/144], loss=88.4630
	step [122/144], loss=92.6930
	step [123/144], loss=88.8556
	step [124/144], loss=97.7370
	step [125/144], loss=96.5178
	step [126/144], loss=91.8048
	step [127/144], loss=92.3586
	step [128/144], loss=89.7892
	step [129/144], loss=79.4917
	step [130/144], loss=74.0205
	step [131/144], loss=80.6570
	step [132/144], loss=70.9803
	step [133/144], loss=93.5139
	step [134/144], loss=96.1454
	step [135/144], loss=82.1335
	step [136/144], loss=99.9434
	step [137/144], loss=90.4342
	step [138/144], loss=88.1431
	step [139/144], loss=84.5912
	step [140/144], loss=89.8365
	step [141/144], loss=62.9481
	step [142/144], loss=71.1496
	step [143/144], loss=70.1015
	step [144/144], loss=34.6178
	Evaluating
	loss=0.0133, precision=0.3597, recall=0.8468, f1=0.5049
Training epoch 66
	step [1/144], loss=99.0104
	step [2/144], loss=101.5015
	step [3/144], loss=89.4253
	step [4/144], loss=84.8060
	step [5/144], loss=86.7245
	step [6/144], loss=73.4022
	step [7/144], loss=86.5989
	step [8/144], loss=97.7641
	step [9/144], loss=94.3172
	step [10/144], loss=92.1858
	step [11/144], loss=85.4076
	step [12/144], loss=86.5538
	step [13/144], loss=73.9327
	step [14/144], loss=84.5777
	step [15/144], loss=81.7048
	step [16/144], loss=98.2657
	step [17/144], loss=77.1113
	step [18/144], loss=80.2410
	step [19/144], loss=83.2244
	step [20/144], loss=87.5010
	step [21/144], loss=90.1817
	step [22/144], loss=74.8566
	step [23/144], loss=76.8067
	step [24/144], loss=80.5074
	step [25/144], loss=73.5337
	step [26/144], loss=83.1324
	step [27/144], loss=78.7013
	step [28/144], loss=74.7622
	step [29/144], loss=71.3535
	step [30/144], loss=82.9975
	step [31/144], loss=108.7336
	step [32/144], loss=96.1645
	step [33/144], loss=87.3651
	step [34/144], loss=82.5819
	step [35/144], loss=84.6924
	step [36/144], loss=101.3678
	step [37/144], loss=97.1562
	step [38/144], loss=89.8640
	step [39/144], loss=80.8605
	step [40/144], loss=76.8504
	step [41/144], loss=86.6779
	step [42/144], loss=75.5854
	step [43/144], loss=73.8337
	step [44/144], loss=84.9508
	step [45/144], loss=101.1380
	step [46/144], loss=71.2863
	step [47/144], loss=101.7971
	step [48/144], loss=72.2491
	step [49/144], loss=84.7757
	step [50/144], loss=83.0365
	step [51/144], loss=86.7803
	step [52/144], loss=72.9237
	step [53/144], loss=75.5992
	step [54/144], loss=83.3131
	step [55/144], loss=90.1996
	step [56/144], loss=83.2133
	step [57/144], loss=89.7713
	step [58/144], loss=103.8728
	step [59/144], loss=92.3674
	step [60/144], loss=90.6809
	step [61/144], loss=92.5508
	step [62/144], loss=85.9992
	step [63/144], loss=91.4350
	step [64/144], loss=75.6758
	step [65/144], loss=112.6624
	step [66/144], loss=76.7980
	step [67/144], loss=82.6879
	step [68/144], loss=81.4656
	step [69/144], loss=75.5630
	step [70/144], loss=77.5597
	step [71/144], loss=93.3553
	step [72/144], loss=85.2588
	step [73/144], loss=78.9548
	step [74/144], loss=89.6291
	step [75/144], loss=81.3646
	step [76/144], loss=79.4841
	step [77/144], loss=91.3372
	step [78/144], loss=82.3633
	step [79/144], loss=95.1958
	step [80/144], loss=80.9825
	step [81/144], loss=71.5015
	step [82/144], loss=108.4415
	step [83/144], loss=80.8071
	step [84/144], loss=80.4333
	step [85/144], loss=83.0581
	step [86/144], loss=93.3576
	step [87/144], loss=90.9371
	step [88/144], loss=80.5414
	step [89/144], loss=68.9173
	step [90/144], loss=76.4843
	step [91/144], loss=85.6273
	step [92/144], loss=80.4015
	step [93/144], loss=88.9709
	step [94/144], loss=79.2509
	step [95/144], loss=88.8103
	step [96/144], loss=89.0886
	step [97/144], loss=96.6003
	step [98/144], loss=92.8133
	step [99/144], loss=75.9625
	step [100/144], loss=84.4347
	step [101/144], loss=74.3149
	step [102/144], loss=82.6592
	step [103/144], loss=84.8196
	step [104/144], loss=82.8487
	step [105/144], loss=88.0782
	step [106/144], loss=82.8318
	step [107/144], loss=84.5142
	step [108/144], loss=80.6969
	step [109/144], loss=90.2168
	step [110/144], loss=101.0157
	step [111/144], loss=85.0767
	step [112/144], loss=83.2040
	step [113/144], loss=101.5227
	step [114/144], loss=90.1572
	step [115/144], loss=98.2789
	step [116/144], loss=80.8145
	step [117/144], loss=97.8736
	step [118/144], loss=84.1124
	step [119/144], loss=84.6759
	step [120/144], loss=77.6572
	step [121/144], loss=71.4224
	step [122/144], loss=89.2457
	step [123/144], loss=89.6101
	step [124/144], loss=72.3489
	step [125/144], loss=108.1246
	step [126/144], loss=80.8557
	step [127/144], loss=83.2572
	step [128/144], loss=80.4689
	step [129/144], loss=83.2673
	step [130/144], loss=83.9594
	step [131/144], loss=82.7838
	step [132/144], loss=89.6032
	step [133/144], loss=74.3777
	step [134/144], loss=96.1149
	step [135/144], loss=85.0899
	step [136/144], loss=87.9560
	step [137/144], loss=80.8106
	step [138/144], loss=69.0088
	step [139/144], loss=101.5679
	step [140/144], loss=82.8935
	step [141/144], loss=69.8985
	step [142/144], loss=81.9268
	step [143/144], loss=95.4131
	step [144/144], loss=29.6175
	Evaluating
	loss=0.0117, precision=0.4158, recall=0.8414, f1=0.5566
Training epoch 67
	step [1/144], loss=90.4046
	step [2/144], loss=85.4053
	step [3/144], loss=79.1171
	step [4/144], loss=67.5767
	step [5/144], loss=88.9091
	step [6/144], loss=75.4518
	step [7/144], loss=69.9493
	step [8/144], loss=75.9032
	step [9/144], loss=90.1036
	step [10/144], loss=84.2783
	step [11/144], loss=83.7863
	step [12/144], loss=111.3911
	step [13/144], loss=107.8646
	step [14/144], loss=74.6824
	step [15/144], loss=73.4206
	step [16/144], loss=81.1350
	step [17/144], loss=87.1707
	step [18/144], loss=92.4737
	step [19/144], loss=76.2182
	step [20/144], loss=93.2900
	step [21/144], loss=95.3381
	step [22/144], loss=88.9582
	step [23/144], loss=80.1217
	step [24/144], loss=91.6341
	step [25/144], loss=81.5454
	step [26/144], loss=73.0559
	step [27/144], loss=73.6064
	step [28/144], loss=67.6563
	step [29/144], loss=85.4787
	step [30/144], loss=89.4666
	step [31/144], loss=71.4589
	step [32/144], loss=77.7435
	step [33/144], loss=99.1078
	step [34/144], loss=86.8133
	step [35/144], loss=84.4493
	step [36/144], loss=70.8928
	step [37/144], loss=81.6406
	step [38/144], loss=91.7462
	step [39/144], loss=96.5661
	step [40/144], loss=103.8948
	step [41/144], loss=76.5406
	step [42/144], loss=87.0633
	step [43/144], loss=79.5399
	step [44/144], loss=71.5499
	step [45/144], loss=95.6364
	step [46/144], loss=90.6428
	step [47/144], loss=77.2724
	step [48/144], loss=111.5097
	step [49/144], loss=84.2177
	step [50/144], loss=98.1319
	step [51/144], loss=87.0781
	step [52/144], loss=98.3582
	step [53/144], loss=76.7416
	step [54/144], loss=84.7626
	step [55/144], loss=81.8511
	step [56/144], loss=90.9279
	step [57/144], loss=86.2660
	step [58/144], loss=84.4371
	step [59/144], loss=92.0450
	step [60/144], loss=67.5705
	step [61/144], loss=79.9447
	step [62/144], loss=83.1673
	step [63/144], loss=93.8475
	step [64/144], loss=79.3061
	step [65/144], loss=79.7916
	step [66/144], loss=76.4877
	step [67/144], loss=76.3008
	step [68/144], loss=96.0215
	step [69/144], loss=71.4971
	step [70/144], loss=82.2914
	step [71/144], loss=86.4774
	step [72/144], loss=85.3185
	step [73/144], loss=80.3940
	step [74/144], loss=85.4997
	step [75/144], loss=75.3583
	step [76/144], loss=85.3378
	step [77/144], loss=91.9543
	step [78/144], loss=84.2389
	step [79/144], loss=91.1473
	step [80/144], loss=79.0673
	step [81/144], loss=80.9922
	step [82/144], loss=82.8110
	step [83/144], loss=74.0867
	step [84/144], loss=94.6282
	step [85/144], loss=93.0562
	step [86/144], loss=79.5643
	step [87/144], loss=80.3337
	step [88/144], loss=87.5770
	step [89/144], loss=74.3308
	step [90/144], loss=80.8018
	step [91/144], loss=76.8365
	step [92/144], loss=74.4933
	step [93/144], loss=90.5874
	step [94/144], loss=80.9081
	step [95/144], loss=92.5854
	step [96/144], loss=98.4038
	step [97/144], loss=89.0252
	step [98/144], loss=74.9061
	step [99/144], loss=80.9069
	step [100/144], loss=91.4419
	step [101/144], loss=103.3506
	step [102/144], loss=86.5187
	step [103/144], loss=93.4264
	step [104/144], loss=74.9789
	step [105/144], loss=96.8214
	step [106/144], loss=78.4361
	step [107/144], loss=88.9965
	step [108/144], loss=78.6445
	step [109/144], loss=82.5056
	step [110/144], loss=83.0905
	step [111/144], loss=83.4214
	step [112/144], loss=88.8413
	step [113/144], loss=100.3665
	step [114/144], loss=87.2235
	step [115/144], loss=99.8946
	step [116/144], loss=73.5238
	step [117/144], loss=91.3973
	step [118/144], loss=79.8317
	step [119/144], loss=71.2181
	step [120/144], loss=86.0919
	step [121/144], loss=84.5652
	step [122/144], loss=81.4267
	step [123/144], loss=67.5628
	step [124/144], loss=88.1694
	step [125/144], loss=95.6523
	step [126/144], loss=84.3073
	step [127/144], loss=85.3055
	step [128/144], loss=86.5249
	step [129/144], loss=89.0871
	step [130/144], loss=102.1561
	step [131/144], loss=83.0865
	step [132/144], loss=73.8168
	step [133/144], loss=104.5461
	step [134/144], loss=101.3546
	step [135/144], loss=94.3215
	step [136/144], loss=92.0519
	step [137/144], loss=79.1491
	step [138/144], loss=88.3331
	step [139/144], loss=93.0210
	step [140/144], loss=85.0888
	step [141/144], loss=88.3433
	step [142/144], loss=83.4725
	step [143/144], loss=76.8496
	step [144/144], loss=24.3845
	Evaluating
	loss=0.0127, precision=0.3698, recall=0.8475, f1=0.5149
Training epoch 68
	step [1/144], loss=79.4688
	step [2/144], loss=82.5956
	step [3/144], loss=66.8679
	step [4/144], loss=97.7236
	step [5/144], loss=94.3484
	step [6/144], loss=78.6043
	step [7/144], loss=78.0227
	step [8/144], loss=72.9703
	step [9/144], loss=86.0823
	step [10/144], loss=99.7870
	step [11/144], loss=80.3300
	step [12/144], loss=92.6971
	step [13/144], loss=73.2162
	step [14/144], loss=79.5678
	step [15/144], loss=87.3806
	step [16/144], loss=99.9804
	step [17/144], loss=103.5380
	step [18/144], loss=86.2376
	step [19/144], loss=79.7584
	step [20/144], loss=81.6887
	step [21/144], loss=85.6562
	step [22/144], loss=73.6994
	step [23/144], loss=69.8374
	step [24/144], loss=91.5704
	step [25/144], loss=91.0702
	step [26/144], loss=72.7109
	step [27/144], loss=86.8783
	step [28/144], loss=103.8891
	step [29/144], loss=72.0567
	step [30/144], loss=81.7878
	step [31/144], loss=81.6172
	step [32/144], loss=75.1947
	step [33/144], loss=107.8206
	step [34/144], loss=98.4510
	step [35/144], loss=79.6844
	step [36/144], loss=94.2181
	step [37/144], loss=91.8711
	step [38/144], loss=96.9600
	step [39/144], loss=84.8388
	step [40/144], loss=91.7549
	step [41/144], loss=81.3861
	step [42/144], loss=98.8513
	step [43/144], loss=86.7694
	step [44/144], loss=73.0063
	step [45/144], loss=105.5320
	step [46/144], loss=81.3027
	step [47/144], loss=82.5731
	step [48/144], loss=84.3974
	step [49/144], loss=93.9387
	step [50/144], loss=91.4062
	step [51/144], loss=83.2536
	step [52/144], loss=76.2208
	step [53/144], loss=84.7102
	step [54/144], loss=80.4622
	step [55/144], loss=96.9343
	step [56/144], loss=80.5360
	step [57/144], loss=73.0807
	step [58/144], loss=64.7238
	step [59/144], loss=85.4005
	step [60/144], loss=92.5048
	step [61/144], loss=77.1311
	step [62/144], loss=70.3899
	step [63/144], loss=82.4552
	step [64/144], loss=86.5526
	step [65/144], loss=79.4115
	step [66/144], loss=79.9888
	step [67/144], loss=74.2699
	step [68/144], loss=69.3927
	step [69/144], loss=89.7068
	step [70/144], loss=89.3112
	step [71/144], loss=95.6347
	step [72/144], loss=85.3519
	step [73/144], loss=91.9866
	step [74/144], loss=82.2561
	step [75/144], loss=87.6206
	step [76/144], loss=92.3806
	step [77/144], loss=102.7496
	step [78/144], loss=92.6977
	step [79/144], loss=93.6943
	step [80/144], loss=82.0767
	step [81/144], loss=90.6543
	step [82/144], loss=91.0518
	step [83/144], loss=87.2085
	step [84/144], loss=89.9843
	step [85/144], loss=74.3474
	step [86/144], loss=76.7654
	step [87/144], loss=87.7804
	step [88/144], loss=84.1883
	step [89/144], loss=72.9768
	step [90/144], loss=81.8065
	step [91/144], loss=72.9802
	step [92/144], loss=96.4759
	step [93/144], loss=80.2465
	step [94/144], loss=81.0207
	step [95/144], loss=97.0978
	step [96/144], loss=82.9515
	step [97/144], loss=78.5225
	step [98/144], loss=79.8569
	step [99/144], loss=102.1596
	step [100/144], loss=73.0466
	step [101/144], loss=66.6862
	step [102/144], loss=91.7070
	step [103/144], loss=94.7274
	step [104/144], loss=81.8297
	step [105/144], loss=86.7711
	step [106/144], loss=95.7827
	step [107/144], loss=89.9686
	step [108/144], loss=72.5873
	step [109/144], loss=74.4560
	step [110/144], loss=86.2526
	step [111/144], loss=70.9551
	step [112/144], loss=112.1907
	step [113/144], loss=81.4129
	step [114/144], loss=79.1260
	step [115/144], loss=103.0900
	step [116/144], loss=84.9537
	step [117/144], loss=79.8210
	step [118/144], loss=89.7248
	step [119/144], loss=100.1049
	step [120/144], loss=81.1190
	step [121/144], loss=77.9212
	step [122/144], loss=78.8692
	step [123/144], loss=77.9979
	step [124/144], loss=62.4843
	step [125/144], loss=102.6420
	step [126/144], loss=97.8485
	step [127/144], loss=86.6750
	step [128/144], loss=81.5707
	step [129/144], loss=91.4352
	step [130/144], loss=87.5615
	step [131/144], loss=87.5275
	step [132/144], loss=84.6515
	step [133/144], loss=86.7498
	step [134/144], loss=90.3360
	step [135/144], loss=80.8250
	step [136/144], loss=84.8409
	step [137/144], loss=86.8298
	step [138/144], loss=87.3646
	step [139/144], loss=94.8282
	step [140/144], loss=84.8469
	step [141/144], loss=69.6680
	step [142/144], loss=76.8569
	step [143/144], loss=85.2303
	step [144/144], loss=27.3143
	Evaluating
	loss=0.0125, precision=0.3877, recall=0.8567, f1=0.5338
Training epoch 69
	step [1/144], loss=87.5086
	step [2/144], loss=81.6888
	step [3/144], loss=95.3793
	step [4/144], loss=81.5442
	step [5/144], loss=96.4758
	step [6/144], loss=73.4140
	step [7/144], loss=96.4599
	step [8/144], loss=75.1164
	step [9/144], loss=84.6292
	step [10/144], loss=78.8842
	step [11/144], loss=85.4752
	step [12/144], loss=75.8216
	step [13/144], loss=82.1458
	step [14/144], loss=83.0471
	step [15/144], loss=83.1174
	step [16/144], loss=92.2005
	step [17/144], loss=83.8643
	step [18/144], loss=73.8689
	step [19/144], loss=88.8050
	step [20/144], loss=70.7057
	step [21/144], loss=79.1354
	step [22/144], loss=88.6060
	step [23/144], loss=72.5413
	step [24/144], loss=74.2227
	step [25/144], loss=85.8606
	step [26/144], loss=73.8200
	step [27/144], loss=101.7793
	step [28/144], loss=80.5918
	step [29/144], loss=88.8097
	step [30/144], loss=111.2160
	step [31/144], loss=81.4037
	step [32/144], loss=87.9794
	step [33/144], loss=95.3743
	step [34/144], loss=87.2711
	step [35/144], loss=86.2073
	step [36/144], loss=85.3400
	step [37/144], loss=71.5558
	step [38/144], loss=88.7617
	step [39/144], loss=80.5503
	step [40/144], loss=85.4137
	step [41/144], loss=90.3144
	step [42/144], loss=78.9230
	step [43/144], loss=94.0902
	step [44/144], loss=88.4484
	step [45/144], loss=71.0044
	step [46/144], loss=96.0689
	step [47/144], loss=81.8344
	step [48/144], loss=95.5620
	step [49/144], loss=71.1722
	step [50/144], loss=98.8278
	step [51/144], loss=65.9754
	step [52/144], loss=72.2287
	step [53/144], loss=73.3235
	step [54/144], loss=88.4103
	step [55/144], loss=97.7269
	step [56/144], loss=94.4024
	step [57/144], loss=99.5186
	step [58/144], loss=85.5481
	step [59/144], loss=71.9642
	step [60/144], loss=81.6317
	step [61/144], loss=74.4149
	step [62/144], loss=84.4651
	step [63/144], loss=88.6893
	step [64/144], loss=91.1730
	step [65/144], loss=87.5876
	step [66/144], loss=89.6501
	step [67/144], loss=85.6760
	step [68/144], loss=90.9044
	step [69/144], loss=88.7545
	step [70/144], loss=79.4594
	step [71/144], loss=76.7755
	step [72/144], loss=101.9458
	step [73/144], loss=75.2002
	step [74/144], loss=83.1255
	step [75/144], loss=83.0365
	step [76/144], loss=90.6145
	step [77/144], loss=77.8132
	step [78/144], loss=87.1860
	step [79/144], loss=93.6803
	step [80/144], loss=89.9778
	step [81/144], loss=84.4859
	step [82/144], loss=79.0049
	step [83/144], loss=79.1381
	step [84/144], loss=82.9879
	step [85/144], loss=86.7001
	step [86/144], loss=86.9330
	step [87/144], loss=85.7407
	step [88/144], loss=75.5180
	step [89/144], loss=103.0871
	step [90/144], loss=77.3313
	step [91/144], loss=66.4316
	step [92/144], loss=73.7958
	step [93/144], loss=74.0331
	step [94/144], loss=79.3796
	step [95/144], loss=77.3191
	step [96/144], loss=66.2379
	step [97/144], loss=81.8741
	step [98/144], loss=78.5966
	step [99/144], loss=81.2367
	step [100/144], loss=83.1197
	step [101/144], loss=69.3915
	step [102/144], loss=89.8511
	step [103/144], loss=93.2698
	step [104/144], loss=72.5114
	step [105/144], loss=88.7291
	step [106/144], loss=90.0665
	step [107/144], loss=85.8768
	step [108/144], loss=78.0243
	step [109/144], loss=91.0386
	step [110/144], loss=76.3963
	step [111/144], loss=90.7529
	step [112/144], loss=98.3474
	step [113/144], loss=89.5827
	step [114/144], loss=87.1715
	step [115/144], loss=93.0203
	step [116/144], loss=89.7902
	step [117/144], loss=91.9697
	step [118/144], loss=90.3137
	step [119/144], loss=70.0001
	step [120/144], loss=81.8241
	step [121/144], loss=88.4241
	step [122/144], loss=75.5313
	step [123/144], loss=82.0725
	step [124/144], loss=89.8539
	step [125/144], loss=81.5084
	step [126/144], loss=82.0813
	step [127/144], loss=77.8532
	step [128/144], loss=68.1037
	step [129/144], loss=73.7647
	step [130/144], loss=82.9835
	step [131/144], loss=77.1481
	step [132/144], loss=89.5042
	step [133/144], loss=79.1583
	step [134/144], loss=77.6735
	step [135/144], loss=92.5470
	step [136/144], loss=95.3024
	step [137/144], loss=92.9628
	step [138/144], loss=80.5666
	step [139/144], loss=95.2652
	step [140/144], loss=80.9759
	step [141/144], loss=87.3909
	step [142/144], loss=84.9891
	step [143/144], loss=110.7901
	step [144/144], loss=35.4614
	Evaluating
	loss=0.0124, precision=0.3837, recall=0.8563, f1=0.5299
Training epoch 70
	step [1/144], loss=78.9469
	step [2/144], loss=91.5418
	step [3/144], loss=82.0083
	step [4/144], loss=81.2757
	step [5/144], loss=82.6434
	step [6/144], loss=74.6841
	step [7/144], loss=95.2416
	step [8/144], loss=85.9155
	step [9/144], loss=90.9861
	step [10/144], loss=74.5286
	step [11/144], loss=98.1180
	step [12/144], loss=82.5229
	step [13/144], loss=97.3889
	step [14/144], loss=85.1607
	step [15/144], loss=85.5501
	step [16/144], loss=98.1092
	step [17/144], loss=75.2863
	step [18/144], loss=63.6883
	step [19/144], loss=83.8279
	step [20/144], loss=90.1077
	step [21/144], loss=85.6396
	step [22/144], loss=77.6084
	step [23/144], loss=88.1562
	step [24/144], loss=72.0538
	step [25/144], loss=74.1071
	step [26/144], loss=76.3539
	step [27/144], loss=78.7551
	step [28/144], loss=81.8895
	step [29/144], loss=85.6269
	step [30/144], loss=78.7507
	step [31/144], loss=88.1498
	step [32/144], loss=97.5949
	step [33/144], loss=88.8431
	step [34/144], loss=78.2938
	step [35/144], loss=78.6414
	step [36/144], loss=77.1568
	step [37/144], loss=67.3793
	step [38/144], loss=94.9350
	step [39/144], loss=77.9026
	step [40/144], loss=86.4394
	step [41/144], loss=71.8890
	step [42/144], loss=85.8456
	step [43/144], loss=72.1704
	step [44/144], loss=79.5716
	step [45/144], loss=74.2065
	step [46/144], loss=71.7061
	step [47/144], loss=69.7324
	step [48/144], loss=74.2538
	step [49/144], loss=108.0704
	step [50/144], loss=76.8781
	step [51/144], loss=100.5899
	step [52/144], loss=75.5468
	step [53/144], loss=78.7572
	step [54/144], loss=86.2320
	step [55/144], loss=81.6518
	step [56/144], loss=84.6436
	step [57/144], loss=95.5087
	step [58/144], loss=92.3884
	step [59/144], loss=82.0550
	step [60/144], loss=91.5353
	step [61/144], loss=97.3833
	step [62/144], loss=95.4340
	step [63/144], loss=88.3444
	step [64/144], loss=78.0857
	step [65/144], loss=81.8002
	step [66/144], loss=83.7856
	step [67/144], loss=89.1076
	step [68/144], loss=90.4735
	step [69/144], loss=88.5378
	step [70/144], loss=79.7420
	step [71/144], loss=88.8818
	step [72/144], loss=72.0428
	step [73/144], loss=94.6948
	step [74/144], loss=70.9942
	step [75/144], loss=70.4391
	step [76/144], loss=77.1741
	step [77/144], loss=82.7677
	step [78/144], loss=89.9762
	step [79/144], loss=74.1612
	step [80/144], loss=72.6112
	step [81/144], loss=82.2730
	step [82/144], loss=74.1187
	step [83/144], loss=94.5471
	step [84/144], loss=98.9774
	step [85/144], loss=73.8726
	step [86/144], loss=91.3342
	step [87/144], loss=71.5122
	step [88/144], loss=85.6999
	step [89/144], loss=91.1465
	step [90/144], loss=98.4670
	step [91/144], loss=86.5373
	step [92/144], loss=96.6870
	step [93/144], loss=82.8837
	step [94/144], loss=99.6478
	step [95/144], loss=84.8792
	step [96/144], loss=64.4563
	step [97/144], loss=68.6647
	step [98/144], loss=95.1021
	step [99/144], loss=88.7166
	step [100/144], loss=86.2828
	step [101/144], loss=78.1460
	step [102/144], loss=74.6524
	step [103/144], loss=103.3662
	step [104/144], loss=90.0469
	step [105/144], loss=86.9583
	step [106/144], loss=65.5752
	step [107/144], loss=81.6636
	step [108/144], loss=90.9909
	step [109/144], loss=75.5241
	step [110/144], loss=75.9601
	step [111/144], loss=71.3410
	step [112/144], loss=80.5636
	step [113/144], loss=84.4695
	step [114/144], loss=79.2357
	step [115/144], loss=89.3759
	step [116/144], loss=93.8182
	step [117/144], loss=96.7809
	step [118/144], loss=89.5364
	step [119/144], loss=80.6745
	step [120/144], loss=96.5172
	step [121/144], loss=96.3640
	step [122/144], loss=81.6532
	step [123/144], loss=90.7582
	step [124/144], loss=80.9557
	step [125/144], loss=90.6291
	step [126/144], loss=78.8275
	step [127/144], loss=80.1659
	step [128/144], loss=84.4933
	step [129/144], loss=80.3411
	step [130/144], loss=93.0272
	step [131/144], loss=78.5524
	step [132/144], loss=68.6515
	step [133/144], loss=85.6791
	step [134/144], loss=81.9716
	step [135/144], loss=105.6795
	step [136/144], loss=92.7662
	step [137/144], loss=82.5115
	step [138/144], loss=76.4215
	step [139/144], loss=84.8083
	step [140/144], loss=73.5294
	step [141/144], loss=91.0036
	step [142/144], loss=73.7121
	step [143/144], loss=84.2387
	step [144/144], loss=29.3088
	Evaluating
	loss=0.0126, precision=0.3734, recall=0.8577, f1=0.5203
Training epoch 71
	step [1/144], loss=78.8524
	step [2/144], loss=74.7905
	step [3/144], loss=83.1149
	step [4/144], loss=80.6036
	step [5/144], loss=85.2310
	step [6/144], loss=93.2812
	step [7/144], loss=76.5757
	step [8/144], loss=87.4574
	step [9/144], loss=76.0390
	step [10/144], loss=86.3019
	step [11/144], loss=78.8393
	step [12/144], loss=85.2225
	step [13/144], loss=79.7480
	step [14/144], loss=92.3569
	step [15/144], loss=85.5813
	step [16/144], loss=89.2308
	step [17/144], loss=75.1659
	step [18/144], loss=93.1754
	step [19/144], loss=77.0522
	step [20/144], loss=92.7958
	step [21/144], loss=82.2627
	step [22/144], loss=88.1264
	step [23/144], loss=79.8726
	step [24/144], loss=91.5462
	step [25/144], loss=77.0966
	step [26/144], loss=82.1760
	step [27/144], loss=76.5324
	step [28/144], loss=75.4692
	step [29/144], loss=81.3223
	step [30/144], loss=82.7739
	step [31/144], loss=76.6460
	step [32/144], loss=82.1717
	step [33/144], loss=90.3574
	step [34/144], loss=77.5020
	step [35/144], loss=82.7616
	step [36/144], loss=81.2779
	step [37/144], loss=74.5933
	step [38/144], loss=71.3651
	step [39/144], loss=72.3563
	step [40/144], loss=91.1497
	step [41/144], loss=80.6231
	step [42/144], loss=76.8016
	step [43/144], loss=92.9568
	step [44/144], loss=82.6073
	step [45/144], loss=67.9142
	step [46/144], loss=96.5497
	step [47/144], loss=89.3800
	step [48/144], loss=79.4103
	step [49/144], loss=80.5429
	step [50/144], loss=79.4320
	step [51/144], loss=98.5569
	step [52/144], loss=98.5914
	step [53/144], loss=80.6027
	step [54/144], loss=92.7973
	step [55/144], loss=90.9160
	step [56/144], loss=85.2975
	step [57/144], loss=82.9234
	step [58/144], loss=85.1735
	step [59/144], loss=76.0745
	step [60/144], loss=80.3306
	step [61/144], loss=76.0507
	step [62/144], loss=80.6034
	step [63/144], loss=76.4191
	step [64/144], loss=85.3366
	step [65/144], loss=77.6691
	step [66/144], loss=73.9163
	step [67/144], loss=89.4429
	step [68/144], loss=96.2683
	step [69/144], loss=81.5549
	step [70/144], loss=86.4197
	step [71/144], loss=92.5705
	step [72/144], loss=80.9758
	step [73/144], loss=81.6111
	step [74/144], loss=77.6841
	step [75/144], loss=86.5871
	step [76/144], loss=80.6662
	step [77/144], loss=97.7158
	step [78/144], loss=79.3146
	step [79/144], loss=94.0181
	step [80/144], loss=86.2407
	step [81/144], loss=84.4729
	step [82/144], loss=90.7166
	step [83/144], loss=91.2068
	step [84/144], loss=88.0766
	step [85/144], loss=90.3085
	step [86/144], loss=82.6136
	step [87/144], loss=75.3110
	step [88/144], loss=86.6038
	step [89/144], loss=72.4340
	step [90/144], loss=98.5373
	step [91/144], loss=84.0567
	step [92/144], loss=73.6837
	step [93/144], loss=88.0432
	step [94/144], loss=72.8726
	step [95/144], loss=76.9763
	step [96/144], loss=87.1185
	step [97/144], loss=80.4665
	step [98/144], loss=82.3892
	step [99/144], loss=75.9664
	step [100/144], loss=79.6121
	step [101/144], loss=74.6761
	step [102/144], loss=73.4533
	step [103/144], loss=84.8383
	step [104/144], loss=97.3090
	step [105/144], loss=78.3886
	step [106/144], loss=87.7569
	step [107/144], loss=80.7160
	step [108/144], loss=85.3564
	step [109/144], loss=91.8054
	step [110/144], loss=86.0083
	step [111/144], loss=81.4521
	step [112/144], loss=78.2365
	step [113/144], loss=99.4448
	step [114/144], loss=71.4248
	step [115/144], loss=95.0026
	step [116/144], loss=73.9308
	step [117/144], loss=85.0781
	step [118/144], loss=74.6308
	step [119/144], loss=82.9744
	step [120/144], loss=92.6518
	step [121/144], loss=85.9573
	step [122/144], loss=84.4760
	step [123/144], loss=73.6117
	step [124/144], loss=80.5132
	step [125/144], loss=75.9957
	step [126/144], loss=75.1885
	step [127/144], loss=90.3637
	step [128/144], loss=82.3314
	step [129/144], loss=86.5580
	step [130/144], loss=78.7014
	step [131/144], loss=91.6937
	step [132/144], loss=71.8365
	step [133/144], loss=86.8088
	step [134/144], loss=87.4631
	step [135/144], loss=78.9458
	step [136/144], loss=95.1811
	step [137/144], loss=76.9865
	step [138/144], loss=93.9002
	step [139/144], loss=85.4897
	step [140/144], loss=96.7542
	step [141/144], loss=73.5437
	step [142/144], loss=84.9386
	step [143/144], loss=92.1356
	step [144/144], loss=29.4854
	Evaluating
	loss=0.0120, precision=0.3858, recall=0.8483, f1=0.5304
Training epoch 72
	step [1/144], loss=91.2278
	step [2/144], loss=84.9898
	step [3/144], loss=84.1452
	step [4/144], loss=93.3578
	step [5/144], loss=98.3842
	step [6/144], loss=72.0808
	step [7/144], loss=82.5355
	step [8/144], loss=84.3860
	step [9/144], loss=88.6533
	step [10/144], loss=93.2825
	step [11/144], loss=76.3260
	step [12/144], loss=77.7546
	step [13/144], loss=75.6110
	step [14/144], loss=61.7319
	step [15/144], loss=73.6257
	step [16/144], loss=84.9214
	step [17/144], loss=90.7368
	step [18/144], loss=84.5530
	step [19/144], loss=84.8355
	step [20/144], loss=83.5583
	step [21/144], loss=94.8534
	step [22/144], loss=92.1723
	step [23/144], loss=75.8692
	step [24/144], loss=89.7985
	step [25/144], loss=96.3273
	step [26/144], loss=80.2885
	step [27/144], loss=84.6665
	step [28/144], loss=73.1014
	step [29/144], loss=86.2887
	step [30/144], loss=84.2626
	step [31/144], loss=72.7446
	step [32/144], loss=87.0507
	step [33/144], loss=90.1849
	step [34/144], loss=80.4881
	step [35/144], loss=66.0884
	step [36/144], loss=105.1652
	step [37/144], loss=80.7880
	step [38/144], loss=84.7273
	step [39/144], loss=87.8294
	step [40/144], loss=99.4416
	step [41/144], loss=69.6835
	step [42/144], loss=89.7410
	step [43/144], loss=79.9883
	step [44/144], loss=68.8069
	step [45/144], loss=95.6512
	step [46/144], loss=80.9870
	step [47/144], loss=88.2998
	step [48/144], loss=74.3115
	step [49/144], loss=80.2476
	step [50/144], loss=90.4481
	step [51/144], loss=90.8491
	step [52/144], loss=91.7457
	step [53/144], loss=88.9900
	step [54/144], loss=93.1774
	step [55/144], loss=89.0128
	step [56/144], loss=94.0823
	step [57/144], loss=85.0259
	step [58/144], loss=84.3966
	step [59/144], loss=70.7464
	step [60/144], loss=84.3299
	step [61/144], loss=80.1972
	step [62/144], loss=84.1257
	step [63/144], loss=90.9602
	step [64/144], loss=76.5155
	step [65/144], loss=71.9557
	step [66/144], loss=101.3070
	step [67/144], loss=73.3105
	step [68/144], loss=76.0482
	step [69/144], loss=71.7954
	step [70/144], loss=82.9737
	step [71/144], loss=84.7659
	step [72/144], loss=82.5545
	step [73/144], loss=83.4957
	step [74/144], loss=90.9112
	step [75/144], loss=99.1425
	step [76/144], loss=99.2050
	step [77/144], loss=79.6243
	step [78/144], loss=84.9821
	step [79/144], loss=82.6668
	step [80/144], loss=95.3262
	step [81/144], loss=82.0821
	step [82/144], loss=83.3775
	step [83/144], loss=95.7112
	step [84/144], loss=81.3437
	step [85/144], loss=82.7202
	step [86/144], loss=86.4618
	step [87/144], loss=69.8909
	step [88/144], loss=81.1641
	step [89/144], loss=62.1246
	step [90/144], loss=78.6170
	step [91/144], loss=85.4259
	step [92/144], loss=92.0331
	step [93/144], loss=89.2328
	step [94/144], loss=90.9440
	step [95/144], loss=68.6218
	step [96/144], loss=71.2776
	step [97/144], loss=72.3218
	step [98/144], loss=71.6254
	step [99/144], loss=70.6416
	step [100/144], loss=84.4692
	step [101/144], loss=74.3388
	step [102/144], loss=76.0523
	step [103/144], loss=78.0587
	step [104/144], loss=67.1014
	step [105/144], loss=86.9600
	step [106/144], loss=79.1818
	step [107/144], loss=74.3731
	step [108/144], loss=70.0669
	step [109/144], loss=80.3613
	step [110/144], loss=98.4713
	step [111/144], loss=76.1378
	step [112/144], loss=97.8164
	step [113/144], loss=96.7139
	step [114/144], loss=83.2464
	step [115/144], loss=80.9016
	step [116/144], loss=76.7249
	step [117/144], loss=92.6500
	step [118/144], loss=81.6765
	step [119/144], loss=96.5630
	step [120/144], loss=83.2818
	step [121/144], loss=83.2165
	step [122/144], loss=90.3635
	step [123/144], loss=96.7607
	step [124/144], loss=88.2899
	step [125/144], loss=81.6340
	step [126/144], loss=95.6898
	step [127/144], loss=89.9486
	step [128/144], loss=93.6025
	step [129/144], loss=81.9730
	step [130/144], loss=89.3696
	step [131/144], loss=72.1461
	step [132/144], loss=74.9189
	step [133/144], loss=81.6091
	step [134/144], loss=85.0593
	step [135/144], loss=93.7335
	step [136/144], loss=76.0005
	step [137/144], loss=76.1200
	step [138/144], loss=85.7130
	step [139/144], loss=98.8288
	step [140/144], loss=90.7881
	step [141/144], loss=72.5922
	step [142/144], loss=85.7958
	step [143/144], loss=72.3542
	step [144/144], loss=35.2762
	Evaluating
	loss=0.0137, precision=0.3416, recall=0.8556, f1=0.4883
Training epoch 73
	step [1/144], loss=86.6604
	step [2/144], loss=99.2281
	step [3/144], loss=91.7237
	step [4/144], loss=77.3262
	step [5/144], loss=82.7190
	step [6/144], loss=101.4539
	step [7/144], loss=79.7536
	step [8/144], loss=69.1107
	step [9/144], loss=69.2813
	step [10/144], loss=79.8465
	step [11/144], loss=83.8139
	step [12/144], loss=86.1711
	step [13/144], loss=80.7756
	step [14/144], loss=77.2477
	step [15/144], loss=86.6863
	step [16/144], loss=70.5302
	step [17/144], loss=99.5555
	step [18/144], loss=85.0310
	step [19/144], loss=97.9500
	step [20/144], loss=73.6168
	step [21/144], loss=76.4886
	step [22/144], loss=84.7988
	step [23/144], loss=84.4076
	step [24/144], loss=94.5451
	step [25/144], loss=70.6693
	step [26/144], loss=69.9875
	step [27/144], loss=96.7176
	step [28/144], loss=85.0668
	step [29/144], loss=70.9728
	step [30/144], loss=79.6297
	step [31/144], loss=85.7816
	step [32/144], loss=75.6111
	step [33/144], loss=79.3807
	step [34/144], loss=93.9963
	step [35/144], loss=89.1737
	step [36/144], loss=80.6906
	step [37/144], loss=88.9021
	step [38/144], loss=77.2655
	step [39/144], loss=62.6112
	step [40/144], loss=73.5448
	step [41/144], loss=74.9729
	step [42/144], loss=84.1424
	step [43/144], loss=88.8513
	step [44/144], loss=79.5199
	step [45/144], loss=84.5885
	step [46/144], loss=78.8336
	step [47/144], loss=65.8571
	step [48/144], loss=95.3218
	step [49/144], loss=78.7422
	step [50/144], loss=76.2617
	step [51/144], loss=83.0375
	step [52/144], loss=78.1225
	step [53/144], loss=85.2092
	step [54/144], loss=84.7658
	step [55/144], loss=88.5851
	step [56/144], loss=100.1170
	step [57/144], loss=87.1883
	step [58/144], loss=99.2771
	step [59/144], loss=89.6517
	step [60/144], loss=81.7218
	step [61/144], loss=90.3638
	step [62/144], loss=74.2648
	step [63/144], loss=86.3335
	step [64/144], loss=85.2051
	step [65/144], loss=78.5780
	step [66/144], loss=94.3675
	step [67/144], loss=79.3562
	step [68/144], loss=71.5778
	step [69/144], loss=91.3848
	step [70/144], loss=86.3084
	step [71/144], loss=80.3784
	step [72/144], loss=75.4159
	step [73/144], loss=92.8732
	step [74/144], loss=85.5607
	step [75/144], loss=67.9676
	step [76/144], loss=70.9053
	step [77/144], loss=85.7972
	step [78/144], loss=84.3540
	step [79/144], loss=73.4324
	step [80/144], loss=82.8685
	step [81/144], loss=70.6594
	step [82/144], loss=89.3544
	step [83/144], loss=88.5060
	step [84/144], loss=86.6737
	step [85/144], loss=86.3304
	step [86/144], loss=84.5025
	step [87/144], loss=78.9532
	step [88/144], loss=85.9150
	step [89/144], loss=91.8773
	step [90/144], loss=83.6554
	step [91/144], loss=78.0947
	step [92/144], loss=88.5446
	step [93/144], loss=80.1905
	step [94/144], loss=100.2085
	step [95/144], loss=76.1521
	step [96/144], loss=75.7030
	step [97/144], loss=99.5261
	step [98/144], loss=72.4251
	step [99/144], loss=79.2266
	step [100/144], loss=81.3063
	step [101/144], loss=83.9542
	step [102/144], loss=85.3866
	step [103/144], loss=71.7681
	step [104/144], loss=79.9447
	step [105/144], loss=71.0181
	step [106/144], loss=83.7282
	step [107/144], loss=89.1599
	step [108/144], loss=82.5871
	step [109/144], loss=87.5926
	step [110/144], loss=79.9781
	step [111/144], loss=81.3969
	step [112/144], loss=82.7285
	step [113/144], loss=89.8027
	step [114/144], loss=94.3527
	step [115/144], loss=88.6329
	step [116/144], loss=85.7001
	step [117/144], loss=73.7130
	step [118/144], loss=84.7193
	step [119/144], loss=94.3352
	step [120/144], loss=90.8407
	step [121/144], loss=97.0597
	step [122/144], loss=83.5390
	step [123/144], loss=87.5893
	step [124/144], loss=72.9087
	step [125/144], loss=87.4698
	step [126/144], loss=86.2015
	step [127/144], loss=81.2099
	step [128/144], loss=76.9233
	step [129/144], loss=73.6957
	step [130/144], loss=84.3747
	step [131/144], loss=91.4043
	step [132/144], loss=90.7906
	step [133/144], loss=80.9545
	step [134/144], loss=92.7342
	step [135/144], loss=94.5230
	step [136/144], loss=76.2192
	step [137/144], loss=83.9798
	step [138/144], loss=74.3588
	step [139/144], loss=100.9989
	step [140/144], loss=66.2039
	step [141/144], loss=72.0780
	step [142/144], loss=70.8243
	step [143/144], loss=86.4710
	step [144/144], loss=26.2122
	Evaluating
	loss=0.0121, precision=0.3900, recall=0.8525, f1=0.5351
Training epoch 74
	step [1/144], loss=87.8440
	step [2/144], loss=87.5043
	step [3/144], loss=67.8825
	step [4/144], loss=75.8837
	step [5/144], loss=74.8784
	step [6/144], loss=86.7942
	step [7/144], loss=88.3806
	step [8/144], loss=93.7360
	step [9/144], loss=73.5000
	step [10/144], loss=69.3477
	step [11/144], loss=104.5897
	step [12/144], loss=82.8980
	step [13/144], loss=79.9846
	step [14/144], loss=78.4660
	step [15/144], loss=84.0400
	step [16/144], loss=66.7215
	step [17/144], loss=77.5447
	step [18/144], loss=76.0870
	step [19/144], loss=80.9907
	step [20/144], loss=80.4868
	step [21/144], loss=81.7103
	step [22/144], loss=87.8980
	step [23/144], loss=85.3957
	step [24/144], loss=88.9271
	step [25/144], loss=71.5588
	step [26/144], loss=89.5865
	step [27/144], loss=81.0089
	step [28/144], loss=81.5249
	step [29/144], loss=81.9956
	step [30/144], loss=78.7205
	step [31/144], loss=77.1670
	step [32/144], loss=86.3504
	step [33/144], loss=77.3401
	step [34/144], loss=78.9364
	step [35/144], loss=84.6556
	step [36/144], loss=87.9087
	step [37/144], loss=74.7418
	step [38/144], loss=88.3435
	step [39/144], loss=82.3996
	step [40/144], loss=95.3781
	step [41/144], loss=90.5643
	step [42/144], loss=82.7967
	step [43/144], loss=84.1574
	step [44/144], loss=79.8143
	step [45/144], loss=80.1352
	step [46/144], loss=75.4766
	step [47/144], loss=87.4034
	step [48/144], loss=75.6601
	step [49/144], loss=77.2904
	step [50/144], loss=77.6494
	step [51/144], loss=109.4738
	step [52/144], loss=71.6960
	step [53/144], loss=72.6001
	step [54/144], loss=73.6378
	step [55/144], loss=81.5527
	step [56/144], loss=79.2622
	step [57/144], loss=72.2633
	step [58/144], loss=85.5483
	step [59/144], loss=68.9725
	step [60/144], loss=84.3408
	step [61/144], loss=93.8351
	step [62/144], loss=71.8888
	step [63/144], loss=93.9187
	step [64/144], loss=85.2320
	step [65/144], loss=79.9519
	step [66/144], loss=87.5768
	step [67/144], loss=93.7337
	step [68/144], loss=73.7377
	step [69/144], loss=76.3018
	step [70/144], loss=90.5257
	step [71/144], loss=83.7630
	step [72/144], loss=85.0571
	step [73/144], loss=82.6729
	step [74/144], loss=81.4237
	step [75/144], loss=89.3454
	step [76/144], loss=84.0358
	step [77/144], loss=70.4893
	step [78/144], loss=74.0485
	step [79/144], loss=80.2362
	step [80/144], loss=89.1352
	step [81/144], loss=94.3796
	step [82/144], loss=79.7402
	step [83/144], loss=80.0325
	step [84/144], loss=78.5683
	step [85/144], loss=70.2798
	step [86/144], loss=78.8969
	step [87/144], loss=87.0593
	step [88/144], loss=89.1824
	step [89/144], loss=80.1315
	step [90/144], loss=77.8226
	step [91/144], loss=79.7305
	step [92/144], loss=104.7263
	step [93/144], loss=73.3574
	step [94/144], loss=95.0998
	step [95/144], loss=84.8028
	step [96/144], loss=92.6147
	step [97/144], loss=81.0189
	step [98/144], loss=93.4592
	step [99/144], loss=68.9843
	step [100/144], loss=77.1384
	step [101/144], loss=83.4186
	step [102/144], loss=87.6526
	step [103/144], loss=95.7132
	step [104/144], loss=77.7865
	step [105/144], loss=79.5484
	step [106/144], loss=94.6705
	step [107/144], loss=93.7618
	step [108/144], loss=85.5613
	step [109/144], loss=77.5947
	step [110/144], loss=78.2915
	step [111/144], loss=81.4176
	step [112/144], loss=91.3069
	step [113/144], loss=69.0372
	step [114/144], loss=93.4671
	step [115/144], loss=74.4610
	step [116/144], loss=82.5455
	step [117/144], loss=78.9456
	step [118/144], loss=91.0026
	step [119/144], loss=81.4799
	step [120/144], loss=79.0602
	step [121/144], loss=80.8479
	step [122/144], loss=83.1645
	step [123/144], loss=68.3839
	step [124/144], loss=99.8778
	step [125/144], loss=100.7361
	step [126/144], loss=98.5515
	step [127/144], loss=82.5813
	step [128/144], loss=83.5302
	step [129/144], loss=76.3945
	step [130/144], loss=69.9696
	step [131/144], loss=88.8535
	step [132/144], loss=92.6629
	step [133/144], loss=87.4933
	step [134/144], loss=86.6034
	step [135/144], loss=79.8326
	step [136/144], loss=74.5604
	step [137/144], loss=90.2707
	step [138/144], loss=91.6383
	step [139/144], loss=87.9147
	step [140/144], loss=84.4220
	step [141/144], loss=83.2600
	step [142/144], loss=82.0186
	step [143/144], loss=77.7377
	step [144/144], loss=24.5704
	Evaluating
	loss=0.0122, precision=0.3852, recall=0.8393, f1=0.5281
Training epoch 75
	step [1/144], loss=96.8688
	step [2/144], loss=102.1425
	step [3/144], loss=90.4224
	step [4/144], loss=73.7321
	step [5/144], loss=87.8949
	step [6/144], loss=80.3266
	step [7/144], loss=91.9775
	step [8/144], loss=66.6365
	step [9/144], loss=97.6464
	step [10/144], loss=86.9610
	step [11/144], loss=84.0644
	step [12/144], loss=76.1172
	step [13/144], loss=85.9168
	step [14/144], loss=83.7619
	step [15/144], loss=79.4939
	step [16/144], loss=82.7277
	step [17/144], loss=75.7391
	step [18/144], loss=86.6148
	step [19/144], loss=89.0306
	step [20/144], loss=79.8560
	step [21/144], loss=97.0300
	step [22/144], loss=70.4582
	step [23/144], loss=82.6123
	step [24/144], loss=92.6000
	step [25/144], loss=87.2495
	step [26/144], loss=93.3461
	step [27/144], loss=65.1728
	step [28/144], loss=91.9545
	step [29/144], loss=107.2291
	step [30/144], loss=71.3687
	step [31/144], loss=78.2969
	step [32/144], loss=86.7241
	step [33/144], loss=74.7847
	step [34/144], loss=80.4129
	step [35/144], loss=83.9527
	step [36/144], loss=77.7999
	step [37/144], loss=83.3298
	step [38/144], loss=87.0464
	step [39/144], loss=79.5509
	step [40/144], loss=85.0800
	step [41/144], loss=95.9145
	step [42/144], loss=83.7233
	step [43/144], loss=76.3396
	step [44/144], loss=81.2117
	step [45/144], loss=85.7185
	step [46/144], loss=99.2033
	step [47/144], loss=78.9590
	step [48/144], loss=96.7625
	step [49/144], loss=80.4414
	step [50/144], loss=79.6904
	step [51/144], loss=83.2260
	step [52/144], loss=91.2170
	step [53/144], loss=90.8864
	step [54/144], loss=77.5114
	step [55/144], loss=85.8842
	step [56/144], loss=81.2938
	step [57/144], loss=78.2341
	step [58/144], loss=65.6312
	step [59/144], loss=55.4126
	step [60/144], loss=69.3244
	step [61/144], loss=70.7309
	step [62/144], loss=67.9951
	step [63/144], loss=80.0219
	step [64/144], loss=76.5752
	step [65/144], loss=80.2716
	step [66/144], loss=81.9937
	step [67/144], loss=80.9398
	step [68/144], loss=69.1168
	step [69/144], loss=93.5842
	step [70/144], loss=85.8410
	step [71/144], loss=92.7996
	step [72/144], loss=85.6891
	step [73/144], loss=84.8343
	step [74/144], loss=82.9221
	step [75/144], loss=84.9108
	step [76/144], loss=78.6782
	step [77/144], loss=72.8662
	step [78/144], loss=91.7090
	step [79/144], loss=78.2146
	step [80/144], loss=93.5546
	step [81/144], loss=69.6562
	step [82/144], loss=72.3294
	step [83/144], loss=81.2178
	step [84/144], loss=82.5603
	step [85/144], loss=87.6068
	step [86/144], loss=77.2265
	step [87/144], loss=89.1870
	step [88/144], loss=79.5877
	step [89/144], loss=84.4250
	step [90/144], loss=85.8940
	step [91/144], loss=107.9869
	step [92/144], loss=71.0625
	step [93/144], loss=88.8279
	step [94/144], loss=80.2732
	step [95/144], loss=76.3466
	step [96/144], loss=85.0055
	step [97/144], loss=74.3370
	step [98/144], loss=62.4295
	step [99/144], loss=79.1364
	step [100/144], loss=85.8220
	step [101/144], loss=99.8002
	step [102/144], loss=78.2562
	step [103/144], loss=82.0616
	step [104/144], loss=84.5404
	step [105/144], loss=88.3163
	step [106/144], loss=77.7432
	step [107/144], loss=73.4058
	step [108/144], loss=106.2442
	step [109/144], loss=89.8220
	step [110/144], loss=87.6888
	step [111/144], loss=73.9610
	step [112/144], loss=81.3915
	step [113/144], loss=77.4594
	step [114/144], loss=85.6174
	step [115/144], loss=84.8604
	step [116/144], loss=86.2570
	step [117/144], loss=95.0143
	step [118/144], loss=78.4138
	step [119/144], loss=80.6582
	step [120/144], loss=76.7055
	step [121/144], loss=90.1482
	step [122/144], loss=99.3514
	step [123/144], loss=92.9837
	step [124/144], loss=80.2695
	step [125/144], loss=64.9005
	step [126/144], loss=90.7835
	step [127/144], loss=87.2306
	step [128/144], loss=73.6113
	step [129/144], loss=85.0724
	step [130/144], loss=85.7038
	step [131/144], loss=77.6122
	step [132/144], loss=75.1384
	step [133/144], loss=72.3204
	step [134/144], loss=76.5692
	step [135/144], loss=86.0625
	step [136/144], loss=80.8568
	step [137/144], loss=88.5446
	step [138/144], loss=82.1036
	step [139/144], loss=82.2418
	step [140/144], loss=78.5966
	step [141/144], loss=71.5348
	step [142/144], loss=63.7334
	step [143/144], loss=69.5224
	step [144/144], loss=25.8363
	Evaluating
	loss=0.0121, precision=0.3795, recall=0.8638, f1=0.5274
Training epoch 76
	step [1/144], loss=76.2338
	step [2/144], loss=77.0838
	step [3/144], loss=74.1674
	step [4/144], loss=90.8853
	step [5/144], loss=86.7354
	step [6/144], loss=88.0098
	step [7/144], loss=78.7631
	step [8/144], loss=87.1248
	step [9/144], loss=95.2346
	step [10/144], loss=82.5304
	step [11/144], loss=71.3272
	step [12/144], loss=81.4595
	step [13/144], loss=81.9571
	step [14/144], loss=89.7471
	step [15/144], loss=78.5476
	step [16/144], loss=72.3001
	step [17/144], loss=99.4514
	step [18/144], loss=82.6266
	step [19/144], loss=66.5901
	step [20/144], loss=72.4459
	step [21/144], loss=89.5516
	step [22/144], loss=85.9968
	step [23/144], loss=74.1166
	step [24/144], loss=109.4550
	step [25/144], loss=78.7682
	step [26/144], loss=94.9265
	step [27/144], loss=72.7767
	step [28/144], loss=87.8721
	step [29/144], loss=82.3136
	step [30/144], loss=88.4526
	step [31/144], loss=89.8859
	step [32/144], loss=77.4043
	step [33/144], loss=81.8430
	step [34/144], loss=88.7395
	step [35/144], loss=85.4239
	step [36/144], loss=75.5274
	step [37/144], loss=85.6341
	step [38/144], loss=93.9306
	step [39/144], loss=81.1233
	step [40/144], loss=76.4387
	step [41/144], loss=84.6216
	step [42/144], loss=81.6709
	step [43/144], loss=78.5613
	step [44/144], loss=73.6954
	step [45/144], loss=66.6323
	step [46/144], loss=81.5386
	step [47/144], loss=82.4777
	step [48/144], loss=84.5130
	step [49/144], loss=76.0633
	step [50/144], loss=91.8111
	step [51/144], loss=98.4505
	step [52/144], loss=62.8295
	step [53/144], loss=80.6000
	step [54/144], loss=100.4432
	step [55/144], loss=83.2577
	step [56/144], loss=90.9580
	step [57/144], loss=82.2809
	step [58/144], loss=95.2888
	step [59/144], loss=71.8421
	step [60/144], loss=94.7992
	step [61/144], loss=74.6184
	step [62/144], loss=85.7838
	step [63/144], loss=81.7054
	step [64/144], loss=79.3830
	step [65/144], loss=76.9140
	step [66/144], loss=80.7946
	step [67/144], loss=79.6434
	step [68/144], loss=82.7755
	step [69/144], loss=100.7180
	step [70/144], loss=99.3712
	step [71/144], loss=81.2634
	step [72/144], loss=77.0054
	step [73/144], loss=77.2049
	step [74/144], loss=98.5083
	step [75/144], loss=82.8130
	step [76/144], loss=102.4054
	step [77/144], loss=88.3264
	step [78/144], loss=74.9214
	step [79/144], loss=72.9359
	step [80/144], loss=80.8686
	step [81/144], loss=103.9555
	step [82/144], loss=79.0816
	step [83/144], loss=72.4747
	step [84/144], loss=76.3499
	step [85/144], loss=58.9417
	step [86/144], loss=89.6284
	step [87/144], loss=90.6155
	step [88/144], loss=74.6683
	step [89/144], loss=68.5576
	step [90/144], loss=87.4439
	step [91/144], loss=78.1079
	step [92/144], loss=98.0864
	step [93/144], loss=87.1871
	step [94/144], loss=74.5328
	step [95/144], loss=85.4540
	step [96/144], loss=83.3050
	step [97/144], loss=84.0392
	step [98/144], loss=94.9053
	step [99/144], loss=91.9261
	step [100/144], loss=86.0611
	step [101/144], loss=70.8857
	step [102/144], loss=84.5154
	step [103/144], loss=91.4529
	step [104/144], loss=87.4454
	step [105/144], loss=71.9976
	step [106/144], loss=84.8008
	step [107/144], loss=62.1060
	step [108/144], loss=77.5231
	step [109/144], loss=70.1556
	step [110/144], loss=70.0147
	step [111/144], loss=77.4497
	step [112/144], loss=91.6941
	step [113/144], loss=72.8225
	step [114/144], loss=103.9200
	step [115/144], loss=79.5200
	step [116/144], loss=87.4414
	step [117/144], loss=71.9762
	step [118/144], loss=72.8393
	step [119/144], loss=84.2347
	step [120/144], loss=79.3845
	step [121/144], loss=81.8280
	step [122/144], loss=80.1721
	step [123/144], loss=91.4099
	step [124/144], loss=76.5734
	step [125/144], loss=77.6633
	step [126/144], loss=80.5165
	step [127/144], loss=91.0322
	step [128/144], loss=87.2994
	step [129/144], loss=87.2260
	step [130/144], loss=71.3874
	step [131/144], loss=88.1705
	step [132/144], loss=76.1178
	step [133/144], loss=82.5845
	step [134/144], loss=74.3303
	step [135/144], loss=77.9471
	step [136/144], loss=93.1234
	step [137/144], loss=73.5635
	step [138/144], loss=85.8186
	step [139/144], loss=76.7680
	step [140/144], loss=77.9680
	step [141/144], loss=87.6639
	step [142/144], loss=70.0837
	step [143/144], loss=85.5343
	step [144/144], loss=33.8501
	Evaluating
	loss=0.0118, precision=0.3915, recall=0.8450, f1=0.5351
Training epoch 77
	step [1/144], loss=83.7909
	step [2/144], loss=71.5617
	step [3/144], loss=79.0631
	step [4/144], loss=70.1412
	step [5/144], loss=70.5547
	step [6/144], loss=82.4587
	step [7/144], loss=77.3714
	step [8/144], loss=84.1757
	step [9/144], loss=73.6173
	step [10/144], loss=84.5762
	step [11/144], loss=77.3573
	step [12/144], loss=78.0139
	step [13/144], loss=82.8081
	step [14/144], loss=86.1839
	step [15/144], loss=75.9330
	step [16/144], loss=95.4362
	step [17/144], loss=64.1545
	step [18/144], loss=85.2150
	step [19/144], loss=80.5288
	step [20/144], loss=81.7858
	step [21/144], loss=74.8602
	step [22/144], loss=82.4076
	step [23/144], loss=85.5220
	step [24/144], loss=84.8631
	step [25/144], loss=94.0448
	step [26/144], loss=76.9496
	step [27/144], loss=77.1745
	step [28/144], loss=96.5165
	step [29/144], loss=83.7664
	step [30/144], loss=73.4418
	step [31/144], loss=88.6712
	step [32/144], loss=87.1602
	step [33/144], loss=83.0754
	step [34/144], loss=70.4146
	step [35/144], loss=78.4926
	step [36/144], loss=72.7204
	step [37/144], loss=79.3982
	step [38/144], loss=68.5955
	step [39/144], loss=78.3736
	step [40/144], loss=82.0911
	step [41/144], loss=85.4318
	step [42/144], loss=88.6119
	step [43/144], loss=86.4292
	step [44/144], loss=72.4246
	step [45/144], loss=81.1534
	step [46/144], loss=81.8933
	step [47/144], loss=82.4638
	step [48/144], loss=78.3124
	step [49/144], loss=91.8395
	step [50/144], loss=75.1658
	step [51/144], loss=88.1679
	step [52/144], loss=89.6881
	step [53/144], loss=84.7846
	step [54/144], loss=97.4075
	step [55/144], loss=82.3847
	step [56/144], loss=83.0010
	step [57/144], loss=78.1714
	step [58/144], loss=80.3972
	step [59/144], loss=81.3285
	step [60/144], loss=83.0999
	step [61/144], loss=84.6005
	step [62/144], loss=68.9413
	step [63/144], loss=74.7210
	step [64/144], loss=89.9225
	step [65/144], loss=98.1364
	step [66/144], loss=76.7951
	step [67/144], loss=74.6071
	step [68/144], loss=79.1954
	step [69/144], loss=72.0706
	step [70/144], loss=92.4005
	step [71/144], loss=79.5201
	step [72/144], loss=69.8845
	step [73/144], loss=97.0246
	step [74/144], loss=84.7547
	step [75/144], loss=83.8090
	step [76/144], loss=86.0388
	step [77/144], loss=73.4989
	step [78/144], loss=83.2124
	step [79/144], loss=80.7852
	step [80/144], loss=77.4389
	step [81/144], loss=77.4518
	step [82/144], loss=81.5637
	step [83/144], loss=88.0528
	step [84/144], loss=80.0268
	step [85/144], loss=100.3516
	step [86/144], loss=78.6593
	step [87/144], loss=77.4044
	step [88/144], loss=75.8534
	step [89/144], loss=75.6498
	step [90/144], loss=85.0105
	step [91/144], loss=73.8526
	step [92/144], loss=92.1792
	step [93/144], loss=100.1454
	step [94/144], loss=77.8796
	step [95/144], loss=81.6252
	step [96/144], loss=73.4450
	step [97/144], loss=100.1922
	step [98/144], loss=79.3168
	step [99/144], loss=78.2155
	step [100/144], loss=76.8481
	step [101/144], loss=75.0763
	step [102/144], loss=71.7830
	step [103/144], loss=81.7428
	step [104/144], loss=89.4225
	step [105/144], loss=93.7635
	step [106/144], loss=84.5765
	step [107/144], loss=79.2431
	step [108/144], loss=86.1652
	step [109/144], loss=84.6019
	step [110/144], loss=89.3863
	step [111/144], loss=93.4799
	step [112/144], loss=89.6072
	step [113/144], loss=93.0950
	step [114/144], loss=92.4935
	step [115/144], loss=73.1111
	step [116/144], loss=75.7897
	step [117/144], loss=83.4462
	step [118/144], loss=85.6101
	step [119/144], loss=73.3243
	step [120/144], loss=83.2524
	step [121/144], loss=79.0042
	step [122/144], loss=108.1134
	step [123/144], loss=77.4012
	step [124/144], loss=82.1032
	step [125/144], loss=91.8003
	step [126/144], loss=72.5730
	step [127/144], loss=85.4848
	step [128/144], loss=83.9178
	step [129/144], loss=88.3937
	step [130/144], loss=87.9099
	step [131/144], loss=84.8800
	step [132/144], loss=62.5530
	step [133/144], loss=85.8174
	step [134/144], loss=67.3304
	step [135/144], loss=93.0772
	step [136/144], loss=89.1448
	step [137/144], loss=90.9099
	step [138/144], loss=78.2080
	step [139/144], loss=69.5457
	step [140/144], loss=74.6401
	step [141/144], loss=79.4663
	step [142/144], loss=66.8123
	step [143/144], loss=76.9283
	step [144/144], loss=23.7384
	Evaluating
	loss=0.0098, precision=0.4513, recall=0.8345, f1=0.5858
saving model as: 1_saved_model.pth
Training epoch 78
	step [1/144], loss=80.3317
	step [2/144], loss=90.8658
	step [3/144], loss=82.1217
	step [4/144], loss=86.8285
	step [5/144], loss=83.2525
	step [6/144], loss=89.7116
	step [7/144], loss=74.0688
	step [8/144], loss=95.5067
	step [9/144], loss=81.2251
	step [10/144], loss=95.2121
	step [11/144], loss=84.9120
	step [12/144], loss=88.8872
	step [13/144], loss=73.7882
	step [14/144], loss=97.7345
	step [15/144], loss=90.8899
	step [16/144], loss=72.5375
	step [17/144], loss=77.7881
	step [18/144], loss=103.4178
	step [19/144], loss=89.8997
	step [20/144], loss=87.3722
	step [21/144], loss=92.9229
	step [22/144], loss=93.1696
	step [23/144], loss=77.0114
	step [24/144], loss=60.9917
	step [25/144], loss=74.3532
	step [26/144], loss=86.2056
	step [27/144], loss=65.2300
	step [28/144], loss=74.1800
	step [29/144], loss=60.1974
	step [30/144], loss=69.2209
	step [31/144], loss=68.6689
	step [32/144], loss=84.8407
	step [33/144], loss=90.8451
	step [34/144], loss=68.7109
	step [35/144], loss=70.6207
	step [36/144], loss=78.0994
	step [37/144], loss=76.1759
	step [38/144], loss=94.8641
	step [39/144], loss=94.0056
	step [40/144], loss=90.9032
	step [41/144], loss=77.1836
	step [42/144], loss=76.4585
	step [43/144], loss=91.0795
	step [44/144], loss=80.0334
	step [45/144], loss=74.7716
	step [46/144], loss=85.8856
	step [47/144], loss=80.6553
	step [48/144], loss=72.2471
	step [49/144], loss=76.6600
	step [50/144], loss=83.1887
	step [51/144], loss=77.2907
	step [52/144], loss=72.4745
	step [53/144], loss=87.4287
	step [54/144], loss=83.8501
	step [55/144], loss=71.8131
	step [56/144], loss=77.4200
	step [57/144], loss=81.6093
	step [58/144], loss=82.4593
	step [59/144], loss=64.4986
	step [60/144], loss=80.8618
	step [61/144], loss=88.0348
	step [62/144], loss=71.4721
	step [63/144], loss=94.9680
	step [64/144], loss=81.6700
	step [65/144], loss=85.8435
	step [66/144], loss=84.6013
	step [67/144], loss=97.5069
	step [68/144], loss=83.4021
	step [69/144], loss=91.4129
	step [70/144], loss=87.2958
	step [71/144], loss=86.2900
	step [72/144], loss=86.5541
	step [73/144], loss=88.9824
	step [74/144], loss=86.5877
	step [75/144], loss=71.8728
	step [76/144], loss=92.4134
	step [77/144], loss=80.8625
	step [78/144], loss=83.6960
	step [79/144], loss=81.4617
	step [80/144], loss=59.5246
	step [81/144], loss=81.0438
	step [82/144], loss=76.0969
	step [83/144], loss=83.4312
	step [84/144], loss=91.9181
	step [85/144], loss=72.7207
	step [86/144], loss=66.0240
	step [87/144], loss=104.0453
	step [88/144], loss=81.9035
	step [89/144], loss=71.2787
	step [90/144], loss=96.7008
	step [91/144], loss=85.0787
	step [92/144], loss=96.7206
	step [93/144], loss=87.4077
	step [94/144], loss=100.4027
	step [95/144], loss=89.2693
	step [96/144], loss=87.5147
	step [97/144], loss=83.3730
	step [98/144], loss=91.8119
	step [99/144], loss=79.8212
	step [100/144], loss=85.1965
	step [101/144], loss=71.1599
	step [102/144], loss=87.4301
	step [103/144], loss=80.5381
	step [104/144], loss=90.8174
	step [105/144], loss=75.0758
	step [106/144], loss=88.0956
	step [107/144], loss=86.1863
	step [108/144], loss=87.6588
	step [109/144], loss=83.8226
	step [110/144], loss=95.6956
	step [111/144], loss=75.3899
	step [112/144], loss=92.0317
	step [113/144], loss=71.9016
	step [114/144], loss=77.9699
	step [115/144], loss=88.1757
	step [116/144], loss=85.0337
	step [117/144], loss=80.1987
	step [118/144], loss=86.6867
	step [119/144], loss=58.0057
	step [120/144], loss=88.7586
	step [121/144], loss=85.7883
	step [122/144], loss=80.1333
	step [123/144], loss=77.6776
	step [124/144], loss=66.8504
	step [125/144], loss=65.4323
	step [126/144], loss=65.9445
	step [127/144], loss=78.6602
	step [128/144], loss=100.8945
	step [129/144], loss=93.6522
	step [130/144], loss=90.4908
	step [131/144], loss=88.9706
	step [132/144], loss=75.8096
	step [133/144], loss=70.3572
	step [134/144], loss=67.0954
	step [135/144], loss=82.8723
	step [136/144], loss=79.0118
	step [137/144], loss=74.3734
	step [138/144], loss=75.4384
	step [139/144], loss=71.1653
	step [140/144], loss=80.4070
	step [141/144], loss=78.4675
	step [142/144], loss=61.9130
	step [143/144], loss=65.7654
	step [144/144], loss=31.0385
	Evaluating
	loss=0.0121, precision=0.3844, recall=0.8522, f1=0.5298
Training epoch 79
	step [1/144], loss=72.9442
	step [2/144], loss=60.6382
	step [3/144], loss=87.9972
	step [4/144], loss=88.9106
	step [5/144], loss=81.6088
	step [6/144], loss=72.0569
	step [7/144], loss=95.4179
	step [8/144], loss=89.8299
	step [9/144], loss=70.2996
	step [10/144], loss=83.3518
	step [11/144], loss=79.7268
	step [12/144], loss=89.6703
	step [13/144], loss=92.8105
	step [14/144], loss=98.9984
	step [15/144], loss=87.3022
	step [16/144], loss=85.6823
	step [17/144], loss=69.1833
	step [18/144], loss=71.9109
	step [19/144], loss=89.6244
	step [20/144], loss=69.2905
	step [21/144], loss=75.4333
	step [22/144], loss=77.9807
	step [23/144], loss=74.4714
	step [24/144], loss=90.5119
	step [25/144], loss=79.6661
	step [26/144], loss=68.7582
	step [27/144], loss=77.3289
	step [28/144], loss=81.7698
	step [29/144], loss=76.0644
	step [30/144], loss=68.5550
	step [31/144], loss=86.0555
	step [32/144], loss=86.8107
	step [33/144], loss=71.9312
	step [34/144], loss=79.3938
	step [35/144], loss=76.9463
	step [36/144], loss=85.3041
	step [37/144], loss=68.3133
	step [38/144], loss=96.8699
	step [39/144], loss=64.6922
	step [40/144], loss=95.5231
	step [41/144], loss=84.8567
	step [42/144], loss=69.1821
	step [43/144], loss=90.5123
	step [44/144], loss=90.5343
	step [45/144], loss=92.4957
	step [46/144], loss=85.4544
	step [47/144], loss=81.5786
	step [48/144], loss=66.9416
	step [49/144], loss=73.4817
	step [50/144], loss=93.0804
	step [51/144], loss=87.1232
	step [52/144], loss=84.5072
	step [53/144], loss=77.5981
	step [54/144], loss=80.7048
	step [55/144], loss=70.2338
	step [56/144], loss=78.4624
	step [57/144], loss=67.2217
	step [58/144], loss=87.1543
	step [59/144], loss=89.3278
	step [60/144], loss=84.3684
	step [61/144], loss=77.0857
	step [62/144], loss=94.1423
	step [63/144], loss=71.6248
	step [64/144], loss=78.0048
	step [65/144], loss=88.2732
	step [66/144], loss=60.3543
	step [67/144], loss=80.1871
	step [68/144], loss=78.6999
	step [69/144], loss=80.5648
	step [70/144], loss=87.5671
	step [71/144], loss=71.2489
	step [72/144], loss=90.4022
	step [73/144], loss=64.7212
	step [74/144], loss=92.0079
	step [75/144], loss=74.8179
	step [76/144], loss=71.6992
	step [77/144], loss=93.8117
	step [78/144], loss=78.2267
	step [79/144], loss=81.2650
	step [80/144], loss=70.3461
	step [81/144], loss=68.7735
	step [82/144], loss=80.3891
	step [83/144], loss=72.9693
	step [84/144], loss=77.5915
	step [85/144], loss=74.0796
	step [86/144], loss=89.5014
	step [87/144], loss=93.2018
	step [88/144], loss=75.1360
	step [89/144], loss=94.0779
	step [90/144], loss=89.3296
	step [91/144], loss=81.2867
	step [92/144], loss=82.6180
	step [93/144], loss=83.1135
	step [94/144], loss=66.4310
	step [95/144], loss=81.1386
	step [96/144], loss=82.6091
	step [97/144], loss=71.7063
	step [98/144], loss=88.4592
	step [99/144], loss=100.8791
	step [100/144], loss=87.9616
	step [101/144], loss=84.4532
	step [102/144], loss=85.2187
	step [103/144], loss=68.8611
	step [104/144], loss=75.9130
	step [105/144], loss=80.1543
	step [106/144], loss=89.7274
	step [107/144], loss=76.0310
	step [108/144], loss=92.7086
	step [109/144], loss=80.4791
	step [110/144], loss=82.9029
	step [111/144], loss=86.2120
	step [112/144], loss=76.8381
	step [113/144], loss=83.6849
	step [114/144], loss=86.4335
	step [115/144], loss=80.6625
	step [116/144], loss=88.2602
	step [117/144], loss=78.2684
	step [118/144], loss=72.8255
	step [119/144], loss=86.0288
	step [120/144], loss=89.2974
	step [121/144], loss=69.7243
	step [122/144], loss=98.9339
	step [123/144], loss=92.9055
	step [124/144], loss=84.6388
	step [125/144], loss=75.9109
	step [126/144], loss=90.7823
	step [127/144], loss=71.2131
	step [128/144], loss=97.2277
	step [129/144], loss=83.1179
	step [130/144], loss=77.2167
	step [131/144], loss=82.9102
	step [132/144], loss=97.8590
	step [133/144], loss=71.7043
	step [134/144], loss=95.9737
	step [135/144], loss=72.6320
	step [136/144], loss=82.1684
	step [137/144], loss=78.9088
	step [138/144], loss=81.1651
	step [139/144], loss=86.0486
	step [140/144], loss=82.2808
	step [141/144], loss=74.0119
	step [142/144], loss=85.2100
	step [143/144], loss=71.7713
	step [144/144], loss=33.8508
	Evaluating
	loss=0.0116, precision=0.3825, recall=0.8417, f1=0.5260
Training epoch 80
	step [1/144], loss=84.0596
	step [2/144], loss=80.6061
	step [3/144], loss=86.1689
	step [4/144], loss=88.9360
	step [5/144], loss=70.6872
	step [6/144], loss=75.6192
	step [7/144], loss=106.8996
	step [8/144], loss=66.8148
	step [9/144], loss=69.9857
	step [10/144], loss=83.4073
	step [11/144], loss=73.6909
	step [12/144], loss=81.3217
	step [13/144], loss=93.9849
	step [14/144], loss=89.9834
	step [15/144], loss=98.2630
	step [16/144], loss=86.9146
	step [17/144], loss=81.0468
	step [18/144], loss=78.7425
	step [19/144], loss=85.1764
	step [20/144], loss=83.4243
	step [21/144], loss=85.3866
	step [22/144], loss=77.5394
	step [23/144], loss=83.4317
	step [24/144], loss=94.9923
	step [25/144], loss=70.5147
	step [26/144], loss=73.4366
	step [27/144], loss=94.7384
	step [28/144], loss=107.0919
	step [29/144], loss=79.3316
	step [30/144], loss=62.5750
	step [31/144], loss=72.5669
	step [32/144], loss=77.5598
	step [33/144], loss=90.0804
	step [34/144], loss=73.2262
	step [35/144], loss=78.0556
	step [36/144], loss=74.9978
	step [37/144], loss=81.0584
	step [38/144], loss=87.8834
	step [39/144], loss=79.5604
	step [40/144], loss=77.6379
	step [41/144], loss=78.7769
	step [42/144], loss=71.2783
	step [43/144], loss=78.0368
	step [44/144], loss=76.5688
	step [45/144], loss=72.9512
	step [46/144], loss=75.2923
	step [47/144], loss=91.9078
	step [48/144], loss=82.7871
	step [49/144], loss=91.6482
	step [50/144], loss=64.5628
	step [51/144], loss=78.6128
	step [52/144], loss=76.7086
	step [53/144], loss=70.0504
	step [54/144], loss=75.4164
	step [55/144], loss=76.6976
	step [56/144], loss=83.8420
	step [57/144], loss=81.9696
	step [58/144], loss=82.5303
	step [59/144], loss=85.7776
	step [60/144], loss=65.4083
	step [61/144], loss=101.3840
	step [62/144], loss=65.7266
	step [63/144], loss=75.6693
	step [64/144], loss=94.0400
	step [65/144], loss=81.4693
	step [66/144], loss=80.5913
	step [67/144], loss=83.1968
	step [68/144], loss=91.9326
	step [69/144], loss=81.9121
	step [70/144], loss=89.7502
	step [71/144], loss=75.3904
	step [72/144], loss=91.4008
	step [73/144], loss=83.7395
	step [74/144], loss=87.7335
	step [75/144], loss=86.2913
	step [76/144], loss=77.3896
	step [77/144], loss=74.6582
	step [78/144], loss=69.4345
	step [79/144], loss=81.8769
	step [80/144], loss=81.5533
	step [81/144], loss=77.8524
	step [82/144], loss=99.2567
	step [83/144], loss=84.1129
	step [84/144], loss=57.7936
	step [85/144], loss=70.7837
	step [86/144], loss=93.0853
	step [87/144], loss=85.2735
	step [88/144], loss=73.5703
	step [89/144], loss=82.9058
	step [90/144], loss=65.5448
	step [91/144], loss=79.1077
	step [92/144], loss=67.4595
	step [93/144], loss=71.7296
	step [94/144], loss=77.3350
	step [95/144], loss=73.9069
	step [96/144], loss=79.9842
	step [97/144], loss=91.6489
	step [98/144], loss=88.6367
	step [99/144], loss=77.2016
	step [100/144], loss=72.8862
	step [101/144], loss=81.1186
	step [102/144], loss=83.3074
	step [103/144], loss=80.9131
	step [104/144], loss=77.0592
	step [105/144], loss=82.8560
	step [106/144], loss=72.4610
	step [107/144], loss=76.7060
	step [108/144], loss=76.0619
	step [109/144], loss=94.5173
	step [110/144], loss=83.2345
	step [111/144], loss=74.4631
	step [112/144], loss=67.3364
	step [113/144], loss=85.6868
	step [114/144], loss=80.2476
	step [115/144], loss=73.4900
	step [116/144], loss=90.1741
	step [117/144], loss=79.5594
	step [118/144], loss=63.9926
	step [119/144], loss=82.6964
	step [120/144], loss=85.2717
	step [121/144], loss=74.6855
	step [122/144], loss=99.4714
	step [123/144], loss=83.3955
	step [124/144], loss=84.1675
	step [125/144], loss=81.4838
	step [126/144], loss=72.6082
	step [127/144], loss=65.6354
	step [128/144], loss=75.1087
	step [129/144], loss=79.2045
	step [130/144], loss=81.0047
	step [131/144], loss=90.4496
	step [132/144], loss=104.6622
	step [133/144], loss=88.0643
	step [134/144], loss=85.4186
	step [135/144], loss=92.2387
	step [136/144], loss=78.0247
	step [137/144], loss=83.0577
	step [138/144], loss=67.6473
	step [139/144], loss=79.5596
	step [140/144], loss=85.8560
	step [141/144], loss=88.0161
	step [142/144], loss=86.3980
	step [143/144], loss=77.3891
	step [144/144], loss=24.1026
	Evaluating
	loss=0.0109, precision=0.4171, recall=0.8314, f1=0.5555
Training epoch 81
	step [1/144], loss=70.2371
	step [2/144], loss=75.6873
	step [3/144], loss=93.4397
	step [4/144], loss=80.1749
	step [5/144], loss=70.1041
	step [6/144], loss=79.2985
	step [7/144], loss=80.8226
	step [8/144], loss=89.7148
	step [9/144], loss=74.5042
	step [10/144], loss=64.4092
	step [11/144], loss=70.0486
	step [12/144], loss=85.9808
	step [13/144], loss=92.2080
	step [14/144], loss=66.7887
	step [15/144], loss=78.0660
	step [16/144], loss=83.0965
	step [17/144], loss=65.4619
	step [18/144], loss=82.8156
	step [19/144], loss=90.2704
	step [20/144], loss=92.0686
	step [21/144], loss=84.7799
	step [22/144], loss=69.8304
	step [23/144], loss=83.9983
	step [24/144], loss=84.4975
	step [25/144], loss=79.9317
	step [26/144], loss=77.6588
	step [27/144], loss=85.4395
	step [28/144], loss=86.5339
	step [29/144], loss=71.9493
	step [30/144], loss=66.6041
	step [31/144], loss=72.1262
	step [32/144], loss=60.6155
	step [33/144], loss=74.1871
	step [34/144], loss=87.4989
	step [35/144], loss=82.1317
	step [36/144], loss=85.8884
	step [37/144], loss=89.0614
	step [38/144], loss=78.6408
	step [39/144], loss=87.5592
	step [40/144], loss=89.0691
	step [41/144], loss=79.2290
	step [42/144], loss=93.5830
	step [43/144], loss=85.0673
	step [44/144], loss=63.8113
	step [45/144], loss=79.4228
	step [46/144], loss=90.8371
	step [47/144], loss=74.8885
	step [48/144], loss=91.2139
	step [49/144], loss=88.6737
	step [50/144], loss=84.2346
	step [51/144], loss=91.5465
	step [52/144], loss=99.0293
	step [53/144], loss=79.5134
	step [54/144], loss=91.8539
	step [55/144], loss=77.1276
	step [56/144], loss=81.3496
	step [57/144], loss=84.1939
	step [58/144], loss=87.3073
	step [59/144], loss=76.7030
	step [60/144], loss=85.3392
	step [61/144], loss=97.6111
	step [62/144], loss=86.9555
	step [63/144], loss=71.1735
	step [64/144], loss=79.3857
	step [65/144], loss=88.6632
	step [66/144], loss=78.1069
	step [67/144], loss=78.6382
	step [68/144], loss=82.4361
	step [69/144], loss=72.3504
	step [70/144], loss=79.6826
	step [71/144], loss=58.9354
	step [72/144], loss=78.0662
	step [73/144], loss=69.6946
	step [74/144], loss=89.6566
	step [75/144], loss=90.3474
	step [76/144], loss=70.0950
	step [77/144], loss=82.2419
	step [78/144], loss=71.2526
	step [79/144], loss=91.6265
	step [80/144], loss=84.8681
	step [81/144], loss=82.5180
	step [82/144], loss=91.5791
	step [83/144], loss=84.5437
	step [84/144], loss=83.6532
	step [85/144], loss=82.3264
	step [86/144], loss=89.2187
	step [87/144], loss=89.4101
	step [88/144], loss=75.5235
	step [89/144], loss=80.4471
	step [90/144], loss=81.7735
	step [91/144], loss=89.1194
	step [92/144], loss=66.8394
	step [93/144], loss=56.9122
	step [94/144], loss=71.7572
	step [95/144], loss=91.3936
	step [96/144], loss=78.6127
	step [97/144], loss=94.2270
	step [98/144], loss=85.7254
	step [99/144], loss=73.5813
	step [100/144], loss=84.6506
	step [101/144], loss=74.5022
	step [102/144], loss=84.5332
	step [103/144], loss=96.0525
	step [104/144], loss=73.1571
	step [105/144], loss=79.7639
	step [106/144], loss=69.8062
	step [107/144], loss=74.3447
	step [108/144], loss=79.8160
	step [109/144], loss=82.5003
	step [110/144], loss=71.8103
	step [111/144], loss=80.2478
	step [112/144], loss=83.0829
	step [113/144], loss=77.2279
	step [114/144], loss=86.5532
	step [115/144], loss=80.6668
	step [116/144], loss=71.9463
	step [117/144], loss=78.2328
	step [118/144], loss=97.9897
	step [119/144], loss=88.8831
	step [120/144], loss=86.4553
	step [121/144], loss=79.9178
	step [122/144], loss=74.3409
	step [123/144], loss=71.1195
	step [124/144], loss=88.6108
	step [125/144], loss=76.4028
	step [126/144], loss=88.3320
	step [127/144], loss=86.8783
	step [128/144], loss=93.6645
	step [129/144], loss=90.6174
	step [130/144], loss=71.1786
	step [131/144], loss=72.2558
	step [132/144], loss=82.7346
	step [133/144], loss=67.5611
	step [134/144], loss=77.0526
	step [135/144], loss=91.0125
	step [136/144], loss=77.9655
	step [137/144], loss=83.2722
	step [138/144], loss=77.4043
	step [139/144], loss=85.8773
	step [140/144], loss=72.8555
	step [141/144], loss=87.1400
	step [142/144], loss=71.8449
	step [143/144], loss=85.1533
	step [144/144], loss=24.9680
	Evaluating
	loss=0.0120, precision=0.3772, recall=0.8375, f1=0.5202
Training epoch 82
	step [1/144], loss=76.0903
	step [2/144], loss=89.9150
	step [3/144], loss=96.3509
	step [4/144], loss=77.7810
	step [5/144], loss=68.3788
	step [6/144], loss=86.1977
	step [7/144], loss=93.7278
	step [8/144], loss=84.3587
	step [9/144], loss=72.1928
	step [10/144], loss=80.2522
	step [11/144], loss=77.1637
	step [12/144], loss=75.0742
	step [13/144], loss=90.0515
	step [14/144], loss=82.5879
	step [15/144], loss=88.1341
	step [16/144], loss=81.7156
	step [17/144], loss=84.8011
	step [18/144], loss=88.0615
	step [19/144], loss=92.0692
	step [20/144], loss=84.3850
	step [21/144], loss=74.9046
	step [22/144], loss=90.1574
	step [23/144], loss=80.2491
	step [24/144], loss=68.6211
	step [25/144], loss=74.3605
	step [26/144], loss=71.2799
	step [27/144], loss=78.5160
	step [28/144], loss=81.5486
	step [29/144], loss=94.9070
	step [30/144], loss=83.3406
	step [31/144], loss=68.6606
	step [32/144], loss=89.7239
	step [33/144], loss=81.4254
	step [34/144], loss=85.4181
	step [35/144], loss=87.0595
	step [36/144], loss=89.7022
	step [37/144], loss=74.8681
	step [38/144], loss=80.9969
	step [39/144], loss=79.6826
	step [40/144], loss=87.2806
	step [41/144], loss=88.7587
	step [42/144], loss=77.8755
	step [43/144], loss=67.8559
	step [44/144], loss=80.9838
	step [45/144], loss=80.6491
	step [46/144], loss=79.5254
	step [47/144], loss=80.5166
	step [48/144], loss=85.5692
	step [49/144], loss=78.9792
	step [50/144], loss=72.1411
	step [51/144], loss=79.3732
	step [52/144], loss=86.9677
	step [53/144], loss=63.5585
	step [54/144], loss=87.6310
	step [55/144], loss=66.2749
	step [56/144], loss=81.2345
	step [57/144], loss=73.7324
	step [58/144], loss=92.0324
	step [59/144], loss=81.0208
	step [60/144], loss=75.9691
	step [61/144], loss=74.4881
	step [62/144], loss=76.3067
	step [63/144], loss=74.9399
	step [64/144], loss=87.5627
	step [65/144], loss=80.1942
	step [66/144], loss=70.7983
	step [67/144], loss=80.2016
	step [68/144], loss=87.5477
	step [69/144], loss=69.4502
	step [70/144], loss=73.2401
	step [71/144], loss=93.1573
	step [72/144], loss=71.1341
	step [73/144], loss=76.1877
	step [74/144], loss=89.6839
	step [75/144], loss=74.8126
	step [76/144], loss=65.6026
	step [77/144], loss=83.6973
	step [78/144], loss=76.0763
	step [79/144], loss=74.4041
	step [80/144], loss=80.0540
	step [81/144], loss=90.1198
	step [82/144], loss=75.7575
	step [83/144], loss=78.3769
	step [84/144], loss=78.3495
	step [85/144], loss=78.6936
	step [86/144], loss=72.5309
	step [87/144], loss=78.0007
	step [88/144], loss=82.3566
	step [89/144], loss=95.7522
	step [90/144], loss=80.5210
	step [91/144], loss=78.4373
	step [92/144], loss=86.7540
	step [93/144], loss=91.8471
	step [94/144], loss=74.8140
	step [95/144], loss=92.2644
	step [96/144], loss=79.3541
	step [97/144], loss=91.8711
	step [98/144], loss=91.2647
	step [99/144], loss=77.7324
	step [100/144], loss=74.0308
	step [101/144], loss=79.6190
	step [102/144], loss=76.7290
	step [103/144], loss=64.9896
	step [104/144], loss=87.5432
	step [105/144], loss=75.3468
	step [106/144], loss=84.1125
	step [107/144], loss=81.0882
	step [108/144], loss=83.0595
	step [109/144], loss=74.4692
	step [110/144], loss=79.2993
	step [111/144], loss=72.2412
	step [112/144], loss=83.4271
	step [113/144], loss=84.4928
	step [114/144], loss=87.2505
	step [115/144], loss=80.2001
	step [116/144], loss=70.6604
	step [117/144], loss=71.2368
	step [118/144], loss=76.7313
	step [119/144], loss=92.1338
	step [120/144], loss=87.2513
	step [121/144], loss=92.9951
	step [122/144], loss=87.7801
	step [123/144], loss=75.2933
	step [124/144], loss=95.5247
	step [125/144], loss=98.7769
	step [126/144], loss=76.2903
	step [127/144], loss=65.1202
	step [128/144], loss=81.9521
	step [129/144], loss=57.2522
	step [130/144], loss=83.3699
	step [131/144], loss=86.1340
	step [132/144], loss=60.4653
	step [133/144], loss=81.1716
	step [134/144], loss=84.5981
	step [135/144], loss=91.4640
	step [136/144], loss=86.0546
	step [137/144], loss=88.0047
	step [138/144], loss=64.4655
	step [139/144], loss=73.4440
	step [140/144], loss=84.4059
	step [141/144], loss=70.8266
	step [142/144], loss=91.6984
	step [143/144], loss=72.5542
	step [144/144], loss=23.9864
	Evaluating
	loss=0.0109, precision=0.4022, recall=0.8486, f1=0.5458
Training epoch 83
	step [1/144], loss=79.1945
	step [2/144], loss=85.3150
	step [3/144], loss=82.9573
	step [4/144], loss=66.5614
	step [5/144], loss=81.5537
	step [6/144], loss=76.7396
	step [7/144], loss=87.1260
	step [8/144], loss=90.1972
	step [9/144], loss=77.1115
	step [10/144], loss=102.3710
	step [11/144], loss=77.2439
	step [12/144], loss=75.8533
	step [13/144], loss=79.8751
	step [14/144], loss=77.9432
	step [15/144], loss=84.8258
	step [16/144], loss=85.1330
	step [17/144], loss=83.0477
	step [18/144], loss=88.9161
	step [19/144], loss=77.1989
	step [20/144], loss=84.6808
	step [21/144], loss=72.7677
	step [22/144], loss=80.3727
	step [23/144], loss=74.3961
	step [24/144], loss=89.9187
	step [25/144], loss=93.8562
	step [26/144], loss=81.7959
	step [27/144], loss=84.9104
	step [28/144], loss=71.2490
	step [29/144], loss=76.5471
	step [30/144], loss=84.3833
	step [31/144], loss=67.6527
	step [32/144], loss=61.2848
	step [33/144], loss=92.7542
	step [34/144], loss=85.5861
	step [35/144], loss=74.4220
	step [36/144], loss=67.7219
	step [37/144], loss=91.6584
	step [38/144], loss=71.9273
	step [39/144], loss=85.2631
	step [40/144], loss=64.1952
	step [41/144], loss=65.2446
	step [42/144], loss=71.3670
	step [43/144], loss=80.3101
	step [44/144], loss=78.8673
	step [45/144], loss=90.2019
	step [46/144], loss=78.5637
	step [47/144], loss=76.0357
	step [48/144], loss=77.6121
	step [49/144], loss=100.6380
	step [50/144], loss=78.7593
	step [51/144], loss=105.7400
	step [52/144], loss=72.3790
	step [53/144], loss=80.0274
	step [54/144], loss=68.5162
	step [55/144], loss=70.9433
	step [56/144], loss=88.2495
	step [57/144], loss=80.6593
	step [58/144], loss=63.9632
	step [59/144], loss=69.6518
	step [60/144], loss=87.6002
	step [61/144], loss=71.7530
	step [62/144], loss=82.1072
	step [63/144], loss=73.2425
	step [64/144], loss=79.6402
	step [65/144], loss=66.5120
	step [66/144], loss=73.7894
	step [67/144], loss=88.9007
	step [68/144], loss=74.1289
	step [69/144], loss=73.4409
	step [70/144], loss=96.4727
	step [71/144], loss=98.0267
	step [72/144], loss=77.4657
	step [73/144], loss=81.1184
	step [74/144], loss=88.6411
	step [75/144], loss=96.0693
	step [76/144], loss=84.9091
	step [77/144], loss=76.0230
	step [78/144], loss=77.7331
	step [79/144], loss=88.5403
	step [80/144], loss=86.0688
	step [81/144], loss=72.3786
	step [82/144], loss=92.1341
	step [83/144], loss=84.5943
	step [84/144], loss=75.0639
	step [85/144], loss=83.2596
	step [86/144], loss=78.4476
	step [87/144], loss=68.0260
	step [88/144], loss=65.0946
	step [89/144], loss=79.5194
	step [90/144], loss=66.7501
	step [91/144], loss=78.2317
	step [92/144], loss=87.1149
	step [93/144], loss=63.2342
	step [94/144], loss=87.4311
	step [95/144], loss=76.2872
	step [96/144], loss=95.2376
	step [97/144], loss=68.8291
	step [98/144], loss=84.7619
	step [99/144], loss=78.7524
	step [100/144], loss=90.6127
	step [101/144], loss=85.5732
	step [102/144], loss=76.0133
	step [103/144], loss=82.0423
	step [104/144], loss=68.2361
	step [105/144], loss=68.5083
	step [106/144], loss=82.2854
	step [107/144], loss=85.3216
	step [108/144], loss=96.8886
	step [109/144], loss=79.8999
	step [110/144], loss=82.5540
	step [111/144], loss=76.7386
	step [112/144], loss=89.7446
	step [113/144], loss=74.4085
	step [114/144], loss=84.3858
	step [115/144], loss=99.4312
	step [116/144], loss=87.2758
	step [117/144], loss=83.5061
	step [118/144], loss=77.7534
	step [119/144], loss=75.2227
	step [120/144], loss=89.3408
	step [121/144], loss=98.8421
	step [122/144], loss=89.6588
	step [123/144], loss=70.1355
	step [124/144], loss=69.5555
	step [125/144], loss=81.9666
	step [126/144], loss=87.8686
	step [127/144], loss=58.9318
	step [128/144], loss=70.9510
	step [129/144], loss=80.4944
	step [130/144], loss=67.8953
	step [131/144], loss=88.1310
	step [132/144], loss=76.5998
	step [133/144], loss=84.2524
	step [134/144], loss=70.8794
	step [135/144], loss=87.0555
	step [136/144], loss=68.4466
	step [137/144], loss=76.7749
	step [138/144], loss=90.8954
	step [139/144], loss=72.4479
	step [140/144], loss=82.0806
	step [141/144], loss=67.8539
	step [142/144], loss=85.9937
	step [143/144], loss=93.0195
	step [144/144], loss=28.7296
	Evaluating
	loss=0.0114, precision=0.3927, recall=0.8613, f1=0.5395
Training epoch 84
	step [1/144], loss=86.6936
	step [2/144], loss=74.2762
	step [3/144], loss=88.1016
	step [4/144], loss=64.6149
	step [5/144], loss=92.7082
	step [6/144], loss=66.5819
	step [7/144], loss=77.1000
	step [8/144], loss=79.5532
	step [9/144], loss=76.3925
	step [10/144], loss=90.7275
	step [11/144], loss=79.9128
	step [12/144], loss=84.7051
	step [13/144], loss=89.0879
	step [14/144], loss=87.8968
	step [15/144], loss=55.3897
	step [16/144], loss=85.5418
	step [17/144], loss=73.0314
	step [18/144], loss=89.1757
	step [19/144], loss=94.5781
	step [20/144], loss=80.9204
	step [21/144], loss=85.8096
	step [22/144], loss=80.4330
	step [23/144], loss=70.0168
	step [24/144], loss=76.1511
	step [25/144], loss=95.3589
	step [26/144], loss=59.5045
	step [27/144], loss=85.2292
	step [28/144], loss=71.7092
	step [29/144], loss=82.6848
	step [30/144], loss=90.5475
	step [31/144], loss=76.8901
	step [32/144], loss=79.9813
	step [33/144], loss=73.0300
	step [34/144], loss=71.6054
	step [35/144], loss=78.2818
	step [36/144], loss=75.5775
	step [37/144], loss=63.0043
	step [38/144], loss=71.2949
	step [39/144], loss=75.9860
	step [40/144], loss=83.8275
	step [41/144], loss=74.8591
	step [42/144], loss=71.3527
	step [43/144], loss=92.3487
	step [44/144], loss=78.4847
	step [45/144], loss=74.6064
	step [46/144], loss=78.3254
	step [47/144], loss=79.9239
	step [48/144], loss=100.1125
	step [49/144], loss=93.6347
	step [50/144], loss=91.4570
	step [51/144], loss=88.6246
	step [52/144], loss=75.9145
	step [53/144], loss=79.7678
	step [54/144], loss=71.7515
	step [55/144], loss=93.7601
	step [56/144], loss=56.5650
	step [57/144], loss=84.3468
	step [58/144], loss=74.6213
	step [59/144], loss=79.7790
	step [60/144], loss=71.3820
	step [61/144], loss=78.6286
	step [62/144], loss=76.0288
	step [63/144], loss=83.0728
	step [64/144], loss=90.9114
	step [65/144], loss=72.2274
	step [66/144], loss=90.9322
	step [67/144], loss=66.7580
	step [68/144], loss=83.8305
	step [69/144], loss=79.5731
	step [70/144], loss=71.9466
	step [71/144], loss=87.7290
	step [72/144], loss=86.5422
	step [73/144], loss=100.8456
	step [74/144], loss=70.1819
	step [75/144], loss=90.3346
	step [76/144], loss=81.6745
	step [77/144], loss=87.1452
	step [78/144], loss=71.9565
	step [79/144], loss=91.8125
	step [80/144], loss=86.8138
	step [81/144], loss=85.0269
	step [82/144], loss=81.0116
	step [83/144], loss=74.5456
	step [84/144], loss=90.8849
	step [85/144], loss=85.8589
	step [86/144], loss=84.1389
	step [87/144], loss=87.6622
	step [88/144], loss=87.8513
	step [89/144], loss=70.2228
	step [90/144], loss=63.8523
	step [91/144], loss=84.0854
	step [92/144], loss=83.5684
	step [93/144], loss=72.4212
	step [94/144], loss=84.4142
	step [95/144], loss=71.2211
	step [96/144], loss=89.5440
	step [97/144], loss=80.1313
	step [98/144], loss=77.5194
	step [99/144], loss=76.7150
	step [100/144], loss=79.5251
	step [101/144], loss=68.5995
	step [102/144], loss=86.7623
	step [103/144], loss=84.9100
	step [104/144], loss=78.9061
	step [105/144], loss=76.1727
	step [106/144], loss=71.6677
	step [107/144], loss=67.0515
	step [108/144], loss=71.4948
	step [109/144], loss=76.0646
	step [110/144], loss=78.9698
	step [111/144], loss=70.6176
	step [112/144], loss=82.4916
	step [113/144], loss=66.5487
	step [114/144], loss=84.5520
	step [115/144], loss=77.2184
	step [116/144], loss=81.8576
	step [117/144], loss=79.8954
	step [118/144], loss=87.5350
	step [119/144], loss=84.5070
	step [120/144], loss=72.5799
	step [121/144], loss=105.9455
	step [122/144], loss=70.9429
	step [123/144], loss=74.7302
	step [124/144], loss=82.5374
	step [125/144], loss=89.5032
	step [126/144], loss=68.4112
	step [127/144], loss=85.0440
	step [128/144], loss=79.2490
	step [129/144], loss=70.3088
	step [130/144], loss=68.0079
	step [131/144], loss=86.6194
	step [132/144], loss=81.4234
	step [133/144], loss=72.3033
	step [134/144], loss=81.4123
	step [135/144], loss=82.0196
	step [136/144], loss=74.9289
	step [137/144], loss=78.2741
	step [138/144], loss=84.0675
	step [139/144], loss=76.5277
	step [140/144], loss=93.0366
	step [141/144], loss=97.7038
	step [142/144], loss=89.4603
	step [143/144], loss=76.4007
	step [144/144], loss=31.2071
	Evaluating
	loss=0.0111, precision=0.4018, recall=0.8333, f1=0.5422
Training epoch 85
	step [1/144], loss=82.2719
	step [2/144], loss=74.0457
	step [3/144], loss=89.2214
	step [4/144], loss=72.5798
	step [5/144], loss=78.8010
	step [6/144], loss=95.8825
	step [7/144], loss=79.2890
	step [8/144], loss=82.2619
	step [9/144], loss=91.1833
	step [10/144], loss=81.8351
	step [11/144], loss=78.4832
	step [12/144], loss=83.7057
	step [13/144], loss=87.0819
	step [14/144], loss=73.6215
	step [15/144], loss=81.1381
	step [16/144], loss=71.3555
	step [17/144], loss=75.4254
	step [18/144], loss=77.0029
	step [19/144], loss=105.6846
	step [20/144], loss=75.8291
	step [21/144], loss=79.6359
	step [22/144], loss=88.2425
	step [23/144], loss=86.2527
	step [24/144], loss=73.3396
	step [25/144], loss=74.2465
	step [26/144], loss=76.8972
	step [27/144], loss=67.9683
	step [28/144], loss=74.4514
	step [29/144], loss=86.1731
	step [30/144], loss=84.8445
	step [31/144], loss=104.8185
	step [32/144], loss=75.8677
	step [33/144], loss=75.2003
	step [34/144], loss=72.1970
	step [35/144], loss=75.1507
	step [36/144], loss=74.1098
	step [37/144], loss=89.9467
	step [38/144], loss=86.2449
	step [39/144], loss=90.0612
	step [40/144], loss=72.3990
	step [41/144], loss=81.1707
	step [42/144], loss=57.5743
	step [43/144], loss=72.0095
	step [44/144], loss=74.1595
	step [45/144], loss=84.7838
	step [46/144], loss=77.4857
	step [47/144], loss=86.5641
	step [48/144], loss=75.9534
	step [49/144], loss=79.4597
	step [50/144], loss=94.5290
	step [51/144], loss=67.4599
	step [52/144], loss=66.7919
	step [53/144], loss=69.1346
	step [54/144], loss=82.3345
	step [55/144], loss=81.8626
	step [56/144], loss=63.8897
	step [57/144], loss=80.7248
	step [58/144], loss=82.4441
	step [59/144], loss=73.1425
	step [60/144], loss=89.9754
	step [61/144], loss=63.0028
	step [62/144], loss=77.7944
	step [63/144], loss=83.3173
	step [64/144], loss=69.6884
	step [65/144], loss=98.7725
	step [66/144], loss=85.2621
	step [67/144], loss=73.3829
	step [68/144], loss=98.4137
	step [69/144], loss=74.0478
	step [70/144], loss=69.9219
	step [71/144], loss=88.6261
	step [72/144], loss=67.1512
	step [73/144], loss=65.8038
	step [74/144], loss=104.7993
	step [75/144], loss=81.9825
	step [76/144], loss=76.0337
	step [77/144], loss=78.0156
	step [78/144], loss=75.7442
	step [79/144], loss=85.0264
	step [80/144], loss=90.1122
	step [81/144], loss=77.5730
	step [82/144], loss=85.2096
	step [83/144], loss=75.2977
	step [84/144], loss=63.9679
	step [85/144], loss=77.4430
	step [86/144], loss=81.5776
	step [87/144], loss=74.2070
	step [88/144], loss=103.0598
	step [89/144], loss=90.6696
	step [90/144], loss=79.2665
	step [91/144], loss=87.8414
	step [92/144], loss=73.8301
	step [93/144], loss=78.5542
	step [94/144], loss=84.5868
	step [95/144], loss=83.5587
	step [96/144], loss=86.0932
	step [97/144], loss=65.5001
	step [98/144], loss=81.4323
	step [99/144], loss=95.5493
	step [100/144], loss=77.2593
	step [101/144], loss=76.1620
	step [102/144], loss=64.6809
	step [103/144], loss=81.4039
	step [104/144], loss=76.4971
	step [105/144], loss=70.9508
	step [106/144], loss=75.9406
	step [107/144], loss=68.8467
	step [108/144], loss=87.4393
	step [109/144], loss=72.9835
	step [110/144], loss=82.7781
	step [111/144], loss=87.2975
	step [112/144], loss=81.3298
	step [113/144], loss=86.1808
	step [114/144], loss=85.4971
	step [115/144], loss=72.5896
	step [116/144], loss=78.6968
	step [117/144], loss=90.4825
	step [118/144], loss=84.6143
	step [119/144], loss=81.2509
	step [120/144], loss=71.2887
	step [121/144], loss=78.4538
	step [122/144], loss=86.9694
	step [123/144], loss=86.4825
	step [124/144], loss=71.9091
	step [125/144], loss=83.7232
	step [126/144], loss=73.3637
	step [127/144], loss=70.5099
	step [128/144], loss=82.1316
	step [129/144], loss=80.3659
	step [130/144], loss=76.8533
	step [131/144], loss=75.1737
	step [132/144], loss=87.4006
	step [133/144], loss=73.7139
	step [134/144], loss=78.7879
	step [135/144], loss=89.8183
	step [136/144], loss=68.3771
	step [137/144], loss=80.3376
	step [138/144], loss=84.9682
	step [139/144], loss=93.6820
	step [140/144], loss=84.4404
	step [141/144], loss=77.6091
	step [142/144], loss=72.5750
	step [143/144], loss=60.0562
	step [144/144], loss=27.6157
	Evaluating
	loss=0.0089, precision=0.4807, recall=0.8282, f1=0.6083
saving model as: 1_saved_model.pth
Training epoch 86
	step [1/144], loss=74.9527
	step [2/144], loss=75.2460
	step [3/144], loss=83.9687
	step [4/144], loss=67.7811
	step [5/144], loss=81.5929
	step [6/144], loss=77.1633
	step [7/144], loss=75.4877
	step [8/144], loss=87.8581
	step [9/144], loss=77.2253
	step [10/144], loss=84.8162
	step [11/144], loss=86.5700
	step [12/144], loss=83.7026
	step [13/144], loss=82.1882
	step [14/144], loss=86.2054
	step [15/144], loss=68.6676
	step [16/144], loss=75.8611
	step [17/144], loss=67.8764
	step [18/144], loss=74.5330
	step [19/144], loss=70.4184
	step [20/144], loss=59.0008
	step [21/144], loss=86.6007
	step [22/144], loss=71.3755
	step [23/144], loss=72.1811
	step [24/144], loss=93.4571
	step [25/144], loss=65.3973
	step [26/144], loss=77.3128
	step [27/144], loss=85.3139
	step [28/144], loss=75.9042
	step [29/144], loss=69.1303
	step [30/144], loss=84.7083
	step [31/144], loss=82.8078
	step [32/144], loss=83.2973
	step [33/144], loss=70.6613
	step [34/144], loss=70.8637
	step [35/144], loss=95.9808
	step [36/144], loss=81.3714
	step [37/144], loss=73.2095
	step [38/144], loss=74.0344
	step [39/144], loss=78.6008
	step [40/144], loss=75.6660
	step [41/144], loss=73.7069
	step [42/144], loss=58.0087
	step [43/144], loss=81.1899
	step [44/144], loss=89.1415
	step [45/144], loss=73.3339
	step [46/144], loss=80.9647
	step [47/144], loss=88.6611
	step [48/144], loss=70.6998
	step [49/144], loss=74.0509
	step [50/144], loss=76.4730
	step [51/144], loss=66.3428
	step [52/144], loss=76.1271
	step [53/144], loss=85.0544
	step [54/144], loss=87.9851
	step [55/144], loss=81.9336
	step [56/144], loss=66.4983
	step [57/144], loss=88.0127
	step [58/144], loss=80.5147
	step [59/144], loss=72.4034
	step [60/144], loss=81.9104
	step [61/144], loss=81.6221
	step [62/144], loss=75.1695
	step [63/144], loss=88.6811
	step [64/144], loss=82.4002
	step [65/144], loss=74.2935
	step [66/144], loss=80.2584
	step [67/144], loss=89.7226
	step [68/144], loss=84.8340
	step [69/144], loss=95.0289
	step [70/144], loss=96.3109
	step [71/144], loss=93.8053
	step [72/144], loss=89.4617
	step [73/144], loss=75.3701
	step [74/144], loss=75.2239
	step [75/144], loss=73.9326
	step [76/144], loss=82.0747
	step [77/144], loss=77.3195
	step [78/144], loss=74.4166
	step [79/144], loss=66.5309
	step [80/144], loss=76.6121
	step [81/144], loss=88.2356
	step [82/144], loss=73.7182
	step [83/144], loss=79.2294
	step [84/144], loss=89.9680
	step [85/144], loss=72.0645
	step [86/144], loss=69.8113
	step [87/144], loss=71.5998
	step [88/144], loss=80.9398
	step [89/144], loss=82.7242
	step [90/144], loss=75.1885
	step [91/144], loss=77.7451
	step [92/144], loss=76.8951
	step [93/144], loss=75.8204
	step [94/144], loss=71.5896
	step [95/144], loss=91.1754
	step [96/144], loss=83.1289
	step [97/144], loss=89.9344
	step [98/144], loss=75.6328
	step [99/144], loss=79.9518
	step [100/144], loss=93.1762
	step [101/144], loss=84.2994
	step [102/144], loss=88.8634
	step [103/144], loss=90.1675
	step [104/144], loss=80.8014
	step [105/144], loss=84.1504
	step [106/144], loss=87.9417
	step [107/144], loss=83.4059
	step [108/144], loss=75.0618
	step [109/144], loss=78.7230
	step [110/144], loss=57.3425
	step [111/144], loss=90.5676
	step [112/144], loss=77.7277
	step [113/144], loss=97.0625
	step [114/144], loss=87.0972
	step [115/144], loss=67.6718
	step [116/144], loss=69.5184
	step [117/144], loss=77.3175
	step [118/144], loss=84.6646
	step [119/144], loss=81.3974
	step [120/144], loss=60.8819
	step [121/144], loss=71.5317
	step [122/144], loss=88.9448
	step [123/144], loss=77.0317
	step [124/144], loss=72.5492
	step [125/144], loss=83.2026
	step [126/144], loss=83.8036
	step [127/144], loss=77.0928
	step [128/144], loss=77.9454
	step [129/144], loss=69.6285
	step [130/144], loss=81.4617
	step [131/144], loss=76.9326
	step [132/144], loss=78.2820
	step [133/144], loss=81.8343
	step [134/144], loss=84.3813
	step [135/144], loss=72.3222
	step [136/144], loss=102.8565
	step [137/144], loss=101.4251
	step [138/144], loss=70.6522
	step [139/144], loss=71.2408
	step [140/144], loss=77.0574
	step [141/144], loss=69.4835
	step [142/144], loss=76.6962
	step [143/144], loss=88.6388
	step [144/144], loss=34.9039
	Evaluating
	loss=0.0105, precision=0.4125, recall=0.8457, f1=0.5546
Training epoch 87
	step [1/144], loss=81.1982
	step [2/144], loss=85.7897
	step [3/144], loss=90.1247
	step [4/144], loss=69.1697
	step [5/144], loss=73.7072
	step [6/144], loss=82.3652
	step [7/144], loss=79.4005
	step [8/144], loss=104.0683
	step [9/144], loss=75.2651
	step [10/144], loss=77.1648
	step [11/144], loss=88.9986
	step [12/144], loss=101.1586
	step [13/144], loss=92.5810
	step [14/144], loss=74.9501
	step [15/144], loss=84.6526
	step [16/144], loss=83.3919
	step [17/144], loss=72.4136
	step [18/144], loss=95.1051
	step [19/144], loss=67.6178
	step [20/144], loss=87.1981
	step [21/144], loss=88.4567
	step [22/144], loss=80.6666
	step [23/144], loss=71.5763
	step [24/144], loss=86.0545
	step [25/144], loss=96.3057
	step [26/144], loss=76.7322
	step [27/144], loss=73.0619
	step [28/144], loss=67.7348
	step [29/144], loss=84.9612
	step [30/144], loss=70.7850
	step [31/144], loss=70.5947
	step [32/144], loss=80.6586
	step [33/144], loss=78.3352
	step [34/144], loss=91.9628
	step [35/144], loss=75.6005
	step [36/144], loss=78.6294
	step [37/144], loss=74.6046
	step [38/144], loss=94.7455
	step [39/144], loss=75.7257
	step [40/144], loss=78.2594
	step [41/144], loss=73.7173
	step [42/144], loss=66.3458
	step [43/144], loss=75.4724
	step [44/144], loss=75.1140
	step [45/144], loss=74.0736
	step [46/144], loss=61.0965
	step [47/144], loss=85.1721
	step [48/144], loss=83.4023
	step [49/144], loss=100.3287
	step [50/144], loss=69.7188
	step [51/144], loss=72.6652
	step [52/144], loss=73.8259
	step [53/144], loss=72.0094
	step [54/144], loss=81.3857
	step [55/144], loss=63.5753
	step [56/144], loss=71.3560
	step [57/144], loss=82.1721
	step [58/144], loss=88.6858
	step [59/144], loss=91.9007
	step [60/144], loss=70.3132
	step [61/144], loss=82.3565
	step [62/144], loss=88.0176
	step [63/144], loss=61.6692
	step [64/144], loss=73.6254
	step [65/144], loss=70.3637
	step [66/144], loss=89.4964
	step [67/144], loss=76.2233
	step [68/144], loss=90.2952
	step [69/144], loss=96.1052
	step [70/144], loss=94.7823
	step [71/144], loss=67.6270
	step [72/144], loss=91.3576
	step [73/144], loss=65.6874
	step [74/144], loss=83.6725
	step [75/144], loss=87.4969
	step [76/144], loss=69.6742
	step [77/144], loss=68.9596
	step [78/144], loss=92.2253
	step [79/144], loss=71.2771
	step [80/144], loss=77.2377
	step [81/144], loss=69.1481
	step [82/144], loss=80.5010
	step [83/144], loss=80.4581
	step [84/144], loss=84.3869
	step [85/144], loss=82.0628
	step [86/144], loss=84.4417
	step [87/144], loss=65.2656
	step [88/144], loss=81.9210
	step [89/144], loss=72.9194
	step [90/144], loss=73.2697
	step [91/144], loss=74.1146
	step [92/144], loss=91.9547
	step [93/144], loss=90.9907
	step [94/144], loss=78.0310
	step [95/144], loss=67.4976
	step [96/144], loss=83.7138
	step [97/144], loss=89.3884
	step [98/144], loss=81.0723
	step [99/144], loss=65.5605
	step [100/144], loss=64.0072
	step [101/144], loss=87.9109
	step [102/144], loss=75.6632
	step [103/144], loss=78.7394
	step [104/144], loss=82.4190
	step [105/144], loss=68.1680
	step [106/144], loss=80.0955
	step [107/144], loss=87.5764
	step [108/144], loss=77.1084
	step [109/144], loss=72.3174
	step [110/144], loss=82.0355
	step [111/144], loss=74.9346
	step [112/144], loss=66.6509
	step [113/144], loss=72.6338
	step [114/144], loss=73.3429
	step [115/144], loss=98.1354
	step [116/144], loss=71.9131
	step [117/144], loss=70.8250
	step [118/144], loss=89.0443
	step [119/144], loss=80.7560
	step [120/144], loss=71.2419
	step [121/144], loss=63.4558
	step [122/144], loss=82.4394
	step [123/144], loss=93.8346
	step [124/144], loss=82.4315
	step [125/144], loss=84.7952
	step [126/144], loss=69.0596
	step [127/144], loss=70.7886
	step [128/144], loss=75.1792
	step [129/144], loss=77.5072
	step [130/144], loss=65.5899
	step [131/144], loss=86.0743
	step [132/144], loss=77.5149
	step [133/144], loss=90.7711
	step [134/144], loss=83.4329
	step [135/144], loss=81.9369
	step [136/144], loss=81.4587
	step [137/144], loss=73.9638
	step [138/144], loss=68.9143
	step [139/144], loss=67.6488
	step [140/144], loss=81.4116
	step [141/144], loss=91.7408
	step [142/144], loss=81.6529
	step [143/144], loss=73.4978
	step [144/144], loss=26.8516
	Evaluating
	loss=0.0118, precision=0.3745, recall=0.8524, f1=0.5204
Training epoch 88
	step [1/144], loss=81.9990
	step [2/144], loss=76.9380
	step [3/144], loss=92.4241
	step [4/144], loss=95.9331
	step [5/144], loss=77.7522
	step [6/144], loss=79.3754
	step [7/144], loss=89.8895
	step [8/144], loss=78.0844
	step [9/144], loss=85.3918
	step [10/144], loss=64.9630
	step [11/144], loss=88.0641
	step [12/144], loss=88.2256
	step [13/144], loss=82.3082
	step [14/144], loss=89.2910
	step [15/144], loss=74.0237
	step [16/144], loss=81.8446
	step [17/144], loss=87.1102
	step [18/144], loss=65.5181
	step [19/144], loss=74.9993
	step [20/144], loss=79.7902
	step [21/144], loss=82.5427
	step [22/144], loss=85.8391
	step [23/144], loss=80.3525
	step [24/144], loss=73.6220
	step [25/144], loss=71.9477
	step [26/144], loss=80.5315
	step [27/144], loss=77.1926
	step [28/144], loss=80.6373
	step [29/144], loss=84.3415
	step [30/144], loss=76.2414
	step [31/144], loss=80.6547
	step [32/144], loss=83.0682
	step [33/144], loss=79.9766
	step [34/144], loss=81.5424
	step [35/144], loss=78.6472
	step [36/144], loss=72.3017
	step [37/144], loss=72.3333
	step [38/144], loss=71.6848
	step [39/144], loss=85.9859
	step [40/144], loss=71.5599
	step [41/144], loss=80.0800
	step [42/144], loss=71.8205
	step [43/144], loss=82.8645
	step [44/144], loss=78.8191
	step [45/144], loss=76.0843
	step [46/144], loss=67.6860
	step [47/144], loss=75.5807
	step [48/144], loss=85.3006
	step [49/144], loss=81.7301
	step [50/144], loss=82.2969
	step [51/144], loss=74.3303
	step [52/144], loss=78.0868
	step [53/144], loss=85.4472
	step [54/144], loss=74.1758
	step [55/144], loss=79.2843
	step [56/144], loss=93.0276
	step [57/144], loss=83.3133
	step [58/144], loss=81.2542
	step [59/144], loss=77.9283
	step [60/144], loss=70.8106
	step [61/144], loss=72.8529
	step [62/144], loss=83.4077
	step [63/144], loss=82.4517
	step [64/144], loss=85.6793
	step [65/144], loss=73.4530
	step [66/144], loss=73.9549
	step [67/144], loss=81.5109
	step [68/144], loss=86.4207
	step [69/144], loss=78.2785
	step [70/144], loss=86.9443
	step [71/144], loss=71.7707
	step [72/144], loss=73.8322
	step [73/144], loss=70.5740
	step [74/144], loss=60.9858
	step [75/144], loss=59.6183
	step [76/144], loss=78.4200
	step [77/144], loss=93.7786
	step [78/144], loss=96.5454
	step [79/144], loss=82.8750
	step [80/144], loss=75.8278
	step [81/144], loss=83.0599
	step [82/144], loss=85.0484
	step [83/144], loss=82.5357
	step [84/144], loss=62.1827
	step [85/144], loss=75.4770
	step [86/144], loss=77.7826
	step [87/144], loss=73.8389
	step [88/144], loss=80.2246
	step [89/144], loss=83.5924
	step [90/144], loss=88.0368
	step [91/144], loss=85.6228
	step [92/144], loss=64.8962
	step [93/144], loss=82.1051
	step [94/144], loss=89.3486
	step [95/144], loss=73.1402
	step [96/144], loss=82.3134
	step [97/144], loss=105.8972
	step [98/144], loss=77.9874
	step [99/144], loss=67.5191
	step [100/144], loss=89.4753
	step [101/144], loss=67.6153
	step [102/144], loss=85.9698
	step [103/144], loss=75.6392
	step [104/144], loss=81.0170
	step [105/144], loss=84.3301
	step [106/144], loss=87.4361
	step [107/144], loss=70.0908
	step [108/144], loss=72.9880
	step [109/144], loss=89.7427
	step [110/144], loss=75.5180
	step [111/144], loss=82.0286
	step [112/144], loss=83.6148
	step [113/144], loss=70.4855
	step [114/144], loss=81.4313
	step [115/144], loss=88.4390
	step [116/144], loss=78.8042
	step [117/144], loss=83.1605
	step [118/144], loss=69.4243
	step [119/144], loss=77.2803
	step [120/144], loss=61.2906
	step [121/144], loss=74.5146
	step [122/144], loss=89.1400
	step [123/144], loss=88.6374
	step [124/144], loss=78.1073
	step [125/144], loss=86.1863
	step [126/144], loss=86.3689
	step [127/144], loss=90.6262
	step [128/144], loss=70.8719
	step [129/144], loss=78.5869
	step [130/144], loss=76.4072
	step [131/144], loss=87.4652
	step [132/144], loss=79.3299
	step [133/144], loss=64.6432
	step [134/144], loss=87.9040
	step [135/144], loss=73.3352
	step [136/144], loss=68.2711
	step [137/144], loss=74.8859
	step [138/144], loss=71.4061
	step [139/144], loss=74.5089
	step [140/144], loss=75.1292
	step [141/144], loss=76.6384
	step [142/144], loss=75.5651
	step [143/144], loss=64.3090
	step [144/144], loss=39.0762
	Evaluating
	loss=0.0120, precision=0.3617, recall=0.8527, f1=0.5079
Training epoch 89
	step [1/144], loss=71.1792
	step [2/144], loss=75.3461
	step [3/144], loss=74.1558
	step [4/144], loss=65.6697
	step [5/144], loss=71.0682
	step [6/144], loss=69.8395
	step [7/144], loss=79.0638
	step [8/144], loss=76.9861
	step [9/144], loss=75.9261
	step [10/144], loss=82.0060
	step [11/144], loss=69.3105
	step [12/144], loss=83.6624
	step [13/144], loss=70.5340
	step [14/144], loss=74.7350
	step [15/144], loss=82.6562
	step [16/144], loss=74.7777
	step [17/144], loss=76.2943
	step [18/144], loss=71.7485
	step [19/144], loss=70.0911
	step [20/144], loss=78.3592
	step [21/144], loss=81.2091
	step [22/144], loss=63.1196
	step [23/144], loss=89.1207
	step [24/144], loss=76.6594
	step [25/144], loss=78.9867
	step [26/144], loss=71.5463
	step [27/144], loss=73.8157
	step [28/144], loss=65.5773
	step [29/144], loss=76.4461
	step [30/144], loss=70.3348
	step [31/144], loss=83.7373
	step [32/144], loss=88.2488
	step [33/144], loss=89.7463
	step [34/144], loss=80.2450
	step [35/144], loss=80.5877
	step [36/144], loss=95.2427
	step [37/144], loss=70.6197
	step [38/144], loss=85.7566
	step [39/144], loss=79.4090
	step [40/144], loss=65.1656
	step [41/144], loss=88.6096
	step [42/144], loss=77.2129
	step [43/144], loss=70.3512
	step [44/144], loss=68.8213
	step [45/144], loss=85.6700
	step [46/144], loss=78.2224
	step [47/144], loss=63.5740
	step [48/144], loss=73.7098
	step [49/144], loss=75.2049
	step [50/144], loss=81.5881
	step [51/144], loss=70.9455
	step [52/144], loss=86.2839
	step [53/144], loss=72.2305
	step [54/144], loss=84.8083
	step [55/144], loss=85.6474
	step [56/144], loss=77.5643
	step [57/144], loss=89.5841
	step [58/144], loss=79.9622
	step [59/144], loss=76.4880
	step [60/144], loss=85.2403
	step [61/144], loss=73.8047
	step [62/144], loss=86.9493
	step [63/144], loss=73.6760
	step [64/144], loss=100.2188
	step [65/144], loss=76.2549
	step [66/144], loss=82.1590
	step [67/144], loss=84.2553
	step [68/144], loss=78.8620
	step [69/144], loss=71.6353
	step [70/144], loss=88.8216
	step [71/144], loss=96.0143
	step [72/144], loss=89.3877
	step [73/144], loss=77.4284
	step [74/144], loss=73.7555
	step [75/144], loss=74.4809
	step [76/144], loss=90.5607
	step [77/144], loss=72.7020
	step [78/144], loss=75.5885
	step [79/144], loss=87.7737
	step [80/144], loss=89.8171
	step [81/144], loss=73.3372
	step [82/144], loss=83.6062
	step [83/144], loss=78.6521
	step [84/144], loss=85.7726
	step [85/144], loss=103.5680
	step [86/144], loss=75.1852
	step [87/144], loss=81.0692
	step [88/144], loss=82.5033
	step [89/144], loss=82.5887
	step [90/144], loss=95.7226
	step [91/144], loss=72.8866
	step [92/144], loss=77.3220
	step [93/144], loss=91.7110
	step [94/144], loss=74.2807
	step [95/144], loss=53.3157
	step [96/144], loss=70.3295
	step [97/144], loss=83.2734
	step [98/144], loss=75.7456
	step [99/144], loss=72.6355
	step [100/144], loss=78.9597
	step [101/144], loss=87.6455
	step [102/144], loss=81.5698
	step [103/144], loss=72.2091
	step [104/144], loss=82.7697
	step [105/144], loss=85.2872
	step [106/144], loss=92.6769
	step [107/144], loss=73.1317
	step [108/144], loss=67.0742
	step [109/144], loss=71.7806
	step [110/144], loss=82.1497
	step [111/144], loss=88.6618
	step [112/144], loss=76.3848
	step [113/144], loss=88.8771
	step [114/144], loss=78.8003
	step [115/144], loss=70.8581
	step [116/144], loss=77.6430
	step [117/144], loss=74.1132
	step [118/144], loss=78.8749
	step [119/144], loss=82.5420
	step [120/144], loss=80.0692
	step [121/144], loss=84.4281
	step [122/144], loss=76.6779
	step [123/144], loss=81.3366
	step [124/144], loss=94.5583
	step [125/144], loss=92.1488
	step [126/144], loss=80.0746
	step [127/144], loss=73.6220
	step [128/144], loss=78.8190
	step [129/144], loss=85.1800
	step [130/144], loss=75.6197
	step [131/144], loss=69.1140
	step [132/144], loss=84.4557
	step [133/144], loss=86.6401
	step [134/144], loss=79.8018
	step [135/144], loss=79.1293
	step [136/144], loss=70.6046
	step [137/144], loss=68.6095
	step [138/144], loss=80.3028
	step [139/144], loss=80.8901
	step [140/144], loss=70.3678
	step [141/144], loss=72.4815
	step [142/144], loss=69.2925
	step [143/144], loss=74.1707
	step [144/144], loss=32.8487
	Evaluating
	loss=0.0099, precision=0.4383, recall=0.8384, f1=0.5757
Training epoch 90
	step [1/144], loss=76.2983
	step [2/144], loss=78.6758
	step [3/144], loss=70.1044
	step [4/144], loss=77.8617
	step [5/144], loss=75.2821
	step [6/144], loss=72.8986
	step [7/144], loss=78.9994
	step [8/144], loss=79.5092
	step [9/144], loss=73.3169
	step [10/144], loss=66.1148
	step [11/144], loss=82.8555
	step [12/144], loss=71.0821
	step [13/144], loss=80.1329
	step [14/144], loss=84.6406
	step [15/144], loss=62.6549
	step [16/144], loss=70.7325
	step [17/144], loss=80.2230
	step [18/144], loss=86.2446
	step [19/144], loss=76.1444
	step [20/144], loss=85.7434
	step [21/144], loss=78.1510
	step [22/144], loss=77.6449
	step [23/144], loss=69.0645
	step [24/144], loss=80.4369
	step [25/144], loss=78.3971
	step [26/144], loss=92.6226
	step [27/144], loss=75.8383
	step [28/144], loss=81.8386
	step [29/144], loss=87.4779
	step [30/144], loss=63.6129
	step [31/144], loss=65.0164
	step [32/144], loss=97.0724
	step [33/144], loss=76.6959
	step [34/144], loss=65.9434
	step [35/144], loss=79.7643
	step [36/144], loss=84.8310
	step [37/144], loss=78.4310
	step [38/144], loss=70.2647
	step [39/144], loss=69.8788
	step [40/144], loss=90.0751
	step [41/144], loss=82.4240
	step [42/144], loss=72.7173
	step [43/144], loss=67.8969
	step [44/144], loss=75.6028
	step [45/144], loss=91.3565
	step [46/144], loss=80.3807
	step [47/144], loss=71.1646
	step [48/144], loss=84.1798
	step [49/144], loss=76.0066
	step [50/144], loss=77.0229
	step [51/144], loss=76.4314
	step [52/144], loss=68.6405
	step [53/144], loss=76.2951
	step [54/144], loss=76.7524
	step [55/144], loss=85.5280
	step [56/144], loss=73.3549
	step [57/144], loss=74.2128
	step [58/144], loss=72.1629
	step [59/144], loss=69.7147
	step [60/144], loss=84.1371
	step [61/144], loss=76.3865
	step [62/144], loss=84.8685
	step [63/144], loss=88.2968
	step [64/144], loss=85.7116
	step [65/144], loss=87.7395
	step [66/144], loss=79.5534
	step [67/144], loss=74.1060
	step [68/144], loss=76.8549
	step [69/144], loss=92.2451
	step [70/144], loss=86.9289
	step [71/144], loss=77.5545
	step [72/144], loss=83.2116
	step [73/144], loss=86.9525
	step [74/144], loss=83.1850
	step [75/144], loss=83.3487
	step [76/144], loss=87.1609
	step [77/144], loss=80.7253
	step [78/144], loss=76.0291
	step [79/144], loss=91.7995
	step [80/144], loss=64.0752
	step [81/144], loss=90.3670
	step [82/144], loss=62.1371
	step [83/144], loss=77.8658
	step [84/144], loss=84.8730
	step [85/144], loss=86.7555
	step [86/144], loss=83.8219
	step [87/144], loss=78.0247
	step [88/144], loss=71.9803
	step [89/144], loss=75.9191
	step [90/144], loss=74.3140
	step [91/144], loss=81.1115
	step [92/144], loss=74.8023
	step [93/144], loss=73.7123
	step [94/144], loss=77.2019
	step [95/144], loss=77.0295
	step [96/144], loss=72.9783
	step [97/144], loss=68.8320
	step [98/144], loss=85.2555
	step [99/144], loss=77.6294
	step [100/144], loss=64.9008
	step [101/144], loss=81.7528
	step [102/144], loss=90.2370
	step [103/144], loss=81.9009
	step [104/144], loss=71.1504
	step [105/144], loss=59.5313
	step [106/144], loss=86.5699
	step [107/144], loss=62.8858
	step [108/144], loss=81.8938
	step [109/144], loss=92.9267
	step [110/144], loss=77.9151
	step [111/144], loss=69.8569
	step [112/144], loss=70.2180
	step [113/144], loss=91.2659
	step [114/144], loss=71.7087
	step [115/144], loss=84.9443
	step [116/144], loss=87.9031
	step [117/144], loss=90.0873
	step [118/144], loss=90.5586
	step [119/144], loss=79.4250
	step [120/144], loss=83.2649
	step [121/144], loss=70.4958
	step [122/144], loss=80.8171
	step [123/144], loss=75.3060
	step [124/144], loss=83.6440
	step [125/144], loss=80.4623
	step [126/144], loss=79.3489
	step [127/144], loss=65.4032
	step [128/144], loss=79.8299
	step [129/144], loss=71.2243
	step [130/144], loss=72.4500
	step [131/144], loss=86.3275
	step [132/144], loss=74.1835
	step [133/144], loss=64.6458
	step [134/144], loss=77.6400
	step [135/144], loss=78.3375
	step [136/144], loss=83.8737
	step [137/144], loss=94.9711
	step [138/144], loss=87.4147
	step [139/144], loss=80.0258
	step [140/144], loss=91.7633
	step [141/144], loss=63.8406
	step [142/144], loss=85.1075
	step [143/144], loss=86.0054
	step [144/144], loss=27.3025
	Evaluating
	loss=0.0109, precision=0.3954, recall=0.8480, f1=0.5393
Training epoch 91
	step [1/144], loss=72.6383
	step [2/144], loss=73.8676
	step [3/144], loss=74.0090
	step [4/144], loss=80.8987
	step [5/144], loss=73.3301
	step [6/144], loss=76.2353
	step [7/144], loss=82.8962
	step [8/144], loss=66.9079
	step [9/144], loss=70.8311
	step [10/144], loss=73.3319
	step [11/144], loss=68.8630
	step [12/144], loss=80.2020
	step [13/144], loss=83.4481
	step [14/144], loss=78.2426
	step [15/144], loss=68.7214
	step [16/144], loss=60.6897
	step [17/144], loss=73.0537
	step [18/144], loss=90.0957
	step [19/144], loss=80.1345
	step [20/144], loss=74.3053
	step [21/144], loss=89.3377
	step [22/144], loss=85.2220
	step [23/144], loss=90.8581
	step [24/144], loss=86.0959
	step [25/144], loss=72.0633
	step [26/144], loss=91.0733
	step [27/144], loss=76.6488
	step [28/144], loss=72.3709
	step [29/144], loss=72.3189
	step [30/144], loss=79.1496
	step [31/144], loss=75.6169
	step [32/144], loss=56.9713
	step [33/144], loss=89.2094
	step [34/144], loss=70.4336
	step [35/144], loss=78.7970
	step [36/144], loss=82.2060
	step [37/144], loss=68.8678
	step [38/144], loss=70.4176
	step [39/144], loss=78.2358
	step [40/144], loss=71.9241
	step [41/144], loss=70.0850
	step [42/144], loss=60.9878
	step [43/144], loss=67.1788
	step [44/144], loss=85.5674
	step [45/144], loss=75.3218
	step [46/144], loss=82.3046
	step [47/144], loss=83.7610
	step [48/144], loss=78.8272
	step [49/144], loss=65.6263
	step [50/144], loss=80.2783
	step [51/144], loss=85.9156
	step [52/144], loss=80.6817
	step [53/144], loss=73.1436
	step [54/144], loss=103.3187
	step [55/144], loss=70.5080
	step [56/144], loss=64.1634
	step [57/144], loss=74.2848
	step [58/144], loss=69.2281
	step [59/144], loss=86.5242
	step [60/144], loss=69.8450
	step [61/144], loss=68.6370
	step [62/144], loss=94.4320
	step [63/144], loss=73.9347
	step [64/144], loss=75.8159
	step [65/144], loss=84.1210
	step [66/144], loss=76.0177
	step [67/144], loss=78.8016
	step [68/144], loss=73.9296
	step [69/144], loss=82.6871
	step [70/144], loss=69.5318
	step [71/144], loss=69.2557
	step [72/144], loss=84.1415
	step [73/144], loss=67.5562
	step [74/144], loss=73.7808
	step [75/144], loss=76.8116
	step [76/144], loss=82.4004
	step [77/144], loss=81.5399
	step [78/144], loss=84.7430
	step [79/144], loss=67.5340
	step [80/144], loss=83.9979
	step [81/144], loss=81.9419
	step [82/144], loss=74.5726
	step [83/144], loss=70.9220
	step [84/144], loss=84.8947
	step [85/144], loss=88.0960
	step [86/144], loss=71.6756
	step [87/144], loss=99.0733
	step [88/144], loss=70.5246
	step [89/144], loss=74.1231
	step [90/144], loss=64.7614
	step [91/144], loss=78.4432
	step [92/144], loss=78.0395
	step [93/144], loss=79.1574
	step [94/144], loss=82.9234
	step [95/144], loss=77.5795
	step [96/144], loss=79.0306
	step [97/144], loss=80.8906
	step [98/144], loss=85.6562
	step [99/144], loss=88.0292
	step [100/144], loss=86.4863
	step [101/144], loss=92.8623
	step [102/144], loss=74.9726
	step [103/144], loss=77.5973
	step [104/144], loss=80.9504
	step [105/144], loss=72.8234
	step [106/144], loss=89.5514
	step [107/144], loss=83.3601
	step [108/144], loss=82.7250
	step [109/144], loss=72.9755
	step [110/144], loss=89.3858
	step [111/144], loss=75.6807
	step [112/144], loss=79.8108
	step [113/144], loss=70.0295
	step [114/144], loss=69.7322
	step [115/144], loss=85.7877
	step [116/144], loss=81.8431
	step [117/144], loss=83.7664
	step [118/144], loss=70.0380
	step [119/144], loss=90.4589
	step [120/144], loss=70.8953
	step [121/144], loss=88.9311
	step [122/144], loss=63.7458
	step [123/144], loss=89.5916
	step [124/144], loss=65.4817
	step [125/144], loss=85.3032
	step [126/144], loss=81.1268
	step [127/144], loss=67.7670
	step [128/144], loss=71.7726
	step [129/144], loss=94.6175
	step [130/144], loss=92.1499
	step [131/144], loss=79.6522
	step [132/144], loss=84.1396
	step [133/144], loss=78.4246
	step [134/144], loss=86.8793
	step [135/144], loss=71.2740
	step [136/144], loss=67.4521
	step [137/144], loss=79.9702
	step [138/144], loss=79.9453
	step [139/144], loss=69.2684
	step [140/144], loss=87.9480
	step [141/144], loss=80.9963
	step [142/144], loss=94.8048
	step [143/144], loss=77.3153
	step [144/144], loss=33.6590
	Evaluating
	loss=0.0120, precision=0.3622, recall=0.8590, f1=0.5095
Training epoch 92
	step [1/144], loss=76.0061
	step [2/144], loss=67.5711
	step [3/144], loss=72.6170
	step [4/144], loss=73.4661
	step [5/144], loss=62.6838
	step [6/144], loss=73.1790
	step [7/144], loss=87.4383
	step [8/144], loss=71.6118
	step [9/144], loss=86.3977
	step [10/144], loss=76.5973
	step [11/144], loss=77.9306
	step [12/144], loss=85.1885
	step [13/144], loss=75.9354
	step [14/144], loss=89.7410
	step [15/144], loss=83.1652
	step [16/144], loss=75.2943
	step [17/144], loss=73.6898
	step [18/144], loss=78.2394
	step [19/144], loss=78.0769
	step [20/144], loss=80.7876
	step [21/144], loss=77.8380
	step [22/144], loss=80.0591
	step [23/144], loss=88.5956
	step [24/144], loss=81.4723
	step [25/144], loss=86.2910
	step [26/144], loss=80.4362
	step [27/144], loss=92.3351
	step [28/144], loss=71.7838
	step [29/144], loss=66.6370
	step [30/144], loss=69.1839
	step [31/144], loss=90.5616
	step [32/144], loss=87.5895
	step [33/144], loss=90.2718
	step [34/144], loss=78.7654
	step [35/144], loss=77.5493
	step [36/144], loss=86.0489
	step [37/144], loss=68.9276
	step [38/144], loss=71.2088
	step [39/144], loss=79.0484
	step [40/144], loss=69.2594
	step [41/144], loss=79.1712
	step [42/144], loss=66.9579
	step [43/144], loss=84.0950
	step [44/144], loss=81.2139
	step [45/144], loss=87.0629
	step [46/144], loss=68.4814
	step [47/144], loss=70.8991
	step [48/144], loss=83.7741
	step [49/144], loss=92.6963
	step [50/144], loss=82.6718
	step [51/144], loss=78.4474
	step [52/144], loss=99.5605
	step [53/144], loss=73.0809
	step [54/144], loss=75.9587
	step [55/144], loss=85.5380
	step [56/144], loss=70.5160
	step [57/144], loss=76.7271
	step [58/144], loss=66.1282
	step [59/144], loss=70.4974
	step [60/144], loss=85.9925
	step [61/144], loss=80.5777
	step [62/144], loss=58.4082
	step [63/144], loss=69.6492
	step [64/144], loss=73.4834
	step [65/144], loss=70.4245
	step [66/144], loss=77.9101
	step [67/144], loss=76.3763
	step [68/144], loss=86.2472
	step [69/144], loss=75.1764
	step [70/144], loss=80.3776
	step [71/144], loss=71.2667
	step [72/144], loss=77.7105
	step [73/144], loss=69.1716
	step [74/144], loss=87.9471
	step [75/144], loss=91.8629
	step [76/144], loss=79.5731
	step [77/144], loss=78.6481
	step [78/144], loss=80.2353
	step [79/144], loss=83.4036
	step [80/144], loss=78.1722
	step [81/144], loss=80.1515
	step [82/144], loss=69.5072
	step [83/144], loss=86.3160
	step [84/144], loss=74.9843
	step [85/144], loss=81.8669
	step [86/144], loss=88.7151
	step [87/144], loss=59.1998
	step [88/144], loss=75.2842
	step [89/144], loss=82.2029
	step [90/144], loss=88.4444
	step [91/144], loss=76.0308
	step [92/144], loss=69.3154
	step [93/144], loss=73.2760
	step [94/144], loss=94.6030
	step [95/144], loss=80.2698
	step [96/144], loss=91.2632
	step [97/144], loss=72.6908
	step [98/144], loss=88.2985
	step [99/144], loss=75.3855
	step [100/144], loss=79.5382
	step [101/144], loss=73.1798
	step [102/144], loss=67.9934
	step [103/144], loss=69.0029
	step [104/144], loss=63.3818
	step [105/144], loss=78.4417
	step [106/144], loss=73.5855
	step [107/144], loss=88.2335
	step [108/144], loss=93.7464
	step [109/144], loss=80.3074
	step [110/144], loss=88.3536
	step [111/144], loss=87.4674
	step [112/144], loss=94.5946
	step [113/144], loss=76.5764
	step [114/144], loss=78.4588
	step [115/144], loss=69.2835
	step [116/144], loss=72.6405
	step [117/144], loss=82.9520
	step [118/144], loss=82.3523
	step [119/144], loss=73.4618
	step [120/144], loss=76.8702
	step [121/144], loss=69.5124
	step [122/144], loss=85.2969
	step [123/144], loss=74.2848
	step [124/144], loss=81.0915
	step [125/144], loss=74.4184
	step [126/144], loss=79.6078
	step [127/144], loss=80.4321
	step [128/144], loss=79.1454
	step [129/144], loss=79.9675
	step [130/144], loss=57.8278
	step [131/144], loss=91.1782
	step [132/144], loss=80.7725
	step [133/144], loss=66.9208
	step [134/144], loss=78.0666
	step [135/144], loss=68.1400
	step [136/144], loss=71.7947
	step [137/144], loss=71.0385
	step [138/144], loss=77.4986
	step [139/144], loss=91.2278
	step [140/144], loss=78.7079
	step [141/144], loss=63.3466
	step [142/144], loss=80.5178
	step [143/144], loss=66.2169
	step [144/144], loss=26.7486
	Evaluating
	loss=0.0094, precision=0.4475, recall=0.8435, f1=0.5848
Training epoch 93
	step [1/144], loss=76.4935
	step [2/144], loss=76.9490
	step [3/144], loss=81.0724
	step [4/144], loss=78.5531
	step [5/144], loss=71.3797
	step [6/144], loss=84.7894
	step [7/144], loss=74.2663
	step [8/144], loss=77.6657
	step [9/144], loss=69.7250
	step [10/144], loss=81.8133
	step [11/144], loss=74.9158
	step [12/144], loss=69.6512
	step [13/144], loss=84.5998
	step [14/144], loss=82.9390
	step [15/144], loss=76.4355
	step [16/144], loss=76.2946
	step [17/144], loss=87.9513
	step [18/144], loss=86.2462
	step [19/144], loss=65.2578
	step [20/144], loss=89.6947
	step [21/144], loss=80.7911
	step [22/144], loss=76.5470
	step [23/144], loss=74.4913
	step [24/144], loss=78.1842
	step [25/144], loss=74.7024
	step [26/144], loss=69.1589
	step [27/144], loss=82.4397
	step [28/144], loss=80.3929
	step [29/144], loss=84.2113
	step [30/144], loss=80.2962
	step [31/144], loss=79.3158
	step [32/144], loss=74.1716
	step [33/144], loss=74.4899
	step [34/144], loss=83.6895
	step [35/144], loss=76.7869
	step [36/144], loss=84.6961
	step [37/144], loss=82.2396
	step [38/144], loss=90.9415
	step [39/144], loss=87.2244
	step [40/144], loss=74.7507
	step [41/144], loss=66.2556
	step [42/144], loss=78.3972
	step [43/144], loss=73.6143
	step [44/144], loss=82.4742
	step [45/144], loss=72.8166
	step [46/144], loss=76.5400
	step [47/144], loss=81.2230
	step [48/144], loss=70.4093
	step [49/144], loss=78.7117
	step [50/144], loss=75.2795
	step [51/144], loss=89.3439
	step [52/144], loss=75.7302
	step [53/144], loss=68.5947
	step [54/144], loss=63.0632
	step [55/144], loss=84.6749
	step [56/144], loss=79.3047
	step [57/144], loss=78.5346
	step [58/144], loss=86.6248
	step [59/144], loss=79.0635
	step [60/144], loss=68.1349
	step [61/144], loss=71.2910
	step [62/144], loss=66.7525
	step [63/144], loss=89.7988
	step [64/144], loss=71.2617
	step [65/144], loss=74.2073
	step [66/144], loss=96.7002
	step [67/144], loss=54.4436
	step [68/144], loss=73.1629
	step [69/144], loss=86.0688
	step [70/144], loss=79.3272
	step [71/144], loss=78.1160
	step [72/144], loss=78.9381
	step [73/144], loss=71.1805
	step [74/144], loss=89.8331
	step [75/144], loss=75.9435
	step [76/144], loss=72.1713
	step [77/144], loss=80.2602
	step [78/144], loss=80.5068
	step [79/144], loss=75.7334
	step [80/144], loss=77.0415
	step [81/144], loss=89.3153
	step [82/144], loss=77.8564
	step [83/144], loss=67.7669
	step [84/144], loss=71.8857
	step [85/144], loss=66.1723
	step [86/144], loss=78.3402
	step [87/144], loss=70.1917
	step [88/144], loss=77.9924
	step [89/144], loss=79.2015
	step [90/144], loss=79.2317
	step [91/144], loss=66.3450
	step [92/144], loss=88.7235
	step [93/144], loss=66.7771
	step [94/144], loss=62.7019
	step [95/144], loss=80.4376
	step [96/144], loss=81.9264
	step [97/144], loss=84.3344
	step [98/144], loss=79.1988
	step [99/144], loss=66.6335
	step [100/144], loss=77.8051
	step [101/144], loss=86.0128
	step [102/144], loss=83.5863
	step [103/144], loss=58.4545
	step [104/144], loss=71.6441
	step [105/144], loss=81.7266
	step [106/144], loss=80.2992
	step [107/144], loss=77.6556
	step [108/144], loss=78.6049
	step [109/144], loss=78.9985
	step [110/144], loss=75.2073
	step [111/144], loss=81.9540
	step [112/144], loss=73.4047
	step [113/144], loss=87.2736
	step [114/144], loss=79.1591
	step [115/144], loss=95.0623
	step [116/144], loss=76.0165
	step [117/144], loss=77.2102
	step [118/144], loss=81.0152
	step [119/144], loss=79.3864
	step [120/144], loss=78.6902
	step [121/144], loss=76.9488
	step [122/144], loss=61.9196
	step [123/144], loss=70.2655
	step [124/144], loss=71.1656
	step [125/144], loss=83.5250
	step [126/144], loss=68.6090
	step [127/144], loss=70.0270
	step [128/144], loss=95.5588
	step [129/144], loss=76.5932
	step [130/144], loss=77.2409
	step [131/144], loss=77.5479
	step [132/144], loss=95.0398
	step [133/144], loss=70.2936
	step [134/144], loss=70.9284
	step [135/144], loss=96.6028
	step [136/144], loss=73.9081
	step [137/144], loss=70.7678
	step [138/144], loss=73.4130
	step [139/144], loss=75.1360
	step [140/144], loss=76.9367
	step [141/144], loss=81.7853
	step [142/144], loss=69.3259
	step [143/144], loss=85.5735
	step [144/144], loss=27.0522
	Evaluating
	loss=0.0092, precision=0.4543, recall=0.8483, f1=0.5918
Training epoch 94
	step [1/144], loss=92.8285
	step [2/144], loss=70.5834
	step [3/144], loss=68.5135
	step [4/144], loss=81.2370
	step [5/144], loss=71.5841
	step [6/144], loss=84.2856
	step [7/144], loss=72.7381
	step [8/144], loss=61.0905
	step [9/144], loss=95.4600
	step [10/144], loss=80.9232
	step [11/144], loss=82.7795
	step [12/144], loss=68.8934
	step [13/144], loss=73.3060
	step [14/144], loss=92.2201
	step [15/144], loss=81.8999
	step [16/144], loss=65.8016
	step [17/144], loss=62.2161
	step [18/144], loss=79.2436
	step [19/144], loss=81.1250
	step [20/144], loss=70.8809
	step [21/144], loss=76.4694
	step [22/144], loss=84.4010
	step [23/144], loss=78.2478
	step [24/144], loss=83.6122
	step [25/144], loss=72.4703
	step [26/144], loss=90.2016
	step [27/144], loss=84.7245
	step [28/144], loss=85.5857
	step [29/144], loss=77.2184
	step [30/144], loss=87.1573
	step [31/144], loss=57.4558
	step [32/144], loss=88.2918
	step [33/144], loss=59.4794
	step [34/144], loss=87.3752
	step [35/144], loss=87.7344
	step [36/144], loss=71.9777
	step [37/144], loss=90.5819
	step [38/144], loss=88.2387
	step [39/144], loss=81.3026
	step [40/144], loss=71.9097
	step [41/144], loss=79.5290
	step [42/144], loss=82.1835
	step [43/144], loss=83.5027
	step [44/144], loss=82.7361
	step [45/144], loss=79.9837
	step [46/144], loss=82.0612
	step [47/144], loss=73.3370
	step [48/144], loss=77.4253
	step [49/144], loss=88.0647
	step [50/144], loss=71.7220
	step [51/144], loss=60.8018
	step [52/144], loss=81.3436
	step [53/144], loss=73.9795
	step [54/144], loss=63.1176
	step [55/144], loss=71.4877
	step [56/144], loss=66.0604
	step [57/144], loss=76.6196
	step [58/144], loss=98.4545
	step [59/144], loss=72.3568
	step [60/144], loss=70.0735
	step [61/144], loss=83.8561
	step [62/144], loss=66.4061
	step [63/144], loss=79.7538
	step [64/144], loss=67.5519
	step [65/144], loss=77.8148
	step [66/144], loss=75.1374
	step [67/144], loss=79.9757
	step [68/144], loss=76.9799
	step [69/144], loss=72.2214
	step [70/144], loss=78.4226
	step [71/144], loss=72.3814
	step [72/144], loss=76.6874
	step [73/144], loss=80.6602
	step [74/144], loss=78.8510
	step [75/144], loss=70.8267
	step [76/144], loss=86.7899
	step [77/144], loss=72.4469
	step [78/144], loss=71.5124
	step [79/144], loss=95.1944
	step [80/144], loss=90.6874
	step [81/144], loss=76.4391
	step [82/144], loss=73.8636
	step [83/144], loss=72.6832
	step [84/144], loss=83.4240
	step [85/144], loss=72.4088
	step [86/144], loss=85.5885
	step [87/144], loss=70.2173
	step [88/144], loss=80.0264
	step [89/144], loss=69.5345
	step [90/144], loss=73.3151
	step [91/144], loss=73.2215
	step [92/144], loss=68.6608
	step [93/144], loss=86.9837
	step [94/144], loss=56.2255
	step [95/144], loss=80.1481
	step [96/144], loss=71.1084
	step [97/144], loss=64.2318
	step [98/144], loss=86.1085
	step [99/144], loss=71.6442
	step [100/144], loss=73.9998
	step [101/144], loss=94.0194
	step [102/144], loss=88.9737
	step [103/144], loss=76.0368
	step [104/144], loss=81.1007
	step [105/144], loss=93.1144
	step [106/144], loss=59.4754
	step [107/144], loss=76.8094
	step [108/144], loss=94.5998
	step [109/144], loss=89.9582
	step [110/144], loss=72.2511
	step [111/144], loss=82.9600
	step [112/144], loss=80.1903
	step [113/144], loss=76.0479
	step [114/144], loss=73.8819
	step [115/144], loss=73.4461
	step [116/144], loss=81.4254
	step [117/144], loss=78.7515
	step [118/144], loss=84.2761
	step [119/144], loss=83.7429
	step [120/144], loss=69.0889
	step [121/144], loss=92.3013
	step [122/144], loss=68.7970
	step [123/144], loss=59.1615
	step [124/144], loss=78.4671
	step [125/144], loss=87.5193
	step [126/144], loss=82.1811
	step [127/144], loss=80.6894
	step [128/144], loss=80.3236
	step [129/144], loss=74.9707
	step [130/144], loss=72.2096
	step [131/144], loss=70.4807
	step [132/144], loss=68.4165
	step [133/144], loss=77.0135
	step [134/144], loss=69.6054
	step [135/144], loss=79.1301
	step [136/144], loss=71.7794
	step [137/144], loss=71.9760
	step [138/144], loss=75.3809
	step [139/144], loss=75.2267
	step [140/144], loss=84.0971
	step [141/144], loss=70.8026
	step [142/144], loss=76.5331
	step [143/144], loss=68.0443
	step [144/144], loss=26.8035
	Evaluating
	loss=0.0093, precision=0.4473, recall=0.8239, f1=0.5798
Training finished
best_f1: 0.6083230221487325
directing: X rim_enhanced: True test_id 2
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 9358 # image files with weight 9358
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 2525 # image files with weight 2525
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/X 9358
Using 4 GPUs
Going to train epochs [45-94]
Training epoch 45
	step [1/147], loss=105.1044
	step [2/147], loss=125.7206
	step [3/147], loss=89.3916
	step [4/147], loss=88.7689
	step [5/147], loss=102.3931
	step [6/147], loss=105.7639
	step [7/147], loss=94.1324
	step [8/147], loss=103.8318
	step [9/147], loss=114.4262
	step [10/147], loss=105.0395
	step [11/147], loss=93.3248
	step [12/147], loss=93.0116
	step [13/147], loss=103.7481
	step [14/147], loss=88.2674
	step [15/147], loss=105.6084
	step [16/147], loss=99.9665
	step [17/147], loss=84.3076
	step [18/147], loss=81.8797
	step [19/147], loss=100.7434
	step [20/147], loss=98.8320
	step [21/147], loss=98.7652
	step [22/147], loss=81.5879
	step [23/147], loss=112.5981
	step [24/147], loss=127.6440
	step [25/147], loss=95.2384
	step [26/147], loss=97.4124
	step [27/147], loss=102.0624
	step [28/147], loss=103.1610
	step [29/147], loss=98.9330
	step [30/147], loss=117.5261
	step [31/147], loss=99.3697
	step [32/147], loss=106.0255
	step [33/147], loss=72.7555
	step [34/147], loss=94.0430
	step [35/147], loss=100.8043
	step [36/147], loss=94.8477
	step [37/147], loss=85.5812
	step [38/147], loss=90.4026
	step [39/147], loss=86.3739
	step [40/147], loss=96.0147
	step [41/147], loss=104.4341
	step [42/147], loss=107.8311
	step [43/147], loss=104.6985
	step [44/147], loss=91.6377
	step [45/147], loss=82.6522
	step [46/147], loss=89.1689
	step [47/147], loss=106.0031
	step [48/147], loss=99.8799
	step [49/147], loss=113.5772
	step [50/147], loss=103.4483
	step [51/147], loss=89.6538
	step [52/147], loss=120.9499
	step [53/147], loss=87.2252
	step [54/147], loss=97.3919
	step [55/147], loss=99.9354
	step [56/147], loss=107.6861
	step [57/147], loss=108.3341
	step [58/147], loss=90.5067
	step [59/147], loss=110.2885
	step [60/147], loss=98.6866
	step [61/147], loss=100.4998
	step [62/147], loss=110.5361
	step [63/147], loss=88.4496
	step [64/147], loss=112.2293
	step [65/147], loss=98.0416
	step [66/147], loss=87.1162
	step [67/147], loss=94.9483
	step [68/147], loss=97.1033
	step [69/147], loss=95.9746
	step [70/147], loss=111.8279
	step [71/147], loss=78.3751
	step [72/147], loss=83.5875
	step [73/147], loss=106.5692
	step [74/147], loss=112.8426
	step [75/147], loss=90.1952
	step [76/147], loss=88.1379
	step [77/147], loss=118.0207
	step [78/147], loss=110.0075
	step [79/147], loss=83.1463
	step [80/147], loss=86.7255
	step [81/147], loss=90.1182
	step [82/147], loss=79.1206
	step [83/147], loss=86.1140
	step [84/147], loss=112.4885
	step [85/147], loss=75.1932
	step [86/147], loss=99.3790
	step [87/147], loss=74.8551
	step [88/147], loss=95.2712
	step [89/147], loss=83.9086
	step [90/147], loss=93.1811
	step [91/147], loss=97.2094
	step [92/147], loss=91.2259
	step [93/147], loss=93.6965
	step [94/147], loss=93.3400
	step [95/147], loss=88.7838
	step [96/147], loss=92.9521
	step [97/147], loss=74.8595
	step [98/147], loss=100.5204
	step [99/147], loss=97.0488
	step [100/147], loss=83.8639
	step [101/147], loss=93.9486
	step [102/147], loss=97.6045
	step [103/147], loss=116.1827
	step [104/147], loss=80.1200
	step [105/147], loss=90.6294
	step [106/147], loss=98.9433
	step [107/147], loss=109.7767
	step [108/147], loss=75.3219
	step [109/147], loss=85.9859
	step [110/147], loss=126.9732
	step [111/147], loss=100.3190
	step [112/147], loss=109.8738
	step [113/147], loss=102.6201
	step [114/147], loss=92.9609
	step [115/147], loss=99.5160
	step [116/147], loss=99.6979
	step [117/147], loss=90.2812
	step [118/147], loss=89.8421
	step [119/147], loss=89.3207
	step [120/147], loss=95.1094
	step [121/147], loss=95.1002
	step [122/147], loss=111.9430
	step [123/147], loss=103.4869
	step [124/147], loss=88.4274
	step [125/147], loss=83.6152
	step [126/147], loss=100.5403
	step [127/147], loss=101.9363
	step [128/147], loss=102.6099
	step [129/147], loss=97.1361
	step [130/147], loss=95.7108
	step [131/147], loss=102.7079
	step [132/147], loss=87.0747
	step [133/147], loss=108.3057
	step [134/147], loss=95.7649
	step [135/147], loss=84.3885
	step [136/147], loss=87.7957
	step [137/147], loss=102.0944
	step [138/147], loss=107.9013
	step [139/147], loss=98.8144
	step [140/147], loss=128.0368
	step [141/147], loss=88.4244
	step [142/147], loss=88.1928
	step [143/147], loss=88.4480
	step [144/147], loss=82.9213
	step [145/147], loss=88.8400
	step [146/147], loss=106.8407
	step [147/147], loss=17.1172
	Evaluating
	loss=0.0160, precision=0.3362, recall=0.8790, f1=0.4863
saving model as: 2_saved_model.pth
Training epoch 46
	step [1/147], loss=101.7256
	step [2/147], loss=108.3812
	step [3/147], loss=83.5300
	step [4/147], loss=86.7140
	step [5/147], loss=91.0373
	step [6/147], loss=102.6701
	step [7/147], loss=99.5986
	step [8/147], loss=114.9910
	step [9/147], loss=107.8102
	step [10/147], loss=86.9241
	step [11/147], loss=86.1461
	step [12/147], loss=105.9077
	step [13/147], loss=97.3575
	step [14/147], loss=93.2461
	step [15/147], loss=105.0366
	step [16/147], loss=107.7644
	step [17/147], loss=88.9278
	step [18/147], loss=104.4229
	step [19/147], loss=91.8467
	step [20/147], loss=90.7666
	step [21/147], loss=97.3594
	step [22/147], loss=92.4784
	step [23/147], loss=91.5160
	step [24/147], loss=104.2822
	step [25/147], loss=97.9541
	step [26/147], loss=98.6713
	step [27/147], loss=76.9345
	step [28/147], loss=71.5205
	step [29/147], loss=93.8186
	step [30/147], loss=100.6093
	step [31/147], loss=102.7182
	step [32/147], loss=106.4692
	step [33/147], loss=89.7771
	step [34/147], loss=101.1909
	step [35/147], loss=87.9511
	step [36/147], loss=93.1685
	step [37/147], loss=90.4006
	step [38/147], loss=88.6061
	step [39/147], loss=99.6000
	step [40/147], loss=92.1820
	step [41/147], loss=96.9214
	step [42/147], loss=86.2325
	step [43/147], loss=94.7228
	step [44/147], loss=94.0651
	step [45/147], loss=100.1673
	step [46/147], loss=110.4121
	step [47/147], loss=98.4207
	step [48/147], loss=94.3320
	step [49/147], loss=94.8213
	step [50/147], loss=113.0802
	step [51/147], loss=88.7061
	step [52/147], loss=90.7874
	step [53/147], loss=100.1688
	step [54/147], loss=83.6568
	step [55/147], loss=76.8307
	step [56/147], loss=96.0650
	step [57/147], loss=84.3948
	step [58/147], loss=93.9325
	step [59/147], loss=90.1573
	step [60/147], loss=99.3335
	step [61/147], loss=108.9406
	step [62/147], loss=79.9101
	step [63/147], loss=109.3145
	step [64/147], loss=116.5602
	step [65/147], loss=87.8014
	step [66/147], loss=98.7879
	step [67/147], loss=95.3241
	step [68/147], loss=100.0807
	step [69/147], loss=96.3756
	step [70/147], loss=85.9886
	step [71/147], loss=104.6624
	step [72/147], loss=87.1581
	step [73/147], loss=93.7188
	step [74/147], loss=93.6825
	step [75/147], loss=104.3705
	step [76/147], loss=97.7809
	step [77/147], loss=82.3908
	step [78/147], loss=112.7350
	step [79/147], loss=92.9176
	step [80/147], loss=94.0112
	step [81/147], loss=112.0665
	step [82/147], loss=118.1742
	step [83/147], loss=107.9083
	step [84/147], loss=86.8410
	step [85/147], loss=100.7982
	step [86/147], loss=80.2091
	step [87/147], loss=96.7035
	step [88/147], loss=77.9161
	step [89/147], loss=84.7517
	step [90/147], loss=94.6988
	step [91/147], loss=111.3020
	step [92/147], loss=78.3136
	step [93/147], loss=97.6996
	step [94/147], loss=98.8971
	step [95/147], loss=112.1210
	step [96/147], loss=86.7379
	step [97/147], loss=92.0433
	step [98/147], loss=82.7169
	step [99/147], loss=88.4650
	step [100/147], loss=85.5687
	step [101/147], loss=101.0642
	step [102/147], loss=93.9545
	step [103/147], loss=105.2739
	step [104/147], loss=90.0716
	step [105/147], loss=109.3955
	step [106/147], loss=86.6355
	step [107/147], loss=91.1771
	step [108/147], loss=91.4953
	step [109/147], loss=89.2894
	step [110/147], loss=103.8023
	step [111/147], loss=98.5735
	step [112/147], loss=85.1391
	step [113/147], loss=85.4978
	step [114/147], loss=96.7411
	step [115/147], loss=113.2820
	step [116/147], loss=102.0881
	step [117/147], loss=104.5563
	step [118/147], loss=92.4281
	step [119/147], loss=102.9570
	step [120/147], loss=99.4048
	step [121/147], loss=115.5644
	step [122/147], loss=114.0797
	step [123/147], loss=106.2070
	step [124/147], loss=104.0219
	step [125/147], loss=100.7191
	step [126/147], loss=105.9643
	step [127/147], loss=97.3033
	step [128/147], loss=112.3850
	step [129/147], loss=101.6438
	step [130/147], loss=89.6028
	step [131/147], loss=100.4052
	step [132/147], loss=113.0034
	step [133/147], loss=108.4056
	step [134/147], loss=88.8916
	step [135/147], loss=91.3753
	step [136/147], loss=91.3167
	step [137/147], loss=96.6079
	step [138/147], loss=88.5746
	step [139/147], loss=88.1578
	step [140/147], loss=95.0586
	step [141/147], loss=102.9667
	step [142/147], loss=107.4253
	step [143/147], loss=105.7155
	step [144/147], loss=90.7678
	step [145/147], loss=90.1339
	step [146/147], loss=89.0605
	step [147/147], loss=33.0668
	Evaluating
	loss=0.0204, precision=0.2618, recall=0.8915, f1=0.4047
Training epoch 47
	step [1/147], loss=90.4781
	step [2/147], loss=94.7942
	step [3/147], loss=82.4885
	step [4/147], loss=86.4378
	step [5/147], loss=105.7032
	step [6/147], loss=98.9263
	step [7/147], loss=94.4060
	step [8/147], loss=97.1366
	step [9/147], loss=105.1972
	step [10/147], loss=86.9294
	step [11/147], loss=90.8840
	step [12/147], loss=103.2231
	step [13/147], loss=103.3736
	step [14/147], loss=115.0064
	step [15/147], loss=105.6705
	step [16/147], loss=100.0348
	step [17/147], loss=85.5143
	step [18/147], loss=88.4174
	step [19/147], loss=84.4022
	step [20/147], loss=98.0165
	step [21/147], loss=98.1736
	step [22/147], loss=96.5511
	step [23/147], loss=85.1355
	step [24/147], loss=92.2338
	step [25/147], loss=104.8567
	step [26/147], loss=113.5831
	step [27/147], loss=96.9692
	step [28/147], loss=103.6057
	step [29/147], loss=105.0347
	step [30/147], loss=90.4301
	step [31/147], loss=84.3693
	step [32/147], loss=102.7109
	step [33/147], loss=88.2678
	step [34/147], loss=102.7977
	step [35/147], loss=103.5837
	step [36/147], loss=102.1785
	step [37/147], loss=96.2126
	step [38/147], loss=102.3194
	step [39/147], loss=86.7873
	step [40/147], loss=110.6022
	step [41/147], loss=86.1816
	step [42/147], loss=86.9729
	step [43/147], loss=99.6994
	step [44/147], loss=102.7307
	step [45/147], loss=99.3075
	step [46/147], loss=82.1469
	step [47/147], loss=88.8005
	step [48/147], loss=91.3223
	step [49/147], loss=95.0112
	step [50/147], loss=113.5749
	step [51/147], loss=83.3650
	step [52/147], loss=93.8416
	step [53/147], loss=101.5600
	step [54/147], loss=73.2446
	step [55/147], loss=87.8972
	step [56/147], loss=105.0161
	step [57/147], loss=86.7032
	step [58/147], loss=92.0113
	step [59/147], loss=107.4678
	step [60/147], loss=107.5130
	step [61/147], loss=82.2529
	step [62/147], loss=83.2615
	step [63/147], loss=91.6713
	step [64/147], loss=91.4159
	step [65/147], loss=87.5901
	step [66/147], loss=106.4607
	step [67/147], loss=92.5382
	step [68/147], loss=86.9910
	step [69/147], loss=101.4785
	step [70/147], loss=105.3780
	step [71/147], loss=111.6018
	step [72/147], loss=90.5483
	step [73/147], loss=77.3601
	step [74/147], loss=96.0438
	step [75/147], loss=100.4892
	step [76/147], loss=91.2330
	step [77/147], loss=113.3717
	step [78/147], loss=102.0372
	step [79/147], loss=99.8233
	step [80/147], loss=117.8510
	step [81/147], loss=97.4650
	step [82/147], loss=78.9585
	step [83/147], loss=101.7798
	step [84/147], loss=94.4824
	step [85/147], loss=101.2589
	step [86/147], loss=77.7399
	step [87/147], loss=81.6906
	step [88/147], loss=104.4592
	step [89/147], loss=96.3195
	step [90/147], loss=101.9802
	step [91/147], loss=79.0213
	step [92/147], loss=102.6960
	step [93/147], loss=94.3086
	step [94/147], loss=86.5637
	step [95/147], loss=102.2478
	step [96/147], loss=81.6012
	step [97/147], loss=86.4497
	step [98/147], loss=117.1380
	step [99/147], loss=97.7847
	step [100/147], loss=89.6559
	step [101/147], loss=99.7032
	step [102/147], loss=106.5262
	step [103/147], loss=93.9764
	step [104/147], loss=106.2824
	step [105/147], loss=83.5215
	step [106/147], loss=113.8902
	step [107/147], loss=83.5677
	step [108/147], loss=94.6826
	step [109/147], loss=99.9399
	step [110/147], loss=92.2590
	step [111/147], loss=84.4487
	step [112/147], loss=92.7831
	step [113/147], loss=96.1738
	step [114/147], loss=93.1573
	step [115/147], loss=84.8970
	step [116/147], loss=111.1848
	step [117/147], loss=92.6745
	step [118/147], loss=85.5203
	step [119/147], loss=103.7215
	step [120/147], loss=102.0553
	step [121/147], loss=101.9609
	step [122/147], loss=108.0063
	step [123/147], loss=97.9217
	step [124/147], loss=132.5897
	step [125/147], loss=92.4162
	step [126/147], loss=103.8431
	step [127/147], loss=88.5921
	step [128/147], loss=81.0564
	step [129/147], loss=99.3984
	step [130/147], loss=99.4255
	step [131/147], loss=89.9863
	step [132/147], loss=109.5247
	step [133/147], loss=109.1099
	step [134/147], loss=77.2975
	step [135/147], loss=102.8468
	step [136/147], loss=88.3309
	step [137/147], loss=88.8205
	step [138/147], loss=101.0636
	step [139/147], loss=106.8296
	step [140/147], loss=90.6698
	step [141/147], loss=109.8347
	step [142/147], loss=99.4517
	step [143/147], loss=90.2391
	step [144/147], loss=90.2162
	step [145/147], loss=89.9008
	step [146/147], loss=86.4853
	step [147/147], loss=25.4603
	Evaluating
	loss=0.0180, precision=0.2997, recall=0.8856, f1=0.4479
Training epoch 48
	step [1/147], loss=100.4065
	step [2/147], loss=83.1800
	step [3/147], loss=80.4864
	step [4/147], loss=81.3490
	step [5/147], loss=89.9409
	step [6/147], loss=86.3988
	step [7/147], loss=97.8883
	step [8/147], loss=101.0030
	step [9/147], loss=105.1481
	step [10/147], loss=87.4450
	step [11/147], loss=97.6646
	step [12/147], loss=110.2626
	step [13/147], loss=97.2785
	step [14/147], loss=102.9543
	step [15/147], loss=95.5176
	step [16/147], loss=94.9119
	step [17/147], loss=104.2675
	step [18/147], loss=110.2239
	step [19/147], loss=105.7656
	step [20/147], loss=83.5860
	step [21/147], loss=96.0849
	step [22/147], loss=85.8910
	step [23/147], loss=86.7739
	step [24/147], loss=86.0206
	step [25/147], loss=97.0155
	step [26/147], loss=93.9631
	step [27/147], loss=98.2119
	step [28/147], loss=83.8941
	step [29/147], loss=93.3980
	step [30/147], loss=90.6295
	step [31/147], loss=92.0100
	step [32/147], loss=95.4610
	step [33/147], loss=95.6418
	step [34/147], loss=87.2762
	step [35/147], loss=106.2828
	step [36/147], loss=90.6692
	step [37/147], loss=83.1006
	step [38/147], loss=108.3677
	step [39/147], loss=100.3123
	step [40/147], loss=97.8320
	step [41/147], loss=101.2627
	step [42/147], loss=93.5972
	step [43/147], loss=88.8942
	step [44/147], loss=101.5753
	step [45/147], loss=88.1333
	step [46/147], loss=69.8880
	step [47/147], loss=85.3620
	step [48/147], loss=100.4794
	step [49/147], loss=108.9160
	step [50/147], loss=101.6598
	step [51/147], loss=90.5617
	step [52/147], loss=100.5550
	step [53/147], loss=112.1089
	step [54/147], loss=91.3542
	step [55/147], loss=85.8858
	step [56/147], loss=107.2846
	step [57/147], loss=85.5020
	step [58/147], loss=82.2170
	step [59/147], loss=94.5859
	step [60/147], loss=99.8905
	step [61/147], loss=92.0231
	step [62/147], loss=111.2643
	step [63/147], loss=102.0343
	step [64/147], loss=76.2605
	step [65/147], loss=75.2182
	step [66/147], loss=88.5667
	step [67/147], loss=80.3981
	step [68/147], loss=99.7187
	step [69/147], loss=82.8274
	step [70/147], loss=108.8944
	step [71/147], loss=95.1498
	step [72/147], loss=78.1410
	step [73/147], loss=109.2942
	step [74/147], loss=102.2757
	step [75/147], loss=88.7482
	step [76/147], loss=100.1313
	step [77/147], loss=114.0289
	step [78/147], loss=99.1207
	step [79/147], loss=87.6964
	step [80/147], loss=94.5821
	step [81/147], loss=93.4159
	step [82/147], loss=104.1055
	step [83/147], loss=107.5511
	step [84/147], loss=92.7565
	step [85/147], loss=110.9531
	step [86/147], loss=99.9077
	step [87/147], loss=94.4595
	step [88/147], loss=79.2897
	step [89/147], loss=83.9834
	step [90/147], loss=95.8442
	step [91/147], loss=85.5434
	step [92/147], loss=96.5208
	step [93/147], loss=88.6585
	step [94/147], loss=99.6783
	step [95/147], loss=80.6136
	step [96/147], loss=101.4398
	step [97/147], loss=95.1212
	step [98/147], loss=99.2252
	step [99/147], loss=98.3240
	step [100/147], loss=94.6629
	step [101/147], loss=122.2430
	step [102/147], loss=106.6522
	step [103/147], loss=100.0899
	step [104/147], loss=91.7442
	step [105/147], loss=88.6678
	step [106/147], loss=88.2053
	step [107/147], loss=88.3097
	step [108/147], loss=112.3743
	step [109/147], loss=92.5127
	step [110/147], loss=99.5158
	step [111/147], loss=98.7171
	step [112/147], loss=75.3849
	step [113/147], loss=106.1734
	step [114/147], loss=96.9889
	step [115/147], loss=92.3233
	step [116/147], loss=95.1671
	step [117/147], loss=87.6481
	step [118/147], loss=103.5831
	step [119/147], loss=86.9843
	step [120/147], loss=96.0922
	step [121/147], loss=90.6596
	step [122/147], loss=92.2368
	step [123/147], loss=82.8605
	step [124/147], loss=101.9773
	step [125/147], loss=98.9217
	step [126/147], loss=95.2826
	step [127/147], loss=117.0506
	step [128/147], loss=112.8609
	step [129/147], loss=85.8539
	step [130/147], loss=84.5691
	step [131/147], loss=101.4444
	step [132/147], loss=79.3167
	step [133/147], loss=104.8741
	step [134/147], loss=90.9112
	step [135/147], loss=117.3268
	step [136/147], loss=104.7070
	step [137/147], loss=101.9514
	step [138/147], loss=83.7890
	step [139/147], loss=101.5674
	step [140/147], loss=102.9868
	step [141/147], loss=105.7719
	step [142/147], loss=104.4694
	step [143/147], loss=93.5570
	step [144/147], loss=104.1862
	step [145/147], loss=98.4974
	step [146/147], loss=106.6506
	step [147/147], loss=20.6155
	Evaluating
	loss=0.0162, precision=0.3261, recall=0.8797, f1=0.4758
Training epoch 49
	step [1/147], loss=98.3752
	step [2/147], loss=83.2819
	step [3/147], loss=95.6183
	step [4/147], loss=96.9864
	step [5/147], loss=97.8728
	step [6/147], loss=94.0708
	step [7/147], loss=102.6527
	step [8/147], loss=89.2860
	step [9/147], loss=98.4624
	step [10/147], loss=93.2881
	step [11/147], loss=90.1123
	step [12/147], loss=102.2520
	step [13/147], loss=72.6281
	step [14/147], loss=88.7695
	step [15/147], loss=114.3512
	step [16/147], loss=98.6614
	step [17/147], loss=109.3625
	step [18/147], loss=92.7223
	step [19/147], loss=116.5821
	step [20/147], loss=82.2875
	step [21/147], loss=111.7252
	step [22/147], loss=102.9654
	step [23/147], loss=103.9340
	step [24/147], loss=104.0138
	step [25/147], loss=88.2176
	step [26/147], loss=99.0669
	step [27/147], loss=105.1351
	step [28/147], loss=85.0691
	step [29/147], loss=101.0038
	step [30/147], loss=92.4746
	step [31/147], loss=92.4846
	step [32/147], loss=82.9273
	step [33/147], loss=102.2597
	step [34/147], loss=98.0580
	step [35/147], loss=84.4708
	step [36/147], loss=85.3568
	step [37/147], loss=90.6129
	step [38/147], loss=99.5972
	step [39/147], loss=107.9456
	step [40/147], loss=90.4954
	step [41/147], loss=98.1708
	step [42/147], loss=92.9289
	step [43/147], loss=76.3374
	step [44/147], loss=77.9738
	step [45/147], loss=91.7653
	step [46/147], loss=94.9106
	step [47/147], loss=97.2689
	step [48/147], loss=96.2417
	step [49/147], loss=110.7977
	step [50/147], loss=86.6412
	step [51/147], loss=89.1097
	step [52/147], loss=112.7142
	step [53/147], loss=104.2529
	step [54/147], loss=97.2405
	step [55/147], loss=82.9453
	step [56/147], loss=105.7150
	step [57/147], loss=84.2836
	step [58/147], loss=93.8635
	step [59/147], loss=103.0438
	step [60/147], loss=86.4389
	step [61/147], loss=87.2911
	step [62/147], loss=92.3237
	step [63/147], loss=90.3854
	step [64/147], loss=98.3329
	step [65/147], loss=85.2870
	step [66/147], loss=84.9550
	step [67/147], loss=97.3455
	step [68/147], loss=95.2232
	step [69/147], loss=101.7554
	step [70/147], loss=95.4075
	step [71/147], loss=104.4823
	step [72/147], loss=93.3474
	step [73/147], loss=101.7261
	step [74/147], loss=83.7998
	step [75/147], loss=84.9670
	step [76/147], loss=85.1450
	step [77/147], loss=97.5768
	step [78/147], loss=96.7871
	step [79/147], loss=114.7418
	step [80/147], loss=101.3719
	step [81/147], loss=95.5998
	step [82/147], loss=99.3880
	step [83/147], loss=105.5431
	step [84/147], loss=92.7264
	step [85/147], loss=93.4398
	step [86/147], loss=87.9405
	step [87/147], loss=100.1799
	step [88/147], loss=93.9703
	step [89/147], loss=93.2670
	step [90/147], loss=84.1649
	step [91/147], loss=105.7305
	step [92/147], loss=88.8816
	step [93/147], loss=75.5341
	step [94/147], loss=92.7597
	step [95/147], loss=84.5873
	step [96/147], loss=96.6389
	step [97/147], loss=84.1078
	step [98/147], loss=92.2926
	step [99/147], loss=82.7130
	step [100/147], loss=92.7466
	step [101/147], loss=90.1489
	step [102/147], loss=99.5099
	step [103/147], loss=102.0041
	step [104/147], loss=85.1025
	step [105/147], loss=88.7914
	step [106/147], loss=86.3728
	step [107/147], loss=92.1220
	step [108/147], loss=83.9630
	step [109/147], loss=91.8809
	step [110/147], loss=103.4034
	step [111/147], loss=107.3801
	step [112/147], loss=87.4693
	step [113/147], loss=82.7424
	step [114/147], loss=104.1172
	step [115/147], loss=103.8837
	step [116/147], loss=88.1697
	step [117/147], loss=107.0776
	step [118/147], loss=71.9106
	step [119/147], loss=99.3052
	step [120/147], loss=97.1121
	step [121/147], loss=92.2632
	step [122/147], loss=101.5913
	step [123/147], loss=87.3918
	step [124/147], loss=112.9706
	step [125/147], loss=87.2689
	step [126/147], loss=99.5339
	step [127/147], loss=90.9588
	step [128/147], loss=94.5683
	step [129/147], loss=93.3954
	step [130/147], loss=99.2548
	step [131/147], loss=93.7595
	step [132/147], loss=103.2346
	step [133/147], loss=90.8213
	step [134/147], loss=90.2535
	step [135/147], loss=94.5586
	step [136/147], loss=100.9874
	step [137/147], loss=88.0231
	step [138/147], loss=107.7814
	step [139/147], loss=100.3673
	step [140/147], loss=98.8175
	step [141/147], loss=103.5922
	step [142/147], loss=91.1253
	step [143/147], loss=93.3880
	step [144/147], loss=95.4470
	step [145/147], loss=102.6022
	step [146/147], loss=96.6321
	step [147/147], loss=16.2655
	Evaluating
	loss=0.0203, precision=0.2648, recall=0.9093, f1=0.4102
Training epoch 50
	step [1/147], loss=85.9064
	step [2/147], loss=102.0307
	step [3/147], loss=84.2541
	step [4/147], loss=98.0811
	step [5/147], loss=85.2417
	step [6/147], loss=103.6735
	step [7/147], loss=87.2027
	step [8/147], loss=83.1668
	step [9/147], loss=73.4116
	step [10/147], loss=110.3831
	step [11/147], loss=100.3718
	step [12/147], loss=92.3370
	step [13/147], loss=99.6505
	step [14/147], loss=100.2856
	step [15/147], loss=79.2173
	step [16/147], loss=93.2386
	step [17/147], loss=95.0092
	step [18/147], loss=93.6927
	step [19/147], loss=109.0421
	step [20/147], loss=87.1309
	step [21/147], loss=89.3417
	step [22/147], loss=98.6239
	step [23/147], loss=90.2355
	step [24/147], loss=102.9328
	step [25/147], loss=91.6991
	step [26/147], loss=83.8183
	step [27/147], loss=93.9199
	step [28/147], loss=76.4828
	step [29/147], loss=106.3815
	step [30/147], loss=105.7022
	step [31/147], loss=102.0840
	step [32/147], loss=86.2489
	step [33/147], loss=109.9541
	step [34/147], loss=111.4224
	step [35/147], loss=87.6083
	step [36/147], loss=94.3107
	step [37/147], loss=107.0524
	step [38/147], loss=91.7290
	step [39/147], loss=103.4754
	step [40/147], loss=99.4508
	step [41/147], loss=114.3067
	step [42/147], loss=104.7579
	step [43/147], loss=110.7946
	step [44/147], loss=99.1566
	step [45/147], loss=100.4650
	step [46/147], loss=112.7624
	step [47/147], loss=89.2651
	step [48/147], loss=110.1393
	step [49/147], loss=96.6506
	step [50/147], loss=90.9760
	step [51/147], loss=94.2135
	step [52/147], loss=78.7539
	step [53/147], loss=100.0047
	step [54/147], loss=89.6169
	step [55/147], loss=94.3123
	step [56/147], loss=100.9944
	step [57/147], loss=74.0610
	step [58/147], loss=94.5298
	step [59/147], loss=92.5483
	step [60/147], loss=88.7113
	step [61/147], loss=102.5894
	step [62/147], loss=88.0427
	step [63/147], loss=81.2151
	step [64/147], loss=80.4093
	step [65/147], loss=89.7271
	step [66/147], loss=87.1734
	step [67/147], loss=84.1340
	step [68/147], loss=105.3102
	step [69/147], loss=84.4872
	step [70/147], loss=86.7723
	step [71/147], loss=81.9025
	step [72/147], loss=100.3248
	step [73/147], loss=101.9641
	step [74/147], loss=94.7638
	step [75/147], loss=89.9806
	step [76/147], loss=100.5090
	step [77/147], loss=71.4659
	step [78/147], loss=93.7426
	step [79/147], loss=92.2770
	step [80/147], loss=86.5851
	step [81/147], loss=97.1563
	step [82/147], loss=95.9571
	step [83/147], loss=92.8090
	step [84/147], loss=79.9593
	step [85/147], loss=104.7754
	step [86/147], loss=74.8428
	step [87/147], loss=99.1465
	step [88/147], loss=101.6673
	step [89/147], loss=85.5880
	step [90/147], loss=72.1347
	step [91/147], loss=93.5972
	step [92/147], loss=102.8580
	step [93/147], loss=85.7883
	step [94/147], loss=80.7870
	step [95/147], loss=104.7583
	step [96/147], loss=86.7414
	step [97/147], loss=110.7329
	step [98/147], loss=97.9995
	step [99/147], loss=85.7413
	step [100/147], loss=112.7368
	step [101/147], loss=96.2694
	step [102/147], loss=107.0586
	step [103/147], loss=92.1730
	step [104/147], loss=103.6129
	step [105/147], loss=108.7615
	step [106/147], loss=108.3977
	step [107/147], loss=95.4490
	step [108/147], loss=75.0438
	step [109/147], loss=90.4951
	step [110/147], loss=94.1922
	step [111/147], loss=92.4660
	step [112/147], loss=105.0212
	step [113/147], loss=86.4076
	step [114/147], loss=91.7743
	step [115/147], loss=93.7755
	step [116/147], loss=101.8871
	step [117/147], loss=92.0729
	step [118/147], loss=94.2820
	step [119/147], loss=76.2980
	step [120/147], loss=98.5304
	step [121/147], loss=95.6644
	step [122/147], loss=80.7964
	step [123/147], loss=93.7420
	step [124/147], loss=99.0385
	step [125/147], loss=100.4467
	step [126/147], loss=78.1025
	step [127/147], loss=110.9787
	step [128/147], loss=110.4972
	step [129/147], loss=100.6893
	step [130/147], loss=80.5322
	step [131/147], loss=108.3841
	step [132/147], loss=103.3903
	step [133/147], loss=82.4210
	step [134/147], loss=100.3602
	step [135/147], loss=103.6985
	step [136/147], loss=81.9020
	step [137/147], loss=102.0546
	step [138/147], loss=84.1865
	step [139/147], loss=85.3194
	step [140/147], loss=86.2573
	step [141/147], loss=95.7509
	step [142/147], loss=98.9912
	step [143/147], loss=108.5250
	step [144/147], loss=105.6834
	step [145/147], loss=94.1030
	step [146/147], loss=109.8508
	step [147/147], loss=16.0041
	Evaluating
	loss=0.0165, precision=0.3260, recall=0.8843, f1=0.4764
Training epoch 51
	step [1/147], loss=107.8930
	step [2/147], loss=79.2573
	step [3/147], loss=102.6040
	step [4/147], loss=100.3770
	step [5/147], loss=98.8173
	step [6/147], loss=86.7821
	step [7/147], loss=92.7902
	step [8/147], loss=85.4296
	step [9/147], loss=95.7191
	step [10/147], loss=101.6575
	step [11/147], loss=108.8804
	step [12/147], loss=87.8448
	step [13/147], loss=99.4600
	step [14/147], loss=88.1738
	step [15/147], loss=108.5714
	step [16/147], loss=93.5266
	step [17/147], loss=92.1211
	step [18/147], loss=100.9290
	step [19/147], loss=105.9391
	step [20/147], loss=104.6875
	step [21/147], loss=91.6335
	step [22/147], loss=102.6878
	step [23/147], loss=91.3186
	step [24/147], loss=87.3267
	step [25/147], loss=81.7858
	step [26/147], loss=127.8294
	step [27/147], loss=82.1980
	step [28/147], loss=98.6511
	step [29/147], loss=100.9106
	step [30/147], loss=96.7192
	step [31/147], loss=82.2042
	step [32/147], loss=91.9364
	step [33/147], loss=94.4226
	step [34/147], loss=81.7750
	step [35/147], loss=78.4944
	step [36/147], loss=101.8005
	step [37/147], loss=103.0251
	step [38/147], loss=90.0002
	step [39/147], loss=98.1952
	step [40/147], loss=79.8986
	step [41/147], loss=89.8006
	step [42/147], loss=109.1014
	step [43/147], loss=85.6956
	step [44/147], loss=110.0665
	step [45/147], loss=100.4894
	step [46/147], loss=103.2701
	step [47/147], loss=116.2456
	step [48/147], loss=91.6458
	step [49/147], loss=102.0109
	step [50/147], loss=96.5072
	step [51/147], loss=90.2724
	step [52/147], loss=84.0695
	step [53/147], loss=84.4499
	step [54/147], loss=97.0745
	step [55/147], loss=98.7622
	step [56/147], loss=89.8963
	step [57/147], loss=99.0848
	step [58/147], loss=84.5767
	step [59/147], loss=86.0536
	step [60/147], loss=100.3202
	step [61/147], loss=105.0122
	step [62/147], loss=85.6415
	step [63/147], loss=85.2825
	step [64/147], loss=98.1254
	step [65/147], loss=103.8776
	step [66/147], loss=95.2880
	step [67/147], loss=108.4887
	step [68/147], loss=94.2741
	step [69/147], loss=85.3588
	step [70/147], loss=86.3548
	step [71/147], loss=110.6567
	step [72/147], loss=95.5404
	step [73/147], loss=89.6891
	step [74/147], loss=83.4020
	step [75/147], loss=91.4456
	step [76/147], loss=87.5740
	step [77/147], loss=95.7748
	step [78/147], loss=83.2479
	step [79/147], loss=106.9206
	step [80/147], loss=84.0579
	step [81/147], loss=101.4691
	step [82/147], loss=77.5515
	step [83/147], loss=103.8759
	step [84/147], loss=77.4313
	step [85/147], loss=97.9342
	step [86/147], loss=117.7413
	step [87/147], loss=79.9208
	step [88/147], loss=100.9400
	step [89/147], loss=84.7357
	step [90/147], loss=103.6083
	step [91/147], loss=94.0927
	step [92/147], loss=91.5318
	step [93/147], loss=89.9686
	step [94/147], loss=109.8389
	step [95/147], loss=88.3421
	step [96/147], loss=98.5028
	step [97/147], loss=77.5186
	step [98/147], loss=90.3266
	step [99/147], loss=84.4097
	step [100/147], loss=97.8484
	step [101/147], loss=86.1154
	step [102/147], loss=91.2032
	step [103/147], loss=86.7744
	step [104/147], loss=107.4431
	step [105/147], loss=77.7594
	step [106/147], loss=86.1966
	step [107/147], loss=108.7797
	step [108/147], loss=80.7427
	step [109/147], loss=93.4517
	step [110/147], loss=86.3864
	step [111/147], loss=85.8869
	step [112/147], loss=108.1280
	step [113/147], loss=95.7562
	step [114/147], loss=114.6665
	step [115/147], loss=103.8775
	step [116/147], loss=124.4869
	step [117/147], loss=92.7778
	step [118/147], loss=89.7801
	step [119/147], loss=94.1697
	step [120/147], loss=100.4590
	step [121/147], loss=106.8579
	step [122/147], loss=97.3550
	step [123/147], loss=88.1363
	step [124/147], loss=80.4459
	step [125/147], loss=87.4073
	step [126/147], loss=75.2180
	step [127/147], loss=91.0691
	step [128/147], loss=98.7215
	step [129/147], loss=83.0447
	step [130/147], loss=94.3793
	step [131/147], loss=93.1879
	step [132/147], loss=99.3782
	step [133/147], loss=81.0930
	step [134/147], loss=92.1060
	step [135/147], loss=81.4122
	step [136/147], loss=70.8538
	step [137/147], loss=102.9218
	step [138/147], loss=91.2969
	step [139/147], loss=72.6543
	step [140/147], loss=94.0217
	step [141/147], loss=91.4102
	step [142/147], loss=116.5414
	step [143/147], loss=82.9056
	step [144/147], loss=91.1508
	step [145/147], loss=105.6789
	step [146/147], loss=78.3337
	step [147/147], loss=27.3618
	Evaluating
	loss=0.0168, precision=0.3033, recall=0.8747, f1=0.4504
Training epoch 52
	step [1/147], loss=92.8399
	step [2/147], loss=81.6032
	step [3/147], loss=86.3592
	step [4/147], loss=108.9386
	step [5/147], loss=91.4605
	step [6/147], loss=95.9245
	step [7/147], loss=105.7884
	step [8/147], loss=104.4561
	step [9/147], loss=85.4016
	step [10/147], loss=104.8221
	step [11/147], loss=84.6612
	step [12/147], loss=97.6620
	step [13/147], loss=93.5764
	step [14/147], loss=96.0155
	step [15/147], loss=90.4486
	step [16/147], loss=87.4130
	step [17/147], loss=81.0321
	step [18/147], loss=92.7930
	step [19/147], loss=97.9496
	step [20/147], loss=87.3538
	step [21/147], loss=99.0728
	step [22/147], loss=100.1688
	step [23/147], loss=90.0834
	step [24/147], loss=105.8304
	step [25/147], loss=91.5671
	step [26/147], loss=102.3696
	step [27/147], loss=81.5881
	step [28/147], loss=105.0150
	step [29/147], loss=86.8887
	step [30/147], loss=89.6026
	step [31/147], loss=99.7043
	step [32/147], loss=86.1072
	step [33/147], loss=78.7523
	step [34/147], loss=89.1356
	step [35/147], loss=109.8025
	step [36/147], loss=94.7630
	step [37/147], loss=67.5700
	step [38/147], loss=87.4940
	step [39/147], loss=100.6985
	step [40/147], loss=76.4221
	step [41/147], loss=94.2851
	step [42/147], loss=94.2674
	step [43/147], loss=105.1173
	step [44/147], loss=91.6068
	step [45/147], loss=88.3249
	step [46/147], loss=95.7381
	step [47/147], loss=87.7930
	step [48/147], loss=104.9932
	step [49/147], loss=92.1967
	step [50/147], loss=100.2078
	step [51/147], loss=103.6457
	step [52/147], loss=92.6593
	step [53/147], loss=93.8261
	step [54/147], loss=94.4854
	step [55/147], loss=105.9935
	step [56/147], loss=88.5058
	step [57/147], loss=85.8526
	step [58/147], loss=92.1328
	step [59/147], loss=83.0132
	step [60/147], loss=89.7578
	step [61/147], loss=86.2464
	step [62/147], loss=114.3508
	step [63/147], loss=92.4712
	step [64/147], loss=89.2593
	step [65/147], loss=120.5380
	step [66/147], loss=87.7892
	step [67/147], loss=91.3532
	step [68/147], loss=100.6172
	step [69/147], loss=88.5480
	step [70/147], loss=108.1184
	step [71/147], loss=76.2504
	step [72/147], loss=100.3083
	step [73/147], loss=93.2016
	step [74/147], loss=101.0721
	step [75/147], loss=91.9060
	step [76/147], loss=87.8413
	step [77/147], loss=93.7178
	step [78/147], loss=98.4915
	step [79/147], loss=89.2690
	step [80/147], loss=78.4285
	step [81/147], loss=107.0931
	step [82/147], loss=76.8751
	step [83/147], loss=80.8901
	step [84/147], loss=94.5655
	step [85/147], loss=96.5427
	step [86/147], loss=81.7830
	step [87/147], loss=112.3392
	step [88/147], loss=94.1589
	step [89/147], loss=95.3050
	step [90/147], loss=113.8287
	step [91/147], loss=106.2972
	step [92/147], loss=79.9295
	step [93/147], loss=84.5508
	step [94/147], loss=122.5815
	step [95/147], loss=91.2029
	step [96/147], loss=62.1249
	step [97/147], loss=92.8234
	step [98/147], loss=85.2258
	step [99/147], loss=84.8958
	step [100/147], loss=106.3981
	step [101/147], loss=86.9947
	step [102/147], loss=75.2494
	step [103/147], loss=82.5115
	step [104/147], loss=116.3583
	step [105/147], loss=90.6608
	step [106/147], loss=112.4307
	step [107/147], loss=85.6129
	step [108/147], loss=92.5988
	step [109/147], loss=87.2439
	step [110/147], loss=90.4636
	step [111/147], loss=96.7604
	step [112/147], loss=94.0633
	step [113/147], loss=101.5488
	step [114/147], loss=99.6895
	step [115/147], loss=93.3033
	step [116/147], loss=100.0519
	step [117/147], loss=86.5305
	step [118/147], loss=97.2808
	step [119/147], loss=89.2964
	step [120/147], loss=103.9599
	step [121/147], loss=101.8333
	step [122/147], loss=89.1699
	step [123/147], loss=91.0779
	step [124/147], loss=114.1412
	step [125/147], loss=93.7302
	step [126/147], loss=93.3405
	step [127/147], loss=101.0131
	step [128/147], loss=79.2069
	step [129/147], loss=86.9645
	step [130/147], loss=102.8785
	step [131/147], loss=101.6610
	step [132/147], loss=100.0640
	step [133/147], loss=98.8419
	step [134/147], loss=89.6272
	step [135/147], loss=87.7763
	step [136/147], loss=91.7739
	step [137/147], loss=87.8015
	step [138/147], loss=101.3147
	step [139/147], loss=89.6642
	step [140/147], loss=83.7014
	step [141/147], loss=94.5146
	step [142/147], loss=103.4709
	step [143/147], loss=85.7328
	step [144/147], loss=99.0362
	step [145/147], loss=93.6294
	step [146/147], loss=97.8462
	step [147/147], loss=26.1694
	Evaluating
	loss=0.0141, precision=0.3619, recall=0.8627, f1=0.5099
saving model as: 2_saved_model.pth
Training epoch 53
	step [1/147], loss=95.0719
	step [2/147], loss=103.4226
	step [3/147], loss=84.7757
	step [4/147], loss=88.9529
	step [5/147], loss=101.1347
	step [6/147], loss=107.1868
	step [7/147], loss=95.9993
	step [8/147], loss=101.4565
	step [9/147], loss=95.1135
	step [10/147], loss=96.8466
	step [11/147], loss=84.7838
	step [12/147], loss=96.7350
	step [13/147], loss=91.2686
	step [14/147], loss=95.1219
	step [15/147], loss=84.6726
	step [16/147], loss=99.8274
	step [17/147], loss=111.6108
	step [18/147], loss=93.5350
	step [19/147], loss=108.3993
	step [20/147], loss=74.5648
	step [21/147], loss=100.2252
	step [22/147], loss=87.8505
	step [23/147], loss=89.3592
	step [24/147], loss=104.9823
	step [25/147], loss=91.2229
	step [26/147], loss=97.8682
	step [27/147], loss=85.1374
	step [28/147], loss=76.3691
	step [29/147], loss=93.7369
	step [30/147], loss=84.8429
	step [31/147], loss=94.0607
	step [32/147], loss=97.7636
	step [33/147], loss=95.0488
	step [34/147], loss=118.2732
	step [35/147], loss=89.7853
	step [36/147], loss=78.6675
	step [37/147], loss=91.9005
	step [38/147], loss=86.9143
	step [39/147], loss=89.4429
	step [40/147], loss=91.0621
	step [41/147], loss=86.8985
	step [42/147], loss=101.4991
	step [43/147], loss=101.9926
	step [44/147], loss=90.5187
	step [45/147], loss=93.4359
	step [46/147], loss=105.1302
	step [47/147], loss=103.5977
	step [48/147], loss=83.3384
	step [49/147], loss=86.8606
	step [50/147], loss=109.4544
	step [51/147], loss=99.4979
	step [52/147], loss=96.6708
	step [53/147], loss=98.3903
	step [54/147], loss=93.7552
	step [55/147], loss=81.0105
	step [56/147], loss=80.9112
	step [57/147], loss=84.0187
	step [58/147], loss=88.6118
	step [59/147], loss=103.8351
	step [60/147], loss=85.0816
	step [61/147], loss=91.5641
	step [62/147], loss=100.4583
	step [63/147], loss=103.0721
	step [64/147], loss=79.4541
	step [65/147], loss=85.1663
	step [66/147], loss=81.7754
	step [67/147], loss=94.5736
	step [68/147], loss=89.7011
	step [69/147], loss=96.1948
	step [70/147], loss=93.6952
	step [71/147], loss=93.1859
	step [72/147], loss=91.0235
	step [73/147], loss=104.1176
	step [74/147], loss=90.0703
	step [75/147], loss=80.1134
	step [76/147], loss=87.1448
	step [77/147], loss=102.1142
	step [78/147], loss=88.4510
	step [79/147], loss=93.3003
	step [80/147], loss=88.7966
	step [81/147], loss=81.1864
	step [82/147], loss=91.3929
	step [83/147], loss=97.6609
	step [84/147], loss=98.9366
	step [85/147], loss=89.2136
	step [86/147], loss=83.3043
	step [87/147], loss=96.3009
	step [88/147], loss=95.5582
	step [89/147], loss=96.3049
	step [90/147], loss=94.2362
	step [91/147], loss=94.7015
	step [92/147], loss=100.6352
	step [93/147], loss=89.9199
	step [94/147], loss=80.1687
	step [95/147], loss=87.9823
	step [96/147], loss=80.4092
	step [97/147], loss=85.9274
	step [98/147], loss=124.7403
	step [99/147], loss=85.9445
	step [100/147], loss=108.2483
	step [101/147], loss=97.0835
	step [102/147], loss=104.3074
	step [103/147], loss=100.9127
	step [104/147], loss=89.0618
	step [105/147], loss=85.9375
	step [106/147], loss=81.7189
	step [107/147], loss=95.6420
	step [108/147], loss=103.2632
	step [109/147], loss=95.2810
	step [110/147], loss=98.4492
	step [111/147], loss=97.7853
	step [112/147], loss=95.3966
	step [113/147], loss=98.5993
	step [114/147], loss=105.6395
	step [115/147], loss=90.1009
	step [116/147], loss=96.1797
	step [117/147], loss=118.2270
	step [118/147], loss=76.9634
	step [119/147], loss=101.0927
	step [120/147], loss=106.9781
	step [121/147], loss=94.8164
	step [122/147], loss=75.9402
	step [123/147], loss=104.2325
	step [124/147], loss=100.0816
	step [125/147], loss=70.1257
	step [126/147], loss=91.1407
	step [127/147], loss=87.6269
	step [128/147], loss=94.0306
	step [129/147], loss=98.0226
	step [130/147], loss=104.8824
	step [131/147], loss=86.6743
	step [132/147], loss=89.8953
	step [133/147], loss=89.5167
	step [134/147], loss=81.0920
	step [135/147], loss=103.1279
	step [136/147], loss=95.6776
	step [137/147], loss=73.6646
	step [138/147], loss=84.8422
	step [139/147], loss=95.3583
	step [140/147], loss=85.7709
	step [141/147], loss=95.2871
	step [142/147], loss=94.7318
	step [143/147], loss=70.2867
	step [144/147], loss=81.2831
	step [145/147], loss=91.2799
	step [146/147], loss=92.6257
	step [147/147], loss=17.5514
	Evaluating
	loss=0.0136, precision=0.3722, recall=0.8751, f1=0.5222
saving model as: 2_saved_model.pth
Training epoch 54
	step [1/147], loss=125.3275
	step [2/147], loss=91.5186
	step [3/147], loss=92.3406
	step [4/147], loss=90.2779
	step [5/147], loss=96.6271
	step [6/147], loss=84.6443
	step [7/147], loss=92.2511
	step [8/147], loss=84.6805
	step [9/147], loss=84.6518
	step [10/147], loss=88.6300
	step [11/147], loss=96.5849
	step [12/147], loss=85.5178
	step [13/147], loss=99.1737
	step [14/147], loss=76.6652
	step [15/147], loss=91.0755
	step [16/147], loss=104.8857
	step [17/147], loss=86.8996
	step [18/147], loss=80.1761
	step [19/147], loss=97.2026
	step [20/147], loss=102.1968
	step [21/147], loss=107.1318
	step [22/147], loss=94.8441
	step [23/147], loss=80.0881
	step [24/147], loss=73.0846
	step [25/147], loss=93.5881
	step [26/147], loss=99.3669
	step [27/147], loss=107.2757
	step [28/147], loss=107.5799
	step [29/147], loss=74.1202
	step [30/147], loss=91.2193
	step [31/147], loss=81.7542
	step [32/147], loss=84.0289
	step [33/147], loss=103.0451
	step [34/147], loss=89.9362
	step [35/147], loss=81.8875
	step [36/147], loss=85.5001
	step [37/147], loss=90.4933
	step [38/147], loss=103.9137
	step [39/147], loss=101.0045
	step [40/147], loss=93.9538
	step [41/147], loss=88.8359
	step [42/147], loss=96.5803
	step [43/147], loss=93.1943
	step [44/147], loss=113.2745
	step [45/147], loss=98.5253
	step [46/147], loss=82.4924
	step [47/147], loss=89.9845
	step [48/147], loss=106.2104
	step [49/147], loss=108.1995
	step [50/147], loss=74.6830
	step [51/147], loss=84.6118
	step [52/147], loss=94.2453
	step [53/147], loss=78.6713
	step [54/147], loss=95.1112
	step [55/147], loss=90.3883
	step [56/147], loss=70.6490
	step [57/147], loss=87.8809
	step [58/147], loss=106.5412
	step [59/147], loss=92.6810
	step [60/147], loss=80.2941
	step [61/147], loss=95.4026
	step [62/147], loss=104.5723
	step [63/147], loss=80.1481
	step [64/147], loss=95.1418
	step [65/147], loss=79.9267
	step [66/147], loss=82.8494
	step [67/147], loss=91.5713
	step [68/147], loss=94.8495
	step [69/147], loss=100.8700
	step [70/147], loss=91.7155
	step [71/147], loss=85.8700
	step [72/147], loss=108.7409
	step [73/147], loss=84.2498
	step [74/147], loss=91.1006
	step [75/147], loss=90.5027
	step [76/147], loss=96.5638
	step [77/147], loss=103.5981
	step [78/147], loss=71.0700
	step [79/147], loss=87.6848
	step [80/147], loss=72.4641
	step [81/147], loss=92.5329
	step [82/147], loss=114.2126
	step [83/147], loss=100.3280
	step [84/147], loss=98.3842
	step [85/147], loss=105.2785
	step [86/147], loss=117.6203
	step [87/147], loss=82.7777
	step [88/147], loss=93.8910
	step [89/147], loss=102.7283
	step [90/147], loss=96.9712
	step [91/147], loss=98.9352
	step [92/147], loss=71.3390
	step [93/147], loss=103.6638
	step [94/147], loss=88.3058
	step [95/147], loss=80.3131
	step [96/147], loss=104.6549
	step [97/147], loss=85.3967
	step [98/147], loss=101.3308
	step [99/147], loss=89.1017
	step [100/147], loss=79.6776
	step [101/147], loss=97.2762
	step [102/147], loss=74.3537
	step [103/147], loss=82.7658
	step [104/147], loss=96.9377
	step [105/147], loss=71.5152
	step [106/147], loss=93.1853
	step [107/147], loss=86.7634
	step [108/147], loss=104.3686
	step [109/147], loss=92.9236
	step [110/147], loss=90.6511
	step [111/147], loss=92.0020
	step [112/147], loss=101.6252
	step [113/147], loss=104.3329
	step [114/147], loss=84.9663
	step [115/147], loss=99.6413
	step [116/147], loss=106.4385
	step [117/147], loss=111.9506
	step [118/147], loss=88.9388
	step [119/147], loss=95.3528
	step [120/147], loss=92.3015
	step [121/147], loss=91.3730
	step [122/147], loss=77.7617
	step [123/147], loss=90.9966
	step [124/147], loss=88.2039
	step [125/147], loss=89.5850
	step [126/147], loss=102.7626
	step [127/147], loss=76.1442
	step [128/147], loss=85.6476
	step [129/147], loss=97.3636
	step [130/147], loss=98.1766
	step [131/147], loss=111.3716
	step [132/147], loss=78.3803
	step [133/147], loss=85.6659
	step [134/147], loss=84.5602
	step [135/147], loss=82.2365
	step [136/147], loss=98.4298
	step [137/147], loss=99.4339
	step [138/147], loss=106.7916
	step [139/147], loss=116.8117
	step [140/147], loss=92.0079
	step [141/147], loss=91.5215
	step [142/147], loss=90.8331
	step [143/147], loss=83.6537
	step [144/147], loss=104.0461
	step [145/147], loss=87.2033
	step [146/147], loss=102.5983
	step [147/147], loss=16.1908
	Evaluating
	loss=0.0142, precision=0.3451, recall=0.8740, f1=0.4948
Training epoch 55
	step [1/147], loss=100.5464
	step [2/147], loss=88.7490
	step [3/147], loss=104.6175
	step [4/147], loss=81.1140
	step [5/147], loss=105.4477
	step [6/147], loss=100.0295
	step [7/147], loss=90.0635
	step [8/147], loss=90.1719
	step [9/147], loss=89.1329
	step [10/147], loss=84.8408
	step [11/147], loss=86.5500
	step [12/147], loss=95.2921
	step [13/147], loss=75.7760
	step [14/147], loss=100.2052
	step [15/147], loss=92.4495
	step [16/147], loss=97.1014
	step [17/147], loss=98.4215
	step [18/147], loss=88.8085
	step [19/147], loss=100.1923
	step [20/147], loss=90.1413
	step [21/147], loss=103.9760
	step [22/147], loss=103.1518
	step [23/147], loss=94.4882
	step [24/147], loss=104.8765
	step [25/147], loss=92.0046
	step [26/147], loss=97.4377
	step [27/147], loss=77.3776
	step [28/147], loss=97.8906
	step [29/147], loss=90.9894
	step [30/147], loss=84.8253
	step [31/147], loss=95.0307
	step [32/147], loss=109.4342
	step [33/147], loss=91.6607
	step [34/147], loss=85.3770
	step [35/147], loss=99.5355
	step [36/147], loss=88.4508
	step [37/147], loss=85.8507
	step [38/147], loss=91.3105
	step [39/147], loss=81.8505
	step [40/147], loss=110.1178
	step [41/147], loss=105.8150
	step [42/147], loss=89.9095
	step [43/147], loss=89.0401
	step [44/147], loss=82.2917
	step [45/147], loss=87.8947
	step [46/147], loss=95.2649
	step [47/147], loss=70.9623
	step [48/147], loss=103.8101
	step [49/147], loss=75.9386
	step [50/147], loss=82.2598
	step [51/147], loss=86.8919
	step [52/147], loss=94.8632
	step [53/147], loss=86.6376
	step [54/147], loss=89.7908
	step [55/147], loss=103.4242
	step [56/147], loss=94.6722
	step [57/147], loss=105.2542
	step [58/147], loss=97.6187
	step [59/147], loss=83.9804
	step [60/147], loss=92.1049
	step [61/147], loss=79.6831
	step [62/147], loss=111.9673
	step [63/147], loss=88.6485
	step [64/147], loss=109.4605
	step [65/147], loss=88.8029
	step [66/147], loss=94.8271
	step [67/147], loss=102.0905
	step [68/147], loss=80.5797
	step [69/147], loss=96.2438
	step [70/147], loss=90.3425
	step [71/147], loss=94.6693
	step [72/147], loss=79.3069
	step [73/147], loss=86.1728
	step [74/147], loss=92.8887
	step [75/147], loss=97.3211
	step [76/147], loss=100.7402
	step [77/147], loss=89.6249
	step [78/147], loss=94.9289
	step [79/147], loss=106.5530
	step [80/147], loss=86.0654
	step [81/147], loss=92.1355
	step [82/147], loss=97.0644
	step [83/147], loss=87.7556
	step [84/147], loss=82.3796
	step [85/147], loss=98.0850
	step [86/147], loss=97.5773
	step [87/147], loss=83.1868
	step [88/147], loss=99.9750
	step [89/147], loss=88.8424
	step [90/147], loss=91.0733
	step [91/147], loss=96.3314
	step [92/147], loss=78.4692
	step [93/147], loss=100.2399
	step [94/147], loss=95.8880
	step [95/147], loss=78.7655
	step [96/147], loss=96.0472
	step [97/147], loss=88.7462
	step [98/147], loss=86.3562
	step [99/147], loss=101.7074
	step [100/147], loss=89.6808
	step [101/147], loss=96.7007
	step [102/147], loss=84.9186
	step [103/147], loss=94.8863
	step [104/147], loss=94.7934
	step [105/147], loss=102.3135
	step [106/147], loss=66.7140
	step [107/147], loss=102.6998
	step [108/147], loss=92.5944
	step [109/147], loss=100.6708
	step [110/147], loss=86.9458
	step [111/147], loss=85.2507
	step [112/147], loss=91.3579
	step [113/147], loss=79.6022
	step [114/147], loss=93.6758
	step [115/147], loss=81.9075
	step [116/147], loss=106.4282
	step [117/147], loss=84.9636
	step [118/147], loss=90.5222
	step [119/147], loss=80.8194
	step [120/147], loss=83.3868
	step [121/147], loss=93.1768
	step [122/147], loss=100.5326
	step [123/147], loss=88.3114
	step [124/147], loss=91.0063
	step [125/147], loss=87.7685
	step [126/147], loss=82.3313
	step [127/147], loss=104.9703
	step [128/147], loss=95.5548
	step [129/147], loss=98.2805
	step [130/147], loss=95.8162
	step [131/147], loss=85.1479
	step [132/147], loss=91.5474
	step [133/147], loss=100.3420
	step [134/147], loss=94.1888
	step [135/147], loss=92.6562
	step [136/147], loss=100.3102
	step [137/147], loss=85.8427
	step [138/147], loss=76.9172
	step [139/147], loss=98.3999
	step [140/147], loss=68.6135
	step [141/147], loss=92.7339
	step [142/147], loss=99.5683
	step [143/147], loss=79.6288
	step [144/147], loss=94.4895
	step [145/147], loss=98.0115
	step [146/147], loss=108.2903
	step [147/147], loss=20.8140
	Evaluating
	loss=0.0175, precision=0.2889, recall=0.8804, f1=0.4350
Training epoch 56
	step [1/147], loss=99.3600
	step [2/147], loss=92.3318
	step [3/147], loss=72.8907
	step [4/147], loss=83.9161
	step [5/147], loss=82.0774
	step [6/147], loss=105.2415
	step [7/147], loss=87.4175
	step [8/147], loss=108.3394
	step [9/147], loss=88.0650
	step [10/147], loss=72.4709
	step [11/147], loss=77.4647
	step [12/147], loss=94.0650
	step [13/147], loss=94.8859
	step [14/147], loss=80.5876
	step [15/147], loss=93.4168
	step [16/147], loss=100.5241
	step [17/147], loss=91.5966
	step [18/147], loss=92.0863
	step [19/147], loss=103.9749
	step [20/147], loss=89.4065
	step [21/147], loss=83.6002
	step [22/147], loss=102.4011
	step [23/147], loss=79.2163
	step [24/147], loss=83.0131
	step [25/147], loss=94.5378
	step [26/147], loss=85.0471
	step [27/147], loss=97.8157
	step [28/147], loss=97.0237
	step [29/147], loss=87.9777
	step [30/147], loss=96.6531
	step [31/147], loss=78.1364
	step [32/147], loss=83.0041
	step [33/147], loss=85.8910
	step [34/147], loss=109.5048
	step [35/147], loss=90.8675
	step [36/147], loss=86.3881
	step [37/147], loss=101.0387
	step [38/147], loss=83.4324
	step [39/147], loss=92.4032
	step [40/147], loss=98.5101
	step [41/147], loss=85.8853
	step [42/147], loss=81.4619
	step [43/147], loss=82.7915
	step [44/147], loss=104.0845
	step [45/147], loss=108.8431
	step [46/147], loss=89.2281
	step [47/147], loss=87.6072
	step [48/147], loss=79.5547
	step [49/147], loss=95.6422
	step [50/147], loss=93.0463
	step [51/147], loss=88.3430
	step [52/147], loss=101.0894
	step [53/147], loss=95.0976
	step [54/147], loss=92.1419
	step [55/147], loss=91.9510
	step [56/147], loss=82.9896
	step [57/147], loss=97.1982
	step [58/147], loss=96.3615
	step [59/147], loss=90.6405
	step [60/147], loss=103.0309
	step [61/147], loss=101.2376
	step [62/147], loss=75.7969
	step [63/147], loss=84.1461
	step [64/147], loss=98.3333
	step [65/147], loss=88.0040
	step [66/147], loss=81.2919
	step [67/147], loss=78.1142
	step [68/147], loss=96.3977
	step [69/147], loss=88.4591
	step [70/147], loss=76.3801
	step [71/147], loss=101.5350
	step [72/147], loss=94.0573
	step [73/147], loss=91.3488
	step [74/147], loss=111.7023
	step [75/147], loss=111.2613
	step [76/147], loss=73.6653
	step [77/147], loss=96.1370
	step [78/147], loss=98.4355
	step [79/147], loss=80.7558
	step [80/147], loss=87.8355
	step [81/147], loss=87.7652
	step [82/147], loss=89.9002
	step [83/147], loss=97.0657
	step [84/147], loss=99.0703
	step [85/147], loss=77.9346
	step [86/147], loss=81.7367
	step [87/147], loss=82.9876
	step [88/147], loss=103.3432
	step [89/147], loss=108.7087
	step [90/147], loss=90.7840
	step [91/147], loss=86.8900
	step [92/147], loss=86.0583
	step [93/147], loss=91.8583
	step [94/147], loss=111.0512
	step [95/147], loss=112.9896
	step [96/147], loss=107.8694
	step [97/147], loss=92.5511
	step [98/147], loss=87.6339
	step [99/147], loss=99.8463
	step [100/147], loss=96.1791
	step [101/147], loss=90.9720
	step [102/147], loss=99.5293
	step [103/147], loss=90.9475
	step [104/147], loss=90.9651
	step [105/147], loss=88.1876
	step [106/147], loss=93.4370
	step [107/147], loss=89.9585
	step [108/147], loss=91.4423
	step [109/147], loss=90.6133
	step [110/147], loss=88.4431
	step [111/147], loss=98.7179
	step [112/147], loss=86.4347
	step [113/147], loss=112.9601
	step [114/147], loss=72.2574
	step [115/147], loss=76.2552
	step [116/147], loss=106.5848
	step [117/147], loss=88.0247
	step [118/147], loss=84.9940
	step [119/147], loss=79.9716
	step [120/147], loss=83.3496
	step [121/147], loss=95.1741
	step [122/147], loss=83.7163
	step [123/147], loss=79.0885
	step [124/147], loss=78.0604
	step [125/147], loss=85.1530
	step [126/147], loss=87.3936
	step [127/147], loss=84.4475
	step [128/147], loss=98.3370
	step [129/147], loss=82.7706
	step [130/147], loss=96.1644
	step [131/147], loss=99.0142
	step [132/147], loss=90.6026
	step [133/147], loss=94.6880
	step [134/147], loss=96.3713
	step [135/147], loss=96.3669
	step [136/147], loss=102.2254
	step [137/147], loss=92.2360
	step [138/147], loss=91.3710
	step [139/147], loss=79.2748
	step [140/147], loss=95.7189
	step [141/147], loss=98.6048
	step [142/147], loss=94.2455
	step [143/147], loss=97.7281
	step [144/147], loss=104.7877
	step [145/147], loss=105.2999
	step [146/147], loss=83.9936
	step [147/147], loss=16.6581
	Evaluating
	loss=0.0189, precision=0.2696, recall=0.8995, f1=0.4149
Training epoch 57
	step [1/147], loss=96.8548
	step [2/147], loss=85.7695
	step [3/147], loss=101.2753
	step [4/147], loss=103.0760
	step [5/147], loss=86.1685
	step [6/147], loss=84.4423
	step [7/147], loss=87.6213
	step [8/147], loss=87.5684
	step [9/147], loss=89.5297
	step [10/147], loss=89.4789
	step [11/147], loss=102.9948
	step [12/147], loss=83.2281
	step [13/147], loss=88.7519
	step [14/147], loss=92.5692
	step [15/147], loss=106.7383
	step [16/147], loss=87.0269
	step [17/147], loss=79.5330
	step [18/147], loss=110.6963
	step [19/147], loss=97.4946
	step [20/147], loss=97.0072
	step [21/147], loss=100.7832
	step [22/147], loss=101.8745
	step [23/147], loss=95.2473
	step [24/147], loss=86.8267
	step [25/147], loss=120.5572
	step [26/147], loss=75.4399
	step [27/147], loss=75.7822
	step [28/147], loss=91.3830
	step [29/147], loss=73.7677
	step [30/147], loss=84.9209
	step [31/147], loss=98.3370
	step [32/147], loss=87.1036
	step [33/147], loss=89.5497
	step [34/147], loss=103.4062
	step [35/147], loss=75.3359
	step [36/147], loss=90.6068
	step [37/147], loss=94.8202
	step [38/147], loss=105.8964
	step [39/147], loss=78.4117
	step [40/147], loss=89.1178
	step [41/147], loss=92.0613
	step [42/147], loss=87.4007
	step [43/147], loss=102.6112
	step [44/147], loss=83.1563
	step [45/147], loss=94.2227
	step [46/147], loss=109.6342
	step [47/147], loss=89.3414
	step [48/147], loss=91.2505
	step [49/147], loss=98.8529
	step [50/147], loss=104.6606
	step [51/147], loss=85.6144
	step [52/147], loss=95.9793
	step [53/147], loss=78.4285
	step [54/147], loss=112.6759
	step [55/147], loss=90.1314
	step [56/147], loss=92.1387
	step [57/147], loss=88.9902
	step [58/147], loss=91.3210
	step [59/147], loss=95.1719
	step [60/147], loss=83.6940
	step [61/147], loss=104.5844
	step [62/147], loss=95.6313
	step [63/147], loss=93.8394
	step [64/147], loss=89.7802
	step [65/147], loss=91.4602
	step [66/147], loss=98.6413
	step [67/147], loss=106.7841
	step [68/147], loss=78.0325
	step [69/147], loss=91.3039
	step [70/147], loss=96.6676
	step [71/147], loss=93.4407
	step [72/147], loss=112.1423
	step [73/147], loss=91.9634
	step [74/147], loss=82.2853
	step [75/147], loss=90.7738
	step [76/147], loss=104.6823
	step [77/147], loss=107.6748
	step [78/147], loss=92.2501
	step [79/147], loss=90.0450
	step [80/147], loss=97.8725
	step [81/147], loss=85.9459
	step [82/147], loss=74.7814
	step [83/147], loss=95.5408
	step [84/147], loss=89.8567
	step [85/147], loss=106.1258
	step [86/147], loss=93.9904
	step [87/147], loss=85.5458
	step [88/147], loss=75.9599
	step [89/147], loss=82.9495
	step [90/147], loss=81.3612
	step [91/147], loss=75.4757
	step [92/147], loss=97.4522
	step [93/147], loss=103.9151
	step [94/147], loss=95.4432
	step [95/147], loss=97.8626
	step [96/147], loss=102.9705
	step [97/147], loss=99.5675
	step [98/147], loss=85.4119
	step [99/147], loss=93.1246
	step [100/147], loss=85.3150
	step [101/147], loss=90.9068
	step [102/147], loss=112.6975
	step [103/147], loss=83.0554
	step [104/147], loss=79.9750
	step [105/147], loss=98.5765
	step [106/147], loss=80.1253
	step [107/147], loss=94.7281
	step [108/147], loss=80.6654
	step [109/147], loss=87.6578
	step [110/147], loss=92.7063
	step [111/147], loss=75.7435
	step [112/147], loss=86.0593
	step [113/147], loss=122.6497
	step [114/147], loss=90.7000
	step [115/147], loss=87.2320
	step [116/147], loss=72.8976
	step [117/147], loss=97.7240
	step [118/147], loss=87.5383
	step [119/147], loss=93.3506
	step [120/147], loss=90.3193
	step [121/147], loss=93.7782
	step [122/147], loss=87.7822
	step [123/147], loss=81.3756
	step [124/147], loss=94.0799
	step [125/147], loss=88.6093
	step [126/147], loss=105.0744
	step [127/147], loss=96.5248
	step [128/147], loss=78.9722
	step [129/147], loss=79.9647
	step [130/147], loss=82.3972
	step [131/147], loss=85.2371
	step [132/147], loss=94.1270
	step [133/147], loss=91.6391
	step [134/147], loss=86.0680
	step [135/147], loss=86.9427
	step [136/147], loss=97.1738
	step [137/147], loss=79.1442
	step [138/147], loss=71.8376
	step [139/147], loss=81.0540
	step [140/147], loss=93.1432
	step [141/147], loss=76.8043
	step [142/147], loss=82.2867
	step [143/147], loss=85.4906
	step [144/147], loss=105.3039
	step [145/147], loss=103.3448
	step [146/147], loss=83.9974
	step [147/147], loss=19.0993
	Evaluating
	loss=0.0164, precision=0.2999, recall=0.8625, f1=0.4451
Training epoch 58
	step [1/147], loss=90.1100
	step [2/147], loss=98.6524
	step [3/147], loss=90.8028
	step [4/147], loss=91.2616
	step [5/147], loss=88.1352
	step [6/147], loss=101.6828
	step [7/147], loss=81.9810
	step [8/147], loss=76.0523
	step [9/147], loss=75.6683
	step [10/147], loss=92.3614
	step [11/147], loss=100.5647
	step [12/147], loss=102.8415
	step [13/147], loss=104.4229
	step [14/147], loss=95.9502
	step [15/147], loss=85.9339
	step [16/147], loss=80.0197
	step [17/147], loss=85.9745
	step [18/147], loss=88.9677
	step [19/147], loss=97.7969
	step [20/147], loss=89.1453
	step [21/147], loss=84.0338
	step [22/147], loss=83.6962
	step [23/147], loss=97.5915
	step [24/147], loss=102.6275
	step [25/147], loss=78.8846
	step [26/147], loss=86.4596
	step [27/147], loss=91.1559
	step [28/147], loss=94.6126
	step [29/147], loss=84.3003
	step [30/147], loss=88.8690
	step [31/147], loss=87.1418
	step [32/147], loss=74.3704
	step [33/147], loss=97.0193
	step [34/147], loss=79.0805
	step [35/147], loss=98.2372
	step [36/147], loss=78.6314
	step [37/147], loss=101.5906
	step [38/147], loss=112.9731
	step [39/147], loss=78.3239
	step [40/147], loss=81.5210
	step [41/147], loss=94.6861
	step [42/147], loss=94.4814
	step [43/147], loss=78.2314
	step [44/147], loss=77.3456
	step [45/147], loss=107.8579
	step [46/147], loss=83.7993
	step [47/147], loss=96.8940
	step [48/147], loss=79.6017
	step [49/147], loss=83.4298
	step [50/147], loss=92.8173
	step [51/147], loss=98.0724
	step [52/147], loss=95.3546
	step [53/147], loss=87.3291
	step [54/147], loss=86.2612
	step [55/147], loss=100.2568
	step [56/147], loss=91.9131
	step [57/147], loss=84.6558
	step [58/147], loss=94.3933
	step [59/147], loss=83.5529
	step [60/147], loss=94.5840
	step [61/147], loss=91.0267
	step [62/147], loss=72.8401
	step [63/147], loss=93.7839
	step [64/147], loss=99.4421
	step [65/147], loss=89.2176
	step [66/147], loss=110.2318
	step [67/147], loss=94.4976
	step [68/147], loss=89.4529
	step [69/147], loss=112.4704
	step [70/147], loss=103.0376
	step [71/147], loss=83.7343
	step [72/147], loss=97.2017
	step [73/147], loss=97.4882
	step [74/147], loss=78.0340
	step [75/147], loss=100.9633
	step [76/147], loss=90.3533
	step [77/147], loss=77.1361
	step [78/147], loss=84.8999
	step [79/147], loss=100.8990
	step [80/147], loss=105.3611
	step [81/147], loss=82.7584
	step [82/147], loss=93.4279
	step [83/147], loss=90.4661
	step [84/147], loss=85.2042
	step [85/147], loss=74.7726
	step [86/147], loss=98.7752
	step [87/147], loss=87.1269
	step [88/147], loss=96.4823
	step [89/147], loss=83.5016
	step [90/147], loss=82.2036
	step [91/147], loss=88.0479
	step [92/147], loss=93.5382
	step [93/147], loss=99.4988
	step [94/147], loss=102.0218
	step [95/147], loss=78.4588
	step [96/147], loss=103.9192
	step [97/147], loss=108.4122
	step [98/147], loss=81.8347
	step [99/147], loss=105.0534
	step [100/147], loss=97.7584
	step [101/147], loss=82.4697
	step [102/147], loss=83.1437
	step [103/147], loss=97.0453
	step [104/147], loss=88.1910
	step [105/147], loss=94.6961
	step [106/147], loss=84.5974
	step [107/147], loss=106.4094
	step [108/147], loss=79.0113
	step [109/147], loss=95.9167
	step [110/147], loss=74.2979
	step [111/147], loss=89.4259
	step [112/147], loss=96.4589
	step [113/147], loss=104.9196
	step [114/147], loss=96.8879
	step [115/147], loss=92.6406
	step [116/147], loss=96.3140
	step [117/147], loss=87.3565
	step [118/147], loss=85.5760
	step [119/147], loss=82.8730
	step [120/147], loss=80.6952
	step [121/147], loss=93.7690
	step [122/147], loss=107.6153
	step [123/147], loss=101.0770
	step [124/147], loss=98.7730
	step [125/147], loss=88.4940
	step [126/147], loss=82.2061
	step [127/147], loss=98.4508
	step [128/147], loss=90.5148
	step [129/147], loss=101.5409
	step [130/147], loss=109.9927
	step [131/147], loss=81.1432
	step [132/147], loss=100.4679
	step [133/147], loss=94.3294
	step [134/147], loss=77.2424
	step [135/147], loss=85.4473
	step [136/147], loss=89.8329
	step [137/147], loss=92.8487
	step [138/147], loss=67.2921
	step [139/147], loss=77.6584
	step [140/147], loss=87.0522
	step [141/147], loss=114.0931
	step [142/147], loss=74.5179
	step [143/147], loss=88.0733
	step [144/147], loss=97.2426
	step [145/147], loss=93.5414
	step [146/147], loss=72.8108
	step [147/147], loss=20.9892
	Evaluating
	loss=0.0149, precision=0.3306, recall=0.8841, f1=0.4813
Training epoch 59
	step [1/147], loss=94.3496
	step [2/147], loss=119.2525
	step [3/147], loss=84.8069
	step [4/147], loss=89.6260
	step [5/147], loss=93.1251
	step [6/147], loss=98.8851
	step [7/147], loss=99.4959
	step [8/147], loss=83.3524
	step [9/147], loss=94.9267
	step [10/147], loss=89.8184
	step [11/147], loss=102.7602
	step [12/147], loss=101.1186
	step [13/147], loss=81.4681
	step [14/147], loss=77.8490
	step [15/147], loss=79.9579
	step [16/147], loss=106.3442
	step [17/147], loss=87.4891
	step [18/147], loss=102.3355
	step [19/147], loss=97.8588
	step [20/147], loss=79.5819
	step [21/147], loss=83.4111
	step [22/147], loss=77.4625
	step [23/147], loss=83.5681
	step [24/147], loss=83.8718
	step [25/147], loss=97.0242
	step [26/147], loss=76.7190
	step [27/147], loss=92.3443
	step [28/147], loss=80.0551
	step [29/147], loss=83.5385
	step [30/147], loss=83.1413
	step [31/147], loss=92.6173
	step [32/147], loss=89.5944
	step [33/147], loss=83.2720
	step [34/147], loss=87.8334
	step [35/147], loss=81.0574
	step [36/147], loss=82.7129
	step [37/147], loss=92.3912
	step [38/147], loss=84.5846
	step [39/147], loss=84.2318
	step [40/147], loss=86.0103
	step [41/147], loss=81.1959
	step [42/147], loss=91.3564
	step [43/147], loss=79.6758
	step [44/147], loss=100.2879
	step [45/147], loss=93.7015
	step [46/147], loss=93.9856
	step [47/147], loss=90.1078
	step [48/147], loss=88.3103
	step [49/147], loss=72.4105
	step [50/147], loss=95.2594
	step [51/147], loss=94.4542
	step [52/147], loss=80.4409
	step [53/147], loss=89.0343
	step [54/147], loss=111.1708
	step [55/147], loss=85.0054
	step [56/147], loss=89.3126
	step [57/147], loss=96.0436
	step [58/147], loss=107.0924
	step [59/147], loss=73.1381
	step [60/147], loss=90.5782
	step [61/147], loss=103.1799
	step [62/147], loss=78.2034
	step [63/147], loss=106.8930
	step [64/147], loss=91.7838
	step [65/147], loss=89.6959
	step [66/147], loss=83.5397
	step [67/147], loss=95.9866
	step [68/147], loss=86.7109
	step [69/147], loss=107.4675
	step [70/147], loss=102.9847
	step [71/147], loss=98.2117
	step [72/147], loss=95.0887
	step [73/147], loss=80.3451
	step [74/147], loss=91.4222
	step [75/147], loss=87.0596
	step [76/147], loss=94.1621
	step [77/147], loss=95.0840
	step [78/147], loss=97.6951
	step [79/147], loss=95.1941
	step [80/147], loss=101.7275
	step [81/147], loss=83.7591
	step [82/147], loss=103.9349
	step [83/147], loss=89.3976
	step [84/147], loss=85.8613
	step [85/147], loss=76.1956
	step [86/147], loss=91.3431
	step [87/147], loss=90.4121
	step [88/147], loss=103.0040
	step [89/147], loss=99.0689
	step [90/147], loss=72.6380
	step [91/147], loss=98.9328
	step [92/147], loss=81.8256
	step [93/147], loss=92.3022
	step [94/147], loss=95.0635
	step [95/147], loss=93.0919
	step [96/147], loss=93.6768
	step [97/147], loss=83.1427
	step [98/147], loss=102.4459
	step [99/147], loss=91.1930
	step [100/147], loss=89.6608
	step [101/147], loss=78.2864
	step [102/147], loss=99.9558
	step [103/147], loss=92.5636
	step [104/147], loss=98.8941
	step [105/147], loss=94.2276
	step [106/147], loss=109.9690
	step [107/147], loss=93.5246
	step [108/147], loss=97.6423
	step [109/147], loss=68.1718
	step [110/147], loss=93.0843
	step [111/147], loss=97.6079
	step [112/147], loss=90.0277
	step [113/147], loss=92.5440
	step [114/147], loss=110.3590
	step [115/147], loss=90.7482
	step [116/147], loss=100.1648
	step [117/147], loss=91.1645
	step [118/147], loss=76.4013
	step [119/147], loss=97.2219
	step [120/147], loss=94.3542
	step [121/147], loss=95.8108
	step [122/147], loss=92.2179
	step [123/147], loss=81.1505
	step [124/147], loss=79.5259
	step [125/147], loss=74.8084
	step [126/147], loss=92.6409
	step [127/147], loss=95.5557
	step [128/147], loss=89.5395
	step [129/147], loss=81.1112
	step [130/147], loss=77.7847
	step [131/147], loss=87.4874
	step [132/147], loss=91.2862
	step [133/147], loss=91.1255
	step [134/147], loss=93.1975
	step [135/147], loss=86.8065
	step [136/147], loss=70.0616
	step [137/147], loss=85.7240
	step [138/147], loss=83.0523
	step [139/147], loss=75.6544
	step [140/147], loss=93.6368
	step [141/147], loss=88.1370
	step [142/147], loss=109.3804
	step [143/147], loss=100.3847
	step [144/147], loss=77.7693
	step [145/147], loss=97.8228
	step [146/147], loss=87.0514
	step [147/147], loss=18.0163
	Evaluating
	loss=0.0162, precision=0.3175, recall=0.8783, f1=0.4664
Training epoch 60
	step [1/147], loss=85.7607
	step [2/147], loss=92.4436
	step [3/147], loss=68.3465
	step [4/147], loss=80.8875
	step [5/147], loss=94.6468
	step [6/147], loss=86.5648
	step [7/147], loss=83.2539
	step [8/147], loss=87.5989
	step [9/147], loss=89.3946
	step [10/147], loss=102.5104
	step [11/147], loss=91.1173
	step [12/147], loss=80.6684
	step [13/147], loss=97.2773
	step [14/147], loss=94.1461
	step [15/147], loss=96.4563
	step [16/147], loss=84.5968
	step [17/147], loss=94.8078
	step [18/147], loss=90.0210
	step [19/147], loss=91.6142
	step [20/147], loss=102.5909
	step [21/147], loss=91.4294
	step [22/147], loss=108.1062
	step [23/147], loss=94.4869
	step [24/147], loss=93.7307
	step [25/147], loss=84.4139
	step [26/147], loss=89.1760
	step [27/147], loss=71.5344
	step [28/147], loss=100.3458
	step [29/147], loss=81.0800
	step [30/147], loss=98.2023
	step [31/147], loss=80.8626
	step [32/147], loss=93.2829
	step [33/147], loss=101.5139
	step [34/147], loss=83.9987
	step [35/147], loss=101.0926
	step [36/147], loss=66.6751
	step [37/147], loss=102.7159
	step [38/147], loss=81.4574
	step [39/147], loss=80.1262
	step [40/147], loss=94.5411
	step [41/147], loss=84.2005
	step [42/147], loss=77.7401
	step [43/147], loss=78.8668
	step [44/147], loss=98.3395
	step [45/147], loss=97.8228
	step [46/147], loss=86.9402
	step [47/147], loss=97.4716
	step [48/147], loss=90.3039
	step [49/147], loss=97.9197
	step [50/147], loss=94.6491
	step [51/147], loss=82.4701
	step [52/147], loss=86.2585
	step [53/147], loss=96.1934
	step [54/147], loss=95.7077
	step [55/147], loss=80.4191
	step [56/147], loss=89.0085
	step [57/147], loss=80.7801
	step [58/147], loss=91.7227
	step [59/147], loss=86.1538
	step [60/147], loss=98.9511
	step [61/147], loss=78.9352
	step [62/147], loss=93.8771
	step [63/147], loss=96.2457
	step [64/147], loss=97.4484
	step [65/147], loss=88.4572
	step [66/147], loss=87.1340
	step [67/147], loss=81.0457
	step [68/147], loss=77.8356
	step [69/147], loss=84.5288
	step [70/147], loss=86.2016
	step [71/147], loss=98.4365
	step [72/147], loss=84.6339
	step [73/147], loss=96.9955
	step [74/147], loss=90.7606
	step [75/147], loss=80.2451
	step [76/147], loss=95.5144
	step [77/147], loss=79.4344
	step [78/147], loss=92.1519
	step [79/147], loss=81.4263
	step [80/147], loss=85.5875
	step [81/147], loss=79.4959
	step [82/147], loss=98.9351
	step [83/147], loss=110.4475
	step [84/147], loss=91.8810
	step [85/147], loss=89.0915
	step [86/147], loss=85.2419
	step [87/147], loss=101.6839
	step [88/147], loss=86.6660
	step [89/147], loss=84.7878
	step [90/147], loss=95.3811
	step [91/147], loss=90.7375
	step [92/147], loss=87.2801
	step [93/147], loss=73.1360
	step [94/147], loss=102.2962
	step [95/147], loss=99.9067
	step [96/147], loss=76.9339
	step [97/147], loss=81.9011
	step [98/147], loss=95.9395
	step [99/147], loss=84.3409
	step [100/147], loss=95.8953
	step [101/147], loss=87.6966
	step [102/147], loss=85.8435
	step [103/147], loss=82.3680
	step [104/147], loss=93.4868
	step [105/147], loss=88.7261
	step [106/147], loss=101.9877
	step [107/147], loss=101.0653
	step [108/147], loss=88.3077
	step [109/147], loss=92.8293
	step [110/147], loss=90.5185
	step [111/147], loss=93.9828
	step [112/147], loss=99.1880
	step [113/147], loss=83.4570
	step [114/147], loss=103.3954
	step [115/147], loss=88.9497
	step [116/147], loss=76.6588
	step [117/147], loss=94.2968
	step [118/147], loss=91.0606
	step [119/147], loss=79.7718
	step [120/147], loss=82.5961
	step [121/147], loss=93.1158
	step [122/147], loss=91.2449
	step [123/147], loss=96.4330
	step [124/147], loss=88.0952
	step [125/147], loss=106.1608
	step [126/147], loss=97.4458
	step [127/147], loss=86.7795
	step [128/147], loss=89.1788
	step [129/147], loss=78.9115
	step [130/147], loss=97.0155
	step [131/147], loss=93.2151
	step [132/147], loss=99.5229
	step [133/147], loss=85.4345
	step [134/147], loss=86.3968
	step [135/147], loss=90.3262
	step [136/147], loss=101.7162
	step [137/147], loss=77.1183
	step [138/147], loss=99.7659
	step [139/147], loss=100.9539
	step [140/147], loss=86.6570
	step [141/147], loss=78.4701
	step [142/147], loss=110.7556
	step [143/147], loss=101.7733
	step [144/147], loss=71.1926
	step [145/147], loss=92.7076
	step [146/147], loss=88.4353
	step [147/147], loss=18.0132
	Evaluating
	loss=0.0136, precision=0.3610, recall=0.8674, f1=0.5098
Training epoch 61
	step [1/147], loss=102.0802
	step [2/147], loss=91.7125
	step [3/147], loss=76.2310
	step [4/147], loss=81.9796
	step [5/147], loss=84.4688
	step [6/147], loss=97.1247
	step [7/147], loss=94.0305
	step [8/147], loss=74.2263
	step [9/147], loss=99.7968
	step [10/147], loss=70.1039
	step [11/147], loss=81.7902
	step [12/147], loss=91.6136
	step [13/147], loss=79.2267
	step [14/147], loss=94.1648
	step [15/147], loss=82.1969
	step [16/147], loss=91.6962
	step [17/147], loss=85.1100
	step [18/147], loss=91.4075
	step [19/147], loss=81.9620
	step [20/147], loss=85.6593
	step [21/147], loss=81.1201
	step [22/147], loss=93.1693
	step [23/147], loss=85.6068
	step [24/147], loss=97.5884
	step [25/147], loss=96.9281
	step [26/147], loss=99.7553
	step [27/147], loss=89.4492
	step [28/147], loss=80.6069
	step [29/147], loss=90.5983
	step [30/147], loss=88.6506
	step [31/147], loss=83.9167
	step [32/147], loss=96.9168
	step [33/147], loss=93.6238
	step [34/147], loss=84.6758
	step [35/147], loss=98.8876
	step [36/147], loss=95.4499
	step [37/147], loss=86.7721
	step [38/147], loss=84.0802
	step [39/147], loss=81.0304
	step [40/147], loss=97.4769
	step [41/147], loss=80.1803
	step [42/147], loss=79.7344
	step [43/147], loss=91.8516
	step [44/147], loss=94.0151
	step [45/147], loss=81.4033
	step [46/147], loss=78.2638
	step [47/147], loss=76.4228
	step [48/147], loss=79.6307
	step [49/147], loss=96.5264
	step [50/147], loss=87.7504
	step [51/147], loss=103.7594
	step [52/147], loss=87.7284
	step [53/147], loss=119.8781
	step [54/147], loss=72.5180
	step [55/147], loss=106.7381
	step [56/147], loss=83.7444
	step [57/147], loss=84.7663
	step [58/147], loss=92.5269
	step [59/147], loss=76.7239
	step [60/147], loss=83.9287
	step [61/147], loss=68.7813
	step [62/147], loss=81.8440
	step [63/147], loss=111.3527
	step [64/147], loss=92.3413
	step [65/147], loss=74.7168
	step [66/147], loss=99.7514
	step [67/147], loss=83.2947
	step [68/147], loss=84.4937
	step [69/147], loss=91.8681
	step [70/147], loss=72.9605
	step [71/147], loss=105.2841
	step [72/147], loss=90.0003
	step [73/147], loss=92.0344
	step [74/147], loss=95.7156
	step [75/147], loss=80.4040
	step [76/147], loss=89.4630
	step [77/147], loss=97.9397
	step [78/147], loss=108.2622
	step [79/147], loss=98.7451
	step [80/147], loss=75.3605
	step [81/147], loss=99.8656
	step [82/147], loss=90.5110
	step [83/147], loss=81.1748
	step [84/147], loss=96.1852
	step [85/147], loss=87.8343
	step [86/147], loss=94.9622
	step [87/147], loss=82.3234
	step [88/147], loss=96.1853
	step [89/147], loss=99.0492
	step [90/147], loss=74.2047
	step [91/147], loss=87.7086
	step [92/147], loss=96.2725
	step [93/147], loss=85.8590
	step [94/147], loss=74.7900
	step [95/147], loss=72.5013
	step [96/147], loss=83.7575
	step [97/147], loss=75.5347
	step [98/147], loss=95.0109
	step [99/147], loss=88.4199
	step [100/147], loss=103.5948
	step [101/147], loss=82.0626
	step [102/147], loss=90.6145
	step [103/147], loss=79.6275
	step [104/147], loss=83.7266
	step [105/147], loss=94.4317
	step [106/147], loss=82.0165
	step [107/147], loss=77.5695
	step [108/147], loss=87.1726
	step [109/147], loss=88.5163
	step [110/147], loss=91.4490
	step [111/147], loss=91.3440
	step [112/147], loss=92.5195
	step [113/147], loss=105.4601
	step [114/147], loss=102.4226
	step [115/147], loss=94.5562
	step [116/147], loss=101.8977
	step [117/147], loss=110.4603
	step [118/147], loss=103.6805
	step [119/147], loss=97.3724
	step [120/147], loss=103.0144
	step [121/147], loss=104.7445
	step [122/147], loss=93.6007
	step [123/147], loss=82.5432
	step [124/147], loss=110.7051
	step [125/147], loss=93.8534
	step [126/147], loss=100.5775
	step [127/147], loss=89.8033
	step [128/147], loss=115.7958
	step [129/147], loss=110.8819
	step [130/147], loss=93.0758
	step [131/147], loss=87.9236
	step [132/147], loss=77.3258
	step [133/147], loss=85.8216
	step [134/147], loss=80.4203
	step [135/147], loss=87.8939
	step [136/147], loss=85.5068
	step [137/147], loss=85.1809
	step [138/147], loss=89.7336
	step [139/147], loss=92.3955
	step [140/147], loss=102.6972
	step [141/147], loss=84.2379
	step [142/147], loss=83.1696
	step [143/147], loss=77.0763
	step [144/147], loss=84.3608
	step [145/147], loss=89.1565
	step [146/147], loss=92.4447
	step [147/147], loss=13.9323
	Evaluating
	loss=0.0176, precision=0.2873, recall=0.8806, f1=0.4332
Training epoch 62
	step [1/147], loss=88.5975
	step [2/147], loss=90.7329
	step [3/147], loss=82.5906
	step [4/147], loss=81.7428
	step [5/147], loss=78.3920
	step [6/147], loss=104.7207
	step [7/147], loss=81.3652
	step [8/147], loss=79.2202
	step [9/147], loss=77.4487
	step [10/147], loss=97.0131
	step [11/147], loss=76.4133
	step [12/147], loss=95.9722
	step [13/147], loss=93.6105
	step [14/147], loss=89.2101
	step [15/147], loss=79.6949
	step [16/147], loss=80.5530
	step [17/147], loss=89.0055
	step [18/147], loss=95.5876
	step [19/147], loss=83.9726
	step [20/147], loss=105.5754
	step [21/147], loss=105.2136
	step [22/147], loss=106.7636
	step [23/147], loss=96.8217
	step [24/147], loss=92.4159
	step [25/147], loss=83.6877
	step [26/147], loss=87.3195
	step [27/147], loss=83.8842
	step [28/147], loss=92.6332
	step [29/147], loss=105.5399
	step [30/147], loss=83.6685
	step [31/147], loss=92.6329
	step [32/147], loss=82.5972
	step [33/147], loss=88.0923
	step [34/147], loss=71.9859
	step [35/147], loss=97.2220
	step [36/147], loss=107.0166
	step [37/147], loss=79.9624
	step [38/147], loss=91.7873
	step [39/147], loss=97.0111
	step [40/147], loss=90.8039
	step [41/147], loss=78.8024
	step [42/147], loss=77.9110
	step [43/147], loss=81.4747
	step [44/147], loss=88.4378
	step [45/147], loss=99.6817
	step [46/147], loss=82.6148
	step [47/147], loss=72.3935
	step [48/147], loss=92.8652
	step [49/147], loss=93.9084
	step [50/147], loss=96.9074
	step [51/147], loss=106.8312
	step [52/147], loss=98.3743
	step [53/147], loss=93.2814
	step [54/147], loss=89.8812
	step [55/147], loss=83.1216
	step [56/147], loss=77.2899
	step [57/147], loss=106.9423
	step [58/147], loss=97.8245
	step [59/147], loss=86.7931
	step [60/147], loss=95.3757
	step [61/147], loss=79.7904
	step [62/147], loss=98.0681
	step [63/147], loss=92.9503
	step [64/147], loss=99.3970
	step [65/147], loss=85.3821
	step [66/147], loss=97.8027
	step [67/147], loss=88.8554
	step [68/147], loss=101.9081
	step [69/147], loss=116.6019
	step [70/147], loss=84.8886
	step [71/147], loss=85.4234
	step [72/147], loss=84.4742
	step [73/147], loss=98.5037
	step [74/147], loss=81.3281
	step [75/147], loss=90.2023
	step [76/147], loss=104.2537
	step [77/147], loss=86.1987
	step [78/147], loss=89.2362
	step [79/147], loss=89.6859
	step [80/147], loss=83.3887
	step [81/147], loss=103.3782
	step [82/147], loss=88.8661
	step [83/147], loss=76.6967
	step [84/147], loss=81.4947
	step [85/147], loss=81.0397
	step [86/147], loss=87.4814
	step [87/147], loss=95.7256
	step [88/147], loss=93.1362
	step [89/147], loss=89.5401
	step [90/147], loss=85.8162
	step [91/147], loss=86.7708
	step [92/147], loss=82.4768
	step [93/147], loss=89.0331
	step [94/147], loss=85.2516
	step [95/147], loss=98.5016
	step [96/147], loss=82.1175
	step [97/147], loss=105.0747
	step [98/147], loss=95.9816
	step [99/147], loss=88.4019
	step [100/147], loss=73.3363
	step [101/147], loss=83.0586
	step [102/147], loss=78.3836
	step [103/147], loss=98.1959
	step [104/147], loss=89.8932
	step [105/147], loss=75.6514
	step [106/147], loss=111.3959
	step [107/147], loss=83.0999
	step [108/147], loss=71.1221
	step [109/147], loss=87.1381
	step [110/147], loss=78.2636
	step [111/147], loss=76.5510
	step [112/147], loss=86.5085
	step [113/147], loss=104.6295
	step [114/147], loss=98.7720
	step [115/147], loss=83.7027
	step [116/147], loss=91.7998
	step [117/147], loss=86.3414
	step [118/147], loss=98.3173
	step [119/147], loss=108.1204
	step [120/147], loss=97.9545
	step [121/147], loss=97.6467
	step [122/147], loss=76.0049
	step [123/147], loss=86.2696
	step [124/147], loss=77.8733
	step [125/147], loss=102.7529
	step [126/147], loss=82.8368
	step [127/147], loss=84.4863
	step [128/147], loss=99.0059
	step [129/147], loss=86.3379
	step [130/147], loss=87.2330
	step [131/147], loss=80.1199
	step [132/147], loss=88.7543
	step [133/147], loss=85.9869
	step [134/147], loss=99.0371
	step [135/147], loss=78.8583
	step [136/147], loss=81.9522
	step [137/147], loss=91.4557
	step [138/147], loss=75.7887
	step [139/147], loss=80.0737
	step [140/147], loss=104.5838
	step [141/147], loss=95.2542
	step [142/147], loss=88.4684
	step [143/147], loss=82.4850
	step [144/147], loss=83.9255
	step [145/147], loss=84.9971
	step [146/147], loss=96.3025
	step [147/147], loss=17.5046
	Evaluating
	loss=0.0126, precision=0.3872, recall=0.8766, f1=0.5372
saving model as: 2_saved_model.pth
Training epoch 63
	step [1/147], loss=68.7758
	step [2/147], loss=89.8801
	step [3/147], loss=91.6332
	step [4/147], loss=78.9151
	step [5/147], loss=91.6920
	step [6/147], loss=91.5202
	step [7/147], loss=99.9296
	step [8/147], loss=104.2624
	step [9/147], loss=76.9112
	step [10/147], loss=79.8400
	step [11/147], loss=94.4001
	step [12/147], loss=65.5167
	step [13/147], loss=95.7973
	step [14/147], loss=92.2940
	step [15/147], loss=74.7327
	step [16/147], loss=86.4259
	step [17/147], loss=103.1840
	step [18/147], loss=100.3982
	step [19/147], loss=87.4359
	step [20/147], loss=92.8981
	step [21/147], loss=88.0919
	step [22/147], loss=70.0626
	step [23/147], loss=95.7526
	step [24/147], loss=99.9245
	step [25/147], loss=71.9417
	step [26/147], loss=72.5263
	step [27/147], loss=92.9891
	step [28/147], loss=73.0163
	step [29/147], loss=77.0642
	step [30/147], loss=104.7909
	step [31/147], loss=92.6394
	step [32/147], loss=85.5814
	step [33/147], loss=110.8472
	step [34/147], loss=80.0696
	step [35/147], loss=106.9350
	step [36/147], loss=95.5109
	step [37/147], loss=75.6388
	step [38/147], loss=88.2374
	step [39/147], loss=85.7578
	step [40/147], loss=90.1549
	step [41/147], loss=79.2622
	step [42/147], loss=114.2621
	step [43/147], loss=87.5104
	step [44/147], loss=90.9556
	step [45/147], loss=81.0722
	step [46/147], loss=99.2890
	step [47/147], loss=104.5961
	step [48/147], loss=81.6461
	step [49/147], loss=90.5939
	step [50/147], loss=85.0995
	step [51/147], loss=78.1575
	step [52/147], loss=86.7017
	step [53/147], loss=94.4131
	step [54/147], loss=92.6733
	step [55/147], loss=96.7857
	step [56/147], loss=81.0874
	step [57/147], loss=94.4127
	step [58/147], loss=85.1856
	step [59/147], loss=93.4404
	step [60/147], loss=85.9900
	step [61/147], loss=97.5014
	step [62/147], loss=93.9476
	step [63/147], loss=107.4914
	step [64/147], loss=84.8870
	step [65/147], loss=111.8578
	step [66/147], loss=84.7205
	step [67/147], loss=93.9963
	step [68/147], loss=101.8846
	step [69/147], loss=85.0513
	step [70/147], loss=87.3958
	step [71/147], loss=95.2197
	step [72/147], loss=76.3862
	step [73/147], loss=94.1312
	step [74/147], loss=82.9491
	step [75/147], loss=98.4024
	step [76/147], loss=84.2473
	step [77/147], loss=84.4000
	step [78/147], loss=102.7748
	step [79/147], loss=99.8515
	step [80/147], loss=82.0683
	step [81/147], loss=72.1021
	step [82/147], loss=104.8619
	step [83/147], loss=83.2858
	step [84/147], loss=90.5537
	step [85/147], loss=96.9936
	step [86/147], loss=81.9730
	step [87/147], loss=81.0527
	step [88/147], loss=103.1787
	step [89/147], loss=80.1909
	step [90/147], loss=93.5426
	step [91/147], loss=90.3983
	step [92/147], loss=94.9473
	step [93/147], loss=76.3614
	step [94/147], loss=94.4513
	step [95/147], loss=81.4931
	step [96/147], loss=76.1505
	step [97/147], loss=80.6547
	step [98/147], loss=100.7032
	step [99/147], loss=87.5513
	step [100/147], loss=91.3676
	step [101/147], loss=100.0200
	step [102/147], loss=89.7667
	step [103/147], loss=106.8491
	step [104/147], loss=84.1405
	step [105/147], loss=94.0891
	step [106/147], loss=94.1005
	step [107/147], loss=80.8269
	step [108/147], loss=77.5960
	step [109/147], loss=109.8221
	step [110/147], loss=81.6956
	step [111/147], loss=81.4847
	step [112/147], loss=93.6983
	step [113/147], loss=113.9596
	step [114/147], loss=96.5233
	step [115/147], loss=80.7825
	step [116/147], loss=75.2931
	step [117/147], loss=86.6979
	step [118/147], loss=77.7191
	step [119/147], loss=92.3466
	step [120/147], loss=80.3154
	step [121/147], loss=76.6005
	step [122/147], loss=76.4155
	step [123/147], loss=94.8959
	step [124/147], loss=81.3789
	step [125/147], loss=103.3064
	step [126/147], loss=84.8045
	step [127/147], loss=94.4591
	step [128/147], loss=86.4919
	step [129/147], loss=86.5264
	step [130/147], loss=93.4954
	step [131/147], loss=85.0227
	step [132/147], loss=91.9327
	step [133/147], loss=84.1530
	step [134/147], loss=86.7167
	step [135/147], loss=86.9046
	step [136/147], loss=88.6241
	step [137/147], loss=96.8268
	step [138/147], loss=95.6241
	step [139/147], loss=84.6254
	step [140/147], loss=83.5642
	step [141/147], loss=92.6319
	step [142/147], loss=89.3543
	step [143/147], loss=71.5203
	step [144/147], loss=89.0029
	step [145/147], loss=89.6278
	step [146/147], loss=103.4970
	step [147/147], loss=22.1550
	Evaluating
	loss=0.0146, precision=0.3353, recall=0.8931, f1=0.4875
Training epoch 64
	step [1/147], loss=75.7713
	step [2/147], loss=106.4277
	step [3/147], loss=90.4935
	step [4/147], loss=80.4498
	step [5/147], loss=83.2898
	step [6/147], loss=68.2658
	step [7/147], loss=99.1079
	step [8/147], loss=80.3156
	step [9/147], loss=96.7043
	step [10/147], loss=81.6300
	step [11/147], loss=101.4461
	step [12/147], loss=101.5308
	step [13/147], loss=102.4612
	step [14/147], loss=89.9271
	step [15/147], loss=90.2493
	step [16/147], loss=80.5803
	step [17/147], loss=73.5434
	step [18/147], loss=87.6702
	step [19/147], loss=93.8239
	step [20/147], loss=89.4256
	step [21/147], loss=103.7734
	step [22/147], loss=87.1303
	step [23/147], loss=68.2649
	step [24/147], loss=76.3179
	step [25/147], loss=82.4560
	step [26/147], loss=85.7038
	step [27/147], loss=95.1641
	step [28/147], loss=89.8055
	step [29/147], loss=81.2843
	step [30/147], loss=95.4743
	step [31/147], loss=96.3742
	step [32/147], loss=87.5941
	step [33/147], loss=70.4573
	step [34/147], loss=99.9877
	step [35/147], loss=85.7401
	step [36/147], loss=89.7922
	step [37/147], loss=89.4501
	step [38/147], loss=85.3163
	step [39/147], loss=80.9448
	step [40/147], loss=80.4287
	step [41/147], loss=82.3682
	step [42/147], loss=84.3120
	step [43/147], loss=83.2343
	step [44/147], loss=81.4666
	step [45/147], loss=94.0278
	step [46/147], loss=84.1822
	step [47/147], loss=99.7303
	step [48/147], loss=74.8133
	step [49/147], loss=99.8852
	step [50/147], loss=81.2949
	step [51/147], loss=92.6816
	step [52/147], loss=86.6799
	step [53/147], loss=97.4048
	step [54/147], loss=99.9899
	step [55/147], loss=87.1491
	step [56/147], loss=100.3065
	step [57/147], loss=94.4294
	step [58/147], loss=101.9959
	step [59/147], loss=88.4254
	step [60/147], loss=89.9570
	step [61/147], loss=90.2827
	step [62/147], loss=87.4005
	step [63/147], loss=88.2080
	step [64/147], loss=77.4583
	step [65/147], loss=96.6229
	step [66/147], loss=94.4975
	step [67/147], loss=100.0679
	step [68/147], loss=103.9842
	step [69/147], loss=96.1992
	step [70/147], loss=90.5427
	step [71/147], loss=91.9489
	step [72/147], loss=94.4294
	step [73/147], loss=102.4951
	step [74/147], loss=77.4218
	step [75/147], loss=89.7500
	step [76/147], loss=86.4659
	step [77/147], loss=81.7838
	step [78/147], loss=79.2911
	step [79/147], loss=102.5059
	step [80/147], loss=91.8154
	step [81/147], loss=85.6191
	step [82/147], loss=108.1349
	step [83/147], loss=85.0640
	step [84/147], loss=99.3607
	step [85/147], loss=70.0775
	step [86/147], loss=81.8223
	step [87/147], loss=72.7517
	step [88/147], loss=83.9401
	step [89/147], loss=97.8147
	step [90/147], loss=90.9603
	step [91/147], loss=85.7947
	step [92/147], loss=87.8380
	step [93/147], loss=79.4446
	step [94/147], loss=98.6020
	step [95/147], loss=83.6768
	step [96/147], loss=86.5732
	step [97/147], loss=101.0579
	step [98/147], loss=79.0194
	step [99/147], loss=89.2792
	step [100/147], loss=79.5792
	step [101/147], loss=81.8027
	step [102/147], loss=101.0170
	step [103/147], loss=82.4158
	step [104/147], loss=92.7536
	step [105/147], loss=71.6482
	step [106/147], loss=87.3803
	step [107/147], loss=108.5722
	step [108/147], loss=89.7732
	step [109/147], loss=90.5993
	step [110/147], loss=97.6578
	step [111/147], loss=83.1089
	step [112/147], loss=105.4185
	step [113/147], loss=80.3171
	step [114/147], loss=77.2653
	step [115/147], loss=85.6644
	step [116/147], loss=93.6465
	step [117/147], loss=82.9637
	step [118/147], loss=104.7825
	step [119/147], loss=96.1977
	step [120/147], loss=73.5840
	step [121/147], loss=88.0718
	step [122/147], loss=89.6315
	step [123/147], loss=98.1468
	step [124/147], loss=74.8982
	step [125/147], loss=84.3430
	step [126/147], loss=88.8691
	step [127/147], loss=88.3274
	step [128/147], loss=86.8183
	step [129/147], loss=81.7228
	step [130/147], loss=74.8963
	step [131/147], loss=93.3371
	step [132/147], loss=98.4302
	step [133/147], loss=76.5714
	step [134/147], loss=89.8478
	step [135/147], loss=76.4124
	step [136/147], loss=82.0754
	step [137/147], loss=93.7360
	step [138/147], loss=74.5290
	step [139/147], loss=103.8007
	step [140/147], loss=92.0242
	step [141/147], loss=86.3007
	step [142/147], loss=86.2728
	step [143/147], loss=86.3635
	step [144/147], loss=90.4539
	step [145/147], loss=93.4000
	step [146/147], loss=82.5341
	step [147/147], loss=25.1259
	Evaluating
	loss=0.0132, precision=0.3700, recall=0.8860, f1=0.5220
Training epoch 65
	step [1/147], loss=87.4272
	step [2/147], loss=105.3307
	step [3/147], loss=68.0018
	step [4/147], loss=108.2564
	step [5/147], loss=107.0482
	step [6/147], loss=76.0525
	step [7/147], loss=94.5198
	step [8/147], loss=90.9164
	step [9/147], loss=106.8541
	step [10/147], loss=94.5972
	step [11/147], loss=82.0663
	step [12/147], loss=84.6842
	step [13/147], loss=81.2748
	step [14/147], loss=94.2924
	step [15/147], loss=94.6032
	step [16/147], loss=80.8775
	step [17/147], loss=94.0943
	step [18/147], loss=89.6194
	step [19/147], loss=103.9931
	step [20/147], loss=80.7833
	step [21/147], loss=80.5711
	step [22/147], loss=97.8406
	step [23/147], loss=85.5196
	step [24/147], loss=85.2478
	step [25/147], loss=82.2024
	step [26/147], loss=94.3997
	step [27/147], loss=92.6730
	step [28/147], loss=80.2935
	step [29/147], loss=94.6748
	step [30/147], loss=72.5454
	step [31/147], loss=83.0350
	step [32/147], loss=79.5256
	step [33/147], loss=99.9738
	step [34/147], loss=83.6065
	step [35/147], loss=91.7566
	step [36/147], loss=88.5157
	step [37/147], loss=72.0976
	step [38/147], loss=77.5514
	step [39/147], loss=83.6860
	step [40/147], loss=100.8882
	step [41/147], loss=76.8141
	step [42/147], loss=99.7712
	step [43/147], loss=83.0044
	step [44/147], loss=91.1384
	step [45/147], loss=87.9831
	step [46/147], loss=81.7391
	step [47/147], loss=91.9683
	step [48/147], loss=70.8456
	step [49/147], loss=82.8784
	step [50/147], loss=96.9480
	step [51/147], loss=88.7917
	step [52/147], loss=81.8114
	step [53/147], loss=76.8139
	step [54/147], loss=95.7237
	step [55/147], loss=95.6970
	step [56/147], loss=95.7138
	step [57/147], loss=80.7069
	step [58/147], loss=85.2389
	step [59/147], loss=88.1362
	step [60/147], loss=79.8943
	step [61/147], loss=82.8481
	step [62/147], loss=102.7404
	step [63/147], loss=98.9669
	step [64/147], loss=94.3078
	step [65/147], loss=84.5024
	step [66/147], loss=86.2976
	step [67/147], loss=74.4165
	step [68/147], loss=111.1019
	step [69/147], loss=83.2069
	step [70/147], loss=85.7175
	step [71/147], loss=78.7946
	step [72/147], loss=79.4748
	step [73/147], loss=71.7926
	step [74/147], loss=87.1185
	step [75/147], loss=83.9543
	step [76/147], loss=87.3510
	step [77/147], loss=70.8767
	step [78/147], loss=85.1505
	step [79/147], loss=96.1938
	step [80/147], loss=83.4673
	step [81/147], loss=97.2521
	step [82/147], loss=97.3618
	step [83/147], loss=88.0976
	step [84/147], loss=93.7012
	step [85/147], loss=74.6698
	step [86/147], loss=84.6607
	step [87/147], loss=99.7237
	step [88/147], loss=87.7446
	step [89/147], loss=78.7446
	step [90/147], loss=75.3170
	step [91/147], loss=101.7818
	step [92/147], loss=81.6794
	step [93/147], loss=88.7334
	step [94/147], loss=78.0725
	step [95/147], loss=82.6129
	step [96/147], loss=78.9729
	step [97/147], loss=98.6558
	step [98/147], loss=71.1491
	step [99/147], loss=58.9428
	step [100/147], loss=98.3730
	step [101/147], loss=83.7477
	step [102/147], loss=78.5201
	step [103/147], loss=86.6409
	step [104/147], loss=101.6240
	step [105/147], loss=97.3489
	step [106/147], loss=98.9455
	step [107/147], loss=86.3128
	step [108/147], loss=86.0042
	step [109/147], loss=96.4871
	step [110/147], loss=86.8210
	step [111/147], loss=92.3662
	step [112/147], loss=86.3031
	step [113/147], loss=73.3968
	step [114/147], loss=97.2482
	step [115/147], loss=94.2366
	step [116/147], loss=91.0284
	step [117/147], loss=97.7343
	step [118/147], loss=94.7616
	step [119/147], loss=102.3217
	step [120/147], loss=96.8365
	step [121/147], loss=77.6096
	step [122/147], loss=92.0565
	step [123/147], loss=97.6006
	step [124/147], loss=100.7868
	step [125/147], loss=93.4521
	step [126/147], loss=90.1254
	step [127/147], loss=96.9800
	step [128/147], loss=89.0365
	step [129/147], loss=92.8450
	step [130/147], loss=100.7285
	step [131/147], loss=90.4096
	step [132/147], loss=88.1914
	step [133/147], loss=86.5007
	step [134/147], loss=91.7710
	step [135/147], loss=88.9730
	step [136/147], loss=91.4870
	step [137/147], loss=85.6387
	step [138/147], loss=99.7653
	step [139/147], loss=82.1417
	step [140/147], loss=97.2988
	step [141/147], loss=76.6273
	step [142/147], loss=106.1231
	step [143/147], loss=102.9558
	step [144/147], loss=80.7337
	step [145/147], loss=82.0934
	step [146/147], loss=83.1399
	step [147/147], loss=32.4689
	Evaluating
	loss=0.0129, precision=0.3772, recall=0.8669, f1=0.5257
Training epoch 66
	step [1/147], loss=82.4786
	step [2/147], loss=82.5017
	step [3/147], loss=94.3468
	step [4/147], loss=94.0920
	step [5/147], loss=104.4793
	step [6/147], loss=82.9000
	step [7/147], loss=90.8369
	step [8/147], loss=106.1744
	step [9/147], loss=90.5505
	step [10/147], loss=92.3312
	step [11/147], loss=80.0759
	step [12/147], loss=90.0730
	step [13/147], loss=99.2035
	step [14/147], loss=100.9567
	step [15/147], loss=90.7122
	step [16/147], loss=84.4330
	step [17/147], loss=86.1973
	step [18/147], loss=81.9707
	step [19/147], loss=91.4869
	step [20/147], loss=120.4016
	step [21/147], loss=81.0516
	step [22/147], loss=79.9714
	step [23/147], loss=92.3931
	step [24/147], loss=71.5943
	step [25/147], loss=88.0988
	step [26/147], loss=91.7504
	step [27/147], loss=81.4004
	step [28/147], loss=88.0576
	step [29/147], loss=85.3476
	step [30/147], loss=79.7293
	step [31/147], loss=75.9752
	step [32/147], loss=83.5152
	step [33/147], loss=91.3860
	step [34/147], loss=84.8650
	step [35/147], loss=87.2774
	step [36/147], loss=80.9201
	step [37/147], loss=89.0989
	step [38/147], loss=71.6540
	step [39/147], loss=94.3914
	step [40/147], loss=78.4173
	step [41/147], loss=85.8302
	step [42/147], loss=88.1492
	step [43/147], loss=83.6079
	step [44/147], loss=87.7167
	step [45/147], loss=103.7934
	step [46/147], loss=111.0612
	step [47/147], loss=75.3388
	step [48/147], loss=69.8152
	step [49/147], loss=93.9206
	step [50/147], loss=84.3318
	step [51/147], loss=90.3646
	step [52/147], loss=69.2998
	step [53/147], loss=82.8131
	step [54/147], loss=84.1916
	step [55/147], loss=87.2930
	step [56/147], loss=84.2534
	step [57/147], loss=115.1365
	step [58/147], loss=74.5167
	step [59/147], loss=92.7330
	step [60/147], loss=104.2767
	step [61/147], loss=88.9097
	step [62/147], loss=90.3663
	step [63/147], loss=81.1694
	step [64/147], loss=75.6221
	step [65/147], loss=100.5708
	step [66/147], loss=90.3122
	step [67/147], loss=78.9397
	step [68/147], loss=75.1651
	step [69/147], loss=96.2946
	step [70/147], loss=105.4323
	step [71/147], loss=96.9746
	step [72/147], loss=93.4086
	step [73/147], loss=83.1161
	step [74/147], loss=79.8887
	step [75/147], loss=87.6029
	step [76/147], loss=86.1080
	step [77/147], loss=110.1342
	step [78/147], loss=88.7545
	step [79/147], loss=82.3978
	step [80/147], loss=87.0292
	step [81/147], loss=88.7877
	step [82/147], loss=110.4347
	step [83/147], loss=76.8960
	step [84/147], loss=84.5599
	step [85/147], loss=80.0223
	step [86/147], loss=91.1969
	step [87/147], loss=100.2743
	step [88/147], loss=88.2837
	step [89/147], loss=91.4965
	step [90/147], loss=97.0450
	step [91/147], loss=81.2067
	step [92/147], loss=74.4991
	step [93/147], loss=81.5690
	step [94/147], loss=90.7791
	step [95/147], loss=87.9020
	step [96/147], loss=108.8852
	step [97/147], loss=92.0224
	step [98/147], loss=87.8518
	step [99/147], loss=85.2748
	step [100/147], loss=86.2139
	step [101/147], loss=84.2932
	step [102/147], loss=90.3605
	step [103/147], loss=73.3530
	step [104/147], loss=89.2175
	step [105/147], loss=93.8869
	step [106/147], loss=77.8800
	step [107/147], loss=79.9032
	step [108/147], loss=77.9824
	step [109/147], loss=104.0247
	step [110/147], loss=106.6803
	step [111/147], loss=93.9997
	step [112/147], loss=89.2735
	step [113/147], loss=84.0138
	step [114/147], loss=85.6026
	step [115/147], loss=79.3505
	step [116/147], loss=78.2746
	step [117/147], loss=106.0472
	step [118/147], loss=82.9249
	step [119/147], loss=90.1558
	step [120/147], loss=98.4559
	step [121/147], loss=75.6567
	step [122/147], loss=93.3861
	step [123/147], loss=79.7144
	step [124/147], loss=75.1750
	step [125/147], loss=101.1380
	step [126/147], loss=99.5018
	step [127/147], loss=70.2484
	step [128/147], loss=86.0302
	step [129/147], loss=84.4317
	step [130/147], loss=81.8785
	step [131/147], loss=87.4800
	step [132/147], loss=93.2087
	step [133/147], loss=86.8231
	step [134/147], loss=88.1715
	step [135/147], loss=97.9359
	step [136/147], loss=76.0294
	step [137/147], loss=66.6282
	step [138/147], loss=82.2580
	step [139/147], loss=77.7194
	step [140/147], loss=99.2660
	step [141/147], loss=104.8516
	step [142/147], loss=84.7716
	step [143/147], loss=105.2106
	step [144/147], loss=103.7712
	step [145/147], loss=82.4128
	step [146/147], loss=97.8673
	step [147/147], loss=18.3109
	Evaluating
	loss=0.0147, precision=0.3337, recall=0.8842, f1=0.4846
Training epoch 67
	step [1/147], loss=90.3121
	step [2/147], loss=97.3075
	step [3/147], loss=97.7507
	step [4/147], loss=85.0248
	step [5/147], loss=77.5548
	step [6/147], loss=88.3068
	step [7/147], loss=116.7507
	step [8/147], loss=78.0061
	step [9/147], loss=93.1969
	step [10/147], loss=75.4478
	step [11/147], loss=84.1540
	step [12/147], loss=85.4380
	step [13/147], loss=91.6061
	step [14/147], loss=86.4822
	step [15/147], loss=89.4733
	step [16/147], loss=80.0217
	step [17/147], loss=86.4598
	step [18/147], loss=84.9892
	step [19/147], loss=80.0501
	step [20/147], loss=88.8225
	step [21/147], loss=89.6630
	step [22/147], loss=86.1941
	step [23/147], loss=96.8087
	step [24/147], loss=93.2418
	step [25/147], loss=91.1013
	step [26/147], loss=91.2854
	step [27/147], loss=96.1875
	step [28/147], loss=71.4977
	step [29/147], loss=91.1656
	step [30/147], loss=83.4428
	step [31/147], loss=89.7863
	step [32/147], loss=69.2335
	step [33/147], loss=73.8934
	step [34/147], loss=79.6799
	step [35/147], loss=104.0428
	step [36/147], loss=95.9421
	step [37/147], loss=99.0529
	step [38/147], loss=81.7982
	step [39/147], loss=90.2819
	step [40/147], loss=86.7152
	step [41/147], loss=95.8131
	step [42/147], loss=94.6000
	step [43/147], loss=75.5037
	step [44/147], loss=85.5748
	step [45/147], loss=77.1297
	step [46/147], loss=75.2417
	step [47/147], loss=87.8215
	step [48/147], loss=77.6379
	step [49/147], loss=90.0009
	step [50/147], loss=105.6665
	step [51/147], loss=77.5591
	step [52/147], loss=102.4152
	step [53/147], loss=85.1344
	step [54/147], loss=73.2191
	step [55/147], loss=87.5585
	step [56/147], loss=92.1490
	step [57/147], loss=88.4899
	step [58/147], loss=70.0051
	step [59/147], loss=85.9212
	step [60/147], loss=85.2406
	step [61/147], loss=91.2829
	step [62/147], loss=85.9430
	step [63/147], loss=75.4388
	step [64/147], loss=69.9919
	step [65/147], loss=82.1195
	step [66/147], loss=84.3188
	step [67/147], loss=91.3384
	step [68/147], loss=103.4239
	step [69/147], loss=75.1000
	step [70/147], loss=96.2125
	step [71/147], loss=97.4316
	step [72/147], loss=84.2923
	step [73/147], loss=91.2602
	step [74/147], loss=90.4933
	step [75/147], loss=92.8777
	step [76/147], loss=97.8322
	step [77/147], loss=95.5345
	step [78/147], loss=91.6903
	step [79/147], loss=86.8367
	step [80/147], loss=93.2357
	step [81/147], loss=80.1818
	step [82/147], loss=96.9969
	step [83/147], loss=78.4288
	step [84/147], loss=101.7986
	step [85/147], loss=86.4197
	step [86/147], loss=82.6706
	step [87/147], loss=71.0217
	step [88/147], loss=106.0196
	step [89/147], loss=94.9694
	step [90/147], loss=82.3397
	step [91/147], loss=101.5044
	step [92/147], loss=81.2296
	step [93/147], loss=83.9146
	step [94/147], loss=97.1618
	step [95/147], loss=90.5258
	step [96/147], loss=83.3996
	step [97/147], loss=83.0725
	step [98/147], loss=96.5203
	step [99/147], loss=85.7342
	step [100/147], loss=82.8156
	step [101/147], loss=90.7386
	step [102/147], loss=79.1878
	step [103/147], loss=79.4368
	step [104/147], loss=90.5984
	step [105/147], loss=84.1384
	step [106/147], loss=89.3327
	step [107/147], loss=77.3828
	step [108/147], loss=87.3899
	step [109/147], loss=87.4231
	step [110/147], loss=79.8182
	step [111/147], loss=81.0945
	step [112/147], loss=90.7071
	step [113/147], loss=74.7479
	step [114/147], loss=90.7407
	step [115/147], loss=82.2404
	step [116/147], loss=89.0561
	step [117/147], loss=99.3032
	step [118/147], loss=93.9007
	step [119/147], loss=85.4422
	step [120/147], loss=90.2669
	step [121/147], loss=87.3311
	step [122/147], loss=73.5230
	step [123/147], loss=82.1977
	step [124/147], loss=78.8904
	step [125/147], loss=75.9804
	step [126/147], loss=101.1488
	step [127/147], loss=80.6877
	step [128/147], loss=88.6135
	step [129/147], loss=76.6924
	step [130/147], loss=88.1549
	step [131/147], loss=93.0706
	step [132/147], loss=95.6001
	step [133/147], loss=96.0918
	step [134/147], loss=95.5294
	step [135/147], loss=91.6777
	step [136/147], loss=91.5502
	step [137/147], loss=98.2412
	step [138/147], loss=91.6224
	step [139/147], loss=77.8057
	step [140/147], loss=81.8397
	step [141/147], loss=99.7572
	step [142/147], loss=104.2823
	step [143/147], loss=94.4518
	step [144/147], loss=84.7679
	step [145/147], loss=85.3707
	step [146/147], loss=81.9259
	step [147/147], loss=23.7073
	Evaluating
	loss=0.0126, precision=0.3832, recall=0.8684, f1=0.5318
Training epoch 68
	step [1/147], loss=98.9342
	step [2/147], loss=82.3110
	step [3/147], loss=73.9575
	step [4/147], loss=83.6453
	step [5/147], loss=72.7679
	step [6/147], loss=93.4896
	step [7/147], loss=93.4390
	step [8/147], loss=94.6463
	step [9/147], loss=87.8492
	step [10/147], loss=76.5048
	step [11/147], loss=72.3867
	step [12/147], loss=88.9267
	step [13/147], loss=88.4108
	step [14/147], loss=83.8466
	step [15/147], loss=69.9828
	step [16/147], loss=79.4682
	step [17/147], loss=89.8820
	step [18/147], loss=84.1102
	step [19/147], loss=78.0388
	step [20/147], loss=83.8422
	step [21/147], loss=91.0076
	step [22/147], loss=96.5849
	step [23/147], loss=79.3749
	step [24/147], loss=79.5188
	step [25/147], loss=71.2090
	step [26/147], loss=84.6441
	step [27/147], loss=93.7930
	step [28/147], loss=93.8449
	step [29/147], loss=77.6323
	step [30/147], loss=89.6230
	step [31/147], loss=91.4069
	step [32/147], loss=103.9879
	step [33/147], loss=78.4991
	step [34/147], loss=92.6516
	step [35/147], loss=89.7781
	step [36/147], loss=82.6614
	step [37/147], loss=73.8936
	step [38/147], loss=100.8185
	step [39/147], loss=81.5111
	step [40/147], loss=91.9127
	step [41/147], loss=96.8859
	step [42/147], loss=90.5266
	step [43/147], loss=95.1310
	step [44/147], loss=81.5835
	step [45/147], loss=94.2419
	step [46/147], loss=86.5984
	step [47/147], loss=108.6227
	step [48/147], loss=80.8352
	step [49/147], loss=86.2909
	step [50/147], loss=83.1153
	step [51/147], loss=81.6523
	step [52/147], loss=85.6318
	step [53/147], loss=76.2718
	step [54/147], loss=77.1268
	step [55/147], loss=85.2901
	step [56/147], loss=86.5255
	step [57/147], loss=88.3699
	step [58/147], loss=77.2465
	step [59/147], loss=95.4224
	step [60/147], loss=92.3253
	step [61/147], loss=89.6358
	step [62/147], loss=73.6751
	step [63/147], loss=71.2677
	step [64/147], loss=81.6874
	step [65/147], loss=100.4603
	step [66/147], loss=112.5424
	step [67/147], loss=78.4029
	step [68/147], loss=79.0621
	step [69/147], loss=79.2736
	step [70/147], loss=88.4439
	step [71/147], loss=96.9051
	step [72/147], loss=81.6831
	step [73/147], loss=99.4333
	step [74/147], loss=87.8845
	step [75/147], loss=100.9014
	step [76/147], loss=92.0663
	step [77/147], loss=84.7665
	step [78/147], loss=105.1825
	step [79/147], loss=80.6688
	step [80/147], loss=89.3068
	step [81/147], loss=74.9271
	step [82/147], loss=100.9286
	step [83/147], loss=115.8195
	step [84/147], loss=81.1178
	step [85/147], loss=84.5071
	step [86/147], loss=83.5331
	step [87/147], loss=88.8548
	step [88/147], loss=106.2020
	step [89/147], loss=90.5482
	step [90/147], loss=91.5669
	step [91/147], loss=81.4299
	step [92/147], loss=94.5412
	step [93/147], loss=92.5527
	step [94/147], loss=84.2954
	step [95/147], loss=95.9261
	step [96/147], loss=79.2688
	step [97/147], loss=103.0064
	step [98/147], loss=83.4712
	step [99/147], loss=79.5616
	step [100/147], loss=91.6468
	step [101/147], loss=93.0589
	step [102/147], loss=97.0839
	step [103/147], loss=74.2988
	step [104/147], loss=88.4597
	step [105/147], loss=108.8964
	step [106/147], loss=91.9682
	step [107/147], loss=86.7822
	step [108/147], loss=92.0072
	step [109/147], loss=88.2336
	step [110/147], loss=77.6208
	step [111/147], loss=85.2888
	step [112/147], loss=76.1041
	step [113/147], loss=93.0573
	step [114/147], loss=77.6443
	step [115/147], loss=92.1414
	step [116/147], loss=92.6914
	step [117/147], loss=97.1259
	step [118/147], loss=83.6424
	step [119/147], loss=90.6463
	step [120/147], loss=98.3762
	step [121/147], loss=73.0939
	step [122/147], loss=94.3746
	step [123/147], loss=89.0308
	step [124/147], loss=82.2705
	step [125/147], loss=79.3894
	step [126/147], loss=99.5877
	step [127/147], loss=84.6146
	step [128/147], loss=73.9431
	step [129/147], loss=88.1611
	step [130/147], loss=86.8544
	step [131/147], loss=93.7105
	step [132/147], loss=92.4336
	step [133/147], loss=75.9852
	step [134/147], loss=83.6282
	step [135/147], loss=99.1558
	step [136/147], loss=83.5802
	step [137/147], loss=94.9418
	step [138/147], loss=79.3058
	step [139/147], loss=87.4723
	step [140/147], loss=67.9014
	step [141/147], loss=78.7383
	step [142/147], loss=86.2131
	step [143/147], loss=72.2185
	step [144/147], loss=102.4545
	step [145/147], loss=84.3781
	step [146/147], loss=95.2171
	step [147/147], loss=24.0723
	Evaluating
	loss=0.0141, precision=0.3452, recall=0.8740, f1=0.4949
Training epoch 69
	step [1/147], loss=83.0604
	step [2/147], loss=81.2538
	step [3/147], loss=77.8654
	step [4/147], loss=87.6688
	step [5/147], loss=89.4230
	step [6/147], loss=86.8950
	step [7/147], loss=75.4040
	step [8/147], loss=94.2396
	step [9/147], loss=93.8739
	step [10/147], loss=93.9174
	step [11/147], loss=89.8870
	step [12/147], loss=80.7980
	step [13/147], loss=83.6393
	step [14/147], loss=82.8596
	step [15/147], loss=90.4877
	step [16/147], loss=92.0461
	step [17/147], loss=100.7190
	step [18/147], loss=81.6266
	step [19/147], loss=84.8958
	step [20/147], loss=92.8308
	step [21/147], loss=87.2130
	step [22/147], loss=77.1487
	step [23/147], loss=93.2643
	step [24/147], loss=101.7425
	step [25/147], loss=94.8511
	step [26/147], loss=89.9843
	step [27/147], loss=71.9155
	step [28/147], loss=84.7180
	step [29/147], loss=81.2357
	step [30/147], loss=98.6183
	step [31/147], loss=90.7293
	step [32/147], loss=80.3546
	step [33/147], loss=85.2945
	step [34/147], loss=85.1850
	step [35/147], loss=87.0346
	step [36/147], loss=81.1332
	step [37/147], loss=77.6105
	step [38/147], loss=83.5848
	step [39/147], loss=76.9357
	step [40/147], loss=95.5602
	step [41/147], loss=93.9336
	step [42/147], loss=88.0610
	step [43/147], loss=102.3743
	step [44/147], loss=78.4783
	step [45/147], loss=80.2877
	step [46/147], loss=66.1184
	step [47/147], loss=72.9977
	step [48/147], loss=96.0968
	step [49/147], loss=75.4799
	step [50/147], loss=85.1881
	step [51/147], loss=80.4019
	step [52/147], loss=68.9491
	step [53/147], loss=97.4815
	step [54/147], loss=80.0869
	step [55/147], loss=116.2301
	step [56/147], loss=83.8285
	step [57/147], loss=81.5920
	step [58/147], loss=85.8798
	step [59/147], loss=74.8923
	step [60/147], loss=89.1167
	step [61/147], loss=87.5221
	step [62/147], loss=91.4175
	step [63/147], loss=85.4859
	step [64/147], loss=79.3940
	step [65/147], loss=65.5593
	step [66/147], loss=78.4881
	step [67/147], loss=92.6037
	step [68/147], loss=100.3359
	step [69/147], loss=103.3752
	step [70/147], loss=102.3641
	step [71/147], loss=92.3801
	step [72/147], loss=74.8401
	step [73/147], loss=69.5800
	step [74/147], loss=95.4620
	step [75/147], loss=81.2704
	step [76/147], loss=81.8140
	step [77/147], loss=75.1076
	step [78/147], loss=85.7285
	step [79/147], loss=90.1358
	step [80/147], loss=91.7992
	step [81/147], loss=80.5196
	step [82/147], loss=85.3744
	step [83/147], loss=79.3604
	step [84/147], loss=86.8525
	step [85/147], loss=95.0456
	step [86/147], loss=109.8134
	step [87/147], loss=97.4948
	step [88/147], loss=90.4521
	step [89/147], loss=95.1212
	step [90/147], loss=90.5190
	step [91/147], loss=91.3813
	step [92/147], loss=84.1430
	step [93/147], loss=86.9428
	step [94/147], loss=91.3362
	step [95/147], loss=82.4903
	step [96/147], loss=82.6270
	step [97/147], loss=86.7405
	step [98/147], loss=111.6526
	step [99/147], loss=87.2395
	step [100/147], loss=92.2995
	step [101/147], loss=77.7942
	step [102/147], loss=95.5741
	step [103/147], loss=81.5825
	step [104/147], loss=88.4709
	step [105/147], loss=103.3271
	step [106/147], loss=98.2838
	step [107/147], loss=91.0650
	step [108/147], loss=84.3497
	step [109/147], loss=79.9531
	step [110/147], loss=79.2558
	step [111/147], loss=92.9890
	step [112/147], loss=86.4287
	step [113/147], loss=82.3382
	step [114/147], loss=92.7487
	step [115/147], loss=71.0175
	step [116/147], loss=86.8478
	step [117/147], loss=85.8935
	step [118/147], loss=96.1915
	step [119/147], loss=75.3431
	step [120/147], loss=87.8542
	step [121/147], loss=79.6088
	step [122/147], loss=90.5021
	step [123/147], loss=94.6197
	step [124/147], loss=91.1737
	step [125/147], loss=79.7118
	step [126/147], loss=76.2744
	step [127/147], loss=89.1548
	step [128/147], loss=90.6033
	step [129/147], loss=92.8718
	step [130/147], loss=97.9143
	step [131/147], loss=75.2938
	step [132/147], loss=90.7491
	step [133/147], loss=99.3390
	step [134/147], loss=88.2547
	step [135/147], loss=84.2448
	step [136/147], loss=93.2640
	step [137/147], loss=92.2131
	step [138/147], loss=74.5321
	step [139/147], loss=91.6720
	step [140/147], loss=76.3370
	step [141/147], loss=74.3189
	step [142/147], loss=86.3019
	step [143/147], loss=90.3272
	step [144/147], loss=70.5580
	step [145/147], loss=95.8432
	step [146/147], loss=85.0685
	step [147/147], loss=20.1669
	Evaluating
	loss=0.0138, precision=0.3498, recall=0.8836, f1=0.5012
Training epoch 70
	step [1/147], loss=75.8457
	step [2/147], loss=84.9102
	step [3/147], loss=90.5013
	step [4/147], loss=83.5481
	step [5/147], loss=103.6954
	step [6/147], loss=81.5630
	step [7/147], loss=91.3324
	step [8/147], loss=86.9013
	step [9/147], loss=83.2935
	step [10/147], loss=84.2192
	step [11/147], loss=95.3170
	step [12/147], loss=78.2576
	step [13/147], loss=98.2044
	step [14/147], loss=79.0867
	step [15/147], loss=86.9695
	step [16/147], loss=85.5828
	step [17/147], loss=80.9091
	step [18/147], loss=93.1674
	step [19/147], loss=105.8473
	step [20/147], loss=73.6500
	step [21/147], loss=95.8250
	step [22/147], loss=84.9077
	step [23/147], loss=87.3883
	step [24/147], loss=92.6109
	step [25/147], loss=76.0112
	step [26/147], loss=88.5059
	step [27/147], loss=73.7386
	step [28/147], loss=85.3591
	step [29/147], loss=89.1010
	step [30/147], loss=87.5507
	step [31/147], loss=80.4850
	step [32/147], loss=83.4947
	step [33/147], loss=80.1402
	step [34/147], loss=82.3037
	step [35/147], loss=91.1642
	step [36/147], loss=97.2340
	step [37/147], loss=75.3194
	step [38/147], loss=68.3862
	step [39/147], loss=93.3460
	step [40/147], loss=73.2444
	step [41/147], loss=100.6238
	step [42/147], loss=77.6766
	step [43/147], loss=92.9616
	step [44/147], loss=79.6103
	step [45/147], loss=93.3980
	step [46/147], loss=91.6247
	step [47/147], loss=77.4128
	step [48/147], loss=80.6888
	step [49/147], loss=97.3755
	step [50/147], loss=82.2797
	step [51/147], loss=112.2318
	step [52/147], loss=92.3106
	step [53/147], loss=96.4147
	step [54/147], loss=87.0421
	step [55/147], loss=80.3729
	step [56/147], loss=89.0392
	step [57/147], loss=88.2159
	step [58/147], loss=92.2065
	step [59/147], loss=96.2789
	step [60/147], loss=84.2487
	step [61/147], loss=100.8564
	step [62/147], loss=99.4529
	step [63/147], loss=83.9057
	step [64/147], loss=87.8731
	step [65/147], loss=91.7057
	step [66/147], loss=95.3028
	step [67/147], loss=89.6508
	step [68/147], loss=77.8931
	step [69/147], loss=106.3673
	step [70/147], loss=96.9944
	step [71/147], loss=104.3307
	step [72/147], loss=94.3706
	step [73/147], loss=82.9190
	step [74/147], loss=86.1507
	step [75/147], loss=78.6191
	step [76/147], loss=72.0552
	step [77/147], loss=91.6620
	step [78/147], loss=79.2079
	step [79/147], loss=68.6455
	step [80/147], loss=81.1307
	step [81/147], loss=84.7584
	step [82/147], loss=82.3567
	step [83/147], loss=108.5266
	step [84/147], loss=82.3500
	step [85/147], loss=86.5779
	step [86/147], loss=82.4202
	step [87/147], loss=83.9393
	step [88/147], loss=74.6022
	step [89/147], loss=80.7016
	step [90/147], loss=76.8821
	step [91/147], loss=92.2875
	step [92/147], loss=89.7932
	step [93/147], loss=86.9255
	step [94/147], loss=87.2812
	step [95/147], loss=84.6524
	step [96/147], loss=67.0906
	step [97/147], loss=79.5974
	step [98/147], loss=107.0025
	step [99/147], loss=81.5399
	step [100/147], loss=102.1156
	step [101/147], loss=89.2938
	step [102/147], loss=65.8981
	step [103/147], loss=74.4546
	step [104/147], loss=93.9461
	step [105/147], loss=72.0077
	step [106/147], loss=83.2932
	step [107/147], loss=89.7864
	step [108/147], loss=68.5927
	step [109/147], loss=90.1649
	step [110/147], loss=86.7498
	step [111/147], loss=97.2248
	step [112/147], loss=71.1909
	step [113/147], loss=82.8894
	step [114/147], loss=77.9943
	step [115/147], loss=89.5642
	step [116/147], loss=73.2176
	step [117/147], loss=104.8287
	step [118/147], loss=92.8206
	step [119/147], loss=99.2241
	step [120/147], loss=68.8402
	step [121/147], loss=83.4886
	step [122/147], loss=108.1202
	step [123/147], loss=73.8559
	step [124/147], loss=100.5391
	step [125/147], loss=93.1722
	step [126/147], loss=70.1569
	step [127/147], loss=85.2175
	step [128/147], loss=74.4175
	step [129/147], loss=96.8529
	step [130/147], loss=82.8091
	step [131/147], loss=95.7181
	step [132/147], loss=71.6022
	step [133/147], loss=67.3674
	step [134/147], loss=71.6791
	step [135/147], loss=102.0897
	step [136/147], loss=75.7660
	step [137/147], loss=87.2338
	step [138/147], loss=89.5702
	step [139/147], loss=75.4945
	step [140/147], loss=76.0389
	step [141/147], loss=85.6491
	step [142/147], loss=87.9307
	step [143/147], loss=75.0943
	step [144/147], loss=77.6448
	step [145/147], loss=82.6516
	step [146/147], loss=101.2589
	step [147/147], loss=31.2529
	Evaluating
	loss=0.0136, precision=0.3527, recall=0.8745, f1=0.5027
Training epoch 71
	step [1/147], loss=90.4314
	step [2/147], loss=87.7816
	step [3/147], loss=79.4531
	step [4/147], loss=76.7809
	step [5/147], loss=75.1004
	step [6/147], loss=79.1305
	step [7/147], loss=74.7652
	step [8/147], loss=69.5945
	step [9/147], loss=92.1178
	step [10/147], loss=92.8646
	step [11/147], loss=82.5047
	step [12/147], loss=91.3114
	step [13/147], loss=82.4022
	step [14/147], loss=90.9046
	step [15/147], loss=103.5160
	step [16/147], loss=94.7478
	step [17/147], loss=77.3299
	step [18/147], loss=92.9516
	step [19/147], loss=84.4191
	step [20/147], loss=89.6797
	step [21/147], loss=81.3615
	step [22/147], loss=90.7315
	step [23/147], loss=77.1161
	step [24/147], loss=81.8747
	step [25/147], loss=95.2337
	step [26/147], loss=103.0213
	step [27/147], loss=87.4186
	step [28/147], loss=82.8840
	step [29/147], loss=78.6785
	step [30/147], loss=100.8360
	step [31/147], loss=93.3858
	step [32/147], loss=74.2818
	step [33/147], loss=94.8864
	step [34/147], loss=89.6775
	step [35/147], loss=89.7041
	step [36/147], loss=78.4803
	step [37/147], loss=101.7295
	step [38/147], loss=88.3982
	step [39/147], loss=105.4573
	step [40/147], loss=90.2055
	step [41/147], loss=85.3439
	step [42/147], loss=92.3032
	step [43/147], loss=77.3458
	step [44/147], loss=81.1492
	step [45/147], loss=97.3800
	step [46/147], loss=87.0801
	step [47/147], loss=81.7212
	step [48/147], loss=73.8920
	step [49/147], loss=70.9404
	step [50/147], loss=90.3659
	step [51/147], loss=78.0956
	step [52/147], loss=82.8463
	step [53/147], loss=76.4559
	step [54/147], loss=101.6626
	step [55/147], loss=81.5063
	step [56/147], loss=83.1666
	step [57/147], loss=90.9697
	step [58/147], loss=91.7486
	step [59/147], loss=97.9442
	step [60/147], loss=93.7983
	step [61/147], loss=84.0457
	step [62/147], loss=96.3104
	step [63/147], loss=75.1149
	step [64/147], loss=85.4302
	step [65/147], loss=84.3114
	step [66/147], loss=81.2431
	step [67/147], loss=82.0700
	step [68/147], loss=73.8824
	step [69/147], loss=82.6010
	step [70/147], loss=90.9239
	step [71/147], loss=81.3669
	step [72/147], loss=83.9382
	step [73/147], loss=98.5322
	step [74/147], loss=110.3755
	step [75/147], loss=86.1190
	step [76/147], loss=83.1215
	step [77/147], loss=88.1816
	step [78/147], loss=75.9945
	step [79/147], loss=89.8656
	step [80/147], loss=96.3578
	step [81/147], loss=101.8980
	step [82/147], loss=81.5231
	step [83/147], loss=91.4561
	step [84/147], loss=76.1655
	step [85/147], loss=95.0717
	step [86/147], loss=106.3652
	step [87/147], loss=91.4104
	step [88/147], loss=85.3972
	step [89/147], loss=73.3886
	step [90/147], loss=89.7419
	step [91/147], loss=106.6933
	step [92/147], loss=81.2181
	step [93/147], loss=64.1723
	step [94/147], loss=93.3762
	step [95/147], loss=76.2821
	step [96/147], loss=82.3667
	step [97/147], loss=98.2679
	step [98/147], loss=82.6641
	step [99/147], loss=83.0578
	step [100/147], loss=94.0055
	step [101/147], loss=102.2179
	step [102/147], loss=81.7111
	step [103/147], loss=83.4421
	step [104/147], loss=101.6759
	step [105/147], loss=88.5207
	step [106/147], loss=88.9261
	step [107/147], loss=78.2104
	step [108/147], loss=87.4491
	step [109/147], loss=78.6803
	step [110/147], loss=86.1758
	step [111/147], loss=85.3119
	step [112/147], loss=71.1929
	step [113/147], loss=94.1274
	step [114/147], loss=93.9245
	step [115/147], loss=88.1109
	step [116/147], loss=72.4547
	step [117/147], loss=74.2318
	step [118/147], loss=79.2125
	step [119/147], loss=99.4615
	step [120/147], loss=83.7030
	step [121/147], loss=71.4203
	step [122/147], loss=96.9659
	step [123/147], loss=82.8567
	step [124/147], loss=93.7250
	step [125/147], loss=93.1193
	step [126/147], loss=83.7374
	step [127/147], loss=84.3352
	step [128/147], loss=66.2504
	step [129/147], loss=76.8989
	step [130/147], loss=84.9575
	step [131/147], loss=86.2842
	step [132/147], loss=84.8067
	step [133/147], loss=86.2043
	step [134/147], loss=93.8538
	step [135/147], loss=81.3308
	step [136/147], loss=89.3119
	step [137/147], loss=71.2549
	step [138/147], loss=83.7848
	step [139/147], loss=68.0639
	step [140/147], loss=76.0633
	step [141/147], loss=96.3243
	step [142/147], loss=83.4989
	step [143/147], loss=87.8381
	step [144/147], loss=73.5808
	step [145/147], loss=91.6769
	step [146/147], loss=85.1883
	step [147/147], loss=18.1126
	Evaluating
	loss=0.0136, precision=0.3574, recall=0.8805, f1=0.5084
Training epoch 72
	step [1/147], loss=93.4488
	step [2/147], loss=85.1038
	step [3/147], loss=93.3806
	step [4/147], loss=96.6553
	step [5/147], loss=83.3973
	step [6/147], loss=85.3294
	step [7/147], loss=94.2141
	step [8/147], loss=89.8833
	step [9/147], loss=86.0327
	step [10/147], loss=83.5738
	step [11/147], loss=86.0474
	step [12/147], loss=89.7807
	step [13/147], loss=105.5223
	step [14/147], loss=56.0438
	step [15/147], loss=84.4833
	step [16/147], loss=76.6278
	step [17/147], loss=81.8382
	step [18/147], loss=87.6159
	step [19/147], loss=83.8499
	step [20/147], loss=80.2060
	step [21/147], loss=77.4918
	step [22/147], loss=82.9200
	step [23/147], loss=75.6504
	step [24/147], loss=95.7350
	step [25/147], loss=108.6304
	step [26/147], loss=98.7657
	step [27/147], loss=84.8873
	step [28/147], loss=88.5596
	step [29/147], loss=103.8053
	step [30/147], loss=91.4015
	step [31/147], loss=85.9541
	step [32/147], loss=89.3218
	step [33/147], loss=87.1168
	step [34/147], loss=77.5807
	step [35/147], loss=89.8516
	step [36/147], loss=88.1899
	step [37/147], loss=89.0677
	step [38/147], loss=94.5490
	step [39/147], loss=86.0713
	step [40/147], loss=77.2951
	step [41/147], loss=72.4722
	step [42/147], loss=80.1757
	step [43/147], loss=87.5000
	step [44/147], loss=90.7910
	step [45/147], loss=79.7567
	step [46/147], loss=100.2412
	step [47/147], loss=97.1915
	step [48/147], loss=90.0341
	step [49/147], loss=74.2873
	step [50/147], loss=66.0873
	step [51/147], loss=85.5699
	step [52/147], loss=91.7506
	step [53/147], loss=83.1860
	step [54/147], loss=90.0227
	step [55/147], loss=100.8408
	step [56/147], loss=74.8255
	step [57/147], loss=90.1931
	step [58/147], loss=86.3496
	step [59/147], loss=78.2489
	step [60/147], loss=88.0948
	step [61/147], loss=79.6879
	step [62/147], loss=99.5347
	step [63/147], loss=84.5935
	step [64/147], loss=83.0479
	step [65/147], loss=99.9298
	step [66/147], loss=74.8604
	step [67/147], loss=85.4102
	step [68/147], loss=100.6858
	step [69/147], loss=85.3069
	step [70/147], loss=104.0061
	step [71/147], loss=98.2522
	step [72/147], loss=88.9160
	step [73/147], loss=68.7742
	step [74/147], loss=88.6624
	step [75/147], loss=86.7428
	step [76/147], loss=92.3855
	step [77/147], loss=100.1417
	step [78/147], loss=83.7544
	step [79/147], loss=62.4084
	step [80/147], loss=83.5149
	step [81/147], loss=74.3837
	step [82/147], loss=86.4884
	step [83/147], loss=76.9154
	step [84/147], loss=89.7101
	step [85/147], loss=89.0301
	step [86/147], loss=87.3941
	step [87/147], loss=84.9245
	step [88/147], loss=94.0962
	step [89/147], loss=85.1317
	step [90/147], loss=72.6775
	step [91/147], loss=84.6140
	step [92/147], loss=75.0431
	step [93/147], loss=84.8679
	step [94/147], loss=92.4818
	step [95/147], loss=78.6383
	step [96/147], loss=76.5818
	step [97/147], loss=72.0788
	step [98/147], loss=68.3559
	step [99/147], loss=86.2852
	step [100/147], loss=92.2355
	step [101/147], loss=84.9373
	step [102/147], loss=93.6757
	step [103/147], loss=81.4596
	step [104/147], loss=78.7366
	step [105/147], loss=86.0381
	step [106/147], loss=71.1441
	step [107/147], loss=89.6252
	step [108/147], loss=86.0124
	step [109/147], loss=100.6897
	step [110/147], loss=81.2320
	step [111/147], loss=78.1521
	step [112/147], loss=91.2793
	step [113/147], loss=77.6794
	step [114/147], loss=95.0534
	step [115/147], loss=73.6245
	step [116/147], loss=75.3209
	step [117/147], loss=90.7052
	step [118/147], loss=94.5484
	step [119/147], loss=84.1847
	step [120/147], loss=76.5079
	step [121/147], loss=79.8114
	step [122/147], loss=75.2613
	step [123/147], loss=83.1728
	step [124/147], loss=69.6646
	step [125/147], loss=83.9581
	step [126/147], loss=88.2125
	step [127/147], loss=100.6006
	step [128/147], loss=93.5941
	step [129/147], loss=73.9593
	step [130/147], loss=92.2745
	step [131/147], loss=105.4489
	step [132/147], loss=97.9401
	step [133/147], loss=80.5193
	step [134/147], loss=80.9916
	step [135/147], loss=81.9451
	step [136/147], loss=92.7897
	step [137/147], loss=86.9642
	step [138/147], loss=90.0413
	step [139/147], loss=86.4812
	step [140/147], loss=92.0051
	step [141/147], loss=102.7194
	step [142/147], loss=75.0821
	step [143/147], loss=57.5127
	step [144/147], loss=79.3509
	step [145/147], loss=92.5291
	step [146/147], loss=88.3269
	step [147/147], loss=24.6769
	Evaluating
	loss=0.0121, precision=0.3881, recall=0.8709, f1=0.5369
Training epoch 73
	step [1/147], loss=83.5219
	step [2/147], loss=88.7478
	step [3/147], loss=89.9775
	step [4/147], loss=86.5077
	step [5/147], loss=91.0302
	step [6/147], loss=84.0062
	step [7/147], loss=99.1002
	step [8/147], loss=92.5272
	step [9/147], loss=76.1677
	step [10/147], loss=84.7719
	step [11/147], loss=92.4327
	step [12/147], loss=94.4074
	step [13/147], loss=94.0313
	step [14/147], loss=68.3686
	step [15/147], loss=79.6106
	step [16/147], loss=92.1924
	step [17/147], loss=91.7911
	step [18/147], loss=98.5962
	step [19/147], loss=84.2306
	step [20/147], loss=101.5142
	step [21/147], loss=70.4139
	step [22/147], loss=97.9478
	step [23/147], loss=92.7680
	step [24/147], loss=75.7172
	step [25/147], loss=82.8197
	step [26/147], loss=73.1780
	step [27/147], loss=90.1178
	step [28/147], loss=88.4047
	step [29/147], loss=75.3462
	step [30/147], loss=82.2444
	step [31/147], loss=64.1713
	step [32/147], loss=92.7507
	step [33/147], loss=85.7536
	step [34/147], loss=90.1533
	step [35/147], loss=82.2552
	step [36/147], loss=68.5144
	step [37/147], loss=92.9041
	step [38/147], loss=83.4596
	step [39/147], loss=94.7786
	step [40/147], loss=83.0342
	step [41/147], loss=91.1591
	step [42/147], loss=95.2958
	step [43/147], loss=81.5677
	step [44/147], loss=78.4828
	step [45/147], loss=58.3752
	step [46/147], loss=82.2174
	step [47/147], loss=74.7694
	step [48/147], loss=94.6100
	step [49/147], loss=81.8664
	step [50/147], loss=85.6575
	step [51/147], loss=86.6361
	step [52/147], loss=80.5354
	step [53/147], loss=83.2923
	step [54/147], loss=74.8032
	step [55/147], loss=90.3729
	step [56/147], loss=86.6871
	step [57/147], loss=87.9558
	step [58/147], loss=77.5076
	step [59/147], loss=95.2010
	step [60/147], loss=95.8236
	step [61/147], loss=86.1463
	step [62/147], loss=72.2940
	step [63/147], loss=92.0955
	step [64/147], loss=93.7772
	step [65/147], loss=86.2437
	step [66/147], loss=85.1983
	step [67/147], loss=89.0537
	step [68/147], loss=83.2951
	step [69/147], loss=80.6513
	step [70/147], loss=73.0661
	step [71/147], loss=74.7076
	step [72/147], loss=78.1983
	step [73/147], loss=86.9053
	step [74/147], loss=78.1766
	step [75/147], loss=89.5756
	step [76/147], loss=80.8389
	step [77/147], loss=93.0278
	step [78/147], loss=87.6632
	step [79/147], loss=73.4919
	step [80/147], loss=92.6960
	step [81/147], loss=79.0922
	step [82/147], loss=72.5513
	step [83/147], loss=84.4937
	step [84/147], loss=74.2840
	step [85/147], loss=79.8658
	step [86/147], loss=85.5024
	step [87/147], loss=92.3078
	step [88/147], loss=102.8391
	step [89/147], loss=81.1709
	step [90/147], loss=78.2767
	step [91/147], loss=101.5291
	step [92/147], loss=89.8362
	step [93/147], loss=81.0754
	step [94/147], loss=89.5493
	step [95/147], loss=90.1182
	step [96/147], loss=80.9369
	step [97/147], loss=77.9741
	step [98/147], loss=90.0772
	step [99/147], loss=75.8348
	step [100/147], loss=88.3897
	step [101/147], loss=89.8413
	step [102/147], loss=93.3579
	step [103/147], loss=82.7504
	step [104/147], loss=83.3756
	step [105/147], loss=92.1131
	step [106/147], loss=90.1635
	step [107/147], loss=85.3537
	step [108/147], loss=85.3485
	step [109/147], loss=86.9600
	step [110/147], loss=102.8080
	step [111/147], loss=102.5381
	step [112/147], loss=73.0953
	step [113/147], loss=80.0772
	step [114/147], loss=86.4491
	step [115/147], loss=75.6878
	step [116/147], loss=75.1876
	step [117/147], loss=92.3131
	step [118/147], loss=93.6391
	step [119/147], loss=87.8572
	step [120/147], loss=92.6537
	step [121/147], loss=74.6281
	step [122/147], loss=74.8389
	step [123/147], loss=79.8698
	step [124/147], loss=98.4199
	step [125/147], loss=66.7737
	step [126/147], loss=101.6623
	step [127/147], loss=95.6219
	step [128/147], loss=73.2045
	step [129/147], loss=94.0179
	step [130/147], loss=80.0453
	step [131/147], loss=94.0777
	step [132/147], loss=90.0470
	step [133/147], loss=90.7990
	step [134/147], loss=89.8955
	step [135/147], loss=92.0106
	step [136/147], loss=79.4379
	step [137/147], loss=84.8305
	step [138/147], loss=77.9212
	step [139/147], loss=82.2486
	step [140/147], loss=96.3295
	step [141/147], loss=77.8000
	step [142/147], loss=87.6676
	step [143/147], loss=83.9530
	step [144/147], loss=101.6500
	step [145/147], loss=68.9975
	step [146/147], loss=93.2123
	step [147/147], loss=14.6030
	Evaluating
	loss=0.0129, precision=0.3709, recall=0.8857, f1=0.5228
Training epoch 74
	step [1/147], loss=83.4927
	step [2/147], loss=101.8246
	step [3/147], loss=72.6525
	step [4/147], loss=82.3993
	step [5/147], loss=80.2139
	step [6/147], loss=85.9741
	step [7/147], loss=89.2789
	step [8/147], loss=79.3778
	step [9/147], loss=110.1872
	step [10/147], loss=84.7888
	step [11/147], loss=91.0123
	step [12/147], loss=76.2865
	step [13/147], loss=80.3267
	step [14/147], loss=78.7096
	step [15/147], loss=90.5096
	step [16/147], loss=83.5160
	step [17/147], loss=74.5674
	step [18/147], loss=84.4051
	step [19/147], loss=85.0476
	step [20/147], loss=75.7550
	step [21/147], loss=72.3983
	step [22/147], loss=81.6181
	step [23/147], loss=98.4700
	step [24/147], loss=96.7090
	step [25/147], loss=89.4042
	step [26/147], loss=94.0929
	step [27/147], loss=84.8446
	step [28/147], loss=79.5058
	step [29/147], loss=89.1865
	step [30/147], loss=83.0750
	step [31/147], loss=90.1736
	step [32/147], loss=82.6197
	step [33/147], loss=81.1227
	step [34/147], loss=86.4064
	step [35/147], loss=78.6948
	step [36/147], loss=85.2707
	step [37/147], loss=83.2614
	step [38/147], loss=84.6756
	step [39/147], loss=81.2644
	step [40/147], loss=101.4432
	step [41/147], loss=89.4780
	step [42/147], loss=83.4623
	step [43/147], loss=83.8854
	step [44/147], loss=83.4030
	step [45/147], loss=93.1569
	step [46/147], loss=75.9435
	step [47/147], loss=90.2200
	step [48/147], loss=86.7545
	step [49/147], loss=107.9699
	step [50/147], loss=77.4191
	step [51/147], loss=93.9343
	step [52/147], loss=93.4309
	step [53/147], loss=76.7211
	step [54/147], loss=84.4624
	step [55/147], loss=80.8515
	step [56/147], loss=75.1374
	step [57/147], loss=87.8817
	step [58/147], loss=70.1774
	step [59/147], loss=85.3224
	step [60/147], loss=81.7443
	step [61/147], loss=110.9143
	step [62/147], loss=71.9689
	step [63/147], loss=84.0485
	step [64/147], loss=89.0585
	step [65/147], loss=90.6947
	step [66/147], loss=82.4984
	step [67/147], loss=77.5217
	step [68/147], loss=86.1030
	step [69/147], loss=97.2228
	step [70/147], loss=90.8685
	step [71/147], loss=80.9627
	step [72/147], loss=81.6819
	step [73/147], loss=85.0288
	step [74/147], loss=94.5292
	step [75/147], loss=91.1335
	step [76/147], loss=93.1746
	step [77/147], loss=82.7610
	step [78/147], loss=74.9016
	step [79/147], loss=76.7317
	step [80/147], loss=89.0039
	step [81/147], loss=80.3767
	step [82/147], loss=95.4247
	step [83/147], loss=94.6358
	step [84/147], loss=90.6837
	step [85/147], loss=95.3088
	step [86/147], loss=92.5719
	step [87/147], loss=86.6958
	step [88/147], loss=80.7289
	step [89/147], loss=91.8594
	step [90/147], loss=74.1364
	step [91/147], loss=88.5610
	step [92/147], loss=85.3453
	step [93/147], loss=93.7002
	step [94/147], loss=80.2464
	step [95/147], loss=75.9463
	step [96/147], loss=75.2579
	step [97/147], loss=85.2594
	step [98/147], loss=79.3814
	step [99/147], loss=78.2611
	step [100/147], loss=84.9528
	step [101/147], loss=80.9694
	step [102/147], loss=98.6758
	step [103/147], loss=96.4038
	step [104/147], loss=76.9343
	step [105/147], loss=89.0361
	step [106/147], loss=79.5392
	step [107/147], loss=81.3526
	step [108/147], loss=72.5040
	step [109/147], loss=97.6652
	step [110/147], loss=89.4034
	step [111/147], loss=61.5261
	step [112/147], loss=82.9779
	step [113/147], loss=68.2548
	step [114/147], loss=88.8506
	step [115/147], loss=80.9671
	step [116/147], loss=96.8065
	step [117/147], loss=93.9812
	step [118/147], loss=88.8300
	step [119/147], loss=69.5112
	step [120/147], loss=90.0071
	step [121/147], loss=75.9477
	step [122/147], loss=79.4463
	step [123/147], loss=68.4300
	step [124/147], loss=73.2985
	step [125/147], loss=88.1214
	step [126/147], loss=102.4502
	step [127/147], loss=86.5855
	step [128/147], loss=85.0138
	step [129/147], loss=73.5921
	step [130/147], loss=86.5719
	step [131/147], loss=91.4382
	step [132/147], loss=83.0729
	step [133/147], loss=84.3784
	step [134/147], loss=94.6039
	step [135/147], loss=82.1479
	step [136/147], loss=84.6434
	step [137/147], loss=78.8875
	step [138/147], loss=80.5287
	step [139/147], loss=87.4809
	step [140/147], loss=84.7382
	step [141/147], loss=92.7094
	step [142/147], loss=95.3870
	step [143/147], loss=75.6093
	step [144/147], loss=81.5345
	step [145/147], loss=79.0903
	step [146/147], loss=88.0127
	step [147/147], loss=16.2621
	Evaluating
	loss=0.0127, precision=0.3694, recall=0.8772, f1=0.5199
Training epoch 75
	step [1/147], loss=91.0876
	step [2/147], loss=64.4875
	step [3/147], loss=85.3294
	step [4/147], loss=77.0668
	step [5/147], loss=107.3425
	step [6/147], loss=92.0367
	step [7/147], loss=88.1259
	step [8/147], loss=95.0939
	step [9/147], loss=88.3507
	step [10/147], loss=91.1154
	step [11/147], loss=92.9787
	step [12/147], loss=78.9146
	step [13/147], loss=85.4325
	step [14/147], loss=89.1906
	step [15/147], loss=72.2030
	step [16/147], loss=98.9773
	step [17/147], loss=84.3081
	step [18/147], loss=90.6822
	step [19/147], loss=86.9833
	step [20/147], loss=74.9581
	step [21/147], loss=92.2110
	step [22/147], loss=75.9409
	step [23/147], loss=91.5568
	step [24/147], loss=80.7428
	step [25/147], loss=99.8869
	step [26/147], loss=80.0013
	step [27/147], loss=73.3613
	step [28/147], loss=87.5391
	step [29/147], loss=75.0142
	step [30/147], loss=80.7059
	step [31/147], loss=77.9554
	step [32/147], loss=81.2778
	step [33/147], loss=91.6034
	step [34/147], loss=95.3857
	step [35/147], loss=72.5670
	step [36/147], loss=79.4032
	step [37/147], loss=79.6425
	step [38/147], loss=80.4776
	step [39/147], loss=84.7154
	step [40/147], loss=82.5030
	step [41/147], loss=86.3739
	step [42/147], loss=79.0109
	step [43/147], loss=72.6455
	step [44/147], loss=83.4592
	step [45/147], loss=81.8649
	step [46/147], loss=81.8985
	step [47/147], loss=85.1311
	step [48/147], loss=81.1806
	step [49/147], loss=93.1370
	step [50/147], loss=86.6016
	step [51/147], loss=89.0220
	step [52/147], loss=82.1366
	step [53/147], loss=99.4268
	step [54/147], loss=96.0648
	step [55/147], loss=91.1374
	step [56/147], loss=80.8496
	step [57/147], loss=70.0554
	step [58/147], loss=74.7591
	step [59/147], loss=81.7770
	step [60/147], loss=83.1985
	step [61/147], loss=77.9837
	step [62/147], loss=68.3943
	step [63/147], loss=77.2884
	step [64/147], loss=82.9105
	step [65/147], loss=80.9308
	step [66/147], loss=86.9742
	step [67/147], loss=82.3133
	step [68/147], loss=85.3951
	step [69/147], loss=79.2975
	step [70/147], loss=71.2462
	step [71/147], loss=88.1740
	step [72/147], loss=95.4182
	step [73/147], loss=95.2032
	step [74/147], loss=86.0138
	step [75/147], loss=96.0059
	step [76/147], loss=82.2807
	step [77/147], loss=91.9050
	step [78/147], loss=81.9110
	step [79/147], loss=86.9521
	step [80/147], loss=78.8233
	step [81/147], loss=73.9179
	step [82/147], loss=97.3032
	step [83/147], loss=78.9361
	step [84/147], loss=93.6725
	step [85/147], loss=91.1954
	step [86/147], loss=95.9238
	step [87/147], loss=92.2692
	step [88/147], loss=75.4748
	step [89/147], loss=77.6145
	step [90/147], loss=82.3379
	step [91/147], loss=89.7068
	step [92/147], loss=93.2452
	step [93/147], loss=92.0364
	step [94/147], loss=78.4041
	step [95/147], loss=83.0290
	step [96/147], loss=81.9450
	step [97/147], loss=90.1610
	step [98/147], loss=88.5401
	step [99/147], loss=80.4265
	step [100/147], loss=105.9986
	step [101/147], loss=77.3445
	step [102/147], loss=87.4433
	step [103/147], loss=78.9606
	step [104/147], loss=96.1249
	step [105/147], loss=79.8864
	step [106/147], loss=82.2189
	step [107/147], loss=69.5998
	step [108/147], loss=74.6732
	step [109/147], loss=79.7761
	step [110/147], loss=93.5412
	step [111/147], loss=74.6790
	step [112/147], loss=92.2954
	step [113/147], loss=79.0038
	step [114/147], loss=88.1887
	step [115/147], loss=82.5670
	step [116/147], loss=77.4500
	step [117/147], loss=79.2350
	step [118/147], loss=78.3225
	step [119/147], loss=85.6432
	step [120/147], loss=96.5568
	step [121/147], loss=76.3022
	step [122/147], loss=93.8235
	step [123/147], loss=76.5899
	step [124/147], loss=65.6873
	step [125/147], loss=102.4400
	step [126/147], loss=93.9500
	step [127/147], loss=88.1515
	step [128/147], loss=90.2522
	step [129/147], loss=94.0950
	step [130/147], loss=91.8632
	step [131/147], loss=100.0360
	step [132/147], loss=104.3708
	step [133/147], loss=73.7408
	step [134/147], loss=81.5517
	step [135/147], loss=80.7852
	step [136/147], loss=75.5519
	step [137/147], loss=78.5725
	step [138/147], loss=87.3050
	step [139/147], loss=84.1005
	step [140/147], loss=89.7459
	step [141/147], loss=85.6569
	step [142/147], loss=85.2113
	step [143/147], loss=88.1901
	step [144/147], loss=74.9507
	step [145/147], loss=91.5489
	step [146/147], loss=94.3835
	step [147/147], loss=20.0820
	Evaluating
	loss=0.0130, precision=0.3608, recall=0.8863, f1=0.5128
Training epoch 76
	step [1/147], loss=91.2231
	step [2/147], loss=78.9111
	step [3/147], loss=71.0253
	step [4/147], loss=77.3128
	step [5/147], loss=77.7111
	step [6/147], loss=78.7641
	step [7/147], loss=72.8450
	step [8/147], loss=95.4334
	step [9/147], loss=88.8774
	step [10/147], loss=74.1769
	step [11/147], loss=83.0505
	step [12/147], loss=79.8767
	step [13/147], loss=86.3366
	step [14/147], loss=77.8589
	step [15/147], loss=88.4209
	step [16/147], loss=74.5623
	step [17/147], loss=86.7766
	step [18/147], loss=84.1751
	step [19/147], loss=75.8854
	step [20/147], loss=90.8565
	step [21/147], loss=96.0793
	step [22/147], loss=82.8687
	step [23/147], loss=80.6023
	step [24/147], loss=82.5915
	step [25/147], loss=84.4214
	step [26/147], loss=69.0508
	step [27/147], loss=97.9343
	step [28/147], loss=101.8703
	step [29/147], loss=85.6193
	step [30/147], loss=71.4512
	step [31/147], loss=99.8154
	step [32/147], loss=79.8666
	step [33/147], loss=78.3874
	step [34/147], loss=76.3984
	step [35/147], loss=108.8400
	step [36/147], loss=88.9935
	step [37/147], loss=90.0120
	step [38/147], loss=84.8948
	step [39/147], loss=96.9460
	step [40/147], loss=72.5861
	step [41/147], loss=85.1493
	step [42/147], loss=85.5415
	step [43/147], loss=70.9358
	step [44/147], loss=86.2958
	step [45/147], loss=83.3600
	step [46/147], loss=102.9116
	step [47/147], loss=86.1568
	step [48/147], loss=81.8286
	step [49/147], loss=73.1632
	step [50/147], loss=81.5372
	step [51/147], loss=77.8784
	step [52/147], loss=78.2680
	step [53/147], loss=82.9670
	step [54/147], loss=103.0337
	step [55/147], loss=86.1859
	step [56/147], loss=74.9310
	step [57/147], loss=90.5874
	step [58/147], loss=82.8808
	step [59/147], loss=77.6267
	step [60/147], loss=85.0864
	step [61/147], loss=74.9326
	step [62/147], loss=79.8581
	step [63/147], loss=99.2135
	step [64/147], loss=91.0911
	step [65/147], loss=84.6071
	step [66/147], loss=72.8579
	step [67/147], loss=84.7728
	step [68/147], loss=77.8546
	step [69/147], loss=96.9687
	step [70/147], loss=95.3970
	step [71/147], loss=88.0887
	step [72/147], loss=79.7544
	step [73/147], loss=91.6039
	step [74/147], loss=76.3140
	step [75/147], loss=86.7957
	step [76/147], loss=89.9668
	step [77/147], loss=92.9398
	step [78/147], loss=88.1636
	step [79/147], loss=91.5913
	step [80/147], loss=85.4109
	step [81/147], loss=81.0375
	step [82/147], loss=88.9030
	step [83/147], loss=88.3737
	step [84/147], loss=88.7566
	step [85/147], loss=100.2569
	step [86/147], loss=90.1317
	step [87/147], loss=87.0957
	step [88/147], loss=71.5213
	step [89/147], loss=76.5344
	step [90/147], loss=89.8108
	step [91/147], loss=82.8444
	step [92/147], loss=89.3650
	step [93/147], loss=80.2208
	step [94/147], loss=97.1466
	step [95/147], loss=63.5275
	step [96/147], loss=78.5021
	step [97/147], loss=68.7149
	step [98/147], loss=85.9252
	step [99/147], loss=91.5775
	step [100/147], loss=96.6210
	step [101/147], loss=81.7352
	step [102/147], loss=111.3054
	step [103/147], loss=78.6517
	step [104/147], loss=65.7001
	step [105/147], loss=93.2522
	step [106/147], loss=97.3187
	step [107/147], loss=77.1684
	step [108/147], loss=85.3571
	step [109/147], loss=76.9361
	step [110/147], loss=71.8342
	step [111/147], loss=103.5018
	step [112/147], loss=63.0146
	step [113/147], loss=70.4510
	step [114/147], loss=94.0296
	step [115/147], loss=96.7883
	step [116/147], loss=89.5008
	step [117/147], loss=85.7308
	step [118/147], loss=84.4934
	step [119/147], loss=86.2130
	step [120/147], loss=93.2282
	step [121/147], loss=92.1786
	step [122/147], loss=84.6926
	step [123/147], loss=82.3564
	step [124/147], loss=89.8900
	step [125/147], loss=90.8898
	step [126/147], loss=69.4214
	step [127/147], loss=90.2975
	step [128/147], loss=81.6218
	step [129/147], loss=92.3680
	step [130/147], loss=89.3259
	step [131/147], loss=87.2328
	step [132/147], loss=85.9857
	step [133/147], loss=87.5446
	step [134/147], loss=73.4872
	step [135/147], loss=81.3959
	step [136/147], loss=89.3228
	step [137/147], loss=88.9116
	step [138/147], loss=72.1359
	step [139/147], loss=79.8527
	step [140/147], loss=78.2505
	step [141/147], loss=82.0006
	step [142/147], loss=76.9638
	step [143/147], loss=81.5583
	step [144/147], loss=71.4550
	step [145/147], loss=73.8939
	step [146/147], loss=87.2605
	step [147/147], loss=14.2944
	Evaluating
	loss=0.0113, precision=0.4005, recall=0.8703, f1=0.5485
saving model as: 2_saved_model.pth
Training epoch 77
	step [1/147], loss=72.5308
	step [2/147], loss=72.7049
	step [3/147], loss=84.9977
	step [4/147], loss=83.8291
	step [5/147], loss=93.7399
	step [6/147], loss=84.9916
	step [7/147], loss=83.3748
	step [8/147], loss=72.9711
	step [9/147], loss=87.8640
	step [10/147], loss=84.2278
	step [11/147], loss=86.0862
	step [12/147], loss=75.4752
	step [13/147], loss=74.3976
	step [14/147], loss=77.0398
	step [15/147], loss=86.3392
	step [16/147], loss=93.6560
	step [17/147], loss=87.8757
	step [18/147], loss=73.7560
	step [19/147], loss=94.7754
	step [20/147], loss=69.4853
	step [21/147], loss=76.2969
	step [22/147], loss=99.5504
	step [23/147], loss=89.2545
	step [24/147], loss=87.2812
	step [25/147], loss=91.2242
	step [26/147], loss=95.6648
	step [27/147], loss=73.5860
	step [28/147], loss=89.4395
	step [29/147], loss=75.5655
	step [30/147], loss=94.7416
	step [31/147], loss=84.6476
	step [32/147], loss=81.4126
	step [33/147], loss=91.3345
	step [34/147], loss=72.1535
	step [35/147], loss=106.2772
	step [36/147], loss=99.3676
	step [37/147], loss=87.7457
	step [38/147], loss=82.3929
	step [39/147], loss=86.8678
	step [40/147], loss=64.7507
	step [41/147], loss=85.7077
	step [42/147], loss=71.4617
	step [43/147], loss=81.4085
	step [44/147], loss=80.6281
	step [45/147], loss=74.8421
	step [46/147], loss=92.8775
	step [47/147], loss=102.1561
	step [48/147], loss=95.3917
	step [49/147], loss=71.9637
	step [50/147], loss=96.2800
	step [51/147], loss=90.7221
	step [52/147], loss=79.4754
	step [53/147], loss=82.7847
	step [54/147], loss=76.3602
	step [55/147], loss=87.5750
	step [56/147], loss=90.0858
	step [57/147], loss=85.4837
	step [58/147], loss=80.3294
	step [59/147], loss=90.5812
	step [60/147], loss=97.5448
	step [61/147], loss=82.4005
	step [62/147], loss=78.9786
	step [63/147], loss=92.7134
	step [64/147], loss=60.5799
	step [65/147], loss=90.8298
	step [66/147], loss=95.0859
	step [67/147], loss=70.5537
	step [68/147], loss=78.9719
	step [69/147], loss=85.0139
	step [70/147], loss=73.4773
	step [71/147], loss=82.4692
	step [72/147], loss=76.4809
	step [73/147], loss=78.8674
	step [74/147], loss=91.7998
	step [75/147], loss=87.0179
	step [76/147], loss=87.8577
	step [77/147], loss=84.2966
	step [78/147], loss=83.2283
	step [79/147], loss=82.4080
	step [80/147], loss=81.7341
	step [81/147], loss=89.9051
	step [82/147], loss=86.4405
	step [83/147], loss=84.1152
	step [84/147], loss=79.1474
	step [85/147], loss=80.7802
	step [86/147], loss=98.7841
	step [87/147], loss=85.1108
	step [88/147], loss=78.1320
	step [89/147], loss=90.2585
	step [90/147], loss=95.7338
	step [91/147], loss=85.2999
	step [92/147], loss=66.4571
	step [93/147], loss=94.9223
	step [94/147], loss=98.3939
	step [95/147], loss=90.6282
	step [96/147], loss=81.1354
	step [97/147], loss=85.8077
	step [98/147], loss=94.5584
	step [99/147], loss=87.1423
	step [100/147], loss=82.0903
	step [101/147], loss=82.8541
	step [102/147], loss=76.6555
	step [103/147], loss=89.8513
	step [104/147], loss=93.2171
	step [105/147], loss=71.3503
	step [106/147], loss=91.0501
	step [107/147], loss=84.7164
	step [108/147], loss=73.0866
	step [109/147], loss=91.3601
	step [110/147], loss=85.6799
	step [111/147], loss=86.3597
	step [112/147], loss=80.6529
	step [113/147], loss=83.1834
	step [114/147], loss=82.8296
	step [115/147], loss=78.6677
	step [116/147], loss=75.1820
	step [117/147], loss=85.2812
	step [118/147], loss=73.4434
	step [119/147], loss=87.9416
	step [120/147], loss=81.1199
	step [121/147], loss=83.0431
	step [122/147], loss=89.5532
	step [123/147], loss=95.3497
	step [124/147], loss=78.1645
	step [125/147], loss=77.2444
	step [126/147], loss=74.6485
	step [127/147], loss=83.0815
	step [128/147], loss=98.5636
	step [129/147], loss=79.7753
	step [130/147], loss=89.6239
	step [131/147], loss=94.6714
	step [132/147], loss=84.5563
	step [133/147], loss=73.0842
	step [134/147], loss=92.2562
	step [135/147], loss=79.4163
	step [136/147], loss=84.7424
	step [137/147], loss=95.7634
	step [138/147], loss=86.2108
	step [139/147], loss=90.6262
	step [140/147], loss=79.8952
	step [141/147], loss=95.5969
	step [142/147], loss=75.3626
	step [143/147], loss=78.0847
	step [144/147], loss=79.3567
	step [145/147], loss=85.6408
	step [146/147], loss=79.3565
	step [147/147], loss=15.6084
	Evaluating
	loss=0.0109, precision=0.4196, recall=0.8843, f1=0.5691
saving model as: 2_saved_model.pth
Training epoch 78
	step [1/147], loss=67.4455
	step [2/147], loss=89.9185
	step [3/147], loss=90.0623
	step [4/147], loss=80.7354
	step [5/147], loss=58.9747
	step [6/147], loss=80.0038
	step [7/147], loss=79.3792
	step [8/147], loss=79.7424
	step [9/147], loss=91.3640
	step [10/147], loss=82.3466
	step [11/147], loss=103.8716
	step [12/147], loss=66.5092
	step [13/147], loss=74.7933
	step [14/147], loss=91.5129
	step [15/147], loss=70.8726
	step [16/147], loss=78.7826
	step [17/147], loss=92.9159
	step [18/147], loss=88.7957
	step [19/147], loss=99.2867
	step [20/147], loss=82.6369
	step [21/147], loss=79.7985
	step [22/147], loss=87.6434
	step [23/147], loss=81.3219
	step [24/147], loss=95.6028
	step [25/147], loss=64.8733
	step [26/147], loss=73.1880
	step [27/147], loss=89.7225
	step [28/147], loss=85.6899
	step [29/147], loss=76.7828
	step [30/147], loss=89.1193
	step [31/147], loss=78.2710
	step [32/147], loss=79.0729
	step [33/147], loss=97.3698
	step [34/147], loss=99.3711
	step [35/147], loss=84.8366
	step [36/147], loss=89.5730
	step [37/147], loss=80.9719
	step [38/147], loss=86.9460
	step [39/147], loss=74.7839
	step [40/147], loss=86.5077
	step [41/147], loss=81.3072
	step [42/147], loss=70.9383
	step [43/147], loss=93.5250
	step [44/147], loss=87.4685
	step [45/147], loss=81.6786
	step [46/147], loss=91.5064
	step [47/147], loss=74.4622
	step [48/147], loss=83.7840
	step [49/147], loss=94.5554
	step [50/147], loss=70.9089
	step [51/147], loss=81.7195
	step [52/147], loss=94.2324
	step [53/147], loss=81.7679
	step [54/147], loss=71.8995
	step [55/147], loss=90.3414
	step [56/147], loss=100.5639
	step [57/147], loss=87.4948
	step [58/147], loss=97.0266
	step [59/147], loss=98.3021
	step [60/147], loss=71.4565
	step [61/147], loss=93.6397
	step [62/147], loss=70.5828
	step [63/147], loss=76.1325
	step [64/147], loss=73.2157
	step [65/147], loss=86.6945
	step [66/147], loss=75.4550
	step [67/147], loss=84.9445
	step [68/147], loss=91.0534
	step [69/147], loss=79.3501
	step [70/147], loss=78.2956
	step [71/147], loss=66.0709
	step [72/147], loss=74.4151
	step [73/147], loss=75.0200
	step [74/147], loss=92.6602
	step [75/147], loss=75.1835
	step [76/147], loss=72.0520
	step [77/147], loss=79.2676
	step [78/147], loss=88.7755
	step [79/147], loss=80.6290
	step [80/147], loss=101.3698
	step [81/147], loss=67.2239
	step [82/147], loss=95.4928
	step [83/147], loss=80.3860
	step [84/147], loss=111.2573
	step [85/147], loss=89.0460
	step [86/147], loss=73.5875
	step [87/147], loss=69.9230
	step [88/147], loss=86.8281
	step [89/147], loss=83.1645
	step [90/147], loss=88.2975
	step [91/147], loss=84.6622
	step [92/147], loss=55.6020
	step [93/147], loss=85.4949
	step [94/147], loss=106.2981
	step [95/147], loss=90.9875
	step [96/147], loss=90.6507
	step [97/147], loss=79.5666
	step [98/147], loss=96.6500
	step [99/147], loss=86.5179
	step [100/147], loss=81.1150
	step [101/147], loss=98.0217
	step [102/147], loss=84.7689
	step [103/147], loss=84.8772
	step [104/147], loss=78.8474
	step [105/147], loss=87.3776
	step [106/147], loss=84.0371
	step [107/147], loss=78.5060
	step [108/147], loss=76.9197
	step [109/147], loss=79.9980
	step [110/147], loss=78.8029
	step [111/147], loss=99.0574
	step [112/147], loss=95.3221
	step [113/147], loss=85.3164
	step [114/147], loss=96.0290
	step [115/147], loss=83.7361
	step [116/147], loss=90.9805
	step [117/147], loss=105.8588
	step [118/147], loss=80.6513
	step [119/147], loss=79.6645
	step [120/147], loss=71.7876
	step [121/147], loss=71.5036
	step [122/147], loss=64.6691
	step [123/147], loss=88.5600
	step [124/147], loss=93.7803
	step [125/147], loss=85.1704
	step [126/147], loss=81.3016
	step [127/147], loss=89.9101
	step [128/147], loss=72.7996
	step [129/147], loss=84.3914
	step [130/147], loss=107.4944
	step [131/147], loss=96.2029
	step [132/147], loss=99.1622
	step [133/147], loss=88.1528
	step [134/147], loss=90.3595
	step [135/147], loss=78.6400
	step [136/147], loss=60.0726
	step [137/147], loss=76.4996
	step [138/147], loss=87.1316
	step [139/147], loss=75.8647
	step [140/147], loss=77.5786
	step [141/147], loss=85.6707
	step [142/147], loss=85.6022
	step [143/147], loss=93.8021
	step [144/147], loss=96.0437
	step [145/147], loss=85.6213
	step [146/147], loss=82.0199
	step [147/147], loss=12.0189
	Evaluating
	loss=0.0124, precision=0.3709, recall=0.8805, f1=0.5220
Training epoch 79
	step [1/147], loss=87.2374
	step [2/147], loss=92.1257
	step [3/147], loss=89.6370
	step [4/147], loss=78.7695
	step [5/147], loss=86.7484
	step [6/147], loss=100.3637
	step [7/147], loss=68.5780
	step [8/147], loss=84.3027
	step [9/147], loss=93.7217
	step [10/147], loss=95.0940
	step [11/147], loss=78.7806
	step [12/147], loss=70.7470
	step [13/147], loss=78.8311
	step [14/147], loss=101.5191
	step [15/147], loss=86.6026
	step [16/147], loss=101.4652
	step [17/147], loss=74.1184
	step [18/147], loss=90.8373
	step [19/147], loss=86.0791
	step [20/147], loss=72.0518
	step [21/147], loss=92.5633
	step [22/147], loss=69.0876
	step [23/147], loss=80.6242
	step [24/147], loss=93.9778
	step [25/147], loss=93.4978
	step [26/147], loss=106.6601
	step [27/147], loss=79.0509
	step [28/147], loss=74.0790
	step [29/147], loss=77.1110
	step [30/147], loss=99.3828
	step [31/147], loss=93.9571
	step [32/147], loss=76.2185
	step [33/147], loss=81.3827
	step [34/147], loss=81.8492
	step [35/147], loss=77.7659
	step [36/147], loss=70.6279
	step [37/147], loss=86.8209
	step [38/147], loss=94.7547
	step [39/147], loss=92.0100
	step [40/147], loss=88.3307
	step [41/147], loss=78.4934
	step [42/147], loss=76.8992
	step [43/147], loss=75.8292
	step [44/147], loss=87.5177
	step [45/147], loss=68.2249
	step [46/147], loss=81.6503
	step [47/147], loss=84.0059
	step [48/147], loss=84.6668
	step [49/147], loss=73.9936
	step [50/147], loss=92.0600
	step [51/147], loss=77.6714
	step [52/147], loss=97.6682
	step [53/147], loss=72.9687
	step [54/147], loss=72.1769
	step [55/147], loss=86.1430
	step [56/147], loss=98.6058
	step [57/147], loss=88.9491
	step [58/147], loss=75.1105
	step [59/147], loss=99.5425
	step [60/147], loss=79.8355
	step [61/147], loss=91.2285
	step [62/147], loss=95.3054
	step [63/147], loss=89.9996
	step [64/147], loss=69.5720
	step [65/147], loss=82.0907
	step [66/147], loss=72.0596
	step [67/147], loss=92.2598
	step [68/147], loss=66.2660
	step [69/147], loss=80.5332
	step [70/147], loss=79.9573
	step [71/147], loss=77.1948
	step [72/147], loss=92.6647
	step [73/147], loss=88.4694
	step [74/147], loss=84.5643
	step [75/147], loss=84.9194
	step [76/147], loss=84.4310
	step [77/147], loss=91.9889
	step [78/147], loss=86.4577
	step [79/147], loss=76.8996
	step [80/147], loss=79.0249
	step [81/147], loss=86.9518
	step [82/147], loss=85.5264
	step [83/147], loss=86.9505
	step [84/147], loss=76.4599
	step [85/147], loss=73.1506
	step [86/147], loss=86.4262
	step [87/147], loss=75.7696
	step [88/147], loss=76.4316
	step [89/147], loss=98.1600
	step [90/147], loss=84.1685
	step [91/147], loss=70.7144
	step [92/147], loss=87.4482
	step [93/147], loss=68.2403
	step [94/147], loss=79.8287
	step [95/147], loss=81.8643
	step [96/147], loss=85.4856
	step [97/147], loss=100.5540
	step [98/147], loss=83.4552
	step [99/147], loss=76.6040
	step [100/147], loss=79.7980
	step [101/147], loss=82.8963
	step [102/147], loss=84.1544
	step [103/147], loss=85.0725
	step [104/147], loss=66.8330
	step [105/147], loss=75.6283
	step [106/147], loss=83.9393
	step [107/147], loss=69.9788
	step [108/147], loss=83.8804
	step [109/147], loss=93.0960
	step [110/147], loss=71.6492
	step [111/147], loss=70.0138
	step [112/147], loss=86.9455
	step [113/147], loss=103.3589
	step [114/147], loss=86.1853
	step [115/147], loss=76.1842
	step [116/147], loss=71.8112
	step [117/147], loss=82.7234
	step [118/147], loss=85.1811
	step [119/147], loss=99.9449
	step [120/147], loss=107.6797
	step [121/147], loss=80.3634
	step [122/147], loss=99.7475
	step [123/147], loss=90.0096
	step [124/147], loss=76.2140
	step [125/147], loss=102.6108
	step [126/147], loss=80.2804
	step [127/147], loss=78.7910
	step [128/147], loss=87.7225
	step [129/147], loss=93.2293
	step [130/147], loss=87.6642
	step [131/147], loss=77.2167
	step [132/147], loss=76.8058
	step [133/147], loss=90.9998
	step [134/147], loss=85.7017
	step [135/147], loss=76.3074
	step [136/147], loss=69.9090
	step [137/147], loss=79.0720
	step [138/147], loss=76.1051
	step [139/147], loss=82.4360
	step [140/147], loss=86.7901
	step [141/147], loss=84.2327
	step [142/147], loss=68.9565
	step [143/147], loss=89.3937
	step [144/147], loss=86.1691
	step [145/147], loss=73.9267
	step [146/147], loss=81.0868
	step [147/147], loss=17.5642
	Evaluating
	loss=0.0134, precision=0.3569, recall=0.8770, f1=0.5074
Training epoch 80
	step [1/147], loss=68.8575
	step [2/147], loss=70.1335
	step [3/147], loss=81.0137
	step [4/147], loss=95.3920
	step [5/147], loss=87.2601
	step [6/147], loss=77.2125
	step [7/147], loss=70.5916
	step [8/147], loss=95.0292
	step [9/147], loss=88.3743
	step [10/147], loss=72.6092
	step [11/147], loss=92.6094
	step [12/147], loss=80.3586
	step [13/147], loss=77.0348
	step [14/147], loss=63.9534
	step [15/147], loss=94.8680
	step [16/147], loss=82.0540
	step [17/147], loss=96.1292
	step [18/147], loss=68.9279
	step [19/147], loss=67.7755
	step [20/147], loss=79.5668
	step [21/147], loss=77.4170
	step [22/147], loss=75.6005
	step [23/147], loss=87.5613
	step [24/147], loss=86.4852
	step [25/147], loss=81.7755
	step [26/147], loss=87.9783
	step [27/147], loss=92.8784
	step [28/147], loss=93.5785
	step [29/147], loss=81.4958
	step [30/147], loss=65.8487
	step [31/147], loss=80.6502
	step [32/147], loss=84.6463
	step [33/147], loss=84.4464
	step [34/147], loss=82.6046
	step [35/147], loss=89.4277
	step [36/147], loss=68.3307
	step [37/147], loss=86.9924
	step [38/147], loss=80.3252
	step [39/147], loss=81.7534
	step [40/147], loss=90.6348
	step [41/147], loss=78.4317
	step [42/147], loss=81.8725
	step [43/147], loss=71.6093
	step [44/147], loss=81.2684
	step [45/147], loss=78.4188
	step [46/147], loss=99.0744
	step [47/147], loss=85.0184
	step [48/147], loss=92.9525
	step [49/147], loss=93.0838
	step [50/147], loss=97.6172
	step [51/147], loss=75.5697
	step [52/147], loss=73.2289
	step [53/147], loss=86.9706
	step [54/147], loss=110.3402
	step [55/147], loss=96.6011
	step [56/147], loss=83.8047
	step [57/147], loss=74.1804
	step [58/147], loss=74.9918
	step [59/147], loss=89.1460
	step [60/147], loss=79.9487
	step [61/147], loss=71.1568
	step [62/147], loss=78.1006
	step [63/147], loss=96.8441
	step [64/147], loss=82.7574
	step [65/147], loss=78.6793
	step [66/147], loss=79.6852
	step [67/147], loss=79.8854
	step [68/147], loss=85.3926
	step [69/147], loss=103.7291
	step [70/147], loss=84.9923
	step [71/147], loss=75.3574
	step [72/147], loss=92.5968
	step [73/147], loss=88.2377
	step [74/147], loss=73.4715
	step [75/147], loss=91.0151
	step [76/147], loss=74.5924
	step [77/147], loss=88.3139
	step [78/147], loss=87.6884
	step [79/147], loss=74.2639
	step [80/147], loss=79.6891
	step [81/147], loss=88.7842
	step [82/147], loss=88.0457
	step [83/147], loss=88.0620
	step [84/147], loss=76.6471
	step [85/147], loss=79.9545
	step [86/147], loss=80.6279
	step [87/147], loss=82.6183
	step [88/147], loss=87.8015
	step [89/147], loss=86.0400
	step [90/147], loss=85.6133
	step [91/147], loss=89.0238
	step [92/147], loss=92.5054
	step [93/147], loss=73.9219
	step [94/147], loss=97.9871
	step [95/147], loss=88.7636
	step [96/147], loss=86.5937
	step [97/147], loss=80.6182
	step [98/147], loss=76.1212
	step [99/147], loss=75.4292
	step [100/147], loss=86.4512
	step [101/147], loss=72.4616
	step [102/147], loss=90.8330
	step [103/147], loss=96.4359
	step [104/147], loss=84.6089
	step [105/147], loss=65.9888
	step [106/147], loss=95.9955
	step [107/147], loss=80.5613
	step [108/147], loss=85.0056
	step [109/147], loss=79.2566
	step [110/147], loss=92.3983
	step [111/147], loss=77.1547
	step [112/147], loss=68.2510
	step [113/147], loss=76.2644
	step [114/147], loss=85.1423
	step [115/147], loss=98.9911
	step [116/147], loss=76.6945
	step [117/147], loss=85.6376
	step [118/147], loss=80.4910
	step [119/147], loss=80.2312
	step [120/147], loss=77.2804
	step [121/147], loss=84.6139
	step [122/147], loss=77.0272
	step [123/147], loss=85.0340
	step [124/147], loss=83.1703
	step [125/147], loss=73.4119
	step [126/147], loss=77.2924
	step [127/147], loss=78.1912
	step [128/147], loss=80.5956
	step [129/147], loss=75.8513
	step [130/147], loss=66.1294
	step [131/147], loss=85.2252
	step [132/147], loss=102.9154
	step [133/147], loss=86.2915
	step [134/147], loss=98.1075
	step [135/147], loss=84.2541
	step [136/147], loss=84.6201
	step [137/147], loss=96.2380
	step [138/147], loss=88.1945
	step [139/147], loss=76.2403
	step [140/147], loss=84.0008
	step [141/147], loss=90.3719
	step [142/147], loss=98.3137
	step [143/147], loss=75.7632
	step [144/147], loss=86.2870
	step [145/147], loss=84.5999
	step [146/147], loss=73.4466
	step [147/147], loss=18.9611
	Evaluating
	loss=0.0109, precision=0.4154, recall=0.8858, f1=0.5656
Training epoch 81
	step [1/147], loss=95.5631
	step [2/147], loss=87.4393
	step [3/147], loss=80.7366
	step [4/147], loss=95.6492
	step [5/147], loss=89.5851
	step [6/147], loss=80.4694
	step [7/147], loss=82.2319
	step [8/147], loss=85.6631
	step [9/147], loss=83.7878
	step [10/147], loss=83.8513
	step [11/147], loss=78.3684
	step [12/147], loss=80.5120
	step [13/147], loss=74.9753
	step [14/147], loss=78.7091
	step [15/147], loss=84.0258
	step [16/147], loss=70.2499
	step [17/147], loss=84.1120
	step [18/147], loss=70.0028
	step [19/147], loss=82.6805
	step [20/147], loss=88.5708
	step [21/147], loss=89.7036
	step [22/147], loss=77.3872
	step [23/147], loss=79.8095
	step [24/147], loss=83.7690
	step [25/147], loss=77.2435
	step [26/147], loss=88.5978
	step [27/147], loss=75.3662
	step [28/147], loss=67.1531
	step [29/147], loss=92.2706
	step [30/147], loss=87.6056
	step [31/147], loss=80.6157
	step [32/147], loss=79.1095
	step [33/147], loss=90.6184
	step [34/147], loss=78.4418
	step [35/147], loss=91.1848
	step [36/147], loss=76.1985
	step [37/147], loss=91.4080
	step [38/147], loss=75.3973
	step [39/147], loss=86.5418
	step [40/147], loss=82.5960
	step [41/147], loss=71.1624
	step [42/147], loss=84.7545
	step [43/147], loss=95.8911
	step [44/147], loss=96.7118
	step [45/147], loss=98.7811
	step [46/147], loss=73.8542
	step [47/147], loss=92.5047
	step [48/147], loss=75.4172
	step [49/147], loss=63.5815
	step [50/147], loss=70.9905
	step [51/147], loss=94.4405
	step [52/147], loss=70.1112
	step [53/147], loss=80.9377
	step [54/147], loss=80.3899
	step [55/147], loss=96.9404
	step [56/147], loss=85.0788
	step [57/147], loss=74.8762
	step [58/147], loss=69.6916
	step [59/147], loss=84.0888
	step [60/147], loss=73.6156
	step [61/147], loss=81.9642
	step [62/147], loss=81.9045
	step [63/147], loss=73.7351
	step [64/147], loss=82.0291
	step [65/147], loss=83.9365
	step [66/147], loss=93.4192
	step [67/147], loss=80.4676
	step [68/147], loss=99.6824
	step [69/147], loss=80.1559
	step [70/147], loss=90.8793
	step [71/147], loss=78.9657
	step [72/147], loss=78.8022
	step [73/147], loss=72.6016
	step [74/147], loss=82.1092
	step [75/147], loss=90.2768
	step [76/147], loss=72.1829
	step [77/147], loss=76.3272
	step [78/147], loss=85.7514
	step [79/147], loss=69.9400
	step [80/147], loss=83.4389
	step [81/147], loss=100.8995
	step [82/147], loss=73.6618
	step [83/147], loss=99.0329
	step [84/147], loss=87.4426
	step [85/147], loss=89.6143
	step [86/147], loss=80.3587
	step [87/147], loss=87.8691
	step [88/147], loss=91.3983
	step [89/147], loss=91.0677
	step [90/147], loss=84.8485
	step [91/147], loss=98.2097
	step [92/147], loss=83.0997
	step [93/147], loss=91.6205
	step [94/147], loss=83.6682
	step [95/147], loss=82.4099
	step [96/147], loss=86.2122
	step [97/147], loss=63.5207
	step [98/147], loss=81.7794
	step [99/147], loss=86.7495
	step [100/147], loss=75.6459
	step [101/147], loss=93.7535
	step [102/147], loss=90.6576
	step [103/147], loss=85.2868
	step [104/147], loss=78.4246
	step [105/147], loss=73.5230
	step [106/147], loss=98.9357
	step [107/147], loss=88.9446
	step [108/147], loss=82.1763
	step [109/147], loss=69.0918
	step [110/147], loss=75.5130
	step [111/147], loss=77.7151
	step [112/147], loss=93.7250
	step [113/147], loss=84.6030
	step [114/147], loss=87.6149
	step [115/147], loss=84.0377
	step [116/147], loss=81.8241
	step [117/147], loss=84.2174
	step [118/147], loss=77.6980
	step [119/147], loss=85.5972
	step [120/147], loss=78.8268
	step [121/147], loss=97.5936
	step [122/147], loss=84.0639
	step [123/147], loss=71.6621
	step [124/147], loss=83.1406
	step [125/147], loss=74.6668
	step [126/147], loss=88.0581
	step [127/147], loss=79.1534
	step [128/147], loss=87.4234
	step [129/147], loss=68.8373
	step [130/147], loss=77.6863
	step [131/147], loss=80.8372
	step [132/147], loss=73.8878
	step [133/147], loss=96.6285
	step [134/147], loss=79.3192
	step [135/147], loss=84.9185
	step [136/147], loss=92.0685
	step [137/147], loss=74.0199
	step [138/147], loss=98.5052
	step [139/147], loss=87.3376
	step [140/147], loss=89.0621
	step [141/147], loss=76.6044
	step [142/147], loss=82.0181
	step [143/147], loss=71.7886
	step [144/147], loss=92.9102
	step [145/147], loss=74.4747
	step [146/147], loss=74.9133
	step [147/147], loss=23.5505
	Evaluating
	loss=0.0116, precision=0.3984, recall=0.8912, f1=0.5506
Training epoch 82
	step [1/147], loss=82.5002
	step [2/147], loss=74.3772
	step [3/147], loss=92.9400
	step [4/147], loss=95.3921
	step [5/147], loss=87.0582
	step [6/147], loss=82.3529
	step [7/147], loss=70.4596
	step [8/147], loss=78.3509
	step [9/147], loss=80.8141
	step [10/147], loss=80.7075
	step [11/147], loss=78.0683
	step [12/147], loss=80.4683
	step [13/147], loss=78.0770
	step [14/147], loss=95.8339
	step [15/147], loss=91.8572
	step [16/147], loss=77.5994
	step [17/147], loss=84.8067
	step [18/147], loss=58.2411
	step [19/147], loss=81.3034
	step [20/147], loss=77.2067
	step [21/147], loss=82.6182
	step [22/147], loss=77.1022
	step [23/147], loss=86.2291
	step [24/147], loss=83.7482
	step [25/147], loss=93.5004
	step [26/147], loss=88.0851
	step [27/147], loss=99.0509
	step [28/147], loss=78.6653
	step [29/147], loss=80.5475
	step [30/147], loss=86.2775
	step [31/147], loss=69.6483
	step [32/147], loss=81.8245
	step [33/147], loss=65.6200
	step [34/147], loss=81.7637
	step [35/147], loss=88.6557
	step [36/147], loss=88.7256
	step [37/147], loss=74.6720
	step [38/147], loss=82.4438
	step [39/147], loss=83.4736
	step [40/147], loss=82.2565
	step [41/147], loss=72.6222
	step [42/147], loss=81.0455
	step [43/147], loss=73.7789
	step [44/147], loss=96.5303
	step [45/147], loss=88.6685
	step [46/147], loss=88.4502
	step [47/147], loss=93.7225
	step [48/147], loss=95.9022
	step [49/147], loss=90.8301
	step [50/147], loss=76.7689
	step [51/147], loss=75.6640
	step [52/147], loss=86.6073
	step [53/147], loss=78.1258
	step [54/147], loss=88.9827
	step [55/147], loss=82.4236
	step [56/147], loss=77.7124
	step [57/147], loss=81.9340
	step [58/147], loss=87.7404
	step [59/147], loss=74.3400
	step [60/147], loss=90.2930
	step [61/147], loss=91.9711
	step [62/147], loss=86.4353
	step [63/147], loss=66.1215
	step [64/147], loss=85.4974
	step [65/147], loss=80.6317
	step [66/147], loss=77.8688
	step [67/147], loss=86.1430
	step [68/147], loss=67.5867
	step [69/147], loss=89.8687
	step [70/147], loss=87.5513
	step [71/147], loss=78.9496
	step [72/147], loss=76.6126
	step [73/147], loss=76.2053
	step [74/147], loss=85.4724
	step [75/147], loss=82.9834
	step [76/147], loss=84.3011
	step [77/147], loss=95.2350
	step [78/147], loss=94.8413
	step [79/147], loss=83.9098
	step [80/147], loss=72.4029
	step [81/147], loss=76.0952
	step [82/147], loss=99.0001
	step [83/147], loss=63.1164
	step [84/147], loss=98.1995
	step [85/147], loss=68.8186
	step [86/147], loss=91.8806
	step [87/147], loss=75.2302
	step [88/147], loss=70.8304
	step [89/147], loss=87.4634
	step [90/147], loss=82.6735
	step [91/147], loss=88.9332
	step [92/147], loss=78.4819
	step [93/147], loss=91.9512
	step [94/147], loss=91.0716
	step [95/147], loss=75.6997
	step [96/147], loss=87.8434
	step [97/147], loss=81.1343
	step [98/147], loss=72.0688
	step [99/147], loss=104.7392
	step [100/147], loss=68.7016
	step [101/147], loss=76.9626
	step [102/147], loss=80.2929
	step [103/147], loss=76.3618
	step [104/147], loss=92.7720
	step [105/147], loss=87.6241
	step [106/147], loss=74.2705
	step [107/147], loss=70.2535
	step [108/147], loss=74.4040
	step [109/147], loss=84.8140
	step [110/147], loss=72.5638
	step [111/147], loss=75.8370
	step [112/147], loss=75.1580
	step [113/147], loss=82.6029
	step [114/147], loss=87.6967
	step [115/147], loss=81.0076
	step [116/147], loss=90.4359
	step [117/147], loss=92.0605
	step [118/147], loss=76.5852
	step [119/147], loss=91.3492
	step [120/147], loss=88.2358
	step [121/147], loss=80.8409
	step [122/147], loss=78.3226
	step [123/147], loss=99.2266
	step [124/147], loss=78.8760
	step [125/147], loss=91.1665
	step [126/147], loss=86.8367
	step [127/147], loss=84.8047
	step [128/147], loss=80.0741
	step [129/147], loss=90.0865
	step [130/147], loss=74.0537
	step [131/147], loss=86.8743
	step [132/147], loss=95.3605
	step [133/147], loss=87.3743
	step [134/147], loss=82.3158
	step [135/147], loss=75.0967
	step [136/147], loss=83.4575
	step [137/147], loss=86.8146
	step [138/147], loss=86.0534
	step [139/147], loss=72.6059
	step [140/147], loss=82.0608
	step [141/147], loss=78.2438
	step [142/147], loss=72.4700
	step [143/147], loss=76.5974
	step [144/147], loss=95.3467
	step [145/147], loss=101.8179
	step [146/147], loss=89.4926
	step [147/147], loss=19.2988
	Evaluating
	loss=0.0129, precision=0.3679, recall=0.8731, f1=0.5177
Training epoch 83
	step [1/147], loss=70.0519
	step [2/147], loss=88.6902
	step [3/147], loss=89.8441
	step [4/147], loss=75.3757
	step [5/147], loss=73.5080
	step [6/147], loss=70.9487
	step [7/147], loss=91.3358
	step [8/147], loss=91.7593
	step [9/147], loss=80.0343
	step [10/147], loss=82.9104
	step [11/147], loss=87.2333
	step [12/147], loss=74.9229
	step [13/147], loss=66.5778
	step [14/147], loss=70.7660
	step [15/147], loss=91.1589
	step [16/147], loss=84.2384
	step [17/147], loss=92.4291
	step [18/147], loss=85.6323
	step [19/147], loss=81.9280
	step [20/147], loss=87.5172
	step [21/147], loss=76.8673
	step [22/147], loss=70.2268
	step [23/147], loss=89.1062
	step [24/147], loss=83.4938
	step [25/147], loss=72.3039
	step [26/147], loss=82.1461
	step [27/147], loss=88.8166
	step [28/147], loss=84.7285
	step [29/147], loss=101.5021
	step [30/147], loss=86.9917
	step [31/147], loss=88.8621
	step [32/147], loss=83.8543
	step [33/147], loss=106.2889
	step [34/147], loss=74.1596
	step [35/147], loss=64.7678
	step [36/147], loss=78.7992
	step [37/147], loss=83.3296
	step [38/147], loss=88.9730
	step [39/147], loss=67.3224
	step [40/147], loss=81.0118
	step [41/147], loss=68.1068
	step [42/147], loss=87.5732
	step [43/147], loss=84.2111
	step [44/147], loss=75.9416
	step [45/147], loss=94.1594
	step [46/147], loss=111.2458
	step [47/147], loss=84.2130
	step [48/147], loss=93.1089
	step [49/147], loss=84.1247
	step [50/147], loss=81.0420
	step [51/147], loss=75.2253
	step [52/147], loss=89.2640
	step [53/147], loss=90.1947
	step [54/147], loss=82.8317
	step [55/147], loss=81.2604
	step [56/147], loss=76.1748
	step [57/147], loss=62.7159
	step [58/147], loss=60.3876
	step [59/147], loss=85.5981
	step [60/147], loss=81.3097
	step [61/147], loss=81.5985
	step [62/147], loss=74.6784
	step [63/147], loss=87.1728
	step [64/147], loss=73.9619
	step [65/147], loss=73.7911
	step [66/147], loss=78.9743
	step [67/147], loss=72.9561
	step [68/147], loss=87.4019
	step [69/147], loss=88.4071
	step [70/147], loss=82.4170
	step [71/147], loss=79.2782
	step [72/147], loss=75.7940
	step [73/147], loss=94.6525
	step [74/147], loss=75.8690
	step [75/147], loss=88.6371
	step [76/147], loss=92.8301
	step [77/147], loss=67.2304
	step [78/147], loss=89.1376
	step [79/147], loss=91.5201
	step [80/147], loss=82.5981
	step [81/147], loss=84.0284
	step [82/147], loss=80.4276
	step [83/147], loss=91.4677
	step [84/147], loss=80.9895
	step [85/147], loss=98.3555
	step [86/147], loss=73.6246
	step [87/147], loss=79.9908
	step [88/147], loss=84.8005
	step [89/147], loss=86.8117
	step [90/147], loss=77.9213
	step [91/147], loss=88.3114
	step [92/147], loss=83.9905
	step [93/147], loss=90.7026
	step [94/147], loss=89.9420
	step [95/147], loss=89.3208
	step [96/147], loss=89.1391
	step [97/147], loss=88.9354
	step [98/147], loss=76.5033
	step [99/147], loss=78.0967
	step [100/147], loss=74.6043
	step [101/147], loss=71.2590
	step [102/147], loss=100.8246
	step [103/147], loss=84.4391
	step [104/147], loss=78.8165
	step [105/147], loss=88.1116
	step [106/147], loss=84.3727
	step [107/147], loss=75.2607
	step [108/147], loss=89.3382
	step [109/147], loss=80.8731
	step [110/147], loss=62.1497
	step [111/147], loss=90.0536
	step [112/147], loss=86.9389
	step [113/147], loss=73.4296
	step [114/147], loss=80.9015
	step [115/147], loss=82.1790
	step [116/147], loss=80.8405
	step [117/147], loss=71.0828
	step [118/147], loss=89.9988
	step [119/147], loss=88.8731
	step [120/147], loss=80.2127
	step [121/147], loss=71.7355
	step [122/147], loss=98.6121
	step [123/147], loss=95.2089
	step [124/147], loss=85.9636
	step [125/147], loss=82.9554
	step [126/147], loss=69.3005
	step [127/147], loss=84.2505
	step [128/147], loss=85.9557
	step [129/147], loss=78.6748
	step [130/147], loss=77.7259
	step [131/147], loss=86.7674
	step [132/147], loss=72.9759
	step [133/147], loss=85.5044
	step [134/147], loss=97.9657
	step [135/147], loss=76.6295
	step [136/147], loss=82.2149
	step [137/147], loss=76.9149
	step [138/147], loss=77.7865
	step [139/147], loss=75.2303
	step [140/147], loss=89.1936
	step [141/147], loss=77.3348
	step [142/147], loss=79.5778
	step [143/147], loss=104.2103
	step [144/147], loss=85.4770
	step [145/147], loss=66.4448
	step [146/147], loss=89.8561
	step [147/147], loss=17.6238
	Evaluating
	loss=0.0118, precision=0.3892, recall=0.8753, f1=0.5388
Training epoch 84
	step [1/147], loss=87.6109
	step [2/147], loss=69.2663
	step [3/147], loss=80.0448
	step [4/147], loss=79.4706
	step [5/147], loss=90.5842
	step [6/147], loss=90.1543
	step [7/147], loss=84.7798
	step [8/147], loss=83.9982
	step [9/147], loss=86.0315
	step [10/147], loss=71.7071
	step [11/147], loss=86.4595
	step [12/147], loss=88.5174
	step [13/147], loss=93.8563
	step [14/147], loss=89.2458
	step [15/147], loss=94.8907
	step [16/147], loss=67.7081
	step [17/147], loss=75.4743
	step [18/147], loss=82.7702
	step [19/147], loss=89.4784
	step [20/147], loss=76.7176
	step [21/147], loss=89.6370
	step [22/147], loss=79.2783
	step [23/147], loss=85.2807
	step [24/147], loss=77.9546
	step [25/147], loss=89.8557
	step [26/147], loss=77.0708
	step [27/147], loss=72.3226
	step [28/147], loss=77.4356
	step [29/147], loss=92.0348
	step [30/147], loss=82.2056
	step [31/147], loss=87.9304
	step [32/147], loss=73.2735
	step [33/147], loss=61.5034
	step [34/147], loss=94.7130
	step [35/147], loss=75.3190
	step [36/147], loss=79.5085
	step [37/147], loss=74.0698
	step [38/147], loss=68.8869
	step [39/147], loss=75.6255
	step [40/147], loss=80.3541
	step [41/147], loss=85.4146
	step [42/147], loss=97.1651
	step [43/147], loss=84.1092
	step [44/147], loss=76.4662
	step [45/147], loss=84.9539
	step [46/147], loss=85.0010
	step [47/147], loss=66.7287
	step [48/147], loss=79.7378
	step [49/147], loss=77.5851
	step [50/147], loss=84.4442
	step [51/147], loss=76.4348
	step [52/147], loss=86.4006
	step [53/147], loss=96.3839
	step [54/147], loss=80.9971
	step [55/147], loss=85.1094
	step [56/147], loss=96.8364
	step [57/147], loss=67.9813
	step [58/147], loss=66.7786
	step [59/147], loss=69.5732
	step [60/147], loss=95.6427
	step [61/147], loss=70.6952
	step [62/147], loss=83.6199
	step [63/147], loss=83.2028
	step [64/147], loss=86.1237
	step [65/147], loss=65.1271
	step [66/147], loss=84.6874
	step [67/147], loss=75.2281
	step [68/147], loss=83.7523
	step [69/147], loss=88.2391
	step [70/147], loss=84.0763
	step [71/147], loss=87.4929
	step [72/147], loss=78.0175
	step [73/147], loss=106.1163
	step [74/147], loss=68.9943
	step [75/147], loss=78.4768
	step [76/147], loss=81.2277
	step [77/147], loss=85.9837
	step [78/147], loss=83.3782
	step [79/147], loss=84.5259
	step [80/147], loss=77.8806
	step [81/147], loss=80.0844
	step [82/147], loss=74.7744
	step [83/147], loss=79.6705
	step [84/147], loss=73.9341
	step [85/147], loss=77.4825
	step [86/147], loss=91.2602
	step [87/147], loss=77.2834
	step [88/147], loss=85.5874
	step [89/147], loss=77.7280
	step [90/147], loss=77.0033
	step [91/147], loss=74.6446
	step [92/147], loss=65.5605
	step [93/147], loss=88.4716
	step [94/147], loss=71.0735
	step [95/147], loss=86.6504
	step [96/147], loss=91.1660
	step [97/147], loss=83.4241
	step [98/147], loss=85.2376
	step [99/147], loss=71.1665
	step [100/147], loss=82.9786
	step [101/147], loss=62.5786
	step [102/147], loss=87.1443
	step [103/147], loss=80.6811
	step [104/147], loss=89.7123
	step [105/147], loss=77.5028
	step [106/147], loss=73.0317
	step [107/147], loss=86.5220
	step [108/147], loss=84.6791
	step [109/147], loss=77.0671
	step [110/147], loss=84.5425
	step [111/147], loss=78.2911
	step [112/147], loss=73.4536
	step [113/147], loss=91.5725
	step [114/147], loss=86.6778
	step [115/147], loss=97.2243
	step [116/147], loss=86.8251
	step [117/147], loss=87.6911
	step [118/147], loss=82.6483
	step [119/147], loss=85.4023
	step [120/147], loss=89.4223
	step [121/147], loss=93.9248
	step [122/147], loss=81.1730
	step [123/147], loss=80.2139
	step [124/147], loss=88.9103
	step [125/147], loss=69.6473
	step [126/147], loss=97.7582
	step [127/147], loss=92.9152
	step [128/147], loss=94.9244
	step [129/147], loss=73.7838
	step [130/147], loss=103.4162
	step [131/147], loss=86.9185
	step [132/147], loss=74.2639
	step [133/147], loss=84.6224
	step [134/147], loss=72.2990
	step [135/147], loss=96.2347
	step [136/147], loss=87.7053
	step [137/147], loss=65.6340
	step [138/147], loss=94.5440
	step [139/147], loss=84.3582
	step [140/147], loss=81.2702
	step [141/147], loss=74.3121
	step [142/147], loss=95.0643
	step [143/147], loss=90.7711
	step [144/147], loss=79.1231
	step [145/147], loss=78.5482
	step [146/147], loss=68.5775
	step [147/147], loss=19.5839
	Evaluating
	loss=0.0109, precision=0.4227, recall=0.8631, f1=0.5675
Training epoch 85
	step [1/147], loss=75.4532
	step [2/147], loss=78.6507
	step [3/147], loss=93.0958
	step [4/147], loss=83.6304
	step [5/147], loss=72.0663
	step [6/147], loss=83.1001
	step [7/147], loss=79.1254
	step [8/147], loss=73.1078
	step [9/147], loss=73.1842
	step [10/147], loss=96.4196
	step [11/147], loss=84.5430
	step [12/147], loss=86.0426
	step [13/147], loss=72.8423
	step [14/147], loss=90.5388
	step [15/147], loss=83.9619
	step [16/147], loss=83.4483
	step [17/147], loss=92.3202
	step [18/147], loss=93.1774
	step [19/147], loss=69.6247
	step [20/147], loss=84.3010
	step [21/147], loss=85.1982
	step [22/147], loss=92.1894
	step [23/147], loss=83.1111
	step [24/147], loss=67.5582
	step [25/147], loss=79.3163
	step [26/147], loss=89.2452
	step [27/147], loss=79.3451
	step [28/147], loss=95.6690
	step [29/147], loss=85.8353
	step [30/147], loss=82.3674
	step [31/147], loss=79.5010
	step [32/147], loss=74.2895
	step [33/147], loss=73.3925
	step [34/147], loss=88.8368
	step [35/147], loss=95.0680
	step [36/147], loss=69.6343
	step [37/147], loss=94.5322
	step [38/147], loss=85.7041
	step [39/147], loss=89.9313
	step [40/147], loss=95.0163
	step [41/147], loss=71.1103
	step [42/147], loss=84.8692
	step [43/147], loss=94.0575
	step [44/147], loss=79.5348
	step [45/147], loss=91.1110
	step [46/147], loss=83.2762
	step [47/147], loss=75.3226
	step [48/147], loss=83.1354
	step [49/147], loss=75.5970
	step [50/147], loss=74.7170
	step [51/147], loss=85.9502
	step [52/147], loss=88.1965
	step [53/147], loss=90.2734
	step [54/147], loss=73.3826
	step [55/147], loss=75.5790
	step [56/147], loss=66.3864
	step [57/147], loss=84.0145
	step [58/147], loss=79.7861
	step [59/147], loss=87.3955
	step [60/147], loss=90.1161
	step [61/147], loss=88.4622
	step [62/147], loss=87.4518
	step [63/147], loss=68.5630
	step [64/147], loss=85.4055
	step [65/147], loss=72.4171
	step [66/147], loss=61.7519
	step [67/147], loss=77.5398
	step [68/147], loss=82.4801
	step [69/147], loss=62.3964
	step [70/147], loss=82.7310
	step [71/147], loss=86.2918
	step [72/147], loss=74.3152
	step [73/147], loss=96.0215
	step [74/147], loss=79.6831
	step [75/147], loss=74.6259
	step [76/147], loss=91.1996
	step [77/147], loss=74.5043
	step [78/147], loss=83.6575
	step [79/147], loss=91.9032
	step [80/147], loss=75.7834
	step [81/147], loss=71.1854
	step [82/147], loss=80.4885
	step [83/147], loss=72.4433
	step [84/147], loss=83.6756
	step [85/147], loss=105.4593
	step [86/147], loss=70.6025
	step [87/147], loss=70.3673
	step [88/147], loss=91.7467
	step [89/147], loss=69.2645
	step [90/147], loss=75.1388
	step [91/147], loss=86.3968
	step [92/147], loss=69.7598
	step [93/147], loss=96.0965
	step [94/147], loss=78.6885
	step [95/147], loss=98.5023
	step [96/147], loss=78.9390
	step [97/147], loss=78.3244
	step [98/147], loss=67.7187
	step [99/147], loss=76.5384
	step [100/147], loss=80.3022
	step [101/147], loss=77.6776
	step [102/147], loss=87.4733
	step [103/147], loss=84.8452
	step [104/147], loss=75.2348
	step [105/147], loss=85.9166
	step [106/147], loss=74.2791
	step [107/147], loss=74.7074
	step [108/147], loss=76.7128
	step [109/147], loss=84.7085
	step [110/147], loss=71.3994
	step [111/147], loss=69.4071
	step [112/147], loss=77.3072
	step [113/147], loss=81.1895
	step [114/147], loss=84.7889
	step [115/147], loss=77.8406
	step [116/147], loss=96.3139
	step [117/147], loss=86.2832
	step [118/147], loss=84.6643
	step [119/147], loss=85.6576
	step [120/147], loss=73.2962
	step [121/147], loss=100.1794
	step [122/147], loss=98.6707
	step [123/147], loss=95.8859
	step [124/147], loss=78.1837
	step [125/147], loss=88.5358
	step [126/147], loss=78.1013
	step [127/147], loss=82.8555
	step [128/147], loss=72.6784
	step [129/147], loss=72.6592
	step [130/147], loss=89.3978
	step [131/147], loss=79.1865
	step [132/147], loss=85.4223
	step [133/147], loss=72.1042
	step [134/147], loss=81.0913
	step [135/147], loss=88.0621
	step [136/147], loss=90.3676
	step [137/147], loss=90.0858
	step [138/147], loss=79.0369
	step [139/147], loss=86.8181
	step [140/147], loss=88.8207
	step [141/147], loss=78.4360
	step [142/147], loss=72.7380
	step [143/147], loss=100.7780
	step [144/147], loss=94.5238
	step [145/147], loss=80.0675
	step [146/147], loss=75.7173
	step [147/147], loss=20.0679
	Evaluating
	loss=0.0121, precision=0.3861, recall=0.8898, f1=0.5385
Training epoch 86
	step [1/147], loss=80.2458
	step [2/147], loss=72.4550
	step [3/147], loss=88.3776
	step [4/147], loss=79.6879
	step [5/147], loss=77.5780
	step [6/147], loss=86.6084
	step [7/147], loss=88.5125
	step [8/147], loss=90.3805
	step [9/147], loss=59.6163
	step [10/147], loss=85.2585
	step [11/147], loss=79.6568
	step [12/147], loss=79.3043
	step [13/147], loss=77.6903
	step [14/147], loss=64.9004
	step [15/147], loss=96.3528
	step [16/147], loss=67.2057
	step [17/147], loss=73.5888
	step [18/147], loss=93.6846
	step [19/147], loss=74.6293
	step [20/147], loss=86.3085
	step [21/147], loss=83.4280
	step [22/147], loss=72.9716
	step [23/147], loss=85.8061
	step [24/147], loss=64.9725
	step [25/147], loss=83.8598
	step [26/147], loss=102.5303
	step [27/147], loss=79.0603
	step [28/147], loss=85.5915
	step [29/147], loss=76.8286
	step [30/147], loss=87.1115
	step [31/147], loss=76.6407
	step [32/147], loss=82.2294
	step [33/147], loss=84.8602
	step [34/147], loss=89.7825
	step [35/147], loss=82.0563
	step [36/147], loss=63.1348
	step [37/147], loss=100.7823
	step [38/147], loss=92.8316
	step [39/147], loss=80.8984
	step [40/147], loss=87.3953
	step [41/147], loss=80.2182
	step [42/147], loss=75.5086
	step [43/147], loss=85.7860
	step [44/147], loss=85.0699
	step [45/147], loss=88.6173
	step [46/147], loss=73.9994
	step [47/147], loss=78.2193
	step [48/147], loss=91.2699
	step [49/147], loss=86.2082
	step [50/147], loss=73.3949
	step [51/147], loss=96.3894
	step [52/147], loss=91.6989
	step [53/147], loss=78.3622
	step [54/147], loss=78.6541
	step [55/147], loss=84.4419
	step [56/147], loss=72.7287
	step [57/147], loss=86.4945
	step [58/147], loss=77.4999
	step [59/147], loss=73.7919
	step [60/147], loss=92.8362
	step [61/147], loss=57.0595
	step [62/147], loss=82.5852
	step [63/147], loss=67.9282
	step [64/147], loss=84.9557
	step [65/147], loss=92.2260
	step [66/147], loss=83.0619
	step [67/147], loss=93.2844
	step [68/147], loss=79.3714
	step [69/147], loss=79.8800
	step [70/147], loss=86.5406
	step [71/147], loss=105.0468
	step [72/147], loss=90.4550
	step [73/147], loss=89.1789
	step [74/147], loss=77.0039
	step [75/147], loss=84.7429
	step [76/147], loss=73.3770
	step [77/147], loss=83.7191
	step [78/147], loss=85.0875
	step [79/147], loss=88.5558
	step [80/147], loss=84.2630
	step [81/147], loss=88.3732
	step [82/147], loss=69.4304
	step [83/147], loss=76.4547
	step [84/147], loss=97.5617
	step [85/147], loss=83.2407
	step [86/147], loss=76.0575
	step [87/147], loss=68.9032
	step [88/147], loss=91.7906
	step [89/147], loss=75.8254
	step [90/147], loss=74.6690
	step [91/147], loss=104.4375
	step [92/147], loss=78.6544
	step [93/147], loss=94.6837
	step [94/147], loss=86.5253
	step [95/147], loss=76.1678
	step [96/147], loss=82.3162
	step [97/147], loss=78.7540
	step [98/147], loss=100.7902
	step [99/147], loss=78.4466
	step [100/147], loss=73.0406
	step [101/147], loss=77.9403
	step [102/147], loss=73.8983
	step [103/147], loss=81.7942
	step [104/147], loss=81.0185
	step [105/147], loss=81.0211
	step [106/147], loss=70.0106
	step [107/147], loss=74.4551
	step [108/147], loss=103.4393
	step [109/147], loss=79.0470
	step [110/147], loss=91.8298
	step [111/147], loss=76.2299
	step [112/147], loss=82.6154
	step [113/147], loss=82.1224
	step [114/147], loss=83.1942
	step [115/147], loss=82.8695
	step [116/147], loss=70.7030
	step [117/147], loss=83.9701
	step [118/147], loss=73.9657
	step [119/147], loss=79.3854
	step [120/147], loss=86.4897
	step [121/147], loss=83.1513
	step [122/147], loss=75.9886
	step [123/147], loss=91.5986
	step [124/147], loss=74.9707
	step [125/147], loss=77.3006
	step [126/147], loss=89.8321
	step [127/147], loss=61.4593
	step [128/147], loss=70.5427
	step [129/147], loss=98.5740
	step [130/147], loss=91.3812
	step [131/147], loss=93.5742
	step [132/147], loss=70.5257
	step [133/147], loss=90.1807
	step [134/147], loss=84.6117
	step [135/147], loss=71.7406
	step [136/147], loss=87.2204
	step [137/147], loss=66.6144
	step [138/147], loss=90.0393
	step [139/147], loss=73.3576
	step [140/147], loss=79.9225
	step [141/147], loss=74.8622
	step [142/147], loss=67.3799
	step [143/147], loss=70.6543
	step [144/147], loss=72.6383
	step [145/147], loss=72.1145
	step [146/147], loss=84.7108
	step [147/147], loss=16.7760
	Evaluating
	loss=0.0114, precision=0.3949, recall=0.8706, f1=0.5433
Training epoch 87
	step [1/147], loss=76.1659
	step [2/147], loss=102.2791
	step [3/147], loss=76.7350
	step [4/147], loss=68.0551
	step [5/147], loss=67.6994
	step [6/147], loss=90.4648
	step [7/147], loss=69.9444
	step [8/147], loss=76.0262
	step [9/147], loss=95.5947
	step [10/147], loss=79.7630
	step [11/147], loss=75.2247
	step [12/147], loss=79.2826
	step [13/147], loss=83.5004
	step [14/147], loss=65.1225
	step [15/147], loss=82.4564
	step [16/147], loss=82.8859
	step [17/147], loss=78.4845
	step [18/147], loss=89.5462
	step [19/147], loss=77.8885
	step [20/147], loss=82.3957
	step [21/147], loss=90.3088
	step [22/147], loss=80.7822
	step [23/147], loss=76.2453
	step [24/147], loss=97.4842
	step [25/147], loss=88.1845
	step [26/147], loss=79.3784
	step [27/147], loss=69.1350
	step [28/147], loss=63.7059
	step [29/147], loss=81.8144
	step [30/147], loss=78.9428
	step [31/147], loss=67.7763
	step [32/147], loss=77.8749
	step [33/147], loss=94.1463
	step [34/147], loss=90.2591
	step [35/147], loss=76.9461
	step [36/147], loss=81.1537
	step [37/147], loss=81.8123
	step [38/147], loss=80.3742
	step [39/147], loss=93.2091
	step [40/147], loss=112.7836
	step [41/147], loss=87.7338
	step [42/147], loss=65.7176
	step [43/147], loss=93.0298
	step [44/147], loss=87.9606
	step [45/147], loss=90.1529
	step [46/147], loss=93.5131
	step [47/147], loss=87.5377
	step [48/147], loss=76.7865
	step [49/147], loss=96.7658
	step [50/147], loss=86.9565
	step [51/147], loss=89.4211
	step [52/147], loss=86.7531
	step [53/147], loss=84.6082
	step [54/147], loss=93.4170
	step [55/147], loss=82.6741
	step [56/147], loss=74.3977
	step [57/147], loss=80.0096
	step [58/147], loss=77.1727
	step [59/147], loss=74.2063
	step [60/147], loss=82.1250
	step [61/147], loss=85.2922
	step [62/147], loss=64.0345
	step [63/147], loss=65.9065
	step [64/147], loss=69.7142
	step [65/147], loss=67.8250
	step [66/147], loss=82.1680
	step [67/147], loss=77.9812
	step [68/147], loss=85.7399
	step [69/147], loss=86.6842
	step [70/147], loss=97.4570
	step [71/147], loss=82.0018
	step [72/147], loss=80.2742
	step [73/147], loss=70.2798
	step [74/147], loss=94.7575
	step [75/147], loss=76.9969
	step [76/147], loss=86.9402
	step [77/147], loss=90.7599
	step [78/147], loss=82.9835
	step [79/147], loss=69.4991
	step [80/147], loss=70.6151
	step [81/147], loss=88.1688
	step [82/147], loss=69.5663
	step [83/147], loss=78.4344
	step [84/147], loss=78.4469
	step [85/147], loss=76.2088
	step [86/147], loss=80.9693
	step [87/147], loss=76.9291
	step [88/147], loss=84.9014
	step [89/147], loss=92.9669
	step [90/147], loss=85.1384
	step [91/147], loss=90.4328
	step [92/147], loss=93.1910
	step [93/147], loss=84.4390
	step [94/147], loss=85.9081
	step [95/147], loss=78.7894
	step [96/147], loss=89.0882
	step [97/147], loss=73.3977
	step [98/147], loss=88.8436
	step [99/147], loss=70.2576
	step [100/147], loss=73.0766
	step [101/147], loss=82.1985
	step [102/147], loss=81.7117
	step [103/147], loss=83.4251
	step [104/147], loss=72.6922
	step [105/147], loss=82.0774
	step [106/147], loss=77.4551
	step [107/147], loss=83.2664
	step [108/147], loss=78.0814
	step [109/147], loss=86.8909
	step [110/147], loss=72.6414
	step [111/147], loss=82.5522
	step [112/147], loss=82.3047
	step [113/147], loss=85.6289
	step [114/147], loss=76.8933
	step [115/147], loss=96.4788
	step [116/147], loss=73.7759
	step [117/147], loss=91.0401
	step [118/147], loss=75.8507
	step [119/147], loss=77.1370
	step [120/147], loss=69.3757
	step [121/147], loss=70.4548
	step [122/147], loss=92.3816
	step [123/147], loss=96.8039
	step [124/147], loss=77.6074
	step [125/147], loss=73.8061
	step [126/147], loss=80.9097
	step [127/147], loss=77.4558
	step [128/147], loss=70.0158
	step [129/147], loss=83.4107
	step [130/147], loss=82.0022
	step [131/147], loss=77.5762
	step [132/147], loss=73.8083
	step [133/147], loss=69.6150
	step [134/147], loss=71.7433
	step [135/147], loss=81.6148
	step [136/147], loss=78.4935
	step [137/147], loss=69.1601
	step [138/147], loss=79.6608
	step [139/147], loss=85.1508
	step [140/147], loss=95.7049
	step [141/147], loss=72.5166
	step [142/147], loss=87.4548
	step [143/147], loss=77.8937
	step [144/147], loss=78.9012
	step [145/147], loss=101.2065
	step [146/147], loss=81.1543
	step [147/147], loss=24.3288
	Evaluating
	loss=0.0119, precision=0.3797, recall=0.8793, f1=0.5304
Training epoch 88
	step [1/147], loss=76.0139
	step [2/147], loss=76.2684
	step [3/147], loss=87.9336
	step [4/147], loss=84.9128
	step [5/147], loss=78.1503
	step [6/147], loss=86.9579
	step [7/147], loss=85.4872
	step [8/147], loss=73.3008
	step [9/147], loss=100.7955
	step [10/147], loss=80.1761
	step [11/147], loss=84.2356
	step [12/147], loss=73.0326
	step [13/147], loss=81.7511
	step [14/147], loss=81.8370
	step [15/147], loss=112.1592
	step [16/147], loss=83.0432
	step [17/147], loss=68.5024
	step [18/147], loss=104.6963
	step [19/147], loss=80.1002
	step [20/147], loss=66.8938
	step [21/147], loss=85.3322
	step [22/147], loss=79.3499
	step [23/147], loss=87.2765
	step [24/147], loss=93.8795
	step [25/147], loss=71.7393
	step [26/147], loss=78.9113
	step [27/147], loss=89.1773
	step [28/147], loss=78.5766
	step [29/147], loss=90.0663
	step [30/147], loss=76.1526
	step [31/147], loss=88.8455
	step [32/147], loss=71.8663
	step [33/147], loss=91.2827
	step [34/147], loss=89.6002
	step [35/147], loss=75.6060
	step [36/147], loss=70.9577
	step [37/147], loss=78.3328
	step [38/147], loss=84.5452
	step [39/147], loss=73.5597
	step [40/147], loss=74.7220
	step [41/147], loss=83.0495
	step [42/147], loss=76.5876
	step [43/147], loss=72.9497
	step [44/147], loss=94.0861
	step [45/147], loss=76.7976
	step [46/147], loss=88.8901
	step [47/147], loss=71.5083
	step [48/147], loss=85.7120
	step [49/147], loss=99.5557
	step [50/147], loss=94.5661
	step [51/147], loss=80.7541
	step [52/147], loss=84.7821
	step [53/147], loss=87.7443
	step [54/147], loss=75.9924
	step [55/147], loss=92.3395
	step [56/147], loss=68.4465
	step [57/147], loss=96.2899
	step [58/147], loss=82.0742
	step [59/147], loss=68.6172
	step [60/147], loss=84.6706
	step [61/147], loss=90.9595
	step [62/147], loss=69.1248
	step [63/147], loss=89.0497
	step [64/147], loss=93.8373
	step [65/147], loss=84.3879
	step [66/147], loss=77.1810
	step [67/147], loss=86.9357
	step [68/147], loss=72.9558
	step [69/147], loss=83.8097
	step [70/147], loss=76.6271
	step [71/147], loss=88.5423
	step [72/147], loss=72.6734
	step [73/147], loss=70.1747
	step [74/147], loss=88.9657
	step [75/147], loss=89.5400
	step [76/147], loss=77.9226
	step [77/147], loss=76.4178
	step [78/147], loss=70.3170
	step [79/147], loss=84.2173
	step [80/147], loss=72.7241
	step [81/147], loss=88.0957
	step [82/147], loss=77.6017
	step [83/147], loss=70.4327
	step [84/147], loss=90.1203
	step [85/147], loss=89.5485
	step [86/147], loss=90.3184
	step [87/147], loss=79.9844
	step [88/147], loss=73.7920
	step [89/147], loss=81.4277
	step [90/147], loss=88.4337
	step [91/147], loss=87.5257
	step [92/147], loss=82.5798
	step [93/147], loss=78.2035
	step [94/147], loss=89.5673
	step [95/147], loss=88.6149
	step [96/147], loss=76.2354
	step [97/147], loss=62.9770
	step [98/147], loss=95.1685
	step [99/147], loss=74.8718
	step [100/147], loss=72.8678
	step [101/147], loss=92.6350
	step [102/147], loss=78.7612
	step [103/147], loss=99.3627
	step [104/147], loss=91.2147
	step [105/147], loss=76.6463
	step [106/147], loss=91.2491
	step [107/147], loss=80.6103
	step [108/147], loss=77.7433
	step [109/147], loss=74.4141
	step [110/147], loss=76.8573
	step [111/147], loss=83.0304
	step [112/147], loss=90.6984
	step [113/147], loss=75.7461
	step [114/147], loss=70.5142
	step [115/147], loss=84.5794
	step [116/147], loss=78.2630
	step [117/147], loss=94.2557
	step [118/147], loss=73.2546
	step [119/147], loss=77.4018
	step [120/147], loss=66.1158
	step [121/147], loss=85.7459
	step [122/147], loss=66.8830
	step [123/147], loss=81.2478
	step [124/147], loss=87.1149
	step [125/147], loss=79.3740
	step [126/147], loss=68.7110
	step [127/147], loss=64.9266
	step [128/147], loss=81.2005
	step [129/147], loss=84.6687
	step [130/147], loss=83.8111
	step [131/147], loss=85.1312
	step [132/147], loss=84.6874
	step [133/147], loss=82.8338
	step [134/147], loss=67.6497
	step [135/147], loss=85.5990
	step [136/147], loss=63.5218
	step [137/147], loss=85.0872
	step [138/147], loss=88.1935
	step [139/147], loss=65.2913
	step [140/147], loss=79.8481
	step [141/147], loss=84.1498
	step [142/147], loss=70.0261
	step [143/147], loss=79.0538
	step [144/147], loss=75.4512
	step [145/147], loss=72.1139
	step [146/147], loss=66.5505
	step [147/147], loss=20.4879
	Evaluating
	loss=0.0117, precision=0.3901, recall=0.8720, f1=0.5391
Training epoch 89
	step [1/147], loss=85.7690
	step [2/147], loss=78.5257
	step [3/147], loss=85.4243
	step [4/147], loss=89.1312
	step [5/147], loss=71.6654
	step [6/147], loss=90.0509
	step [7/147], loss=77.1691
	step [8/147], loss=88.9539
	step [9/147], loss=81.9977
	step [10/147], loss=66.7773
	step [11/147], loss=96.2162
	step [12/147], loss=75.3332
	step [13/147], loss=87.5673
	step [14/147], loss=94.0494
	step [15/147], loss=84.1784
	step [16/147], loss=70.6178
	step [17/147], loss=87.2521
	step [18/147], loss=82.8835
	step [19/147], loss=82.1895
	step [20/147], loss=88.1751
	step [21/147], loss=92.9071
	step [22/147], loss=78.0489
	step [23/147], loss=84.8437
	step [24/147], loss=84.9568
	step [25/147], loss=67.9306
	step [26/147], loss=86.8236
	step [27/147], loss=78.6166
	step [28/147], loss=88.2670
	step [29/147], loss=77.5491
	step [30/147], loss=76.1747
	step [31/147], loss=89.3539
	step [32/147], loss=101.3821
	step [33/147], loss=75.5607
	step [34/147], loss=80.4179
	step [35/147], loss=82.9435
	step [36/147], loss=50.0268
	step [37/147], loss=83.1894
	step [38/147], loss=83.3364
	step [39/147], loss=63.7532
	step [40/147], loss=82.7768
	step [41/147], loss=78.1390
	step [42/147], loss=75.0996
	step [43/147], loss=80.7928
	step [44/147], loss=79.4944
	step [45/147], loss=80.8582
	step [46/147], loss=72.6226
	step [47/147], loss=77.3397
	step [48/147], loss=78.7082
	step [49/147], loss=66.9260
	step [50/147], loss=89.0141
	step [51/147], loss=77.0980
	step [52/147], loss=74.9164
	step [53/147], loss=75.6973
	step [54/147], loss=102.5821
	step [55/147], loss=84.0892
	step [56/147], loss=78.9359
	step [57/147], loss=79.4011
	step [58/147], loss=79.7609
	step [59/147], loss=79.6455
	step [60/147], loss=94.0095
	step [61/147], loss=77.1237
	step [62/147], loss=88.0965
	step [63/147], loss=71.0280
	step [64/147], loss=89.8133
	step [65/147], loss=93.2345
	step [66/147], loss=70.8150
	step [67/147], loss=72.4296
	step [68/147], loss=85.2110
	step [69/147], loss=73.8163
	step [70/147], loss=92.4887
	step [71/147], loss=81.9483
	step [72/147], loss=88.1219
	step [73/147], loss=84.2685
	step [74/147], loss=72.6749
	step [75/147], loss=84.4027
	step [76/147], loss=72.1202
	step [77/147], loss=81.4122
	step [78/147], loss=86.2906
	step [79/147], loss=87.5069
	step [80/147], loss=82.4500
	step [81/147], loss=79.6847
	step [82/147], loss=79.6792
	step [83/147], loss=77.5716
	step [84/147], loss=87.6520
	step [85/147], loss=76.4560
	step [86/147], loss=75.2533
	step [87/147], loss=70.4819
	step [88/147], loss=73.3199
	step [89/147], loss=93.9523
	step [90/147], loss=84.9250
	step [91/147], loss=58.8548
	step [92/147], loss=80.9116
	step [93/147], loss=69.0267
	step [94/147], loss=61.1792
	step [95/147], loss=78.9483
	step [96/147], loss=74.3328
	step [97/147], loss=85.7834
	step [98/147], loss=73.2631
	step [99/147], loss=96.2401
	step [100/147], loss=81.4351
	step [101/147], loss=66.1302
	step [102/147], loss=80.0909
	step [103/147], loss=77.2926
	step [104/147], loss=75.7604
	step [105/147], loss=74.9900
	step [106/147], loss=70.0520
	step [107/147], loss=76.1059
	step [108/147], loss=98.9278
	step [109/147], loss=70.7303
	step [110/147], loss=77.6938
	step [111/147], loss=83.2807
	step [112/147], loss=84.5175
	step [113/147], loss=80.3850
	step [114/147], loss=71.8422
	step [115/147], loss=70.1617
	step [116/147], loss=77.2600
	step [117/147], loss=77.8491
	step [118/147], loss=79.7834
	step [119/147], loss=63.7896
	step [120/147], loss=88.3357
	step [121/147], loss=94.8381
	step [122/147], loss=92.8811
	step [123/147], loss=77.0856
	step [124/147], loss=72.0928
	step [125/147], loss=91.1429
	step [126/147], loss=72.4819
	step [127/147], loss=81.5219
	step [128/147], loss=85.2456
	step [129/147], loss=72.3194
	step [130/147], loss=82.9593
	step [131/147], loss=81.7114
	step [132/147], loss=87.7585
	step [133/147], loss=95.3581
	step [134/147], loss=85.2834
	step [135/147], loss=76.7763
	step [136/147], loss=87.9920
	step [137/147], loss=88.5843
	step [138/147], loss=73.0840
	step [139/147], loss=91.4838
	step [140/147], loss=79.5526
	step [141/147], loss=75.3340
	step [142/147], loss=82.3692
	step [143/147], loss=78.4073
	step [144/147], loss=85.0523
	step [145/147], loss=80.1743
	step [146/147], loss=103.1923
	step [147/147], loss=20.0500
	Evaluating
	loss=0.0106, precision=0.4229, recall=0.8781, f1=0.5709
saving model as: 2_saved_model.pth
Training epoch 90
	step [1/147], loss=81.5753
	step [2/147], loss=84.6361
	step [3/147], loss=75.1183
	step [4/147], loss=85.0439
	step [5/147], loss=76.5333
	step [6/147], loss=80.7823
	step [7/147], loss=90.0427
	step [8/147], loss=81.0857
	step [9/147], loss=82.6324
	step [10/147], loss=65.8182
	step [11/147], loss=77.7203
	step [12/147], loss=73.3267
	step [13/147], loss=66.2984
	step [14/147], loss=72.3351
	step [15/147], loss=95.4984
	step [16/147], loss=86.1681
	step [17/147], loss=91.5519
	step [18/147], loss=83.0651
	step [19/147], loss=90.9097
	step [20/147], loss=81.3002
	step [21/147], loss=72.8234
	step [22/147], loss=76.6194
	step [23/147], loss=82.3773
	step [24/147], loss=78.5209
	step [25/147], loss=64.6305
	step [26/147], loss=100.4233
	step [27/147], loss=99.7474
	step [28/147], loss=72.4630
	step [29/147], loss=82.0125
	step [30/147], loss=72.7634
	step [31/147], loss=71.6996
	step [32/147], loss=68.2220
	step [33/147], loss=79.9675
	step [34/147], loss=78.8524
	step [35/147], loss=85.4932
	step [36/147], loss=68.7890
	step [37/147], loss=85.4627
	step [38/147], loss=88.5208
	step [39/147], loss=70.4908
	step [40/147], loss=86.7592
	step [41/147], loss=91.2384
	step [42/147], loss=86.0708
	step [43/147], loss=76.6867
	step [44/147], loss=73.4433
	step [45/147], loss=79.6611
	step [46/147], loss=88.6204
	step [47/147], loss=78.0472
	step [48/147], loss=88.1425
	step [49/147], loss=86.0355
	step [50/147], loss=72.9997
	step [51/147], loss=83.5735
	step [52/147], loss=71.8323
	step [53/147], loss=75.3149
	step [54/147], loss=80.4003
	step [55/147], loss=75.6334
	step [56/147], loss=81.3177
	step [57/147], loss=85.5197
	step [58/147], loss=83.7829
	step [59/147], loss=79.0914
	step [60/147], loss=87.7532
	step [61/147], loss=78.7542
	step [62/147], loss=87.5455
	step [63/147], loss=72.7165
	step [64/147], loss=91.6463
	step [65/147], loss=82.3532
	step [66/147], loss=76.5520
	step [67/147], loss=75.2645
	step [68/147], loss=80.1901
	step [69/147], loss=67.7853
	step [70/147], loss=76.9855
	step [71/147], loss=66.4622
	step [72/147], loss=111.1544
	step [73/147], loss=81.2446
	step [74/147], loss=87.0511
	step [75/147], loss=76.2819
	step [76/147], loss=84.0183
	step [77/147], loss=89.0285
	step [78/147], loss=73.6548
	step [79/147], loss=71.5082
	step [80/147], loss=83.9907
	step [81/147], loss=72.7736
	step [82/147], loss=66.0003
	step [83/147], loss=88.8745
	step [84/147], loss=85.5952
	step [85/147], loss=69.9841
	step [86/147], loss=71.7679
	step [87/147], loss=81.7086
	step [88/147], loss=77.5931
	step [89/147], loss=84.0946
	step [90/147], loss=75.4890
	step [91/147], loss=73.0442
	step [92/147], loss=82.9739
	step [93/147], loss=82.2498
	step [94/147], loss=76.0950
	step [95/147], loss=84.2244
	step [96/147], loss=60.1598
	step [97/147], loss=81.3065
	step [98/147], loss=87.2210
	step [99/147], loss=73.1550
	step [100/147], loss=68.4579
	step [101/147], loss=87.2396
	step [102/147], loss=88.8638
	step [103/147], loss=63.7306
	step [104/147], loss=84.9190
	step [105/147], loss=82.6191
	step [106/147], loss=76.7140
	step [107/147], loss=85.5760
	step [108/147], loss=73.6918
	step [109/147], loss=72.6650
	step [110/147], loss=75.3519
	step [111/147], loss=76.5104
	step [112/147], loss=80.0466
	step [113/147], loss=78.9567
	step [114/147], loss=90.3272
	step [115/147], loss=78.5443
	step [116/147], loss=86.8804
	step [117/147], loss=86.8754
	step [118/147], loss=85.4108
	step [119/147], loss=95.4453
	step [120/147], loss=81.6145
	step [121/147], loss=77.4395
	step [122/147], loss=75.6960
	step [123/147], loss=101.6820
	step [124/147], loss=91.7599
	step [125/147], loss=86.9951
	step [126/147], loss=76.2056
	step [127/147], loss=91.6182
	step [128/147], loss=89.2893
	step [129/147], loss=73.4222
	step [130/147], loss=90.8966
	step [131/147], loss=67.1848
	step [132/147], loss=70.9404
	step [133/147], loss=79.4643
	step [134/147], loss=79.6239
	step [135/147], loss=79.4053
	step [136/147], loss=78.7364
	step [137/147], loss=89.5346
	step [138/147], loss=88.0903
	step [139/147], loss=74.6871
	step [140/147], loss=99.6415
	step [141/147], loss=104.8216
	step [142/147], loss=79.9347
	step [143/147], loss=87.8398
	step [144/147], loss=87.0963
	step [145/147], loss=77.0568
	step [146/147], loss=78.7633
	step [147/147], loss=12.7279
	Evaluating
	loss=0.0120, precision=0.3816, recall=0.8831, f1=0.5329
Training epoch 91
	step [1/147], loss=92.1437
	step [2/147], loss=81.0639
	step [3/147], loss=99.3627
	step [4/147], loss=79.4295
	step [5/147], loss=84.9996
	step [6/147], loss=75.3305
	step [7/147], loss=85.0142
	step [8/147], loss=69.0989
	step [9/147], loss=86.0191
	step [10/147], loss=71.7962
	step [11/147], loss=79.2297
	step [12/147], loss=91.3189
	step [13/147], loss=91.3680
	step [14/147], loss=75.4069
	step [15/147], loss=77.0074
	step [16/147], loss=83.8737
	step [17/147], loss=72.7051
	step [18/147], loss=92.1668
	step [19/147], loss=67.8060
	step [20/147], loss=81.4902
	step [21/147], loss=91.8454
	step [22/147], loss=83.1296
	step [23/147], loss=84.7115
	step [24/147], loss=72.8378
	step [25/147], loss=76.0568
	step [26/147], loss=63.4350
	step [27/147], loss=85.9348
	step [28/147], loss=73.8318
	step [29/147], loss=70.7541
	step [30/147], loss=85.7714
	step [31/147], loss=74.8442
	step [32/147], loss=82.0619
	step [33/147], loss=63.9035
	step [34/147], loss=76.8921
	step [35/147], loss=76.6702
	step [36/147], loss=89.3672
	step [37/147], loss=74.1113
	step [38/147], loss=86.7867
	step [39/147], loss=85.7849
	step [40/147], loss=77.8038
	step [41/147], loss=85.7839
	step [42/147], loss=76.9057
	step [43/147], loss=69.5500
	step [44/147], loss=73.9295
	step [45/147], loss=76.6686
	step [46/147], loss=78.1411
	step [47/147], loss=77.7643
	step [48/147], loss=73.8099
	step [49/147], loss=67.4605
	step [50/147], loss=78.1932
	step [51/147], loss=74.3947
	step [52/147], loss=86.2585
	step [53/147], loss=93.6085
	step [54/147], loss=77.0769
	step [55/147], loss=97.4541
	step [56/147], loss=76.7967
	step [57/147], loss=85.3954
	step [58/147], loss=76.6720
	step [59/147], loss=71.0766
	step [60/147], loss=94.7385
	step [61/147], loss=97.1254
	step [62/147], loss=79.5652
	step [63/147], loss=87.0953
	step [64/147], loss=79.3530
	step [65/147], loss=90.5261
	step [66/147], loss=67.5164
	step [67/147], loss=93.2363
	step [68/147], loss=76.4791
	step [69/147], loss=83.8246
	step [70/147], loss=76.5223
	step [71/147], loss=86.0785
	step [72/147], loss=62.7707
	step [73/147], loss=78.8269
	step [74/147], loss=90.7008
	step [75/147], loss=89.8990
	step [76/147], loss=71.3147
	step [77/147], loss=73.0808
	step [78/147], loss=85.2871
	step [79/147], loss=77.7744
	step [80/147], loss=82.8891
	step [81/147], loss=78.1459
	step [82/147], loss=96.5000
	step [83/147], loss=76.8465
	step [84/147], loss=84.2355
	step [85/147], loss=72.3144
	step [86/147], loss=82.3785
	step [87/147], loss=82.6154
	step [88/147], loss=80.3280
	step [89/147], loss=91.2305
	step [90/147], loss=78.9504
	step [91/147], loss=85.3327
	step [92/147], loss=74.4098
	step [93/147], loss=72.3756
	step [94/147], loss=82.0473
	step [95/147], loss=70.3552
	step [96/147], loss=79.6073
	step [97/147], loss=81.2191
	step [98/147], loss=89.9734
	step [99/147], loss=83.6437
	step [100/147], loss=78.1163
	step [101/147], loss=83.8128
	step [102/147], loss=78.6408
	step [103/147], loss=89.2041
	step [104/147], loss=80.5675
	step [105/147], loss=66.0439
	step [106/147], loss=74.3961
	step [107/147], loss=66.6412
	step [108/147], loss=81.1377
	step [109/147], loss=85.3724
	step [110/147], loss=81.4975
	step [111/147], loss=82.7881
	step [112/147], loss=75.3007
	step [113/147], loss=77.3242
	step [114/147], loss=77.1513
	step [115/147], loss=66.5161
	step [116/147], loss=72.3513
	step [117/147], loss=71.8553
	step [118/147], loss=79.9345
	step [119/147], loss=77.5393
	step [120/147], loss=79.1949
	step [121/147], loss=83.8042
	step [122/147], loss=86.0504
	step [123/147], loss=94.5216
	step [124/147], loss=71.9500
	step [125/147], loss=85.1945
	step [126/147], loss=80.4979
	step [127/147], loss=95.1921
	step [128/147], loss=99.2561
	step [129/147], loss=93.9043
	step [130/147], loss=78.3611
	step [131/147], loss=83.2413
	step [132/147], loss=85.3594
	step [133/147], loss=82.0450
	step [134/147], loss=85.7544
	step [135/147], loss=83.1134
	step [136/147], loss=81.3047
	step [137/147], loss=100.1688
	step [138/147], loss=77.7116
	step [139/147], loss=79.2827
	step [140/147], loss=80.9534
	step [141/147], loss=85.4409
	step [142/147], loss=71.9867
	step [143/147], loss=75.8720
	step [144/147], loss=77.4733
	step [145/147], loss=75.2176
	step [146/147], loss=80.4983
	step [147/147], loss=19.8870
	Evaluating
	loss=0.0094, precision=0.4605, recall=0.8761, f1=0.6036
saving model as: 2_saved_model.pth
Training epoch 92
	step [1/147], loss=69.9603
	step [2/147], loss=108.8231
	step [3/147], loss=79.2540
	step [4/147], loss=77.9919
	step [5/147], loss=78.2701
	step [6/147], loss=77.4633
	step [7/147], loss=95.8183
	step [8/147], loss=83.0382
	step [9/147], loss=77.0203
	step [10/147], loss=71.2579
	step [11/147], loss=69.5700
	step [12/147], loss=75.6255
	step [13/147], loss=78.0087
	step [14/147], loss=71.2772
	step [15/147], loss=86.2713
	step [16/147], loss=73.8373
	step [17/147], loss=88.6279
	step [18/147], loss=78.3395
	step [19/147], loss=76.3823
	step [20/147], loss=66.0439
	step [21/147], loss=83.8647
	step [22/147], loss=81.0480
	step [23/147], loss=75.0629
	step [24/147], loss=81.8296
	step [25/147], loss=76.5102
	step [26/147], loss=73.7684
	step [27/147], loss=91.7249
	step [28/147], loss=78.9074
	step [29/147], loss=84.5232
	step [30/147], loss=92.2471
	step [31/147], loss=74.2592
	step [32/147], loss=73.6709
	step [33/147], loss=76.3900
	step [34/147], loss=78.8045
	step [35/147], loss=87.1050
	step [36/147], loss=79.0481
	step [37/147], loss=80.5930
	step [38/147], loss=85.9794
	step [39/147], loss=73.1207
	step [40/147], loss=79.2976
	step [41/147], loss=83.9740
	step [42/147], loss=86.6404
	step [43/147], loss=94.4287
	step [44/147], loss=71.1203
	step [45/147], loss=86.2573
	step [46/147], loss=86.1013
	step [47/147], loss=80.3941
	step [48/147], loss=93.4201
	step [49/147], loss=83.7601
	step [50/147], loss=65.2753
	step [51/147], loss=90.7814
	step [52/147], loss=71.9515
	step [53/147], loss=67.9842
	step [54/147], loss=83.3082
	step [55/147], loss=84.8013
	step [56/147], loss=73.3279
	step [57/147], loss=74.1504
	step [58/147], loss=84.0287
	step [59/147], loss=74.2663
	step [60/147], loss=85.7647
	step [61/147], loss=76.8522
	step [62/147], loss=82.6187
	step [63/147], loss=100.9516
	step [64/147], loss=79.7030
	step [65/147], loss=71.9333
	step [66/147], loss=81.7106
	step [67/147], loss=80.5683
	step [68/147], loss=89.5834
	step [69/147], loss=92.9861
	step [70/147], loss=77.8181
	step [71/147], loss=85.7782
	step [72/147], loss=86.0748
	step [73/147], loss=77.2606
	step [74/147], loss=76.1885
	step [75/147], loss=78.3556
	step [76/147], loss=94.4039
	step [77/147], loss=92.9835
	step [78/147], loss=88.1878
	step [79/147], loss=91.0979
	step [80/147], loss=76.2202
	step [81/147], loss=75.3801
	step [82/147], loss=75.1784
	step [83/147], loss=78.0108
	step [84/147], loss=76.6358
	step [85/147], loss=83.8805
	step [86/147], loss=77.8502
	step [87/147], loss=71.5681
	step [88/147], loss=84.0698
	step [89/147], loss=96.3586
	step [90/147], loss=101.1160
	step [91/147], loss=76.1408
	step [92/147], loss=79.9309
	step [93/147], loss=91.6363
	step [94/147], loss=65.4221
	step [95/147], loss=76.8816
	step [96/147], loss=88.8244
	step [97/147], loss=65.8589
	step [98/147], loss=77.9236
	step [99/147], loss=74.4017
	step [100/147], loss=78.3584
	step [101/147], loss=61.3685
	step [102/147], loss=81.8041
	step [103/147], loss=85.5199
	step [104/147], loss=82.1881
	step [105/147], loss=77.7702
	step [106/147], loss=77.7481
	step [107/147], loss=70.0141
	step [108/147], loss=98.6000
	step [109/147], loss=72.2745
	step [110/147], loss=73.8135
	step [111/147], loss=78.4874
	step [112/147], loss=72.5206
	step [113/147], loss=66.5029
	step [114/147], loss=90.2819
	step [115/147], loss=81.3475
	step [116/147], loss=75.5072
	step [117/147], loss=73.9587
	step [118/147], loss=82.5582
	step [119/147], loss=87.8733
	step [120/147], loss=90.8144
	step [121/147], loss=93.7387
	step [122/147], loss=79.7301
	step [123/147], loss=87.4800
	step [124/147], loss=74.8065
	step [125/147], loss=64.5199
	step [126/147], loss=83.7237
	step [127/147], loss=75.4240
	step [128/147], loss=77.3313
	step [129/147], loss=67.4497
	step [130/147], loss=87.7606
	step [131/147], loss=78.9795
	step [132/147], loss=88.0291
	step [133/147], loss=70.7183
	step [134/147], loss=86.1802
	step [135/147], loss=87.5258
	step [136/147], loss=86.4674
	step [137/147], loss=84.2262
	step [138/147], loss=71.4348
	step [139/147], loss=83.6582
	step [140/147], loss=72.0492
	step [141/147], loss=69.4580
	step [142/147], loss=82.2517
	step [143/147], loss=65.7913
	step [144/147], loss=77.3647
	step [145/147], loss=63.1475
	step [146/147], loss=71.8817
	step [147/147], loss=19.1184
	Evaluating
	loss=0.0101, precision=0.4293, recall=0.8611, f1=0.5729
Training epoch 93
	step [1/147], loss=84.0532
	step [2/147], loss=89.8241
	step [3/147], loss=85.1483
	step [4/147], loss=76.3259
	step [5/147], loss=78.7230
	step [6/147], loss=86.1755
	step [7/147], loss=72.0597
	step [8/147], loss=69.4968
	step [9/147], loss=83.7869
	step [10/147], loss=89.5467
	step [11/147], loss=85.8546
	step [12/147], loss=80.8039
	step [13/147], loss=73.3279
	step [14/147], loss=79.0167
	step [15/147], loss=78.2415
	step [16/147], loss=74.0836
	step [17/147], loss=88.9009
	step [18/147], loss=76.1438
	step [19/147], loss=79.9904
	step [20/147], loss=72.8521
	step [21/147], loss=76.4295
	step [22/147], loss=88.4834
	step [23/147], loss=91.1033
	step [24/147], loss=80.7708
	step [25/147], loss=70.2506
	step [26/147], loss=69.0762
	step [27/147], loss=74.0258
	step [28/147], loss=76.1114
	step [29/147], loss=84.8715
	step [30/147], loss=78.5811
	step [31/147], loss=81.3592
	step [32/147], loss=84.9016
	step [33/147], loss=78.0405
	step [34/147], loss=75.1877
	step [35/147], loss=90.1928
	step [36/147], loss=73.6648
	step [37/147], loss=76.8454
	step [38/147], loss=82.9300
	step [39/147], loss=96.4055
	step [40/147], loss=76.6252
	step [41/147], loss=86.3852
	step [42/147], loss=73.3089
	step [43/147], loss=86.3227
	step [44/147], loss=103.9984
	step [45/147], loss=81.1884
	step [46/147], loss=78.3135
	step [47/147], loss=82.7232
	step [48/147], loss=68.7689
	step [49/147], loss=67.6545
	step [50/147], loss=75.8531
	step [51/147], loss=76.6955
	step [52/147], loss=74.0350
	step [53/147], loss=75.9362
	step [54/147], loss=74.1988
	step [55/147], loss=64.8548
	step [56/147], loss=78.0783
	step [57/147], loss=78.2743
	step [58/147], loss=96.4194
	step [59/147], loss=78.6882
	step [60/147], loss=81.8543
	step [61/147], loss=83.4204
	step [62/147], loss=89.1772
	step [63/147], loss=86.5555
	step [64/147], loss=91.2574
	step [65/147], loss=82.5379
	step [66/147], loss=78.3987
	step [67/147], loss=87.7446
	step [68/147], loss=87.8414
	step [69/147], loss=75.5699
	step [70/147], loss=95.3092
	step [71/147], loss=68.7666
	step [72/147], loss=72.3700
	step [73/147], loss=82.8271
	step [74/147], loss=70.7785
	step [75/147], loss=72.0518
	step [76/147], loss=86.4616
	step [77/147], loss=79.0806
	step [78/147], loss=94.0827
	step [79/147], loss=68.6463
	step [80/147], loss=73.9925
	step [81/147], loss=67.5334
	step [82/147], loss=85.6323
	step [83/147], loss=82.8079
	step [84/147], loss=76.5181
	step [85/147], loss=81.8502
	step [86/147], loss=88.1156
	step [87/147], loss=78.4100
	step [88/147], loss=88.7593
	step [89/147], loss=86.6865
	step [90/147], loss=82.3855
	step [91/147], loss=81.4551
	step [92/147], loss=68.6307
	step [93/147], loss=86.6197
	step [94/147], loss=78.6482
	step [95/147], loss=80.4500
	step [96/147], loss=57.6440
	step [97/147], loss=79.0567
	step [98/147], loss=89.2920
	step [99/147], loss=68.8712
	step [100/147], loss=72.9197
	step [101/147], loss=74.6224
	step [102/147], loss=86.3001
	step [103/147], loss=85.0668
	step [104/147], loss=69.7489
	step [105/147], loss=74.2909
	step [106/147], loss=91.1049
	step [107/147], loss=76.8684
	step [108/147], loss=72.0789
	step [109/147], loss=91.3280
	step [110/147], loss=70.2528
	step [111/147], loss=80.9109
	step [112/147], loss=69.1158
	step [113/147], loss=90.7822
	step [114/147], loss=72.8473
	step [115/147], loss=79.4331
	step [116/147], loss=86.8304
	step [117/147], loss=92.7413
	step [118/147], loss=74.4898
	step [119/147], loss=80.7251
	step [120/147], loss=79.1488
	step [121/147], loss=89.6965
	step [122/147], loss=72.4485
	step [123/147], loss=74.7885
	step [124/147], loss=65.5113
	step [125/147], loss=77.0918
	step [126/147], loss=77.4727
	step [127/147], loss=72.6907
	step [128/147], loss=78.9892
	step [129/147], loss=83.5365
	step [130/147], loss=84.2263
	step [131/147], loss=91.9477
	step [132/147], loss=86.2885
	step [133/147], loss=74.0892
	step [134/147], loss=76.2384
	step [135/147], loss=76.3665
	step [136/147], loss=63.9671
	step [137/147], loss=74.0109
	step [138/147], loss=78.1281
	step [139/147], loss=87.7999
	step [140/147], loss=82.3770
	step [141/147], loss=71.4360
	step [142/147], loss=82.1740
	step [143/147], loss=61.6982
	step [144/147], loss=85.5248
	step [145/147], loss=81.3100
	step [146/147], loss=77.8902
	step [147/147], loss=14.5484
	Evaluating
	loss=0.0103, precision=0.4224, recall=0.8830, f1=0.5715
Training epoch 94
	step [1/147], loss=92.2931
	step [2/147], loss=65.5431
	step [3/147], loss=78.3639
	step [4/147], loss=82.1518
	step [5/147], loss=75.3926
	step [6/147], loss=76.4810
	step [7/147], loss=81.9508
	step [8/147], loss=83.8503
	step [9/147], loss=82.3619
	step [10/147], loss=78.8857
	step [11/147], loss=79.9579
	step [12/147], loss=78.5483
	step [13/147], loss=90.9826
	step [14/147], loss=81.7850
	step [15/147], loss=76.7961
	step [16/147], loss=100.7051
	step [17/147], loss=81.4975
	step [18/147], loss=91.5801
	step [19/147], loss=79.8795
	step [20/147], loss=90.2470
	step [21/147], loss=68.3339
	step [22/147], loss=83.9903
	step [23/147], loss=83.1790
	step [24/147], loss=77.2587
	step [25/147], loss=83.3383
	step [26/147], loss=80.7334
	step [27/147], loss=77.4946
	step [28/147], loss=64.9204
	step [29/147], loss=80.4705
	step [30/147], loss=76.3397
	step [31/147], loss=69.1645
	step [32/147], loss=82.3055
	step [33/147], loss=91.1992
	step [34/147], loss=88.4609
	step [35/147], loss=61.6341
	step [36/147], loss=70.1377
	step [37/147], loss=68.7431
	step [38/147], loss=84.3852
	step [39/147], loss=61.4422
	step [40/147], loss=86.9588
	step [41/147], loss=90.0417
	step [42/147], loss=78.9154
	step [43/147], loss=83.7383
	step [44/147], loss=84.5277
	step [45/147], loss=67.8596
	step [46/147], loss=72.6018
	step [47/147], loss=83.4489
	step [48/147], loss=99.0735
	step [49/147], loss=89.4041
	step [50/147], loss=80.2564
	step [51/147], loss=78.1316
	step [52/147], loss=68.7249
	step [53/147], loss=68.3640
	step [54/147], loss=80.0336
	step [55/147], loss=73.3028
	step [56/147], loss=70.0676
	step [57/147], loss=80.0833
	step [58/147], loss=76.1463
	step [59/147], loss=71.6033
	step [60/147], loss=85.3850
	step [61/147], loss=106.1200
	step [62/147], loss=73.0808
	step [63/147], loss=76.2538
	step [64/147], loss=86.9275
	step [65/147], loss=72.0908
	step [66/147], loss=81.0424
	step [67/147], loss=82.9087
	step [68/147], loss=73.9377
	step [69/147], loss=67.2490
	step [70/147], loss=81.2422
	step [71/147], loss=59.6250
	step [72/147], loss=76.4041
	step [73/147], loss=81.2715
	step [74/147], loss=61.6960
	step [75/147], loss=84.1490
	step [76/147], loss=77.9041
	step [77/147], loss=73.2701
	step [78/147], loss=71.9268
	step [79/147], loss=78.1317
	step [80/147], loss=92.3059
	step [81/147], loss=91.3910
	step [82/147], loss=84.4997
	step [83/147], loss=81.2854
	step [84/147], loss=77.8211
	step [85/147], loss=64.4301
	step [86/147], loss=74.5218
	step [87/147], loss=94.4372
	step [88/147], loss=88.9963
	step [89/147], loss=79.1834
	step [90/147], loss=70.3939
	step [91/147], loss=69.6705
	step [92/147], loss=72.4927
	step [93/147], loss=76.7129
	step [94/147], loss=81.2163
	step [95/147], loss=93.2725
	step [96/147], loss=86.5113
	step [97/147], loss=91.6027
	step [98/147], loss=78.1007
	step [99/147], loss=77.5287
	step [100/147], loss=87.0105
	step [101/147], loss=86.0985
	step [102/147], loss=77.4390
	step [103/147], loss=81.0849
	step [104/147], loss=72.1665
	step [105/147], loss=72.2780
	step [106/147], loss=89.1471
	step [107/147], loss=81.7972
	step [108/147], loss=77.7738
	step [109/147], loss=85.5198
	step [110/147], loss=79.6018
	step [111/147], loss=97.2669
	step [112/147], loss=67.5745
	step [113/147], loss=83.3729
	step [114/147], loss=74.1492
	step [115/147], loss=85.7940
	step [116/147], loss=68.4738
	step [117/147], loss=77.6424
	step [118/147], loss=72.0679
	step [119/147], loss=78.7028
	step [120/147], loss=92.7217
	step [121/147], loss=82.8212
	step [122/147], loss=82.6313
	step [123/147], loss=86.1784
	step [124/147], loss=92.6241
	step [125/147], loss=85.4294
	step [126/147], loss=71.8739
	step [127/147], loss=63.8478
	step [128/147], loss=67.2137
	step [129/147], loss=68.2026
	step [130/147], loss=70.6391
	step [131/147], loss=69.8214
	step [132/147], loss=84.9472
	step [133/147], loss=74.8286
	step [134/147], loss=76.9753
	step [135/147], loss=76.5469
	step [136/147], loss=90.8434
	step [137/147], loss=70.6651
	step [138/147], loss=83.5809
	step [139/147], loss=76.3616
	step [140/147], loss=97.1256
	step [141/147], loss=86.2285
	step [142/147], loss=83.1018
	step [143/147], loss=92.3832
	step [144/147], loss=80.7899
	step [145/147], loss=82.6299
	step [146/147], loss=92.0890
	step [147/147], loss=13.1908
	Evaluating
	loss=0.0105, precision=0.4208, recall=0.8763, f1=0.5686
Training finished
best_f1: 0.603641647583668
directing: X rim_enhanced: True test_id 3
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 9361 # image files with weight 9361
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 2522 # image files with weight 2522
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/X 9361
Using 4 GPUs
Going to train epochs [60-109]
Training epoch 60
	step [1/147], loss=89.1871
	step [2/147], loss=88.3486
	step [3/147], loss=105.5270
	step [4/147], loss=93.7191
	step [5/147], loss=92.0843
	step [6/147], loss=99.6033
	step [7/147], loss=89.6373
	step [8/147], loss=78.9122
	step [9/147], loss=124.7056
	step [10/147], loss=84.5032
	step [11/147], loss=86.6746
	step [12/147], loss=94.6107
	step [13/147], loss=96.5198
	step [14/147], loss=82.3155
	step [15/147], loss=84.5933
	step [16/147], loss=96.1789
	step [17/147], loss=102.0351
	step [18/147], loss=89.0302
	step [19/147], loss=93.5644
	step [20/147], loss=107.2446
	step [21/147], loss=82.4986
	step [22/147], loss=85.1324
	step [23/147], loss=96.5707
	step [24/147], loss=84.6307
	step [25/147], loss=89.3388
	step [26/147], loss=81.7971
	step [27/147], loss=75.6729
	step [28/147], loss=73.2327
	step [29/147], loss=96.6643
	step [30/147], loss=75.6932
	step [31/147], loss=89.3484
	step [32/147], loss=90.3056
	step [33/147], loss=93.0469
	step [34/147], loss=89.5421
	step [35/147], loss=85.2683
	step [36/147], loss=81.3758
	step [37/147], loss=121.8283
	step [38/147], loss=97.4427
	step [39/147], loss=91.1544
	step [40/147], loss=86.1893
	step [41/147], loss=100.4981
	step [42/147], loss=96.2111
	step [43/147], loss=96.8952
	step [44/147], loss=114.3346
	step [45/147], loss=91.0780
	step [46/147], loss=84.1398
	step [47/147], loss=104.8394
	step [48/147], loss=85.3939
	step [49/147], loss=100.2205
	step [50/147], loss=94.3897
	step [51/147], loss=114.6435
	step [52/147], loss=82.7201
	step [53/147], loss=86.0820
	step [54/147], loss=90.1790
	step [55/147], loss=91.5027
	step [56/147], loss=100.6600
	step [57/147], loss=102.2180
	step [58/147], loss=90.2619
	step [59/147], loss=91.1324
	step [60/147], loss=89.1404
	step [61/147], loss=82.8755
	step [62/147], loss=84.5578
	step [63/147], loss=83.3725
	step [64/147], loss=71.7095
	step [65/147], loss=72.3708
	step [66/147], loss=92.6818
	step [67/147], loss=83.4055
	step [68/147], loss=97.4916
	step [69/147], loss=84.4611
	step [70/147], loss=88.9485
	step [71/147], loss=87.2816
	step [72/147], loss=100.2131
	step [73/147], loss=89.4638
	step [74/147], loss=100.6796
	step [75/147], loss=81.8190
	step [76/147], loss=95.4882
	step [77/147], loss=86.8517
	step [78/147], loss=95.0347
	step [79/147], loss=91.2943
	step [80/147], loss=79.4106
	step [81/147], loss=85.3253
	step [82/147], loss=112.6069
	step [83/147], loss=72.2618
	step [84/147], loss=70.2600
	step [85/147], loss=91.1013
	step [86/147], loss=107.1007
	step [87/147], loss=108.8282
	step [88/147], loss=95.1123
	step [89/147], loss=96.4145
	step [90/147], loss=97.6472
	step [91/147], loss=107.0079
	step [92/147], loss=105.8078
	step [93/147], loss=94.4878
	step [94/147], loss=100.7712
	step [95/147], loss=103.6311
	step [96/147], loss=110.8298
	step [97/147], loss=109.4941
	step [98/147], loss=88.3171
	step [99/147], loss=77.8521
	step [100/147], loss=96.5854
	step [101/147], loss=100.9498
	step [102/147], loss=84.9764
	step [103/147], loss=76.2288
	step [104/147], loss=112.0804
	step [105/147], loss=98.3616
	step [106/147], loss=74.1763
	step [107/147], loss=101.3647
	step [108/147], loss=99.0274
	step [109/147], loss=101.0515
	step [110/147], loss=102.8706
	step [111/147], loss=87.8196
	step [112/147], loss=103.5587
	step [113/147], loss=91.8337
	step [114/147], loss=86.7043
	step [115/147], loss=67.4138
	step [116/147], loss=101.5837
	step [117/147], loss=103.4770
	step [118/147], loss=94.0539
	step [119/147], loss=86.2434
	step [120/147], loss=96.6855
	step [121/147], loss=84.3689
	step [122/147], loss=101.5678
	step [123/147], loss=85.8778
	step [124/147], loss=84.7114
	step [125/147], loss=93.3595
	step [126/147], loss=103.6310
	step [127/147], loss=101.4126
	step [128/147], loss=95.3232
	step [129/147], loss=92.0580
	step [130/147], loss=93.5372
	step [131/147], loss=96.9858
	step [132/147], loss=75.3659
	step [133/147], loss=101.3056
	step [134/147], loss=106.0960
	step [135/147], loss=108.9820
	step [136/147], loss=117.1525
	step [137/147], loss=99.7846
	step [138/147], loss=89.6275
	step [139/147], loss=80.3913
	step [140/147], loss=118.9383
	step [141/147], loss=83.7839
	step [142/147], loss=105.1172
	step [143/147], loss=83.9674
	step [144/147], loss=89.4032
	step [145/147], loss=87.1700
	step [146/147], loss=85.9843
	step [147/147], loss=32.9333
	Evaluating
	loss=0.0122, precision=0.3394, recall=0.8439, f1=0.4841
saving model as: 3_saved_model.pth
Training epoch 61
	step [1/147], loss=98.6696
	step [2/147], loss=95.5697
	step [3/147], loss=90.4133
	step [4/147], loss=89.4214
	step [5/147], loss=112.5744
	step [6/147], loss=110.4210
	step [7/147], loss=93.6388
	step [8/147], loss=77.9137
	step [9/147], loss=103.5758
	step [10/147], loss=89.7743
	step [11/147], loss=114.4423
	step [12/147], loss=92.2141
	step [13/147], loss=105.6810
	step [14/147], loss=90.3689
	step [15/147], loss=72.2460
	step [16/147], loss=73.1204
	step [17/147], loss=86.6902
	step [18/147], loss=100.3581
	step [19/147], loss=104.3167
	step [20/147], loss=83.8532
	step [21/147], loss=99.9411
	step [22/147], loss=96.3254
	step [23/147], loss=76.7640
	step [24/147], loss=100.7251
	step [25/147], loss=91.9903
	step [26/147], loss=95.9557
	step [27/147], loss=86.2332
	step [28/147], loss=91.7647
	step [29/147], loss=80.9313
	step [30/147], loss=94.2482
	step [31/147], loss=92.2259
	step [32/147], loss=96.5726
	step [33/147], loss=96.5449
	step [34/147], loss=111.2282
	step [35/147], loss=87.7925
	step [36/147], loss=98.5426
	step [37/147], loss=92.0496
	step [38/147], loss=77.9548
	step [39/147], loss=88.1379
	step [40/147], loss=109.3874
	step [41/147], loss=77.6556
	step [42/147], loss=96.9305
	step [43/147], loss=107.3847
	step [44/147], loss=80.3583
	step [45/147], loss=98.0474
	step [46/147], loss=91.0889
	step [47/147], loss=100.3886
	step [48/147], loss=97.9676
	step [49/147], loss=73.9392
	step [50/147], loss=100.8506
	step [51/147], loss=79.3761
	step [52/147], loss=89.4649
	step [53/147], loss=91.0178
	step [54/147], loss=94.9393
	step [55/147], loss=101.0290
	step [56/147], loss=76.1312
	step [57/147], loss=106.1572
	step [58/147], loss=96.5467
	step [59/147], loss=94.6210
	step [60/147], loss=107.6104
	step [61/147], loss=88.2351
	step [62/147], loss=110.7647
	step [63/147], loss=91.7328
	step [64/147], loss=94.8201
	step [65/147], loss=87.7669
	step [66/147], loss=96.6914
	step [67/147], loss=78.7085
	step [68/147], loss=113.3844
	step [69/147], loss=90.8920
	step [70/147], loss=109.2874
	step [71/147], loss=80.0667
	step [72/147], loss=89.2922
	step [73/147], loss=102.0213
	step [74/147], loss=84.2262
	step [75/147], loss=94.5819
	step [76/147], loss=97.4163
	step [77/147], loss=98.0511
	step [78/147], loss=79.4002
	step [79/147], loss=90.8008
	step [80/147], loss=91.3228
	step [81/147], loss=95.8521
	step [82/147], loss=78.1429
	step [83/147], loss=80.8599
	step [84/147], loss=85.7840
	step [85/147], loss=89.4982
	step [86/147], loss=102.4800
	step [87/147], loss=81.9099
	step [88/147], loss=93.1892
	step [89/147], loss=101.9761
	step [90/147], loss=90.3971
	step [91/147], loss=96.4754
	step [92/147], loss=93.3971
	step [93/147], loss=76.2554
	step [94/147], loss=87.4014
	step [95/147], loss=80.2153
	step [96/147], loss=96.5194
	step [97/147], loss=72.9483
	step [98/147], loss=88.4085
	step [99/147], loss=98.4108
	step [100/147], loss=112.5576
	step [101/147], loss=85.7732
	step [102/147], loss=86.8415
	step [103/147], loss=72.0985
	step [104/147], loss=94.1489
	step [105/147], loss=86.5083
	step [106/147], loss=69.4060
	step [107/147], loss=98.7822
	step [108/147], loss=98.0608
	step [109/147], loss=103.1041
	step [110/147], loss=84.3272
	step [111/147], loss=94.5685
	step [112/147], loss=102.5772
	step [113/147], loss=90.4097
	step [114/147], loss=91.7399
	step [115/147], loss=100.2785
	step [116/147], loss=89.4728
	step [117/147], loss=89.2607
	step [118/147], loss=99.3417
	step [119/147], loss=79.7037
	step [120/147], loss=108.8409
	step [121/147], loss=112.5557
	step [122/147], loss=103.0776
	step [123/147], loss=70.5078
	step [124/147], loss=96.1077
	step [125/147], loss=92.3747
	step [126/147], loss=83.8844
	step [127/147], loss=97.6335
	step [128/147], loss=84.1060
	step [129/147], loss=85.8157
	step [130/147], loss=91.7866
	step [131/147], loss=97.2782
	step [132/147], loss=80.8331
	step [133/147], loss=85.6155
	step [134/147], loss=76.6307
	step [135/147], loss=110.6632
	step [136/147], loss=82.3546
	step [137/147], loss=95.3394
	step [138/147], loss=102.8249
	step [139/147], loss=94.1784
	step [140/147], loss=102.0393
	step [141/147], loss=106.1173
	step [142/147], loss=83.2561
	step [143/147], loss=103.3407
	step [144/147], loss=87.5011
	step [145/147], loss=69.6359
	step [146/147], loss=103.6477
	step [147/147], loss=24.9204
	Evaluating
	loss=0.0108, precision=0.3761, recall=0.8338, f1=0.5184
saving model as: 3_saved_model.pth
Training epoch 62
	step [1/147], loss=97.6662
	step [2/147], loss=86.1817
	step [3/147], loss=88.9322
	step [4/147], loss=97.9259
	step [5/147], loss=91.4982
	step [6/147], loss=78.2695
	step [7/147], loss=103.7041
	step [8/147], loss=92.5783
	step [9/147], loss=108.1986
	step [10/147], loss=88.2856
	step [11/147], loss=99.6692
	step [12/147], loss=103.6984
	step [13/147], loss=82.6840
	step [14/147], loss=81.2164
	step [15/147], loss=84.5448
	step [16/147], loss=94.8797
	step [17/147], loss=105.1714
	step [18/147], loss=106.5722
	step [19/147], loss=127.0464
	step [20/147], loss=99.5278
	step [21/147], loss=80.6503
	step [22/147], loss=88.8596
	step [23/147], loss=109.2598
	step [24/147], loss=76.2796
	step [25/147], loss=81.8932
	step [26/147], loss=96.0964
	step [27/147], loss=79.0008
	step [28/147], loss=100.7878
	step [29/147], loss=100.6559
	step [30/147], loss=87.1324
	step [31/147], loss=88.9792
	step [32/147], loss=102.2548
	step [33/147], loss=85.4446
	step [34/147], loss=87.6307
	step [35/147], loss=92.2238
	step [36/147], loss=85.8622
	step [37/147], loss=90.5416
	step [38/147], loss=97.0402
	step [39/147], loss=107.5336
	step [40/147], loss=103.3334
	step [41/147], loss=83.8058
	step [42/147], loss=98.6401
	step [43/147], loss=80.9529
	step [44/147], loss=112.5540
	step [45/147], loss=84.9435
	step [46/147], loss=94.0513
	step [47/147], loss=84.8550
	step [48/147], loss=112.4202
	step [49/147], loss=88.4578
	step [50/147], loss=93.7337
	step [51/147], loss=98.7655
	step [52/147], loss=79.7399
	step [53/147], loss=76.9338
	step [54/147], loss=99.4475
	step [55/147], loss=75.6683
	step [56/147], loss=92.5629
	step [57/147], loss=95.7908
	step [58/147], loss=86.3618
	step [59/147], loss=97.8906
	step [60/147], loss=92.4996
	step [61/147], loss=86.0442
	step [62/147], loss=85.6346
	step [63/147], loss=92.8580
	step [64/147], loss=70.3504
	step [65/147], loss=83.7850
	step [66/147], loss=89.5075
	step [67/147], loss=69.2786
	step [68/147], loss=88.2202
	step [69/147], loss=103.8528
	step [70/147], loss=91.9978
	step [71/147], loss=86.0953
	step [72/147], loss=95.6338
	step [73/147], loss=95.0098
	step [74/147], loss=89.5784
	step [75/147], loss=99.2915
	step [76/147], loss=117.3139
	step [77/147], loss=73.9756
	step [78/147], loss=91.4493
	step [79/147], loss=87.0574
	step [80/147], loss=104.6861
	step [81/147], loss=97.5591
	step [82/147], loss=78.8882
	step [83/147], loss=106.9501
	step [84/147], loss=91.7331
	step [85/147], loss=94.8416
	step [86/147], loss=104.1249
	step [87/147], loss=101.8370
	step [88/147], loss=77.1006
	step [89/147], loss=85.7448
	step [90/147], loss=90.1800
	step [91/147], loss=93.4401
	step [92/147], loss=95.6207
	step [93/147], loss=90.7820
	step [94/147], loss=95.3625
	step [95/147], loss=82.0782
	step [96/147], loss=97.8629
	step [97/147], loss=83.6786
	step [98/147], loss=76.0849
	step [99/147], loss=94.3928
	step [100/147], loss=88.8099
	step [101/147], loss=78.7453
	step [102/147], loss=91.5739
	step [103/147], loss=103.3386
	step [104/147], loss=87.7341
	step [105/147], loss=86.9928
	step [106/147], loss=98.7664
	step [107/147], loss=92.8709
	step [108/147], loss=94.3962
	step [109/147], loss=96.6266
	step [110/147], loss=83.5940
	step [111/147], loss=85.5225
	step [112/147], loss=81.7599
	step [113/147], loss=93.3344
	step [114/147], loss=83.7143
	step [115/147], loss=78.3933
	step [116/147], loss=99.8946
	step [117/147], loss=92.1261
	step [118/147], loss=82.9756
	step [119/147], loss=96.7680
	step [120/147], loss=75.0714
	step [121/147], loss=92.9643
	step [122/147], loss=113.2521
	step [123/147], loss=92.4878
	step [124/147], loss=88.5777
	step [125/147], loss=91.4674
	step [126/147], loss=84.7524
	step [127/147], loss=103.5082
	step [128/147], loss=114.5953
	step [129/147], loss=79.5716
	step [130/147], loss=97.0772
	step [131/147], loss=99.3764
	step [132/147], loss=93.4090
	step [133/147], loss=92.9277
	step [134/147], loss=109.2119
	step [135/147], loss=105.9077
	step [136/147], loss=82.1236
	step [137/147], loss=111.2903
	step [138/147], loss=83.3399
	step [139/147], loss=87.8421
	step [140/147], loss=81.5112
	step [141/147], loss=86.7904
	step [142/147], loss=85.7035
	step [143/147], loss=84.7216
	step [144/147], loss=85.6387
	step [145/147], loss=94.0887
	step [146/147], loss=100.7027
	step [147/147], loss=26.2455
	Evaluating
	loss=0.0130, precision=0.3140, recall=0.8438, f1=0.4576
Training epoch 63
	step [1/147], loss=83.5192
	step [2/147], loss=106.6842
	step [3/147], loss=89.5329
	step [4/147], loss=90.8913
	step [5/147], loss=78.4205
	step [6/147], loss=82.2347
	step [7/147], loss=76.2576
	step [8/147], loss=99.8668
	step [9/147], loss=90.2032
	step [10/147], loss=88.9477
	step [11/147], loss=84.0004
	step [12/147], loss=84.1544
	step [13/147], loss=102.6475
	step [14/147], loss=87.1272
	step [15/147], loss=101.9627
	step [16/147], loss=87.5887
	step [17/147], loss=85.0968
	step [18/147], loss=83.8509
	step [19/147], loss=79.2320
	step [20/147], loss=95.7229
	step [21/147], loss=99.1271
	step [22/147], loss=89.0180
	step [23/147], loss=88.9046
	step [24/147], loss=98.8358
	step [25/147], loss=95.2989
	step [26/147], loss=84.0118
	step [27/147], loss=94.1919
	step [28/147], loss=97.1239
	step [29/147], loss=85.6538
	step [30/147], loss=98.0354
	step [31/147], loss=91.9308
	step [32/147], loss=82.7216
	step [33/147], loss=90.3780
	step [34/147], loss=84.8644
	step [35/147], loss=92.8347
	step [36/147], loss=85.9482
	step [37/147], loss=79.6395
	step [38/147], loss=83.4671
	step [39/147], loss=102.1562
	step [40/147], loss=95.0108
	step [41/147], loss=99.0717
	step [42/147], loss=82.6700
	step [43/147], loss=101.7957
	step [44/147], loss=78.4632
	step [45/147], loss=108.2408
	step [46/147], loss=85.1598
	step [47/147], loss=85.7466
	step [48/147], loss=103.8482
	step [49/147], loss=104.5115
	step [50/147], loss=97.6573
	step [51/147], loss=94.2105
	step [52/147], loss=89.4136
	step [53/147], loss=90.2446
	step [54/147], loss=109.7384
	step [55/147], loss=97.5726
	step [56/147], loss=94.8177
	step [57/147], loss=98.8734
	step [58/147], loss=105.2908
	step [59/147], loss=96.9496
	step [60/147], loss=84.3826
	step [61/147], loss=101.3599
	step [62/147], loss=95.9580
	step [63/147], loss=108.7226
	step [64/147], loss=97.9437
	step [65/147], loss=95.4026
	step [66/147], loss=99.6434
	step [67/147], loss=98.3261
	step [68/147], loss=81.5277
	step [69/147], loss=83.6444
	step [70/147], loss=95.9108
	step [71/147], loss=77.6498
	step [72/147], loss=98.5254
	step [73/147], loss=90.6636
	step [74/147], loss=108.2141
	step [75/147], loss=99.7936
	step [76/147], loss=102.0264
	step [77/147], loss=102.2284
	step [78/147], loss=102.4544
	step [79/147], loss=87.3744
	step [80/147], loss=98.6739
	step [81/147], loss=92.5340
	step [82/147], loss=84.0817
	step [83/147], loss=73.8851
	step [84/147], loss=88.3897
	step [85/147], loss=77.9891
	step [86/147], loss=90.8206
	step [87/147], loss=96.0664
	step [88/147], loss=77.7494
	step [89/147], loss=100.0305
	step [90/147], loss=80.0820
	step [91/147], loss=90.8109
	step [92/147], loss=87.6953
	step [93/147], loss=93.5497
	step [94/147], loss=78.9109
	step [95/147], loss=90.3969
	step [96/147], loss=68.6678
	step [97/147], loss=100.8367
	step [98/147], loss=101.3904
	step [99/147], loss=89.3123
	step [100/147], loss=115.1376
	step [101/147], loss=85.7284
	step [102/147], loss=101.6048
	step [103/147], loss=100.2791
	step [104/147], loss=83.1443
	step [105/147], loss=84.1801
	step [106/147], loss=81.6490
	step [107/147], loss=83.7121
	step [108/147], loss=88.5184
	step [109/147], loss=86.4018
	step [110/147], loss=95.4975
	step [111/147], loss=98.4349
	step [112/147], loss=98.5177
	step [113/147], loss=92.5619
	step [114/147], loss=92.9467
	step [115/147], loss=99.9698
	step [116/147], loss=104.1539
	step [117/147], loss=91.5967
	step [118/147], loss=90.1315
	step [119/147], loss=84.1333
	step [120/147], loss=82.7868
	step [121/147], loss=86.1751
	step [122/147], loss=77.4947
	step [123/147], loss=92.9295
	step [124/147], loss=89.4221
	step [125/147], loss=113.0943
	step [126/147], loss=82.1703
	step [127/147], loss=91.5972
	step [128/147], loss=95.0816
	step [129/147], loss=84.7403
	step [130/147], loss=67.6148
	step [131/147], loss=102.0158
	step [132/147], loss=81.9674
	step [133/147], loss=106.5877
	step [134/147], loss=82.6696
	step [135/147], loss=85.2735
	step [136/147], loss=91.0617
	step [137/147], loss=92.5734
	step [138/147], loss=97.0034
	step [139/147], loss=86.5578
	step [140/147], loss=93.2041
	step [141/147], loss=98.5116
	step [142/147], loss=97.2923
	step [143/147], loss=108.0766
	step [144/147], loss=85.0554
	step [145/147], loss=91.2645
	step [146/147], loss=78.9402
	step [147/147], loss=25.0179
	Evaluating
	loss=0.0121, precision=0.3358, recall=0.8244, f1=0.4772
Training epoch 64
	step [1/147], loss=95.4612
	step [2/147], loss=90.1106
	step [3/147], loss=88.5852
	step [4/147], loss=90.7528
	step [5/147], loss=100.7331
	step [6/147], loss=74.8960
	step [7/147], loss=95.7184
	step [8/147], loss=111.6867
	step [9/147], loss=94.9932
	step [10/147], loss=98.5089
	step [11/147], loss=98.2793
	step [12/147], loss=105.3559
	step [13/147], loss=114.7749
	step [14/147], loss=75.9041
	step [15/147], loss=72.2579
	step [16/147], loss=85.0736
	step [17/147], loss=80.5547
	step [18/147], loss=93.0339
	step [19/147], loss=95.5533
	step [20/147], loss=97.0696
	step [21/147], loss=91.6484
	step [22/147], loss=112.5484
	step [23/147], loss=81.4331
	step [24/147], loss=85.1494
	step [25/147], loss=104.1585
	step [26/147], loss=89.4136
	step [27/147], loss=87.8810
	step [28/147], loss=92.3894
	step [29/147], loss=83.4080
	step [30/147], loss=95.1554
	step [31/147], loss=79.2246
	step [32/147], loss=97.5105
	step [33/147], loss=85.7694
	step [34/147], loss=78.9779
	step [35/147], loss=94.5161
	step [36/147], loss=86.6412
	step [37/147], loss=80.0588
	step [38/147], loss=74.4854
	step [39/147], loss=108.3353
	step [40/147], loss=93.8805
	step [41/147], loss=99.8312
	step [42/147], loss=81.7457
	step [43/147], loss=105.3064
	step [44/147], loss=83.3585
	step [45/147], loss=105.1598
	step [46/147], loss=74.1047
	step [47/147], loss=87.3036
	step [48/147], loss=87.0153
	step [49/147], loss=82.4457
	step [50/147], loss=98.3913
	step [51/147], loss=111.2239
	step [52/147], loss=93.9181
	step [53/147], loss=82.1294
	step [54/147], loss=80.7608
	step [55/147], loss=92.8963
	step [56/147], loss=84.0647
	step [57/147], loss=99.6059
	step [58/147], loss=89.1766
	step [59/147], loss=95.6187
	step [60/147], loss=102.8518
	step [61/147], loss=92.0548
	step [62/147], loss=86.8944
	step [63/147], loss=86.4354
	step [64/147], loss=111.3859
	step [65/147], loss=88.9794
	step [66/147], loss=96.0503
	step [67/147], loss=88.2768
	step [68/147], loss=97.7442
	step [69/147], loss=94.7313
	step [70/147], loss=98.3698
	step [71/147], loss=104.1651
	step [72/147], loss=103.2003
	step [73/147], loss=94.4308
	step [74/147], loss=90.5231
	step [75/147], loss=94.7710
	step [76/147], loss=89.1775
	step [77/147], loss=85.5932
	step [78/147], loss=93.5878
	step [79/147], loss=98.5178
	step [80/147], loss=82.0324
	step [81/147], loss=92.4218
	step [82/147], loss=93.8482
	step [83/147], loss=88.4941
	step [84/147], loss=87.6239
	step [85/147], loss=81.7991
	step [86/147], loss=90.4277
	step [87/147], loss=83.1539
	step [88/147], loss=100.1376
	step [89/147], loss=91.4286
	step [90/147], loss=90.6150
	step [91/147], loss=86.1015
	step [92/147], loss=83.5904
	step [93/147], loss=81.0175
	step [94/147], loss=113.2823
	step [95/147], loss=94.7858
	step [96/147], loss=103.3743
	step [97/147], loss=93.8913
	step [98/147], loss=91.1629
	step [99/147], loss=85.5350
	step [100/147], loss=104.2852
	step [101/147], loss=98.5035
	step [102/147], loss=87.3949
	step [103/147], loss=101.9324
	step [104/147], loss=93.8881
	step [105/147], loss=91.6620
	step [106/147], loss=92.4551
	step [107/147], loss=100.3387
	step [108/147], loss=101.0536
	step [109/147], loss=106.4275
	step [110/147], loss=93.7639
	step [111/147], loss=94.9206
	step [112/147], loss=92.3756
	step [113/147], loss=86.4262
	step [114/147], loss=103.9856
	step [115/147], loss=78.1565
	step [116/147], loss=89.5541
	step [117/147], loss=88.7913
	step [118/147], loss=91.2670
	step [119/147], loss=81.3910
	step [120/147], loss=101.3342
	step [121/147], loss=83.4626
	step [122/147], loss=98.4207
	step [123/147], loss=84.8244
	step [124/147], loss=79.6970
	step [125/147], loss=91.8720
	step [126/147], loss=102.2986
	step [127/147], loss=92.6234
	step [128/147], loss=81.6295
	step [129/147], loss=85.0949
	step [130/147], loss=90.6258
	step [131/147], loss=79.3430
	step [132/147], loss=89.0160
	step [133/147], loss=88.3207
	step [134/147], loss=96.1336
	step [135/147], loss=79.6490
	step [136/147], loss=92.3853
	step [137/147], loss=94.3586
	step [138/147], loss=75.7912
	step [139/147], loss=104.1280
	step [140/147], loss=78.4005
	step [141/147], loss=89.9959
	step [142/147], loss=86.8781
	step [143/147], loss=97.8360
	step [144/147], loss=74.3680
	step [145/147], loss=90.9981
	step [146/147], loss=80.8626
	step [147/147], loss=17.3976
	Evaluating
	loss=0.0107, precision=0.3838, recall=0.8369, f1=0.5262
saving model as: 3_saved_model.pth
Training epoch 65
	step [1/147], loss=101.3463
	step [2/147], loss=109.4424
	step [3/147], loss=100.8222
	step [4/147], loss=89.5065
	step [5/147], loss=85.4988
	step [6/147], loss=97.1257
	step [7/147], loss=76.3581
	step [8/147], loss=82.5964
	step [9/147], loss=88.4229
	step [10/147], loss=107.8724
	step [11/147], loss=74.8123
	step [12/147], loss=93.9788
	step [13/147], loss=96.7207
	step [14/147], loss=92.8232
	step [15/147], loss=95.8747
	step [16/147], loss=92.0005
	step [17/147], loss=103.1075
	step [18/147], loss=86.3843
	step [19/147], loss=99.5513
	step [20/147], loss=97.8688
	step [21/147], loss=105.3347
	step [22/147], loss=87.0952
	step [23/147], loss=96.4291
	step [24/147], loss=84.6240
	step [25/147], loss=76.1762
	step [26/147], loss=87.9571
	step [27/147], loss=83.9511
	step [28/147], loss=104.0214
	step [29/147], loss=86.2214
	step [30/147], loss=92.0952
	step [31/147], loss=90.5002
	step [32/147], loss=90.2315
	step [33/147], loss=81.5558
	step [34/147], loss=96.2705
	step [35/147], loss=81.8136
	step [36/147], loss=95.6623
	step [37/147], loss=70.7135
	step [38/147], loss=109.1552
	step [39/147], loss=96.2289
	step [40/147], loss=96.3281
	step [41/147], loss=97.1907
	step [42/147], loss=100.8822
	step [43/147], loss=82.2637
	step [44/147], loss=84.1220
	step [45/147], loss=88.1411
	step [46/147], loss=82.9886
	step [47/147], loss=82.9467
	step [48/147], loss=83.3342
	step [49/147], loss=97.1416
	step [50/147], loss=81.7608
	step [51/147], loss=95.9668
	step [52/147], loss=80.6200
	step [53/147], loss=105.2617
	step [54/147], loss=101.1422
	step [55/147], loss=86.9926
	step [56/147], loss=73.0071
	step [57/147], loss=93.4779
	step [58/147], loss=93.6434
	step [59/147], loss=86.3602
	step [60/147], loss=87.6758
	step [61/147], loss=79.6458
	step [62/147], loss=85.2520
	step [63/147], loss=112.5070
	step [64/147], loss=89.6069
	step [65/147], loss=84.3475
	step [66/147], loss=103.1598
	step [67/147], loss=80.5047
	step [68/147], loss=96.6441
	step [69/147], loss=87.2728
	step [70/147], loss=92.7862
	step [71/147], loss=107.5962
	step [72/147], loss=91.9754
	step [73/147], loss=77.3456
	step [74/147], loss=77.6302
	step [75/147], loss=117.0413
	step [76/147], loss=85.8663
	step [77/147], loss=84.8701
	step [78/147], loss=103.0919
	step [79/147], loss=94.5150
	step [80/147], loss=91.4808
	step [81/147], loss=99.3465
	step [82/147], loss=104.5404
	step [83/147], loss=94.4719
	step [84/147], loss=85.2408
	step [85/147], loss=87.6932
	step [86/147], loss=86.4540
	step [87/147], loss=102.8963
	step [88/147], loss=99.5594
	step [89/147], loss=92.3321
	step [90/147], loss=89.8163
	step [91/147], loss=87.1359
	step [92/147], loss=86.1182
	step [93/147], loss=90.8002
	step [94/147], loss=116.2446
	step [95/147], loss=84.1823
	step [96/147], loss=90.6300
	step [97/147], loss=94.4698
	step [98/147], loss=110.9032
	step [99/147], loss=81.3768
	step [100/147], loss=99.4814
	step [101/147], loss=84.2042
	step [102/147], loss=108.6961
	step [103/147], loss=82.7646
	step [104/147], loss=95.9933
	step [105/147], loss=83.8631
	step [106/147], loss=105.8980
	step [107/147], loss=102.1231
	step [108/147], loss=88.7325
	step [109/147], loss=89.1283
	step [110/147], loss=86.1651
	step [111/147], loss=89.2767
	step [112/147], loss=85.0445
	step [113/147], loss=105.0209
	step [114/147], loss=91.1951
	step [115/147], loss=96.2917
	step [116/147], loss=80.5227
	step [117/147], loss=87.6665
	step [118/147], loss=91.0679
	step [119/147], loss=82.6816
	step [120/147], loss=73.7857
	step [121/147], loss=74.5604
	step [122/147], loss=92.0607
	step [123/147], loss=101.1334
	step [124/147], loss=81.0469
	step [125/147], loss=86.8934
	step [126/147], loss=67.4957
	step [127/147], loss=100.5094
	step [128/147], loss=75.3523
	step [129/147], loss=98.2477
	step [130/147], loss=76.7252
	step [131/147], loss=78.6961
	step [132/147], loss=89.2062
	step [133/147], loss=85.0502
	step [134/147], loss=102.1592
	step [135/147], loss=58.7279
	step [136/147], loss=95.5671
	step [137/147], loss=106.4311
	step [138/147], loss=93.3408
	step [139/147], loss=76.3559
	step [140/147], loss=86.7228
	step [141/147], loss=109.8079
	step [142/147], loss=92.8225
	step [143/147], loss=80.4322
	step [144/147], loss=101.9982
	step [145/147], loss=100.0168
	step [146/147], loss=81.8012
	step [147/147], loss=21.8110
	Evaluating
	loss=0.0101, precision=0.3924, recall=0.8452, f1=0.5360
saving model as: 3_saved_model.pth
Training epoch 66
	step [1/147], loss=94.3676
	step [2/147], loss=101.9780
	step [3/147], loss=104.4269
	step [4/147], loss=95.2378
	step [5/147], loss=107.0282
	step [6/147], loss=95.5807
	step [7/147], loss=79.9781
	step [8/147], loss=97.1365
	step [9/147], loss=99.1944
	step [10/147], loss=83.7627
	step [11/147], loss=86.3202
	step [12/147], loss=94.8151
	step [13/147], loss=98.5011
	step [14/147], loss=86.4724
	step [15/147], loss=102.5319
	step [16/147], loss=100.8994
	step [17/147], loss=91.1061
	step [18/147], loss=81.8274
	step [19/147], loss=87.8191
	step [20/147], loss=99.3079
	step [21/147], loss=90.7679
	step [22/147], loss=94.2934
	step [23/147], loss=89.8609
	step [24/147], loss=87.6643
	step [25/147], loss=100.5169
	step [26/147], loss=97.4334
	step [27/147], loss=105.3583
	step [28/147], loss=98.5895
	step [29/147], loss=83.5454
	step [30/147], loss=86.4245
	step [31/147], loss=87.3199
	step [32/147], loss=86.7328
	step [33/147], loss=86.1401
	step [34/147], loss=103.2898
	step [35/147], loss=87.4961
	step [36/147], loss=85.4858
	step [37/147], loss=89.7497
	step [38/147], loss=70.5448
	step [39/147], loss=97.9693
	step [40/147], loss=104.4141
	step [41/147], loss=88.4066
	step [42/147], loss=99.1220
	step [43/147], loss=89.6009
	step [44/147], loss=93.6872
	step [45/147], loss=90.8397
	step [46/147], loss=93.9123
	step [47/147], loss=86.4935
	step [48/147], loss=76.7991
	step [49/147], loss=93.9060
	step [50/147], loss=91.5518
	step [51/147], loss=77.6606
	step [52/147], loss=96.2635
	step [53/147], loss=86.0741
	step [54/147], loss=86.1165
	step [55/147], loss=96.0420
	step [56/147], loss=114.7146
	step [57/147], loss=90.4094
	step [58/147], loss=96.8993
	step [59/147], loss=83.5709
	step [60/147], loss=87.6663
	step [61/147], loss=80.9518
	step [62/147], loss=93.4922
	step [63/147], loss=96.5238
	step [64/147], loss=80.3785
	step [65/147], loss=100.5646
	step [66/147], loss=107.0099
	step [67/147], loss=92.3643
	step [68/147], loss=81.4468
	step [69/147], loss=89.5841
	step [70/147], loss=103.1401
	step [71/147], loss=75.3003
	step [72/147], loss=93.9702
	step [73/147], loss=88.0374
	step [74/147], loss=82.1808
	step [75/147], loss=79.0204
	step [76/147], loss=81.1734
	step [77/147], loss=85.8250
	step [78/147], loss=85.4729
	step [79/147], loss=92.7743
	step [80/147], loss=91.1867
	step [81/147], loss=80.5303
	step [82/147], loss=95.7898
	step [83/147], loss=105.6119
	step [84/147], loss=90.2297
	step [85/147], loss=83.1798
	step [86/147], loss=85.0215
	step [87/147], loss=81.5468
	step [88/147], loss=81.3779
	step [89/147], loss=68.6567
	step [90/147], loss=95.8331
	step [91/147], loss=108.7173
	step [92/147], loss=101.9363
	step [93/147], loss=88.5248
	step [94/147], loss=96.9017
	step [95/147], loss=90.4981
	step [96/147], loss=85.3753
	step [97/147], loss=93.2147
	step [98/147], loss=85.8308
	step [99/147], loss=88.9717
	step [100/147], loss=88.9981
	step [101/147], loss=88.4930
	step [102/147], loss=80.0989
	step [103/147], loss=89.4654
	step [104/147], loss=95.3631
	step [105/147], loss=83.6796
	step [106/147], loss=78.4713
	step [107/147], loss=90.9401
	step [108/147], loss=96.6812
	step [109/147], loss=87.5651
	step [110/147], loss=97.0895
	step [111/147], loss=90.3576
	step [112/147], loss=106.6439
	step [113/147], loss=89.6461
	step [114/147], loss=68.6947
	step [115/147], loss=89.5157
	step [116/147], loss=92.1439
	step [117/147], loss=102.7916
	step [118/147], loss=79.9284
	step [119/147], loss=80.6477
	step [120/147], loss=82.9737
	step [121/147], loss=101.0240
	step [122/147], loss=111.7755
	step [123/147], loss=111.2298
	step [124/147], loss=92.5181
	step [125/147], loss=78.2976
	step [126/147], loss=91.2013
	step [127/147], loss=86.7365
	step [128/147], loss=85.1561
	step [129/147], loss=90.8103
	step [130/147], loss=99.5738
	step [131/147], loss=102.8615
	step [132/147], loss=85.4330
	step [133/147], loss=84.4754
	step [134/147], loss=92.2793
	step [135/147], loss=93.2023
	step [136/147], loss=94.2677
	step [137/147], loss=84.1645
	step [138/147], loss=71.4035
	step [139/147], loss=81.1918
	step [140/147], loss=79.8138
	step [141/147], loss=79.5234
	step [142/147], loss=96.7600
	step [143/147], loss=95.4795
	step [144/147], loss=94.0577
	step [145/147], loss=84.3799
	step [146/147], loss=81.0803
	step [147/147], loss=21.9497
	Evaluating
	loss=0.0092, precision=0.4098, recall=0.8049, f1=0.5431
saving model as: 3_saved_model.pth
Training epoch 67
	step [1/147], loss=78.5026
	step [2/147], loss=84.7121
	step [3/147], loss=93.1697
	step [4/147], loss=108.3629
	step [5/147], loss=92.6419
	step [6/147], loss=91.3597
	step [7/147], loss=74.3880
	step [8/147], loss=80.5341
	step [9/147], loss=76.1545
	step [10/147], loss=84.5040
	step [11/147], loss=95.5357
	step [12/147], loss=100.3849
	step [13/147], loss=97.8754
	step [14/147], loss=120.7969
	step [15/147], loss=90.7763
	step [16/147], loss=85.9475
	step [17/147], loss=89.7272
	step [18/147], loss=107.7164
	step [19/147], loss=81.6019
	step [20/147], loss=77.1012
	step [21/147], loss=78.3673
	step [22/147], loss=84.7017
	step [23/147], loss=91.3832
	step [24/147], loss=74.4457
	step [25/147], loss=108.2368
	step [26/147], loss=87.8716
	step [27/147], loss=87.8162
	step [28/147], loss=93.1291
	step [29/147], loss=73.9486
	step [30/147], loss=88.2169
	step [31/147], loss=85.2489
	step [32/147], loss=112.3768
	step [33/147], loss=93.5549
	step [34/147], loss=77.5905
	step [35/147], loss=91.2406
	step [36/147], loss=79.8246
	step [37/147], loss=103.0977
	step [38/147], loss=88.5277
	step [39/147], loss=103.2944
	step [40/147], loss=72.1907
	step [41/147], loss=89.4201
	step [42/147], loss=102.8108
	step [43/147], loss=92.7994
	step [44/147], loss=75.2852
	step [45/147], loss=108.5583
	step [46/147], loss=95.2451
	step [47/147], loss=74.4286
	step [48/147], loss=89.7144
	step [49/147], loss=94.7545
	step [50/147], loss=93.3568
	step [51/147], loss=72.4254
	step [52/147], loss=92.5609
	step [53/147], loss=84.2027
	step [54/147], loss=96.6875
	step [55/147], loss=83.7856
	step [56/147], loss=99.9462
	step [57/147], loss=75.6997
	step [58/147], loss=92.9753
	step [59/147], loss=95.6223
	step [60/147], loss=86.0298
	step [61/147], loss=91.3975
	step [62/147], loss=92.3035
	step [63/147], loss=81.8378
	step [64/147], loss=94.0252
	step [65/147], loss=87.3598
	step [66/147], loss=92.3234
	step [67/147], loss=70.2069
	step [68/147], loss=94.4390
	step [69/147], loss=89.8105
	step [70/147], loss=103.0556
	step [71/147], loss=71.4290
	step [72/147], loss=99.4421
	step [73/147], loss=97.6850
	step [74/147], loss=94.1987
	step [75/147], loss=89.5131
	step [76/147], loss=82.1775
	step [77/147], loss=94.4076
	step [78/147], loss=78.5029
	step [79/147], loss=87.3496
	step [80/147], loss=96.0632
	step [81/147], loss=102.8426
	step [82/147], loss=97.1336
	step [83/147], loss=100.8638
	step [84/147], loss=110.6460
	step [85/147], loss=91.4630
	step [86/147], loss=86.6970
	step [87/147], loss=78.9784
	step [88/147], loss=84.5398
	step [89/147], loss=93.0516
	step [90/147], loss=84.9396
	step [91/147], loss=93.5899
	step [92/147], loss=101.3816
	step [93/147], loss=97.3694
	step [94/147], loss=86.9255
	step [95/147], loss=95.0849
	step [96/147], loss=95.8608
	step [97/147], loss=97.5383
	step [98/147], loss=84.7614
	step [99/147], loss=93.4335
	step [100/147], loss=86.3322
	step [101/147], loss=96.0495
	step [102/147], loss=118.7217
	step [103/147], loss=73.8803
	step [104/147], loss=108.6041
	step [105/147], loss=112.2602
	step [106/147], loss=100.9137
	step [107/147], loss=75.0803
	step [108/147], loss=95.9909
	step [109/147], loss=72.1729
	step [110/147], loss=93.1170
	step [111/147], loss=80.3450
	step [112/147], loss=80.2807
	step [113/147], loss=88.1769
	step [114/147], loss=88.2038
	step [115/147], loss=87.9668
	step [116/147], loss=97.8439
	step [117/147], loss=83.3747
	step [118/147], loss=82.8627
	step [119/147], loss=96.6252
	step [120/147], loss=88.8930
	step [121/147], loss=90.8474
	step [122/147], loss=83.1381
	step [123/147], loss=100.0597
	step [124/147], loss=90.6642
	step [125/147], loss=94.9715
	step [126/147], loss=93.4297
	step [127/147], loss=99.6739
	step [128/147], loss=97.0441
	step [129/147], loss=91.1822
	step [130/147], loss=103.2053
	step [131/147], loss=85.9086
	step [132/147], loss=90.6474
	step [133/147], loss=80.7976
	step [134/147], loss=92.0002
	step [135/147], loss=94.9798
	step [136/147], loss=79.6055
	step [137/147], loss=82.6403
	step [138/147], loss=93.0155
	step [139/147], loss=98.8781
	step [140/147], loss=95.2010
	step [141/147], loss=69.7589
	step [142/147], loss=92.1176
	step [143/147], loss=87.9603
	step [144/147], loss=104.6441
	step [145/147], loss=98.7971
	step [146/147], loss=82.8127
	step [147/147], loss=26.3687
	Evaluating
	loss=0.0114, precision=0.3332, recall=0.8403, f1=0.4772
Training epoch 68
	step [1/147], loss=90.8701
	step [2/147], loss=85.4957
	step [3/147], loss=95.2273
	step [4/147], loss=83.8198
	step [5/147], loss=95.2868
	step [6/147], loss=107.2794
	step [7/147], loss=96.3302
	step [8/147], loss=93.7444
	step [9/147], loss=86.0945
	step [10/147], loss=87.1648
	step [11/147], loss=86.5029
	step [12/147], loss=83.4085
	step [13/147], loss=114.2676
	step [14/147], loss=91.4193
	step [15/147], loss=98.4611
	step [16/147], loss=86.5745
	step [17/147], loss=86.0776
	step [18/147], loss=85.2115
	step [19/147], loss=78.3918
	step [20/147], loss=93.4627
	step [21/147], loss=87.0226
	step [22/147], loss=102.5799
	step [23/147], loss=81.8590
	step [24/147], loss=79.8087
	step [25/147], loss=83.3785
	step [26/147], loss=94.4597
	step [27/147], loss=97.8469
	step [28/147], loss=86.5985
	step [29/147], loss=91.5989
	step [30/147], loss=91.0117
	step [31/147], loss=87.5224
	step [32/147], loss=83.3788
	step [33/147], loss=84.1500
	step [34/147], loss=98.7876
	step [35/147], loss=91.1021
	step [36/147], loss=96.5009
	step [37/147], loss=93.3336
	step [38/147], loss=109.9700
	step [39/147], loss=87.3751
	step [40/147], loss=88.1113
	step [41/147], loss=97.3298
	step [42/147], loss=98.8602
	step [43/147], loss=103.1636
	step [44/147], loss=90.9002
	step [45/147], loss=87.0462
	step [46/147], loss=84.3295
	step [47/147], loss=90.5428
	step [48/147], loss=97.7540
	step [49/147], loss=90.2766
	step [50/147], loss=94.8248
	step [51/147], loss=95.1569
	step [52/147], loss=94.4526
	step [53/147], loss=112.0380
	step [54/147], loss=83.1968
	step [55/147], loss=83.8456
	step [56/147], loss=87.9361
	step [57/147], loss=97.4940
	step [58/147], loss=100.4487
	step [59/147], loss=90.1713
	step [60/147], loss=89.0930
	step [61/147], loss=93.8867
	step [62/147], loss=97.1459
	step [63/147], loss=96.2171
	step [64/147], loss=71.8593
	step [65/147], loss=104.8215
	step [66/147], loss=87.2841
	step [67/147], loss=104.4296
	step [68/147], loss=74.9942
	step [69/147], loss=103.5342
	step [70/147], loss=80.3523
	step [71/147], loss=87.5054
	step [72/147], loss=78.0644
	step [73/147], loss=98.5052
	step [74/147], loss=92.8893
	step [75/147], loss=90.1559
	step [76/147], loss=92.9496
	step [77/147], loss=83.7530
	step [78/147], loss=101.3069
	step [79/147], loss=97.6196
	step [80/147], loss=85.6292
	step [81/147], loss=99.7219
	step [82/147], loss=100.6924
	step [83/147], loss=74.3334
	step [84/147], loss=95.1543
	step [85/147], loss=106.9707
	step [86/147], loss=90.9845
	step [87/147], loss=96.9393
	step [88/147], loss=96.5550
	step [89/147], loss=84.3476
	step [90/147], loss=81.5550
	step [91/147], loss=75.8333
	step [92/147], loss=91.1291
	step [93/147], loss=91.4756
	step [94/147], loss=75.5875
	step [95/147], loss=85.3644
	step [96/147], loss=87.9632
	step [97/147], loss=68.5765
	step [98/147], loss=99.8410
	step [99/147], loss=96.9730
	step [100/147], loss=87.7441
	step [101/147], loss=94.6294
	step [102/147], loss=87.2018
	step [103/147], loss=83.1142
	step [104/147], loss=94.1789
	step [105/147], loss=83.0112
	step [106/147], loss=91.9241
	step [107/147], loss=97.4077
	step [108/147], loss=80.8510
	step [109/147], loss=80.8939
	step [110/147], loss=79.3153
	step [111/147], loss=105.4651
	step [112/147], loss=77.7085
	step [113/147], loss=88.6944
	step [114/147], loss=65.2018
	step [115/147], loss=67.6710
	step [116/147], loss=74.0112
	step [117/147], loss=96.2275
	step [118/147], loss=86.8269
	step [119/147], loss=90.9577
	step [120/147], loss=102.0073
	step [121/147], loss=82.7591
	step [122/147], loss=98.4205
	step [123/147], loss=88.6469
	step [124/147], loss=99.4916
	step [125/147], loss=86.0378
	step [126/147], loss=83.1839
	step [127/147], loss=81.3267
	step [128/147], loss=91.8129
	step [129/147], loss=92.4224
	step [130/147], loss=85.8982
	step [131/147], loss=71.5437
	step [132/147], loss=98.4867
	step [133/147], loss=104.8605
	step [134/147], loss=85.6488
	step [135/147], loss=76.0020
	step [136/147], loss=85.5517
	step [137/147], loss=86.6545
	step [138/147], loss=87.0863
	step [139/147], loss=93.4424
	step [140/147], loss=80.6696
	step [141/147], loss=86.4350
	step [142/147], loss=82.8703
	step [143/147], loss=88.8063
	step [144/147], loss=87.9420
	step [145/147], loss=92.3025
	step [146/147], loss=87.5339
	step [147/147], loss=20.3812
	Evaluating
	loss=0.0099, precision=0.3905, recall=0.8467, f1=0.5345
Training epoch 69
	step [1/147], loss=79.8904
	step [2/147], loss=103.0581
	step [3/147], loss=93.0003
	step [4/147], loss=94.9374
	step [5/147], loss=87.9654
	step [6/147], loss=84.2602
	step [7/147], loss=85.6850
	step [8/147], loss=94.5243
	step [9/147], loss=91.7273
	step [10/147], loss=74.2828
	step [11/147], loss=100.4059
	step [12/147], loss=109.0163
	step [13/147], loss=101.2306
	step [14/147], loss=102.3803
	step [15/147], loss=75.6801
	step [16/147], loss=77.9533
	step [17/147], loss=76.0000
	step [18/147], loss=78.8960
	step [19/147], loss=95.8018
	step [20/147], loss=97.1279
	step [21/147], loss=81.7196
	step [22/147], loss=84.6000
	step [23/147], loss=98.8050
	step [24/147], loss=100.1794
	step [25/147], loss=86.7665
	step [26/147], loss=102.5420
	step [27/147], loss=81.7202
	step [28/147], loss=91.3501
	step [29/147], loss=81.2102
	step [30/147], loss=98.9356
	step [31/147], loss=89.9565
	step [32/147], loss=90.5782
	step [33/147], loss=110.1516
	step [34/147], loss=86.6894
	step [35/147], loss=82.3536
	step [36/147], loss=86.6217
	step [37/147], loss=105.7789
	step [38/147], loss=80.3630
	step [39/147], loss=78.4229
	step [40/147], loss=87.8928
	step [41/147], loss=91.2314
	step [42/147], loss=97.1401
	step [43/147], loss=88.6909
	step [44/147], loss=85.7482
	step [45/147], loss=85.6981
	step [46/147], loss=90.5378
	step [47/147], loss=101.0687
	step [48/147], loss=106.4551
	step [49/147], loss=82.9730
	step [50/147], loss=95.5216
	step [51/147], loss=88.6949
	step [52/147], loss=107.2244
	step [53/147], loss=91.3202
	step [54/147], loss=81.5982
	step [55/147], loss=100.4009
	step [56/147], loss=93.2143
	step [57/147], loss=96.7774
	step [58/147], loss=90.7561
	step [59/147], loss=97.2941
	step [60/147], loss=107.0979
	step [61/147], loss=78.2383
	step [62/147], loss=99.3687
	step [63/147], loss=81.6577
	step [64/147], loss=87.4509
	step [65/147], loss=83.3534
	step [66/147], loss=93.2745
	step [67/147], loss=84.5459
	step [68/147], loss=92.2883
	step [69/147], loss=99.6071
	step [70/147], loss=89.7475
	step [71/147], loss=97.3393
	step [72/147], loss=100.4188
	step [73/147], loss=91.9456
	step [74/147], loss=102.8333
	step [75/147], loss=103.7432
	step [76/147], loss=95.6694
	step [77/147], loss=95.3571
	step [78/147], loss=83.4651
	step [79/147], loss=88.8823
	step [80/147], loss=72.2932
	step [81/147], loss=80.2843
	step [82/147], loss=107.8221
	step [83/147], loss=100.1560
	step [84/147], loss=82.6722
	step [85/147], loss=94.3563
	step [86/147], loss=82.6467
	step [87/147], loss=80.7894
	step [88/147], loss=94.2074
	step [89/147], loss=66.5899
	step [90/147], loss=101.5550
	step [91/147], loss=91.1931
	step [92/147], loss=97.1135
	step [93/147], loss=83.2643
	step [94/147], loss=87.5403
	step [95/147], loss=82.2183
	step [96/147], loss=75.8063
	step [97/147], loss=86.6409
	step [98/147], loss=99.2121
	step [99/147], loss=80.9186
	step [100/147], loss=92.9963
	step [101/147], loss=84.0121
	step [102/147], loss=74.9704
	step [103/147], loss=91.1837
	step [104/147], loss=89.8395
	step [105/147], loss=62.8120
	step [106/147], loss=87.5386
	step [107/147], loss=104.2576
	step [108/147], loss=94.0958
	step [109/147], loss=92.7647
	step [110/147], loss=85.3316
	step [111/147], loss=88.7407
	step [112/147], loss=95.4678
	step [113/147], loss=95.0964
	step [114/147], loss=94.5015
	step [115/147], loss=89.1644
	step [116/147], loss=63.6060
	step [117/147], loss=75.7714
	step [118/147], loss=92.3574
	step [119/147], loss=78.5154
	step [120/147], loss=80.1968
	step [121/147], loss=99.0007
	step [122/147], loss=88.5232
	step [123/147], loss=90.4345
	step [124/147], loss=103.6834
	step [125/147], loss=117.4697
	step [126/147], loss=81.6705
	step [127/147], loss=85.7719
	step [128/147], loss=81.1400
	step [129/147], loss=87.9046
	step [130/147], loss=74.8329
	step [131/147], loss=84.0025
	step [132/147], loss=86.2572
	step [133/147], loss=84.9792
	step [134/147], loss=93.9455
	step [135/147], loss=77.2223
	step [136/147], loss=102.8803
	step [137/147], loss=75.9228
	step [138/147], loss=92.4658
	step [139/147], loss=85.9993
	step [140/147], loss=85.6365
	step [141/147], loss=98.6598
	step [142/147], loss=88.5671
	step [143/147], loss=72.9182
	step [144/147], loss=73.9543
	step [145/147], loss=86.8722
	step [146/147], loss=88.2509
	step [147/147], loss=27.4408
	Evaluating
	loss=0.0118, precision=0.3460, recall=0.8438, f1=0.4907
Training epoch 70
	step [1/147], loss=87.4926
	step [2/147], loss=94.5853
	step [3/147], loss=97.6486
	step [4/147], loss=89.8304
	step [5/147], loss=85.4705
	step [6/147], loss=87.7050
	step [7/147], loss=96.0508
	step [8/147], loss=95.4551
	step [9/147], loss=91.8152
	step [10/147], loss=83.8705
	step [11/147], loss=103.3050
	step [12/147], loss=88.7623
	step [13/147], loss=87.4848
	step [14/147], loss=71.2287
	step [15/147], loss=74.1200
	step [16/147], loss=96.8669
	step [17/147], loss=77.0089
	step [18/147], loss=82.2485
	step [19/147], loss=93.6686
	step [20/147], loss=92.7475
	step [21/147], loss=95.8334
	step [22/147], loss=77.0029
	step [23/147], loss=68.0865
	step [24/147], loss=91.6225
	step [25/147], loss=80.8975
	step [26/147], loss=89.1321
	step [27/147], loss=87.6142
	step [28/147], loss=85.4648
	step [29/147], loss=69.9576
	step [30/147], loss=102.7810
	step [31/147], loss=86.4416
	step [32/147], loss=94.7980
	step [33/147], loss=90.0937
	step [34/147], loss=76.9884
	step [35/147], loss=73.4567
	step [36/147], loss=94.1250
	step [37/147], loss=85.8936
	step [38/147], loss=92.1477
	step [39/147], loss=98.6064
	step [40/147], loss=78.3598
	step [41/147], loss=91.3949
	step [42/147], loss=89.3078
	step [43/147], loss=91.9282
	step [44/147], loss=88.2266
	step [45/147], loss=104.1485
	step [46/147], loss=100.6843
	step [47/147], loss=96.9467
	step [48/147], loss=93.3540
	step [49/147], loss=93.8102
	step [50/147], loss=87.4040
	step [51/147], loss=89.6990
	step [52/147], loss=80.6990
	step [53/147], loss=87.4227
	step [54/147], loss=96.5358
	step [55/147], loss=86.5555
	step [56/147], loss=86.0584
	step [57/147], loss=88.4760
	step [58/147], loss=80.2983
	step [59/147], loss=94.0017
	step [60/147], loss=97.4190
	step [61/147], loss=96.1943
	step [62/147], loss=92.2767
	step [63/147], loss=83.0988
	step [64/147], loss=73.3177
	step [65/147], loss=78.4499
	step [66/147], loss=91.0159
	step [67/147], loss=79.7007
	step [68/147], loss=90.7365
	step [69/147], loss=92.7185
	step [70/147], loss=73.9793
	step [71/147], loss=96.9829
	step [72/147], loss=100.0143
	step [73/147], loss=101.1510
	step [74/147], loss=107.0629
	step [75/147], loss=89.7824
	step [76/147], loss=111.1733
	step [77/147], loss=92.1162
	step [78/147], loss=90.4668
	step [79/147], loss=96.5527
	step [80/147], loss=90.2711
	step [81/147], loss=95.6086
	step [82/147], loss=80.9073
	step [83/147], loss=91.2387
	step [84/147], loss=97.1595
	step [85/147], loss=60.7812
	step [86/147], loss=94.6298
	step [87/147], loss=78.4725
	step [88/147], loss=78.9790
	step [89/147], loss=80.0457
	step [90/147], loss=91.0526
	step [91/147], loss=90.1846
	step [92/147], loss=80.2634
	step [93/147], loss=83.3374
	step [94/147], loss=78.0696
	step [95/147], loss=84.2976
	step [96/147], loss=121.3990
	step [97/147], loss=69.5039
	step [98/147], loss=98.6860
	step [99/147], loss=101.9360
	step [100/147], loss=89.3994
	step [101/147], loss=81.4638
	step [102/147], loss=81.9768
	step [103/147], loss=93.9383
	step [104/147], loss=91.6306
	step [105/147], loss=85.4315
	step [106/147], loss=89.2083
	step [107/147], loss=88.9753
	step [108/147], loss=98.0327
	step [109/147], loss=100.2370
	step [110/147], loss=88.5944
	step [111/147], loss=83.9760
	step [112/147], loss=90.5450
	step [113/147], loss=96.0474
	step [114/147], loss=87.3219
	step [115/147], loss=97.4596
	step [116/147], loss=91.7325
	step [117/147], loss=93.5068
	step [118/147], loss=87.3431
	step [119/147], loss=76.9175
	step [120/147], loss=81.8731
	step [121/147], loss=91.2046
	step [122/147], loss=90.3675
	step [123/147], loss=97.2822
	step [124/147], loss=102.7746
	step [125/147], loss=94.5459
	step [126/147], loss=92.0038
	step [127/147], loss=83.3055
	step [128/147], loss=105.5986
	step [129/147], loss=91.2500
	step [130/147], loss=97.7973
	step [131/147], loss=79.7020
	step [132/147], loss=103.0053
	step [133/147], loss=97.2029
	step [134/147], loss=86.7984
	step [135/147], loss=84.7656
	step [136/147], loss=89.4291
	step [137/147], loss=88.1199
	step [138/147], loss=75.0655
	step [139/147], loss=77.1306
	step [140/147], loss=96.4796
	step [141/147], loss=76.9376
	step [142/147], loss=110.4848
	step [143/147], loss=89.2369
	step [144/147], loss=89.6373
	step [145/147], loss=87.0776
	step [146/147], loss=85.7133
	step [147/147], loss=22.4787
	Evaluating
	loss=0.0102, precision=0.3904, recall=0.8354, f1=0.5321
Training epoch 71
	step [1/147], loss=82.0782
	step [2/147], loss=99.8461
	step [3/147], loss=85.3243
	step [4/147], loss=81.2707
	step [5/147], loss=88.6754
	step [6/147], loss=90.8299
	step [7/147], loss=89.7097
	step [8/147], loss=86.6188
	step [9/147], loss=86.9831
	step [10/147], loss=84.2059
	step [11/147], loss=82.5460
	step [12/147], loss=79.5169
	step [13/147], loss=89.8273
	step [14/147], loss=88.4346
	step [15/147], loss=91.4709
	step [16/147], loss=91.3533
	step [17/147], loss=80.6446
	step [18/147], loss=74.3555
	step [19/147], loss=94.1664
	step [20/147], loss=81.0828
	step [21/147], loss=92.4107
	step [22/147], loss=93.3120
	step [23/147], loss=61.8242
	step [24/147], loss=94.9342
	step [25/147], loss=90.9952
	step [26/147], loss=99.1514
	step [27/147], loss=81.4792
	step [28/147], loss=82.7300
	step [29/147], loss=96.9740
	step [30/147], loss=100.2104
	step [31/147], loss=88.3308
	step [32/147], loss=104.9405
	step [33/147], loss=91.6537
	step [34/147], loss=100.5520
	step [35/147], loss=98.4228
	step [36/147], loss=86.4987
	step [37/147], loss=97.8105
	step [38/147], loss=92.6785
	step [39/147], loss=78.5718
	step [40/147], loss=98.0019
	step [41/147], loss=73.3718
	step [42/147], loss=79.2535
	step [43/147], loss=87.7365
	step [44/147], loss=77.8751
	step [45/147], loss=81.1909
	step [46/147], loss=90.9205
	step [47/147], loss=104.4472
	step [48/147], loss=104.1083
	step [49/147], loss=81.4618
	step [50/147], loss=94.3586
	step [51/147], loss=83.9747
	step [52/147], loss=96.8671
	step [53/147], loss=103.9031
	step [54/147], loss=76.3290
	step [55/147], loss=94.6929
	step [56/147], loss=82.1517
	step [57/147], loss=82.9044
	step [58/147], loss=93.2606
	step [59/147], loss=88.3278
	step [60/147], loss=96.7379
	step [61/147], loss=92.6263
	step [62/147], loss=87.4059
	step [63/147], loss=99.3848
	step [64/147], loss=94.5342
	step [65/147], loss=87.9874
	step [66/147], loss=112.9157
	step [67/147], loss=83.1955
	step [68/147], loss=91.5543
	step [69/147], loss=94.8858
	step [70/147], loss=88.5005
	step [71/147], loss=70.8986
	step [72/147], loss=95.0829
	step [73/147], loss=84.0651
	step [74/147], loss=97.0305
	step [75/147], loss=97.2672
	step [76/147], loss=105.4407
	step [77/147], loss=98.0895
	step [78/147], loss=87.5196
	step [79/147], loss=90.0421
	step [80/147], loss=73.8430
	step [81/147], loss=74.4562
	step [82/147], loss=83.5832
	step [83/147], loss=79.0065
	step [84/147], loss=98.3410
	step [85/147], loss=72.6507
	step [86/147], loss=91.8296
	step [87/147], loss=95.1147
	step [88/147], loss=93.0835
	step [89/147], loss=90.9491
	step [90/147], loss=88.1131
	step [91/147], loss=97.2499
	step [92/147], loss=88.3255
	step [93/147], loss=88.0795
	step [94/147], loss=73.4638
	step [95/147], loss=97.7060
	step [96/147], loss=93.6579
	step [97/147], loss=76.5495
	step [98/147], loss=76.4382
	step [99/147], loss=87.4193
	step [100/147], loss=89.8480
	step [101/147], loss=89.5696
	step [102/147], loss=98.0196
	step [103/147], loss=95.5611
	step [104/147], loss=88.5831
	step [105/147], loss=80.8792
	step [106/147], loss=105.2675
	step [107/147], loss=76.3023
	step [108/147], loss=87.3843
	step [109/147], loss=83.4080
	step [110/147], loss=87.2707
	step [111/147], loss=79.6790
	step [112/147], loss=100.3269
	step [113/147], loss=90.3018
	step [114/147], loss=74.0769
	step [115/147], loss=113.6866
	step [116/147], loss=93.8584
	step [117/147], loss=93.8466
	step [118/147], loss=81.0979
	step [119/147], loss=88.3420
	step [120/147], loss=92.6642
	step [121/147], loss=82.7470
	step [122/147], loss=83.4457
	step [123/147], loss=80.2482
	step [124/147], loss=93.2259
	step [125/147], loss=76.2827
	step [126/147], loss=99.5363
	step [127/147], loss=88.5394
	step [128/147], loss=78.7577
	step [129/147], loss=70.2886
	step [130/147], loss=108.2301
	step [131/147], loss=88.5563
	step [132/147], loss=90.3055
	step [133/147], loss=95.7576
	step [134/147], loss=80.4486
	step [135/147], loss=86.6470
	step [136/147], loss=92.6348
	step [137/147], loss=81.8780
	step [138/147], loss=102.1450
	step [139/147], loss=99.7458
	step [140/147], loss=84.6753
	step [141/147], loss=92.5026
	step [142/147], loss=95.9002
	step [143/147], loss=87.4235
	step [144/147], loss=99.8547
	step [145/147], loss=81.1669
	step [146/147], loss=80.4169
	step [147/147], loss=29.9839
	Evaluating
	loss=0.0113, precision=0.3485, recall=0.8310, f1=0.4910
Training epoch 72
	step [1/147], loss=85.8307
	step [2/147], loss=94.2490
	step [3/147], loss=98.2456
	step [4/147], loss=95.3959
	step [5/147], loss=98.7868
	step [6/147], loss=63.3079
	step [7/147], loss=78.3245
	step [8/147], loss=99.3903
	step [9/147], loss=87.2696
	step [10/147], loss=78.9445
	step [11/147], loss=92.9942
	step [12/147], loss=88.1183
	step [13/147], loss=97.1341
	step [14/147], loss=107.9270
	step [15/147], loss=98.1627
	step [16/147], loss=77.2292
	step [17/147], loss=95.3997
	step [18/147], loss=104.6913
	step [19/147], loss=82.9844
	step [20/147], loss=81.8276
	step [21/147], loss=86.1632
	step [22/147], loss=97.0981
	step [23/147], loss=78.5769
	step [24/147], loss=75.8430
	step [25/147], loss=78.4097
	step [26/147], loss=77.9194
	step [27/147], loss=86.3575
	step [28/147], loss=81.0131
	step [29/147], loss=84.7107
	step [30/147], loss=89.6404
	step [31/147], loss=84.1776
	step [32/147], loss=95.4115
	step [33/147], loss=88.9482
	step [34/147], loss=96.2498
	step [35/147], loss=72.2604
	step [36/147], loss=92.5929
	step [37/147], loss=86.5015
	step [38/147], loss=99.4520
	step [39/147], loss=100.1548
	step [40/147], loss=82.4402
	step [41/147], loss=96.2007
	step [42/147], loss=99.6763
	step [43/147], loss=93.1348
	step [44/147], loss=98.2957
	step [45/147], loss=89.8997
	step [46/147], loss=83.7454
	step [47/147], loss=107.4704
	step [48/147], loss=83.2109
	step [49/147], loss=82.3427
	step [50/147], loss=84.1642
	step [51/147], loss=75.5734
	step [52/147], loss=94.9985
	step [53/147], loss=74.9609
	step [54/147], loss=95.7950
	step [55/147], loss=83.9919
	step [56/147], loss=89.9042
	step [57/147], loss=82.8981
	step [58/147], loss=101.7626
	step [59/147], loss=86.9945
	step [60/147], loss=91.0645
	step [61/147], loss=84.2726
	step [62/147], loss=97.7932
	step [63/147], loss=88.7559
	step [64/147], loss=92.7973
	step [65/147], loss=87.9035
	step [66/147], loss=94.7090
	step [67/147], loss=92.8543
	step [68/147], loss=99.0103
	step [69/147], loss=89.7313
	step [70/147], loss=89.9285
	step [71/147], loss=89.2352
	step [72/147], loss=87.2353
	step [73/147], loss=81.3173
	step [74/147], loss=93.8418
	step [75/147], loss=93.6185
	step [76/147], loss=105.9336
	step [77/147], loss=73.3008
	step [78/147], loss=98.9423
	step [79/147], loss=88.7072
	step [80/147], loss=96.3192
	step [81/147], loss=104.3971
	step [82/147], loss=83.6269
	step [83/147], loss=88.5367
	step [84/147], loss=84.6343
	step [85/147], loss=69.3545
	step [86/147], loss=84.8741
	step [87/147], loss=71.4310
	step [88/147], loss=89.4582
	step [89/147], loss=99.8620
	step [90/147], loss=112.1317
	step [91/147], loss=76.4577
	step [92/147], loss=93.4250
	step [93/147], loss=95.2144
	step [94/147], loss=77.0180
	step [95/147], loss=90.8997
	step [96/147], loss=98.4944
	step [97/147], loss=95.6914
	step [98/147], loss=97.9540
	step [99/147], loss=91.4781
	step [100/147], loss=79.9385
	step [101/147], loss=84.2857
	step [102/147], loss=102.1209
	step [103/147], loss=108.0297
	step [104/147], loss=79.5973
	step [105/147], loss=80.7630
	step [106/147], loss=93.5183
	step [107/147], loss=83.0584
	step [108/147], loss=81.4862
	step [109/147], loss=82.3714
	step [110/147], loss=97.2768
	step [111/147], loss=89.4661
	step [112/147], loss=82.1434
	step [113/147], loss=100.0762
	step [114/147], loss=93.1701
	step [115/147], loss=85.6643
	step [116/147], loss=88.9604
	step [117/147], loss=99.2202
	step [118/147], loss=100.7403
	step [119/147], loss=82.8901
	step [120/147], loss=83.1969
	step [121/147], loss=94.1256
	step [122/147], loss=68.9193
	step [123/147], loss=98.7335
	step [124/147], loss=90.9575
	step [125/147], loss=86.1747
	step [126/147], loss=63.5368
	step [127/147], loss=74.5077
	step [128/147], loss=94.9043
	step [129/147], loss=74.3541
	step [130/147], loss=69.1622
	step [131/147], loss=71.5519
	step [132/147], loss=85.7439
	step [133/147], loss=88.2630
	step [134/147], loss=87.3654
	step [135/147], loss=90.3315
	step [136/147], loss=91.9249
	step [137/147], loss=87.0156
	step [138/147], loss=87.7395
	step [139/147], loss=90.1040
	step [140/147], loss=90.6501
	step [141/147], loss=66.0074
	step [142/147], loss=86.3652
	step [143/147], loss=85.0844
	step [144/147], loss=81.2030
	step [145/147], loss=91.4563
	step [146/147], loss=88.8462
	step [147/147], loss=16.3871
	Evaluating
	loss=0.0106, precision=0.3659, recall=0.8483, f1=0.5112
Training epoch 73
	step [1/147], loss=106.2802
	step [2/147], loss=82.8095
	step [3/147], loss=85.0526
	step [4/147], loss=74.1879
	step [5/147], loss=78.3140
	step [6/147], loss=75.9725
	step [7/147], loss=78.3816
	step [8/147], loss=95.3116
	step [9/147], loss=92.7527
	step [10/147], loss=86.4709
	step [11/147], loss=67.8797
	step [12/147], loss=87.8495
	step [13/147], loss=87.5213
	step [14/147], loss=88.2988
	step [15/147], loss=72.5656
	step [16/147], loss=81.8428
	step [17/147], loss=94.0970
	step [18/147], loss=80.2448
	step [19/147], loss=76.2404
	step [20/147], loss=73.8366
	step [21/147], loss=84.7412
	step [22/147], loss=83.7580
	step [23/147], loss=101.3076
	step [24/147], loss=84.3781
	step [25/147], loss=71.0745
	step [26/147], loss=104.6480
	step [27/147], loss=76.8908
	step [28/147], loss=99.1568
	step [29/147], loss=94.0282
	step [30/147], loss=102.8401
	step [31/147], loss=82.4252
	step [32/147], loss=85.5500
	step [33/147], loss=99.8997
	step [34/147], loss=100.0800
	step [35/147], loss=84.8688
	step [36/147], loss=89.7278
	step [37/147], loss=84.7343
	step [38/147], loss=92.9219
	step [39/147], loss=102.5841
	step [40/147], loss=90.2212
	step [41/147], loss=102.0576
	step [42/147], loss=87.6931
	step [43/147], loss=76.7809
	step [44/147], loss=93.9076
	step [45/147], loss=93.9477
	step [46/147], loss=103.0155
	step [47/147], loss=71.0572
	step [48/147], loss=76.9730
	step [49/147], loss=98.5217
	step [50/147], loss=83.0583
	step [51/147], loss=93.8158
	step [52/147], loss=70.3845
	step [53/147], loss=88.9024
	step [54/147], loss=79.8359
	step [55/147], loss=89.9089
	step [56/147], loss=94.9648
	step [57/147], loss=91.7059
	step [58/147], loss=107.2807
	step [59/147], loss=83.3736
	step [60/147], loss=77.9103
	step [61/147], loss=85.2151
	step [62/147], loss=85.5918
	step [63/147], loss=83.5761
	step [64/147], loss=75.2513
	step [65/147], loss=88.0870
	step [66/147], loss=87.3436
	step [67/147], loss=91.0678
	step [68/147], loss=87.7856
	step [69/147], loss=84.3232
	step [70/147], loss=97.1223
	step [71/147], loss=91.1385
	step [72/147], loss=94.7613
	step [73/147], loss=77.7073
	step [74/147], loss=90.8579
	step [75/147], loss=79.6400
	step [76/147], loss=92.5447
	step [77/147], loss=69.8214
	step [78/147], loss=94.5876
	step [79/147], loss=84.4907
	step [80/147], loss=94.4959
	step [81/147], loss=83.7921
	step [82/147], loss=102.1118
	step [83/147], loss=87.8619
	step [84/147], loss=89.6223
	step [85/147], loss=97.6529
	step [86/147], loss=75.1836
	step [87/147], loss=79.6468
	step [88/147], loss=84.9769
	step [89/147], loss=99.6019
	step [90/147], loss=97.6827
	step [91/147], loss=87.9576
	step [92/147], loss=104.9723
	step [93/147], loss=94.0877
	step [94/147], loss=86.0168
	step [95/147], loss=103.3697
	step [96/147], loss=85.5172
	step [97/147], loss=80.9998
	step [98/147], loss=97.5920
	step [99/147], loss=94.8996
	step [100/147], loss=89.6798
	step [101/147], loss=97.5388
	step [102/147], loss=82.7310
	step [103/147], loss=85.3480
	step [104/147], loss=98.4559
	step [105/147], loss=97.5127
	step [106/147], loss=88.1912
	step [107/147], loss=93.7581
	step [108/147], loss=99.3853
	step [109/147], loss=84.3675
	step [110/147], loss=78.8277
	step [111/147], loss=78.6610
	step [112/147], loss=85.5590
	step [113/147], loss=85.2604
	step [114/147], loss=84.0131
	step [115/147], loss=87.4241
	step [116/147], loss=82.1450
	step [117/147], loss=77.0690
	step [118/147], loss=92.9538
	step [119/147], loss=83.0384
	step [120/147], loss=83.6860
	step [121/147], loss=108.9800
	step [122/147], loss=92.8228
	step [123/147], loss=74.7823
	step [124/147], loss=87.0437
	step [125/147], loss=74.4379
	step [126/147], loss=78.0836
	step [127/147], loss=99.9635
	step [128/147], loss=101.8379
	step [129/147], loss=92.6932
	step [130/147], loss=80.7602
	step [131/147], loss=83.1648
	step [132/147], loss=95.4308
	step [133/147], loss=76.0652
	step [134/147], loss=78.0591
	step [135/147], loss=94.8385
	step [136/147], loss=95.7564
	step [137/147], loss=106.3210
	step [138/147], loss=88.4692
	step [139/147], loss=95.3089
	step [140/147], loss=83.7253
	step [141/147], loss=101.7923
	step [142/147], loss=79.1304
	step [143/147], loss=77.0309
	step [144/147], loss=92.6831
	step [145/147], loss=87.2896
	step [146/147], loss=100.4885
	step [147/147], loss=24.4049
	Evaluating
	loss=0.0104, precision=0.3705, recall=0.8270, f1=0.5117
Training epoch 74
	step [1/147], loss=81.4263
	step [2/147], loss=75.1441
	step [3/147], loss=83.7698
	step [4/147], loss=83.7078
	step [5/147], loss=95.0746
	step [6/147], loss=98.7293
	step [7/147], loss=94.8677
	step [8/147], loss=85.1552
	step [9/147], loss=85.3809
	step [10/147], loss=84.3056
	step [11/147], loss=87.8996
	step [12/147], loss=91.8672
	step [13/147], loss=95.1969
	step [14/147], loss=71.5719
	step [15/147], loss=92.2563
	step [16/147], loss=94.5383
	step [17/147], loss=112.2858
	step [18/147], loss=82.0844
	step [19/147], loss=87.4097
	step [20/147], loss=81.7741
	step [21/147], loss=75.4623
	step [22/147], loss=72.4298
	step [23/147], loss=94.2891
	step [24/147], loss=92.4168
	step [25/147], loss=88.7110
	step [26/147], loss=83.4459
	step [27/147], loss=105.3389
	step [28/147], loss=70.7299
	step [29/147], loss=76.9610
	step [30/147], loss=73.5624
	step [31/147], loss=99.2292
	step [32/147], loss=81.3417
	step [33/147], loss=86.4043
	step [34/147], loss=85.2177
	step [35/147], loss=76.7858
	step [36/147], loss=79.0301
	step [37/147], loss=81.9073
	step [38/147], loss=78.7079
	step [39/147], loss=89.3898
	step [40/147], loss=93.4364
	step [41/147], loss=99.0858
	step [42/147], loss=89.6676
	step [43/147], loss=81.8127
	step [44/147], loss=71.6333
	step [45/147], loss=95.3170
	step [46/147], loss=85.0255
	step [47/147], loss=87.3488
	step [48/147], loss=82.6409
	step [49/147], loss=70.7806
	step [50/147], loss=86.4868
	step [51/147], loss=85.8704
	step [52/147], loss=83.2480
	step [53/147], loss=72.4069
	step [54/147], loss=86.3181
	step [55/147], loss=97.9278
	step [56/147], loss=95.6239
	step [57/147], loss=70.7944
	step [58/147], loss=87.6599
	step [59/147], loss=81.3270
	step [60/147], loss=79.8058
	step [61/147], loss=91.1595
	step [62/147], loss=90.0276
	step [63/147], loss=94.1432
	step [64/147], loss=93.1221
	step [65/147], loss=85.0858
	step [66/147], loss=103.4913
	step [67/147], loss=84.8695
	step [68/147], loss=101.3432
	step [69/147], loss=79.4402
	step [70/147], loss=88.3313
	step [71/147], loss=79.5282
	step [72/147], loss=81.2388
	step [73/147], loss=84.7702
	step [74/147], loss=80.5905
	step [75/147], loss=96.1708
	step [76/147], loss=94.3687
	step [77/147], loss=88.1893
	step [78/147], loss=82.3237
	step [79/147], loss=86.5769
	step [80/147], loss=91.5080
	step [81/147], loss=92.6836
	step [82/147], loss=73.1730
	step [83/147], loss=83.1646
	step [84/147], loss=84.7163
	step [85/147], loss=97.6004
	step [86/147], loss=103.1573
	step [87/147], loss=82.9774
	step [88/147], loss=77.8089
	step [89/147], loss=99.6013
	step [90/147], loss=93.7663
	step [91/147], loss=70.9066
	step [92/147], loss=83.4799
	step [93/147], loss=80.9970
	step [94/147], loss=85.2301
	step [95/147], loss=92.2294
	step [96/147], loss=97.0869
	step [97/147], loss=88.8695
	step [98/147], loss=94.5372
	step [99/147], loss=85.6153
	step [100/147], loss=87.2099
	step [101/147], loss=89.1771
	step [102/147], loss=86.6388
	step [103/147], loss=91.5972
	step [104/147], loss=102.3096
	step [105/147], loss=89.7848
	step [106/147], loss=98.0438
	step [107/147], loss=100.7820
	step [108/147], loss=79.6397
	step [109/147], loss=87.8246
	step [110/147], loss=91.6818
	step [111/147], loss=98.7733
	step [112/147], loss=95.5650
	step [113/147], loss=109.9475
	step [114/147], loss=96.2348
	step [115/147], loss=84.3617
	step [116/147], loss=80.0624
	step [117/147], loss=91.6032
	step [118/147], loss=80.0363
	step [119/147], loss=97.0127
	step [120/147], loss=95.0895
	step [121/147], loss=84.9258
	step [122/147], loss=71.0713
	step [123/147], loss=87.5961
	step [124/147], loss=91.3160
	step [125/147], loss=96.4516
	step [126/147], loss=93.0553
	step [127/147], loss=90.1443
	step [128/147], loss=75.2557
	step [129/147], loss=77.5928
	step [130/147], loss=88.1538
	step [131/147], loss=89.5413
	step [132/147], loss=96.2645
	step [133/147], loss=94.5895
	step [134/147], loss=115.3056
	step [135/147], loss=86.9446
	step [136/147], loss=86.1946
	step [137/147], loss=97.9903
	step [138/147], loss=72.3115
	step [139/147], loss=92.6808
	step [140/147], loss=96.7202
	step [141/147], loss=94.6845
	step [142/147], loss=99.1165
	step [143/147], loss=94.0852
	step [144/147], loss=83.5385
	step [145/147], loss=87.1985
	step [146/147], loss=100.8224
	step [147/147], loss=16.7473
	Evaluating
	loss=0.0129, precision=0.3042, recall=0.8272, f1=0.4449
Training epoch 75
	step [1/147], loss=78.0563
	step [2/147], loss=89.1188
	step [3/147], loss=86.7075
	step [4/147], loss=68.9548
	step [5/147], loss=101.0702
	step [6/147], loss=84.6536
	step [7/147], loss=81.8673
	step [8/147], loss=68.2668
	step [9/147], loss=102.9330
	step [10/147], loss=85.3577
	step [11/147], loss=83.3534
	step [12/147], loss=96.0359
	step [13/147], loss=73.2825
	step [14/147], loss=103.8347
	step [15/147], loss=97.2067
	step [16/147], loss=98.3414
	step [17/147], loss=87.0260
	step [18/147], loss=84.5107
	step [19/147], loss=80.2140
	step [20/147], loss=93.5496
	step [21/147], loss=80.0039
	step [22/147], loss=105.9449
	step [23/147], loss=97.6802
	step [24/147], loss=95.8944
	step [25/147], loss=78.2533
	step [26/147], loss=92.2831
	step [27/147], loss=81.4077
	step [28/147], loss=78.8328
	step [29/147], loss=100.5293
	step [30/147], loss=96.6032
	step [31/147], loss=76.5736
	step [32/147], loss=88.7991
	step [33/147], loss=96.1222
	step [34/147], loss=86.0734
	step [35/147], loss=71.3270
	step [36/147], loss=92.5739
	step [37/147], loss=95.6152
	step [38/147], loss=92.0317
	step [39/147], loss=83.8429
	step [40/147], loss=89.4533
	step [41/147], loss=100.6821
	step [42/147], loss=91.6548
	step [43/147], loss=103.1468
	step [44/147], loss=99.8329
	step [45/147], loss=72.9959
	step [46/147], loss=80.0868
	step [47/147], loss=80.6512
	step [48/147], loss=94.2553
	step [49/147], loss=81.7437
	step [50/147], loss=82.2362
	step [51/147], loss=80.7089
	step [52/147], loss=78.0810
	step [53/147], loss=87.0393
	step [54/147], loss=92.5462
	step [55/147], loss=79.4946
	step [56/147], loss=90.2332
	step [57/147], loss=76.5072
	step [58/147], loss=80.8612
	step [59/147], loss=94.2176
	step [60/147], loss=81.6205
	step [61/147], loss=90.5430
	step [62/147], loss=83.4309
	step [63/147], loss=91.9520
	step [64/147], loss=88.9817
	step [65/147], loss=94.9274
	step [66/147], loss=73.2687
	step [67/147], loss=78.5890
	step [68/147], loss=87.3840
	step [69/147], loss=84.5665
	step [70/147], loss=109.2296
	step [71/147], loss=85.6561
	step [72/147], loss=84.2274
	step [73/147], loss=98.1070
	step [74/147], loss=88.4357
	step [75/147], loss=93.6635
	step [76/147], loss=84.1694
	step [77/147], loss=79.5470
	step [78/147], loss=84.3210
	step [79/147], loss=75.5351
	step [80/147], loss=77.6045
	step [81/147], loss=94.9211
	step [82/147], loss=94.5220
	step [83/147], loss=89.9890
	step [84/147], loss=83.5068
	step [85/147], loss=103.9565
	step [86/147], loss=83.4995
	step [87/147], loss=78.8901
	step [88/147], loss=98.6779
	step [89/147], loss=108.0869
	step [90/147], loss=86.0584
	step [91/147], loss=92.8385
	step [92/147], loss=99.6071
	step [93/147], loss=97.5834
	step [94/147], loss=108.0096
	step [95/147], loss=109.5276
	step [96/147], loss=88.2303
	step [97/147], loss=74.8556
	step [98/147], loss=95.3199
	step [99/147], loss=76.1195
	step [100/147], loss=78.9213
	step [101/147], loss=88.5115
	step [102/147], loss=71.6304
	step [103/147], loss=91.5009
	step [104/147], loss=86.8067
	step [105/147], loss=79.7836
	step [106/147], loss=80.8911
	step [107/147], loss=76.1666
	step [108/147], loss=86.8424
	step [109/147], loss=78.1196
	step [110/147], loss=92.4676
	step [111/147], loss=90.4839
	step [112/147], loss=87.4303
	step [113/147], loss=74.5646
	step [114/147], loss=91.5613
	step [115/147], loss=78.6589
	step [116/147], loss=71.7678
	step [117/147], loss=90.3257
	step [118/147], loss=105.7244
	step [119/147], loss=83.1515
	step [120/147], loss=86.3023
	step [121/147], loss=90.9042
	step [122/147], loss=86.9154
	step [123/147], loss=115.3655
	step [124/147], loss=84.2691
	step [125/147], loss=74.6044
	step [126/147], loss=80.2865
	step [127/147], loss=79.7895
	step [128/147], loss=91.1015
	step [129/147], loss=81.1779
	step [130/147], loss=97.2730
	step [131/147], loss=92.6902
	step [132/147], loss=103.9369
	step [133/147], loss=97.8326
	step [134/147], loss=80.7352
	step [135/147], loss=97.8251
	step [136/147], loss=75.7241
	step [137/147], loss=98.2241
	step [138/147], loss=86.8109
	step [139/147], loss=95.0098
	step [140/147], loss=85.0786
	step [141/147], loss=72.1422
	step [142/147], loss=83.4134
	step [143/147], loss=84.2112
	step [144/147], loss=75.5730
	step [145/147], loss=103.4374
	step [146/147], loss=86.2076
	step [147/147], loss=23.5299
	Evaluating
	loss=0.0104, precision=0.3616, recall=0.8393, f1=0.5054
Training epoch 76
	step [1/147], loss=93.0383
	step [2/147], loss=83.3504
	step [3/147], loss=84.6406
	step [4/147], loss=82.1182
	step [5/147], loss=101.5757
	step [6/147], loss=86.3203
	step [7/147], loss=81.8563
	step [8/147], loss=93.9553
	step [9/147], loss=94.8016
	step [10/147], loss=83.6361
	step [11/147], loss=98.8109
	step [12/147], loss=80.8449
	step [13/147], loss=102.0795
	step [14/147], loss=109.2056
	step [15/147], loss=91.1217
	step [16/147], loss=86.7284
	step [17/147], loss=85.7646
	step [18/147], loss=79.5519
	step [19/147], loss=77.6217
	step [20/147], loss=99.8438
	step [21/147], loss=91.0846
	step [22/147], loss=98.4930
	step [23/147], loss=88.9894
	step [24/147], loss=88.2035
	step [25/147], loss=70.8606
	step [26/147], loss=87.5527
	step [27/147], loss=103.6853
	step [28/147], loss=92.6586
	step [29/147], loss=79.8577
	step [30/147], loss=76.6216
	step [31/147], loss=92.1111
	step [32/147], loss=99.5577
	step [33/147], loss=91.9867
	step [34/147], loss=90.8851
	step [35/147], loss=77.0642
	step [36/147], loss=81.0147
	step [37/147], loss=79.1931
	step [38/147], loss=81.3280
	step [39/147], loss=95.2056
	step [40/147], loss=102.1249
	step [41/147], loss=74.2917
	step [42/147], loss=96.4594
	step [43/147], loss=82.4618
	step [44/147], loss=88.9971
	step [45/147], loss=71.1611
	step [46/147], loss=79.7592
	step [47/147], loss=88.9222
	step [48/147], loss=88.1960
	step [49/147], loss=77.8472
	step [50/147], loss=105.0895
	step [51/147], loss=90.4495
	step [52/147], loss=90.8354
	step [53/147], loss=75.6928
	step [54/147], loss=90.4683
	step [55/147], loss=88.0834
	step [56/147], loss=87.6472
	step [57/147], loss=85.8560
	step [58/147], loss=101.2930
	step [59/147], loss=89.7094
	step [60/147], loss=85.8237
	step [61/147], loss=102.3881
	step [62/147], loss=88.5749
	step [63/147], loss=82.5334
	step [64/147], loss=102.3429
	step [65/147], loss=80.5931
	step [66/147], loss=78.4277
	step [67/147], loss=88.2276
	step [68/147], loss=87.2490
	step [69/147], loss=82.2513
	step [70/147], loss=89.6343
	step [71/147], loss=110.1642
	step [72/147], loss=81.8222
	step [73/147], loss=82.6986
	step [74/147], loss=80.1358
	step [75/147], loss=76.9049
	step [76/147], loss=85.5902
	step [77/147], loss=80.5227
	step [78/147], loss=89.7922
	step [79/147], loss=98.4895
	step [80/147], loss=98.4184
	step [81/147], loss=76.7755
	step [82/147], loss=83.5348
	step [83/147], loss=73.6210
	step [84/147], loss=94.2267
	step [85/147], loss=91.8236
	step [86/147], loss=77.7340
	step [87/147], loss=84.6788
	step [88/147], loss=105.5534
	step [89/147], loss=81.1896
	step [90/147], loss=93.5602
	step [91/147], loss=77.8759
	step [92/147], loss=72.9124
	step [93/147], loss=72.4140
	step [94/147], loss=76.2108
	step [95/147], loss=77.6702
	step [96/147], loss=87.0083
	step [97/147], loss=91.5970
	step [98/147], loss=86.2825
	step [99/147], loss=82.5339
	step [100/147], loss=88.0773
	step [101/147], loss=87.7918
	step [102/147], loss=77.3889
	step [103/147], loss=109.3883
	step [104/147], loss=90.5979
	step [105/147], loss=86.7635
	step [106/147], loss=86.1333
	step [107/147], loss=79.6060
	step [108/147], loss=92.4212
	step [109/147], loss=81.3608
	step [110/147], loss=100.3492
	step [111/147], loss=88.0128
	step [112/147], loss=84.5385
	step [113/147], loss=103.2332
	step [114/147], loss=93.5687
	step [115/147], loss=89.8062
	step [116/147], loss=72.3357
	step [117/147], loss=74.5674
	step [118/147], loss=86.5168
	step [119/147], loss=89.8631
	step [120/147], loss=94.1753
	step [121/147], loss=87.7569
	step [122/147], loss=94.3521
	step [123/147], loss=77.4557
	step [124/147], loss=102.0534
	step [125/147], loss=88.5826
	step [126/147], loss=83.6667
	step [127/147], loss=79.5031
	step [128/147], loss=105.6094
	step [129/147], loss=92.4334
	step [130/147], loss=92.1300
	step [131/147], loss=83.2124
	step [132/147], loss=84.2073
	step [133/147], loss=85.4530
	step [134/147], loss=90.1987
	step [135/147], loss=111.3323
	step [136/147], loss=66.0229
	step [137/147], loss=75.4866
	step [138/147], loss=95.5406
	step [139/147], loss=98.0216
	step [140/147], loss=93.2287
	step [141/147], loss=75.4264
	step [142/147], loss=88.4708
	step [143/147], loss=88.3953
	step [144/147], loss=83.8210
	step [145/147], loss=80.2367
	step [146/147], loss=83.0763
	step [147/147], loss=19.6506
	Evaluating
	loss=0.0111, precision=0.3452, recall=0.8173, f1=0.4854
Training epoch 77
	step [1/147], loss=104.3633
	step [2/147], loss=75.1338
	step [3/147], loss=91.1376
	step [4/147], loss=84.2888
	step [5/147], loss=78.1517
	step [6/147], loss=91.6050
	step [7/147], loss=76.8977
	step [8/147], loss=80.1571
	step [9/147], loss=79.7240
	step [10/147], loss=87.5233
	step [11/147], loss=81.6452
	step [12/147], loss=87.5870
	step [13/147], loss=92.4911
	step [14/147], loss=80.5820
	step [15/147], loss=81.6753
	step [16/147], loss=79.2592
	step [17/147], loss=93.4286
	step [18/147], loss=79.7943
	step [19/147], loss=77.0463
	step [20/147], loss=85.7629
	step [21/147], loss=99.9821
	step [22/147], loss=78.1943
	step [23/147], loss=94.3798
	step [24/147], loss=116.9350
	step [25/147], loss=96.9699
	step [26/147], loss=86.3616
	step [27/147], loss=78.0433
	step [28/147], loss=85.0136
	step [29/147], loss=79.0129
	step [30/147], loss=86.2082
	step [31/147], loss=86.7333
	step [32/147], loss=78.3351
	step [33/147], loss=103.0315
	step [34/147], loss=78.3033
	step [35/147], loss=75.5322
	step [36/147], loss=84.6621
	step [37/147], loss=76.2283
	step [38/147], loss=61.1613
	step [39/147], loss=70.8629
	step [40/147], loss=96.6918
	step [41/147], loss=89.9920
	step [42/147], loss=94.2636
	step [43/147], loss=84.6908
	step [44/147], loss=86.0703
	step [45/147], loss=80.6622
	step [46/147], loss=79.3282
	step [47/147], loss=110.7655
	step [48/147], loss=110.0382
	step [49/147], loss=74.6935
	step [50/147], loss=100.0244
	step [51/147], loss=76.8807
	step [52/147], loss=91.3046
	step [53/147], loss=97.1364
	step [54/147], loss=92.3360
	step [55/147], loss=85.2784
	step [56/147], loss=95.0609
	step [57/147], loss=75.9021
	step [58/147], loss=88.2576
	step [59/147], loss=94.3359
	step [60/147], loss=81.5595
	step [61/147], loss=98.7594
	step [62/147], loss=77.6797
	step [63/147], loss=78.6167
	step [64/147], loss=102.0484
	step [65/147], loss=90.9874
	step [66/147], loss=105.1223
	step [67/147], loss=86.0374
	step [68/147], loss=103.5770
	step [69/147], loss=98.2562
	step [70/147], loss=77.0875
	step [71/147], loss=82.3123
	step [72/147], loss=78.2739
	step [73/147], loss=93.8353
	step [74/147], loss=86.6009
	step [75/147], loss=88.8709
	step [76/147], loss=85.3176
	step [77/147], loss=85.6297
	step [78/147], loss=91.3008
	step [79/147], loss=90.6132
	step [80/147], loss=84.3795
	step [81/147], loss=93.8029
	step [82/147], loss=85.4649
	step [83/147], loss=90.7355
	step [84/147], loss=89.3307
	step [85/147], loss=107.2803
	step [86/147], loss=75.5359
	step [87/147], loss=84.0694
	step [88/147], loss=82.7235
	step [89/147], loss=85.6353
	step [90/147], loss=107.2015
	step [91/147], loss=82.4173
	step [92/147], loss=75.7949
	step [93/147], loss=90.3503
	step [94/147], loss=103.1423
	step [95/147], loss=75.9292
	step [96/147], loss=88.1737
	step [97/147], loss=100.0024
	step [98/147], loss=76.3930
	step [99/147], loss=83.3450
	step [100/147], loss=87.1600
	step [101/147], loss=85.6354
	step [102/147], loss=80.8136
	step [103/147], loss=96.6519
	step [104/147], loss=86.8940
	step [105/147], loss=95.6100
	step [106/147], loss=88.7417
	step [107/147], loss=89.0487
	step [108/147], loss=81.2584
	step [109/147], loss=77.9983
	step [110/147], loss=88.9867
	step [111/147], loss=79.7488
	step [112/147], loss=86.0521
	step [113/147], loss=85.0372
	step [114/147], loss=76.9181
	step [115/147], loss=85.9968
	step [116/147], loss=88.3227
	step [117/147], loss=105.9318
	step [118/147], loss=63.2519
	step [119/147], loss=87.4981
	step [120/147], loss=94.0430
	step [121/147], loss=88.5887
	step [122/147], loss=82.4983
	step [123/147], loss=82.4547
	step [124/147], loss=93.3737
	step [125/147], loss=108.1358
	step [126/147], loss=86.2939
	step [127/147], loss=72.9197
	step [128/147], loss=96.9104
	step [129/147], loss=80.9604
	step [130/147], loss=77.4769
	step [131/147], loss=96.6859
	step [132/147], loss=82.7578
	step [133/147], loss=98.3286
	step [134/147], loss=84.8866
	step [135/147], loss=85.8258
	step [136/147], loss=76.3402
	step [137/147], loss=77.0914
	step [138/147], loss=91.8597
	step [139/147], loss=102.6269
	step [140/147], loss=104.7108
	step [141/147], loss=87.3145
	step [142/147], loss=80.7982
	step [143/147], loss=84.2804
	step [144/147], loss=76.1864
	step [145/147], loss=92.9555
	step [146/147], loss=88.2169
	step [147/147], loss=31.1124
	Evaluating
	loss=0.0089, precision=0.4271, recall=0.8386, f1=0.5659
saving model as: 3_saved_model.pth
Training epoch 78
	step [1/147], loss=89.8928
	step [2/147], loss=98.4621
	step [3/147], loss=99.2344
	step [4/147], loss=95.1430
	step [5/147], loss=88.6070
	step [6/147], loss=90.4367
	step [7/147], loss=89.6248
	step [8/147], loss=96.9330
	step [9/147], loss=98.7755
	step [10/147], loss=92.6774
	step [11/147], loss=76.6528
	step [12/147], loss=97.8439
	step [13/147], loss=71.1809
	step [14/147], loss=95.2950
	step [15/147], loss=87.2083
	step [16/147], loss=83.4209
	step [17/147], loss=69.6641
	step [18/147], loss=75.2174
	step [19/147], loss=111.3836
	step [20/147], loss=101.6175
	step [21/147], loss=79.0915
	step [22/147], loss=87.3141
	step [23/147], loss=89.3900
	step [24/147], loss=90.5952
	step [25/147], loss=99.4437
	step [26/147], loss=92.2886
	step [27/147], loss=83.6702
	step [28/147], loss=96.2966
	step [29/147], loss=81.1421
	step [30/147], loss=93.9890
	step [31/147], loss=68.8603
	step [32/147], loss=90.3573
	step [33/147], loss=84.6196
	step [34/147], loss=86.5883
	step [35/147], loss=73.3497
	step [36/147], loss=82.5855
	step [37/147], loss=104.8283
	step [38/147], loss=109.8637
	step [39/147], loss=94.1245
	step [40/147], loss=73.0742
	step [41/147], loss=82.8876
	step [42/147], loss=81.9969
	step [43/147], loss=92.9892
	step [44/147], loss=87.6724
	step [45/147], loss=94.2047
	step [46/147], loss=102.4260
	step [47/147], loss=92.6506
	step [48/147], loss=81.0373
	step [49/147], loss=73.3966
	step [50/147], loss=77.5134
	step [51/147], loss=88.6162
	step [52/147], loss=71.1587
	step [53/147], loss=98.2290
	step [54/147], loss=92.3774
	step [55/147], loss=94.6107
	step [56/147], loss=93.0049
	step [57/147], loss=90.7512
	step [58/147], loss=86.8226
	step [59/147], loss=101.6431
	step [60/147], loss=96.7181
	step [61/147], loss=97.6450
	step [62/147], loss=100.0075
	step [63/147], loss=85.0823
	step [64/147], loss=69.4542
	step [65/147], loss=82.4275
	step [66/147], loss=79.3144
	step [67/147], loss=84.6635
	step [68/147], loss=92.3169
	step [69/147], loss=90.0833
	step [70/147], loss=87.4743
	step [71/147], loss=79.1498
	step [72/147], loss=66.8214
	step [73/147], loss=88.8833
	step [74/147], loss=86.0064
	step [75/147], loss=82.2694
	step [76/147], loss=91.5570
	step [77/147], loss=77.5134
	step [78/147], loss=84.4177
	step [79/147], loss=80.3434
	step [80/147], loss=99.7033
	step [81/147], loss=78.7545
	step [82/147], loss=92.5451
	step [83/147], loss=73.7552
	step [84/147], loss=87.0732
	step [85/147], loss=81.6871
	step [86/147], loss=106.5986
	step [87/147], loss=83.3247
	step [88/147], loss=78.1295
	step [89/147], loss=81.4635
	step [90/147], loss=78.2602
	step [91/147], loss=96.9909
	step [92/147], loss=92.5490
	step [93/147], loss=77.0795
	step [94/147], loss=93.4545
	step [95/147], loss=75.4640
	step [96/147], loss=84.4823
	step [97/147], loss=84.9380
	step [98/147], loss=96.7454
	step [99/147], loss=77.9686
	step [100/147], loss=87.3598
	step [101/147], loss=75.1259
	step [102/147], loss=82.2516
	step [103/147], loss=91.6007
	step [104/147], loss=69.1262
	step [105/147], loss=78.1376
	step [106/147], loss=85.5996
	step [107/147], loss=92.4653
	step [108/147], loss=81.9107
	step [109/147], loss=68.9297
	step [110/147], loss=104.7544
	step [111/147], loss=102.0060
	step [112/147], loss=93.7216
	step [113/147], loss=77.6035
	step [114/147], loss=92.8971
	step [115/147], loss=87.3584
	step [116/147], loss=86.1338
	step [117/147], loss=78.1769
	step [118/147], loss=80.2627
	step [119/147], loss=96.5605
	step [120/147], loss=89.6533
	step [121/147], loss=89.0195
	step [122/147], loss=85.6022
	step [123/147], loss=95.4224
	step [124/147], loss=88.8977
	step [125/147], loss=80.3607
	step [126/147], loss=75.9602
	step [127/147], loss=87.8899
	step [128/147], loss=78.7713
	step [129/147], loss=80.0392
	step [130/147], loss=81.1578
	step [131/147], loss=84.2104
	step [132/147], loss=78.3323
	step [133/147], loss=76.8825
	step [134/147], loss=92.4558
	step [135/147], loss=88.0498
	step [136/147], loss=85.2588
	step [137/147], loss=88.9880
	step [138/147], loss=90.7141
	step [139/147], loss=73.6974
	step [140/147], loss=75.9694
	step [141/147], loss=95.0177
	step [142/147], loss=92.6526
	step [143/147], loss=83.3892
	step [144/147], loss=70.4885
	step [145/147], loss=102.7411
	step [146/147], loss=74.2140
	step [147/147], loss=24.2695
	Evaluating
	loss=0.0083, precision=0.4350, recall=0.8059, f1=0.5650
Training epoch 79
	step [1/147], loss=83.9401
	step [2/147], loss=91.3403
	step [3/147], loss=88.7851
	step [4/147], loss=82.9812
	step [5/147], loss=95.7785
	step [6/147], loss=96.3250
	step [7/147], loss=102.7401
	step [8/147], loss=95.5744
	step [9/147], loss=77.3508
	step [10/147], loss=95.0521
	step [11/147], loss=83.3769
	step [12/147], loss=94.1821
	step [13/147], loss=90.9692
	step [14/147], loss=85.4444
	step [15/147], loss=79.8078
	step [16/147], loss=100.0770
	step [17/147], loss=86.9637
	step [18/147], loss=86.2401
	step [19/147], loss=89.6136
	step [20/147], loss=94.1365
	step [21/147], loss=69.0372
	step [22/147], loss=85.3367
	step [23/147], loss=72.4784
	step [24/147], loss=61.2118
	step [25/147], loss=80.8060
	step [26/147], loss=101.9886
	step [27/147], loss=76.6149
	step [28/147], loss=97.6338
	step [29/147], loss=88.5954
	step [30/147], loss=72.9156
	step [31/147], loss=105.3419
	step [32/147], loss=108.7619
	step [33/147], loss=91.2036
	step [34/147], loss=95.7939
	step [35/147], loss=88.0096
	step [36/147], loss=89.7148
	step [37/147], loss=95.4428
	step [38/147], loss=81.0647
	step [39/147], loss=87.7819
	step [40/147], loss=80.8632
	step [41/147], loss=91.2311
	step [42/147], loss=87.7071
	step [43/147], loss=87.8272
	step [44/147], loss=81.1774
	step [45/147], loss=78.1012
	step [46/147], loss=99.6470
	step [47/147], loss=86.6816
	step [48/147], loss=87.3444
	step [49/147], loss=87.9591
	step [50/147], loss=86.7936
	step [51/147], loss=92.8995
	step [52/147], loss=83.5775
	step [53/147], loss=91.5441
	step [54/147], loss=80.5803
	step [55/147], loss=84.3530
	step [56/147], loss=81.4828
	step [57/147], loss=94.1935
	step [58/147], loss=74.7816
	step [59/147], loss=80.0209
	step [60/147], loss=79.3419
	step [61/147], loss=71.7296
	step [62/147], loss=99.3950
	step [63/147], loss=87.2603
	step [64/147], loss=75.5089
	step [65/147], loss=88.5543
	step [66/147], loss=97.5753
	step [67/147], loss=83.0333
	step [68/147], loss=91.9276
	step [69/147], loss=102.7135
	step [70/147], loss=94.3609
	step [71/147], loss=99.3070
	step [72/147], loss=101.8018
	step [73/147], loss=84.1148
	step [74/147], loss=97.6352
	step [75/147], loss=76.7986
	step [76/147], loss=92.8252
	step [77/147], loss=79.8672
	step [78/147], loss=87.2217
	step [79/147], loss=88.1775
	step [80/147], loss=94.8620
	step [81/147], loss=91.0220
	step [82/147], loss=87.3209
	step [83/147], loss=68.1111
	step [84/147], loss=85.8990
	step [85/147], loss=82.9534
	step [86/147], loss=78.5921
	step [87/147], loss=84.7277
	step [88/147], loss=75.8380
	step [89/147], loss=100.0535
	step [90/147], loss=78.8347
	step [91/147], loss=84.4491
	step [92/147], loss=86.9842
	step [93/147], loss=89.3932
	step [94/147], loss=81.7694
	step [95/147], loss=91.8470
	step [96/147], loss=86.7171
	step [97/147], loss=65.9900
	step [98/147], loss=69.0623
	step [99/147], loss=81.7111
	step [100/147], loss=107.3020
	step [101/147], loss=84.4749
	step [102/147], loss=91.9381
	step [103/147], loss=82.2716
	step [104/147], loss=85.6328
	step [105/147], loss=82.4633
	step [106/147], loss=87.4171
	step [107/147], loss=93.6286
	step [108/147], loss=81.6503
	step [109/147], loss=79.3170
	step [110/147], loss=72.8082
	step [111/147], loss=91.2910
	step [112/147], loss=87.9469
	step [113/147], loss=91.2671
	step [114/147], loss=90.8185
	step [115/147], loss=67.6395
	step [116/147], loss=64.2110
	step [117/147], loss=76.0819
	step [118/147], loss=73.4228
	step [119/147], loss=90.0498
	step [120/147], loss=95.3914
	step [121/147], loss=96.6266
	step [122/147], loss=85.7705
	step [123/147], loss=76.1709
	step [124/147], loss=100.7204
	step [125/147], loss=87.0394
	step [126/147], loss=76.9342
	step [127/147], loss=98.2039
	step [128/147], loss=67.6239
	step [129/147], loss=93.3533
	step [130/147], loss=74.9924
	step [131/147], loss=101.6478
	step [132/147], loss=80.5264
	step [133/147], loss=99.5615
	step [134/147], loss=79.7717
	step [135/147], loss=110.2909
	step [136/147], loss=91.7505
	step [137/147], loss=95.5228
	step [138/147], loss=98.1990
	step [139/147], loss=78.8909
	step [140/147], loss=87.2083
	step [141/147], loss=78.2746
	step [142/147], loss=92.2118
	step [143/147], loss=83.1633
	step [144/147], loss=75.9624
	step [145/147], loss=71.1736
	step [146/147], loss=86.5038
	step [147/147], loss=14.7415
	Evaluating
	loss=0.0089, precision=0.4148, recall=0.8221, f1=0.5514
Training epoch 80
	step [1/147], loss=82.3083
	step [2/147], loss=89.5117
	step [3/147], loss=87.8359
	step [4/147], loss=97.1788
	step [5/147], loss=79.4270
	step [6/147], loss=78.5521
	step [7/147], loss=84.4497
	step [8/147], loss=99.7374
	step [9/147], loss=83.9373
	step [10/147], loss=84.3398
	step [11/147], loss=80.8740
	step [12/147], loss=78.2940
	step [13/147], loss=82.6828
	step [14/147], loss=85.2095
	step [15/147], loss=80.7189
	step [16/147], loss=100.0030
	step [17/147], loss=75.9601
	step [18/147], loss=97.9176
	step [19/147], loss=72.6680
	step [20/147], loss=100.7300
	step [21/147], loss=82.4695
	step [22/147], loss=82.0441
	step [23/147], loss=89.3159
	step [24/147], loss=83.7593
	step [25/147], loss=82.8755
	step [26/147], loss=93.0652
	step [27/147], loss=102.8473
	step [28/147], loss=89.5279
	step [29/147], loss=79.1713
	step [30/147], loss=78.0472
	step [31/147], loss=87.6270
	step [32/147], loss=91.5834
	step [33/147], loss=87.3465
	step [34/147], loss=79.2478
	step [35/147], loss=88.1607
	step [36/147], loss=96.9982
	step [37/147], loss=88.0247
	step [38/147], loss=81.0307
	step [39/147], loss=75.1393
	step [40/147], loss=75.9590
	step [41/147], loss=98.6736
	step [42/147], loss=93.8739
	step [43/147], loss=81.7847
	step [44/147], loss=88.4558
	step [45/147], loss=83.0414
	step [46/147], loss=82.8593
	step [47/147], loss=82.4133
	step [48/147], loss=92.4592
	step [49/147], loss=96.0618
	step [50/147], loss=83.1960
	step [51/147], loss=97.9913
	step [52/147], loss=85.1617
	step [53/147], loss=81.7729
	step [54/147], loss=113.1036
	step [55/147], loss=75.6612
	step [56/147], loss=101.9294
	step [57/147], loss=77.9115
	step [58/147], loss=96.2320
	step [59/147], loss=80.6789
	step [60/147], loss=81.1355
	step [61/147], loss=75.8513
	step [62/147], loss=82.1578
	step [63/147], loss=96.5250
	step [64/147], loss=84.3755
	step [65/147], loss=75.9807
	step [66/147], loss=91.1965
	step [67/147], loss=76.3037
	step [68/147], loss=89.4936
	step [69/147], loss=96.6890
	step [70/147], loss=96.5767
	step [71/147], loss=79.0606
	step [72/147], loss=72.8657
	step [73/147], loss=69.6788
	step [74/147], loss=76.6162
	step [75/147], loss=84.7594
	step [76/147], loss=96.0900
	step [77/147], loss=88.7000
	step [78/147], loss=87.9351
	step [79/147], loss=74.6115
	step [80/147], loss=76.8451
	step [81/147], loss=89.8391
	step [82/147], loss=85.2025
	step [83/147], loss=90.4868
	step [84/147], loss=76.2085
	step [85/147], loss=88.2061
	step [86/147], loss=69.4363
	step [87/147], loss=79.5832
	step [88/147], loss=95.0803
	step [89/147], loss=95.8038
	step [90/147], loss=81.7773
	step [91/147], loss=87.5425
	step [92/147], loss=81.3749
	step [93/147], loss=77.0637
	step [94/147], loss=77.6338
	step [95/147], loss=95.8709
	step [96/147], loss=78.2138
	step [97/147], loss=75.7299
	step [98/147], loss=91.5992
	step [99/147], loss=95.7679
	step [100/147], loss=82.3452
	step [101/147], loss=90.7568
	step [102/147], loss=87.4867
	step [103/147], loss=83.7292
	step [104/147], loss=83.1609
	step [105/147], loss=75.8232
	step [106/147], loss=91.8875
	step [107/147], loss=93.6956
	step [108/147], loss=86.9566
	step [109/147], loss=97.0155
	step [110/147], loss=89.5872
	step [111/147], loss=86.7933
	step [112/147], loss=96.9809
	step [113/147], loss=87.5602
	step [114/147], loss=97.9068
	step [115/147], loss=87.9817
	step [116/147], loss=88.4214
	step [117/147], loss=80.8203
	step [118/147], loss=81.5343
	step [119/147], loss=60.7379
	step [120/147], loss=92.2602
	step [121/147], loss=85.2586
	step [122/147], loss=94.2612
	step [123/147], loss=80.1513
	step [124/147], loss=98.6440
	step [125/147], loss=92.7556
	step [126/147], loss=78.8129
	step [127/147], loss=85.0274
	step [128/147], loss=80.8749
	step [129/147], loss=94.3111
	step [130/147], loss=87.2621
	step [131/147], loss=94.7316
	step [132/147], loss=94.1856
	step [133/147], loss=80.1571
	step [134/147], loss=92.7343
	step [135/147], loss=92.4467
	step [136/147], loss=90.9261
	step [137/147], loss=83.3280
	step [138/147], loss=95.9980
	step [139/147], loss=88.4730
	step [140/147], loss=75.3002
	step [141/147], loss=90.1054
	step [142/147], loss=100.9174
	step [143/147], loss=85.1456
	step [144/147], loss=76.3161
	step [145/147], loss=68.5556
	step [146/147], loss=91.3801
	step [147/147], loss=23.3318
	Evaluating
	loss=0.0089, precision=0.4097, recall=0.8322, f1=0.5491
Training epoch 81
	step [1/147], loss=82.0461
	step [2/147], loss=88.0987
	step [3/147], loss=78.9430
	step [4/147], loss=84.8462
	step [5/147], loss=93.9066
	step [6/147], loss=107.6042
	step [7/147], loss=76.8633
	step [8/147], loss=91.9707
	step [9/147], loss=84.4828
	step [10/147], loss=75.1648
	step [11/147], loss=89.0826
	step [12/147], loss=88.9754
	step [13/147], loss=88.8729
	step [14/147], loss=83.8103
	step [15/147], loss=76.0491
	step [16/147], loss=96.4721
	step [17/147], loss=83.0150
	step [18/147], loss=85.7642
	step [19/147], loss=70.5890
	step [20/147], loss=94.6031
	step [21/147], loss=94.5221
	step [22/147], loss=78.3284
	step [23/147], loss=77.3123
	step [24/147], loss=82.7527
	step [25/147], loss=77.1475
	step [26/147], loss=81.7333
	step [27/147], loss=92.4193
	step [28/147], loss=91.8807
	step [29/147], loss=82.1916
	step [30/147], loss=88.8079
	step [31/147], loss=96.5675
	step [32/147], loss=83.6984
	step [33/147], loss=92.5882
	step [34/147], loss=74.4929
	step [35/147], loss=80.9145
	step [36/147], loss=80.6237
	step [37/147], loss=80.8904
	step [38/147], loss=84.4807
	step [39/147], loss=85.6886
	step [40/147], loss=96.3605
	step [41/147], loss=88.7295
	step [42/147], loss=100.6569
	step [43/147], loss=65.7396
	step [44/147], loss=91.0104
	step [45/147], loss=80.1740
	step [46/147], loss=95.7709
	step [47/147], loss=92.0560
	step [48/147], loss=83.6280
	step [49/147], loss=91.4726
	step [50/147], loss=88.1178
	step [51/147], loss=86.8310
	step [52/147], loss=72.1596
	step [53/147], loss=91.5719
	step [54/147], loss=79.2417
	step [55/147], loss=83.4042
	step [56/147], loss=90.6911
	step [57/147], loss=84.9078
	step [58/147], loss=79.6402
	step [59/147], loss=83.3708
	step [60/147], loss=80.9229
	step [61/147], loss=76.0663
	step [62/147], loss=93.9178
	step [63/147], loss=102.4517
	step [64/147], loss=95.4602
	step [65/147], loss=75.3256
	step [66/147], loss=94.4104
	step [67/147], loss=66.5348
	step [68/147], loss=81.8658
	step [69/147], loss=87.1568
	step [70/147], loss=75.7710
	step [71/147], loss=92.1290
	step [72/147], loss=82.2775
	step [73/147], loss=85.1754
	step [74/147], loss=100.7495
	step [75/147], loss=76.0421
	step [76/147], loss=85.8250
	step [77/147], loss=72.1605
	step [78/147], loss=93.2412
	step [79/147], loss=107.3977
	step [80/147], loss=80.8401
	step [81/147], loss=72.4191
	step [82/147], loss=87.5744
	step [83/147], loss=93.7690
	step [84/147], loss=70.3349
	step [85/147], loss=75.9944
	step [86/147], loss=104.5150
	step [87/147], loss=92.0356
	step [88/147], loss=78.3441
	step [89/147], loss=94.7148
	step [90/147], loss=93.8418
	step [91/147], loss=86.4633
	step [92/147], loss=96.4465
	step [93/147], loss=88.4135
	step [94/147], loss=81.0365
	step [95/147], loss=90.4366
	step [96/147], loss=89.5165
	step [97/147], loss=85.1643
	step [98/147], loss=81.2060
	step [99/147], loss=78.0946
	step [100/147], loss=78.1430
	step [101/147], loss=82.8202
	step [102/147], loss=84.0560
	step [103/147], loss=89.6103
	step [104/147], loss=94.4123
	step [105/147], loss=86.8759
	step [106/147], loss=76.3683
	step [107/147], loss=83.6425
	step [108/147], loss=93.4993
	step [109/147], loss=87.5261
	step [110/147], loss=68.9407
	step [111/147], loss=79.1645
	step [112/147], loss=82.2507
	step [113/147], loss=87.4769
	step [114/147], loss=85.2682
	step [115/147], loss=74.8052
	step [116/147], loss=96.9546
	step [117/147], loss=87.1299
	step [118/147], loss=87.4685
	step [119/147], loss=91.2327
	step [120/147], loss=84.2440
	step [121/147], loss=85.2133
	step [122/147], loss=82.8394
	step [123/147], loss=83.9315
	step [124/147], loss=88.5226
	step [125/147], loss=82.1413
	step [126/147], loss=85.6003
	step [127/147], loss=81.4912
	step [128/147], loss=93.4668
	step [129/147], loss=85.5555
	step [130/147], loss=96.1103
	step [131/147], loss=91.3436
	step [132/147], loss=73.8796
	step [133/147], loss=66.7521
	step [134/147], loss=94.9387
	step [135/147], loss=87.9745
	step [136/147], loss=83.9290
	step [137/147], loss=89.3721
	step [138/147], loss=95.5717
	step [139/147], loss=75.4604
	step [140/147], loss=73.9149
	step [141/147], loss=81.8801
	step [142/147], loss=92.7902
	step [143/147], loss=86.3232
	step [144/147], loss=93.9841
	step [145/147], loss=76.0366
	step [146/147], loss=88.7749
	step [147/147], loss=23.2718
	Evaluating
	loss=0.0089, precision=0.4170, recall=0.8166, f1=0.5521
Training epoch 82
	step [1/147], loss=88.0993
	step [2/147], loss=76.5270
	step [3/147], loss=86.1583
	step [4/147], loss=84.1467
	step [5/147], loss=79.4231
	step [6/147], loss=96.1511
	step [7/147], loss=97.5137
	step [8/147], loss=84.4945
	step [9/147], loss=102.3835
	step [10/147], loss=80.7345
	step [11/147], loss=100.7045
	step [12/147], loss=79.8813
	step [13/147], loss=73.8730
	step [14/147], loss=91.8615
	step [15/147], loss=82.2072
	step [16/147], loss=82.7162
	step [17/147], loss=97.9242
	step [18/147], loss=79.1672
	step [19/147], loss=75.2858
	step [20/147], loss=91.6431
	step [21/147], loss=91.2131
	step [22/147], loss=88.0835
	step [23/147], loss=88.6945
	step [24/147], loss=85.7157
	step [25/147], loss=84.9660
	step [26/147], loss=94.0097
	step [27/147], loss=90.5661
	step [28/147], loss=108.7385
	step [29/147], loss=84.5183
	step [30/147], loss=64.2039
	step [31/147], loss=72.6954
	step [32/147], loss=88.1467
	step [33/147], loss=89.6162
	step [34/147], loss=84.6722
	step [35/147], loss=75.9920
	step [36/147], loss=76.7207
	step [37/147], loss=90.1019
	step [38/147], loss=57.7739
	step [39/147], loss=97.3719
	step [40/147], loss=81.6083
	step [41/147], loss=88.9370
	step [42/147], loss=84.5216
	step [43/147], loss=74.0423
	step [44/147], loss=71.5833
	step [45/147], loss=88.7153
	step [46/147], loss=86.5090
	step [47/147], loss=84.0597
	step [48/147], loss=63.2059
	step [49/147], loss=72.7237
	step [50/147], loss=77.3886
	step [51/147], loss=93.7818
	step [52/147], loss=68.9986
	step [53/147], loss=89.7094
	step [54/147], loss=82.5935
	step [55/147], loss=80.3314
	step [56/147], loss=80.5806
	step [57/147], loss=70.0526
	step [58/147], loss=83.0002
	step [59/147], loss=75.9175
	step [60/147], loss=94.6384
	step [61/147], loss=83.3688
	step [62/147], loss=78.8990
	step [63/147], loss=97.2042
	step [64/147], loss=84.8208
	step [65/147], loss=92.7656
	step [66/147], loss=99.7115
	step [67/147], loss=101.1522
	step [68/147], loss=87.5017
	step [69/147], loss=89.4252
	step [70/147], loss=92.8498
	step [71/147], loss=108.6808
	step [72/147], loss=72.2532
	step [73/147], loss=103.7931
	step [74/147], loss=83.4060
	step [75/147], loss=94.7267
	step [76/147], loss=79.2106
	step [77/147], loss=83.4423
	step [78/147], loss=92.5773
	step [79/147], loss=92.0813
	step [80/147], loss=91.8066
	step [81/147], loss=85.9552
	step [82/147], loss=79.1305
	step [83/147], loss=106.8634
	step [84/147], loss=75.0973
	step [85/147], loss=85.0946
	step [86/147], loss=82.8507
	step [87/147], loss=77.9749
	step [88/147], loss=78.5696
	step [89/147], loss=96.2319
	step [90/147], loss=68.2380
	step [91/147], loss=92.3896
	step [92/147], loss=91.8965
	step [93/147], loss=89.0547
	step [94/147], loss=80.8165
	step [95/147], loss=75.3322
	step [96/147], loss=87.6372
	step [97/147], loss=84.2265
	step [98/147], loss=97.4613
	step [99/147], loss=90.3873
	step [100/147], loss=91.5238
	step [101/147], loss=92.6777
	step [102/147], loss=93.2492
	step [103/147], loss=83.7551
	step [104/147], loss=98.0321
	step [105/147], loss=66.2163
	step [106/147], loss=80.1564
	step [107/147], loss=86.0464
	step [108/147], loss=73.4859
	step [109/147], loss=74.9237
	step [110/147], loss=81.1703
	step [111/147], loss=92.0361
	step [112/147], loss=93.7424
	step [113/147], loss=91.3666
	step [114/147], loss=79.0944
	step [115/147], loss=76.8571
	step [116/147], loss=85.7757
	step [117/147], loss=88.4537
	step [118/147], loss=86.7834
	step [119/147], loss=81.1864
	step [120/147], loss=86.9366
	step [121/147], loss=83.3202
	step [122/147], loss=87.3941
	step [123/147], loss=98.4884
	step [124/147], loss=73.0683
	step [125/147], loss=89.0493
	step [126/147], loss=105.8664
	step [127/147], loss=76.8591
	step [128/147], loss=101.7543
	step [129/147], loss=85.7279
	step [130/147], loss=89.1389
	step [131/147], loss=102.1458
	step [132/147], loss=72.4420
	step [133/147], loss=84.8300
	step [134/147], loss=83.7767
	step [135/147], loss=82.1153
	step [136/147], loss=98.0030
	step [137/147], loss=77.0679
	step [138/147], loss=73.8580
	step [139/147], loss=74.7209
	step [140/147], loss=85.2782
	step [141/147], loss=73.2234
	step [142/147], loss=93.3105
	step [143/147], loss=78.9341
	step [144/147], loss=92.2123
	step [145/147], loss=94.5451
	step [146/147], loss=83.1792
	step [147/147], loss=20.7037
	Evaluating
	loss=0.0099, precision=0.3742, recall=0.8429, f1=0.5183
Training epoch 83
	step [1/147], loss=91.0989
	step [2/147], loss=79.5167
	step [3/147], loss=67.2408
	step [4/147], loss=79.6400
	step [5/147], loss=109.0208
	step [6/147], loss=76.7936
	step [7/147], loss=86.6889
	step [8/147], loss=69.7807
	step [9/147], loss=83.1294
	step [10/147], loss=86.5101
	step [11/147], loss=88.9842
	step [12/147], loss=64.2699
	step [13/147], loss=84.1308
	step [14/147], loss=75.1572
	step [15/147], loss=75.5212
	step [16/147], loss=86.7072
	step [17/147], loss=77.9070
	step [18/147], loss=72.7769
	step [19/147], loss=98.0796
	step [20/147], loss=78.2232
	step [21/147], loss=77.8160
	step [22/147], loss=107.4953
	step [23/147], loss=99.8411
	step [24/147], loss=85.7687
	step [25/147], loss=80.2658
	step [26/147], loss=87.7843
	step [27/147], loss=105.5746
	step [28/147], loss=100.1516
	step [29/147], loss=90.5901
	step [30/147], loss=74.5607
	step [31/147], loss=84.7596
	step [32/147], loss=91.2727
	step [33/147], loss=89.4465
	step [34/147], loss=83.2504
	step [35/147], loss=85.4227
	step [36/147], loss=75.1732
	step [37/147], loss=74.4370
	step [38/147], loss=90.0731
	step [39/147], loss=86.8094
	step [40/147], loss=79.8597
	step [41/147], loss=93.9745
	step [42/147], loss=86.3269
	step [43/147], loss=93.3075
	step [44/147], loss=81.4340
	step [45/147], loss=85.0669
	step [46/147], loss=84.2650
	step [47/147], loss=92.0730
	step [48/147], loss=84.4504
	step [49/147], loss=85.5983
	step [50/147], loss=89.6511
	step [51/147], loss=84.2046
	step [52/147], loss=92.3042
	step [53/147], loss=88.7797
	step [54/147], loss=78.6757
	step [55/147], loss=73.1156
	step [56/147], loss=104.9850
	step [57/147], loss=69.3993
	step [58/147], loss=86.1054
	step [59/147], loss=81.0085
	step [60/147], loss=86.0188
	step [61/147], loss=70.1777
	step [62/147], loss=79.2708
	step [63/147], loss=90.4948
	step [64/147], loss=80.4552
	step [65/147], loss=73.9800
	step [66/147], loss=86.3042
	step [67/147], loss=84.8931
	step [68/147], loss=87.3513
	step [69/147], loss=73.2392
	step [70/147], loss=88.0896
	step [71/147], loss=85.8171
	step [72/147], loss=91.9352
	step [73/147], loss=82.2091
	step [74/147], loss=97.1536
	step [75/147], loss=81.2585
	step [76/147], loss=101.0664
	step [77/147], loss=72.9084
	step [78/147], loss=91.9655
	step [79/147], loss=85.6409
	step [80/147], loss=85.7851
	step [81/147], loss=96.3135
	step [82/147], loss=85.6432
	step [83/147], loss=83.9398
	step [84/147], loss=87.9535
	step [85/147], loss=81.1557
	step [86/147], loss=79.9733
	step [87/147], loss=78.0792
	step [88/147], loss=65.0656
	step [89/147], loss=79.7740
	step [90/147], loss=74.0259
	step [91/147], loss=84.6815
	step [92/147], loss=72.1848
	step [93/147], loss=90.0505
	step [94/147], loss=77.7356
	step [95/147], loss=72.1372
	step [96/147], loss=82.4210
	step [97/147], loss=86.2208
	step [98/147], loss=79.9323
	step [99/147], loss=84.1762
	step [100/147], loss=79.1389
	step [101/147], loss=96.1838
	step [102/147], loss=79.2727
	step [103/147], loss=85.8985
	step [104/147], loss=94.9722
	step [105/147], loss=108.6945
	step [106/147], loss=88.0482
	step [107/147], loss=83.0427
	step [108/147], loss=83.5143
	step [109/147], loss=93.4198
	step [110/147], loss=95.0752
	step [111/147], loss=97.1585
	step [112/147], loss=99.1417
	step [113/147], loss=92.0957
	step [114/147], loss=89.0164
	step [115/147], loss=77.3747
	step [116/147], loss=79.3441
	step [117/147], loss=102.0380
	step [118/147], loss=88.4199
	step [119/147], loss=69.4540
	step [120/147], loss=79.2949
	step [121/147], loss=109.6370
	step [122/147], loss=91.0199
	step [123/147], loss=80.2620
	step [124/147], loss=90.1286
	step [125/147], loss=80.1632
	step [126/147], loss=85.2322
	step [127/147], loss=83.8427
	step [128/147], loss=84.3542
	step [129/147], loss=91.1777
	step [130/147], loss=96.3606
	step [131/147], loss=91.5086
	step [132/147], loss=88.4827
	step [133/147], loss=92.5183
	step [134/147], loss=77.7918
	step [135/147], loss=85.8648
	step [136/147], loss=89.6288
	step [137/147], loss=80.7806
	step [138/147], loss=76.8045
	step [139/147], loss=94.5263
	step [140/147], loss=68.6268
	step [141/147], loss=79.6707
	step [142/147], loss=99.6172
	step [143/147], loss=94.8262
	step [144/147], loss=88.6257
	step [145/147], loss=99.0892
	step [146/147], loss=72.0885
	step [147/147], loss=21.6151
	Evaluating
	loss=0.0093, precision=0.3995, recall=0.8420, f1=0.5419
Training epoch 84
	step [1/147], loss=78.0210
	step [2/147], loss=82.1680
	step [3/147], loss=79.0343
	step [4/147], loss=82.5531
	step [5/147], loss=86.8992
	step [6/147], loss=96.1698
	step [7/147], loss=96.7813
	step [8/147], loss=88.7784
	step [9/147], loss=75.8105
	step [10/147], loss=67.3560
	step [11/147], loss=79.7549
	step [12/147], loss=86.6455
	step [13/147], loss=94.3512
	step [14/147], loss=101.4421
	step [15/147], loss=81.3304
	step [16/147], loss=92.6831
	step [17/147], loss=88.1000
	step [18/147], loss=75.4630
	step [19/147], loss=105.7837
	step [20/147], loss=84.6489
	step [21/147], loss=90.8772
	step [22/147], loss=83.4862
	step [23/147], loss=64.9525
	step [24/147], loss=78.4422
	step [25/147], loss=84.7057
	step [26/147], loss=72.3952
	step [27/147], loss=92.3502
	step [28/147], loss=85.7241
	step [29/147], loss=96.4358
	step [30/147], loss=89.8819
	step [31/147], loss=97.1857
	step [32/147], loss=98.5101
	step [33/147], loss=79.6788
	step [34/147], loss=74.7129
	step [35/147], loss=86.5485
	step [36/147], loss=76.2767
	step [37/147], loss=84.2150
	step [38/147], loss=88.9021
	step [39/147], loss=89.8932
	step [40/147], loss=75.2810
	step [41/147], loss=111.9139
	step [42/147], loss=85.5577
	step [43/147], loss=80.8093
	step [44/147], loss=81.1192
	step [45/147], loss=74.0992
	step [46/147], loss=94.2856
	step [47/147], loss=86.9951
	step [48/147], loss=66.6909
	step [49/147], loss=104.2692
	step [50/147], loss=88.4401
	step [51/147], loss=98.5456
	step [52/147], loss=88.3041
	step [53/147], loss=91.0164
	step [54/147], loss=105.3199
	step [55/147], loss=74.1278
	step [56/147], loss=71.0704
	step [57/147], loss=71.3100
	step [58/147], loss=73.0718
	step [59/147], loss=91.2424
	step [60/147], loss=74.5338
	step [61/147], loss=88.6247
	step [62/147], loss=94.6776
	step [63/147], loss=92.4408
	step [64/147], loss=76.9291
	step [65/147], loss=97.7819
	step [66/147], loss=76.6715
	step [67/147], loss=81.2253
	step [68/147], loss=85.9210
	step [69/147], loss=100.4478
	step [70/147], loss=83.3166
	step [71/147], loss=90.7424
	step [72/147], loss=77.1935
	step [73/147], loss=83.2839
	step [74/147], loss=81.4015
	step [75/147], loss=85.2676
	step [76/147], loss=95.2093
	step [77/147], loss=68.5510
	step [78/147], loss=71.8185
	step [79/147], loss=82.2395
	step [80/147], loss=67.2738
	step [81/147], loss=85.7158
	step [82/147], loss=78.8596
	step [83/147], loss=89.5707
	step [84/147], loss=74.1216
	step [85/147], loss=94.5736
	step [86/147], loss=80.6523
	step [87/147], loss=75.6102
	step [88/147], loss=86.2538
	step [89/147], loss=83.3944
	step [90/147], loss=97.0781
	step [91/147], loss=90.2129
	step [92/147], loss=65.1776
	step [93/147], loss=81.3207
	step [94/147], loss=93.8576
	step [95/147], loss=90.1385
	step [96/147], loss=87.0785
	step [97/147], loss=81.8035
	step [98/147], loss=84.7133
	step [99/147], loss=76.7708
	step [100/147], loss=80.6086
	step [101/147], loss=70.5378
	step [102/147], loss=87.0595
	step [103/147], loss=79.4793
	step [104/147], loss=77.0224
	step [105/147], loss=89.3281
	step [106/147], loss=88.3048
	step [107/147], loss=77.0421
	step [108/147], loss=65.6352
	step [109/147], loss=78.4638
	step [110/147], loss=84.0775
	step [111/147], loss=88.8895
	step [112/147], loss=79.4821
	step [113/147], loss=92.1050
	step [114/147], loss=95.3329
	step [115/147], loss=74.0609
	step [116/147], loss=87.0826
	step [117/147], loss=92.5701
	step [118/147], loss=80.5848
	step [119/147], loss=81.3557
	step [120/147], loss=73.1466
	step [121/147], loss=85.4449
	step [122/147], loss=88.0337
	step [123/147], loss=95.6328
	step [124/147], loss=77.2058
	step [125/147], loss=85.2001
	step [126/147], loss=90.9854
	step [127/147], loss=96.6271
	step [128/147], loss=85.0258
	step [129/147], loss=93.4832
	step [130/147], loss=89.2552
	step [131/147], loss=83.4465
	step [132/147], loss=76.0692
	step [133/147], loss=92.2846
	step [134/147], loss=87.7793
	step [135/147], loss=69.0734
	step [136/147], loss=78.7767
	step [137/147], loss=85.6526
	step [138/147], loss=98.5761
	step [139/147], loss=95.3203
	step [140/147], loss=83.5911
	step [141/147], loss=92.7802
	step [142/147], loss=77.3284
	step [143/147], loss=96.6973
	step [144/147], loss=86.6062
	step [145/147], loss=90.2599
	step [146/147], loss=77.9938
	step [147/147], loss=26.3297
	Evaluating
	loss=0.0079, precision=0.4596, recall=0.8208, f1=0.5892
saving model as: 3_saved_model.pth
Training epoch 85
	step [1/147], loss=97.9383
	step [2/147], loss=88.2465
	step [3/147], loss=93.1101
	step [4/147], loss=73.6190
	step [5/147], loss=90.4637
	step [6/147], loss=92.2728
	step [7/147], loss=80.1267
	step [8/147], loss=82.4636
	step [9/147], loss=87.2769
	step [10/147], loss=72.8763
	step [11/147], loss=69.1110
	step [12/147], loss=77.3603
	step [13/147], loss=82.2902
	step [14/147], loss=85.6967
	step [15/147], loss=95.9816
	step [16/147], loss=88.6339
	step [17/147], loss=70.8389
	step [18/147], loss=85.9400
	step [19/147], loss=77.8044
	step [20/147], loss=83.0997
	step [21/147], loss=85.6560
	step [22/147], loss=74.8998
	step [23/147], loss=91.0578
	step [24/147], loss=88.4377
	step [25/147], loss=78.9295
	step [26/147], loss=75.1042
	step [27/147], loss=79.3416
	step [28/147], loss=89.2509
	step [29/147], loss=78.2934
	step [30/147], loss=90.9234
	step [31/147], loss=81.4010
	step [32/147], loss=75.2570
	step [33/147], loss=84.4923
	step [34/147], loss=90.2433
	step [35/147], loss=82.0815
	step [36/147], loss=84.3615
	step [37/147], loss=77.0173
	step [38/147], loss=108.1646
	step [39/147], loss=75.4145
	step [40/147], loss=93.5462
	step [41/147], loss=84.5823
	step [42/147], loss=86.8151
	step [43/147], loss=69.2639
	step [44/147], loss=79.7154
	step [45/147], loss=91.2389
	step [46/147], loss=88.0137
	step [47/147], loss=90.8423
	step [48/147], loss=71.7859
	step [49/147], loss=82.4261
	step [50/147], loss=89.5517
	step [51/147], loss=82.8416
	step [52/147], loss=80.7691
	step [53/147], loss=83.0393
	step [54/147], loss=78.0071
	step [55/147], loss=76.9328
	step [56/147], loss=84.5800
	step [57/147], loss=93.2047
	step [58/147], loss=89.1684
	step [59/147], loss=64.2178
	step [60/147], loss=91.6492
	step [61/147], loss=69.3670
	step [62/147], loss=63.1674
	step [63/147], loss=78.7815
	step [64/147], loss=78.5692
	step [65/147], loss=76.2762
	step [66/147], loss=85.5047
	step [67/147], loss=74.8791
	step [68/147], loss=97.6939
	step [69/147], loss=78.6871
	step [70/147], loss=83.3842
	step [71/147], loss=89.3145
	step [72/147], loss=86.3192
	step [73/147], loss=63.0806
	step [74/147], loss=80.5271
	step [75/147], loss=97.6399
	step [76/147], loss=79.5595
	step [77/147], loss=92.4859
	step [78/147], loss=80.9588
	step [79/147], loss=80.7405
	step [80/147], loss=118.2415
	step [81/147], loss=92.9533
	step [82/147], loss=89.3257
	step [83/147], loss=71.9387
	step [84/147], loss=98.5360
	step [85/147], loss=94.0073
	step [86/147], loss=101.9837
	step [87/147], loss=71.5173
	step [88/147], loss=89.4783
	step [89/147], loss=94.5476
	step [90/147], loss=86.5573
	step [91/147], loss=77.0245
	step [92/147], loss=83.3895
	step [93/147], loss=78.9681
	step [94/147], loss=87.3875
	step [95/147], loss=76.4889
	step [96/147], loss=74.7472
	step [97/147], loss=87.7656
	step [98/147], loss=86.6724
	step [99/147], loss=89.2215
	step [100/147], loss=99.3875
	step [101/147], loss=94.0079
	step [102/147], loss=77.6699
	step [103/147], loss=83.7076
	step [104/147], loss=104.1817
	step [105/147], loss=96.3725
	step [106/147], loss=87.7181
	step [107/147], loss=89.0291
	step [108/147], loss=77.0626
	step [109/147], loss=83.4953
	step [110/147], loss=83.7915
	step [111/147], loss=75.0823
	step [112/147], loss=75.6125
	step [113/147], loss=87.6168
	step [114/147], loss=86.8441
	step [115/147], loss=88.7888
	step [116/147], loss=87.6330
	step [117/147], loss=76.0531
	step [118/147], loss=82.2468
	step [119/147], loss=89.9291
	step [120/147], loss=89.1651
	step [121/147], loss=72.7249
	step [122/147], loss=90.6983
	step [123/147], loss=85.9122
	step [124/147], loss=86.5579
	step [125/147], loss=90.1142
	step [126/147], loss=100.8525
	step [127/147], loss=71.9323
	step [128/147], loss=79.6810
	step [129/147], loss=88.7495
	step [130/147], loss=83.0520
	step [131/147], loss=90.0047
	step [132/147], loss=82.9624
	step [133/147], loss=89.3474
	step [134/147], loss=113.9693
	step [135/147], loss=89.8057
	step [136/147], loss=79.7643
	step [137/147], loss=94.0428
	step [138/147], loss=93.7225
	step [139/147], loss=96.9681
	step [140/147], loss=73.8509
	step [141/147], loss=85.1413
	step [142/147], loss=91.4764
	step [143/147], loss=86.2404
	step [144/147], loss=87.4356
	step [145/147], loss=79.7507
	step [146/147], loss=88.5537
	step [147/147], loss=28.0239
	Evaluating
	loss=0.0092, precision=0.4048, recall=0.8356, f1=0.5454
Training epoch 86
	step [1/147], loss=94.3924
	step [2/147], loss=102.5464
	step [3/147], loss=79.2801
	step [4/147], loss=82.6001
	step [5/147], loss=80.9020
	step [6/147], loss=80.0459
	step [7/147], loss=69.8591
	step [8/147], loss=100.5766
	step [9/147], loss=96.3549
	step [10/147], loss=94.5282
	step [11/147], loss=95.6172
	step [12/147], loss=92.5510
	step [13/147], loss=93.7360
	step [14/147], loss=86.0487
	step [15/147], loss=82.1662
	step [16/147], loss=92.4356
	step [17/147], loss=91.2086
	step [18/147], loss=79.5039
	step [19/147], loss=96.0234
	step [20/147], loss=87.8800
	step [21/147], loss=76.1153
	step [22/147], loss=83.5837
	step [23/147], loss=75.7600
	step [24/147], loss=80.8729
	step [25/147], loss=82.7836
	step [26/147], loss=80.0606
	step [27/147], loss=92.5160
	step [28/147], loss=85.8307
	step [29/147], loss=90.6870
	step [30/147], loss=74.2390
	step [31/147], loss=102.8811
	step [32/147], loss=88.6904
	step [33/147], loss=74.5849
	step [34/147], loss=81.7217
	step [35/147], loss=78.1556
	step [36/147], loss=74.9670
	step [37/147], loss=78.7720
	step [38/147], loss=85.9312
	step [39/147], loss=89.4402
	step [40/147], loss=86.3067
	step [41/147], loss=80.4878
	step [42/147], loss=98.1742
	step [43/147], loss=85.6724
	step [44/147], loss=85.7572
	step [45/147], loss=82.4215
	step [46/147], loss=71.1845
	step [47/147], loss=72.6411
	step [48/147], loss=81.0916
	step [49/147], loss=76.5368
	step [50/147], loss=81.9814
	step [51/147], loss=80.4187
	step [52/147], loss=81.5201
	step [53/147], loss=73.4377
	step [54/147], loss=76.7106
	step [55/147], loss=92.9202
	step [56/147], loss=84.0704
	step [57/147], loss=77.3251
	step [58/147], loss=68.8508
	step [59/147], loss=88.0442
	step [60/147], loss=86.2543
	step [61/147], loss=83.9231
	step [62/147], loss=77.6329
	step [63/147], loss=96.7439
	step [64/147], loss=80.9564
	step [65/147], loss=106.7960
	step [66/147], loss=87.0244
	step [67/147], loss=81.5145
	step [68/147], loss=77.2956
	step [69/147], loss=80.9758
	step [70/147], loss=92.3704
	step [71/147], loss=82.7882
	step [72/147], loss=92.1471
	step [73/147], loss=71.0092
	step [74/147], loss=88.6913
	step [75/147], loss=80.6159
	step [76/147], loss=79.2779
	step [77/147], loss=82.3619
	step [78/147], loss=98.6393
	step [79/147], loss=88.3496
	step [80/147], loss=82.9732
	step [81/147], loss=79.8977
	step [82/147], loss=85.6211
	step [83/147], loss=90.1252
	step [84/147], loss=82.0865
	step [85/147], loss=91.2163
	step [86/147], loss=90.5110
	step [87/147], loss=103.3023
	step [88/147], loss=67.0826
	step [89/147], loss=99.3481
	step [90/147], loss=72.4321
	step [91/147], loss=94.5442
	step [92/147], loss=80.8368
	step [93/147], loss=86.6934
	step [94/147], loss=78.9602
	step [95/147], loss=102.6312
	step [96/147], loss=77.2138
	step [97/147], loss=72.4883
	step [98/147], loss=86.4742
	step [99/147], loss=94.2797
	step [100/147], loss=92.3074
	step [101/147], loss=96.0766
	step [102/147], loss=83.2975
	step [103/147], loss=65.1761
	step [104/147], loss=86.5302
	step [105/147], loss=92.3552
	step [106/147], loss=93.8328
	step [107/147], loss=68.9841
	step [108/147], loss=84.5138
	step [109/147], loss=73.1184
	step [110/147], loss=81.5274
	step [111/147], loss=88.2466
	step [112/147], loss=75.8344
	step [113/147], loss=80.0671
	step [114/147], loss=91.3723
	step [115/147], loss=83.3603
	step [116/147], loss=89.0467
	step [117/147], loss=81.4918
	step [118/147], loss=85.6555
	step [119/147], loss=74.6782
	step [120/147], loss=99.0347
	step [121/147], loss=71.7479
	step [122/147], loss=75.4979
	step [123/147], loss=89.2527
	step [124/147], loss=84.9219
	step [125/147], loss=88.8763
	step [126/147], loss=80.8236
	step [127/147], loss=82.9840
	step [128/147], loss=86.7159
	step [129/147], loss=82.2329
	step [130/147], loss=84.9619
	step [131/147], loss=101.5427
	step [132/147], loss=86.6850
	step [133/147], loss=81.8523
	step [134/147], loss=79.1663
	step [135/147], loss=80.1714
	step [136/147], loss=85.0429
	step [137/147], loss=76.5313
	step [138/147], loss=72.8952
	step [139/147], loss=81.9157
	step [140/147], loss=73.1220
	step [141/147], loss=81.1044
	step [142/147], loss=83.4521
	step [143/147], loss=82.8207
	step [144/147], loss=71.2720
	step [145/147], loss=94.5200
	step [146/147], loss=80.7567
	step [147/147], loss=22.0452
	Evaluating
	loss=0.0112, precision=0.3313, recall=0.8496, f1=0.4767
Training epoch 87
	step [1/147], loss=74.5135
	step [2/147], loss=83.4427
	step [3/147], loss=70.6005
	step [4/147], loss=80.4326
	step [5/147], loss=90.2247
	step [6/147], loss=77.8059
	step [7/147], loss=76.4511
	step [8/147], loss=80.2340
	step [9/147], loss=98.5455
	step [10/147], loss=92.3039
	step [11/147], loss=79.2020
	step [12/147], loss=105.4834
	step [13/147], loss=81.8797
	step [14/147], loss=79.6696
	step [15/147], loss=78.8592
	step [16/147], loss=96.1221
	step [17/147], loss=68.0864
	step [18/147], loss=76.0847
	step [19/147], loss=85.0081
	step [20/147], loss=76.4587
	step [21/147], loss=90.1029
	step [22/147], loss=92.7550
	step [23/147], loss=99.8328
	step [24/147], loss=80.6424
	step [25/147], loss=78.7840
	step [26/147], loss=77.8648
	step [27/147], loss=86.8831
	step [28/147], loss=87.8446
	step [29/147], loss=89.6167
	step [30/147], loss=91.7195
	step [31/147], loss=95.1077
	step [32/147], loss=81.6763
	step [33/147], loss=72.7505
	step [34/147], loss=100.1039
	step [35/147], loss=83.8140
	step [36/147], loss=77.3574
	step [37/147], loss=87.6581
	step [38/147], loss=76.7327
	step [39/147], loss=84.7675
	step [40/147], loss=79.4972
	step [41/147], loss=82.5161
	step [42/147], loss=88.8974
	step [43/147], loss=83.4281
	step [44/147], loss=83.8078
	step [45/147], loss=80.0503
	step [46/147], loss=78.7569
	step [47/147], loss=77.3766
	step [48/147], loss=87.3518
	step [49/147], loss=93.4217
	step [50/147], loss=82.5022
	step [51/147], loss=99.1420
	step [52/147], loss=75.7081
	step [53/147], loss=74.2601
	step [54/147], loss=95.1776
	step [55/147], loss=94.8371
	step [56/147], loss=67.8631
	step [57/147], loss=85.0064
	step [58/147], loss=82.1897
	step [59/147], loss=85.5568
	step [60/147], loss=101.5891
	step [61/147], loss=80.1746
	step [62/147], loss=81.4224
	step [63/147], loss=90.5301
	step [64/147], loss=87.3261
	step [65/147], loss=81.5306
	step [66/147], loss=71.7060
	step [67/147], loss=73.8577
	step [68/147], loss=76.0278
	step [69/147], loss=69.5632
	step [70/147], loss=88.5438
	step [71/147], loss=88.8386
	step [72/147], loss=91.6678
	step [73/147], loss=77.8014
	step [74/147], loss=79.1641
	step [75/147], loss=80.2667
	step [76/147], loss=84.8264
	step [77/147], loss=84.3224
	step [78/147], loss=96.0759
	step [79/147], loss=90.2717
	step [80/147], loss=86.2906
	step [81/147], loss=71.6438
	step [82/147], loss=81.9056
	step [83/147], loss=88.8221
	step [84/147], loss=84.7368
	step [85/147], loss=83.2337
	step [86/147], loss=74.9077
	step [87/147], loss=80.9741
	step [88/147], loss=88.0667
	step [89/147], loss=78.7102
	step [90/147], loss=79.9358
	step [91/147], loss=81.1224
	step [92/147], loss=86.0307
	step [93/147], loss=80.0526
	step [94/147], loss=88.8325
	step [95/147], loss=90.5385
	step [96/147], loss=74.4127
	step [97/147], loss=79.2766
	step [98/147], loss=72.9668
	step [99/147], loss=88.8672
	step [100/147], loss=72.3066
	step [101/147], loss=80.6270
	step [102/147], loss=82.5926
	step [103/147], loss=75.3078
	step [104/147], loss=64.4151
	step [105/147], loss=76.4647
	step [106/147], loss=101.9726
	step [107/147], loss=84.7469
	step [108/147], loss=82.9874
	step [109/147], loss=85.8508
	step [110/147], loss=76.4024
	step [111/147], loss=77.6830
	step [112/147], loss=89.7084
	step [113/147], loss=91.9925
	step [114/147], loss=85.1623
	step [115/147], loss=90.8965
	step [116/147], loss=82.9018
	step [117/147], loss=69.1295
	step [118/147], loss=98.8834
	step [119/147], loss=69.9290
	step [120/147], loss=81.6151
	step [121/147], loss=81.4974
	step [122/147], loss=102.3087
	step [123/147], loss=88.7854
	step [124/147], loss=80.8357
	step [125/147], loss=76.9787
	step [126/147], loss=96.8302
	step [127/147], loss=82.4490
	step [128/147], loss=86.5463
	step [129/147], loss=76.1181
	step [130/147], loss=80.4931
	step [131/147], loss=81.6700
	step [132/147], loss=90.7226
	step [133/147], loss=81.2650
	step [134/147], loss=95.3539
	step [135/147], loss=90.9160
	step [136/147], loss=102.6771
	step [137/147], loss=98.6782
	step [138/147], loss=84.7248
	step [139/147], loss=84.4540
	step [140/147], loss=84.0971
	step [141/147], loss=99.9979
	step [142/147], loss=90.7093
	step [143/147], loss=83.9176
	step [144/147], loss=68.0111
	step [145/147], loss=84.4485
	step [146/147], loss=89.1946
	step [147/147], loss=24.0254
	Evaluating
	loss=0.0084, precision=0.4220, recall=0.8379, f1=0.5613
Training epoch 88
	step [1/147], loss=87.1653
	step [2/147], loss=84.5686
	step [3/147], loss=73.5226
	step [4/147], loss=70.9008
	step [5/147], loss=73.1618
	step [6/147], loss=89.8717
	step [7/147], loss=86.2430
	step [8/147], loss=88.0247
	step [9/147], loss=76.3184
	step [10/147], loss=80.7128
	step [11/147], loss=87.2291
	step [12/147], loss=66.1463
	step [13/147], loss=93.8353
	step [14/147], loss=79.8848
	step [15/147], loss=87.4204
	step [16/147], loss=85.5545
	step [17/147], loss=88.4161
	step [18/147], loss=79.7640
	step [19/147], loss=73.3396
	step [20/147], loss=98.9337
	step [21/147], loss=83.8222
	step [22/147], loss=87.3606
	step [23/147], loss=84.7241
	step [24/147], loss=84.0104
	step [25/147], loss=76.0784
	step [26/147], loss=91.1357
	step [27/147], loss=82.2748
	step [28/147], loss=86.8059
	step [29/147], loss=91.7642
	step [30/147], loss=79.4653
	step [31/147], loss=88.9892
	step [32/147], loss=91.4349
	step [33/147], loss=88.6809
	step [34/147], loss=92.2711
	step [35/147], loss=91.7660
	step [36/147], loss=78.0675
	step [37/147], loss=69.8002
	step [38/147], loss=81.7618
	step [39/147], loss=93.7013
	step [40/147], loss=71.7927
	step [41/147], loss=70.3856
	step [42/147], loss=84.1897
	step [43/147], loss=60.1446
	step [44/147], loss=89.7980
	step [45/147], loss=88.4135
	step [46/147], loss=78.8596
	step [47/147], loss=104.2046
	step [48/147], loss=72.0104
	step [49/147], loss=85.5905
	step [50/147], loss=79.4603
	step [51/147], loss=91.2667
	step [52/147], loss=87.0041
	step [53/147], loss=72.8390
	step [54/147], loss=79.3016
	step [55/147], loss=102.8507
	step [56/147], loss=88.0801
	step [57/147], loss=86.8978
	step [58/147], loss=64.7504
	step [59/147], loss=95.1129
	step [60/147], loss=69.5737
	step [61/147], loss=94.5983
	step [62/147], loss=72.1226
	step [63/147], loss=83.9897
	step [64/147], loss=91.3761
	step [65/147], loss=76.1524
	step [66/147], loss=69.3177
	step [67/147], loss=92.7794
	step [68/147], loss=76.7876
	step [69/147], loss=72.9561
	step [70/147], loss=75.5367
	step [71/147], loss=74.5290
	step [72/147], loss=80.1517
	step [73/147], loss=108.5591
	step [74/147], loss=82.8561
	step [75/147], loss=88.7621
	step [76/147], loss=76.1466
	step [77/147], loss=77.6579
	step [78/147], loss=88.4835
	step [79/147], loss=84.4596
	step [80/147], loss=80.8475
	step [81/147], loss=95.8498
	step [82/147], loss=91.6495
	step [83/147], loss=97.7578
	step [84/147], loss=82.0479
	step [85/147], loss=82.2454
	step [86/147], loss=84.5011
	step [87/147], loss=84.0113
	step [88/147], loss=83.2695
	step [89/147], loss=82.6066
	step [90/147], loss=78.0266
	step [91/147], loss=96.5039
	step [92/147], loss=90.2083
	step [93/147], loss=75.5578
	step [94/147], loss=88.6169
	step [95/147], loss=84.5245
	step [96/147], loss=89.1150
	step [97/147], loss=89.3562
	step [98/147], loss=77.3814
	step [99/147], loss=99.5278
	step [100/147], loss=90.8236
	step [101/147], loss=76.4954
	step [102/147], loss=76.9537
	step [103/147], loss=95.7410
	step [104/147], loss=90.0705
	step [105/147], loss=76.5545
	step [106/147], loss=95.2364
	step [107/147], loss=77.7147
	step [108/147], loss=84.2359
	step [109/147], loss=89.1856
	step [110/147], loss=95.7078
	step [111/147], loss=112.1459
	step [112/147], loss=83.0145
	step [113/147], loss=81.6822
	step [114/147], loss=78.5465
	step [115/147], loss=86.7285
	step [116/147], loss=80.6680
	step [117/147], loss=89.3113
	step [118/147], loss=77.9530
	step [119/147], loss=70.8454
	step [120/147], loss=79.2818
	step [121/147], loss=85.5033
	step [122/147], loss=83.0086
	step [123/147], loss=71.4631
	step [124/147], loss=90.1736
	step [125/147], loss=71.0875
	step [126/147], loss=83.3493
	step [127/147], loss=84.4735
	step [128/147], loss=68.9273
	step [129/147], loss=79.8212
	step [130/147], loss=85.9397
	step [131/147], loss=96.6293
	step [132/147], loss=83.2969
	step [133/147], loss=89.6855
	step [134/147], loss=88.3023
	step [135/147], loss=89.1832
	step [136/147], loss=82.3128
	step [137/147], loss=90.8350
	step [138/147], loss=99.4120
	step [139/147], loss=91.6408
	step [140/147], loss=77.1052
	step [141/147], loss=78.0710
	step [142/147], loss=90.6135
	step [143/147], loss=88.9298
	step [144/147], loss=90.1335
	step [145/147], loss=91.9820
	step [146/147], loss=94.0926
	step [147/147], loss=21.5658
	Evaluating
	loss=0.0096, precision=0.3794, recall=0.8331, f1=0.5214
Training epoch 89
	step [1/147], loss=68.1037
	step [2/147], loss=80.7001
	step [3/147], loss=79.4294
	step [4/147], loss=74.8919
	step [5/147], loss=79.7422
	step [6/147], loss=92.5229
	step [7/147], loss=87.0529
	step [8/147], loss=81.7000
	step [9/147], loss=82.3193
	step [10/147], loss=82.3161
	step [11/147], loss=81.0589
	step [12/147], loss=82.3391
	step [13/147], loss=88.0623
	step [14/147], loss=89.9790
	step [15/147], loss=84.8557
	step [16/147], loss=91.1938
	step [17/147], loss=85.9288
	step [18/147], loss=78.7477
	step [19/147], loss=90.9546
	step [20/147], loss=82.7542
	step [21/147], loss=69.9926
	step [22/147], loss=82.5895
	step [23/147], loss=91.8240
	step [24/147], loss=82.6062
	step [25/147], loss=111.4860
	step [26/147], loss=82.8154
	step [27/147], loss=73.8183
	step [28/147], loss=90.8267
	step [29/147], loss=65.7847
	step [30/147], loss=73.7987
	step [31/147], loss=82.9925
	step [32/147], loss=85.9314
	step [33/147], loss=86.9093
	step [34/147], loss=91.4253
	step [35/147], loss=102.4768
	step [36/147], loss=87.9918
	step [37/147], loss=83.1013
	step [38/147], loss=81.5301
	step [39/147], loss=86.4924
	step [40/147], loss=79.9175
	step [41/147], loss=85.8250
	step [42/147], loss=87.3881
	step [43/147], loss=95.9416
	step [44/147], loss=75.8787
	step [45/147], loss=81.2334
	step [46/147], loss=87.5836
	step [47/147], loss=73.7085
	step [48/147], loss=97.8447
	step [49/147], loss=87.6595
	step [50/147], loss=79.9431
	step [51/147], loss=77.3035
	step [52/147], loss=89.7974
	step [53/147], loss=93.2597
	step [54/147], loss=86.6030
	step [55/147], loss=93.0272
	step [56/147], loss=69.6578
	step [57/147], loss=68.2856
	step [58/147], loss=98.2256
	step [59/147], loss=89.8676
	step [60/147], loss=84.3900
	step [61/147], loss=76.4482
	step [62/147], loss=92.8751
	step [63/147], loss=78.7640
	step [64/147], loss=91.8444
	step [65/147], loss=89.2757
	step [66/147], loss=87.7014
	step [67/147], loss=83.9201
	step [68/147], loss=77.0843
	step [69/147], loss=89.1417
	step [70/147], loss=80.2673
	step [71/147], loss=82.1269
	step [72/147], loss=75.1366
	step [73/147], loss=66.3190
	step [74/147], loss=76.3858
	step [75/147], loss=74.5325
	step [76/147], loss=90.5091
	step [77/147], loss=77.0566
	step [78/147], loss=79.1303
	step [79/147], loss=80.6031
	step [80/147], loss=88.1634
	step [81/147], loss=90.6685
	step [82/147], loss=77.9220
	step [83/147], loss=69.2455
	step [84/147], loss=88.9431
	step [85/147], loss=104.2159
	step [86/147], loss=72.1496
	step [87/147], loss=83.9488
	step [88/147], loss=83.1400
	step [89/147], loss=94.5766
	step [90/147], loss=77.7028
	step [91/147], loss=76.6034
	step [92/147], loss=73.4786
	step [93/147], loss=88.3160
	step [94/147], loss=105.6357
	step [95/147], loss=83.7132
	step [96/147], loss=90.4883
	step [97/147], loss=75.5884
	step [98/147], loss=85.5393
	step [99/147], loss=85.9909
	step [100/147], loss=65.0197
	step [101/147], loss=86.9229
	step [102/147], loss=93.4599
	step [103/147], loss=92.9597
	step [104/147], loss=87.2372
	step [105/147], loss=74.5095
	step [106/147], loss=92.0801
	step [107/147], loss=85.8295
	step [108/147], loss=81.9892
	step [109/147], loss=88.8056
	step [110/147], loss=85.7599
	step [111/147], loss=92.0156
	step [112/147], loss=89.4232
	step [113/147], loss=82.2525
	step [114/147], loss=98.3865
	step [115/147], loss=93.0913
	step [116/147], loss=63.7716
	step [117/147], loss=71.9474
	step [118/147], loss=63.4930
	step [119/147], loss=80.1398
	step [120/147], loss=89.5657
	step [121/147], loss=89.6824
	step [122/147], loss=78.3224
	step [123/147], loss=100.9967
	step [124/147], loss=75.2163
	step [125/147], loss=78.2544
	step [126/147], loss=66.9882
	step [127/147], loss=74.1413
	step [128/147], loss=85.4808
	step [129/147], loss=89.9790
	step [130/147], loss=88.6782
	step [131/147], loss=88.6263
	step [132/147], loss=72.5496
	step [133/147], loss=67.1239
	step [134/147], loss=97.2855
	step [135/147], loss=84.2496
	step [136/147], loss=83.5409
	step [137/147], loss=100.2055
	step [138/147], loss=66.4091
	step [139/147], loss=77.5953
	step [140/147], loss=92.6706
	step [141/147], loss=85.6663
	step [142/147], loss=87.1190
	step [143/147], loss=67.7423
	step [144/147], loss=86.7169
	step [145/147], loss=99.8195
	step [146/147], loss=98.5730
	step [147/147], loss=26.2556
	Evaluating
	loss=0.0089, precision=0.3998, recall=0.8343, f1=0.5406
Training epoch 90
	step [1/147], loss=78.7554
	step [2/147], loss=75.0333
	step [3/147], loss=75.5373
	step [4/147], loss=107.2134
	step [5/147], loss=76.4661
	step [6/147], loss=85.7507
	step [7/147], loss=72.3541
	step [8/147], loss=82.8596
	step [9/147], loss=82.3484
	step [10/147], loss=89.3481
	step [11/147], loss=76.9032
	step [12/147], loss=87.7413
	step [13/147], loss=102.5307
	step [14/147], loss=92.9135
	step [15/147], loss=85.1750
	step [16/147], loss=78.4735
	step [17/147], loss=83.5047
	step [18/147], loss=85.1652
	step [19/147], loss=85.9917
	step [20/147], loss=102.7724
	step [21/147], loss=88.8638
	step [22/147], loss=87.8794
	step [23/147], loss=80.6042
	step [24/147], loss=65.3541
	step [25/147], loss=83.4690
	step [26/147], loss=85.4301
	step [27/147], loss=87.7116
	step [28/147], loss=72.2132
	step [29/147], loss=79.0214
	step [30/147], loss=88.2941
	step [31/147], loss=83.0464
	step [32/147], loss=86.3211
	step [33/147], loss=91.5016
	step [34/147], loss=71.2023
	step [35/147], loss=90.9723
	step [36/147], loss=73.1097
	step [37/147], loss=73.8619
	step [38/147], loss=90.6666
	step [39/147], loss=84.2361
	step [40/147], loss=82.6509
	step [41/147], loss=90.0457
	step [42/147], loss=88.1119
	step [43/147], loss=83.8968
	step [44/147], loss=80.1084
	step [45/147], loss=88.1972
	step [46/147], loss=73.1192
	step [47/147], loss=69.6235
	step [48/147], loss=91.8995
	step [49/147], loss=71.3117
	step [50/147], loss=85.2977
	step [51/147], loss=73.8397
	step [52/147], loss=85.1052
	step [53/147], loss=103.4581
	step [54/147], loss=108.0919
	step [55/147], loss=80.0762
	step [56/147], loss=79.8527
	step [57/147], loss=88.6271
	step [58/147], loss=78.6580
	step [59/147], loss=70.3195
	step [60/147], loss=85.5452
	step [61/147], loss=87.3595
	step [62/147], loss=81.7876
	step [63/147], loss=77.7099
	step [64/147], loss=89.1940
	step [65/147], loss=83.0071
	step [66/147], loss=96.1133
	step [67/147], loss=78.4015
	step [68/147], loss=71.8516
	step [69/147], loss=86.3601
	step [70/147], loss=74.1843
	step [71/147], loss=84.9763
	step [72/147], loss=83.6964
	step [73/147], loss=77.8790
	step [74/147], loss=80.0713
	step [75/147], loss=90.7526
	step [76/147], loss=86.1488
	step [77/147], loss=96.2831
	step [78/147], loss=83.9743
	step [79/147], loss=92.8429
	step [80/147], loss=87.7165
	step [81/147], loss=78.2803
	step [82/147], loss=81.8811
	step [83/147], loss=92.6028
	step [84/147], loss=77.9219
	step [85/147], loss=100.5655
	step [86/147], loss=87.9276
	step [87/147], loss=86.6276
	step [88/147], loss=79.2973
	step [89/147], loss=84.3950
	step [90/147], loss=78.2051
	step [91/147], loss=73.8979
	step [92/147], loss=66.3060
	step [93/147], loss=70.0451
	step [94/147], loss=96.3933
	step [95/147], loss=97.9600
	step [96/147], loss=81.2397
	step [97/147], loss=86.5475
	step [98/147], loss=85.4343
	step [99/147], loss=79.8260
	step [100/147], loss=83.2253
	step [101/147], loss=76.7413
	step [102/147], loss=88.1575
	step [103/147], loss=90.9195
	step [104/147], loss=77.5620
	step [105/147], loss=66.3892
	step [106/147], loss=89.4361
	step [107/147], loss=74.2880
	step [108/147], loss=73.9078
	step [109/147], loss=66.6295
	step [110/147], loss=88.1164
	step [111/147], loss=83.0227
	step [112/147], loss=92.0065
	step [113/147], loss=94.1189
	step [114/147], loss=86.9811
	step [115/147], loss=85.0201
	step [116/147], loss=86.2254
	step [117/147], loss=92.0037
	step [118/147], loss=98.0504
	step [119/147], loss=86.2153
	step [120/147], loss=97.8979
	step [121/147], loss=86.1703
	step [122/147], loss=77.2985
	step [123/147], loss=79.2227
	step [124/147], loss=75.5549
	step [125/147], loss=91.0545
	step [126/147], loss=71.3183
	step [127/147], loss=66.5676
	step [128/147], loss=87.9481
	step [129/147], loss=71.2386
	step [130/147], loss=75.2615
	step [131/147], loss=87.1872
	step [132/147], loss=102.4175
	step [133/147], loss=79.4582
	step [134/147], loss=79.1663
	step [135/147], loss=76.8456
	step [136/147], loss=85.0183
	step [137/147], loss=90.4452
	step [138/147], loss=67.8611
	step [139/147], loss=81.4434
	step [140/147], loss=84.2934
	step [141/147], loss=96.6407
	step [142/147], loss=98.6992
	step [143/147], loss=85.2616
	step [144/147], loss=72.6459
	step [145/147], loss=92.0066
	step [146/147], loss=77.0601
	step [147/147], loss=18.7787
	Evaluating
	loss=0.0080, precision=0.4455, recall=0.8386, f1=0.5818
Training epoch 91
	step [1/147], loss=93.0182
	step [2/147], loss=74.1289
	step [3/147], loss=89.0631
	step [4/147], loss=94.1859
	step [5/147], loss=81.8774
	step [6/147], loss=81.2772
	step [7/147], loss=74.6491
	step [8/147], loss=85.1441
	step [9/147], loss=85.6066
	step [10/147], loss=89.9942
	step [11/147], loss=71.8046
	step [12/147], loss=89.5969
	step [13/147], loss=89.2472
	step [14/147], loss=87.4062
	step [15/147], loss=93.4303
	step [16/147], loss=95.0131
	step [17/147], loss=92.7160
	step [18/147], loss=68.6939
	step [19/147], loss=83.0561
	step [20/147], loss=88.0954
	step [21/147], loss=87.9778
	step [22/147], loss=83.7250
	step [23/147], loss=78.4799
	step [24/147], loss=80.2126
	step [25/147], loss=90.4218
	step [26/147], loss=97.2726
	step [27/147], loss=92.1630
	step [28/147], loss=76.3523
	step [29/147], loss=66.1373
	step [30/147], loss=90.3290
	step [31/147], loss=74.9467
	step [32/147], loss=77.7938
	step [33/147], loss=94.3933
	step [34/147], loss=88.1803
	step [35/147], loss=81.7013
	step [36/147], loss=93.4838
	step [37/147], loss=81.8271
	step [38/147], loss=83.7783
	step [39/147], loss=81.3034
	step [40/147], loss=83.5068
	step [41/147], loss=82.7552
	step [42/147], loss=78.0611
	step [43/147], loss=70.0450
	step [44/147], loss=73.5594
	step [45/147], loss=95.8267
	step [46/147], loss=72.9179
	step [47/147], loss=82.7992
	step [48/147], loss=81.6345
	step [49/147], loss=88.3519
	step [50/147], loss=74.4094
	step [51/147], loss=90.8713
	step [52/147], loss=85.9758
	step [53/147], loss=73.2866
	step [54/147], loss=85.7721
	step [55/147], loss=74.8720
	step [56/147], loss=81.8539
	step [57/147], loss=69.3660
	step [58/147], loss=73.9115
	step [59/147], loss=88.8996
	step [60/147], loss=84.2543
	step [61/147], loss=82.7705
	step [62/147], loss=70.1795
	step [63/147], loss=100.6791
	step [64/147], loss=85.7174
	step [65/147], loss=76.2153
	step [66/147], loss=85.9356
	step [67/147], loss=79.0905
	step [68/147], loss=80.7999
	step [69/147], loss=86.2044
	step [70/147], loss=90.2619
	step [71/147], loss=72.8020
	step [72/147], loss=85.8971
	step [73/147], loss=92.6986
	step [74/147], loss=83.6255
	step [75/147], loss=76.8056
	step [76/147], loss=78.4904
	step [77/147], loss=74.3436
	step [78/147], loss=85.0644
	step [79/147], loss=83.2108
	step [80/147], loss=86.5107
	step [81/147], loss=83.4766
	step [82/147], loss=89.6089
	step [83/147], loss=76.8819
	step [84/147], loss=70.5346
	step [85/147], loss=80.1726
	step [86/147], loss=79.9091
	step [87/147], loss=80.2354
	step [88/147], loss=81.4058
	step [89/147], loss=86.5407
	step [90/147], loss=86.7410
	step [91/147], loss=93.1893
	step [92/147], loss=81.7266
	step [93/147], loss=83.2114
	step [94/147], loss=76.1553
	step [95/147], loss=91.6352
	step [96/147], loss=100.9075
	step [97/147], loss=84.4949
	step [98/147], loss=74.6529
	step [99/147], loss=77.7754
	step [100/147], loss=73.8013
	step [101/147], loss=84.4307
	step [102/147], loss=83.0589
	step [103/147], loss=89.3714
	step [104/147], loss=94.6711
	step [105/147], loss=96.7772
	step [106/147], loss=88.9140
	step [107/147], loss=97.0503
	step [108/147], loss=88.2367
	step [109/147], loss=85.4205
	step [110/147], loss=88.5968
	step [111/147], loss=70.0512
	step [112/147], loss=64.9198
	step [113/147], loss=69.0802
	step [114/147], loss=80.5265
	step [115/147], loss=75.4775
	step [116/147], loss=65.7147
	step [117/147], loss=81.4352
	step [118/147], loss=103.2081
	step [119/147], loss=81.6979
	step [120/147], loss=81.4955
	step [121/147], loss=87.8557
	step [122/147], loss=79.3886
	step [123/147], loss=89.2565
	step [124/147], loss=74.7803
	step [125/147], loss=95.4158
	step [126/147], loss=85.1690
	step [127/147], loss=89.1217
	step [128/147], loss=73.8348
	step [129/147], loss=88.5232
	step [130/147], loss=90.9052
	step [131/147], loss=74.2730
	step [132/147], loss=79.9715
	step [133/147], loss=79.1602
	step [134/147], loss=73.4052
	step [135/147], loss=71.4839
	step [136/147], loss=92.2481
	step [137/147], loss=91.4912
	step [138/147], loss=85.1275
	step [139/147], loss=81.6778
	step [140/147], loss=88.9522
	step [141/147], loss=85.9365
	step [142/147], loss=71.1292
	step [143/147], loss=75.1450
	step [144/147], loss=97.8966
	step [145/147], loss=96.0543
	step [146/147], loss=82.3575
	step [147/147], loss=16.7478
	Evaluating
	loss=0.0087, precision=0.4193, recall=0.8595, f1=0.5636
Training epoch 92
	step [1/147], loss=77.9425
	step [2/147], loss=84.4970
	step [3/147], loss=110.4349
	step [4/147], loss=80.2780
	step [5/147], loss=84.3341
	step [6/147], loss=79.6164
	step [7/147], loss=88.0549
	step [8/147], loss=84.3911
	step [9/147], loss=86.0296
	step [10/147], loss=88.3960
	step [11/147], loss=83.9476
	step [12/147], loss=91.9892
	step [13/147], loss=79.3743
	step [14/147], loss=98.7870
	step [15/147], loss=88.4405
	step [16/147], loss=69.7846
	step [17/147], loss=98.8464
	step [18/147], loss=78.5623
	step [19/147], loss=64.8384
	step [20/147], loss=74.3920
	step [21/147], loss=69.3255
	step [22/147], loss=76.5361
	step [23/147], loss=94.8268
	step [24/147], loss=84.9976
	step [25/147], loss=90.9469
	step [26/147], loss=85.2674
	step [27/147], loss=67.9452
	step [28/147], loss=78.6627
	step [29/147], loss=75.6123
	step [30/147], loss=73.4286
	step [31/147], loss=68.5218
	step [32/147], loss=81.9212
	step [33/147], loss=68.7169
	step [34/147], loss=95.6052
	step [35/147], loss=78.4495
	step [36/147], loss=99.7919
	step [37/147], loss=82.5483
	step [38/147], loss=73.2806
	step [39/147], loss=90.7712
	step [40/147], loss=90.3238
	step [41/147], loss=85.0761
	step [42/147], loss=65.3991
	step [43/147], loss=77.2097
	step [44/147], loss=90.6243
	step [45/147], loss=70.0191
	step [46/147], loss=82.8542
	step [47/147], loss=77.0273
	step [48/147], loss=93.4912
	step [49/147], loss=82.4923
	step [50/147], loss=79.6444
	step [51/147], loss=83.7774
	step [52/147], loss=76.6129
	step [53/147], loss=102.3103
	step [54/147], loss=92.0665
	step [55/147], loss=87.0259
	step [56/147], loss=86.3391
	step [57/147], loss=78.8165
	step [58/147], loss=89.8039
	step [59/147], loss=81.4352
	step [60/147], loss=77.6986
	step [61/147], loss=81.7471
	step [62/147], loss=86.4526
	step [63/147], loss=69.1367
	step [64/147], loss=84.0857
	step [65/147], loss=95.3387
	step [66/147], loss=89.9722
	step [67/147], loss=79.9570
	step [68/147], loss=91.3408
	step [69/147], loss=76.8743
	step [70/147], loss=90.0001
	step [71/147], loss=74.5174
	step [72/147], loss=80.4757
	step [73/147], loss=86.2671
	step [74/147], loss=84.1164
	step [75/147], loss=90.4849
	step [76/147], loss=78.7037
	step [77/147], loss=91.8846
	step [78/147], loss=76.5737
	step [79/147], loss=86.9322
	step [80/147], loss=85.1493
	step [81/147], loss=63.6523
	step [82/147], loss=79.0191
	step [83/147], loss=68.3514
	step [84/147], loss=93.0104
	step [85/147], loss=80.8593
	step [86/147], loss=83.8715
	step [87/147], loss=83.7759
	step [88/147], loss=75.1059
	step [89/147], loss=93.7071
	step [90/147], loss=73.6445
	step [91/147], loss=70.8637
	step [92/147], loss=81.7215
	step [93/147], loss=102.2391
	step [94/147], loss=71.9019
	step [95/147], loss=94.6232
	step [96/147], loss=69.6482
	step [97/147], loss=82.7969
	step [98/147], loss=88.7554
	step [99/147], loss=81.4587
	step [100/147], loss=69.3239
	step [101/147], loss=71.7660
	step [102/147], loss=71.0512
	step [103/147], loss=74.7928
	step [104/147], loss=74.7959
	step [105/147], loss=63.2606
	step [106/147], loss=79.2486
	step [107/147], loss=87.3885
	step [108/147], loss=87.0118
	step [109/147], loss=95.7962
	step [110/147], loss=88.1610
	step [111/147], loss=96.2672
	step [112/147], loss=85.6754
	step [113/147], loss=83.6092
	step [114/147], loss=86.4027
	step [115/147], loss=64.3468
	step [116/147], loss=78.5194
	step [117/147], loss=76.7682
	step [118/147], loss=91.1101
	step [119/147], loss=87.1689
	step [120/147], loss=83.0281
	step [121/147], loss=78.4140
	step [122/147], loss=75.7883
	step [123/147], loss=84.7082
	step [124/147], loss=90.3685
	step [125/147], loss=76.8237
	step [126/147], loss=87.6345
	step [127/147], loss=73.9596
	step [128/147], loss=80.2995
	step [129/147], loss=77.9130
	step [130/147], loss=86.7906
	step [131/147], loss=76.0286
	step [132/147], loss=81.1222
	step [133/147], loss=90.1167
	step [134/147], loss=84.9896
	step [135/147], loss=90.6159
	step [136/147], loss=80.8245
	step [137/147], loss=78.4650
	step [138/147], loss=77.9294
	step [139/147], loss=95.6572
	step [140/147], loss=101.4132
	step [141/147], loss=72.1507
	step [142/147], loss=97.0100
	step [143/147], loss=86.2729
	step [144/147], loss=88.9380
	step [145/147], loss=87.6730
	step [146/147], loss=83.8829
	step [147/147], loss=21.8963
	Evaluating
	loss=0.0109, precision=0.3409, recall=0.8593, f1=0.4881
Training epoch 93
	step [1/147], loss=87.1536
	step [2/147], loss=71.7413
	step [3/147], loss=83.7516
	step [4/147], loss=85.4153
	step [5/147], loss=86.2111
	step [6/147], loss=76.0205
	step [7/147], loss=87.4633
	step [8/147], loss=85.7770
	step [9/147], loss=84.5690
	step [10/147], loss=80.6023
	step [11/147], loss=80.3823
	step [12/147], loss=81.4079
	step [13/147], loss=85.7811
	step [14/147], loss=95.6141
	step [15/147], loss=75.3143
	step [16/147], loss=80.2211
	step [17/147], loss=74.1878
	step [18/147], loss=72.6286
	step [19/147], loss=82.8601
	step [20/147], loss=90.4008
	step [21/147], loss=89.2096
	step [22/147], loss=79.1567
	step [23/147], loss=89.6867
	step [24/147], loss=86.7531
	step [25/147], loss=82.7294
	step [26/147], loss=96.6758
	step [27/147], loss=87.9221
	step [28/147], loss=72.4837
	step [29/147], loss=78.9870
	step [30/147], loss=77.5456
	step [31/147], loss=96.9004
	step [32/147], loss=79.1161
	step [33/147], loss=90.4628
	step [34/147], loss=82.1254
	step [35/147], loss=79.0642
	step [36/147], loss=90.6892
	step [37/147], loss=100.6138
	step [38/147], loss=80.2467
	step [39/147], loss=89.1885
	step [40/147], loss=81.0977
	step [41/147], loss=89.0476
	step [42/147], loss=81.6826
	step [43/147], loss=83.8559
	step [44/147], loss=87.9248
	step [45/147], loss=59.7411
	step [46/147], loss=71.3112
	step [47/147], loss=105.0130
	step [48/147], loss=72.5071
	step [49/147], loss=91.7799
	step [50/147], loss=83.1772
	step [51/147], loss=75.9929
	step [52/147], loss=95.5540
	step [53/147], loss=94.2633
	step [54/147], loss=78.8041
	step [55/147], loss=75.4893
	step [56/147], loss=86.7859
	step [57/147], loss=84.8505
	step [58/147], loss=78.7926
	step [59/147], loss=74.1992
	step [60/147], loss=77.4663
	step [61/147], loss=82.0239
	step [62/147], loss=87.5379
	step [63/147], loss=79.1125
	step [64/147], loss=95.0828
	step [65/147], loss=91.4155
	step [66/147], loss=78.5807
	step [67/147], loss=82.1607
	step [68/147], loss=72.3665
	step [69/147], loss=96.8273
	step [70/147], loss=86.6141
	step [71/147], loss=89.3607
	step [72/147], loss=91.8087
	step [73/147], loss=88.4970
	step [74/147], loss=83.9836
	step [75/147], loss=93.7431
	step [76/147], loss=70.0868
	step [77/147], loss=81.4621
	step [78/147], loss=69.8314
	step [79/147], loss=77.8913
	step [80/147], loss=95.5669
	step [81/147], loss=72.9656
	step [82/147], loss=73.1076
	step [83/147], loss=82.1389
	step [84/147], loss=90.6669
	step [85/147], loss=75.0901
	step [86/147], loss=69.6407
	step [87/147], loss=81.5467
	step [88/147], loss=94.6549
	step [89/147], loss=84.4217
	step [90/147], loss=85.0020
	step [91/147], loss=94.0660
	step [92/147], loss=95.9550
	step [93/147], loss=87.5020
	step [94/147], loss=91.2024
	step [95/147], loss=89.8727
	step [96/147], loss=85.2183
	step [97/147], loss=82.1225
	step [98/147], loss=88.1123
	step [99/147], loss=68.2058
	step [100/147], loss=77.8234
	step [101/147], loss=83.4616
	step [102/147], loss=70.6590
	step [103/147], loss=90.9615
	step [104/147], loss=81.2074
	step [105/147], loss=90.7490
	step [106/147], loss=79.0947
	step [107/147], loss=78.3750
	step [108/147], loss=80.4702
	step [109/147], loss=88.0047
	step [110/147], loss=70.0673
	step [111/147], loss=79.6334
	step [112/147], loss=77.0377
	step [113/147], loss=93.0237
	step [114/147], loss=72.0367
	step [115/147], loss=99.9716
	step [116/147], loss=62.6890
	step [117/147], loss=86.4518
	step [118/147], loss=83.0243
	step [119/147], loss=70.4035
	step [120/147], loss=79.0282
	step [121/147], loss=80.1650
	step [122/147], loss=90.0440
	step [123/147], loss=96.4700
	step [124/147], loss=100.7784
	step [125/147], loss=79.8497
	step [126/147], loss=79.3055
	step [127/147], loss=77.1570
	step [128/147], loss=87.0999
	step [129/147], loss=77.0788
	step [130/147], loss=77.4555
	step [131/147], loss=79.2629
	step [132/147], loss=91.9462
	step [133/147], loss=87.1566
	step [134/147], loss=79.4079
	step [135/147], loss=65.7163
	step [136/147], loss=82.8559
	step [137/147], loss=64.2756
	step [138/147], loss=76.9795
	step [139/147], loss=67.5239
	step [140/147], loss=76.7661
	step [141/147], loss=86.8609
	step [142/147], loss=78.0159
	step [143/147], loss=85.8051
	step [144/147], loss=79.7507
	step [145/147], loss=76.8907
	step [146/147], loss=88.4909
	step [147/147], loss=32.5279
	Evaluating
	loss=0.0076, precision=0.4595, recall=0.8298, f1=0.5915
saving model as: 3_saved_model.pth
Training epoch 94
	step [1/147], loss=83.5236
	step [2/147], loss=81.6271
	step [3/147], loss=85.0591
	step [4/147], loss=81.8566
	step [5/147], loss=56.5214
	step [6/147], loss=91.1368
	step [7/147], loss=89.4519
	step [8/147], loss=94.7758
	step [9/147], loss=67.7898
	step [10/147], loss=104.1105
	step [11/147], loss=81.3588
	step [12/147], loss=70.2172
	step [13/147], loss=89.6761
	step [14/147], loss=84.2338
	step [15/147], loss=83.0944
	step [16/147], loss=84.6984
	step [17/147], loss=80.5033
	step [18/147], loss=82.4333
	step [19/147], loss=70.1393
	step [20/147], loss=103.3640
	step [21/147], loss=82.9436
	step [22/147], loss=107.5225
	step [23/147], loss=93.6256
	step [24/147], loss=92.3511
	step [25/147], loss=63.9728
	step [26/147], loss=81.6503
	step [27/147], loss=68.4446
	step [28/147], loss=75.6519
	step [29/147], loss=76.3038
	step [30/147], loss=76.3991
	step [31/147], loss=78.7027
	step [32/147], loss=75.9823
	step [33/147], loss=83.9762
	step [34/147], loss=81.8698
	step [35/147], loss=66.8447
	step [36/147], loss=89.2592
	step [37/147], loss=84.9947
	step [38/147], loss=93.0100
	step [39/147], loss=82.0357
	step [40/147], loss=83.3666
	step [41/147], loss=98.5816
	step [42/147], loss=84.7037
	step [43/147], loss=78.3433
	step [44/147], loss=79.0316
	step [45/147], loss=80.7718
	step [46/147], loss=88.2274
	step [47/147], loss=92.6660
	step [48/147], loss=86.6213
	step [49/147], loss=87.5805
	step [50/147], loss=89.4560
	step [51/147], loss=77.0940
	step [52/147], loss=81.5026
	step [53/147], loss=74.3193
	step [54/147], loss=88.2679
	step [55/147], loss=75.8462
	step [56/147], loss=69.2523
	step [57/147], loss=84.9710
	step [58/147], loss=95.3823
	step [59/147], loss=87.2824
	step [60/147], loss=88.6682
	step [61/147], loss=79.9886
	step [62/147], loss=75.7519
	step [63/147], loss=77.5881
	step [64/147], loss=78.4134
	step [65/147], loss=89.0698
	step [66/147], loss=78.7397
	step [67/147], loss=96.1340
	step [68/147], loss=88.7754
	step [69/147], loss=75.5727
	step [70/147], loss=76.5100
	step [71/147], loss=71.2487
	step [72/147], loss=75.8682
	step [73/147], loss=78.5554
	step [74/147], loss=71.1520
	step [75/147], loss=81.0989
	step [76/147], loss=88.3860
	step [77/147], loss=78.2553
	step [78/147], loss=73.7283
	step [79/147], loss=97.4402
	step [80/147], loss=77.2004
	step [81/147], loss=90.7840
	step [82/147], loss=83.1332
	step [83/147], loss=74.6698
	step [84/147], loss=86.3843
	step [85/147], loss=73.2084
	step [86/147], loss=82.0349
	step [87/147], loss=90.0636
	step [88/147], loss=85.2956
	step [89/147], loss=75.0890
	step [90/147], loss=92.3825
	step [91/147], loss=82.5254
	step [92/147], loss=88.0869
	step [93/147], loss=73.7177
	step [94/147], loss=77.5718
	step [95/147], loss=76.2131
	step [96/147], loss=71.7642
	step [97/147], loss=84.6343
	step [98/147], loss=97.8300
	step [99/147], loss=79.1220
	step [100/147], loss=80.5553
	step [101/147], loss=80.3254
	step [102/147], loss=69.0287
	step [103/147], loss=76.6076
	step [104/147], loss=75.1589
	step [105/147], loss=102.5430
	step [106/147], loss=76.7061
	step [107/147], loss=74.9277
	step [108/147], loss=87.7976
	step [109/147], loss=74.7980
	step [110/147], loss=79.3452
	step [111/147], loss=92.9000
	step [112/147], loss=66.4310
	step [113/147], loss=86.0976
	step [114/147], loss=85.8928
	step [115/147], loss=70.3253
	step [116/147], loss=89.8552
	step [117/147], loss=86.2857
	step [118/147], loss=91.3793
	step [119/147], loss=89.0252
	step [120/147], loss=85.2146
	step [121/147], loss=91.2216
	step [122/147], loss=85.8819
	step [123/147], loss=90.5662
	step [124/147], loss=78.1861
	step [125/147], loss=84.3641
	step [126/147], loss=97.0183
	step [127/147], loss=88.3617
	step [128/147], loss=83.8126
	step [129/147], loss=80.0415
	step [130/147], loss=83.0483
	step [131/147], loss=68.7287
	step [132/147], loss=77.9526
	step [133/147], loss=93.9858
	step [134/147], loss=91.5249
	step [135/147], loss=71.9653
	step [136/147], loss=93.5244
	step [137/147], loss=68.1646
	step [138/147], loss=99.7208
	step [139/147], loss=70.4089
	step [140/147], loss=78.4754
	step [141/147], loss=84.0600
	step [142/147], loss=71.2152
	step [143/147], loss=88.0377
	step [144/147], loss=87.1966
	step [145/147], loss=67.7693
	step [146/147], loss=76.5030
	step [147/147], loss=20.3340
	Evaluating
	loss=0.0076, precision=0.4536, recall=0.8336, f1=0.5875
Training epoch 95
	step [1/147], loss=98.4947
	step [2/147], loss=85.3798
	step [3/147], loss=80.1067
	step [4/147], loss=75.6876
	step [5/147], loss=85.7590
	step [6/147], loss=78.0889
	step [7/147], loss=87.9104
	step [8/147], loss=82.8939
	step [9/147], loss=75.0302
	step [10/147], loss=88.9844
	step [11/147], loss=67.9976
	step [12/147], loss=69.2785
	step [13/147], loss=80.3124
	step [14/147], loss=87.5255
	step [15/147], loss=78.1179
	step [16/147], loss=73.5535
	step [17/147], loss=85.8138
	step [18/147], loss=84.9472
	step [19/147], loss=75.8051
	step [20/147], loss=78.1546
	step [21/147], loss=80.3489
	step [22/147], loss=75.7307
	step [23/147], loss=71.7745
	step [24/147], loss=80.0828
	step [25/147], loss=90.4959
	step [26/147], loss=76.5140
	step [27/147], loss=92.0080
	step [28/147], loss=64.6704
	step [29/147], loss=85.3297
	step [30/147], loss=110.5441
	step [31/147], loss=89.5387
	step [32/147], loss=84.4157
	step [33/147], loss=91.2897
	step [34/147], loss=97.5106
	step [35/147], loss=79.2536
	step [36/147], loss=85.4980
	step [37/147], loss=76.1249
	step [38/147], loss=80.1651
	step [39/147], loss=88.9292
	step [40/147], loss=72.7886
	step [41/147], loss=74.5784
	step [42/147], loss=92.7742
	step [43/147], loss=82.6656
	step [44/147], loss=91.1776
	step [45/147], loss=82.6838
	step [46/147], loss=73.7849
	step [47/147], loss=79.5030
	step [48/147], loss=73.5687
	step [49/147], loss=73.4487
	step [50/147], loss=85.3145
	step [51/147], loss=82.9176
	step [52/147], loss=90.7260
	step [53/147], loss=74.4808
	step [54/147], loss=86.2852
	step [55/147], loss=80.7168
	step [56/147], loss=77.2406
	step [57/147], loss=89.2909
	step [58/147], loss=81.5563
	step [59/147], loss=78.8144
	step [60/147], loss=82.2644
	step [61/147], loss=81.0544
	step [62/147], loss=77.0734
	step [63/147], loss=80.4336
	step [64/147], loss=81.1140
	step [65/147], loss=79.1395
	step [66/147], loss=83.7947
	step [67/147], loss=85.4663
	step [68/147], loss=78.4353
	step [69/147], loss=68.1025
	step [70/147], loss=84.5299
	step [71/147], loss=80.3104
	step [72/147], loss=70.8661
	step [73/147], loss=76.1925
	step [74/147], loss=89.1475
	step [75/147], loss=85.5633
	step [76/147], loss=73.6993
	step [77/147], loss=94.5050
	step [78/147], loss=77.3809
	step [79/147], loss=83.6064
	step [80/147], loss=98.3788
	step [81/147], loss=74.4806
	step [82/147], loss=100.8425
	step [83/147], loss=77.7842
	step [84/147], loss=86.1073
	step [85/147], loss=80.7471
	step [86/147], loss=68.5492
	step [87/147], loss=79.0892
	step [88/147], loss=87.7736
	step [89/147], loss=83.0897
	step [90/147], loss=71.4032
	step [91/147], loss=77.3156
	step [92/147], loss=83.5792
	step [93/147], loss=81.5303
	step [94/147], loss=82.1028
	step [95/147], loss=91.4601
	step [96/147], loss=84.6246
	step [97/147], loss=77.0969
	step [98/147], loss=84.5710
	step [99/147], loss=91.7010
	step [100/147], loss=75.9003
	step [101/147], loss=73.8618
	step [102/147], loss=92.0826
	step [103/147], loss=101.7378
	step [104/147], loss=75.3318
	step [105/147], loss=81.9973
	step [106/147], loss=78.5132
	step [107/147], loss=77.5901
	step [108/147], loss=84.1260
	step [109/147], loss=85.4822
	step [110/147], loss=101.1215
	step [111/147], loss=84.6997
	step [112/147], loss=67.3887
	step [113/147], loss=74.5986
	step [114/147], loss=96.9248
	step [115/147], loss=74.9282
	step [116/147], loss=106.0882
	step [117/147], loss=88.9802
	step [118/147], loss=76.0219
	step [119/147], loss=79.9762
	step [120/147], loss=93.9940
	step [121/147], loss=83.6804
	step [122/147], loss=99.0753
	step [123/147], loss=75.2758
	step [124/147], loss=97.1389
	step [125/147], loss=80.7244
	step [126/147], loss=82.4474
	step [127/147], loss=75.4899
	step [128/147], loss=84.8601
	step [129/147], loss=89.6778
	step [130/147], loss=88.0513
	step [131/147], loss=79.7107
	step [132/147], loss=81.2439
	step [133/147], loss=82.5699
	step [134/147], loss=87.1449
	step [135/147], loss=92.5336
	step [136/147], loss=82.4924
	step [137/147], loss=95.7309
	step [138/147], loss=74.8757
	step [139/147], loss=92.4934
	step [140/147], loss=81.7068
	step [141/147], loss=69.0045
	step [142/147], loss=84.7197
	step [143/147], loss=78.3241
	step [144/147], loss=77.5349
	step [145/147], loss=82.6063
	step [146/147], loss=65.9311
	step [147/147], loss=19.2379
	Evaluating
	loss=0.0076, precision=0.4597, recall=0.8329, f1=0.5924
saving model as: 3_saved_model.pth
Training epoch 96
	step [1/147], loss=84.6863
	step [2/147], loss=86.5033
	step [3/147], loss=72.3351
	step [4/147], loss=92.6426
	step [5/147], loss=80.1875
	step [6/147], loss=76.1378
	step [7/147], loss=72.8962
	step [8/147], loss=88.2941
	step [9/147], loss=86.6126
	step [10/147], loss=89.2442
	step [11/147], loss=82.9010
	step [12/147], loss=80.3381
	step [13/147], loss=83.4734
	step [14/147], loss=90.2997
	step [15/147], loss=69.3906
	step [16/147], loss=84.9199
	step [17/147], loss=72.5876
	step [18/147], loss=96.7978
	step [19/147], loss=91.7978
	step [20/147], loss=74.7678
	step [21/147], loss=89.6502
	step [22/147], loss=79.0349
	step [23/147], loss=70.5887
	step [24/147], loss=72.4842
	step [25/147], loss=94.2492
	step [26/147], loss=85.2394
	step [27/147], loss=70.0749
	step [28/147], loss=68.0906
	step [29/147], loss=95.8235
	step [30/147], loss=86.0766
	step [31/147], loss=92.9541
	step [32/147], loss=76.9715
	step [33/147], loss=68.0955
	step [34/147], loss=77.2178
	step [35/147], loss=82.1918
	step [36/147], loss=85.5524
	step [37/147], loss=77.1244
	step [38/147], loss=69.4098
	step [39/147], loss=84.0219
	step [40/147], loss=68.3245
	step [41/147], loss=92.1977
	step [42/147], loss=80.8320
	step [43/147], loss=89.3721
	step [44/147], loss=61.5022
	step [45/147], loss=80.8285
	step [46/147], loss=79.9706
	step [47/147], loss=75.1602
	step [48/147], loss=84.1846
	step [49/147], loss=70.9071
	step [50/147], loss=88.3921
	step [51/147], loss=87.9453
	step [52/147], loss=69.5371
	step [53/147], loss=76.0574
	step [54/147], loss=87.4667
	step [55/147], loss=76.9665
	step [56/147], loss=74.3324
	step [57/147], loss=66.1031
	step [58/147], loss=91.6228
	step [59/147], loss=90.0169
	step [60/147], loss=97.1579
	step [61/147], loss=78.3829
	step [62/147], loss=85.4290
	step [63/147], loss=83.8139
	step [64/147], loss=82.0051
	step [65/147], loss=76.4043
	step [66/147], loss=88.1727
	step [67/147], loss=77.7593
	step [68/147], loss=90.0550
	step [69/147], loss=84.9183
	step [70/147], loss=88.6985
	step [71/147], loss=80.4008
	step [72/147], loss=84.1056
	step [73/147], loss=73.5646
	step [74/147], loss=74.4697
	step [75/147], loss=91.9959
	step [76/147], loss=77.6976
	step [77/147], loss=83.6428
	step [78/147], loss=83.1201
	step [79/147], loss=77.8963
	step [80/147], loss=88.3302
	step [81/147], loss=83.3829
	step [82/147], loss=71.0647
	step [83/147], loss=90.7642
	step [84/147], loss=80.3637
	step [85/147], loss=74.0825
	step [86/147], loss=75.0501
	step [87/147], loss=99.9317
	step [88/147], loss=76.8914
	step [89/147], loss=78.1846
	step [90/147], loss=92.0291
	step [91/147], loss=77.4243
	step [92/147], loss=82.0480
	step [93/147], loss=82.8608
	step [94/147], loss=74.7470
	step [95/147], loss=78.0562
	step [96/147], loss=90.1988
	step [97/147], loss=72.6602
	step [98/147], loss=87.7005
	step [99/147], loss=96.6460
	step [100/147], loss=87.3164
	step [101/147], loss=84.0834
	step [102/147], loss=90.9601
	step [103/147], loss=67.0088
	step [104/147], loss=70.5280
	step [105/147], loss=77.9001
	step [106/147], loss=78.1731
	step [107/147], loss=89.1110
	step [108/147], loss=101.6766
	step [109/147], loss=75.5055
	step [110/147], loss=83.0074
	step [111/147], loss=64.9040
	step [112/147], loss=95.6271
	step [113/147], loss=59.2806
	step [114/147], loss=75.1739
	step [115/147], loss=103.1459
	step [116/147], loss=70.0442
	step [117/147], loss=86.2939
	step [118/147], loss=92.0545
	step [119/147], loss=76.3406
	step [120/147], loss=91.3576
	step [121/147], loss=72.2349
	step [122/147], loss=82.4454
	step [123/147], loss=98.2555
	step [124/147], loss=70.7315
	step [125/147], loss=74.0851
	step [126/147], loss=79.9874
	step [127/147], loss=70.5456
	step [128/147], loss=83.3839
	step [129/147], loss=87.2406
	step [130/147], loss=77.7542
	step [131/147], loss=82.4144
	step [132/147], loss=83.0523
	step [133/147], loss=94.2627
	step [134/147], loss=91.3382
	step [135/147], loss=87.2503
	step [136/147], loss=93.4356
	step [137/147], loss=82.8548
	step [138/147], loss=90.0482
	step [139/147], loss=76.4146
	step [140/147], loss=74.3600
	step [141/147], loss=89.9980
	step [142/147], loss=77.8435
	step [143/147], loss=93.3300
	step [144/147], loss=75.3149
	step [145/147], loss=84.0607
	step [146/147], loss=71.5510
	step [147/147], loss=23.2613
	Evaluating
	loss=0.0075, precision=0.4606, recall=0.8235, f1=0.5908
Training epoch 97
	step [1/147], loss=80.2234
	step [2/147], loss=98.6200
	step [3/147], loss=83.9592
	step [4/147], loss=83.6525
	step [5/147], loss=88.6217
	step [6/147], loss=86.3511
	step [7/147], loss=75.0812
	step [8/147], loss=97.3146
	step [9/147], loss=82.8278
	step [10/147], loss=69.2016
	step [11/147], loss=99.9069
	step [12/147], loss=80.7717
	step [13/147], loss=85.2357
	step [14/147], loss=74.7840
	step [15/147], loss=86.1123
	step [16/147], loss=68.0176
	step [17/147], loss=99.2580
	step [18/147], loss=99.4813
	step [19/147], loss=90.9901
	step [20/147], loss=81.3491
	step [21/147], loss=71.8247
	step [22/147], loss=89.2278
	step [23/147], loss=83.6221
	step [24/147], loss=85.6710
	step [25/147], loss=94.2364
	step [26/147], loss=99.9183
	step [27/147], loss=82.4616
	step [28/147], loss=79.2306
	step [29/147], loss=83.1273
	step [30/147], loss=65.6395
	step [31/147], loss=74.5855
	step [32/147], loss=73.8304
	step [33/147], loss=70.4094
	step [34/147], loss=78.1155
	step [35/147], loss=79.7510
	step [36/147], loss=77.2655
	step [37/147], loss=68.1976
	step [38/147], loss=76.0516
	step [39/147], loss=72.2476
	step [40/147], loss=70.4557
	step [41/147], loss=68.4174
	step [42/147], loss=74.1496
	step [43/147], loss=73.9224
	step [44/147], loss=86.6073
	step [45/147], loss=74.0809
	step [46/147], loss=90.8862
	step [47/147], loss=86.2935
	step [48/147], loss=93.3210
	step [49/147], loss=86.9056
	step [50/147], loss=79.1961
	step [51/147], loss=99.9437
	step [52/147], loss=79.3616
	step [53/147], loss=85.6426
	step [54/147], loss=82.2779
	step [55/147], loss=84.7592
	step [56/147], loss=73.4739
	step [57/147], loss=76.3635
	step [58/147], loss=78.7241
	step [59/147], loss=93.0383
	step [60/147], loss=81.9747
	step [61/147], loss=83.5248
	step [62/147], loss=76.3499
	step [63/147], loss=87.7468
	step [64/147], loss=92.6792
	step [65/147], loss=72.6232
	step [66/147], loss=86.0186
	step [67/147], loss=65.7788
	step [68/147], loss=81.4234
	step [69/147], loss=77.6003
	step [70/147], loss=78.0977
	step [71/147], loss=78.2036
	step [72/147], loss=101.9624
	step [73/147], loss=88.9431
	step [74/147], loss=70.6066
	step [75/147], loss=78.8002
	step [76/147], loss=82.6468
	step [77/147], loss=88.8021
	step [78/147], loss=73.3866
	step [79/147], loss=80.1831
	step [80/147], loss=94.4081
	step [81/147], loss=88.0778
	step [82/147], loss=82.5667
	step [83/147], loss=94.1754
	step [84/147], loss=80.8886
	step [85/147], loss=82.7017
	step [86/147], loss=69.2249
	step [87/147], loss=70.9445
	step [88/147], loss=76.3536
	step [89/147], loss=69.6980
	step [90/147], loss=78.6351
	step [91/147], loss=88.6084
	step [92/147], loss=81.1751
	step [93/147], loss=100.4604
	step [94/147], loss=65.8254
	step [95/147], loss=77.2901
	step [96/147], loss=77.2359
	step [97/147], loss=61.5655
	step [98/147], loss=79.6804
	step [99/147], loss=75.0063
	step [100/147], loss=62.6018
	step [101/147], loss=71.3000
	step [102/147], loss=71.0962
	step [103/147], loss=84.1137
	step [104/147], loss=77.4799
	step [105/147], loss=79.1982
	step [106/147], loss=75.1489
	step [107/147], loss=77.9013
	step [108/147], loss=96.5005
	step [109/147], loss=78.9155
	step [110/147], loss=76.8762
	step [111/147], loss=77.9585
	step [112/147], loss=80.1090
	step [113/147], loss=96.5852
	step [114/147], loss=72.6600
	step [115/147], loss=71.8449
	step [116/147], loss=101.7389
	step [117/147], loss=73.5506
	step [118/147], loss=69.3981
	step [119/147], loss=89.7645
	step [120/147], loss=103.6849
	step [121/147], loss=85.0504
	step [122/147], loss=71.3846
	step [123/147], loss=80.2232
	step [124/147], loss=83.1370
	step [125/147], loss=87.1589
	step [126/147], loss=84.2921
	step [127/147], loss=75.4793
	step [128/147], loss=81.5514
	step [129/147], loss=92.4837
	step [130/147], loss=96.1535
	step [131/147], loss=71.0517
	step [132/147], loss=80.6689
	step [133/147], loss=75.2458
	step [134/147], loss=90.3572
	step [135/147], loss=73.1902
	step [136/147], loss=74.9614
	step [137/147], loss=80.5328
	step [138/147], loss=90.4337
	step [139/147], loss=72.7188
	step [140/147], loss=98.9692
	step [141/147], loss=74.8478
	step [142/147], loss=94.3532
	step [143/147], loss=66.6736
	step [144/147], loss=84.9180
	step [145/147], loss=74.0939
	step [146/147], loss=75.7969
	step [147/147], loss=26.2773
	Evaluating
	loss=0.0082, precision=0.4279, recall=0.8368, f1=0.5663
Training epoch 98
	step [1/147], loss=80.3530
	step [2/147], loss=82.8407
	step [3/147], loss=61.8368
	step [4/147], loss=75.0535
	step [5/147], loss=87.5081
	step [6/147], loss=61.1673
	step [7/147], loss=84.2249
	step [8/147], loss=78.0475
	step [9/147], loss=101.5220
	step [10/147], loss=71.6619
	step [11/147], loss=81.4565
	step [12/147], loss=72.2152
	step [13/147], loss=74.0158
	step [14/147], loss=78.5976
	step [15/147], loss=81.3508
	step [16/147], loss=84.2488
	step [17/147], loss=69.6725
	step [18/147], loss=83.8676
	step [19/147], loss=69.2533
	step [20/147], loss=95.2598
	step [21/147], loss=87.5261
	step [22/147], loss=73.2346
	step [23/147], loss=77.9906
	step [24/147], loss=69.3997
	step [25/147], loss=64.7641
	step [26/147], loss=79.6534
	step [27/147], loss=87.6338
	step [28/147], loss=90.9868
	step [29/147], loss=78.7421
	step [30/147], loss=67.9700
	step [31/147], loss=83.6848
	step [32/147], loss=77.5861
	step [33/147], loss=70.3569
	step [34/147], loss=83.0359
	step [35/147], loss=71.3850
	step [36/147], loss=82.5987
	step [37/147], loss=76.9014
	step [38/147], loss=91.3558
	step [39/147], loss=76.8832
	step [40/147], loss=97.5297
	step [41/147], loss=70.5282
	step [42/147], loss=71.3328
	step [43/147], loss=79.1663
	step [44/147], loss=89.8239
	step [45/147], loss=89.9264
	step [46/147], loss=92.3200
	step [47/147], loss=81.1339
	step [48/147], loss=107.0884
	step [49/147], loss=87.3150
	step [50/147], loss=77.9453
	step [51/147], loss=75.2280
	step [52/147], loss=88.4358
	step [53/147], loss=78.8606
	step [54/147], loss=76.2907
	step [55/147], loss=88.7132
	step [56/147], loss=85.9822
	step [57/147], loss=87.5317
	step [58/147], loss=82.8823
	step [59/147], loss=83.9673
	step [60/147], loss=88.1111
	step [61/147], loss=78.3992
	step [62/147], loss=76.9953
	step [63/147], loss=82.5055
	step [64/147], loss=80.9513
	step [65/147], loss=80.1777
	step [66/147], loss=86.5877
	step [67/147], loss=77.9518
	step [68/147], loss=78.3570
	step [69/147], loss=81.9276
	step [70/147], loss=88.8012
	step [71/147], loss=84.8553
	step [72/147], loss=83.1729
	step [73/147], loss=73.9337
	step [74/147], loss=69.4411
	step [75/147], loss=64.0957
	step [76/147], loss=84.3660
	step [77/147], loss=77.2225
	step [78/147], loss=79.0134
	step [79/147], loss=93.1702
	step [80/147], loss=104.5144
	step [81/147], loss=80.8447
	step [82/147], loss=71.0621
	step [83/147], loss=70.5800
	step [84/147], loss=80.2410
	step [85/147], loss=89.0817
	step [86/147], loss=78.8950
	step [87/147], loss=78.6447
	step [88/147], loss=74.4749
	step [89/147], loss=84.0892
	step [90/147], loss=88.0905
	step [91/147], loss=74.6724
	step [92/147], loss=86.5124
	step [93/147], loss=87.9406
	step [94/147], loss=86.0663
	step [95/147], loss=63.0022
	step [96/147], loss=84.4544
	step [97/147], loss=76.0120
	step [98/147], loss=83.3908
	step [99/147], loss=84.5977
	step [100/147], loss=81.1933
	step [101/147], loss=73.2759
	step [102/147], loss=92.7771
	step [103/147], loss=81.7056
	step [104/147], loss=83.2130
	step [105/147], loss=94.9074
	step [106/147], loss=86.1500
	step [107/147], loss=69.6802
	step [108/147], loss=76.5019
	step [109/147], loss=81.9172
	step [110/147], loss=72.3662
	step [111/147], loss=70.9559
	step [112/147], loss=77.1331
	step [113/147], loss=78.0482
	step [114/147], loss=74.2381
	step [115/147], loss=84.3482
	step [116/147], loss=76.2923
	step [117/147], loss=72.4413
	step [118/147], loss=93.8134
	step [119/147], loss=76.3774
	step [120/147], loss=78.4156
	step [121/147], loss=91.5768
	step [122/147], loss=87.4662
	step [123/147], loss=87.2070
	step [124/147], loss=84.0112
	step [125/147], loss=82.4464
	step [126/147], loss=78.2124
	step [127/147], loss=78.1839
	step [128/147], loss=74.9026
	step [129/147], loss=93.8507
	step [130/147], loss=79.6297
	step [131/147], loss=91.5257
	step [132/147], loss=93.7894
	step [133/147], loss=86.6484
	step [134/147], loss=86.1322
	step [135/147], loss=77.3898
	step [136/147], loss=79.7410
	step [137/147], loss=78.9988
	step [138/147], loss=93.5764
	step [139/147], loss=80.3517
	step [140/147], loss=92.7620
	step [141/147], loss=89.0818
	step [142/147], loss=92.4304
	step [143/147], loss=67.3575
	step [144/147], loss=97.5923
	step [145/147], loss=75.8061
	step [146/147], loss=73.9598
	step [147/147], loss=20.5053
	Evaluating
	loss=0.0088, precision=0.4060, recall=0.8211, f1=0.5434
Training epoch 99
	step [1/147], loss=94.6238
	step [2/147], loss=72.9400
	step [3/147], loss=103.3649
	step [4/147], loss=94.4015
	step [5/147], loss=79.9516
	step [6/147], loss=74.8524
	step [7/147], loss=79.5390
	step [8/147], loss=113.1761
	step [9/147], loss=84.1056
	step [10/147], loss=90.7509
	step [11/147], loss=76.6374
	step [12/147], loss=94.6695
	step [13/147], loss=87.4350
	step [14/147], loss=82.5306
	step [15/147], loss=85.2402
	step [16/147], loss=82.6454
	step [17/147], loss=74.2156
	step [18/147], loss=70.7247
	step [19/147], loss=89.8774
	step [20/147], loss=86.0941
	step [21/147], loss=80.3957
	step [22/147], loss=65.0202
	step [23/147], loss=77.2538
	step [24/147], loss=76.8128
	step [25/147], loss=70.5260
	step [26/147], loss=72.7040
	step [27/147], loss=81.7579
	step [28/147], loss=87.3641
	step [29/147], loss=82.7603
	step [30/147], loss=90.9409
	step [31/147], loss=73.8386
	step [32/147], loss=79.1334
	step [33/147], loss=88.0505
	step [34/147], loss=81.2123
	step [35/147], loss=78.8866
	step [36/147], loss=85.2608
	step [37/147], loss=82.0779
	step [38/147], loss=77.4202
	step [39/147], loss=78.6715
	step [40/147], loss=78.5686
	step [41/147], loss=69.6595
	step [42/147], loss=85.1535
	step [43/147], loss=78.1980
	step [44/147], loss=81.2032
	step [45/147], loss=84.7482
	step [46/147], loss=97.7550
	step [47/147], loss=86.6912
	step [48/147], loss=96.0588
	step [49/147], loss=77.6149
	step [50/147], loss=76.1919
	step [51/147], loss=74.3387
	step [52/147], loss=72.6720
	step [53/147], loss=85.2823
	step [54/147], loss=74.4634
	step [55/147], loss=81.6095
	step [56/147], loss=81.5307
	step [57/147], loss=113.1315
	step [58/147], loss=80.6687
	step [59/147], loss=90.3775
	step [60/147], loss=75.4915
	step [61/147], loss=79.2442
	step [62/147], loss=88.5387
	step [63/147], loss=78.1290
	step [64/147], loss=71.7326
	step [65/147], loss=72.3354
	step [66/147], loss=86.0457
	step [67/147], loss=91.5576
	step [68/147], loss=77.9732
	step [69/147], loss=71.6464
	step [70/147], loss=74.5252
	step [71/147], loss=90.3104
	step [72/147], loss=63.4517
	step [73/147], loss=72.3275
	step [74/147], loss=70.0358
	step [75/147], loss=84.8182
	step [76/147], loss=68.8819
	step [77/147], loss=72.6091
	step [78/147], loss=99.2044
	step [79/147], loss=88.3441
	step [80/147], loss=68.9370
	step [81/147], loss=75.7677
	step [82/147], loss=79.3001
	step [83/147], loss=97.0769
	step [84/147], loss=83.1559
	step [85/147], loss=72.9316
	step [86/147], loss=77.1888
	step [87/147], loss=82.9604
	step [88/147], loss=92.9874
	step [89/147], loss=79.2514
	step [90/147], loss=70.4666
	step [91/147], loss=84.8349
	step [92/147], loss=55.7745
	step [93/147], loss=81.3961
	step [94/147], loss=89.8250
	step [95/147], loss=93.0774
	step [96/147], loss=82.6679
	step [97/147], loss=76.8737
	step [98/147], loss=83.0382
	step [99/147], loss=69.5525
	step [100/147], loss=80.3888
	step [101/147], loss=90.5910
	step [102/147], loss=68.2460
	step [103/147], loss=75.5337
	step [104/147], loss=81.0456
	step [105/147], loss=92.5854
	step [106/147], loss=88.4710
	step [107/147], loss=90.6912
	step [108/147], loss=82.1884
	step [109/147], loss=69.6246
	step [110/147], loss=79.2404
	step [111/147], loss=79.6020
	step [112/147], loss=80.4896
	step [113/147], loss=68.6179
	step [114/147], loss=74.2897
	step [115/147], loss=77.0910
	step [116/147], loss=85.7090
	step [117/147], loss=78.8162
	step [118/147], loss=66.1283
	step [119/147], loss=69.7948
	step [120/147], loss=87.1561
	step [121/147], loss=82.7386
	step [122/147], loss=83.3207
	step [123/147], loss=80.8811
	step [124/147], loss=86.1050
	step [125/147], loss=74.9218
	step [126/147], loss=89.4780
	step [127/147], loss=93.6703
	step [128/147], loss=77.4736
	step [129/147], loss=79.1312
	step [130/147], loss=71.6347
	step [131/147], loss=84.1570
	step [132/147], loss=91.2044
	step [133/147], loss=76.4854
	step [134/147], loss=83.8826
	step [135/147], loss=83.6887
	step [136/147], loss=80.4821
	step [137/147], loss=83.2729
	step [138/147], loss=71.7201
	step [139/147], loss=77.6030
	step [140/147], loss=75.9603
	step [141/147], loss=80.4176
	step [142/147], loss=92.2852
	step [143/147], loss=88.1392
	step [144/147], loss=94.1892
	step [145/147], loss=80.9235
	step [146/147], loss=85.6287
	step [147/147], loss=20.0022
	Evaluating
	loss=0.0083, precision=0.4221, recall=0.8354, f1=0.5608
Training epoch 100
	step [1/147], loss=92.1158
	step [2/147], loss=68.8976
	step [3/147], loss=61.5309
	step [4/147], loss=82.7636
	step [5/147], loss=64.8772
	step [6/147], loss=81.1783
	step [7/147], loss=82.9350
	step [8/147], loss=70.0013
	step [9/147], loss=87.4036
	step [10/147], loss=80.8025
	step [11/147], loss=96.4255
	step [12/147], loss=73.3784
	step [13/147], loss=65.1447
	step [14/147], loss=88.1984
	step [15/147], loss=76.4879
	step [16/147], loss=75.1758
	step [17/147], loss=91.2726
	step [18/147], loss=71.1867
	step [19/147], loss=73.5577
	step [20/147], loss=72.1854
	step [21/147], loss=80.9538
	step [22/147], loss=81.1266
	step [23/147], loss=70.5050
	step [24/147], loss=88.3455
	step [25/147], loss=75.1868
	step [26/147], loss=92.0156
	step [27/147], loss=72.1911
	step [28/147], loss=96.0551
	step [29/147], loss=86.9440
	step [30/147], loss=85.6730
	step [31/147], loss=91.9736
	step [32/147], loss=76.4403
	step [33/147], loss=80.9623
	step [34/147], loss=81.5731
	step [35/147], loss=70.9169
	step [36/147], loss=72.7204
	step [37/147], loss=78.1127
	step [38/147], loss=79.7565
	step [39/147], loss=93.9707
	step [40/147], loss=77.1404
	step [41/147], loss=89.6309
	step [42/147], loss=72.9551
	step [43/147], loss=84.6698
	step [44/147], loss=87.8445
	step [45/147], loss=78.1121
	step [46/147], loss=73.6701
	step [47/147], loss=93.0916
	step [48/147], loss=65.3386
	step [49/147], loss=87.1197
	step [50/147], loss=79.9123
	step [51/147], loss=89.7963
	step [52/147], loss=84.9170
	step [53/147], loss=91.3340
	step [54/147], loss=78.4123
	step [55/147], loss=77.4743
	step [56/147], loss=81.2840
	step [57/147], loss=77.8161
	step [58/147], loss=74.2929
	step [59/147], loss=71.9929
	step [60/147], loss=89.0468
	step [61/147], loss=77.9148
	step [62/147], loss=78.0186
	step [63/147], loss=79.8074
	step [64/147], loss=84.6116
	step [65/147], loss=73.4979
	step [66/147], loss=72.3826
	step [67/147], loss=72.6337
	step [68/147], loss=76.7771
	step [69/147], loss=85.5364
	step [70/147], loss=95.4563
	step [71/147], loss=83.3896
	step [72/147], loss=89.3714
	step [73/147], loss=80.5968
	step [74/147], loss=88.0714
	step [75/147], loss=90.8596
	step [76/147], loss=77.5487
	step [77/147], loss=86.4691
	step [78/147], loss=90.6943
	step [79/147], loss=69.2593
	step [80/147], loss=90.5669
	step [81/147], loss=84.0801
	step [82/147], loss=95.1558
	step [83/147], loss=87.9089
	step [84/147], loss=94.7885
	step [85/147], loss=82.0790
	step [86/147], loss=103.5468
	step [87/147], loss=89.4873
	step [88/147], loss=84.7263
	step [89/147], loss=90.0628
	step [90/147], loss=74.9323
	step [91/147], loss=80.6879
	step [92/147], loss=72.5173
	step [93/147], loss=79.1880
	step [94/147], loss=98.7770
	step [95/147], loss=84.0889
	step [96/147], loss=75.7450
	step [97/147], loss=74.3665
	step [98/147], loss=81.7261
	step [99/147], loss=76.0492
	step [100/147], loss=73.7420
	step [101/147], loss=71.1707
	step [102/147], loss=74.0473
	step [103/147], loss=82.4338
	step [104/147], loss=75.4767
	step [105/147], loss=82.1363
	step [106/147], loss=82.7620
	step [107/147], loss=87.9528
	step [108/147], loss=74.8983
	step [109/147], loss=82.4156
	step [110/147], loss=78.8649
	step [111/147], loss=71.9151
	step [112/147], loss=91.8066
	step [113/147], loss=84.8158
	step [114/147], loss=80.9069
	step [115/147], loss=75.0378
	step [116/147], loss=74.3594
	step [117/147], loss=71.2586
	step [118/147], loss=88.0901
	step [119/147], loss=88.7695
	step [120/147], loss=74.4697
	step [121/147], loss=68.5732
	step [122/147], loss=75.4979
	step [123/147], loss=85.7348
	step [124/147], loss=72.4965
	step [125/147], loss=88.2934
	step [126/147], loss=97.6008
	step [127/147], loss=78.7187
	step [128/147], loss=68.6236
	step [129/147], loss=70.0093
	step [130/147], loss=77.1253
	step [131/147], loss=82.8455
	step [132/147], loss=94.5758
	step [133/147], loss=71.2569
	step [134/147], loss=88.8055
	step [135/147], loss=80.7514
	step [136/147], loss=77.1209
	step [137/147], loss=81.9145
	step [138/147], loss=80.4163
	step [139/147], loss=90.8323
	step [140/147], loss=90.9765
	step [141/147], loss=69.3437
	step [142/147], loss=76.8200
	step [143/147], loss=74.6211
	step [144/147], loss=86.0995
	step [145/147], loss=84.7656
	step [146/147], loss=75.5271
	step [147/147], loss=27.2386
	Evaluating
	loss=0.0098, precision=0.3680, recall=0.8415, f1=0.5121
Training epoch 101
	step [1/147], loss=84.1263
	step [2/147], loss=70.8220
	step [3/147], loss=87.5279
	step [4/147], loss=75.6530
	step [5/147], loss=91.5356
	step [6/147], loss=70.0019
	step [7/147], loss=77.8126
	step [8/147], loss=73.2348
	step [9/147], loss=75.5811
	step [10/147], loss=77.3783
	step [11/147], loss=80.5191
	step [12/147], loss=64.6039
	step [13/147], loss=69.7001
	step [14/147], loss=77.5039
	step [15/147], loss=65.6325
	step [16/147], loss=82.4405
	step [17/147], loss=66.4984
	step [18/147], loss=101.2118
	step [19/147], loss=85.6486
	step [20/147], loss=73.9634
	step [21/147], loss=83.0942
	step [22/147], loss=91.3712
	step [23/147], loss=82.2959
	step [24/147], loss=74.7520
	step [25/147], loss=94.3023
	step [26/147], loss=86.8030
	step [27/147], loss=88.2482
	step [28/147], loss=74.2073
	step [29/147], loss=90.8322
	step [30/147], loss=76.4372
	step [31/147], loss=80.6807
	step [32/147], loss=86.2598
	step [33/147], loss=80.6107
	step [34/147], loss=69.9238
	step [35/147], loss=76.0221
	step [36/147], loss=87.9801
	step [37/147], loss=89.8261
	step [38/147], loss=78.4468
	step [39/147], loss=91.6725
	step [40/147], loss=88.0926
	step [41/147], loss=62.7305
	step [42/147], loss=84.3429
	step [43/147], loss=77.6421
	step [44/147], loss=77.5798
	step [45/147], loss=74.6169
	step [46/147], loss=73.3578
	step [47/147], loss=80.3915
	step [48/147], loss=74.4198
	step [49/147], loss=91.7206
	step [50/147], loss=83.9733
	step [51/147], loss=87.8348
	step [52/147], loss=78.3103
	step [53/147], loss=91.7983
	step [54/147], loss=90.3199
	step [55/147], loss=77.9728
	step [56/147], loss=66.1993
	step [57/147], loss=64.5357
	step [58/147], loss=92.0817
	step [59/147], loss=75.3886
	step [60/147], loss=78.6801
	step [61/147], loss=87.7551
	step [62/147], loss=85.2148
	step [63/147], loss=98.2162
	step [64/147], loss=87.3623
	step [65/147], loss=79.0985
	step [66/147], loss=73.0061
	step [67/147], loss=86.3635
	step [68/147], loss=87.6148
	step [69/147], loss=80.2948
	step [70/147], loss=73.6080
	step [71/147], loss=70.7621
	step [72/147], loss=92.5604
	step [73/147], loss=81.2794
	step [74/147], loss=94.8109
	step [75/147], loss=81.2931
	step [76/147], loss=91.3341
	step [77/147], loss=74.4610
	step [78/147], loss=64.0038
	step [79/147], loss=59.5194
	step [80/147], loss=80.1444
	step [81/147], loss=81.1873
	step [82/147], loss=79.5481
	step [83/147], loss=69.8903
	step [84/147], loss=82.6682
	step [85/147], loss=77.5210
	step [86/147], loss=94.2604
	step [87/147], loss=86.1324
	step [88/147], loss=72.9659
	step [89/147], loss=88.8790
	step [90/147], loss=93.5286
	step [91/147], loss=77.1774
	step [92/147], loss=77.1393
	step [93/147], loss=80.4416
	step [94/147], loss=84.7927
	step [95/147], loss=93.5457
	step [96/147], loss=76.8236
	step [97/147], loss=80.0036
	step [98/147], loss=75.1962
	step [99/147], loss=84.5608
	step [100/147], loss=94.9572
	step [101/147], loss=74.0440
	step [102/147], loss=97.9348
	step [103/147], loss=77.7572
	step [104/147], loss=80.9369
	step [105/147], loss=92.2363
	step [106/147], loss=81.2919
	step [107/147], loss=92.8280
	step [108/147], loss=64.2925
	step [109/147], loss=69.6218
	step [110/147], loss=91.1176
	step [111/147], loss=85.6823
	step [112/147], loss=78.1637
	step [113/147], loss=80.7882
	step [114/147], loss=80.6512
	step [115/147], loss=74.5600
	step [116/147], loss=74.0019
	step [117/147], loss=88.8854
	step [118/147], loss=75.2243
	step [119/147], loss=83.2496
	step [120/147], loss=79.3821
	step [121/147], loss=67.4255
	step [122/147], loss=82.4200
	step [123/147], loss=74.1657
	step [124/147], loss=74.7271
	step [125/147], loss=74.4390
	step [126/147], loss=84.7529
	step [127/147], loss=84.3870
	step [128/147], loss=96.1420
	step [129/147], loss=76.2191
	step [130/147], loss=71.4627
	step [131/147], loss=84.8364
	step [132/147], loss=82.0295
	step [133/147], loss=73.1839
	step [134/147], loss=84.3444
	step [135/147], loss=78.4885
	step [136/147], loss=85.2860
	step [137/147], loss=85.5074
	step [138/147], loss=82.2577
	step [139/147], loss=82.5485
	step [140/147], loss=79.3822
	step [141/147], loss=76.4491
	step [142/147], loss=74.2042
	step [143/147], loss=79.9263
	step [144/147], loss=86.5818
	step [145/147], loss=77.7311
	step [146/147], loss=75.0415
	step [147/147], loss=20.2710
	Evaluating
	loss=0.0075, precision=0.4562, recall=0.8214, f1=0.5866
Training epoch 102
	step [1/147], loss=92.0881
	step [2/147], loss=77.6241
	step [3/147], loss=88.6673
	step [4/147], loss=72.3240
	step [5/147], loss=72.5047
	step [6/147], loss=77.1046
	step [7/147], loss=71.4057
	step [8/147], loss=90.7251
	step [9/147], loss=69.5050
	step [10/147], loss=73.7391
	step [11/147], loss=74.6419
	step [12/147], loss=83.3204
	step [13/147], loss=61.3319
	step [14/147], loss=67.6049
	step [15/147], loss=92.8773
	step [16/147], loss=77.3851
	step [17/147], loss=70.1283
	step [18/147], loss=98.0586
	step [19/147], loss=69.4748
	step [20/147], loss=64.7213
	step [21/147], loss=77.4145
	step [22/147], loss=71.8180
	step [23/147], loss=70.2749
	step [24/147], loss=74.1156
	step [25/147], loss=68.5729
	step [26/147], loss=73.5709
	step [27/147], loss=79.2519
	step [28/147], loss=79.4612
	step [29/147], loss=82.3969
	step [30/147], loss=80.0071
	step [31/147], loss=71.4118
	step [32/147], loss=82.9788
	step [33/147], loss=76.7839
	step [34/147], loss=65.8742
	step [35/147], loss=81.0717
	step [36/147], loss=77.6987
	step [37/147], loss=67.4734
	step [38/147], loss=104.1639
	step [39/147], loss=85.0935
	step [40/147], loss=95.2189
	step [41/147], loss=75.8783
	step [42/147], loss=76.1018
	step [43/147], loss=61.9230
	step [44/147], loss=62.6846
	step [45/147], loss=72.5797
	step [46/147], loss=85.6459
	step [47/147], loss=84.5427
	step [48/147], loss=83.6518
	step [49/147], loss=85.7787
	step [50/147], loss=82.1996
	step [51/147], loss=83.5737
	step [52/147], loss=86.1516
	step [53/147], loss=58.3518
	step [54/147], loss=77.6851
	step [55/147], loss=70.5850
	step [56/147], loss=88.4209
	step [57/147], loss=81.9570
	step [58/147], loss=64.2807
	step [59/147], loss=67.0420
	step [60/147], loss=71.7451
	step [61/147], loss=76.0584
	step [62/147], loss=71.7082
	step [63/147], loss=97.0926
	step [64/147], loss=82.0314
	step [65/147], loss=108.0641
	step [66/147], loss=80.1654
	step [67/147], loss=73.9067
	step [68/147], loss=84.5610
	step [69/147], loss=71.9884
	step [70/147], loss=82.9897
	step [71/147], loss=86.8928
	step [72/147], loss=76.6612
	step [73/147], loss=70.5722
	step [74/147], loss=75.6908
	step [75/147], loss=78.2804
	step [76/147], loss=82.5638
	step [77/147], loss=88.5578
	step [78/147], loss=89.0139
	step [79/147], loss=99.9574
	step [80/147], loss=75.5416
	step [81/147], loss=86.4706
	step [82/147], loss=66.1813
	step [83/147], loss=77.7117
	step [84/147], loss=80.0386
	step [85/147], loss=83.2676
	step [86/147], loss=73.8621
	step [87/147], loss=75.4719
	step [88/147], loss=94.8297
	step [89/147], loss=79.0699
	step [90/147], loss=88.0187
	step [91/147], loss=98.0962
	step [92/147], loss=77.3793
	step [93/147], loss=67.2428
	step [94/147], loss=87.5747
	step [95/147], loss=98.0862
	step [96/147], loss=86.2351
	step [97/147], loss=82.9577
	step [98/147], loss=96.0995
	step [99/147], loss=82.3559
	step [100/147], loss=88.3211
	step [101/147], loss=62.6325
	step [102/147], loss=77.8199
	step [103/147], loss=88.1924
	step [104/147], loss=75.7129
	step [105/147], loss=101.0391
	step [106/147], loss=77.3610
	step [107/147], loss=74.5029
	step [108/147], loss=70.1091
	step [109/147], loss=62.9398
	step [110/147], loss=70.6017
	step [111/147], loss=97.2981
	step [112/147], loss=92.9476
	step [113/147], loss=74.6231
	step [114/147], loss=66.5029
	step [115/147], loss=99.0092
	step [116/147], loss=80.9712
	step [117/147], loss=94.0027
	step [118/147], loss=82.7287
	step [119/147], loss=82.2720
	step [120/147], loss=67.4451
	step [121/147], loss=105.0124
	step [122/147], loss=81.1798
	step [123/147], loss=78.8615
	step [124/147], loss=83.7052
	step [125/147], loss=83.1369
	step [126/147], loss=93.5635
	step [127/147], loss=89.3173
	step [128/147], loss=78.5808
	step [129/147], loss=91.5210
	step [130/147], loss=79.5114
	step [131/147], loss=83.1831
	step [132/147], loss=78.8681
	step [133/147], loss=81.9842
	step [134/147], loss=82.7788
	step [135/147], loss=71.8286
	step [136/147], loss=93.8283
	step [137/147], loss=82.2057
	step [138/147], loss=74.7956
	step [139/147], loss=88.0762
	step [140/147], loss=87.0912
	step [141/147], loss=78.2921
	step [142/147], loss=90.5426
	step [143/147], loss=88.8188
	step [144/147], loss=77.0521
	step [145/147], loss=75.9763
	step [146/147], loss=91.7609
	step [147/147], loss=23.1151
	Evaluating
	loss=0.0082, precision=0.4212, recall=0.8315, f1=0.5592
Training epoch 103
	step [1/147], loss=89.1185
	step [2/147], loss=69.2723
	step [3/147], loss=82.2606
	step [4/147], loss=90.6394
	step [5/147], loss=76.2585
	step [6/147], loss=87.7182
	step [7/147], loss=72.0415
	step [8/147], loss=77.2927
	step [9/147], loss=82.8293
	step [10/147], loss=81.0871
	step [11/147], loss=81.6923
	step [12/147], loss=72.6002
	step [13/147], loss=79.2312
	step [14/147], loss=90.6323
	step [15/147], loss=66.9070
	step [16/147], loss=92.3454
	step [17/147], loss=70.3189
	step [18/147], loss=76.7374
	step [19/147], loss=80.3069
	step [20/147], loss=71.8489
	step [21/147], loss=90.2884
	step [22/147], loss=76.3790
	step [23/147], loss=79.4326
	step [24/147], loss=70.9575
	step [25/147], loss=93.5500
	step [26/147], loss=87.7078
	step [27/147], loss=75.2144
	step [28/147], loss=73.8470
	step [29/147], loss=69.3952
	step [30/147], loss=80.4716
	step [31/147], loss=82.1328
	step [32/147], loss=83.2004
	step [33/147], loss=79.8729
	step [34/147], loss=73.9654
	step [35/147], loss=59.3583
	step [36/147], loss=97.4760
	step [37/147], loss=85.0239
	step [38/147], loss=77.5531
	step [39/147], loss=70.5390
	step [40/147], loss=82.1699
	step [41/147], loss=77.9165
	step [42/147], loss=67.9304
	step [43/147], loss=90.3747
	step [44/147], loss=83.0749
	step [45/147], loss=81.3615
	step [46/147], loss=101.0544
	step [47/147], loss=78.6360
	step [48/147], loss=77.0332
	step [49/147], loss=88.1863
	step [50/147], loss=74.0647
	step [51/147], loss=81.7171
	step [52/147], loss=89.9980
	step [53/147], loss=77.7097
	step [54/147], loss=87.7224
	step [55/147], loss=87.8690
	step [56/147], loss=74.6410
	step [57/147], loss=77.2300
	step [58/147], loss=83.1697
	step [59/147], loss=80.3420
	step [60/147], loss=95.9474
	step [61/147], loss=80.6536
	step [62/147], loss=60.3925
	step [63/147], loss=82.4492
	step [64/147], loss=90.4844
	step [65/147], loss=60.2551
	step [66/147], loss=68.1798
	step [67/147], loss=77.0050
	step [68/147], loss=83.6083
	step [69/147], loss=82.6194
	step [70/147], loss=82.7027
	step [71/147], loss=81.2133
	step [72/147], loss=73.7075
	step [73/147], loss=72.8493
	step [74/147], loss=83.6847
	step [75/147], loss=85.5429
	step [76/147], loss=85.0940
	step [77/147], loss=73.3833
	step [78/147], loss=76.4709
	step [79/147], loss=83.3614
	step [80/147], loss=81.5421
	step [81/147], loss=77.8826
	step [82/147], loss=93.0753
	step [83/147], loss=72.9867
	step [84/147], loss=71.6733
	step [85/147], loss=87.5191
	step [86/147], loss=66.9919
	step [87/147], loss=75.6111
	step [88/147], loss=78.5426
	step [89/147], loss=82.2554
	step [90/147], loss=79.6986
	step [91/147], loss=64.1666
	step [92/147], loss=82.4024
	step [93/147], loss=82.9951
	step [94/147], loss=87.0005
	step [95/147], loss=72.0818
	step [96/147], loss=93.0730
	step [97/147], loss=92.4778
	step [98/147], loss=88.2147
	step [99/147], loss=70.4947
	step [100/147], loss=71.3989
	step [101/147], loss=78.0827
	step [102/147], loss=107.0906
	step [103/147], loss=68.8810
	step [104/147], loss=89.6473
	step [105/147], loss=78.0211
	step [106/147], loss=63.3775
	step [107/147], loss=75.4633
	step [108/147], loss=78.6312
	step [109/147], loss=79.9060
	step [110/147], loss=69.0704
	step [111/147], loss=90.6907
	step [112/147], loss=75.4546
	step [113/147], loss=93.8037
	step [114/147], loss=80.7391
	step [115/147], loss=74.2495
	step [116/147], loss=77.1839
	step [117/147], loss=81.5536
	step [118/147], loss=75.6475
	step [119/147], loss=64.0247
	step [120/147], loss=79.5105
	step [121/147], loss=83.0490
	step [122/147], loss=83.5917
	step [123/147], loss=83.8826
	step [124/147], loss=77.6401
	step [125/147], loss=83.6059
	step [126/147], loss=87.8994
	step [127/147], loss=92.2254
	step [128/147], loss=84.0885
	step [129/147], loss=74.4561
	step [130/147], loss=80.1882
	step [131/147], loss=80.0624
	step [132/147], loss=76.8775
	step [133/147], loss=85.3063
	step [134/147], loss=80.4226
	step [135/147], loss=78.6121
	step [136/147], loss=77.5905
	step [137/147], loss=87.3946
	step [138/147], loss=79.4407
	step [139/147], loss=88.9021
	step [140/147], loss=72.4713
	step [141/147], loss=92.5523
	step [142/147], loss=80.3126
	step [143/147], loss=71.0146
	step [144/147], loss=76.6376
	step [145/147], loss=79.7815
	step [146/147], loss=84.3624
	step [147/147], loss=22.2761
	Evaluating
	loss=0.0086, precision=0.4017, recall=0.8291, f1=0.5412
Training epoch 104
	step [1/147], loss=75.9156
	step [2/147], loss=76.8126
	step [3/147], loss=86.8886
	step [4/147], loss=57.0716
	step [5/147], loss=77.5778
	step [6/147], loss=78.7034
	step [7/147], loss=71.4803
	step [8/147], loss=79.6404
	step [9/147], loss=95.3465
	step [10/147], loss=76.2295
	step [11/147], loss=100.5073
	step [12/147], loss=82.0635
	step [13/147], loss=93.0488
	step [14/147], loss=85.1764
	step [15/147], loss=88.3092
	step [16/147], loss=67.0460
	step [17/147], loss=80.8348
	step [18/147], loss=93.9996
	step [19/147], loss=92.7580
	step [20/147], loss=67.4697
	step [21/147], loss=68.2816
	step [22/147], loss=94.3949
	step [23/147], loss=78.2832
	step [24/147], loss=93.9015
	step [25/147], loss=81.9191
	step [26/147], loss=81.3345
	step [27/147], loss=81.1457
	step [28/147], loss=77.6033
	step [29/147], loss=67.5564
	step [30/147], loss=87.5081
	step [31/147], loss=71.7087
	step [32/147], loss=68.7462
	step [33/147], loss=79.9342
	step [34/147], loss=81.4640
	step [35/147], loss=74.7393
	step [36/147], loss=70.9229
	step [37/147], loss=78.0783
	step [38/147], loss=81.9741
	step [39/147], loss=81.8463
	step [40/147], loss=88.4343
	step [41/147], loss=74.3441
	step [42/147], loss=97.6992
	step [43/147], loss=75.2200
	step [44/147], loss=77.4396
	step [45/147], loss=71.2898
	step [46/147], loss=81.2323
	step [47/147], loss=79.3795
	step [48/147], loss=71.2356
	step [49/147], loss=79.5816
	step [50/147], loss=80.8529
	step [51/147], loss=72.7500
	step [52/147], loss=76.9636
	step [53/147], loss=77.3022
	step [54/147], loss=76.0287
	step [55/147], loss=77.0741
	step [56/147], loss=76.1091
	step [57/147], loss=68.5341
	step [58/147], loss=80.2316
	step [59/147], loss=77.0873
	step [60/147], loss=89.2885
	step [61/147], loss=88.3362
	step [62/147], loss=92.5504
	step [63/147], loss=66.8876
	step [64/147], loss=87.1992
	step [65/147], loss=72.8074
	step [66/147], loss=84.1606
	step [67/147], loss=94.9444
	step [68/147], loss=83.7751
	step [69/147], loss=74.7009
	step [70/147], loss=76.3018
	step [71/147], loss=78.8811
	step [72/147], loss=71.2515
	step [73/147], loss=97.8752
	step [74/147], loss=69.9527
	step [75/147], loss=63.6121
	step [76/147], loss=70.9574
	step [77/147], loss=69.5995
	step [78/147], loss=81.2575
	step [79/147], loss=71.0524
	step [80/147], loss=73.7677
	step [81/147], loss=87.9334
	step [82/147], loss=87.7610
	step [83/147], loss=76.6114
	step [84/147], loss=73.2504
	step [85/147], loss=69.7173
	step [86/147], loss=95.4821
	step [87/147], loss=77.5604
	step [88/147], loss=91.4829
	step [89/147], loss=71.8303
	step [90/147], loss=74.7037
	step [91/147], loss=88.1320
	step [92/147], loss=84.5510
	step [93/147], loss=65.6478
	step [94/147], loss=65.0129
	step [95/147], loss=87.3208
	step [96/147], loss=82.4477
	step [97/147], loss=80.7949
	step [98/147], loss=90.0491
	step [99/147], loss=82.3409
	step [100/147], loss=81.9119
	step [101/147], loss=77.7832
	step [102/147], loss=75.4537
	step [103/147], loss=70.2416
	step [104/147], loss=83.3776
	step [105/147], loss=86.1757
	step [106/147], loss=76.1235
	step [107/147], loss=82.0810
	step [108/147], loss=75.9615
	step [109/147], loss=74.6816
	step [110/147], loss=79.5812
	step [111/147], loss=71.5883
	step [112/147], loss=77.5723
	step [113/147], loss=68.1817
	step [114/147], loss=75.4048
	step [115/147], loss=89.4152
	step [116/147], loss=92.3066
	step [117/147], loss=94.2002
	step [118/147], loss=77.4421
	step [119/147], loss=92.2867
	step [120/147], loss=74.4882
	step [121/147], loss=75.1650
	step [122/147], loss=74.1053
	step [123/147], loss=93.0984
	step [124/147], loss=79.6101
	step [125/147], loss=85.8616
	step [126/147], loss=80.1940
	step [127/147], loss=81.9948
	step [128/147], loss=69.2257
	step [129/147], loss=78.7996
	step [130/147], loss=96.2152
	step [131/147], loss=91.4251
	step [132/147], loss=73.1317
	step [133/147], loss=77.7892
	step [134/147], loss=81.2159
	step [135/147], loss=77.0941
	step [136/147], loss=92.5094
	step [137/147], loss=98.2972
	step [138/147], loss=90.6202
	step [139/147], loss=94.8273
	step [140/147], loss=82.5180
	step [141/147], loss=74.7502
	step [142/147], loss=87.8179
	step [143/147], loss=77.1721
	step [144/147], loss=71.0183
	step [145/147], loss=74.7016
	step [146/147], loss=79.3105
	step [147/147], loss=30.9559
	Evaluating
	loss=0.0078, precision=0.4468, recall=0.8201, f1=0.5784
Training epoch 105
	step [1/147], loss=87.7087
	step [2/147], loss=78.3190
	step [3/147], loss=69.3356
	step [4/147], loss=89.5152
	step [5/147], loss=88.0568
	step [6/147], loss=63.2469
	step [7/147], loss=88.2225
	step [8/147], loss=76.0666
	step [9/147], loss=83.7326
	step [10/147], loss=73.4595
	step [11/147], loss=68.3354
	step [12/147], loss=91.5368
	step [13/147], loss=88.6816
	step [14/147], loss=71.9724
	step [15/147], loss=69.6409
	step [16/147], loss=82.7156
	step [17/147], loss=85.9464
	step [18/147], loss=82.5861
	step [19/147], loss=76.9533
	step [20/147], loss=76.4335
	step [21/147], loss=71.2476
	step [22/147], loss=70.0221
	step [23/147], loss=78.6981
	step [24/147], loss=91.8584
	step [25/147], loss=85.7191
	step [26/147], loss=80.0715
	step [27/147], loss=86.1886
	step [28/147], loss=79.6986
	step [29/147], loss=77.5541
	step [30/147], loss=91.5062
	step [31/147], loss=66.0033
	step [32/147], loss=87.9536
	step [33/147], loss=77.7375
	step [34/147], loss=61.9437
	step [35/147], loss=82.5219
	step [36/147], loss=75.4990
	step [37/147], loss=94.6097
	step [38/147], loss=79.9243
	step [39/147], loss=67.1149
	step [40/147], loss=82.8123
	step [41/147], loss=90.0600
	step [42/147], loss=83.2907
	step [43/147], loss=92.6206
	step [44/147], loss=76.0741
	step [45/147], loss=88.5397
	step [46/147], loss=72.6975
	step [47/147], loss=73.6709
	step [48/147], loss=88.5712
	step [49/147], loss=77.7708
	step [50/147], loss=78.8529
	step [51/147], loss=75.0187
	step [52/147], loss=79.2417
	step [53/147], loss=82.7175
	step [54/147], loss=70.5775
	step [55/147], loss=67.5842
	step [56/147], loss=71.1911
	step [57/147], loss=81.8168
	step [58/147], loss=85.2913
	step [59/147], loss=79.0938
	step [60/147], loss=79.2889
	step [61/147], loss=83.9197
	step [62/147], loss=83.3435
	step [63/147], loss=94.2220
	step [64/147], loss=74.8358
	step [65/147], loss=70.8024
	step [66/147], loss=80.2822
	step [67/147], loss=68.9006
	step [68/147], loss=77.4110
	step [69/147], loss=75.6474
	step [70/147], loss=87.9672
	step [71/147], loss=83.6101
	step [72/147], loss=87.7675
	step [73/147], loss=79.1738
	step [74/147], loss=89.1443
	step [75/147], loss=84.2547
	step [76/147], loss=79.4024
	step [77/147], loss=72.4917
	step [78/147], loss=74.1265
	step [79/147], loss=63.7778
	step [80/147], loss=96.3452
	step [81/147], loss=99.5650
	step [82/147], loss=83.2650
	step [83/147], loss=69.7713
	step [84/147], loss=72.8526
	step [85/147], loss=70.3349
	step [86/147], loss=102.2531
	step [87/147], loss=74.6774
	step [88/147], loss=85.3064
	step [89/147], loss=98.3169
	step [90/147], loss=68.7197
	step [91/147], loss=83.2444
	step [92/147], loss=88.0882
	step [93/147], loss=80.2983
	step [94/147], loss=94.8194
	step [95/147], loss=84.1414
	step [96/147], loss=96.4705
	step [97/147], loss=82.6347
	step [98/147], loss=75.0180
	step [99/147], loss=73.2008
	step [100/147], loss=83.5489
	step [101/147], loss=94.9429
	step [102/147], loss=87.6375
	step [103/147], loss=76.5070
	step [104/147], loss=73.4961
	step [105/147], loss=70.3611
	step [106/147], loss=85.4593
	step [107/147], loss=71.4728
	step [108/147], loss=76.0679
	step [109/147], loss=74.3887
	step [110/147], loss=71.8903
	step [111/147], loss=70.4866
	step [112/147], loss=69.9359
	step [113/147], loss=83.1519
	step [114/147], loss=72.7688
	step [115/147], loss=87.6539
	step [116/147], loss=71.9248
	step [117/147], loss=77.7029
	step [118/147], loss=76.8697
	step [119/147], loss=77.7916
	step [120/147], loss=79.7222
	step [121/147], loss=75.4709
	step [122/147], loss=85.3853
	step [123/147], loss=75.6894
	step [124/147], loss=95.0395
	step [125/147], loss=75.5538
	step [126/147], loss=75.6383
	step [127/147], loss=66.9868
	step [128/147], loss=82.6208
	step [129/147], loss=68.1316
	step [130/147], loss=72.0756
	step [131/147], loss=90.7874
	step [132/147], loss=79.7894
	step [133/147], loss=74.0974
	step [134/147], loss=75.6324
	step [135/147], loss=79.8556
	step [136/147], loss=81.0715
	step [137/147], loss=79.3569
	step [138/147], loss=84.5408
	step [139/147], loss=85.4520
	step [140/147], loss=82.1799
	step [141/147], loss=76.7082
	step [142/147], loss=82.8556
	step [143/147], loss=84.2958
	step [144/147], loss=76.2489
	step [145/147], loss=91.1285
	step [146/147], loss=80.0000
	step [147/147], loss=20.6799
	Evaluating
	loss=0.0070, precision=0.4773, recall=0.8217, f1=0.6039
saving model as: 3_saved_model.pth
Training epoch 106
	step [1/147], loss=85.8538
	step [2/147], loss=73.7707
	step [3/147], loss=83.6504
	step [4/147], loss=76.1802
	step [5/147], loss=69.8864
	step [6/147], loss=71.0013
	step [7/147], loss=85.3860
	step [8/147], loss=70.0602
	step [9/147], loss=88.4013
	step [10/147], loss=79.4674
	step [11/147], loss=86.4670
	step [12/147], loss=88.8216
	step [13/147], loss=78.6754
	step [14/147], loss=61.3378
	step [15/147], loss=81.4387
	step [16/147], loss=70.5850
	step [17/147], loss=82.5448
	step [18/147], loss=74.8910
	step [19/147], loss=66.9827
	step [20/147], loss=91.7254
	step [21/147], loss=93.4449
	step [22/147], loss=68.7276
	step [23/147], loss=81.8323
	step [24/147], loss=91.4088
	step [25/147], loss=97.0317
	step [26/147], loss=73.1916
	step [27/147], loss=74.3437
	step [28/147], loss=74.3571
	step [29/147], loss=66.3146
	step [30/147], loss=75.0576
	step [31/147], loss=83.5299
	step [32/147], loss=85.2613
	step [33/147], loss=92.4664
	step [34/147], loss=80.7222
	step [35/147], loss=77.6696
	step [36/147], loss=72.6618
	step [37/147], loss=72.5172
	step [38/147], loss=71.9637
	step [39/147], loss=92.4167
	step [40/147], loss=80.2300
	step [41/147], loss=75.2554
	step [42/147], loss=81.5029
	step [43/147], loss=94.2951
	step [44/147], loss=74.4097
	step [45/147], loss=70.3898
	step [46/147], loss=89.3071
	step [47/147], loss=68.3668
	step [48/147], loss=80.5198
	step [49/147], loss=72.7165
	step [50/147], loss=69.0171
	step [51/147], loss=79.5922
	step [52/147], loss=80.1061
	step [53/147], loss=83.6314
	step [54/147], loss=79.9925
	step [55/147], loss=67.6909
	step [56/147], loss=73.0663
	step [57/147], loss=76.7757
	step [58/147], loss=71.8267
	step [59/147], loss=87.2175
	step [60/147], loss=80.0710
	step [61/147], loss=95.6164
	step [62/147], loss=80.1448
	step [63/147], loss=84.7662
	step [64/147], loss=66.6010
	step [65/147], loss=80.7496
	step [66/147], loss=71.3836
	step [67/147], loss=84.3492
	step [68/147], loss=89.6006
	step [69/147], loss=82.4069
	step [70/147], loss=83.5405
	step [71/147], loss=97.5348
	step [72/147], loss=86.0758
	step [73/147], loss=72.8047
	step [74/147], loss=70.4628
	step [75/147], loss=103.3413
	step [76/147], loss=92.0023
	step [77/147], loss=69.1965
	step [78/147], loss=83.7822
	step [79/147], loss=72.4661
	step [80/147], loss=87.9732
	step [81/147], loss=73.3175
	step [82/147], loss=88.6230
	step [83/147], loss=73.4701
	step [84/147], loss=81.4046
	step [85/147], loss=88.1545
	step [86/147], loss=77.0320
	step [87/147], loss=71.9893
	step [88/147], loss=77.1174
	step [89/147], loss=63.7554
	step [90/147], loss=77.6667
	step [91/147], loss=80.1248
	step [92/147], loss=77.5062
	step [93/147], loss=85.1859
	step [94/147], loss=89.9248
	step [95/147], loss=78.8882
	step [96/147], loss=82.6650
	step [97/147], loss=87.5991
	step [98/147], loss=77.4714
	step [99/147], loss=72.8240
	step [100/147], loss=77.1517
	step [101/147], loss=73.8613
	step [102/147], loss=73.0846
	step [103/147], loss=74.4207
	step [104/147], loss=72.4035
	step [105/147], loss=86.6603
	step [106/147], loss=75.0718
	step [107/147], loss=85.1461
	step [108/147], loss=70.0269
	step [109/147], loss=87.2775
	step [110/147], loss=66.7608
	step [111/147], loss=75.1387
	step [112/147], loss=88.0075
	step [113/147], loss=72.4331
	step [114/147], loss=92.0404
	step [115/147], loss=86.6984
	step [116/147], loss=88.4099
	step [117/147], loss=98.7704
	step [118/147], loss=73.7164
	step [119/147], loss=77.8651
	step [120/147], loss=90.1155
	step [121/147], loss=97.8816
	step [122/147], loss=85.6982
	step [123/147], loss=79.9386
	step [124/147], loss=78.3759
	step [125/147], loss=81.0380
	step [126/147], loss=67.6276
	step [127/147], loss=85.0257
	step [128/147], loss=82.5480
	step [129/147], loss=80.3124
	step [130/147], loss=90.1210
	step [131/147], loss=75.2324
	step [132/147], loss=79.8771
	step [133/147], loss=80.5672
	step [134/147], loss=89.7787
	step [135/147], loss=73.6213
	step [136/147], loss=73.0181
	step [137/147], loss=81.8133
	step [138/147], loss=83.6363
	step [139/147], loss=67.7483
	step [140/147], loss=75.2299
	step [141/147], loss=89.0045
	step [142/147], loss=95.1083
	step [143/147], loss=79.0429
	step [144/147], loss=93.7642
	step [145/147], loss=76.3568
	step [146/147], loss=70.6117
	step [147/147], loss=15.9803
	Evaluating
	loss=0.0067, precision=0.4901, recall=0.7991, f1=0.6076
saving model as: 3_saved_model.pth
Training epoch 107
	step [1/147], loss=85.7754
	step [2/147], loss=88.5393
	step [3/147], loss=74.3144
	step [4/147], loss=81.6537
	step [5/147], loss=77.5904
	step [6/147], loss=73.7858
	step [7/147], loss=94.3486
	step [8/147], loss=87.5250
	step [9/147], loss=86.5659
	step [10/147], loss=76.1612
	step [11/147], loss=97.3758
	step [12/147], loss=76.8200
	step [13/147], loss=81.3584
	step [14/147], loss=78.2453
	step [15/147], loss=67.7765
	step [16/147], loss=65.0846
	step [17/147], loss=93.6729
	step [18/147], loss=73.1751
	step [19/147], loss=72.2251
	step [20/147], loss=81.8201
	step [21/147], loss=75.3278
	step [22/147], loss=73.3626
	step [23/147], loss=76.9422
	step [24/147], loss=73.0553
	step [25/147], loss=79.1226
	step [26/147], loss=77.2099
	step [27/147], loss=70.6721
	step [28/147], loss=75.6846
	step [29/147], loss=85.9796
	step [30/147], loss=70.4659
	step [31/147], loss=87.0956
	step [32/147], loss=72.5054
	step [33/147], loss=72.6398
	step [34/147], loss=88.3777
	step [35/147], loss=76.3646
	step [36/147], loss=69.2696
	step [37/147], loss=65.4705
	step [38/147], loss=74.7651
	step [39/147], loss=63.2489
	step [40/147], loss=79.3145
	step [41/147], loss=91.0157
	step [42/147], loss=85.0132
	step [43/147], loss=83.1039
	step [44/147], loss=73.8293
	step [45/147], loss=85.7939
	step [46/147], loss=81.8138
	step [47/147], loss=63.5586
	step [48/147], loss=83.1822
	step [49/147], loss=75.3553
	step [50/147], loss=72.0767
	step [51/147], loss=80.7533
	step [52/147], loss=82.2718
	step [53/147], loss=82.8912
	step [54/147], loss=73.3184
	step [55/147], loss=85.0080
	step [56/147], loss=87.2241
	step [57/147], loss=79.5412
	step [58/147], loss=87.8925
	step [59/147], loss=79.1429
	step [60/147], loss=75.9732
	step [61/147], loss=77.4399
	step [62/147], loss=87.4329
	step [63/147], loss=86.2265
	step [64/147], loss=89.1234
	step [65/147], loss=74.9275
	step [66/147], loss=97.2437
	step [67/147], loss=74.9327
	step [68/147], loss=79.1118
	step [69/147], loss=82.0577
	step [70/147], loss=82.5935
	step [71/147], loss=87.9382
	step [72/147], loss=78.1056
	step [73/147], loss=64.0731
	step [74/147], loss=79.4373
	step [75/147], loss=74.0971
	step [76/147], loss=70.2519
	step [77/147], loss=79.7964
	step [78/147], loss=80.9927
	step [79/147], loss=75.4626
	step [80/147], loss=74.1369
	step [81/147], loss=81.0045
	step [82/147], loss=88.7404
	step [83/147], loss=81.4589
	step [84/147], loss=95.3519
	step [85/147], loss=79.5367
	step [86/147], loss=75.1698
	step [87/147], loss=81.4929
	step [88/147], loss=67.3165
	step [89/147], loss=83.5535
	step [90/147], loss=86.7783
	step [91/147], loss=90.3411
	step [92/147], loss=76.6530
	step [93/147], loss=86.5914
	step [94/147], loss=89.8994
	step [95/147], loss=74.7785
	step [96/147], loss=78.7996
	step [97/147], loss=82.4490
	step [98/147], loss=61.6984
	step [99/147], loss=78.9493
	step [100/147], loss=77.7742
	step [101/147], loss=73.0938
	step [102/147], loss=92.0861
	step [103/147], loss=68.2557
	step [104/147], loss=82.8515
	step [105/147], loss=87.9317
	step [106/147], loss=74.6554
	step [107/147], loss=77.1726
	step [108/147], loss=80.7795
	step [109/147], loss=78.0334
	step [110/147], loss=79.7223
	step [111/147], loss=67.9293
	step [112/147], loss=69.1773
	step [113/147], loss=65.3534
	step [114/147], loss=91.7017
	step [115/147], loss=105.2445
	step [116/147], loss=86.8026
	step [117/147], loss=69.8927
	step [118/147], loss=88.6494
	step [119/147], loss=81.3355
	step [120/147], loss=83.2135
	step [121/147], loss=67.5760
	step [122/147], loss=71.7143
	step [123/147], loss=81.6262
	step [124/147], loss=77.6378
	step [125/147], loss=81.7827
	step [126/147], loss=76.1049
	step [127/147], loss=92.2170
	step [128/147], loss=76.3491
	step [129/147], loss=85.0056
	step [130/147], loss=76.5178
	step [131/147], loss=97.3232
	step [132/147], loss=65.1088
	step [133/147], loss=67.6056
	step [134/147], loss=91.4249
	step [135/147], loss=82.5031
	step [136/147], loss=65.5763
	step [137/147], loss=80.8027
	step [138/147], loss=95.3709
	step [139/147], loss=91.0256
	step [140/147], loss=77.8449
	step [141/147], loss=85.3347
	step [142/147], loss=73.6195
	step [143/147], loss=69.5399
	step [144/147], loss=70.2162
	step [145/147], loss=87.7500
	step [146/147], loss=86.0306
	step [147/147], loss=19.4557
	Evaluating
	loss=0.0072, precision=0.4634, recall=0.8304, f1=0.5948
Training epoch 108
	step [1/147], loss=83.1180
	step [2/147], loss=71.6134
	step [3/147], loss=75.0224
	step [4/147], loss=72.6488
	step [5/147], loss=70.4738
	step [6/147], loss=68.6321
	step [7/147], loss=89.5234
	step [8/147], loss=80.5244
	step [9/147], loss=83.8597
	step [10/147], loss=72.4445
	step [11/147], loss=69.3618
	step [12/147], loss=78.6104
	step [13/147], loss=80.4032
	step [14/147], loss=75.8287
	step [15/147], loss=79.2563
	step [16/147], loss=74.7063
	step [17/147], loss=83.2101
	step [18/147], loss=80.0803
	step [19/147], loss=78.7843
	step [20/147], loss=72.8456
	step [21/147], loss=82.7261
	step [22/147], loss=80.4231
	step [23/147], loss=74.2894
	step [24/147], loss=71.5990
	step [25/147], loss=70.4307
	step [26/147], loss=73.1630
	step [27/147], loss=71.3320
	step [28/147], loss=70.3578
	step [29/147], loss=71.7288
	step [30/147], loss=76.8816
	step [31/147], loss=79.3611
	step [32/147], loss=75.8589
	step [33/147], loss=76.5395
	step [34/147], loss=79.5910
	step [35/147], loss=71.6203
	step [36/147], loss=86.4863
	step [37/147], loss=74.4763
	step [38/147], loss=79.1111
	step [39/147], loss=85.6602
	step [40/147], loss=65.8215
	step [41/147], loss=88.1520
	step [42/147], loss=82.3943
	step [43/147], loss=82.9439
	step [44/147], loss=83.6676
	step [45/147], loss=83.2923
	step [46/147], loss=72.0811
	step [47/147], loss=71.7762
	step [48/147], loss=73.3690
	step [49/147], loss=78.6647
	step [50/147], loss=81.4323
	step [51/147], loss=72.1858
	step [52/147], loss=77.9574
	step [53/147], loss=65.9841
	step [54/147], loss=73.3282
	step [55/147], loss=81.9370
	step [56/147], loss=84.3462
	step [57/147], loss=71.2763
	step [58/147], loss=72.0263
	step [59/147], loss=70.6559
	step [60/147], loss=86.8767
	step [61/147], loss=89.3151
	step [62/147], loss=70.5316
	step [63/147], loss=70.3681
	step [64/147], loss=86.6102
	step [65/147], loss=77.8723
	step [66/147], loss=94.6608
	step [67/147], loss=73.4660
	step [68/147], loss=70.8124
	step [69/147], loss=75.2165
	step [70/147], loss=78.3614
	step [71/147], loss=87.9035
	step [72/147], loss=78.2535
	step [73/147], loss=67.0473
	step [74/147], loss=79.5556
	step [75/147], loss=82.2001
	step [76/147], loss=64.7097
	step [77/147], loss=77.3883
	step [78/147], loss=95.5343
	step [79/147], loss=63.1723
	step [80/147], loss=88.0798
	step [81/147], loss=82.8012
	step [82/147], loss=73.9847
	step [83/147], loss=94.5181
	step [84/147], loss=73.1637
	step [85/147], loss=77.1329
	step [86/147], loss=68.1208
	step [87/147], loss=94.8136
	step [88/147], loss=84.5662
	step [89/147], loss=85.7429
	step [90/147], loss=85.5965
	step [91/147], loss=95.8739
	step [92/147], loss=70.6540
	step [93/147], loss=87.8981
	step [94/147], loss=76.8573
	step [95/147], loss=87.1447
	step [96/147], loss=82.8522
	step [97/147], loss=79.8353
	step [98/147], loss=75.1115
	step [99/147], loss=86.0646
	step [100/147], loss=62.2342
	step [101/147], loss=81.5704
	step [102/147], loss=81.4687
	step [103/147], loss=70.7244
	step [104/147], loss=80.1206
	step [105/147], loss=75.4822
	step [106/147], loss=73.8524
	step [107/147], loss=76.6104
	step [108/147], loss=83.7104
	step [109/147], loss=107.2579
	step [110/147], loss=88.4918
	step [111/147], loss=77.6001
	step [112/147], loss=78.9996
	step [113/147], loss=82.9496
	step [114/147], loss=73.0061
	step [115/147], loss=72.1573
	step [116/147], loss=61.1242
	step [117/147], loss=76.4796
	step [118/147], loss=78.3421
	step [119/147], loss=91.1266
	step [120/147], loss=94.9973
	step [121/147], loss=114.3187
	step [122/147], loss=92.1452
	step [123/147], loss=70.2744
	step [124/147], loss=85.2434
	step [125/147], loss=76.8150
	step [126/147], loss=77.6763
	step [127/147], loss=78.1786
	step [128/147], loss=72.8525
	step [129/147], loss=73.9826
	step [130/147], loss=88.6431
	step [131/147], loss=83.6734
	step [132/147], loss=90.5069
	step [133/147], loss=82.0497
	step [134/147], loss=85.9342
	step [135/147], loss=66.7436
	step [136/147], loss=85.5317
	step [137/147], loss=76.1586
	step [138/147], loss=70.9078
	step [139/147], loss=83.5489
	step [140/147], loss=86.2856
	step [141/147], loss=87.1373
	step [142/147], loss=87.3328
	step [143/147], loss=76.6988
	step [144/147], loss=92.3356
	step [145/147], loss=67.9157
	step [146/147], loss=75.0998
	step [147/147], loss=29.2473
	Evaluating
	loss=0.0090, precision=0.3895, recall=0.8316, f1=0.5305
Training epoch 109
	step [1/147], loss=86.8294
	step [2/147], loss=83.4617
	step [3/147], loss=81.7761
	step [4/147], loss=73.9847
	step [5/147], loss=83.3152
	step [6/147], loss=78.6748
	step [7/147], loss=71.5287
	step [8/147], loss=89.6557
	step [9/147], loss=72.9408
	step [10/147], loss=77.1643
	step [11/147], loss=70.4944
	step [12/147], loss=73.0891
	step [13/147], loss=76.5658
	step [14/147], loss=84.9537
	step [15/147], loss=93.1084
	step [16/147], loss=72.3398
	step [17/147], loss=80.6727
	step [18/147], loss=80.2986
	step [19/147], loss=78.1006
	step [20/147], loss=62.4409
	step [21/147], loss=95.1210
	step [22/147], loss=81.8898
	step [23/147], loss=88.8118
	step [24/147], loss=74.7385
	step [25/147], loss=87.1581
	step [26/147], loss=70.2278
	step [27/147], loss=77.9730
	step [28/147], loss=62.1624
	step [29/147], loss=79.0295
	step [30/147], loss=94.0115
	step [31/147], loss=63.5563
	step [32/147], loss=91.8508
	step [33/147], loss=67.9465
	step [34/147], loss=87.3170
	step [35/147], loss=72.9753
	step [36/147], loss=67.8112
	step [37/147], loss=78.0593
	step [38/147], loss=72.7628
	step [39/147], loss=77.8702
	step [40/147], loss=77.3995
	step [41/147], loss=82.2889
	step [42/147], loss=83.3849
	step [43/147], loss=94.3289
	step [44/147], loss=77.5811
	step [45/147], loss=79.0740
	step [46/147], loss=77.1535
	step [47/147], loss=81.3088
	step [48/147], loss=77.2150
	step [49/147], loss=88.3531
	step [50/147], loss=76.2419
	step [51/147], loss=75.3883
	step [52/147], loss=69.2217
	step [53/147], loss=88.0096
	step [54/147], loss=82.5498
	step [55/147], loss=75.8145
	step [56/147], loss=77.6447
	step [57/147], loss=80.9456
	step [58/147], loss=82.0340
	step [59/147], loss=87.4261
	step [60/147], loss=77.9694
	step [61/147], loss=88.9478
	step [62/147], loss=83.3712
	step [63/147], loss=76.7780
	step [64/147], loss=76.4819
	step [65/147], loss=76.3543
	step [66/147], loss=81.2581
	step [67/147], loss=71.9678
	step [68/147], loss=61.4772
	step [69/147], loss=69.3210
	step [70/147], loss=77.5778
	step [71/147], loss=75.2300
	step [72/147], loss=79.3235
	step [73/147], loss=65.7105
	step [74/147], loss=82.6876
	step [75/147], loss=85.3842
	step [76/147], loss=87.4153
	step [77/147], loss=59.6009
	step [78/147], loss=82.1775
	step [79/147], loss=82.6602
	step [80/147], loss=84.9216
	step [81/147], loss=86.3937
	step [82/147], loss=74.0522
	step [83/147], loss=78.5829
	step [84/147], loss=83.7626
	step [85/147], loss=83.9587
	step [86/147], loss=82.0276
	step [87/147], loss=97.7531
	step [88/147], loss=85.6868
	step [89/147], loss=89.7645
	step [90/147], loss=80.0122
	step [91/147], loss=72.5993
	step [92/147], loss=74.7502
	step [93/147], loss=88.0923
	step [94/147], loss=74.8047
	step [95/147], loss=71.2771
	step [96/147], loss=91.8427
	step [97/147], loss=72.2394
	step [98/147], loss=82.1296
	step [99/147], loss=85.8972
	step [100/147], loss=95.2885
	step [101/147], loss=79.5009
	step [102/147], loss=75.4257
	step [103/147], loss=76.6989
	step [104/147], loss=61.0727
	step [105/147], loss=92.7869
	step [106/147], loss=88.4440
	step [107/147], loss=84.7008
	step [108/147], loss=74.8500
	step [109/147], loss=95.9944
	step [110/147], loss=64.7750
	step [111/147], loss=86.6339
	step [112/147], loss=82.5452
	step [113/147], loss=76.4936
	step [114/147], loss=67.0069
	step [115/147], loss=69.4838
	step [116/147], loss=78.3829
	step [117/147], loss=74.1946
	step [118/147], loss=69.8626
	step [119/147], loss=71.6142
	step [120/147], loss=85.1837
	step [121/147], loss=89.5159
	step [122/147], loss=71.8012
	step [123/147], loss=82.2354
	step [124/147], loss=68.1013
	step [125/147], loss=85.5585
	step [126/147], loss=96.0536
	step [127/147], loss=94.0770
	step [128/147], loss=73.4364
	step [129/147], loss=83.9876
	step [130/147], loss=58.5968
	step [131/147], loss=73.5509
	step [132/147], loss=85.3467
	step [133/147], loss=78.0947
	step [134/147], loss=60.4723
	step [135/147], loss=89.0755
	step [136/147], loss=80.9427
	step [137/147], loss=79.0389
	step [138/147], loss=65.2973
	step [139/147], loss=65.3778
	step [140/147], loss=68.0540
	step [141/147], loss=82.5645
	step [142/147], loss=83.6574
	step [143/147], loss=69.1548
	step [144/147], loss=83.7617
	step [145/147], loss=79.4798
	step [146/147], loss=92.3008
	step [147/147], loss=21.5983
	Evaluating
	loss=0.0071, precision=0.4672, recall=0.8362, f1=0.5995
Training finished
best_f1: 0.6075692855671677
directing: X rim_enhanced: True test_id 4
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 10226 # image files with weight 10226
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
# all image files: 12930 # all weight files in weight_dir: 1657 # image files with weight 1657
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples/X 10226
Using 4 GPUs
Going to train epochs [49-98]
Training epoch 49
	step [1/160], loss=96.4121
	step [2/160], loss=89.4227
	step [3/160], loss=89.4403
	step [4/160], loss=92.7750
	step [5/160], loss=112.3451
	step [6/160], loss=106.2387
	step [7/160], loss=96.3006
	step [8/160], loss=95.4022
	step [9/160], loss=92.9618
	step [10/160], loss=92.2453
	step [11/160], loss=94.5377
	step [12/160], loss=111.8580
	step [13/160], loss=80.3253
	step [14/160], loss=105.1731
	step [15/160], loss=90.9328
	step [16/160], loss=86.9091
	step [17/160], loss=100.3017
	step [18/160], loss=100.6520
	step [19/160], loss=97.8230
	step [20/160], loss=90.7736
	step [21/160], loss=96.1490
	step [22/160], loss=90.3710
	step [23/160], loss=81.3537
	step [24/160], loss=85.9279
	step [25/160], loss=94.6825
	step [26/160], loss=88.3119
	step [27/160], loss=85.6556
	step [28/160], loss=104.1455
	step [29/160], loss=105.6062
	step [30/160], loss=101.9471
	step [31/160], loss=90.0123
	step [32/160], loss=84.1782
	step [33/160], loss=84.9923
	step [34/160], loss=92.5821
	step [35/160], loss=92.3157
	step [36/160], loss=100.2691
	step [37/160], loss=99.3103
	step [38/160], loss=82.6114
	step [39/160], loss=102.0788
	step [40/160], loss=91.4701
	step [41/160], loss=96.4661
	step [42/160], loss=98.8649
	step [43/160], loss=95.6824
	step [44/160], loss=105.7456
	step [45/160], loss=98.0276
	step [46/160], loss=87.7023
	step [47/160], loss=105.1002
	step [48/160], loss=94.7253
	step [49/160], loss=94.1823
	step [50/160], loss=98.2051
	step [51/160], loss=100.8624
	step [52/160], loss=102.3168
	step [53/160], loss=95.5835
	step [54/160], loss=90.6367
	step [55/160], loss=82.9544
	step [56/160], loss=88.2014
	step [57/160], loss=107.5570
	step [58/160], loss=98.5318
	step [59/160], loss=102.2909
	step [60/160], loss=111.6493
	step [61/160], loss=115.3870
	step [62/160], loss=84.6724
	step [63/160], loss=114.6314
	step [64/160], loss=95.8100
	step [65/160], loss=109.2432
	step [66/160], loss=88.3053
	step [67/160], loss=94.4406
	step [68/160], loss=85.6990
	step [69/160], loss=103.9010
	step [70/160], loss=95.1644
	step [71/160], loss=99.5008
	step [72/160], loss=98.2373
	step [73/160], loss=115.5385
	step [74/160], loss=86.6528
	step [75/160], loss=94.8525
	step [76/160], loss=86.4824
	step [77/160], loss=90.6869
	step [78/160], loss=85.1883
	step [79/160], loss=107.2077
	step [80/160], loss=93.3489
	step [81/160], loss=82.0531
	step [82/160], loss=89.1708
	step [83/160], loss=103.4782
	step [84/160], loss=96.8434
	step [85/160], loss=70.6752
	step [86/160], loss=78.0942
	step [87/160], loss=88.9121
	step [88/160], loss=101.8307
	step [89/160], loss=108.9595
	step [90/160], loss=87.4015
	step [91/160], loss=96.0986
	step [92/160], loss=87.2675
	step [93/160], loss=100.7778
	step [94/160], loss=111.6146
	step [95/160], loss=105.7128
	step [96/160], loss=106.5607
	step [97/160], loss=82.7268
	step [98/160], loss=89.8744
	step [99/160], loss=97.4044
	step [100/160], loss=97.9535
	step [101/160], loss=93.7895
	step [102/160], loss=111.0049
	step [103/160], loss=83.6324
	step [104/160], loss=91.2927
	step [105/160], loss=89.7498
	step [106/160], loss=88.7301
	step [107/160], loss=91.8106
	step [108/160], loss=91.1528
	step [109/160], loss=91.8702
	step [110/160], loss=98.6777
	step [111/160], loss=90.6728
	step [112/160], loss=78.6920
	step [113/160], loss=98.7664
	step [114/160], loss=101.1974
	step [115/160], loss=90.0306
	step [116/160], loss=103.2169
	step [117/160], loss=100.5343
	step [118/160], loss=110.5060
	step [119/160], loss=88.6374
	step [120/160], loss=94.5752
	step [121/160], loss=98.2311
	step [122/160], loss=95.2151
	step [123/160], loss=93.7775
	step [124/160], loss=101.3653
	step [125/160], loss=102.4196
	step [126/160], loss=104.5366
	step [127/160], loss=93.7533
	step [128/160], loss=103.3098
	step [129/160], loss=81.2221
	step [130/160], loss=97.6444
	step [131/160], loss=75.1980
	step [132/160], loss=74.3125
	step [133/160], loss=90.2569
	step [134/160], loss=102.6857
	step [135/160], loss=98.1252
	step [136/160], loss=98.3634
	step [137/160], loss=95.6820
	step [138/160], loss=99.6250
	step [139/160], loss=102.6771
	step [140/160], loss=105.8676
	step [141/160], loss=89.5714
	step [142/160], loss=88.5053
	step [143/160], loss=105.0572
	step [144/160], loss=120.6074
	step [145/160], loss=93.5207
	step [146/160], loss=94.6701
	step [147/160], loss=101.2616
	step [148/160], loss=70.2056
	step [149/160], loss=93.0055
	step [150/160], loss=95.6633
	step [151/160], loss=111.4260
	step [152/160], loss=84.8322
	step [153/160], loss=96.6518
	step [154/160], loss=104.3075
	step [155/160], loss=88.8830
	step [156/160], loss=101.6475
	step [157/160], loss=97.5891
	step [158/160], loss=96.5953
	step [159/160], loss=111.2493
	step [160/160], loss=73.7034
	Evaluating
	loss=0.0198, precision=0.2495, recall=0.9067, f1=0.3913
saving model as: 4_saved_model.pth
Training epoch 50
	step [1/160], loss=118.2185
	step [2/160], loss=101.9867
	step [3/160], loss=88.3086
	step [4/160], loss=97.0452
	step [5/160], loss=92.6016
	step [6/160], loss=110.1257
	step [7/160], loss=75.6788
	step [8/160], loss=77.3049
	step [9/160], loss=90.4214
	step [10/160], loss=102.6569
	step [11/160], loss=99.4496
	step [12/160], loss=89.8917
	step [13/160], loss=92.1840
	step [14/160], loss=96.2163
	step [15/160], loss=81.8874
	step [16/160], loss=114.4138
	step [17/160], loss=85.3686
	step [18/160], loss=114.6130
	step [19/160], loss=101.6601
	step [20/160], loss=92.3297
	step [21/160], loss=100.4458
	step [22/160], loss=98.5105
	step [23/160], loss=97.7755
	step [24/160], loss=96.9517
	step [25/160], loss=103.8925
	step [26/160], loss=76.9970
	step [27/160], loss=97.5440
	step [28/160], loss=86.9036
	step [29/160], loss=83.8848
	step [30/160], loss=88.7455
	step [31/160], loss=95.5768
	step [32/160], loss=80.5511
	step [33/160], loss=81.8393
	step [34/160], loss=87.6161
	step [35/160], loss=91.5042
	step [36/160], loss=106.2647
	step [37/160], loss=89.3782
	step [38/160], loss=104.1542
	step [39/160], loss=105.2879
	step [40/160], loss=104.6152
	step [41/160], loss=99.8427
	step [42/160], loss=94.8109
	step [43/160], loss=102.7729
	step [44/160], loss=89.7795
	step [45/160], loss=84.0931
	step [46/160], loss=104.4551
	step [47/160], loss=65.9353
	step [48/160], loss=83.1856
	step [49/160], loss=98.0059
	step [50/160], loss=79.9753
	step [51/160], loss=105.1019
	step [52/160], loss=77.1215
	step [53/160], loss=103.7869
	step [54/160], loss=96.4236
	step [55/160], loss=100.8472
	step [56/160], loss=110.2299
	step [57/160], loss=89.9727
	step [58/160], loss=101.7496
	step [59/160], loss=97.9934
	step [60/160], loss=100.2852
	step [61/160], loss=103.9638
	step [62/160], loss=84.5147
	step [63/160], loss=110.5340
	step [64/160], loss=83.3015
	step [65/160], loss=88.1932
	step [66/160], loss=91.5929
	step [67/160], loss=93.9924
	step [68/160], loss=91.5242
	step [69/160], loss=98.6188
	step [70/160], loss=74.2597
	step [71/160], loss=86.3598
	step [72/160], loss=103.2737
	step [73/160], loss=92.6462
	step [74/160], loss=96.6521
	step [75/160], loss=83.0088
	step [76/160], loss=104.2194
	step [77/160], loss=101.2804
	step [78/160], loss=89.3095
	step [79/160], loss=87.2679
	step [80/160], loss=89.4704
	step [81/160], loss=91.9443
	step [82/160], loss=102.7063
	step [83/160], loss=103.6465
	step [84/160], loss=83.1765
	step [85/160], loss=101.5969
	step [86/160], loss=94.2041
	step [87/160], loss=96.4977
	step [88/160], loss=90.4285
	step [89/160], loss=90.0318
	step [90/160], loss=108.8961
	step [91/160], loss=110.1927
	step [92/160], loss=104.1872
	step [93/160], loss=112.5812
	step [94/160], loss=107.4445
	step [95/160], loss=95.3849
	step [96/160], loss=89.6362
	step [97/160], loss=71.7766
	step [98/160], loss=84.8277
	step [99/160], loss=92.5617
	step [100/160], loss=95.0956
	step [101/160], loss=105.6605
	step [102/160], loss=96.0107
	step [103/160], loss=96.9803
	step [104/160], loss=100.6243
	step [105/160], loss=79.6248
	step [106/160], loss=88.6316
	step [107/160], loss=92.8944
	step [108/160], loss=93.1739
	step [109/160], loss=106.6775
	step [110/160], loss=113.3768
	step [111/160], loss=94.0807
	step [112/160], loss=86.7853
	step [113/160], loss=97.1147
	step [114/160], loss=109.8384
	step [115/160], loss=104.9804
	step [116/160], loss=84.0421
	step [117/160], loss=95.8388
	step [118/160], loss=89.2327
	step [119/160], loss=105.5951
	step [120/160], loss=100.4096
	step [121/160], loss=101.8817
	step [122/160], loss=106.4243
	step [123/160], loss=95.4673
	step [124/160], loss=99.7948
	step [125/160], loss=107.7483
	step [126/160], loss=89.6564
	step [127/160], loss=90.4365
	step [128/160], loss=93.2513
	step [129/160], loss=81.9788
	step [130/160], loss=95.8767
	step [131/160], loss=84.3881
	step [132/160], loss=84.4466
	step [133/160], loss=81.8624
	step [134/160], loss=86.6354
	step [135/160], loss=81.4239
	step [136/160], loss=96.3447
	step [137/160], loss=92.5761
	step [138/160], loss=92.4470
	step [139/160], loss=92.2912
	step [140/160], loss=105.2394
	step [141/160], loss=115.0783
	step [142/160], loss=107.8733
	step [143/160], loss=97.4459
	step [144/160], loss=78.9883
	step [145/160], loss=92.7176
	step [146/160], loss=99.0140
	step [147/160], loss=101.9830
	step [148/160], loss=106.3030
	step [149/160], loss=97.9759
	step [150/160], loss=99.3671
	step [151/160], loss=99.3585
	step [152/160], loss=97.0533
	step [153/160], loss=90.9270
	step [154/160], loss=102.0072
	step [155/160], loss=83.7827
	step [156/160], loss=115.7996
	step [157/160], loss=102.6808
	step [158/160], loss=63.1978
	step [159/160], loss=84.2304
	step [160/160], loss=83.6784
	Evaluating
	loss=0.0164, precision=0.3052, recall=0.8952, f1=0.4552
saving model as: 4_saved_model.pth
Training epoch 51
	step [1/160], loss=89.3439
	step [2/160], loss=90.7000
	step [3/160], loss=90.8312
	step [4/160], loss=107.5825
	step [5/160], loss=95.8458
	step [6/160], loss=107.3516
	step [7/160], loss=92.5321
	step [8/160], loss=94.9661
	step [9/160], loss=81.6593
	step [10/160], loss=90.3521
	step [11/160], loss=100.7713
	step [12/160], loss=81.6031
	step [13/160], loss=82.3359
	step [14/160], loss=94.2821
	step [15/160], loss=89.4222
	step [16/160], loss=114.0302
	step [17/160], loss=93.3399
	step [18/160], loss=99.6623
	step [19/160], loss=90.2428
	step [20/160], loss=87.1130
	step [21/160], loss=102.0922
	step [22/160], loss=113.8760
	step [23/160], loss=87.5379
	step [24/160], loss=81.1339
	step [25/160], loss=85.5760
	step [26/160], loss=92.0051
	step [27/160], loss=111.2984
	step [28/160], loss=87.3420
	step [29/160], loss=82.5395
	step [30/160], loss=84.8841
	step [31/160], loss=104.4190
	step [32/160], loss=85.9489
	step [33/160], loss=96.4998
	step [34/160], loss=102.3033
	step [35/160], loss=97.9877
	step [36/160], loss=87.4321
	step [37/160], loss=83.6317
	step [38/160], loss=111.2109
	step [39/160], loss=89.2936
	step [40/160], loss=87.0571
	step [41/160], loss=91.0100
	step [42/160], loss=109.3002
	step [43/160], loss=90.8393
	step [44/160], loss=102.0912
	step [45/160], loss=80.7007
	step [46/160], loss=95.0818
	step [47/160], loss=86.5338
	step [48/160], loss=86.1462
	step [49/160], loss=104.2475
	step [50/160], loss=91.4194
	step [51/160], loss=101.4194
	step [52/160], loss=88.0237
	step [53/160], loss=95.0056
	step [54/160], loss=84.3094
	step [55/160], loss=90.1736
	step [56/160], loss=90.5619
	step [57/160], loss=97.6001
	step [58/160], loss=89.7112
	step [59/160], loss=84.3660
	step [60/160], loss=75.6670
	step [61/160], loss=88.1440
	step [62/160], loss=78.7129
	step [63/160], loss=82.2884
	step [64/160], loss=87.8623
	step [65/160], loss=81.8847
	step [66/160], loss=73.5332
	step [67/160], loss=85.6414
	step [68/160], loss=87.3463
	step [69/160], loss=107.4890
	step [70/160], loss=91.7950
	step [71/160], loss=103.2805
	step [72/160], loss=94.3459
	step [73/160], loss=93.5672
	step [74/160], loss=97.4911
	step [75/160], loss=104.9454
	step [76/160], loss=114.6773
	step [77/160], loss=111.3458
	step [78/160], loss=102.9916
	step [79/160], loss=72.2304
	step [80/160], loss=97.7488
	step [81/160], loss=76.9142
	step [82/160], loss=83.5566
	step [83/160], loss=93.3939
	step [84/160], loss=85.7062
	step [85/160], loss=93.7605
	step [86/160], loss=86.1443
	step [87/160], loss=107.8475
	step [88/160], loss=112.9608
	step [89/160], loss=93.3632
	step [90/160], loss=94.5491
	step [91/160], loss=92.0706
	step [92/160], loss=97.3305
	step [93/160], loss=89.2385
	step [94/160], loss=98.8928
	step [95/160], loss=112.7220
	step [96/160], loss=110.6126
	step [97/160], loss=101.5935
	step [98/160], loss=119.8553
	step [99/160], loss=85.0871
	step [100/160], loss=101.4462
	step [101/160], loss=114.9055
	step [102/160], loss=112.0259
	step [103/160], loss=99.5845
	step [104/160], loss=106.3123
	step [105/160], loss=99.1910
	step [106/160], loss=94.2846
	step [107/160], loss=99.4834
	step [108/160], loss=108.3705
	step [109/160], loss=102.5463
	step [110/160], loss=81.8214
	step [111/160], loss=83.0043
	step [112/160], loss=92.9348
	step [113/160], loss=91.0282
	step [114/160], loss=96.8412
	step [115/160], loss=91.9496
	step [116/160], loss=100.1959
	step [117/160], loss=95.4394
	step [118/160], loss=93.7479
	step [119/160], loss=94.2212
	step [120/160], loss=85.4035
	step [121/160], loss=94.5634
	step [122/160], loss=104.0169
	step [123/160], loss=109.0042
	step [124/160], loss=90.8029
	step [125/160], loss=108.9700
	step [126/160], loss=104.5821
	step [127/160], loss=107.6301
	step [128/160], loss=94.1448
	step [129/160], loss=104.4437
	step [130/160], loss=91.3687
	step [131/160], loss=98.2576
	step [132/160], loss=90.4307
	step [133/160], loss=88.2964
	step [134/160], loss=99.5178
	step [135/160], loss=91.2673
	step [136/160], loss=79.8692
	step [137/160], loss=94.1069
	step [138/160], loss=92.3823
	step [139/160], loss=91.5112
	step [140/160], loss=90.4638
	step [141/160], loss=111.6015
	step [142/160], loss=100.3932
	step [143/160], loss=100.2896
	step [144/160], loss=95.7077
	step [145/160], loss=97.2140
	step [146/160], loss=96.9515
	step [147/160], loss=93.1933
	step [148/160], loss=96.9864
	step [149/160], loss=81.0555
	step [150/160], loss=98.2252
	step [151/160], loss=90.0916
	step [152/160], loss=88.6401
	step [153/160], loss=93.6475
	step [154/160], loss=93.9850
	step [155/160], loss=96.0121
	step [156/160], loss=84.0976
	step [157/160], loss=79.9865
	step [158/160], loss=80.8620
	step [159/160], loss=88.1633
	step [160/160], loss=83.2377
	Evaluating
	loss=0.0171, precision=0.2893, recall=0.8999, f1=0.4378
Training epoch 52
	step [1/160], loss=78.7896
	step [2/160], loss=100.5661
	step [3/160], loss=95.2258
	step [4/160], loss=78.6567
	step [5/160], loss=109.1193
	step [6/160], loss=113.1279
	step [7/160], loss=95.4893
	step [8/160], loss=101.2181
	step [9/160], loss=91.2194
	step [10/160], loss=98.7123
	step [11/160], loss=83.7716
	step [12/160], loss=73.2458
	step [13/160], loss=94.3438
	step [14/160], loss=78.7885
	step [15/160], loss=110.9939
	step [16/160], loss=97.4311
	step [17/160], loss=87.1578
	step [18/160], loss=111.1236
	step [19/160], loss=90.8268
	step [20/160], loss=91.8419
	step [21/160], loss=99.6236
	step [22/160], loss=85.4716
	step [23/160], loss=95.0047
	step [24/160], loss=95.9789
	step [25/160], loss=97.3681
	step [26/160], loss=86.1778
	step [27/160], loss=117.7379
	step [28/160], loss=88.4618
	step [29/160], loss=90.5967
	step [30/160], loss=90.7902
	step [31/160], loss=90.4206
	step [32/160], loss=105.6897
	step [33/160], loss=108.0242
	step [34/160], loss=115.3186
	step [35/160], loss=91.6970
	step [36/160], loss=94.5426
	step [37/160], loss=100.4861
	step [38/160], loss=90.2030
	step [39/160], loss=92.1998
	step [40/160], loss=92.5856
	step [41/160], loss=96.4969
	step [42/160], loss=85.2534
	step [43/160], loss=91.4123
	step [44/160], loss=95.6497
	step [45/160], loss=86.9409
	step [46/160], loss=94.0568
	step [47/160], loss=93.1729
	step [48/160], loss=100.6478
	step [49/160], loss=107.6205
	step [50/160], loss=81.4112
	step [51/160], loss=89.1714
	step [52/160], loss=90.5762
	step [53/160], loss=82.0738
	step [54/160], loss=108.0381
	step [55/160], loss=102.5334
	step [56/160], loss=103.7344
	step [57/160], loss=85.7373
	step [58/160], loss=112.3487
	step [59/160], loss=107.3210
	step [60/160], loss=104.6578
	step [61/160], loss=88.5601
	step [62/160], loss=99.0314
	step [63/160], loss=99.7219
	step [64/160], loss=97.8021
	step [65/160], loss=96.4570
	step [66/160], loss=98.0490
	step [67/160], loss=99.2214
	step [68/160], loss=91.0998
	step [69/160], loss=86.2082
	step [70/160], loss=92.8802
	step [71/160], loss=84.8916
	step [72/160], loss=89.8560
	step [73/160], loss=110.6726
	step [74/160], loss=97.5165
	step [75/160], loss=82.0698
	step [76/160], loss=79.0847
	step [77/160], loss=79.9997
	step [78/160], loss=80.4998
	step [79/160], loss=91.4434
	step [80/160], loss=84.8454
	step [81/160], loss=93.3443
	step [82/160], loss=103.2926
	step [83/160], loss=93.6109
	step [84/160], loss=82.8739
	step [85/160], loss=87.1958
	step [86/160], loss=74.5314
	step [87/160], loss=79.2006
	step [88/160], loss=85.4364
	step [89/160], loss=92.0195
	step [90/160], loss=90.9604
	step [91/160], loss=93.7842
	step [92/160], loss=99.9472
	step [93/160], loss=88.2331
	step [94/160], loss=86.0514
	step [95/160], loss=80.8189
	step [96/160], loss=83.0277
	step [97/160], loss=92.7128
	step [98/160], loss=76.0910
	step [99/160], loss=106.0903
	step [100/160], loss=114.1067
	step [101/160], loss=89.7805
	step [102/160], loss=94.1187
	step [103/160], loss=91.8140
	step [104/160], loss=99.9387
	step [105/160], loss=79.4158
	step [106/160], loss=87.6187
	step [107/160], loss=103.2978
	step [108/160], loss=100.8214
	step [109/160], loss=98.1104
	step [110/160], loss=78.8771
	step [111/160], loss=81.1938
	step [112/160], loss=86.6829
	step [113/160], loss=106.4293
	step [114/160], loss=90.6392
	step [115/160], loss=103.2928
	step [116/160], loss=93.2044
	step [117/160], loss=91.4280
	step [118/160], loss=86.4507
	step [119/160], loss=103.6023
	step [120/160], loss=73.2232
	step [121/160], loss=95.4663
	step [122/160], loss=98.5599
	step [123/160], loss=90.1610
	step [124/160], loss=87.9044
	step [125/160], loss=123.6614
	step [126/160], loss=103.8588
	step [127/160], loss=94.7205
	step [128/160], loss=94.0205
	step [129/160], loss=89.2495
	step [130/160], loss=108.1218
	step [131/160], loss=81.8567
	step [132/160], loss=115.2189
	step [133/160], loss=92.6314
	step [134/160], loss=95.4492
	step [135/160], loss=99.6916
	step [136/160], loss=92.8734
	step [137/160], loss=108.6195
	step [138/160], loss=87.8230
	step [139/160], loss=101.3540
	step [140/160], loss=101.8362
	step [141/160], loss=85.9834
	step [142/160], loss=98.7243
	step [143/160], loss=98.0911
	step [144/160], loss=92.6836
	step [145/160], loss=88.6508
	step [146/160], loss=93.2488
	step [147/160], loss=96.3616
	step [148/160], loss=91.0258
	step [149/160], loss=89.0761
	step [150/160], loss=107.3721
	step [151/160], loss=94.9981
	step [152/160], loss=96.8440
	step [153/160], loss=94.9837
	step [154/160], loss=90.2387
	step [155/160], loss=90.3315
	step [156/160], loss=99.4409
	step [157/160], loss=103.0683
	step [158/160], loss=79.3256
	step [159/160], loss=89.3347
	step [160/160], loss=54.7698
	Evaluating
	loss=0.0167, precision=0.2951, recall=0.9005, f1=0.4445
Training epoch 53
	step [1/160], loss=108.9111
	step [2/160], loss=95.1534
	step [3/160], loss=107.4457
	step [4/160], loss=91.8952
	step [5/160], loss=81.8175
	step [6/160], loss=85.6127
	step [7/160], loss=104.6401
	step [8/160], loss=92.6037
	step [9/160], loss=96.9178
	step [10/160], loss=98.4236
	step [11/160], loss=92.6190
	step [12/160], loss=94.1140
	step [13/160], loss=104.7662
	step [14/160], loss=96.2585
	step [15/160], loss=94.1030
	step [16/160], loss=97.4673
	step [17/160], loss=81.8687
	step [18/160], loss=78.6543
	step [19/160], loss=90.8975
	step [20/160], loss=86.2556
	step [21/160], loss=102.6476
	step [22/160], loss=96.6203
	step [23/160], loss=91.7713
	step [24/160], loss=108.0871
	step [25/160], loss=87.7224
	step [26/160], loss=90.5376
	step [27/160], loss=81.9339
	step [28/160], loss=96.1454
	step [29/160], loss=91.0728
	step [30/160], loss=102.1675
	step [31/160], loss=94.2903
	step [32/160], loss=78.3694
	step [33/160], loss=93.3307
	step [34/160], loss=89.7816
	step [35/160], loss=108.8250
	step [36/160], loss=97.1459
	step [37/160], loss=72.1786
	step [38/160], loss=96.9178
	step [39/160], loss=93.7529
	step [40/160], loss=82.8099
	step [41/160], loss=80.8441
	step [42/160], loss=98.0661
	step [43/160], loss=96.0954
	step [44/160], loss=87.7505
	step [45/160], loss=89.1106
	step [46/160], loss=96.7477
	step [47/160], loss=86.1877
	step [48/160], loss=93.9334
	step [49/160], loss=111.9774
	step [50/160], loss=77.5454
	step [51/160], loss=93.5848
	step [52/160], loss=103.4846
	step [53/160], loss=87.3919
	step [54/160], loss=107.5588
	step [55/160], loss=97.1166
	step [56/160], loss=100.7287
	step [57/160], loss=94.6907
	step [58/160], loss=97.7968
	step [59/160], loss=107.2286
	step [60/160], loss=100.9331
	step [61/160], loss=87.3241
	step [62/160], loss=84.9148
	step [63/160], loss=93.6384
	step [64/160], loss=83.5609
	step [65/160], loss=99.1021
	step [66/160], loss=76.0480
	step [67/160], loss=85.6112
	step [68/160], loss=89.0628
	step [69/160], loss=103.9987
	step [70/160], loss=98.6838
	step [71/160], loss=90.8760
	step [72/160], loss=73.7556
	step [73/160], loss=82.9818
	step [74/160], loss=96.7581
	step [75/160], loss=93.3144
	step [76/160], loss=89.4007
	step [77/160], loss=77.9417
	step [78/160], loss=82.1165
	step [79/160], loss=92.4186
	step [80/160], loss=98.8209
	step [81/160], loss=108.4214
	step [82/160], loss=108.2536
	step [83/160], loss=101.8933
	step [84/160], loss=93.6504
	step [85/160], loss=95.3253
	step [86/160], loss=88.2286
	step [87/160], loss=83.3639
	step [88/160], loss=95.4927
	step [89/160], loss=95.5256
	step [90/160], loss=88.9599
	step [91/160], loss=88.2552
	step [92/160], loss=84.7941
	step [93/160], loss=91.2196
	step [94/160], loss=85.5454
	step [95/160], loss=79.4554
	step [96/160], loss=109.1498
	step [97/160], loss=89.0307
	step [98/160], loss=92.8710
	step [99/160], loss=95.2139
	step [100/160], loss=88.3434
	step [101/160], loss=103.1235
	step [102/160], loss=82.0253
	step [103/160], loss=96.5327
	step [104/160], loss=95.3594
	step [105/160], loss=87.6131
	step [106/160], loss=92.7798
	step [107/160], loss=95.8710
	step [108/160], loss=92.8745
	step [109/160], loss=106.4984
	step [110/160], loss=102.2906
	step [111/160], loss=107.2543
	step [112/160], loss=95.8082
	step [113/160], loss=97.2212
	step [114/160], loss=96.5803
	step [115/160], loss=88.0969
	step [116/160], loss=101.2582
	step [117/160], loss=93.5492
	step [118/160], loss=93.2846
	step [119/160], loss=89.0705
	step [120/160], loss=83.1770
	step [121/160], loss=93.0832
	step [122/160], loss=98.0382
	step [123/160], loss=87.1809
	step [124/160], loss=86.2429
	step [125/160], loss=103.4354
	step [126/160], loss=80.1977
	step [127/160], loss=100.7463
	step [128/160], loss=102.1713
	step [129/160], loss=71.4017
	step [130/160], loss=90.3585
	step [131/160], loss=102.7765
	step [132/160], loss=85.4134
	step [133/160], loss=89.7379
	step [134/160], loss=90.8350
	step [135/160], loss=75.4146
	step [136/160], loss=87.9588
	step [137/160], loss=97.5871
	step [138/160], loss=94.7469
	step [139/160], loss=94.6130
	step [140/160], loss=100.0335
	step [141/160], loss=99.3504
	step [142/160], loss=112.3232
	step [143/160], loss=97.6395
	step [144/160], loss=97.0764
	step [145/160], loss=92.9977
	step [146/160], loss=92.0612
	step [147/160], loss=99.8094
	step [148/160], loss=95.8058
	step [149/160], loss=92.6140
	step [150/160], loss=84.6066
	step [151/160], loss=98.3021
	step [152/160], loss=97.6822
	step [153/160], loss=95.1206
	step [154/160], loss=101.9372
	step [155/160], loss=93.2979
	step [156/160], loss=111.4794
	step [157/160], loss=79.5743
	step [158/160], loss=103.1184
	step [159/160], loss=96.0240
	step [160/160], loss=73.6625
	Evaluating
	loss=0.0177, precision=0.2745, recall=0.9035, f1=0.4210
Training epoch 54
	step [1/160], loss=87.1863
	step [2/160], loss=85.8503
	step [3/160], loss=77.8522
	step [4/160], loss=83.2233
	step [5/160], loss=84.7873
	step [6/160], loss=89.1993
	step [7/160], loss=87.5118
	step [8/160], loss=92.7363
	step [9/160], loss=77.9971
	step [10/160], loss=109.6184
	step [11/160], loss=98.2798
	step [12/160], loss=97.3632
	step [13/160], loss=108.3221
	step [14/160], loss=89.3636
	step [15/160], loss=93.0637
	step [16/160], loss=92.0879
	step [17/160], loss=87.9994
	step [18/160], loss=91.6003
	step [19/160], loss=101.3520
	step [20/160], loss=87.1267
	step [21/160], loss=106.7879
	step [22/160], loss=89.1470
	step [23/160], loss=80.1306
	step [24/160], loss=96.5101
	step [25/160], loss=95.0926
	step [26/160], loss=94.2696
	step [27/160], loss=109.2727
	step [28/160], loss=104.2570
	step [29/160], loss=95.6388
	step [30/160], loss=100.7815
	step [31/160], loss=87.7196
	step [32/160], loss=83.3441
	step [33/160], loss=87.3864
	step [34/160], loss=121.0793
	step [35/160], loss=100.3254
	step [36/160], loss=89.6290
	step [37/160], loss=94.2256
	step [38/160], loss=92.5874
	step [39/160], loss=102.1814
	step [40/160], loss=89.0149
	step [41/160], loss=102.5406
	step [42/160], loss=90.3339
	step [43/160], loss=115.3523
	step [44/160], loss=100.7931
	step [45/160], loss=98.1491
	step [46/160], loss=109.4993
	step [47/160], loss=93.4360
	step [48/160], loss=99.5519
	step [49/160], loss=97.1275
	step [50/160], loss=97.6647
	step [51/160], loss=97.4211
	step [52/160], loss=84.6316
	step [53/160], loss=82.6242
	step [54/160], loss=85.9304
	step [55/160], loss=94.5449
	step [56/160], loss=87.8556
	step [57/160], loss=77.9993
	step [58/160], loss=87.5225
	step [59/160], loss=107.4071
	step [60/160], loss=94.0806
	step [61/160], loss=98.8161
	step [62/160], loss=100.0311
	step [63/160], loss=97.2049
	step [64/160], loss=88.8031
	step [65/160], loss=78.9383
	step [66/160], loss=75.2899
	step [67/160], loss=104.6183
	step [68/160], loss=79.4105
	step [69/160], loss=91.4921
	step [70/160], loss=90.0379
	step [71/160], loss=91.1328
	step [72/160], loss=88.2211
	step [73/160], loss=110.7215
	step [74/160], loss=92.9378
	step [75/160], loss=96.3415
	step [76/160], loss=83.8125
	step [77/160], loss=86.9326
	step [78/160], loss=88.6156
	step [79/160], loss=121.1768
	step [80/160], loss=86.4845
	step [81/160], loss=78.4923
	step [82/160], loss=78.6372
	step [83/160], loss=124.0380
	step [84/160], loss=92.1637
	step [85/160], loss=79.2695
	step [86/160], loss=83.4384
	step [87/160], loss=85.3180
	step [88/160], loss=93.6920
	step [89/160], loss=94.8265
	step [90/160], loss=83.3360
	step [91/160], loss=82.3207
	step [92/160], loss=94.8706
	step [93/160], loss=86.5913
	step [94/160], loss=100.5144
	step [95/160], loss=98.6583
	step [96/160], loss=93.3510
	step [97/160], loss=82.6871
	step [98/160], loss=89.8071
	step [99/160], loss=102.1549
	step [100/160], loss=89.7094
	step [101/160], loss=109.3456
	step [102/160], loss=80.5043
	step [103/160], loss=95.0113
	step [104/160], loss=85.8332
	step [105/160], loss=105.9136
	step [106/160], loss=71.1630
	step [107/160], loss=96.9690
	step [108/160], loss=105.1044
	step [109/160], loss=83.0899
	step [110/160], loss=85.0341
	step [111/160], loss=90.6721
	step [112/160], loss=94.8626
	step [113/160], loss=87.1038
	step [114/160], loss=83.0283
	step [115/160], loss=99.1025
	step [116/160], loss=89.6949
	step [117/160], loss=90.2556
	step [118/160], loss=89.6266
	step [119/160], loss=91.1013
	step [120/160], loss=90.4290
	step [121/160], loss=91.7579
	step [122/160], loss=108.7633
	step [123/160], loss=86.5699
	step [124/160], loss=84.3681
	step [125/160], loss=96.3453
	step [126/160], loss=92.4116
	step [127/160], loss=85.7549
	step [128/160], loss=90.0597
	step [129/160], loss=92.2333
	step [130/160], loss=101.6202
	step [131/160], loss=109.1331
	step [132/160], loss=99.8846
	step [133/160], loss=94.7110
	step [134/160], loss=90.1240
	step [135/160], loss=90.6147
	step [136/160], loss=101.8159
	step [137/160], loss=89.8566
	step [138/160], loss=101.1505
	step [139/160], loss=83.8221
	step [140/160], loss=82.7855
	step [141/160], loss=106.8272
	step [142/160], loss=96.4765
	step [143/160], loss=88.3605
	step [144/160], loss=107.7226
	step [145/160], loss=100.1490
	step [146/160], loss=85.2849
	step [147/160], loss=96.5829
	step [148/160], loss=85.6565
	step [149/160], loss=82.5760
	step [150/160], loss=114.8822
	step [151/160], loss=79.5366
	step [152/160], loss=102.7143
	step [153/160], loss=87.2025
	step [154/160], loss=86.3506
	step [155/160], loss=101.0255
	step [156/160], loss=80.8793
	step [157/160], loss=79.5799
	step [158/160], loss=103.3247
	step [159/160], loss=93.9743
	step [160/160], loss=69.1197
	Evaluating
	loss=0.0160, precision=0.3043, recall=0.8975, f1=0.4545
Training epoch 55
	step [1/160], loss=99.4431
	step [2/160], loss=85.8205
	step [3/160], loss=86.1422
	step [4/160], loss=84.4066
	step [5/160], loss=85.6434
	step [6/160], loss=94.9393
	step [7/160], loss=89.5628
	step [8/160], loss=100.6713
	step [9/160], loss=109.2478
	step [10/160], loss=88.2977
	step [11/160], loss=96.1373
	step [12/160], loss=92.0119
	step [13/160], loss=110.3178
	step [14/160], loss=90.2283
	step [15/160], loss=94.2068
	step [16/160], loss=95.5652
	step [17/160], loss=92.5849
	step [18/160], loss=90.4625
	step [19/160], loss=95.0593
	step [20/160], loss=102.6544
	step [21/160], loss=80.6282
	step [22/160], loss=93.5766
	step [23/160], loss=111.7752
	step [24/160], loss=99.5970
	step [25/160], loss=83.8456
	step [26/160], loss=82.0278
	step [27/160], loss=99.9750
	step [28/160], loss=102.6932
	step [29/160], loss=79.4341
	step [30/160], loss=93.2596
	step [31/160], loss=84.4462
	step [32/160], loss=83.7629
	step [33/160], loss=90.6394
	step [34/160], loss=74.5004
	step [35/160], loss=93.5362
	step [36/160], loss=90.7044
	step [37/160], loss=85.4217
	step [38/160], loss=83.9677
	step [39/160], loss=98.0495
	step [40/160], loss=95.8704
	step [41/160], loss=99.6379
	step [42/160], loss=93.2999
	step [43/160], loss=98.3830
	step [44/160], loss=101.4255
	step [45/160], loss=91.2389
	step [46/160], loss=78.5865
	step [47/160], loss=95.7560
	step [48/160], loss=85.9230
	step [49/160], loss=92.0810
	step [50/160], loss=102.8026
	step [51/160], loss=102.6397
	step [52/160], loss=93.7700
	step [53/160], loss=104.2153
	step [54/160], loss=92.3091
	step [55/160], loss=100.8407
	step [56/160], loss=71.3479
	step [57/160], loss=123.7404
	step [58/160], loss=98.5001
	step [59/160], loss=96.6071
	step [60/160], loss=97.9678
	step [61/160], loss=102.9382
	step [62/160], loss=90.4240
	step [63/160], loss=86.4924
	step [64/160], loss=102.3381
	step [65/160], loss=97.9555
	step [66/160], loss=96.5106
	step [67/160], loss=112.8158
	step [68/160], loss=108.0556
	step [69/160], loss=112.7141
	step [70/160], loss=94.9028
	step [71/160], loss=104.6276
	step [72/160], loss=97.0335
	step [73/160], loss=80.1836
	step [74/160], loss=108.5476
	step [75/160], loss=83.4304
	step [76/160], loss=92.2839
	step [77/160], loss=93.3818
	step [78/160], loss=79.9010
	step [79/160], loss=77.5630
	step [80/160], loss=103.7416
	step [81/160], loss=95.5088
	step [82/160], loss=94.5455
	step [83/160], loss=97.4670
	step [84/160], loss=93.1308
	step [85/160], loss=93.2994
	step [86/160], loss=84.3756
	step [87/160], loss=72.2388
	step [88/160], loss=100.8350
	step [89/160], loss=78.2293
	step [90/160], loss=80.3174
	step [91/160], loss=88.2930
	step [92/160], loss=100.1831
	step [93/160], loss=88.6479
	step [94/160], loss=98.0027
	step [95/160], loss=79.2729
	step [96/160], loss=102.3323
	step [97/160], loss=101.3165
	step [98/160], loss=92.7732
	step [99/160], loss=74.1428
	step [100/160], loss=80.7411
	step [101/160], loss=75.9066
	step [102/160], loss=81.8185
	step [103/160], loss=87.9030
	step [104/160], loss=97.5444
	step [105/160], loss=86.7017
	step [106/160], loss=99.5022
	step [107/160], loss=102.3081
	step [108/160], loss=82.3452
	step [109/160], loss=96.1280
	step [110/160], loss=85.8734
	step [111/160], loss=87.8730
	step [112/160], loss=106.5995
	step [113/160], loss=87.1242
	step [114/160], loss=91.1672
	step [115/160], loss=109.5435
	step [116/160], loss=83.0584
	step [117/160], loss=99.0793
	step [118/160], loss=80.2894
	step [119/160], loss=69.2141
	step [120/160], loss=98.5131
	step [121/160], loss=78.2566
	step [122/160], loss=85.2298
	step [123/160], loss=89.0979
	step [124/160], loss=101.6696
	step [125/160], loss=89.6416
	step [126/160], loss=102.7362
	step [127/160], loss=84.2567
	step [128/160], loss=95.7982
	step [129/160], loss=88.8039
	step [130/160], loss=93.2808
	step [131/160], loss=86.1942
	step [132/160], loss=94.8462
	step [133/160], loss=112.6122
	step [134/160], loss=104.9070
	step [135/160], loss=104.5664
	step [136/160], loss=68.8540
	step [137/160], loss=80.6051
	step [138/160], loss=100.3052
	step [139/160], loss=98.0387
	step [140/160], loss=111.2097
	step [141/160], loss=91.1772
	step [142/160], loss=108.9200
	step [143/160], loss=82.0296
	step [144/160], loss=91.0754
	step [145/160], loss=102.6830
	step [146/160], loss=79.5473
	step [147/160], loss=86.2312
	step [148/160], loss=79.9693
	step [149/160], loss=72.7765
	step [150/160], loss=96.1376
	step [151/160], loss=112.7745
	step [152/160], loss=104.3103
	step [153/160], loss=86.2109
	step [154/160], loss=105.7360
	step [155/160], loss=91.5564
	step [156/160], loss=102.2080
	step [157/160], loss=99.3244
	step [158/160], loss=89.6658
	step [159/160], loss=82.7016
	step [160/160], loss=75.1003
	Evaluating
	loss=0.0115, precision=0.3872, recall=0.8861, f1=0.5389
saving model as: 4_saved_model.pth
Training epoch 56
	step [1/160], loss=82.8495
	step [2/160], loss=99.0379
	step [3/160], loss=87.7054
	step [4/160], loss=95.1156
	step [5/160], loss=92.9619
	step [6/160], loss=90.9260
	step [7/160], loss=99.7623
	step [8/160], loss=103.3519
	step [9/160], loss=78.3867
	step [10/160], loss=78.6929
	step [11/160], loss=97.7317
	step [12/160], loss=86.0173
	step [13/160], loss=92.2418
	step [14/160], loss=102.0076
	step [15/160], loss=95.5542
	step [16/160], loss=114.2929
	step [17/160], loss=112.7955
	step [18/160], loss=81.1326
	step [19/160], loss=103.8356
	step [20/160], loss=97.5179
	step [21/160], loss=107.6861
	step [22/160], loss=91.1035
	step [23/160], loss=97.9715
	step [24/160], loss=87.1044
	step [25/160], loss=85.0079
	step [26/160], loss=108.1260
	step [27/160], loss=80.7150
	step [28/160], loss=83.0919
	step [29/160], loss=88.7775
	step [30/160], loss=102.7070
	step [31/160], loss=111.5694
	step [32/160], loss=85.6125
	step [33/160], loss=96.4755
	step [34/160], loss=97.0538
	step [35/160], loss=81.7569
	step [36/160], loss=93.6133
	step [37/160], loss=91.7076
	step [38/160], loss=89.0559
	step [39/160], loss=103.0232
	step [40/160], loss=91.8112
	step [41/160], loss=89.9040
	step [42/160], loss=95.3606
	step [43/160], loss=101.7466
	step [44/160], loss=97.7841
	step [45/160], loss=106.8637
	step [46/160], loss=99.5478
	step [47/160], loss=81.4188
	step [48/160], loss=100.1275
	step [49/160], loss=95.3252
	step [50/160], loss=94.6436
	step [51/160], loss=85.1363
	step [52/160], loss=76.3082
	step [53/160], loss=87.4979
	step [54/160], loss=111.1902
	step [55/160], loss=88.6000
	step [56/160], loss=98.9367
	step [57/160], loss=97.6870
	step [58/160], loss=67.6278
	step [59/160], loss=95.0449
	step [60/160], loss=88.6908
	step [61/160], loss=101.2775
	step [62/160], loss=89.5297
	step [63/160], loss=85.7844
	step [64/160], loss=89.5618
	step [65/160], loss=82.5939
	step [66/160], loss=100.0909
	step [67/160], loss=109.5041
	step [68/160], loss=82.9754
	step [69/160], loss=92.1092
	step [70/160], loss=110.5190
	step [71/160], loss=94.9417
	step [72/160], loss=84.2242
	step [73/160], loss=104.0073
	step [74/160], loss=98.9398
	step [75/160], loss=97.3661
	step [76/160], loss=95.9248
	step [77/160], loss=98.2346
	step [78/160], loss=83.6088
	step [79/160], loss=93.0520
	step [80/160], loss=86.9377
	step [81/160], loss=85.0602
	step [82/160], loss=90.4963
	step [83/160], loss=95.5096
	step [84/160], loss=89.9522
	step [85/160], loss=90.2623
	step [86/160], loss=86.0334
	step [87/160], loss=97.1199
	step [88/160], loss=88.8946
	step [89/160], loss=90.8015
	step [90/160], loss=95.1717
	step [91/160], loss=91.6186
	step [92/160], loss=98.1408
	step [93/160], loss=81.9926
	step [94/160], loss=86.1991
	step [95/160], loss=80.6438
	step [96/160], loss=96.1382
	step [97/160], loss=78.2220
	step [98/160], loss=96.8741
	step [99/160], loss=90.4139
	step [100/160], loss=105.3560
	step [101/160], loss=94.2878
	step [102/160], loss=97.6992
	step [103/160], loss=104.5814
	step [104/160], loss=89.5546
	step [105/160], loss=92.1762
	step [106/160], loss=91.3991
	step [107/160], loss=97.3687
	step [108/160], loss=89.0060
	step [109/160], loss=100.3953
	step [110/160], loss=70.6857
	step [111/160], loss=81.2128
	step [112/160], loss=69.8696
	step [113/160], loss=91.0560
	step [114/160], loss=98.3212
	step [115/160], loss=91.3110
	step [116/160], loss=103.1359
	step [117/160], loss=94.5244
	step [118/160], loss=92.5924
	step [119/160], loss=107.0529
	step [120/160], loss=89.5614
	step [121/160], loss=77.5758
	step [122/160], loss=96.9729
	step [123/160], loss=87.8478
	step [124/160], loss=99.3736
	step [125/160], loss=93.1676
	step [126/160], loss=72.5054
	step [127/160], loss=78.8312
	step [128/160], loss=85.0811
	step [129/160], loss=97.0689
	step [130/160], loss=106.6739
	step [131/160], loss=96.4478
	step [132/160], loss=101.2806
	step [133/160], loss=69.5784
	step [134/160], loss=98.7594
	step [135/160], loss=68.7068
	step [136/160], loss=85.7186
	step [137/160], loss=92.0802
	step [138/160], loss=84.8592
	step [139/160], loss=84.2214
	step [140/160], loss=93.9144
	step [141/160], loss=92.4299
	step [142/160], loss=86.1111
	step [143/160], loss=84.5420
	step [144/160], loss=105.5101
	step [145/160], loss=87.7772
	step [146/160], loss=86.7003
	step [147/160], loss=81.2219
	step [148/160], loss=96.8968
	step [149/160], loss=95.6153
	step [150/160], loss=90.5311
	step [151/160], loss=78.5356
	step [152/160], loss=79.8038
	step [153/160], loss=82.5675
	step [154/160], loss=100.1383
	step [155/160], loss=81.1000
	step [156/160], loss=95.5815
	step [157/160], loss=80.8098
	step [158/160], loss=90.8594
	step [159/160], loss=107.3578
	step [160/160], loss=65.5858
	Evaluating
	loss=0.0132, precision=0.3506, recall=0.8811, f1=0.5016
Training epoch 57
	step [1/160], loss=87.2132
	step [2/160], loss=97.9008
	step [3/160], loss=77.5305
	step [4/160], loss=77.2576
	step [5/160], loss=86.2335
	step [6/160], loss=91.9808
	step [7/160], loss=107.0212
	step [8/160], loss=94.9046
	step [9/160], loss=84.0118
	step [10/160], loss=88.0443
	step [11/160], loss=95.3625
	step [12/160], loss=84.9516
	step [13/160], loss=83.1537
	step [14/160], loss=94.5972
	step [15/160], loss=84.1779
	step [16/160], loss=91.2849
	step [17/160], loss=89.9315
	step [18/160], loss=88.5992
	step [19/160], loss=88.4281
	step [20/160], loss=86.2355
	step [21/160], loss=98.1972
	step [22/160], loss=95.0938
	step [23/160], loss=87.2068
	step [24/160], loss=91.1712
	step [25/160], loss=91.4390
	step [26/160], loss=85.7206
	step [27/160], loss=101.5507
	step [28/160], loss=97.4665
	step [29/160], loss=101.6904
	step [30/160], loss=103.3555
	step [31/160], loss=97.0406
	step [32/160], loss=86.4147
	step [33/160], loss=89.0746
	step [34/160], loss=77.5292
	step [35/160], loss=77.6514
	step [36/160], loss=87.9031
	step [37/160], loss=99.1596
	step [38/160], loss=73.8116
	step [39/160], loss=98.9528
	step [40/160], loss=96.7510
	step [41/160], loss=92.8943
	step [42/160], loss=92.1946
	step [43/160], loss=91.1089
	step [44/160], loss=83.0858
	step [45/160], loss=93.4564
	step [46/160], loss=88.0766
	step [47/160], loss=115.5119
	step [48/160], loss=106.0887
	step [49/160], loss=82.0636
	step [50/160], loss=89.6216
	step [51/160], loss=92.6591
	step [52/160], loss=83.8180
	step [53/160], loss=103.3942
	step [54/160], loss=89.2453
	step [55/160], loss=97.3666
	step [56/160], loss=94.7777
	step [57/160], loss=83.7569
	step [58/160], loss=101.3394
	step [59/160], loss=83.7780
	step [60/160], loss=74.9034
	step [61/160], loss=98.1142
	step [62/160], loss=91.6737
	step [63/160], loss=98.9285
	step [64/160], loss=91.9611
	step [65/160], loss=96.6604
	step [66/160], loss=100.6651
	step [67/160], loss=94.1364
	step [68/160], loss=77.5721
	step [69/160], loss=79.5594
	step [70/160], loss=116.3584
	step [71/160], loss=73.6445
	step [72/160], loss=96.3648
	step [73/160], loss=96.9123
	step [74/160], loss=97.5764
	step [75/160], loss=90.2908
	step [76/160], loss=97.6806
	step [77/160], loss=89.1090
	step [78/160], loss=99.2558
	step [79/160], loss=96.3217
	step [80/160], loss=72.8048
	step [81/160], loss=96.7619
	step [82/160], loss=97.7505
	step [83/160], loss=105.0419
	step [84/160], loss=82.8401
	step [85/160], loss=79.9366
	step [86/160], loss=96.3043
	step [87/160], loss=92.6734
	step [88/160], loss=104.8631
	step [89/160], loss=99.5420
	step [90/160], loss=92.9226
	step [91/160], loss=100.6189
	step [92/160], loss=82.6957
	step [93/160], loss=96.1121
	step [94/160], loss=86.6985
	step [95/160], loss=97.5897
	step [96/160], loss=96.6572
	step [97/160], loss=82.3896
	step [98/160], loss=90.5021
	step [99/160], loss=106.2357
	step [100/160], loss=84.0531
	step [101/160], loss=88.9573
	step [102/160], loss=85.2116
	step [103/160], loss=94.1626
	step [104/160], loss=82.2036
	step [105/160], loss=75.7221
	step [106/160], loss=95.0675
	step [107/160], loss=95.7170
	step [108/160], loss=94.0738
	step [109/160], loss=87.0243
	step [110/160], loss=94.7556
	step [111/160], loss=86.6915
	step [112/160], loss=99.2885
	step [113/160], loss=89.0346
	step [114/160], loss=99.9209
	step [115/160], loss=99.6132
	step [116/160], loss=98.0306
	step [117/160], loss=97.8363
	step [118/160], loss=109.8652
	step [119/160], loss=91.8418
	step [120/160], loss=96.5924
	step [121/160], loss=101.7762
	step [122/160], loss=77.5364
	step [123/160], loss=75.5247
	step [124/160], loss=97.8457
	step [125/160], loss=87.4644
	step [126/160], loss=104.2495
	step [127/160], loss=80.4255
	step [128/160], loss=97.8188
	step [129/160], loss=75.8300
	step [130/160], loss=101.1061
	step [131/160], loss=91.0483
	step [132/160], loss=80.5882
	step [133/160], loss=96.9030
	step [134/160], loss=103.3406
	step [135/160], loss=98.3353
	step [136/160], loss=113.0635
	step [137/160], loss=84.2921
	step [138/160], loss=85.5818
	step [139/160], loss=86.6218
	step [140/160], loss=92.8500
	step [141/160], loss=91.0472
	step [142/160], loss=83.8074
	step [143/160], loss=95.3733
	step [144/160], loss=90.5292
	step [145/160], loss=88.2622
	step [146/160], loss=92.6953
	step [147/160], loss=82.9172
	step [148/160], loss=98.7149
	step [149/160], loss=92.8631
	step [150/160], loss=84.5808
	step [151/160], loss=103.1901
	step [152/160], loss=95.0360
	step [153/160], loss=87.5124
	step [154/160], loss=95.9838
	step [155/160], loss=72.0249
	step [156/160], loss=82.8797
	step [157/160], loss=93.3314
	step [158/160], loss=87.4681
	step [159/160], loss=78.5434
	step [160/160], loss=79.5632
	Evaluating
	loss=0.0156, precision=0.3106, recall=0.8980, f1=0.4616
Training epoch 58
	step [1/160], loss=93.5246
	step [2/160], loss=90.8539
	step [3/160], loss=81.7253
	step [4/160], loss=80.4635
	step [5/160], loss=93.0450
	step [6/160], loss=79.4815
	step [7/160], loss=90.3812
	step [8/160], loss=92.4970
	step [9/160], loss=70.6530
	step [10/160], loss=83.6593
	step [11/160], loss=100.8559
	step [12/160], loss=91.4782
	step [13/160], loss=90.9953
	step [14/160], loss=87.7778
	step [15/160], loss=79.5207
	step [16/160], loss=95.6616
	step [17/160], loss=95.0869
	step [18/160], loss=89.6030
	step [19/160], loss=68.1359
	step [20/160], loss=89.5420
	step [21/160], loss=89.6992
	step [22/160], loss=95.5680
	step [23/160], loss=96.3985
	step [24/160], loss=99.7032
	step [25/160], loss=72.2488
	step [26/160], loss=98.8709
	step [27/160], loss=91.8016
	step [28/160], loss=86.4637
	step [29/160], loss=113.5244
	step [30/160], loss=86.4997
	step [31/160], loss=89.2416
	step [32/160], loss=86.5386
	step [33/160], loss=98.2810
	step [34/160], loss=93.6898
	step [35/160], loss=79.9816
	step [36/160], loss=88.2386
	step [37/160], loss=73.5063
	step [38/160], loss=81.6492
	step [39/160], loss=97.3625
	step [40/160], loss=82.6366
	step [41/160], loss=89.2886
	step [42/160], loss=84.4508
	step [43/160], loss=85.6561
	step [44/160], loss=88.1154
	step [45/160], loss=91.8851
	step [46/160], loss=95.5872
	step [47/160], loss=92.2074
	step [48/160], loss=88.1445
	step [49/160], loss=86.8392
	step [50/160], loss=88.2088
	step [51/160], loss=106.3296
	step [52/160], loss=84.1986
	step [53/160], loss=91.0853
	step [54/160], loss=97.9429
	step [55/160], loss=87.2054
	step [56/160], loss=94.7175
	step [57/160], loss=79.9955
	step [58/160], loss=102.0696
	step [59/160], loss=95.8803
	step [60/160], loss=88.4639
	step [61/160], loss=100.7310
	step [62/160], loss=100.9216
	step [63/160], loss=96.2679
	step [64/160], loss=91.6090
	step [65/160], loss=85.8038
	step [66/160], loss=107.7119
	step [67/160], loss=75.1962
	step [68/160], loss=91.5226
	step [69/160], loss=97.5558
	step [70/160], loss=74.5212
	step [71/160], loss=87.7333
	step [72/160], loss=109.2113
	step [73/160], loss=91.4719
	step [74/160], loss=74.2873
	step [75/160], loss=84.2644
	step [76/160], loss=83.2437
	step [77/160], loss=95.3668
	step [78/160], loss=96.1354
	step [79/160], loss=79.7832
	step [80/160], loss=93.7092
	step [81/160], loss=92.4622
	step [82/160], loss=98.8273
	step [83/160], loss=113.7602
	step [84/160], loss=89.4076
	step [85/160], loss=109.9312
	step [86/160], loss=122.8616
	step [87/160], loss=106.1257
	step [88/160], loss=100.1592
	step [89/160], loss=100.1501
	step [90/160], loss=78.7371
	step [91/160], loss=92.7919
	step [92/160], loss=109.0700
	step [93/160], loss=89.2518
	step [94/160], loss=75.4512
	step [95/160], loss=95.9129
	step [96/160], loss=91.7950
	step [97/160], loss=83.7323
	step [98/160], loss=78.0644
	step [99/160], loss=89.3258
	step [100/160], loss=88.8561
	step [101/160], loss=81.7438
	step [102/160], loss=103.9475
	step [103/160], loss=86.1264
	step [104/160], loss=75.5553
	step [105/160], loss=80.8038
	step [106/160], loss=113.4036
	step [107/160], loss=83.7639
	step [108/160], loss=103.8947
	step [109/160], loss=73.1703
	step [110/160], loss=79.6791
	step [111/160], loss=99.1569
	step [112/160], loss=90.0804
	step [113/160], loss=84.1846
	step [114/160], loss=106.2828
	step [115/160], loss=80.4686
	step [116/160], loss=90.0598
	step [117/160], loss=97.9725
	step [118/160], loss=102.1015
	step [119/160], loss=77.2106
	step [120/160], loss=80.8686
	step [121/160], loss=99.7907
	step [122/160], loss=99.6620
	step [123/160], loss=96.0294
	step [124/160], loss=111.5596
	step [125/160], loss=96.6896
	step [126/160], loss=90.9526
	step [127/160], loss=95.7442
	step [128/160], loss=92.1966
	step [129/160], loss=83.9769
	step [130/160], loss=101.8399
	step [131/160], loss=79.8735
	step [132/160], loss=79.2874
	step [133/160], loss=97.0286
	step [134/160], loss=107.8154
	step [135/160], loss=99.8688
	step [136/160], loss=93.2609
	step [137/160], loss=101.2766
	step [138/160], loss=94.0938
	step [139/160], loss=102.4033
	step [140/160], loss=88.6953
	step [141/160], loss=102.0601
	step [142/160], loss=89.7967
	step [143/160], loss=101.3836
	step [144/160], loss=81.0147
	step [145/160], loss=89.9668
	step [146/160], loss=106.9549
	step [147/160], loss=100.2840
	step [148/160], loss=77.5892
	step [149/160], loss=82.1915
	step [150/160], loss=98.8426
	step [151/160], loss=106.1759
	step [152/160], loss=88.8519
	step [153/160], loss=78.4225
	step [154/160], loss=109.7897
	step [155/160], loss=78.2210
	step [156/160], loss=92.3046
	step [157/160], loss=72.8286
	step [158/160], loss=93.3721
	step [159/160], loss=102.6340
	step [160/160], loss=74.4722
	Evaluating
	loss=0.0155, precision=0.2983, recall=0.9119, f1=0.4495
Training epoch 59
	step [1/160], loss=91.0349
	step [2/160], loss=106.2596
	step [3/160], loss=102.3654
	step [4/160], loss=88.6472
	step [5/160], loss=93.3981
	step [6/160], loss=85.5836
	step [7/160], loss=97.6541
	step [8/160], loss=109.2082
	step [9/160], loss=83.2155
	step [10/160], loss=99.2370
	step [11/160], loss=80.8670
	step [12/160], loss=93.3813
	step [13/160], loss=98.7725
	step [14/160], loss=97.6676
	step [15/160], loss=90.1478
	step [16/160], loss=73.7175
	step [17/160], loss=94.7336
	step [18/160], loss=83.8323
	step [19/160], loss=100.9608
	step [20/160], loss=85.7181
	step [21/160], loss=89.8566
	step [22/160], loss=88.9261
	step [23/160], loss=96.1725
	step [24/160], loss=80.7743
	step [25/160], loss=83.4126
	step [26/160], loss=87.5892
	step [27/160], loss=100.6045
	step [28/160], loss=96.6416
	step [29/160], loss=100.4666
	step [30/160], loss=65.2015
	step [31/160], loss=108.2920
	step [32/160], loss=113.8457
	step [33/160], loss=103.3977
	step [34/160], loss=80.8162
	step [35/160], loss=114.2641
	step [36/160], loss=111.0103
	step [37/160], loss=92.1092
	step [38/160], loss=89.4094
	step [39/160], loss=92.0137
	step [40/160], loss=90.5755
	step [41/160], loss=100.3554
	step [42/160], loss=84.7836
	step [43/160], loss=92.9225
	step [44/160], loss=87.0999
	step [45/160], loss=88.7310
	step [46/160], loss=94.6364
	step [47/160], loss=92.2261
	step [48/160], loss=91.3337
	step [49/160], loss=93.4072
	step [50/160], loss=69.5264
	step [51/160], loss=81.9946
	step [52/160], loss=95.4651
	step [53/160], loss=97.8092
	step [54/160], loss=75.9590
	step [55/160], loss=87.6101
	step [56/160], loss=81.5793
	step [57/160], loss=106.7607
	step [58/160], loss=88.2567
	step [59/160], loss=82.6471
	step [60/160], loss=97.1365
	step [61/160], loss=76.8192
	step [62/160], loss=84.3555
	step [63/160], loss=96.5343
	step [64/160], loss=87.3863
	step [65/160], loss=98.2328
	step [66/160], loss=82.8898
	step [67/160], loss=91.1528
	step [68/160], loss=107.9983
	step [69/160], loss=101.2339
	step [70/160], loss=92.4184
	step [71/160], loss=91.4124
	step [72/160], loss=111.8020
	step [73/160], loss=94.8914
	step [74/160], loss=88.9424
	step [75/160], loss=90.3725
	step [76/160], loss=100.0607
	step [77/160], loss=90.4518
	step [78/160], loss=106.0424
	step [79/160], loss=82.9838
	step [80/160], loss=93.7441
	step [81/160], loss=90.0712
	step [82/160], loss=101.1971
	step [83/160], loss=104.0664
	step [84/160], loss=87.1567
	step [85/160], loss=91.4711
	step [86/160], loss=75.5708
	step [87/160], loss=92.6635
	step [88/160], loss=112.7047
	step [89/160], loss=82.2759
	step [90/160], loss=80.3560
	step [91/160], loss=81.5649
	step [92/160], loss=70.2670
	step [93/160], loss=95.8294
	step [94/160], loss=81.1792
	step [95/160], loss=94.2213
	step [96/160], loss=84.1231
	step [97/160], loss=90.8745
	step [98/160], loss=80.9823
	step [99/160], loss=96.0586
	step [100/160], loss=100.7944
	step [101/160], loss=90.9156
	step [102/160], loss=76.2487
	step [103/160], loss=96.7800
	step [104/160], loss=93.1999
	step [105/160], loss=98.8278
	step [106/160], loss=85.6619
	step [107/160], loss=99.6827
	step [108/160], loss=96.6804
	step [109/160], loss=103.2179
	step [110/160], loss=83.8876
	step [111/160], loss=94.9700
	step [112/160], loss=91.0177
	step [113/160], loss=95.4008
	step [114/160], loss=79.0530
	step [115/160], loss=81.1478
	step [116/160], loss=90.6950
	step [117/160], loss=85.8007
	step [118/160], loss=82.5529
	step [119/160], loss=97.9799
	step [120/160], loss=75.7627
	step [121/160], loss=92.5780
	step [122/160], loss=105.3015
	step [123/160], loss=102.7568
	step [124/160], loss=80.9947
	step [125/160], loss=83.8332
	step [126/160], loss=94.8048
	step [127/160], loss=94.7339
	step [128/160], loss=96.0358
	step [129/160], loss=81.3249
	step [130/160], loss=87.5541
	step [131/160], loss=83.2821
	step [132/160], loss=87.8764
	step [133/160], loss=77.6307
	step [134/160], loss=82.1479
	step [135/160], loss=90.4861
	step [136/160], loss=82.3220
	step [137/160], loss=74.9854
	step [138/160], loss=104.2522
	step [139/160], loss=94.8157
	step [140/160], loss=99.9163
	step [141/160], loss=90.5163
	step [142/160], loss=99.7000
	step [143/160], loss=89.0069
	step [144/160], loss=91.7647
	step [145/160], loss=90.2234
	step [146/160], loss=77.2162
	step [147/160], loss=75.6408
	step [148/160], loss=93.5388
	step [149/160], loss=80.1699
	step [150/160], loss=102.0410
	step [151/160], loss=99.0556
	step [152/160], loss=78.8443
	step [153/160], loss=84.3361
	step [154/160], loss=81.7780
	step [155/160], loss=88.4778
	step [156/160], loss=89.7786
	step [157/160], loss=105.0656
	step [158/160], loss=73.5438
	step [159/160], loss=86.5831
	step [160/160], loss=71.3452
	Evaluating
	loss=0.0186, precision=0.2590, recall=0.8950, f1=0.4017
Training epoch 60
	step [1/160], loss=99.9263
	step [2/160], loss=93.2371
	step [3/160], loss=81.2151
	step [4/160], loss=78.4949
	step [5/160], loss=88.1526
	step [6/160], loss=90.0829
	step [7/160], loss=75.1301
	step [8/160], loss=80.3589
	step [9/160], loss=92.2413
	step [10/160], loss=101.9259
	step [11/160], loss=73.8464
	step [12/160], loss=94.5820
	step [13/160], loss=82.0556
	step [14/160], loss=85.1324
	step [15/160], loss=110.7295
	step [16/160], loss=81.8299
	step [17/160], loss=86.6203
	step [18/160], loss=83.6190
	step [19/160], loss=86.7316
	step [20/160], loss=92.1857
	step [21/160], loss=78.8294
	step [22/160], loss=82.9256
	step [23/160], loss=95.9231
	step [24/160], loss=97.9210
	step [25/160], loss=85.9485
	step [26/160], loss=84.0657
	step [27/160], loss=79.6617
	step [28/160], loss=112.1850
	step [29/160], loss=91.5406
	step [30/160], loss=87.7053
	step [31/160], loss=89.6181
	step [32/160], loss=80.2481
	step [33/160], loss=86.4105
	step [34/160], loss=91.5135
	step [35/160], loss=93.0771
	step [36/160], loss=107.5623
	step [37/160], loss=80.3656
	step [38/160], loss=97.4887
	step [39/160], loss=104.3601
	step [40/160], loss=71.0082
	step [41/160], loss=101.5682
	step [42/160], loss=88.6373
	step [43/160], loss=108.3984
	step [44/160], loss=85.9272
	step [45/160], loss=93.3515
	step [46/160], loss=80.4750
	step [47/160], loss=100.5314
	step [48/160], loss=90.6660
	step [49/160], loss=104.0910
	step [50/160], loss=96.3339
	step [51/160], loss=89.2690
	step [52/160], loss=88.5079
	step [53/160], loss=98.0997
	step [54/160], loss=95.4318
	step [55/160], loss=94.7984
	step [56/160], loss=87.2139
	step [57/160], loss=97.8968
	step [58/160], loss=97.8697
	step [59/160], loss=79.7086
	step [60/160], loss=93.7561
	step [61/160], loss=89.7352
	step [62/160], loss=86.1048
	step [63/160], loss=86.0174
	step [64/160], loss=106.0811
	step [65/160], loss=70.0807
	step [66/160], loss=102.0739
	step [67/160], loss=80.7878
	step [68/160], loss=101.0007
	step [69/160], loss=76.9131
	step [70/160], loss=83.9074
	step [71/160], loss=104.6524
	step [72/160], loss=102.2804
	step [73/160], loss=102.9720
	step [74/160], loss=86.0000
	step [75/160], loss=86.8402
	step [76/160], loss=99.0760
	step [77/160], loss=83.7474
	step [78/160], loss=83.9243
	step [79/160], loss=92.3655
	step [80/160], loss=96.3022
	step [81/160], loss=98.7698
	step [82/160], loss=101.5901
	step [83/160], loss=95.6653
	step [84/160], loss=80.4551
	step [85/160], loss=87.4749
	step [86/160], loss=106.3203
	step [87/160], loss=92.9893
	step [88/160], loss=91.5192
	step [89/160], loss=79.7651
	step [90/160], loss=84.6893
	step [91/160], loss=90.2578
	step [92/160], loss=90.9591
	step [93/160], loss=99.3685
	step [94/160], loss=97.8236
	step [95/160], loss=88.3570
	step [96/160], loss=92.7709
	step [97/160], loss=91.5651
	step [98/160], loss=84.5798
	step [99/160], loss=88.6897
	step [100/160], loss=86.8522
	step [101/160], loss=96.5883
	step [102/160], loss=93.6291
	step [103/160], loss=87.9984
	step [104/160], loss=110.6028
	step [105/160], loss=78.9499
	step [106/160], loss=90.9614
	step [107/160], loss=78.2019
	step [108/160], loss=90.2452
	step [109/160], loss=96.6240
	step [110/160], loss=101.5930
	step [111/160], loss=99.0845
	step [112/160], loss=89.7596
	step [113/160], loss=86.3830
	step [114/160], loss=81.8877
	step [115/160], loss=83.1250
	step [116/160], loss=74.1112
	step [117/160], loss=95.7107
	step [118/160], loss=88.0682
	step [119/160], loss=95.1789
	step [120/160], loss=102.8046
	step [121/160], loss=88.1340
	step [122/160], loss=88.0574
	step [123/160], loss=76.9773
	step [124/160], loss=89.8078
	step [125/160], loss=84.2579
	step [126/160], loss=91.4171
	step [127/160], loss=83.9199
	step [128/160], loss=102.6072
	step [129/160], loss=68.5382
	step [130/160], loss=98.9605
	step [131/160], loss=79.4284
	step [132/160], loss=76.8223
	step [133/160], loss=101.3555
	step [134/160], loss=115.7981
	step [135/160], loss=101.1429
	step [136/160], loss=103.8393
	step [137/160], loss=90.7908
	step [138/160], loss=99.4004
	step [139/160], loss=95.5560
	step [140/160], loss=70.9525
	step [141/160], loss=93.1961
	step [142/160], loss=85.9155
	step [143/160], loss=88.7199
	step [144/160], loss=83.7660
	step [145/160], loss=84.6256
	step [146/160], loss=101.9377
	step [147/160], loss=88.4870
	step [148/160], loss=117.8469
	step [149/160], loss=96.1097
	step [150/160], loss=101.3641
	step [151/160], loss=86.9204
	step [152/160], loss=88.5225
	step [153/160], loss=94.3731
	step [154/160], loss=87.0878
	step [155/160], loss=85.1794
	step [156/160], loss=107.6977
	step [157/160], loss=89.9212
	step [158/160], loss=84.3245
	step [159/160], loss=96.6773
	step [160/160], loss=76.2725
	Evaluating
	loss=0.0144, precision=0.3188, recall=0.8876, f1=0.4691
Training epoch 61
	step [1/160], loss=86.0785
	step [2/160], loss=94.5573
	step [3/160], loss=77.8083
	step [4/160], loss=105.0287
	step [5/160], loss=87.9457
	step [6/160], loss=90.8985
	step [7/160], loss=112.7460
	step [8/160], loss=94.2174
	step [9/160], loss=95.6701
	step [10/160], loss=78.6527
	step [11/160], loss=100.9628
	step [12/160], loss=82.3527
	step [13/160], loss=80.7894
	step [14/160], loss=78.6862
	step [15/160], loss=107.3441
	step [16/160], loss=75.5073
	step [17/160], loss=106.2349
	step [18/160], loss=89.5574
	step [19/160], loss=96.9396
	step [20/160], loss=95.1290
	step [21/160], loss=91.5096
	step [22/160], loss=82.0225
	step [23/160], loss=91.7747
	step [24/160], loss=98.5806
	step [25/160], loss=81.0090
	step [26/160], loss=76.5939
	step [27/160], loss=86.3447
	step [28/160], loss=81.2563
	step [29/160], loss=92.4039
	step [30/160], loss=93.4841
	step [31/160], loss=102.8371
	step [32/160], loss=85.5969
	step [33/160], loss=85.5605
	step [34/160], loss=82.1010
	step [35/160], loss=83.2011
	step [36/160], loss=86.5801
	step [37/160], loss=94.7652
	step [38/160], loss=81.2924
	step [39/160], loss=95.4988
	step [40/160], loss=83.6809
	step [41/160], loss=97.3978
	step [42/160], loss=111.1264
	step [43/160], loss=90.0581
	step [44/160], loss=79.7122
	step [45/160], loss=80.5545
	step [46/160], loss=96.0890
	step [47/160], loss=98.0487
	step [48/160], loss=87.1640
	step [49/160], loss=70.7851
	step [50/160], loss=93.3952
	step [51/160], loss=100.4305
	step [52/160], loss=91.1780
	step [53/160], loss=85.1046
	step [54/160], loss=81.2569
	step [55/160], loss=88.7235
	step [56/160], loss=85.9466
	step [57/160], loss=99.9723
	step [58/160], loss=77.4435
	step [59/160], loss=99.1334
	step [60/160], loss=89.2895
	step [61/160], loss=102.6073
	step [62/160], loss=106.7175
	step [63/160], loss=89.8490
	step [64/160], loss=85.6954
	step [65/160], loss=96.1087
	step [66/160], loss=91.5929
	step [67/160], loss=92.9666
	step [68/160], loss=88.0364
	step [69/160], loss=80.3202
	step [70/160], loss=85.4111
	step [71/160], loss=93.1211
	step [72/160], loss=102.4755
	step [73/160], loss=92.4095
	step [74/160], loss=93.9223
	step [75/160], loss=85.5496
	step [76/160], loss=82.6563
	step [77/160], loss=105.1811
	step [78/160], loss=91.9651
	step [79/160], loss=92.6251
	step [80/160], loss=79.1721
	step [81/160], loss=91.3146
	step [82/160], loss=85.8945
	step [83/160], loss=87.8677
	step [84/160], loss=87.7186
	step [85/160], loss=93.4701
	step [86/160], loss=103.9314
	step [87/160], loss=81.1744
	step [88/160], loss=80.7392
	step [89/160], loss=100.9156
	step [90/160], loss=98.4768
	step [91/160], loss=104.0573
	step [92/160], loss=102.7509
	step [93/160], loss=87.7667
	step [94/160], loss=96.0344
	step [95/160], loss=81.3571
	step [96/160], loss=101.2536
	step [97/160], loss=80.0900
	step [98/160], loss=91.4455
	step [99/160], loss=91.5870
	step [100/160], loss=90.4198
	step [101/160], loss=83.0587
	step [102/160], loss=82.0481
	step [103/160], loss=98.2345
	step [104/160], loss=91.3793
	step [105/160], loss=98.4389
	step [106/160], loss=74.9992
	step [107/160], loss=96.5459
	step [108/160], loss=79.0111
	step [109/160], loss=76.2409
	step [110/160], loss=92.7038
	step [111/160], loss=80.4586
	step [112/160], loss=91.8559
	step [113/160], loss=86.7961
	step [114/160], loss=92.5502
	step [115/160], loss=91.6145
	step [116/160], loss=82.8700
	step [117/160], loss=94.9820
	step [118/160], loss=69.0746
	step [119/160], loss=71.3233
	step [120/160], loss=111.2490
	step [121/160], loss=75.7195
	step [122/160], loss=93.6027
	step [123/160], loss=112.4977
	step [124/160], loss=80.9462
	step [125/160], loss=99.4009
	step [126/160], loss=83.2383
	step [127/160], loss=89.7912
	step [128/160], loss=96.4729
	step [129/160], loss=91.0907
	step [130/160], loss=82.5155
	step [131/160], loss=86.1328
	step [132/160], loss=84.4480
	step [133/160], loss=73.1539
	step [134/160], loss=89.2117
	step [135/160], loss=94.0256
	step [136/160], loss=89.1653
	step [137/160], loss=117.2281
	step [138/160], loss=86.7767
	step [139/160], loss=84.6361
	step [140/160], loss=78.5210
	step [141/160], loss=83.6040
	step [142/160], loss=92.4033
	step [143/160], loss=100.1301
	step [144/160], loss=87.4559
	step [145/160], loss=89.0135
	step [146/160], loss=82.6820
	step [147/160], loss=102.9654
	step [148/160], loss=90.2460
	step [149/160], loss=95.3307
	step [150/160], loss=98.4401
	step [151/160], loss=96.9726
	step [152/160], loss=89.5523
	step [153/160], loss=93.8112
	step [154/160], loss=100.9392
	step [155/160], loss=90.0628
	step [156/160], loss=92.7793
	step [157/160], loss=93.9899
	step [158/160], loss=93.7203
	step [159/160], loss=69.3340
	step [160/160], loss=59.5499
	Evaluating
	loss=0.0125, precision=0.3633, recall=0.9046, f1=0.5184
Training epoch 62
	step [1/160], loss=82.9683
	step [2/160], loss=83.2393
	step [3/160], loss=96.7769
	step [4/160], loss=85.8630
	step [5/160], loss=95.1431
	step [6/160], loss=89.4938
	step [7/160], loss=114.7979
	step [8/160], loss=73.3809
	step [9/160], loss=97.0256
	step [10/160], loss=84.6924
	step [11/160], loss=94.4982
	step [12/160], loss=78.3565
	step [13/160], loss=102.6740
	step [14/160], loss=85.4208
	step [15/160], loss=107.4330
	step [16/160], loss=77.2875
	step [17/160], loss=82.1950
	step [18/160], loss=94.0589
	step [19/160], loss=91.3707
	step [20/160], loss=91.3802
	step [21/160], loss=84.7536
	step [22/160], loss=85.8988
	step [23/160], loss=87.3599
	step [24/160], loss=81.1974
	step [25/160], loss=88.2646
	step [26/160], loss=78.6581
	step [27/160], loss=91.5199
	step [28/160], loss=82.6065
	step [29/160], loss=101.7671
	step [30/160], loss=115.0682
	step [31/160], loss=88.9621
	step [32/160], loss=91.5797
	step [33/160], loss=91.7628
	step [34/160], loss=90.2039
	step [35/160], loss=79.3515
	step [36/160], loss=99.9657
	step [37/160], loss=83.5213
	step [38/160], loss=95.3728
	step [39/160], loss=84.1499
	step [40/160], loss=101.4800
	step [41/160], loss=90.4140
	step [42/160], loss=86.6478
	step [43/160], loss=101.1851
	step [44/160], loss=91.4177
	step [45/160], loss=73.8673
	step [46/160], loss=98.9444
	step [47/160], loss=99.2305
	step [48/160], loss=99.4507
	step [49/160], loss=74.3206
	step [50/160], loss=99.8129
	step [51/160], loss=83.6707
	step [52/160], loss=81.6408
	step [53/160], loss=87.8062
	step [54/160], loss=96.5237
	step [55/160], loss=106.3793
	step [56/160], loss=73.9462
	step [57/160], loss=92.3237
	step [58/160], loss=89.3491
	step [59/160], loss=86.7806
	step [60/160], loss=87.2462
	step [61/160], loss=84.7078
	step [62/160], loss=84.7217
	step [63/160], loss=102.9469
	step [64/160], loss=80.4621
	step [65/160], loss=92.5529
	step [66/160], loss=84.9584
	step [67/160], loss=92.3829
	step [68/160], loss=77.0050
	step [69/160], loss=86.7605
	step [70/160], loss=93.1986
	step [71/160], loss=89.1858
	step [72/160], loss=101.8510
	step [73/160], loss=98.2820
	step [74/160], loss=92.8521
	step [75/160], loss=82.7805
	step [76/160], loss=103.4049
	step [77/160], loss=79.7010
	step [78/160], loss=89.9949
	step [79/160], loss=92.6222
	step [80/160], loss=80.4142
	step [81/160], loss=96.5064
	step [82/160], loss=93.2200
	step [83/160], loss=95.5639
	step [84/160], loss=89.5738
	step [85/160], loss=77.4089
	step [86/160], loss=100.0114
	step [87/160], loss=102.3518
	step [88/160], loss=80.0886
	step [89/160], loss=87.4346
	step [90/160], loss=82.1909
	step [91/160], loss=102.2838
	step [92/160], loss=93.4618
	step [93/160], loss=73.0320
	step [94/160], loss=84.5441
	step [95/160], loss=103.8598
	step [96/160], loss=73.7876
	step [97/160], loss=89.9733
	step [98/160], loss=84.8112
	step [99/160], loss=98.1229
	step [100/160], loss=84.1151
	step [101/160], loss=89.2638
	step [102/160], loss=88.0504
	step [103/160], loss=94.4470
	step [104/160], loss=84.0176
	step [105/160], loss=93.9842
	step [106/160], loss=82.6976
	step [107/160], loss=74.6430
	step [108/160], loss=87.6610
	step [109/160], loss=85.9494
	step [110/160], loss=84.0120
	step [111/160], loss=97.0369
	step [112/160], loss=91.6001
	step [113/160], loss=106.7257
	step [114/160], loss=77.3531
	step [115/160], loss=97.9476
	step [116/160], loss=80.6987
	step [117/160], loss=85.5735
	step [118/160], loss=77.5776
	step [119/160], loss=95.5751
	step [120/160], loss=88.8146
	step [121/160], loss=77.4020
	step [122/160], loss=89.7365
	step [123/160], loss=117.3173
	step [124/160], loss=82.9987
	step [125/160], loss=88.1658
	step [126/160], loss=83.7048
	step [127/160], loss=88.3746
	step [128/160], loss=91.4540
	step [129/160], loss=76.3085
	step [130/160], loss=89.1082
	step [131/160], loss=78.9026
	step [132/160], loss=79.9988
	step [133/160], loss=76.0606
	step [134/160], loss=95.8743
	step [135/160], loss=81.6778
	step [136/160], loss=100.0221
	step [137/160], loss=91.1973
	step [138/160], loss=81.8050
	step [139/160], loss=104.0664
	step [140/160], loss=101.1329
	step [141/160], loss=91.2399
	step [142/160], loss=102.0690
	step [143/160], loss=106.5819
	step [144/160], loss=81.7591
	step [145/160], loss=96.9646
	step [146/160], loss=90.9286
	step [147/160], loss=97.6433
	step [148/160], loss=102.4483
	step [149/160], loss=91.8255
	step [150/160], loss=90.3095
	step [151/160], loss=100.3306
	step [152/160], loss=75.1972
	step [153/160], loss=85.9167
	step [154/160], loss=93.3351
	step [155/160], loss=83.8956
	step [156/160], loss=96.4682
	step [157/160], loss=77.7429
	step [158/160], loss=96.0851
	step [159/160], loss=100.5577
	step [160/160], loss=68.8023
	Evaluating
	loss=0.0176, precision=0.2682, recall=0.9100, f1=0.4142
Training epoch 63
	step [1/160], loss=97.6273
	step [2/160], loss=101.9820
	step [3/160], loss=81.2070
	step [4/160], loss=85.2350
	step [5/160], loss=84.7871
	step [6/160], loss=85.4131
	step [7/160], loss=84.5563
	step [8/160], loss=86.0774
	step [9/160], loss=88.8953
	step [10/160], loss=85.6449
	step [11/160], loss=108.1023
	step [12/160], loss=88.6548
	step [13/160], loss=89.9248
	step [14/160], loss=95.3247
	step [15/160], loss=94.3544
	step [16/160], loss=77.1907
	step [17/160], loss=96.0246
	step [18/160], loss=90.9901
	step [19/160], loss=85.9741
	step [20/160], loss=82.5143
	step [21/160], loss=79.6467
	step [22/160], loss=89.2752
	step [23/160], loss=84.5831
	step [24/160], loss=90.0192
	step [25/160], loss=86.2607
	step [26/160], loss=101.8912
	step [27/160], loss=82.4301
	step [28/160], loss=97.8151
	step [29/160], loss=92.7022
	step [30/160], loss=79.7972
	step [31/160], loss=82.6321
	step [32/160], loss=89.0023
	step [33/160], loss=71.9160
	step [34/160], loss=101.9847
	step [35/160], loss=85.8930
	step [36/160], loss=98.5146
	step [37/160], loss=80.7044
	step [38/160], loss=90.8454
	step [39/160], loss=61.0298
	step [40/160], loss=96.7604
	step [41/160], loss=94.7688
	step [42/160], loss=85.7624
	step [43/160], loss=84.9095
	step [44/160], loss=93.4484
	step [45/160], loss=104.5994
	step [46/160], loss=84.7026
	step [47/160], loss=102.8801
	step [48/160], loss=88.9498
	step [49/160], loss=91.2760
	step [50/160], loss=86.1388
	step [51/160], loss=85.6403
	step [52/160], loss=93.5917
	step [53/160], loss=84.1964
	step [54/160], loss=88.0358
	step [55/160], loss=105.6509
	step [56/160], loss=91.4807
	step [57/160], loss=78.1457
	step [58/160], loss=86.3727
	step [59/160], loss=96.0138
	step [60/160], loss=88.1893
	step [61/160], loss=80.8780
	step [62/160], loss=89.6591
	step [63/160], loss=71.7240
	step [64/160], loss=94.1427
	step [65/160], loss=75.1028
	step [66/160], loss=81.0444
	step [67/160], loss=97.7327
	step [68/160], loss=97.1467
	step [69/160], loss=90.4648
	step [70/160], loss=99.5166
	step [71/160], loss=91.1180
	step [72/160], loss=82.0485
	step [73/160], loss=83.7209
	step [74/160], loss=88.9253
	step [75/160], loss=92.9415
	step [76/160], loss=94.8215
	step [77/160], loss=82.5381
	step [78/160], loss=95.5680
	step [79/160], loss=101.0535
	step [80/160], loss=93.8791
	step [81/160], loss=73.3734
	step [82/160], loss=92.8603
	step [83/160], loss=86.6347
	step [84/160], loss=77.7893
	step [85/160], loss=96.8423
	step [86/160], loss=84.6074
	step [87/160], loss=95.0051
	step [88/160], loss=92.0472
	step [89/160], loss=84.4886
	step [90/160], loss=91.0413
	step [91/160], loss=91.9660
	step [92/160], loss=86.7448
	step [93/160], loss=92.9020
	step [94/160], loss=89.9650
	step [95/160], loss=97.4697
	step [96/160], loss=94.8918
	step [97/160], loss=90.9094
	step [98/160], loss=91.6275
	step [99/160], loss=93.9472
	step [100/160], loss=95.4787
	step [101/160], loss=95.7928
	step [102/160], loss=107.6596
	step [103/160], loss=74.2867
	step [104/160], loss=96.1143
	step [105/160], loss=96.3264
	step [106/160], loss=95.6305
	step [107/160], loss=78.6507
	step [108/160], loss=76.5488
	step [109/160], loss=83.1616
	step [110/160], loss=100.2765
	step [111/160], loss=78.5743
	step [112/160], loss=92.0271
	step [113/160], loss=111.1776
	step [114/160], loss=89.0540
	step [115/160], loss=91.9067
	step [116/160], loss=83.0479
	step [117/160], loss=93.9635
	step [118/160], loss=92.2868
	step [119/160], loss=78.6648
	step [120/160], loss=77.0984
	step [121/160], loss=89.3661
	step [122/160], loss=84.8225
	step [123/160], loss=81.9538
	step [124/160], loss=83.6147
	step [125/160], loss=85.9595
	step [126/160], loss=97.2607
	step [127/160], loss=94.9270
	step [128/160], loss=79.0597
	step [129/160], loss=94.6202
	step [130/160], loss=73.6642
	step [131/160], loss=99.3377
	step [132/160], loss=84.3323
	step [133/160], loss=77.4579
	step [134/160], loss=88.4727
	step [135/160], loss=88.8597
	step [136/160], loss=99.4335
	step [137/160], loss=82.7353
	step [138/160], loss=74.1061
	step [139/160], loss=81.9238
	step [140/160], loss=127.1870
	step [141/160], loss=69.4105
	step [142/160], loss=96.3619
	step [143/160], loss=88.3403
	step [144/160], loss=86.0457
	step [145/160], loss=101.6848
	step [146/160], loss=85.6641
	step [147/160], loss=101.1136
	step [148/160], loss=101.0325
	step [149/160], loss=85.2527
	step [150/160], loss=105.8929
	step [151/160], loss=89.9809
	step [152/160], loss=93.9938
	step [153/160], loss=79.7856
	step [154/160], loss=93.5053
	step [155/160], loss=87.6818
	step [156/160], loss=94.0953
	step [157/160], loss=103.4688
	step [158/160], loss=86.6514
	step [159/160], loss=83.6382
	step [160/160], loss=80.1814
	Evaluating
	loss=0.0157, precision=0.2913, recall=0.9058, f1=0.4409
Training epoch 64
	step [1/160], loss=89.2942
	step [2/160], loss=99.4962
	step [3/160], loss=79.9026
	step [4/160], loss=94.4132
	step [5/160], loss=84.1258
	step [6/160], loss=95.1789
	step [7/160], loss=80.4035
	step [8/160], loss=96.4286
	step [9/160], loss=88.0709
	step [10/160], loss=83.6880
	step [11/160], loss=98.0614
	step [12/160], loss=95.1777
	step [13/160], loss=74.2538
	step [14/160], loss=78.0766
	step [15/160], loss=76.8847
	step [16/160], loss=95.1210
	step [17/160], loss=75.8665
	step [18/160], loss=88.2211
	step [19/160], loss=85.5663
	step [20/160], loss=81.4580
	step [21/160], loss=75.6372
	step [22/160], loss=90.2732
	step [23/160], loss=84.3611
	step [24/160], loss=80.7957
	step [25/160], loss=74.3547
	step [26/160], loss=90.4727
	step [27/160], loss=90.1885
	step [28/160], loss=103.9032
	step [29/160], loss=78.2428
	step [30/160], loss=95.5218
	step [31/160], loss=83.9635
	step [32/160], loss=93.8381
	step [33/160], loss=83.1811
	step [34/160], loss=87.2034
	step [35/160], loss=84.0600
	step [36/160], loss=76.5889
	step [37/160], loss=79.7626
	step [38/160], loss=104.0287
	step [39/160], loss=91.5068
	step [40/160], loss=83.6824
	step [41/160], loss=82.1667
	step [42/160], loss=100.8115
	step [43/160], loss=93.8337
	step [44/160], loss=96.4875
	step [45/160], loss=81.9733
	step [46/160], loss=105.1725
	step [47/160], loss=86.2552
	step [48/160], loss=107.5827
	step [49/160], loss=109.5845
	step [50/160], loss=84.7491
	step [51/160], loss=75.8886
	step [52/160], loss=88.4452
	step [53/160], loss=81.5844
	step [54/160], loss=82.4903
	step [55/160], loss=81.4784
	step [56/160], loss=86.0331
	step [57/160], loss=103.8860
	step [58/160], loss=89.7133
	step [59/160], loss=86.7257
	step [60/160], loss=90.7775
	step [61/160], loss=87.2506
	step [62/160], loss=100.9655
	step [63/160], loss=76.0565
	step [64/160], loss=91.3532
	step [65/160], loss=88.0009
	step [66/160], loss=78.5174
	step [67/160], loss=79.3279
	step [68/160], loss=78.8720
	step [69/160], loss=90.3593
	step [70/160], loss=83.7449
	step [71/160], loss=82.9967
	step [72/160], loss=97.0548
	step [73/160], loss=85.8773
	step [74/160], loss=86.7270
	step [75/160], loss=82.7148
	step [76/160], loss=94.5003
	step [77/160], loss=105.9161
	step [78/160], loss=87.5184
	step [79/160], loss=86.4512
	step [80/160], loss=86.2031
	step [81/160], loss=91.1356
	step [82/160], loss=106.9437
	step [83/160], loss=86.8440
	step [84/160], loss=82.1083
	step [85/160], loss=91.4901
	step [86/160], loss=78.6100
	step [87/160], loss=80.0055
	step [88/160], loss=71.8300
	step [89/160], loss=95.7977
	step [90/160], loss=80.2068
	step [91/160], loss=110.3305
	step [92/160], loss=86.7631
	step [93/160], loss=79.0781
	step [94/160], loss=77.3285
	step [95/160], loss=92.8047
	step [96/160], loss=93.0317
	step [97/160], loss=92.4431
	step [98/160], loss=94.8712
	step [99/160], loss=89.8922
	step [100/160], loss=93.6082
	step [101/160], loss=99.6306
	step [102/160], loss=84.9410
	step [103/160], loss=86.9464
	step [104/160], loss=83.3013
	step [105/160], loss=96.5999
	step [106/160], loss=81.3395
	step [107/160], loss=103.3491
	step [108/160], loss=105.8528
	step [109/160], loss=99.3058
	step [110/160], loss=89.4769
	step [111/160], loss=93.6283
	step [112/160], loss=89.2727
	step [113/160], loss=102.4034
	step [114/160], loss=94.6007
	step [115/160], loss=78.5632
	step [116/160], loss=87.9205
	step [117/160], loss=90.1975
	step [118/160], loss=108.1648
	step [119/160], loss=99.1739
	step [120/160], loss=86.4946
	step [121/160], loss=81.9890
	step [122/160], loss=94.2056
	step [123/160], loss=93.4885
	step [124/160], loss=90.8789
	step [125/160], loss=94.8978
	step [126/160], loss=98.3401
	step [127/160], loss=84.7716
	step [128/160], loss=94.8432
	step [129/160], loss=84.0004
	step [130/160], loss=73.1352
	step [131/160], loss=99.4311
	step [132/160], loss=110.6155
	step [133/160], loss=80.2633
	step [134/160], loss=88.2438
	step [135/160], loss=89.6685
	step [136/160], loss=102.6482
	step [137/160], loss=84.7841
	step [138/160], loss=88.4980
	step [139/160], loss=103.4846
	step [140/160], loss=89.8175
	step [141/160], loss=92.8836
	step [142/160], loss=77.2130
	step [143/160], loss=97.3586
	step [144/160], loss=77.2667
	step [145/160], loss=83.0638
	step [146/160], loss=80.6836
	step [147/160], loss=79.7213
	step [148/160], loss=90.8306
	step [149/160], loss=90.4461
	step [150/160], loss=77.1525
	step [151/160], loss=82.4814
	step [152/160], loss=84.8235
	step [153/160], loss=85.6794
	step [154/160], loss=85.6827
	step [155/160], loss=94.9674
	step [156/160], loss=91.1854
	step [157/160], loss=83.6991
	step [158/160], loss=85.4123
	step [159/160], loss=104.9637
	step [160/160], loss=78.7068
	Evaluating
	loss=0.0131, precision=0.3520, recall=0.9014, f1=0.5062
Training epoch 65
	step [1/160], loss=100.3520
	step [2/160], loss=84.5295
	step [3/160], loss=80.3354
	step [4/160], loss=88.8844
	step [5/160], loss=108.7532
	step [6/160], loss=86.0181
	step [7/160], loss=96.2235
	step [8/160], loss=78.7923
	step [9/160], loss=91.4495
	step [10/160], loss=83.6409
	step [11/160], loss=86.7986
	step [12/160], loss=83.3464
	step [13/160], loss=93.8546
	step [14/160], loss=100.5967
	step [15/160], loss=90.5240
	step [16/160], loss=78.0702
	step [17/160], loss=90.1278
	step [18/160], loss=99.0193
	step [19/160], loss=78.2676
	step [20/160], loss=94.7116
	step [21/160], loss=80.3362
	step [22/160], loss=90.2577
	step [23/160], loss=109.3643
	step [24/160], loss=110.4887
	step [25/160], loss=90.9364
	step [26/160], loss=91.3434
	step [27/160], loss=98.1452
	step [28/160], loss=97.6897
	step [29/160], loss=92.5314
	step [30/160], loss=82.0219
	step [31/160], loss=82.1404
	step [32/160], loss=93.8474
	step [33/160], loss=92.6855
	step [34/160], loss=93.7735
	step [35/160], loss=81.4276
	step [36/160], loss=82.3046
	step [37/160], loss=101.9342
	step [38/160], loss=82.8167
	step [39/160], loss=83.6134
	step [40/160], loss=93.2924
	step [41/160], loss=97.8351
	step [42/160], loss=94.5612
	step [43/160], loss=84.8875
	step [44/160], loss=94.0393
	step [45/160], loss=91.0224
	step [46/160], loss=67.7914
	step [47/160], loss=96.2712
	step [48/160], loss=81.8071
	step [49/160], loss=83.0446
	step [50/160], loss=83.9817
	step [51/160], loss=82.3607
	step [52/160], loss=88.7890
	step [53/160], loss=59.9349
	step [54/160], loss=95.8964
	step [55/160], loss=83.8141
	step [56/160], loss=91.3064
	step [57/160], loss=79.2364
	step [58/160], loss=83.0728
	step [59/160], loss=94.6123
	step [60/160], loss=77.5899
	step [61/160], loss=103.7077
	step [62/160], loss=94.5855
	step [63/160], loss=91.5945
	step [64/160], loss=99.7819
	step [65/160], loss=81.1388
	step [66/160], loss=79.7489
	step [67/160], loss=73.1771
	step [68/160], loss=75.0603
	step [69/160], loss=94.0069
	step [70/160], loss=94.5769
	step [71/160], loss=76.6196
	step [72/160], loss=89.0925
	step [73/160], loss=84.4842
	step [74/160], loss=81.7745
	step [75/160], loss=100.7639
	step [76/160], loss=89.4086
	step [77/160], loss=93.9309
	step [78/160], loss=87.2625
	step [79/160], loss=102.0195
	step [80/160], loss=108.7858
	step [81/160], loss=96.9360
	step [82/160], loss=88.9085
	step [83/160], loss=91.3930
	step [84/160], loss=86.0125
	step [85/160], loss=96.7227
	step [86/160], loss=80.0046
	step [87/160], loss=83.6590
	step [88/160], loss=86.9288
	step [89/160], loss=90.4045
	step [90/160], loss=91.0573
	step [91/160], loss=97.9679
	step [92/160], loss=87.2050
	step [93/160], loss=80.5173
	step [94/160], loss=92.7962
	step [95/160], loss=89.4071
	step [96/160], loss=80.5595
	step [97/160], loss=99.6306
	step [98/160], loss=87.4265
	step [99/160], loss=102.9604
	step [100/160], loss=75.2386
	step [101/160], loss=107.3170
	step [102/160], loss=77.3517
	step [103/160], loss=84.4616
	step [104/160], loss=90.4536
	step [105/160], loss=87.5826
	step [106/160], loss=84.0930
	step [107/160], loss=88.6584
	step [108/160], loss=81.2291
	step [109/160], loss=82.9558
	step [110/160], loss=81.8685
	step [111/160], loss=93.3329
	step [112/160], loss=81.8457
	step [113/160], loss=84.2779
	step [114/160], loss=95.3107
	step [115/160], loss=93.5509
	step [116/160], loss=83.4612
	step [117/160], loss=109.4699
	step [118/160], loss=95.1519
	step [119/160], loss=95.5887
	step [120/160], loss=77.6289
	step [121/160], loss=81.8964
	step [122/160], loss=82.2366
	step [123/160], loss=83.4607
	step [124/160], loss=79.2251
	step [125/160], loss=92.7520
	step [126/160], loss=106.5771
	step [127/160], loss=73.3477
	step [128/160], loss=91.8275
	step [129/160], loss=92.8327
	step [130/160], loss=82.2655
	step [131/160], loss=94.0864
	step [132/160], loss=65.7037
	step [133/160], loss=92.8351
	step [134/160], loss=85.7650
	step [135/160], loss=77.7872
	step [136/160], loss=88.3857
	step [137/160], loss=94.7364
	step [138/160], loss=80.6809
	step [139/160], loss=80.5788
	step [140/160], loss=101.4942
	step [141/160], loss=85.0952
	step [142/160], loss=103.2768
	step [143/160], loss=102.8581
	step [144/160], loss=75.4874
	step [145/160], loss=65.8247
	step [146/160], loss=93.5082
	step [147/160], loss=88.8600
	step [148/160], loss=90.5490
	step [149/160], loss=96.2660
	step [150/160], loss=82.4338
	step [151/160], loss=87.5063
	step [152/160], loss=89.3338
	step [153/160], loss=85.6318
	step [154/160], loss=79.8924
	step [155/160], loss=77.8534
	step [156/160], loss=98.1805
	step [157/160], loss=90.5878
	step [158/160], loss=81.5072
	step [159/160], loss=95.4398
	step [160/160], loss=65.9758
	Evaluating
	loss=0.0128, precision=0.3526, recall=0.8923, f1=0.5055
Training epoch 66
	step [1/160], loss=92.5650
	step [2/160], loss=78.9419
	step [3/160], loss=85.9844
	step [4/160], loss=91.3751
	step [5/160], loss=112.6669
	step [6/160], loss=82.7851
	step [7/160], loss=85.6889
	step [8/160], loss=90.3874
	step [9/160], loss=72.6284
	step [10/160], loss=83.3412
	step [11/160], loss=87.3334
	step [12/160], loss=88.4491
	step [13/160], loss=86.4003
	step [14/160], loss=80.8395
	step [15/160], loss=84.2264
	step [16/160], loss=83.3327
	step [17/160], loss=100.1511
	step [18/160], loss=92.9647
	step [19/160], loss=76.5953
	step [20/160], loss=79.9051
	step [21/160], loss=96.2713
	step [22/160], loss=89.7339
	step [23/160], loss=79.8684
	step [24/160], loss=87.6109
	step [25/160], loss=97.6212
	step [26/160], loss=79.2877
	step [27/160], loss=90.7162
	step [28/160], loss=97.6256
	step [29/160], loss=93.0270
	step [30/160], loss=96.3575
	step [31/160], loss=90.1469
	step [32/160], loss=92.5963
	step [33/160], loss=100.4360
	step [34/160], loss=90.6289
	step [35/160], loss=93.5688
	step [36/160], loss=88.6144
	step [37/160], loss=83.4332
	step [38/160], loss=112.6980
	step [39/160], loss=98.2472
	step [40/160], loss=89.3150
	step [41/160], loss=76.7213
	step [42/160], loss=99.5909
	step [43/160], loss=89.9404
	step [44/160], loss=76.0074
	step [45/160], loss=80.2421
	step [46/160], loss=86.3485
	step [47/160], loss=89.5678
	step [48/160], loss=82.7610
	step [49/160], loss=105.5932
	step [50/160], loss=92.0409
	step [51/160], loss=89.4248
	step [52/160], loss=90.9606
	step [53/160], loss=92.0180
	step [54/160], loss=107.4093
	step [55/160], loss=91.1704
	step [56/160], loss=75.7092
	step [57/160], loss=109.6115
	step [58/160], loss=86.4831
	step [59/160], loss=83.3344
	step [60/160], loss=81.1290
	step [61/160], loss=94.8906
	step [62/160], loss=96.2305
	step [63/160], loss=86.1470
	step [64/160], loss=96.7219
	step [65/160], loss=90.8018
	step [66/160], loss=76.5175
	step [67/160], loss=95.5497
	step [68/160], loss=82.6072
	step [69/160], loss=85.1358
	step [70/160], loss=76.9221
	step [71/160], loss=77.3880
	step [72/160], loss=95.4324
	step [73/160], loss=87.9043
	step [74/160], loss=86.8546
	step [75/160], loss=102.6142
	step [76/160], loss=84.9761
	step [77/160], loss=77.3182
	step [78/160], loss=79.1278
	step [79/160], loss=80.3851
	step [80/160], loss=80.3954
	step [81/160], loss=77.6100
	step [82/160], loss=88.3043
	step [83/160], loss=102.7987
	step [84/160], loss=78.3719
	step [85/160], loss=103.7144
	step [86/160], loss=77.5661
	step [87/160], loss=83.4104
	step [88/160], loss=87.6624
	step [89/160], loss=82.7802
	step [90/160], loss=81.5856
	step [91/160], loss=91.7036
	step [92/160], loss=90.2587
	step [93/160], loss=93.9774
	step [94/160], loss=86.9623
	step [95/160], loss=103.6488
	step [96/160], loss=84.3904
	step [97/160], loss=87.8996
	step [98/160], loss=94.6795
	step [99/160], loss=98.3454
	step [100/160], loss=94.0549
	step [101/160], loss=67.6906
	step [102/160], loss=104.8869
	step [103/160], loss=85.6519
	step [104/160], loss=85.8801
	step [105/160], loss=99.3417
	step [106/160], loss=92.9515
	step [107/160], loss=89.2280
	step [108/160], loss=83.1449
	step [109/160], loss=76.0203
	step [110/160], loss=90.8319
	step [111/160], loss=94.6618
	step [112/160], loss=100.9698
	step [113/160], loss=91.1052
	step [114/160], loss=83.1681
	step [115/160], loss=76.7240
	step [116/160], loss=72.5282
	step [117/160], loss=89.8429
	step [118/160], loss=89.6383
	step [119/160], loss=86.5583
	step [120/160], loss=79.3895
	step [121/160], loss=77.8878
	step [122/160], loss=92.5752
	step [123/160], loss=90.7432
	step [124/160], loss=78.2598
	step [125/160], loss=75.9191
	step [126/160], loss=97.7955
	step [127/160], loss=88.7659
	step [128/160], loss=89.3765
	step [129/160], loss=89.7408
	step [130/160], loss=92.8580
	step [131/160], loss=100.3344
	step [132/160], loss=86.6765
	step [133/160], loss=79.9962
	step [134/160], loss=95.5258
	step [135/160], loss=82.5064
	step [136/160], loss=97.6584
	step [137/160], loss=83.8165
	step [138/160], loss=89.8273
	step [139/160], loss=69.8372
	step [140/160], loss=95.9352
	step [141/160], loss=85.3064
	step [142/160], loss=85.5275
	step [143/160], loss=86.3424
	step [144/160], loss=83.5033
	step [145/160], loss=87.2846
	step [146/160], loss=85.3417
	step [147/160], loss=85.8163
	step [148/160], loss=70.8229
	step [149/160], loss=90.0845
	step [150/160], loss=90.6766
	step [151/160], loss=78.0878
	step [152/160], loss=70.3004
	step [153/160], loss=78.0963
	step [154/160], loss=101.0275
	step [155/160], loss=96.0808
	step [156/160], loss=95.1525
	step [157/160], loss=74.9407
	step [158/160], loss=104.5387
	step [159/160], loss=86.5440
	step [160/160], loss=69.7001
	Evaluating
	loss=0.0128, precision=0.3509, recall=0.8923, f1=0.5037
Training epoch 67
	step [1/160], loss=88.8273
	step [2/160], loss=88.4787
	step [3/160], loss=84.5264
	step [4/160], loss=84.8372
	step [5/160], loss=84.5114
	step [6/160], loss=95.0097
	step [7/160], loss=89.9351
	step [8/160], loss=80.3260
	step [9/160], loss=99.1372
	step [10/160], loss=76.4678
	step [11/160], loss=95.9698
	step [12/160], loss=84.6250
	step [13/160], loss=80.0637
	step [14/160], loss=94.2424
	step [15/160], loss=88.4575
	step [16/160], loss=84.9039
	step [17/160], loss=83.6899
	step [18/160], loss=98.7575
	step [19/160], loss=82.4699
	step [20/160], loss=99.1942
	step [21/160], loss=98.1921
	step [22/160], loss=93.4256
	step [23/160], loss=75.8583
	step [24/160], loss=93.5635
	step [25/160], loss=83.2161
	step [26/160], loss=85.4017
	step [27/160], loss=103.1543
	step [28/160], loss=76.5491
	step [29/160], loss=72.1242
	step [30/160], loss=85.9635
	step [31/160], loss=82.4067
	step [32/160], loss=104.6266
	step [33/160], loss=90.6696
	step [34/160], loss=98.8883
	step [35/160], loss=84.7500
	step [36/160], loss=85.3057
	step [37/160], loss=89.5562
	step [38/160], loss=88.9531
	step [39/160], loss=86.4043
	step [40/160], loss=86.0269
	step [41/160], loss=97.1297
	step [42/160], loss=95.8767
	step [43/160], loss=80.4885
	step [44/160], loss=94.2334
	step [45/160], loss=91.7398
	step [46/160], loss=94.5886
	step [47/160], loss=87.7624
	step [48/160], loss=83.8589
	step [49/160], loss=67.2852
	step [50/160], loss=75.6897
	step [51/160], loss=91.8266
	step [52/160], loss=77.2270
	step [53/160], loss=97.3472
	step [54/160], loss=88.0641
	step [55/160], loss=92.4224
	step [56/160], loss=81.4055
	step [57/160], loss=86.0057
	step [58/160], loss=109.3859
	step [59/160], loss=84.5902
	step [60/160], loss=103.6400
	step [61/160], loss=86.4476
	step [62/160], loss=81.0841
	step [63/160], loss=85.7877
	step [64/160], loss=85.7137
	step [65/160], loss=94.2136
	step [66/160], loss=90.8400
	step [67/160], loss=90.7641
	step [68/160], loss=82.0879
	step [69/160], loss=100.6538
	step [70/160], loss=94.3609
	step [71/160], loss=88.7911
	step [72/160], loss=86.3376
	step [73/160], loss=96.1758
	step [74/160], loss=87.9680
	step [75/160], loss=97.6386
	step [76/160], loss=100.3082
	step [77/160], loss=77.7684
	step [78/160], loss=91.1893
	step [79/160], loss=98.4560
	step [80/160], loss=77.9185
	step [81/160], loss=92.9033
	step [82/160], loss=102.5729
	step [83/160], loss=90.5598
	step [84/160], loss=80.6002
	step [85/160], loss=99.2177
	step [86/160], loss=76.8556
	step [87/160], loss=95.4836
	step [88/160], loss=78.4963
	step [89/160], loss=84.5403
	step [90/160], loss=91.2460
	step [91/160], loss=71.3266
	step [92/160], loss=96.3381
	step [93/160], loss=87.6829
	step [94/160], loss=100.5252
	step [95/160], loss=85.9564
	step [96/160], loss=89.1793
	step [97/160], loss=96.2485
	step [98/160], loss=96.5905
	step [99/160], loss=89.8784
	step [100/160], loss=94.6173
	step [101/160], loss=103.8089
	step [102/160], loss=94.3352
	step [103/160], loss=100.4345
	step [104/160], loss=88.6415
	step [105/160], loss=82.4697
	step [106/160], loss=83.3845
	step [107/160], loss=86.2233
	step [108/160], loss=80.0892
	step [109/160], loss=89.1494
	step [110/160], loss=57.9922
	step [111/160], loss=88.5443
	step [112/160], loss=81.8054
	step [113/160], loss=84.4962
	step [114/160], loss=96.6707
	step [115/160], loss=95.0287
	step [116/160], loss=86.1523
	step [117/160], loss=91.9178
	step [118/160], loss=87.3742
	step [119/160], loss=93.3973
	step [120/160], loss=114.6936
	step [121/160], loss=91.6055
	step [122/160], loss=81.0826
	step [123/160], loss=87.6971
	step [124/160], loss=79.9988
	step [125/160], loss=92.7223
	step [126/160], loss=78.7894
	step [127/160], loss=92.1573
	step [128/160], loss=86.3526
	step [129/160], loss=84.1106
	step [130/160], loss=88.4988
	step [131/160], loss=81.8180
	step [132/160], loss=83.5055
	step [133/160], loss=103.1529
	step [134/160], loss=85.7092
	step [135/160], loss=84.6588
	step [136/160], loss=99.2753
	step [137/160], loss=108.0060
	step [138/160], loss=104.4077
	step [139/160], loss=84.1248
	step [140/160], loss=87.5448
	step [141/160], loss=92.0550
	step [142/160], loss=90.2005
	step [143/160], loss=91.6749
	step [144/160], loss=70.9134
	step [145/160], loss=92.9930
	step [146/160], loss=64.3429
	step [147/160], loss=75.8951
	step [148/160], loss=69.5085
	step [149/160], loss=82.4755
	step [150/160], loss=97.0442
	step [151/160], loss=68.2002
	step [152/160], loss=98.1859
	step [153/160], loss=77.4772
	step [154/160], loss=91.9140
	step [155/160], loss=86.7489
	step [156/160], loss=93.4084
	step [157/160], loss=79.6550
	step [158/160], loss=89.8239
	step [159/160], loss=81.1359
	step [160/160], loss=56.1119
	Evaluating
	loss=0.0128, precision=0.3540, recall=0.9069, f1=0.5092
Training epoch 68
	step [1/160], loss=78.9623
	step [2/160], loss=68.6016
	step [3/160], loss=83.3415
	step [4/160], loss=92.7058
	step [5/160], loss=106.4362
	step [6/160], loss=90.3089
	step [7/160], loss=87.1644
	step [8/160], loss=87.5850
	step [9/160], loss=89.4240
	step [10/160], loss=99.9500
	step [11/160], loss=99.4354
	step [12/160], loss=73.3346
	step [13/160], loss=99.6291
	step [14/160], loss=96.0480
	step [15/160], loss=101.8424
	step [16/160], loss=88.1884
	step [17/160], loss=107.6222
	step [18/160], loss=105.0882
	step [19/160], loss=89.2678
	step [20/160], loss=96.4959
	step [21/160], loss=90.3942
	step [22/160], loss=84.8255
	step [23/160], loss=84.1939
	step [24/160], loss=85.5529
	step [25/160], loss=106.0214
	step [26/160], loss=73.3781
	step [27/160], loss=91.9839
	step [28/160], loss=84.5278
	step [29/160], loss=84.9405
	step [30/160], loss=77.9632
	step [31/160], loss=96.5158
	step [32/160], loss=99.6053
	step [33/160], loss=111.2484
	step [34/160], loss=98.8787
	step [35/160], loss=94.3546
	step [36/160], loss=92.9917
	step [37/160], loss=74.0026
	step [38/160], loss=78.5040
	step [39/160], loss=79.5215
	step [40/160], loss=87.1590
	step [41/160], loss=81.9693
	step [42/160], loss=91.3126
	step [43/160], loss=78.5408
	step [44/160], loss=88.7163
	step [45/160], loss=84.0694
	step [46/160], loss=81.0107
	step [47/160], loss=79.2453
	step [48/160], loss=82.3463
	step [49/160], loss=87.3053
	step [50/160], loss=68.4960
	step [51/160], loss=81.9940
	step [52/160], loss=101.6574
	step [53/160], loss=103.3330
	step [54/160], loss=85.4182
	step [55/160], loss=84.3080
	step [56/160], loss=85.7841
	step [57/160], loss=94.2281
	step [58/160], loss=94.8010
	step [59/160], loss=78.6634
	step [60/160], loss=81.9594
	step [61/160], loss=81.9090
	step [62/160], loss=78.3638
	step [63/160], loss=92.4034
	step [64/160], loss=89.8678
	step [65/160], loss=75.2632
	step [66/160], loss=99.2276
	step [67/160], loss=81.2340
	step [68/160], loss=75.5454
	step [69/160], loss=78.1863
	step [70/160], loss=89.5915
	step [71/160], loss=87.2278
	step [72/160], loss=81.4454
	step [73/160], loss=92.8197
	step [74/160], loss=85.1192
	step [75/160], loss=81.3805
	step [76/160], loss=79.1173
	step [77/160], loss=90.8707
	step [78/160], loss=83.8417
	step [79/160], loss=63.6740
	step [80/160], loss=78.1750
	step [81/160], loss=88.9380
	step [82/160], loss=82.2058
	step [83/160], loss=82.0295
	step [84/160], loss=99.5288
	step [85/160], loss=84.3829
	step [86/160], loss=85.7380
	step [87/160], loss=89.8176
	step [88/160], loss=101.8317
	step [89/160], loss=105.6716
	step [90/160], loss=72.3777
	step [91/160], loss=82.5788
	step [92/160], loss=80.9252
	step [93/160], loss=89.5106
	step [94/160], loss=77.8606
	step [95/160], loss=81.1355
	step [96/160], loss=79.5151
	step [97/160], loss=101.6729
	step [98/160], loss=96.2488
	step [99/160], loss=94.3670
	step [100/160], loss=80.1653
	step [101/160], loss=89.4095
	step [102/160], loss=82.5465
	step [103/160], loss=82.9336
	step [104/160], loss=85.8220
	step [105/160], loss=87.3533
	step [106/160], loss=108.3689
	step [107/160], loss=104.8896
	step [108/160], loss=83.2268
	step [109/160], loss=83.7369
	step [110/160], loss=78.3528
	step [111/160], loss=75.5573
	step [112/160], loss=78.8819
	step [113/160], loss=85.5535
	step [114/160], loss=116.3336
	step [115/160], loss=105.5094
	step [116/160], loss=97.3529
	step [117/160], loss=92.7375
	step [118/160], loss=89.3414
	step [119/160], loss=101.4123
	step [120/160], loss=84.4511
	step [121/160], loss=88.1885
	step [122/160], loss=93.6020
	step [123/160], loss=81.0901
	step [124/160], loss=90.7377
	step [125/160], loss=96.0355
	step [126/160], loss=97.2637
	step [127/160], loss=90.0044
	step [128/160], loss=82.4650
	step [129/160], loss=70.4135
	step [130/160], loss=97.3873
	step [131/160], loss=84.3929
	step [132/160], loss=86.3127
	step [133/160], loss=85.1359
	step [134/160], loss=79.5225
	step [135/160], loss=93.4732
	step [136/160], loss=85.6638
	step [137/160], loss=80.1478
	step [138/160], loss=94.6349
	step [139/160], loss=81.7327
	step [140/160], loss=90.2007
	step [141/160], loss=94.2228
	step [142/160], loss=84.5318
	step [143/160], loss=84.9147
	step [144/160], loss=78.7664
	step [145/160], loss=70.7086
	step [146/160], loss=90.6830
	step [147/160], loss=81.2850
	step [148/160], loss=82.4503
	step [149/160], loss=88.9098
	step [150/160], loss=95.9753
	step [151/160], loss=97.8703
	step [152/160], loss=93.0560
	step [153/160], loss=74.5376
	step [154/160], loss=90.3379
	step [155/160], loss=88.4675
	step [156/160], loss=78.8906
	step [157/160], loss=91.7772
	step [158/160], loss=88.1114
	step [159/160], loss=103.7996
	step [160/160], loss=55.9153
	Evaluating
	loss=0.0139, precision=0.3226, recall=0.9006, f1=0.4750
Training epoch 69
	step [1/160], loss=78.4878
	step [2/160], loss=73.0756
	step [3/160], loss=92.4762
	step [4/160], loss=85.0538
	step [5/160], loss=94.7893
	step [6/160], loss=88.2689
	step [7/160], loss=84.0099
	step [8/160], loss=89.1703
	step [9/160], loss=102.6029
	step [10/160], loss=88.8131
	step [11/160], loss=84.9048
	step [12/160], loss=85.5257
	step [13/160], loss=77.7968
	step [14/160], loss=88.8166
	step [15/160], loss=91.7535
	step [16/160], loss=82.1751
	step [17/160], loss=86.9349
	step [18/160], loss=78.7311
	step [19/160], loss=82.3486
	step [20/160], loss=94.5399
	step [21/160], loss=91.5536
	step [22/160], loss=95.1208
	step [23/160], loss=97.0501
	step [24/160], loss=84.3963
	step [25/160], loss=86.3145
	step [26/160], loss=80.7090
	step [27/160], loss=92.2029
	step [28/160], loss=85.8227
	step [29/160], loss=95.1239
	step [30/160], loss=107.8127
	step [31/160], loss=80.9583
	step [32/160], loss=76.2621
	step [33/160], loss=91.4655
	step [34/160], loss=86.6169
	step [35/160], loss=85.6920
	step [36/160], loss=78.9077
	step [37/160], loss=99.0389
	step [38/160], loss=80.1254
	step [39/160], loss=95.6113
	step [40/160], loss=97.2915
	step [41/160], loss=84.8354
	step [42/160], loss=102.3161
	step [43/160], loss=95.4356
	step [44/160], loss=91.1827
	step [45/160], loss=89.0003
	step [46/160], loss=96.0264
	step [47/160], loss=96.3408
	step [48/160], loss=92.3115
	step [49/160], loss=92.7085
	step [50/160], loss=77.5334
	step [51/160], loss=85.2057
	step [52/160], loss=100.7875
	step [53/160], loss=87.6679
	step [54/160], loss=76.7319
	step [55/160], loss=105.0118
	step [56/160], loss=90.9722
	step [57/160], loss=95.2707
	step [58/160], loss=79.4803
	step [59/160], loss=70.1600
	step [60/160], loss=100.2000
	step [61/160], loss=78.4995
	step [62/160], loss=100.2613
	step [63/160], loss=104.2230
	step [64/160], loss=92.8753
	step [65/160], loss=77.7710
	step [66/160], loss=70.4009
	step [67/160], loss=86.8639
	step [68/160], loss=91.9508
	step [69/160], loss=91.4881
	step [70/160], loss=94.7043
	step [71/160], loss=83.9256
	step [72/160], loss=86.1634
	step [73/160], loss=82.3063
	step [74/160], loss=90.1370
	step [75/160], loss=80.0589
	step [76/160], loss=82.5416
	step [77/160], loss=90.9856
	step [78/160], loss=110.2248
	step [79/160], loss=86.6896
	step [80/160], loss=85.4758
	step [81/160], loss=79.3392
	step [82/160], loss=79.7380
	step [83/160], loss=73.9581
	step [84/160], loss=107.8880
	step [85/160], loss=97.1780
	step [86/160], loss=88.2991
	step [87/160], loss=88.3561
	step [88/160], loss=89.4011
	step [89/160], loss=76.9958
	step [90/160], loss=94.0099
	step [91/160], loss=80.0219
	step [92/160], loss=89.4353
	step [93/160], loss=88.9433
	step [94/160], loss=77.4035
	step [95/160], loss=89.6655
	step [96/160], loss=88.1661
	step [97/160], loss=87.2127
	step [98/160], loss=81.4797
	step [99/160], loss=80.7508
	step [100/160], loss=78.4784
	step [101/160], loss=92.4371
	step [102/160], loss=89.9486
	step [103/160], loss=78.2150
	step [104/160], loss=96.6964
	step [105/160], loss=106.1954
	step [106/160], loss=100.6428
	step [107/160], loss=83.8763
	step [108/160], loss=84.6217
	step [109/160], loss=84.1707
	step [110/160], loss=63.0630
	step [111/160], loss=77.6390
	step [112/160], loss=80.8599
	step [113/160], loss=92.1782
	step [114/160], loss=86.1894
	step [115/160], loss=73.8380
	step [116/160], loss=76.8567
	step [117/160], loss=105.0220
	step [118/160], loss=95.7095
	step [119/160], loss=79.4199
	step [120/160], loss=90.8942
	step [121/160], loss=76.8391
	step [122/160], loss=86.5560
	step [123/160], loss=72.4557
	step [124/160], loss=81.8168
	step [125/160], loss=80.1375
	step [126/160], loss=94.5297
	step [127/160], loss=86.1782
	step [128/160], loss=88.1868
	step [129/160], loss=92.0874
	step [130/160], loss=106.5540
	step [131/160], loss=92.5833
	step [132/160], loss=79.6760
	step [133/160], loss=73.7242
	step [134/160], loss=81.0390
	step [135/160], loss=98.6860
	step [136/160], loss=98.9695
	step [137/160], loss=85.1229
	step [138/160], loss=82.2949
	step [139/160], loss=84.4061
	step [140/160], loss=79.6686
	step [141/160], loss=98.0743
	step [142/160], loss=90.6308
	step [143/160], loss=90.4515
	step [144/160], loss=95.6591
	step [145/160], loss=77.9072
	step [146/160], loss=90.7511
	step [147/160], loss=95.0199
	step [148/160], loss=76.1476
	step [149/160], loss=89.6302
	step [150/160], loss=78.0148
	step [151/160], loss=90.4975
	step [152/160], loss=82.6209
	step [153/160], loss=75.8855
	step [154/160], loss=84.5618
	step [155/160], loss=87.8146
	step [156/160], loss=73.2413
	step [157/160], loss=100.7469
	step [158/160], loss=64.6344
	step [159/160], loss=85.6650
	step [160/160], loss=71.0522
	Evaluating
	loss=0.0144, precision=0.3141, recall=0.8977, f1=0.4654
Training epoch 70
	step [1/160], loss=103.7479
	step [2/160], loss=84.9442
	step [3/160], loss=94.7114
	step [4/160], loss=84.6857
	step [5/160], loss=81.0787
	step [6/160], loss=78.8980
	step [7/160], loss=90.1666
	step [8/160], loss=89.7440
	step [9/160], loss=81.8793
	step [10/160], loss=103.8937
	step [11/160], loss=89.5816
	step [12/160], loss=90.5944
	step [13/160], loss=102.4336
	step [14/160], loss=78.4025
	step [15/160], loss=82.5145
	step [16/160], loss=80.9541
	step [17/160], loss=85.9899
	step [18/160], loss=73.1653
	step [19/160], loss=87.4364
	step [20/160], loss=83.4190
	step [21/160], loss=91.0697
	step [22/160], loss=85.8259
	step [23/160], loss=97.5848
	step [24/160], loss=86.8081
	step [25/160], loss=81.8269
	step [26/160], loss=90.5133
	step [27/160], loss=97.0083
	step [28/160], loss=89.4364
	step [29/160], loss=92.2700
	step [30/160], loss=83.2558
	step [31/160], loss=89.7047
	step [32/160], loss=102.5154
	step [33/160], loss=104.8107
	step [34/160], loss=85.3472
	step [35/160], loss=81.0573
	step [36/160], loss=72.1871
	step [37/160], loss=90.0976
	step [38/160], loss=83.4791
	step [39/160], loss=84.8274
	step [40/160], loss=82.0555
	step [41/160], loss=83.6156
	step [42/160], loss=86.8017
	step [43/160], loss=92.9689
	step [44/160], loss=91.2044
	step [45/160], loss=75.5959
	step [46/160], loss=86.1871
	step [47/160], loss=80.2930
	step [48/160], loss=88.3976
	step [49/160], loss=93.3098
	step [50/160], loss=82.9780
	step [51/160], loss=84.5271
	step [52/160], loss=82.2282
	step [53/160], loss=79.3086
	step [54/160], loss=76.6100
	step [55/160], loss=75.0006
	step [56/160], loss=87.4465
	step [57/160], loss=93.6073
	step [58/160], loss=92.8408
	step [59/160], loss=87.3516
	step [60/160], loss=84.8431
	step [61/160], loss=84.9983
	step [62/160], loss=76.7398
	step [63/160], loss=76.0472
	step [64/160], loss=92.2414
	step [65/160], loss=92.6240
	step [66/160], loss=91.7096
	step [67/160], loss=75.9905
	step [68/160], loss=80.8575
	step [69/160], loss=86.4989
	step [70/160], loss=83.5804
	step [71/160], loss=95.6468
	step [72/160], loss=83.6326
	step [73/160], loss=91.4347
	step [74/160], loss=88.2652
	step [75/160], loss=82.0152
	step [76/160], loss=79.6212
	step [77/160], loss=91.1261
	step [78/160], loss=84.5712
	step [79/160], loss=84.3205
	step [80/160], loss=99.0155
	step [81/160], loss=88.3522
	step [82/160], loss=76.4114
	step [83/160], loss=82.6297
	step [84/160], loss=82.2092
	step [85/160], loss=83.9782
	step [86/160], loss=84.8455
	step [87/160], loss=83.6790
	step [88/160], loss=85.0949
	step [89/160], loss=87.1631
	step [90/160], loss=85.4879
	step [91/160], loss=90.2407
	step [92/160], loss=77.2247
	step [93/160], loss=82.2211
	step [94/160], loss=92.9141
	step [95/160], loss=84.9678
	step [96/160], loss=92.2219
	step [97/160], loss=90.5967
	step [98/160], loss=90.7915
	step [99/160], loss=76.0699
	step [100/160], loss=78.6232
	step [101/160], loss=99.3196
	step [102/160], loss=83.5171
	step [103/160], loss=81.9655
	step [104/160], loss=98.9211
	step [105/160], loss=93.3430
	step [106/160], loss=69.3875
	step [107/160], loss=79.1259
	step [108/160], loss=81.9380
	step [109/160], loss=92.8309
	step [110/160], loss=69.3248
	step [111/160], loss=93.3311
	step [112/160], loss=88.6286
	step [113/160], loss=85.8576
	step [114/160], loss=89.5179
	step [115/160], loss=96.5158
	step [116/160], loss=66.7547
	step [117/160], loss=76.2506
	step [118/160], loss=94.6557
	step [119/160], loss=92.7692
	step [120/160], loss=88.0593
	step [121/160], loss=86.6350
	step [122/160], loss=78.0060
	step [123/160], loss=88.0999
	step [124/160], loss=102.5895
	step [125/160], loss=77.4837
	step [126/160], loss=101.3883
	step [127/160], loss=101.6561
	step [128/160], loss=64.8981
	step [129/160], loss=91.5941
	step [130/160], loss=84.3587
	step [131/160], loss=84.8070
	step [132/160], loss=74.2444
	step [133/160], loss=73.2285
	step [134/160], loss=82.9591
	step [135/160], loss=96.9821
	step [136/160], loss=93.4189
	step [137/160], loss=74.8593
	step [138/160], loss=85.3195
	step [139/160], loss=90.5653
	step [140/160], loss=92.7830
	step [141/160], loss=92.6427
	step [142/160], loss=70.3428
	step [143/160], loss=86.6266
	step [144/160], loss=86.1269
	step [145/160], loss=97.5040
	step [146/160], loss=94.0785
	step [147/160], loss=93.8788
	step [148/160], loss=94.8022
	step [149/160], loss=85.8416
	step [150/160], loss=95.2461
	step [151/160], loss=99.4001
	step [152/160], loss=83.8546
	step [153/160], loss=92.6618
	step [154/160], loss=98.0209
	step [155/160], loss=87.9654
	step [156/160], loss=88.5129
	step [157/160], loss=94.6368
	step [158/160], loss=92.1371
	step [159/160], loss=81.7795
	step [160/160], loss=78.6643
	Evaluating
	loss=0.0115, precision=0.3788, recall=0.8902, f1=0.5315
Training epoch 71
	step [1/160], loss=111.4456
	step [2/160], loss=84.0174
	step [3/160], loss=92.1272
	step [4/160], loss=98.8280
	step [5/160], loss=76.0417
	step [6/160], loss=96.0206
	step [7/160], loss=91.1629
	step [8/160], loss=77.4793
	step [9/160], loss=89.6321
	step [10/160], loss=95.8843
	step [11/160], loss=88.1761
	step [12/160], loss=101.8455
	step [13/160], loss=80.8973
	step [14/160], loss=69.1325
	step [15/160], loss=81.3109
	step [16/160], loss=85.9847
	step [17/160], loss=86.3850
	step [18/160], loss=86.9877
	step [19/160], loss=95.7765
	step [20/160], loss=76.5904
	step [21/160], loss=75.7744
	step [22/160], loss=85.1153
	step [23/160], loss=86.2127
	step [24/160], loss=82.6486
	step [25/160], loss=94.8834
	step [26/160], loss=74.3443
	step [27/160], loss=89.6484
	step [28/160], loss=94.9608
	step [29/160], loss=78.4914
	step [30/160], loss=91.1440
	step [31/160], loss=86.8614
	step [32/160], loss=88.4158
	step [33/160], loss=96.4008
	step [34/160], loss=100.6968
	step [35/160], loss=101.9716
	step [36/160], loss=72.9438
	step [37/160], loss=83.1461
	step [38/160], loss=89.4506
	step [39/160], loss=89.0543
	step [40/160], loss=69.6458
	step [41/160], loss=83.7122
	step [42/160], loss=73.7737
	step [43/160], loss=97.4551
	step [44/160], loss=81.7071
	step [45/160], loss=85.6979
	step [46/160], loss=77.5435
	step [47/160], loss=77.7608
	step [48/160], loss=86.4150
	step [49/160], loss=76.8679
	step [50/160], loss=70.0391
	step [51/160], loss=84.0112
	step [52/160], loss=96.0463
	step [53/160], loss=84.8884
	step [54/160], loss=83.4834
	step [55/160], loss=96.1545
	step [56/160], loss=90.8295
	step [57/160], loss=97.9994
	step [58/160], loss=87.7545
	step [59/160], loss=86.5092
	step [60/160], loss=71.8472
	step [61/160], loss=92.0453
	step [62/160], loss=81.3663
	step [63/160], loss=79.2410
	step [64/160], loss=81.5111
	step [65/160], loss=93.1449
	step [66/160], loss=90.4460
	step [67/160], loss=101.4075
	step [68/160], loss=78.5906
	step [69/160], loss=103.9253
	step [70/160], loss=91.4610
	step [71/160], loss=79.4324
	step [72/160], loss=95.8315
	step [73/160], loss=96.2258
	step [74/160], loss=95.2768
	step [75/160], loss=89.8064
	step [76/160], loss=90.0057
	step [77/160], loss=75.7661
	step [78/160], loss=88.7030
	step [79/160], loss=86.7355
	step [80/160], loss=93.5844
	step [81/160], loss=89.8568
	step [82/160], loss=83.0942
	step [83/160], loss=70.9492
	step [84/160], loss=84.0634
	step [85/160], loss=83.0363
	step [86/160], loss=76.8240
	step [87/160], loss=77.9841
	step [88/160], loss=84.9574
	step [89/160], loss=89.9513
	step [90/160], loss=80.7149
	step [91/160], loss=81.6419
	step [92/160], loss=83.1919
	step [93/160], loss=72.5191
	step [94/160], loss=94.0265
	step [95/160], loss=74.8083
	step [96/160], loss=80.6178
	step [97/160], loss=86.9925
	step [98/160], loss=74.6012
	step [99/160], loss=96.9722
	step [100/160], loss=83.6657
	step [101/160], loss=80.1583
	step [102/160], loss=78.4744
	step [103/160], loss=84.6044
	step [104/160], loss=90.6989
	step [105/160], loss=83.9220
	step [106/160], loss=74.9901
	step [107/160], loss=80.2398
	step [108/160], loss=82.8612
	step [109/160], loss=77.1096
	step [110/160], loss=74.4416
	step [111/160], loss=91.2031
	step [112/160], loss=96.8669
	step [113/160], loss=85.6605
	step [114/160], loss=78.0866
	step [115/160], loss=103.5414
	step [116/160], loss=90.3996
	step [117/160], loss=72.0038
	step [118/160], loss=92.4569
	step [119/160], loss=96.5914
	step [120/160], loss=83.7492
	step [121/160], loss=93.0574
	step [122/160], loss=80.7272
	step [123/160], loss=81.8418
	step [124/160], loss=87.6401
	step [125/160], loss=93.1623
	step [126/160], loss=67.4062
	step [127/160], loss=80.2675
	step [128/160], loss=90.5978
	step [129/160], loss=82.5111
	step [130/160], loss=72.4957
	step [131/160], loss=90.9099
	step [132/160], loss=81.2659
	step [133/160], loss=105.5290
	step [134/160], loss=94.9928
	step [135/160], loss=85.2623
	step [136/160], loss=81.7022
	step [137/160], loss=93.0020
	step [138/160], loss=95.1940
	step [139/160], loss=101.3015
	step [140/160], loss=85.0121
	step [141/160], loss=97.7236
	step [142/160], loss=98.6799
	step [143/160], loss=87.2884
	step [144/160], loss=92.0625
	step [145/160], loss=77.4193
	step [146/160], loss=95.4433
	step [147/160], loss=103.3328
	step [148/160], loss=86.6767
	step [149/160], loss=75.0724
	step [150/160], loss=79.0059
	step [151/160], loss=91.3351
	step [152/160], loss=83.1509
	step [153/160], loss=89.8250
	step [154/160], loss=81.9493
	step [155/160], loss=106.3037
	step [156/160], loss=85.7387
	step [157/160], loss=101.6239
	step [158/160], loss=97.0857
	step [159/160], loss=79.0770
	step [160/160], loss=66.5267
	Evaluating
	loss=0.0121, precision=0.3579, recall=0.8895, f1=0.5105
Training epoch 72
	step [1/160], loss=77.9099
	step [2/160], loss=81.9379
	step [3/160], loss=70.2536
	step [4/160], loss=95.4854
	step [5/160], loss=93.0146
	step [6/160], loss=90.1481
	step [7/160], loss=98.2536
	step [8/160], loss=65.6244
	step [9/160], loss=91.2178
	step [10/160], loss=102.2074
	step [11/160], loss=91.6945
	step [12/160], loss=84.1778
	step [13/160], loss=85.4079
	step [14/160], loss=83.9348
	step [15/160], loss=92.6996
	step [16/160], loss=90.7421
	step [17/160], loss=102.4722
	step [18/160], loss=83.8544
	step [19/160], loss=78.0419
	step [20/160], loss=90.2203
	step [21/160], loss=89.3355
	step [22/160], loss=85.1376
	step [23/160], loss=89.1902
	step [24/160], loss=85.9675
	step [25/160], loss=84.2458
	step [26/160], loss=72.6337
	step [27/160], loss=82.8169
	step [28/160], loss=94.1940
	step [29/160], loss=102.3977
	step [30/160], loss=94.6255
	step [31/160], loss=76.7108
	step [32/160], loss=88.7037
	step [33/160], loss=77.3605
	step [34/160], loss=90.2886
	step [35/160], loss=88.8995
	step [36/160], loss=97.4738
	step [37/160], loss=86.2188
	step [38/160], loss=95.3412
	step [39/160], loss=90.5503
	step [40/160], loss=69.2271
	step [41/160], loss=82.7024
	step [42/160], loss=86.9907
	step [43/160], loss=83.4370
	step [44/160], loss=75.5476
	step [45/160], loss=91.4315
	step [46/160], loss=80.1697
	step [47/160], loss=87.9706
	step [48/160], loss=74.6724
	step [49/160], loss=71.7365
	step [50/160], loss=83.1465
	step [51/160], loss=73.1798
	step [52/160], loss=87.6499
	step [53/160], loss=86.8961
	step [54/160], loss=94.2414
	step [55/160], loss=75.4650
	step [56/160], loss=77.3453
	step [57/160], loss=108.1820
	step [58/160], loss=77.3187
	step [59/160], loss=80.4837
	step [60/160], loss=82.2373
	step [61/160], loss=76.5055
	step [62/160], loss=80.8886
	step [63/160], loss=96.3598
	step [64/160], loss=74.7445
	step [65/160], loss=83.3820
	step [66/160], loss=94.9818
	step [67/160], loss=94.5706
	step [68/160], loss=64.6802
	step [69/160], loss=105.7989
	step [70/160], loss=74.4374
	step [71/160], loss=84.1521
	step [72/160], loss=75.0845
	step [73/160], loss=89.7535
	step [74/160], loss=82.8162
	step [75/160], loss=80.1017
	step [76/160], loss=88.1218
	step [77/160], loss=79.5772
	step [78/160], loss=81.7124
	step [79/160], loss=93.5650
	step [80/160], loss=87.8484
	step [81/160], loss=88.8047
	step [82/160], loss=88.2754
	step [83/160], loss=81.8136
	step [84/160], loss=80.2424
	step [85/160], loss=81.6404
	step [86/160], loss=90.8093
	step [87/160], loss=86.4595
	step [88/160], loss=102.2692
	step [89/160], loss=78.2081
	step [90/160], loss=81.0257
	step [91/160], loss=107.6809
	step [92/160], loss=87.0370
	step [93/160], loss=106.4939
	step [94/160], loss=90.6179
	step [95/160], loss=98.3471
	step [96/160], loss=78.8004
	step [97/160], loss=75.5627
	step [98/160], loss=86.1420
	step [99/160], loss=84.8638
	step [100/160], loss=90.5541
	step [101/160], loss=85.9526
	step [102/160], loss=74.9023
	step [103/160], loss=85.7780
	step [104/160], loss=91.0454
	step [105/160], loss=84.2324
	step [106/160], loss=97.1217
	step [107/160], loss=95.6347
	step [108/160], loss=94.0866
	step [109/160], loss=75.1518
	step [110/160], loss=91.4123
	step [111/160], loss=80.6260
	step [112/160], loss=79.9780
	step [113/160], loss=78.0888
	step [114/160], loss=84.4039
	step [115/160], loss=89.4953
	step [116/160], loss=84.3230
	step [117/160], loss=99.6640
	step [118/160], loss=87.2395
	step [119/160], loss=94.5441
	step [120/160], loss=81.3822
	step [121/160], loss=88.6473
	step [122/160], loss=94.9155
	step [123/160], loss=72.4517
	step [124/160], loss=90.9252
	step [125/160], loss=92.3033
	step [126/160], loss=95.6771
	step [127/160], loss=77.1367
	step [128/160], loss=103.6926
	step [129/160], loss=91.5749
	step [130/160], loss=86.1078
	step [131/160], loss=76.7520
	step [132/160], loss=82.7911
	step [133/160], loss=98.3421
	step [134/160], loss=89.0207
	step [135/160], loss=89.7313
	step [136/160], loss=76.6265
	step [137/160], loss=88.7052
	step [138/160], loss=94.3330
	step [139/160], loss=84.4774
	step [140/160], loss=92.2558
	step [141/160], loss=83.1855
	step [142/160], loss=83.3017
	step [143/160], loss=78.2412
	step [144/160], loss=78.2652
	step [145/160], loss=75.5514
	step [146/160], loss=79.4080
	step [147/160], loss=87.3663
	step [148/160], loss=78.9591
	step [149/160], loss=77.5899
	step [150/160], loss=93.9152
	step [151/160], loss=79.8047
	step [152/160], loss=97.1941
	step [153/160], loss=79.0982
	step [154/160], loss=86.4683
	step [155/160], loss=98.2448
	step [156/160], loss=89.3342
	step [157/160], loss=81.2724
	step [158/160], loss=93.8314
	step [159/160], loss=76.9644
	step [160/160], loss=75.6488
	Evaluating
	loss=0.0121, precision=0.3682, recall=0.8994, f1=0.5225
Training epoch 73
	step [1/160], loss=85.4831
	step [2/160], loss=120.9333
	step [3/160], loss=89.3061
	step [4/160], loss=80.0358
	step [5/160], loss=83.4021
	step [6/160], loss=104.1053
	step [7/160], loss=100.6570
	step [8/160], loss=82.8503
	step [9/160], loss=85.1667
	step [10/160], loss=90.3653
	step [11/160], loss=77.6187
	step [12/160], loss=82.8895
	step [13/160], loss=109.6800
	step [14/160], loss=80.3302
	step [15/160], loss=100.7204
	step [16/160], loss=79.3530
	step [17/160], loss=84.1288
	step [18/160], loss=82.1683
	step [19/160], loss=87.6936
	step [20/160], loss=100.7175
	step [21/160], loss=69.2947
	step [22/160], loss=87.8679
	step [23/160], loss=98.3449
	step [24/160], loss=87.9937
	step [25/160], loss=77.2191
	step [26/160], loss=88.4748
	step [27/160], loss=72.0620
	step [28/160], loss=95.8859
	step [29/160], loss=82.3878
	step [30/160], loss=99.4063
	step [31/160], loss=98.4815
	step [32/160], loss=95.7069
	step [33/160], loss=85.5381
	step [34/160], loss=81.3259
	step [35/160], loss=94.3738
	step [36/160], loss=85.8166
	step [37/160], loss=88.9648
	step [38/160], loss=88.1366
	step [39/160], loss=82.0615
	step [40/160], loss=81.0095
	step [41/160], loss=93.0660
	step [42/160], loss=93.8354
	step [43/160], loss=65.2420
	step [44/160], loss=87.7114
	step [45/160], loss=79.3237
	step [46/160], loss=89.9159
	step [47/160], loss=81.8097
	step [48/160], loss=78.7080
	step [49/160], loss=73.4914
	step [50/160], loss=81.5029
	step [51/160], loss=97.6024
	step [52/160], loss=86.6937
	step [53/160], loss=98.7693
	step [54/160], loss=96.8720
	step [55/160], loss=76.3827
	step [56/160], loss=83.5350
	step [57/160], loss=78.7161
	step [58/160], loss=83.6757
	step [59/160], loss=77.7286
	step [60/160], loss=84.7778
	step [61/160], loss=77.2256
	step [62/160], loss=78.4420
	step [63/160], loss=96.5589
	step [64/160], loss=82.5795
	step [65/160], loss=92.0262
	step [66/160], loss=81.6600
	step [67/160], loss=98.1439
	step [68/160], loss=80.4303
	step [69/160], loss=67.9116
	step [70/160], loss=79.8088
	step [71/160], loss=72.5097
	step [72/160], loss=95.9139
	step [73/160], loss=88.4885
	step [74/160], loss=83.6934
	step [75/160], loss=88.4271
	step [76/160], loss=75.0595
	step [77/160], loss=85.0825
	step [78/160], loss=93.1806
	step [79/160], loss=81.5314
	step [80/160], loss=99.4541
	step [81/160], loss=88.1281
	step [82/160], loss=93.6348
	step [83/160], loss=84.3727
	step [84/160], loss=77.8976
	step [85/160], loss=74.9153
	step [86/160], loss=87.4674
	step [87/160], loss=97.6261
	step [88/160], loss=90.2918
	step [89/160], loss=94.0822
	step [90/160], loss=87.3173
	step [91/160], loss=82.2488
	step [92/160], loss=92.0472
	step [93/160], loss=91.9738
	step [94/160], loss=88.2149
	step [95/160], loss=85.5614
	step [96/160], loss=85.7654
	step [97/160], loss=90.7838
	step [98/160], loss=98.2005
	step [99/160], loss=96.2902
	step [100/160], loss=77.8997
	step [101/160], loss=88.7677
	step [102/160], loss=82.4216
	step [103/160], loss=76.1186
	step [104/160], loss=79.3274
	step [105/160], loss=79.9209
	step [106/160], loss=107.1506
	step [107/160], loss=94.8775
	step [108/160], loss=92.5986
	step [109/160], loss=90.7980
	step [110/160], loss=97.5184
	step [111/160], loss=74.0650
	step [112/160], loss=61.4443
	step [113/160], loss=69.9007
	step [114/160], loss=74.1654
	step [115/160], loss=90.9795
	step [116/160], loss=78.7507
	step [117/160], loss=105.8059
	step [118/160], loss=74.5852
	step [119/160], loss=87.0750
	step [120/160], loss=98.1054
	step [121/160], loss=67.3045
	step [122/160], loss=90.9788
	step [123/160], loss=93.0828
	step [124/160], loss=86.7708
	step [125/160], loss=93.8830
	step [126/160], loss=89.1811
	step [127/160], loss=91.2667
	step [128/160], loss=85.0342
	step [129/160], loss=103.4530
	step [130/160], loss=65.6367
	step [131/160], loss=78.5876
	step [132/160], loss=79.5160
	step [133/160], loss=82.2496
	step [134/160], loss=76.1265
	step [135/160], loss=78.5207
	step [136/160], loss=74.8323
	step [137/160], loss=92.2588
	step [138/160], loss=88.7525
	step [139/160], loss=92.3666
	step [140/160], loss=83.1427
	step [141/160], loss=81.8916
	step [142/160], loss=83.4315
	step [143/160], loss=87.0820
	step [144/160], loss=78.2937
	step [145/160], loss=74.0145
	step [146/160], loss=73.7449
	step [147/160], loss=104.4216
	step [148/160], loss=100.7215
	step [149/160], loss=86.7106
	step [150/160], loss=79.6942
	step [151/160], loss=99.4346
	step [152/160], loss=96.1920
	step [153/160], loss=78.6751
	step [154/160], loss=83.5347
	step [155/160], loss=88.8360
	step [156/160], loss=73.5994
	step [157/160], loss=93.1285
	step [158/160], loss=92.2335
	step [159/160], loss=75.8317
	step [160/160], loss=55.6145
	Evaluating
	loss=0.0124, precision=0.3484, recall=0.8876, f1=0.5004
Training epoch 74
	step [1/160], loss=91.9650
	step [2/160], loss=83.6131
	step [3/160], loss=71.3983
	step [4/160], loss=91.7297
	step [5/160], loss=81.5420
	step [6/160], loss=96.5473
	step [7/160], loss=79.6868
	step [8/160], loss=90.0300
	step [9/160], loss=70.2621
	step [10/160], loss=92.5398
	step [11/160], loss=94.6108
	step [12/160], loss=83.2972
	step [13/160], loss=88.5224
	step [14/160], loss=75.9382
	step [15/160], loss=88.1050
	step [16/160], loss=91.7657
	step [17/160], loss=76.5390
	step [18/160], loss=81.1818
	step [19/160], loss=92.0847
	step [20/160], loss=85.0124
	step [21/160], loss=89.0687
	step [22/160], loss=80.1731
	step [23/160], loss=103.6048
	step [24/160], loss=81.2280
	step [25/160], loss=87.7545
	step [26/160], loss=89.9990
	step [27/160], loss=92.6919
	step [28/160], loss=88.0943
	step [29/160], loss=102.3777
	step [30/160], loss=82.5567
	step [31/160], loss=87.1116
	step [32/160], loss=92.4461
	step [33/160], loss=66.8254
	step [34/160], loss=101.7748
	step [35/160], loss=65.7932
	step [36/160], loss=84.0291
	step [37/160], loss=87.9852
	step [38/160], loss=93.5270
	step [39/160], loss=100.2243
	step [40/160], loss=85.6812
	step [41/160], loss=90.8671
	step [42/160], loss=90.1585
	step [43/160], loss=66.9114
	step [44/160], loss=85.6815
	step [45/160], loss=93.0918
	step [46/160], loss=72.1255
	step [47/160], loss=79.4879
	step [48/160], loss=96.6733
	step [49/160], loss=93.0981
	step [50/160], loss=92.3674
	step [51/160], loss=79.0807
	step [52/160], loss=81.5694
	step [53/160], loss=92.3464
	step [54/160], loss=93.5982
	step [55/160], loss=104.2173
	step [56/160], loss=81.4767
	step [57/160], loss=86.7045
	step [58/160], loss=83.6593
	step [59/160], loss=104.1729
	step [60/160], loss=87.2606
	step [61/160], loss=78.2953
	step [62/160], loss=69.5437
	step [63/160], loss=72.8874
	step [64/160], loss=89.3475
	step [65/160], loss=99.4903
	step [66/160], loss=86.3417
	step [67/160], loss=80.8059
	step [68/160], loss=90.0722
	step [69/160], loss=79.9644
	step [70/160], loss=95.5042
	step [71/160], loss=86.3182
	step [72/160], loss=89.3578
	step [73/160], loss=68.2664
	step [74/160], loss=92.1932
	step [75/160], loss=97.7669
	step [76/160], loss=93.2210
	step [77/160], loss=84.5294
	step [78/160], loss=68.7276
	step [79/160], loss=86.1545
	step [80/160], loss=96.8479
	step [81/160], loss=79.9062
	step [82/160], loss=86.2974
	step [83/160], loss=75.6416
	step [84/160], loss=75.1161
	step [85/160], loss=88.4859
	step [86/160], loss=87.7607
	step [87/160], loss=79.8438
	step [88/160], loss=102.6504
	step [89/160], loss=77.9679
	step [90/160], loss=95.0374
	step [91/160], loss=75.1286
	step [92/160], loss=72.1700
	step [93/160], loss=76.0949
	step [94/160], loss=80.5471
	step [95/160], loss=94.1482
	step [96/160], loss=92.8741
	step [97/160], loss=101.3397
	step [98/160], loss=87.9717
	step [99/160], loss=99.3690
	step [100/160], loss=78.9538
	step [101/160], loss=91.6379
	step [102/160], loss=77.6724
	step [103/160], loss=79.5345
	step [104/160], loss=82.1704
	step [105/160], loss=88.7536
	step [106/160], loss=91.6533
	step [107/160], loss=97.9541
	step [108/160], loss=67.0318
	step [109/160], loss=92.8415
	step [110/160], loss=89.6108
	step [111/160], loss=93.1996
	step [112/160], loss=71.0580
	step [113/160], loss=93.0272
	step [114/160], loss=85.6338
	step [115/160], loss=89.8752
	step [116/160], loss=84.2484
	step [117/160], loss=102.6084
	step [118/160], loss=88.4048
	step [119/160], loss=87.6870
	step [120/160], loss=86.2068
	step [121/160], loss=77.0394
	step [122/160], loss=75.2973
	step [123/160], loss=80.2185
	step [124/160], loss=75.6861
	step [125/160], loss=93.1127
	step [126/160], loss=83.7595
	step [127/160], loss=83.1828
	step [128/160], loss=76.4080
	step [129/160], loss=70.9933
	step [130/160], loss=82.8037
	step [131/160], loss=68.2342
	step [132/160], loss=80.8404
	step [133/160], loss=85.6870
	step [134/160], loss=73.5287
	step [135/160], loss=88.4518
	step [136/160], loss=79.4678
	step [137/160], loss=92.9184
	step [138/160], loss=85.4877
	step [139/160], loss=100.3375
	step [140/160], loss=70.7221
	step [141/160], loss=97.6343
	step [142/160], loss=80.8780
	step [143/160], loss=86.1995
	step [144/160], loss=93.5170
	step [145/160], loss=71.6771
	step [146/160], loss=84.0808
	step [147/160], loss=92.8046
	step [148/160], loss=98.3051
	step [149/160], loss=75.4791
	step [150/160], loss=100.9698
	step [151/160], loss=87.8238
	step [152/160], loss=79.3919
	step [153/160], loss=78.6783
	step [154/160], loss=73.4743
	step [155/160], loss=89.3742
	step [156/160], loss=85.3267
	step [157/160], loss=93.3096
	step [158/160], loss=89.8637
	step [159/160], loss=83.2447
	step [160/160], loss=60.9260
	Evaluating
	loss=0.0118, precision=0.3619, recall=0.8961, f1=0.5156
Training epoch 75
	step [1/160], loss=80.9813
	step [2/160], loss=83.7626
	step [3/160], loss=85.4587
	step [4/160], loss=65.9971
	step [5/160], loss=81.6280
	step [6/160], loss=97.4351
	step [7/160], loss=67.3422
	step [8/160], loss=81.4308
	step [9/160], loss=78.5324
	step [10/160], loss=79.4697
	step [11/160], loss=84.1891
	step [12/160], loss=78.4197
	step [13/160], loss=88.5741
	step [14/160], loss=84.1869
	step [15/160], loss=74.5273
	step [16/160], loss=77.2877
	step [17/160], loss=92.2092
	step [18/160], loss=82.6710
	step [19/160], loss=84.5664
	step [20/160], loss=72.2862
	step [21/160], loss=66.0439
	step [22/160], loss=76.6530
	step [23/160], loss=88.6080
	step [24/160], loss=84.2337
	step [25/160], loss=97.4759
	step [26/160], loss=77.2316
	step [27/160], loss=89.4022
	step [28/160], loss=76.2105
	step [29/160], loss=90.0081
	step [30/160], loss=92.8258
	step [31/160], loss=77.2451
	step [32/160], loss=85.7736
	step [33/160], loss=80.4374
	step [34/160], loss=85.0458
	step [35/160], loss=76.5096
	step [36/160], loss=90.9390
	step [37/160], loss=81.5770
	step [38/160], loss=104.4232
	step [39/160], loss=107.2873
	step [40/160], loss=81.7194
	step [41/160], loss=71.0084
	step [42/160], loss=88.9835
	step [43/160], loss=85.3267
	step [44/160], loss=82.9180
	step [45/160], loss=89.9174
	step [46/160], loss=89.9416
	step [47/160], loss=98.5978
	step [48/160], loss=90.3450
	step [49/160], loss=67.6201
	step [50/160], loss=86.0475
	step [51/160], loss=90.0655
	step [52/160], loss=88.8240
	step [53/160], loss=90.8469
	step [54/160], loss=99.2063
	step [55/160], loss=91.2656
	step [56/160], loss=91.8673
	step [57/160], loss=80.0286
	step [58/160], loss=64.3446
	step [59/160], loss=83.1350
	step [60/160], loss=80.5857
	step [61/160], loss=83.4197
	step [62/160], loss=87.3464
	step [63/160], loss=73.9022
	step [64/160], loss=81.5232
	step [65/160], loss=85.9355
	step [66/160], loss=85.5239
	step [67/160], loss=74.2935
	step [68/160], loss=104.6902
	step [69/160], loss=84.8866
	step [70/160], loss=84.4948
	step [71/160], loss=89.5618
	step [72/160], loss=82.7572
	step [73/160], loss=95.9040
	step [74/160], loss=82.0679
	step [75/160], loss=74.4124
	step [76/160], loss=92.8262
	step [77/160], loss=89.0647
	step [78/160], loss=65.5339
	step [79/160], loss=76.6961
	step [80/160], loss=86.3616
	step [81/160], loss=94.3636
	step [82/160], loss=89.9532
	step [83/160], loss=77.8346
	step [84/160], loss=89.7036
	step [85/160], loss=94.1389
	step [86/160], loss=77.9463
	step [87/160], loss=104.3599
	step [88/160], loss=83.8266
	step [89/160], loss=89.0002
	step [90/160], loss=73.7067
	step [91/160], loss=84.2180
	step [92/160], loss=103.4805
	step [93/160], loss=79.5103
	step [94/160], loss=97.4492
	step [95/160], loss=99.6131
	step [96/160], loss=79.9339
	step [97/160], loss=90.3849
	step [98/160], loss=82.8122
	step [99/160], loss=84.0108
	step [100/160], loss=78.1937
	step [101/160], loss=71.6558
	step [102/160], loss=80.6908
	step [103/160], loss=103.5853
	step [104/160], loss=89.3563
	step [105/160], loss=97.1208
	step [106/160], loss=74.5797
	step [107/160], loss=89.4968
	step [108/160], loss=97.3933
	step [109/160], loss=79.3858
	step [110/160], loss=91.4550
	step [111/160], loss=87.8802
	step [112/160], loss=101.4374
	step [113/160], loss=91.3581
	step [114/160], loss=73.7486
	step [115/160], loss=107.3167
	step [116/160], loss=83.3751
	step [117/160], loss=90.1855
	step [118/160], loss=84.4644
	step [119/160], loss=81.0906
	step [120/160], loss=86.7687
	step [121/160], loss=87.3015
	step [122/160], loss=90.9614
	step [123/160], loss=79.2984
	step [124/160], loss=92.4183
	step [125/160], loss=77.0842
	step [126/160], loss=68.4160
	step [127/160], loss=86.4654
	step [128/160], loss=90.9593
	step [129/160], loss=79.9709
	step [130/160], loss=73.8438
	step [131/160], loss=83.1520
	step [132/160], loss=86.1106
	step [133/160], loss=83.7703
	step [134/160], loss=82.3572
	step [135/160], loss=80.3173
	step [136/160], loss=82.0727
	step [137/160], loss=87.5122
	step [138/160], loss=92.0089
	step [139/160], loss=86.5379
	step [140/160], loss=73.1291
	step [141/160], loss=93.5636
	step [142/160], loss=92.6160
	step [143/160], loss=96.0375
	step [144/160], loss=97.3503
	step [145/160], loss=90.7447
	step [146/160], loss=84.5084
	step [147/160], loss=64.4146
	step [148/160], loss=84.4116
	step [149/160], loss=92.8202
	step [150/160], loss=82.8348
	step [151/160], loss=84.4823
	step [152/160], loss=95.9335
	step [153/160], loss=84.1432
	step [154/160], loss=87.1687
	step [155/160], loss=104.8108
	step [156/160], loss=86.6919
	step [157/160], loss=93.9052
	step [158/160], loss=76.1772
	step [159/160], loss=83.1396
	step [160/160], loss=71.8121
	Evaluating
	loss=0.0095, precision=0.4413, recall=0.8792, f1=0.5877
saving model as: 4_saved_model.pth
Training epoch 76
	step [1/160], loss=90.0715
	step [2/160], loss=71.5851
	step [3/160], loss=91.6372
	step [4/160], loss=98.0829
	step [5/160], loss=66.8486
	step [6/160], loss=73.6975
	step [7/160], loss=94.5394
	step [8/160], loss=81.1437
	step [9/160], loss=96.1819
	step [10/160], loss=82.8731
	step [11/160], loss=83.0547
	step [12/160], loss=85.5082
	step [13/160], loss=76.9949
	step [14/160], loss=96.3550
	step [15/160], loss=70.8473
	step [16/160], loss=74.8159
	step [17/160], loss=90.0185
	step [18/160], loss=93.1393
	step [19/160], loss=95.6456
	step [20/160], loss=71.6409
	step [21/160], loss=86.2314
	step [22/160], loss=85.2074
	step [23/160], loss=79.2298
	step [24/160], loss=79.8754
	step [25/160], loss=90.2441
	step [26/160], loss=77.7056
	step [27/160], loss=81.4064
	step [28/160], loss=83.9242
	step [29/160], loss=78.0740
	step [30/160], loss=77.6277
	step [31/160], loss=76.0584
	step [32/160], loss=80.5635
	step [33/160], loss=77.8687
	step [34/160], loss=102.1689
	step [35/160], loss=80.1948
	step [36/160], loss=79.1190
	step [37/160], loss=67.5883
	step [38/160], loss=87.8369
	step [39/160], loss=74.4295
	step [40/160], loss=92.4108
	step [41/160], loss=90.5227
	step [42/160], loss=92.6620
	step [43/160], loss=84.8218
	step [44/160], loss=85.5334
	step [45/160], loss=78.1331
	step [46/160], loss=81.9567
	step [47/160], loss=72.1040
	step [48/160], loss=68.3531
	step [49/160], loss=80.1784
	step [50/160], loss=90.6863
	step [51/160], loss=81.2979
	step [52/160], loss=85.0472
	step [53/160], loss=78.9055
	step [54/160], loss=90.1803
	step [55/160], loss=74.6882
	step [56/160], loss=83.8329
	step [57/160], loss=95.5099
	step [58/160], loss=100.9157
	step [59/160], loss=77.6837
	step [60/160], loss=105.8025
	step [61/160], loss=92.6239
	step [62/160], loss=87.4682
	step [63/160], loss=81.4827
	step [64/160], loss=71.1447
	step [65/160], loss=76.4195
	step [66/160], loss=78.9697
	step [67/160], loss=84.6562
	step [68/160], loss=77.6562
	step [69/160], loss=81.3905
	step [70/160], loss=82.6709
	step [71/160], loss=83.2031
	step [72/160], loss=98.6863
	step [73/160], loss=88.1202
	step [74/160], loss=86.1248
	step [75/160], loss=98.1024
	step [76/160], loss=81.5041
	step [77/160], loss=87.5359
	step [78/160], loss=67.4010
	step [79/160], loss=71.7141
	step [80/160], loss=86.7899
	step [81/160], loss=83.6087
	step [82/160], loss=96.1460
	step [83/160], loss=78.3070
	step [84/160], loss=86.5599
	step [85/160], loss=82.3975
	step [86/160], loss=103.0373
	step [87/160], loss=89.7574
	step [88/160], loss=90.6982
	step [89/160], loss=95.5004
	step [90/160], loss=88.1391
	step [91/160], loss=77.2740
	step [92/160], loss=75.2979
	step [93/160], loss=105.7080
	step [94/160], loss=85.4083
	step [95/160], loss=75.6310
	step [96/160], loss=79.2422
	step [97/160], loss=85.7675
	step [98/160], loss=85.1483
	step [99/160], loss=90.7052
	step [100/160], loss=98.6580
	step [101/160], loss=90.7228
	step [102/160], loss=72.7893
	step [103/160], loss=88.5765
	step [104/160], loss=95.8590
	step [105/160], loss=80.7196
	step [106/160], loss=89.4915
	step [107/160], loss=73.7855
	step [108/160], loss=76.5922
	step [109/160], loss=74.3689
	step [110/160], loss=78.0912
	step [111/160], loss=96.6717
	step [112/160], loss=100.8981
	step [113/160], loss=84.9937
	step [114/160], loss=100.2149
	step [115/160], loss=93.8027
	step [116/160], loss=89.0019
	step [117/160], loss=85.7291
	step [118/160], loss=89.8780
	step [119/160], loss=71.5585
	step [120/160], loss=76.1935
	step [121/160], loss=73.2540
	step [122/160], loss=90.1881
	step [123/160], loss=79.1357
	step [124/160], loss=67.2994
	step [125/160], loss=79.4608
	step [126/160], loss=86.4807
	step [127/160], loss=92.5295
	step [128/160], loss=86.1909
	step [129/160], loss=77.5036
	step [130/160], loss=108.7392
	step [131/160], loss=80.4656
	step [132/160], loss=86.4541
	step [133/160], loss=90.2408
	step [134/160], loss=83.8651
	step [135/160], loss=82.9417
	step [136/160], loss=92.9598
	step [137/160], loss=96.0535
	step [138/160], loss=70.3706
	step [139/160], loss=74.3086
	step [140/160], loss=80.2995
	step [141/160], loss=96.9484
	step [142/160], loss=101.9335
	step [143/160], loss=89.1626
	step [144/160], loss=89.2910
	step [145/160], loss=85.6329
	step [146/160], loss=89.2453
	step [147/160], loss=80.7547
	step [148/160], loss=94.9195
	step [149/160], loss=99.7804
	step [150/160], loss=79.9747
	step [151/160], loss=78.3839
	step [152/160], loss=98.2213
	step [153/160], loss=87.0395
	step [154/160], loss=89.8725
	step [155/160], loss=82.9447
	step [156/160], loss=78.1217
	step [157/160], loss=98.5927
	step [158/160], loss=78.5814
	step [159/160], loss=94.6713
	step [160/160], loss=62.3066
	Evaluating
	loss=0.0112, precision=0.3721, recall=0.8953, f1=0.5257
Training epoch 77
	step [1/160], loss=95.4778
	step [2/160], loss=70.6583
	step [3/160], loss=91.8087
	step [4/160], loss=78.7560
	step [5/160], loss=76.4089
	step [6/160], loss=96.1149
	step [7/160], loss=84.3619
	step [8/160], loss=95.5228
	step [9/160], loss=81.4899
	step [10/160], loss=84.5257
	step [11/160], loss=89.2095
	step [12/160], loss=92.4279
	step [13/160], loss=74.6865
	step [14/160], loss=76.0953
	step [15/160], loss=85.5856
	step [16/160], loss=76.0176
	step [17/160], loss=79.2336
	step [18/160], loss=91.3779
	step [19/160], loss=72.3274
	step [20/160], loss=76.7627
	step [21/160], loss=78.1758
	step [22/160], loss=74.6320
	step [23/160], loss=82.4919
	step [24/160], loss=100.4586
	step [25/160], loss=96.8774
	step [26/160], loss=88.1252
	step [27/160], loss=71.0257
	step [28/160], loss=85.1901
	step [29/160], loss=86.9143
	step [30/160], loss=85.3869
	step [31/160], loss=70.1611
	step [32/160], loss=86.0493
	step [33/160], loss=71.9107
	step [34/160], loss=84.2094
	step [35/160], loss=82.0979
	step [36/160], loss=79.4867
	step [37/160], loss=80.2395
	step [38/160], loss=76.6312
	step [39/160], loss=89.4545
	step [40/160], loss=89.3286
	step [41/160], loss=94.1359
	step [42/160], loss=89.5218
	step [43/160], loss=104.9600
	step [44/160], loss=84.4941
	step [45/160], loss=68.3445
	step [46/160], loss=81.8031
	step [47/160], loss=98.4903
	step [48/160], loss=86.2963
	step [49/160], loss=78.1842
	step [50/160], loss=99.0760
	step [51/160], loss=84.8140
	step [52/160], loss=72.5182
	step [53/160], loss=89.3917
	step [54/160], loss=77.4087
	step [55/160], loss=72.7957
	step [56/160], loss=84.7209
	step [57/160], loss=77.3575
	step [58/160], loss=73.2370
	step [59/160], loss=85.0197
	step [60/160], loss=87.9457
	step [61/160], loss=102.6201
	step [62/160], loss=91.4026
	step [63/160], loss=92.1960
	step [64/160], loss=82.3322
	step [65/160], loss=72.3748
	step [66/160], loss=83.5104
	step [67/160], loss=79.0133
	step [68/160], loss=83.9875
	step [69/160], loss=89.4004
	step [70/160], loss=81.1879
	step [71/160], loss=94.5723
	step [72/160], loss=77.8127
	step [73/160], loss=92.8482
	step [74/160], loss=82.1477
	step [75/160], loss=75.4570
	step [76/160], loss=82.7024
	step [77/160], loss=81.8082
	step [78/160], loss=86.6120
	step [79/160], loss=95.0475
	step [80/160], loss=77.4979
	step [81/160], loss=82.2578
	step [82/160], loss=87.6169
	step [83/160], loss=84.9978
	step [84/160], loss=81.1013
	step [85/160], loss=76.2404
	step [86/160], loss=80.4764
	step [87/160], loss=93.6769
	step [88/160], loss=89.3899
	step [89/160], loss=83.7942
	step [90/160], loss=87.4098
	step [91/160], loss=78.9148
	step [92/160], loss=75.8896
	step [93/160], loss=97.5507
	step [94/160], loss=89.4878
	step [95/160], loss=82.9765
	step [96/160], loss=92.6206
	step [97/160], loss=75.9653
	step [98/160], loss=97.0993
	step [99/160], loss=74.4818
	step [100/160], loss=95.0049
	step [101/160], loss=72.0034
	step [102/160], loss=92.1370
	step [103/160], loss=82.6564
	step [104/160], loss=104.2393
	step [105/160], loss=94.5644
	step [106/160], loss=80.5868
	step [107/160], loss=81.6023
	step [108/160], loss=82.1856
	step [109/160], loss=80.7177
	step [110/160], loss=88.7881
	step [111/160], loss=79.6804
	step [112/160], loss=85.0678
	step [113/160], loss=100.9660
	step [114/160], loss=84.6392
	step [115/160], loss=91.1351
	step [116/160], loss=90.6738
	step [117/160], loss=88.2604
	step [118/160], loss=95.8433
	step [119/160], loss=79.7342
	step [120/160], loss=80.7595
	step [121/160], loss=88.8279
	step [122/160], loss=90.3794
	step [123/160], loss=94.1280
	step [124/160], loss=75.5109
	step [125/160], loss=85.1611
	step [126/160], loss=87.1738
	step [127/160], loss=78.5035
	step [128/160], loss=74.9384
	step [129/160], loss=86.1813
	step [130/160], loss=87.7264
	step [131/160], loss=89.8284
	step [132/160], loss=86.4519
	step [133/160], loss=83.5033
	step [134/160], loss=79.0909
	step [135/160], loss=99.3105
	step [136/160], loss=68.9484
	step [137/160], loss=100.6750
	step [138/160], loss=71.3824
	step [139/160], loss=87.2392
	step [140/160], loss=77.5940
	step [141/160], loss=89.9533
	step [142/160], loss=73.4011
	step [143/160], loss=78.1609
	step [144/160], loss=105.6220
	step [145/160], loss=95.9205
	step [146/160], loss=71.7684
	step [147/160], loss=85.2782
	step [148/160], loss=71.4938
	step [149/160], loss=97.5968
	step [150/160], loss=83.6766
	step [151/160], loss=81.6648
	step [152/160], loss=69.2468
	step [153/160], loss=94.0895
	step [154/160], loss=93.8811
	step [155/160], loss=94.3464
	step [156/160], loss=86.2735
	step [157/160], loss=79.7195
	step [158/160], loss=104.4590
	step [159/160], loss=86.0941
	step [160/160], loss=76.7816
	Evaluating
	loss=0.0136, precision=0.3204, recall=0.9041, f1=0.4732
Training epoch 78
	step [1/160], loss=84.8767
	step [2/160], loss=84.7775
	step [3/160], loss=65.8867
	step [4/160], loss=82.8886
	step [5/160], loss=108.1748
	step [6/160], loss=78.2881
	step [7/160], loss=106.2960
	step [8/160], loss=94.3834
	step [9/160], loss=102.7259
	step [10/160], loss=80.2704
	step [11/160], loss=83.8268
	step [12/160], loss=98.9412
	step [13/160], loss=87.5483
	step [14/160], loss=76.1319
	step [15/160], loss=74.9272
	step [16/160], loss=79.3668
	step [17/160], loss=66.0009
	step [18/160], loss=74.0292
	step [19/160], loss=84.2867
	step [20/160], loss=83.7631
	step [21/160], loss=92.8783
	step [22/160], loss=81.8186
	step [23/160], loss=76.8119
	step [24/160], loss=90.7296
	step [25/160], loss=90.5544
	step [26/160], loss=90.1464
	step [27/160], loss=79.0295
	step [28/160], loss=72.1394
	step [29/160], loss=82.3159
	step [30/160], loss=89.7684
	step [31/160], loss=91.5378
	step [32/160], loss=87.5919
	step [33/160], loss=83.3750
	step [34/160], loss=76.5904
	step [35/160], loss=72.6962
	step [36/160], loss=85.2876
	step [37/160], loss=73.4411
	step [38/160], loss=73.0987
	step [39/160], loss=85.3701
	step [40/160], loss=88.3852
	step [41/160], loss=74.2896
	step [42/160], loss=103.7312
	step [43/160], loss=77.0430
	step [44/160], loss=78.2813
	step [45/160], loss=86.5714
	step [46/160], loss=78.5824
	step [47/160], loss=90.3492
	step [48/160], loss=81.3440
	step [49/160], loss=73.9537
	step [50/160], loss=105.1855
	step [51/160], loss=91.6704
	step [52/160], loss=69.9873
	step [53/160], loss=70.6970
	step [54/160], loss=94.3414
	step [55/160], loss=72.7300
	step [56/160], loss=79.4776
	step [57/160], loss=89.8662
	step [58/160], loss=95.8239
	step [59/160], loss=94.3738
	step [60/160], loss=83.7696
	step [61/160], loss=89.6950
	step [62/160], loss=86.6132
	step [63/160], loss=82.9718
	step [64/160], loss=99.5710
	step [65/160], loss=75.6807
	step [66/160], loss=78.7665
	step [67/160], loss=70.4643
	step [68/160], loss=81.4297
	step [69/160], loss=94.5054
	step [70/160], loss=88.4855
	step [71/160], loss=101.4418
	step [72/160], loss=65.0409
	step [73/160], loss=75.1507
	step [74/160], loss=89.6156
	step [75/160], loss=71.3805
	step [76/160], loss=92.4383
	step [77/160], loss=84.4929
	step [78/160], loss=83.8728
	step [79/160], loss=83.4819
	step [80/160], loss=98.3081
	step [81/160], loss=87.7111
	step [82/160], loss=87.6941
	step [83/160], loss=70.3654
	step [84/160], loss=80.1258
	step [85/160], loss=78.7124
	step [86/160], loss=93.2635
	step [87/160], loss=80.3979
	step [88/160], loss=79.0693
	step [89/160], loss=97.1071
	step [90/160], loss=81.0825
	step [91/160], loss=83.5345
	step [92/160], loss=84.6364
	step [93/160], loss=72.2861
	step [94/160], loss=97.0664
	step [95/160], loss=87.8296
	step [96/160], loss=71.7769
	step [97/160], loss=68.5938
	step [98/160], loss=79.1020
	step [99/160], loss=71.4882
	step [100/160], loss=85.9398
	step [101/160], loss=87.7068
	step [102/160], loss=100.1164
	step [103/160], loss=86.3283
	step [104/160], loss=85.8175
	step [105/160], loss=96.2822
	step [106/160], loss=78.2801
	step [107/160], loss=91.8496
	step [108/160], loss=96.0788
	step [109/160], loss=80.7124
	step [110/160], loss=75.3278
	step [111/160], loss=94.9131
	step [112/160], loss=88.8443
	step [113/160], loss=71.6811
	step [114/160], loss=76.4936
	step [115/160], loss=77.3516
	step [116/160], loss=91.7223
	step [117/160], loss=99.6814
	step [118/160], loss=77.5358
	step [119/160], loss=97.1573
	step [120/160], loss=90.4143
	step [121/160], loss=88.8549
	step [122/160], loss=83.1462
	step [123/160], loss=86.5869
	step [124/160], loss=72.4517
	step [125/160], loss=82.8616
	step [126/160], loss=85.8154
	step [127/160], loss=86.7648
	step [128/160], loss=82.7465
	step [129/160], loss=77.3004
	step [130/160], loss=75.8231
	step [131/160], loss=101.1033
	step [132/160], loss=81.0528
	step [133/160], loss=87.4003
	step [134/160], loss=93.2359
	step [135/160], loss=81.3279
	step [136/160], loss=89.6374
	step [137/160], loss=81.2226
	step [138/160], loss=76.8937
	step [139/160], loss=87.2737
	step [140/160], loss=75.5525
	step [141/160], loss=94.1845
	step [142/160], loss=88.9019
	step [143/160], loss=72.9647
	step [144/160], loss=82.6310
	step [145/160], loss=88.7367
	step [146/160], loss=78.6487
	step [147/160], loss=99.3420
	step [148/160], loss=91.9763
	step [149/160], loss=82.8705
	step [150/160], loss=90.4183
	step [151/160], loss=90.4581
	step [152/160], loss=80.9228
	step [153/160], loss=85.2732
	step [154/160], loss=94.5551
	step [155/160], loss=79.5619
	step [156/160], loss=77.9391
	step [157/160], loss=75.8084
	step [158/160], loss=72.2262
	step [159/160], loss=92.4826
	step [160/160], loss=59.2422
	Evaluating
	loss=0.0116, precision=0.3657, recall=0.8895, f1=0.5183
Training epoch 79
	step [1/160], loss=90.7844
	step [2/160], loss=78.9160
	step [3/160], loss=79.4681
	step [4/160], loss=95.3868
	step [5/160], loss=77.4623
	step [6/160], loss=74.0439
	step [7/160], loss=95.9080
	step [8/160], loss=90.2646
	step [9/160], loss=81.7910
	step [10/160], loss=92.3614
	step [11/160], loss=87.2566
	step [12/160], loss=79.8026
	step [13/160], loss=72.8373
	step [14/160], loss=96.7039
	step [15/160], loss=92.4461
	step [16/160], loss=92.2115
	step [17/160], loss=97.7194
	step [18/160], loss=85.2648
	step [19/160], loss=87.4074
	step [20/160], loss=71.1490
	step [21/160], loss=78.5897
	step [22/160], loss=84.9063
	step [23/160], loss=89.5219
	step [24/160], loss=77.4977
	step [25/160], loss=74.2334
	step [26/160], loss=85.3923
	step [27/160], loss=97.9653
	step [28/160], loss=77.1663
	step [29/160], loss=93.1987
	step [30/160], loss=79.2433
	step [31/160], loss=99.3543
	step [32/160], loss=85.5072
	step [33/160], loss=77.8563
	step [34/160], loss=75.6756
	step [35/160], loss=88.0885
	step [36/160], loss=88.6298
	step [37/160], loss=78.3845
	step [38/160], loss=87.7335
	step [39/160], loss=74.3487
	step [40/160], loss=81.9606
	step [41/160], loss=87.2069
	step [42/160], loss=88.6782
	step [43/160], loss=82.5286
	step [44/160], loss=101.4590
	step [45/160], loss=80.9884
	step [46/160], loss=92.0010
	step [47/160], loss=87.3248
	step [48/160], loss=80.0734
	step [49/160], loss=71.7223
	step [50/160], loss=87.2621
	step [51/160], loss=85.3827
	step [52/160], loss=85.8199
	step [53/160], loss=82.8574
	step [54/160], loss=89.5774
	step [55/160], loss=84.5695
	step [56/160], loss=85.2253
	step [57/160], loss=92.8560
	step [58/160], loss=93.6146
	step [59/160], loss=73.6639
	step [60/160], loss=83.3239
	step [61/160], loss=83.5700
	step [62/160], loss=77.7854
	step [63/160], loss=77.4560
	step [64/160], loss=78.2843
	step [65/160], loss=75.8325
	step [66/160], loss=81.6454
	step [67/160], loss=64.8871
	step [68/160], loss=65.2141
	step [69/160], loss=82.3816
	step [70/160], loss=78.0960
	step [71/160], loss=107.5914
	step [72/160], loss=85.5602
	step [73/160], loss=96.4466
	step [74/160], loss=77.0498
	step [75/160], loss=78.6305
	step [76/160], loss=90.0924
	step [77/160], loss=79.1960
	step [78/160], loss=96.6362
	step [79/160], loss=83.3161
	step [80/160], loss=83.6470
	step [81/160], loss=84.3346
	step [82/160], loss=78.5498
	step [83/160], loss=90.8250
	step [84/160], loss=80.9113
	step [85/160], loss=91.8845
	step [86/160], loss=78.9635
	step [87/160], loss=70.2636
	step [88/160], loss=98.4510
	step [89/160], loss=70.5576
	step [90/160], loss=98.4931
	step [91/160], loss=101.0091
	step [92/160], loss=96.5677
	step [93/160], loss=89.5158
	step [94/160], loss=76.9124
	step [95/160], loss=75.0677
	step [96/160], loss=71.2543
	step [97/160], loss=77.7101
	step [98/160], loss=79.2731
	step [99/160], loss=73.5694
	step [100/160], loss=87.7496
	step [101/160], loss=94.9662
	step [102/160], loss=75.7545
	step [103/160], loss=77.5267
	step [104/160], loss=101.6460
	step [105/160], loss=84.0155
	step [106/160], loss=96.6075
	step [107/160], loss=82.4695
	step [108/160], loss=91.6521
	step [109/160], loss=103.3655
	step [110/160], loss=108.4984
	step [111/160], loss=78.7901
	step [112/160], loss=83.6750
	step [113/160], loss=69.1921
	step [114/160], loss=92.5492
	step [115/160], loss=85.9751
	step [116/160], loss=75.2637
	step [117/160], loss=94.5794
	step [118/160], loss=90.4814
	step [119/160], loss=72.5531
	step [120/160], loss=86.7316
	step [121/160], loss=83.7194
	step [122/160], loss=80.9186
	step [123/160], loss=81.0105
	step [124/160], loss=87.8184
	step [125/160], loss=84.5675
	step [126/160], loss=80.3500
	step [127/160], loss=74.6968
	step [128/160], loss=101.6790
	step [129/160], loss=92.2659
	step [130/160], loss=83.5439
	step [131/160], loss=92.2173
	step [132/160], loss=80.3930
	step [133/160], loss=95.5414
	step [134/160], loss=90.0084
	step [135/160], loss=97.5512
	step [136/160], loss=88.9923
	step [137/160], loss=84.4334
	step [138/160], loss=86.4831
	step [139/160], loss=73.4736
	step [140/160], loss=90.4885
	step [141/160], loss=56.5333
	step [142/160], loss=68.8614
	step [143/160], loss=79.4731
	step [144/160], loss=71.5863
	step [145/160], loss=87.4668
	step [146/160], loss=95.8503
	step [147/160], loss=80.4019
	step [148/160], loss=75.6267
	step [149/160], loss=86.2771
	step [150/160], loss=75.3505
	step [151/160], loss=89.9580
	step [152/160], loss=83.5998
	step [153/160], loss=93.8857
	step [154/160], loss=91.7229
	step [155/160], loss=89.6644
	step [156/160], loss=80.9186
	step [157/160], loss=95.3890
	step [158/160], loss=86.3025
	step [159/160], loss=67.1634
	step [160/160], loss=70.4879
	Evaluating
	loss=0.0105, precision=0.3932, recall=0.8915, f1=0.5457
Training epoch 80
	step [1/160], loss=85.1731
	step [2/160], loss=82.8991
	step [3/160], loss=69.8316
	step [4/160], loss=91.4368
	step [5/160], loss=79.6985
	step [6/160], loss=94.6816
	step [7/160], loss=74.7405
	step [8/160], loss=98.4797
	step [9/160], loss=81.4153
	step [10/160], loss=85.8697
	step [11/160], loss=63.7004
	step [12/160], loss=87.0610
	step [13/160], loss=85.8900
	step [14/160], loss=92.4968
	step [15/160], loss=78.1211
	step [16/160], loss=92.5919
	step [17/160], loss=71.4679
	step [18/160], loss=81.0915
	step [19/160], loss=74.9374
	step [20/160], loss=86.6734
	step [21/160], loss=98.9753
	step [22/160], loss=78.5460
	step [23/160], loss=86.0677
	step [24/160], loss=102.2374
	step [25/160], loss=65.0796
	step [26/160], loss=77.4882
	step [27/160], loss=94.8942
	step [28/160], loss=97.6660
	step [29/160], loss=90.3623
	step [30/160], loss=85.6700
	step [31/160], loss=63.4921
	step [32/160], loss=86.9923
	step [33/160], loss=76.1448
	step [34/160], loss=76.0285
	step [35/160], loss=102.1935
	step [36/160], loss=82.7798
	step [37/160], loss=78.1209
	step [38/160], loss=94.6172
	step [39/160], loss=81.6596
	step [40/160], loss=82.4599
	step [41/160], loss=94.2404
	step [42/160], loss=78.2354
	step [43/160], loss=91.1471
	step [44/160], loss=81.9647
	step [45/160], loss=108.3337
	step [46/160], loss=85.8758
	step [47/160], loss=73.3820
	step [48/160], loss=75.2523
	step [49/160], loss=67.7813
	step [50/160], loss=81.0613
	step [51/160], loss=92.1374
	step [52/160], loss=75.0017
	step [53/160], loss=86.2607
	step [54/160], loss=67.1852
	step [55/160], loss=81.9072
	step [56/160], loss=85.7154
	step [57/160], loss=92.3423
	step [58/160], loss=82.6147
	step [59/160], loss=86.2444
	step [60/160], loss=87.9440
	step [61/160], loss=87.0283
	step [62/160], loss=75.8040
	step [63/160], loss=97.2730
	step [64/160], loss=74.7839
	step [65/160], loss=81.8864
	step [66/160], loss=81.8526
	step [67/160], loss=103.3009
	step [68/160], loss=87.2789
	step [69/160], loss=81.1243
	step [70/160], loss=80.1129
	step [71/160], loss=90.6435
	step [72/160], loss=77.3688
	step [73/160], loss=81.8887
	step [74/160], loss=85.2563
	step [75/160], loss=83.0237
	step [76/160], loss=113.1648
	step [77/160], loss=83.4806
	step [78/160], loss=80.9237
	step [79/160], loss=87.9316
	step [80/160], loss=80.7397
	step [81/160], loss=76.3024
	step [82/160], loss=75.3327
	step [83/160], loss=77.7499
	step [84/160], loss=79.9978
	step [85/160], loss=94.4158
	step [86/160], loss=81.1123
	step [87/160], loss=89.2055
	step [88/160], loss=73.7099
	step [89/160], loss=81.8619
	step [90/160], loss=88.1165
	step [91/160], loss=87.5368
	step [92/160], loss=93.7260
	step [93/160], loss=87.0237
	step [94/160], loss=87.8579
	step [95/160], loss=101.8584
	step [96/160], loss=82.5926
	step [97/160], loss=95.9088
	step [98/160], loss=78.7077
	step [99/160], loss=85.1814
	step [100/160], loss=99.0236
	step [101/160], loss=80.3428
	step [102/160], loss=74.7606
	step [103/160], loss=80.3950
	step [104/160], loss=73.7328
	step [105/160], loss=79.6062
	step [106/160], loss=90.8060
	step [107/160], loss=87.2877
	step [108/160], loss=78.3705
	step [109/160], loss=90.1378
	step [110/160], loss=75.1650
	step [111/160], loss=73.4562
	step [112/160], loss=91.8599
	step [113/160], loss=82.5438
	step [114/160], loss=89.9855
	step [115/160], loss=87.1129
	step [116/160], loss=76.8577
	step [117/160], loss=88.3361
	step [118/160], loss=83.5445
	step [119/160], loss=66.5293
	step [120/160], loss=84.1632
	step [121/160], loss=96.5280
	step [122/160], loss=91.3711
	step [123/160], loss=82.1630
	step [124/160], loss=84.6575
	step [125/160], loss=84.5513
	step [126/160], loss=85.2932
	step [127/160], loss=89.8238
	step [128/160], loss=83.3968
	step [129/160], loss=91.2411
	step [130/160], loss=74.1389
	step [131/160], loss=104.7242
	step [132/160], loss=87.6843
	step [133/160], loss=88.6836
	step [134/160], loss=82.1336
	step [135/160], loss=78.0286
	step [136/160], loss=86.1475
	step [137/160], loss=66.0675
	step [138/160], loss=85.1838
	step [139/160], loss=72.3819
	step [140/160], loss=70.2297
	step [141/160], loss=83.9117
	step [142/160], loss=74.3465
	step [143/160], loss=79.3045
	step [144/160], loss=88.9252
	step [145/160], loss=84.4304
	step [146/160], loss=96.6456
	step [147/160], loss=78.2647
	step [148/160], loss=89.1485
	step [149/160], loss=77.4609
	step [150/160], loss=76.2097
	step [151/160], loss=82.6613
	step [152/160], loss=72.1329
	step [153/160], loss=92.8628
	step [154/160], loss=76.2356
	step [155/160], loss=88.0931
	step [156/160], loss=85.2216
	step [157/160], loss=77.7290
	step [158/160], loss=82.9923
	step [159/160], loss=97.7136
	step [160/160], loss=71.9971
	Evaluating
	loss=0.0122, precision=0.3525, recall=0.9002, f1=0.5066
Training epoch 81
	step [1/160], loss=93.2468
	step [2/160], loss=78.2964
	step [3/160], loss=80.6225
	step [4/160], loss=76.7095
	step [5/160], loss=83.3435
	step [6/160], loss=82.2229
	step [7/160], loss=84.2828
	step [8/160], loss=78.5672
	step [9/160], loss=74.1467
	step [10/160], loss=90.4985
	step [11/160], loss=105.1654
	step [12/160], loss=86.1167
	step [13/160], loss=83.4077
	step [14/160], loss=82.2039
	step [15/160], loss=88.6215
	step [16/160], loss=73.6208
	step [17/160], loss=76.8330
	step [18/160], loss=92.7265
	step [19/160], loss=94.7372
	step [20/160], loss=80.9001
	step [21/160], loss=86.9801
	step [22/160], loss=76.6996
	step [23/160], loss=104.4491
	step [24/160], loss=79.8982
	step [25/160], loss=86.3444
	step [26/160], loss=82.9808
	step [27/160], loss=84.3274
	step [28/160], loss=78.8343
	step [29/160], loss=72.3452
	step [30/160], loss=71.0572
	step [31/160], loss=94.7153
	step [32/160], loss=100.7082
	step [33/160], loss=86.8479
	step [34/160], loss=78.6640
	step [35/160], loss=98.7141
	step [36/160], loss=83.6644
	step [37/160], loss=80.4502
	step [38/160], loss=78.4023
	step [39/160], loss=66.4086
	step [40/160], loss=91.2153
	step [41/160], loss=74.5415
	step [42/160], loss=95.9878
	step [43/160], loss=98.9623
	step [44/160], loss=96.8692
	step [45/160], loss=66.1350
	step [46/160], loss=85.5446
	step [47/160], loss=87.9441
	step [48/160], loss=78.4061
	step [49/160], loss=72.6890
	step [50/160], loss=100.0485
	step [51/160], loss=86.2473
	step [52/160], loss=65.2500
	step [53/160], loss=94.9982
	step [54/160], loss=78.2113
	step [55/160], loss=99.0356
	step [56/160], loss=87.9352
	step [57/160], loss=71.4610
	step [58/160], loss=73.4911
	step [59/160], loss=80.7043
	step [60/160], loss=82.1994
	step [61/160], loss=75.7881
	step [62/160], loss=89.1641
	step [63/160], loss=78.2006
	step [64/160], loss=106.3752
	step [65/160], loss=88.0226
	step [66/160], loss=85.1940
	step [67/160], loss=102.3427
	step [68/160], loss=80.3254
	step [69/160], loss=90.1303
	step [70/160], loss=74.2590
	step [71/160], loss=84.3151
	step [72/160], loss=74.8083
	step [73/160], loss=97.3207
	step [74/160], loss=77.2152
	step [75/160], loss=84.7926
	step [76/160], loss=66.5194
	step [77/160], loss=86.2271
	step [78/160], loss=91.8126
	step [79/160], loss=81.1522
	step [80/160], loss=84.3013
	step [81/160], loss=86.4375
	step [82/160], loss=80.4692
	step [83/160], loss=94.9198
	step [84/160], loss=84.7153
	step [85/160], loss=85.6228
	step [86/160], loss=79.8739
	step [87/160], loss=78.5253
	step [88/160], loss=75.5241
	step [89/160], loss=74.8252
	step [90/160], loss=84.7166
	step [91/160], loss=74.3047
	step [92/160], loss=80.7790
	step [93/160], loss=93.3076
	step [94/160], loss=77.5714
	step [95/160], loss=65.2383
	step [96/160], loss=77.6491
	step [97/160], loss=80.3991
	step [98/160], loss=83.7028
	step [99/160], loss=78.6761
	step [100/160], loss=88.3037
	step [101/160], loss=80.0760
	step [102/160], loss=82.5696
	step [103/160], loss=86.4191
	step [104/160], loss=77.6325
	step [105/160], loss=83.2591
	step [106/160], loss=92.3149
	step [107/160], loss=79.3703
	step [108/160], loss=84.0847
	step [109/160], loss=93.8180
	step [110/160], loss=71.0215
	step [111/160], loss=84.2282
	step [112/160], loss=87.2702
	step [113/160], loss=74.8970
	step [114/160], loss=80.0844
	step [115/160], loss=75.5409
	step [116/160], loss=74.5837
	step [117/160], loss=78.4531
	step [118/160], loss=80.6875
	step [119/160], loss=80.5155
	step [120/160], loss=82.8121
	step [121/160], loss=84.7450
	step [122/160], loss=90.6097
	step [123/160], loss=76.0045
	step [124/160], loss=78.6765
	step [125/160], loss=71.9651
	step [126/160], loss=102.4509
	step [127/160], loss=75.6738
	step [128/160], loss=81.0327
	step [129/160], loss=77.2498
	step [130/160], loss=83.1896
	step [131/160], loss=83.1984
	step [132/160], loss=80.8221
	step [133/160], loss=84.3285
	step [134/160], loss=86.5721
	step [135/160], loss=84.0408
	step [136/160], loss=84.5618
	step [137/160], loss=80.5343
	step [138/160], loss=96.4531
	step [139/160], loss=78.0134
	step [140/160], loss=80.4698
	step [141/160], loss=68.8107
	step [142/160], loss=90.1796
	step [143/160], loss=81.8135
	step [144/160], loss=85.4770
	step [145/160], loss=74.2565
	step [146/160], loss=90.9499
	step [147/160], loss=88.0240
	step [148/160], loss=84.3729
	step [149/160], loss=75.8021
	step [150/160], loss=71.4667
	step [151/160], loss=97.3542
	step [152/160], loss=70.6850
	step [153/160], loss=78.5737
	step [154/160], loss=87.8149
	step [155/160], loss=88.0472
	step [156/160], loss=96.4012
	step [157/160], loss=86.8638
	step [158/160], loss=101.2060
	step [159/160], loss=79.3494
	step [160/160], loss=67.0339
	Evaluating
	loss=0.0122, precision=0.3480, recall=0.8947, f1=0.5011
Training epoch 82
	step [1/160], loss=85.3654
	step [2/160], loss=76.1469
	step [3/160], loss=87.1849
	step [4/160], loss=86.7122
	step [5/160], loss=86.8412
	step [6/160], loss=90.4529
	step [7/160], loss=88.8554
	step [8/160], loss=79.7539
	step [9/160], loss=89.1005
	step [10/160], loss=81.1987
	step [11/160], loss=69.0630
	step [12/160], loss=90.0336
	step [13/160], loss=88.1346
	step [14/160], loss=83.6219
	step [15/160], loss=68.5836
	step [16/160], loss=84.4735
	step [17/160], loss=94.7750
	step [18/160], loss=82.6012
	step [19/160], loss=83.2034
	step [20/160], loss=77.4657
	step [21/160], loss=80.5961
	step [22/160], loss=89.1767
	step [23/160], loss=77.1479
	step [24/160], loss=88.8774
	step [25/160], loss=75.6243
	step [26/160], loss=76.8758
	step [27/160], loss=71.9089
	step [28/160], loss=81.7592
	step [29/160], loss=87.1258
	step [30/160], loss=77.1902
	step [31/160], loss=67.7222
	step [32/160], loss=78.7476
	step [33/160], loss=91.3139
	step [34/160], loss=106.8981
	step [35/160], loss=85.7550
	step [36/160], loss=79.2767
	step [37/160], loss=97.7059
	step [38/160], loss=88.7495
	step [39/160], loss=81.5934
	step [40/160], loss=84.6435
	step [41/160], loss=77.5510
	step [42/160], loss=78.0564
	step [43/160], loss=86.3273
	step [44/160], loss=94.1252
	step [45/160], loss=73.3708
	step [46/160], loss=87.8444
	step [47/160], loss=82.8484
	step [48/160], loss=80.1938
	step [49/160], loss=82.0482
	step [50/160], loss=91.2678
	step [51/160], loss=78.9692
	step [52/160], loss=86.8516
	step [53/160], loss=88.1858
	step [54/160], loss=87.8539
	step [55/160], loss=68.5402
	step [56/160], loss=86.5849
	step [57/160], loss=84.2117
	step [58/160], loss=70.8198
	step [59/160], loss=77.0057
	step [60/160], loss=79.1727
	step [61/160], loss=81.1945
	step [62/160], loss=88.1997
	step [63/160], loss=85.0023
	step [64/160], loss=94.8677
	step [65/160], loss=78.4776
	step [66/160], loss=74.4540
	step [67/160], loss=70.1567
	step [68/160], loss=75.9087
	step [69/160], loss=81.1646
	step [70/160], loss=85.5894
	step [71/160], loss=80.2646
	step [72/160], loss=75.6009
	step [73/160], loss=80.8471
	step [74/160], loss=74.1488
	step [75/160], loss=70.6390
	step [76/160], loss=87.8584
	step [77/160], loss=81.8122
	step [78/160], loss=77.4956
	step [79/160], loss=78.3532
	step [80/160], loss=67.5361
	step [81/160], loss=66.8194
	step [82/160], loss=83.6731
	step [83/160], loss=62.3603
	step [84/160], loss=79.7964
	step [85/160], loss=81.7601
	step [86/160], loss=83.5788
	step [87/160], loss=78.9294
	step [88/160], loss=70.9083
	step [89/160], loss=76.9767
	step [90/160], loss=81.4388
	step [91/160], loss=63.1503
	step [92/160], loss=79.4445
	step [93/160], loss=101.3642
	step [94/160], loss=102.2680
	step [95/160], loss=89.8036
	step [96/160], loss=80.7431
	step [97/160], loss=79.9842
	step [98/160], loss=84.3587
	step [99/160], loss=89.2751
	step [100/160], loss=80.6514
	step [101/160], loss=95.9712
	step [102/160], loss=105.5174
	step [103/160], loss=99.0679
	step [104/160], loss=91.7923
	step [105/160], loss=91.5060
	step [106/160], loss=96.0874
	step [107/160], loss=85.8608
	step [108/160], loss=83.7818
	step [109/160], loss=84.4106
	step [110/160], loss=89.9926
	step [111/160], loss=72.9282
	step [112/160], loss=94.2234
	step [113/160], loss=75.9902
	step [114/160], loss=86.9908
	step [115/160], loss=71.6408
	step [116/160], loss=90.1608
	step [117/160], loss=70.5780
	step [118/160], loss=83.4190
	step [119/160], loss=69.8167
	step [120/160], loss=66.4450
	step [121/160], loss=97.9138
	step [122/160], loss=109.1863
	step [123/160], loss=85.9103
	step [124/160], loss=82.3538
	step [125/160], loss=69.2163
	step [126/160], loss=98.8196
	step [127/160], loss=91.9698
	step [128/160], loss=71.4322
	step [129/160], loss=76.7236
	step [130/160], loss=89.5870
	step [131/160], loss=86.0264
	step [132/160], loss=85.9330
	step [133/160], loss=84.3884
	step [134/160], loss=91.6992
	step [135/160], loss=93.7138
	step [136/160], loss=74.3733
	step [137/160], loss=80.6708
	step [138/160], loss=83.2095
	step [139/160], loss=96.0399
	step [140/160], loss=78.4874
	step [141/160], loss=79.8662
	step [142/160], loss=72.1696
	step [143/160], loss=77.5399
	step [144/160], loss=81.8028
	step [145/160], loss=84.3442
	step [146/160], loss=74.1239
	step [147/160], loss=86.0148
	step [148/160], loss=70.4094
	step [149/160], loss=92.3712
	step [150/160], loss=92.5144
	step [151/160], loss=82.8665
	step [152/160], loss=90.3013
	step [153/160], loss=88.8552
	step [154/160], loss=100.7145
	step [155/160], loss=101.8268
	step [156/160], loss=102.4892
	step [157/160], loss=75.1422
	step [158/160], loss=89.0273
	step [159/160], loss=76.3625
	step [160/160], loss=53.8920
	Evaluating
	loss=0.0113, precision=0.3699, recall=0.8868, f1=0.5220
Training epoch 83
	step [1/160], loss=87.0227
	step [2/160], loss=74.9174
	step [3/160], loss=89.8880
	step [4/160], loss=83.1602
	step [5/160], loss=86.9534
	step [6/160], loss=78.3079
	step [7/160], loss=76.0990
	step [8/160], loss=81.4669
	step [9/160], loss=96.8013
	step [10/160], loss=79.0348
	step [11/160], loss=85.6232
	step [12/160], loss=71.9991
	step [13/160], loss=78.5536
	step [14/160], loss=88.1541
	step [15/160], loss=91.1984
	step [16/160], loss=76.0165
	step [17/160], loss=92.5870
	step [18/160], loss=89.4840
	step [19/160], loss=76.1002
	step [20/160], loss=63.2911
	step [21/160], loss=88.9627
	step [22/160], loss=84.2240
	step [23/160], loss=80.8990
	step [24/160], loss=89.2206
	step [25/160], loss=83.7048
	step [26/160], loss=85.7865
	step [27/160], loss=85.9659
	step [28/160], loss=99.4917
	step [29/160], loss=83.5253
	step [30/160], loss=85.3985
	step [31/160], loss=93.6856
	step [32/160], loss=82.6720
	step [33/160], loss=81.4055
	step [34/160], loss=80.2948
	step [35/160], loss=72.0215
	step [36/160], loss=82.7390
	step [37/160], loss=82.3472
	step [38/160], loss=93.5128
	step [39/160], loss=98.0986
	step [40/160], loss=79.2985
	step [41/160], loss=84.7638
	step [42/160], loss=71.3956
	step [43/160], loss=81.5780
	step [44/160], loss=71.3348
	step [45/160], loss=65.4697
	step [46/160], loss=83.2160
	step [47/160], loss=70.8736
	step [48/160], loss=80.5749
	step [49/160], loss=84.8548
	step [50/160], loss=92.6764
	step [51/160], loss=70.9409
	step [52/160], loss=82.9408
	step [53/160], loss=74.6242
	step [54/160], loss=84.0143
	step [55/160], loss=97.4126
	step [56/160], loss=78.8047
	step [57/160], loss=90.9054
	step [58/160], loss=90.6658
	step [59/160], loss=74.0131
	step [60/160], loss=73.6330
	step [61/160], loss=74.0138
	step [62/160], loss=79.5608
	step [63/160], loss=74.5491
	step [64/160], loss=90.0229
	step [65/160], loss=100.2903
	step [66/160], loss=80.2925
	step [67/160], loss=69.9786
	step [68/160], loss=60.0611
	step [69/160], loss=91.3866
	step [70/160], loss=86.0170
	step [71/160], loss=77.3891
	step [72/160], loss=71.8232
	step [73/160], loss=86.2084
	step [74/160], loss=91.0678
	step [75/160], loss=77.5464
	step [76/160], loss=72.3472
	step [77/160], loss=90.3226
	step [78/160], loss=84.5954
	step [79/160], loss=78.2963
	step [80/160], loss=79.9754
	step [81/160], loss=89.7500
	step [82/160], loss=80.3251
	step [83/160], loss=89.2924
	step [84/160], loss=92.1415
	step [85/160], loss=83.2145
	step [86/160], loss=100.2986
	step [87/160], loss=83.6685
	step [88/160], loss=81.1194
	step [89/160], loss=89.8456
	step [90/160], loss=95.4676
	step [91/160], loss=70.8928
	step [92/160], loss=81.4518
	step [93/160], loss=91.4624
	step [94/160], loss=79.6543
	step [95/160], loss=71.3411
	step [96/160], loss=105.2895
	step [97/160], loss=84.7941
	step [98/160], loss=82.4781
	step [99/160], loss=73.6329
	step [100/160], loss=84.2340
	step [101/160], loss=79.0071
	step [102/160], loss=89.0572
	step [103/160], loss=91.5376
	step [104/160], loss=85.7187
	step [105/160], loss=83.6492
	step [106/160], loss=80.0393
	step [107/160], loss=78.6118
	step [108/160], loss=77.4874
	step [109/160], loss=78.7720
	step [110/160], loss=82.6824
	step [111/160], loss=83.6030
	step [112/160], loss=87.4545
	step [113/160], loss=82.6644
	step [114/160], loss=82.6109
	step [115/160], loss=64.1612
	step [116/160], loss=79.8663
	step [117/160], loss=67.3317
	step [118/160], loss=92.9785
	step [119/160], loss=90.6726
	step [120/160], loss=78.2576
	step [121/160], loss=79.2792
	step [122/160], loss=80.8270
	step [123/160], loss=86.9814
	step [124/160], loss=81.2198
	step [125/160], loss=82.5754
	step [126/160], loss=90.1897
	step [127/160], loss=86.4422
	step [128/160], loss=63.7555
	step [129/160], loss=83.3143
	step [130/160], loss=89.2992
	step [131/160], loss=85.5460
	step [132/160], loss=90.6514
	step [133/160], loss=86.0818
	step [134/160], loss=96.1486
	step [135/160], loss=85.8682
	step [136/160], loss=76.0581
	step [137/160], loss=98.3784
	step [138/160], loss=71.5066
	step [139/160], loss=78.0881
	step [140/160], loss=90.9293
	step [141/160], loss=80.8690
	step [142/160], loss=98.0588
	step [143/160], loss=84.4660
	step [144/160], loss=88.4452
	step [145/160], loss=90.9272
	step [146/160], loss=79.9117
	step [147/160], loss=72.6789
	step [148/160], loss=77.8565
	step [149/160], loss=79.9449
	step [150/160], loss=85.3993
	step [151/160], loss=76.3887
	step [152/160], loss=82.6402
	step [153/160], loss=83.7664
	step [154/160], loss=87.4421
	step [155/160], loss=97.4305
	step [156/160], loss=97.8584
	step [157/160], loss=86.9584
	step [158/160], loss=74.6118
	step [159/160], loss=75.2679
	step [160/160], loss=59.9787
	Evaluating
	loss=0.0102, precision=0.4026, recall=0.8988, f1=0.5561
Training epoch 84
	step [1/160], loss=95.0648
	step [2/160], loss=87.9594
	step [3/160], loss=75.1280
	step [4/160], loss=89.9263
	step [5/160], loss=73.3070
	step [6/160], loss=79.1682
	step [7/160], loss=93.9441
	step [8/160], loss=73.2978
	step [9/160], loss=81.4361
	step [10/160], loss=78.1822
	step [11/160], loss=91.3835
	step [12/160], loss=89.6351
	step [13/160], loss=79.3446
	step [14/160], loss=75.9641
	step [15/160], loss=80.6936
	step [16/160], loss=78.4334
	step [17/160], loss=91.9810
	step [18/160], loss=82.8994
	step [19/160], loss=94.9768
	step [20/160], loss=76.6369
	step [21/160], loss=85.1242
	step [22/160], loss=88.3331
	step [23/160], loss=74.1611
	step [24/160], loss=80.4907
	step [25/160], loss=69.9631
	step [26/160], loss=75.6026
	step [27/160], loss=64.7898
	step [28/160], loss=87.2648
	step [29/160], loss=72.3594
	step [30/160], loss=95.3783
	step [31/160], loss=93.9027
	step [32/160], loss=79.6141
	step [33/160], loss=69.4044
	step [34/160], loss=77.2545
	step [35/160], loss=84.9969
	step [36/160], loss=98.0146
	step [37/160], loss=75.0271
	step [38/160], loss=81.1271
	step [39/160], loss=74.3227
	step [40/160], loss=74.2818
	step [41/160], loss=84.6808
	step [42/160], loss=81.9771
	step [43/160], loss=84.2183
	step [44/160], loss=77.8186
	step [45/160], loss=77.5309
	step [46/160], loss=83.3717
	step [47/160], loss=93.2987
	step [48/160], loss=72.3532
	step [49/160], loss=81.0719
	step [50/160], loss=88.3402
	step [51/160], loss=107.8184
	step [52/160], loss=89.5515
	step [53/160], loss=87.4077
	step [54/160], loss=83.3948
	step [55/160], loss=88.7613
	step [56/160], loss=87.0882
	step [57/160], loss=80.8113
	step [58/160], loss=74.3835
	step [59/160], loss=93.9234
	step [60/160], loss=75.6955
	step [61/160], loss=76.5795
	step [62/160], loss=83.3121
	step [63/160], loss=74.1351
	step [64/160], loss=82.3481
	step [65/160], loss=77.4699
	step [66/160], loss=87.4056
	step [67/160], loss=84.2916
	step [68/160], loss=67.2001
	step [69/160], loss=77.2398
	step [70/160], loss=78.3251
	step [71/160], loss=95.4311
	step [72/160], loss=76.0750
	step [73/160], loss=80.3491
	step [74/160], loss=84.5702
	step [75/160], loss=86.4341
	step [76/160], loss=77.1008
	step [77/160], loss=73.1856
	step [78/160], loss=89.3710
	step [79/160], loss=74.4347
	step [80/160], loss=88.6107
	step [81/160], loss=82.0318
	step [82/160], loss=87.1467
	step [83/160], loss=93.8138
	step [84/160], loss=86.5192
	step [85/160], loss=81.8259
	step [86/160], loss=75.8648
	step [87/160], loss=86.4990
	step [88/160], loss=85.3851
	step [89/160], loss=84.3897
	step [90/160], loss=96.3162
	step [91/160], loss=91.8004
	step [92/160], loss=92.4962
	step [93/160], loss=93.0856
	step [94/160], loss=100.2313
	step [95/160], loss=76.7076
	step [96/160], loss=78.5373
	step [97/160], loss=91.2343
	step [98/160], loss=79.8271
	step [99/160], loss=85.9003
	step [100/160], loss=76.9019
	step [101/160], loss=93.4832
	step [102/160], loss=81.6510
	step [103/160], loss=89.3664
	step [104/160], loss=80.2667
	step [105/160], loss=99.0303
	step [106/160], loss=76.5940
	step [107/160], loss=98.9771
	step [108/160], loss=73.7593
	step [109/160], loss=99.7470
	step [110/160], loss=72.5311
	step [111/160], loss=68.5896
	step [112/160], loss=85.6443
	step [113/160], loss=78.7018
	step [114/160], loss=81.6302
	step [115/160], loss=76.7350
	step [116/160], loss=78.6727
	step [117/160], loss=74.2721
	step [118/160], loss=78.6723
	step [119/160], loss=97.3594
	step [120/160], loss=81.5255
	step [121/160], loss=71.7168
	step [122/160], loss=96.0989
	step [123/160], loss=83.4398
	step [124/160], loss=88.9403
	step [125/160], loss=86.8172
	step [126/160], loss=80.8628
	step [127/160], loss=72.3784
	step [128/160], loss=74.3346
	step [129/160], loss=83.9754
	step [130/160], loss=79.8006
	step [131/160], loss=67.2099
	step [132/160], loss=98.2011
	step [133/160], loss=80.6968
	step [134/160], loss=86.3250
	step [135/160], loss=87.8237
	step [136/160], loss=81.4345
	step [137/160], loss=79.2393
	step [138/160], loss=79.2847
	step [139/160], loss=86.8850
	step [140/160], loss=78.3279
	step [141/160], loss=94.6263
	step [142/160], loss=82.5579
	step [143/160], loss=87.1135
	step [144/160], loss=92.8773
	step [145/160], loss=79.0032
	step [146/160], loss=91.6266
	step [147/160], loss=65.6895
	step [148/160], loss=78.7287
	step [149/160], loss=65.4032
	step [150/160], loss=85.1417
	step [151/160], loss=85.1532
	step [152/160], loss=80.3927
	step [153/160], loss=79.7285
	step [154/160], loss=85.0169
	step [155/160], loss=83.5450
	step [156/160], loss=84.3434
	step [157/160], loss=77.4335
	step [158/160], loss=86.7298
	step [159/160], loss=91.3725
	step [160/160], loss=63.3168
	Evaluating
	loss=0.0111, precision=0.3716, recall=0.8943, f1=0.5250
Training epoch 85
	step [1/160], loss=87.4427
	step [2/160], loss=89.8226
	step [3/160], loss=97.0256
	step [4/160], loss=79.6183
	step [5/160], loss=75.0318
	step [6/160], loss=67.9540
	step [7/160], loss=89.1413
	step [8/160], loss=72.1752
	step [9/160], loss=90.5593
	step [10/160], loss=83.1552
	step [11/160], loss=87.1049
	step [12/160], loss=93.2958
	step [13/160], loss=86.7628
	step [14/160], loss=86.1720
	step [15/160], loss=75.9884
	step [16/160], loss=86.2349
	step [17/160], loss=80.1039
	step [18/160], loss=75.9878
	step [19/160], loss=76.4100
	step [20/160], loss=91.4290
	step [21/160], loss=101.5766
	step [22/160], loss=78.9422
	step [23/160], loss=76.3016
	step [24/160], loss=91.2448
	step [25/160], loss=86.2281
	step [26/160], loss=85.7671
	step [27/160], loss=84.8700
	step [28/160], loss=84.2879
	step [29/160], loss=77.1043
	step [30/160], loss=80.0667
	step [31/160], loss=81.9184
	step [32/160], loss=84.8261
	step [33/160], loss=78.6772
	step [34/160], loss=75.4461
	step [35/160], loss=83.8441
	step [36/160], loss=96.3201
	step [37/160], loss=85.8026
	step [38/160], loss=91.4454
	step [39/160], loss=73.3067
	step [40/160], loss=78.8204
	step [41/160], loss=80.1201
	step [42/160], loss=79.4786
	step [43/160], loss=79.2306
	step [44/160], loss=82.9422
	step [45/160], loss=84.1710
	step [46/160], loss=71.8803
	step [47/160], loss=76.2197
	step [48/160], loss=99.1567
	step [49/160], loss=76.3866
	step [50/160], loss=88.2627
	step [51/160], loss=78.8610
	step [52/160], loss=80.5680
	step [53/160], loss=89.6032
	step [54/160], loss=87.8639
	step [55/160], loss=67.7922
	step [56/160], loss=93.8204
	step [57/160], loss=86.8634
	step [58/160], loss=83.1042
	step [59/160], loss=78.3249
	step [60/160], loss=85.2081
	step [61/160], loss=79.5602
	step [62/160], loss=71.8722
	step [63/160], loss=70.9197
	step [64/160], loss=73.3634
	step [65/160], loss=92.1363
	step [66/160], loss=87.6655
	step [67/160], loss=97.2362
	step [68/160], loss=79.5264
	step [69/160], loss=85.6242
	step [70/160], loss=80.0120
	step [71/160], loss=77.4719
	step [72/160], loss=78.3821
	step [73/160], loss=81.4280
	step [74/160], loss=84.4314
	step [75/160], loss=86.8327
	step [76/160], loss=78.8863
	step [77/160], loss=81.6270
	step [78/160], loss=80.2785
	step [79/160], loss=75.6237
	step [80/160], loss=93.1968
	step [81/160], loss=77.8614
	step [82/160], loss=92.1830
	step [83/160], loss=79.1829
	step [84/160], loss=80.8756
	step [85/160], loss=87.0164
	step [86/160], loss=96.9687
	step [87/160], loss=74.7687
	step [88/160], loss=74.1220
	step [89/160], loss=78.5530
	step [90/160], loss=80.1167
	step [91/160], loss=98.0391
	step [92/160], loss=78.6632
	step [93/160], loss=86.4666
	step [94/160], loss=94.2264
	step [95/160], loss=81.1416
	step [96/160], loss=91.3561
	step [97/160], loss=89.4469
	step [98/160], loss=91.3703
	step [99/160], loss=74.2272
	step [100/160], loss=82.5021
	step [101/160], loss=88.3490
	step [102/160], loss=97.7895
	step [103/160], loss=95.4778
	step [104/160], loss=98.0220
	step [105/160], loss=77.9538
	step [106/160], loss=68.2977
	step [107/160], loss=63.1187
	step [108/160], loss=62.0475
	step [109/160], loss=91.4708
	step [110/160], loss=89.5766
	step [111/160], loss=75.1325
	step [112/160], loss=81.6729
	step [113/160], loss=86.1860
	step [114/160], loss=87.7664
	step [115/160], loss=98.7397
	step [116/160], loss=82.5582
	step [117/160], loss=76.8666
	step [118/160], loss=77.9750
	step [119/160], loss=79.3672
	step [120/160], loss=86.5728
	step [121/160], loss=83.8255
	step [122/160], loss=74.3305
	step [123/160], loss=69.2108
	step [124/160], loss=79.4941
	step [125/160], loss=85.1232
	step [126/160], loss=72.8995
	step [127/160], loss=75.5983
	step [128/160], loss=74.2439
	step [129/160], loss=89.4263
	step [130/160], loss=78.3746
	step [131/160], loss=77.6471
	step [132/160], loss=85.1806
	step [133/160], loss=71.2856
	step [134/160], loss=95.3055
	step [135/160], loss=87.9886
	step [136/160], loss=79.2350
	step [137/160], loss=85.7770
	step [138/160], loss=73.5037
	step [139/160], loss=90.6817
	step [140/160], loss=82.0351
	step [141/160], loss=78.5662
	step [142/160], loss=98.2047
	step [143/160], loss=90.9813
	step [144/160], loss=78.2601
	step [145/160], loss=66.5801
	step [146/160], loss=84.6380
	step [147/160], loss=75.0026
	step [148/160], loss=76.9205
	step [149/160], loss=89.9983
	step [150/160], loss=87.0910
	step [151/160], loss=84.0896
	step [152/160], loss=75.9600
	step [153/160], loss=87.3256
	step [154/160], loss=87.5604
	step [155/160], loss=78.6676
	step [156/160], loss=86.9025
	step [157/160], loss=85.0108
	step [158/160], loss=65.0962
	step [159/160], loss=87.5962
	step [160/160], loss=76.3028
	Evaluating
	loss=0.0125, precision=0.3391, recall=0.8877, f1=0.4907
Training epoch 86
