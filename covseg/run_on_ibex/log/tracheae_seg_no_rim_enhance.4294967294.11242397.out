Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: False test_id 0
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9414 # image files with weight 9372
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2468 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_two/X 9372
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/196], loss=273.7615
	step [2/196], loss=186.8431
	step [3/196], loss=161.3343
	step [4/196], loss=156.1241
	step [5/196], loss=148.2184
	step [6/196], loss=145.1284
	step [7/196], loss=141.6558
	step [8/196], loss=140.5439
	step [9/196], loss=137.7285
	step [10/196], loss=139.1558
	step [11/196], loss=135.0403
	step [12/196], loss=133.3640
	step [13/196], loss=131.7947
	step [14/196], loss=129.2799
	step [15/196], loss=128.7230
	step [16/196], loss=126.3857
	step [17/196], loss=125.1688
	step [18/196], loss=124.2504
	step [19/196], loss=124.9828
	step [20/196], loss=121.4101
	step [21/196], loss=121.8997
	step [22/196], loss=121.1377
	step [23/196], loss=117.0218
	step [24/196], loss=118.1817
	step [25/196], loss=114.8464
	step [26/196], loss=113.4252
	step [27/196], loss=112.3417
	step [28/196], loss=115.1906
	step [29/196], loss=111.1411
	step [30/196], loss=110.8783
	step [31/196], loss=108.9799
	step [32/196], loss=109.3997
	step [33/196], loss=108.6267
	step [34/196], loss=105.6425
	step [35/196], loss=104.5045
	step [36/196], loss=104.4764
	step [37/196], loss=103.8410
	step [38/196], loss=105.3804
	step [39/196], loss=106.0008
	step [40/196], loss=103.7222
	step [41/196], loss=102.8587
	step [42/196], loss=99.0276
	step [43/196], loss=98.3403
	step [44/196], loss=96.6357
	step [45/196], loss=97.6186
	step [46/196], loss=97.5243
	step [47/196], loss=95.5337
	step [48/196], loss=94.2861
	step [49/196], loss=98.0442
	step [50/196], loss=92.7042
	step [51/196], loss=94.6224
	step [52/196], loss=92.8756
	step [53/196], loss=92.4381
	step [54/196], loss=91.2998
	step [55/196], loss=89.6041
	step [56/196], loss=88.2396
	step [57/196], loss=89.1550
	step [58/196], loss=91.4149
	step [59/196], loss=88.4415
	step [60/196], loss=93.4193
	step [61/196], loss=91.5784
	step [62/196], loss=89.4195
	step [63/196], loss=87.9784
	step [64/196], loss=86.9201
	step [65/196], loss=85.7809
	step [66/196], loss=85.4288
	step [67/196], loss=87.7690
	step [68/196], loss=86.4288
	step [69/196], loss=86.1136
	step [70/196], loss=83.4766
	step [71/196], loss=86.8479
	step [72/196], loss=84.3482
	step [73/196], loss=81.5324
	step [74/196], loss=83.5969
	step [75/196], loss=83.6961
	step [76/196], loss=81.8293
	step [77/196], loss=84.3336
	step [78/196], loss=82.6874
	step [79/196], loss=84.6776
	step [80/196], loss=82.2840
	step [81/196], loss=82.7848
	step [82/196], loss=82.5983
	step [83/196], loss=80.5056
	step [84/196], loss=80.1007
	step [85/196], loss=81.5690
	step [86/196], loss=80.4638
	step [87/196], loss=79.9091
	step [88/196], loss=80.6435
	step [89/196], loss=80.0118
	step [90/196], loss=82.2433
	step [91/196], loss=80.3039
	step [92/196], loss=77.9695
	step [93/196], loss=79.2665
	step [94/196], loss=78.1536
	step [95/196], loss=78.1413
	step [96/196], loss=78.7171
	step [97/196], loss=79.7390
	step [98/196], loss=76.8670
	step [99/196], loss=78.1236
	step [100/196], loss=79.3494
	step [101/196], loss=75.7007
	step [102/196], loss=76.8048
	step [103/196], loss=74.9716
	step [104/196], loss=77.9366
	step [105/196], loss=80.1345
	step [106/196], loss=76.2943
	step [107/196], loss=77.7750
	step [108/196], loss=75.3920
	step [109/196], loss=75.3462
	step [110/196], loss=79.0097
	step [111/196], loss=75.5048
	step [112/196], loss=78.3284
	step [113/196], loss=78.1286
	step [114/196], loss=76.3141
	step [115/196], loss=75.6187
	step [116/196], loss=75.3459
	step [117/196], loss=76.2162
	step [118/196], loss=74.1907
	step [119/196], loss=75.3929
	step [120/196], loss=75.4555
	step [121/196], loss=75.4766
	step [122/196], loss=73.7320
	step [123/196], loss=74.6609
	step [124/196], loss=73.3694
	step [125/196], loss=72.0298
	step [126/196], loss=73.7809
	step [127/196], loss=72.3174
	step [128/196], loss=72.3497
	step [129/196], loss=73.0981
	step [130/196], loss=73.2819
	step [131/196], loss=75.3052
	step [132/196], loss=73.4064
	step [133/196], loss=73.3882
	step [134/196], loss=72.7716
	step [135/196], loss=71.3807
	step [136/196], loss=70.8578
	step [137/196], loss=73.9607
	step [138/196], loss=71.0845
	step [139/196], loss=69.8623
	step [140/196], loss=70.8261
	step [141/196], loss=71.5201
	step [142/196], loss=69.5647
	step [143/196], loss=69.8594
	step [144/196], loss=69.1246
	step [145/196], loss=69.3748
	step [146/196], loss=70.0732
	step [147/196], loss=68.6342
	step [148/196], loss=73.0556
	step [149/196], loss=69.1456
	step [150/196], loss=69.1051
	step [151/196], loss=70.6914
	step [152/196], loss=69.5573
	step [153/196], loss=70.0230
	step [154/196], loss=70.2795
	step [155/196], loss=70.2719
	step [156/196], loss=68.4076
	step [157/196], loss=68.6813
	step [158/196], loss=70.8678
	step [159/196], loss=68.8452
	step [160/196], loss=70.2026
	step [161/196], loss=68.0847
	step [162/196], loss=71.1929
	step [163/196], loss=68.1822
	step [164/196], loss=68.4589
	step [165/196], loss=68.6358
	step [166/196], loss=67.8896
	step [167/196], loss=70.4982
	step [168/196], loss=69.8123
	step [169/196], loss=67.9197
	step [170/196], loss=67.8665
	step [171/196], loss=68.1076
	step [172/196], loss=70.4198
	step [173/196], loss=68.7348
	step [174/196], loss=68.2843
	step [175/196], loss=67.6113
	step [176/196], loss=67.8001
	step [177/196], loss=71.2926
	step [178/196], loss=66.7138
	step [179/196], loss=68.4141
	step [180/196], loss=68.7217
	step [181/196], loss=66.9012
	step [182/196], loss=66.7962
	step [183/196], loss=68.8548
	step [184/196], loss=64.8881
	step [185/196], loss=67.0580
	step [186/196], loss=64.8022
	step [187/196], loss=64.5095
	step [188/196], loss=64.7265
	step [189/196], loss=66.5877
	step [190/196], loss=63.9903
	step [191/196], loss=66.4454
	step [192/196], loss=65.3043
	step [193/196], loss=65.3329
	step [194/196], loss=65.9374
	step [195/196], loss=67.0925
	step [196/196], loss=18.8447
	Evaluating
	loss=0.3159, precision=0.1425, recall=0.9867, f1=0.6196
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/196], loss=63.6814
	step [2/196], loss=64.0050
	step [3/196], loss=65.5824
	step [4/196], loss=62.8219
	step [5/196], loss=64.0533
	step [6/196], loss=64.6438
	step [7/196], loss=66.0183
	step [8/196], loss=65.2714
	step [9/196], loss=66.7032
	step [10/196], loss=65.0672
	step [11/196], loss=63.2955
	step [12/196], loss=65.0412
	step [13/196], loss=62.6970
	step [14/196], loss=63.1403
	step [15/196], loss=63.7226
	step [16/196], loss=63.3226
	step [17/196], loss=64.8139
	step [18/196], loss=61.4582
	step [19/196], loss=63.0479
	step [20/196], loss=61.8459
	step [21/196], loss=61.5788
	step [22/196], loss=63.3568
	step [23/196], loss=64.3617
	step [24/196], loss=63.6039
	step [25/196], loss=62.2403
	step [26/196], loss=64.8023
	step [27/196], loss=62.6215
	step [28/196], loss=62.8681
	step [29/196], loss=63.5218
	step [30/196], loss=61.2472
	step [31/196], loss=63.7324
	step [32/196], loss=63.7789
	step [33/196], loss=63.1305
	step [34/196], loss=61.6691
	step [35/196], loss=61.4809
	step [36/196], loss=63.6876
	step [37/196], loss=61.5258
	step [38/196], loss=59.6675
	step [39/196], loss=61.7375
	step [40/196], loss=61.2509
	step [41/196], loss=61.6510
	step [42/196], loss=61.4689
	step [43/196], loss=59.9900
	step [44/196], loss=59.7638
	step [45/196], loss=60.9628
	step [46/196], loss=60.0083
	step [47/196], loss=58.9045
	step [48/196], loss=61.8192
	step [49/196], loss=62.9536
	step [50/196], loss=59.8844
	step [51/196], loss=61.7010
	step [52/196], loss=59.7420
	step [53/196], loss=61.2178
	step [54/196], loss=60.0313
	step [55/196], loss=61.4455
	step [56/196], loss=59.1389
	step [57/196], loss=60.4030
	step [58/196], loss=60.9963
	step [59/196], loss=59.8124
	step [60/196], loss=59.7689
	step [61/196], loss=61.6652
	step [62/196], loss=59.8997
	step [63/196], loss=59.2738
	step [64/196], loss=58.9222
	step [65/196], loss=57.5128
	step [66/196], loss=60.4386
	step [67/196], loss=58.1270
	step [68/196], loss=58.9480
	step [69/196], loss=60.3160
	step [70/196], loss=58.2449
	step [71/196], loss=58.9137
	step [72/196], loss=59.1729
	step [73/196], loss=58.0039
	step [74/196], loss=56.6461
	step [75/196], loss=59.0865
	step [76/196], loss=58.4988
	step [77/196], loss=56.6003
	step [78/196], loss=57.9539
	step [79/196], loss=59.0151
	step [80/196], loss=58.3555
	step [81/196], loss=62.6070
	step [82/196], loss=59.5112
	step [83/196], loss=57.6826
	step [84/196], loss=55.7616
	step [85/196], loss=57.0252
	step [86/196], loss=56.1536
	step [87/196], loss=58.2301
	step [88/196], loss=56.1057
	step [89/196], loss=59.0322
	step [90/196], loss=59.5257
	step [91/196], loss=56.1944
	step [92/196], loss=58.7972
	step [93/196], loss=58.8322
	step [94/196], loss=58.1944
	step [95/196], loss=55.7576
	step [96/196], loss=55.0128
	step [97/196], loss=58.1508
	step [98/196], loss=54.9471
	step [99/196], loss=56.3937
	step [100/196], loss=56.0491
	step [101/196], loss=54.3766
	step [102/196], loss=56.9223
	step [103/196], loss=57.2934
	step [104/196], loss=54.7380
	step [105/196], loss=55.0889
	step [106/196], loss=57.2870
	step [107/196], loss=56.5657
	step [108/196], loss=58.5935
	step [109/196], loss=54.7015
	step [110/196], loss=54.2723
	step [111/196], loss=54.9032
	step [112/196], loss=55.9031
	step [113/196], loss=54.0214
	step [114/196], loss=55.6344
	step [115/196], loss=55.2323
	step [116/196], loss=56.1023
	step [117/196], loss=57.8288
	step [118/196], loss=55.6478
	step [119/196], loss=54.1833
	step [120/196], loss=54.9004
	step [121/196], loss=53.2338
	step [122/196], loss=54.1465
	step [123/196], loss=54.5913
	step [124/196], loss=56.0593
	step [125/196], loss=55.6915
	step [126/196], loss=55.6169
	step [127/196], loss=51.6328
	step [128/196], loss=53.9316
	step [129/196], loss=55.4746
	step [130/196], loss=53.5650
	step [131/196], loss=55.8784
	step [132/196], loss=54.9109
	step [133/196], loss=53.7299
	step [134/196], loss=54.4711
	step [135/196], loss=52.9321
	step [136/196], loss=53.3703
	step [137/196], loss=54.4159
	step [138/196], loss=54.0463
	step [139/196], loss=54.2069
	step [140/196], loss=52.2754
	step [141/196], loss=52.5853
	step [142/196], loss=53.3004
	step [143/196], loss=51.5451
	step [144/196], loss=54.9367
	step [145/196], loss=52.3780
	step [146/196], loss=53.4146
	step [147/196], loss=54.9924
	step [148/196], loss=52.4508
	step [149/196], loss=52.6786
	step [150/196], loss=51.0272
	step [151/196], loss=52.8975
	step [152/196], loss=54.1060
	step [153/196], loss=52.2167
	step [154/196], loss=53.5469
	step [155/196], loss=52.6204
	step [156/196], loss=51.8630
	step [157/196], loss=51.7935
	step [158/196], loss=51.1720
	step [159/196], loss=52.5613
	step [160/196], loss=52.6921
	step [161/196], loss=51.3505
	step [162/196], loss=52.3358
	step [163/196], loss=51.3043
	step [164/196], loss=52.0474
	step [165/196], loss=51.4690
	step [166/196], loss=52.3190
	step [167/196], loss=53.7199
	step [168/196], loss=50.4215
	step [169/196], loss=50.4995
	step [170/196], loss=50.3946
	step [171/196], loss=50.1600
	step [172/196], loss=50.8696
	step [173/196], loss=52.4848
	step [174/196], loss=50.2392
	step [175/196], loss=51.8751
	step [176/196], loss=51.3081
	step [177/196], loss=50.4608
	step [178/196], loss=52.2309
	step [179/196], loss=51.4947
	step [180/196], loss=52.6738
	step [181/196], loss=49.3913
	step [182/196], loss=49.0359
	step [183/196], loss=50.6000
	step [184/196], loss=48.4569
	step [185/196], loss=50.9574
	step [186/196], loss=50.8452
	step [187/196], loss=49.4762
	step [188/196], loss=50.0429
	step [189/196], loss=49.1270
	step [190/196], loss=50.9792
	step [191/196], loss=47.2356
	step [192/196], loss=47.7675
	step [193/196], loss=49.3837
	step [194/196], loss=48.9586
	step [195/196], loss=48.3628
	step [196/196], loss=12.5361
	Evaluating
	loss=0.2228, precision=0.1763, recall=0.9868, f1=0.6760
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/196], loss=51.3015
	step [2/196], loss=50.2070
	step [3/196], loss=49.1043
	step [4/196], loss=48.6223
	step [5/196], loss=47.2688
	step [6/196], loss=47.5600
	step [7/196], loss=47.5169
	step [8/196], loss=47.3642
	step [9/196], loss=51.1134
	step [10/196], loss=49.3758
	step [11/196], loss=50.3766
	step [12/196], loss=50.1525
	step [13/196], loss=49.1709
	step [14/196], loss=49.2225
	step [15/196], loss=47.5250
	step [16/196], loss=49.1327
	step [17/196], loss=47.4814
	step [18/196], loss=49.5227
	step [19/196], loss=48.7439
	step [20/196], loss=51.4841
	step [21/196], loss=47.3780
	step [22/196], loss=48.9055
	step [23/196], loss=46.1448
	step [24/196], loss=45.6867
	step [25/196], loss=49.6161
	step [26/196], loss=48.3085
	step [27/196], loss=49.0154
	step [28/196], loss=46.8736
	step [29/196], loss=47.2176
	step [30/196], loss=49.0870
	step [31/196], loss=47.5726
	step [32/196], loss=48.1958
	step [33/196], loss=47.6435
	step [34/196], loss=44.9534
	step [35/196], loss=46.0699
	step [36/196], loss=45.7367
	step [37/196], loss=46.6024
	step [38/196], loss=45.6295
	step [39/196], loss=47.7486
	step [40/196], loss=48.2142
	step [41/196], loss=48.7454
	step [42/196], loss=46.5324
	step [43/196], loss=47.5604
	step [44/196], loss=48.0138
	step [45/196], loss=44.7083
	step [46/196], loss=45.7837
	step [47/196], loss=47.2049
	step [48/196], loss=47.4578
	step [49/196], loss=45.3842
	step [50/196], loss=46.3977
	step [51/196], loss=49.1371
	step [52/196], loss=49.5573
	step [53/196], loss=48.8120
	step [54/196], loss=48.2266
	step [55/196], loss=47.4276
	step [56/196], loss=46.0104
	step [57/196], loss=46.0315
	step [58/196], loss=47.6782
	step [59/196], loss=45.3214
	step [60/196], loss=46.7979
	step [61/196], loss=46.6828
	step [62/196], loss=44.9971
	step [63/196], loss=46.8669
	step [64/196], loss=46.0546
	step [65/196], loss=46.3365
	step [66/196], loss=46.6569
	step [67/196], loss=44.9684
	step [68/196], loss=45.6075
	step [69/196], loss=43.8821
	step [70/196], loss=45.8632
	step [71/196], loss=43.9219
	step [72/196], loss=45.4191
	step [73/196], loss=45.3055
	step [74/196], loss=45.1617
	step [75/196], loss=43.8061
	step [76/196], loss=45.7089
	step [77/196], loss=45.7672
	step [78/196], loss=43.8653
	step [79/196], loss=45.2695
	step [80/196], loss=43.9166
	step [81/196], loss=45.8520
	step [82/196], loss=45.3382
	step [83/196], loss=44.3812
	step [84/196], loss=43.4488
	step [85/196], loss=45.7573
	step [86/196], loss=42.7924
	step [87/196], loss=45.4920
	step [88/196], loss=45.5775
	step [89/196], loss=42.5456
	step [90/196], loss=45.6309
	step [91/196], loss=43.4747
	step [92/196], loss=43.7623
	step [93/196], loss=44.9027
	step [94/196], loss=42.1470
	step [95/196], loss=42.4533
	step [96/196], loss=44.8774
	step [97/196], loss=42.8309
	step [98/196], loss=45.6266
	step [99/196], loss=42.5091
	step [100/196], loss=42.9128
	step [101/196], loss=43.5579
	step [102/196], loss=44.8457
	step [103/196], loss=41.4365
	step [104/196], loss=42.6553
	step [105/196], loss=41.4940
	step [106/196], loss=41.5384
	step [107/196], loss=46.3085
	step [108/196], loss=44.2123
	step [109/196], loss=42.5443
	step [110/196], loss=47.3520
	step [111/196], loss=41.6018
	step [112/196], loss=44.9833
	step [113/196], loss=42.5516
	step [114/196], loss=41.9989
	step [115/196], loss=43.7504
	step [116/196], loss=43.9508
	step [117/196], loss=44.4280
	step [118/196], loss=42.1832
	step [119/196], loss=42.0571
	step [120/196], loss=41.0618
	step [121/196], loss=43.1834
	step [122/196], loss=46.0328
	step [123/196], loss=40.8356
	step [124/196], loss=42.9396
	step [125/196], loss=43.4987
	step [126/196], loss=41.9544
	step [127/196], loss=41.8957
	step [128/196], loss=41.1313
	step [129/196], loss=40.0208
	step [130/196], loss=42.9985
	step [131/196], loss=42.4836
	step [132/196], loss=40.3961
	step [133/196], loss=41.7724
	step [134/196], loss=42.5585
	step [135/196], loss=41.8387
	step [136/196], loss=41.9304
	step [137/196], loss=42.9650
	step [138/196], loss=41.8804
	step [139/196], loss=40.6068
	step [140/196], loss=43.2965
	step [141/196], loss=43.7110
	step [142/196], loss=41.7308
	step [143/196], loss=40.6459
	step [144/196], loss=41.6049
	step [145/196], loss=40.4390
	step [146/196], loss=39.6555
	step [147/196], loss=42.3501
	step [148/196], loss=41.0656
	step [149/196], loss=41.5243
	step [150/196], loss=42.0767
	step [151/196], loss=40.4749
	step [152/196], loss=42.4553
	step [153/196], loss=40.4559
	step [154/196], loss=39.0964
	step [155/196], loss=44.4368
	step [156/196], loss=40.4161
	step [157/196], loss=41.3260
	step [158/196], loss=41.9835
	step [159/196], loss=39.5649
	step [160/196], loss=40.9887
	step [161/196], loss=41.6270
	step [162/196], loss=43.9664
	step [163/196], loss=40.1058
	step [164/196], loss=39.6121
	step [165/196], loss=39.1650
	step [166/196], loss=41.1544
	step [167/196], loss=40.1015
	step [168/196], loss=41.8215
	step [169/196], loss=42.5987
	step [170/196], loss=39.0257
	step [171/196], loss=40.0182
	step [172/196], loss=41.9971
	step [173/196], loss=40.1000
	step [174/196], loss=38.0202
	step [175/196], loss=38.1666
	step [176/196], loss=38.3795
	step [177/196], loss=41.5726
	step [178/196], loss=41.4199
	step [179/196], loss=37.3765
	step [180/196], loss=39.4391
	step [181/196], loss=38.2023
	step [182/196], loss=36.3630
	step [183/196], loss=39.1177
	step [184/196], loss=40.7281
	step [185/196], loss=39.5519
	step [186/196], loss=38.2896
	step [187/196], loss=37.4600
	step [188/196], loss=38.9001
	step [189/196], loss=39.1162
	step [190/196], loss=38.9176
	step [191/196], loss=38.2687
	step [192/196], loss=37.7820
	step [193/196], loss=40.9133
	step [194/196], loss=39.1558
	step [195/196], loss=41.5798
	step [196/196], loss=11.8950
	Evaluating
	loss=0.1876, precision=0.0920, recall=0.9924, f1=0.5017
Training epoch 4
	step [1/196], loss=37.9860
	step [2/196], loss=38.5167
	step [3/196], loss=39.7373
	step [4/196], loss=39.7483
	step [5/196], loss=39.8862
	step [6/196], loss=39.5756
	step [7/196], loss=36.4392
	step [8/196], loss=36.9281
	step [9/196], loss=38.9103
	step [10/196], loss=39.6291
	step [11/196], loss=36.0387
	step [12/196], loss=38.5148
	step [13/196], loss=38.5997
	step [14/196], loss=40.0337
	step [15/196], loss=37.0331
	step [16/196], loss=38.5788
	step [17/196], loss=40.6877
	step [18/196], loss=38.0076
	step [19/196], loss=37.7286
	step [20/196], loss=39.5331
	step [21/196], loss=36.9521
	step [22/196], loss=38.5425
	step [23/196], loss=37.3846
	step [24/196], loss=37.6053
	step [25/196], loss=39.9997
	step [26/196], loss=38.3600
	step [27/196], loss=37.4168
	step [28/196], loss=37.7548
	step [29/196], loss=37.7972
	step [30/196], loss=37.3812
	step [31/196], loss=36.9508
	step [32/196], loss=39.4294
	step [33/196], loss=35.1766
	step [34/196], loss=37.6639
	step [35/196], loss=36.5466
	step [36/196], loss=39.9112
	step [37/196], loss=38.2167
	step [38/196], loss=41.3239
	step [39/196], loss=37.0549
	step [40/196], loss=39.2716
	step [41/196], loss=37.3522
	step [42/196], loss=37.6430
	step [43/196], loss=34.8416
	step [44/196], loss=37.2601
	step [45/196], loss=37.4919
	step [46/196], loss=37.8518
	step [47/196], loss=35.7535
	step [48/196], loss=36.2556
	step [49/196], loss=36.8044
	step [50/196], loss=34.9593
	step [51/196], loss=35.5803
	step [52/196], loss=35.0637
	step [53/196], loss=35.3669
	step [54/196], loss=35.0916
	step [55/196], loss=38.4356
	step [56/196], loss=38.4364
	step [57/196], loss=35.3436
	step [58/196], loss=36.2325
	step [59/196], loss=35.7728
	step [60/196], loss=35.5839
	step [61/196], loss=34.4520
	step [62/196], loss=35.3487
	step [63/196], loss=36.2826
	step [64/196], loss=35.3487
	step [65/196], loss=37.2752
	step [66/196], loss=33.7682
	step [67/196], loss=38.4608
	step [68/196], loss=35.7284
	step [69/196], loss=34.5204
	step [70/196], loss=36.8381
	step [71/196], loss=35.6002
	step [72/196], loss=35.5200
	step [73/196], loss=34.0718
	step [74/196], loss=35.7051
	step [75/196], loss=34.1909
	step [76/196], loss=36.3709
	step [77/196], loss=38.6866
	step [78/196], loss=35.4072
	step [79/196], loss=36.9407
	step [80/196], loss=36.8148
	step [81/196], loss=36.6064
	step [82/196], loss=33.9803
	step [83/196], loss=36.2130
	step [84/196], loss=35.3373
	step [85/196], loss=36.8081
	step [86/196], loss=37.1167
	step [87/196], loss=34.3337
	step [88/196], loss=36.0538
	step [89/196], loss=34.7873
	step [90/196], loss=35.7952
	step [91/196], loss=37.0418
	step [92/196], loss=33.8094
	step [93/196], loss=37.0447
	step [94/196], loss=37.0921
	step [95/196], loss=36.6588
	step [96/196], loss=36.9606
	step [97/196], loss=35.5709
	step [98/196], loss=34.0083
	step [99/196], loss=34.5128
	step [100/196], loss=35.0374
	step [101/196], loss=39.0273
	step [102/196], loss=37.2708
	step [103/196], loss=36.6197
	step [104/196], loss=33.7312
	step [105/196], loss=35.0934
	step [106/196], loss=35.9571
	step [107/196], loss=34.0041
	step [108/196], loss=32.7621
	step [109/196], loss=35.4001
	step [110/196], loss=34.5437
	step [111/196], loss=35.1343
	step [112/196], loss=33.5850
	step [113/196], loss=35.8571
	step [114/196], loss=35.4186
	step [115/196], loss=34.5782
	step [116/196], loss=35.9762
	step [117/196], loss=33.7880
	step [118/196], loss=35.8231
	step [119/196], loss=34.2789
	step [120/196], loss=33.5540
	step [121/196], loss=34.5199
	step [122/196], loss=33.7264
	step [123/196], loss=34.4369
	step [124/196], loss=35.6717
	step [125/196], loss=33.6158
	step [126/196], loss=35.4766
	step [127/196], loss=33.2565
	step [128/196], loss=34.3722
	step [129/196], loss=32.8812
	step [130/196], loss=33.2490
	step [131/196], loss=33.2827
	step [132/196], loss=33.5953
	step [133/196], loss=32.4914
	step [134/196], loss=31.3619
	step [135/196], loss=36.0060
	step [136/196], loss=30.6481
	step [137/196], loss=32.5738
	step [138/196], loss=32.5440
	step [139/196], loss=34.0367
	step [140/196], loss=32.9865
	step [141/196], loss=32.2822
	step [142/196], loss=33.2035
	step [143/196], loss=33.3119
	step [144/196], loss=32.8779
	step [145/196], loss=33.5681
	step [146/196], loss=31.9404
	step [147/196], loss=33.1040
	step [148/196], loss=33.9579
	step [149/196], loss=33.6818
	step [150/196], loss=31.7355
	step [151/196], loss=34.0879
	step [152/196], loss=33.4703
	step [153/196], loss=32.9825
	step [154/196], loss=34.1039
	step [155/196], loss=33.2516
	step [156/196], loss=32.7293
	step [157/196], loss=32.8929
	step [158/196], loss=34.5818
	step [159/196], loss=32.2968
	step [160/196], loss=32.7452
	step [161/196], loss=31.1148
	step [162/196], loss=33.7157
	step [163/196], loss=33.1750
	step [164/196], loss=31.2867
	step [165/196], loss=31.1183
	step [166/196], loss=30.9786
	step [167/196], loss=33.0345
	step [168/196], loss=32.7619
	step [169/196], loss=32.4264
	step [170/196], loss=33.5586
	step [171/196], loss=31.2508
	step [172/196], loss=32.2225
	step [173/196], loss=34.9059
	step [174/196], loss=36.5333
	step [175/196], loss=32.7128
	step [176/196], loss=33.3034
	step [177/196], loss=32.3276
	step [178/196], loss=31.8722
	step [179/196], loss=30.4666
	step [180/196], loss=31.1716
	step [181/196], loss=29.6615
	step [182/196], loss=32.2919
	step [183/196], loss=29.1273
	step [184/196], loss=31.7538
	step [185/196], loss=31.5724
	step [186/196], loss=32.0752
	step [187/196], loss=29.2143
	step [188/196], loss=31.7010
	step [189/196], loss=32.2586
	step [190/196], loss=30.0302
	step [191/196], loss=30.5273
	step [192/196], loss=31.0233
	step [193/196], loss=30.6330
	step [194/196], loss=31.1850
	step [195/196], loss=32.5044
	step [196/196], loss=8.8213
	Evaluating
	loss=0.1330, precision=0.1542, recall=0.9904, f1=0.6422
Training epoch 5
	step [1/196], loss=30.5325
	step [2/196], loss=33.9778
	step [3/196], loss=30.5672
	step [4/196], loss=31.8842
	step [5/196], loss=32.5987
	step [6/196], loss=32.2824
	step [7/196], loss=30.8186
	step [8/196], loss=30.6182
	step [9/196], loss=31.3575
	step [10/196], loss=32.5764
	step [11/196], loss=33.7222
	step [12/196], loss=31.1124
	step [13/196], loss=29.0549
	step [14/196], loss=32.4666
	step [15/196], loss=31.1211
	step [16/196], loss=33.8762
	step [17/196], loss=32.0239
	step [18/196], loss=31.2135
	step [19/196], loss=29.3205
	step [20/196], loss=31.4549
	step [21/196], loss=31.7837
	step [22/196], loss=31.0847
	step [23/196], loss=34.4471
	step [24/196], loss=32.3733
	step [25/196], loss=30.9787
	step [26/196], loss=30.5172
	step [27/196], loss=30.8352
	step [28/196], loss=33.8582
	step [29/196], loss=31.9430
	step [30/196], loss=29.8323
	step [31/196], loss=31.5561
	step [32/196], loss=31.7594
	step [33/196], loss=30.6797
	step [34/196], loss=31.1577
	step [35/196], loss=29.7749
	step [36/196], loss=31.7168
	step [37/196], loss=28.9099
	step [38/196], loss=30.6444
	step [39/196], loss=29.8460
	step [40/196], loss=29.6416
	step [41/196], loss=33.3494
	step [42/196], loss=30.0668
	step [43/196], loss=31.7045
	step [44/196], loss=30.9235
	step [45/196], loss=30.4460
	step [46/196], loss=30.3082
	step [47/196], loss=33.2828
	step [48/196], loss=29.1737
	step [49/196], loss=29.0535
	step [50/196], loss=31.7224
	step [51/196], loss=32.7018
	step [52/196], loss=30.4031
	step [53/196], loss=32.1519
	step [54/196], loss=30.7170
	step [55/196], loss=29.6226
	step [56/196], loss=29.9060
	step [57/196], loss=33.1125
	step [58/196], loss=28.1016
	step [59/196], loss=34.4358
	step [60/196], loss=28.8039
	step [61/196], loss=31.2136
	step [62/196], loss=30.3385
	step [63/196], loss=28.9097
	step [64/196], loss=28.8604
	step [65/196], loss=30.1756
	step [66/196], loss=27.9468
	step [67/196], loss=30.9023
	step [68/196], loss=30.2065
	step [69/196], loss=30.0014
	step [70/196], loss=31.3525
	step [71/196], loss=29.7223
	step [72/196], loss=29.6196
	step [73/196], loss=29.8069
	step [74/196], loss=30.0574
	step [75/196], loss=29.9283
	step [76/196], loss=30.1448
	step [77/196], loss=28.7788
	step [78/196], loss=33.1074
	step [79/196], loss=28.4320
	step [80/196], loss=29.9113
	step [81/196], loss=29.7385
	step [82/196], loss=27.8557
	step [83/196], loss=32.2615
	step [84/196], loss=28.5929
	step [85/196], loss=31.9380
	step [86/196], loss=30.6369
	step [87/196], loss=28.2655
	step [88/196], loss=29.7026
	step [89/196], loss=31.1763
	step [90/196], loss=27.6319
	step [91/196], loss=30.5290
	step [92/196], loss=28.3159
	step [93/196], loss=27.2668
	step [94/196], loss=29.4030
	step [95/196], loss=31.1015
	step [96/196], loss=28.0203
	step [97/196], loss=28.3219
	step [98/196], loss=30.2406
	step [99/196], loss=27.5357
	step [100/196], loss=29.5969
	step [101/196], loss=30.7571
	step [102/196], loss=28.1853
	step [103/196], loss=29.5230
	step [104/196], loss=29.4451
	step [105/196], loss=29.5674
	step [106/196], loss=28.7002
	step [107/196], loss=28.2502
	step [108/196], loss=27.9334
	step [109/196], loss=27.6516
	step [110/196], loss=28.4837
	step [111/196], loss=28.7583
	step [112/196], loss=28.8576
	step [113/196], loss=29.4570
	step [114/196], loss=30.5970
	step [115/196], loss=28.2108
	step [116/196], loss=28.3102
	step [117/196], loss=30.7315
	step [118/196], loss=28.3493
	step [119/196], loss=27.8817
	step [120/196], loss=30.1228
	step [121/196], loss=28.5315
	step [122/196], loss=28.2107
	step [123/196], loss=27.4226
	step [124/196], loss=27.7145
	step [125/196], loss=28.1503
	step [126/196], loss=27.5527
	step [127/196], loss=26.2994
	step [128/196], loss=29.6962
	step [129/196], loss=28.7534
	step [130/196], loss=27.1282
	step [131/196], loss=28.0389
	step [132/196], loss=28.4882
	step [133/196], loss=28.8042
	step [134/196], loss=27.8668
	step [135/196], loss=27.4572
	step [136/196], loss=26.6512
	step [137/196], loss=27.3761
	step [138/196], loss=29.4764
	step [139/196], loss=27.5619
	step [140/196], loss=27.1919
	step [141/196], loss=30.3434
	step [142/196], loss=29.3526
	step [143/196], loss=27.7948
	step [144/196], loss=27.7931
	step [145/196], loss=27.5161
	step [146/196], loss=27.1300
	step [147/196], loss=29.7288
	step [148/196], loss=29.6265
	step [149/196], loss=26.7001
	step [150/196], loss=28.4372
	step [151/196], loss=26.6877
	step [152/196], loss=27.5257
	step [153/196], loss=27.4770
	step [154/196], loss=26.5454
	step [155/196], loss=27.2405
	step [156/196], loss=28.1219
	step [157/196], loss=26.4856
	step [158/196], loss=26.4099
	step [159/196], loss=27.4645
	step [160/196], loss=25.6927
	step [161/196], loss=26.9361
	step [162/196], loss=26.5155
	step [163/196], loss=27.5466
	step [164/196], loss=27.6522
	step [165/196], loss=27.3432
	step [166/196], loss=26.8192
	step [167/196], loss=27.3312
	step [168/196], loss=26.0919
	step [169/196], loss=28.7713
	step [170/196], loss=27.3494
	step [171/196], loss=26.1093
	step [172/196], loss=27.0241
	step [173/196], loss=26.6359
	step [174/196], loss=24.6405
	step [175/196], loss=29.1852
	step [176/196], loss=28.0735
	step [177/196], loss=28.6513
	step [178/196], loss=24.8525
	step [179/196], loss=26.5705
	step [180/196], loss=27.2522
	step [181/196], loss=25.2069
	step [182/196], loss=24.8561
	step [183/196], loss=25.5151
	step [184/196], loss=31.2287
	step [185/196], loss=24.8128
	step [186/196], loss=30.5617
	step [187/196], loss=26.7804
	step [188/196], loss=26.1722
	step [189/196], loss=26.5880
	step [190/196], loss=26.7237
	step [191/196], loss=26.7662
	step [192/196], loss=27.3777
	step [193/196], loss=26.8253
	step [194/196], loss=27.7339
	step [195/196], loss=26.6152
	step [196/196], loss=7.1619
	Evaluating
	loss=0.1096, precision=0.1481, recall=0.9900, f1=0.6312
Training epoch 6
	step [1/196], loss=25.7517
	step [2/196], loss=26.9819
	step [3/196], loss=26.4609
	step [4/196], loss=25.5877
	step [5/196], loss=24.1946
	step [6/196], loss=28.6857
	step [7/196], loss=28.4515
	step [8/196], loss=26.2691
	step [9/196], loss=26.9182
	step [10/196], loss=25.0316
	step [11/196], loss=27.5864
	step [12/196], loss=25.3714
	step [13/196], loss=25.9374
	step [14/196], loss=25.4739
	step [15/196], loss=30.6343
	step [16/196], loss=25.8718
	step [17/196], loss=26.1308
	step [18/196], loss=24.5935
	step [19/196], loss=26.8233
	step [20/196], loss=26.3131
	step [21/196], loss=25.6842
	step [22/196], loss=25.1354
	step [23/196], loss=26.6982
	step [24/196], loss=25.7429
	step [25/196], loss=26.5055
	step [26/196], loss=28.9784
	step [27/196], loss=26.0962
	step [28/196], loss=26.1189
	step [29/196], loss=26.0425
	step [30/196], loss=26.9350
	step [31/196], loss=26.9539
	step [32/196], loss=25.4015
	step [33/196], loss=25.6224
	step [34/196], loss=24.4198
	step [35/196], loss=26.8249
	step [36/196], loss=27.2100
	step [37/196], loss=26.4581
	step [38/196], loss=24.3124
	step [39/196], loss=25.6425
	step [40/196], loss=28.0429
	step [41/196], loss=27.1816
	step [42/196], loss=27.8975
	step [43/196], loss=24.5361
	step [44/196], loss=26.2996
	step [45/196], loss=25.9222
	step [46/196], loss=26.8635
	step [47/196], loss=25.1528
	step [48/196], loss=28.0856
	step [49/196], loss=26.7951
	step [50/196], loss=25.9534
	step [51/196], loss=24.8079
	step [52/196], loss=26.8598
	step [53/196], loss=25.0535
	step [54/196], loss=26.1453
	step [55/196], loss=24.2416
	step [56/196], loss=28.9305
	step [57/196], loss=25.9914
	step [58/196], loss=23.2826
	step [59/196], loss=26.0012
	step [60/196], loss=24.2362
	step [61/196], loss=25.4076
	step [62/196], loss=26.8998
	step [63/196], loss=23.6849
	step [64/196], loss=25.5485
	step [65/196], loss=26.0268
	step [66/196], loss=25.3464
	step [67/196], loss=24.8643
	step [68/196], loss=25.1059
	step [69/196], loss=26.6555
	step [70/196], loss=25.4508
	step [71/196], loss=27.7533
	step [72/196], loss=28.2995
	step [73/196], loss=26.2662
	step [74/196], loss=27.5422
	step [75/196], loss=24.5144
	step [76/196], loss=23.8252
	step [77/196], loss=23.1276
	step [78/196], loss=26.3464
	step [79/196], loss=28.8540
	step [80/196], loss=24.3389
	step [81/196], loss=23.1133
	step [82/196], loss=23.5360
	step [83/196], loss=25.3298
	step [84/196], loss=25.3388
	step [85/196], loss=27.8207
	step [86/196], loss=23.7657
	step [87/196], loss=26.7227
	step [88/196], loss=25.6500
	step [89/196], loss=24.3977
	step [90/196], loss=26.6328
	step [91/196], loss=26.5559
	step [92/196], loss=24.5257
	step [93/196], loss=26.6854
	step [94/196], loss=26.4055
	step [95/196], loss=24.0647
	step [96/196], loss=25.5496
	step [97/196], loss=26.8257
	step [98/196], loss=23.1649
	step [99/196], loss=27.2760
	step [100/196], loss=24.3632
	step [101/196], loss=26.0302
	step [102/196], loss=25.8235
	step [103/196], loss=26.5976
	step [104/196], loss=23.8606
	step [105/196], loss=24.8936
	step [106/196], loss=22.6461
	step [107/196], loss=28.5772
	step [108/196], loss=26.2982
	step [109/196], loss=25.8903
	step [110/196], loss=26.6286
	step [111/196], loss=25.6736
	step [112/196], loss=23.0865
	step [113/196], loss=25.4158
	step [114/196], loss=24.1792
	step [115/196], loss=24.7262
	step [116/196], loss=22.9905
	step [117/196], loss=25.2873
	step [118/196], loss=24.8850
	step [119/196], loss=25.4035
	step [120/196], loss=23.3537
	step [121/196], loss=23.1584
	step [122/196], loss=25.9184
	step [123/196], loss=27.6005
	step [124/196], loss=24.8922
	step [125/196], loss=25.1063
	step [126/196], loss=25.2439
	step [127/196], loss=23.0500
	step [128/196], loss=26.6848
	step [129/196], loss=22.7816
	step [130/196], loss=23.5386
	step [131/196], loss=22.9299
	step [132/196], loss=24.1064
	step [133/196], loss=25.2478
	step [134/196], loss=24.8930
	step [135/196], loss=27.1251
	step [136/196], loss=22.3503
	step [137/196], loss=24.1752
	step [138/196], loss=24.2491
	step [139/196], loss=25.0586
	step [140/196], loss=23.4145
	step [141/196], loss=23.6897
	step [142/196], loss=26.9427
	step [143/196], loss=25.2840
	step [144/196], loss=25.5106
	step [145/196], loss=26.6123
	step [146/196], loss=23.0462
	step [147/196], loss=22.9248
	step [148/196], loss=24.4742
	step [149/196], loss=22.3959
	step [150/196], loss=25.7183
	step [151/196], loss=25.0642
	step [152/196], loss=24.3772
	step [153/196], loss=23.3856
	step [154/196], loss=22.3263
	step [155/196], loss=25.4670
	step [156/196], loss=26.0752
	step [157/196], loss=23.9610
	step [158/196], loss=23.9743
	step [159/196], loss=22.5861
	step [160/196], loss=23.6139
	step [161/196], loss=22.0981
	step [162/196], loss=25.7813
	step [163/196], loss=26.0382
	step [164/196], loss=23.1882
	step [165/196], loss=24.5769
	step [166/196], loss=22.6800
	step [167/196], loss=26.3087
	step [168/196], loss=23.1124
	step [169/196], loss=22.9216
	step [170/196], loss=24.6247
	step [171/196], loss=24.2670
	step [172/196], loss=22.4233
	step [173/196], loss=23.9270
	step [174/196], loss=24.5153
	step [175/196], loss=25.1853
	step [176/196], loss=23.3304
	step [177/196], loss=21.1139
	step [178/196], loss=24.4689
	step [179/196], loss=24.9918
	step [180/196], loss=23.0885
	step [181/196], loss=23.9823
	step [182/196], loss=22.7670
	step [183/196], loss=23.9051
	step [184/196], loss=25.6572
	step [185/196], loss=23.7609
	step [186/196], loss=23.0848
	step [187/196], loss=21.7516
	step [188/196], loss=24.7131
	step [189/196], loss=25.7155
	step [190/196], loss=22.0298
	step [191/196], loss=21.0955
	step [192/196], loss=21.9186
	step [193/196], loss=25.5219
	step [194/196], loss=27.8328
	step [195/196], loss=23.2842
	step [196/196], loss=5.6579
	Evaluating
	loss=0.0865, precision=0.1956, recall=0.9879, f1=0.7031
saving model as: 0_saved_model.pth
Training epoch 7
	step [1/196], loss=26.0177
	step [2/196], loss=22.8167
	step [3/196], loss=23.3673
	step [4/196], loss=22.6777
	step [5/196], loss=23.6614
	step [6/196], loss=23.2563
	step [7/196], loss=22.9472
	step [8/196], loss=24.7041
	step [9/196], loss=22.7817
	step [10/196], loss=21.7738
	step [11/196], loss=22.4573
	step [12/196], loss=24.2437
	step [13/196], loss=23.6457
	step [14/196], loss=23.3327
	step [15/196], loss=22.6838
	step [16/196], loss=20.8636
	step [17/196], loss=21.7977
	step [18/196], loss=24.0872
	step [19/196], loss=22.6366
	step [20/196], loss=23.7487
	step [21/196], loss=22.2289
	step [22/196], loss=23.5049
	step [23/196], loss=21.5290
	step [24/196], loss=25.1484
	step [25/196], loss=22.3557
	step [26/196], loss=24.8761
	step [27/196], loss=23.2368
	step [28/196], loss=21.9993
	step [29/196], loss=24.5898
	step [30/196], loss=24.6457
	step [31/196], loss=21.7597
	step [32/196], loss=24.6177
	step [33/196], loss=22.8217
	step [34/196], loss=21.0836
	step [35/196], loss=21.4959
	step [36/196], loss=26.4294
	step [37/196], loss=23.1325
	step [38/196], loss=22.2700
	step [39/196], loss=22.3090
	step [40/196], loss=21.7150
	step [41/196], loss=20.5227
	step [42/196], loss=23.0157
	step [43/196], loss=21.0757
	step [44/196], loss=21.8593
	step [45/196], loss=21.6627
	step [46/196], loss=21.1784
	step [47/196], loss=23.4601
	step [48/196], loss=20.2908
	step [49/196], loss=22.2823
	step [50/196], loss=21.8284
	step [51/196], loss=20.8046
	step [52/196], loss=22.9410
	step [53/196], loss=24.0557
	step [54/196], loss=23.6757
	step [55/196], loss=23.3441
	step [56/196], loss=21.8068
	step [57/196], loss=21.4479
	step [58/196], loss=23.1038
	step [59/196], loss=24.7167
	step [60/196], loss=21.5895
	step [61/196], loss=22.4836
	step [62/196], loss=21.6601
	step [63/196], loss=21.3988
	step [64/196], loss=22.6289
	step [65/196], loss=22.2339
	step [66/196], loss=23.9447
	step [67/196], loss=23.8334
	step [68/196], loss=23.2557
	step [69/196], loss=24.7822
	step [70/196], loss=23.2449
	step [71/196], loss=22.5939
	step [72/196], loss=23.6673
	step [73/196], loss=22.1951
	step [74/196], loss=21.4237
	step [75/196], loss=24.6711
	step [76/196], loss=21.6755
	step [77/196], loss=22.7637
	step [78/196], loss=21.7369
	step [79/196], loss=22.3605
	step [80/196], loss=23.7377
	step [81/196], loss=22.1764
	step [82/196], loss=20.5983
	step [83/196], loss=19.8261
	step [84/196], loss=22.5967
	step [85/196], loss=22.6732
	step [86/196], loss=23.8283
	step [87/196], loss=20.9204
	step [88/196], loss=21.4699
	step [89/196], loss=23.5081
	step [90/196], loss=22.3958
	step [91/196], loss=21.7440
	step [92/196], loss=21.6947
	step [93/196], loss=21.4365
	step [94/196], loss=22.4593
	step [95/196], loss=22.3863
	step [96/196], loss=23.8773
	step [97/196], loss=20.1137
	step [98/196], loss=22.7848
	step [99/196], loss=21.1108
	step [100/196], loss=22.7731
	step [101/196], loss=21.3203
	step [102/196], loss=22.1104
	step [103/196], loss=21.8918
	step [104/196], loss=22.6230
	step [105/196], loss=22.7512
	step [106/196], loss=22.7212
	step [107/196], loss=21.0094
	step [108/196], loss=21.6483
	step [109/196], loss=21.1137
	step [110/196], loss=21.7287
	step [111/196], loss=23.1557
	step [112/196], loss=20.8781
	step [113/196], loss=23.1790
	step [114/196], loss=21.4704
	step [115/196], loss=22.0526
	step [116/196], loss=21.2329
	step [117/196], loss=21.5372
	step [118/196], loss=22.4880
	step [119/196], loss=21.1317
	step [120/196], loss=23.9304
	step [121/196], loss=20.8674
	step [122/196], loss=21.0188
	step [123/196], loss=19.5168
	step [124/196], loss=23.1218
	step [125/196], loss=24.0859
	step [126/196], loss=22.5220
	step [127/196], loss=20.3367
	step [128/196], loss=20.8937
	step [129/196], loss=21.5032
	step [130/196], loss=22.4040
	step [131/196], loss=25.8687
	step [132/196], loss=22.0657
	step [133/196], loss=22.1798
	step [134/196], loss=20.9848
	step [135/196], loss=22.2234
	step [136/196], loss=21.2624
	step [137/196], loss=22.8368
	step [138/196], loss=25.6071
	step [139/196], loss=23.4048
	step [140/196], loss=21.4657
	step [141/196], loss=21.2901
	step [142/196], loss=21.6475
	step [143/196], loss=22.0664
	step [144/196], loss=24.5010
	step [145/196], loss=22.0138
	step [146/196], loss=20.9160
	step [147/196], loss=21.6283
	step [148/196], loss=20.3191
	step [149/196], loss=20.4948
	step [150/196], loss=19.6404
	step [151/196], loss=23.5617
	step [152/196], loss=23.9996
	step [153/196], loss=22.1895
	step [154/196], loss=19.3104
	step [155/196], loss=21.2675
	step [156/196], loss=21.7682
	step [157/196], loss=23.4003
	step [158/196], loss=21.1395
	step [159/196], loss=19.3676
	step [160/196], loss=24.0652
	step [161/196], loss=24.7231
	step [162/196], loss=22.4189
	step [163/196], loss=21.1376
	step [164/196], loss=22.4976
	step [165/196], loss=24.0802
	step [166/196], loss=21.0926
	step [167/196], loss=20.0419
	step [168/196], loss=23.9282
	step [169/196], loss=22.3210
	step [170/196], loss=19.4634
	step [171/196], loss=24.3478
	step [172/196], loss=19.6105
	step [173/196], loss=20.3029
	step [174/196], loss=22.1874
	step [175/196], loss=21.1410
	step [176/196], loss=20.7622
	step [177/196], loss=21.8748
	step [178/196], loss=20.1122
	step [179/196], loss=23.0166
	step [180/196], loss=19.4187
	step [181/196], loss=19.5476
	step [182/196], loss=21.0350
	step [183/196], loss=20.3827
	step [184/196], loss=22.3360
	step [185/196], loss=21.6698
	step [186/196], loss=21.5812
	step [187/196], loss=20.6668
	step [188/196], loss=20.6881
	step [189/196], loss=19.8966
	step [190/196], loss=21.3604
	step [191/196], loss=22.2556
	step [192/196], loss=21.6256
	step [193/196], loss=21.9441
	step [194/196], loss=22.2061
	step [195/196], loss=19.4492
	step [196/196], loss=5.8713
	Evaluating
	loss=0.0905, precision=0.1021, recall=0.9930, f1=0.5302
Training epoch 8
	step [1/196], loss=20.4068
	step [2/196], loss=23.8187
	step [3/196], loss=20.3486
	step [4/196], loss=19.5215
	step [5/196], loss=21.2997
	step [6/196], loss=22.4381
	step [7/196], loss=20.3063
	step [8/196], loss=22.3279
	step [9/196], loss=21.9614
	step [10/196], loss=21.4882
	step [11/196], loss=18.6371
	step [12/196], loss=21.5671
	step [13/196], loss=20.6585
	step [14/196], loss=19.0071
	step [15/196], loss=21.2816
	step [16/196], loss=19.7616
	step [17/196], loss=20.4476
	step [18/196], loss=21.2568
	step [19/196], loss=19.5484
	step [20/196], loss=19.5726
	step [21/196], loss=21.9691
	step [22/196], loss=21.6959
	step [23/196], loss=18.3385
	step [24/196], loss=21.0295
	step [25/196], loss=23.1269
	step [26/196], loss=19.0015
	step [27/196], loss=21.0644
	step [28/196], loss=19.7001
	step [29/196], loss=20.9774
	step [30/196], loss=19.1479
	step [31/196], loss=18.1634
	step [32/196], loss=18.6464
	step [33/196], loss=21.2087
	step [34/196], loss=20.3545
	step [35/196], loss=20.2681
	step [36/196], loss=19.4421
	step [37/196], loss=20.0349
	step [38/196], loss=21.5764
	step [39/196], loss=17.6240
	step [40/196], loss=18.1430
	step [41/196], loss=19.8137
	step [42/196], loss=22.1275
	step [43/196], loss=20.5504
	step [44/196], loss=21.1168
	step [45/196], loss=19.1539
	step [46/196], loss=20.7625
	step [47/196], loss=21.9432
	step [48/196], loss=20.2079
	step [49/196], loss=22.2652
	step [50/196], loss=19.3736
	step [51/196], loss=21.2387
	step [52/196], loss=16.0471
	step [53/196], loss=20.5960
	step [54/196], loss=19.1753
	step [55/196], loss=21.2429
	step [56/196], loss=21.6018
	step [57/196], loss=20.0492
	step [58/196], loss=22.3984
	step [59/196], loss=20.1407
	step [60/196], loss=21.0649
	step [61/196], loss=20.9446
	step [62/196], loss=19.1561
	step [63/196], loss=19.1076
	step [64/196], loss=18.1125
	step [65/196], loss=24.2235
	step [66/196], loss=19.0320
	step [67/196], loss=22.0888
	step [68/196], loss=18.8164
	step [69/196], loss=19.2972
	step [70/196], loss=21.9012
	step [71/196], loss=20.4656
	step [72/196], loss=22.2780
	step [73/196], loss=20.6620
	step [74/196], loss=20.7725
	step [75/196], loss=19.6457
	step [76/196], loss=19.5485
	step [77/196], loss=21.4921
	step [78/196], loss=21.4127
	step [79/196], loss=21.8097
	step [80/196], loss=21.0233
	step [81/196], loss=22.2983
	step [82/196], loss=19.9893
	step [83/196], loss=20.4441
	step [84/196], loss=18.8706
	step [85/196], loss=22.6696
	step [86/196], loss=23.0007
	step [87/196], loss=20.2336
	step [88/196], loss=18.5080
	step [89/196], loss=20.8309
	step [90/196], loss=20.3179
	step [91/196], loss=20.2526
	step [92/196], loss=21.0182
	step [93/196], loss=21.0612
	step [94/196], loss=19.7737
	step [95/196], loss=19.7022
	step [96/196], loss=18.1656
	step [97/196], loss=19.2362
	step [98/196], loss=19.4097
	step [99/196], loss=22.1396
	step [100/196], loss=20.9340
	step [101/196], loss=19.8696
	step [102/196], loss=21.3570
	step [103/196], loss=21.8291
	step [104/196], loss=20.0424
	step [105/196], loss=21.7439
	step [106/196], loss=19.5610
	step [107/196], loss=19.2722
	step [108/196], loss=17.5842
	step [109/196], loss=19.0176
	step [110/196], loss=18.0534
	step [111/196], loss=19.8849
	step [112/196], loss=20.1808
	step [113/196], loss=18.5156
	step [114/196], loss=19.8652
	step [115/196], loss=21.4774
	step [116/196], loss=18.7247
	step [117/196], loss=20.4088
	step [118/196], loss=20.9111
	step [119/196], loss=20.0759
	step [120/196], loss=19.1548
	step [121/196], loss=20.7412
	step [122/196], loss=19.2355
	step [123/196], loss=17.4320
	step [124/196], loss=19.7888
	step [125/196], loss=19.3406
	step [126/196], loss=19.9483
	step [127/196], loss=19.7275
	step [128/196], loss=19.9871
	step [129/196], loss=22.1853
	step [130/196], loss=18.9259
	step [131/196], loss=18.8695
	step [132/196], loss=22.1614
	step [133/196], loss=19.0142
	step [134/196], loss=20.1683
	step [135/196], loss=19.1533
	step [136/196], loss=18.8548
	step [137/196], loss=17.8891
	step [138/196], loss=17.0909
	step [139/196], loss=18.3069
	step [140/196], loss=21.3005
	step [141/196], loss=19.8826
	step [142/196], loss=18.5880
	step [143/196], loss=19.3201
	step [144/196], loss=21.1624
	step [145/196], loss=18.5807
	step [146/196], loss=19.1198
	step [147/196], loss=21.4305
	step [148/196], loss=21.4440
	step [149/196], loss=19.5335
	step [150/196], loss=20.4788
	step [151/196], loss=19.6596
	step [152/196], loss=19.2093
	step [153/196], loss=17.8816
	step [154/196], loss=18.0621
	step [155/196], loss=19.6669
	step [156/196], loss=20.1844
	step [157/196], loss=18.8664
	step [158/196], loss=20.5553
	step [159/196], loss=19.4880
	step [160/196], loss=19.2516
	step [161/196], loss=19.2064
	step [162/196], loss=21.1288
	step [163/196], loss=18.3715
	step [164/196], loss=17.6870
	step [165/196], loss=19.7053
	step [166/196], loss=24.3321
	step [167/196], loss=20.1000
	step [168/196], loss=18.5134
	step [169/196], loss=20.9983
	step [170/196], loss=20.1805
	step [171/196], loss=20.5436
	step [172/196], loss=20.0060
	step [173/196], loss=20.5129
	step [174/196], loss=17.8979
	step [175/196], loss=20.6278
	step [176/196], loss=17.9611
	step [177/196], loss=19.3994
	step [178/196], loss=19.1307
	step [179/196], loss=18.8676
	step [180/196], loss=20.9505
	step [181/196], loss=18.3807
	step [182/196], loss=19.5905
	step [183/196], loss=18.9165
	step [184/196], loss=19.6168
	step [185/196], loss=19.4319
	step [186/196], loss=19.9234
	step [187/196], loss=18.6369
	step [188/196], loss=18.9001
	step [189/196], loss=21.0745
	step [190/196], loss=19.7676
	step [191/196], loss=19.4203
	step [192/196], loss=21.0895
	step [193/196], loss=19.6007
	step [194/196], loss=20.4875
	step [195/196], loss=19.6115
	step [196/196], loss=7.1839
	Evaluating
	loss=0.0777, precision=0.1237, recall=0.9913, f1=0.5827
Training epoch 9
	step [1/196], loss=19.5073
	step [2/196], loss=19.5915
	step [3/196], loss=18.9263
	step [4/196], loss=20.8816
	step [5/196], loss=18.7334
	step [6/196], loss=18.0873
	step [7/196], loss=17.1102
	step [8/196], loss=18.0393
	step [9/196], loss=23.3368
	step [10/196], loss=18.4083
	step [11/196], loss=20.9880
	step [12/196], loss=17.3882
	step [13/196], loss=19.1464
	step [14/196], loss=18.5491
	step [15/196], loss=18.4942
	step [16/196], loss=18.7993
	step [17/196], loss=17.6429
	step [18/196], loss=20.9506
	step [19/196], loss=18.9585
	step [20/196], loss=20.5142
	step [21/196], loss=20.2111
	step [22/196], loss=19.7713
	step [23/196], loss=20.9547
	step [24/196], loss=18.2717
	step [25/196], loss=21.8217
	step [26/196], loss=19.1797
	step [27/196], loss=18.9624
	step [28/196], loss=20.0244
	step [29/196], loss=18.6850
	step [30/196], loss=21.2269
	step [31/196], loss=20.7665
	step [32/196], loss=20.6118
	step [33/196], loss=19.5251
	step [34/196], loss=18.6690
	step [35/196], loss=17.8046
	step [36/196], loss=18.1131
	step [37/196], loss=20.3285
	step [38/196], loss=19.1677
	step [39/196], loss=17.9924
	step [40/196], loss=18.2215
	step [41/196], loss=19.6049
	step [42/196], loss=18.3484
	step [43/196], loss=18.6327
	step [44/196], loss=19.8982
	step [45/196], loss=17.7972
	step [46/196], loss=18.6095
	step [47/196], loss=19.3589
	step [48/196], loss=19.5648
	step [49/196], loss=18.8351
	step [50/196], loss=19.1850
	step [51/196], loss=18.1377
	step [52/196], loss=20.9021
	step [53/196], loss=20.2917
	step [54/196], loss=17.0677
	step [55/196], loss=16.6865
	step [56/196], loss=20.2058
	step [57/196], loss=16.4371
	step [58/196], loss=17.0803
	step [59/196], loss=18.4229
	step [60/196], loss=19.7309
	step [61/196], loss=20.9936
	step [62/196], loss=19.5132
	step [63/196], loss=19.4270
	step [64/196], loss=16.5051
	step [65/196], loss=18.6260
	step [66/196], loss=18.0650
	step [67/196], loss=16.8070
	step [68/196], loss=18.7667
	step [69/196], loss=20.2862
	step [70/196], loss=20.8412
	step [71/196], loss=19.1514
	step [72/196], loss=17.2311
	step [73/196], loss=18.6130
	step [74/196], loss=20.1550
	step [75/196], loss=18.4767
	step [76/196], loss=16.8868
	step [77/196], loss=18.2321
	step [78/196], loss=16.3285
	step [79/196], loss=19.6415
	step [80/196], loss=21.5690
	step [81/196], loss=22.8072
	step [82/196], loss=20.4437
	step [83/196], loss=18.9205
	step [84/196], loss=18.2234
	step [85/196], loss=19.5060
	step [86/196], loss=19.3952
	step [87/196], loss=18.0463
	step [88/196], loss=20.0196
	step [89/196], loss=17.8052
	step [90/196], loss=20.0182
	step [91/196], loss=18.8626
	step [92/196], loss=19.9580
	step [93/196], loss=18.2387
	step [94/196], loss=20.1043
	step [95/196], loss=17.1585
	step [96/196], loss=18.6580
	step [97/196], loss=19.2919
	step [98/196], loss=17.8369
	step [99/196], loss=18.9496
	step [100/196], loss=18.2963
	step [101/196], loss=17.1974
	step [102/196], loss=17.7823
	step [103/196], loss=18.4950
	step [104/196], loss=18.7260
	step [105/196], loss=17.5303
	step [106/196], loss=20.3512
	step [107/196], loss=20.1569
	step [108/196], loss=18.9464
	step [109/196], loss=17.6786
	step [110/196], loss=18.6104
	step [111/196], loss=16.8184
	step [112/196], loss=19.5646
	step [113/196], loss=17.3188
	step [114/196], loss=16.5396
	step [115/196], loss=19.1701
	step [116/196], loss=18.0055
	step [117/196], loss=19.7478
	step [118/196], loss=18.5781
	step [119/196], loss=17.0100
	step [120/196], loss=19.6456
	step [121/196], loss=17.8687
	step [122/196], loss=18.1479
	step [123/196], loss=16.2251
	step [124/196], loss=18.9703
	step [125/196], loss=18.6541
	step [126/196], loss=16.3954
	step [127/196], loss=17.5374
	step [128/196], loss=16.9779
	step [129/196], loss=18.9340
	step [130/196], loss=17.1484
	step [131/196], loss=20.7665
	step [132/196], loss=18.6923
	step [133/196], loss=18.6304
	step [134/196], loss=19.1546
	step [135/196], loss=20.6235
	step [136/196], loss=17.6551
	step [137/196], loss=18.3031
	step [138/196], loss=17.2838
	step [139/196], loss=16.2901
	step [140/196], loss=18.6277
	step [141/196], loss=17.3060
	step [142/196], loss=16.4005
	step [143/196], loss=17.9411
	step [144/196], loss=18.4837
	step [145/196], loss=18.6257
	step [146/196], loss=17.6461
	step [147/196], loss=18.7731
	step [148/196], loss=18.0981
	step [149/196], loss=17.5059
	step [150/196], loss=18.4977
	step [151/196], loss=19.4217
	step [152/196], loss=16.1412
	step [153/196], loss=17.8522
	step [154/196], loss=17.2796
	step [155/196], loss=17.2243
	step [156/196], loss=17.3760
	step [157/196], loss=17.4438
	step [158/196], loss=16.9492
	step [159/196], loss=16.4564
	step [160/196], loss=16.8140
	step [161/196], loss=20.5273
	step [162/196], loss=16.9935
	step [163/196], loss=15.3611
	step [164/196], loss=17.3110
	step [165/196], loss=17.5738
	step [166/196], loss=14.7819
	step [167/196], loss=17.5287
	step [168/196], loss=17.0156
	step [169/196], loss=18.1389
	step [170/196], loss=17.8467
	step [171/196], loss=17.4214
	step [172/196], loss=16.6149
	step [173/196], loss=19.5547
	step [174/196], loss=18.1357
	step [175/196], loss=14.9392
	step [176/196], loss=16.8435
	step [177/196], loss=18.5171
	step [178/196], loss=17.6654
	step [179/196], loss=19.1632
	step [180/196], loss=20.6438
	step [181/196], loss=18.8760
	step [182/196], loss=17.2657
	step [183/196], loss=16.6138
	step [184/196], loss=18.8490
	step [185/196], loss=17.2093
	step [186/196], loss=18.4705
	step [187/196], loss=17.7487
	step [188/196], loss=18.5826
	step [189/196], loss=19.2106
	step [190/196], loss=18.6787
	step [191/196], loss=18.5696
	step [192/196], loss=16.1893
	step [193/196], loss=17.3692
	step [194/196], loss=18.7581
	step [195/196], loss=17.1780
	step [196/196], loss=4.4362
	Evaluating
	loss=0.0678, precision=0.1237, recall=0.9924, f1=0.5831
Training epoch 10
	step [1/196], loss=15.4083
	step [2/196], loss=15.9418
	step [3/196], loss=17.1554
	step [4/196], loss=16.5201
	step [5/196], loss=16.2959
	step [6/196], loss=18.3462
	step [7/196], loss=19.1312
	step [8/196], loss=18.4831
	step [9/196], loss=15.4089
	step [10/196], loss=15.9360
	step [11/196], loss=15.4556
	step [12/196], loss=16.3252
	step [13/196], loss=17.6826
	step [14/196], loss=18.2039
	step [15/196], loss=16.9671
	step [16/196], loss=17.0980
	step [17/196], loss=15.0248
	step [18/196], loss=17.2087
	step [19/196], loss=20.4821
	step [20/196], loss=18.4958
	step [21/196], loss=19.3192
	step [22/196], loss=17.7481
	step [23/196], loss=18.8325
	step [24/196], loss=17.1898
	step [25/196], loss=18.1714
	step [26/196], loss=17.4516
	step [27/196], loss=18.7452
	step [28/196], loss=15.0336
	step [29/196], loss=16.9509
	step [30/196], loss=18.1081
	step [31/196], loss=15.2246
	step [32/196], loss=17.9123
	step [33/196], loss=16.9902
	step [34/196], loss=16.5136
	step [35/196], loss=19.3743
	step [36/196], loss=16.3752
	step [37/196], loss=16.5369
	step [38/196], loss=14.7884
	step [39/196], loss=17.2335
	step [40/196], loss=19.8789
	step [41/196], loss=21.6397
	step [42/196], loss=16.0735
	step [43/196], loss=15.5461
	step [44/196], loss=16.7594
	step [45/196], loss=17.9089
	step [46/196], loss=17.0333
	step [47/196], loss=17.9984
	step [48/196], loss=20.5942
	step [49/196], loss=17.8238
	step [50/196], loss=18.2599
	step [51/196], loss=17.1183
	step [52/196], loss=17.3470
	step [53/196], loss=18.3056
	step [54/196], loss=17.8692
	step [55/196], loss=17.7765
	step [56/196], loss=15.6831
	step [57/196], loss=15.8674
	step [58/196], loss=16.8863
	step [59/196], loss=17.7994
	step [60/196], loss=19.3548
	step [61/196], loss=19.7135
	step [62/196], loss=16.5233
	step [63/196], loss=17.4792
	step [64/196], loss=17.9343
	step [65/196], loss=17.1864
	step [66/196], loss=17.9088
	step [67/196], loss=16.8011
	step [68/196], loss=17.1854
	step [69/196], loss=16.2157
	step [70/196], loss=18.1751
	step [71/196], loss=17.6468
	step [72/196], loss=17.0054
	step [73/196], loss=19.3716
	step [74/196], loss=17.5258
	step [75/196], loss=16.0009
	step [76/196], loss=16.5756
	step [77/196], loss=18.1076
	step [78/196], loss=17.8364
	step [79/196], loss=19.2303
	step [80/196], loss=16.6316
	step [81/196], loss=18.8243
	step [82/196], loss=17.1592
	step [83/196], loss=16.6108
	step [84/196], loss=18.1695
	step [85/196], loss=17.4982
	step [86/196], loss=16.7005
	step [87/196], loss=14.4019
	step [88/196], loss=16.9553
	step [89/196], loss=18.2911
	step [90/196], loss=16.2706
	step [91/196], loss=17.9982
	step [92/196], loss=18.3261
	step [93/196], loss=17.2173
	step [94/196], loss=18.6402
	step [95/196], loss=16.7348
	step [96/196], loss=17.6351
	step [97/196], loss=15.5881
	step [98/196], loss=19.0265
	step [99/196], loss=16.4374
	step [100/196], loss=14.8871
	step [101/196], loss=16.4082
	step [102/196], loss=19.2108
	step [103/196], loss=17.9062
	step [104/196], loss=19.7921
	step [105/196], loss=17.3237
	step [106/196], loss=16.1145
	step [107/196], loss=16.0593
	step [108/196], loss=17.8839
	step [109/196], loss=17.3614
	step [110/196], loss=15.7955
	step [111/196], loss=19.0625
	step [112/196], loss=16.8691
	step [113/196], loss=16.2713
	step [114/196], loss=14.8325
	step [115/196], loss=15.9214
	step [116/196], loss=16.5931
	step [117/196], loss=18.5597
	step [118/196], loss=20.3264
	step [119/196], loss=14.7586
	step [120/196], loss=15.0524
	step [121/196], loss=17.6059
	step [122/196], loss=16.5421
	step [123/196], loss=14.3380
	step [124/196], loss=20.1261
	step [125/196], loss=18.1401
	step [126/196], loss=15.4601
	step [127/196], loss=16.2015
	step [128/196], loss=15.1282
	step [129/196], loss=16.1205
	step [130/196], loss=17.1692
	step [131/196], loss=14.1230
	step [132/196], loss=17.6690
	step [133/196], loss=13.8377
	step [134/196], loss=14.7715
	step [135/196], loss=17.8590
	step [136/196], loss=16.2296
	step [137/196], loss=17.4951
	step [138/196], loss=16.6406
	step [139/196], loss=15.9877
	step [140/196], loss=14.0872
	step [141/196], loss=19.1558
	step [142/196], loss=15.7883
	step [143/196], loss=17.7837
	step [144/196], loss=17.2237
	step [145/196], loss=14.6651
	step [146/196], loss=16.0819
	step [147/196], loss=14.5197
	step [148/196], loss=17.3535
	step [149/196], loss=17.7291
	step [150/196], loss=16.8579
	step [151/196], loss=18.0549
	step [152/196], loss=16.4572
	step [153/196], loss=16.9675
	step [154/196], loss=16.9862
	step [155/196], loss=14.4071
	step [156/196], loss=14.6330
	step [157/196], loss=15.6475
	step [158/196], loss=16.2789
	step [159/196], loss=17.4226
	step [160/196], loss=15.6553
	step [161/196], loss=16.2212
	step [162/196], loss=16.0046
	step [163/196], loss=16.6641
	step [164/196], loss=15.6905
	step [165/196], loss=15.0707
	step [166/196], loss=17.9590
	step [167/196], loss=14.9210
	step [168/196], loss=16.1339
	step [169/196], loss=17.1048
	step [170/196], loss=15.7739
	step [171/196], loss=20.9750
	step [172/196], loss=16.3485
	step [173/196], loss=17.3491
	step [174/196], loss=16.9485
	step [175/196], loss=16.8309
	step [176/196], loss=16.7426
	step [177/196], loss=16.9831
	step [178/196], loss=15.0065
	step [179/196], loss=17.0726
	step [180/196], loss=19.1293
	step [181/196], loss=15.1826
	step [182/196], loss=15.5304
	step [183/196], loss=14.5668
	step [184/196], loss=16.2435
	step [185/196], loss=17.0844
	step [186/196], loss=14.1462
	step [187/196], loss=15.9367
	step [188/196], loss=16.0240
	step [189/196], loss=16.3809
	step [190/196], loss=13.6330
	step [191/196], loss=16.6090
	step [192/196], loss=15.3510
	step [193/196], loss=13.8731
	step [194/196], loss=14.4509
	step [195/196], loss=17.3770
	step [196/196], loss=5.2820
	Evaluating
	loss=0.0562, precision=0.1684, recall=0.9905, f1=0.6656
Training epoch 11
	step [1/196], loss=16.3974
	step [2/196], loss=16.0338
	step [3/196], loss=15.6211
	step [4/196], loss=13.9594
	step [5/196], loss=15.2894
	step [6/196], loss=16.7827
	step [7/196], loss=17.2921
	step [8/196], loss=16.7927
	step [9/196], loss=15.8239
	step [10/196], loss=14.5119
	step [11/196], loss=17.4933
	step [12/196], loss=14.5952
	step [13/196], loss=15.0565
	step [14/196], loss=15.9056
	step [15/196], loss=18.6622
	step [16/196], loss=15.6419
	step [17/196], loss=15.4616
	step [18/196], loss=15.9556
	step [19/196], loss=14.4649
	step [20/196], loss=16.6919
	step [21/196], loss=14.9994
	step [22/196], loss=16.7522
	step [23/196], loss=18.2673
	step [24/196], loss=15.5043
	step [25/196], loss=18.2177
	step [26/196], loss=14.3601
	step [27/196], loss=15.5152
	step [28/196], loss=14.3021
	step [29/196], loss=14.7521
	step [30/196], loss=17.1686
	step [31/196], loss=14.8836
	step [32/196], loss=19.7831
	step [33/196], loss=18.5278
	step [34/196], loss=15.4774
	step [35/196], loss=15.5939
	step [36/196], loss=16.0750
	step [37/196], loss=15.7073
	step [38/196], loss=17.1389
	step [39/196], loss=16.0307
	step [40/196], loss=16.2572
	step [41/196], loss=15.4682
	step [42/196], loss=16.6013
	step [43/196], loss=16.2294
	step [44/196], loss=14.4862
	step [45/196], loss=15.8688
	step [46/196], loss=15.5951
	step [47/196], loss=14.7757
	step [48/196], loss=17.0233
	step [49/196], loss=14.9640
	step [50/196], loss=15.8483
	step [51/196], loss=15.5350
	step [52/196], loss=14.3177
	step [53/196], loss=16.8457
	step [54/196], loss=17.2643
	step [55/196], loss=16.9216
	step [56/196], loss=15.2620
	step [57/196], loss=14.4948
	step [58/196], loss=17.8650
	step [59/196], loss=16.5860
	step [60/196], loss=16.1454
	step [61/196], loss=16.9578
	step [62/196], loss=17.0203
	step [63/196], loss=15.3918
	step [64/196], loss=15.5719
	step [65/196], loss=15.9243
	step [66/196], loss=16.8088
	step [67/196], loss=16.6423
	step [68/196], loss=15.4434
	step [69/196], loss=18.4728
	step [70/196], loss=16.3028
	step [71/196], loss=13.5206
	step [72/196], loss=15.4586
	step [73/196], loss=17.1520
	step [74/196], loss=15.0921
	step [75/196], loss=15.4260
	step [76/196], loss=13.7093
	step [77/196], loss=13.8698
	step [78/196], loss=15.6904
	step [79/196], loss=17.1340
	step [80/196], loss=14.9821
	step [81/196], loss=14.7034
	step [82/196], loss=17.5449
	step [83/196], loss=14.1335
	step [84/196], loss=14.3102
	step [85/196], loss=15.7798
	step [86/196], loss=15.8361
	step [87/196], loss=15.5141
	step [88/196], loss=16.9228
	step [89/196], loss=14.3239
	step [90/196], loss=15.3146
	step [91/196], loss=14.4109
	step [92/196], loss=12.9697
	step [93/196], loss=17.8744
	step [94/196], loss=21.5586
	step [95/196], loss=15.1918
	step [96/196], loss=16.3922
	step [97/196], loss=14.0106
	step [98/196], loss=16.3824
	step [99/196], loss=13.8219
	step [100/196], loss=17.5238
	step [101/196], loss=16.0886
	step [102/196], loss=15.2829
	step [103/196], loss=15.3535
	step [104/196], loss=17.8815
	step [105/196], loss=15.2727
	step [106/196], loss=17.2623
	step [107/196], loss=15.0610
	step [108/196], loss=17.2598
	step [109/196], loss=17.0808
	step [110/196], loss=15.5721
	step [111/196], loss=14.5479
	step [112/196], loss=17.7883
	step [113/196], loss=16.0983
	step [114/196], loss=13.8699
	step [115/196], loss=13.6453
	step [116/196], loss=14.1484
	step [117/196], loss=15.5132
	step [118/196], loss=15.4991
	step [119/196], loss=15.3623
	step [120/196], loss=17.7851
	step [121/196], loss=13.7542
	step [122/196], loss=15.0660
	step [123/196], loss=15.2159
	step [124/196], loss=14.7241
	step [125/196], loss=18.4150
	step [126/196], loss=15.5178
	step [127/196], loss=16.6828
	step [128/196], loss=16.3370
	step [129/196], loss=16.4124
	step [130/196], loss=15.9592
	step [131/196], loss=14.2017
	step [132/196], loss=15.7770
	step [133/196], loss=15.0287
	step [134/196], loss=17.3459
	step [135/196], loss=14.7499
	step [136/196], loss=14.0078
	step [137/196], loss=15.5963
	step [138/196], loss=16.1596
	step [139/196], loss=15.7215
	step [140/196], loss=14.7507
	step [141/196], loss=16.4207
	step [142/196], loss=14.8555
	step [143/196], loss=15.2458
	step [144/196], loss=15.1845
	step [145/196], loss=15.9740
	step [146/196], loss=16.4538
	step [147/196], loss=14.8320
	step [148/196], loss=16.3183
	step [149/196], loss=15.5217
	step [150/196], loss=17.3957
	step [151/196], loss=13.7709
	step [152/196], loss=16.4925
	step [153/196], loss=16.1788
	step [154/196], loss=13.8449
	step [155/196], loss=13.9704
	step [156/196], loss=14.6198
	step [157/196], loss=15.5705
	step [158/196], loss=15.7125
	step [159/196], loss=15.7514
	step [160/196], loss=13.8258
	step [161/196], loss=17.0455
	step [162/196], loss=18.3980
	step [163/196], loss=18.1219
	step [164/196], loss=14.1602
	step [165/196], loss=14.0970
	step [166/196], loss=15.2717
	step [167/196], loss=18.5058
	step [168/196], loss=18.4333
	step [169/196], loss=14.9978
	step [170/196], loss=15.3368
	step [171/196], loss=14.5528
	step [172/196], loss=16.1188
	step [173/196], loss=15.9821
	step [174/196], loss=16.4711
	step [175/196], loss=16.2524
	step [176/196], loss=16.8804
	step [177/196], loss=13.8401
	step [178/196], loss=15.3207
	step [179/196], loss=14.1223
	step [180/196], loss=13.8589
	step [181/196], loss=16.1915
	step [182/196], loss=16.2407
	step [183/196], loss=14.4808
	step [184/196], loss=17.5483
	step [185/196], loss=15.1473
	step [186/196], loss=16.8232
	step [187/196], loss=15.5789
	step [188/196], loss=15.3903
	step [189/196], loss=14.9264
	step [190/196], loss=14.2946
	step [191/196], loss=13.9962
	step [192/196], loss=15.1675
	step [193/196], loss=14.6369
	step [194/196], loss=17.7518
	step [195/196], loss=14.8653
	step [196/196], loss=5.0614
	Evaluating
	loss=0.0537, precision=0.1489, recall=0.9909, f1=0.6329
Training epoch 12
	step [1/196], loss=14.8102
	step [2/196], loss=13.8614
	step [3/196], loss=15.4366
	step [4/196], loss=13.8443
	step [5/196], loss=16.1152
	step [6/196], loss=17.7686
	step [7/196], loss=16.5762
	step [8/196], loss=14.0465
	step [9/196], loss=13.6967
	step [10/196], loss=13.2852
	step [11/196], loss=13.5845
	step [12/196], loss=13.6706
	step [13/196], loss=15.0840
	step [14/196], loss=11.7678
	step [15/196], loss=14.4971
	step [16/196], loss=14.2422
	step [17/196], loss=13.0391
	step [18/196], loss=16.4933
	step [19/196], loss=14.2216
	step [20/196], loss=13.5290
	step [21/196], loss=16.2561
	step [22/196], loss=14.1625
	step [23/196], loss=14.5243
	step [24/196], loss=16.2632
	step [25/196], loss=15.0633
	step [26/196], loss=14.5156
	step [27/196], loss=16.5975
	step [28/196], loss=13.9425
	step [29/196], loss=15.0844
	step [30/196], loss=16.4431
	step [31/196], loss=15.6280
	step [32/196], loss=14.6204
	step [33/196], loss=15.6163
	step [34/196], loss=13.1022
	step [35/196], loss=13.4709
	step [36/196], loss=13.9573
	step [37/196], loss=14.2277
	step [38/196], loss=13.3694
	step [39/196], loss=17.3897
	step [40/196], loss=13.9965
	step [41/196], loss=15.5722
	step [42/196], loss=13.0150
	step [43/196], loss=12.9358
	step [44/196], loss=14.5811
	step [45/196], loss=14.3690
	step [46/196], loss=12.9736
	step [47/196], loss=16.9819
	step [48/196], loss=16.6425
	step [49/196], loss=15.1080
	step [50/196], loss=17.4069
	step [51/196], loss=13.2873
	step [52/196], loss=15.7807
	step [53/196], loss=15.7583
	step [54/196], loss=13.4545
	step [55/196], loss=15.4106
	step [56/196], loss=14.8483
	step [57/196], loss=14.6969
	step [58/196], loss=14.8862
	step [59/196], loss=14.1116
	step [60/196], loss=15.2227
	step [61/196], loss=13.2766
	step [62/196], loss=14.8021
	step [63/196], loss=13.7149
	step [64/196], loss=15.8759
	step [65/196], loss=14.1773
	step [66/196], loss=15.3946
	step [67/196], loss=14.9634
	step [68/196], loss=14.3725
	step [69/196], loss=14.4730
	step [70/196], loss=14.2192
	step [71/196], loss=17.8659
	step [72/196], loss=14.5932
	step [73/196], loss=14.9535
	step [74/196], loss=14.4578
	step [75/196], loss=15.5887
	step [76/196], loss=15.2635
	step [77/196], loss=17.1075
	step [78/196], loss=16.6718
	step [79/196], loss=16.0051
	step [80/196], loss=14.8102
	step [81/196], loss=15.8449
	step [82/196], loss=15.7060
	step [83/196], loss=11.7456
	step [84/196], loss=16.0187
	step [85/196], loss=15.4121
	step [86/196], loss=15.5184
	step [87/196], loss=15.3260
	step [88/196], loss=13.5795
	step [89/196], loss=17.0241
	step [90/196], loss=14.8363
	step [91/196], loss=14.3518
	step [92/196], loss=15.0533
	step [93/196], loss=15.7124
	step [94/196], loss=13.0651
	step [95/196], loss=14.2063
	step [96/196], loss=15.7132
	step [97/196], loss=15.3654
	step [98/196], loss=13.1016
	step [99/196], loss=16.8781
	step [100/196], loss=17.1097
	step [101/196], loss=16.6982
	step [102/196], loss=13.5003
	step [103/196], loss=14.0469
	step [104/196], loss=13.5039
	step [105/196], loss=14.2999
	step [106/196], loss=14.0200
	step [107/196], loss=16.6687
	step [108/196], loss=14.6623
	step [109/196], loss=16.9822
	step [110/196], loss=14.9648
	step [111/196], loss=15.0875
	step [112/196], loss=14.6906
	step [113/196], loss=13.4949
	step [114/196], loss=14.4453
	step [115/196], loss=13.4901
	step [116/196], loss=15.5425
	step [117/196], loss=14.2915
	step [118/196], loss=14.6707
	step [119/196], loss=17.0778
	step [120/196], loss=18.5275
	step [121/196], loss=12.7725
	step [122/196], loss=15.3547
	step [123/196], loss=13.7282
	step [124/196], loss=13.3879
	step [125/196], loss=14.5885
	step [126/196], loss=14.4314
	step [127/196], loss=14.8995
	step [128/196], loss=14.1751
	step [129/196], loss=15.7180
	step [130/196], loss=13.2500
	step [131/196], loss=13.4670
	step [132/196], loss=14.8981
	step [133/196], loss=16.4653
	step [134/196], loss=14.3935
	step [135/196], loss=14.1835
	step [136/196], loss=14.8256
	step [137/196], loss=14.0882
	step [138/196], loss=14.9509
	step [139/196], loss=14.0839
	step [140/196], loss=14.4995
	step [141/196], loss=15.5664
	step [142/196], loss=13.7575
	step [143/196], loss=13.9949
	step [144/196], loss=15.1033
	step [145/196], loss=13.3428
	step [146/196], loss=14.2061
	step [147/196], loss=12.3100
	step [148/196], loss=18.8715
	step [149/196], loss=16.4674
	step [150/196], loss=16.6496
	step [151/196], loss=11.8872
	step [152/196], loss=14.1441
	step [153/196], loss=13.8266
	step [154/196], loss=14.9728
	step [155/196], loss=16.1683
	step [156/196], loss=15.7953
	step [157/196], loss=14.2702
	step [158/196], loss=12.6515
	step [159/196], loss=14.4882
	step [160/196], loss=14.8479
	step [161/196], loss=14.7699
	step [162/196], loss=16.4657
	step [163/196], loss=13.0557
	step [164/196], loss=12.7889
	step [165/196], loss=14.1561
	step [166/196], loss=12.4100
	step [167/196], loss=15.4095
	step [168/196], loss=14.5680
	step [169/196], loss=14.9908
	step [170/196], loss=13.6197
	step [171/196], loss=13.6686
	step [172/196], loss=14.4693
	step [173/196], loss=13.9739
	step [174/196], loss=13.9363
	step [175/196], loss=14.1252
	step [176/196], loss=16.6108
	step [177/196], loss=14.1092
	step [178/196], loss=15.2786
	step [179/196], loss=19.3963
	step [180/196], loss=15.5566
	step [181/196], loss=15.4667
	step [182/196], loss=15.4270
	step [183/196], loss=12.5560
	step [184/196], loss=15.7466
	step [185/196], loss=14.2877
	step [186/196], loss=17.9897
	step [187/196], loss=13.5155
	step [188/196], loss=15.9504
	step [189/196], loss=13.8047
	step [190/196], loss=16.2825
	step [191/196], loss=14.8174
	step [192/196], loss=14.8470
	step [193/196], loss=14.4173
	step [194/196], loss=14.6549
	step [195/196], loss=14.2948
	step [196/196], loss=3.3543
	Evaluating
	loss=0.0462, precision=0.1603, recall=0.9903, f1=0.6525
Training epoch 13
	step [1/196], loss=14.6332
	step [2/196], loss=16.2958
	step [3/196], loss=14.9115
	step [4/196], loss=14.2572
	step [5/196], loss=14.5429
	step [6/196], loss=14.7686
	step [7/196], loss=17.6066
	step [8/196], loss=13.0582
	step [9/196], loss=13.9097
	step [10/196], loss=13.9848
	step [11/196], loss=13.5945
	step [12/196], loss=16.7541
	step [13/196], loss=12.8839
	step [14/196], loss=12.9243
	step [15/196], loss=12.5908
	step [16/196], loss=13.0572
	step [17/196], loss=13.0664
	step [18/196], loss=14.4276
	step [19/196], loss=15.5094
	step [20/196], loss=14.8729
	step [21/196], loss=14.4263
	step [22/196], loss=15.6775
	step [23/196], loss=13.5690
	step [24/196], loss=13.2287
	step [25/196], loss=13.6955
	step [26/196], loss=14.2562
	step [27/196], loss=15.0462
	step [28/196], loss=17.1804
	step [29/196], loss=16.1802
	step [30/196], loss=15.6336
	step [31/196], loss=14.7854
	step [32/196], loss=12.7429
	step [33/196], loss=13.5237
	step [34/196], loss=13.9884
	step [35/196], loss=14.7992
	step [36/196], loss=14.1187
	step [37/196], loss=12.6019
	step [38/196], loss=14.4917
	step [39/196], loss=13.4023
	step [40/196], loss=13.7470
	step [41/196], loss=12.8356
	step [42/196], loss=14.4060
	step [43/196], loss=11.0189
	step [44/196], loss=14.3942
	step [45/196], loss=13.4074
	step [46/196], loss=15.2028
	step [47/196], loss=13.7425
	step [48/196], loss=15.1118
	step [49/196], loss=14.2581
	step [50/196], loss=13.7669
	step [51/196], loss=12.1983
	step [52/196], loss=15.5798
	step [53/196], loss=13.5583
	step [54/196], loss=14.7951
	step [55/196], loss=18.8386
	step [56/196], loss=14.2014
	step [57/196], loss=14.1787
	step [58/196], loss=14.3349
	step [59/196], loss=17.0723
	step [60/196], loss=15.5715
	step [61/196], loss=14.0984
	step [62/196], loss=14.7113
	step [63/196], loss=16.3742
	step [64/196], loss=13.9920
	step [65/196], loss=14.1528
	step [66/196], loss=12.8966
	step [67/196], loss=13.4086
	step [68/196], loss=12.8559
	step [69/196], loss=13.7786
	step [70/196], loss=13.9541
	step [71/196], loss=12.6164
	step [72/196], loss=14.3440
	step [73/196], loss=14.5861
	step [74/196], loss=13.3473
	step [75/196], loss=14.3319
	step [76/196], loss=15.1991
	step [77/196], loss=15.8779
	step [78/196], loss=14.5009
	step [79/196], loss=14.2621
	step [80/196], loss=13.9525
	step [81/196], loss=13.7347
	step [82/196], loss=12.7431
	step [83/196], loss=17.8931
	step [84/196], loss=13.6016
	step [85/196], loss=13.6891
	step [86/196], loss=16.3681
	step [87/196], loss=13.5874
	step [88/196], loss=16.9614
	step [89/196], loss=13.9874
	step [90/196], loss=15.9467
	step [91/196], loss=15.4481
	step [92/196], loss=13.8272
	step [93/196], loss=13.7509
	step [94/196], loss=14.5044
	step [95/196], loss=12.8189
	step [96/196], loss=15.9753
	step [97/196], loss=14.8370
	step [98/196], loss=13.2298
	step [99/196], loss=12.5704
	step [100/196], loss=14.4977
	step [101/196], loss=14.7572
	step [102/196], loss=13.1681
	step [103/196], loss=13.1507
	step [104/196], loss=13.9881
	step [105/196], loss=14.1249
	step [106/196], loss=11.8705
	step [107/196], loss=12.5836
	step [108/196], loss=15.1394
	step [109/196], loss=13.0604
	step [110/196], loss=13.8357
	step [111/196], loss=15.5563
	step [112/196], loss=12.4777
	step [113/196], loss=12.3085
	step [114/196], loss=14.0340
	step [115/196], loss=12.7101
	step [116/196], loss=13.0491
	step [117/196], loss=13.1546
	step [118/196], loss=14.8559
	step [119/196], loss=12.9950
	step [120/196], loss=13.7139
	step [121/196], loss=15.4189
	step [122/196], loss=12.7241
	step [123/196], loss=13.7516
	step [124/196], loss=10.9183
	step [125/196], loss=15.0756
	step [126/196], loss=14.7046
	step [127/196], loss=15.5453
	step [128/196], loss=13.9323
	step [129/196], loss=12.0853
	step [130/196], loss=13.2524
	step [131/196], loss=14.9059
	step [132/196], loss=13.8414
	step [133/196], loss=14.8120
	step [134/196], loss=13.5579
	step [135/196], loss=13.2003
	step [136/196], loss=13.8282
	step [137/196], loss=14.9545
	step [138/196], loss=14.5149
	step [139/196], loss=12.7952
	step [140/196], loss=13.9446
	step [141/196], loss=15.9308
	step [142/196], loss=12.1232
	step [143/196], loss=11.9143
	step [144/196], loss=13.5309
	step [145/196], loss=12.8925
	step [146/196], loss=12.5097
	step [147/196], loss=12.7279
	step [148/196], loss=13.2749
	step [149/196], loss=14.0237
	step [150/196], loss=13.5423
	step [151/196], loss=13.1172
	step [152/196], loss=13.0941
	step [153/196], loss=12.9728
	step [154/196], loss=12.9167
	step [155/196], loss=14.2206
	step [156/196], loss=13.3868
	step [157/196], loss=11.5789
	step [158/196], loss=13.6903
	step [159/196], loss=13.2659
	step [160/196], loss=13.9952
	step [161/196], loss=13.0380
	step [162/196], loss=12.5527
	step [163/196], loss=12.2625
	step [164/196], loss=10.6624
	step [165/196], loss=12.0172
	step [166/196], loss=11.3372
	step [167/196], loss=12.1199
	step [168/196], loss=13.2475
	step [169/196], loss=13.3590
	step [170/196], loss=11.4658
	step [171/196], loss=10.6995
	step [172/196], loss=11.8108
	step [173/196], loss=14.2146
	step [174/196], loss=12.5322
	step [175/196], loss=12.0461
	step [176/196], loss=10.8427
	step [177/196], loss=13.7003
	step [178/196], loss=14.4513
	step [179/196], loss=14.0071
	step [180/196], loss=16.0928
	step [181/196], loss=14.8636
	step [182/196], loss=14.5643
	step [183/196], loss=12.7763
	step [184/196], loss=14.2845
	step [185/196], loss=12.5911
	step [186/196], loss=14.9011
	step [187/196], loss=13.3112
	step [188/196], loss=13.5336
	step [189/196], loss=14.0573
	step [190/196], loss=12.4759
	step [191/196], loss=14.0184
	step [192/196], loss=14.5050
	step [193/196], loss=12.7409
	step [194/196], loss=12.2743
	step [195/196], loss=12.9111
	step [196/196], loss=3.7855
	Evaluating
	loss=0.0460, precision=0.1602, recall=0.9911, f1=0.6526
Training epoch 14
	step [1/196], loss=12.4765
	step [2/196], loss=11.7790
	step [3/196], loss=12.2331
	step [4/196], loss=14.9260
	step [5/196], loss=13.5745
	step [6/196], loss=11.4500
	step [7/196], loss=14.6081
	step [8/196], loss=15.0639
	step [9/196], loss=12.9631
	step [10/196], loss=15.9125
	step [11/196], loss=14.6092
	step [12/196], loss=14.7388
	step [13/196], loss=12.4788
	step [14/196], loss=14.7464
	step [15/196], loss=12.4689
	step [16/196], loss=11.8514
	step [17/196], loss=13.6815
	step [18/196], loss=12.4111
	step [19/196], loss=13.3194
	step [20/196], loss=12.7162
	step [21/196], loss=11.6066
	step [22/196], loss=14.2966
	step [23/196], loss=12.9715
	step [24/196], loss=11.6349
	step [25/196], loss=13.3647
	step [26/196], loss=14.0915
	step [27/196], loss=16.3393
	step [28/196], loss=14.4101
	step [29/196], loss=14.6159
	step [30/196], loss=11.5883
	step [31/196], loss=14.9698
	step [32/196], loss=12.9936
	step [33/196], loss=11.7886
	step [34/196], loss=11.0061
	step [35/196], loss=13.5122
	step [36/196], loss=15.8216
	step [37/196], loss=12.0887
	step [38/196], loss=13.3199
	step [39/196], loss=11.7518
	step [40/196], loss=11.8328
	step [41/196], loss=13.8042
	step [42/196], loss=13.0395
	step [43/196], loss=13.2715
	step [44/196], loss=11.6196
	step [45/196], loss=11.6947
	step [46/196], loss=12.8682
	step [47/196], loss=13.1865
	step [48/196], loss=14.5949
	step [49/196], loss=12.6511
	step [50/196], loss=11.4223
	step [51/196], loss=14.2086
	step [52/196], loss=15.3659
	step [53/196], loss=12.3922
	step [54/196], loss=14.0349
	step [55/196], loss=13.7166
	step [56/196], loss=13.0866
	step [57/196], loss=11.4950
	step [58/196], loss=14.5015
	step [59/196], loss=13.9518
	step [60/196], loss=13.6414
	step [61/196], loss=12.6334
	step [62/196], loss=15.3701
	step [63/196], loss=13.2103
	step [64/196], loss=11.6954
	step [65/196], loss=13.5328
	step [66/196], loss=12.4492
	step [67/196], loss=12.5831
	step [68/196], loss=11.4583
	step [69/196], loss=12.8961
	step [70/196], loss=13.2646
	step [71/196], loss=13.4782
	step [72/196], loss=13.8843
	step [73/196], loss=13.7019
	step [74/196], loss=12.8099
	step [75/196], loss=11.9897
	step [76/196], loss=11.8988
	step [77/196], loss=15.5701
	step [78/196], loss=11.0937
	step [79/196], loss=13.6065
	step [80/196], loss=15.3251
	step [81/196], loss=12.8859
	step [82/196], loss=13.6986
	step [83/196], loss=13.5751
	step [84/196], loss=11.8696
	step [85/196], loss=12.9147
	step [86/196], loss=13.1321
	step [87/196], loss=10.1088
	step [88/196], loss=14.5826
	step [89/196], loss=10.9571
	step [90/196], loss=11.6822
	step [91/196], loss=13.0516
	step [92/196], loss=11.2704
	step [93/196], loss=14.7829
	step [94/196], loss=11.9671
	step [95/196], loss=12.9950
	step [96/196], loss=13.3181
	step [97/196], loss=13.0119
	step [98/196], loss=14.7366
	step [99/196], loss=13.8433
	step [100/196], loss=13.8080
	step [101/196], loss=12.8623
	step [102/196], loss=10.7438
	step [103/196], loss=14.8080
	step [104/196], loss=14.3419
	step [105/196], loss=13.6881
	step [106/196], loss=12.3622
	step [107/196], loss=13.8411
	step [108/196], loss=14.3180
	step [109/196], loss=12.8483
	step [110/196], loss=11.9952
	step [111/196], loss=13.3074
	step [112/196], loss=11.6243
	step [113/196], loss=12.4228
	step [114/196], loss=12.4122
	step [115/196], loss=11.4103
	step [116/196], loss=12.9914
	step [117/196], loss=12.1757
	step [118/196], loss=14.3840
	step [119/196], loss=19.1005
	step [120/196], loss=16.0276
	step [121/196], loss=12.6891
	step [122/196], loss=12.7188
	step [123/196], loss=11.5600
	step [124/196], loss=10.6577
	step [125/196], loss=12.7552
	step [126/196], loss=12.6431
	step [127/196], loss=12.6230
	step [128/196], loss=12.8088
	step [129/196], loss=13.3406
	step [130/196], loss=13.3361
	step [131/196], loss=12.6508
	step [132/196], loss=11.2424
	step [133/196], loss=12.9349
	step [134/196], loss=14.7883
	step [135/196], loss=13.2461
	step [136/196], loss=13.3355
	step [137/196], loss=12.3437
	step [138/196], loss=13.3021
	step [139/196], loss=13.2117
	step [140/196], loss=13.5856
	step [141/196], loss=12.3302
	step [142/196], loss=12.8347
	step [143/196], loss=13.3183
	step [144/196], loss=13.0262
	step [145/196], loss=12.8811
	step [146/196], loss=14.7224
	step [147/196], loss=13.0955
	step [148/196], loss=14.9096
	step [149/196], loss=13.3451
	step [150/196], loss=12.1352
	step [151/196], loss=13.5624
	step [152/196], loss=11.2606
	step [153/196], loss=10.9902
	step [154/196], loss=12.2674
	step [155/196], loss=11.9557
	step [156/196], loss=12.1260
	step [157/196], loss=11.5957
	step [158/196], loss=12.8199
	step [159/196], loss=13.3996
	step [160/196], loss=11.7608
	step [161/196], loss=12.7958
	step [162/196], loss=12.8907
	step [163/196], loss=14.3612
	step [164/196], loss=14.3186
	step [165/196], loss=12.6156
	step [166/196], loss=15.1245
	step [167/196], loss=13.7450
	step [168/196], loss=13.7822
	step [169/196], loss=11.2635
	step [170/196], loss=13.7293
	step [171/196], loss=13.8186
	step [172/196], loss=12.3059
	step [173/196], loss=12.1755
	step [174/196], loss=14.4206
	step [175/196], loss=13.1645
	step [176/196], loss=14.9003
	step [177/196], loss=11.1011
	step [178/196], loss=12.8542
	step [179/196], loss=13.0624
	step [180/196], loss=13.0525
	step [181/196], loss=11.8244
	step [182/196], loss=14.4661
	step [183/196], loss=12.7841
	step [184/196], loss=12.6392
	step [185/196], loss=13.8107
	step [186/196], loss=13.5796
	step [187/196], loss=11.7442
	step [188/196], loss=13.6548
	step [189/196], loss=12.9422
	step [190/196], loss=16.6619
	step [191/196], loss=12.7514
	step [192/196], loss=11.7393
	step [193/196], loss=12.6490
	step [194/196], loss=12.9603
	step [195/196], loss=12.3713
	step [196/196], loss=3.7786
	Evaluating
	loss=0.0390, precision=0.1785, recall=0.9898, f1=0.6805
Training epoch 15
	step [1/196], loss=12.2257
	step [2/196], loss=11.9923
	step [3/196], loss=12.1752
	step [4/196], loss=11.4177
	step [5/196], loss=11.7124
	step [6/196], loss=12.7403
	step [7/196], loss=12.4670
	step [8/196], loss=13.3014
	step [9/196], loss=13.8564
	step [10/196], loss=13.6017
	step [11/196], loss=12.4437
	step [12/196], loss=13.4994
	step [13/196], loss=11.7544
	step [14/196], loss=14.6972
	step [15/196], loss=11.5809
	step [16/196], loss=13.6129
	step [17/196], loss=13.5083
	step [18/196], loss=13.7556
	step [19/196], loss=14.1939
	step [20/196], loss=12.8530
	step [21/196], loss=15.2084
	step [22/196], loss=12.8903
	step [23/196], loss=13.3617
	step [24/196], loss=13.0729
	step [25/196], loss=13.3591
	step [26/196], loss=13.3832
	step [27/196], loss=13.0989
	step [28/196], loss=12.5159
	step [29/196], loss=13.1361
	step [30/196], loss=11.4666
	step [31/196], loss=11.4525
	step [32/196], loss=12.7847
	step [33/196], loss=12.7605
	step [34/196], loss=13.3947
	step [35/196], loss=10.9893
	step [36/196], loss=14.8969
	step [37/196], loss=12.6665
	step [38/196], loss=12.5103
	step [39/196], loss=11.6940
	step [40/196], loss=14.2178
	step [41/196], loss=10.8805
	step [42/196], loss=10.1282
	step [43/196], loss=10.7676
	step [44/196], loss=13.9306
	step [45/196], loss=12.2067
	step [46/196], loss=10.4542
	step [47/196], loss=11.1443
	step [48/196], loss=13.7681
	step [49/196], loss=15.1916
	step [50/196], loss=12.7531
	step [51/196], loss=12.9757
	step [52/196], loss=13.3069
	step [53/196], loss=13.0226
	step [54/196], loss=10.9890
	step [55/196], loss=12.9982
	step [56/196], loss=10.5510
	step [57/196], loss=12.1645
	step [58/196], loss=14.2042
	step [59/196], loss=15.1893
	step [60/196], loss=13.2981
	step [61/196], loss=11.4331
	step [62/196], loss=12.8277
	step [63/196], loss=12.2370
	step [64/196], loss=12.4577
	step [65/196], loss=13.2427
	step [66/196], loss=11.3057
	step [67/196], loss=11.6599
	step [68/196], loss=12.8683
	step [69/196], loss=16.4623
	step [70/196], loss=12.3370
	step [71/196], loss=13.7796
	step [72/196], loss=12.1096
	step [73/196], loss=13.8999
	step [74/196], loss=13.2089
	step [75/196], loss=12.1157
	step [76/196], loss=11.6626
	step [77/196], loss=11.1174
	step [78/196], loss=11.2654
	step [79/196], loss=13.0203
	step [80/196], loss=13.5120
	step [81/196], loss=11.5106
	step [82/196], loss=11.4141
	step [83/196], loss=12.3922
	step [84/196], loss=13.6563
	step [85/196], loss=10.7717
	step [86/196], loss=13.3952
	step [87/196], loss=12.5628
	step [88/196], loss=11.1681
	step [89/196], loss=12.1161
	step [90/196], loss=11.9358
	step [91/196], loss=12.7360
	step [92/196], loss=11.1760
	step [93/196], loss=12.8090
	step [94/196], loss=12.2036
	step [95/196], loss=11.2947
	step [96/196], loss=13.0209
	step [97/196], loss=12.5050
	step [98/196], loss=13.0075
	step [99/196], loss=12.6575
	step [100/196], loss=13.7998
	step [101/196], loss=12.6620
	step [102/196], loss=11.6026
	step [103/196], loss=11.4320
	step [104/196], loss=11.3208
	step [105/196], loss=12.1309
	step [106/196], loss=12.3746
	step [107/196], loss=12.0590
	step [108/196], loss=11.9619
	step [109/196], loss=10.9546
	step [110/196], loss=12.8978
	step [111/196], loss=11.1918
	step [112/196], loss=11.8563
	step [113/196], loss=10.6704
	step [114/196], loss=12.4444
	step [115/196], loss=10.4027
	step [116/196], loss=11.3918
	step [117/196], loss=15.1549
	step [118/196], loss=10.5761
	step [119/196], loss=12.7660
	step [120/196], loss=10.8394
	step [121/196], loss=12.8664
	step [122/196], loss=11.7078
	step [123/196], loss=11.8835
	step [124/196], loss=12.9981
	step [125/196], loss=12.2466
	step [126/196], loss=13.8471
	step [127/196], loss=11.0050
	step [128/196], loss=10.7981
	step [129/196], loss=10.8408
	step [130/196], loss=12.9437
	step [131/196], loss=12.3799
	step [132/196], loss=12.3647
	step [133/196], loss=13.1235
	step [134/196], loss=10.7884
	step [135/196], loss=11.0563
	step [136/196], loss=12.5322
	step [137/196], loss=12.9776
	step [138/196], loss=14.5650
	step [139/196], loss=11.3598
	step [140/196], loss=12.7044
	step [141/196], loss=10.3937
	step [142/196], loss=12.2447
	step [143/196], loss=12.1449
	step [144/196], loss=12.8324
	step [145/196], loss=12.8215
	step [146/196], loss=11.2801
	step [147/196], loss=10.3231
	step [148/196], loss=11.5319
	step [149/196], loss=11.9022
	step [150/196], loss=11.2561
	step [151/196], loss=11.2404
	step [152/196], loss=11.1393
	step [153/196], loss=10.9564
	step [154/196], loss=12.4860
	step [155/196], loss=11.8856
	step [156/196], loss=13.7171
	step [157/196], loss=12.5800
	step [158/196], loss=10.6649
	step [159/196], loss=11.2670
	step [160/196], loss=12.0763
	step [161/196], loss=12.9118
	step [162/196], loss=12.8495
	step [163/196], loss=12.1320
	step [164/196], loss=11.9033
	step [165/196], loss=11.4686
	step [166/196], loss=11.3292
	step [167/196], loss=12.4478
	step [168/196], loss=12.5237
	step [169/196], loss=12.8525
	step [170/196], loss=12.7539
	step [171/196], loss=13.7952
	step [172/196], loss=11.3230
	step [173/196], loss=13.2103
	step [174/196], loss=13.1612
	step [175/196], loss=13.5291
	step [176/196], loss=11.7119
	step [177/196], loss=11.0398
	step [178/196], loss=12.4113
	step [179/196], loss=13.4389
	step [180/196], loss=10.2550
	step [181/196], loss=13.6176
	step [182/196], loss=11.5063
	step [183/196], loss=11.3803
	step [184/196], loss=10.5783
	step [185/196], loss=11.7503
	step [186/196], loss=11.9075
	step [187/196], loss=11.0740
	step [188/196], loss=15.9465
	step [189/196], loss=13.0012
	step [190/196], loss=10.9129
	step [191/196], loss=14.0113
	step [192/196], loss=11.3727
	step [193/196], loss=13.9127
	step [194/196], loss=11.3347
	step [195/196], loss=11.4964
	step [196/196], loss=3.0127
	Evaluating
	loss=0.0405, precision=0.1813, recall=0.9898, f1=0.6845
Training epoch 16
	step [1/196], loss=9.4207
	step [2/196], loss=12.7582
	step [3/196], loss=14.0078
	step [4/196], loss=10.1262
	step [5/196], loss=12.8094
	step [6/196], loss=12.0890
	step [7/196], loss=12.5795
	step [8/196], loss=10.6414
	step [9/196], loss=12.0853
	step [10/196], loss=12.5176
	step [11/196], loss=11.1430
	step [12/196], loss=14.0246
	step [13/196], loss=10.5822
	step [14/196], loss=12.0616
	step [15/196], loss=9.5489
	step [16/196], loss=9.6298
	step [17/196], loss=11.6796
	step [18/196], loss=12.0718
	step [19/196], loss=13.0112
	step [20/196], loss=10.8624
	step [21/196], loss=11.1168
	step [22/196], loss=12.5815
	step [23/196], loss=11.3026
	step [24/196], loss=13.1300
	step [25/196], loss=9.9975
	step [26/196], loss=12.5315
	step [27/196], loss=12.6940
	step [28/196], loss=15.3777
	step [29/196], loss=12.5693
	step [30/196], loss=12.2005
	step [31/196], loss=11.2259
	step [32/196], loss=12.0206
	step [33/196], loss=11.2389
	step [34/196], loss=12.6920
	step [35/196], loss=15.5181
	step [36/196], loss=10.5918
	step [37/196], loss=11.4839
	step [38/196], loss=11.5308
	step [39/196], loss=12.3868
	step [40/196], loss=12.2415
	step [41/196], loss=12.1058
	step [42/196], loss=12.1687
	step [43/196], loss=11.5919
	step [44/196], loss=10.7030
	step [45/196], loss=11.3243
	step [46/196], loss=12.2126
	step [47/196], loss=11.0968
	step [48/196], loss=12.2928
	step [49/196], loss=10.4036
	step [50/196], loss=11.3030
	step [51/196], loss=11.7269
	step [52/196], loss=12.9249
	step [53/196], loss=11.7414
	step [54/196], loss=14.5242
	step [55/196], loss=11.3758
	step [56/196], loss=11.2089
	step [57/196], loss=12.5500
	step [58/196], loss=12.0329
	step [59/196], loss=11.6868
	step [60/196], loss=12.5259
	step [61/196], loss=10.9431
	step [62/196], loss=11.0317
	step [63/196], loss=10.1875
	step [64/196], loss=9.9593
	step [65/196], loss=12.6975
	step [66/196], loss=12.3476
	step [67/196], loss=10.5978
	step [68/196], loss=11.1080
	step [69/196], loss=12.9815
	step [70/196], loss=10.0381
	step [71/196], loss=12.4012
	step [72/196], loss=14.9357
	step [73/196], loss=13.6195
	step [74/196], loss=11.2189
	step [75/196], loss=11.2228
	step [76/196], loss=13.3630
	step [77/196], loss=12.4707
	step [78/196], loss=11.3134
	step [79/196], loss=13.8672
	step [80/196], loss=11.4243
	step [81/196], loss=10.8430
	step [82/196], loss=12.2493
	step [83/196], loss=11.6542
	step [84/196], loss=11.7473
	step [85/196], loss=11.2006
	step [86/196], loss=10.6434
	step [87/196], loss=10.9084
	step [88/196], loss=12.7114
	step [89/196], loss=10.3991
	step [90/196], loss=11.9126
	step [91/196], loss=11.5508
	step [92/196], loss=11.9489
	step [93/196], loss=11.7419
	step [94/196], loss=12.3646
	step [95/196], loss=11.7121
	step [96/196], loss=12.0336
	step [97/196], loss=11.2590
	step [98/196], loss=11.6876
	step [99/196], loss=13.5074
	step [100/196], loss=11.8308
	step [101/196], loss=9.3638
	step [102/196], loss=10.5002
	step [103/196], loss=11.0186
	step [104/196], loss=10.1709
	step [105/196], loss=10.3282
	step [106/196], loss=10.1840
	step [107/196], loss=13.2567
	step [108/196], loss=10.8814
	step [109/196], loss=13.3517
	step [110/196], loss=10.7014
	step [111/196], loss=14.1426
	step [112/196], loss=13.1732
	step [113/196], loss=10.8218
	step [114/196], loss=12.3880
	step [115/196], loss=11.1799
	step [116/196], loss=12.3242
	step [117/196], loss=10.9948
	step [118/196], loss=14.3191
	step [119/196], loss=10.9279
	step [120/196], loss=11.4311
	step [121/196], loss=12.0439
	step [122/196], loss=13.3939
	step [123/196], loss=11.8824
	step [124/196], loss=12.9262
	step [125/196], loss=11.0606
	step [126/196], loss=10.1834
	step [127/196], loss=11.4598
	step [128/196], loss=12.2085
	step [129/196], loss=10.9145
	step [130/196], loss=12.8189
	step [131/196], loss=12.3534
	step [132/196], loss=12.4976
	step [133/196], loss=11.1937
	step [134/196], loss=10.6263
	step [135/196], loss=11.6961
	step [136/196], loss=10.4250
	step [137/196], loss=10.8714
	step [138/196], loss=9.6696
	step [139/196], loss=12.4483
	step [140/196], loss=11.6906
	step [141/196], loss=13.6901
	step [142/196], loss=11.1108
	step [143/196], loss=10.8604
	step [144/196], loss=12.5090
	step [145/196], loss=10.4204
	step [146/196], loss=11.3452
	step [147/196], loss=11.1678
	step [148/196], loss=11.6446
	step [149/196], loss=10.7108
	step [150/196], loss=10.5876
	step [151/196], loss=13.4383
	step [152/196], loss=11.6537
	step [153/196], loss=13.6115
	step [154/196], loss=11.5437
	step [155/196], loss=11.8147
	step [156/196], loss=11.2043
	step [157/196], loss=12.1581
	step [158/196], loss=11.9490
	step [159/196], loss=10.9124
	step [160/196], loss=11.1826
	step [161/196], loss=11.1930
	step [162/196], loss=12.6647
	step [163/196], loss=11.6560
	step [164/196], loss=14.3495
	step [165/196], loss=10.4192
	step [166/196], loss=13.1370
	step [167/196], loss=11.8927
	step [168/196], loss=10.4288
	step [169/196], loss=10.5230
	step [170/196], loss=11.0309
	step [171/196], loss=11.8151
	step [172/196], loss=11.0126
	step [173/196], loss=13.1351
	step [174/196], loss=12.0428
	step [175/196], loss=9.5365
	step [176/196], loss=12.5732
	step [177/196], loss=12.1955
	step [178/196], loss=11.9366
	step [179/196], loss=10.7674
	step [180/196], loss=13.1183
	step [181/196], loss=10.8016
	step [182/196], loss=13.3592
	step [183/196], loss=11.9710
	step [184/196], loss=11.9357
	step [185/196], loss=11.8654
	step [186/196], loss=9.8900
	step [187/196], loss=11.4232
	step [188/196], loss=10.9803
	step [189/196], loss=12.4419
	step [190/196], loss=12.3145
	step [191/196], loss=12.1424
	step [192/196], loss=11.6961
	step [193/196], loss=9.7693
	step [194/196], loss=10.4869
	step [195/196], loss=11.0477
	step [196/196], loss=2.2895
	Evaluating
	loss=0.0377, precision=0.1727, recall=0.9901, f1=0.6720
Training epoch 17
	step [1/196], loss=11.0977
	step [2/196], loss=10.7108
	step [3/196], loss=10.5358
	step [4/196], loss=12.9859
	step [5/196], loss=9.9731
	step [6/196], loss=12.1755
	step [7/196], loss=12.4686
	step [8/196], loss=11.7778
	step [9/196], loss=10.5048
	step [10/196], loss=11.5362
	step [11/196], loss=11.6169
	step [12/196], loss=11.6141
	step [13/196], loss=10.7781
	step [14/196], loss=10.4538
	step [15/196], loss=10.1611
	step [16/196], loss=11.5421
	step [17/196], loss=10.7903
	step [18/196], loss=13.0496
	step [19/196], loss=10.9361
	step [20/196], loss=11.4323
	step [21/196], loss=12.7830
	step [22/196], loss=12.3793
	step [23/196], loss=11.0335
	step [24/196], loss=9.7094
	step [25/196], loss=10.4704
	step [26/196], loss=13.9251
	step [27/196], loss=10.2212
	step [28/196], loss=12.6795
	step [29/196], loss=10.4454
	step [30/196], loss=9.3364
	step [31/196], loss=10.5367
	step [32/196], loss=11.7509
	step [33/196], loss=10.9476
	step [34/196], loss=10.4171
	step [35/196], loss=10.7771
	step [36/196], loss=10.0370
	step [37/196], loss=12.1444
	step [38/196], loss=10.7740
	step [39/196], loss=9.5282
	step [40/196], loss=9.8647
	step [41/196], loss=11.1570
	step [42/196], loss=10.9943
	step [43/196], loss=9.8255
	step [44/196], loss=11.5709
	step [45/196], loss=9.7995
	step [46/196], loss=9.6903
	step [47/196], loss=12.2412
	step [48/196], loss=10.4472
	step [49/196], loss=9.4308
	step [50/196], loss=10.5846
	step [51/196], loss=10.3901
	step [52/196], loss=10.3107
	step [53/196], loss=10.9971
	step [54/196], loss=12.1056
	step [55/196], loss=13.2444
	step [56/196], loss=11.1591
	step [57/196], loss=12.3436
	step [58/196], loss=10.3714
	step [59/196], loss=11.9082
	step [60/196], loss=10.2181
	step [61/196], loss=9.7268
	step [62/196], loss=11.8135
	step [63/196], loss=10.9296
	step [64/196], loss=9.6473
	step [65/196], loss=11.5788
	step [66/196], loss=10.0947
	step [67/196], loss=8.6442
	step [68/196], loss=12.1748
	step [69/196], loss=13.4084
	step [70/196], loss=12.8523
	step [71/196], loss=12.4618
	step [72/196], loss=13.1216
	step [73/196], loss=11.5959
	step [74/196], loss=11.1619
	step [75/196], loss=11.6493
	step [76/196], loss=11.4168
	step [77/196], loss=11.1621
	step [78/196], loss=9.7013
	step [79/196], loss=10.1030
	step [80/196], loss=9.3212
	step [81/196], loss=11.2755
	step [82/196], loss=11.1850
	step [83/196], loss=9.7690
	step [84/196], loss=10.8472
	step [85/196], loss=10.4259
	step [86/196], loss=10.8380
	step [87/196], loss=10.3874
	step [88/196], loss=12.1428
	step [89/196], loss=13.2414
	step [90/196], loss=12.1916
	step [91/196], loss=11.6020
	step [92/196], loss=11.7510
	step [93/196], loss=10.8344
	step [94/196], loss=11.6251
	step [95/196], loss=10.4513
	step [96/196], loss=9.7170
	step [97/196], loss=9.4558
	step [98/196], loss=11.5047
	step [99/196], loss=11.5954
	step [100/196], loss=9.2600
	step [101/196], loss=12.0170
	step [102/196], loss=12.2635
	step [103/196], loss=10.0171
	step [104/196], loss=11.3157
	step [105/196], loss=10.0705
	step [106/196], loss=10.7228
	step [107/196], loss=11.8655
	step [108/196], loss=12.9531
	step [109/196], loss=10.9916
	step [110/196], loss=11.0729
	step [111/196], loss=11.0690
	step [112/196], loss=8.7698
	step [113/196], loss=11.0752
	step [114/196], loss=11.6625
	step [115/196], loss=13.0957
	step [116/196], loss=10.8474
	step [117/196], loss=10.8158
	step [118/196], loss=10.7710
	step [119/196], loss=10.7146
	step [120/196], loss=11.1209
	step [121/196], loss=10.3726
	step [122/196], loss=12.5182
	step [123/196], loss=12.4171
	step [124/196], loss=11.6884
	step [125/196], loss=13.8803
	step [126/196], loss=10.5643
	step [127/196], loss=10.7140
	step [128/196], loss=11.7736
	step [129/196], loss=11.7618
	step [130/196], loss=9.8411
	step [131/196], loss=10.7860
	step [132/196], loss=12.3814
	step [133/196], loss=11.4733
	step [134/196], loss=11.1553
	step [135/196], loss=12.1316
	step [136/196], loss=12.4471
	step [137/196], loss=11.6398
	step [138/196], loss=12.9141
	step [139/196], loss=10.6398
	step [140/196], loss=10.5264
	step [141/196], loss=10.3618
	step [142/196], loss=9.2220
	step [143/196], loss=9.9226
	step [144/196], loss=10.3839
	step [145/196], loss=10.2542
	step [146/196], loss=10.8805
	step [147/196], loss=13.0445
	step [148/196], loss=8.9027
	step [149/196], loss=12.0122
	step [150/196], loss=11.4687
	step [151/196], loss=12.7138
	step [152/196], loss=10.0695
	step [153/196], loss=11.6686
	step [154/196], loss=12.4681
	step [155/196], loss=10.2210
	step [156/196], loss=10.2914
	step [157/196], loss=11.3465
	step [158/196], loss=12.1392
	step [159/196], loss=10.2936
	step [160/196], loss=10.1774
	step [161/196], loss=12.0400
	step [162/196], loss=10.5156
	step [163/196], loss=10.6067
	step [164/196], loss=13.1768
	step [165/196], loss=10.4398
	step [166/196], loss=10.9997
	step [167/196], loss=11.6510
	step [168/196], loss=12.1564
	step [169/196], loss=12.1692
	step [170/196], loss=11.8543
	step [171/196], loss=12.0551
	step [172/196], loss=10.3451
	step [173/196], loss=10.1949
	step [174/196], loss=9.2169
	step [175/196], loss=10.2134
	step [176/196], loss=9.5275
	step [177/196], loss=11.4461
	step [178/196], loss=9.1376
	step [179/196], loss=12.2092
	step [180/196], loss=11.2751
	step [181/196], loss=9.3779
	step [182/196], loss=10.2150
	step [183/196], loss=12.2824
	step [184/196], loss=12.1274
	step [185/196], loss=11.0152
	step [186/196], loss=11.5275
	step [187/196], loss=11.3345
	step [188/196], loss=13.0582
	step [189/196], loss=12.9162
	step [190/196], loss=11.2363
	step [191/196], loss=11.0487
	step [192/196], loss=11.1301
	step [193/196], loss=11.5324
	step [194/196], loss=10.8887
	step [195/196], loss=9.8722
	step [196/196], loss=4.0197
	Evaluating
	loss=0.0359, precision=0.1753, recall=0.9901, f1=0.6759
Training epoch 18
	step [1/196], loss=12.6338
	step [2/196], loss=9.9780
	step [3/196], loss=12.7967
	step [4/196], loss=11.2471
	step [5/196], loss=10.3939
	step [6/196], loss=9.6258
	step [7/196], loss=9.1759
	step [8/196], loss=11.7507
	step [9/196], loss=10.1757
	step [10/196], loss=11.4134
	step [11/196], loss=11.6456
	step [12/196], loss=11.2652
	step [13/196], loss=11.7912
	step [14/196], loss=11.6383
	step [15/196], loss=10.8408
	step [16/196], loss=10.9344
	step [17/196], loss=11.3680
	step [18/196], loss=10.2703
	step [19/196], loss=10.2887
	step [20/196], loss=10.2517
	step [21/196], loss=11.3839
	step [22/196], loss=9.9314
	step [23/196], loss=11.0595
	step [24/196], loss=10.0327
	step [25/196], loss=9.6161
	step [26/196], loss=9.9255
	step [27/196], loss=10.3132
	step [28/196], loss=10.4596
	step [29/196], loss=11.4072
	step [30/196], loss=10.6786
	step [31/196], loss=10.6163
	step [32/196], loss=10.6488
	step [33/196], loss=11.9181
	step [34/196], loss=11.2619
	step [35/196], loss=11.0302
	step [36/196], loss=12.3060
	step [37/196], loss=10.8924
	step [38/196], loss=10.5325
	step [39/196], loss=11.0138
	step [40/196], loss=11.6188
	step [41/196], loss=10.6413
	step [42/196], loss=10.4680
	step [43/196], loss=9.7079
	step [44/196], loss=11.4682
	step [45/196], loss=9.1173
	step [46/196], loss=11.3484
	step [47/196], loss=9.5073
	step [48/196], loss=10.3891
	step [49/196], loss=11.9742
	step [50/196], loss=13.0554
	step [51/196], loss=11.0901
	step [52/196], loss=10.8583
	step [53/196], loss=10.7282
	step [54/196], loss=11.8252
	step [55/196], loss=11.2772
	step [56/196], loss=10.2548
	step [57/196], loss=10.5060
	step [58/196], loss=9.3785
	step [59/196], loss=11.1392
	step [60/196], loss=11.0598
	step [61/196], loss=9.6507
	step [62/196], loss=10.7029
	step [63/196], loss=9.8913
	step [64/196], loss=10.0491
	step [65/196], loss=12.2366
	step [66/196], loss=9.5069
	step [67/196], loss=10.7251
	step [68/196], loss=10.6153
	step [69/196], loss=9.6938
	step [70/196], loss=12.5629
	step [71/196], loss=9.6317
	step [72/196], loss=10.9083
	step [73/196], loss=10.5406
	step [74/196], loss=13.3320
	step [75/196], loss=10.5459
	step [76/196], loss=9.9241
	step [77/196], loss=9.3077
	step [78/196], loss=10.5120
	step [79/196], loss=9.0472
	step [80/196], loss=11.0520
	step [81/196], loss=9.5844
	step [82/196], loss=11.5334
	step [83/196], loss=9.6629
	step [84/196], loss=10.0369
	step [85/196], loss=11.7423
	step [86/196], loss=9.5990
	step [87/196], loss=9.1997
	step [88/196], loss=10.0424
	step [89/196], loss=10.4072
	step [90/196], loss=9.5890
	step [91/196], loss=11.0095
	step [92/196], loss=9.1809
	step [93/196], loss=12.6300
	step [94/196], loss=11.2519
	step [95/196], loss=10.1056
	step [96/196], loss=12.4539
	step [97/196], loss=10.7838
	step [98/196], loss=9.5479
	step [99/196], loss=10.2997
	step [100/196], loss=13.8088
	step [101/196], loss=11.6215
	step [102/196], loss=10.7704
	step [103/196], loss=11.5004
	step [104/196], loss=10.4537
	step [105/196], loss=11.2036
	step [106/196], loss=9.5645
	step [107/196], loss=10.0164
	step [108/196], loss=11.1333
	step [109/196], loss=9.4921
	step [110/196], loss=12.1843
	step [111/196], loss=8.8953
	step [112/196], loss=10.9528
	step [113/196], loss=11.4041
	step [114/196], loss=9.1695
	step [115/196], loss=9.3819
	step [116/196], loss=9.2337
	step [117/196], loss=10.1207
	step [118/196], loss=11.9849
	step [119/196], loss=10.5325
	step [120/196], loss=10.4134
	step [121/196], loss=12.2009
	step [122/196], loss=8.6346
	step [123/196], loss=9.1952
	step [124/196], loss=10.3008
	step [125/196], loss=9.4638
	step [126/196], loss=9.7594
	step [127/196], loss=10.2319
	step [128/196], loss=9.9013
	step [129/196], loss=10.3255
	step [130/196], loss=11.8635
	step [131/196], loss=10.4698
	step [132/196], loss=9.4229
	step [133/196], loss=10.1020
	step [134/196], loss=9.9417
	step [135/196], loss=10.2070
	step [136/196], loss=10.9714
	step [137/196], loss=8.9582
	step [138/196], loss=12.0685
	step [139/196], loss=9.0392
	step [140/196], loss=8.9557
	step [141/196], loss=12.7162
	step [142/196], loss=11.6875
	step [143/196], loss=10.1556
	step [144/196], loss=9.3254
	step [145/196], loss=10.8538
	step [146/196], loss=11.2006
	step [147/196], loss=9.7525
	step [148/196], loss=9.3174
	step [149/196], loss=11.4975
	step [150/196], loss=11.6082
	step [151/196], loss=11.4124
	step [152/196], loss=10.2129
	step [153/196], loss=12.9370
	step [154/196], loss=10.0706
	step [155/196], loss=10.8341
	step [156/196], loss=12.2960
	step [157/196], loss=9.5284
	step [158/196], loss=10.2357
	step [159/196], loss=9.5158
	step [160/196], loss=9.9065
	step [161/196], loss=10.3132
	step [162/196], loss=10.7848
	step [163/196], loss=10.6214
	step [164/196], loss=9.0065
	step [165/196], loss=11.0443
	step [166/196], loss=8.5463
	step [167/196], loss=10.3097
	step [168/196], loss=11.1339
	step [169/196], loss=10.7822
	step [170/196], loss=9.6329
	step [171/196], loss=9.5447
	step [172/196], loss=10.1097
	step [173/196], loss=9.9684
	step [174/196], loss=9.3410
	step [175/196], loss=9.5370
	step [176/196], loss=9.3393
	step [177/196], loss=11.9911
	step [178/196], loss=10.1700
	step [179/196], loss=9.3056
	step [180/196], loss=9.6029
	step [181/196], loss=11.3293
	step [182/196], loss=10.9856
	step [183/196], loss=8.5150
	step [184/196], loss=10.1037
	step [185/196], loss=11.7651
	step [186/196], loss=9.8772
	step [187/196], loss=9.7714
	step [188/196], loss=9.7695
	step [189/196], loss=10.0322
	step [190/196], loss=11.0343
	step [191/196], loss=11.6610
	step [192/196], loss=11.6546
	step [193/196], loss=8.5095
	step [194/196], loss=8.4603
	step [195/196], loss=11.6384
	step [196/196], loss=2.3314
	Evaluating
	loss=0.0361, precision=0.1612, recall=0.9905, f1=0.6540
Training epoch 19
	step [1/196], loss=8.6269
	step [2/196], loss=10.9399
	step [3/196], loss=10.8276
	step [4/196], loss=10.2867
	step [5/196], loss=10.5588
	step [6/196], loss=11.7666
	step [7/196], loss=11.2638
	step [8/196], loss=10.4912
	step [9/196], loss=9.3211
	step [10/196], loss=9.5696
	step [11/196], loss=9.3513
	step [12/196], loss=11.8531
	step [13/196], loss=8.7893
	step [14/196], loss=8.9084
	step [15/196], loss=9.6436
	step [16/196], loss=10.7490
	step [17/196], loss=12.4658
	step [18/196], loss=9.1480
	step [19/196], loss=9.3237
	step [20/196], loss=9.4205
	step [21/196], loss=7.2167
	step [22/196], loss=11.5534
	step [23/196], loss=8.7278
	step [24/196], loss=9.5963
	step [25/196], loss=10.4552
	step [26/196], loss=8.0472
	step [27/196], loss=9.4893
	step [28/196], loss=10.3759
	step [29/196], loss=9.7728
	step [30/196], loss=10.4091
	step [31/196], loss=12.2161
	step [32/196], loss=9.0670
	step [33/196], loss=9.4164
	step [34/196], loss=9.7115
	step [35/196], loss=8.9797
	step [36/196], loss=9.7529
	step [37/196], loss=10.2164
	step [38/196], loss=8.6936
	step [39/196], loss=9.7603
	step [40/196], loss=9.5370
	step [41/196], loss=10.8036
	step [42/196], loss=12.0786
	step [43/196], loss=10.4753
	step [44/196], loss=11.1086
	step [45/196], loss=9.2973
	step [46/196], loss=11.4842
	step [47/196], loss=10.0034
	step [48/196], loss=11.2031
	step [49/196], loss=9.3312
	step [50/196], loss=9.0684
	step [51/196], loss=10.5714
	step [52/196], loss=9.9288
	step [53/196], loss=11.6059
	step [54/196], loss=9.7234
	step [55/196], loss=10.9997
	step [56/196], loss=8.7003
	step [57/196], loss=11.4808
	step [58/196], loss=9.9477
	step [59/196], loss=8.9297
	step [60/196], loss=8.6001
	step [61/196], loss=9.0918
	step [62/196], loss=10.1999
	step [63/196], loss=9.4738
	step [64/196], loss=10.6530
	step [65/196], loss=10.7851
	step [66/196], loss=10.5169
	step [67/196], loss=11.5318
	step [68/196], loss=8.8937
	step [69/196], loss=10.6562
	step [70/196], loss=9.8094
	step [71/196], loss=10.1372
	step [72/196], loss=9.7474
	step [73/196], loss=10.6011
	step [74/196], loss=9.6736
	step [75/196], loss=9.3368
	step [76/196], loss=11.2429
	step [77/196], loss=11.4972
	step [78/196], loss=9.8297
	step [79/196], loss=10.5228
	step [80/196], loss=9.6725
	step [81/196], loss=10.0052
	step [82/196], loss=11.0493
	step [83/196], loss=8.9474
	step [84/196], loss=10.0660
	step [85/196], loss=9.2010
	step [86/196], loss=8.7184
	step [87/196], loss=10.7906
	step [88/196], loss=10.1257
	step [89/196], loss=11.1789
	step [90/196], loss=9.3201
	step [91/196], loss=8.5184
	step [92/196], loss=11.9982
	step [93/196], loss=9.4001
	step [94/196], loss=9.5640
	step [95/196], loss=10.5327
	step [96/196], loss=8.5870
	step [97/196], loss=9.4279
	step [98/196], loss=10.3103
	step [99/196], loss=10.1597
	step [100/196], loss=10.4552
	step [101/196], loss=11.2220
	step [102/196], loss=9.7560
	step [103/196], loss=9.5567
	step [104/196], loss=10.2480
	step [105/196], loss=9.4944
	step [106/196], loss=10.5943
	step [107/196], loss=12.0749
	step [108/196], loss=8.6507
	step [109/196], loss=9.5576
	step [110/196], loss=12.7775
	step [111/196], loss=9.1829
	step [112/196], loss=11.2690
	step [113/196], loss=9.8430
	step [114/196], loss=9.4200
	step [115/196], loss=9.4468
	step [116/196], loss=11.0028
	step [117/196], loss=9.4397
	step [118/196], loss=12.3308
	step [119/196], loss=11.3170
	step [120/196], loss=9.3419
	step [121/196], loss=9.0163
	step [122/196], loss=11.3306
	step [123/196], loss=9.2023
	step [124/196], loss=11.4741
	step [125/196], loss=10.2722
	step [126/196], loss=10.5742
	step [127/196], loss=9.9161
	step [128/196], loss=11.3083
	step [129/196], loss=11.0183
	step [130/196], loss=9.7244
	step [131/196], loss=9.8298
	step [132/196], loss=8.1603
	step [133/196], loss=11.6294
	step [134/196], loss=9.5799
	step [135/196], loss=9.7764
	step [136/196], loss=13.1171
	step [137/196], loss=10.2930
	step [138/196], loss=9.8720
	step [139/196], loss=10.6819
	step [140/196], loss=10.0908
	step [141/196], loss=10.5267
	step [142/196], loss=10.7923
	step [143/196], loss=9.2117
	step [144/196], loss=9.8671
	step [145/196], loss=12.0498
	step [146/196], loss=10.0517
	step [147/196], loss=12.0452
	step [148/196], loss=9.7487
	step [149/196], loss=9.8898
	step [150/196], loss=9.7004
	step [151/196], loss=8.8495
	step [152/196], loss=9.9586
	step [153/196], loss=10.1186
	step [154/196], loss=9.3373
	step [155/196], loss=10.6875
	step [156/196], loss=9.8904
	step [157/196], loss=8.4770
	step [158/196], loss=10.8433
	step [159/196], loss=10.6727
	step [160/196], loss=9.6951
	step [161/196], loss=9.7715
	step [162/196], loss=10.0918
	step [163/196], loss=9.0610
	step [164/196], loss=9.9989
	step [165/196], loss=9.2490
	step [166/196], loss=8.9844
	step [167/196], loss=10.7039
	step [168/196], loss=9.5664
	step [169/196], loss=10.1948
	step [170/196], loss=9.5068
	step [171/196], loss=10.7692
	step [172/196], loss=10.6376
	step [173/196], loss=10.4112
	step [174/196], loss=7.8846
	step [175/196], loss=12.1399
	step [176/196], loss=9.3724
	step [177/196], loss=10.0748
	step [178/196], loss=11.5634
	step [179/196], loss=8.8959
	step [180/196], loss=10.3672
	step [181/196], loss=8.9964
	step [182/196], loss=7.1650
	step [183/196], loss=10.8980
	step [184/196], loss=9.4164
	step [185/196], loss=10.0216
	step [186/196], loss=6.9744
	step [187/196], loss=7.6732
	step [188/196], loss=9.3780
	step [189/196], loss=8.9655
	step [190/196], loss=9.5848
	step [191/196], loss=10.2938
	step [192/196], loss=10.4447
	step [193/196], loss=9.7050
	step [194/196], loss=10.1134
	step [195/196], loss=9.4156
	step [196/196], loss=3.5546
	Evaluating
	loss=0.0306, precision=0.1951, recall=0.9886, f1=0.7028
Training epoch 20
	step [1/196], loss=8.6590
	step [2/196], loss=9.0348
	step [3/196], loss=8.4452
	step [4/196], loss=10.5080
	step [5/196], loss=10.6118
	step [6/196], loss=7.6833
	step [7/196], loss=8.4572
	step [8/196], loss=9.8569
	step [9/196], loss=7.6539
	step [10/196], loss=9.8967
	step [11/196], loss=10.4713
	step [12/196], loss=10.6766
	step [13/196], loss=9.9271
	step [14/196], loss=10.8832
	step [15/196], loss=9.4044
	step [16/196], loss=10.1039
	step [17/196], loss=9.1504
	step [18/196], loss=8.8827
	step [19/196], loss=9.3783
	step [20/196], loss=9.7509
	step [21/196], loss=9.9089
	step [22/196], loss=9.0177
	step [23/196], loss=7.4660
	step [24/196], loss=6.7327
	step [25/196], loss=9.0709
	step [26/196], loss=8.1009
	step [27/196], loss=8.9344
	step [28/196], loss=11.9575
	step [29/196], loss=10.0096
	step [30/196], loss=12.2158
	step [31/196], loss=10.7428
	step [32/196], loss=9.8348
	step [33/196], loss=8.0477
	step [34/196], loss=8.7012
	step [35/196], loss=11.5278
	step [36/196], loss=9.0707
	step [37/196], loss=11.8825
	step [38/196], loss=10.5049
	step [39/196], loss=9.0865
	step [40/196], loss=11.7079
	step [41/196], loss=10.9255
	step [42/196], loss=9.4664
	step [43/196], loss=10.3243
	step [44/196], loss=9.9394
	step [45/196], loss=10.9501
	step [46/196], loss=9.0497
	step [47/196], loss=9.7279
	step [48/196], loss=10.0515
	step [49/196], loss=10.2749
	step [50/196], loss=10.0263
	step [51/196], loss=10.8038
	step [52/196], loss=9.6663
	step [53/196], loss=9.2635
	step [54/196], loss=9.4871
	step [55/196], loss=10.6948
	step [56/196], loss=11.3656
	step [57/196], loss=8.7366
	step [58/196], loss=11.2899
	step [59/196], loss=9.4935
	step [60/196], loss=9.3609
	step [61/196], loss=9.9339
	step [62/196], loss=8.6999
	step [63/196], loss=9.5593
	step [64/196], loss=12.3154
	step [65/196], loss=11.3124
	step [66/196], loss=9.1433
	step [67/196], loss=9.2004
	step [68/196], loss=8.4323
	step [69/196], loss=8.7401
	step [70/196], loss=8.4461
	step [71/196], loss=10.5384
	step [72/196], loss=8.8546
	step [73/196], loss=11.3263
	step [74/196], loss=9.8308
	step [75/196], loss=10.6665
	step [76/196], loss=10.7894
	step [77/196], loss=9.7967
	step [78/196], loss=10.0609
	step [79/196], loss=9.4920
	step [80/196], loss=7.5343
	step [81/196], loss=10.6601
	step [82/196], loss=9.2205
	step [83/196], loss=9.4767
	step [84/196], loss=11.6833
	step [85/196], loss=10.3149
	step [86/196], loss=13.5610
	step [87/196], loss=8.5282
	step [88/196], loss=9.6320
	step [89/196], loss=9.5675
	step [90/196], loss=10.1576
	step [91/196], loss=9.1501
	step [92/196], loss=8.6976
	step [93/196], loss=8.4822
	step [94/196], loss=8.9620
	step [95/196], loss=8.7094
	step [96/196], loss=8.3668
	step [97/196], loss=9.2717
	step [98/196], loss=8.4720
	step [99/196], loss=9.9839
	step [100/196], loss=9.8924
	step [101/196], loss=9.3838
	step [102/196], loss=9.5180
	step [103/196], loss=9.1363
	step [104/196], loss=10.4449
	step [105/196], loss=10.1489
	step [106/196], loss=10.0733
	step [107/196], loss=10.5570
	step [108/196], loss=9.4636
	step [109/196], loss=7.6585
	step [110/196], loss=12.2606
	step [111/196], loss=10.9532
	step [112/196], loss=9.5058
	step [113/196], loss=10.8622
	step [114/196], loss=9.0175
	step [115/196], loss=9.1259
	step [116/196], loss=9.6933
	step [117/196], loss=10.5314
	step [118/196], loss=7.6842
	step [119/196], loss=11.3529
	step [120/196], loss=9.6247
	step [121/196], loss=9.4457
	step [122/196], loss=8.8338
	step [123/196], loss=8.5394
	step [124/196], loss=8.8835
	step [125/196], loss=9.3264
	step [126/196], loss=9.7430
	step [127/196], loss=9.2812
	step [128/196], loss=8.5139
	step [129/196], loss=8.4720
	step [130/196], loss=8.9478
	step [131/196], loss=9.5712
	step [132/196], loss=9.3437
	step [133/196], loss=10.7209
	step [134/196], loss=8.8982
	step [135/196], loss=7.0171
	step [136/196], loss=8.3383
	step [137/196], loss=10.5447
	step [138/196], loss=9.6234
	step [139/196], loss=10.1161
	step [140/196], loss=10.3475
	step [141/196], loss=9.7012
	step [142/196], loss=9.2651
	step [143/196], loss=9.8900
	step [144/196], loss=10.1712
	step [145/196], loss=8.1396
	step [146/196], loss=9.9799
	step [147/196], loss=8.2026
	step [148/196], loss=9.2237
	step [149/196], loss=8.5682
	step [150/196], loss=9.7835
	step [151/196], loss=8.5082
	step [152/196], loss=9.3561
	step [153/196], loss=10.4124
	step [154/196], loss=10.7170
	step [155/196], loss=11.3125
	step [156/196], loss=8.9505
	step [157/196], loss=9.5934
	step [158/196], loss=8.9775
	step [159/196], loss=8.0617
	step [160/196], loss=8.6840
	step [161/196], loss=10.4956
	step [162/196], loss=9.3475
	step [163/196], loss=7.8486
	step [164/196], loss=9.8283
	step [165/196], loss=8.6482
	step [166/196], loss=7.5916
	step [167/196], loss=8.6128
	step [168/196], loss=8.7840
	step [169/196], loss=9.7785
	step [170/196], loss=10.2836
	step [171/196], loss=9.5825
	step [172/196], loss=9.9874
	step [173/196], loss=9.3848
	step [174/196], loss=10.2684
	step [175/196], loss=9.2973
	step [176/196], loss=9.1076
	step [177/196], loss=8.8729
	step [178/196], loss=8.3041
	step [179/196], loss=9.8030
	step [180/196], loss=8.9069
	step [181/196], loss=9.3384
	step [182/196], loss=11.0057
	step [183/196], loss=9.2905
	step [184/196], loss=8.5458
	step [185/196], loss=10.1960
	step [186/196], loss=11.3858
	step [187/196], loss=9.9248
	step [188/196], loss=9.0557
	step [189/196], loss=10.3729
	step [190/196], loss=11.1998
	step [191/196], loss=8.3306
	step [192/196], loss=8.9414
	step [193/196], loss=8.9480
	step [194/196], loss=8.0980
	step [195/196], loss=9.4247
	step [196/196], loss=2.1097
	Evaluating
	loss=0.0316, precision=0.1911, recall=0.9893, f1=0.6979
Training epoch 21
	step [1/196], loss=9.7778
	step [2/196], loss=9.9280
	step [3/196], loss=9.0304
	step [4/196], loss=8.1459
	step [5/196], loss=9.3711
	step [6/196], loss=9.0060
	step [7/196], loss=8.0322
	step [8/196], loss=10.7629
	step [9/196], loss=8.2036
	step [10/196], loss=9.8194
	step [11/196], loss=10.8498
	step [12/196], loss=9.4398
	step [13/196], loss=8.9395
	step [14/196], loss=9.6203
	step [15/196], loss=10.3214
	step [16/196], loss=10.3921
	step [17/196], loss=8.6769
	step [18/196], loss=9.1699
	step [19/196], loss=9.2572
	step [20/196], loss=10.1842
	step [21/196], loss=10.2517
	step [22/196], loss=9.2535
	step [23/196], loss=9.0986
	step [24/196], loss=10.3081
	step [25/196], loss=9.6357
	step [26/196], loss=9.0715
	step [27/196], loss=10.6522
	step [28/196], loss=9.6218
	step [29/196], loss=8.4136
	step [30/196], loss=9.8170
	step [31/196], loss=8.0227
	step [32/196], loss=9.1942
	step [33/196], loss=10.8206
	step [34/196], loss=9.0340
	step [35/196], loss=7.8930
	step [36/196], loss=8.5994
	step [37/196], loss=9.1149
	step [38/196], loss=8.9483
	step [39/196], loss=8.6703
	step [40/196], loss=10.9134
	step [41/196], loss=9.2699
	step [42/196], loss=9.4292
	step [43/196], loss=9.8442
	step [44/196], loss=9.6076
	step [45/196], loss=10.7954
	step [46/196], loss=7.7977
	step [47/196], loss=9.3609
	step [48/196], loss=9.4938
	step [49/196], loss=8.2409
	step [50/196], loss=7.7963
	step [51/196], loss=9.7369
	step [52/196], loss=12.6933
	step [53/196], loss=9.2848
	step [54/196], loss=7.6205
	step [55/196], loss=10.5304
	step [56/196], loss=10.1502
	step [57/196], loss=8.2126
	step [58/196], loss=10.1199
	step [59/196], loss=8.2239
	step [60/196], loss=8.7817
	step [61/196], loss=8.9865
	step [62/196], loss=7.6149
	step [63/196], loss=7.7833
	step [64/196], loss=10.2975
	step [65/196], loss=10.4480
	step [66/196], loss=10.0830
	step [67/196], loss=8.4860
	step [68/196], loss=10.5904
	step [69/196], loss=10.0427
	step [70/196], loss=8.8063
	step [71/196], loss=9.7851
	step [72/196], loss=8.5510
	step [73/196], loss=8.8427
	step [74/196], loss=8.9514
	step [75/196], loss=10.6947
	step [76/196], loss=9.5005
	step [77/196], loss=8.6165
	step [78/196], loss=10.2933
	step [79/196], loss=8.9001
	step [80/196], loss=8.5213
	step [81/196], loss=9.7214
	step [82/196], loss=11.0674
	step [83/196], loss=8.9267
	step [84/196], loss=8.0723
	step [85/196], loss=9.3424
	step [86/196], loss=8.6285
	step [87/196], loss=9.4831
	step [88/196], loss=9.7481
	step [89/196], loss=10.0343
	step [90/196], loss=8.4874
	step [91/196], loss=8.4009
	step [92/196], loss=9.9427
	step [93/196], loss=8.5227
	step [94/196], loss=8.7284
	step [95/196], loss=9.6619
	step [96/196], loss=9.4372
	step [97/196], loss=8.7799
	step [98/196], loss=7.6336
	step [99/196], loss=8.9372
	step [100/196], loss=8.9503
	step [101/196], loss=8.6902
	step [102/196], loss=9.7964
	step [103/196], loss=10.2742
	step [104/196], loss=10.5116
	step [105/196], loss=10.9459
	step [106/196], loss=9.4291
	step [107/196], loss=10.2819
	step [108/196], loss=8.3296
	step [109/196], loss=11.1612
	step [110/196], loss=9.3173
	step [111/196], loss=9.6933
	step [112/196], loss=9.6254
	step [113/196], loss=9.1318
	step [114/196], loss=9.1782
	step [115/196], loss=11.0204
	step [116/196], loss=10.6098
	step [117/196], loss=10.5663
	step [118/196], loss=10.6823
	step [119/196], loss=8.9315
	step [120/196], loss=9.0593
	step [121/196], loss=9.0138
	step [122/196], loss=9.0292
	step [123/196], loss=10.8029
	step [124/196], loss=8.9691
	step [125/196], loss=10.9581
	step [126/196], loss=13.4804
	step [127/196], loss=7.9702
	step [128/196], loss=8.5918
	step [129/196], loss=8.8535
	step [130/196], loss=9.8938
	step [131/196], loss=9.9659
	step [132/196], loss=8.7242
	step [133/196], loss=9.2860
	step [134/196], loss=12.1278
	step [135/196], loss=8.5580
	step [136/196], loss=8.6002
	step [137/196], loss=9.4826
	step [138/196], loss=9.7622
	step [139/196], loss=10.7732
	step [140/196], loss=8.5377
	step [141/196], loss=8.7415
	step [142/196], loss=11.0996
	step [143/196], loss=10.2435
	step [144/196], loss=8.7989
	step [145/196], loss=8.9799
	step [146/196], loss=7.7716
	step [147/196], loss=9.8770
	step [148/196], loss=8.3647
	step [149/196], loss=8.9501
	step [150/196], loss=8.1460
	step [151/196], loss=8.6167
	step [152/196], loss=7.9138
	step [153/196], loss=9.8527
	step [154/196], loss=8.9431
	step [155/196], loss=8.4065
	step [156/196], loss=9.3418
	step [157/196], loss=9.9331
	step [158/196], loss=7.6718
	step [159/196], loss=11.6417
	step [160/196], loss=8.4405
	step [161/196], loss=9.8322
	step [162/196], loss=8.0313
	step [163/196], loss=9.0944
	step [164/196], loss=9.1116
	step [165/196], loss=9.4764
	step [166/196], loss=9.7243
	step [167/196], loss=9.9226
	step [168/196], loss=8.4008
	step [169/196], loss=8.3273
	step [170/196], loss=8.2617
	step [171/196], loss=9.7096
	step [172/196], loss=10.0992
	step [173/196], loss=9.3496
	step [174/196], loss=11.5708
	step [175/196], loss=10.5953
	step [176/196], loss=11.6809
	step [177/196], loss=9.4026
	step [178/196], loss=8.9657
	step [179/196], loss=9.9942
	step [180/196], loss=8.0386
	step [181/196], loss=9.2838
	step [182/196], loss=9.0496
	step [183/196], loss=8.9070
	step [184/196], loss=8.5680
	step [185/196], loss=8.2460
	step [186/196], loss=8.8140
	step [187/196], loss=8.5986
	step [188/196], loss=7.8557
	step [189/196], loss=9.2892
	step [190/196], loss=9.7662
	step [191/196], loss=9.0938
	step [192/196], loss=9.2035
	step [193/196], loss=9.2596
	step [194/196], loss=9.7013
	step [195/196], loss=8.2426
	step [196/196], loss=2.3484
	Evaluating
	loss=0.0315, precision=0.1807, recall=0.9903, f1=0.6839
Training epoch 22
	step [1/196], loss=8.3416
	step [2/196], loss=8.5439
	step [3/196], loss=7.4373
	step [4/196], loss=9.0514
	step [5/196], loss=8.4965
	step [6/196], loss=9.5067
	step [7/196], loss=7.8728
	step [8/196], loss=8.9132
	step [9/196], loss=9.7236
	step [10/196], loss=8.0517
	step [11/196], loss=11.0027
	step [12/196], loss=9.4575
	step [13/196], loss=6.8346
	step [14/196], loss=9.3883
	step [15/196], loss=8.9682
	step [16/196], loss=7.9115
	step [17/196], loss=9.4104
	step [18/196], loss=9.4303
	step [19/196], loss=10.0202
	step [20/196], loss=10.8181
	step [21/196], loss=10.5903
	step [22/196], loss=9.0725
	step [23/196], loss=9.4539
	step [24/196], loss=9.3532
	step [25/196], loss=10.8629
	step [26/196], loss=7.9975
	step [27/196], loss=7.8046
	step [28/196], loss=8.7775
	step [29/196], loss=9.0051
	step [30/196], loss=8.4667
	step [31/196], loss=9.7512
	step [32/196], loss=7.8453
	step [33/196], loss=8.2797
	step [34/196], loss=7.2941
	step [35/196], loss=8.8685
	step [36/196], loss=9.6026
	step [37/196], loss=9.5052
	step [38/196], loss=9.1684
	step [39/196], loss=7.6354
	step [40/196], loss=9.0720
	step [41/196], loss=8.2391
	step [42/196], loss=9.4541
	step [43/196], loss=9.5757
	step [44/196], loss=9.3016
	step [45/196], loss=8.2404
	step [46/196], loss=8.1237
	step [47/196], loss=7.4897
	step [48/196], loss=9.0753
	step [49/196], loss=9.0083
	step [50/196], loss=9.2703
	step [51/196], loss=9.5727
	step [52/196], loss=8.8694
	step [53/196], loss=8.0516
	step [54/196], loss=8.7993
	step [55/196], loss=8.6546
	step [56/196], loss=8.9453
	step [57/196], loss=8.7378
	step [58/196], loss=7.4238
	step [59/196], loss=9.6174
	step [60/196], loss=10.5376
	step [61/196], loss=9.8633
	step [62/196], loss=7.8770
	step [63/196], loss=9.6595
	step [64/196], loss=7.4199
	step [65/196], loss=8.5598
	step [66/196], loss=9.0916
	step [67/196], loss=7.8967
	step [68/196], loss=10.2555
	step [69/196], loss=9.5087
	step [70/196], loss=9.0294
	step [71/196], loss=9.2653
	step [72/196], loss=10.0067
	step [73/196], loss=9.4336
	step [74/196], loss=9.4858
	step [75/196], loss=10.4315
	step [76/196], loss=9.3134
	step [77/196], loss=9.5007
	step [78/196], loss=9.2739
	step [79/196], loss=10.3041
	step [80/196], loss=8.5338
	step [81/196], loss=9.1573
	step [82/196], loss=9.0243
	step [83/196], loss=8.2789
	step [84/196], loss=10.3779
	step [85/196], loss=12.2548
	step [86/196], loss=8.1370
	step [87/196], loss=9.7979
	step [88/196], loss=10.3055
	step [89/196], loss=8.9682
	step [90/196], loss=8.5486
	step [91/196], loss=9.9889
	step [92/196], loss=9.9281
	step [93/196], loss=9.9730
	step [94/196], loss=7.9141
	step [95/196], loss=9.9336
	step [96/196], loss=8.9982
	step [97/196], loss=10.5054
	step [98/196], loss=8.5193
	step [99/196], loss=9.6432
	step [100/196], loss=8.2372
	step [101/196], loss=9.2899
	step [102/196], loss=9.8562
	step [103/196], loss=8.3177
	step [104/196], loss=7.1629
	step [105/196], loss=7.6058
	step [106/196], loss=9.2184
	step [107/196], loss=8.9458
	step [108/196], loss=8.4812
	step [109/196], loss=9.9249
	step [110/196], loss=8.4390
	step [111/196], loss=7.6879
	step [112/196], loss=7.5184
	step [113/196], loss=10.6657
	step [114/196], loss=7.8908
	step [115/196], loss=7.1264
	step [116/196], loss=7.8012
	step [117/196], loss=9.0519
	step [118/196], loss=9.1618
	step [119/196], loss=8.4970
	step [120/196], loss=7.8196
	step [121/196], loss=8.3798
	step [122/196], loss=9.0054
	step [123/196], loss=10.3412
	step [124/196], loss=7.3044
	step [125/196], loss=8.4626
	step [126/196], loss=8.8232
	step [127/196], loss=8.1209
	step [128/196], loss=7.4357
	step [129/196], loss=7.9339
	step [130/196], loss=9.5373
	step [131/196], loss=7.9214
	step [132/196], loss=8.6255
	step [133/196], loss=9.2042
	step [134/196], loss=8.9779
	step [135/196], loss=10.1902
	step [136/196], loss=8.3638
	step [137/196], loss=10.7947
	step [138/196], loss=7.6273
	step [139/196], loss=7.5546
	step [140/196], loss=8.4431
	step [141/196], loss=6.9823
	step [142/196], loss=10.9114
	step [143/196], loss=7.6249
	step [144/196], loss=11.3914
	step [145/196], loss=8.8020
	step [146/196], loss=9.1046
	step [147/196], loss=9.2519
	step [148/196], loss=8.7192
	step [149/196], loss=9.3278
	step [150/196], loss=8.3314
	step [151/196], loss=9.5552
	step [152/196], loss=9.5159
	step [153/196], loss=7.3883
	step [154/196], loss=9.3780
	step [155/196], loss=9.9414
	step [156/196], loss=10.3631
	step [157/196], loss=9.2554
	step [158/196], loss=7.0415
	step [159/196], loss=8.0811
	step [160/196], loss=8.2832
	step [161/196], loss=8.7021
	step [162/196], loss=7.4336
	step [163/196], loss=8.7320
	step [164/196], loss=8.1721
	step [165/196], loss=8.5733
	step [166/196], loss=9.4047
	step [167/196], loss=7.9014
	step [168/196], loss=8.9618
	step [169/196], loss=7.2338
	step [170/196], loss=9.5224
	step [171/196], loss=9.6947
	step [172/196], loss=10.0352
	step [173/196], loss=7.3867
	step [174/196], loss=8.5025
	step [175/196], loss=8.0238
	step [176/196], loss=9.3850
	step [177/196], loss=8.2938
	step [178/196], loss=9.0683
	step [179/196], loss=7.9152
	step [180/196], loss=7.2848
	step [181/196], loss=9.0248
	step [182/196], loss=9.1995
	step [183/196], loss=6.9338
	step [184/196], loss=8.4823
	step [185/196], loss=9.2800
	step [186/196], loss=10.0508
	step [187/196], loss=8.9433
	step [188/196], loss=9.2195
	step [189/196], loss=8.4152
	step [190/196], loss=10.3782
	step [191/196], loss=8.4138
	step [192/196], loss=7.8265
	step [193/196], loss=9.0544
	step [194/196], loss=9.0224
	step [195/196], loss=8.9863
	step [196/196], loss=2.4871
	Evaluating
	loss=0.0328, precision=0.1775, recall=0.9894, f1=0.6788
Training epoch 23
	step [1/196], loss=9.1051
	step [2/196], loss=8.4687
	step [3/196], loss=8.1575
	step [4/196], loss=8.5824
	step [5/196], loss=10.0489
	step [6/196], loss=8.9509
	step [7/196], loss=8.9906
	step [8/196], loss=9.1099
	step [9/196], loss=9.6240
	step [10/196], loss=8.9987
	step [11/196], loss=8.6138
	step [12/196], loss=7.8493
	step [13/196], loss=9.8139
	step [14/196], loss=8.3592
	step [15/196], loss=7.9843
	step [16/196], loss=7.9662
	step [17/196], loss=9.1522
	step [18/196], loss=9.3227
	step [19/196], loss=8.1649
	step [20/196], loss=8.7895
	step [21/196], loss=8.2714
	step [22/196], loss=7.0470
	step [23/196], loss=8.1830
	step [24/196], loss=8.5333
	step [25/196], loss=8.4829
	step [26/196], loss=8.4696
	step [27/196], loss=9.6827
	step [28/196], loss=8.2533
	step [29/196], loss=7.4420
	step [30/196], loss=9.0412
	step [31/196], loss=8.1529
	step [32/196], loss=9.6849
	step [33/196], loss=8.4111
	step [34/196], loss=8.2426
	step [35/196], loss=7.9803
	step [36/196], loss=8.4844
	step [37/196], loss=7.7105
	step [38/196], loss=9.1953
	step [39/196], loss=8.1872
	step [40/196], loss=7.6936
	step [41/196], loss=7.9153
	step [42/196], loss=9.8294
	step [43/196], loss=8.8814
	step [44/196], loss=7.9540
	step [45/196], loss=9.3863
	step [46/196], loss=8.2851
	step [47/196], loss=8.1367
	step [48/196], loss=9.3588
	step [49/196], loss=9.5425
	step [50/196], loss=8.4215
	step [51/196], loss=8.4759
	step [52/196], loss=8.6188
	step [53/196], loss=8.8477
	step [54/196], loss=8.2815
	step [55/196], loss=6.6677
	step [56/196], loss=7.3441
	step [57/196], loss=9.6073
	step [58/196], loss=8.7106
	step [59/196], loss=8.1505
	step [60/196], loss=8.1125
	step [61/196], loss=8.8273
	step [62/196], loss=8.1088
	step [63/196], loss=8.6226
	step [64/196], loss=7.9083
	step [65/196], loss=6.9804
	step [66/196], loss=8.0590
	step [67/196], loss=7.9825
	step [68/196], loss=7.8609
	step [69/196], loss=9.4598
	step [70/196], loss=7.8847
	step [71/196], loss=11.1515
	step [72/196], loss=7.1873
	step [73/196], loss=8.9407
	step [74/196], loss=7.8765
	step [75/196], loss=8.6576
	step [76/196], loss=8.9226
	step [77/196], loss=8.6313
	step [78/196], loss=7.2665
	step [79/196], loss=8.7526
	step [80/196], loss=9.0086
	step [81/196], loss=7.6250
	step [82/196], loss=10.5188
	step [83/196], loss=7.4973
	step [84/196], loss=8.3510
	step [85/196], loss=8.4497
	step [86/196], loss=10.0441
	step [87/196], loss=9.2113
	step [88/196], loss=6.7849
	step [89/196], loss=8.6420
	step [90/196], loss=10.9140
	step [91/196], loss=7.0974
	step [92/196], loss=7.4089
	step [93/196], loss=7.5267
	step [94/196], loss=9.3333
	step [95/196], loss=10.1666
	step [96/196], loss=8.8050
	step [97/196], loss=8.3844
	step [98/196], loss=8.7408
	step [99/196], loss=8.3641
	step [100/196], loss=8.3448
	step [101/196], loss=7.6386
	step [102/196], loss=11.1417
	step [103/196], loss=8.9657
	step [104/196], loss=7.2292
	step [105/196], loss=9.0700
	step [106/196], loss=8.3310
	step [107/196], loss=8.8678
	step [108/196], loss=7.7848
	step [109/196], loss=9.2299
	step [110/196], loss=6.9718
	step [111/196], loss=9.4068
	step [112/196], loss=9.9886
	step [113/196], loss=7.3776
	step [114/196], loss=8.9513
	step [115/196], loss=8.4661
	step [116/196], loss=9.9061
	step [117/196], loss=7.1915
	step [118/196], loss=7.7011
	step [119/196], loss=8.3302
	step [120/196], loss=10.2355
	step [121/196], loss=8.4158
	step [122/196], loss=10.7871
	step [123/196], loss=9.8509
	step [124/196], loss=7.5154
	step [125/196], loss=8.7381
	step [126/196], loss=7.9760
	step [127/196], loss=7.4923
	step [128/196], loss=7.5247
	step [129/196], loss=7.6898
	step [130/196], loss=9.2787
	step [131/196], loss=7.1922
	step [132/196], loss=9.0456
	step [133/196], loss=10.2385
	step [134/196], loss=8.1986
	step [135/196], loss=9.4507
	step [136/196], loss=9.6273
	step [137/196], loss=9.3080
	step [138/196], loss=9.7156
	step [139/196], loss=8.2178
	step [140/196], loss=8.2058
	step [141/196], loss=8.2855
	step [142/196], loss=7.8168
	step [143/196], loss=8.6678
	step [144/196], loss=8.3745
	step [145/196], loss=10.3613
	step [146/196], loss=9.2234
	step [147/196], loss=8.3279
	step [148/196], loss=8.6783
	step [149/196], loss=9.5209
	step [150/196], loss=10.4569
	step [151/196], loss=8.8284
	step [152/196], loss=9.4260
	step [153/196], loss=7.4195
	step [154/196], loss=7.8152
	step [155/196], loss=8.3953
	step [156/196], loss=7.4973
	step [157/196], loss=8.0966
	step [158/196], loss=9.4483
	step [159/196], loss=8.1098
	step [160/196], loss=9.5332
	step [161/196], loss=8.1239
	step [162/196], loss=9.2914
	step [163/196], loss=9.2153
	step [164/196], loss=7.3087
	step [165/196], loss=6.5918
	step [166/196], loss=8.0243
	step [167/196], loss=7.2706
	step [168/196], loss=8.5280
	step [169/196], loss=7.3001
	step [170/196], loss=11.4504
	step [171/196], loss=8.2315
	step [172/196], loss=8.5622
	step [173/196], loss=7.9117
	step [174/196], loss=7.9963
	step [175/196], loss=8.3966
	step [176/196], loss=10.2450
	step [177/196], loss=7.8880
	step [178/196], loss=8.0958
	step [179/196], loss=9.1523
	step [180/196], loss=10.4149
	step [181/196], loss=8.8513
	step [182/196], loss=7.4538
	step [183/196], loss=8.1056
	step [184/196], loss=8.8938
	step [185/196], loss=7.7578
	step [186/196], loss=9.2943
	step [187/196], loss=11.4637
	step [188/196], loss=7.8154
	step [189/196], loss=8.4507
	step [190/196], loss=8.8253
	step [191/196], loss=8.0621
	step [192/196], loss=8.5164
	step [193/196], loss=7.8553
	step [194/196], loss=9.1744
	step [195/196], loss=8.2245
	step [196/196], loss=2.7567
	Evaluating
	loss=0.0277, precision=0.2057, recall=0.9887, f1=0.7161
saving model as: 0_saved_model.pth
Training epoch 24
	step [1/196], loss=7.8527
	step [2/196], loss=9.1995
	step [3/196], loss=9.4909
	step [4/196], loss=10.3801
	step [5/196], loss=8.3555
	step [6/196], loss=7.9782
	step [7/196], loss=9.6345
	step [8/196], loss=8.7555
	step [9/196], loss=8.0754
	step [10/196], loss=8.7934
	step [11/196], loss=9.0400
	step [12/196], loss=7.5731
	step [13/196], loss=8.6248
	step [14/196], loss=8.2764
	step [15/196], loss=8.8394
	step [16/196], loss=8.3120
	step [17/196], loss=8.4435
	step [18/196], loss=8.2026
	step [19/196], loss=8.0131
	step [20/196], loss=7.6996
	step [21/196], loss=8.2401
	step [22/196], loss=9.3063
	step [23/196], loss=9.5902
	step [24/196], loss=10.1868
	step [25/196], loss=7.7163
	step [26/196], loss=7.1949
	step [27/196], loss=7.5388
	step [28/196], loss=7.7974
	step [29/196], loss=7.5181
	step [30/196], loss=7.8404
	step [31/196], loss=8.1979
	step [32/196], loss=5.8842
	step [33/196], loss=7.2671
	step [34/196], loss=6.2979
	step [35/196], loss=8.7985
	step [36/196], loss=8.6764
	step [37/196], loss=7.8671
	step [38/196], loss=7.7049
	step [39/196], loss=9.6412
	step [40/196], loss=7.5560
	step [41/196], loss=9.6792
	step [42/196], loss=8.0567
	step [43/196], loss=8.9798
	step [44/196], loss=8.7292
	step [45/196], loss=8.4275
	step [46/196], loss=7.1020
	step [47/196], loss=9.2287
	step [48/196], loss=8.3702
	step [49/196], loss=9.6134
	step [50/196], loss=8.4253
	step [51/196], loss=7.8706
	step [52/196], loss=8.6727
	step [53/196], loss=7.5764
	step [54/196], loss=8.7626
	step [55/196], loss=8.7031
	step [56/196], loss=11.0417
	step [57/196], loss=7.7950
	step [58/196], loss=8.1254
	step [59/196], loss=7.5164
	step [60/196], loss=7.4894
	step [61/196], loss=6.8789
	step [62/196], loss=7.5273
	step [63/196], loss=7.1922
	step [64/196], loss=10.6021
	step [65/196], loss=8.2133
	step [66/196], loss=7.2039
	step [67/196], loss=8.7833
	step [68/196], loss=8.5383
	step [69/196], loss=7.9231
	step [70/196], loss=8.3610
	step [71/196], loss=7.1313
	step [72/196], loss=7.3830
	step [73/196], loss=7.6314
	step [74/196], loss=10.4301
	step [75/196], loss=10.7015
	step [76/196], loss=8.7262
	step [77/196], loss=8.6879
	step [78/196], loss=7.3675
	step [79/196], loss=9.0090
	step [80/196], loss=10.4085
	step [81/196], loss=8.6877
	step [82/196], loss=9.3274
	step [83/196], loss=8.6129
	step [84/196], loss=7.9295
	step [85/196], loss=8.1311
	step [86/196], loss=7.6063
	step [87/196], loss=7.9359
	step [88/196], loss=8.7825
	step [89/196], loss=8.2635
	step [90/196], loss=9.4214
	step [91/196], loss=9.0447
	step [92/196], loss=8.3037
	step [93/196], loss=8.4303
	step [94/196], loss=8.9091
	step [95/196], loss=9.5254
	step [96/196], loss=10.2870
	step [97/196], loss=7.5488
	step [98/196], loss=10.5424
	step [99/196], loss=9.0567
	step [100/196], loss=7.5535
	step [101/196], loss=7.8367
	step [102/196], loss=7.4123
	step [103/196], loss=7.9934
	step [104/196], loss=8.0159
	step [105/196], loss=9.5722
	step [106/196], loss=7.0653
	step [107/196], loss=7.6867
	step [108/196], loss=7.0081
	step [109/196], loss=6.9274
	step [110/196], loss=6.8575
	step [111/196], loss=8.8534
	step [112/196], loss=7.5392
	step [113/196], loss=8.2120
	step [114/196], loss=7.5764
	step [115/196], loss=8.4302
	step [116/196], loss=7.9593
	step [117/196], loss=8.5239
	step [118/196], loss=8.2753
	step [119/196], loss=8.4412
	step [120/196], loss=7.5684
	step [121/196], loss=8.4714
	step [122/196], loss=9.6803
	step [123/196], loss=7.7899
	step [124/196], loss=6.8838
	step [125/196], loss=7.7570
	step [126/196], loss=9.6713
	step [127/196], loss=7.2723
	step [128/196], loss=7.5410
	step [129/196], loss=7.5220
	step [130/196], loss=7.9051
	step [131/196], loss=8.5136
	step [132/196], loss=8.4502
	step [133/196], loss=6.9761
	step [134/196], loss=6.4876
	step [135/196], loss=7.2562
	step [136/196], loss=8.7030
	step [137/196], loss=7.6987
	step [138/196], loss=8.7629
	step [139/196], loss=8.6624
	step [140/196], loss=7.5175
	step [141/196], loss=8.8789
	step [142/196], loss=9.4521
	step [143/196], loss=8.1611
	step [144/196], loss=8.6851
	step [145/196], loss=8.7442
	step [146/196], loss=7.9438
	step [147/196], loss=7.2756
	step [148/196], loss=9.0507
	step [149/196], loss=8.9908
	step [150/196], loss=8.2098
	step [151/196], loss=7.3342
	step [152/196], loss=7.9365
	step [153/196], loss=7.1666
	step [154/196], loss=6.9117
	step [155/196], loss=7.4360
	step [156/196], loss=10.1066
	step [157/196], loss=7.0691
	step [158/196], loss=8.7981
	step [159/196], loss=7.7395
	step [160/196], loss=6.9883
	step [161/196], loss=8.9420
	step [162/196], loss=7.4372
	step [163/196], loss=8.4609
	step [164/196], loss=9.9395
	step [165/196], loss=8.0414
	step [166/196], loss=7.7460
	step [167/196], loss=7.3842
	step [168/196], loss=9.6049
	step [169/196], loss=7.3049
	step [170/196], loss=7.5634
	step [171/196], loss=8.2871
	step [172/196], loss=7.0502
	step [173/196], loss=9.0455
	step [174/196], loss=8.2763
	step [175/196], loss=7.9382
	step [176/196], loss=7.5656
	step [177/196], loss=7.6112
	step [178/196], loss=8.3393
	step [179/196], loss=8.2689
	step [180/196], loss=7.4838
	step [181/196], loss=8.5935
	step [182/196], loss=6.7588
	step [183/196], loss=7.9803
	step [184/196], loss=6.8564
	step [185/196], loss=7.2679
	step [186/196], loss=8.4043
	step [187/196], loss=8.8065
	step [188/196], loss=10.1995
	step [189/196], loss=8.3995
	step [190/196], loss=8.0833
	step [191/196], loss=10.7752
	step [192/196], loss=9.4126
	step [193/196], loss=6.6437
	step [194/196], loss=8.6610
	step [195/196], loss=7.5447
	step [196/196], loss=1.7085
	Evaluating
	loss=0.0284, precision=0.2031, recall=0.9892, f1=0.7131
Training epoch 25
	step [1/196], loss=8.7928
	step [2/196], loss=6.8073
	step [3/196], loss=6.7988
	step [4/196], loss=9.8649
	step [5/196], loss=9.2977
	step [6/196], loss=6.8410
	step [7/196], loss=8.6018
	step [8/196], loss=8.9696
	step [9/196], loss=7.4335
	step [10/196], loss=7.5902
	step [11/196], loss=8.0887
	step [12/196], loss=9.2554
	step [13/196], loss=8.1440
	step [14/196], loss=8.9043
	step [15/196], loss=7.7004
	step [16/196], loss=8.2572
	step [17/196], loss=7.8819
	step [18/196], loss=8.5346
	step [19/196], loss=7.0392
	step [20/196], loss=7.6046
	step [21/196], loss=7.7503
	step [22/196], loss=7.8327
	step [23/196], loss=7.6744
	step [24/196], loss=6.5001
	step [25/196], loss=9.5402
	step [26/196], loss=7.7974
	step [27/196], loss=8.1768
	step [28/196], loss=6.3860
	step [29/196], loss=8.0081
	step [30/196], loss=7.6568
	step [31/196], loss=7.7312
	step [32/196], loss=7.5983
	step [33/196], loss=7.9337
	step [34/196], loss=8.2139
	step [35/196], loss=8.4497
	step [36/196], loss=8.7487
	step [37/196], loss=8.6772
	step [38/196], loss=7.3057
	step [39/196], loss=7.6541
	step [40/196], loss=7.9190
	step [41/196], loss=8.3648
	step [42/196], loss=6.7024
	step [43/196], loss=7.8706
	step [44/196], loss=8.8031
	step [45/196], loss=8.6294
	step [46/196], loss=6.7877
	step [47/196], loss=7.5867
	step [48/196], loss=8.4633
	step [49/196], loss=8.4760
	step [50/196], loss=8.5699
	step [51/196], loss=8.2364
	step [52/196], loss=9.0592
	step [53/196], loss=8.8897
	step [54/196], loss=6.9334
	step [55/196], loss=9.7120
	step [56/196], loss=7.8792
	step [57/196], loss=8.3620
	step [58/196], loss=7.2444
	step [59/196], loss=9.8098
	step [60/196], loss=8.6158
	step [61/196], loss=9.7964
	step [62/196], loss=7.8605
	step [63/196], loss=7.5293
	step [64/196], loss=8.0258
	step [65/196], loss=8.5160
	step [66/196], loss=6.9558
	step [67/196], loss=8.7637
	step [68/196], loss=7.5900
	step [69/196], loss=8.6740
	step [70/196], loss=7.4903
	step [71/196], loss=7.8215
	step [72/196], loss=8.1981
	step [73/196], loss=7.4893
	step [74/196], loss=8.6351
	step [75/196], loss=7.2316
	step [76/196], loss=7.6173
	step [77/196], loss=7.5249
	step [78/196], loss=7.2080
	step [79/196], loss=8.6346
	step [80/196], loss=8.2265
	step [81/196], loss=9.2214
	step [82/196], loss=7.7482
	step [83/196], loss=7.9766
	step [84/196], loss=8.7160
	step [85/196], loss=7.6986
	step [86/196], loss=7.7368
	step [87/196], loss=7.7888
	step [88/196], loss=6.8147
	step [89/196], loss=6.8745
	step [90/196], loss=8.0073
	step [91/196], loss=7.7867
	step [92/196], loss=7.6626
	step [93/196], loss=10.0134
	step [94/196], loss=7.2718
	step [95/196], loss=9.2376
	step [96/196], loss=8.6753
	step [97/196], loss=8.3426
	step [98/196], loss=7.9163
	step [99/196], loss=9.5525
	step [100/196], loss=8.5722
	step [101/196], loss=7.5833
	step [102/196], loss=7.8010
	step [103/196], loss=8.1891
	step [104/196], loss=8.0140
	step [105/196], loss=11.1167
	step [106/196], loss=8.9708
	step [107/196], loss=7.4688
	step [108/196], loss=7.6294
	step [109/196], loss=9.3080
	step [110/196], loss=7.5902
	step [111/196], loss=8.5579
	step [112/196], loss=9.8614
	step [113/196], loss=9.5192
	step [114/196], loss=7.3559
	step [115/196], loss=7.4791
	step [116/196], loss=7.7630
	step [117/196], loss=8.9998
	step [118/196], loss=7.3862
	step [119/196], loss=8.1567
	step [120/196], loss=9.9705
	step [121/196], loss=6.6023
	step [122/196], loss=8.1360
	step [123/196], loss=7.1790
	step [124/196], loss=10.4381
	step [125/196], loss=7.1738
	step [126/196], loss=8.7411
	step [127/196], loss=8.0347
	step [128/196], loss=8.5973
	step [129/196], loss=7.3259
	step [130/196], loss=8.8462
	step [131/196], loss=7.5602
	step [132/196], loss=8.8128
	step [133/196], loss=7.9859
	step [134/196], loss=8.6557
	step [135/196], loss=6.9633
	step [136/196], loss=7.2800
	step [137/196], loss=8.8376
	step [138/196], loss=8.8958
	step [139/196], loss=7.9722
	step [140/196], loss=7.8223
	step [141/196], loss=7.8005
	step [142/196], loss=7.3061
	step [143/196], loss=7.9236
	step [144/196], loss=7.0839
	step [145/196], loss=8.0475
	step [146/196], loss=8.0662
	step [147/196], loss=8.6183
	step [148/196], loss=7.2752
	step [149/196], loss=7.0408
	step [150/196], loss=7.3410
	step [151/196], loss=7.8241
	step [152/196], loss=7.0206
	step [153/196], loss=6.7787
	step [154/196], loss=8.2951
	step [155/196], loss=7.1310
	step [156/196], loss=8.0627
	step [157/196], loss=6.7761
	step [158/196], loss=7.3103
	step [159/196], loss=8.0741
	step [160/196], loss=8.3344
	step [161/196], loss=7.7928
	step [162/196], loss=9.5464
	step [163/196], loss=7.2696
	step [164/196], loss=8.6835
	step [165/196], loss=7.2236
	step [166/196], loss=6.9096
	step [167/196], loss=7.3508
	step [168/196], loss=6.3014
	step [169/196], loss=8.3128
	step [170/196], loss=7.7851
	step [171/196], loss=8.2297
	step [172/196], loss=10.0152
	step [173/196], loss=8.9709
	step [174/196], loss=6.9642
	step [175/196], loss=8.2807
	step [176/196], loss=7.3487
	step [177/196], loss=6.5153
	step [178/196], loss=7.0219
	step [179/196], loss=8.2992
	step [180/196], loss=6.5109
	step [181/196], loss=9.2045
	step [182/196], loss=7.0852
	step [183/196], loss=9.0944
	step [184/196], loss=7.7486
	step [185/196], loss=8.6106
	step [186/196], loss=5.9108
	step [187/196], loss=7.1956
	step [188/196], loss=8.4049
	step [189/196], loss=8.0086
	step [190/196], loss=9.8263
	step [191/196], loss=9.5914
	step [192/196], loss=7.6455
	step [193/196], loss=7.8623
	step [194/196], loss=7.6022
	step [195/196], loss=7.2622
	step [196/196], loss=3.0122
	Evaluating
	loss=0.0284, precision=0.1997, recall=0.9881, f1=0.7084
Training epoch 26
	step [1/196], loss=6.6604
	step [2/196], loss=6.9070
	step [3/196], loss=7.7536
	step [4/196], loss=8.7474
	step [5/196], loss=6.3754
	step [6/196], loss=9.6022
	step [7/196], loss=6.3662
	step [8/196], loss=6.7526
	step [9/196], loss=6.0995
	step [10/196], loss=7.0556
	step [11/196], loss=7.6549
	step [12/196], loss=7.5453
	step [13/196], loss=8.3791
	step [14/196], loss=8.6112
	step [15/196], loss=7.2074
	step [16/196], loss=7.7136
	step [17/196], loss=9.4654
	step [18/196], loss=8.4288
	step [19/196], loss=7.6093
	step [20/196], loss=8.3988
	step [21/196], loss=7.9055
	step [22/196], loss=9.8621
	step [23/196], loss=7.8631
	step [24/196], loss=7.6626
	step [25/196], loss=8.8966
	step [26/196], loss=7.8251
	step [27/196], loss=8.7157
	step [28/196], loss=7.6294
	step [29/196], loss=7.3330
	step [30/196], loss=6.8968
	step [31/196], loss=7.8015
	step [32/196], loss=7.3221
	step [33/196], loss=6.7673
	step [34/196], loss=7.8600
	step [35/196], loss=8.4355
	step [36/196], loss=9.1367
	step [37/196], loss=8.9812
	step [38/196], loss=7.0082
	step [39/196], loss=7.4897
	step [40/196], loss=10.2045
	step [41/196], loss=6.8038
	step [42/196], loss=7.8724
	step [43/196], loss=9.4927
	step [44/196], loss=7.4085
	step [45/196], loss=7.2193
	step [46/196], loss=8.1506
	step [47/196], loss=7.2413
	step [48/196], loss=8.2602
	step [49/196], loss=7.5808
	step [50/196], loss=8.5785
	step [51/196], loss=7.0604
	step [52/196], loss=7.2375
	step [53/196], loss=6.3274
	step [54/196], loss=7.9943
	step [55/196], loss=8.7120
	step [56/196], loss=8.0121
	step [57/196], loss=7.2042
	step [58/196], loss=8.0775
	step [59/196], loss=7.3050
	step [60/196], loss=7.5950
	step [61/196], loss=8.2100
	step [62/196], loss=7.4740
	step [63/196], loss=7.1088
	step [64/196], loss=7.8959
	step [65/196], loss=7.5236
	step [66/196], loss=7.6159
	step [67/196], loss=7.1852
	step [68/196], loss=8.3597
	step [69/196], loss=8.6540
	step [70/196], loss=7.6585
	step [71/196], loss=7.9764
	step [72/196], loss=8.3796
	step [73/196], loss=8.3518
	step [74/196], loss=8.5778
	step [75/196], loss=6.8836
	step [76/196], loss=6.6375
	step [77/196], loss=6.5437
	step [78/196], loss=7.2667
	step [79/196], loss=6.7334
	step [80/196], loss=8.8694
	step [81/196], loss=7.5526
	step [82/196], loss=7.1279
	step [83/196], loss=8.0386
	step [84/196], loss=8.9289
	step [85/196], loss=7.7511
	step [86/196], loss=6.7213
	step [87/196], loss=8.4555
	step [88/196], loss=6.8346
	step [89/196], loss=7.0044
	step [90/196], loss=7.1844
	step [91/196], loss=7.8997
	step [92/196], loss=8.3410
	step [93/196], loss=7.7402
	step [94/196], loss=6.8108
	step [95/196], loss=8.4169
	step [96/196], loss=8.4209
	step [97/196], loss=6.8007
	step [98/196], loss=6.9963
	step [99/196], loss=7.2970
	step [100/196], loss=8.7561
	step [101/196], loss=8.7899
	step [102/196], loss=6.7046
	step [103/196], loss=7.5349
	step [104/196], loss=7.2435
	step [105/196], loss=6.4570
	step [106/196], loss=7.5552
	step [107/196], loss=7.5836
	step [108/196], loss=7.6127
	step [109/196], loss=7.2301
	step [110/196], loss=7.8619
	step [111/196], loss=7.9096
	step [112/196], loss=8.9053
	step [113/196], loss=8.3584
	step [114/196], loss=7.6506
	step [115/196], loss=8.5093
	step [116/196], loss=6.7728
	step [117/196], loss=8.2007
	step [118/196], loss=8.0423
	step [119/196], loss=7.3254
	step [120/196], loss=6.9448
	step [121/196], loss=8.4204
	step [122/196], loss=7.1507
	step [123/196], loss=8.6926
	step [124/196], loss=8.0686
	step [125/196], loss=5.9415
	step [126/196], loss=7.1450
	step [127/196], loss=7.9482
	step [128/196], loss=7.6561
	step [129/196], loss=7.5172
	step [130/196], loss=7.7290
	step [131/196], loss=8.6981
	step [132/196], loss=8.0213
	step [133/196], loss=7.4911
	step [134/196], loss=8.8147
	step [135/196], loss=8.3720
	step [136/196], loss=6.9523
	step [137/196], loss=6.8403
	step [138/196], loss=7.3074
	step [139/196], loss=6.3234
	step [140/196], loss=8.7009
	step [141/196], loss=7.2766
	step [142/196], loss=8.7591
	step [143/196], loss=6.8740
	step [144/196], loss=7.8598
	step [145/196], loss=7.0092
	step [146/196], loss=6.6626
	step [147/196], loss=8.1780
	step [148/196], loss=7.7424
	step [149/196], loss=8.9492
	step [150/196], loss=9.4089
	step [151/196], loss=6.7095
	step [152/196], loss=7.3273
	step [153/196], loss=7.7508
	step [154/196], loss=7.8005
	step [155/196], loss=7.3834
	step [156/196], loss=7.2144
	step [157/196], loss=9.2725
	step [158/196], loss=8.2021
	step [159/196], loss=6.7786
	step [160/196], loss=6.7579
	step [161/196], loss=8.5249
	step [162/196], loss=7.0603
	step [163/196], loss=9.8652
	step [164/196], loss=6.8334
	step [165/196], loss=7.3759
	step [166/196], loss=8.8511
	step [167/196], loss=6.1667
	step [168/196], loss=8.7864
	step [169/196], loss=7.9789
	step [170/196], loss=7.3481
	step [171/196], loss=6.6944
	step [172/196], loss=7.1161
	step [173/196], loss=7.5066
	step [174/196], loss=7.6143
	step [175/196], loss=9.0464
	step [176/196], loss=7.2317
	step [177/196], loss=9.5030
	step [178/196], loss=7.0623
	step [179/196], loss=7.4165
	step [180/196], loss=8.1171
	step [181/196], loss=7.3801
	step [182/196], loss=7.2886
	step [183/196], loss=7.4338
	step [184/196], loss=7.2265
	step [185/196], loss=8.6518
	step [186/196], loss=9.2247
	step [187/196], loss=8.4662
	step [188/196], loss=6.9440
	step [189/196], loss=7.7953
	step [190/196], loss=7.3759
	step [191/196], loss=8.5771
	step [192/196], loss=8.3213
	step [193/196], loss=7.1831
	step [194/196], loss=8.4290
	step [195/196], loss=6.2331
	step [196/196], loss=1.9620
	Evaluating
	loss=0.0290, precision=0.1898, recall=0.9891, f1=0.6960
Training epoch 27
	step [1/196], loss=7.8510
	step [2/196], loss=6.1956
	step [3/196], loss=7.3800
	step [4/196], loss=7.7190
	step [5/196], loss=8.5096
	step [6/196], loss=8.3517
	step [7/196], loss=7.5580
	step [8/196], loss=7.2753
	step [9/196], loss=9.5690
	step [10/196], loss=7.8245
	step [11/196], loss=8.3694
	step [12/196], loss=7.0736
	step [13/196], loss=6.7991
	step [14/196], loss=7.6450
	step [15/196], loss=7.1061
	step [16/196], loss=7.2800
	step [17/196], loss=6.0230
	step [18/196], loss=8.1405
	step [19/196], loss=7.3092
	step [20/196], loss=7.0423
	step [21/196], loss=7.1259
	step [22/196], loss=7.4762
	step [23/196], loss=8.7288
	step [24/196], loss=7.3538
	step [25/196], loss=8.8837
	step [26/196], loss=7.5231
	step [27/196], loss=7.3459
	step [28/196], loss=7.4035
	step [29/196], loss=6.6433
	step [30/196], loss=8.2663
	step [31/196], loss=7.8863
	step [32/196], loss=7.5314
	step [33/196], loss=7.8731
	step [34/196], loss=8.6956
	step [35/196], loss=6.6475
	step [36/196], loss=5.9523
	step [37/196], loss=7.3572
	step [38/196], loss=6.2261
	step [39/196], loss=6.9583
	step [40/196], loss=6.9340
	step [41/196], loss=6.2001
	step [42/196], loss=5.9514
	step [43/196], loss=7.8458
	step [44/196], loss=7.2633
	step [45/196], loss=6.1873
	step [46/196], loss=8.2678
	step [47/196], loss=7.4504
	step [48/196], loss=6.8619
	step [49/196], loss=6.5692
	step [50/196], loss=7.4470
	step [51/196], loss=7.0443
	step [52/196], loss=9.1340
	step [53/196], loss=7.4859
	step [54/196], loss=6.6248
	step [55/196], loss=7.0825
	step [56/196], loss=7.5311
	step [57/196], loss=8.2822
	step [58/196], loss=6.7717
	step [59/196], loss=9.2656
	step [60/196], loss=8.8360
	step [61/196], loss=8.5600
	step [62/196], loss=7.9003
	step [63/196], loss=7.3504
	step [64/196], loss=7.9204
	step [65/196], loss=7.7912
	step [66/196], loss=7.1825
	step [67/196], loss=7.1309
	step [68/196], loss=7.9183
	step [69/196], loss=7.7006
	step [70/196], loss=7.5827
	step [71/196], loss=8.2869
	step [72/196], loss=8.4312
	step [73/196], loss=7.2478
	step [74/196], loss=8.5585
	step [75/196], loss=7.6828
	step [76/196], loss=6.7931
	step [77/196], loss=8.0362
	step [78/196], loss=7.8286
	step [79/196], loss=7.4389
	step [80/196], loss=5.7602
	step [81/196], loss=8.2902
	step [82/196], loss=7.1740
	step [83/196], loss=7.0024
	step [84/196], loss=7.5590
	step [85/196], loss=7.1535
	step [86/196], loss=8.6648
	step [87/196], loss=8.0125
	step [88/196], loss=6.9799
	step [89/196], loss=7.6649
	step [90/196], loss=7.6233
	step [91/196], loss=7.5530
	step [92/196], loss=6.9517
	step [93/196], loss=7.8886
	step [94/196], loss=8.4272
	step [95/196], loss=6.7158
	step [96/196], loss=7.7166
	step [97/196], loss=6.9424
	step [98/196], loss=7.4268
	step [99/196], loss=6.6047
	step [100/196], loss=6.7614
	step [101/196], loss=7.1724
	step [102/196], loss=8.6037
	step [103/196], loss=8.5703
	step [104/196], loss=8.2257
	step [105/196], loss=7.3615
	step [106/196], loss=7.4181
	step [107/196], loss=6.4766
	step [108/196], loss=7.0934
	step [109/196], loss=7.8846
	step [110/196], loss=7.1623
	step [111/196], loss=6.7240
	step [112/196], loss=6.1897
	step [113/196], loss=9.6441
	step [114/196], loss=7.8284
	step [115/196], loss=6.4590
	step [116/196], loss=6.6853
	step [117/196], loss=6.2234
	step [118/196], loss=7.7340
	step [119/196], loss=6.5781
	step [120/196], loss=6.1260
	step [121/196], loss=7.5972
	step [122/196], loss=7.5298
	step [123/196], loss=7.9321
	step [124/196], loss=7.1794
	step [125/196], loss=7.9688
	step [126/196], loss=8.0279
	step [127/196], loss=7.0556
	step [128/196], loss=7.2013
	step [129/196], loss=6.4052
	step [130/196], loss=8.2679
	step [131/196], loss=7.6613
	step [132/196], loss=8.2800
	step [133/196], loss=9.9233
	step [134/196], loss=8.2795
	step [135/196], loss=8.9387
	step [136/196], loss=7.9944
	step [137/196], loss=6.9717
	step [138/196], loss=8.4670
	step [139/196], loss=7.6635
	step [140/196], loss=8.2718
	step [141/196], loss=7.5823
	step [142/196], loss=6.6648
	step [143/196], loss=7.4543
	step [144/196], loss=8.0537
	step [145/196], loss=6.4177
	step [146/196], loss=8.4386
	step [147/196], loss=6.9922
	step [148/196], loss=7.1608
	step [149/196], loss=7.3663
	step [150/196], loss=7.1698
	step [151/196], loss=6.1324
	step [152/196], loss=6.9157
	step [153/196], loss=6.8668
	step [154/196], loss=6.5485
	step [155/196], loss=7.9775
	step [156/196], loss=7.5619
	step [157/196], loss=7.0543
	step [158/196], loss=9.7339
	step [159/196], loss=5.8721
	step [160/196], loss=8.1421
	step [161/196], loss=7.3783
	step [162/196], loss=7.2058
	step [163/196], loss=8.1379
	step [164/196], loss=7.3015
	step [165/196], loss=6.5716
	step [166/196], loss=6.9784
	step [167/196], loss=6.4321
	step [168/196], loss=7.5588
	step [169/196], loss=7.8344
	step [170/196], loss=6.0788
	step [171/196], loss=7.5530
	step [172/196], loss=8.1060
	step [173/196], loss=7.1387
	step [174/196], loss=8.3824
	step [175/196], loss=7.6503
	step [176/196], loss=7.8554
	step [177/196], loss=8.5934
	step [178/196], loss=8.2650
	step [179/196], loss=6.0253
	step [180/196], loss=8.1119
	step [181/196], loss=8.7145
	step [182/196], loss=7.5343
	step [183/196], loss=7.8448
	step [184/196], loss=7.9793
	step [185/196], loss=7.6242
	step [186/196], loss=6.8766
	step [187/196], loss=7.7956
	step [188/196], loss=8.6011
	step [189/196], loss=5.7868
	step [190/196], loss=8.2362
	step [191/196], loss=9.8721
	step [192/196], loss=6.7047
	step [193/196], loss=7.2101
	step [194/196], loss=7.1722
	step [195/196], loss=8.1227
	step [196/196], loss=2.2332
	Evaluating
	loss=0.0271, precision=0.2015, recall=0.9885, f1=0.7108
Training epoch 28
	step [1/196], loss=7.1177
	step [2/196], loss=8.3833
	step [3/196], loss=6.4760
	step [4/196], loss=8.3152
	step [5/196], loss=6.8925
	step [6/196], loss=8.1010
	step [7/196], loss=6.9108
	step [8/196], loss=7.4612
	step [9/196], loss=6.9512
	step [10/196], loss=7.5654
	step [11/196], loss=7.6924
	step [12/196], loss=8.3863
	step [13/196], loss=6.9167
	step [14/196], loss=7.6412
	step [15/196], loss=7.7496
	step [16/196], loss=6.6986
	step [17/196], loss=7.4911
	step [18/196], loss=7.2183
	step [19/196], loss=7.9501
	step [20/196], loss=8.3713
	step [21/196], loss=6.4152
	step [22/196], loss=7.3084
	step [23/196], loss=6.6574
	step [24/196], loss=8.1891
	step [25/196], loss=7.3532
	step [26/196], loss=5.5087
	step [27/196], loss=8.2375
	step [28/196], loss=9.4572
	step [29/196], loss=7.1931
	step [30/196], loss=6.9682
	step [31/196], loss=7.0108
	step [32/196], loss=9.3152
	step [33/196], loss=7.4263
	step [34/196], loss=8.1828
	step [35/196], loss=7.3903
	step [36/196], loss=9.0376
	step [37/196], loss=6.3348
	step [38/196], loss=6.6377
	step [39/196], loss=7.9983
	step [40/196], loss=7.3140
	step [41/196], loss=7.0216
	step [42/196], loss=8.3095
	step [43/196], loss=6.8630
	step [44/196], loss=7.3060
	step [45/196], loss=7.5461
	step [46/196], loss=6.3171
	step [47/196], loss=7.1019
	step [48/196], loss=8.7179
	step [49/196], loss=7.5842
	step [50/196], loss=7.5381
	step [51/196], loss=9.3711
	step [52/196], loss=7.3773
	step [53/196], loss=7.7528
	step [54/196], loss=6.2250
	step [55/196], loss=8.4935
	step [56/196], loss=7.8956
	step [57/196], loss=7.3902
	step [58/196], loss=8.2929
	step [59/196], loss=7.4953
	step [60/196], loss=7.7318
	step [61/196], loss=7.2499
	step [62/196], loss=7.0789
	step [63/196], loss=6.0307
	step [64/196], loss=7.1747
	step [65/196], loss=8.8494
	step [66/196], loss=6.1836
	step [67/196], loss=6.9411
	step [68/196], loss=6.5378
	step [69/196], loss=7.1318
	step [70/196], loss=5.9718
	step [71/196], loss=7.5773
	step [72/196], loss=7.2084
	step [73/196], loss=6.5736
	step [74/196], loss=7.5055
	step [75/196], loss=7.7007
	step [76/196], loss=6.4732
	step [77/196], loss=7.4733
	step [78/196], loss=9.0571
	step [79/196], loss=6.5884
	step [80/196], loss=8.7156
	step [81/196], loss=6.9400
	step [82/196], loss=6.8468
	step [83/196], loss=6.4965
	step [84/196], loss=9.0870
	step [85/196], loss=7.2121
	step [86/196], loss=7.1508
	step [87/196], loss=7.9771
	step [88/196], loss=6.0462
	step [89/196], loss=7.2252
	step [90/196], loss=6.6414
	step [91/196], loss=7.2251
	step [92/196], loss=7.0400
	step [93/196], loss=6.7001
	step [94/196], loss=5.6357
	step [95/196], loss=7.9726
	step [96/196], loss=7.4658
	step [97/196], loss=7.3908
	step [98/196], loss=7.8482
	step [99/196], loss=7.3896
	step [100/196], loss=9.2182
	step [101/196], loss=8.0792
	step [102/196], loss=7.4878
	step [103/196], loss=7.8532
	step [104/196], loss=6.8975
	step [105/196], loss=6.1691
	step [106/196], loss=6.4488
	step [107/196], loss=8.8582
	step [108/196], loss=5.8581
	step [109/196], loss=6.9426
	step [110/196], loss=7.2130
	step [111/196], loss=6.3708
	step [112/196], loss=7.0846
	step [113/196], loss=6.8442
	step [114/196], loss=7.7576
	step [115/196], loss=8.2163
	step [116/196], loss=8.2298
	step [117/196], loss=7.7718
	step [118/196], loss=7.1113
	step [119/196], loss=7.6600
	step [120/196], loss=7.8667
	step [121/196], loss=7.1534
	step [122/196], loss=6.8074
	step [123/196], loss=9.3514
	step [124/196], loss=7.7516
	step [125/196], loss=7.0199
	step [126/196], loss=7.3825
	step [127/196], loss=7.1596
	step [128/196], loss=8.8072
	step [129/196], loss=7.5381
	step [130/196], loss=7.1528
	step [131/196], loss=7.6296
	step [132/196], loss=6.4352
	step [133/196], loss=6.3186
	step [134/196], loss=7.4049
	step [135/196], loss=7.6719
	step [136/196], loss=6.9212
	step [137/196], loss=6.4953
	step [138/196], loss=7.6678
	step [139/196], loss=6.4714
	step [140/196], loss=7.1717
	step [141/196], loss=7.5659
	step [142/196], loss=5.7629
	step [143/196], loss=7.1091
	step [144/196], loss=7.1942
	step [145/196], loss=8.1913
	step [146/196], loss=7.1847
	step [147/196], loss=8.1059
	step [148/196], loss=8.1201
	step [149/196], loss=6.3654
	step [150/196], loss=6.7876
	step [151/196], loss=8.2887
	step [152/196], loss=9.0256
	step [153/196], loss=7.3392
	step [154/196], loss=8.3487
	step [155/196], loss=7.0403
	step [156/196], loss=6.8574
	step [157/196], loss=7.3272
	step [158/196], loss=8.8110
	step [159/196], loss=7.8675
	step [160/196], loss=6.4285
	step [161/196], loss=8.2445
	step [162/196], loss=7.1031
	step [163/196], loss=7.7617
	step [164/196], loss=7.9425
	step [165/196], loss=6.6059
	step [166/196], loss=6.9988
	step [167/196], loss=7.9142
	step [168/196], loss=6.9242
	step [169/196], loss=7.6035
	step [170/196], loss=6.2681
	step [171/196], loss=8.1624
	step [172/196], loss=7.8300
	step [173/196], loss=6.3641
	step [174/196], loss=6.8390
	step [175/196], loss=6.8506
	step [176/196], loss=6.5624
	step [177/196], loss=7.0761
	step [178/196], loss=7.1224
	step [179/196], loss=7.5241
	step [180/196], loss=7.5931
	step [181/196], loss=6.8862
	step [182/196], loss=6.9862
	step [183/196], loss=7.6184
	step [184/196], loss=6.5118
	step [185/196], loss=7.2047
	step [186/196], loss=7.2733
	step [187/196], loss=8.3808
	step [188/196], loss=7.7321
	step [189/196], loss=7.0042
	step [190/196], loss=6.8787
	step [191/196], loss=6.9476
	step [192/196], loss=8.9005
	step [193/196], loss=7.0212
	step [194/196], loss=7.2646
	step [195/196], loss=6.8963
	step [196/196], loss=1.8942
	Evaluating
	loss=0.0246, precision=0.2271, recall=0.9872, f1=0.7397
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/196], loss=6.8685
	step [2/196], loss=6.9602
	step [3/196], loss=7.8984
	step [4/196], loss=7.6412
	step [5/196], loss=6.5240
	step [6/196], loss=7.2426
	step [7/196], loss=8.5095
	step [8/196], loss=6.7974
	step [9/196], loss=9.2235
	step [10/196], loss=6.0471
	step [11/196], loss=7.7785
	step [12/196], loss=6.8674
	step [13/196], loss=6.0900
	step [14/196], loss=6.4920
	step [15/196], loss=7.7918
	step [16/196], loss=6.6122
	step [17/196], loss=8.6873
	step [18/196], loss=6.0388
	step [19/196], loss=6.6480
	step [20/196], loss=8.1426
	step [21/196], loss=8.1871
	step [22/196], loss=7.0644
	step [23/196], loss=6.5626
	step [24/196], loss=6.9941
	step [25/196], loss=7.9993
	step [26/196], loss=6.6922
	step [27/196], loss=7.4032
	step [28/196], loss=6.5815
	step [29/196], loss=8.4703
	step [30/196], loss=7.7233
	step [31/196], loss=6.6932
	step [32/196], loss=8.2535
	step [33/196], loss=7.4332
	step [34/196], loss=5.8907
	step [35/196], loss=6.3198
	step [36/196], loss=7.4699
	step [37/196], loss=7.2041
	step [38/196], loss=8.3356
	step [39/196], loss=8.8848
	step [40/196], loss=7.9574
	step [41/196], loss=6.2835
	step [42/196], loss=7.7276
	step [43/196], loss=7.2082
	step [44/196], loss=7.1342
	step [45/196], loss=6.2261
	step [46/196], loss=6.6181
	step [47/196], loss=5.7517
	step [48/196], loss=6.4688
	step [49/196], loss=8.0215
	step [50/196], loss=7.6043
	step [51/196], loss=5.9380
	step [52/196], loss=8.1118
	step [53/196], loss=6.4413
	step [54/196], loss=6.7322
	step [55/196], loss=6.1364
	step [56/196], loss=9.6568
	step [57/196], loss=7.7629
	step [58/196], loss=7.2103
	step [59/196], loss=6.7930
	step [60/196], loss=5.8937
	step [61/196], loss=7.6160
	step [62/196], loss=8.7214
	step [63/196], loss=7.9791
	step [64/196], loss=6.5664
	step [65/196], loss=10.0703
	step [66/196], loss=6.7338
	step [67/196], loss=7.1677
	step [68/196], loss=6.7021
	step [69/196], loss=7.7360
	step [70/196], loss=7.0163
	step [71/196], loss=6.9689
	step [72/196], loss=6.1465
	step [73/196], loss=7.7422
	step [74/196], loss=7.3061
	step [75/196], loss=7.1828
	step [76/196], loss=7.2228
	step [77/196], loss=7.8503
	step [78/196], loss=7.7439
	step [79/196], loss=7.5452
	step [80/196], loss=6.0870
	step [81/196], loss=7.8460
	step [82/196], loss=7.8407
	step [83/196], loss=7.8662
	step [84/196], loss=7.7976
	step [85/196], loss=7.1207
	step [86/196], loss=7.7628
	step [87/196], loss=6.7664
	step [88/196], loss=8.5044
	step [89/196], loss=7.1141
	step [90/196], loss=5.8285
	step [91/196], loss=6.4685
	step [92/196], loss=6.6781
	step [93/196], loss=6.3231
	step [94/196], loss=7.1282
	step [95/196], loss=6.8818
	step [96/196], loss=6.7860
	step [97/196], loss=7.2159
	step [98/196], loss=8.2586
	step [99/196], loss=7.2808
	step [100/196], loss=8.5656
	step [101/196], loss=7.1343
	step [102/196], loss=6.1449
	step [103/196], loss=6.9819
	step [104/196], loss=7.6501
	step [105/196], loss=7.9527
	step [106/196], loss=7.4963
	step [107/196], loss=7.5961
	step [108/196], loss=6.9083
	step [109/196], loss=7.6500
	step [110/196], loss=7.0353
	step [111/196], loss=7.5027
	step [112/196], loss=6.9080
	step [113/196], loss=7.4093
	step [114/196], loss=5.8233
	step [115/196], loss=7.4679
	step [116/196], loss=6.4163
	step [117/196], loss=9.0006
	step [118/196], loss=6.7130
	step [119/196], loss=5.9064
	step [120/196], loss=7.9368
	step [121/196], loss=7.3936
	step [122/196], loss=7.0286
	step [123/196], loss=7.4352
	step [124/196], loss=8.0255
	step [125/196], loss=7.4026
	step [126/196], loss=7.1728
	step [127/196], loss=7.4472
	step [128/196], loss=6.1111
	step [129/196], loss=7.9626
	step [130/196], loss=7.0065
	step [131/196], loss=6.7655
	step [132/196], loss=6.1796
	step [133/196], loss=7.0545
	step [134/196], loss=6.3153
	step [135/196], loss=6.3388
	step [136/196], loss=6.8125
	step [137/196], loss=7.7135
	step [138/196], loss=7.0472
	step [139/196], loss=8.3058
	step [140/196], loss=7.5285
	step [141/196], loss=6.8639
	step [142/196], loss=7.3530
	step [143/196], loss=5.8855
	step [144/196], loss=6.5562
	step [145/196], loss=7.1411
	step [146/196], loss=6.9154
	step [147/196], loss=7.5226
	step [148/196], loss=6.8981
	step [149/196], loss=6.6766
	step [150/196], loss=6.3788
	step [151/196], loss=6.6257
	step [152/196], loss=8.4550
	step [153/196], loss=6.9451
	step [154/196], loss=6.4847
	step [155/196], loss=6.2210
	step [156/196], loss=6.7176
	step [157/196], loss=6.7231
	step [158/196], loss=7.3084
	step [159/196], loss=5.5511
	step [160/196], loss=7.7262
	step [161/196], loss=6.7870
	step [162/196], loss=6.8008
	step [163/196], loss=7.8222
	step [164/196], loss=7.0225
	step [165/196], loss=5.8264
	step [166/196], loss=7.3945
	step [167/196], loss=7.2653
	step [168/196], loss=6.7046
	step [169/196], loss=6.8684
	step [170/196], loss=7.6975
	step [171/196], loss=8.9996
	step [172/196], loss=5.7630
	step [173/196], loss=6.4976
	step [174/196], loss=7.8040
	step [175/196], loss=5.7101
	step [176/196], loss=5.9922
	step [177/196], loss=6.7088
	step [178/196], loss=5.8195
	step [179/196], loss=7.6894
	step [180/196], loss=6.5806
	step [181/196], loss=7.1304
	step [182/196], loss=7.4032
	step [183/196], loss=7.6203
	step [184/196], loss=6.8490
	step [185/196], loss=7.3747
	step [186/196], loss=7.8184
	step [187/196], loss=7.3969
	step [188/196], loss=7.1179
	step [189/196], loss=6.9843
	step [190/196], loss=5.6992
	step [191/196], loss=6.4257
	step [192/196], loss=5.9132
	step [193/196], loss=7.5301
	step [194/196], loss=7.1330
	step [195/196], loss=7.3125
	step [196/196], loss=2.5179
	Evaluating
	loss=0.0305, precision=0.1906, recall=0.9893, f1=0.6972
Training epoch 30
	step [1/196], loss=7.0115
	step [2/196], loss=7.9211
	step [3/196], loss=6.8069
	step [4/196], loss=6.5763
	step [5/196], loss=7.4006
	step [6/196], loss=6.5189
	step [7/196], loss=5.7034
	step [8/196], loss=7.8218
	step [9/196], loss=6.5156
	step [10/196], loss=7.9587
	step [11/196], loss=6.3577
	step [12/196], loss=6.6791
	step [13/196], loss=6.4970
	step [14/196], loss=8.2568
	step [15/196], loss=7.0136
	step [16/196], loss=5.4383
	step [17/196], loss=5.9804
	step [18/196], loss=6.9065
	step [19/196], loss=5.9531
	step [20/196], loss=6.3881
	step [21/196], loss=7.8231
	step [22/196], loss=6.8566
	step [23/196], loss=5.9858
	step [24/196], loss=6.4193
	step [25/196], loss=6.8751
	step [26/196], loss=7.4133
	step [27/196], loss=7.4752
	step [28/196], loss=5.9917
	step [29/196], loss=8.0647
	step [30/196], loss=8.3320
	step [31/196], loss=5.9753
	step [32/196], loss=6.2498
	step [33/196], loss=7.3198
	step [34/196], loss=5.9995
	step [35/196], loss=6.1244
	step [36/196], loss=7.2587
	step [37/196], loss=6.0842
	step [38/196], loss=5.7298
	step [39/196], loss=7.0027
	step [40/196], loss=6.5024
	step [41/196], loss=5.0473
	step [42/196], loss=6.5423
	step [43/196], loss=8.0198
	step [44/196], loss=5.8868
	step [45/196], loss=8.0745
	step [46/196], loss=6.8376
	step [47/196], loss=6.5424
	step [48/196], loss=6.2168
	step [49/196], loss=6.7446
	step [50/196], loss=6.7950
	step [51/196], loss=7.7261
	step [52/196], loss=6.8372
	step [53/196], loss=6.2530
	step [54/196], loss=7.3384
	step [55/196], loss=7.2434
	step [56/196], loss=4.4178
	step [57/196], loss=6.9017
	step [58/196], loss=6.4474
	step [59/196], loss=6.2260
	step [60/196], loss=8.4286
	step [61/196], loss=6.4965
	step [62/196], loss=7.1837
	step [63/196], loss=7.6443
	step [64/196], loss=6.9178
	step [65/196], loss=8.6727
	step [66/196], loss=6.6825
	step [67/196], loss=8.7284
	step [68/196], loss=6.3964
	step [69/196], loss=6.7296
	step [70/196], loss=6.4929
	step [71/196], loss=5.3803
	step [72/196], loss=7.4359
	step [73/196], loss=6.8352
	step [74/196], loss=7.4277
	step [75/196], loss=7.3636
	step [76/196], loss=7.2068
	step [77/196], loss=6.7759
	step [78/196], loss=7.7236
	step [79/196], loss=7.3251
	step [80/196], loss=6.5125
	step [81/196], loss=7.7488
	step [82/196], loss=7.0245
	step [83/196], loss=7.3870
	step [84/196], loss=6.8357
	step [85/196], loss=8.8485
	step [86/196], loss=5.8593
	step [87/196], loss=9.2861
	step [88/196], loss=6.1059
	step [89/196], loss=7.4185
	step [90/196], loss=7.3742
	step [91/196], loss=8.2795
	step [92/196], loss=6.4159
	step [93/196], loss=7.6797
	step [94/196], loss=5.9321
	step [95/196], loss=7.0829
	step [96/196], loss=6.9320
	step [97/196], loss=7.4290
	step [98/196], loss=6.5080
	step [99/196], loss=6.9415
	step [100/196], loss=5.7892
	step [101/196], loss=6.8645
	step [102/196], loss=6.6983
	step [103/196], loss=8.1605
	step [104/196], loss=8.3933
	step [105/196], loss=6.7856
	step [106/196], loss=6.3251
	step [107/196], loss=6.1287
	step [108/196], loss=6.5061
	step [109/196], loss=7.5445
	step [110/196], loss=7.4472
	step [111/196], loss=8.1591
	step [112/196], loss=6.9018
	step [113/196], loss=6.0792
	step [114/196], loss=5.8993
	step [115/196], loss=7.4973
	step [116/196], loss=7.1660
	step [117/196], loss=7.1056
	step [118/196], loss=6.9117
	step [119/196], loss=6.3599
	step [120/196], loss=7.5395
	step [121/196], loss=7.6154
	step [122/196], loss=6.9195
	step [123/196], loss=6.8524
	step [124/196], loss=5.8719
	step [125/196], loss=7.1263
	step [126/196], loss=6.5665
	step [127/196], loss=7.6210
	step [128/196], loss=6.8967
	step [129/196], loss=8.6279
	step [130/196], loss=7.0829
	step [131/196], loss=6.7763
	step [132/196], loss=7.7552
	step [133/196], loss=7.6580
	step [134/196], loss=6.5270
	step [135/196], loss=8.0351
	step [136/196], loss=6.8473
	step [137/196], loss=6.8111
	step [138/196], loss=7.9890
	step [139/196], loss=6.5289
	step [140/196], loss=7.6777
	step [141/196], loss=6.5748
	step [142/196], loss=6.3863
	step [143/196], loss=6.8748
	step [144/196], loss=6.5991
	step [145/196], loss=8.1053
	step [146/196], loss=5.6951
	step [147/196], loss=7.0289
	step [148/196], loss=6.8493
	step [149/196], loss=6.3711
	step [150/196], loss=6.6872
	step [151/196], loss=5.9631
	step [152/196], loss=7.7702
	step [153/196], loss=6.8595
	step [154/196], loss=5.4273
	step [155/196], loss=6.2144
	step [156/196], loss=6.6257
	step [157/196], loss=7.2786
	step [158/196], loss=8.4222
	step [159/196], loss=5.9406
	step [160/196], loss=7.7310
	step [161/196], loss=6.6251
	step [162/196], loss=5.8327
	step [163/196], loss=6.8056
	step [164/196], loss=7.7444
	step [165/196], loss=7.7382
	step [166/196], loss=5.7142
	step [167/196], loss=7.5083
	step [168/196], loss=7.0372
	step [169/196], loss=6.9291
	step [170/196], loss=6.9609
	step [171/196], loss=7.9234
	step [172/196], loss=5.5862
	step [173/196], loss=6.2756
	step [174/196], loss=7.6576
	step [175/196], loss=7.6811
	step [176/196], loss=6.4762
	step [177/196], loss=7.2735
	step [178/196], loss=9.3723
	step [179/196], loss=6.8475
	step [180/196], loss=6.2718
	step [181/196], loss=6.1436
	step [182/196], loss=7.4503
	step [183/196], loss=6.3502
	step [184/196], loss=5.7377
	step [185/196], loss=5.8040
	step [186/196], loss=7.4474
	step [187/196], loss=7.1902
	step [188/196], loss=8.4429
	step [189/196], loss=7.4165
	step [190/196], loss=8.8511
	step [191/196], loss=8.3805
	step [192/196], loss=7.4133
	step [193/196], loss=6.9412
	step [194/196], loss=8.2340
	step [195/196], loss=7.9593
	step [196/196], loss=3.0282
	Evaluating
	loss=0.0233, precision=0.2267, recall=0.9863, f1=0.7387
Training finished
best_f1: 0.7396665763383367
directing: Y rim_enhanced: False test_id 0
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15956 # image files with weight 15917
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4127 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_two/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/332], loss=328.9330
	step [2/332], loss=185.2969
	step [3/332], loss=138.8310
	step [4/332], loss=130.4705
	step [5/332], loss=122.2015
	step [6/332], loss=119.9641
	step [7/332], loss=115.9159
	step [8/332], loss=114.9267
	step [9/332], loss=110.6025
	step [10/332], loss=111.6903
	step [11/332], loss=108.4403
	step [12/332], loss=106.7291
	step [13/332], loss=107.3483
	step [14/332], loss=103.5986
	step [15/332], loss=105.0542
	step [16/332], loss=102.0434
	step [17/332], loss=100.2915
	step [18/332], loss=103.3130
	step [19/332], loss=101.7922
	step [20/332], loss=100.2295
	step [21/332], loss=99.8369
	step [22/332], loss=96.9918
	step [23/332], loss=97.2209
	step [24/332], loss=97.1784
	step [25/332], loss=96.9840
	step [26/332], loss=93.9668
	step [27/332], loss=94.8617
	step [28/332], loss=94.5518
	step [29/332], loss=93.5872
	step [30/332], loss=91.8068
	step [31/332], loss=91.6328
	step [32/332], loss=89.0709
	step [33/332], loss=88.8630
	step [34/332], loss=88.6535
	step [35/332], loss=88.5462
	step [36/332], loss=86.1625
	step [37/332], loss=86.5093
	step [38/332], loss=83.9375
	step [39/332], loss=86.7400
	step [40/332], loss=89.0537
	step [41/332], loss=87.8155
	step [42/332], loss=84.0582
	step [43/332], loss=87.5045
	step [44/332], loss=85.7340
	step [45/332], loss=82.9969
	step [46/332], loss=85.4271
	step [47/332], loss=81.2878
	step [48/332], loss=81.9714
	step [49/332], loss=79.9134
	step [50/332], loss=83.3150
	step [51/332], loss=82.1050
	step [52/332], loss=79.2768
	step [53/332], loss=80.8908
	step [54/332], loss=79.4886
	step [55/332], loss=80.9797
	step [56/332], loss=78.0574
	step [57/332], loss=77.1608
	step [58/332], loss=76.3860
	step [59/332], loss=75.6554
	step [60/332], loss=76.6035
	step [61/332], loss=76.3981
	step [62/332], loss=74.3919
	step [63/332], loss=75.3685
	step [64/332], loss=74.4935
	step [65/332], loss=76.0974
	step [66/332], loss=75.9360
	step [67/332], loss=73.5925
	step [68/332], loss=75.6567
	step [69/332], loss=74.6733
	step [70/332], loss=73.5808
	step [71/332], loss=74.6209
	step [72/332], loss=74.2183
	step [73/332], loss=74.5998
	step [74/332], loss=74.3554
	step [75/332], loss=76.1512
	step [76/332], loss=72.3583
	step [77/332], loss=75.4606
	step [78/332], loss=74.3151
	step [79/332], loss=71.9818
	step [80/332], loss=72.7710
	step [81/332], loss=71.1483
	step [82/332], loss=72.5491
	step [83/332], loss=69.3424
	step [84/332], loss=70.4400
	step [85/332], loss=68.6648
	step [86/332], loss=69.9025
	step [87/332], loss=73.3255
	step [88/332], loss=72.1410
	step [89/332], loss=72.0443
	step [90/332], loss=67.8123
	step [91/332], loss=68.6554
	step [92/332], loss=69.5319
	step [93/332], loss=68.8392
	step [94/332], loss=69.1613
	step [95/332], loss=68.4806
	step [96/332], loss=68.2280
	step [97/332], loss=66.1104
	step [98/332], loss=70.5758
	step [99/332], loss=66.7821
	step [100/332], loss=68.6514
	step [101/332], loss=64.6704
	step [102/332], loss=69.0025
	step [103/332], loss=66.2337
	step [104/332], loss=66.7814
	step [105/332], loss=69.5330
	step [106/332], loss=66.5079
	step [107/332], loss=65.7330
	step [108/332], loss=66.2442
	step [109/332], loss=65.9945
	step [110/332], loss=64.5391
	step [111/332], loss=70.7325
	step [112/332], loss=63.6096
	step [113/332], loss=64.1199
	step [114/332], loss=64.9386
	step [115/332], loss=63.5385
	step [116/332], loss=62.7698
	step [117/332], loss=62.9777
	step [118/332], loss=64.4313
	step [119/332], loss=63.9051
	step [120/332], loss=63.8610
	step [121/332], loss=61.6619
	step [122/332], loss=65.3829
	step [123/332], loss=62.7171
	step [124/332], loss=64.9296
	step [125/332], loss=62.3416
	step [126/332], loss=61.3141
	step [127/332], loss=63.8517
	step [128/332], loss=60.1392
	step [129/332], loss=63.5246
	step [130/332], loss=60.8100
	step [131/332], loss=61.4754
	step [132/332], loss=62.6481
	step [133/332], loss=62.4990
	step [134/332], loss=61.7839
	step [135/332], loss=61.5155
	step [136/332], loss=61.3092
	step [137/332], loss=60.6931
	step [138/332], loss=61.4067
	step [139/332], loss=58.6534
	step [140/332], loss=59.0151
	step [141/332], loss=59.0203
	step [142/332], loss=60.1082
	step [143/332], loss=61.0341
	step [144/332], loss=60.5978
	step [145/332], loss=60.0733
	step [146/332], loss=61.4490
	step [147/332], loss=59.8022
	step [148/332], loss=60.0756
	step [149/332], loss=59.0651
	step [150/332], loss=59.7535
	step [151/332], loss=57.1969
	step [152/332], loss=58.8888
	step [153/332], loss=58.8281
	step [154/332], loss=62.7519
	step [155/332], loss=60.3996
	step [156/332], loss=59.5912
	step [157/332], loss=57.7185
	step [158/332], loss=59.8186
	step [159/332], loss=58.1478
	step [160/332], loss=56.9016
	step [161/332], loss=57.4027
	step [162/332], loss=57.8857
	step [163/332], loss=59.3348
	step [164/332], loss=60.4687
	step [165/332], loss=59.0608
	step [166/332], loss=59.1828
	step [167/332], loss=56.2278
	step [168/332], loss=58.4749
	step [169/332], loss=56.5042
	step [170/332], loss=56.4915
	step [171/332], loss=60.3248
	step [172/332], loss=56.8721
	step [173/332], loss=58.1181
	step [174/332], loss=58.2728
	step [175/332], loss=59.4737
	step [176/332], loss=56.9544
	step [177/332], loss=58.1567
	step [178/332], loss=54.4709
	step [179/332], loss=57.6701
	step [180/332], loss=56.1084
	step [181/332], loss=54.6987
	step [182/332], loss=59.3594
	step [183/332], loss=58.7544
	step [184/332], loss=56.6874
	step [185/332], loss=55.5728
	step [186/332], loss=55.7401
	step [187/332], loss=54.3990
	step [188/332], loss=57.4709
	step [189/332], loss=52.8720
	step [190/332], loss=55.6670
	step [191/332], loss=57.1758
	step [192/332], loss=55.2120
	step [193/332], loss=54.2700
	step [194/332], loss=54.5629
	step [195/332], loss=54.6840
	step [196/332], loss=53.0382
	step [197/332], loss=55.6685
	step [198/332], loss=54.2456
	step [199/332], loss=54.9419
	step [200/332], loss=52.6494
	step [201/332], loss=54.4412
	step [202/332], loss=54.8668
	step [203/332], loss=55.1077
	step [204/332], loss=53.4836
	step [205/332], loss=56.0142
	step [206/332], loss=54.4702
	step [207/332], loss=56.5710
	step [208/332], loss=52.5401
	step [209/332], loss=56.4232
	step [210/332], loss=55.5755
	step [211/332], loss=56.0654
	step [212/332], loss=53.0730
	step [213/332], loss=52.9373
	step [214/332], loss=52.8876
	step [215/332], loss=52.0670
	step [216/332], loss=55.3418
	step [217/332], loss=51.9438
	step [218/332], loss=53.3694
	step [219/332], loss=52.5547
	step [220/332], loss=53.3847
	step [221/332], loss=52.2359
	step [222/332], loss=52.2270
	step [223/332], loss=53.6141
	step [224/332], loss=52.1389
	step [225/332], loss=54.6884
	step [226/332], loss=51.3903
	step [227/332], loss=54.1674
	step [228/332], loss=52.6423
	step [229/332], loss=50.9853
	step [230/332], loss=51.6940
	step [231/332], loss=52.3447
	step [232/332], loss=52.6254
	step [233/332], loss=53.0800
	step [234/332], loss=50.5915
	step [235/332], loss=50.5548
	step [236/332], loss=51.2458
	step [237/332], loss=51.4042
	step [238/332], loss=49.0537
	step [239/332], loss=49.8725
	step [240/332], loss=50.4024
	step [241/332], loss=49.3227
	step [242/332], loss=52.2592
	step [243/332], loss=50.1024
	step [244/332], loss=50.1202
	step [245/332], loss=50.0706
	step [246/332], loss=48.8322
	step [247/332], loss=49.1127
	step [248/332], loss=48.4887
	step [249/332], loss=50.3042
	step [250/332], loss=49.5491
	step [251/332], loss=49.4302
	step [252/332], loss=53.6435
	step [253/332], loss=49.6021
	step [254/332], loss=49.2058
	step [255/332], loss=53.2568
	step [256/332], loss=49.0427
	step [257/332], loss=49.4334
	step [258/332], loss=51.2125
	step [259/332], loss=48.1946
	step [260/332], loss=47.5163
	step [261/332], loss=48.9187
	step [262/332], loss=49.3972
	step [263/332], loss=49.6201
	step [264/332], loss=48.7906
	step [265/332], loss=48.1721
	step [266/332], loss=47.5647
	step [267/332], loss=47.7588
	step [268/332], loss=50.5161
	step [269/332], loss=47.3639
	step [270/332], loss=48.5584
	step [271/332], loss=47.4350
	step [272/332], loss=49.2028
	step [273/332], loss=48.5429
	step [274/332], loss=48.8720
	step [275/332], loss=48.5095
	step [276/332], loss=46.4199
	step [277/332], loss=47.3149
	step [278/332], loss=49.0844
	step [279/332], loss=47.5492
	step [280/332], loss=48.5204
	step [281/332], loss=48.3575
	step [282/332], loss=47.6998
	step [283/332], loss=44.2643
	step [284/332], loss=49.1603
	step [285/332], loss=47.7443
	step [286/332], loss=47.4541
	step [287/332], loss=46.7205
	step [288/332], loss=49.4675
	step [289/332], loss=47.6077
	step [290/332], loss=46.4508
	step [291/332], loss=46.2798
	step [292/332], loss=48.6234
	step [293/332], loss=50.7314
	step [294/332], loss=48.2782
	step [295/332], loss=46.6861
	step [296/332], loss=48.0081
	step [297/332], loss=46.0614
	step [298/332], loss=49.6113
	step [299/332], loss=48.9491
	step [300/332], loss=45.7834
	step [301/332], loss=47.6369
	step [302/332], loss=45.7159
	step [303/332], loss=45.7298
	step [304/332], loss=45.9171
	step [305/332], loss=45.9038
	step [306/332], loss=46.0321
	step [307/332], loss=45.9610
	step [308/332], loss=45.4395
	step [309/332], loss=44.7714
	step [310/332], loss=47.6566
	step [311/332], loss=43.0303
	step [312/332], loss=44.9387
	step [313/332], loss=45.5573
	step [314/332], loss=46.9882
	step [315/332], loss=42.9280
	step [316/332], loss=47.5164
	step [317/332], loss=44.4948
	step [318/332], loss=46.1891
	step [319/332], loss=43.1022
	step [320/332], loss=45.4039
	step [321/332], loss=45.9780
	step [322/332], loss=45.3819
	step [323/332], loss=46.3987
	step [324/332], loss=46.4166
	step [325/332], loss=44.2460
	step [326/332], loss=46.3698
	step [327/332], loss=46.3717
	step [328/332], loss=42.9701
	step [329/332], loss=44.7444
	step [330/332], loss=46.2159
	step [331/332], loss=43.5114
	step [332/332], loss=26.5680
	Evaluating
	loss=0.2114, precision=0.0820, recall=0.9924, f1=0.4702
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/332], loss=44.1843
	step [2/332], loss=43.2677
	step [3/332], loss=43.0704
	step [4/332], loss=42.5985
	step [5/332], loss=44.7836
	step [6/332], loss=44.0082
	step [7/332], loss=41.5137
	step [8/332], loss=43.6073
	step [9/332], loss=48.8834
	step [10/332], loss=40.4278
	step [11/332], loss=42.4674
	step [12/332], loss=43.3424
	step [13/332], loss=45.7191
	step [14/332], loss=42.7020
	step [15/332], loss=45.0661
	step [16/332], loss=43.3390
	step [17/332], loss=42.5811
	step [18/332], loss=43.6830
	step [19/332], loss=45.7844
	step [20/332], loss=44.0597
	step [21/332], loss=46.1514
	step [22/332], loss=43.7638
	step [23/332], loss=41.6255
	step [24/332], loss=42.6371
	step [25/332], loss=41.0433
	step [26/332], loss=42.2718
	step [27/332], loss=44.2634
	step [28/332], loss=45.4352
	step [29/332], loss=44.2654
	step [30/332], loss=42.7889
	step [31/332], loss=41.0727
	step [32/332], loss=42.9519
	step [33/332], loss=39.8240
	step [34/332], loss=41.2159
	step [35/332], loss=44.2665
	step [36/332], loss=42.5084
	step [37/332], loss=43.0583
	step [38/332], loss=41.4647
	step [39/332], loss=44.3093
	step [40/332], loss=42.7251
	step [41/332], loss=43.5307
	step [42/332], loss=40.8735
	step [43/332], loss=41.5680
	step [44/332], loss=40.4288
	step [45/332], loss=42.2339
	step [46/332], loss=41.8512
	step [47/332], loss=42.1895
	step [48/332], loss=42.9398
	step [49/332], loss=41.0650
	step [50/332], loss=40.5398
	step [51/332], loss=40.2947
	step [52/332], loss=41.5884
	step [53/332], loss=41.8361
	step [54/332], loss=37.2480
	step [55/332], loss=38.7473
	step [56/332], loss=42.4586
	step [57/332], loss=40.8731
	step [58/332], loss=44.9870
	step [59/332], loss=39.6789
	step [60/332], loss=39.5856
	step [61/332], loss=39.9309
	step [62/332], loss=41.6455
	step [63/332], loss=40.8881
	step [64/332], loss=38.4435
	step [65/332], loss=39.7818
	step [66/332], loss=41.0718
	step [67/332], loss=42.2718
	step [68/332], loss=43.0847
	step [69/332], loss=40.4039
	step [70/332], loss=42.1846
	step [71/332], loss=38.0286
	step [72/332], loss=40.5461
	step [73/332], loss=41.9279
	step [74/332], loss=41.4836
	step [75/332], loss=39.7307
	step [76/332], loss=41.1504
	step [77/332], loss=40.2120
	step [78/332], loss=39.9755
	step [79/332], loss=39.1476
	step [80/332], loss=41.2687
	step [81/332], loss=41.3311
	step [82/332], loss=36.8587
	step [83/332], loss=40.1163
	step [84/332], loss=38.6678
	step [85/332], loss=44.4963
	step [86/332], loss=38.4635
	step [87/332], loss=38.2430
	step [88/332], loss=38.9916
	step [89/332], loss=38.7363
	step [90/332], loss=39.1070
	step [91/332], loss=40.7353
	step [92/332], loss=39.1294
	step [93/332], loss=39.8128
	step [94/332], loss=37.1934
	step [95/332], loss=39.4143
	step [96/332], loss=38.5013
	step [97/332], loss=40.5477
	step [98/332], loss=40.3881
	step [99/332], loss=38.9651
	step [100/332], loss=38.6680
	step [101/332], loss=38.3356
	step [102/332], loss=38.2775
	step [103/332], loss=38.5126
	step [104/332], loss=39.0118
	step [105/332], loss=37.7567
	step [106/332], loss=36.2351
	step [107/332], loss=37.1723
	step [108/332], loss=37.1776
	step [109/332], loss=37.4315
	step [110/332], loss=34.7424
	step [111/332], loss=38.6680
	step [112/332], loss=39.9206
	step [113/332], loss=36.8895
	step [114/332], loss=38.3355
	step [115/332], loss=37.4281
	step [116/332], loss=40.6692
	step [117/332], loss=37.3679
	step [118/332], loss=35.0226
	step [119/332], loss=36.6330
	step [120/332], loss=36.4134
	step [121/332], loss=36.8518
	step [122/332], loss=37.5628
	step [123/332], loss=37.6235
	step [124/332], loss=35.1786
	step [125/332], loss=36.8822
	step [126/332], loss=38.0615
	step [127/332], loss=37.8015
	step [128/332], loss=35.8494
	step [129/332], loss=35.8456
	step [130/332], loss=37.1995
	step [131/332], loss=36.5202
	step [132/332], loss=34.7859
	step [133/332], loss=38.4867
	step [134/332], loss=37.8389
	step [135/332], loss=36.0679
	step [136/332], loss=36.7664
	step [137/332], loss=37.5719
	step [138/332], loss=37.1938
	step [139/332], loss=35.1989
	step [140/332], loss=35.9198
	step [141/332], loss=37.1279
	step [142/332], loss=38.9976
	step [143/332], loss=39.2423
	step [144/332], loss=36.0159
	step [145/332], loss=36.8804
	step [146/332], loss=35.7617
	step [147/332], loss=36.4792
	step [148/332], loss=37.2863
	step [149/332], loss=35.6144
	step [150/332], loss=37.9130
	step [151/332], loss=36.7314
	step [152/332], loss=36.5368
	step [153/332], loss=35.6763
	step [154/332], loss=34.9809
	step [155/332], loss=36.8774
	step [156/332], loss=34.8870
	step [157/332], loss=35.0501
	step [158/332], loss=33.7377
	step [159/332], loss=35.0927
	step [160/332], loss=36.4293
	step [161/332], loss=34.8639
	step [162/332], loss=37.5690
	step [163/332], loss=36.0716
	step [164/332], loss=36.5867
	step [165/332], loss=35.3322
	step [166/332], loss=37.3289
	step [167/332], loss=34.2443
	step [168/332], loss=35.3179
	step [169/332], loss=34.7492
	step [170/332], loss=36.4403
	step [171/332], loss=36.4572
	step [172/332], loss=36.3418
	step [173/332], loss=34.7360
	step [174/332], loss=33.4793
	step [175/332], loss=34.3015
	step [176/332], loss=33.5938
	step [177/332], loss=34.7946
	step [178/332], loss=34.1388
	step [179/332], loss=34.5394
	step [180/332], loss=33.6465
	step [181/332], loss=36.4708
	step [182/332], loss=33.6839
	step [183/332], loss=37.3398
	step [184/332], loss=35.8569
	step [185/332], loss=34.3859
	step [186/332], loss=33.2464
	step [187/332], loss=34.8249
	step [188/332], loss=33.6489
	step [189/332], loss=33.9180
	step [190/332], loss=34.1203
	step [191/332], loss=31.8142
	step [192/332], loss=36.0081
	step [193/332], loss=33.0775
	step [194/332], loss=34.8545
	step [195/332], loss=32.3553
	step [196/332], loss=32.7388
	step [197/332], loss=32.7315
	step [198/332], loss=36.4366
	step [199/332], loss=33.4033
	step [200/332], loss=34.1409
	step [201/332], loss=33.1630
	step [202/332], loss=34.0005
	step [203/332], loss=34.3903
	step [204/332], loss=32.6781
	step [205/332], loss=33.5289
	step [206/332], loss=36.5553
	step [207/332], loss=35.4172
	step [208/332], loss=32.1470
	step [209/332], loss=33.3941
	step [210/332], loss=35.4897
	step [211/332], loss=35.1746
	step [212/332], loss=31.8490
	step [213/332], loss=33.2507
	step [214/332], loss=34.0024
	step [215/332], loss=33.5897
	step [216/332], loss=30.9795
	step [217/332], loss=34.5209
	step [218/332], loss=32.6037
	step [219/332], loss=33.2995
	step [220/332], loss=33.1765
	step [221/332], loss=34.1493
	step [222/332], loss=33.6384
	step [223/332], loss=32.3446
	step [224/332], loss=33.0711
	step [225/332], loss=30.7810
	step [226/332], loss=32.7762
	step [227/332], loss=29.9781
	step [228/332], loss=35.3903
	step [229/332], loss=33.0690
	step [230/332], loss=31.1065
	step [231/332], loss=31.5546
	step [232/332], loss=32.1866
	step [233/332], loss=30.6425
	step [234/332], loss=33.0962
	step [235/332], loss=33.4348
	step [236/332], loss=32.4763
	step [237/332], loss=31.9529
	step [238/332], loss=30.9551
	step [239/332], loss=34.0662
	step [240/332], loss=34.4422
	step [241/332], loss=31.8207
	step [242/332], loss=34.1654
	step [243/332], loss=31.9317
	step [244/332], loss=30.6297
	step [245/332], loss=33.3455
	step [246/332], loss=31.0205
	step [247/332], loss=31.9319
	step [248/332], loss=31.8354
	step [249/332], loss=31.7424
	step [250/332], loss=29.8947
	step [251/332], loss=31.7507
	step [252/332], loss=30.2878
	step [253/332], loss=33.0519
	step [254/332], loss=33.2064
	step [255/332], loss=31.5821
	step [256/332], loss=33.5574
	step [257/332], loss=30.4569
	step [258/332], loss=33.5242
	step [259/332], loss=31.6884
	step [260/332], loss=31.5673
	step [261/332], loss=32.2277
	step [262/332], loss=30.3663
	step [263/332], loss=31.1230
	step [264/332], loss=30.2390
	step [265/332], loss=33.0398
	step [266/332], loss=29.4026
	step [267/332], loss=30.9650
	step [268/332], loss=31.2169
	step [269/332], loss=29.6709
	step [270/332], loss=31.2711
	step [271/332], loss=31.4216
	step [272/332], loss=30.8390
	step [273/332], loss=28.7274
	step [274/332], loss=30.6631
	step [275/332], loss=29.2115
	step [276/332], loss=28.4471
	step [277/332], loss=30.2989
	step [278/332], loss=30.9292
	step [279/332], loss=31.1688
	step [280/332], loss=31.1190
	step [281/332], loss=29.4525
	step [282/332], loss=32.0194
	step [283/332], loss=31.4420
	step [284/332], loss=31.6146
	step [285/332], loss=29.9200
	step [286/332], loss=30.8722
	step [287/332], loss=30.2371
	step [288/332], loss=29.0604
	step [289/332], loss=29.7841
	step [290/332], loss=30.2088
	step [291/332], loss=31.1139
	step [292/332], loss=30.1284
	step [293/332], loss=30.9529
	step [294/332], loss=28.6038
	step [295/332], loss=29.2270
	step [296/332], loss=27.9211
	step [297/332], loss=28.4106
	step [298/332], loss=30.4324
	step [299/332], loss=32.5497
	step [300/332], loss=29.9151
	step [301/332], loss=28.7476
	step [302/332], loss=29.4186
	step [303/332], loss=28.1703
	step [304/332], loss=31.1201
	step [305/332], loss=31.0612
	step [306/332], loss=29.7640
	step [307/332], loss=30.2232
	step [308/332], loss=30.6040
	step [309/332], loss=29.4018
	step [310/332], loss=30.6268
	step [311/332], loss=27.6382
	step [312/332], loss=30.3133
	step [313/332], loss=29.5450
	step [314/332], loss=28.3917
	step [315/332], loss=30.0426
	step [316/332], loss=30.2199
	step [317/332], loss=28.5465
	step [318/332], loss=29.5772
	step [319/332], loss=28.6800
	step [320/332], loss=28.3182
	step [321/332], loss=27.8727
	step [322/332], loss=30.6014
	step [323/332], loss=28.2172
	step [324/332], loss=30.1647
	step [325/332], loss=29.6041
	step [326/332], loss=26.5481
	step [327/332], loss=28.6298
	step [328/332], loss=28.6343
	step [329/332], loss=27.7777
	step [330/332], loss=27.9004
	step [331/332], loss=30.4892
	step [332/332], loss=17.9584
	Evaluating
	loss=0.1282, precision=0.1055, recall=0.9923, f1=0.5392
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/332], loss=28.4870
	step [2/332], loss=31.8189
	step [3/332], loss=28.2961
	step [4/332], loss=29.8408
	step [5/332], loss=26.9140
	step [6/332], loss=28.3277
	step [7/332], loss=27.5343
	step [8/332], loss=29.6610
	step [9/332], loss=29.5351
	step [10/332], loss=28.2340
	step [11/332], loss=30.1395
	step [12/332], loss=29.2601
	step [13/332], loss=28.5435
	step [14/332], loss=25.9986
	step [15/332], loss=30.4377
	step [16/332], loss=29.1966
	step [17/332], loss=30.2126
	step [18/332], loss=28.8352
	step [19/332], loss=27.3555
	step [20/332], loss=26.7624
	step [21/332], loss=28.0077
	step [22/332], loss=28.8246
	step [23/332], loss=33.4564
	step [24/332], loss=28.5647
	step [25/332], loss=31.4351
	step [26/332], loss=28.2561
	step [27/332], loss=29.7058
	step [28/332], loss=28.9102
	step [29/332], loss=25.7869
	step [30/332], loss=27.5572
	step [31/332], loss=26.5503
	step [32/332], loss=28.3757
	step [33/332], loss=29.4031
	step [34/332], loss=29.6929
	step [35/332], loss=27.4707
	step [36/332], loss=30.1699
	step [37/332], loss=29.4147
	step [38/332], loss=29.1601
	step [39/332], loss=27.3045
	step [40/332], loss=29.1460
	step [41/332], loss=28.0533
	step [42/332], loss=27.8123
	step [43/332], loss=26.7116
	step [44/332], loss=24.9679
	step [45/332], loss=27.1616
	step [46/332], loss=26.8431
	step [47/332], loss=26.0639
	step [48/332], loss=26.4834
	step [49/332], loss=26.8534
	step [50/332], loss=30.5517
	step [51/332], loss=25.3587
	step [52/332], loss=27.9735
	step [53/332], loss=27.5224
	step [54/332], loss=27.3013
	step [55/332], loss=28.3035
	step [56/332], loss=28.7355
	step [57/332], loss=27.5630
	step [58/332], loss=28.1624
	step [59/332], loss=27.8049
	step [60/332], loss=27.8143
	step [61/332], loss=28.9689
	step [62/332], loss=25.9641
	step [63/332], loss=24.8690
	step [64/332], loss=28.1266
	step [65/332], loss=24.2846
	step [66/332], loss=27.3609
	step [67/332], loss=25.8979
	step [68/332], loss=24.3563
	step [69/332], loss=24.3120
	step [70/332], loss=28.2548
	step [71/332], loss=29.6725
	step [72/332], loss=27.6359
	step [73/332], loss=29.3196
	step [74/332], loss=25.3298
	step [75/332], loss=27.8210
	step [76/332], loss=25.8006
	step [77/332], loss=28.2533
	step [78/332], loss=28.8684
	step [79/332], loss=25.7208
	step [80/332], loss=29.1517
	step [81/332], loss=27.5363
	step [82/332], loss=26.2584
	step [83/332], loss=25.6948
	step [84/332], loss=27.0314
	step [85/332], loss=26.1287
	step [86/332], loss=26.5739
	step [87/332], loss=27.4515
	step [88/332], loss=25.7458
	step [89/332], loss=25.3184
	step [90/332], loss=25.4804
	step [91/332], loss=25.5118
	step [92/332], loss=24.7660
	step [93/332], loss=25.1975
	step [94/332], loss=24.5095
	step [95/332], loss=27.1017
	step [96/332], loss=24.9118
	step [97/332], loss=28.4074
	step [98/332], loss=25.3287
	step [99/332], loss=28.1442
	step [100/332], loss=25.4651
	step [101/332], loss=24.1900
	step [102/332], loss=25.9141
	step [103/332], loss=26.0730
	step [104/332], loss=23.9595
	step [105/332], loss=24.2532
	step [106/332], loss=24.7901
	step [107/332], loss=26.1232
	step [108/332], loss=26.0775
	step [109/332], loss=25.5672
	step [110/332], loss=27.1541
	step [111/332], loss=24.1991
	step [112/332], loss=25.5161
	step [113/332], loss=25.2450
	step [114/332], loss=24.7187
	step [115/332], loss=24.1118
	step [116/332], loss=26.1488
	step [117/332], loss=23.5805
	step [118/332], loss=25.5040
	step [119/332], loss=25.7390
	step [120/332], loss=23.9230
	step [121/332], loss=25.0969
	step [122/332], loss=26.9089
	step [123/332], loss=27.0414
	step [124/332], loss=22.4678
	step [125/332], loss=25.1338
	step [126/332], loss=25.7477
	step [127/332], loss=24.0797
	step [128/332], loss=24.5892
	step [129/332], loss=26.2184
	step [130/332], loss=23.4250
	step [131/332], loss=24.3535
	step [132/332], loss=23.9087
	step [133/332], loss=25.2292
	step [134/332], loss=24.3466
	step [135/332], loss=23.4054
	step [136/332], loss=22.5341
	step [137/332], loss=27.0416
	step [138/332], loss=25.2544
	step [139/332], loss=23.4250
	step [140/332], loss=23.7267
	step [141/332], loss=25.5936
	step [142/332], loss=26.7252
	step [143/332], loss=24.6184
	step [144/332], loss=23.6236
	step [145/332], loss=26.6390
	step [146/332], loss=24.8455
	step [147/332], loss=25.2118
	step [148/332], loss=23.5967
	step [149/332], loss=25.0014
	step [150/332], loss=24.7021
	step [151/332], loss=25.2647
	step [152/332], loss=23.8117
	step [153/332], loss=24.9354
	step [154/332], loss=24.1318
	step [155/332], loss=23.3223
	step [156/332], loss=28.3758
	step [157/332], loss=22.5677
	step [158/332], loss=25.6221
	step [159/332], loss=25.4771
	step [160/332], loss=26.7801
	step [161/332], loss=22.9048
	step [162/332], loss=25.7118
	step [163/332], loss=23.7231
	step [164/332], loss=23.7225
	step [165/332], loss=25.2474
	step [166/332], loss=23.0842
	step [167/332], loss=28.0084
	step [168/332], loss=24.8588
	step [169/332], loss=22.5449
	step [170/332], loss=23.7812
	step [171/332], loss=22.1616
	step [172/332], loss=23.0616
	step [173/332], loss=25.6641
	step [174/332], loss=23.4340
	step [175/332], loss=25.8440
	step [176/332], loss=24.8813
	step [177/332], loss=25.1404
	step [178/332], loss=22.7652
	step [179/332], loss=21.0606
	step [180/332], loss=25.0458
	step [181/332], loss=26.1525
	step [182/332], loss=22.9900
	step [183/332], loss=24.3048
	step [184/332], loss=25.2949
	step [185/332], loss=22.4049
	step [186/332], loss=23.5855
	step [187/332], loss=22.7979
	step [188/332], loss=25.3139
	step [189/332], loss=22.9724
	step [190/332], loss=25.3194
	step [191/332], loss=21.9759
	step [192/332], loss=26.9835
	step [193/332], loss=22.7627
	step [194/332], loss=23.0805
	step [195/332], loss=23.5909
	step [196/332], loss=23.7972
	step [197/332], loss=21.3139
	step [198/332], loss=21.9967
	step [199/332], loss=22.5193
	step [200/332], loss=23.6855
	step [201/332], loss=24.6719
	step [202/332], loss=25.1483
	step [203/332], loss=22.3822
	step [204/332], loss=25.3384
	step [205/332], loss=22.2960
	step [206/332], loss=23.0741
	step [207/332], loss=23.8086
	step [208/332], loss=23.3807
	step [209/332], loss=22.5058
	step [210/332], loss=21.7613
	step [211/332], loss=26.7165
	step [212/332], loss=24.6951
	step [213/332], loss=22.5952
	step [214/332], loss=23.6200
	step [215/332], loss=23.1835
	step [216/332], loss=23.8312
	step [217/332], loss=22.7034
	step [218/332], loss=23.2118
	step [219/332], loss=24.5872
	step [220/332], loss=24.0530
	step [221/332], loss=22.6793
	step [222/332], loss=23.6674
	step [223/332], loss=21.9707
	step [224/332], loss=22.4961
	step [225/332], loss=22.9138
	step [226/332], loss=25.9815
	step [227/332], loss=24.7531
	step [228/332], loss=23.6766
	step [229/332], loss=22.0797
	step [230/332], loss=22.2103
	step [231/332], loss=22.4250
	step [232/332], loss=22.7672
	step [233/332], loss=20.5404
	step [234/332], loss=20.2303
	step [235/332], loss=25.6799
	step [236/332], loss=23.9853
	step [237/332], loss=22.2693
	step [238/332], loss=21.1197
	step [239/332], loss=22.3563
	step [240/332], loss=20.4499
	step [241/332], loss=20.7673
	step [242/332], loss=25.5059
	step [243/332], loss=25.1386
	step [244/332], loss=23.3392
	step [245/332], loss=21.4746
	step [246/332], loss=21.7720
	step [247/332], loss=21.2494
	step [248/332], loss=21.1432
	step [249/332], loss=23.0120
	step [250/332], loss=25.1074
	step [251/332], loss=20.4328
	step [252/332], loss=23.8432
	step [253/332], loss=22.6986
	step [254/332], loss=21.1488
	step [255/332], loss=20.8330
	step [256/332], loss=22.2788
	step [257/332], loss=23.0330
	step [258/332], loss=23.2006
	step [259/332], loss=27.0749
	step [260/332], loss=23.1288
	step [261/332], loss=21.8939
	step [262/332], loss=23.5255
	step [263/332], loss=21.3880
	step [264/332], loss=23.6634
	step [265/332], loss=21.8044
	step [266/332], loss=22.2043
	step [267/332], loss=22.9893
	step [268/332], loss=21.0818
	step [269/332], loss=22.9805
	step [270/332], loss=23.4468
	step [271/332], loss=21.6201
	step [272/332], loss=21.3633
	step [273/332], loss=23.0158
	step [274/332], loss=23.3194
	step [275/332], loss=21.1506
	step [276/332], loss=21.1143
	step [277/332], loss=20.5424
	step [278/332], loss=22.0869
	step [279/332], loss=21.2752
	step [280/332], loss=21.4409
	step [281/332], loss=21.4307
	step [282/332], loss=21.6532
	step [283/332], loss=22.6545
	step [284/332], loss=22.4143
	step [285/332], loss=24.2838
	step [286/332], loss=21.2753
	step [287/332], loss=22.6707
	step [288/332], loss=22.9833
	step [289/332], loss=20.7787
	step [290/332], loss=21.0604
	step [291/332], loss=22.0291
	step [292/332], loss=23.3968
	step [293/332], loss=22.9999
	step [294/332], loss=22.3035
	step [295/332], loss=24.0767
	step [296/332], loss=20.5863
	step [297/332], loss=23.9564
	step [298/332], loss=21.1847
	step [299/332], loss=20.0330
	step [300/332], loss=21.1670
	step [301/332], loss=25.0972
	step [302/332], loss=22.9131
	step [303/332], loss=24.5637
	step [304/332], loss=19.7839
	step [305/332], loss=21.2844
	step [306/332], loss=21.7722
	step [307/332], loss=18.4232
	step [308/332], loss=22.3239
	step [309/332], loss=22.4557
	step [310/332], loss=20.3620
	step [311/332], loss=22.5249
	step [312/332], loss=19.5225
	step [313/332], loss=20.9297
	step [314/332], loss=21.3295
	step [315/332], loss=20.6796
	step [316/332], loss=23.9383
	step [317/332], loss=20.4756
	step [318/332], loss=18.5200
	step [319/332], loss=23.9171
	step [320/332], loss=20.4743
	step [321/332], loss=21.7552
	step [322/332], loss=21.6816
	step [323/332], loss=22.4997
	step [324/332], loss=24.9863
	step [325/332], loss=21.6903
	step [326/332], loss=19.7176
	step [327/332], loss=19.0525
	step [328/332], loss=20.7561
	step [329/332], loss=21.4475
	step [330/332], loss=20.5880
	step [331/332], loss=20.7869
	step [332/332], loss=13.5307
	Evaluating
	loss=0.0818, precision=0.1507, recall=0.9897, f1=0.6357
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/332], loss=21.7832
	step [2/332], loss=23.5278
	step [3/332], loss=22.2030
	step [4/332], loss=20.0131
	step [5/332], loss=22.7729
	step [6/332], loss=22.1919
	step [7/332], loss=21.4270
	step [8/332], loss=19.2573
	step [9/332], loss=22.7026
	step [10/332], loss=23.9735
	step [11/332], loss=20.4287
	step [12/332], loss=20.2365
	step [13/332], loss=24.9479
	step [14/332], loss=18.9203
	step [15/332], loss=18.7325
	step [16/332], loss=20.3950
	step [17/332], loss=21.4940
	step [18/332], loss=21.0234
	step [19/332], loss=20.4684
	step [20/332], loss=19.2199
	step [21/332], loss=20.9519
	step [22/332], loss=19.3707
	step [23/332], loss=19.6884
	step [24/332], loss=18.5450
	step [25/332], loss=21.1620
	step [26/332], loss=18.6399
	step [27/332], loss=20.4464
	step [28/332], loss=23.2194
	step [29/332], loss=20.7237
	step [30/332], loss=20.2755
	step [31/332], loss=18.3255
	step [32/332], loss=22.4331
	step [33/332], loss=22.7738
	step [34/332], loss=23.6859
	step [35/332], loss=19.6750
	step [36/332], loss=19.9758
	step [37/332], loss=19.8297
	step [38/332], loss=22.1188
	step [39/332], loss=18.3459
	step [40/332], loss=21.2936
	step [41/332], loss=23.0407
	step [42/332], loss=19.9758
	step [43/332], loss=19.3708
	step [44/332], loss=19.2067
	step [45/332], loss=20.5640
	step [46/332], loss=19.7524
	step [47/332], loss=19.9551
	step [48/332], loss=22.9361
	step [49/332], loss=17.9489
	step [50/332], loss=20.2209
	step [51/332], loss=18.7351
	step [52/332], loss=21.8873
	step [53/332], loss=20.2385
	step [54/332], loss=21.7691
	step [55/332], loss=19.6026
	step [56/332], loss=17.8039
	step [57/332], loss=18.8673
	step [58/332], loss=21.1958
	step [59/332], loss=23.6300
	step [60/332], loss=17.3665
	step [61/332], loss=20.6597
	step [62/332], loss=17.7839
	step [63/332], loss=19.6096
	step [64/332], loss=18.9945
	step [65/332], loss=18.5258
	step [66/332], loss=20.5260
	step [67/332], loss=21.1964
	step [68/332], loss=23.3269
	step [69/332], loss=18.9843
	step [70/332], loss=22.8301
	step [71/332], loss=19.3302
	step [72/332], loss=21.9233
	step [73/332], loss=18.6913
	step [74/332], loss=17.2907
	step [75/332], loss=19.2907
	step [76/332], loss=18.6851
	step [77/332], loss=18.8173
	step [78/332], loss=20.2356
	step [79/332], loss=21.1942
	step [80/332], loss=21.0560
	step [81/332], loss=22.4247
	step [82/332], loss=17.6985
	step [83/332], loss=19.1761
	step [84/332], loss=22.0495
	step [85/332], loss=22.0110
	step [86/332], loss=20.1248
	step [87/332], loss=18.5408
	step [88/332], loss=20.1352
	step [89/332], loss=18.4491
	step [90/332], loss=20.9335
	step [91/332], loss=18.9858
	step [92/332], loss=18.3942
	step [93/332], loss=19.6468
	step [94/332], loss=17.5733
	step [95/332], loss=18.4845
	step [96/332], loss=16.7332
	step [97/332], loss=19.3879
	step [98/332], loss=17.7033
	step [99/332], loss=19.1446
	step [100/332], loss=17.9311
	step [101/332], loss=19.8148
	step [102/332], loss=18.9236
	step [103/332], loss=19.0283
	step [104/332], loss=19.1434
	step [105/332], loss=19.7445
	step [106/332], loss=19.4991
	step [107/332], loss=19.8253
	step [108/332], loss=18.5998
	step [109/332], loss=20.2690
	step [110/332], loss=19.5505
	step [111/332], loss=20.8660
	step [112/332], loss=19.2643
	step [113/332], loss=18.4094
	step [114/332], loss=16.5807
	step [115/332], loss=18.6338
	step [116/332], loss=18.0094
	step [117/332], loss=16.8765
	step [118/332], loss=19.9282
	step [119/332], loss=19.9212
	step [120/332], loss=18.6537
	step [121/332], loss=22.7210
	step [122/332], loss=21.0526
	step [123/332], loss=20.6731
	step [124/332], loss=20.1359
	step [125/332], loss=21.6853
	step [126/332], loss=19.3575
	step [127/332], loss=19.9389
	step [128/332], loss=21.1741
	step [129/332], loss=17.7366
	step [130/332], loss=22.3519
	step [131/332], loss=21.8786
	step [132/332], loss=21.5291
	step [133/332], loss=20.8275
	step [134/332], loss=22.6548
	step [135/332], loss=17.1987
	step [136/332], loss=18.3619
	step [137/332], loss=18.9793
	step [138/332], loss=19.4118
	step [139/332], loss=18.3541
	step [140/332], loss=18.5355
	step [141/332], loss=17.4937
	step [142/332], loss=18.6751
	step [143/332], loss=19.1841
	step [144/332], loss=18.9259
	step [145/332], loss=18.5038
	step [146/332], loss=20.2450
	step [147/332], loss=16.9294
	step [148/332], loss=18.0913
	step [149/332], loss=15.9804
	step [150/332], loss=21.1249
	step [151/332], loss=16.6277
	step [152/332], loss=17.4527
	step [153/332], loss=19.8563
	step [154/332], loss=16.6295
	step [155/332], loss=20.0571
	step [156/332], loss=18.4493
	step [157/332], loss=18.5619
	step [158/332], loss=17.8858
	step [159/332], loss=17.5727
	step [160/332], loss=18.9875
	step [161/332], loss=17.6761
	step [162/332], loss=18.6423
	step [163/332], loss=20.5737
	step [164/332], loss=16.9876
	step [165/332], loss=20.4570
	step [166/332], loss=18.6979
	step [167/332], loss=17.7897
	step [168/332], loss=17.0758
	step [169/332], loss=19.5131
	step [170/332], loss=16.9980
	step [171/332], loss=16.6463
	step [172/332], loss=16.9660
	step [173/332], loss=18.6536
	step [174/332], loss=19.3118
	step [175/332], loss=17.9352
	step [176/332], loss=17.7748
	step [177/332], loss=16.9767
	step [178/332], loss=22.7302
	step [179/332], loss=18.5823
	step [180/332], loss=18.2548
	step [181/332], loss=17.8263
	step [182/332], loss=17.5161
	step [183/332], loss=17.9810
	step [184/332], loss=18.3303
	step [185/332], loss=17.1098
	step [186/332], loss=17.2755
	step [187/332], loss=18.8919
	step [188/332], loss=17.0390
	step [189/332], loss=16.2208
	step [190/332], loss=18.4146
	step [191/332], loss=17.4089
	step [192/332], loss=17.9753
	step [193/332], loss=18.3045
	step [194/332], loss=17.8819
	step [195/332], loss=20.4954
	step [196/332], loss=17.6013
	step [197/332], loss=19.9352
	step [198/332], loss=17.0165
	step [199/332], loss=19.6208
	step [200/332], loss=18.3818
	step [201/332], loss=20.8006
	step [202/332], loss=20.9788
	step [203/332], loss=18.3038
	step [204/332], loss=18.7016
	step [205/332], loss=18.3541
	step [206/332], loss=17.9191
	step [207/332], loss=22.6812
	step [208/332], loss=19.6089
	step [209/332], loss=16.6845
	step [210/332], loss=17.4504
	step [211/332], loss=19.5011
	step [212/332], loss=18.9416
	step [213/332], loss=18.8264
	step [214/332], loss=18.1941
	step [215/332], loss=16.0034
	step [216/332], loss=20.6406
	step [217/332], loss=17.5446
	step [218/332], loss=15.4462
	step [219/332], loss=19.2210
	step [220/332], loss=18.1938
	step [221/332], loss=16.9391
	step [222/332], loss=17.5323
	step [223/332], loss=18.4316
	step [224/332], loss=18.3735
	step [225/332], loss=19.7165
	step [226/332], loss=17.5372
	step [227/332], loss=16.6141
	step [228/332], loss=16.8159
	step [229/332], loss=16.3027
	step [230/332], loss=19.4169
	step [231/332], loss=16.9347
	step [232/332], loss=20.9245
	step [233/332], loss=22.4869
	step [234/332], loss=16.3220
	step [235/332], loss=17.5808
	step [236/332], loss=17.3015
	step [237/332], loss=17.1526
	step [238/332], loss=18.0810
	step [239/332], loss=17.5880
	step [240/332], loss=19.2577
	step [241/332], loss=17.4549
	step [242/332], loss=16.1910
	step [243/332], loss=18.8763
	step [244/332], loss=17.6886
	step [245/332], loss=18.9072
	step [246/332], loss=18.1908
	step [247/332], loss=15.8494
	step [248/332], loss=20.8359
	step [249/332], loss=15.5375
	step [250/332], loss=22.9361
	step [251/332], loss=19.1811
	step [252/332], loss=18.2859
	step [253/332], loss=16.9675
	step [254/332], loss=17.7468
	step [255/332], loss=18.2205
	step [256/332], loss=17.4230
	step [257/332], loss=14.9464
	step [258/332], loss=16.2070
	step [259/332], loss=15.1057
	step [260/332], loss=20.3805
	step [261/332], loss=20.0170
	step [262/332], loss=18.3092
	step [263/332], loss=15.7814
	step [264/332], loss=17.7738
	step [265/332], loss=17.5456
	step [266/332], loss=16.2311
	step [267/332], loss=17.8558
	step [268/332], loss=15.4167
	step [269/332], loss=18.9968
	step [270/332], loss=17.3109
	step [271/332], loss=16.8426
	step [272/332], loss=17.2427
	step [273/332], loss=17.8983
	step [274/332], loss=20.1772
	step [275/332], loss=19.9031
	step [276/332], loss=14.3510
	step [277/332], loss=17.5545
	step [278/332], loss=16.7449
	step [279/332], loss=18.5931
	step [280/332], loss=17.9940
	step [281/332], loss=17.7160
	step [282/332], loss=15.6530
	step [283/332], loss=15.8342
	step [284/332], loss=22.3894
	step [285/332], loss=18.4234
	step [286/332], loss=17.7388
	step [287/332], loss=19.3886
	step [288/332], loss=18.7989
	step [289/332], loss=17.7878
	step [290/332], loss=21.5223
	step [291/332], loss=21.1396
	step [292/332], loss=17.3841
	step [293/332], loss=18.7016
	step [294/332], loss=17.1556
	step [295/332], loss=15.8688
	step [296/332], loss=16.7544
	step [297/332], loss=15.1713
	step [298/332], loss=15.8877
	step [299/332], loss=16.9692
	step [300/332], loss=14.9849
	step [301/332], loss=19.6688
	step [302/332], loss=14.8039
	step [303/332], loss=17.9710
	step [304/332], loss=15.7903
	step [305/332], loss=16.6071
	step [306/332], loss=15.1384
	step [307/332], loss=17.5108
	step [308/332], loss=15.3325
	step [309/332], loss=14.9682
	step [310/332], loss=19.3653
	step [311/332], loss=15.4679
	step [312/332], loss=15.8832
	step [313/332], loss=19.3701
	step [314/332], loss=15.4234
	step [315/332], loss=15.8016
	step [316/332], loss=16.3319
	step [317/332], loss=17.5278
	step [318/332], loss=15.3812
	step [319/332], loss=15.9164
	step [320/332], loss=20.7392
	step [321/332], loss=14.4598
	step [322/332], loss=16.4615
	step [323/332], loss=16.4923
	step [324/332], loss=15.6534
	step [325/332], loss=16.4177
	step [326/332], loss=19.8186
	step [327/332], loss=16.9189
	step [328/332], loss=16.4276
	step [329/332], loss=15.7889
	step [330/332], loss=17.6066
	step [331/332], loss=16.3830
	step [332/332], loss=11.5878
	Evaluating
	loss=0.0709, precision=0.0939, recall=0.9935, f1=0.5073
Training epoch 5
	step [1/332], loss=17.0480
	step [2/332], loss=15.5598
	step [3/332], loss=15.5750
	step [4/332], loss=17.8087
	step [5/332], loss=16.5384
	step [6/332], loss=14.5403
	step [7/332], loss=16.2161
	step [8/332], loss=15.2825
	step [9/332], loss=15.0453
	step [10/332], loss=16.1298
	step [11/332], loss=18.0628
	step [12/332], loss=15.8590
	step [13/332], loss=14.6967
	step [14/332], loss=17.5671
	step [15/332], loss=16.5089
	step [16/332], loss=18.3137
	step [17/332], loss=17.6499
	step [18/332], loss=14.5473
	step [19/332], loss=17.5195
	step [20/332], loss=15.8026
	step [21/332], loss=22.1988
	step [22/332], loss=16.3512
	step [23/332], loss=16.2519
	step [24/332], loss=17.3217
	step [25/332], loss=15.2886
	step [26/332], loss=16.6703
	step [27/332], loss=17.6603
	step [28/332], loss=17.4052
	step [29/332], loss=16.5307
	step [30/332], loss=15.7774
	step [31/332], loss=15.6538
	step [32/332], loss=18.2230
	step [33/332], loss=18.9101
	step [34/332], loss=16.4203
	step [35/332], loss=14.8869
	step [36/332], loss=16.5760
	step [37/332], loss=16.3293
	step [38/332], loss=15.1453
	step [39/332], loss=15.7551
	step [40/332], loss=17.3844
	step [41/332], loss=19.3047
	step [42/332], loss=16.5936
	step [43/332], loss=13.9434
	step [44/332], loss=16.0699
	step [45/332], loss=15.7300
	step [46/332], loss=13.6268
	step [47/332], loss=16.6231
	step [48/332], loss=16.8270
	step [49/332], loss=13.6601
	step [50/332], loss=15.9665
	step [51/332], loss=14.5082
	step [52/332], loss=18.5865
	step [53/332], loss=17.2490
	step [54/332], loss=16.0797
	step [55/332], loss=16.9064
	step [56/332], loss=16.7680
	step [57/332], loss=16.9633
	step [58/332], loss=15.9386
	step [59/332], loss=17.4575
	step [60/332], loss=17.3547
	step [61/332], loss=16.3980
	step [62/332], loss=14.2578
	step [63/332], loss=15.1562
	step [64/332], loss=15.0119
	step [65/332], loss=17.3235
	step [66/332], loss=14.6297
	step [67/332], loss=18.2687
	step [68/332], loss=15.6354
	step [69/332], loss=14.6774
	step [70/332], loss=21.5216
	step [71/332], loss=16.1314
	step [72/332], loss=18.7576
	step [73/332], loss=16.6250
	step [74/332], loss=18.6567
	step [75/332], loss=16.1452
	step [76/332], loss=16.6408
	step [77/332], loss=16.8852
	step [78/332], loss=16.2640
	step [79/332], loss=15.7124
	step [80/332], loss=14.7723
	step [81/332], loss=15.9727
	step [82/332], loss=14.8373
	step [83/332], loss=16.9789
	step [84/332], loss=16.8234
	step [85/332], loss=15.8364
	step [86/332], loss=15.7058
	step [87/332], loss=17.3201
	step [88/332], loss=14.9748
	step [89/332], loss=17.6113
	step [90/332], loss=17.5060
	step [91/332], loss=15.1302
	step [92/332], loss=17.8112
	step [93/332], loss=16.2356
	step [94/332], loss=18.8810
	step [95/332], loss=18.0120
	step [96/332], loss=16.8904
	step [97/332], loss=16.2896
	step [98/332], loss=15.4399
	step [99/332], loss=15.2699
	step [100/332], loss=15.8355
	step [101/332], loss=14.2576
	step [102/332], loss=14.4417
	step [103/332], loss=14.0534
	step [104/332], loss=18.4772
	step [105/332], loss=15.7338
	step [106/332], loss=14.3785
	step [107/332], loss=15.0388
	step [108/332], loss=16.7830
	step [109/332], loss=14.4824
	step [110/332], loss=15.3571
	step [111/332], loss=17.5145
	step [112/332], loss=14.9610
	step [113/332], loss=18.0088
	step [114/332], loss=16.0421
	step [115/332], loss=14.0827
	step [116/332], loss=16.0616
	step [117/332], loss=15.7230
	step [118/332], loss=15.3883
	step [119/332], loss=15.1843
	step [120/332], loss=13.4699
	step [121/332], loss=15.6421
	step [122/332], loss=15.5070
	step [123/332], loss=14.4039
	step [124/332], loss=16.4920
	step [125/332], loss=14.9111
	step [126/332], loss=17.3264
	step [127/332], loss=16.7760
	step [128/332], loss=13.5816
	step [129/332], loss=14.7918
	step [130/332], loss=15.8294
	step [131/332], loss=16.8540
	step [132/332], loss=17.7817
	step [133/332], loss=14.6938
	step [134/332], loss=15.3353
	step [135/332], loss=14.5075
	step [136/332], loss=17.0135
	step [137/332], loss=13.7835
	step [138/332], loss=17.3919
	step [139/332], loss=12.7169
	step [140/332], loss=14.8449
	step [141/332], loss=15.7091
	step [142/332], loss=14.0300
	step [143/332], loss=13.5481
	step [144/332], loss=14.9104
	step [145/332], loss=13.8935
	step [146/332], loss=13.6337
	step [147/332], loss=15.6468
	step [148/332], loss=13.0525
	step [149/332], loss=16.7175
	step [150/332], loss=16.9089
	step [151/332], loss=21.1339
	step [152/332], loss=14.8801
	step [153/332], loss=18.6929
	step [154/332], loss=18.4961
	step [155/332], loss=17.0461
	step [156/332], loss=16.1148
	step [157/332], loss=15.7024
	step [158/332], loss=14.6407
	step [159/332], loss=15.2012
	step [160/332], loss=13.9476
	step [161/332], loss=15.3307
	step [162/332], loss=19.0817
	step [163/332], loss=15.8327
	step [164/332], loss=14.9121
	step [165/332], loss=16.0205
	step [166/332], loss=15.0989
	step [167/332], loss=18.2358
	step [168/332], loss=17.8946
	step [169/332], loss=17.7218
	step [170/332], loss=16.0820
	step [171/332], loss=15.8435
	step [172/332], loss=16.5718
	step [173/332], loss=15.2895
	step [174/332], loss=16.6947
	step [175/332], loss=15.3110
	step [176/332], loss=13.1660
	step [177/332], loss=17.3217
	step [178/332], loss=13.6767
	step [179/332], loss=15.7020
	step [180/332], loss=16.0793
	step [181/332], loss=13.5727
	step [182/332], loss=16.3093
	step [183/332], loss=15.9877
	step [184/332], loss=13.8747
	step [185/332], loss=16.8810
	step [186/332], loss=14.4659
	step [187/332], loss=15.2435
	step [188/332], loss=16.0368
	step [189/332], loss=15.3557
	step [190/332], loss=15.7307
	step [191/332], loss=16.2326
	step [192/332], loss=17.1036
	step [193/332], loss=11.9576
	step [194/332], loss=14.1645
	step [195/332], loss=13.8548
	step [196/332], loss=17.2325
	step [197/332], loss=15.1417
	step [198/332], loss=14.9436
	step [199/332], loss=13.4055
	step [200/332], loss=14.4870
	step [201/332], loss=15.3104
	step [202/332], loss=17.4270
	step [203/332], loss=16.5993
	step [204/332], loss=14.7153
	step [205/332], loss=17.3289
	step [206/332], loss=13.8644
	step [207/332], loss=15.1195
	step [208/332], loss=15.4539
	step [209/332], loss=14.1374
	step [210/332], loss=12.8646
	step [211/332], loss=15.7981
	step [212/332], loss=13.5934
	step [213/332], loss=12.3739
	step [214/332], loss=12.8405
	step [215/332], loss=13.9816
	step [216/332], loss=11.2090
	step [217/332], loss=17.2238
	step [218/332], loss=14.8359
	step [219/332], loss=16.0634
	step [220/332], loss=14.4503
	step [221/332], loss=14.7199
	step [222/332], loss=14.8112
	step [223/332], loss=14.6164
	step [224/332], loss=14.1205
	step [225/332], loss=13.6460
	step [226/332], loss=15.8637
	step [227/332], loss=14.3901
	step [228/332], loss=13.2455
	step [229/332], loss=16.4716
	step [230/332], loss=14.4025
	step [231/332], loss=13.9675
	step [232/332], loss=15.1961
	step [233/332], loss=13.4719
	step [234/332], loss=16.7895
	step [235/332], loss=16.0851
	step [236/332], loss=14.3053
	step [237/332], loss=13.3182
	step [238/332], loss=15.1256
	step [239/332], loss=17.1619
	step [240/332], loss=14.5717
	step [241/332], loss=16.0945
	step [242/332], loss=13.1786
	step [243/332], loss=14.0020
	step [244/332], loss=15.5664
	step [245/332], loss=14.2909
	step [246/332], loss=13.6766
	step [247/332], loss=13.6793
	step [248/332], loss=15.1950
	step [249/332], loss=14.3243
	step [250/332], loss=14.1679
	step [251/332], loss=16.4305
	step [252/332], loss=12.2734
	step [253/332], loss=16.9675
	step [254/332], loss=16.5746
	step [255/332], loss=14.1477
	step [256/332], loss=14.1199
	step [257/332], loss=16.2843
	step [258/332], loss=16.6195
	step [259/332], loss=12.3402
	step [260/332], loss=15.3039
	step [261/332], loss=15.1437
	step [262/332], loss=13.1084
	step [263/332], loss=13.2161
	step [264/332], loss=12.1249
	step [265/332], loss=18.5844
	step [266/332], loss=16.3810
	step [267/332], loss=13.7759
	step [268/332], loss=14.2317
	step [269/332], loss=16.4015
	step [270/332], loss=14.8583
	step [271/332], loss=14.9650
	step [272/332], loss=15.6160
	step [273/332], loss=14.8618
	step [274/332], loss=15.6753
	step [275/332], loss=14.3178
	step [276/332], loss=16.9899
	step [277/332], loss=13.6242
	step [278/332], loss=15.3548
	step [279/332], loss=17.9050
	step [280/332], loss=14.4659
	step [281/332], loss=17.0587
	step [282/332], loss=15.7436
	step [283/332], loss=14.9599
	step [284/332], loss=14.6212
	step [285/332], loss=12.7952
	step [286/332], loss=14.4813
	step [287/332], loss=14.9850
	step [288/332], loss=14.3705
	step [289/332], loss=12.5781
	step [290/332], loss=15.0897
	step [291/332], loss=13.9333
	step [292/332], loss=11.9638
	step [293/332], loss=16.4464
	step [294/332], loss=13.7421
	step [295/332], loss=14.9308
	step [296/332], loss=12.6182
	step [297/332], loss=15.5100
	step [298/332], loss=15.4906
	step [299/332], loss=14.1999
	step [300/332], loss=14.8456
	step [301/332], loss=13.3201
	step [302/332], loss=14.2298
	step [303/332], loss=13.1164
	step [304/332], loss=15.0451
	step [305/332], loss=14.6794
	step [306/332], loss=14.5381
	step [307/332], loss=12.4140
	step [308/332], loss=12.8045
	step [309/332], loss=13.8811
	step [310/332], loss=15.0274
	step [311/332], loss=16.0955
	step [312/332], loss=16.2922
	step [313/332], loss=14.4914
	step [314/332], loss=13.1175
	step [315/332], loss=13.0584
	step [316/332], loss=15.2052
	step [317/332], loss=12.5216
	step [318/332], loss=14.5076
	step [319/332], loss=13.0048
	step [320/332], loss=13.1540
	step [321/332], loss=14.3192
	step [322/332], loss=13.9391
	step [323/332], loss=13.1860
	step [324/332], loss=13.5631
	step [325/332], loss=14.8227
	step [326/332], loss=15.0016
	step [327/332], loss=13.7811
	step [328/332], loss=14.9062
	step [329/332], loss=11.0413
	step [330/332], loss=16.4335
	step [331/332], loss=13.1029
	step [332/332], loss=6.7669
	Evaluating
	loss=0.0559, precision=0.0961, recall=0.9944, f1=0.5140
Training epoch 6
	step [1/332], loss=13.1673
	step [2/332], loss=15.0448
	step [3/332], loss=16.9829
	step [4/332], loss=14.4692
	step [5/332], loss=13.6214
	step [6/332], loss=14.2358
	step [7/332], loss=15.7955
	step [8/332], loss=14.4724
	step [9/332], loss=13.4455
	step [10/332], loss=15.3572
	step [11/332], loss=15.4906
	step [12/332], loss=15.0272
	step [13/332], loss=13.2322
	step [14/332], loss=13.7343
	step [15/332], loss=11.9681
	step [16/332], loss=11.6885
	step [17/332], loss=14.5486
	step [18/332], loss=12.4115
	step [19/332], loss=18.4450
	step [20/332], loss=16.0863
	step [21/332], loss=17.4075
	step [22/332], loss=12.0104
	step [23/332], loss=12.3462
	step [24/332], loss=12.2802
	step [25/332], loss=15.3126
	step [26/332], loss=13.9538
	step [27/332], loss=14.4443
	step [28/332], loss=15.8106
	step [29/332], loss=13.5247
	step [30/332], loss=12.7048
	step [31/332], loss=16.2593
	step [32/332], loss=14.1263
	step [33/332], loss=14.2938
	step [34/332], loss=15.0136
	step [35/332], loss=14.9949
	step [36/332], loss=13.3698
	step [37/332], loss=12.7811
	step [38/332], loss=13.7775
	step [39/332], loss=13.1355
	step [40/332], loss=16.6421
	step [41/332], loss=14.7133
	step [42/332], loss=12.3268
	step [43/332], loss=12.9293
	step [44/332], loss=12.1962
	step [45/332], loss=12.8827
	step [46/332], loss=15.1589
	step [47/332], loss=15.1211
	step [48/332], loss=14.8008
	step [49/332], loss=11.4438
	step [50/332], loss=12.7037
	step [51/332], loss=14.9599
	step [52/332], loss=15.3794
	step [53/332], loss=13.3362
	step [54/332], loss=15.0044
	step [55/332], loss=14.2790
	step [56/332], loss=12.3365
	step [57/332], loss=14.0686
	step [58/332], loss=15.3319
	step [59/332], loss=18.3017
	step [60/332], loss=12.6167
	step [61/332], loss=14.3221
	step [62/332], loss=15.8058
	step [63/332], loss=16.7027
	step [64/332], loss=13.5250
	step [65/332], loss=13.9875
	step [66/332], loss=12.1011
	step [67/332], loss=14.1023
	step [68/332], loss=13.4616
	step [69/332], loss=12.1905
	step [70/332], loss=12.5933
	step [71/332], loss=14.2341
	step [72/332], loss=12.8759
	step [73/332], loss=13.9222
	step [74/332], loss=15.1361
	step [75/332], loss=15.9433
	step [76/332], loss=13.5821
	step [77/332], loss=12.0233
	step [78/332], loss=11.7768
	step [79/332], loss=15.0020
	step [80/332], loss=13.8159
	step [81/332], loss=14.1055
	step [82/332], loss=13.3886
	step [83/332], loss=15.3462
	step [84/332], loss=15.4935
	step [85/332], loss=13.2987
	step [86/332], loss=14.9183
	step [87/332], loss=13.3547
	step [88/332], loss=15.3528
	step [89/332], loss=12.4743
	step [90/332], loss=15.0390
	step [91/332], loss=14.9098
	step [92/332], loss=13.0373
	step [93/332], loss=12.9576
	step [94/332], loss=13.0625
	step [95/332], loss=14.4549
	step [96/332], loss=13.4879
	step [97/332], loss=13.2751
	step [98/332], loss=13.9014
	step [99/332], loss=11.9347
	step [100/332], loss=12.1251
	step [101/332], loss=12.2031
	step [102/332], loss=13.4438
	step [103/332], loss=11.3010
	step [104/332], loss=12.7521
	step [105/332], loss=12.9708
	step [106/332], loss=13.2750
	step [107/332], loss=13.0023
	step [108/332], loss=12.6634
	step [109/332], loss=14.1663
	step [110/332], loss=15.0756
	step [111/332], loss=13.3213
	step [112/332], loss=12.8079
	step [113/332], loss=13.1735
	step [114/332], loss=15.7198
	step [115/332], loss=14.6478
	step [116/332], loss=12.2210
	step [117/332], loss=13.1795
	step [118/332], loss=13.7189
	step [119/332], loss=15.6045
	step [120/332], loss=12.7290
	step [121/332], loss=13.1194
	step [122/332], loss=14.2167
	step [123/332], loss=11.7016
	step [124/332], loss=14.2871
	step [125/332], loss=13.2126
	step [126/332], loss=14.0464
	step [127/332], loss=14.5281
	step [128/332], loss=13.4199
	step [129/332], loss=12.9072
	step [130/332], loss=12.3979
	step [131/332], loss=14.5518
	step [132/332], loss=13.6400
	step [133/332], loss=12.9468
	step [134/332], loss=10.7714
	step [135/332], loss=12.7876
	step [136/332], loss=12.1375
	step [137/332], loss=15.7575
	step [138/332], loss=13.9575
	step [139/332], loss=13.1896
	step [140/332], loss=12.3809
	step [141/332], loss=12.1475
	step [142/332], loss=13.9409
	step [143/332], loss=13.0701
	step [144/332], loss=12.7240
	step [145/332], loss=14.2476
	step [146/332], loss=15.2605
	step [147/332], loss=10.9954
	step [148/332], loss=13.6381
	step [149/332], loss=12.7865
	step [150/332], loss=15.2682
	step [151/332], loss=14.1674
	step [152/332], loss=13.1377
	step [153/332], loss=12.4364
	step [154/332], loss=13.6933
	step [155/332], loss=14.1337
	step [156/332], loss=11.9593
	step [157/332], loss=12.6143
	step [158/332], loss=14.0630
	step [159/332], loss=11.5975
	step [160/332], loss=11.7335
	step [161/332], loss=14.2819
	step [162/332], loss=15.8679
	step [163/332], loss=15.2563
	step [164/332], loss=12.7246
	step [165/332], loss=14.0540
	step [166/332], loss=13.9097
	step [167/332], loss=11.7228
	step [168/332], loss=13.0149
	step [169/332], loss=15.1869
	step [170/332], loss=14.0284
	step [171/332], loss=14.1750
	step [172/332], loss=11.0841
	step [173/332], loss=14.0060
	step [174/332], loss=15.3605
	step [175/332], loss=13.3216
	step [176/332], loss=13.0322
	step [177/332], loss=15.4667
	step [178/332], loss=13.6188
	step [179/332], loss=11.7612
	step [180/332], loss=14.2501
	step [181/332], loss=15.1434
	step [182/332], loss=13.4759
	step [183/332], loss=17.2145
	step [184/332], loss=10.3977
	step [185/332], loss=14.0619
	step [186/332], loss=12.9171
	step [187/332], loss=14.0899
	step [188/332], loss=13.0225
	step [189/332], loss=11.1642
	step [190/332], loss=13.5918
	step [191/332], loss=12.5819
	step [192/332], loss=16.0663
	step [193/332], loss=13.9527
	step [194/332], loss=13.8555
	step [195/332], loss=11.9629
	step [196/332], loss=14.5440
	step [197/332], loss=13.4535
	step [198/332], loss=13.1187
	step [199/332], loss=13.6912
	step [200/332], loss=11.5493
	step [201/332], loss=13.0283
	step [202/332], loss=13.0481
	step [203/332], loss=12.3560
	step [204/332], loss=12.3410
	step [205/332], loss=14.2304
	step [206/332], loss=14.6395
	step [207/332], loss=12.7600
	step [208/332], loss=12.3184
	step [209/332], loss=13.9322
	step [210/332], loss=14.7992
	step [211/332], loss=14.0855
	step [212/332], loss=13.0358
	step [213/332], loss=13.5787
	step [214/332], loss=13.9733
	step [215/332], loss=11.1698
	step [216/332], loss=12.7673
	step [217/332], loss=14.8920
	step [218/332], loss=12.1596
	step [219/332], loss=11.7404
	step [220/332], loss=12.3039
	step [221/332], loss=12.0801
	step [222/332], loss=13.3284
	step [223/332], loss=13.5191
	step [224/332], loss=15.1267
	step [225/332], loss=13.8443
	step [226/332], loss=13.4376
	step [227/332], loss=15.5989
	step [228/332], loss=15.7840
	step [229/332], loss=13.8371
	step [230/332], loss=14.6581
	step [231/332], loss=13.2787
	step [232/332], loss=13.7571
	step [233/332], loss=16.0547
	step [234/332], loss=12.5484
	step [235/332], loss=13.8112
	step [236/332], loss=11.9798
	step [237/332], loss=12.7330
	step [238/332], loss=12.0101
	step [239/332], loss=12.3301
	step [240/332], loss=15.2095
	step [241/332], loss=11.8626
	step [242/332], loss=14.0574
	step [243/332], loss=13.8744
	step [244/332], loss=13.1295
	step [245/332], loss=12.1328
	step [246/332], loss=13.3989
	step [247/332], loss=10.4838
	step [248/332], loss=10.6679
	step [249/332], loss=13.5984
	step [250/332], loss=11.7146
	step [251/332], loss=11.0909
	step [252/332], loss=13.4935
	step [253/332], loss=12.4296
	step [254/332], loss=12.0637
	step [255/332], loss=14.2845
	step [256/332], loss=11.8772
	step [257/332], loss=11.4983
	step [258/332], loss=12.4238
	step [259/332], loss=11.0449
	step [260/332], loss=13.5365
	step [261/332], loss=14.7877
	step [262/332], loss=12.6548
	step [263/332], loss=13.2679
	step [264/332], loss=9.4679
	step [265/332], loss=14.1593
	step [266/332], loss=11.5550
	step [267/332], loss=13.6983
	step [268/332], loss=12.3250
	step [269/332], loss=9.9450
	step [270/332], loss=13.2299
	step [271/332], loss=13.8982
	step [272/332], loss=10.6043
	step [273/332], loss=11.2775
	step [274/332], loss=10.5619
	step [275/332], loss=12.5523
	step [276/332], loss=10.6936
	step [277/332], loss=13.4099
	step [278/332], loss=15.2127
	step [279/332], loss=14.2348
	step [280/332], loss=14.3015
	step [281/332], loss=12.5660
	step [282/332], loss=13.4150
	step [283/332], loss=12.8960
	step [284/332], loss=11.9702
	step [285/332], loss=13.2790
	step [286/332], loss=14.7941
	step [287/332], loss=14.9022
	step [288/332], loss=13.2424
	step [289/332], loss=12.9050
	step [290/332], loss=12.0941
	step [291/332], loss=12.8166
	step [292/332], loss=10.1462
	step [293/332], loss=13.5055
	step [294/332], loss=12.3365
	step [295/332], loss=13.9533
	step [296/332], loss=13.6387
	step [297/332], loss=11.3863
	step [298/332], loss=12.2590
	step [299/332], loss=13.1377
	step [300/332], loss=11.6036
	step [301/332], loss=11.8174
	step [302/332], loss=11.7999
	step [303/332], loss=13.1432
	step [304/332], loss=12.2126
	step [305/332], loss=12.9598
	step [306/332], loss=14.0925
	step [307/332], loss=10.4587
	step [308/332], loss=12.2772
	step [309/332], loss=15.0106
	step [310/332], loss=12.2668
	step [311/332], loss=16.6500
	step [312/332], loss=10.8127
	step [313/332], loss=11.8436
	step [314/332], loss=11.6565
	step [315/332], loss=12.4163
	step [316/332], loss=10.2005
	step [317/332], loss=12.6796
	step [318/332], loss=14.3902
	step [319/332], loss=12.3985
	step [320/332], loss=13.0122
	step [321/332], loss=12.2630
	step [322/332], loss=12.6187
	step [323/332], loss=14.3810
	step [324/332], loss=12.8873
	step [325/332], loss=11.9712
	step [326/332], loss=11.7720
	step [327/332], loss=17.2285
	step [328/332], loss=11.1939
	step [329/332], loss=13.2828
	step [330/332], loss=12.4829
	step [331/332], loss=12.9776
	step [332/332], loss=6.2185
	Evaluating
	loss=0.0421, precision=0.1320, recall=0.9916, f1=0.6006
Training epoch 7
	step [1/332], loss=10.2940
	step [2/332], loss=12.0980
	step [3/332], loss=11.9660
	step [4/332], loss=13.1601
	step [5/332], loss=12.0910
	step [6/332], loss=14.7747
	step [7/332], loss=12.8129
	step [8/332], loss=14.1450
	step [9/332], loss=12.1176
	step [10/332], loss=11.1034
	step [11/332], loss=13.1499
	step [12/332], loss=14.5350
	step [13/332], loss=12.3924
	step [14/332], loss=12.0505
	step [15/332], loss=12.6714
	step [16/332], loss=11.0114
	step [17/332], loss=11.3609
	step [18/332], loss=11.4544
	step [19/332], loss=12.8984
	step [20/332], loss=12.2622
	step [21/332], loss=13.4117
	step [22/332], loss=11.8858
	step [23/332], loss=14.7284
	step [24/332], loss=11.9204
	step [25/332], loss=10.8390
	step [26/332], loss=11.0886
	step [27/332], loss=12.9125
	step [28/332], loss=10.8794
	step [29/332], loss=13.3781
	step [30/332], loss=13.4302
	step [31/332], loss=15.9285
	step [32/332], loss=10.6891
	step [33/332], loss=12.0354
	step [34/332], loss=12.2000
	step [35/332], loss=12.2467
	step [36/332], loss=14.0779
	step [37/332], loss=13.9421
	step [38/332], loss=13.1952
	step [39/332], loss=11.9080
	step [40/332], loss=13.1621
	step [41/332], loss=13.9295
	step [42/332], loss=13.0472
	step [43/332], loss=10.8722
	step [44/332], loss=14.1079
	step [45/332], loss=15.2516
	step [46/332], loss=12.7137
	step [47/332], loss=10.5140
	step [48/332], loss=10.7758
	step [49/332], loss=11.3594
	step [50/332], loss=11.3488
	step [51/332], loss=12.3992
	step [52/332], loss=10.4744
	step [53/332], loss=12.3277
	step [54/332], loss=13.6647
	step [55/332], loss=12.9636
	step [56/332], loss=12.4426
	step [57/332], loss=13.2117
	step [58/332], loss=12.4005
	step [59/332], loss=13.9712
	step [60/332], loss=12.0415
	step [61/332], loss=14.2385
	step [62/332], loss=13.9339
	step [63/332], loss=12.0748
	step [64/332], loss=11.8037
	step [65/332], loss=11.9216
	step [66/332], loss=14.6817
	step [67/332], loss=13.0964
	step [68/332], loss=11.8217
	step [69/332], loss=10.7312
	step [70/332], loss=11.9077
	step [71/332], loss=13.1267
	step [72/332], loss=12.6262
	step [73/332], loss=10.5024
	step [74/332], loss=15.2989
	step [75/332], loss=12.9706
	step [76/332], loss=13.2276
	step [77/332], loss=14.0859
	step [78/332], loss=10.2578
	step [79/332], loss=12.8087
	step [80/332], loss=13.4794
	step [81/332], loss=10.2984
	step [82/332], loss=12.0791
	step [83/332], loss=11.4788
	step [84/332], loss=13.7286
	step [85/332], loss=14.0521
	step [86/332], loss=11.9821
	step [87/332], loss=15.0504
	step [88/332], loss=13.4982
	step [89/332], loss=13.8604
	step [90/332], loss=12.2389
	step [91/332], loss=13.3150
	step [92/332], loss=12.3428
	step [93/332], loss=12.9014
	step [94/332], loss=14.1164
	step [95/332], loss=13.5680
	step [96/332], loss=10.7552
	step [97/332], loss=13.2297
	step [98/332], loss=12.9582
	step [99/332], loss=12.9068
	step [100/332], loss=11.5164
	step [101/332], loss=14.1515
	step [102/332], loss=13.7364
	step [103/332], loss=12.1092
	step [104/332], loss=10.4931
	step [105/332], loss=12.8002
	step [106/332], loss=17.0476
	step [107/332], loss=10.9791
	step [108/332], loss=14.6511
	step [109/332], loss=10.6274
	step [110/332], loss=12.4595
	step [111/332], loss=11.4034
	step [112/332], loss=11.1330
	step [113/332], loss=10.4073
	step [114/332], loss=11.5139
	step [115/332], loss=9.6178
	step [116/332], loss=10.8061
	step [117/332], loss=12.3913
	step [118/332], loss=12.5150
	step [119/332], loss=11.6296
	step [120/332], loss=12.5894
	step [121/332], loss=9.0365
	step [122/332], loss=12.3180
	step [123/332], loss=12.6265
	step [124/332], loss=13.5700
	step [125/332], loss=10.8910
	step [126/332], loss=12.3352
	step [127/332], loss=14.2450
	step [128/332], loss=11.1851
	step [129/332], loss=12.8763
	step [130/332], loss=11.2448
	step [131/332], loss=11.7698
	step [132/332], loss=12.5352
	step [133/332], loss=13.3097
	step [134/332], loss=10.0689
	step [135/332], loss=10.4445
	step [136/332], loss=12.1320
	step [137/332], loss=12.1058
	step [138/332], loss=11.0812
	step [139/332], loss=9.5467
	step [140/332], loss=14.0309
	step [141/332], loss=10.0115
	step [142/332], loss=10.4615
	step [143/332], loss=14.7070
	step [144/332], loss=11.8641
	step [145/332], loss=12.5162
	step [146/332], loss=11.7472
	step [147/332], loss=12.0709
	step [148/332], loss=9.9471
	step [149/332], loss=13.2906
	step [150/332], loss=12.3932
	step [151/332], loss=11.3624
	step [152/332], loss=9.9962
	step [153/332], loss=13.1433
	step [154/332], loss=12.3102
	step [155/332], loss=12.5081
	step [156/332], loss=10.5549
	step [157/332], loss=10.9564
	step [158/332], loss=9.7979
	step [159/332], loss=12.1441
	step [160/332], loss=11.5374
	step [161/332], loss=14.5189
	step [162/332], loss=11.6613
	step [163/332], loss=11.5387
	step [164/332], loss=10.9686
	step [165/332], loss=11.9348
	step [166/332], loss=13.2571
	step [167/332], loss=11.2234
	step [168/332], loss=13.5303
	step [169/332], loss=10.9947
	step [170/332], loss=12.2374
	step [171/332], loss=11.8838
	step [172/332], loss=11.5635
	step [173/332], loss=12.8864
	step [174/332], loss=11.4100
	step [175/332], loss=11.2348
	step [176/332], loss=15.6215
	step [177/332], loss=12.6370
	step [178/332], loss=11.6840
	step [179/332], loss=10.0229
	step [180/332], loss=12.7022
	step [181/332], loss=11.2339
	step [182/332], loss=11.6855
	step [183/332], loss=10.2744
	step [184/332], loss=10.9587
	step [185/332], loss=14.2689
	step [186/332], loss=11.7219
	step [187/332], loss=11.6180
	step [188/332], loss=9.8561
	step [189/332], loss=14.1820
	step [190/332], loss=11.7758
	step [191/332], loss=10.8334
	step [192/332], loss=12.6080
	step [193/332], loss=11.8232
	step [194/332], loss=10.8719
	step [195/332], loss=11.9562
	step [196/332], loss=11.1273
	step [197/332], loss=13.8137
	step [198/332], loss=12.1376
	step [199/332], loss=12.6295
	step [200/332], loss=10.7725
	step [201/332], loss=12.9233
	step [202/332], loss=10.7151
	step [203/332], loss=14.8385
	step [204/332], loss=11.1179
	step [205/332], loss=11.6416
	step [206/332], loss=9.6259
	step [207/332], loss=11.5286
	step [208/332], loss=9.7779
	step [209/332], loss=12.8769
	step [210/332], loss=12.5607
	step [211/332], loss=12.5032
	step [212/332], loss=9.6927
	step [213/332], loss=13.1193
	step [214/332], loss=11.6295
	step [215/332], loss=12.6098
	step [216/332], loss=12.1330
	step [217/332], loss=11.5039
	step [218/332], loss=9.6351
	step [219/332], loss=11.6312
	step [220/332], loss=14.2938
	step [221/332], loss=10.5754
	step [222/332], loss=11.4395
	step [223/332], loss=10.8218
	step [224/332], loss=10.8678
	step [225/332], loss=11.1157
	step [226/332], loss=13.4595
	step [227/332], loss=12.0768
	step [228/332], loss=14.2838
	step [229/332], loss=12.0820
	step [230/332], loss=11.1955
	step [231/332], loss=9.7959
	step [232/332], loss=11.6993
	step [233/332], loss=10.5867
	step [234/332], loss=13.1936
	step [235/332], loss=9.9350
	step [236/332], loss=12.3687
	step [237/332], loss=13.5954
	step [238/332], loss=13.0974
	step [239/332], loss=10.3437
	step [240/332], loss=13.1354
	step [241/332], loss=14.7906
	step [242/332], loss=11.8028
	step [243/332], loss=13.0913
	step [244/332], loss=13.2296
	step [245/332], loss=11.9800
	step [246/332], loss=10.0388
	step [247/332], loss=11.9478
	step [248/332], loss=8.9289
	step [249/332], loss=10.9240
	step [250/332], loss=10.1668
	step [251/332], loss=12.9086
	step [252/332], loss=13.9286
	step [253/332], loss=12.3930
	step [254/332], loss=13.1326
	step [255/332], loss=12.2007
	step [256/332], loss=13.0021
	step [257/332], loss=12.9145
	step [258/332], loss=9.0663
	step [259/332], loss=11.1211
	step [260/332], loss=12.4964
	step [261/332], loss=13.7541
	step [262/332], loss=11.7199
	step [263/332], loss=10.7856
	step [264/332], loss=11.2642
	step [265/332], loss=11.5453
	step [266/332], loss=12.6014
	step [267/332], loss=10.6064
	step [268/332], loss=13.7215
	step [269/332], loss=14.1343
	step [270/332], loss=10.4710
	step [271/332], loss=13.3401
	step [272/332], loss=9.9309
	step [273/332], loss=11.5375
	step [274/332], loss=13.2515
	step [275/332], loss=10.7647
	step [276/332], loss=10.6469
	step [277/332], loss=11.4405
	step [278/332], loss=9.8175
	step [279/332], loss=11.6511
	step [280/332], loss=10.9691
	step [281/332], loss=10.9188
	step [282/332], loss=10.2043
	step [283/332], loss=11.5482
	step [284/332], loss=9.2567
	step [285/332], loss=10.2576
	step [286/332], loss=10.2663
	step [287/332], loss=13.6559
	step [288/332], loss=13.1022
	step [289/332], loss=14.5072
	step [290/332], loss=10.2261
	step [291/332], loss=13.0044
	step [292/332], loss=11.8591
	step [293/332], loss=12.4110
	step [294/332], loss=10.9489
	step [295/332], loss=17.3065
	step [296/332], loss=11.8782
	step [297/332], loss=12.1012
	step [298/332], loss=11.7082
	step [299/332], loss=11.2461
	step [300/332], loss=10.0558
	step [301/332], loss=15.3840
	step [302/332], loss=9.9681
	step [303/332], loss=12.2620
	step [304/332], loss=11.5340
	step [305/332], loss=13.1602
	step [306/332], loss=11.2386
	step [307/332], loss=9.7090
	step [308/332], loss=10.7684
	step [309/332], loss=11.3108
	step [310/332], loss=13.5714
	step [311/332], loss=11.8637
	step [312/332], loss=13.0155
	step [313/332], loss=10.7728
	step [314/332], loss=13.0280
	step [315/332], loss=11.5077
	step [316/332], loss=10.6213
	step [317/332], loss=14.6588
	step [318/332], loss=12.4768
	step [319/332], loss=12.6365
	step [320/332], loss=10.5475
	step [321/332], loss=10.7981
	step [322/332], loss=10.4160
	step [323/332], loss=12.0927
	step [324/332], loss=12.3626
	step [325/332], loss=12.9026
	step [326/332], loss=11.7015
	step [327/332], loss=11.2202
	step [328/332], loss=8.3709
	step [329/332], loss=12.1820
	step [330/332], loss=9.2524
	step [331/332], loss=11.7973
	step [332/332], loss=6.5930
	Evaluating
	loss=0.0372, precision=0.1243, recall=0.9932, f1=0.5846
Training epoch 8
	step [1/332], loss=9.4755
	step [2/332], loss=10.9775
	step [3/332], loss=10.7994
	step [4/332], loss=10.3014
	step [5/332], loss=12.0529
	step [6/332], loss=11.5237
	step [7/332], loss=10.6946
	step [8/332], loss=10.8544
	step [9/332], loss=10.2552
	step [10/332], loss=13.0842
	step [11/332], loss=12.5237
	step [12/332], loss=12.6844
	step [13/332], loss=12.5756
	step [14/332], loss=10.3572
	step [15/332], loss=11.4713
	step [16/332], loss=11.9609
	step [17/332], loss=10.6004
	step [18/332], loss=11.9925
	step [19/332], loss=11.0974
	step [20/332], loss=11.0495
	step [21/332], loss=11.9304
	step [22/332], loss=12.9801
	step [23/332], loss=11.1166
	step [24/332], loss=13.0995
	step [25/332], loss=11.8311
	step [26/332], loss=10.4479
	step [27/332], loss=12.6847
	step [28/332], loss=12.1147
	step [29/332], loss=9.3232
	step [30/332], loss=11.3298
	step [31/332], loss=12.9976
	step [32/332], loss=10.2764
	step [33/332], loss=9.6703
	step [34/332], loss=9.8789
	step [35/332], loss=10.4795
	step [36/332], loss=9.0804
	step [37/332], loss=10.6912
	step [38/332], loss=10.0367
	step [39/332], loss=17.3445
	step [40/332], loss=11.0964
	step [41/332], loss=10.8149
	step [42/332], loss=12.0871
	step [43/332], loss=14.5035
	step [44/332], loss=13.5577
	step [45/332], loss=11.2796
	step [46/332], loss=13.4332
	step [47/332], loss=10.8341
	step [48/332], loss=10.2779
	step [49/332], loss=12.2903
	step [50/332], loss=10.8702
	step [51/332], loss=8.8905
	step [52/332], loss=10.2294
	step [53/332], loss=11.4030
	step [54/332], loss=10.9600
	step [55/332], loss=10.3058
	step [56/332], loss=12.2771
	step [57/332], loss=9.7416
	step [58/332], loss=11.6252
	step [59/332], loss=9.7145
	step [60/332], loss=10.4171
	step [61/332], loss=11.3833
	step [62/332], loss=11.7525
	step [63/332], loss=10.6001
	step [64/332], loss=9.5880
	step [65/332], loss=10.6930
	step [66/332], loss=12.5309
	step [67/332], loss=13.1680
	step [68/332], loss=10.8163
	step [69/332], loss=10.4102
	step [70/332], loss=10.6253
	step [71/332], loss=10.4168
	step [72/332], loss=10.9544
	step [73/332], loss=10.3504
	step [74/332], loss=13.8285
	step [75/332], loss=12.4440
	step [76/332], loss=10.3776
	step [77/332], loss=10.4316
	step [78/332], loss=13.4590
	step [79/332], loss=11.8638
	step [80/332], loss=12.0789
	step [81/332], loss=10.7131
	step [82/332], loss=10.9888
	step [83/332], loss=10.4211
	step [84/332], loss=10.1262
	step [85/332], loss=9.7220
	step [86/332], loss=12.5007
	step [87/332], loss=10.1334
	step [88/332], loss=11.2497
	step [89/332], loss=11.4709
	step [90/332], loss=13.6457
	step [91/332], loss=10.1317
	step [92/332], loss=10.2820
	step [93/332], loss=13.5015
	step [94/332], loss=11.8757
	step [95/332], loss=11.1328
	step [96/332], loss=12.7884
	step [97/332], loss=12.4285
	step [98/332], loss=10.5462
	step [99/332], loss=9.9073
	step [100/332], loss=11.4246
	step [101/332], loss=16.3507
	step [102/332], loss=11.2718
	step [103/332], loss=11.8863
	step [104/332], loss=9.5464
	step [105/332], loss=12.3675
	step [106/332], loss=10.9851
	step [107/332], loss=9.7572
	step [108/332], loss=12.4270
	step [109/332], loss=10.0647
	step [110/332], loss=11.7086
	step [111/332], loss=12.1135
	step [112/332], loss=10.0344
	step [113/332], loss=10.0878
	step [114/332], loss=11.4746
	step [115/332], loss=11.4181
	step [116/332], loss=10.1735
	step [117/332], loss=9.4946
	step [118/332], loss=13.4037
	step [119/332], loss=9.1071
	step [120/332], loss=12.8237
	step [121/332], loss=10.8621
	step [122/332], loss=11.7891
	step [123/332], loss=10.8360
	step [124/332], loss=13.5695
	step [125/332], loss=10.0384
	step [126/332], loss=10.2243
	step [127/332], loss=13.4322
	step [128/332], loss=11.5337
	step [129/332], loss=13.6810
	step [130/332], loss=10.7665
	step [131/332], loss=10.1133
	step [132/332], loss=13.6808
	step [133/332], loss=10.6614
	step [134/332], loss=9.7274
	step [135/332], loss=12.9250
	step [136/332], loss=8.9350
	step [137/332], loss=11.9873
	step [138/332], loss=11.0351
	step [139/332], loss=11.7006
	step [140/332], loss=14.2260
	step [141/332], loss=11.1077
	step [142/332], loss=10.5028
	step [143/332], loss=13.9322
	step [144/332], loss=11.8777
	step [145/332], loss=10.9831
	step [146/332], loss=10.7988
	step [147/332], loss=11.6972
	step [148/332], loss=14.0419
	step [149/332], loss=9.2868
	step [150/332], loss=11.2402
	step [151/332], loss=7.7139
	step [152/332], loss=9.9687
	step [153/332], loss=13.2954
	step [154/332], loss=12.3340
	step [155/332], loss=12.2802
	step [156/332], loss=9.2593
	step [157/332], loss=8.1700
	step [158/332], loss=11.3860
	step [159/332], loss=10.4681
	step [160/332], loss=11.9561
	step [161/332], loss=11.6209
	step [162/332], loss=8.8344
	step [163/332], loss=9.7411
	step [164/332], loss=9.1696
	step [165/332], loss=11.1176
	step [166/332], loss=10.7476
	step [167/332], loss=9.0599
	step [168/332], loss=11.1833
	step [169/332], loss=11.0669
	step [170/332], loss=8.8787
	step [171/332], loss=9.9819
	step [172/332], loss=10.2983
	step [173/332], loss=9.6060
	step [174/332], loss=11.4055
	step [175/332], loss=10.8604
	step [176/332], loss=9.8637
	step [177/332], loss=10.5520
	step [178/332], loss=10.6558
	step [179/332], loss=12.9141
	step [180/332], loss=11.1712
	step [181/332], loss=9.6280
	step [182/332], loss=11.4894
	step [183/332], loss=11.1506
	step [184/332], loss=11.6835
	step [185/332], loss=11.4607
	step [186/332], loss=10.9116
	step [187/332], loss=13.5393
	step [188/332], loss=11.0657
	step [189/332], loss=10.3488
	step [190/332], loss=12.2558
	step [191/332], loss=9.4966
	step [192/332], loss=12.5155
	step [193/332], loss=12.2141
	step [194/332], loss=11.9607
	step [195/332], loss=11.0531
	step [196/332], loss=12.7842
	step [197/332], loss=13.0355
	step [198/332], loss=10.8078
	step [199/332], loss=10.5697
	step [200/332], loss=8.6617
	step [201/332], loss=9.6756
	step [202/332], loss=8.8692
	step [203/332], loss=10.7447
	step [204/332], loss=10.3989
	step [205/332], loss=10.6413
	step [206/332], loss=9.1004
	step [207/332], loss=10.4584
	step [208/332], loss=10.7433
	step [209/332], loss=9.3699
	step [210/332], loss=12.1080
	step [211/332], loss=10.1497
	step [212/332], loss=10.0019
	step [213/332], loss=12.8670
	step [214/332], loss=11.1717
	step [215/332], loss=10.6544
	step [216/332], loss=11.5848
	step [217/332], loss=10.7200
	step [218/332], loss=8.5529
	step [219/332], loss=13.5738
	step [220/332], loss=11.6193
	step [221/332], loss=10.3049
	step [222/332], loss=10.2854
	step [223/332], loss=11.4342
	step [224/332], loss=11.1596
	step [225/332], loss=11.9253
	step [226/332], loss=11.5425
	step [227/332], loss=10.3266
	step [228/332], loss=14.0197
	step [229/332], loss=9.9912
	step [230/332], loss=10.1602
	step [231/332], loss=11.6094
	step [232/332], loss=10.3703
	step [233/332], loss=11.3636
	step [234/332], loss=11.3369
	step [235/332], loss=10.2328
	step [236/332], loss=10.1256
	step [237/332], loss=14.0196
	step [238/332], loss=14.2553
	step [239/332], loss=11.4911
	step [240/332], loss=9.7888
	step [241/332], loss=11.2128
	step [242/332], loss=10.4109
	step [243/332], loss=8.8682
	step [244/332], loss=10.1922
	step [245/332], loss=10.7303
	step [246/332], loss=13.5463
	step [247/332], loss=8.7356
	step [248/332], loss=10.6563
	step [249/332], loss=11.5831
	step [250/332], loss=10.0661
	step [251/332], loss=9.9856
	step [252/332], loss=10.7921
	step [253/332], loss=9.7537
	step [254/332], loss=9.1600
	step [255/332], loss=13.3496
	step [256/332], loss=13.0010
	step [257/332], loss=11.7751
	step [258/332], loss=10.5441
	step [259/332], loss=11.3003
	step [260/332], loss=10.1932
	step [261/332], loss=8.7870
	step [262/332], loss=12.9713
	step [263/332], loss=9.4065
	step [264/332], loss=11.5522
	step [265/332], loss=9.9246
	step [266/332], loss=8.4741
	step [267/332], loss=13.3936
	step [268/332], loss=10.5901
	step [269/332], loss=10.4037
	step [270/332], loss=9.7293
	step [271/332], loss=10.9535
	step [272/332], loss=11.3998
	step [273/332], loss=8.7099
	step [274/332], loss=8.6386
	step [275/332], loss=10.2705
	step [276/332], loss=11.2002
	step [277/332], loss=12.5070
	step [278/332], loss=14.4092
	step [279/332], loss=9.2174
	step [280/332], loss=11.0717
	step [281/332], loss=10.3545
	step [282/332], loss=13.8188
	step [283/332], loss=11.4450
	step [284/332], loss=11.5440
	step [285/332], loss=12.0298
	step [286/332], loss=10.2357
	step [287/332], loss=9.4523
	step [288/332], loss=11.6199
	step [289/332], loss=9.1358
	step [290/332], loss=8.7276
	step [291/332], loss=11.8407
	step [292/332], loss=10.7010
	step [293/332], loss=11.0048
	step [294/332], loss=11.1228
	step [295/332], loss=11.3952
	step [296/332], loss=13.0428
	step [297/332], loss=9.2475
	step [298/332], loss=10.7875
	step [299/332], loss=9.3571
	step [300/332], loss=11.7059
	step [301/332], loss=10.8020
	step [302/332], loss=12.2892
	step [303/332], loss=8.6120
	step [304/332], loss=8.9888
	step [305/332], loss=9.4520
	step [306/332], loss=11.7210
	step [307/332], loss=9.4882
	step [308/332], loss=9.5515
	step [309/332], loss=10.6892
	step [310/332], loss=10.1348
	step [311/332], loss=9.5199
	step [312/332], loss=10.7219
	step [313/332], loss=11.6291
	step [314/332], loss=11.4318
	step [315/332], loss=10.0544
	step [316/332], loss=12.5246
	step [317/332], loss=9.3266
	step [318/332], loss=11.1564
	step [319/332], loss=11.2349
	step [320/332], loss=11.3871
	step [321/332], loss=10.8576
	step [322/332], loss=12.5539
	step [323/332], loss=12.6000
	step [324/332], loss=12.6597
	step [325/332], loss=14.2515
	step [326/332], loss=11.1261
	step [327/332], loss=11.4350
	step [328/332], loss=11.1431
	step [329/332], loss=11.7503
	step [330/332], loss=12.5547
	step [331/332], loss=11.1296
	step [332/332], loss=5.4617
	Evaluating
	loss=0.0353, precision=0.1300, recall=0.9931, f1=0.5969
Training epoch 9
	step [1/332], loss=10.5431
	step [2/332], loss=9.7969
	step [3/332], loss=11.2098
	step [4/332], loss=11.5198
	step [5/332], loss=10.1162
	step [6/332], loss=11.7282
	step [7/332], loss=11.8941
	step [8/332], loss=10.2688
	step [9/332], loss=12.5635
	step [10/332], loss=9.1133
	step [11/332], loss=12.5019
	step [12/332], loss=10.7502
	step [13/332], loss=10.7896
	step [14/332], loss=10.5607
	step [15/332], loss=10.2937
	step [16/332], loss=11.5358
	step [17/332], loss=9.4041
	step [18/332], loss=10.4415
	step [19/332], loss=12.5847
	step [20/332], loss=10.2260
	step [21/332], loss=9.1112
	step [22/332], loss=9.6955
	step [23/332], loss=9.4847
	step [24/332], loss=9.4483
	step [25/332], loss=8.8344
	step [26/332], loss=10.5215
	step [27/332], loss=12.1219
	step [28/332], loss=10.7126
	step [29/332], loss=8.1650
	step [30/332], loss=10.8683
	step [31/332], loss=9.2972
	step [32/332], loss=10.5631
	step [33/332], loss=7.9407
	step [34/332], loss=11.6203
	step [35/332], loss=10.0235
	step [36/332], loss=8.9574
	step [37/332], loss=10.6969
	step [38/332], loss=11.7907
	step [39/332], loss=11.2010
	step [40/332], loss=11.5234
	step [41/332], loss=9.6726
	step [42/332], loss=11.4900
	step [43/332], loss=10.5705
	step [44/332], loss=10.3958
	step [45/332], loss=9.3507
	step [46/332], loss=9.4080
	step [47/332], loss=9.7764
	step [48/332], loss=10.5198
	step [49/332], loss=11.9827
	step [50/332], loss=10.5257
	step [51/332], loss=10.1851
	step [52/332], loss=10.0751
	step [53/332], loss=10.0569
	step [54/332], loss=11.2443
	step [55/332], loss=12.1658
	step [56/332], loss=9.3048
	step [57/332], loss=9.6834
	step [58/332], loss=9.8391
	step [59/332], loss=7.9843
	step [60/332], loss=11.4132
	step [61/332], loss=10.8544
	step [62/332], loss=9.5572
	step [63/332], loss=11.0050
	step [64/332], loss=10.5195
	step [65/332], loss=10.8664
	step [66/332], loss=10.9190
	step [67/332], loss=11.2708
	step [68/332], loss=10.0405
	step [69/332], loss=10.2721
	step [70/332], loss=10.0742
	step [71/332], loss=11.1946
	step [72/332], loss=12.0416
	step [73/332], loss=7.4256
	step [74/332], loss=10.4690
	step [75/332], loss=9.1655
	step [76/332], loss=10.1551
	step [77/332], loss=9.8204
	step [78/332], loss=11.2768
	step [79/332], loss=9.9487
	step [80/332], loss=10.5569
	step [81/332], loss=10.3921
	step [82/332], loss=10.2837
	step [83/332], loss=11.0334
	step [84/332], loss=10.5006
	step [85/332], loss=10.8429
	step [86/332], loss=8.8795
	step [87/332], loss=11.6569
	step [88/332], loss=8.5956
	step [89/332], loss=10.8005
	step [90/332], loss=9.4756
	step [91/332], loss=9.8190
	step [92/332], loss=11.4135
	step [93/332], loss=12.2664
	step [94/332], loss=12.7399
	step [95/332], loss=8.3075
	step [96/332], loss=10.4812
	step [97/332], loss=10.0741
	step [98/332], loss=10.3099
	step [99/332], loss=10.5486
	step [100/332], loss=10.1835
	step [101/332], loss=10.3999
	step [102/332], loss=12.4275
	step [103/332], loss=9.6862
	step [104/332], loss=9.5826
	step [105/332], loss=10.1128
	step [106/332], loss=11.9761
	step [107/332], loss=11.0480
	step [108/332], loss=11.1882
	step [109/332], loss=10.3200
	step [110/332], loss=9.2000
	step [111/332], loss=14.0143
	step [112/332], loss=11.2708
	step [113/332], loss=9.3873
	step [114/332], loss=11.3068
	step [115/332], loss=9.1543
	step [116/332], loss=9.8353
	step [117/332], loss=9.4710
	step [118/332], loss=10.5512
	step [119/332], loss=11.2086
	step [120/332], loss=10.6169
	step [121/332], loss=10.8709
	step [122/332], loss=13.0559
	step [123/332], loss=12.2876
	step [124/332], loss=7.3946
	step [125/332], loss=9.7491
	step [126/332], loss=10.1275
	step [127/332], loss=7.9563
	step [128/332], loss=9.6159
	step [129/332], loss=11.0835
	step [130/332], loss=11.1148
	step [131/332], loss=12.2916
	step [132/332], loss=8.9111
	step [133/332], loss=9.8296
	step [134/332], loss=10.1349
	step [135/332], loss=11.2483
	step [136/332], loss=10.9556
	step [137/332], loss=12.2565
	step [138/332], loss=10.8909
	step [139/332], loss=9.8200
	step [140/332], loss=8.9715
	step [141/332], loss=11.9776
	step [142/332], loss=11.1916
	step [143/332], loss=12.0632
	step [144/332], loss=10.1674
	step [145/332], loss=10.7859
	step [146/332], loss=10.6256
	step [147/332], loss=9.7709
	step [148/332], loss=9.5294
	step [149/332], loss=9.2917
	step [150/332], loss=11.7801
	step [151/332], loss=9.4967
	step [152/332], loss=8.9853
	step [153/332], loss=12.4628
	step [154/332], loss=11.3573
	step [155/332], loss=8.4430
	step [156/332], loss=9.4036
	step [157/332], loss=10.9126
	step [158/332], loss=11.1529
	step [159/332], loss=8.1464
	step [160/332], loss=10.3944
	step [161/332], loss=9.7280
	step [162/332], loss=10.6648
	step [163/332], loss=11.9397
	step [164/332], loss=10.6055
	step [165/332], loss=11.3302
	step [166/332], loss=10.2056
	step [167/332], loss=11.0467
	step [168/332], loss=10.6580
	step [169/332], loss=10.2767
	step [170/332], loss=8.7957
	step [171/332], loss=10.1900
	step [172/332], loss=8.6541
	step [173/332], loss=10.1227
	step [174/332], loss=11.9245
	step [175/332], loss=8.7689
	step [176/332], loss=9.4219
	step [177/332], loss=9.5307
	step [178/332], loss=11.7489
	step [179/332], loss=8.7700
	step [180/332], loss=10.9254
	step [181/332], loss=8.9905
	step [182/332], loss=9.9825
	step [183/332], loss=8.9384
	step [184/332], loss=8.8521
	step [185/332], loss=8.8526
	step [186/332], loss=9.7490
	step [187/332], loss=9.5417
	step [188/332], loss=10.0145
	step [189/332], loss=10.4260
	step [190/332], loss=12.4310
	step [191/332], loss=9.9963
	step [192/332], loss=10.5230
	step [193/332], loss=9.7562
	step [194/332], loss=9.6662
	step [195/332], loss=8.5693
	step [196/332], loss=8.4677
	step [197/332], loss=12.0741
	step [198/332], loss=8.7586
	step [199/332], loss=10.0381
	step [200/332], loss=8.7515
	step [201/332], loss=7.4900
	step [202/332], loss=8.1047
	step [203/332], loss=9.8042
	step [204/332], loss=9.0590
	step [205/332], loss=9.1338
	step [206/332], loss=9.5665
	step [207/332], loss=11.2138
	step [208/332], loss=10.5289
	step [209/332], loss=10.2982
	step [210/332], loss=9.6539
	step [211/332], loss=10.4952
	step [212/332], loss=9.5372
	step [213/332], loss=12.2191
	step [214/332], loss=9.0178
	step [215/332], loss=8.0495
	step [216/332], loss=11.6080
	step [217/332], loss=11.5523
	step [218/332], loss=10.5032
	step [219/332], loss=7.7421
	step [220/332], loss=9.3199
	step [221/332], loss=12.4991
	step [222/332], loss=10.2583
	step [223/332], loss=9.8866
	step [224/332], loss=11.2821
	step [225/332], loss=9.7341
	step [226/332], loss=8.0166
	step [227/332], loss=10.7607
	step [228/332], loss=10.9369
	step [229/332], loss=12.5377
	step [230/332], loss=7.8209
	step [231/332], loss=9.5173
	step [232/332], loss=14.0722
	step [233/332], loss=9.5411
	step [234/332], loss=9.1568
	step [235/332], loss=11.0512
	step [236/332], loss=10.1004
	step [237/332], loss=11.5808
	step [238/332], loss=10.5903
	step [239/332], loss=10.2710
	step [240/332], loss=10.3639
	step [241/332], loss=9.8504
	step [242/332], loss=9.4549
	step [243/332], loss=11.6717
	step [244/332], loss=9.1517
	step [245/332], loss=9.4115
	step [246/332], loss=7.6321
	step [247/332], loss=10.1545
	step [248/332], loss=8.2027
	step [249/332], loss=12.9578
	step [250/332], loss=8.3248
	step [251/332], loss=10.3245
	step [252/332], loss=10.3173
	step [253/332], loss=10.5224
	step [254/332], loss=10.5331
	step [255/332], loss=13.0995
	step [256/332], loss=11.7178
	step [257/332], loss=10.9409
	step [258/332], loss=9.9160
	step [259/332], loss=11.6379
	step [260/332], loss=10.8191
	step [261/332], loss=9.9980
	step [262/332], loss=10.8121
	step [263/332], loss=9.8102
	step [264/332], loss=10.7040
	step [265/332], loss=9.6049
	step [266/332], loss=9.0466
	step [267/332], loss=9.0208
	step [268/332], loss=9.6102
	step [269/332], loss=8.0269
	step [270/332], loss=11.4678
	step [271/332], loss=10.6522
	step [272/332], loss=9.2343
	step [273/332], loss=9.7738
	step [274/332], loss=12.8317
	step [275/332], loss=10.5965
	step [276/332], loss=9.1783
	step [277/332], loss=9.5215
	step [278/332], loss=12.3142
	step [279/332], loss=9.5062
	step [280/332], loss=9.6355
	step [281/332], loss=10.9368
	step [282/332], loss=11.5060
	step [283/332], loss=10.7535
	step [284/332], loss=9.7803
	step [285/332], loss=10.2755
	step [286/332], loss=11.6558
	step [287/332], loss=8.9216
	step [288/332], loss=14.5182
	step [289/332], loss=9.8537
	step [290/332], loss=8.3291
	step [291/332], loss=11.7030
	step [292/332], loss=11.9983
	step [293/332], loss=10.9260
	step [294/332], loss=9.8416
	step [295/332], loss=11.0769
	step [296/332], loss=12.5044
	step [297/332], loss=9.6158
	step [298/332], loss=9.9446
	step [299/332], loss=9.5197
	step [300/332], loss=10.0021
	step [301/332], loss=11.1208
	step [302/332], loss=8.7981
	step [303/332], loss=10.1319
	step [304/332], loss=11.0989
	step [305/332], loss=8.4985
	step [306/332], loss=8.8027
	step [307/332], loss=10.7189
	step [308/332], loss=10.2266
	step [309/332], loss=9.6494
	step [310/332], loss=9.3109
	step [311/332], loss=8.8355
	step [312/332], loss=10.5524
	step [313/332], loss=9.6728
	step [314/332], loss=11.5624
	step [315/332], loss=10.4688
	step [316/332], loss=10.0349
	step [317/332], loss=10.2353
	step [318/332], loss=9.9898
	step [319/332], loss=11.8458
	step [320/332], loss=8.5709
	step [321/332], loss=8.8378
	step [322/332], loss=11.6987
	step [323/332], loss=11.8309
	step [324/332], loss=12.6908
	step [325/332], loss=9.3552
	step [326/332], loss=11.4366
	step [327/332], loss=8.7120
	step [328/332], loss=11.0048
	step [329/332], loss=8.8883
	step [330/332], loss=9.6165
	step [331/332], loss=11.2948
	step [332/332], loss=6.9110
	Evaluating
	loss=0.0326, precision=0.1213, recall=0.9933, f1=0.5779
Training epoch 10
	step [1/332], loss=9.4237
	step [2/332], loss=10.4853
	step [3/332], loss=9.7686
	step [4/332], loss=11.1857
	step [5/332], loss=8.8626
	step [6/332], loss=9.3211
	step [7/332], loss=10.0983
	step [8/332], loss=8.1336
	step [9/332], loss=8.9820
	step [10/332], loss=13.1258
	step [11/332], loss=10.6048
	step [12/332], loss=8.2601
	step [13/332], loss=8.0835
	step [14/332], loss=9.0500
	step [15/332], loss=10.7536
	step [16/332], loss=12.2336
	step [17/332], loss=9.2584
	step [18/332], loss=8.5975
	step [19/332], loss=9.6673
	step [20/332], loss=8.9898
	step [21/332], loss=9.4081
	step [22/332], loss=9.3890
	step [23/332], loss=10.4913
	step [24/332], loss=10.2833
	step [25/332], loss=11.4254
	step [26/332], loss=10.6359
	step [27/332], loss=8.1147
	step [28/332], loss=9.5492
	step [29/332], loss=10.1153
	step [30/332], loss=9.4992
	step [31/332], loss=10.3290
	step [32/332], loss=8.6305
	step [33/332], loss=11.4776
	step [34/332], loss=13.5267
	step [35/332], loss=10.1984
	step [36/332], loss=12.2920
	step [37/332], loss=9.5782
	step [38/332], loss=9.3471
	step [39/332], loss=11.7981
	step [40/332], loss=14.1762
	step [41/332], loss=10.1135
	step [42/332], loss=11.0317
	step [43/332], loss=9.1668
	step [44/332], loss=11.4436
	step [45/332], loss=8.3191
	step [46/332], loss=9.2239
	step [47/332], loss=8.4017
	step [48/332], loss=8.9520
	step [49/332], loss=8.2914
	step [50/332], loss=9.3766
	step [51/332], loss=8.1368
	step [52/332], loss=10.9399
	step [53/332], loss=11.9512
	step [54/332], loss=13.7724
	step [55/332], loss=9.6669
	step [56/332], loss=11.5369
	step [57/332], loss=9.5444
	step [58/332], loss=10.0181
	step [59/332], loss=11.0090
	step [60/332], loss=9.4201
	step [61/332], loss=10.0024
	step [62/332], loss=12.0765
	step [63/332], loss=12.4718
	step [64/332], loss=8.3008
	step [65/332], loss=11.0558
	step [66/332], loss=9.3659
	step [67/332], loss=9.2545
	step [68/332], loss=7.7821
	step [69/332], loss=10.2317
	step [70/332], loss=10.3743
	step [71/332], loss=11.0669
	step [72/332], loss=8.8394
	step [73/332], loss=8.8368
	step [74/332], loss=10.0385
	step [75/332], loss=10.4437
	step [76/332], loss=10.2771
	step [77/332], loss=7.4074
	step [78/332], loss=12.9117
	step [79/332], loss=9.1829
	step [80/332], loss=9.3797
	step [81/332], loss=9.8030
	step [82/332], loss=11.3472
	step [83/332], loss=9.1663
	step [84/332], loss=10.3167
	step [85/332], loss=10.1186
	step [86/332], loss=11.4590
	step [87/332], loss=8.6614
	step [88/332], loss=9.4921
	step [89/332], loss=8.3685
	step [90/332], loss=7.6623
	step [91/332], loss=10.3381
	step [92/332], loss=8.2887
	step [93/332], loss=9.7139
	step [94/332], loss=10.0723
	step [95/332], loss=7.9591
	step [96/332], loss=14.1974
	step [97/332], loss=9.9493
	step [98/332], loss=10.9831
	step [99/332], loss=12.3112
	step [100/332], loss=9.8138
	step [101/332], loss=10.3514
	step [102/332], loss=7.5779
	step [103/332], loss=11.8936
	step [104/332], loss=9.5726
	step [105/332], loss=11.0453
	step [106/332], loss=12.0255
	step [107/332], loss=10.4340
	step [108/332], loss=8.7425
	step [109/332], loss=9.6525
	step [110/332], loss=9.5408
	step [111/332], loss=8.6659
	step [112/332], loss=12.8223
	step [113/332], loss=7.5612
	step [114/332], loss=9.2949
	step [115/332], loss=8.9201
	step [116/332], loss=9.4492
	step [117/332], loss=11.0021
	step [118/332], loss=10.5109
	step [119/332], loss=10.6225
	step [120/332], loss=9.7487
	step [121/332], loss=10.4931
	step [122/332], loss=13.8233
	step [123/332], loss=10.5063
	step [124/332], loss=8.8083
	step [125/332], loss=9.0310
	step [126/332], loss=8.1188
	step [127/332], loss=10.1252
	step [128/332], loss=12.8667
	step [129/332], loss=11.8491
	step [130/332], loss=10.4310
	step [131/332], loss=9.7566
	step [132/332], loss=9.5504
	step [133/332], loss=10.3677
	step [134/332], loss=11.2745
	step [135/332], loss=8.9524
	step [136/332], loss=8.4337
	step [137/332], loss=10.7307
	step [138/332], loss=10.7348
	step [139/332], loss=11.3251
	step [140/332], loss=8.3135
	step [141/332], loss=11.1988
	step [142/332], loss=8.2587
	step [143/332], loss=8.0137
	step [144/332], loss=8.7398
	step [145/332], loss=9.3063
	step [146/332], loss=7.7545
	step [147/332], loss=10.0008
	step [148/332], loss=8.2189
	step [149/332], loss=10.8388
	step [150/332], loss=8.2986
	step [151/332], loss=10.4850
	step [152/332], loss=8.6420
	step [153/332], loss=10.3590
	step [154/332], loss=9.9230
	step [155/332], loss=10.7158
	step [156/332], loss=9.7709
	step [157/332], loss=8.4926
	step [158/332], loss=9.4325
	step [159/332], loss=9.1293
	step [160/332], loss=7.2691
	step [161/332], loss=11.0511
	step [162/332], loss=11.7580
	step [163/332], loss=10.8800
	step [164/332], loss=8.2970
	step [165/332], loss=10.1675
	step [166/332], loss=10.9079
	step [167/332], loss=10.1064
	step [168/332], loss=9.9323
	step [169/332], loss=8.4802
	step [170/332], loss=9.3517
	step [171/332], loss=11.7162
	step [172/332], loss=7.0381
	step [173/332], loss=13.4665
	step [174/332], loss=9.6339
	step [175/332], loss=10.5089
	step [176/332], loss=9.0847
	step [177/332], loss=9.9741
	step [178/332], loss=11.4979
	step [179/332], loss=11.7138
	step [180/332], loss=10.9718
	step [181/332], loss=7.5887
	step [182/332], loss=9.5395
	step [183/332], loss=10.5643
	step [184/332], loss=8.9681
	step [185/332], loss=9.7815
	step [186/332], loss=9.1557
	step [187/332], loss=9.3744
	step [188/332], loss=9.6274
	step [189/332], loss=10.1828
	step [190/332], loss=9.2664
	step [191/332], loss=7.9068
	step [192/332], loss=9.0850
	step [193/332], loss=9.6474
	step [194/332], loss=8.0522
	step [195/332], loss=10.4654
	step [196/332], loss=7.8627
	step [197/332], loss=8.8711
	step [198/332], loss=11.2236
	step [199/332], loss=8.4679
	step [200/332], loss=8.3788
	step [201/332], loss=10.7811
	step [202/332], loss=11.2211
	step [203/332], loss=8.1907
	step [204/332], loss=10.9469
	step [205/332], loss=7.1137
	step [206/332], loss=9.2215
	step [207/332], loss=8.4014
	step [208/332], loss=10.6183
	step [209/332], loss=9.3282
	step [210/332], loss=10.7289
	step [211/332], loss=10.6252
	step [212/332], loss=8.7498
	step [213/332], loss=10.0073
	step [214/332], loss=8.2399
	step [215/332], loss=10.0301
	step [216/332], loss=11.1002
	step [217/332], loss=11.6513
	step [218/332], loss=7.7804
	step [219/332], loss=7.2295
	step [220/332], loss=9.1324
	step [221/332], loss=9.0556
	step [222/332], loss=10.4711
	step [223/332], loss=7.9597
	step [224/332], loss=8.8689
	step [225/332], loss=9.7073
	step [226/332], loss=11.2803
	step [227/332], loss=11.1056
	step [228/332], loss=8.9190
	step [229/332], loss=8.7403
	step [230/332], loss=11.5210
	step [231/332], loss=9.3216
	step [232/332], loss=9.5773
	step [233/332], loss=9.4715
	step [234/332], loss=7.3626
	step [235/332], loss=9.0370
	step [236/332], loss=10.1382
	step [237/332], loss=9.6645
	step [238/332], loss=9.3896
	step [239/332], loss=13.6956
	step [240/332], loss=10.4488
	step [241/332], loss=9.7763
	step [242/332], loss=8.2683
	step [243/332], loss=8.1010
	step [244/332], loss=11.0830
	step [245/332], loss=8.0091
	step [246/332], loss=10.9995
	step [247/332], loss=9.1343
	step [248/332], loss=10.8127
	step [249/332], loss=7.3822
	step [250/332], loss=8.4219
	step [251/332], loss=9.1265
	step [252/332], loss=9.3968
	step [253/332], loss=8.6783
	step [254/332], loss=12.6764
	step [255/332], loss=7.7665
	step [256/332], loss=9.1605
	step [257/332], loss=7.4140
	step [258/332], loss=9.7893
	step [259/332], loss=8.8340
	step [260/332], loss=8.8702
	step [261/332], loss=9.8187
	step [262/332], loss=8.4920
	step [263/332], loss=8.6666
	step [264/332], loss=9.4340
	step [265/332], loss=10.5046
	step [266/332], loss=12.4266
	step [267/332], loss=9.8723
	step [268/332], loss=8.5013
	step [269/332], loss=10.0083
	step [270/332], loss=10.0868
	step [271/332], loss=8.7845
	step [272/332], loss=11.0191
	step [273/332], loss=9.5343
	step [274/332], loss=8.6480
	step [275/332], loss=7.6980
	step [276/332], loss=8.6846
	step [277/332], loss=7.2539
	step [278/332], loss=10.9401
	step [279/332], loss=9.0256
	step [280/332], loss=10.0592
	step [281/332], loss=8.6817
	step [282/332], loss=8.3305
	step [283/332], loss=10.1427
	step [284/332], loss=10.3146
	step [285/332], loss=10.1717
	step [286/332], loss=9.5759
	step [287/332], loss=8.3490
	step [288/332], loss=7.8884
	step [289/332], loss=11.8375
	step [290/332], loss=11.5860
	step [291/332], loss=8.5684
	step [292/332], loss=6.9294
	step [293/332], loss=8.0004
	step [294/332], loss=9.7985
	step [295/332], loss=10.1268
	step [296/332], loss=8.9367
	step [297/332], loss=8.4687
	step [298/332], loss=10.7431
	step [299/332], loss=9.6788
	step [300/332], loss=11.5131
	step [301/332], loss=9.2511
	step [302/332], loss=9.9604
	step [303/332], loss=8.9113
	step [304/332], loss=8.4360
	step [305/332], loss=7.8999
	step [306/332], loss=10.6870
	step [307/332], loss=7.4996
	step [308/332], loss=6.4051
	step [309/332], loss=9.9170
	step [310/332], loss=9.5109
	step [311/332], loss=8.5396
	step [312/332], loss=10.1122
	step [313/332], loss=7.5447
	step [314/332], loss=9.1363
	step [315/332], loss=8.6885
	step [316/332], loss=8.8913
	step [317/332], loss=10.2285
	step [318/332], loss=7.4873
	step [319/332], loss=7.2102
	step [320/332], loss=8.8732
	step [321/332], loss=8.9609
	step [322/332], loss=9.3456
	step [323/332], loss=11.1057
	step [324/332], loss=8.6003
	step [325/332], loss=9.8334
	step [326/332], loss=9.3451
	step [327/332], loss=9.5083
	step [328/332], loss=10.2352
	step [329/332], loss=10.1128
	step [330/332], loss=9.8463
	step [331/332], loss=9.2684
	step [332/332], loss=4.4299
	Evaluating
	loss=0.0392, precision=0.0927, recall=0.9955, f1=0.5044
Training epoch 11
	step [1/332], loss=9.3193
	step [2/332], loss=9.6635
	step [3/332], loss=6.8294
	step [4/332], loss=9.0429
	step [5/332], loss=6.4802
	step [6/332], loss=8.5708
	step [7/332], loss=9.9791
	step [8/332], loss=8.8056
	step [9/332], loss=8.1331
	step [10/332], loss=8.5046
	step [11/332], loss=10.0178
	step [12/332], loss=9.4956
	step [13/332], loss=10.6796
	step [14/332], loss=8.5938
	step [15/332], loss=8.4742
	step [16/332], loss=10.2874
	step [17/332], loss=8.4041
	step [18/332], loss=7.9114
	step [19/332], loss=9.7150
	step [20/332], loss=10.0367
	step [21/332], loss=10.2366
	step [22/332], loss=8.9949
	step [23/332], loss=10.9551
	step [24/332], loss=9.2723
	step [25/332], loss=10.5366
	step [26/332], loss=9.8157
	step [27/332], loss=8.7357
	step [28/332], loss=9.5548
	step [29/332], loss=9.5002
	step [30/332], loss=8.6921
	step [31/332], loss=8.8178
	step [32/332], loss=9.7053
	step [33/332], loss=10.6274
	step [34/332], loss=8.7967
	step [35/332], loss=9.1763
	step [36/332], loss=7.2791
	step [37/332], loss=8.7964
	step [38/332], loss=8.6511
	step [39/332], loss=8.7750
	step [40/332], loss=9.0857
	step [41/332], loss=8.6693
	step [42/332], loss=11.0544
	step [43/332], loss=10.9590
	step [44/332], loss=7.1782
	step [45/332], loss=10.0483
	step [46/332], loss=8.5876
	step [47/332], loss=11.0059
	step [48/332], loss=12.9107
	step [49/332], loss=9.3730
	step [50/332], loss=11.7922
	step [51/332], loss=9.1091
	step [52/332], loss=11.1365
	step [53/332], loss=8.5712
	step [54/332], loss=9.5304
	step [55/332], loss=9.1879
	step [56/332], loss=9.9480
	step [57/332], loss=8.1175
	step [58/332], loss=8.7335
	step [59/332], loss=9.0659
	step [60/332], loss=9.9245
	step [61/332], loss=10.7214
	step [62/332], loss=9.6723
	step [63/332], loss=9.7073
	step [64/332], loss=9.7714
	step [65/332], loss=8.7309
	step [66/332], loss=9.1895
	step [67/332], loss=12.0714
	step [68/332], loss=9.3922
	step [69/332], loss=10.2996
	step [70/332], loss=10.2618
	step [71/332], loss=7.0515
	step [72/332], loss=11.8963
	step [73/332], loss=12.1151
	step [74/332], loss=10.4052
	step [75/332], loss=9.0529
	step [76/332], loss=12.3570
	step [77/332], loss=11.0261
	step [78/332], loss=8.3611
	step [79/332], loss=9.4670
	step [80/332], loss=11.1034
	step [81/332], loss=7.3368
	step [82/332], loss=13.3193
	step [83/332], loss=9.8037
	step [84/332], loss=11.5860
	step [85/332], loss=10.4994
	step [86/332], loss=8.5776
	step [87/332], loss=8.6776
	step [88/332], loss=7.6675
	step [89/332], loss=7.6383
	step [90/332], loss=11.2488
	step [91/332], loss=9.6481
	step [92/332], loss=11.1177
	step [93/332], loss=10.0421
	step [94/332], loss=10.4720
	step [95/332], loss=9.3131
	step [96/332], loss=9.4912
	step [97/332], loss=8.0320
	step [98/332], loss=8.4146
	step [99/332], loss=11.6370
	step [100/332], loss=9.2420
	step [101/332], loss=9.8260
	step [102/332], loss=9.0721
	step [103/332], loss=8.0675
	step [104/332], loss=10.9813
	step [105/332], loss=11.3068
	step [106/332], loss=12.2340
	step [107/332], loss=8.2483
	step [108/332], loss=10.0058
	step [109/332], loss=9.7444
	step [110/332], loss=8.7070
	step [111/332], loss=8.3438
	step [112/332], loss=10.1742
	step [113/332], loss=9.7857
	step [114/332], loss=11.4186
	step [115/332], loss=7.4072
	step [116/332], loss=9.3094
	step [117/332], loss=6.7219
	step [118/332], loss=9.2165
	step [119/332], loss=8.9832
	step [120/332], loss=8.7133
	step [121/332], loss=8.4730
	step [122/332], loss=10.4980
	step [123/332], loss=9.9388
	step [124/332], loss=9.9617
	step [125/332], loss=10.9491
	step [126/332], loss=9.0482
	step [127/332], loss=9.0909
	step [128/332], loss=10.7744
	step [129/332], loss=8.3482
	step [130/332], loss=9.0539
	step [131/332], loss=8.6297
	step [132/332], loss=7.7461
	step [133/332], loss=10.4815
	step [134/332], loss=10.8326
	step [135/332], loss=9.8084
	step [136/332], loss=9.4738
	step [137/332], loss=7.6187
	step [138/332], loss=7.8247
	step [139/332], loss=9.3354
	step [140/332], loss=10.6979
	step [141/332], loss=8.6173
	step [142/332], loss=7.1491
	step [143/332], loss=9.4742
	step [144/332], loss=8.1074
	step [145/332], loss=7.0907
	step [146/332], loss=9.7493
	step [147/332], loss=8.3586
	step [148/332], loss=8.8576
	step [149/332], loss=7.5639
	step [150/332], loss=7.5939
	step [151/332], loss=8.7796
	step [152/332], loss=7.9966
	step [153/332], loss=8.9027
	step [154/332], loss=8.3483
	step [155/332], loss=11.3184
	step [156/332], loss=8.5407
	step [157/332], loss=7.6680
	step [158/332], loss=8.2157
	step [159/332], loss=9.8900
	step [160/332], loss=9.8478
	step [161/332], loss=8.1405
	step [162/332], loss=9.3491
	step [163/332], loss=9.4617
	step [164/332], loss=10.3832
	step [165/332], loss=8.0661
	step [166/332], loss=10.6263
	step [167/332], loss=9.4568
	step [168/332], loss=11.3376
	step [169/332], loss=10.6506
	step [170/332], loss=7.6695
	step [171/332], loss=8.7419
	step [172/332], loss=11.0677
	step [173/332], loss=9.4548
	step [174/332], loss=7.3046
	step [175/332], loss=10.2094
	step [176/332], loss=9.7413
	step [177/332], loss=8.1203
	step [178/332], loss=7.8081
	step [179/332], loss=8.5950
	step [180/332], loss=8.4631
	step [181/332], loss=10.4291
	step [182/332], loss=8.8623
	step [183/332], loss=8.9195
	step [184/332], loss=7.3923
	step [185/332], loss=11.5442
	step [186/332], loss=11.3173
	step [187/332], loss=7.6073
	step [188/332], loss=9.8702
	step [189/332], loss=8.8480
	step [190/332], loss=9.1290
	step [191/332], loss=12.3660
	step [192/332], loss=8.9643
	step [193/332], loss=7.9064
	step [194/332], loss=9.1318
	step [195/332], loss=8.5115
	step [196/332], loss=10.0698
	step [197/332], loss=10.6719
	step [198/332], loss=11.3489
	step [199/332], loss=8.8642
	step [200/332], loss=7.3091
	step [201/332], loss=7.9877
	step [202/332], loss=8.0296
	step [203/332], loss=10.4894
	step [204/332], loss=9.1898
	step [205/332], loss=7.6462
	step [206/332], loss=9.7136
	step [207/332], loss=8.6470
	step [208/332], loss=7.5327
	step [209/332], loss=8.4208
	step [210/332], loss=9.4338
	step [211/332], loss=7.8413
	step [212/332], loss=7.0418
	step [213/332], loss=9.8289
	step [214/332], loss=6.9567
	step [215/332], loss=7.9232
	step [216/332], loss=8.7363
	step [217/332], loss=11.6913
	step [218/332], loss=10.2560
	step [219/332], loss=7.9049
	step [220/332], loss=7.0819
	step [221/332], loss=10.4103
	step [222/332], loss=7.9612
	step [223/332], loss=8.4451
	step [224/332], loss=8.9681
	step [225/332], loss=9.1781
	step [226/332], loss=7.1295
	step [227/332], loss=8.4544
	step [228/332], loss=11.9488
	step [229/332], loss=10.3659
	step [230/332], loss=8.6937
	step [231/332], loss=7.8552
	step [232/332], loss=9.3384
	step [233/332], loss=7.3133
	step [234/332], loss=11.8114
	step [235/332], loss=11.7105
	step [236/332], loss=9.3373
	step [237/332], loss=10.7550
	step [238/332], loss=13.4546
	step [239/332], loss=7.6069
	step [240/332], loss=7.4640
	step [241/332], loss=7.7018
	step [242/332], loss=10.4931
	step [243/332], loss=11.1069
	step [244/332], loss=10.1560
	step [245/332], loss=10.3663
	step [246/332], loss=8.9683
	step [247/332], loss=7.9063
	step [248/332], loss=9.3022
	step [249/332], loss=9.1229
	step [250/332], loss=6.9850
	step [251/332], loss=7.7254
	step [252/332], loss=6.0516
	step [253/332], loss=8.5136
	step [254/332], loss=9.0463
	step [255/332], loss=9.1273
	step [256/332], loss=7.8362
	step [257/332], loss=8.3748
	step [258/332], loss=8.7317
	step [259/332], loss=8.3463
	step [260/332], loss=9.6644
	step [261/332], loss=8.4622
	step [262/332], loss=12.3762
	step [263/332], loss=11.4271
	step [264/332], loss=8.1678
	step [265/332], loss=8.0613
	step [266/332], loss=9.3655
	step [267/332], loss=12.6350
	step [268/332], loss=10.6484
	step [269/332], loss=8.8688
	step [270/332], loss=8.5370
	step [271/332], loss=8.9711
	step [272/332], loss=9.1114
	step [273/332], loss=9.3495
	step [274/332], loss=8.5108
	step [275/332], loss=9.4432
	step [276/332], loss=8.2973
	step [277/332], loss=9.1861
	step [278/332], loss=9.7069
	step [279/332], loss=9.5099
	step [280/332], loss=9.7552
	step [281/332], loss=10.7247
	step [282/332], loss=8.0317
	step [283/332], loss=11.4935
	step [284/332], loss=11.2095
	step [285/332], loss=9.0969
	step [286/332], loss=11.4492
	step [287/332], loss=10.2695
	step [288/332], loss=9.5549
	step [289/332], loss=8.4323
	step [290/332], loss=8.9375
	step [291/332], loss=9.0537
	step [292/332], loss=9.2371
	step [293/332], loss=10.1240
	step [294/332], loss=7.5298
	step [295/332], loss=10.6827
	step [296/332], loss=10.1501
	step [297/332], loss=7.6197
	step [298/332], loss=7.2005
	step [299/332], loss=8.7904
	step [300/332], loss=7.0393
	step [301/332], loss=7.5460
	step [302/332], loss=7.6185
	step [303/332], loss=11.0284
	step [304/332], loss=9.1912
	step [305/332], loss=10.8470
	step [306/332], loss=8.6313
	step [307/332], loss=8.8139
	step [308/332], loss=9.7121
	step [309/332], loss=8.1128
	step [310/332], loss=8.6382
	step [311/332], loss=9.1571
	step [312/332], loss=8.9643
	step [313/332], loss=9.8013
	step [314/332], loss=8.8723
	step [315/332], loss=10.0559
	step [316/332], loss=9.8629
	step [317/332], loss=10.2301
	step [318/332], loss=9.0253
	step [319/332], loss=12.5798
	step [320/332], loss=8.5452
	step [321/332], loss=9.2375
	step [322/332], loss=8.2948
	step [323/332], loss=8.0432
	step [324/332], loss=10.5594
	step [325/332], loss=9.9276
	step [326/332], loss=7.4755
	step [327/332], loss=9.7049
	step [328/332], loss=8.3100
	step [329/332], loss=10.4925
	step [330/332], loss=9.1535
	step [331/332], loss=9.6994
	step [332/332], loss=3.8315
	Evaluating
	loss=0.0283, precision=0.1227, recall=0.9938, f1=0.5813
Training epoch 12
	step [1/332], loss=9.6479
	step [2/332], loss=7.7640
	step [3/332], loss=8.4956
	step [4/332], loss=9.4683
	step [5/332], loss=7.1589
	step [6/332], loss=8.0512
	step [7/332], loss=8.7809
	step [8/332], loss=8.4012
	step [9/332], loss=9.1636
	step [10/332], loss=11.2929
	step [11/332], loss=9.9791
	step [12/332], loss=9.5173
	step [13/332], loss=8.0771
	step [14/332], loss=9.1828
	step [15/332], loss=9.5765
	step [16/332], loss=9.0348
	step [17/332], loss=8.2974
	step [18/332], loss=7.4976
	step [19/332], loss=8.4555
	step [20/332], loss=7.8265
	step [21/332], loss=9.1833
	step [22/332], loss=7.6012
	step [23/332], loss=9.1234
	step [24/332], loss=8.7880
	step [25/332], loss=8.1753
	step [26/332], loss=13.5654
	step [27/332], loss=9.0177
	step [28/332], loss=10.0735
	step [29/332], loss=7.7540
	step [30/332], loss=10.7764
	step [31/332], loss=9.6825
	step [32/332], loss=8.4835
	step [33/332], loss=9.0071
	step [34/332], loss=8.0670
	step [35/332], loss=9.0198
	step [36/332], loss=8.5151
	step [37/332], loss=10.6916
	step [38/332], loss=8.3536
	step [39/332], loss=7.8377
	step [40/332], loss=9.8775
	step [41/332], loss=8.8144
	step [42/332], loss=8.5597
	step [43/332], loss=9.0700
	step [44/332], loss=9.1009
	step [45/332], loss=8.2655
	step [46/332], loss=8.7570
	step [47/332], loss=6.4677
	step [48/332], loss=9.2455
	step [49/332], loss=10.7084
	step [50/332], loss=7.1510
	step [51/332], loss=7.3382
	step [52/332], loss=11.0769
	step [53/332], loss=9.1902
	step [54/332], loss=8.1752
	step [55/332], loss=11.0517
	step [56/332], loss=8.1355
	step [57/332], loss=7.9499
	step [58/332], loss=9.1947
	step [59/332], loss=9.9344
	step [60/332], loss=8.8429
	step [61/332], loss=8.1152
	step [62/332], loss=8.6222
	step [63/332], loss=7.8860
	step [64/332], loss=9.1171
	step [65/332], loss=8.3306
	step [66/332], loss=9.0572
	step [67/332], loss=9.0948
	step [68/332], loss=6.8297
	step [69/332], loss=7.8332
	step [70/332], loss=7.6466
	step [71/332], loss=9.3408
	step [72/332], loss=9.4997
	step [73/332], loss=7.8805
	step [74/332], loss=10.4259
	step [75/332], loss=7.2215
	step [76/332], loss=11.2480
	step [77/332], loss=9.1183
	step [78/332], loss=7.9379
	step [79/332], loss=7.9183
	step [80/332], loss=8.7347
	step [81/332], loss=9.9100
	step [82/332], loss=10.3494
	step [83/332], loss=11.4448
	step [84/332], loss=7.4424
	step [85/332], loss=7.8446
	step [86/332], loss=10.5306
	step [87/332], loss=8.5152
	step [88/332], loss=9.7160
	step [89/332], loss=7.9817
	step [90/332], loss=8.0483
	step [91/332], loss=7.1001
	step [92/332], loss=8.4389
	step [93/332], loss=8.9338
	step [94/332], loss=7.1586
	step [95/332], loss=7.6680
	step [96/332], loss=12.4656
	step [97/332], loss=9.2549
	step [98/332], loss=8.9839
	step [99/332], loss=9.4791
	step [100/332], loss=10.0193
	step [101/332], loss=8.3419
	step [102/332], loss=11.0773
	step [103/332], loss=8.8837
	step [104/332], loss=10.4540
	step [105/332], loss=7.9739
	step [106/332], loss=10.5556
	step [107/332], loss=7.5157
	step [108/332], loss=9.1617
	step [109/332], loss=7.7439
	step [110/332], loss=7.9066
	step [111/332], loss=13.3500
	step [112/332], loss=7.6043
	step [113/332], loss=9.4139
	step [114/332], loss=9.4397
	step [115/332], loss=8.5823
	step [116/332], loss=9.6214
	step [117/332], loss=7.3316
	step [118/332], loss=9.1206
	step [119/332], loss=10.5736
	step [120/332], loss=10.5042
	step [121/332], loss=8.6396
	step [122/332], loss=9.0860
	step [123/332], loss=7.8094
	step [124/332], loss=7.2192
	step [125/332], loss=8.7890
	step [126/332], loss=9.8299
	step [127/332], loss=9.6914
	step [128/332], loss=10.7533
	step [129/332], loss=9.8332
	step [130/332], loss=8.0446
	step [131/332], loss=9.3809
	step [132/332], loss=10.2339
	step [133/332], loss=7.4894
	step [134/332], loss=9.7039
	step [135/332], loss=9.9138
	step [136/332], loss=11.5496
	step [137/332], loss=8.4190
	step [138/332], loss=10.7421
	step [139/332], loss=8.5650
	step [140/332], loss=8.4822
	step [141/332], loss=10.0722
	step [142/332], loss=9.4153
	step [143/332], loss=10.6077
	step [144/332], loss=5.9208
	step [145/332], loss=8.3082
	step [146/332], loss=7.9675
	step [147/332], loss=9.2701
	step [148/332], loss=8.5522
	step [149/332], loss=7.8691
	step [150/332], loss=9.7767
	step [151/332], loss=9.4065
	step [152/332], loss=7.6308
	step [153/332], loss=9.3063
	step [154/332], loss=8.0948
	step [155/332], loss=10.9582
	step [156/332], loss=8.8271
	step [157/332], loss=10.7499
	step [158/332], loss=8.0996
	step [159/332], loss=6.3888
	step [160/332], loss=8.3986
	step [161/332], loss=9.1951
	step [162/332], loss=7.7653
	step [163/332], loss=8.4276
	step [164/332], loss=8.6550
	step [165/332], loss=7.4042
	step [166/332], loss=11.8158
	step [167/332], loss=9.0254
	step [168/332], loss=7.9057
	step [169/332], loss=7.9641
	step [170/332], loss=7.3749
	step [171/332], loss=9.4291
	step [172/332], loss=8.9555
	step [173/332], loss=9.5831
	step [174/332], loss=12.9246
	step [175/332], loss=11.5299
	step [176/332], loss=7.8039
	step [177/332], loss=7.8169
	step [178/332], loss=9.2617
	step [179/332], loss=9.7191
	step [180/332], loss=7.3171
	step [181/332], loss=10.8633
	step [182/332], loss=8.1500
	step [183/332], loss=7.6561
	step [184/332], loss=7.8376
	step [185/332], loss=8.3397
	step [186/332], loss=8.8456
	step [187/332], loss=10.1117
	step [188/332], loss=9.7159
	step [189/332], loss=6.7445
	step [190/332], loss=10.1781
	step [191/332], loss=9.1786
	step [192/332], loss=9.1816
	step [193/332], loss=9.8645
	step [194/332], loss=9.4516
	step [195/332], loss=10.9973
	step [196/332], loss=9.2560
	step [197/332], loss=8.8023
	step [198/332], loss=8.3886
	step [199/332], loss=8.2434
	step [200/332], loss=9.6906
	step [201/332], loss=8.2239
	step [202/332], loss=8.0013
	step [203/332], loss=8.8008
	step [204/332], loss=7.4171
	step [205/332], loss=10.5467
	step [206/332], loss=9.9880
	step [207/332], loss=9.5463
	step [208/332], loss=9.4178
	step [209/332], loss=7.0415
	step [210/332], loss=8.5000
	step [211/332], loss=7.6570
	step [212/332], loss=9.1892
	step [213/332], loss=8.4204
	step [214/332], loss=8.9222
	step [215/332], loss=12.9910
	step [216/332], loss=9.4044
	step [217/332], loss=9.6981
	step [218/332], loss=10.6561
	step [219/332], loss=10.4041
	step [220/332], loss=9.3099
	step [221/332], loss=8.6789
	step [222/332], loss=8.1111
	step [223/332], loss=7.4145
	step [224/332], loss=7.8420
	step [225/332], loss=10.5801
	step [226/332], loss=8.8693
	step [227/332], loss=8.8301
	step [228/332], loss=8.7371
	step [229/332], loss=11.0769
	step [230/332], loss=9.6390
	step [231/332], loss=8.5464
	step [232/332], loss=9.5757
	step [233/332], loss=6.1469
	step [234/332], loss=6.9841
	step [235/332], loss=9.7528
	step [236/332], loss=6.9326
	step [237/332], loss=9.0581
	step [238/332], loss=8.5959
	step [239/332], loss=9.2827
	step [240/332], loss=7.1764
	step [241/332], loss=9.0150
	step [242/332], loss=6.5169
	step [243/332], loss=9.2195
	step [244/332], loss=8.0756
	step [245/332], loss=7.9234
	step [246/332], loss=6.9027
	step [247/332], loss=8.3009
	step [248/332], loss=10.6009
	step [249/332], loss=7.2092
	step [250/332], loss=8.6858
	step [251/332], loss=9.6272
	step [252/332], loss=10.4544
	step [253/332], loss=11.4266
	step [254/332], loss=8.6993
	step [255/332], loss=8.1123
	step [256/332], loss=10.3360
	step [257/332], loss=8.3094
	step [258/332], loss=8.7750
	step [259/332], loss=9.3931
	step [260/332], loss=10.4278
	step [261/332], loss=5.8284
	step [262/332], loss=10.1088
	step [263/332], loss=7.4543
	step [264/332], loss=6.1358
	step [265/332], loss=10.0610
	step [266/332], loss=11.0195
	step [267/332], loss=10.6983
	step [268/332], loss=8.4211
	step [269/332], loss=7.4411
	step [270/332], loss=8.5261
	step [271/332], loss=7.6334
	step [272/332], loss=8.8802
	step [273/332], loss=11.4467
	step [274/332], loss=10.3914
	step [275/332], loss=10.0415
	step [276/332], loss=8.6778
	step [277/332], loss=10.6101
	step [278/332], loss=8.4361
	step [279/332], loss=11.4192
	step [280/332], loss=8.3091
	step [281/332], loss=10.2055
	step [282/332], loss=8.5043
	step [283/332], loss=7.9563
	step [284/332], loss=8.4763
	step [285/332], loss=8.1770
	step [286/332], loss=7.4929
	step [287/332], loss=7.3626
	step [288/332], loss=9.2565
	step [289/332], loss=8.0602
	step [290/332], loss=6.4237
	step [291/332], loss=8.2149
	step [292/332], loss=9.0682
	step [293/332], loss=6.2717
	step [294/332], loss=8.7495
	step [295/332], loss=8.1197
	step [296/332], loss=10.2325
	step [297/332], loss=8.1334
	step [298/332], loss=8.7058
	step [299/332], loss=10.1270
	step [300/332], loss=7.8277
	step [301/332], loss=7.7330
	step [302/332], loss=7.7344
	step [303/332], loss=8.1648
	step [304/332], loss=8.1638
	step [305/332], loss=11.8036
	step [306/332], loss=6.9176
	step [307/332], loss=7.8874
	step [308/332], loss=9.1726
	step [309/332], loss=11.1731
	step [310/332], loss=8.5947
	step [311/332], loss=9.3368
	step [312/332], loss=9.4318
	step [313/332], loss=11.0060
	step [314/332], loss=8.6862
	step [315/332], loss=8.1946
	step [316/332], loss=9.8083
	step [317/332], loss=8.4164
	step [318/332], loss=7.6411
	step [319/332], loss=7.7282
	step [320/332], loss=8.3232
	step [321/332], loss=8.8420
	step [322/332], loss=9.9079
	step [323/332], loss=7.7610
	step [324/332], loss=9.3161
	step [325/332], loss=10.1372
	step [326/332], loss=7.7869
	step [327/332], loss=8.8041
	step [328/332], loss=8.2909
	step [329/332], loss=7.9267
	step [330/332], loss=9.4710
	step [331/332], loss=9.5772
	step [332/332], loss=4.3744
	Evaluating
	loss=0.0260, precision=0.1406, recall=0.9933, f1=0.6183
Training epoch 13
	step [1/332], loss=9.7226
	step [2/332], loss=9.6931
	step [3/332], loss=7.9712
	step [4/332], loss=7.7665
	step [5/332], loss=8.4088
	step [6/332], loss=10.0998
	step [7/332], loss=8.5786
	step [8/332], loss=8.2625
	step [9/332], loss=8.9473
	step [10/332], loss=6.9346
	step [11/332], loss=8.1954
	step [12/332], loss=11.0988
	step [13/332], loss=6.3535
	step [14/332], loss=8.6483
	step [15/332], loss=8.6996
	step [16/332], loss=9.2452
	step [17/332], loss=8.5267
	step [18/332], loss=9.2521
	step [19/332], loss=10.5414
	step [20/332], loss=8.2977
	step [21/332], loss=7.5689
	step [22/332], loss=8.2115
	step [23/332], loss=8.5775
	step [24/332], loss=9.0301
	step [25/332], loss=7.1989
	step [26/332], loss=9.0150
	step [27/332], loss=10.6144
	step [28/332], loss=8.1278
	step [29/332], loss=9.2556
	step [30/332], loss=7.3639
	step [31/332], loss=8.3474
	step [32/332], loss=6.5825
	step [33/332], loss=8.2059
	step [34/332], loss=7.8299
	step [35/332], loss=6.6033
	step [36/332], loss=8.7112
	step [37/332], loss=9.7289
	step [38/332], loss=6.5729
	step [39/332], loss=8.1967
	step [40/332], loss=8.3858
	step [41/332], loss=12.1151
	step [42/332], loss=9.0171
	step [43/332], loss=7.9538
	step [44/332], loss=7.8030
	step [45/332], loss=9.9010
	step [46/332], loss=9.2102
	step [47/332], loss=9.7914
	step [48/332], loss=9.7215
	step [49/332], loss=9.8412
	step [50/332], loss=8.8121
	step [51/332], loss=11.5751
	step [52/332], loss=7.3463
	step [53/332], loss=9.4794
	step [54/332], loss=8.7219
	step [55/332], loss=7.7487
	step [56/332], loss=8.5026
	step [57/332], loss=8.6942
	step [58/332], loss=7.8868
	step [59/332], loss=7.9897
	step [60/332], loss=7.8809
	step [61/332], loss=9.0510
	step [62/332], loss=6.7889
	step [63/332], loss=8.6374
	step [64/332], loss=8.8765
	step [65/332], loss=8.1289
	step [66/332], loss=8.3275
	step [67/332], loss=8.2558
	step [68/332], loss=9.5881
	step [69/332], loss=7.7956
	step [70/332], loss=7.6753
	step [71/332], loss=9.8229
	step [72/332], loss=8.2635
	step [73/332], loss=8.3446
	step [74/332], loss=7.2483
	step [75/332], loss=8.3148
	step [76/332], loss=7.5531
	step [77/332], loss=11.0467
	step [78/332], loss=6.5974
	step [79/332], loss=8.5635
	step [80/332], loss=8.7247
	step [81/332], loss=6.5312
	step [82/332], loss=9.1763
	step [83/332], loss=8.5809
	step [84/332], loss=8.6128
	step [85/332], loss=7.3960
	step [86/332], loss=6.5977
	step [87/332], loss=7.4716
	step [88/332], loss=9.6733
	step [89/332], loss=6.3745
	step [90/332], loss=8.7768
	step [91/332], loss=8.8440
	step [92/332], loss=8.3088
	step [93/332], loss=7.5965
	step [94/332], loss=8.5625
	step [95/332], loss=10.3706
	step [96/332], loss=6.6895
	step [97/332], loss=8.6625
	step [98/332], loss=10.5510
	step [99/332], loss=8.5862
	step [100/332], loss=8.8119
	step [101/332], loss=8.1697
	step [102/332], loss=7.7744
	step [103/332], loss=7.6054
	step [104/332], loss=6.9133
	step [105/332], loss=7.9696
	step [106/332], loss=9.2755
	step [107/332], loss=10.4692
	step [108/332], loss=7.7174
	step [109/332], loss=8.9654
	step [110/332], loss=8.7525
	step [111/332], loss=8.7319
	step [112/332], loss=7.7119
	step [113/332], loss=6.8088
	step [114/332], loss=8.6615
	step [115/332], loss=10.7934
	step [116/332], loss=10.4913
	step [117/332], loss=8.8927
	step [118/332], loss=8.3731
	step [119/332], loss=7.3905
	step [120/332], loss=10.5173
	step [121/332], loss=9.9660
	step [122/332], loss=9.3263
	step [123/332], loss=8.7343
	step [124/332], loss=11.8521
	step [125/332], loss=9.4801
	step [126/332], loss=7.4097
	step [127/332], loss=10.0031
	step [128/332], loss=9.3435
	step [129/332], loss=8.5517
	step [130/332], loss=7.5646
	step [131/332], loss=7.1057
	step [132/332], loss=8.8558
	step [133/332], loss=8.2639
	step [134/332], loss=7.6193
	step [135/332], loss=7.7448
	step [136/332], loss=8.1892
	step [137/332], loss=8.3759
	step [138/332], loss=8.3029
	step [139/332], loss=9.2887
	step [140/332], loss=10.2036
	step [141/332], loss=9.0207
	step [142/332], loss=11.1954
	step [143/332], loss=8.2638
	step [144/332], loss=7.3849
	step [145/332], loss=7.6886
	step [146/332], loss=7.2350
	step [147/332], loss=6.7568
	step [148/332], loss=8.5876
	step [149/332], loss=7.3391
	step [150/332], loss=6.9070
	step [151/332], loss=7.6643
	step [152/332], loss=7.9194
	step [153/332], loss=9.9477
	step [154/332], loss=6.9989
	step [155/332], loss=9.0541
	step [156/332], loss=6.8554
	step [157/332], loss=8.4521
	step [158/332], loss=9.3492
	step [159/332], loss=10.0003
	step [160/332], loss=8.2396
	step [161/332], loss=9.5357
	step [162/332], loss=8.6525
	step [163/332], loss=6.7310
	step [164/332], loss=9.0658
	step [165/332], loss=7.6500
	step [166/332], loss=8.5993
	step [167/332], loss=6.7069
	step [168/332], loss=8.7667
	step [169/332], loss=9.5024
	step [170/332], loss=9.1426
	step [171/332], loss=7.7505
	step [172/332], loss=6.7622
	step [173/332], loss=10.1742
	step [174/332], loss=9.1177
	step [175/332], loss=8.2191
	step [176/332], loss=8.3249
	step [177/332], loss=10.4073
	step [178/332], loss=10.0387
	step [179/332], loss=10.1740
	step [180/332], loss=7.6158
	step [181/332], loss=6.5327
	step [182/332], loss=9.1271
	step [183/332], loss=7.0387
	step [184/332], loss=8.1181
	step [185/332], loss=7.3083
	step [186/332], loss=9.1194
	step [187/332], loss=9.6677
	step [188/332], loss=7.7911
	step [189/332], loss=11.5504
	step [190/332], loss=9.9673
	step [191/332], loss=9.5493
	step [192/332], loss=7.2422
	step [193/332], loss=7.7825
	step [194/332], loss=9.0021
	step [195/332], loss=7.7606
	step [196/332], loss=11.2184
	step [197/332], loss=7.8424
	step [198/332], loss=9.3891
	step [199/332], loss=8.4464
	step [200/332], loss=7.3015
	step [201/332], loss=8.4615
	step [202/332], loss=7.4868
	step [203/332], loss=8.4780
	step [204/332], loss=6.6024
	step [205/332], loss=7.5809
	step [206/332], loss=8.3491
	step [207/332], loss=8.1884
	step [208/332], loss=7.5364
	step [209/332], loss=6.8801
	step [210/332], loss=10.0509
	step [211/332], loss=6.8150
	step [212/332], loss=7.5476
	step [213/332], loss=9.8166
	step [214/332], loss=9.7631
	step [215/332], loss=9.9990
	step [216/332], loss=8.3088
	step [217/332], loss=9.5744
	step [218/332], loss=8.8770
	step [219/332], loss=10.7571
	step [220/332], loss=8.1321
	step [221/332], loss=6.9991
	step [222/332], loss=7.9102
	step [223/332], loss=8.1603
	step [224/332], loss=10.7072
	step [225/332], loss=9.6829
	step [226/332], loss=7.3043
	step [227/332], loss=10.3208
	step [228/332], loss=8.3774
	step [229/332], loss=7.9807
	step [230/332], loss=7.3290
	step [231/332], loss=8.8980
	step [232/332], loss=6.8876
	step [233/332], loss=6.2959
	step [234/332], loss=9.6921
	step [235/332], loss=8.9555
	step [236/332], loss=7.9343
	step [237/332], loss=8.0854
	step [238/332], loss=6.3042
	step [239/332], loss=6.7117
	step [240/332], loss=7.1726
	step [241/332], loss=7.7636
	step [242/332], loss=9.0730
	step [243/332], loss=10.6487
	step [244/332], loss=7.6683
	step [245/332], loss=8.6589
	step [246/332], loss=6.2174
	step [247/332], loss=7.7914
	step [248/332], loss=8.1163
	step [249/332], loss=10.0676
	step [250/332], loss=8.9845
	step [251/332], loss=7.9818
	step [252/332], loss=7.6194
	step [253/332], loss=8.4587
	step [254/332], loss=7.1321
	step [255/332], loss=6.6125
	step [256/332], loss=7.6387
	step [257/332], loss=6.3183
	step [258/332], loss=10.8719
	step [259/332], loss=7.8107
	step [260/332], loss=7.5026
	step [261/332], loss=7.9812
	step [262/332], loss=6.6899
	step [263/332], loss=7.9490
	step [264/332], loss=8.7557
	step [265/332], loss=8.2105
	step [266/332], loss=8.7069
	step [267/332], loss=8.8757
	step [268/332], loss=6.5287
	step [269/332], loss=7.6215
	step [270/332], loss=7.3431
	step [271/332], loss=7.9251
	step [272/332], loss=7.5060
	step [273/332], loss=8.4409
	step [274/332], loss=8.5639
	step [275/332], loss=10.5877
	step [276/332], loss=6.8811
	step [277/332], loss=7.6910
	step [278/332], loss=8.7070
	step [279/332], loss=8.2788
	step [280/332], loss=7.7176
	step [281/332], loss=8.1612
	step [282/332], loss=7.2932
	step [283/332], loss=8.6571
	step [284/332], loss=9.5249
	step [285/332], loss=10.0386
	step [286/332], loss=8.4502
	step [287/332], loss=9.1204
	step [288/332], loss=8.7790
	step [289/332], loss=7.3496
	step [290/332], loss=8.7266
	step [291/332], loss=7.9880
	step [292/332], loss=8.4046
	step [293/332], loss=8.9049
	step [294/332], loss=7.7768
	step [295/332], loss=8.6179
	step [296/332], loss=10.3575
	step [297/332], loss=10.7032
	step [298/332], loss=8.8159
	step [299/332], loss=6.3179
	step [300/332], loss=7.7416
	step [301/332], loss=6.0733
	step [302/332], loss=7.3207
	step [303/332], loss=8.4810
	step [304/332], loss=8.2683
	step [305/332], loss=7.2984
	step [306/332], loss=9.1652
	step [307/332], loss=8.8611
	step [308/332], loss=8.0265
	step [309/332], loss=8.1130
	step [310/332], loss=7.9191
	step [311/332], loss=8.4941
	step [312/332], loss=8.0172
	step [313/332], loss=8.0990
	step [314/332], loss=6.8382
	step [315/332], loss=8.4263
	step [316/332], loss=8.1635
	step [317/332], loss=8.7658
	step [318/332], loss=8.3866
	step [319/332], loss=9.5441
	step [320/332], loss=6.9714
	step [321/332], loss=9.4051
	step [322/332], loss=6.9075
	step [323/332], loss=8.4757
	step [324/332], loss=7.4674
	step [325/332], loss=8.2240
	step [326/332], loss=7.4585
	step [327/332], loss=8.8165
	step [328/332], loss=13.2239
	step [329/332], loss=8.6885
	step [330/332], loss=6.7340
	step [331/332], loss=7.5318
	step [332/332], loss=4.8602
	Evaluating
	loss=0.0317, precision=0.1077, recall=0.9948, f1=0.5455
Training epoch 14
	step [1/332], loss=8.2796
	step [2/332], loss=8.2797
	step [3/332], loss=9.0929
	step [4/332], loss=6.3823
	step [5/332], loss=9.7109
	step [6/332], loss=8.0854
	step [7/332], loss=7.3783
	step [8/332], loss=9.2823
	step [9/332], loss=7.1366
	step [10/332], loss=8.9468
	step [11/332], loss=8.7020
	step [12/332], loss=9.4303
	step [13/332], loss=8.0014
	step [14/332], loss=7.8609
	step [15/332], loss=6.4211
	step [16/332], loss=7.3482
	step [17/332], loss=7.5500
	step [18/332], loss=7.8970
	step [19/332], loss=9.5723
	step [20/332], loss=8.2730
	step [21/332], loss=8.9027
	step [22/332], loss=8.1289
	step [23/332], loss=10.2235
	step [24/332], loss=8.6125
	step [25/332], loss=7.1782
	step [26/332], loss=8.2585
	step [27/332], loss=8.0058
	step [28/332], loss=10.6178
	step [29/332], loss=7.2106
	step [30/332], loss=8.3544
	step [31/332], loss=9.1262
	step [32/332], loss=9.5971
	step [33/332], loss=8.8694
	step [34/332], loss=6.3622
	step [35/332], loss=7.5588
	step [36/332], loss=8.5370
	step [37/332], loss=6.6160
	step [38/332], loss=9.2005
	step [39/332], loss=7.4413
	step [40/332], loss=6.8177
	step [41/332], loss=6.8931
	step [42/332], loss=6.1412
	step [43/332], loss=8.7037
	step [44/332], loss=7.0374
	step [45/332], loss=8.7376
	step [46/332], loss=7.8425
	step [47/332], loss=7.5247
	step [48/332], loss=7.9459
	step [49/332], loss=8.5411
	step [50/332], loss=6.7673
	step [51/332], loss=8.1635
	step [52/332], loss=7.0847
	step [53/332], loss=9.2601
	step [54/332], loss=8.8741
	step [55/332], loss=7.3829
	step [56/332], loss=7.6984
	step [57/332], loss=8.5009
	step [58/332], loss=9.8292
	step [59/332], loss=7.6763
	step [60/332], loss=7.2324
	step [61/332], loss=8.2471
	step [62/332], loss=7.3801
	step [63/332], loss=5.9267
	step [64/332], loss=9.3057
	step [65/332], loss=7.8449
	step [66/332], loss=9.5755
	step [67/332], loss=7.3981
	step [68/332], loss=9.9123
	step [69/332], loss=8.6857
	step [70/332], loss=7.2904
	step [71/332], loss=8.8427
	step [72/332], loss=9.6079
	step [73/332], loss=7.1350
	step [74/332], loss=8.6822
	step [75/332], loss=8.2652
	step [76/332], loss=7.5225
	step [77/332], loss=9.0391
	step [78/332], loss=9.2454
	step [79/332], loss=7.4146
	step [80/332], loss=8.0998
	step [81/332], loss=7.9798
	step [82/332], loss=8.4195
	step [83/332], loss=8.5691
	step [84/332], loss=6.7450
	step [85/332], loss=9.6973
	step [86/332], loss=7.6351
	step [87/332], loss=10.9765
	step [88/332], loss=8.1861
	step [89/332], loss=8.1358
	step [90/332], loss=8.4043
	step [91/332], loss=8.5843
	step [92/332], loss=7.4230
	step [93/332], loss=6.3753
	step [94/332], loss=9.3641
	step [95/332], loss=8.8486
	step [96/332], loss=7.3737
	step [97/332], loss=6.7502
	step [98/332], loss=7.5549
	step [99/332], loss=9.3214
	step [100/332], loss=10.2223
	step [101/332], loss=7.1273
	step [102/332], loss=7.7844
	step [103/332], loss=8.4242
	step [104/332], loss=9.1975
	step [105/332], loss=8.3067
	step [106/332], loss=7.3456
	step [107/332], loss=7.6848
	step [108/332], loss=8.6915
	step [109/332], loss=7.0154
	step [110/332], loss=7.9814
	step [111/332], loss=8.3594
	step [112/332], loss=8.1983
	step [113/332], loss=7.1820
	step [114/332], loss=7.4868
	step [115/332], loss=9.2108
	step [116/332], loss=6.8744
	step [117/332], loss=7.2156
	step [118/332], loss=8.4534
	step [119/332], loss=7.1518
	step [120/332], loss=10.2172
	step [121/332], loss=7.9200
	step [122/332], loss=7.7559
	step [123/332], loss=8.9806
	step [124/332], loss=9.4808
	step [125/332], loss=6.3504
	step [126/332], loss=6.8039
	step [127/332], loss=9.3765
	step [128/332], loss=9.0888
	step [129/332], loss=5.9212
	step [130/332], loss=8.7145
	step [131/332], loss=7.8898
	step [132/332], loss=8.1878
	step [133/332], loss=5.8316
	step [134/332], loss=8.3582
	step [135/332], loss=11.1866
	step [136/332], loss=9.5581
	step [137/332], loss=6.7419
	step [138/332], loss=7.1060
	step [139/332], loss=6.8088
	step [140/332], loss=8.0078
	step [141/332], loss=6.4077
	step [142/332], loss=7.7602
	step [143/332], loss=11.0850
	step [144/332], loss=6.8430
	step [145/332], loss=6.3167
	step [146/332], loss=7.4774
	step [147/332], loss=7.2331
	step [148/332], loss=9.4311
	step [149/332], loss=8.0694
	step [150/332], loss=8.3158
	step [151/332], loss=8.1202
	step [152/332], loss=8.2102
	step [153/332], loss=7.3340
	step [154/332], loss=6.4654
	step [155/332], loss=6.9781
	step [156/332], loss=8.6600
	step [157/332], loss=8.2536
	step [158/332], loss=8.4594
	step [159/332], loss=6.9172
	step [160/332], loss=9.0002
	step [161/332], loss=6.8822
	step [162/332], loss=6.6525
	step [163/332], loss=8.6138
	step [164/332], loss=7.2218
	step [165/332], loss=8.6948
	step [166/332], loss=10.8967
	step [167/332], loss=7.2069
	step [168/332], loss=7.8352
	step [169/332], loss=10.2817
	step [170/332], loss=8.6216
	step [171/332], loss=8.0190
	step [172/332], loss=7.5348
	step [173/332], loss=7.5931
	step [174/332], loss=7.4009
	step [175/332], loss=11.1486
	step [176/332], loss=10.4818
	step [177/332], loss=7.0038
	step [178/332], loss=8.4787
	step [179/332], loss=8.0608
	step [180/332], loss=8.3816
	step [181/332], loss=7.8643
	step [182/332], loss=9.5766
	step [183/332], loss=7.4093
	step [184/332], loss=7.3302
	step [185/332], loss=7.9169
	step [186/332], loss=8.2782
	step [187/332], loss=6.2901
	step [188/332], loss=6.9972
	step [189/332], loss=6.1912
	step [190/332], loss=8.0225
	step [191/332], loss=9.2435
	step [192/332], loss=9.0359
	step [193/332], loss=8.7057
	step [194/332], loss=7.7203
	step [195/332], loss=8.6739
	step [196/332], loss=8.6462
	step [197/332], loss=7.4352
	step [198/332], loss=9.6286
	step [199/332], loss=8.9732
	step [200/332], loss=8.0636
	step [201/332], loss=8.3621
	step [202/332], loss=9.2242
	step [203/332], loss=6.0152
	step [204/332], loss=7.4361
	step [205/332], loss=9.0187
	step [206/332], loss=6.9525
	step [207/332], loss=6.8088
	step [208/332], loss=9.1092
	step [209/332], loss=6.9071
	step [210/332], loss=8.3845
	step [211/332], loss=7.6017
	step [212/332], loss=8.1844
	step [213/332], loss=9.0507
	step [214/332], loss=7.7652
	step [215/332], loss=7.6225
	step [216/332], loss=5.9605
	step [217/332], loss=9.2145
	step [218/332], loss=7.0946
	step [219/332], loss=6.9206
	step [220/332], loss=7.6668
	step [221/332], loss=7.8282
	step [222/332], loss=9.2188
	step [223/332], loss=11.5520
	step [224/332], loss=9.8633
	step [225/332], loss=8.1730
	step [226/332], loss=8.4350
	step [227/332], loss=8.7535
	step [228/332], loss=8.2810
	step [229/332], loss=6.8220
	step [230/332], loss=8.4533
	step [231/332], loss=8.7042
	step [232/332], loss=11.4960
	step [233/332], loss=6.8430
	step [234/332], loss=6.8147
	step [235/332], loss=9.4221
	step [236/332], loss=7.2814
	step [237/332], loss=7.8116
	step [238/332], loss=7.1920
	step [239/332], loss=6.3563
	step [240/332], loss=7.8290
	step [241/332], loss=7.7393
	step [242/332], loss=9.4196
	step [243/332], loss=9.6159
	step [244/332], loss=7.3285
	step [245/332], loss=8.7740
	step [246/332], loss=8.3112
	step [247/332], loss=8.6255
	step [248/332], loss=6.5515
	step [249/332], loss=7.3799
	step [250/332], loss=8.5770
	step [251/332], loss=7.6459
	step [252/332], loss=10.0091
	step [253/332], loss=8.1906
	step [254/332], loss=8.8306
	step [255/332], loss=9.3040
	step [256/332], loss=7.2989
	step [257/332], loss=9.3186
	step [258/332], loss=10.7799
	step [259/332], loss=8.3191
	step [260/332], loss=9.1599
	step [261/332], loss=7.2800
	step [262/332], loss=7.2105
	step [263/332], loss=10.7527
	step [264/332], loss=7.1490
	step [265/332], loss=7.5371
	step [266/332], loss=7.7167
	step [267/332], loss=5.9034
	step [268/332], loss=9.0256
	step [269/332], loss=7.4832
	step [270/332], loss=8.3685
	step [271/332], loss=9.3666
	step [272/332], loss=7.3073
	step [273/332], loss=7.3919
	step [274/332], loss=7.2532
	step [275/332], loss=7.0699
	step [276/332], loss=7.3123
	step [277/332], loss=9.4409
	step [278/332], loss=5.3070
	step [279/332], loss=6.8790
	step [280/332], loss=8.5048
	step [281/332], loss=7.3360
	step [282/332], loss=8.4356
	step [283/332], loss=6.3878
	step [284/332], loss=6.5315
	step [285/332], loss=8.4876
	step [286/332], loss=8.6971
	step [287/332], loss=8.1958
	step [288/332], loss=8.1539
	step [289/332], loss=7.0233
	step [290/332], loss=7.7545
	step [291/332], loss=7.2233
	step [292/332], loss=7.6741
	step [293/332], loss=6.9640
	step [294/332], loss=7.7346
	step [295/332], loss=9.1178
	step [296/332], loss=11.0247
	step [297/332], loss=6.0030
	step [298/332], loss=5.9206
	step [299/332], loss=8.1708
	step [300/332], loss=8.3385
	step [301/332], loss=7.4418
	step [302/332], loss=7.2994
	step [303/332], loss=8.4120
	step [304/332], loss=8.5901
	step [305/332], loss=8.2058
	step [306/332], loss=7.3218
	step [307/332], loss=8.8407
	step [308/332], loss=9.3079
	step [309/332], loss=8.0257
	step [310/332], loss=8.8216
	step [311/332], loss=8.8370
	step [312/332], loss=7.9259
	step [313/332], loss=8.3599
	step [314/332], loss=7.4162
	step [315/332], loss=6.5012
	step [316/332], loss=7.6392
	step [317/332], loss=8.3537
	step [318/332], loss=8.9438
	step [319/332], loss=7.4443
	step [320/332], loss=4.8993
	step [321/332], loss=7.2001
	step [322/332], loss=7.7601
	step [323/332], loss=7.2983
	step [324/332], loss=6.6128
	step [325/332], loss=9.5509
	step [326/332], loss=7.2386
	step [327/332], loss=7.6781
	step [328/332], loss=7.8351
	step [329/332], loss=5.5224
	step [330/332], loss=8.9187
	step [331/332], loss=8.6390
	step [332/332], loss=4.7029
	Evaluating
	loss=0.0245, precision=0.1385, recall=0.9932, f1=0.6142
Training epoch 15
	step [1/332], loss=5.6914
	step [2/332], loss=7.7462
	step [3/332], loss=7.9068
	step [4/332], loss=6.2717
	step [5/332], loss=6.7442
	step [6/332], loss=8.0088
	step [7/332], loss=8.7517
	step [8/332], loss=7.3420
	step [9/332], loss=7.0312
	step [10/332], loss=9.7246
	step [11/332], loss=9.7437
	step [12/332], loss=6.7440
	step [13/332], loss=7.4733
	step [14/332], loss=8.2608
	step [15/332], loss=8.2690
	step [16/332], loss=7.8229
	step [17/332], loss=8.2500
	step [18/332], loss=7.4209
	step [19/332], loss=6.5159
	step [20/332], loss=7.9930
	step [21/332], loss=6.7648
	step [22/332], loss=8.2777
	step [23/332], loss=8.9450
	step [24/332], loss=7.3109
	step [25/332], loss=6.6590
	step [26/332], loss=8.0963
	step [27/332], loss=7.6075
	step [28/332], loss=8.2345
	step [29/332], loss=8.0338
	step [30/332], loss=9.2293
	step [31/332], loss=6.6150
	step [32/332], loss=7.3503
	step [33/332], loss=7.7971
	step [34/332], loss=6.7437
	step [35/332], loss=8.1008
	step [36/332], loss=7.8653
	step [37/332], loss=6.5582
	step [38/332], loss=7.7111
	step [39/332], loss=8.3708
	step [40/332], loss=8.4184
	step [41/332], loss=7.5240
	step [42/332], loss=7.9511
	step [43/332], loss=9.0152
	step [44/332], loss=7.0348
	step [45/332], loss=8.4884
	step [46/332], loss=7.8475
	step [47/332], loss=9.6587
	step [48/332], loss=8.6321
	step [49/332], loss=7.2322
	step [50/332], loss=5.7455
	step [51/332], loss=8.0207
	step [52/332], loss=9.5672
	step [53/332], loss=6.5049
	step [54/332], loss=6.2175
	step [55/332], loss=8.6423
	step [56/332], loss=9.6669
	step [57/332], loss=6.8692
	step [58/332], loss=8.6076
	step [59/332], loss=7.0862
	step [60/332], loss=7.0675
	step [61/332], loss=6.7094
	step [62/332], loss=8.5752
	step [63/332], loss=9.6185
	step [64/332], loss=7.6892
	step [65/332], loss=8.6479
	step [66/332], loss=6.7695
	step [67/332], loss=6.7435
	step [68/332], loss=8.7456
	step [69/332], loss=6.7700
	step [70/332], loss=9.2768
	step [71/332], loss=8.0718
	step [72/332], loss=8.8314
	step [73/332], loss=8.2921
	step [74/332], loss=7.7953
	step [75/332], loss=8.8894
	step [76/332], loss=7.5381
	step [77/332], loss=6.2018
	step [78/332], loss=8.4542
	step [79/332], loss=7.0512
	step [80/332], loss=6.2778
	step [81/332], loss=7.2315
	step [82/332], loss=7.7452
	step [83/332], loss=8.0125
	step [84/332], loss=8.5453
	step [85/332], loss=9.0225
	step [86/332], loss=7.5479
	step [87/332], loss=6.7069
	step [88/332], loss=7.0506
	step [89/332], loss=10.4731
	step [90/332], loss=8.0661
	step [91/332], loss=7.8089
	step [92/332], loss=7.1211
	step [93/332], loss=8.5913
	step [94/332], loss=7.7649
	step [95/332], loss=8.6016
	step [96/332], loss=8.1493
	step [97/332], loss=5.6524
	step [98/332], loss=10.4089
	step [99/332], loss=7.8578
	step [100/332], loss=6.0118
	step [101/332], loss=7.4078
	step [102/332], loss=9.3163
	step [103/332], loss=7.2522
	step [104/332], loss=7.3713
	step [105/332], loss=8.9531
	step [106/332], loss=8.6083
	step [107/332], loss=7.0674
	step [108/332], loss=7.7580
	step [109/332], loss=6.3196
	step [110/332], loss=9.5013
	step [111/332], loss=7.3341
	step [112/332], loss=8.3215
	step [113/332], loss=6.0557
	step [114/332], loss=8.4222
	step [115/332], loss=9.2591
	step [116/332], loss=10.2027
	step [117/332], loss=8.3414
	step [118/332], loss=7.4940
	step [119/332], loss=5.9598
	step [120/332], loss=6.6968
	step [121/332], loss=8.4397
	step [122/332], loss=8.8707
	step [123/332], loss=7.6132
	step [124/332], loss=6.8671
	step [125/332], loss=7.6045
	step [126/332], loss=8.9791
	step [127/332], loss=7.0540
	step [128/332], loss=7.9698
	step [129/332], loss=7.9867
	step [130/332], loss=5.2862
	step [131/332], loss=7.6983
	step [132/332], loss=8.7002
	step [133/332], loss=7.9303
	step [134/332], loss=6.6228
	step [135/332], loss=8.0678
	step [136/332], loss=6.9043
	step [137/332], loss=6.4188
	step [138/332], loss=7.8460
	step [139/332], loss=8.9347
	step [140/332], loss=6.2148
	step [141/332], loss=8.2862
	step [142/332], loss=8.5710
	step [143/332], loss=6.2835
	step [144/332], loss=8.1855
	step [145/332], loss=8.6134
	step [146/332], loss=8.2422
	step [147/332], loss=7.8550
	step [148/332], loss=8.4583
	step [149/332], loss=7.2669
	step [150/332], loss=5.6763
	step [151/332], loss=8.5320
	step [152/332], loss=7.3401
	step [153/332], loss=6.5699
	step [154/332], loss=9.1501
	step [155/332], loss=8.6716
	step [156/332], loss=7.6167
	step [157/332], loss=7.8003
	step [158/332], loss=7.0584
	step [159/332], loss=8.0467
	step [160/332], loss=7.6022
	step [161/332], loss=6.9146
	step [162/332], loss=6.5420
	step [163/332], loss=7.5313
	step [164/332], loss=7.3004
	step [165/332], loss=8.6751
	step [166/332], loss=9.9605
	step [167/332], loss=7.1878
	step [168/332], loss=7.4829
	step [169/332], loss=7.5670
	step [170/332], loss=8.5321
	step [171/332], loss=8.4583
	step [172/332], loss=5.9648
	step [173/332], loss=7.6502
	step [174/332], loss=8.2856
	step [175/332], loss=9.2769
	step [176/332], loss=6.3323
	step [177/332], loss=5.4877
	step [178/332], loss=7.4314
	step [179/332], loss=6.6506
	step [180/332], loss=8.1905
	step [181/332], loss=6.8154
	step [182/332], loss=7.8907
	step [183/332], loss=7.6657
	step [184/332], loss=11.7548
	step [185/332], loss=5.5608
	step [186/332], loss=7.6472
	step [187/332], loss=6.3663
	step [188/332], loss=9.4437
	step [189/332], loss=8.7890
	step [190/332], loss=6.9393
	step [191/332], loss=9.4477
	step [192/332], loss=8.9378
	step [193/332], loss=6.5342
	step [194/332], loss=8.0063
	step [195/332], loss=7.4674
	step [196/332], loss=7.3610
	step [197/332], loss=8.1173
	step [198/332], loss=7.7047
	step [199/332], loss=7.2624
	step [200/332], loss=8.4326
	step [201/332], loss=6.5507
	step [202/332], loss=7.9111
	step [203/332], loss=8.1183
	step [204/332], loss=8.3387
	step [205/332], loss=8.6028
	step [206/332], loss=7.2122
	step [207/332], loss=6.2230
	step [208/332], loss=7.6722
	step [209/332], loss=9.7187
	step [210/332], loss=8.2123
	step [211/332], loss=8.0560
	step [212/332], loss=7.8269
	step [213/332], loss=8.8874
	step [214/332], loss=8.1200
	step [215/332], loss=6.3252
	step [216/332], loss=7.4305
	step [217/332], loss=7.1645
	step [218/332], loss=6.1800
	step [219/332], loss=8.1621
	step [220/332], loss=6.1676
	step [221/332], loss=7.4749
	step [222/332], loss=6.1000
	step [223/332], loss=11.4544
	step [224/332], loss=6.7436
	step [225/332], loss=7.3005
	step [226/332], loss=6.2043
	step [227/332], loss=7.4968
	step [228/332], loss=6.6415
	step [229/332], loss=6.7938
	step [230/332], loss=7.7058
	step [231/332], loss=7.2979
	step [232/332], loss=5.9926
	step [233/332], loss=8.3077
	step [234/332], loss=5.8615
	step [235/332], loss=7.4027
	step [236/332], loss=7.3964
	step [237/332], loss=8.1820
	step [238/332], loss=8.5365
	step [239/332], loss=7.6197
	step [240/332], loss=8.0033
	step [241/332], loss=7.5604
	step [242/332], loss=7.6753
	step [243/332], loss=8.6670
	step [244/332], loss=8.8545
	step [245/332], loss=8.9200
	step [246/332], loss=6.2627
	step [247/332], loss=7.3323
	step [248/332], loss=8.0973
	step [249/332], loss=7.4730
	step [250/332], loss=6.4598
	step [251/332], loss=8.5342
	step [252/332], loss=8.3310
	step [253/332], loss=8.0776
	step [254/332], loss=8.2142
	step [255/332], loss=8.5139
	step [256/332], loss=9.0490
	step [257/332], loss=8.8407
	step [258/332], loss=8.2123
	step [259/332], loss=7.5471
	step [260/332], loss=8.3084
	step [261/332], loss=6.3437
	step [262/332], loss=8.6477
	step [263/332], loss=6.9281
	step [264/332], loss=8.0607
	step [265/332], loss=7.4667
	step [266/332], loss=6.5568
	step [267/332], loss=7.8394
	step [268/332], loss=7.7992
	step [269/332], loss=9.4390
	step [270/332], loss=7.9607
	step [271/332], loss=10.3814
	step [272/332], loss=9.9971
	step [273/332], loss=8.0492
	step [274/332], loss=8.0737
	step [275/332], loss=8.0717
	step [276/332], loss=11.1725
	step [277/332], loss=8.4946
	step [278/332], loss=8.6700
	step [279/332], loss=8.3857
	step [280/332], loss=8.6290
	step [281/332], loss=7.7651
	step [282/332], loss=7.1211
	step [283/332], loss=6.7900
	step [284/332], loss=7.4764
	step [285/332], loss=6.4515
	step [286/332], loss=11.0938
	step [287/332], loss=8.0158
	step [288/332], loss=9.0992
	step [289/332], loss=7.6932
	step [290/332], loss=6.4948
	step [291/332], loss=6.0548
	step [292/332], loss=8.7005
	step [293/332], loss=6.7078
	step [294/332], loss=7.0909
	step [295/332], loss=7.7041
	step [296/332], loss=8.5954
	step [297/332], loss=7.1050
	step [298/332], loss=6.2133
	step [299/332], loss=7.3875
	step [300/332], loss=7.2853
	step [301/332], loss=6.5202
	step [302/332], loss=6.0424
	step [303/332], loss=8.4426
	step [304/332], loss=8.5913
	step [305/332], loss=8.9315
	step [306/332], loss=6.8561
	step [307/332], loss=11.4714
	step [308/332], loss=7.5770
	step [309/332], loss=7.3496
	step [310/332], loss=6.8440
	step [311/332], loss=7.7809
	step [312/332], loss=8.5278
	step [313/332], loss=7.3940
	step [314/332], loss=6.8659
	step [315/332], loss=9.0972
	step [316/332], loss=8.9115
	step [317/332], loss=8.3362
	step [318/332], loss=5.9524
	step [319/332], loss=7.9642
	step [320/332], loss=7.8226
	step [321/332], loss=8.5020
	step [322/332], loss=5.5183
	step [323/332], loss=6.9790
	step [324/332], loss=7.3662
	step [325/332], loss=7.2658
	step [326/332], loss=7.5666
	step [327/332], loss=9.2986
	step [328/332], loss=6.6227
	step [329/332], loss=7.2662
	step [330/332], loss=7.1995
	step [331/332], loss=6.7396
	step [332/332], loss=3.6691
	Evaluating
	loss=0.0257, precision=0.1316, recall=0.9941, f1=0.6005
Training epoch 16
	step [1/332], loss=5.7444
	step [2/332], loss=5.8832
	step [3/332], loss=7.4936
	step [4/332], loss=6.7600
	step [5/332], loss=7.1746
	step [6/332], loss=7.4602
	step [7/332], loss=5.7732
	step [8/332], loss=8.6587
	step [9/332], loss=7.8520
	step [10/332], loss=8.1540
	step [11/332], loss=9.0447
	step [12/332], loss=6.5861
	step [13/332], loss=9.1265
	step [14/332], loss=7.2182
	step [15/332], loss=7.9245
	step [16/332], loss=6.8697
	step [17/332], loss=8.2178
	step [18/332], loss=7.7555
	step [19/332], loss=6.9759
	step [20/332], loss=8.9507
	step [21/332], loss=9.3497
	step [22/332], loss=10.1512
	step [23/332], loss=6.3162
	step [24/332], loss=9.9637
	step [25/332], loss=6.7380
	step [26/332], loss=7.5266
	step [27/332], loss=8.0297
	step [28/332], loss=7.0545
	step [29/332], loss=7.3330
	step [30/332], loss=7.2946
	step [31/332], loss=6.8823
	step [32/332], loss=7.7063
	step [33/332], loss=6.9710
	step [34/332], loss=7.4571
	step [35/332], loss=10.4385
	step [36/332], loss=7.1888
	step [37/332], loss=7.3836
	step [38/332], loss=6.5075
	step [39/332], loss=6.6026
	step [40/332], loss=8.9086
	step [41/332], loss=8.4460
	step [42/332], loss=5.5681
	step [43/332], loss=6.6206
	step [44/332], loss=8.6886
	step [45/332], loss=6.2025
	step [46/332], loss=7.5084
	step [47/332], loss=4.9909
	step [48/332], loss=7.1221
	step [49/332], loss=10.0156
	step [50/332], loss=6.3866
	step [51/332], loss=6.5349
	step [52/332], loss=8.1557
	step [53/332], loss=6.9268
	step [54/332], loss=7.5576
	step [55/332], loss=8.1028
	step [56/332], loss=7.0976
	step [57/332], loss=7.1892
	step [58/332], loss=8.5513
	step [59/332], loss=6.9206
	step [60/332], loss=7.6042
	step [61/332], loss=7.3465
	step [62/332], loss=6.1847
	step [63/332], loss=9.3456
	step [64/332], loss=7.6587
	step [65/332], loss=8.7534
	step [66/332], loss=6.4123
	step [67/332], loss=8.0305
	step [68/332], loss=7.4264
	step [69/332], loss=6.9279
	step [70/332], loss=6.7598
	step [71/332], loss=9.6952
	step [72/332], loss=7.7283
	step [73/332], loss=6.7448
	step [74/332], loss=7.0488
	step [75/332], loss=7.1222
	step [76/332], loss=7.7074
	step [77/332], loss=8.1036
	step [78/332], loss=8.9711
	step [79/332], loss=8.2692
	step [80/332], loss=7.4049
	step [81/332], loss=8.1585
	step [82/332], loss=6.9086
	step [83/332], loss=7.0714
	step [84/332], loss=7.4986
	step [85/332], loss=7.9509
	step [86/332], loss=6.4090
	step [87/332], loss=7.9498
	step [88/332], loss=8.0251
	step [89/332], loss=8.3781
	step [90/332], loss=7.8288
	step [91/332], loss=7.4604
	step [92/332], loss=7.5737
	step [93/332], loss=5.9618
	step [94/332], loss=7.8762
	step [95/332], loss=8.3293
	step [96/332], loss=6.6219
	step [97/332], loss=7.6833
	step [98/332], loss=7.2850
	step [99/332], loss=7.0531
	step [100/332], loss=6.0180
	step [101/332], loss=8.9959
	step [102/332], loss=7.0497
	step [103/332], loss=8.1279
	step [104/332], loss=5.7195
	step [105/332], loss=6.8965
	step [106/332], loss=8.8268
	step [107/332], loss=7.7008
	step [108/332], loss=8.6743
	step [109/332], loss=8.8062
	step [110/332], loss=7.8111
	step [111/332], loss=9.9193
	step [112/332], loss=6.2151
	step [113/332], loss=7.3493
	step [114/332], loss=6.6097
	step [115/332], loss=9.6475
	step [116/332], loss=7.7268
	step [117/332], loss=8.6902
	step [118/332], loss=6.8897
	step [119/332], loss=11.1365
	step [120/332], loss=8.7717
	step [121/332], loss=9.2199
	step [122/332], loss=9.9538
	step [123/332], loss=5.5692
	step [124/332], loss=6.9728
	step [125/332], loss=6.7234
	step [126/332], loss=8.7620
	step [127/332], loss=8.2158
	step [128/332], loss=6.5270
	step [129/332], loss=6.8725
	step [130/332], loss=6.8784
	step [131/332], loss=5.3817
	step [132/332], loss=8.8846
	step [133/332], loss=6.5846
	step [134/332], loss=6.9332
	step [135/332], loss=7.2346
	step [136/332], loss=9.3208
	step [137/332], loss=8.0907
	step [138/332], loss=8.3226
	step [139/332], loss=9.6750
	step [140/332], loss=9.3652
	step [141/332], loss=8.6485
	step [142/332], loss=6.6117
	step [143/332], loss=8.4899
	step [144/332], loss=6.3963
	step [145/332], loss=6.6979
	step [146/332], loss=8.6375
	step [147/332], loss=8.3692
	step [148/332], loss=5.1232
	step [149/332], loss=7.3289
	step [150/332], loss=5.8783
	step [151/332], loss=6.6959
	step [152/332], loss=9.4432
	step [153/332], loss=5.9858
	step [154/332], loss=10.3172
	step [155/332], loss=5.5280
	step [156/332], loss=8.0264
	step [157/332], loss=8.7019
	step [158/332], loss=8.1794
	step [159/332], loss=7.3606
	step [160/332], loss=8.1743
	step [161/332], loss=7.1398
	step [162/332], loss=7.1742
	step [163/332], loss=8.0854
	step [164/332], loss=6.8943
	step [165/332], loss=6.5056
	step [166/332], loss=6.8486
	step [167/332], loss=10.7479
	step [168/332], loss=8.7883
	step [169/332], loss=4.8357
	step [170/332], loss=7.1576
	step [171/332], loss=9.0255
	step [172/332], loss=6.7955
	step [173/332], loss=8.0826
	step [174/332], loss=5.2730
	step [175/332], loss=5.2391
	step [176/332], loss=7.9041
	step [177/332], loss=7.5297
	step [178/332], loss=6.1311
	step [179/332], loss=7.6055
	step [180/332], loss=5.2611
	step [181/332], loss=7.9864
	step [182/332], loss=8.3110
	step [183/332], loss=7.3743
	step [184/332], loss=7.3076
	step [185/332], loss=10.8662
	step [186/332], loss=6.7752
	step [187/332], loss=6.3162
	step [188/332], loss=8.2016
	step [189/332], loss=6.2729
	step [190/332], loss=5.9692
	step [191/332], loss=7.8151
	step [192/332], loss=6.6614
	step [193/332], loss=8.1837
	step [194/332], loss=7.6544
	step [195/332], loss=7.8544
	step [196/332], loss=7.5948
	step [197/332], loss=7.1689
	step [198/332], loss=8.0343
	step [199/332], loss=7.4034
	step [200/332], loss=5.4003
	step [201/332], loss=6.3966
	step [202/332], loss=6.9739
	step [203/332], loss=6.2559
	step [204/332], loss=6.4127
	step [205/332], loss=10.3265
	step [206/332], loss=9.4692
	step [207/332], loss=7.4007
	step [208/332], loss=6.2501
	step [209/332], loss=6.5718
	step [210/332], loss=8.0057
	step [211/332], loss=6.9571
	step [212/332], loss=6.7976
	step [213/332], loss=5.7666
	step [214/332], loss=7.5639
	step [215/332], loss=5.1508
	step [216/332], loss=6.5377
	step [217/332], loss=10.0591
	step [218/332], loss=7.5262
	step [219/332], loss=9.1059
	step [220/332], loss=7.0718
	step [221/332], loss=6.7785
	step [222/332], loss=6.7098
	step [223/332], loss=5.9114
	step [224/332], loss=6.1300
	step [225/332], loss=7.2591
	step [226/332], loss=6.6077
	step [227/332], loss=4.9040
	step [228/332], loss=8.2011
	step [229/332], loss=8.0420
	step [230/332], loss=10.1703
	step [231/332], loss=6.8339
	step [232/332], loss=10.1000
	step [233/332], loss=7.1694
	step [234/332], loss=7.0083
	step [235/332], loss=8.7114
	step [236/332], loss=7.5833
	step [237/332], loss=6.9910
	step [238/332], loss=6.9822
	step [239/332], loss=5.9882
	step [240/332], loss=8.4146
	step [241/332], loss=8.0813
	step [242/332], loss=7.1443
	step [243/332], loss=7.9652
	step [244/332], loss=8.2973
	step [245/332], loss=7.5503
	step [246/332], loss=5.8597
	step [247/332], loss=7.7908
	step [248/332], loss=5.6177
	step [249/332], loss=7.1320
	step [250/332], loss=7.9514
	step [251/332], loss=6.1427
	step [252/332], loss=7.5424
	step [253/332], loss=6.2452
	step [254/332], loss=5.8616
	step [255/332], loss=7.8022
	step [256/332], loss=7.9888
	step [257/332], loss=7.1092
	step [258/332], loss=8.1489
	step [259/332], loss=5.5354
	step [260/332], loss=6.2306
	step [261/332], loss=7.9186
	step [262/332], loss=8.2779
	step [263/332], loss=6.1745
	step [264/332], loss=6.9000
	step [265/332], loss=6.1824
	step [266/332], loss=8.1080
	step [267/332], loss=6.5647
	step [268/332], loss=7.9112
	step [269/332], loss=8.6627
	step [270/332], loss=7.2287
	step [271/332], loss=5.5631
	step [272/332], loss=6.6678
	step [273/332], loss=7.8265
	step [274/332], loss=7.3481
	step [275/332], loss=6.4418
	step [276/332], loss=5.2156
	step [277/332], loss=6.9919
	step [278/332], loss=10.3180
	step [279/332], loss=8.2338
	step [280/332], loss=7.3215
	step [281/332], loss=10.5487
	step [282/332], loss=8.4445
	step [283/332], loss=8.9394
	step [284/332], loss=7.9629
	step [285/332], loss=6.9632
	step [286/332], loss=7.0462
	step [287/332], loss=7.6766
	step [288/332], loss=6.8803
	step [289/332], loss=11.0800
	step [290/332], loss=8.0099
	step [291/332], loss=7.0270
	step [292/332], loss=8.0344
	step [293/332], loss=6.9772
	step [294/332], loss=7.6838
	step [295/332], loss=7.2309
	step [296/332], loss=8.6746
	step [297/332], loss=10.4597
	step [298/332], loss=7.3426
	step [299/332], loss=7.8415
	step [300/332], loss=6.2411
	step [301/332], loss=7.6585
	step [302/332], loss=7.2838
	step [303/332], loss=6.0823
	step [304/332], loss=7.7990
	step [305/332], loss=7.3221
	step [306/332], loss=7.0677
	step [307/332], loss=8.8501
	step [308/332], loss=10.2966
	step [309/332], loss=6.5747
	step [310/332], loss=6.3504
	step [311/332], loss=7.8169
	step [312/332], loss=8.5044
	step [313/332], loss=7.4283
	step [314/332], loss=6.9839
	step [315/332], loss=5.9339
	step [316/332], loss=5.9664
	step [317/332], loss=9.6150
	step [318/332], loss=7.6285
	step [319/332], loss=6.1831
	step [320/332], loss=7.3544
	step [321/332], loss=7.7277
	step [322/332], loss=5.4007
	step [323/332], loss=6.4664
	step [324/332], loss=6.4315
	step [325/332], loss=5.6409
	step [326/332], loss=6.1114
	step [327/332], loss=7.9467
	step [328/332], loss=6.8553
	step [329/332], loss=6.3694
	step [330/332], loss=6.5024
	step [331/332], loss=7.6458
	step [332/332], loss=5.0402
	Evaluating
	loss=0.0238, precision=0.1438, recall=0.9934, f1=0.6244
Training epoch 17
	step [1/332], loss=6.2619
	step [2/332], loss=6.0865
	step [3/332], loss=5.8495
	step [4/332], loss=8.5216
	step [5/332], loss=7.7015
	step [6/332], loss=8.4558
	step [7/332], loss=6.3478
	step [8/332], loss=6.6047
	step [9/332], loss=6.0115
	step [10/332], loss=7.7686
	step [11/332], loss=6.6349
	step [12/332], loss=9.4645
	step [13/332], loss=7.9132
	step [14/332], loss=7.3677
	step [15/332], loss=7.9083
	step [16/332], loss=8.8086
	step [17/332], loss=7.2325
	step [18/332], loss=6.0266
	step [19/332], loss=8.5614
	step [20/332], loss=7.8286
	step [21/332], loss=6.8607
	step [22/332], loss=5.1352
	step [23/332], loss=7.8042
	step [24/332], loss=7.7700
	step [25/332], loss=5.7947
	step [26/332], loss=7.4130
	step [27/332], loss=5.9771
	step [28/332], loss=7.2113
	step [29/332], loss=9.1764
	step [30/332], loss=6.6773
	step [31/332], loss=5.5001
	step [32/332], loss=6.7807
	step [33/332], loss=7.0344
	step [34/332], loss=7.5747
	step [35/332], loss=8.3918
	step [36/332], loss=5.8629
	step [37/332], loss=5.8655
	step [38/332], loss=9.5400
	step [39/332], loss=7.4719
	step [40/332], loss=6.2106
	step [41/332], loss=8.2938
	step [42/332], loss=7.1546
	step [43/332], loss=6.1712
	step [44/332], loss=6.2361
	step [45/332], loss=7.1824
	step [46/332], loss=5.7419
	step [47/332], loss=5.5325
	step [48/332], loss=6.6683
	step [49/332], loss=8.9340
	step [50/332], loss=4.9003
	step [51/332], loss=9.3129
	step [52/332], loss=6.2552
	step [53/332], loss=6.8190
	step [54/332], loss=7.3440
	step [55/332], loss=7.0857
	step [56/332], loss=6.4527
	step [57/332], loss=6.6547
	step [58/332], loss=6.0562
	step [59/332], loss=5.0677
	step [60/332], loss=6.5689
	step [61/332], loss=7.0453
	step [62/332], loss=6.1634
	step [63/332], loss=5.0698
	step [64/332], loss=7.1251
	step [65/332], loss=7.3329
	step [66/332], loss=7.1713
	step [67/332], loss=6.9362
	step [68/332], loss=6.6714
	step [69/332], loss=6.4398
	step [70/332], loss=6.6186
	step [71/332], loss=7.6031
	step [72/332], loss=7.3476
	step [73/332], loss=6.3725
	step [74/332], loss=6.3200
	step [75/332], loss=7.7231
	step [76/332], loss=6.9409
	step [77/332], loss=7.1224
	step [78/332], loss=8.3750
	step [79/332], loss=9.0199
	step [80/332], loss=6.9256
	step [81/332], loss=8.2278
	step [82/332], loss=8.1547
	step [83/332], loss=7.5699
	step [84/332], loss=6.6601
	step [85/332], loss=5.9931
	step [86/332], loss=8.4431
	step [87/332], loss=7.2953
	step [88/332], loss=6.9160
	step [89/332], loss=5.5941
	step [90/332], loss=7.2748
	step [91/332], loss=8.0335
	step [92/332], loss=8.0509
	step [93/332], loss=5.3450
	step [94/332], loss=7.6132
	step [95/332], loss=6.8365
	step [96/332], loss=6.8922
	step [97/332], loss=6.2990
	step [98/332], loss=7.4682
	step [99/332], loss=8.2861
	step [100/332], loss=6.6062
	step [101/332], loss=8.4683
	step [102/332], loss=7.0113
	step [103/332], loss=7.2848
	step [104/332], loss=5.2915
	step [105/332], loss=8.4074
	step [106/332], loss=6.6293
	step [107/332], loss=7.0864
	step [108/332], loss=5.9045
	step [109/332], loss=9.0236
	step [110/332], loss=7.7750
	step [111/332], loss=7.2628
	step [112/332], loss=8.2407
	step [113/332], loss=5.7921
	step [114/332], loss=7.5951
	step [115/332], loss=7.2833
	step [116/332], loss=8.0922
	step [117/332], loss=7.1350
	step [118/332], loss=6.2341
	step [119/332], loss=6.7375
	step [120/332], loss=9.5018
	step [121/332], loss=6.6895
	step [122/332], loss=6.9157
	step [123/332], loss=8.0899
	step [124/332], loss=7.3085
	step [125/332], loss=6.6570
	step [126/332], loss=6.8480
	step [127/332], loss=6.0041
	step [128/332], loss=7.7761
	step [129/332], loss=7.0354
	step [130/332], loss=6.5922
	step [131/332], loss=6.6338
	step [132/332], loss=6.4761
	step [133/332], loss=7.9840
	step [134/332], loss=6.0023
	step [135/332], loss=7.7392
	step [136/332], loss=9.1276
	step [137/332], loss=6.2788
	step [138/332], loss=5.8258
	step [139/332], loss=6.2240
	step [140/332], loss=6.9453
	step [141/332], loss=5.6813
	step [142/332], loss=7.8576
	step [143/332], loss=7.2564
	step [144/332], loss=7.5696
	step [145/332], loss=8.2444
	step [146/332], loss=7.0888
	step [147/332], loss=6.8592
	step [148/332], loss=6.9242
	step [149/332], loss=6.4224
	step [150/332], loss=5.6127
	step [151/332], loss=9.8061
	step [152/332], loss=8.8727
	step [153/332], loss=9.6021
	step [154/332], loss=7.2834
	step [155/332], loss=6.4380
	step [156/332], loss=6.7141
	step [157/332], loss=6.7613
	step [158/332], loss=6.7293
	step [159/332], loss=9.1192
	step [160/332], loss=6.0462
	step [161/332], loss=6.5313
	step [162/332], loss=6.7052
	step [163/332], loss=7.0032
	step [164/332], loss=7.6056
	step [165/332], loss=8.6023
	step [166/332], loss=6.5738
	step [167/332], loss=5.5884
	step [168/332], loss=7.8007
	step [169/332], loss=6.8215
	step [170/332], loss=8.2967
	step [171/332], loss=8.3732
	step [172/332], loss=7.3052
	step [173/332], loss=6.3189
	step [174/332], loss=7.4594
	step [175/332], loss=6.4454
	step [176/332], loss=5.8363
	step [177/332], loss=6.1730
	step [178/332], loss=7.2566
	step [179/332], loss=6.5939
	step [180/332], loss=6.9308
	step [181/332], loss=8.5146
	step [182/332], loss=6.6801
	step [183/332], loss=5.7200
	step [184/332], loss=6.7552
	step [185/332], loss=8.2286
	step [186/332], loss=7.1667
	step [187/332], loss=6.9079
	step [188/332], loss=6.7372
	step [189/332], loss=5.3658
	step [190/332], loss=6.8386
	step [191/332], loss=6.2078
	step [192/332], loss=7.0211
	step [193/332], loss=5.4717
	step [194/332], loss=7.8549
	step [195/332], loss=5.9656
	step [196/332], loss=6.1701
	step [197/332], loss=8.1430
	step [198/332], loss=6.9624
	step [199/332], loss=7.0548
	step [200/332], loss=6.5505
	step [201/332], loss=7.3167
	step [202/332], loss=7.8711
	step [203/332], loss=5.7409
	step [204/332], loss=6.5824
	step [205/332], loss=8.3870
	step [206/332], loss=8.2416
	step [207/332], loss=7.4910
	step [208/332], loss=5.6558
	step [209/332], loss=7.0665
	step [210/332], loss=7.6845
	step [211/332], loss=6.9037
	step [212/332], loss=6.8245
	step [213/332], loss=6.7673
	step [214/332], loss=8.8649
	step [215/332], loss=6.5558
	step [216/332], loss=7.5740
	step [217/332], loss=5.5764
	step [218/332], loss=6.4103
	step [219/332], loss=7.8228
	step [220/332], loss=6.7670
	step [221/332], loss=6.3848
	step [222/332], loss=8.0415
	step [223/332], loss=6.4300
	step [224/332], loss=6.8515
	step [225/332], loss=6.2445
	step [226/332], loss=6.1090
	step [227/332], loss=6.6840
	step [228/332], loss=7.3851
	step [229/332], loss=6.2612
	step [230/332], loss=5.9688
	step [231/332], loss=7.4297
	step [232/332], loss=8.6559
	step [233/332], loss=7.8392
	step [234/332], loss=6.7687
	step [235/332], loss=5.8444
	step [236/332], loss=7.2798
	step [237/332], loss=6.3948
	step [238/332], loss=7.8286
	step [239/332], loss=8.1751
	step [240/332], loss=6.2946
	step [241/332], loss=6.4296
	step [242/332], loss=6.7184
	step [243/332], loss=6.9088
	step [244/332], loss=5.5504
	step [245/332], loss=8.4750
	step [246/332], loss=6.9087
	step [247/332], loss=6.3759
	step [248/332], loss=8.6182
	step [249/332], loss=6.7611
	step [250/332], loss=8.3428
	step [251/332], loss=6.8638
	step [252/332], loss=7.6513
	step [253/332], loss=6.8440
	step [254/332], loss=7.0581
	step [255/332], loss=6.4844
	step [256/332], loss=7.5790
	step [257/332], loss=5.8794
	step [258/332], loss=7.6297
	step [259/332], loss=8.9876
	step [260/332], loss=5.7615
	step [261/332], loss=6.7408
	step [262/332], loss=5.2408
	step [263/332], loss=8.2283
	step [264/332], loss=6.4837
	step [265/332], loss=6.3773
	step [266/332], loss=9.4414
	step [267/332], loss=6.8792
	step [268/332], loss=5.3644
	step [269/332], loss=7.3305
	step [270/332], loss=7.4825
	step [271/332], loss=7.2235
	step [272/332], loss=8.2706
	step [273/332], loss=6.3669
	step [274/332], loss=8.0425
	step [275/332], loss=5.9584
	step [276/332], loss=8.8271
	step [277/332], loss=6.2863
	step [278/332], loss=8.1290
	step [279/332], loss=9.8769
	step [280/332], loss=7.9501
	step [281/332], loss=8.7316
	step [282/332], loss=6.8533
	step [283/332], loss=7.5660
	step [284/332], loss=6.3185
	step [285/332], loss=8.2486
	step [286/332], loss=7.3196
	step [287/332], loss=8.8711
	step [288/332], loss=8.1558
	step [289/332], loss=7.7016
	step [290/332], loss=7.6622
	step [291/332], loss=6.8329
	step [292/332], loss=6.7299
	step [293/332], loss=5.4046
	step [294/332], loss=6.0309
	step [295/332], loss=7.9868
	step [296/332], loss=8.3426
	step [297/332], loss=6.1732
	step [298/332], loss=7.6240
	step [299/332], loss=6.9511
	step [300/332], loss=6.7486
	step [301/332], loss=6.0768
	step [302/332], loss=6.9596
	step [303/332], loss=8.2275
	step [304/332], loss=9.7370
	step [305/332], loss=7.5013
	step [306/332], loss=7.7796
	step [307/332], loss=8.3858
	step [308/332], loss=8.7077
	step [309/332], loss=5.9768
	step [310/332], loss=6.1158
	step [311/332], loss=6.9914
	step [312/332], loss=6.5794
	step [313/332], loss=5.8824
	step [314/332], loss=7.1038
	step [315/332], loss=8.5571
	step [316/332], loss=6.6329
	step [317/332], loss=8.1547
	step [318/332], loss=6.3725
	step [319/332], loss=7.5986
	step [320/332], loss=7.9355
	step [321/332], loss=8.8007
	step [322/332], loss=6.4845
	step [323/332], loss=6.8230
	step [324/332], loss=10.0675
	step [325/332], loss=8.5606
	step [326/332], loss=7.7186
	step [327/332], loss=7.2236
	step [328/332], loss=6.7218
	step [329/332], loss=7.7276
	step [330/332], loss=5.5264
	step [331/332], loss=8.2210
	step [332/332], loss=5.8064
	Evaluating
	loss=0.0225, precision=0.1468, recall=0.9933, f1=0.6300
Training epoch 18
	step [1/332], loss=6.2632
	step [2/332], loss=7.0893
	step [3/332], loss=6.4245
	step [4/332], loss=6.9531
	step [5/332], loss=7.9648
	step [6/332], loss=6.6601
	step [7/332], loss=7.2516
	step [8/332], loss=6.5955
	step [9/332], loss=5.0433
	step [10/332], loss=5.8944
	step [11/332], loss=6.5321
	step [12/332], loss=6.8582
	step [13/332], loss=5.9475
	step [14/332], loss=8.3980
	step [15/332], loss=7.5250
	step [16/332], loss=4.5804
	step [17/332], loss=8.9510
	step [18/332], loss=5.7620
	step [19/332], loss=7.9021
	step [20/332], loss=8.2334
	step [21/332], loss=6.3209
	step [22/332], loss=7.5737
	step [23/332], loss=7.2355
	step [24/332], loss=9.9687
	step [25/332], loss=7.3733
	step [26/332], loss=8.7492
	step [27/332], loss=7.6922
	step [28/332], loss=6.2260
	step [29/332], loss=5.9867
	step [30/332], loss=7.6695
	step [31/332], loss=6.8172
	step [32/332], loss=8.6229
	step [33/332], loss=6.8352
	step [34/332], loss=6.0069
	step [35/332], loss=8.2418
	step [36/332], loss=6.2298
	step [37/332], loss=5.3854
	step [38/332], loss=8.0142
	step [39/332], loss=7.5440
	step [40/332], loss=5.0485
	step [41/332], loss=7.0901
	step [42/332], loss=7.4887
	step [43/332], loss=7.3491
	step [44/332], loss=6.3345
	step [45/332], loss=7.1233
	step [46/332], loss=6.6532
	step [47/332], loss=7.3353
	step [48/332], loss=5.7001
	step [49/332], loss=6.5766
	step [50/332], loss=7.1792
	step [51/332], loss=8.2952
	step [52/332], loss=7.4088
	step [53/332], loss=6.6930
	step [54/332], loss=8.2530
	step [55/332], loss=6.7870
	step [56/332], loss=8.8540
	step [57/332], loss=10.3448
	step [58/332], loss=7.0169
	step [59/332], loss=6.9989
	step [60/332], loss=7.1841
	step [61/332], loss=6.9569
	step [62/332], loss=5.8813
	step [63/332], loss=6.1039
	step [64/332], loss=8.1498
	step [65/332], loss=7.2304
	step [66/332], loss=6.7031
	step [67/332], loss=5.6880
	step [68/332], loss=6.2110
	step [69/332], loss=7.2483
	step [70/332], loss=6.3964
	step [71/332], loss=5.0469
	step [72/332], loss=6.0065
	step [73/332], loss=7.5162
	step [74/332], loss=7.0035
	step [75/332], loss=6.8568
	step [76/332], loss=5.6021
	step [77/332], loss=5.7257
	step [78/332], loss=5.4882
	step [79/332], loss=6.7640
	step [80/332], loss=6.1076
	step [81/332], loss=6.4915
	step [82/332], loss=6.7993
	step [83/332], loss=6.7603
	step [84/332], loss=8.2499
	step [85/332], loss=6.3276
	step [86/332], loss=5.5967
	step [87/332], loss=6.5362
	step [88/332], loss=8.2404
	step [89/332], loss=7.3626
	step [90/332], loss=4.4984
	step [91/332], loss=7.8317
	step [92/332], loss=6.0029
	step [93/332], loss=6.0496
	step [94/332], loss=6.6422
	step [95/332], loss=6.6279
	step [96/332], loss=5.2736
	step [97/332], loss=7.2083
	step [98/332], loss=6.6829
	step [99/332], loss=6.1916
	step [100/332], loss=8.1750
	step [101/332], loss=8.2536
	step [102/332], loss=5.2048
	step [103/332], loss=6.9490
	step [104/332], loss=8.0008
	step [105/332], loss=9.3377
	step [106/332], loss=6.1211
	step [107/332], loss=8.4895
	step [108/332], loss=8.3230
	step [109/332], loss=7.2007
	step [110/332], loss=8.9196
	step [111/332], loss=6.9166
	step [112/332], loss=6.6340
	step [113/332], loss=6.3107
	step [114/332], loss=8.6374
	step [115/332], loss=6.9380
	step [116/332], loss=7.5638
	step [117/332], loss=5.9659
	step [118/332], loss=6.6830
	step [119/332], loss=6.6011
	step [120/332], loss=6.8679
	step [121/332], loss=6.8800
	step [122/332], loss=6.9236
	step [123/332], loss=7.8724
	step [124/332], loss=6.0494
	step [125/332], loss=7.5200
	step [126/332], loss=8.4482
	step [127/332], loss=6.4677
	step [128/332], loss=7.0879
	step [129/332], loss=6.4434
	step [130/332], loss=6.9908
	step [131/332], loss=7.5216
	step [132/332], loss=7.0408
	step [133/332], loss=6.0417
	step [134/332], loss=7.4537
	step [135/332], loss=6.5417
	step [136/332], loss=6.5499
	step [137/332], loss=7.2388
	step [138/332], loss=6.8458
	step [139/332], loss=8.5102
	step [140/332], loss=5.8094
	step [141/332], loss=5.0742
	step [142/332], loss=6.3760
	step [143/332], loss=7.2764
	step [144/332], loss=7.2783
	step [145/332], loss=6.4635
	step [146/332], loss=6.0678
	step [147/332], loss=6.0459
	step [148/332], loss=6.6768
	step [149/332], loss=6.0645
	step [150/332], loss=7.9018
	step [151/332], loss=5.8026
	step [152/332], loss=6.3497
	step [153/332], loss=6.5281
	step [154/332], loss=6.7970
	step [155/332], loss=7.0144
	step [156/332], loss=5.9997
	step [157/332], loss=6.1652
	step [158/332], loss=6.1726
	step [159/332], loss=4.7553
	step [160/332], loss=6.8369
	step [161/332], loss=6.3784
	step [162/332], loss=5.7137
	step [163/332], loss=6.4339
	step [164/332], loss=5.2847
	step [165/332], loss=8.2750
	step [166/332], loss=7.3052
	step [167/332], loss=8.5488
	step [168/332], loss=5.8924
	step [169/332], loss=6.5322
	step [170/332], loss=7.0180
	step [171/332], loss=6.5329
	step [172/332], loss=7.0325
	step [173/332], loss=5.4299
	step [174/332], loss=6.7632
	step [175/332], loss=6.8666
	step [176/332], loss=8.8962
	step [177/332], loss=8.3419
	step [178/332], loss=5.7740
	step [179/332], loss=7.0674
	step [180/332], loss=6.4872
	step [181/332], loss=5.5824
	step [182/332], loss=7.0030
	step [183/332], loss=8.0136
	step [184/332], loss=6.4934
	step [185/332], loss=5.5640
	step [186/332], loss=6.4452
	step [187/332], loss=6.2121
	step [188/332], loss=5.2187
	step [189/332], loss=8.5802
	step [190/332], loss=7.4640
	step [191/332], loss=7.2773
	step [192/332], loss=7.3476
	step [193/332], loss=5.4796
	step [194/332], loss=6.5872
	step [195/332], loss=6.7388
	step [196/332], loss=5.4186
	step [197/332], loss=6.8678
	step [198/332], loss=7.1933
	step [199/332], loss=9.7298
	step [200/332], loss=6.9096
	step [201/332], loss=6.2194
	step [202/332], loss=8.0125
	step [203/332], loss=6.5124
	step [204/332], loss=6.1127
	step [205/332], loss=6.8990
	step [206/332], loss=7.3480
	step [207/332], loss=5.7403
	step [208/332], loss=7.1109
	step [209/332], loss=7.0567
	step [210/332], loss=6.2309
	step [211/332], loss=7.7736
	step [212/332], loss=5.8422
	step [213/332], loss=6.0532
	step [214/332], loss=4.9661
	step [215/332], loss=5.8936
	step [216/332], loss=6.9761
	step [217/332], loss=6.4909
	step [218/332], loss=6.4115
	step [219/332], loss=5.6005
	step [220/332], loss=6.7244
	step [221/332], loss=6.5990
	step [222/332], loss=6.5648
	step [223/332], loss=7.6314
	step [224/332], loss=5.3639
	step [225/332], loss=8.5741
	step [226/332], loss=6.8433
	step [227/332], loss=7.3839
	step [228/332], loss=6.4050
	step [229/332], loss=6.0746
	step [230/332], loss=5.4710
	step [231/332], loss=8.7537
	step [232/332], loss=4.9685
	step [233/332], loss=6.7410
	step [234/332], loss=7.1324
	step [235/332], loss=5.4142
	step [236/332], loss=6.0455
	step [237/332], loss=5.8359
	step [238/332], loss=5.9380
	step [239/332], loss=6.7782
	step [240/332], loss=5.8194
	step [241/332], loss=6.0573
	step [242/332], loss=4.3373
	step [243/332], loss=5.9016
	step [244/332], loss=7.0815
	step [245/332], loss=6.0097
	step [246/332], loss=7.0953
	step [247/332], loss=7.0934
	step [248/332], loss=5.3880
	step [249/332], loss=8.6762
	step [250/332], loss=5.8508
	step [251/332], loss=6.4364
	step [252/332], loss=5.0880
	step [253/332], loss=7.7975
	step [254/332], loss=6.6225
	step [255/332], loss=7.0438
	step [256/332], loss=8.2408
	step [257/332], loss=5.7357
	step [258/332], loss=6.3774
	step [259/332], loss=6.4872
	step [260/332], loss=7.4921
	step [261/332], loss=5.9913
	step [262/332], loss=8.0111
	step [263/332], loss=6.4479
	step [264/332], loss=7.0223
	step [265/332], loss=6.1076
	step [266/332], loss=6.4008
	step [267/332], loss=8.7313
	step [268/332], loss=5.7683
	step [269/332], loss=7.4445
	step [270/332], loss=5.9705
	step [271/332], loss=6.2236
	step [272/332], loss=8.9516
	step [273/332], loss=5.7406
	step [274/332], loss=7.8140
	step [275/332], loss=6.2045
	step [276/332], loss=5.7685
	step [277/332], loss=5.5547
	step [278/332], loss=5.9885
	step [279/332], loss=5.7129
	step [280/332], loss=6.2698
	step [281/332], loss=5.3852
	step [282/332], loss=5.3554
	step [283/332], loss=6.3258
	step [284/332], loss=8.9625
	step [285/332], loss=8.1998
	step [286/332], loss=6.8747
	step [287/332], loss=7.2359
	step [288/332], loss=7.9763
	step [289/332], loss=8.6625
	step [290/332], loss=6.6846
	step [291/332], loss=7.6007
	step [292/332], loss=7.1423
	step [293/332], loss=6.6384
	step [294/332], loss=6.9194
	step [295/332], loss=6.6411
	step [296/332], loss=7.7799
	step [297/332], loss=5.7248
	step [298/332], loss=7.4075
	step [299/332], loss=6.3344
	step [300/332], loss=6.4989
	step [301/332], loss=6.8557
	step [302/332], loss=8.1590
	step [303/332], loss=7.8496
	step [304/332], loss=8.1187
	step [305/332], loss=5.7129
	step [306/332], loss=7.3710
	step [307/332], loss=7.6538
	step [308/332], loss=5.1907
	step [309/332], loss=8.5946
	step [310/332], loss=8.7542
	step [311/332], loss=8.0951
	step [312/332], loss=7.2767
	step [313/332], loss=6.5267
	step [314/332], loss=6.5664
	step [315/332], loss=9.8642
	step [316/332], loss=6.1741
	step [317/332], loss=7.6037
	step [318/332], loss=7.3404
	step [319/332], loss=5.2152
	step [320/332], loss=6.5543
	step [321/332], loss=6.4285
	step [322/332], loss=6.8771
	step [323/332], loss=6.9989
	step [324/332], loss=7.2450
	step [325/332], loss=7.2566
	step [326/332], loss=7.1707
	step [327/332], loss=7.4769
	step [328/332], loss=5.9848
	step [329/332], loss=6.2023
	step [330/332], loss=7.1262
	step [331/332], loss=9.2116
	step [332/332], loss=7.2780
	Evaluating
	loss=0.0252, precision=0.1315, recall=0.9934, f1=0.6001
Training epoch 19
	step [1/332], loss=5.3926
	step [2/332], loss=5.5417
	step [3/332], loss=6.1842
	step [4/332], loss=6.7427
	step [5/332], loss=6.4295
	step [6/332], loss=4.9055
	step [7/332], loss=6.1405
	step [8/332], loss=7.0159
	step [9/332], loss=6.2177
	step [10/332], loss=6.0514
	step [11/332], loss=5.5370
	step [12/332], loss=7.8174
	step [13/332], loss=5.6761
	step [14/332], loss=7.9111
	step [15/332], loss=7.0863
	step [16/332], loss=6.5888
	step [17/332], loss=7.3710
	step [18/332], loss=6.2881
	step [19/332], loss=6.4796
	step [20/332], loss=5.0086
	step [21/332], loss=6.6610
	step [22/332], loss=6.8646
	step [23/332], loss=7.5085
	step [24/332], loss=6.0279
	step [25/332], loss=7.2163
	step [26/332], loss=8.4120
	step [27/332], loss=6.4161
	step [28/332], loss=5.6423
	step [29/332], loss=6.0982
	step [30/332], loss=6.4594
	step [31/332], loss=8.0939
	step [32/332], loss=6.7656
	step [33/332], loss=7.7918
	step [34/332], loss=7.4890
	step [35/332], loss=6.5422
	step [36/332], loss=5.7488
	step [37/332], loss=6.2863
	step [38/332], loss=8.2963
	step [39/332], loss=6.1941
	step [40/332], loss=7.6265
	step [41/332], loss=6.6663
	step [42/332], loss=6.0321
	step [43/332], loss=6.6599
	step [44/332], loss=7.7100
	step [45/332], loss=5.6486
	step [46/332], loss=7.4728
	step [47/332], loss=5.2611
	step [48/332], loss=6.4974
	step [49/332], loss=5.9747
	step [50/332], loss=5.5603
	step [51/332], loss=5.9520
	step [52/332], loss=6.4935
	step [53/332], loss=5.7892
	step [54/332], loss=8.4533
	step [55/332], loss=6.9822
	step [56/332], loss=8.9651
	step [57/332], loss=7.4814
	step [58/332], loss=6.6668
	step [59/332], loss=5.9734
	step [60/332], loss=6.4053
	step [61/332], loss=7.3907
	step [62/332], loss=6.2450
	step [63/332], loss=6.9941
	step [64/332], loss=6.0063
	step [65/332], loss=5.1445
	step [66/332], loss=5.9733
	step [67/332], loss=5.0570
	step [68/332], loss=7.0989
	step [69/332], loss=8.1807
	step [70/332], loss=6.3449
	step [71/332], loss=7.6929
	step [72/332], loss=6.3806
	step [73/332], loss=6.9405
	step [74/332], loss=6.1600
	step [75/332], loss=7.3672
	step [76/332], loss=6.7283
	step [77/332], loss=5.8975
	step [78/332], loss=6.8993
	step [79/332], loss=6.0960
	step [80/332], loss=7.3189
	step [81/332], loss=8.0638
	step [82/332], loss=6.0604
	step [83/332], loss=6.5769
	step [84/332], loss=5.4244
	step [85/332], loss=6.0156
	step [86/332], loss=5.6696
	step [87/332], loss=6.1548
	step [88/332], loss=4.9343
	step [89/332], loss=7.3443
	step [90/332], loss=7.1462
	step [91/332], loss=5.9409
	step [92/332], loss=6.0868
	step [93/332], loss=5.8647
	step [94/332], loss=6.1465
	step [95/332], loss=6.0864
	step [96/332], loss=5.3327
	step [97/332], loss=5.9586
	step [98/332], loss=6.3612
	step [99/332], loss=6.4082
	step [100/332], loss=7.0511
	step [101/332], loss=7.2794
	step [102/332], loss=5.6895
	step [103/332], loss=5.2625
	step [104/332], loss=6.9316
	step [105/332], loss=7.5900
	step [106/332], loss=6.6808
	step [107/332], loss=6.0284
	step [108/332], loss=5.7928
	step [109/332], loss=7.7084
	step [110/332], loss=6.6104
	step [111/332], loss=6.9812
	step [112/332], loss=4.9834
	step [113/332], loss=6.7238
	step [114/332], loss=6.6544
	step [115/332], loss=7.0402
	step [116/332], loss=6.1314
	step [117/332], loss=6.5957
	step [118/332], loss=5.3986
	step [119/332], loss=6.9679
	step [120/332], loss=6.8179
	step [121/332], loss=9.3948
	step [122/332], loss=7.6326
	step [123/332], loss=7.3848
	step [124/332], loss=5.8377
	step [125/332], loss=8.1564
	step [126/332], loss=7.0877
	step [127/332], loss=8.8720
	step [128/332], loss=6.4006
	step [129/332], loss=6.5566
	step [130/332], loss=7.2853
	step [131/332], loss=6.2373
	step [132/332], loss=7.7645
	step [133/332], loss=7.5421
	step [134/332], loss=6.2652
	step [135/332], loss=5.8896
	step [136/332], loss=5.3005
	step [137/332], loss=8.3177
	step [138/332], loss=7.5128
	step [139/332], loss=6.5032
	step [140/332], loss=5.4897
	step [141/332], loss=8.2921
	step [142/332], loss=5.9511
	step [143/332], loss=7.4022
	step [144/332], loss=4.7310
	step [145/332], loss=6.3459
	step [146/332], loss=7.4821
	step [147/332], loss=7.0329
	step [148/332], loss=6.8061
	step [149/332], loss=7.4049
	step [150/332], loss=6.2648
	step [151/332], loss=6.1656
	step [152/332], loss=5.9769
	step [153/332], loss=6.6003
	step [154/332], loss=6.2665
	step [155/332], loss=5.2861
	step [156/332], loss=7.4618
	step [157/332], loss=6.7636
	step [158/332], loss=6.6841
	step [159/332], loss=5.7848
	step [160/332], loss=6.8114
	step [161/332], loss=5.6667
	step [162/332], loss=6.4982
	step [163/332], loss=7.2111
	step [164/332], loss=4.7802
	step [165/332], loss=6.2347
	step [166/332], loss=7.4417
	step [167/332], loss=5.3707
	step [168/332], loss=6.8456
	step [169/332], loss=6.3623
	step [170/332], loss=4.8775
	step [171/332], loss=5.8225
	step [172/332], loss=6.3228
	step [173/332], loss=6.0977
	step [174/332], loss=6.0455
	step [175/332], loss=6.6477
	step [176/332], loss=8.2391
	step [177/332], loss=7.0980
	step [178/332], loss=5.5661
	step [179/332], loss=5.9708
	step [180/332], loss=6.7601
	step [181/332], loss=7.2099
	step [182/332], loss=5.9161
	step [183/332], loss=7.7999
	step [184/332], loss=6.4916
	step [185/332], loss=7.4628
	step [186/332], loss=5.6167
	step [187/332], loss=5.4761
	step [188/332], loss=6.1134
	step [189/332], loss=8.6085
	step [190/332], loss=8.2909
	step [191/332], loss=6.7267
	step [192/332], loss=6.1670
	step [193/332], loss=8.1817
	step [194/332], loss=5.9823
	step [195/332], loss=6.6213
	step [196/332], loss=6.9693
	step [197/332], loss=6.4899
	step [198/332], loss=6.3016
	step [199/332], loss=6.7103
	step [200/332], loss=6.3661
	step [201/332], loss=6.3964
	step [202/332], loss=7.7991
	step [203/332], loss=8.2888
	step [204/332], loss=5.6508
	step [205/332], loss=6.5874
	step [206/332], loss=7.7546
	step [207/332], loss=7.8277
	step [208/332], loss=5.9349
	step [209/332], loss=7.2511
	step [210/332], loss=4.9945
	step [211/332], loss=5.6794
	step [212/332], loss=6.9801
	step [213/332], loss=6.0206
	step [214/332], loss=6.9450
	step [215/332], loss=6.6424
	step [216/332], loss=6.7762
	step [217/332], loss=7.3033
	step [218/332], loss=6.4580
	step [219/332], loss=5.4311
	step [220/332], loss=7.4560
	step [221/332], loss=7.9327
	step [222/332], loss=6.2709
	step [223/332], loss=5.6943
	step [224/332], loss=7.1316
	step [225/332], loss=5.7329
	step [226/332], loss=6.6349
	step [227/332], loss=9.2183
	step [228/332], loss=7.2465
	step [229/332], loss=6.3449
	step [230/332], loss=6.8858
	step [231/332], loss=5.4047
	step [232/332], loss=7.3341
	step [233/332], loss=6.6759
	step [234/332], loss=6.2318
	step [235/332], loss=6.9489
	step [236/332], loss=6.2588
	step [237/332], loss=7.4319
	step [238/332], loss=6.2451
	step [239/332], loss=6.2679
	step [240/332], loss=7.8702
	step [241/332], loss=6.5871
	step [242/332], loss=5.8375
	step [243/332], loss=6.4127
	step [244/332], loss=7.2473
	step [245/332], loss=6.9465
	step [246/332], loss=4.8646
	step [247/332], loss=5.5191
	step [248/332], loss=8.7311
	step [249/332], loss=4.3451
	step [250/332], loss=5.1581
	step [251/332], loss=7.3260
	step [252/332], loss=8.0262
	step [253/332], loss=6.9826
	step [254/332], loss=6.6221
	step [255/332], loss=6.7402
	step [256/332], loss=8.2295
	step [257/332], loss=5.1872
	step [258/332], loss=5.4691
	step [259/332], loss=6.8883
	step [260/332], loss=7.3051
	step [261/332], loss=5.3559
	step [262/332], loss=5.7757
	step [263/332], loss=6.6926
	step [264/332], loss=5.1250
	step [265/332], loss=9.0592
	step [266/332], loss=6.8330
	step [267/332], loss=5.2002
	step [268/332], loss=6.1632
	step [269/332], loss=6.5971
	step [270/332], loss=6.3699
	step [271/332], loss=6.3140
	step [272/332], loss=6.7670
	step [273/332], loss=7.1288
	step [274/332], loss=6.1305
	step [275/332], loss=5.8567
	step [276/332], loss=4.8881
	step [277/332], loss=6.5904
	step [278/332], loss=6.6846
	step [279/332], loss=7.9361
	step [280/332], loss=6.1148
	step [281/332], loss=6.3318
	step [282/332], loss=7.4519
	step [283/332], loss=6.7001
	step [284/332], loss=5.6260
	step [285/332], loss=6.8713
	step [286/332], loss=5.6587
	step [287/332], loss=4.3284
	step [288/332], loss=6.4184
	step [289/332], loss=6.8614
	step [290/332], loss=6.4750
	step [291/332], loss=4.6824
	step [292/332], loss=7.3582
	step [293/332], loss=5.7859
	step [294/332], loss=7.0052
	step [295/332], loss=8.5860
	step [296/332], loss=5.9562
	step [297/332], loss=6.7506
	step [298/332], loss=10.1138
	step [299/332], loss=5.1780
	step [300/332], loss=5.9217
	step [301/332], loss=6.1324
	step [302/332], loss=7.9060
	step [303/332], loss=5.8606
	step [304/332], loss=6.7630
	step [305/332], loss=6.2555
	step [306/332], loss=6.9500
	step [307/332], loss=5.3168
	step [308/332], loss=6.1071
	step [309/332], loss=4.9735
	step [310/332], loss=6.5107
	step [311/332], loss=5.5784
	step [312/332], loss=6.5503
	step [313/332], loss=7.9968
	step [314/332], loss=5.9287
	step [315/332], loss=5.5778
	step [316/332], loss=7.4779
	step [317/332], loss=6.8597
	step [318/332], loss=5.8289
	step [319/332], loss=8.1267
	step [320/332], loss=6.7769
	step [321/332], loss=8.5960
	step [322/332], loss=6.8207
	step [323/332], loss=8.3325
	step [324/332], loss=6.1644
	step [325/332], loss=6.8789
	step [326/332], loss=6.2725
	step [327/332], loss=7.1040
	step [328/332], loss=6.1026
	step [329/332], loss=5.5553
	step [330/332], loss=5.0076
	step [331/332], loss=5.9393
	step [332/332], loss=4.7755
	Evaluating
	loss=0.0221, precision=0.1477, recall=0.9930, f1=0.6316
Training epoch 20
	step [1/332], loss=6.6487
	step [2/332], loss=7.8048
	step [3/332], loss=7.0316
	step [4/332], loss=7.5687
	step [5/332], loss=6.0879
	step [6/332], loss=8.8940
	step [7/332], loss=6.1936
	step [8/332], loss=6.2917
	step [9/332], loss=7.2139
	step [10/332], loss=7.1897
	step [11/332], loss=6.3234
	step [12/332], loss=5.3468
	step [13/332], loss=6.1411
	step [14/332], loss=6.7358
	step [15/332], loss=7.0465
	step [16/332], loss=8.8649
	step [17/332], loss=6.5568
	step [18/332], loss=5.4235
	step [19/332], loss=7.5140
	step [20/332], loss=8.0682
	step [21/332], loss=7.2204
	step [22/332], loss=7.0762
	step [23/332], loss=4.8978
	step [24/332], loss=6.9025
	step [25/332], loss=7.3305
	step [26/332], loss=5.4650
	step [27/332], loss=8.2096
	step [28/332], loss=5.2126
	step [29/332], loss=7.6892
	step [30/332], loss=5.6424
	step [31/332], loss=5.6222
	step [32/332], loss=6.9040
	step [33/332], loss=5.3368
	step [34/332], loss=8.2099
	step [35/332], loss=6.3212
	step [36/332], loss=5.9802
	step [37/332], loss=5.6057
	step [38/332], loss=6.6694
	step [39/332], loss=6.1478
	step [40/332], loss=5.7441
	step [41/332], loss=5.5624
	step [42/332], loss=6.5967
	step [43/332], loss=6.0484
	step [44/332], loss=5.4446
	step [45/332], loss=7.5871
	step [46/332], loss=6.8773
	step [47/332], loss=5.8267
	step [48/332], loss=7.4084
	step [49/332], loss=8.9579
	step [50/332], loss=6.0817
	step [51/332], loss=6.6123
	step [52/332], loss=5.6936
	step [53/332], loss=5.2289
	step [54/332], loss=6.1525
	step [55/332], loss=5.2058
	step [56/332], loss=8.2797
	step [57/332], loss=5.4146
	step [58/332], loss=4.8402
	step [59/332], loss=6.8393
	step [60/332], loss=6.5828
	step [61/332], loss=6.8400
	step [62/332], loss=6.5912
	step [63/332], loss=6.9056
	step [64/332], loss=7.4873
	step [65/332], loss=5.2616
	step [66/332], loss=6.0861
	step [67/332], loss=7.6029
	step [68/332], loss=5.3944
	step [69/332], loss=6.3674
	step [70/332], loss=5.9187
	step [71/332], loss=4.9535
	step [72/332], loss=5.5943
	step [73/332], loss=6.5430
	step [74/332], loss=5.3312
	step [75/332], loss=5.0584
	step [76/332], loss=6.9563
	step [77/332], loss=6.2237
	step [78/332], loss=5.8155
	step [79/332], loss=7.3049
	step [80/332], loss=7.5463
	step [81/332], loss=5.6758
	step [82/332], loss=6.4386
	step [83/332], loss=7.3463
	step [84/332], loss=4.8144
	step [85/332], loss=5.8998
	step [86/332], loss=7.3247
	step [87/332], loss=6.3069
	step [88/332], loss=5.8208
	step [89/332], loss=8.4348
	step [90/332], loss=4.9133
	step [91/332], loss=8.4238
	step [92/332], loss=6.5220
	step [93/332], loss=6.6776
	step [94/332], loss=7.0435
	step [95/332], loss=6.4781
	step [96/332], loss=5.4216
	step [97/332], loss=6.1626
	step [98/332], loss=5.2568
	step [99/332], loss=6.6535
	step [100/332], loss=6.2077
	step [101/332], loss=6.3897
	step [102/332], loss=5.9206
	step [103/332], loss=7.5582
	step [104/332], loss=4.6002
	step [105/332], loss=4.9412
	step [106/332], loss=6.3537
	step [107/332], loss=6.1563
	step [108/332], loss=6.1176
	step [109/332], loss=5.9690
	step [110/332], loss=8.5695
	step [111/332], loss=6.6363
	step [112/332], loss=6.0427
	step [113/332], loss=5.6664
	step [114/332], loss=5.7054
	step [115/332], loss=8.4969
	step [116/332], loss=6.9601
	step [117/332], loss=5.3683
	step [118/332], loss=7.0382
	step [119/332], loss=6.7513
	step [120/332], loss=6.0179
	step [121/332], loss=6.8628
	step [122/332], loss=5.4593
	step [123/332], loss=7.1042
	step [124/332], loss=6.3238
	step [125/332], loss=5.8608
	step [126/332], loss=5.2950
	step [127/332], loss=5.5697
	step [128/332], loss=6.0530
	step [129/332], loss=6.1565
	step [130/332], loss=4.4338
	step [131/332], loss=6.2382
	step [132/332], loss=5.4991
	step [133/332], loss=6.7935
	step [134/332], loss=8.9841
	step [135/332], loss=7.2715
	step [136/332], loss=6.4059
	step [137/332], loss=5.2960
	step [138/332], loss=6.5373
	step [139/332], loss=6.5845
	step [140/332], loss=6.1140
	step [141/332], loss=6.8582
	step [142/332], loss=5.3520
	step [143/332], loss=7.0204
	step [144/332], loss=5.7020
	step [145/332], loss=6.8346
	step [146/332], loss=6.4139
	step [147/332], loss=7.6905
	step [148/332], loss=6.4500
	step [149/332], loss=6.6005
	step [150/332], loss=6.3458
	step [151/332], loss=5.7179
	step [152/332], loss=5.6508
	step [153/332], loss=6.4391
	step [154/332], loss=6.8145
	step [155/332], loss=5.4411
	step [156/332], loss=8.8395
	step [157/332], loss=5.2429
	step [158/332], loss=6.4639
	step [159/332], loss=6.2919
	step [160/332], loss=5.9763
	step [161/332], loss=6.6035
	step [162/332], loss=7.8437
	step [163/332], loss=4.9063
	step [164/332], loss=5.9733
	step [165/332], loss=7.4377
	step [166/332], loss=5.1740
	step [167/332], loss=7.4236
	step [168/332], loss=5.0188
	step [169/332], loss=6.8253
	step [170/332], loss=5.1221
	step [171/332], loss=6.0392
	step [172/332], loss=7.3784
	step [173/332], loss=4.8170
	step [174/332], loss=7.8105
	step [175/332], loss=6.3873
	step [176/332], loss=6.5891
	step [177/332], loss=4.6436
	step [178/332], loss=5.4127
	step [179/332], loss=6.6304
	step [180/332], loss=7.4164
	step [181/332], loss=7.1395
	step [182/332], loss=4.9587
	step [183/332], loss=6.1129
	step [184/332], loss=6.5253
	step [185/332], loss=6.2079
	step [186/332], loss=6.9552
	step [187/332], loss=6.0632
	step [188/332], loss=6.2146
	step [189/332], loss=7.8765
	step [190/332], loss=6.7518
	step [191/332], loss=9.2375
	step [192/332], loss=7.4392
	step [193/332], loss=6.6550
	step [194/332], loss=5.1533
	step [195/332], loss=7.5085
	step [196/332], loss=6.7672
	step [197/332], loss=6.7734
	step [198/332], loss=7.3786
	step [199/332], loss=6.0892
	step [200/332], loss=5.8130
	step [201/332], loss=5.1460
	step [202/332], loss=6.9225
	step [203/332], loss=6.3806
	step [204/332], loss=6.6905
	step [205/332], loss=5.7820
	step [206/332], loss=6.6900
	step [207/332], loss=6.1924
	step [208/332], loss=7.9859
	step [209/332], loss=6.1374
	step [210/332], loss=5.9991
	step [211/332], loss=5.9118
	step [212/332], loss=6.1512
	step [213/332], loss=6.7020
	step [214/332], loss=5.1123
	step [215/332], loss=7.1861
	step [216/332], loss=6.7496
	step [217/332], loss=5.9889
	step [218/332], loss=6.3439
	step [219/332], loss=6.5931
	step [220/332], loss=4.7467
	step [221/332], loss=6.6918
	step [222/332], loss=6.5259
	step [223/332], loss=6.0345
	step [224/332], loss=6.4195
	step [225/332], loss=5.2395
	step [226/332], loss=7.6418
	step [227/332], loss=6.1904
	step [228/332], loss=7.4865
	step [229/332], loss=7.3343
	step [230/332], loss=6.2456
	step [231/332], loss=6.3340
	step [232/332], loss=5.2269
	step [233/332], loss=6.3794
	step [234/332], loss=7.0613
	step [235/332], loss=7.8263
	step [236/332], loss=6.0673
	step [237/332], loss=6.4374
	step [238/332], loss=6.9467
	step [239/332], loss=6.3777
	step [240/332], loss=7.0894
	step [241/332], loss=5.7224
	step [242/332], loss=5.2659
	step [243/332], loss=7.4620
	step [244/332], loss=8.2882
	step [245/332], loss=6.9402
	step [246/332], loss=4.4044
	step [247/332], loss=5.4482
	step [248/332], loss=5.3452
	step [249/332], loss=5.1561
	step [250/332], loss=6.3291
	step [251/332], loss=6.0253
	step [252/332], loss=7.0868
	step [253/332], loss=6.9869
	step [254/332], loss=6.3783
	step [255/332], loss=5.6511
	step [256/332], loss=6.8915
	step [257/332], loss=7.9779
	step [258/332], loss=7.8531
	step [259/332], loss=5.6792
	step [260/332], loss=6.3869
	step [261/332], loss=5.7992
	step [262/332], loss=7.2608
	step [263/332], loss=5.5344
	step [264/332], loss=6.1941
	step [265/332], loss=6.7338
	step [266/332], loss=7.7889
	step [267/332], loss=6.6986
	step [268/332], loss=7.2085
	step [269/332], loss=5.6991
	step [270/332], loss=6.4212
	step [271/332], loss=7.5593
	step [272/332], loss=7.5759
	step [273/332], loss=7.1111
	step [274/332], loss=5.4507
	step [275/332], loss=6.1171
	step [276/332], loss=4.4571
	step [277/332], loss=7.5000
	step [278/332], loss=5.9022
	step [279/332], loss=5.1484
	step [280/332], loss=5.3813
	step [281/332], loss=5.8977
	step [282/332], loss=6.2077
	step [283/332], loss=6.7718
	step [284/332], loss=5.5285
	step [285/332], loss=6.6534
	step [286/332], loss=5.7378
	step [287/332], loss=6.7289
	step [288/332], loss=5.7982
	step [289/332], loss=6.9425
	step [290/332], loss=6.8369
	step [291/332], loss=5.3179
	step [292/332], loss=5.8334
	step [293/332], loss=6.4803
	step [294/332], loss=7.3038
	step [295/332], loss=6.6031
	step [296/332], loss=7.7825
	step [297/332], loss=6.8035
	step [298/332], loss=7.4250
	step [299/332], loss=6.6732
	step [300/332], loss=5.8807
	step [301/332], loss=6.7031
	step [302/332], loss=6.3541
	step [303/332], loss=5.7312
	step [304/332], loss=6.0408
	step [305/332], loss=5.3587
	step [306/332], loss=7.0420
	step [307/332], loss=6.3705
	step [308/332], loss=5.7917
	step [309/332], loss=6.8176
	step [310/332], loss=6.6323
	step [311/332], loss=5.8253
	step [312/332], loss=6.5253
	step [313/332], loss=6.0250
	step [314/332], loss=6.0789
	step [315/332], loss=5.8596
	step [316/332], loss=5.3528
	step [317/332], loss=7.0752
	step [318/332], loss=6.1525
	step [319/332], loss=6.9736
	step [320/332], loss=6.0857
	step [321/332], loss=7.2385
	step [322/332], loss=5.1853
	step [323/332], loss=8.1791
	step [324/332], loss=6.1002
	step [325/332], loss=6.9653
	step [326/332], loss=5.6016
	step [327/332], loss=7.0914
	step [328/332], loss=5.9563
	step [329/332], loss=5.2994
	step [330/332], loss=5.2409
	step [331/332], loss=6.6501
	step [332/332], loss=3.7922
	Evaluating
	loss=0.0236, precision=0.1399, recall=0.9936, f1=0.6170
Training epoch 21
	step [1/332], loss=5.2965
	step [2/332], loss=5.4343
	step [3/332], loss=5.6014
	step [4/332], loss=6.3470
	step [5/332], loss=4.0655
	step [6/332], loss=6.4781
	step [7/332], loss=7.5177
	step [8/332], loss=6.2544
	step [9/332], loss=4.7475
	step [10/332], loss=7.6403
	step [11/332], loss=7.0533
	step [12/332], loss=6.5931
	step [13/332], loss=5.1936
	step [14/332], loss=8.3842
	step [15/332], loss=4.4800
	step [16/332], loss=5.3502
	step [17/332], loss=5.2720
	step [18/332], loss=7.3503
	step [19/332], loss=5.7678
	step [20/332], loss=6.4030
	step [21/332], loss=6.3212
	step [22/332], loss=5.9547
	step [23/332], loss=5.6396
	step [24/332], loss=5.7241
	step [25/332], loss=5.6611
	step [26/332], loss=5.7370
	step [27/332], loss=5.1745
	step [28/332], loss=6.7232
	step [29/332], loss=8.8199
	step [30/332], loss=6.7698
	step [31/332], loss=6.2086
	step [32/332], loss=7.1105
	step [33/332], loss=5.9931
	step [34/332], loss=4.8802
	step [35/332], loss=5.5296
	step [36/332], loss=7.4124
	step [37/332], loss=7.3939
	step [38/332], loss=6.3102
	step [39/332], loss=7.0670
	step [40/332], loss=6.4092
	step [41/332], loss=5.7052
	step [42/332], loss=5.9996
	step [43/332], loss=5.8705
	step [44/332], loss=6.3569
	step [45/332], loss=6.9287
	step [46/332], loss=5.9361
	step [47/332], loss=7.4737
	step [48/332], loss=5.8961
	step [49/332], loss=5.9698
	step [50/332], loss=6.8381
	step [51/332], loss=7.7553
	step [52/332], loss=7.1773
	step [53/332], loss=7.4778
	step [54/332], loss=4.5384
	step [55/332], loss=5.9735
	step [56/332], loss=4.3825
	step [57/332], loss=6.9100
	step [58/332], loss=6.5594
	step [59/332], loss=7.0705
	step [60/332], loss=5.6645
	step [61/332], loss=5.3899
	step [62/332], loss=6.2458
	step [63/332], loss=4.9927
	step [64/332], loss=6.7431
	step [65/332], loss=4.8492
	step [66/332], loss=6.4349
	step [67/332], loss=5.6458
	step [68/332], loss=6.1818
	step [69/332], loss=6.1799
	step [70/332], loss=5.8200
	step [71/332], loss=6.2449
	step [72/332], loss=5.7700
	step [73/332], loss=6.7420
	step [74/332], loss=5.4393
	step [75/332], loss=6.6811
	step [76/332], loss=4.7311
	step [77/332], loss=5.2088
	step [78/332], loss=6.5607
	step [79/332], loss=6.8361
	step [80/332], loss=5.3323
	step [81/332], loss=6.4600
	step [82/332], loss=6.5048
	step [83/332], loss=6.0292
	step [84/332], loss=5.2015
	step [85/332], loss=6.2794
	step [86/332], loss=6.4090
	step [87/332], loss=6.4567
	step [88/332], loss=6.7471
	step [89/332], loss=5.0592
	step [90/332], loss=5.9788
	step [91/332], loss=5.4563
	step [92/332], loss=7.9122
	step [93/332], loss=7.6834
	step [94/332], loss=6.2248
	step [95/332], loss=6.5931
	step [96/332], loss=6.2311
	step [97/332], loss=6.0432
	step [98/332], loss=7.1762
	step [99/332], loss=8.9448
	step [100/332], loss=6.4196
	step [101/332], loss=4.4376
	step [102/332], loss=5.7656
	step [103/332], loss=7.0809
	step [104/332], loss=7.0432
	step [105/332], loss=7.0637
	step [106/332], loss=5.6108
	step [107/332], loss=7.1906
	step [108/332], loss=5.9008
	step [109/332], loss=5.8402
	step [110/332], loss=5.1323
	step [111/332], loss=5.5561
	step [112/332], loss=7.1281
	step [113/332], loss=6.3823
	step [114/332], loss=6.0882
	step [115/332], loss=7.7833
	step [116/332], loss=6.3332
	step [117/332], loss=6.4184
	step [118/332], loss=5.1860
	step [119/332], loss=5.1964
	step [120/332], loss=6.1639
	step [121/332], loss=5.1970
	step [122/332], loss=6.8188
	step [123/332], loss=6.4462
	step [124/332], loss=7.9132
	step [125/332], loss=5.9660
	step [126/332], loss=5.5816
	step [127/332], loss=5.9272
	step [128/332], loss=7.2088
	step [129/332], loss=5.9500
	step [130/332], loss=6.5294
	step [131/332], loss=5.1878
	step [132/332], loss=5.4972
	step [133/332], loss=7.2727
	step [134/332], loss=6.5506
	step [135/332], loss=7.2018
	step [136/332], loss=6.0244
	step [137/332], loss=5.1687
	step [138/332], loss=4.9504
	step [139/332], loss=5.9829
	step [140/332], loss=5.4248
	step [141/332], loss=7.5099
	step [142/332], loss=5.3488
	step [143/332], loss=6.3794
	step [144/332], loss=5.0729
	step [145/332], loss=5.3009
	step [146/332], loss=6.4461
	step [147/332], loss=5.7816
	step [148/332], loss=7.0509
	step [149/332], loss=5.7892
	step [150/332], loss=6.0585
	step [151/332], loss=6.0849
	step [152/332], loss=5.4070
	step [153/332], loss=4.8741
	step [154/332], loss=6.8481
	step [155/332], loss=7.7233
	step [156/332], loss=4.2686
	step [157/332], loss=7.4070
	step [158/332], loss=5.6121
	step [159/332], loss=5.8170
	step [160/332], loss=5.5910
	step [161/332], loss=5.2500
	step [162/332], loss=6.1894
	step [163/332], loss=5.5257
	step [164/332], loss=5.6549
	step [165/332], loss=5.5883
	step [166/332], loss=7.1844
	step [167/332], loss=6.5123
	step [168/332], loss=6.3566
	step [169/332], loss=5.8415
	step [170/332], loss=7.4280
	step [171/332], loss=6.2745
	step [172/332], loss=6.0468
	step [173/332], loss=7.2120
	step [174/332], loss=5.4886
	step [175/332], loss=8.8730
	step [176/332], loss=4.8561
	step [177/332], loss=6.1048
	step [178/332], loss=5.2711
	step [179/332], loss=6.4417
	step [180/332], loss=5.4408
	step [181/332], loss=7.8052
	step [182/332], loss=7.9382
	step [183/332], loss=5.0482
	step [184/332], loss=6.7566
	step [185/332], loss=6.4841
	step [186/332], loss=6.1940
	step [187/332], loss=6.3559
	step [188/332], loss=5.6031
	step [189/332], loss=5.3894
	step [190/332], loss=6.8476
	step [191/332], loss=7.2954
	step [192/332], loss=5.6422
	step [193/332], loss=7.2556
	step [194/332], loss=5.5487
	step [195/332], loss=4.9417
	step [196/332], loss=5.2447
	step [197/332], loss=6.2745
	step [198/332], loss=7.6031
	step [199/332], loss=5.4749
	step [200/332], loss=6.7041
	step [201/332], loss=6.5726
	step [202/332], loss=6.6500
	step [203/332], loss=5.7514
	step [204/332], loss=5.7471
	step [205/332], loss=5.6192
	step [206/332], loss=7.2377
	step [207/332], loss=6.3693
	step [208/332], loss=5.9651
	step [209/332], loss=4.9981
	step [210/332], loss=5.7897
	step [211/332], loss=5.4924
	step [212/332], loss=5.8808
	step [213/332], loss=5.3311
	step [214/332], loss=6.5271
	step [215/332], loss=5.4997
	step [216/332], loss=4.7451
	step [217/332], loss=5.7831
	step [218/332], loss=6.5952
	step [219/332], loss=5.8670
	step [220/332], loss=5.5419
	step [221/332], loss=6.3749
	step [222/332], loss=8.3279
	step [223/332], loss=5.8977
	step [224/332], loss=5.3476
	step [225/332], loss=5.6015
	step [226/332], loss=6.4648
	step [227/332], loss=4.8481
	step [228/332], loss=5.1041
	step [229/332], loss=7.3204
	step [230/332], loss=6.4368
	step [231/332], loss=5.0835
	step [232/332], loss=5.9659
	step [233/332], loss=8.3006
	step [234/332], loss=7.3091
	step [235/332], loss=7.6275
	step [236/332], loss=6.4538
	step [237/332], loss=7.5355
	step [238/332], loss=6.0504
	step [239/332], loss=6.5225
	step [240/332], loss=6.5138
	step [241/332], loss=6.1721
	step [242/332], loss=5.7794
	step [243/332], loss=7.2033
	step [244/332], loss=6.1897
	step [245/332], loss=5.0576
	step [246/332], loss=6.3916
	step [247/332], loss=4.4924
	step [248/332], loss=5.3623
	step [249/332], loss=5.7736
	step [250/332], loss=7.2318
	step [251/332], loss=7.1949
	step [252/332], loss=5.1025
	step [253/332], loss=4.9503
	step [254/332], loss=6.9089
	step [255/332], loss=5.5333
	step [256/332], loss=8.1600
	step [257/332], loss=6.5404
	step [258/332], loss=6.4357
	step [259/332], loss=6.1164
	step [260/332], loss=6.3117
	step [261/332], loss=7.9648
	step [262/332], loss=5.1764
	step [263/332], loss=5.6688
	step [264/332], loss=6.5142
	step [265/332], loss=5.6112
	step [266/332], loss=7.6150
	step [267/332], loss=6.4180
	step [268/332], loss=6.1981
	step [269/332], loss=5.8816
	step [270/332], loss=5.6173
	step [271/332], loss=5.0790
	step [272/332], loss=6.1025
	step [273/332], loss=5.3984
	step [274/332], loss=4.3497
	step [275/332], loss=6.6202
	step [276/332], loss=6.8952
	step [277/332], loss=6.7605
	step [278/332], loss=6.4962
	step [279/332], loss=4.7311
	step [280/332], loss=8.5062
	step [281/332], loss=5.8901
	step [282/332], loss=5.9755
	step [283/332], loss=4.4400
	step [284/332], loss=6.4950
	step [285/332], loss=6.0234
	step [286/332], loss=6.3840
	step [287/332], loss=6.7216
	step [288/332], loss=6.7928
	step [289/332], loss=5.9152
	step [290/332], loss=5.7839
	step [291/332], loss=6.2354
	step [292/332], loss=6.6769
	step [293/332], loss=5.8199
	step [294/332], loss=6.4533
	step [295/332], loss=4.6128
	step [296/332], loss=6.3320
	step [297/332], loss=6.2865
	step [298/332], loss=4.5391
	step [299/332], loss=4.4543
	step [300/332], loss=6.1273
	step [301/332], loss=6.2374
	step [302/332], loss=5.9568
	step [303/332], loss=6.8458
	step [304/332], loss=5.5078
	step [305/332], loss=5.1096
	step [306/332], loss=5.8135
	step [307/332], loss=5.2220
	step [308/332], loss=7.2484
	step [309/332], loss=7.3438
	step [310/332], loss=6.3289
	step [311/332], loss=6.0175
	step [312/332], loss=5.6112
	step [313/332], loss=7.1073
	step [314/332], loss=5.1907
	step [315/332], loss=5.6515
	step [316/332], loss=5.4662
	step [317/332], loss=5.8519
	step [318/332], loss=5.5752
	step [319/332], loss=5.2863
	step [320/332], loss=8.0721
	step [321/332], loss=6.2579
	step [322/332], loss=6.0010
	step [323/332], loss=5.8693
	step [324/332], loss=6.6093
	step [325/332], loss=5.9113
	step [326/332], loss=5.7927
	step [327/332], loss=4.9587
	step [328/332], loss=5.6736
	step [329/332], loss=6.1979
	step [330/332], loss=5.3505
	step [331/332], loss=5.2689
	step [332/332], loss=4.6906
	Evaluating
	loss=0.0210, precision=0.1538, recall=0.9930, f1=0.6425
saving model as: 0_saved_model.pth
Training epoch 22
	step [1/332], loss=4.4851
	step [2/332], loss=5.0151
	step [3/332], loss=4.7293
	step [4/332], loss=6.0276
	step [5/332], loss=4.6854
	step [6/332], loss=5.2742
	step [7/332], loss=6.5094
	step [8/332], loss=4.2037
	step [9/332], loss=5.4837
	step [10/332], loss=4.3405
	step [11/332], loss=4.7272
	step [12/332], loss=6.5750
	step [13/332], loss=5.1192
	step [14/332], loss=7.0487
	step [15/332], loss=5.8736
	step [16/332], loss=4.9904
	step [17/332], loss=5.9809
	step [18/332], loss=5.2511
	step [19/332], loss=4.8277
	step [20/332], loss=4.3028
	step [21/332], loss=6.0274
	step [22/332], loss=7.2197
	step [23/332], loss=5.9762
	step [24/332], loss=5.3778
	step [25/332], loss=5.5705
	step [26/332], loss=5.9421
	step [27/332], loss=5.7344
	step [28/332], loss=6.6483
	step [29/332], loss=4.2779
	step [30/332], loss=5.3850
	step [31/332], loss=6.3089
	step [32/332], loss=5.5799
	step [33/332], loss=5.1669
	step [34/332], loss=6.4581
	step [35/332], loss=7.4401
	step [36/332], loss=5.2105
	step [37/332], loss=6.6573
	step [38/332], loss=6.9479
	step [39/332], loss=5.8067
	step [40/332], loss=6.7192
	step [41/332], loss=8.2912
	step [42/332], loss=5.9007
	step [43/332], loss=5.8311
	step [44/332], loss=5.5561
	step [45/332], loss=7.6099
	step [46/332], loss=5.5355
	step [47/332], loss=6.4846
	step [48/332], loss=5.1049
	step [49/332], loss=6.7581
	step [50/332], loss=5.3116
	step [51/332], loss=4.2306
	step [52/332], loss=6.9509
	step [53/332], loss=5.9262
	step [54/332], loss=6.7979
	step [55/332], loss=7.1471
	step [56/332], loss=6.4106
	step [57/332], loss=6.6696
	step [58/332], loss=6.2905
	step [59/332], loss=6.4219
	step [60/332], loss=6.2782
	step [61/332], loss=6.4631
	step [62/332], loss=5.6688
	step [63/332], loss=6.7032
	step [64/332], loss=5.1588
	step [65/332], loss=6.5677
	step [66/332], loss=8.2291
	step [67/332], loss=5.1070
	step [68/332], loss=5.5228
	step [69/332], loss=5.5930
	step [70/332], loss=6.9815
	step [71/332], loss=5.3702
	step [72/332], loss=6.9410
	step [73/332], loss=5.6598
	step [74/332], loss=5.4026
	step [75/332], loss=4.8351
	step [76/332], loss=6.1284
	step [77/332], loss=5.6296
	step [78/332], loss=6.6565
	step [79/332], loss=6.0945
	step [80/332], loss=6.8795
	step [81/332], loss=6.6358
	step [82/332], loss=5.2735
	step [83/332], loss=6.2622
	step [84/332], loss=4.7954
	step [85/332], loss=5.3507
	step [86/332], loss=5.0359
	step [87/332], loss=6.0494
	step [88/332], loss=5.1383
	step [89/332], loss=6.6255
	step [90/332], loss=6.2433
	step [91/332], loss=5.5504
	step [92/332], loss=7.3640
	step [93/332], loss=7.6418
	step [94/332], loss=6.5898
	step [95/332], loss=7.0413
	step [96/332], loss=7.1953
	step [97/332], loss=6.0715
	step [98/332], loss=5.6810
	step [99/332], loss=4.8186
	step [100/332], loss=6.1554
	step [101/332], loss=9.1293
	step [102/332], loss=5.9286
	step [103/332], loss=6.9213
	step [104/332], loss=6.8261
	step [105/332], loss=5.8781
	step [106/332], loss=6.8429
	step [107/332], loss=5.5415
	step [108/332], loss=6.7498
	step [109/332], loss=6.5332
	step [110/332], loss=6.5140
	step [111/332], loss=6.8098
	step [112/332], loss=7.1892
	step [113/332], loss=6.1836
	step [114/332], loss=5.4945
	step [115/332], loss=5.0838
	step [116/332], loss=5.7801
	step [117/332], loss=6.8878
	step [118/332], loss=6.5790
	step [119/332], loss=6.8216
	step [120/332], loss=6.4775
	step [121/332], loss=4.8142
	step [122/332], loss=5.8719
	step [123/332], loss=5.1015
	step [124/332], loss=4.1000
	step [125/332], loss=5.9135
	step [126/332], loss=5.6690
	step [127/332], loss=6.9760
	step [128/332], loss=4.4589
	step [129/332], loss=5.1874
	step [130/332], loss=6.0883
	step [131/332], loss=6.1655
	step [132/332], loss=5.5404
	step [133/332], loss=5.1075
	step [134/332], loss=5.5957
	step [135/332], loss=7.1392
	step [136/332], loss=6.6146
	step [137/332], loss=6.3746
	step [138/332], loss=5.7776
	step [139/332], loss=5.8733
	step [140/332], loss=6.1053
	step [141/332], loss=6.4718
	step [142/332], loss=6.6196
	step [143/332], loss=4.8845
	step [144/332], loss=6.4790
	step [145/332], loss=6.3854
	step [146/332], loss=6.3121
	step [147/332], loss=5.4588
	step [148/332], loss=5.6936
	step [149/332], loss=5.2628
	step [150/332], loss=7.3541
	step [151/332], loss=5.1634
	step [152/332], loss=5.5631
	step [153/332], loss=6.1615
	step [154/332], loss=5.2193
	step [155/332], loss=6.0366
	step [156/332], loss=5.9870
	step [157/332], loss=5.4111
	step [158/332], loss=5.0038
	step [159/332], loss=6.2163
	step [160/332], loss=5.7142
	step [161/332], loss=5.1324
	step [162/332], loss=5.2002
	step [163/332], loss=6.2866
	step [164/332], loss=5.5844
	step [165/332], loss=5.8594
	step [166/332], loss=6.8176
	step [167/332], loss=6.7991
	step [168/332], loss=5.6749
	step [169/332], loss=3.9850
	step [170/332], loss=7.1350
	step [171/332], loss=7.4291
	step [172/332], loss=5.3823
	step [173/332], loss=6.4378
	step [174/332], loss=6.5714
	step [175/332], loss=5.5413
	step [176/332], loss=6.0780
	step [177/332], loss=6.6098
	step [178/332], loss=5.5192
	step [179/332], loss=6.7080
	step [180/332], loss=5.8905
	step [181/332], loss=5.9583
	step [182/332], loss=7.6792
	step [183/332], loss=7.0588
	step [184/332], loss=8.5855
	step [185/332], loss=6.5749
	step [186/332], loss=5.6549
	step [187/332], loss=5.3913
	step [188/332], loss=5.0696
	step [189/332], loss=6.2600
	step [190/332], loss=6.5409
	step [191/332], loss=5.7098
	step [192/332], loss=6.2329
	step [193/332], loss=6.1272
	step [194/332], loss=6.7082
	step [195/332], loss=5.6774
	step [196/332], loss=5.9790
	step [197/332], loss=4.8521
	step [198/332], loss=6.6529
	step [199/332], loss=6.8725
	step [200/332], loss=5.0394
	step [201/332], loss=5.3004
	step [202/332], loss=5.2240
	step [203/332], loss=6.2546
	step [204/332], loss=5.7721
	step [205/332], loss=4.9884
	step [206/332], loss=4.4035
	step [207/332], loss=6.9214
	step [208/332], loss=6.2949
	step [209/332], loss=5.4611
	step [210/332], loss=6.6950
	step [211/332], loss=6.3020
	step [212/332], loss=4.8561
	step [213/332], loss=5.7521
	step [214/332], loss=6.0766
	step [215/332], loss=4.7552
	step [216/332], loss=6.3150
	step [217/332], loss=5.5918
	step [218/332], loss=5.6056
	step [219/332], loss=6.2465
	step [220/332], loss=5.6463
	step [221/332], loss=5.1592
	step [222/332], loss=6.6274
	step [223/332], loss=6.2030
	step [224/332], loss=5.6875
	step [225/332], loss=5.4690
	step [226/332], loss=4.4203
	step [227/332], loss=6.2182
	step [228/332], loss=4.2161
	step [229/332], loss=5.5386
	step [230/332], loss=6.5001
	step [231/332], loss=5.5156
	step [232/332], loss=5.1979
	step [233/332], loss=5.3230
	step [234/332], loss=6.1700
	step [235/332], loss=6.0690
	step [236/332], loss=6.4088
	step [237/332], loss=5.3174
	step [238/332], loss=4.8893
	step [239/332], loss=4.9134
	step [240/332], loss=4.7227
	step [241/332], loss=6.0310
	step [242/332], loss=5.7735
	step [243/332], loss=4.9922
	step [244/332], loss=5.3238
	step [245/332], loss=4.8986
	step [246/332], loss=5.4402
	step [247/332], loss=4.7855
	step [248/332], loss=4.4877
	step [249/332], loss=5.6024
	step [250/332], loss=5.1028
	step [251/332], loss=6.3129
	step [252/332], loss=5.1198
	step [253/332], loss=5.9604
	step [254/332], loss=5.2095
	step [255/332], loss=4.3893
	step [256/332], loss=5.5139
	step [257/332], loss=7.0363
	step [258/332], loss=6.4026
	step [259/332], loss=5.2593
	step [260/332], loss=5.4983
	step [261/332], loss=7.1736
	step [262/332], loss=5.1264
	step [263/332], loss=6.3577
	step [264/332], loss=6.6075
	step [265/332], loss=5.2287
	step [266/332], loss=5.0985
	step [267/332], loss=6.4222
	step [268/332], loss=6.3984
	step [269/332], loss=5.5308
	step [270/332], loss=6.0018
	step [271/332], loss=5.2935
	step [272/332], loss=5.9374
	step [273/332], loss=4.9210
	step [274/332], loss=5.9897
	step [275/332], loss=4.6084
	step [276/332], loss=8.3330
	step [277/332], loss=7.6631
	step [278/332], loss=6.0490
	step [279/332], loss=4.7245
	step [280/332], loss=5.7710
	step [281/332], loss=5.8165
	step [282/332], loss=5.6235
	step [283/332], loss=5.0747
	step [284/332], loss=5.9803
	step [285/332], loss=6.3173
	step [286/332], loss=5.5100
	step [287/332], loss=5.1800
	step [288/332], loss=5.1205
	step [289/332], loss=5.2681
	step [290/332], loss=6.5869
	step [291/332], loss=4.6916
	step [292/332], loss=6.1330
	step [293/332], loss=6.9886
	step [294/332], loss=6.7516
	step [295/332], loss=5.1373
	step [296/332], loss=6.6406
	step [297/332], loss=7.8548
	step [298/332], loss=5.7369
	step [299/332], loss=7.2432
	step [300/332], loss=4.6159
	step [301/332], loss=6.7575
	step [302/332], loss=5.1996
	step [303/332], loss=4.9275
	step [304/332], loss=5.6999
	step [305/332], loss=5.2423
	step [306/332], loss=5.8943
	step [307/332], loss=4.4491
	step [308/332], loss=4.5928
	step [309/332], loss=5.3757
	step [310/332], loss=5.8029
	step [311/332], loss=5.4187
	step [312/332], loss=7.1903
	step [313/332], loss=5.9139
	step [314/332], loss=5.0524
	step [315/332], loss=5.9099
	step [316/332], loss=5.8223
	step [317/332], loss=5.6695
	step [318/332], loss=5.3322
	step [319/332], loss=5.9629
	step [320/332], loss=5.4404
	step [321/332], loss=5.9582
	step [322/332], loss=7.5956
	step [323/332], loss=5.5808
	step [324/332], loss=8.6130
	step [325/332], loss=5.8689
	step [326/332], loss=5.5428
	step [327/332], loss=6.6360
	step [328/332], loss=5.3240
	step [329/332], loss=5.0895
	step [330/332], loss=5.3111
	step [331/332], loss=6.2051
	step [332/332], loss=3.3820
	Evaluating
	loss=0.0195, precision=0.1648, recall=0.9915, f1=0.6603
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/332], loss=5.5992
	step [2/332], loss=5.4656
	step [3/332], loss=4.5194
	step [4/332], loss=5.3350
	step [5/332], loss=5.9294
	step [6/332], loss=5.5430
	step [7/332], loss=7.4535
	step [8/332], loss=6.9909
	step [9/332], loss=6.5395
	step [10/332], loss=5.3175
	step [11/332], loss=4.8369
	step [12/332], loss=6.7941
	step [13/332], loss=6.5563
	step [14/332], loss=6.2036
	step [15/332], loss=6.2937
	step [16/332], loss=5.2518
	step [17/332], loss=5.6856
	step [18/332], loss=4.7400
	step [19/332], loss=4.8894
	step [20/332], loss=5.3448
	step [21/332], loss=5.8021
	step [22/332], loss=7.5046
	step [23/332], loss=6.7815
	step [24/332], loss=5.4087
	step [25/332], loss=7.0907
	step [26/332], loss=5.1598
	step [27/332], loss=6.1289
	step [28/332], loss=4.4588
	step [29/332], loss=7.5283
	step [30/332], loss=5.5941
	step [31/332], loss=6.3164
	step [32/332], loss=4.4695
	step [33/332], loss=6.0114
	step [34/332], loss=5.8074
	step [35/332], loss=4.8146
	step [36/332], loss=6.4954
	step [37/332], loss=4.7786
	step [38/332], loss=5.3269
	step [39/332], loss=4.8064
	step [40/332], loss=5.1676
	step [41/332], loss=5.0421
	step [42/332], loss=6.2605
	step [43/332], loss=4.4258
	step [44/332], loss=5.6264
	step [45/332], loss=6.0227
	step [46/332], loss=5.6459
	step [47/332], loss=5.8954
	step [48/332], loss=6.0312
	step [49/332], loss=4.8742
	step [50/332], loss=5.4690
	step [51/332], loss=4.1348
	step [52/332], loss=4.8103
	step [53/332], loss=6.6992
	step [54/332], loss=4.7410
	step [55/332], loss=5.6816
	step [56/332], loss=5.9984
	step [57/332], loss=5.3314
	step [58/332], loss=7.5033
	step [59/332], loss=6.4205
	step [60/332], loss=7.7207
	step [61/332], loss=5.1355
	step [62/332], loss=5.9421
	step [63/332], loss=7.7769
	step [64/332], loss=4.9064
	step [65/332], loss=5.2470
	step [66/332], loss=5.7250
	step [67/332], loss=6.3584
	step [68/332], loss=5.9384
	step [69/332], loss=4.9841
	step [70/332], loss=5.3579
	step [71/332], loss=6.8750
	step [72/332], loss=6.1944
	step [73/332], loss=4.8777
	step [74/332], loss=5.8146
	step [75/332], loss=5.7926
	step [76/332], loss=6.0252
	step [77/332], loss=6.0479
	step [78/332], loss=7.2972
	step [79/332], loss=6.2520
	step [80/332], loss=5.2859
	step [81/332], loss=7.1424
	step [82/332], loss=6.4132
	step [83/332], loss=7.6788
	step [84/332], loss=5.3070
	step [85/332], loss=5.2949
	step [86/332], loss=4.8578
	step [87/332], loss=5.2372
	step [88/332], loss=6.0172
	step [89/332], loss=5.1312
	step [90/332], loss=4.8518
	step [91/332], loss=7.2247
	step [92/332], loss=5.2702
	step [93/332], loss=5.9578
	step [94/332], loss=5.2827
	step [95/332], loss=5.0069
	step [96/332], loss=6.7724
	step [97/332], loss=5.8295
	step [98/332], loss=4.5350
	step [99/332], loss=4.6401
	step [100/332], loss=6.0211
	step [101/332], loss=6.8393
	step [102/332], loss=9.5868
	step [103/332], loss=5.5848
	step [104/332], loss=4.4898
	step [105/332], loss=6.1303
	step [106/332], loss=6.0279
	step [107/332], loss=5.7880
	step [108/332], loss=6.2299
	step [109/332], loss=4.8875
	step [110/332], loss=5.2641
	step [111/332], loss=6.1533
	step [112/332], loss=5.2214
	step [113/332], loss=8.9395
	step [114/332], loss=5.7300
	step [115/332], loss=3.9922
	step [116/332], loss=4.7163
	step [117/332], loss=5.4118
	step [118/332], loss=6.1774
	step [119/332], loss=5.7921
	step [120/332], loss=4.7950
	step [121/332], loss=5.9536
	step [122/332], loss=6.1630
	step [123/332], loss=5.4242
	step [124/332], loss=4.9233
	step [125/332], loss=8.0734
	step [126/332], loss=5.0084
	step [127/332], loss=5.2216
	step [128/332], loss=7.2238
	step [129/332], loss=5.4773
	step [130/332], loss=4.3551
	step [131/332], loss=6.9208
	step [132/332], loss=6.1048
	step [133/332], loss=4.9938
	step [134/332], loss=6.7975
	step [135/332], loss=5.8338
	step [136/332], loss=5.4973
	step [137/332], loss=4.2977
	step [138/332], loss=4.3214
	step [139/332], loss=5.9509
	step [140/332], loss=5.0054
	step [141/332], loss=5.0178
	step [142/332], loss=5.2263
	step [143/332], loss=6.3384
	step [144/332], loss=5.8595
	step [145/332], loss=4.2067
	step [146/332], loss=5.7224
	step [147/332], loss=5.0276
	step [148/332], loss=7.2159
	step [149/332], loss=7.0196
	step [150/332], loss=5.6499
	step [151/332], loss=6.1892
	step [152/332], loss=5.4422
	step [153/332], loss=4.4101
	step [154/332], loss=7.0832
	step [155/332], loss=4.5877
	step [156/332], loss=6.8875
	step [157/332], loss=5.3812
	step [158/332], loss=5.3474
	step [159/332], loss=4.2338
	step [160/332], loss=4.6656
	step [161/332], loss=5.2844
	step [162/332], loss=5.2285
	step [163/332], loss=5.9364
	step [164/332], loss=6.2939
	step [165/332], loss=4.8865
	step [166/332], loss=5.9489
	step [167/332], loss=5.1824
	step [168/332], loss=4.8915
	step [169/332], loss=6.8931
	step [170/332], loss=4.0757
	step [171/332], loss=5.6657
	step [172/332], loss=5.7858
	step [173/332], loss=5.2008
	step [174/332], loss=6.2621
	step [175/332], loss=6.0757
	step [176/332], loss=5.8458
	step [177/332], loss=6.1820
	step [178/332], loss=6.7297
	step [179/332], loss=4.5419
	step [180/332], loss=5.1543
	step [181/332], loss=5.9873
	step [182/332], loss=5.8433
	step [183/332], loss=6.9158
	step [184/332], loss=6.1450
	step [185/332], loss=5.4092
	step [186/332], loss=6.7449
	step [187/332], loss=5.5709
	step [188/332], loss=6.0643
	step [189/332], loss=5.2613
	step [190/332], loss=7.0551
	step [191/332], loss=5.5487
	step [192/332], loss=7.1298
	step [193/332], loss=5.5615
	step [194/332], loss=6.8297
	step [195/332], loss=4.5985
	step [196/332], loss=6.3464
	step [197/332], loss=4.8738
	step [198/332], loss=4.8788
	step [199/332], loss=5.3842
	step [200/332], loss=4.9722
	step [201/332], loss=4.9857
	step [202/332], loss=4.3912
	step [203/332], loss=4.3369
	step [204/332], loss=5.6847
	step [205/332], loss=5.2627
	step [206/332], loss=5.5768
	step [207/332], loss=6.5358
	step [208/332], loss=5.5475
	step [209/332], loss=6.5024
	step [210/332], loss=6.3332
	step [211/332], loss=6.3131
	step [212/332], loss=6.7434
	step [213/332], loss=4.7743
	step [214/332], loss=4.5048
	step [215/332], loss=6.6349
	step [216/332], loss=7.1945
	step [217/332], loss=4.6960
	step [218/332], loss=6.6971
	step [219/332], loss=5.9562
	step [220/332], loss=5.9275
	step [221/332], loss=5.2515
	step [222/332], loss=7.0245
	step [223/332], loss=4.6439
	step [224/332], loss=5.8699
	step [225/332], loss=5.1834
	step [226/332], loss=6.6274
	step [227/332], loss=5.8049
	step [228/332], loss=5.7845
	step [229/332], loss=5.3792
	step [230/332], loss=5.3203
	step [231/332], loss=5.0575
	step [232/332], loss=4.5282
	step [233/332], loss=6.9131
	step [234/332], loss=5.1790
	step [235/332], loss=5.6449
	step [236/332], loss=5.3777
	step [237/332], loss=6.7272
	step [238/332], loss=5.4968
	step [239/332], loss=7.4459
	step [240/332], loss=5.5586
	step [241/332], loss=5.7270
	step [242/332], loss=6.2670
	step [243/332], loss=6.5821
	step [244/332], loss=5.6152
	step [245/332], loss=5.2423
	step [246/332], loss=5.4219
	step [247/332], loss=6.3140
	step [248/332], loss=5.0265
	step [249/332], loss=5.1301
	step [250/332], loss=6.0179
	step [251/332], loss=4.8081
	step [252/332], loss=4.9981
	step [253/332], loss=4.8931
	step [254/332], loss=4.3984
	step [255/332], loss=5.1335
	step [256/332], loss=5.2306
	step [257/332], loss=6.0142
	step [258/332], loss=5.9359
	step [259/332], loss=5.3614
	step [260/332], loss=6.5188
	step [261/332], loss=6.6661
	step [262/332], loss=4.2923
	step [263/332], loss=5.6525
	step [264/332], loss=4.4133
	step [265/332], loss=6.2964
	step [266/332], loss=5.3334
	step [267/332], loss=5.5626
	step [268/332], loss=4.5415
	step [269/332], loss=6.0332
	step [270/332], loss=6.7992
	step [271/332], loss=5.2575
	step [272/332], loss=4.5121
	step [273/332], loss=5.9878
	step [274/332], loss=5.0632
	step [275/332], loss=6.2983
	step [276/332], loss=6.3708
	step [277/332], loss=5.9440
	step [278/332], loss=4.3314
	step [279/332], loss=6.4779
	step [280/332], loss=5.3537
	step [281/332], loss=4.4257
	step [282/332], loss=5.2848
	step [283/332], loss=4.9132
	step [284/332], loss=5.7235
	step [285/332], loss=5.9414
	step [286/332], loss=6.0956
	step [287/332], loss=5.0993
	step [288/332], loss=4.1229
	step [289/332], loss=5.7856
	step [290/332], loss=5.1968
	step [291/332], loss=6.0548
	step [292/332], loss=6.2492
	step [293/332], loss=5.0383
	step [294/332], loss=6.2185
	step [295/332], loss=6.2474
	step [296/332], loss=5.6051
	step [297/332], loss=6.5392
	step [298/332], loss=5.8586
	step [299/332], loss=7.0779
	step [300/332], loss=6.6102
	step [301/332], loss=5.4339
	step [302/332], loss=6.0084
	step [303/332], loss=6.1384
	step [304/332], loss=4.8848
	step [305/332], loss=7.8972
	step [306/332], loss=5.3562
	step [307/332], loss=6.1700
	step [308/332], loss=6.5498
	step [309/332], loss=5.6175
	step [310/332], loss=5.4598
	step [311/332], loss=5.5276
	step [312/332], loss=6.5257
	step [313/332], loss=5.9511
	step [314/332], loss=6.5859
	step [315/332], loss=6.1853
	step [316/332], loss=6.0077
	step [317/332], loss=5.7216
	step [318/332], loss=6.2825
	step [319/332], loss=5.5718
	step [320/332], loss=6.0677
	step [321/332], loss=5.4509
	step [322/332], loss=5.6725
	step [323/332], loss=6.5360
	step [324/332], loss=6.2046
	step [325/332], loss=5.9471
	step [326/332], loss=5.8072
	step [327/332], loss=6.6655
	step [328/332], loss=6.8601
	step [329/332], loss=4.8619
	step [330/332], loss=6.1961
	step [331/332], loss=5.6607
	step [332/332], loss=5.6036
	Evaluating
	loss=0.0202, precision=0.1645, recall=0.9923, f1=0.6602
Training epoch 24
	step [1/332], loss=4.3575
	step [2/332], loss=4.8561
	step [3/332], loss=6.8560
	step [4/332], loss=5.2375
	step [5/332], loss=5.5591
	step [6/332], loss=7.1418
	step [7/332], loss=5.6192
	step [8/332], loss=6.1803
	step [9/332], loss=5.9521
	step [10/332], loss=4.6244
	step [11/332], loss=6.2017
	step [12/332], loss=5.1938
	step [13/332], loss=5.5683
	step [14/332], loss=4.5256
	step [15/332], loss=4.6434
	step [16/332], loss=5.5271
	step [17/332], loss=5.2299
	step [18/332], loss=6.5687
	step [19/332], loss=5.4707
	step [20/332], loss=6.7402
	step [21/332], loss=4.4729
	step [22/332], loss=4.6221
	step [23/332], loss=4.7997
	step [24/332], loss=5.4276
	step [25/332], loss=6.7385
	step [26/332], loss=7.2059
	step [27/332], loss=6.4769
	step [28/332], loss=5.6830
	step [29/332], loss=5.4242
	step [30/332], loss=4.7782
	step [31/332], loss=5.6389
	step [32/332], loss=6.0486
	step [33/332], loss=5.0849
	step [34/332], loss=5.2088
	step [35/332], loss=5.3523
	step [36/332], loss=5.0150
	step [37/332], loss=5.6117
	step [38/332], loss=8.3190
	step [39/332], loss=6.5449
	step [40/332], loss=5.2478
	step [41/332], loss=5.4816
	step [42/332], loss=5.0913
	step [43/332], loss=5.7133
	step [44/332], loss=6.1004
	step [45/332], loss=8.2770
	step [46/332], loss=5.4678
	step [47/332], loss=6.5346
	step [48/332], loss=6.2311
	step [49/332], loss=5.4693
	step [50/332], loss=4.7605
	step [51/332], loss=5.5392
	step [52/332], loss=6.9995
	step [53/332], loss=5.2842
	step [54/332], loss=6.5333
	step [55/332], loss=6.6631
	step [56/332], loss=5.3928
	step [57/332], loss=5.0100
	step [58/332], loss=6.9184
	step [59/332], loss=5.8891
	step [60/332], loss=4.9229
	step [61/332], loss=5.0245
	step [62/332], loss=6.2100
	step [63/332], loss=4.9001
	step [64/332], loss=4.6175
	step [65/332], loss=6.4585
	step [66/332], loss=5.1554
	step [67/332], loss=7.4383
	step [68/332], loss=4.7614
	step [69/332], loss=5.6396
	step [70/332], loss=4.4590
	step [71/332], loss=5.3697
	step [72/332], loss=5.8536
	step [73/332], loss=5.2248
	step [74/332], loss=6.4623
	step [75/332], loss=5.2143
	step [76/332], loss=5.0837
	step [77/332], loss=4.9173
	step [78/332], loss=4.7404
	step [79/332], loss=7.8272
	step [80/332], loss=5.5508
	step [81/332], loss=5.7440
	step [82/332], loss=5.9571
	step [83/332], loss=3.8461
	step [84/332], loss=7.1224
	step [85/332], loss=4.9586
	step [86/332], loss=5.2086
	step [87/332], loss=4.6166
	step [88/332], loss=4.6992
	step [89/332], loss=5.8199
	step [90/332], loss=6.3103
	step [91/332], loss=6.3026
	step [92/332], loss=7.3650
	step [93/332], loss=5.8407
	step [94/332], loss=7.0806
	step [95/332], loss=6.1641
	step [96/332], loss=6.5756
	step [97/332], loss=5.6517
	step [98/332], loss=5.6764
	step [99/332], loss=5.3652
	step [100/332], loss=4.9052
	step [101/332], loss=5.7991
	step [102/332], loss=5.9000
	step [103/332], loss=5.0640
	step [104/332], loss=4.8980
	step [105/332], loss=5.2936
	step [106/332], loss=5.7883
	step [107/332], loss=7.1758
	step [108/332], loss=5.1250
	step [109/332], loss=5.2147
	step [110/332], loss=5.0176
	step [111/332], loss=5.7724
	step [112/332], loss=4.9708
	step [113/332], loss=4.4459
	step [114/332], loss=4.4758
	step [115/332], loss=6.5256
	step [116/332], loss=5.5541
	step [117/332], loss=4.9603
	step [118/332], loss=5.5296
	step [119/332], loss=7.4233
	step [120/332], loss=6.3287
	step [121/332], loss=6.1486
	step [122/332], loss=6.0977
	step [123/332], loss=6.3723
	step [124/332], loss=6.7975
	step [125/332], loss=5.0142
	step [126/332], loss=5.6712
	step [127/332], loss=6.1883
	step [128/332], loss=5.8724
	step [129/332], loss=4.5894
	step [130/332], loss=6.3808
	step [131/332], loss=5.8584
	step [132/332], loss=5.5583
	step [133/332], loss=5.9584
	step [134/332], loss=5.4229
	step [135/332], loss=6.4196
	step [136/332], loss=4.8230
	step [137/332], loss=4.8702
	step [138/332], loss=6.0731
	step [139/332], loss=4.5325
	step [140/332], loss=4.9630
	step [141/332], loss=4.1421
	step [142/332], loss=5.9584
	step [143/332], loss=6.9978
	step [144/332], loss=4.0942
	step [145/332], loss=4.0888
	step [146/332], loss=4.4538
	step [147/332], loss=5.7048
	step [148/332], loss=5.4380
	step [149/332], loss=4.5203
	step [150/332], loss=4.3759
	step [151/332], loss=5.5553
	step [152/332], loss=4.5532
	step [153/332], loss=6.0491
	step [154/332], loss=4.0629
	step [155/332], loss=6.6936
	step [156/332], loss=4.6716
	step [157/332], loss=5.0640
	step [158/332], loss=4.9323
	step [159/332], loss=5.1229
	step [160/332], loss=6.2953
	step [161/332], loss=5.9738
	step [162/332], loss=5.2617
	step [163/332], loss=5.1538
	step [164/332], loss=5.4790
	step [165/332], loss=5.4025
	step [166/332], loss=5.5834
	step [167/332], loss=5.4860
	step [168/332], loss=5.8374
	step [169/332], loss=5.3939
	step [170/332], loss=4.0482
	step [171/332], loss=4.6620
	step [172/332], loss=6.1118
	step [173/332], loss=6.5962
	step [174/332], loss=6.1710
	step [175/332], loss=5.9447
	step [176/332], loss=5.4830
	step [177/332], loss=4.6922
	step [178/332], loss=4.9028
	step [179/332], loss=5.2813
	step [180/332], loss=6.0091
	step [181/332], loss=4.4263
	step [182/332], loss=5.3128
	step [183/332], loss=6.4409
	step [184/332], loss=5.8752
	step [185/332], loss=5.7744
	step [186/332], loss=5.0931
	step [187/332], loss=5.9688
	step [188/332], loss=5.1529
	step [189/332], loss=5.4579
	step [190/332], loss=5.5197
	step [191/332], loss=7.0322
	step [192/332], loss=4.4342
	step [193/332], loss=4.8971
	step [194/332], loss=6.0176
	step [195/332], loss=5.1091
	step [196/332], loss=5.3721
	step [197/332], loss=6.3559
	step [198/332], loss=4.5747
	step [199/332], loss=5.2529
	step [200/332], loss=4.8714
	step [201/332], loss=6.3468
	step [202/332], loss=5.3060
	step [203/332], loss=5.9833
	step [204/332], loss=4.8435
	step [205/332], loss=5.5257
	step [206/332], loss=4.7819
	step [207/332], loss=5.8125
	step [208/332], loss=4.6573
	step [209/332], loss=4.9590
	step [210/332], loss=5.1734
	step [211/332], loss=5.0929
	step [212/332], loss=5.3531
	step [213/332], loss=6.1830
	step [214/332], loss=6.2448
	step [215/332], loss=6.8023
	step [216/332], loss=6.0203
	step [217/332], loss=4.0844
	step [218/332], loss=6.1335
	step [219/332], loss=5.6250
	step [220/332], loss=5.2279
	step [221/332], loss=4.8917
	step [222/332], loss=5.9615
	step [223/332], loss=5.7987
	step [224/332], loss=5.4511
	step [225/332], loss=6.4644
	step [226/332], loss=5.5783
	step [227/332], loss=5.8554
	step [228/332], loss=4.5324
	step [229/332], loss=5.9598
	step [230/332], loss=4.8265
	step [231/332], loss=7.1271
	step [232/332], loss=5.4962
	step [233/332], loss=4.7972
	step [234/332], loss=5.6308
	step [235/332], loss=4.5313
	step [236/332], loss=5.9490
	step [237/332], loss=4.7375
	step [238/332], loss=5.1764
	step [239/332], loss=6.8570
	step [240/332], loss=5.4036
	step [241/332], loss=5.6132
	step [242/332], loss=7.9103
	step [243/332], loss=5.7202
	step [244/332], loss=6.0078
	step [245/332], loss=5.9452
	step [246/332], loss=5.7690
	step [247/332], loss=6.2285
	step [248/332], loss=6.1881
	step [249/332], loss=5.8910
	step [250/332], loss=4.9581
	step [251/332], loss=4.4874
	step [252/332], loss=4.5604
	step [253/332], loss=5.4648
	step [254/332], loss=5.5251
	step [255/332], loss=5.2838
	step [256/332], loss=5.1333
	step [257/332], loss=4.9512
	step [258/332], loss=6.0172
	step [259/332], loss=4.7244
	step [260/332], loss=4.6988
	step [261/332], loss=6.3153
	step [262/332], loss=6.0029
	step [263/332], loss=5.3204
	step [264/332], loss=6.6498
	step [265/332], loss=4.9824
	step [266/332], loss=6.2675
	step [267/332], loss=5.5135
	step [268/332], loss=4.6606
	step [269/332], loss=4.4395
	step [270/332], loss=3.7007
	step [271/332], loss=5.5055
	step [272/332], loss=6.3103
	step [273/332], loss=4.9626
	step [274/332], loss=4.8771
	step [275/332], loss=4.4793
	step [276/332], loss=5.9522
	step [277/332], loss=7.5646
	step [278/332], loss=5.2869
	step [279/332], loss=5.1286
	step [280/332], loss=3.9367
	step [281/332], loss=5.5439
	step [282/332], loss=4.6387
	step [283/332], loss=4.6022
	step [284/332], loss=5.4250
	step [285/332], loss=5.7839
	step [286/332], loss=6.4140
	step [287/332], loss=5.0673
	step [288/332], loss=6.0229
	step [289/332], loss=5.5780
	step [290/332], loss=7.5690
	step [291/332], loss=5.4062
	step [292/332], loss=4.2377
	step [293/332], loss=4.8584
	step [294/332], loss=5.5043
	step [295/332], loss=5.2741
	step [296/332], loss=5.1869
	step [297/332], loss=7.3109
	step [298/332], loss=5.4579
	step [299/332], loss=5.9766
	step [300/332], loss=5.6003
	step [301/332], loss=5.0907
	step [302/332], loss=6.3019
	step [303/332], loss=5.0149
	step [304/332], loss=4.5174
	step [305/332], loss=5.3607
	step [306/332], loss=6.0211
	step [307/332], loss=5.5629
	step [308/332], loss=5.6438
	step [309/332], loss=5.9734
	step [310/332], loss=5.3984
	step [311/332], loss=5.6959
	step [312/332], loss=5.3323
	step [313/332], loss=5.4653
	step [314/332], loss=5.1779
	step [315/332], loss=4.2618
	step [316/332], loss=6.2506
	step [317/332], loss=3.6396
	step [318/332], loss=5.8309
	step [319/332], loss=6.3804
	step [320/332], loss=4.8096
	step [321/332], loss=4.8323
	step [322/332], loss=5.0390
	step [323/332], loss=5.3900
	step [324/332], loss=4.9398
	step [325/332], loss=5.8208
	step [326/332], loss=5.3334
	step [327/332], loss=4.9739
	step [328/332], loss=5.2783
	step [329/332], loss=5.0168
	step [330/332], loss=6.6312
	step [331/332], loss=5.3377
	step [332/332], loss=2.7614
	Evaluating
	loss=0.0188, precision=0.1744, recall=0.9920, f1=0.6754
saving model as: 0_saved_model.pth
Training epoch 25
	step [1/332], loss=4.9850
	step [2/332], loss=5.0770
	step [3/332], loss=6.1331
	step [4/332], loss=5.1790
	step [5/332], loss=4.4020
	step [6/332], loss=5.7630
	step [7/332], loss=5.0963
	step [8/332], loss=5.4297
	step [9/332], loss=4.8069
	step [10/332], loss=5.7928
	step [11/332], loss=5.1441
	step [12/332], loss=4.8938
	step [13/332], loss=5.6602
	step [14/332], loss=5.9371
	step [15/332], loss=5.3680
	step [16/332], loss=4.3749
	step [17/332], loss=5.4816
	step [18/332], loss=4.2005
	step [19/332], loss=5.1197
	step [20/332], loss=4.6785
	step [21/332], loss=5.4621
	step [22/332], loss=6.7073
	step [23/332], loss=5.0484
	step [24/332], loss=4.9182
	step [25/332], loss=4.6937
	step [26/332], loss=6.0943
	step [27/332], loss=7.0268
	step [28/332], loss=6.2943
	step [29/332], loss=6.5568
	step [30/332], loss=4.3303
	step [31/332], loss=4.0022
	step [32/332], loss=5.1066
	step [33/332], loss=5.0728
	step [34/332], loss=5.9172
	step [35/332], loss=5.7233
	step [36/332], loss=5.8691
	step [37/332], loss=4.9730
	step [38/332], loss=4.8005
	step [39/332], loss=6.3325
	step [40/332], loss=5.0104
	step [41/332], loss=5.2450
	step [42/332], loss=4.7274
	step [43/332], loss=6.1697
	step [44/332], loss=5.1996
	step [45/332], loss=5.5877
	step [46/332], loss=4.8130
	step [47/332], loss=4.7245
	step [48/332], loss=4.8981
	step [49/332], loss=5.9668
	step [50/332], loss=5.9770
	step [51/332], loss=5.1878
	step [52/332], loss=5.5865
	step [53/332], loss=5.3018
	step [54/332], loss=4.4880
	step [55/332], loss=4.9754
	step [56/332], loss=5.5810
	step [57/332], loss=3.5291
	step [58/332], loss=5.0642
	step [59/332], loss=7.0299
	step [60/332], loss=4.2777
	step [61/332], loss=4.5278
	step [62/332], loss=5.8261
	step [63/332], loss=6.7012
	step [64/332], loss=5.2738
	step [65/332], loss=4.5028
	step [66/332], loss=5.2321
	step [67/332], loss=5.4760
	step [68/332], loss=7.4611
	step [69/332], loss=4.9234
	step [70/332], loss=5.3196
	step [71/332], loss=6.0555
	step [72/332], loss=4.2943
	step [73/332], loss=5.0940
	step [74/332], loss=6.0921
	step [75/332], loss=5.2009
	step [76/332], loss=5.1032
	step [77/332], loss=4.4211
	step [78/332], loss=4.4945
	step [79/332], loss=7.3590
	step [80/332], loss=5.6498
	step [81/332], loss=5.4443
	step [82/332], loss=5.8584
	step [83/332], loss=6.0197
	step [84/332], loss=4.8159
	step [85/332], loss=5.3218
	step [86/332], loss=6.1638
	step [87/332], loss=5.2811
	step [88/332], loss=4.7630
	step [89/332], loss=5.9116
	step [90/332], loss=4.7862
	step [91/332], loss=5.6618
	step [92/332], loss=4.3023
	step [93/332], loss=4.2631
	step [94/332], loss=4.6616
	step [95/332], loss=5.9586
	step [96/332], loss=6.0987
	step [97/332], loss=5.8282
	step [98/332], loss=6.0857
	step [99/332], loss=5.6224
	step [100/332], loss=5.7296
	step [101/332], loss=5.3358
	step [102/332], loss=6.6974
	step [103/332], loss=4.8689
	step [104/332], loss=4.4643
	step [105/332], loss=4.6283
	step [106/332], loss=4.4908
	step [107/332], loss=6.0439
	step [108/332], loss=5.3053
	step [109/332], loss=5.2986
	step [110/332], loss=6.0234
	step [111/332], loss=4.4132
	step [112/332], loss=6.3696
	step [113/332], loss=4.2017
	step [114/332], loss=6.3593
	step [115/332], loss=5.2220
	step [116/332], loss=6.4099
	step [117/332], loss=5.0216
	step [118/332], loss=5.2148
	step [119/332], loss=4.4797
	step [120/332], loss=5.6378
	step [121/332], loss=5.4383
	step [122/332], loss=6.0651
	step [123/332], loss=5.2512
	step [124/332], loss=3.9402
	step [125/332], loss=4.1457
	step [126/332], loss=5.8901
	step [127/332], loss=4.4675
	step [128/332], loss=4.8267
	step [129/332], loss=4.8010
	step [130/332], loss=4.7363
	step [131/332], loss=6.1932
	step [132/332], loss=5.8261
	step [133/332], loss=5.1709
	step [134/332], loss=4.4694
	step [135/332], loss=6.0583
	step [136/332], loss=4.6493
	step [137/332], loss=5.8317
	step [138/332], loss=5.6339
	step [139/332], loss=4.5549
	step [140/332], loss=6.2666
	step [141/332], loss=6.4743
	step [142/332], loss=6.2531
	step [143/332], loss=5.6074
	step [144/332], loss=4.5015
	step [145/332], loss=7.7788
	step [146/332], loss=4.9480
	step [147/332], loss=4.9039
	step [148/332], loss=4.1565
	step [149/332], loss=4.6421
	step [150/332], loss=6.3904
	step [151/332], loss=5.3182
	step [152/332], loss=7.1396
	step [153/332], loss=5.1970
	step [154/332], loss=5.4272
	step [155/332], loss=6.0965
	step [156/332], loss=5.7455
	step [157/332], loss=4.5152
	step [158/332], loss=5.0913
	step [159/332], loss=4.7906
	step [160/332], loss=6.6386
	step [161/332], loss=6.1131
	step [162/332], loss=5.4104
	step [163/332], loss=5.5146
	step [164/332], loss=7.4483
	step [165/332], loss=6.8136
	step [166/332], loss=3.9834
	step [167/332], loss=6.8298
	step [168/332], loss=6.3437
	step [169/332], loss=5.5189
	step [170/332], loss=5.8073
	step [171/332], loss=5.6191
	step [172/332], loss=4.9230
	step [173/332], loss=7.7558
	step [174/332], loss=4.6410
	step [175/332], loss=5.3326
	step [176/332], loss=5.0865
	step [177/332], loss=5.7655
	step [178/332], loss=4.1439
	step [179/332], loss=6.2382
	step [180/332], loss=5.2708
	step [181/332], loss=5.6456
	step [182/332], loss=3.4534
	step [183/332], loss=5.0692
	step [184/332], loss=4.1478
	step [185/332], loss=5.8790
	step [186/332], loss=5.8749
	step [187/332], loss=5.3846
	step [188/332], loss=5.3870
	step [189/332], loss=4.8083
	step [190/332], loss=4.0377
	step [191/332], loss=4.2117
	step [192/332], loss=4.2036
	step [193/332], loss=4.6041
	step [194/332], loss=6.8680
	step [195/332], loss=5.2862
	step [196/332], loss=5.2444
	step [197/332], loss=6.1120
	step [198/332], loss=5.0843
	step [199/332], loss=5.5865
	step [200/332], loss=6.7874
	step [201/332], loss=4.8130
	step [202/332], loss=4.3219
	step [203/332], loss=7.8174
	step [204/332], loss=5.9244
	step [205/332], loss=4.6685
	step [206/332], loss=5.8697
	step [207/332], loss=4.0032
	step [208/332], loss=5.0603
	step [209/332], loss=6.4473
	step [210/332], loss=5.6760
	step [211/332], loss=5.2450
	step [212/332], loss=6.0486
	step [213/332], loss=5.2634
	step [214/332], loss=7.1805
	step [215/332], loss=4.4462
	step [216/332], loss=7.0541
	step [217/332], loss=5.5437
	step [218/332], loss=5.4760
	step [219/332], loss=4.9981
	step [220/332], loss=4.6235
	step [221/332], loss=6.2226
	step [222/332], loss=5.0082
	step [223/332], loss=4.8307
	step [224/332], loss=4.7909
	step [225/332], loss=4.0609
	step [226/332], loss=6.6926
	step [227/332], loss=6.8158
	step [228/332], loss=5.0916
	step [229/332], loss=3.7224
	step [230/332], loss=4.8655
	step [231/332], loss=5.4274
	step [232/332], loss=5.8527
	step [233/332], loss=4.8126
	step [234/332], loss=5.9043
	step [235/332], loss=5.5316
	step [236/332], loss=6.5008
	step [237/332], loss=6.0919
	step [238/332], loss=5.9655
	step [239/332], loss=5.4262
	step [240/332], loss=5.1107
	step [241/332], loss=5.5557
	step [242/332], loss=4.7540
	step [243/332], loss=4.3281
	step [244/332], loss=4.1625
	step [245/332], loss=4.5648
	step [246/332], loss=5.9450
	step [247/332], loss=5.4477
	step [248/332], loss=5.3275
	step [249/332], loss=4.7051
	step [250/332], loss=6.3604
	step [251/332], loss=6.3131
	step [252/332], loss=6.4031
	step [253/332], loss=4.8045
	step [254/332], loss=5.6503
	step [255/332], loss=6.8373
	step [256/332], loss=4.7402
	step [257/332], loss=4.2208
	step [258/332], loss=5.6791
	step [259/332], loss=4.2112
	step [260/332], loss=4.7536
	step [261/332], loss=7.4637
	step [262/332], loss=4.3170
	step [263/332], loss=5.6368
	step [264/332], loss=5.1604
	step [265/332], loss=5.5253
	step [266/332], loss=5.3277
	step [267/332], loss=4.2874
	step [268/332], loss=5.3673
	step [269/332], loss=5.0508
	step [270/332], loss=3.9881
	step [271/332], loss=5.1189
	step [272/332], loss=5.1623
	step [273/332], loss=5.4510
	step [274/332], loss=4.5917
	step [275/332], loss=5.6653
	step [276/332], loss=6.3224
	step [277/332], loss=4.9390
	step [278/332], loss=4.8850
	step [279/332], loss=6.5487
	step [280/332], loss=4.2366
	step [281/332], loss=5.5331
	step [282/332], loss=5.5269
	step [283/332], loss=4.5182
	step [284/332], loss=4.8101
	step [285/332], loss=4.8812
	step [286/332], loss=6.1817
	step [287/332], loss=6.0226
	step [288/332], loss=4.6123
	step [289/332], loss=6.3251
	step [290/332], loss=4.1481
	step [291/332], loss=5.0702
	step [292/332], loss=5.3940
	step [293/332], loss=4.8658
	step [294/332], loss=6.9359
	step [295/332], loss=4.4890
	step [296/332], loss=5.2657
	step [297/332], loss=6.8204
	step [298/332], loss=5.1900
	step [299/332], loss=5.2399
	step [300/332], loss=4.8346
	step [301/332], loss=4.6881
	step [302/332], loss=5.1507
	step [303/332], loss=4.6414
	step [304/332], loss=5.9444
	step [305/332], loss=6.3011
	step [306/332], loss=4.6765
	step [307/332], loss=6.9275
	step [308/332], loss=4.5528
	step [309/332], loss=4.5460
	step [310/332], loss=5.7193
	step [311/332], loss=5.9980
	step [312/332], loss=4.4946
	step [313/332], loss=5.9373
	step [314/332], loss=8.3042
	step [315/332], loss=7.0358
	step [316/332], loss=5.8686
	step [317/332], loss=5.0733
	step [318/332], loss=5.6680
	step [319/332], loss=5.8238
	step [320/332], loss=6.8292
	step [321/332], loss=5.5293
	step [322/332], loss=4.9647
	step [323/332], loss=4.9787
	step [324/332], loss=4.6114
	step [325/332], loss=5.6269
	step [326/332], loss=6.2830
	step [327/332], loss=5.7182
	step [328/332], loss=5.7466
	step [329/332], loss=4.6795
	step [330/332], loss=4.5943
	step [331/332], loss=5.8150
	step [332/332], loss=2.5710
	Evaluating
	loss=0.0225, precision=0.1503, recall=0.9932, f1=0.6363
Training epoch 26
	step [1/332], loss=6.1075
	step [2/332], loss=6.0930
	step [3/332], loss=4.7276
	step [4/332], loss=6.3407
	step [5/332], loss=5.0787
	step [6/332], loss=3.9315
	step [7/332], loss=7.2368
	step [8/332], loss=5.9286
	step [9/332], loss=4.4785
	step [10/332], loss=4.2841
	step [11/332], loss=3.5925
	step [12/332], loss=5.3526
	step [13/332], loss=4.4880
	step [14/332], loss=5.7574
	step [15/332], loss=4.4267
	step [16/332], loss=6.8099
	step [17/332], loss=6.1488
	step [18/332], loss=4.9896
	step [19/332], loss=5.5065
	step [20/332], loss=5.9330
	step [21/332], loss=5.3860
	step [22/332], loss=4.0051
	step [23/332], loss=4.8903
	step [24/332], loss=5.3685
	step [25/332], loss=4.8880
	step [26/332], loss=4.1683
	step [27/332], loss=5.6153
	step [28/332], loss=5.9919
	step [29/332], loss=4.1168
	step [30/332], loss=4.6834
	step [31/332], loss=4.6688
	step [32/332], loss=5.8070
	step [33/332], loss=4.7604
	step [34/332], loss=5.9290
	step [35/332], loss=4.0285
	step [36/332], loss=5.6955
	step [37/332], loss=6.6358
	step [38/332], loss=4.9746
	step [39/332], loss=6.1071
	step [40/332], loss=4.6852
	step [41/332], loss=4.7309
	step [42/332], loss=4.2603
	step [43/332], loss=4.9066
	step [44/332], loss=8.1411
	step [45/332], loss=6.4481
	step [46/332], loss=5.5376
	step [47/332], loss=5.2969
	step [48/332], loss=4.6268
	step [49/332], loss=4.6869
	step [50/332], loss=4.4341
	step [51/332], loss=6.7597
	step [52/332], loss=4.6807
	step [53/332], loss=4.4146
	step [54/332], loss=5.1648
	step [55/332], loss=5.3190
	step [56/332], loss=7.5501
	step [57/332], loss=5.4732
	step [58/332], loss=5.8136
	step [59/332], loss=5.6305
	step [60/332], loss=5.4855
	step [61/332], loss=5.5361
	step [62/332], loss=4.6976
	step [63/332], loss=4.2373
	step [64/332], loss=4.5627
	step [65/332], loss=5.4792
	step [66/332], loss=4.7564
	step [67/332], loss=5.3552
	step [68/332], loss=7.9297
	step [69/332], loss=7.2406
	step [70/332], loss=4.6905
	step [71/332], loss=4.2510
	step [72/332], loss=6.1587
	step [73/332], loss=5.0342
	step [74/332], loss=7.4328
	step [75/332], loss=6.4166
	step [76/332], loss=6.1435
	step [77/332], loss=5.7677
	step [78/332], loss=5.8219
	step [79/332], loss=5.0079
	step [80/332], loss=4.9134
	step [81/332], loss=4.9808
	step [82/332], loss=5.4367
	step [83/332], loss=5.9979
	step [84/332], loss=6.3815
	step [85/332], loss=4.5774
	step [86/332], loss=4.5308
	step [87/332], loss=5.7443
	step [88/332], loss=4.0351
	step [89/332], loss=4.7999
	step [90/332], loss=4.7652
	step [91/332], loss=5.1745
	step [92/332], loss=5.1673
	step [93/332], loss=3.8998
	step [94/332], loss=6.2601
	step [95/332], loss=4.6508
	step [96/332], loss=6.6285
	step [97/332], loss=5.4232
	step [98/332], loss=4.5883
	step [99/332], loss=4.7970
	step [100/332], loss=6.0074
	step [101/332], loss=5.6573
	step [102/332], loss=4.2288
	step [103/332], loss=4.8184
	step [104/332], loss=5.1497
	step [105/332], loss=6.0189
	step [106/332], loss=6.5858
	step [107/332], loss=6.2912
	step [108/332], loss=6.9999
	step [109/332], loss=4.9467
	step [110/332], loss=5.6189
	step [111/332], loss=4.9092
	step [112/332], loss=6.8455
	step [113/332], loss=5.0202
	step [114/332], loss=4.9547
	step [115/332], loss=5.6749
	step [116/332], loss=5.2451
	step [117/332], loss=4.1679
	step [118/332], loss=4.6182
	step [119/332], loss=5.4898
	step [120/332], loss=5.9216
	step [121/332], loss=4.4187
	step [122/332], loss=4.8449
	step [123/332], loss=5.1904
	step [124/332], loss=4.8574
	step [125/332], loss=4.8260
	step [126/332], loss=4.5135
	step [127/332], loss=5.1345
	step [128/332], loss=6.5311
	step [129/332], loss=6.4942
	step [130/332], loss=4.8447
	step [131/332], loss=4.1387
	step [132/332], loss=5.5943
	step [133/332], loss=4.6068
	step [134/332], loss=5.5382
	step [135/332], loss=3.7376
	step [136/332], loss=4.8270
	step [137/332], loss=4.6263
	step [138/332], loss=5.7077
	step [139/332], loss=3.2069
	step [140/332], loss=4.6320
	step [141/332], loss=5.0894
	step [142/332], loss=4.0710
	step [143/332], loss=6.0800
	step [144/332], loss=5.1999
	step [145/332], loss=4.9068
	step [146/332], loss=4.7174
	step [147/332], loss=5.8256
	step [148/332], loss=5.4127
	step [149/332], loss=5.3291
	step [150/332], loss=5.6682
	step [151/332], loss=4.7238
	step [152/332], loss=4.8144
	step [153/332], loss=5.0314
	step [154/332], loss=4.5887
	step [155/332], loss=6.1330
	step [156/332], loss=4.4878
	step [157/332], loss=5.6114
	step [158/332], loss=5.3754
	step [159/332], loss=5.2352
	step [160/332], loss=5.3380
	step [161/332], loss=4.1906
	step [162/332], loss=4.7620
	step [163/332], loss=6.1089
	step [164/332], loss=5.3260
	step [165/332], loss=5.4771
	step [166/332], loss=6.0356
	step [167/332], loss=4.7348
	step [168/332], loss=4.2739
	step [169/332], loss=5.3859
	step [170/332], loss=5.2182
	step [171/332], loss=4.5210
	step [172/332], loss=4.1159
	step [173/332], loss=4.6126
	step [174/332], loss=6.2567
	step [175/332], loss=5.8433
	step [176/332], loss=4.2699
	step [177/332], loss=4.7403
	step [178/332], loss=4.6753
	step [179/332], loss=5.4976
	step [180/332], loss=5.3865
	step [181/332], loss=5.7569
	step [182/332], loss=5.3070
	step [183/332], loss=5.2886
	step [184/332], loss=5.3006
	step [185/332], loss=5.6545
	step [186/332], loss=6.1643
	step [187/332], loss=4.6925
	step [188/332], loss=5.2379
	step [189/332], loss=4.7046
	step [190/332], loss=5.1107
	step [191/332], loss=3.5937
	step [192/332], loss=4.9137
	step [193/332], loss=4.2804
	step [194/332], loss=4.3142
	step [195/332], loss=5.3781
	step [196/332], loss=4.3024
	step [197/332], loss=5.7362
	step [198/332], loss=5.5966
	step [199/332], loss=5.1700
	step [200/332], loss=3.4537
	step [201/332], loss=5.6293
	step [202/332], loss=5.0673
	step [203/332], loss=5.3118
	step [204/332], loss=4.9165
	step [205/332], loss=5.4658
	step [206/332], loss=5.2404
	step [207/332], loss=4.8045
	step [208/332], loss=3.8925
	step [209/332], loss=6.0497
	step [210/332], loss=4.9690
	step [211/332], loss=5.0000
	step [212/332], loss=5.5567
	step [213/332], loss=4.3872
	step [214/332], loss=6.4178
	step [215/332], loss=5.8773
	step [216/332], loss=4.2902
	step [217/332], loss=7.5272
	step [218/332], loss=6.0087
	step [219/332], loss=5.0479
	step [220/332], loss=6.4368
	step [221/332], loss=5.4374
	step [222/332], loss=4.3514
	step [223/332], loss=4.3179
	step [224/332], loss=4.7924
	step [225/332], loss=4.8972
	step [226/332], loss=4.6152
	step [227/332], loss=4.9990
	step [228/332], loss=4.5193
	step [229/332], loss=5.7418
	step [230/332], loss=5.1477
	step [231/332], loss=5.1918
	step [232/332], loss=6.4681
	step [233/332], loss=5.7976
	step [234/332], loss=4.5325
	step [235/332], loss=4.8142
	step [236/332], loss=4.3815
	step [237/332], loss=5.3349
	step [238/332], loss=5.7452
	step [239/332], loss=5.3469
	step [240/332], loss=6.0582
	step [241/332], loss=7.7423
	step [242/332], loss=6.2407
	step [243/332], loss=4.2102
	step [244/332], loss=5.5737
	step [245/332], loss=5.0269
	step [246/332], loss=5.4811
	step [247/332], loss=4.2217
	step [248/332], loss=5.1074
	step [249/332], loss=6.0802
	step [250/332], loss=7.0971
	step [251/332], loss=4.8280
	step [252/332], loss=4.9296
	step [253/332], loss=4.5252
	step [254/332], loss=4.7255
	step [255/332], loss=4.9617
	step [256/332], loss=6.0623
	step [257/332], loss=6.4464
	step [258/332], loss=5.8645
	step [259/332], loss=6.2325
	step [260/332], loss=6.6244
	step [261/332], loss=4.8845
	step [262/332], loss=4.8563
	step [263/332], loss=4.5446
	step [264/332], loss=5.8039
	step [265/332], loss=4.3809
	step [266/332], loss=4.6806
	step [267/332], loss=4.5178
	step [268/332], loss=5.0397
	step [269/332], loss=5.1504
	step [270/332], loss=5.8484
	step [271/332], loss=5.6062
	step [272/332], loss=4.7673
	step [273/332], loss=4.6783
	step [274/332], loss=4.1089
	step [275/332], loss=6.4348
	step [276/332], loss=4.1757
	step [277/332], loss=5.3802
	step [278/332], loss=5.6976
	step [279/332], loss=4.6823
	step [280/332], loss=5.2788
	step [281/332], loss=5.4884
	step [282/332], loss=5.9129
	step [283/332], loss=5.7431
	step [284/332], loss=5.7488
	step [285/332], loss=4.4750
	step [286/332], loss=5.6801
	step [287/332], loss=5.9704
	step [288/332], loss=6.1150
	step [289/332], loss=5.2788
	step [290/332], loss=5.2427
	step [291/332], loss=4.8781
	step [292/332], loss=6.8627
	step [293/332], loss=4.6689
	step [294/332], loss=5.7419
	step [295/332], loss=5.0293
	step [296/332], loss=5.9287
	step [297/332], loss=6.2427
	step [298/332], loss=5.8089
	step [299/332], loss=6.1191
	step [300/332], loss=4.7203
	step [301/332], loss=3.9241
	step [302/332], loss=4.8905
	step [303/332], loss=4.6162
	step [304/332], loss=5.4293
	step [305/332], loss=5.3722
	step [306/332], loss=5.1788
	step [307/332], loss=6.6388
	step [308/332], loss=4.8490
	step [309/332], loss=5.2371
	step [310/332], loss=4.3799
	step [311/332], loss=4.8358
	step [312/332], loss=4.5395
	step [313/332], loss=5.1805
	step [314/332], loss=5.4755
	step [315/332], loss=4.5778
	step [316/332], loss=5.8596
	step [317/332], loss=5.0044
	step [318/332], loss=4.8331
	step [319/332], loss=5.8343
	step [320/332], loss=5.1797
	step [321/332], loss=5.3134
	step [322/332], loss=4.9274
	step [323/332], loss=5.8016
	step [324/332], loss=4.7603
	step [325/332], loss=5.1561
	step [326/332], loss=5.3348
	step [327/332], loss=5.2628
	step [328/332], loss=4.7356
	step [329/332], loss=5.0700
	step [330/332], loss=5.3428
	step [331/332], loss=5.5250
	step [332/332], loss=3.2625
	Evaluating
	loss=0.0175, precision=0.1895, recall=0.9912, f1=0.6965
saving model as: 0_saved_model.pth
Training epoch 27
	step [1/332], loss=4.3045
	step [2/332], loss=4.1833
	step [3/332], loss=5.4841
	step [4/332], loss=3.9577
	step [5/332], loss=4.1871
	step [6/332], loss=5.0993
	step [7/332], loss=5.0278
	step [8/332], loss=3.7151
	step [9/332], loss=4.3225
	step [10/332], loss=5.8707
	step [11/332], loss=6.2794
	step [12/332], loss=5.0079
	step [13/332], loss=4.5826
	step [14/332], loss=5.6270
	step [15/332], loss=5.3594
	step [16/332], loss=3.9165
	step [17/332], loss=4.4836
	step [18/332], loss=4.4976
	step [19/332], loss=4.6664
	step [20/332], loss=4.8932
	step [21/332], loss=5.2278
	step [22/332], loss=5.9949
	step [23/332], loss=5.8250
	step [24/332], loss=5.3294
	step [25/332], loss=4.2602
	step [26/332], loss=6.2350
	step [27/332], loss=4.6113
	step [28/332], loss=5.5153
	step [29/332], loss=5.2285
	step [30/332], loss=5.0180
	step [31/332], loss=4.9591
	step [32/332], loss=6.0477
	step [33/332], loss=5.0607
	step [34/332], loss=5.0319
	step [35/332], loss=4.5163
	step [36/332], loss=4.7840
	step [37/332], loss=5.2146
	step [38/332], loss=5.3088
	step [39/332], loss=4.7190
	step [40/332], loss=5.0925
	step [41/332], loss=3.8052
	step [42/332], loss=6.9830
	step [43/332], loss=5.6900
	step [44/332], loss=4.8493
	step [45/332], loss=4.4683
	step [46/332], loss=4.5376
	step [47/332], loss=5.5007
	step [48/332], loss=4.1911
	step [49/332], loss=4.2220
	step [50/332], loss=5.5645
	step [51/332], loss=5.2169
	step [52/332], loss=5.3396
	step [53/332], loss=6.5361
	step [54/332], loss=4.9841
	step [55/332], loss=4.5381
	step [56/332], loss=4.4534
	step [57/332], loss=6.7163
	step [58/332], loss=5.3292
	step [59/332], loss=5.7359
	step [60/332], loss=4.8436
	step [61/332], loss=4.3101
	step [62/332], loss=6.8901
	step [63/332], loss=5.1849
	step [64/332], loss=5.3075
	step [65/332], loss=4.3728
	step [66/332], loss=5.5829
	step [67/332], loss=4.9667
	step [68/332], loss=4.3044
	step [69/332], loss=6.9633
	step [70/332], loss=5.6006
	step [71/332], loss=4.5785
	step [72/332], loss=4.6846
	step [73/332], loss=5.8773
	step [74/332], loss=5.3864
	step [75/332], loss=4.1216
	step [76/332], loss=4.3209
	step [77/332], loss=5.3238
	step [78/332], loss=4.9164
	step [79/332], loss=5.8946
	step [80/332], loss=5.5250
	step [81/332], loss=4.2922
	step [82/332], loss=5.4647
	step [83/332], loss=4.7245
	step [84/332], loss=4.7642
	step [85/332], loss=5.5069
	step [86/332], loss=5.6779
	step [87/332], loss=5.2398
	step [88/332], loss=4.0852
	step [89/332], loss=3.9049
	step [90/332], loss=4.4345
	step [91/332], loss=5.5634
	step [92/332], loss=5.3921
	step [93/332], loss=4.8928
	step [94/332], loss=4.7226
	step [95/332], loss=5.1204
	step [96/332], loss=5.8944
	step [97/332], loss=3.8665
	step [98/332], loss=3.7510
	step [99/332], loss=4.2071
	step [100/332], loss=4.2668
	step [101/332], loss=6.2994
	step [102/332], loss=3.9778
	step [103/332], loss=4.2728
	step [104/332], loss=3.9717
	step [105/332], loss=5.3404
	step [106/332], loss=6.5652
	step [107/332], loss=5.0300
	step [108/332], loss=4.0059
	step [109/332], loss=4.1350
	step [110/332], loss=5.5888
	step [111/332], loss=5.2857
	step [112/332], loss=6.2604
	step [113/332], loss=4.7875
	step [114/332], loss=4.6347
	step [115/332], loss=3.5528
	step [116/332], loss=5.4294
	step [117/332], loss=5.0805
	step [118/332], loss=5.8443
	step [119/332], loss=4.0883
	step [120/332], loss=4.6984
	step [121/332], loss=5.1579
	step [122/332], loss=5.7793
	step [123/332], loss=5.0863
	step [124/332], loss=5.2529
	step [125/332], loss=5.5377
	step [126/332], loss=4.9001
	step [127/332], loss=4.2051
	step [128/332], loss=5.3710
	step [129/332], loss=5.1075
	step [130/332], loss=4.1257
	step [131/332], loss=5.9802
	step [132/332], loss=5.1066
	step [133/332], loss=4.8307
	step [134/332], loss=6.0548
	step [135/332], loss=5.1477
	step [136/332], loss=5.0138
	step [137/332], loss=5.9007
	step [138/332], loss=4.8608
	step [139/332], loss=5.7387
	step [140/332], loss=4.2636
	step [141/332], loss=6.7488
	step [142/332], loss=8.2792
	step [143/332], loss=3.7648
	step [144/332], loss=5.7416
	step [145/332], loss=4.7296
	step [146/332], loss=5.9700
	step [147/332], loss=5.1431
	step [148/332], loss=4.6423
	step [149/332], loss=4.8376
	step [150/332], loss=4.2390
	step [151/332], loss=4.8406
	step [152/332], loss=4.7054
	step [153/332], loss=4.6687
	step [154/332], loss=5.3160
	step [155/332], loss=5.5540
	step [156/332], loss=4.2885
	step [157/332], loss=5.8514
	step [158/332], loss=6.1652
	step [159/332], loss=4.6107
	step [160/332], loss=5.8676
	step [161/332], loss=5.3736
	step [162/332], loss=5.7241
	step [163/332], loss=4.4970
	step [164/332], loss=5.3252
	step [165/332], loss=5.1876
	step [166/332], loss=5.2669
	step [167/332], loss=5.6512
	step [168/332], loss=4.0758
	step [169/332], loss=4.5442
	step [170/332], loss=6.7209
	step [171/332], loss=5.1810
	step [172/332], loss=5.5104
	step [173/332], loss=4.4725
	step [174/332], loss=5.0836
	step [175/332], loss=5.8207
	step [176/332], loss=5.6183
	step [177/332], loss=5.0620
	step [178/332], loss=4.3281
	step [179/332], loss=5.2066
	step [180/332], loss=5.8266
	step [181/332], loss=4.8013
	step [182/332], loss=4.6125
	step [183/332], loss=4.9987
	step [184/332], loss=6.5687
	step [185/332], loss=5.1425
	step [186/332], loss=4.7783
	step [187/332], loss=5.1914
	step [188/332], loss=4.1099
	step [189/332], loss=5.7342
	step [190/332], loss=6.4579
	step [191/332], loss=5.2178
	step [192/332], loss=9.1807
	step [193/332], loss=5.3545
	step [194/332], loss=5.9148
	step [195/332], loss=4.8832
	step [196/332], loss=6.3893
	step [197/332], loss=4.9575
	step [198/332], loss=4.5884
	step [199/332], loss=5.5501
	step [200/332], loss=4.8549
	step [201/332], loss=4.3552
	step [202/332], loss=5.5655
	step [203/332], loss=6.9487
	step [204/332], loss=4.7768
	step [205/332], loss=4.0995
	step [206/332], loss=6.3611
	step [207/332], loss=7.9051
	step [208/332], loss=6.2800
	step [209/332], loss=4.8529
	step [210/332], loss=5.3825
	step [211/332], loss=4.6908
	step [212/332], loss=3.8308
	step [213/332], loss=6.2439
	step [214/332], loss=5.7856
	step [215/332], loss=4.6546
	step [216/332], loss=4.6548
	step [217/332], loss=4.6211
	step [218/332], loss=5.0314
	step [219/332], loss=5.4333
	step [220/332], loss=4.7943
	step [221/332], loss=4.7678
	step [222/332], loss=6.4405
	step [223/332], loss=5.7115
	step [224/332], loss=5.5355
	step [225/332], loss=5.0753
	step [226/332], loss=5.0244
	step [227/332], loss=4.9048
	step [228/332], loss=5.1830
	step [229/332], loss=6.5345
	step [230/332], loss=4.5965
	step [231/332], loss=4.9575
	step [232/332], loss=5.2528
	step [233/332], loss=5.5068
	step [234/332], loss=5.3968
	step [235/332], loss=5.2315
	step [236/332], loss=4.6637
	step [237/332], loss=4.9794
	step [238/332], loss=6.3613
	step [239/332], loss=5.4855
	step [240/332], loss=5.2700
	step [241/332], loss=4.6277
	step [242/332], loss=4.9139
	step [243/332], loss=4.8090
	step [244/332], loss=7.4767
	step [245/332], loss=5.4993
	step [246/332], loss=4.2274
	step [247/332], loss=4.2282
	step [248/332], loss=5.9675
	step [249/332], loss=5.5995
	step [250/332], loss=4.5976
	step [251/332], loss=4.8142
	step [252/332], loss=4.1191
	step [253/332], loss=4.9679
	step [254/332], loss=4.6704
	step [255/332], loss=4.9845
	step [256/332], loss=6.4941
	step [257/332], loss=4.0754
	step [258/332], loss=6.5580
	step [259/332], loss=5.9531
	step [260/332], loss=6.4105
	step [261/332], loss=4.9379
	step [262/332], loss=4.5856
	step [263/332], loss=4.1805
	step [264/332], loss=6.0728
	step [265/332], loss=4.4500
	step [266/332], loss=5.1267
	step [267/332], loss=5.5982
	step [268/332], loss=4.2275
	step [269/332], loss=4.8760
	step [270/332], loss=5.3519
	step [271/332], loss=4.6648
	step [272/332], loss=5.2039
	step [273/332], loss=5.2245
	step [274/332], loss=4.1969
	step [275/332], loss=5.5128
	step [276/332], loss=5.0433
	step [277/332], loss=4.6510
	step [278/332], loss=4.8675
	step [279/332], loss=4.6089
	step [280/332], loss=4.8642
	step [281/332], loss=4.8551
	step [282/332], loss=6.4928
	step [283/332], loss=4.3048
	step [284/332], loss=4.4990
	step [285/332], loss=3.9830
	step [286/332], loss=4.4340
	step [287/332], loss=3.9203
	step [288/332], loss=5.6530
	step [289/332], loss=6.0419
	step [290/332], loss=3.9196
	step [291/332], loss=4.0972
	step [292/332], loss=5.0758
	step [293/332], loss=5.9233
	step [294/332], loss=4.4997
	step [295/332], loss=5.5264
	step [296/332], loss=4.1821
	step [297/332], loss=4.7145
	step [298/332], loss=5.5957
	step [299/332], loss=5.1412
	step [300/332], loss=4.3860
	step [301/332], loss=4.8358
	step [302/332], loss=4.3901
	step [303/332], loss=6.0322
	step [304/332], loss=4.4739
	step [305/332], loss=5.8564
	step [306/332], loss=4.6036
	step [307/332], loss=5.0235
	step [308/332], loss=5.7573
	step [309/332], loss=5.0436
	step [310/332], loss=4.8522
	step [311/332], loss=4.9057
	step [312/332], loss=6.2093
	step [313/332], loss=5.3480
	step [314/332], loss=4.4844
	step [315/332], loss=4.7779
	step [316/332], loss=4.8178
	step [317/332], loss=5.1746
	step [318/332], loss=4.7143
	step [319/332], loss=5.5965
	step [320/332], loss=5.0185
	step [321/332], loss=3.9183
	step [322/332], loss=5.7026
	step [323/332], loss=5.5003
	step [324/332], loss=5.7938
	step [325/332], loss=5.0425
	step [326/332], loss=3.9800
	step [327/332], loss=5.9841
	step [328/332], loss=5.0618
	step [329/332], loss=6.0116
	step [330/332], loss=5.7968
	step [331/332], loss=4.6537
	step [332/332], loss=3.9929
	Evaluating
	loss=0.0183, precision=0.1796, recall=0.9919, f1=0.6830
Training epoch 28
	step [1/332], loss=4.8595
	step [2/332], loss=5.5795
	step [3/332], loss=6.2014
	step [4/332], loss=5.1964
	step [5/332], loss=5.5134
	step [6/332], loss=4.8299
	step [7/332], loss=3.7460
	step [8/332], loss=7.7959
	step [9/332], loss=4.1622
	step [10/332], loss=5.8696
	step [11/332], loss=4.5545
	step [12/332], loss=4.5901
	step [13/332], loss=4.2797
	step [14/332], loss=5.2527
	step [15/332], loss=4.9862
	step [16/332], loss=4.1096
	step [17/332], loss=5.0102
	step [18/332], loss=4.7673
	step [19/332], loss=4.8113
	step [20/332], loss=3.9463
	step [21/332], loss=5.4817
	step [22/332], loss=3.9377
	step [23/332], loss=5.2192
	step [24/332], loss=4.3039
	step [25/332], loss=3.7370
	step [26/332], loss=6.1144
	step [27/332], loss=5.1563
	step [28/332], loss=4.4977
	step [29/332], loss=5.1022
	step [30/332], loss=4.8673
	step [31/332], loss=4.6092
	step [32/332], loss=5.6145
	step [33/332], loss=5.2151
	step [34/332], loss=4.6379
	step [35/332], loss=3.8524
	step [36/332], loss=4.8781
	step [37/332], loss=5.1371
	step [38/332], loss=6.0158
	step [39/332], loss=4.3864
	step [40/332], loss=5.0251
	step [41/332], loss=5.6271
	step [42/332], loss=4.8635
	step [43/332], loss=3.8912
	step [44/332], loss=5.4542
	step [45/332], loss=5.1266
	step [46/332], loss=5.0037
	step [47/332], loss=4.3864
	step [48/332], loss=5.7133
	step [49/332], loss=4.9338
	step [50/332], loss=3.3086
	step [51/332], loss=4.0185
	step [52/332], loss=4.0796
	step [53/332], loss=5.0751
	step [54/332], loss=4.5269
	step [55/332], loss=4.1337
	step [56/332], loss=5.2781
	step [57/332], loss=5.2196
	step [58/332], loss=4.8621
	step [59/332], loss=5.5397
	step [60/332], loss=5.0049
	step [61/332], loss=4.5737
	step [62/332], loss=4.2117
	step [63/332], loss=4.8018
	step [64/332], loss=4.9120
	step [65/332], loss=3.1928
	step [66/332], loss=5.2133
	step [67/332], loss=3.9965
	step [68/332], loss=4.1996
	step [69/332], loss=5.3383
	step [70/332], loss=4.5020
	step [71/332], loss=4.5238
	step [72/332], loss=4.7124
	step [73/332], loss=6.7363
	step [74/332], loss=5.0911
	step [75/332], loss=4.2750
	step [76/332], loss=5.4055
	step [77/332], loss=3.6868
	step [78/332], loss=4.7943
	step [79/332], loss=5.8797
	step [80/332], loss=5.1089
	step [81/332], loss=6.0232
	step [82/332], loss=4.6042
	step [83/332], loss=5.5167
	step [84/332], loss=4.2787
	step [85/332], loss=6.4007
	step [86/332], loss=4.6922
	step [87/332], loss=4.5283
	step [88/332], loss=5.4066
	step [89/332], loss=5.5642
	step [90/332], loss=4.6715
	step [91/332], loss=4.6548
	step [92/332], loss=6.6648
	step [93/332], loss=5.4951
	step [94/332], loss=5.6209
	step [95/332], loss=4.5526
	step [96/332], loss=4.8565
	step [97/332], loss=4.6629
	step [98/332], loss=3.9492
	step [99/332], loss=4.7470
	step [100/332], loss=5.7721
	step [101/332], loss=4.7058
	step [102/332], loss=4.1680
	step [103/332], loss=5.5574
	step [104/332], loss=6.2120
	step [105/332], loss=5.6161
	step [106/332], loss=4.9205
	step [107/332], loss=3.8723
	step [108/332], loss=6.3744
	step [109/332], loss=6.0923
	step [110/332], loss=5.9889
	step [111/332], loss=4.6560
	step [112/332], loss=3.4056
	step [113/332], loss=6.6925
	step [114/332], loss=4.5866
	step [115/332], loss=4.1663
	step [116/332], loss=4.7761
	step [117/332], loss=6.5021
	step [118/332], loss=5.2834
	step [119/332], loss=5.2778
	step [120/332], loss=5.1426
	step [121/332], loss=4.2513
	step [122/332], loss=4.8165
	step [123/332], loss=6.8484
	step [124/332], loss=5.9243
	step [125/332], loss=4.4755
	step [126/332], loss=3.8044
	step [127/332], loss=5.8731
	step [128/332], loss=6.0430
	step [129/332], loss=5.7893
	step [130/332], loss=4.5573
	step [131/332], loss=4.2150
	step [132/332], loss=4.0500
	step [133/332], loss=4.0352
	step [134/332], loss=4.3369
	step [135/332], loss=4.2714
	step [136/332], loss=5.0121
	step [137/332], loss=4.3772
	step [138/332], loss=5.2677
	step [139/332], loss=5.0352
	step [140/332], loss=6.2604
	step [141/332], loss=4.3569
	step [142/332], loss=5.0320
	step [143/332], loss=4.5209
	step [144/332], loss=4.8081
	step [145/332], loss=5.2089
	step [146/332], loss=4.9415
	step [147/332], loss=4.9112
	step [148/332], loss=4.5445
	step [149/332], loss=5.4809
	step [150/332], loss=5.7892
	step [151/332], loss=5.1253
	step [152/332], loss=4.5077
	step [153/332], loss=4.4128
	step [154/332], loss=5.1994
	step [155/332], loss=4.1899
	step [156/332], loss=5.6476
	step [157/332], loss=5.0174
	step [158/332], loss=6.1521
	step [159/332], loss=6.8643
	step [160/332], loss=6.0930
	step [161/332], loss=3.7947
	step [162/332], loss=6.7807
	step [163/332], loss=5.1075
	step [164/332], loss=5.5838
	step [165/332], loss=5.5537
	step [166/332], loss=6.1977
	step [167/332], loss=5.4570
	step [168/332], loss=5.0586
	step [169/332], loss=4.8834
	step [170/332], loss=4.9753
	step [171/332], loss=5.6896
	step [172/332], loss=4.9483
	step [173/332], loss=6.2366
	step [174/332], loss=4.3726
	step [175/332], loss=4.7466
	step [176/332], loss=4.3361
	step [177/332], loss=5.3440
	step [178/332], loss=5.1374
	step [179/332], loss=4.8481
	step [180/332], loss=6.1668
	step [181/332], loss=5.4641
	step [182/332], loss=4.6766
	step [183/332], loss=4.0406
	step [184/332], loss=5.4286
	step [185/332], loss=5.6198
	step [186/332], loss=6.1620
	step [187/332], loss=5.2332
	step [188/332], loss=5.1589
	step [189/332], loss=4.6672
	step [190/332], loss=5.3319
	step [191/332], loss=5.0309
	step [192/332], loss=4.7553
	step [193/332], loss=5.3587
	step [194/332], loss=5.9351
	step [195/332], loss=4.1027
	step [196/332], loss=4.9288
	step [197/332], loss=6.0201
	step [198/332], loss=5.1941
	step [199/332], loss=6.6805
	step [200/332], loss=4.6617
	step [201/332], loss=4.9307
	step [202/332], loss=4.9343
	step [203/332], loss=4.7468
	step [204/332], loss=6.6293
	step [205/332], loss=4.9308
	step [206/332], loss=5.8647
	step [207/332], loss=4.4512
	step [208/332], loss=4.3490
	step [209/332], loss=5.4238
	step [210/332], loss=4.3331
	step [211/332], loss=5.5035
	step [212/332], loss=4.0147
	step [213/332], loss=5.0750
	step [214/332], loss=5.5824
	step [215/332], loss=5.1421
	step [216/332], loss=3.8140
	step [217/332], loss=6.1510
	step [218/332], loss=4.2487
	step [219/332], loss=5.2823
	step [220/332], loss=5.2204
	step [221/332], loss=5.0016
	step [222/332], loss=3.6836
	step [223/332], loss=5.3962
	step [224/332], loss=6.3719
	step [225/332], loss=4.4195
	step [226/332], loss=6.6371
	step [227/332], loss=4.3118
	step [228/332], loss=4.2091
	step [229/332], loss=4.9138
	step [230/332], loss=4.2759
	step [231/332], loss=4.6978
	step [232/332], loss=4.4035
	step [233/332], loss=5.1003
	step [234/332], loss=4.2794
	step [235/332], loss=5.0675
	step [236/332], loss=3.3939
	step [237/332], loss=3.9911
	step [238/332], loss=4.2487
	step [239/332], loss=3.4500
	step [240/332], loss=6.4585
	step [241/332], loss=7.0652
	step [242/332], loss=6.0797
	step [243/332], loss=6.8241
	step [244/332], loss=4.0757
	step [245/332], loss=4.2952
	step [246/332], loss=4.7740
	step [247/332], loss=5.0159
	step [248/332], loss=5.3571
	step [249/332], loss=3.8538
	step [250/332], loss=5.0282
	step [251/332], loss=5.5658
	step [252/332], loss=4.5188
	step [253/332], loss=4.6679
	step [254/332], loss=4.6153
	step [255/332], loss=4.4332
	step [256/332], loss=4.7421
	step [257/332], loss=5.6354
	step [258/332], loss=5.7990
	step [259/332], loss=5.5584
	step [260/332], loss=4.4053
	step [261/332], loss=5.5160
	step [262/332], loss=5.6748
	step [263/332], loss=3.3744
	step [264/332], loss=5.0095
	step [265/332], loss=4.7742
	step [266/332], loss=4.4526
	step [267/332], loss=3.2336
	step [268/332], loss=5.7172
	step [269/332], loss=5.0615
	step [270/332], loss=6.2241
	step [271/332], loss=4.5110
	step [272/332], loss=4.1519
	step [273/332], loss=4.9212
	step [274/332], loss=5.1739
	step [275/332], loss=5.1838
	step [276/332], loss=5.1220
	step [277/332], loss=5.9077
	step [278/332], loss=5.3308
	step [279/332], loss=5.1325
	step [280/332], loss=5.2536
	step [281/332], loss=4.2872
	step [282/332], loss=4.9981
	step [283/332], loss=4.9419
	step [284/332], loss=5.8969
	step [285/332], loss=5.4292
	step [286/332], loss=4.9007
	step [287/332], loss=4.5576
	step [288/332], loss=5.6281
	step [289/332], loss=5.5471
	step [290/332], loss=3.8747
	step [291/332], loss=6.3376
	step [292/332], loss=5.4534
	step [293/332], loss=5.7655
	step [294/332], loss=5.8654
	step [295/332], loss=4.3708
	step [296/332], loss=4.9375
	step [297/332], loss=5.0262
	step [298/332], loss=4.3504
	step [299/332], loss=4.6350
	step [300/332], loss=4.8035
	step [301/332], loss=5.1807
	step [302/332], loss=5.3904
	step [303/332], loss=5.8381
	step [304/332], loss=3.9601
	step [305/332], loss=5.5860
	step [306/332], loss=6.0621
	step [307/332], loss=5.1149
	step [308/332], loss=4.7205
	step [309/332], loss=6.0434
	step [310/332], loss=4.4646
	step [311/332], loss=4.5768
	step [312/332], loss=4.5283
	step [313/332], loss=6.8171
	step [314/332], loss=5.6539
	step [315/332], loss=4.4188
	step [316/332], loss=4.0018
	step [317/332], loss=3.8080
	step [318/332], loss=5.3940
	step [319/332], loss=5.6893
	step [320/332], loss=5.1602
	step [321/332], loss=3.5803
	step [322/332], loss=5.6039
	step [323/332], loss=4.1758
	step [324/332], loss=6.0652
	step [325/332], loss=5.4871
	step [326/332], loss=5.0689
	step [327/332], loss=4.8374
	step [328/332], loss=5.7809
	step [329/332], loss=4.5676
	step [330/332], loss=4.4994
	step [331/332], loss=4.7601
	step [332/332], loss=3.6812
	Evaluating
	loss=0.0167, precision=0.2000, recall=0.9916, f1=0.7104
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/332], loss=5.1652
	step [2/332], loss=6.3950
	step [3/332], loss=6.8250
	step [4/332], loss=4.9392
	step [5/332], loss=5.1274
	step [6/332], loss=4.7231
	step [7/332], loss=4.6588
	step [8/332], loss=3.8382
	step [9/332], loss=4.6199
	step [10/332], loss=3.6360
	step [11/332], loss=5.0319
	step [12/332], loss=4.9900
	step [13/332], loss=5.6248
	step [14/332], loss=5.7720
	step [15/332], loss=4.5985
	step [16/332], loss=4.2580
	step [17/332], loss=4.6045
	step [18/332], loss=5.3886
	step [19/332], loss=6.2131
	step [20/332], loss=5.4710
	step [21/332], loss=4.6325
	step [22/332], loss=5.1232
	step [23/332], loss=5.4306
	step [24/332], loss=6.5114
	step [25/332], loss=4.9935
	step [26/332], loss=4.2082
	step [27/332], loss=4.2383
	step [28/332], loss=5.2040
	step [29/332], loss=5.3430
	step [30/332], loss=4.2325
	step [31/332], loss=5.3505
	step [32/332], loss=4.5827
	step [33/332], loss=4.0707
	step [34/332], loss=4.3277
	step [35/332], loss=4.2435
	step [36/332], loss=4.9680
	step [37/332], loss=4.0247
	step [38/332], loss=4.9856
	step [39/332], loss=4.0566
	step [40/332], loss=5.1677
	step [41/332], loss=4.5419
	step [42/332], loss=5.2422
	step [43/332], loss=3.4888
	step [44/332], loss=3.9475
	step [45/332], loss=4.0891
	step [46/332], loss=3.7751
	step [47/332], loss=5.2833
	step [48/332], loss=4.2680
	step [49/332], loss=3.6909
	step [50/332], loss=4.0808
	step [51/332], loss=4.4752
	step [52/332], loss=5.1528
	step [53/332], loss=4.4622
	step [54/332], loss=4.2550
	step [55/332], loss=4.5434
	step [56/332], loss=5.7867
	step [57/332], loss=4.6466
	step [58/332], loss=4.6458
	step [59/332], loss=5.9005
	step [60/332], loss=5.2666
	step [61/332], loss=5.6010
	step [62/332], loss=3.7576
	step [63/332], loss=4.4784
	step [64/332], loss=3.4649
	step [65/332], loss=5.5296
	step [66/332], loss=4.9157
	step [67/332], loss=5.2141
	step [68/332], loss=4.9048
	step [69/332], loss=4.2120
	step [70/332], loss=3.7400
	step [71/332], loss=5.0358
	step [72/332], loss=5.0630
	step [73/332], loss=5.1325
	step [74/332], loss=4.7487
	step [75/332], loss=3.7469
	step [76/332], loss=5.6306
	step [77/332], loss=4.7546
	step [78/332], loss=3.9832
	step [79/332], loss=5.3853
	step [80/332], loss=4.2433
	step [81/332], loss=4.9224
	step [82/332], loss=4.1971
	step [83/332], loss=4.9677
	step [84/332], loss=4.4427
	step [85/332], loss=5.1713
	step [86/332], loss=5.9007
	step [87/332], loss=4.0709
	step [88/332], loss=6.8397
	step [89/332], loss=5.1313
	step [90/332], loss=4.3774
	step [91/332], loss=5.5099
	step [92/332], loss=4.5604
	step [93/332], loss=3.9221
	step [94/332], loss=4.7565
	step [95/332], loss=4.3123
	step [96/332], loss=4.0654
	step [97/332], loss=3.6384
	step [98/332], loss=4.8496
	step [99/332], loss=5.4668
	step [100/332], loss=3.9250
	step [101/332], loss=4.9255
	step [102/332], loss=5.9272
	step [103/332], loss=4.9749
	step [104/332], loss=5.4871
	step [105/332], loss=4.7483
	step [106/332], loss=5.1786
	step [107/332], loss=5.0790
	step [108/332], loss=5.2779
	step [109/332], loss=4.8170
	step [110/332], loss=4.2379
	step [111/332], loss=4.8276
	step [112/332], loss=3.6765
	step [113/332], loss=5.3971
	step [114/332], loss=4.9349
	step [115/332], loss=3.9822
	step [116/332], loss=4.4509
	step [117/332], loss=4.5423
	step [118/332], loss=4.0307
	step [119/332], loss=5.0755
	step [120/332], loss=4.6884
	step [121/332], loss=4.2613
	step [122/332], loss=4.2752
	step [123/332], loss=5.3158
	step [124/332], loss=5.1294
	step [125/332], loss=5.1867
	step [126/332], loss=5.6146
	step [127/332], loss=4.2448
	step [128/332], loss=4.4742
	step [129/332], loss=4.5559
	step [130/332], loss=4.7587
	step [131/332], loss=5.6702
	step [132/332], loss=4.9356
	step [133/332], loss=5.0095
	step [134/332], loss=4.3928
	step [135/332], loss=5.4365
	step [136/332], loss=4.9510
	step [137/332], loss=6.0479
	step [138/332], loss=4.1523
	step [139/332], loss=4.0543
	step [140/332], loss=3.7031
	step [141/332], loss=5.9179
	step [142/332], loss=5.4412
	step [143/332], loss=4.9710
	step [144/332], loss=5.5186
	step [145/332], loss=3.3618
	step [146/332], loss=3.8796
	step [147/332], loss=5.5424
	step [148/332], loss=4.8808
	step [149/332], loss=7.5951
	step [150/332], loss=4.0937
	step [151/332], loss=6.2128
	step [152/332], loss=5.6639
	step [153/332], loss=3.8976
	step [154/332], loss=5.3528
	step [155/332], loss=4.9682
	step [156/332], loss=5.8687
	step [157/332], loss=5.1256
	step [158/332], loss=4.8655
	step [159/332], loss=4.9068
	step [160/332], loss=4.3628
	step [161/332], loss=5.2641
	step [162/332], loss=5.7611
	step [163/332], loss=3.9606
	step [164/332], loss=4.2363
	step [165/332], loss=3.8168
	step [166/332], loss=4.1690
	step [167/332], loss=4.6926
	step [168/332], loss=4.9059
	step [169/332], loss=4.2733
	step [170/332], loss=4.6725
	step [171/332], loss=4.5633
	step [172/332], loss=4.2546
	step [173/332], loss=5.7493
	step [174/332], loss=3.4899
	step [175/332], loss=3.7804
	step [176/332], loss=4.3214
	step [177/332], loss=4.9040
	step [178/332], loss=4.4988
	step [179/332], loss=5.6228
	step [180/332], loss=5.8881
	step [181/332], loss=5.6181
	step [182/332], loss=4.7618
	step [183/332], loss=4.9486
	step [184/332], loss=4.9892
	step [185/332], loss=4.1806
	step [186/332], loss=4.9400
	step [187/332], loss=5.6387
	step [188/332], loss=3.9011
	step [189/332], loss=3.7408
	step [190/332], loss=5.1918
	step [191/332], loss=3.6991
	step [192/332], loss=4.8314
	step [193/332], loss=5.4800
	step [194/332], loss=4.0560
	step [195/332], loss=5.8923
	step [196/332], loss=4.8014
	step [197/332], loss=4.1810
	step [198/332], loss=4.0189
	step [199/332], loss=3.8993
	step [200/332], loss=4.2753
	step [201/332], loss=4.7412
	step [202/332], loss=4.6909
	step [203/332], loss=4.9021
	step [204/332], loss=4.6627
	step [205/332], loss=4.4196
	step [206/332], loss=4.2757
	step [207/332], loss=6.1844
	step [208/332], loss=4.1578
	step [209/332], loss=5.2489
	step [210/332], loss=4.6739
	step [211/332], loss=5.8661
	step [212/332], loss=4.6719
	step [213/332], loss=5.6960
	step [214/332], loss=4.4432
	step [215/332], loss=5.8010
	step [216/332], loss=5.4254
	step [217/332], loss=4.6679
	step [218/332], loss=4.5934
	step [219/332], loss=4.2082
	step [220/332], loss=3.4717
	step [221/332], loss=4.0594
	step [222/332], loss=4.4511
	step [223/332], loss=3.5565
	step [224/332], loss=4.2868
	step [225/332], loss=5.6041
	step [226/332], loss=5.4155
	step [227/332], loss=4.3694
	step [228/332], loss=4.4427
	step [229/332], loss=5.5563
	step [230/332], loss=5.8607
	step [231/332], loss=4.8869
	step [232/332], loss=5.7724
	step [233/332], loss=5.4472
	step [234/332], loss=4.2995
	step [235/332], loss=4.9236
	step [236/332], loss=4.8841
	step [237/332], loss=7.2926
	step [238/332], loss=4.3976
	step [239/332], loss=4.5075
	step [240/332], loss=4.3511
	step [241/332], loss=4.7012
	step [242/332], loss=4.4267
	step [243/332], loss=5.3737
	step [244/332], loss=5.2534
	step [245/332], loss=4.5648
	step [246/332], loss=4.8686
	step [247/332], loss=4.8971
	step [248/332], loss=5.3163
	step [249/332], loss=4.3818
	step [250/332], loss=5.3130
	step [251/332], loss=6.2222
	step [252/332], loss=5.0649
	step [253/332], loss=4.7467
	step [254/332], loss=5.6738
	step [255/332], loss=5.0405
	step [256/332], loss=5.2861
	step [257/332], loss=4.0564
	step [258/332], loss=4.3106
	step [259/332], loss=4.4060
	step [260/332], loss=4.8303
	step [261/332], loss=3.9025
	step [262/332], loss=4.8497
	step [263/332], loss=4.3981
	step [264/332], loss=4.8392
	step [265/332], loss=4.0433
	step [266/332], loss=4.0826
	step [267/332], loss=5.0120
	step [268/332], loss=4.5375
	step [269/332], loss=4.9715
	step [270/332], loss=4.6529
	step [271/332], loss=4.2802
	step [272/332], loss=4.3722
	step [273/332], loss=3.7368
	step [274/332], loss=5.4994
	step [275/332], loss=6.0294
	step [276/332], loss=3.9542
	step [277/332], loss=4.0269
	step [278/332], loss=6.6710
	step [279/332], loss=3.7331
	step [280/332], loss=4.0438
	step [281/332], loss=5.0546
	step [282/332], loss=4.4546
	step [283/332], loss=4.9406
	step [284/332], loss=4.7246
	step [285/332], loss=6.4687
	step [286/332], loss=3.7909
	step [287/332], loss=3.8347
	step [288/332], loss=3.8332
	step [289/332], loss=4.5316
	step [290/332], loss=4.6496
	step [291/332], loss=6.0244
	step [292/332], loss=5.7922
	step [293/332], loss=4.0124
	step [294/332], loss=4.5945
	step [295/332], loss=5.2910
	step [296/332], loss=3.9283
	step [297/332], loss=5.2702
	step [298/332], loss=4.3779
	step [299/332], loss=5.1931
	step [300/332], loss=5.5960
	step [301/332], loss=4.8131
	step [302/332], loss=4.8487
	step [303/332], loss=4.9800
	step [304/332], loss=4.7336
	step [305/332], loss=6.4108
	step [306/332], loss=3.8601
	step [307/332], loss=3.9463
	step [308/332], loss=3.7349
	step [309/332], loss=4.6515
	step [310/332], loss=4.7566
	step [311/332], loss=4.5291
	step [312/332], loss=3.4849
	step [313/332], loss=5.3180
	step [314/332], loss=6.0445
	step [315/332], loss=5.1411
	step [316/332], loss=4.6675
	step [317/332], loss=4.5573
	step [318/332], loss=4.9917
	step [319/332], loss=5.1313
	step [320/332], loss=4.4915
	step [321/332], loss=4.8290
	step [322/332], loss=5.4116
	step [323/332], loss=5.6620
	step [324/332], loss=4.5181
	step [325/332], loss=5.4161
	step [326/332], loss=5.4686
	step [327/332], loss=5.8732
	step [328/332], loss=4.8181
	step [329/332], loss=4.9954
	step [330/332], loss=4.4355
	step [331/332], loss=6.0898
	step [332/332], loss=3.9141
	Evaluating
	loss=0.0169, precision=0.1979, recall=0.9914, f1=0.7076
Training epoch 30
	step [1/332], loss=5.2142
	step [2/332], loss=4.2168
	step [3/332], loss=4.5231
	step [4/332], loss=4.5280
	step [5/332], loss=5.3400
	step [6/332], loss=4.9751
	step [7/332], loss=4.5239
	step [8/332], loss=4.8574
	step [9/332], loss=4.3464
	step [10/332], loss=4.9838
	step [11/332], loss=5.8345
	step [12/332], loss=5.2973
	step [13/332], loss=5.3372
	step [14/332], loss=4.4186
	step [15/332], loss=4.4703
	step [16/332], loss=3.9355
	step [17/332], loss=5.9827
	step [18/332], loss=4.4756
	step [19/332], loss=5.4608
	step [20/332], loss=4.7623
	step [21/332], loss=4.4592
	step [22/332], loss=5.1199
	step [23/332], loss=5.5220
	step [24/332], loss=4.5277
	step [25/332], loss=5.4179
	step [26/332], loss=5.5601
	step [27/332], loss=5.8196
	step [28/332], loss=3.9790
	step [29/332], loss=3.4548
	step [30/332], loss=4.0402
	step [31/332], loss=3.8844
	step [32/332], loss=5.6718
	step [33/332], loss=5.7627
	step [34/332], loss=4.7466
	step [35/332], loss=4.6972
	step [36/332], loss=3.7074
	step [37/332], loss=5.0872
	step [38/332], loss=5.5258
	step [39/332], loss=5.9964
	step [40/332], loss=3.9990
	step [41/332], loss=4.6162
	step [42/332], loss=4.8870
	step [43/332], loss=4.8968
	step [44/332], loss=6.1074
	step [45/332], loss=4.8168
	step [46/332], loss=5.5475
	step [47/332], loss=3.7216
	step [48/332], loss=4.3537
	step [49/332], loss=5.8334
	step [50/332], loss=4.1201
	step [51/332], loss=5.3086
	step [52/332], loss=4.6753
	step [53/332], loss=5.9501
	step [54/332], loss=3.9966
	step [55/332], loss=3.9639
	step [56/332], loss=4.3405
	step [57/332], loss=4.4397
	step [58/332], loss=3.8358
	step [59/332], loss=5.6340
	step [60/332], loss=4.9469
	step [61/332], loss=4.0622
	step [62/332], loss=5.1940
	step [63/332], loss=5.1758
	step [64/332], loss=3.5811
	step [65/332], loss=3.8840
	step [66/332], loss=5.2652
	step [67/332], loss=5.3563
	step [68/332], loss=4.7874
	step [69/332], loss=4.9186
	step [70/332], loss=4.3085
	step [71/332], loss=4.9842
	step [72/332], loss=4.6172
	step [73/332], loss=4.9269
	step [74/332], loss=5.7632
	step [75/332], loss=5.3296
	step [76/332], loss=4.7058
	step [77/332], loss=4.1355
	step [78/332], loss=5.6168
	step [79/332], loss=5.0699
	step [80/332], loss=5.7289
	step [81/332], loss=5.5228
	step [82/332], loss=4.4058
	step [83/332], loss=3.9226
	step [84/332], loss=4.1877
	step [85/332], loss=4.8946
	step [86/332], loss=4.8300
	step [87/332], loss=5.8298
	step [88/332], loss=4.6609
	step [89/332], loss=4.2616
	step [90/332], loss=4.2725
	step [91/332], loss=5.4420
	step [92/332], loss=4.9365
	step [93/332], loss=4.5756
	step [94/332], loss=4.0057
	step [95/332], loss=4.8822
	step [96/332], loss=5.7211
	step [97/332], loss=4.1993
	step [98/332], loss=4.7022
	step [99/332], loss=4.7784
	step [100/332], loss=5.7248
	step [101/332], loss=5.2625
	step [102/332], loss=5.6509
	step [103/332], loss=5.3563
	step [104/332], loss=5.5868
	step [105/332], loss=4.3554
	step [106/332], loss=3.8465
	step [107/332], loss=4.3040
	step [108/332], loss=4.7772
	step [109/332], loss=4.5622
	step [110/332], loss=4.8085
	step [111/332], loss=6.9891
	step [112/332], loss=5.2444
	step [113/332], loss=4.3370
	step [114/332], loss=4.9737
	step [115/332], loss=4.8900
	step [116/332], loss=4.8266
	step [117/332], loss=5.0608
	step [118/332], loss=4.2960
	step [119/332], loss=4.9914
	step [120/332], loss=4.5116
	step [121/332], loss=4.6197
	step [122/332], loss=6.3724
	step [123/332], loss=3.9499
	step [124/332], loss=5.7458
	step [125/332], loss=6.0904
	step [126/332], loss=5.1919
	step [127/332], loss=4.4156
	step [128/332], loss=5.5587
	step [129/332], loss=5.8124
	step [130/332], loss=5.7229
	step [131/332], loss=5.7149
	step [132/332], loss=3.7908
	step [133/332], loss=3.9712
	step [134/332], loss=5.1340
	step [135/332], loss=5.0956
	step [136/332], loss=4.0427
	step [137/332], loss=4.0029
	step [138/332], loss=4.1395
	step [139/332], loss=4.2104
	step [140/332], loss=4.2226
	step [141/332], loss=4.5078
	step [142/332], loss=4.9807
	step [143/332], loss=3.3688
	step [144/332], loss=4.6851
	step [145/332], loss=4.7472
	step [146/332], loss=3.5556
	step [147/332], loss=4.9603
	step [148/332], loss=4.7103
	step [149/332], loss=4.4420
	step [150/332], loss=3.8284
	step [151/332], loss=6.1832
	step [152/332], loss=4.3258
	step [153/332], loss=6.4175
	step [154/332], loss=4.6388
	step [155/332], loss=3.8727
	step [156/332], loss=4.1212
	step [157/332], loss=4.6014
	step [158/332], loss=3.4345
	step [159/332], loss=3.9337
	step [160/332], loss=5.5532
	step [161/332], loss=4.9806
	step [162/332], loss=4.7493
	step [163/332], loss=4.3930
	step [164/332], loss=4.4438
	step [165/332], loss=4.8861
	step [166/332], loss=5.6694
	step [167/332], loss=5.6406
	step [168/332], loss=4.6406
	step [169/332], loss=4.4698
	step [170/332], loss=4.5536
	step [171/332], loss=4.7459
	step [172/332], loss=4.1972
	step [173/332], loss=3.8486
	step [174/332], loss=5.1584
	step [175/332], loss=4.4699
	step [176/332], loss=3.9401
	step [177/332], loss=4.1146
	step [178/332], loss=4.7751
	step [179/332], loss=5.1480
	step [180/332], loss=4.4007
	step [181/332], loss=4.0797
	step [182/332], loss=4.4288
	step [183/332], loss=4.0399
	step [184/332], loss=4.9835
	step [185/332], loss=5.3778
	step [186/332], loss=4.1476
	step [187/332], loss=5.7924
	step [188/332], loss=4.2088
	step [189/332], loss=5.0008
	step [190/332], loss=4.6075
	step [191/332], loss=5.5491
	step [192/332], loss=3.4646
	step [193/332], loss=4.0944
	step [194/332], loss=4.0658
	step [195/332], loss=4.1770
	step [196/332], loss=5.1287
	step [197/332], loss=3.4361
	step [198/332], loss=4.4374
	step [199/332], loss=4.3633
	step [200/332], loss=4.9257
	step [201/332], loss=5.0566
	step [202/332], loss=5.7703
	step [203/332], loss=4.4622
	step [204/332], loss=5.0587
	step [205/332], loss=4.6553
	step [206/332], loss=4.0415
	step [207/332], loss=4.5070
	step [208/332], loss=4.1335
	step [209/332], loss=6.4494
	step [210/332], loss=3.5530
	step [211/332], loss=5.3214
	step [212/332], loss=5.1531
	step [213/332], loss=5.7269
	step [214/332], loss=5.3907
	step [215/332], loss=5.0227
	step [216/332], loss=4.8626
	step [217/332], loss=4.6431
	step [218/332], loss=4.4840
	step [219/332], loss=3.6833
	step [220/332], loss=4.0563
	step [221/332], loss=4.1792
	step [222/332], loss=4.2204
	step [223/332], loss=4.3736
	step [224/332], loss=3.4442
	step [225/332], loss=4.4662
	step [226/332], loss=4.9002
	step [227/332], loss=4.6308
	step [228/332], loss=4.8508
	step [229/332], loss=4.9457
	step [230/332], loss=4.8240
	step [231/332], loss=4.0592
	step [232/332], loss=4.9197
	step [233/332], loss=4.8627
	step [234/332], loss=5.3872
	step [235/332], loss=3.7398
	step [236/332], loss=4.2647
	step [237/332], loss=4.1582
	step [238/332], loss=3.7875
	step [239/332], loss=4.8357
	step [240/332], loss=5.5693
	step [241/332], loss=5.4140
	step [242/332], loss=4.2886
	step [243/332], loss=3.9304
	step [244/332], loss=4.7228
	step [245/332], loss=3.8325
	step [246/332], loss=5.0552
	step [247/332], loss=4.2069
	step [248/332], loss=4.0525
	step [249/332], loss=4.5787
	step [250/332], loss=5.7543
	step [251/332], loss=4.0377
	step [252/332], loss=4.8374
	step [253/332], loss=5.1175
	step [254/332], loss=4.1904
	step [255/332], loss=5.9447
	step [256/332], loss=4.5264
	step [257/332], loss=4.2988
	step [258/332], loss=3.8346
	step [259/332], loss=5.2409
	step [260/332], loss=5.4664
	step [261/332], loss=5.4108
	step [262/332], loss=4.4256
	step [263/332], loss=4.9118
	step [264/332], loss=5.2329
	step [265/332], loss=4.8356
	step [266/332], loss=4.4731
	step [267/332], loss=4.3919
	step [268/332], loss=5.9986
	step [269/332], loss=4.3111
	step [270/332], loss=5.2987
	step [271/332], loss=4.5492
	step [272/332], loss=3.6257
	step [273/332], loss=4.9078
	step [274/332], loss=3.6773
	step [275/332], loss=5.0429
	step [276/332], loss=4.2443
	step [277/332], loss=5.0357
	step [278/332], loss=5.5351
	step [279/332], loss=6.3037
	step [280/332], loss=3.4982
	step [281/332], loss=4.3833
	step [282/332], loss=4.7164
	step [283/332], loss=3.6523
	step [284/332], loss=4.0263
	step [285/332], loss=5.0295
	step [286/332], loss=4.3602
	step [287/332], loss=4.7702
	step [288/332], loss=3.5289
	step [289/332], loss=4.1447
	step [290/332], loss=6.8550
	step [291/332], loss=4.8158
	step [292/332], loss=4.6526
	step [293/332], loss=4.5214
	step [294/332], loss=3.8210
	step [295/332], loss=7.5859
	step [296/332], loss=5.3084
	step [297/332], loss=5.7598
	step [298/332], loss=3.8101
	step [299/332], loss=5.4035
	step [300/332], loss=4.1355
	step [301/332], loss=5.0669
	step [302/332], loss=5.9489
	step [303/332], loss=3.9186
	step [304/332], loss=4.5480
	step [305/332], loss=4.8846
	step [306/332], loss=4.4871
	step [307/332], loss=4.4873
	step [308/332], loss=4.5681
	step [309/332], loss=3.8209
	step [310/332], loss=4.7576
	step [311/332], loss=4.6244
	step [312/332], loss=4.0825
	step [313/332], loss=5.1506
	step [314/332], loss=4.1716
	step [315/332], loss=5.2024
	step [316/332], loss=4.6795
	step [317/332], loss=4.8364
	step [318/332], loss=4.7630
	step [319/332], loss=6.4025
	step [320/332], loss=5.1486
	step [321/332], loss=4.0832
	step [322/332], loss=4.3847
	step [323/332], loss=4.5845
	step [324/332], loss=4.0784
	step [325/332], loss=5.2020
	step [326/332], loss=5.2033
	step [327/332], loss=4.6302
	step [328/332], loss=4.9067
	step [329/332], loss=4.4578
	step [330/332], loss=3.1660
	step [331/332], loss=4.6186
	step [332/332], loss=2.2172
	Evaluating
	loss=0.0170, precision=0.2033, recall=0.9911, f1=0.7144
saving model as: 0_saved_model.pth
Training finished
best_f1: 0.7143778116766294
directing: Z rim_enhanced: False test_id 0
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12263 # image files with weight 12232
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3256 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_two/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/255], loss=156.7067
	step [2/255], loss=134.3614
	step [3/255], loss=131.6218
	step [4/255], loss=129.1009
	step [5/255], loss=128.4555
	step [6/255], loss=124.1517
	step [7/255], loss=122.9184
	step [8/255], loss=124.4449
	step [9/255], loss=119.6741
	step [10/255], loss=118.0342
	step [11/255], loss=115.8653
	step [12/255], loss=111.8223
	step [13/255], loss=110.5531
	step [14/255], loss=112.5981
	step [15/255], loss=106.2388
	step [16/255], loss=104.5444
	step [17/255], loss=107.4806
	step [18/255], loss=103.8107
	step [19/255], loss=99.8812
	step [20/255], loss=98.5480
	step [21/255], loss=98.7360
	step [22/255], loss=100.9186
	step [23/255], loss=97.2148
	step [24/255], loss=95.3891
	step [25/255], loss=95.7135
	step [26/255], loss=91.9899
	step [27/255], loss=91.6892
	step [28/255], loss=91.8591
	step [29/255], loss=87.4253
	step [30/255], loss=87.2338
	step [31/255], loss=88.4100
	step [32/255], loss=88.6223
	step [33/255], loss=88.3203
	step [34/255], loss=86.6020
	step [35/255], loss=84.5139
	step [36/255], loss=83.4859
	step [37/255], loss=85.5487
	step [38/255], loss=83.1845
	step [39/255], loss=81.9813
	step [40/255], loss=79.5680
	step [41/255], loss=82.0304
	step [42/255], loss=78.2684
	step [43/255], loss=78.9132
	step [44/255], loss=77.9253
	step [45/255], loss=76.0175
	step [46/255], loss=76.5429
	step [47/255], loss=79.3731
	step [48/255], loss=76.5696
	step [49/255], loss=75.8721
	step [50/255], loss=78.2348
	step [51/255], loss=76.7388
	step [52/255], loss=77.3266
	step [53/255], loss=72.1403
	step [54/255], loss=69.3661
	step [55/255], loss=74.6466
	step [56/255], loss=73.5942
	step [57/255], loss=73.5046
	step [58/255], loss=73.2451
	step [59/255], loss=74.4712
	step [60/255], loss=71.5887
	step [61/255], loss=71.7165
	step [62/255], loss=69.7969
	step [63/255], loss=71.5792
	step [64/255], loss=69.3760
	step [65/255], loss=70.4283
	step [66/255], loss=68.3078
	step [67/255], loss=68.4755
	step [68/255], loss=70.5788
	step [69/255], loss=69.3624
	step [70/255], loss=69.5785
	step [71/255], loss=70.1740
	step [72/255], loss=68.1242
	step [73/255], loss=67.8238
	step [74/255], loss=67.2639
	step [75/255], loss=67.1329
	step [76/255], loss=65.9280
	step [77/255], loss=66.4039
	step [78/255], loss=68.5454
	step [79/255], loss=67.9376
	step [80/255], loss=66.8218
	step [81/255], loss=68.7494
	step [82/255], loss=67.6074
	step [83/255], loss=65.3155
	step [84/255], loss=67.4852
	step [85/255], loss=63.4039
	step [86/255], loss=66.0708
	step [87/255], loss=64.5200
	step [88/255], loss=62.8061
	step [89/255], loss=63.9955
	step [90/255], loss=66.4198
	step [91/255], loss=64.6426
	step [92/255], loss=65.4377
	step [93/255], loss=63.9381
	step [94/255], loss=66.2297
	step [95/255], loss=63.5209
	step [96/255], loss=63.5141
	step [97/255], loss=62.0844
	step [98/255], loss=63.2099
	step [99/255], loss=65.0538
	step [100/255], loss=63.5494
	step [101/255], loss=62.1973
	step [102/255], loss=60.4964
	step [103/255], loss=62.4820
	step [104/255], loss=64.0890
	step [105/255], loss=63.4996
	step [106/255], loss=63.7771
	step [107/255], loss=63.5820
	step [108/255], loss=64.7384
	step [109/255], loss=62.1497
	step [110/255], loss=63.4432
	step [111/255], loss=62.4743
	step [112/255], loss=60.9992
	step [113/255], loss=63.3768
	step [114/255], loss=62.7216
	step [115/255], loss=65.7024
	step [116/255], loss=62.7984
	step [117/255], loss=58.8527
	step [118/255], loss=58.4869
	step [119/255], loss=61.6375
	step [120/255], loss=59.3764
	step [121/255], loss=61.1883
	step [122/255], loss=58.8016
	step [123/255], loss=62.9917
	step [124/255], loss=61.3583
	step [125/255], loss=61.3799
	step [126/255], loss=59.3301
	step [127/255], loss=61.2440
	step [128/255], loss=59.2385
	step [129/255], loss=59.0178
	step [130/255], loss=61.7960
	step [131/255], loss=60.6261
	step [132/255], loss=57.9061
	step [133/255], loss=59.9286
	step [134/255], loss=60.5773
	step [135/255], loss=63.2741
	step [136/255], loss=60.5311
	step [137/255], loss=57.3426
	step [138/255], loss=59.7980
	step [139/255], loss=58.0565
	step [140/255], loss=59.7898
	step [141/255], loss=58.6815
	step [142/255], loss=61.0700
	step [143/255], loss=59.2513
	step [144/255], loss=57.3061
	step [145/255], loss=57.0760
	step [146/255], loss=59.5103
	step [147/255], loss=59.2898
	step [148/255], loss=58.8089
	step [149/255], loss=59.0953
	step [150/255], loss=57.8083
	step [151/255], loss=57.0657
	step [152/255], loss=56.5627
	step [153/255], loss=55.8336
	step [154/255], loss=58.3356
	step [155/255], loss=55.9225
	step [156/255], loss=56.0712
	step [157/255], loss=58.6623
	step [158/255], loss=56.7625
	step [159/255], loss=57.1159
	step [160/255], loss=57.2499
	step [161/255], loss=56.5604
	step [162/255], loss=56.0435
	step [163/255], loss=55.4830
	step [164/255], loss=56.3176
	step [165/255], loss=55.1407
	step [166/255], loss=54.8053
	step [167/255], loss=54.6373
	step [168/255], loss=56.0600
	step [169/255], loss=55.9678
	step [170/255], loss=57.6982
	step [171/255], loss=53.8642
	step [172/255], loss=55.9931
	step [173/255], loss=55.6045
	step [174/255], loss=54.3900
	step [175/255], loss=54.2103
	step [176/255], loss=52.7044
	step [177/255], loss=55.8858
	step [178/255], loss=56.1962
	step [179/255], loss=56.5732
	step [180/255], loss=53.5765
	step [181/255], loss=54.0892
	step [182/255], loss=54.4476
	step [183/255], loss=52.4013
	step [184/255], loss=54.2007
	step [185/255], loss=54.3740
	step [186/255], loss=51.9024
	step [187/255], loss=54.4049
	step [188/255], loss=55.2744
	step [189/255], loss=55.2854
	step [190/255], loss=52.5209
	step [191/255], loss=55.7351
	step [192/255], loss=53.2549
	step [193/255], loss=53.4485
	step [194/255], loss=52.5837
	step [195/255], loss=54.1101
	step [196/255], loss=52.3614
	step [197/255], loss=51.5939
	step [198/255], loss=53.7750
	step [199/255], loss=55.0078
	step [200/255], loss=50.6302
	step [201/255], loss=52.6007
	step [202/255], loss=52.1981
	step [203/255], loss=51.5793
	step [204/255], loss=52.7984
	step [205/255], loss=54.9131
	step [206/255], loss=51.5953
	step [207/255], loss=50.4089
	step [208/255], loss=51.1664
	step [209/255], loss=54.2850
	step [210/255], loss=51.5918
	step [211/255], loss=51.3718
	step [212/255], loss=49.0403
	step [213/255], loss=51.1167
	step [214/255], loss=51.5304
	step [215/255], loss=51.3171
	step [216/255], loss=51.9387
	step [217/255], loss=51.3556
	step [218/255], loss=51.5383
	step [219/255], loss=50.9212
	step [220/255], loss=49.8713
	step [221/255], loss=51.4558
	step [222/255], loss=50.3258
	step [223/255], loss=51.0495
	step [224/255], loss=53.8858
	step [225/255], loss=52.8319
	step [226/255], loss=48.8822
	step [227/255], loss=52.8508
	step [228/255], loss=49.6244
	step [229/255], loss=50.2533
	step [230/255], loss=52.3808
	step [231/255], loss=49.0441
	step [232/255], loss=49.8333
	step [233/255], loss=49.2042
	step [234/255], loss=51.9166
	step [235/255], loss=49.3461
	step [236/255], loss=50.9566
	step [237/255], loss=49.1615
	step [238/255], loss=50.1021
	step [239/255], loss=46.9087
	step [240/255], loss=50.6383
	step [241/255], loss=47.6600
	step [242/255], loss=49.0404
	step [243/255], loss=48.5626
	step [244/255], loss=49.6972
	step [245/255], loss=48.5848
	step [246/255], loss=48.5739
	step [247/255], loss=47.7724
	step [248/255], loss=49.1531
	step [249/255], loss=49.7906
	step [250/255], loss=47.8867
	step [251/255], loss=51.4459
	step [252/255], loss=47.2434
	step [253/255], loss=50.5155
	step [254/255], loss=48.1590
	step [255/255], loss=40.0511
	Evaluating
	loss=0.2352, precision=0.1254, recall=0.9916, f1=0.5865
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/255], loss=49.4769
	step [2/255], loss=46.4826
	step [3/255], loss=48.0415
	step [4/255], loss=52.1837
	step [5/255], loss=48.2143
	step [6/255], loss=46.5195
	step [7/255], loss=46.8651
	step [8/255], loss=47.1345
	step [9/255], loss=47.7914
	step [10/255], loss=48.5584
	step [11/255], loss=48.9546
	step [12/255], loss=47.6572
	step [13/255], loss=48.1563
	step [14/255], loss=47.4999
	step [15/255], loss=48.7607
	step [16/255], loss=48.5810
	step [17/255], loss=46.0497
	step [18/255], loss=45.1883
	step [19/255], loss=46.0553
	step [20/255], loss=47.3874
	step [21/255], loss=47.6901
	step [22/255], loss=46.5933
	step [23/255], loss=47.7077
	step [24/255], loss=45.1775
	step [25/255], loss=45.9978
	step [26/255], loss=46.4388
	step [27/255], loss=44.9405
	step [28/255], loss=46.8627
	step [29/255], loss=48.2626
	step [30/255], loss=44.4756
	step [31/255], loss=47.7958
	step [32/255], loss=46.5686
	step [33/255], loss=45.3448
	step [34/255], loss=44.3467
	step [35/255], loss=47.1584
	step [36/255], loss=45.3805
	step [37/255], loss=45.0432
	step [38/255], loss=46.5337
	step [39/255], loss=42.7567
	step [40/255], loss=45.5603
	step [41/255], loss=48.3785
	step [42/255], loss=44.9046
	step [43/255], loss=46.0122
	step [44/255], loss=44.4338
	step [45/255], loss=45.5457
	step [46/255], loss=43.9288
	step [47/255], loss=44.8596
	step [48/255], loss=44.7134
	step [49/255], loss=45.6374
	step [50/255], loss=42.8831
	step [51/255], loss=44.9354
	step [52/255], loss=43.0543
	step [53/255], loss=43.8029
	step [54/255], loss=43.0972
	step [55/255], loss=43.4423
	step [56/255], loss=44.1451
	step [57/255], loss=45.0525
	step [58/255], loss=44.5757
	step [59/255], loss=44.6903
	step [60/255], loss=45.8834
	step [61/255], loss=43.4197
	step [62/255], loss=42.5532
	step [63/255], loss=42.2402
	step [64/255], loss=43.1967
	step [65/255], loss=44.1554
	step [66/255], loss=46.5155
	step [67/255], loss=44.0503
	step [68/255], loss=44.1564
	step [69/255], loss=43.8638
	step [70/255], loss=44.3320
	step [71/255], loss=44.4229
	step [72/255], loss=41.3078
	step [73/255], loss=42.4047
	step [74/255], loss=42.9549
	step [75/255], loss=45.7535
	step [76/255], loss=43.0742
	step [77/255], loss=42.5430
	step [78/255], loss=41.2108
	step [79/255], loss=43.1402
	step [80/255], loss=41.9293
	step [81/255], loss=41.1249
	step [82/255], loss=43.0026
	step [83/255], loss=40.1600
	step [84/255], loss=45.2537
	step [85/255], loss=43.8948
	step [86/255], loss=42.0427
	step [87/255], loss=40.9625
	step [88/255], loss=44.8411
	step [89/255], loss=41.9513
	step [90/255], loss=40.4654
	step [91/255], loss=40.9635
	step [92/255], loss=43.1955
	step [93/255], loss=42.7012
	step [94/255], loss=38.6932
	step [95/255], loss=41.3065
	step [96/255], loss=41.0806
	step [97/255], loss=40.8215
	step [98/255], loss=40.1677
	step [99/255], loss=40.0743
	step [100/255], loss=42.7126
	step [101/255], loss=42.4717
	step [102/255], loss=40.6099
	step [103/255], loss=41.2631
	step [104/255], loss=40.2320
	step [105/255], loss=41.4863
	step [106/255], loss=42.6368
	step [107/255], loss=38.9994
	step [108/255], loss=39.5291
	step [109/255], loss=38.9165
	step [110/255], loss=39.6071
	step [111/255], loss=39.6174
	step [112/255], loss=39.6668
	step [113/255], loss=38.4456
	step [114/255], loss=39.9843
	step [115/255], loss=40.4599
	step [116/255], loss=37.3970
	step [117/255], loss=41.6224
	step [118/255], loss=40.7052
	step [119/255], loss=39.2744
	step [120/255], loss=40.8073
	step [121/255], loss=39.3279
	step [122/255], loss=38.0103
	step [123/255], loss=40.6458
	step [124/255], loss=42.3296
	step [125/255], loss=43.3491
	step [126/255], loss=40.5750
	step [127/255], loss=39.1314
	step [128/255], loss=41.2134
	step [129/255], loss=38.7759
	step [130/255], loss=39.9177
	step [131/255], loss=38.2259
	step [132/255], loss=37.9592
	step [133/255], loss=40.7522
	step [134/255], loss=37.8552
	step [135/255], loss=39.3741
	step [136/255], loss=41.9325
	step [137/255], loss=39.5810
	step [138/255], loss=39.2556
	step [139/255], loss=37.6498
	step [140/255], loss=40.4285
	step [141/255], loss=39.8794
	step [142/255], loss=41.7475
	step [143/255], loss=42.2073
	step [144/255], loss=37.8457
	step [145/255], loss=39.7775
	step [146/255], loss=35.8825
	step [147/255], loss=37.6474
	step [148/255], loss=40.2145
	step [149/255], loss=39.9432
	step [150/255], loss=39.2970
	step [151/255], loss=37.6337
	step [152/255], loss=38.6333
	step [153/255], loss=37.5968
	step [154/255], loss=35.8554
	step [155/255], loss=39.6350
	step [156/255], loss=35.8084
	step [157/255], loss=41.1401
	step [158/255], loss=41.0121
	step [159/255], loss=36.3371
	step [160/255], loss=37.8032
	step [161/255], loss=37.9910
	step [162/255], loss=37.5188
	step [163/255], loss=38.8949
	step [164/255], loss=36.6036
	step [165/255], loss=38.1955
	step [166/255], loss=39.3580
	step [167/255], loss=36.7428
	step [168/255], loss=38.9678
	step [169/255], loss=36.9216
	step [170/255], loss=38.6265
	step [171/255], loss=37.9221
	step [172/255], loss=37.5491
	step [173/255], loss=39.1439
	step [174/255], loss=35.7770
	step [175/255], loss=38.3368
	step [176/255], loss=38.7881
	step [177/255], loss=38.1922
	step [178/255], loss=37.0589
	step [179/255], loss=36.2627
	step [180/255], loss=35.7540
	step [181/255], loss=34.6985
	step [182/255], loss=36.6912
	step [183/255], loss=37.5373
	step [184/255], loss=38.2886
	step [185/255], loss=37.8292
	step [186/255], loss=35.7071
	step [187/255], loss=34.1089
	step [188/255], loss=36.4806
	step [189/255], loss=35.5184
	step [190/255], loss=35.9490
	step [191/255], loss=35.1721
	step [192/255], loss=35.3211
	step [193/255], loss=36.9541
	step [194/255], loss=36.3515
	step [195/255], loss=36.3058
	step [196/255], loss=36.3010
	step [197/255], loss=39.1384
	step [198/255], loss=35.2792
	step [199/255], loss=34.0212
	step [200/255], loss=35.3626
	step [201/255], loss=36.7702
	step [202/255], loss=34.5508
	step [203/255], loss=34.6246
	step [204/255], loss=35.5909
	step [205/255], loss=36.8509
	step [206/255], loss=35.6872
	step [207/255], loss=36.6642
	step [208/255], loss=33.4562
	step [209/255], loss=35.3038
	step [210/255], loss=35.0054
	step [211/255], loss=35.2600
	step [212/255], loss=36.2641
	step [213/255], loss=34.6623
	step [214/255], loss=33.3613
	step [215/255], loss=35.3545
	step [216/255], loss=35.7848
	step [217/255], loss=36.4232
	step [218/255], loss=35.3889
	step [219/255], loss=34.7576
	step [220/255], loss=34.2506
	step [221/255], loss=34.2992
	step [222/255], loss=35.4482
	step [223/255], loss=35.2653
	step [224/255], loss=35.9235
	step [225/255], loss=33.7744
	step [226/255], loss=34.6186
	step [227/255], loss=35.0483
	step [228/255], loss=34.4318
	step [229/255], loss=33.1198
	step [230/255], loss=35.1853
	step [231/255], loss=36.4482
	step [232/255], loss=34.0370
	step [233/255], loss=35.3758
	step [234/255], loss=36.0507
	step [235/255], loss=33.3515
	step [236/255], loss=33.4746
	step [237/255], loss=33.4159
	step [238/255], loss=33.2779
	step [239/255], loss=34.7526
	step [240/255], loss=34.6757
	step [241/255], loss=36.9450
	step [242/255], loss=33.4475
	step [243/255], loss=32.4732
	step [244/255], loss=31.9000
	step [245/255], loss=34.2450
	step [246/255], loss=32.6300
	step [247/255], loss=35.0738
	step [248/255], loss=32.4571
	step [249/255], loss=33.4056
	step [250/255], loss=33.0547
	step [251/255], loss=32.8236
	step [252/255], loss=32.2325
	step [253/255], loss=31.1929
	step [254/255], loss=34.9034
	step [255/255], loss=27.1268
	Evaluating
	loss=0.1492, precision=0.1582, recall=0.9913, f1=0.6493
saving model as: 0_saved_model.pth
Training epoch 3
	step [1/255], loss=32.9979
	step [2/255], loss=33.1639
	step [3/255], loss=31.6109
	step [4/255], loss=32.5046
	step [5/255], loss=32.9494
	step [6/255], loss=31.3175
	step [7/255], loss=32.7720
	step [8/255], loss=31.9445
	step [9/255], loss=34.8306
	step [10/255], loss=32.2292
	step [11/255], loss=33.6756
	step [12/255], loss=31.9428
	step [13/255], loss=32.4712
	step [14/255], loss=33.4377
	step [15/255], loss=33.5580
	step [16/255], loss=32.4143
	step [17/255], loss=31.5989
	step [18/255], loss=32.9992
	step [19/255], loss=32.9312
	step [20/255], loss=33.9169
	step [21/255], loss=32.3615
	step [22/255], loss=32.8049
	step [23/255], loss=33.0827
	step [24/255], loss=32.5338
	step [25/255], loss=31.9672
	step [26/255], loss=30.9707
	step [27/255], loss=32.2425
	step [28/255], loss=33.8875
	step [29/255], loss=31.9588
	step [30/255], loss=31.3288
	step [31/255], loss=33.0180
	step [32/255], loss=30.2074
	step [33/255], loss=32.0420
	step [34/255], loss=30.8812
	step [35/255], loss=29.8034
	step [36/255], loss=33.3275
	step [37/255], loss=32.6535
	step [38/255], loss=30.5446
	step [39/255], loss=30.2724
	step [40/255], loss=33.8138
	step [41/255], loss=32.2617
	step [42/255], loss=30.4533
	step [43/255], loss=29.0941
	step [44/255], loss=28.2897
	step [45/255], loss=35.7871
	step [46/255], loss=30.6793
	step [47/255], loss=31.6885
	step [48/255], loss=30.9848
	step [49/255], loss=31.5037
	step [50/255], loss=31.8581
	step [51/255], loss=31.2350
	step [52/255], loss=32.7752
	step [53/255], loss=32.4149
	step [54/255], loss=30.7986
	step [55/255], loss=30.4825
	step [56/255], loss=30.9884
	step [57/255], loss=30.3462
	step [58/255], loss=31.4414
	step [59/255], loss=29.5360
	step [60/255], loss=32.6177
	step [61/255], loss=32.2647
	step [62/255], loss=31.0099
	step [63/255], loss=30.3983
	step [64/255], loss=28.3721
	step [65/255], loss=31.7001
	step [66/255], loss=30.4580
	step [67/255], loss=31.1956
	step [68/255], loss=29.4186
	step [69/255], loss=31.2848
	step [70/255], loss=29.9356
	step [71/255], loss=27.8040
	step [72/255], loss=28.9311
	step [73/255], loss=30.8819
	step [74/255], loss=29.8037
	step [75/255], loss=30.6135
	step [76/255], loss=30.1960
	step [77/255], loss=29.3783
	step [78/255], loss=28.8826
	step [79/255], loss=29.5501
	step [80/255], loss=29.7922
	step [81/255], loss=27.8061
	step [82/255], loss=30.2340
	step [83/255], loss=31.8601
	step [84/255], loss=30.9747
	step [85/255], loss=32.3086
	step [86/255], loss=28.4392
	step [87/255], loss=31.0814
	step [88/255], loss=30.7002
	step [89/255], loss=32.1643
	step [90/255], loss=31.4007
	step [91/255], loss=30.2119
	step [92/255], loss=29.1275
	step [93/255], loss=28.4744
	step [94/255], loss=31.1448
	step [95/255], loss=30.1886
	step [96/255], loss=27.9659
	step [97/255], loss=29.9014
	step [98/255], loss=26.7715
	step [99/255], loss=29.0671
	step [100/255], loss=28.8769
	step [101/255], loss=31.3569
	step [102/255], loss=31.9583
	step [103/255], loss=27.4432
	step [104/255], loss=29.7875
	step [105/255], loss=29.6116
	step [106/255], loss=27.7831
	step [107/255], loss=27.9636
	step [108/255], loss=27.8421
	step [109/255], loss=28.0155
	step [110/255], loss=30.2327
	step [111/255], loss=28.9758
	step [112/255], loss=29.3937
	step [113/255], loss=29.7556
	step [114/255], loss=29.8133
	step [115/255], loss=29.2363
	step [116/255], loss=29.5002
	step [117/255], loss=28.8054
	step [118/255], loss=28.9197
	step [119/255], loss=27.6221
	step [120/255], loss=27.5429
	step [121/255], loss=27.8641
	step [122/255], loss=27.7922
	step [123/255], loss=34.4187
	step [124/255], loss=27.4907
	step [125/255], loss=34.3999
	step [126/255], loss=29.2028
	step [127/255], loss=30.3866
	step [128/255], loss=31.4742
	step [129/255], loss=28.9188
	step [130/255], loss=32.3735
	step [131/255], loss=29.4907
	step [132/255], loss=31.0313
	step [133/255], loss=28.0169
	step [134/255], loss=28.9437
	step [135/255], loss=28.9057
	step [136/255], loss=30.7004
	step [137/255], loss=27.9269
	step [138/255], loss=29.2254
	step [139/255], loss=26.8682
	step [140/255], loss=26.6880
	step [141/255], loss=27.1231
	step [142/255], loss=26.7930
	step [143/255], loss=26.4112
	step [144/255], loss=26.2145
	step [145/255], loss=27.7043
	step [146/255], loss=29.7916
	step [147/255], loss=26.3269
	step [148/255], loss=28.0713
	step [149/255], loss=28.0036
	step [150/255], loss=29.9094
	step [151/255], loss=27.6566
	step [152/255], loss=28.4429
	step [153/255], loss=28.7126
	step [154/255], loss=28.7376
	step [155/255], loss=28.9489
	step [156/255], loss=28.3856
	step [157/255], loss=27.6303
	step [158/255], loss=31.0906
	step [159/255], loss=29.9955
	step [160/255], loss=29.7408
	step [161/255], loss=28.4156
	step [162/255], loss=27.1261
	step [163/255], loss=27.2532
	step [164/255], loss=26.6697
	step [165/255], loss=26.4939
	step [166/255], loss=28.1077
	step [167/255], loss=25.9456
	step [168/255], loss=28.7661
	step [169/255], loss=27.5474
	step [170/255], loss=25.1971
	step [171/255], loss=26.6604
	step [172/255], loss=26.4639
	step [173/255], loss=28.2763
	step [174/255], loss=25.5664
	step [175/255], loss=28.2967
	step [176/255], loss=28.4681
	step [177/255], loss=28.1245
	step [178/255], loss=25.0470
	step [179/255], loss=30.1574
	step [180/255], loss=29.3408
	step [181/255], loss=26.4549
	step [182/255], loss=28.0294
	step [183/255], loss=26.6613
	step [184/255], loss=26.3925
	step [185/255], loss=26.3127
	step [186/255], loss=25.8088
	step [187/255], loss=26.9347
	step [188/255], loss=23.8525
	step [189/255], loss=27.5799
	step [190/255], loss=25.7575
	step [191/255], loss=24.5073
	step [192/255], loss=28.7684
	step [193/255], loss=27.5802
	step [194/255], loss=27.3521
	step [195/255], loss=24.8627
	step [196/255], loss=26.0498
	step [197/255], loss=28.0254
	step [198/255], loss=24.9461
	step [199/255], loss=24.3950
	step [200/255], loss=28.4353
	step [201/255], loss=28.0172
	step [202/255], loss=26.7654
	step [203/255], loss=25.1276
	step [204/255], loss=25.6783
	step [205/255], loss=25.6744
	step [206/255], loss=24.2206
	step [207/255], loss=26.7452
	step [208/255], loss=25.1776
	step [209/255], loss=27.3380
	step [210/255], loss=28.0561
	step [211/255], loss=26.7438
	step [212/255], loss=25.4270
	step [213/255], loss=27.0627
	step [214/255], loss=24.9684
	step [215/255], loss=26.5042
	step [216/255], loss=24.6532
	step [217/255], loss=24.5600
	step [218/255], loss=27.7001
	step [219/255], loss=25.8266
	step [220/255], loss=25.3030
	step [221/255], loss=24.9852
	step [222/255], loss=26.6799
	step [223/255], loss=24.1411
	step [224/255], loss=29.4656
	step [225/255], loss=23.3689
	step [226/255], loss=24.9869
	step [227/255], loss=23.7659
	step [228/255], loss=25.6173
	step [229/255], loss=26.4302
	step [230/255], loss=26.8497
	step [231/255], loss=26.5183
	step [232/255], loss=25.9044
	step [233/255], loss=25.4551
	step [234/255], loss=27.1965
	step [235/255], loss=24.1775
	step [236/255], loss=26.7815
	step [237/255], loss=27.7371
	step [238/255], loss=25.9525
	step [239/255], loss=24.4937
	step [240/255], loss=28.3188
	step [241/255], loss=24.5064
	step [242/255], loss=25.0681
	step [243/255], loss=27.4710
	step [244/255], loss=25.4456
	step [245/255], loss=24.4386
	step [246/255], loss=22.6375
	step [247/255], loss=28.2073
	step [248/255], loss=24.8367
	step [249/255], loss=23.4854
	step [250/255], loss=24.4997
	step [251/255], loss=24.4305
	step [252/255], loss=27.2656
	step [253/255], loss=24.6936
	step [254/255], loss=24.7984
	step [255/255], loss=20.4556
	Evaluating
	loss=0.1066, precision=0.1412, recall=0.9924, f1=0.6192
Training epoch 4
	step [1/255], loss=26.1051
	step [2/255], loss=24.1913
	step [3/255], loss=22.9135
	step [4/255], loss=24.0814
	step [5/255], loss=25.1578
	step [6/255], loss=24.7153
	step [7/255], loss=24.9152
	step [8/255], loss=25.3058
	step [9/255], loss=24.2926
	step [10/255], loss=25.5010
	step [11/255], loss=24.3996
	step [12/255], loss=25.3352
	step [13/255], loss=23.9254
	step [14/255], loss=27.7095
	step [15/255], loss=23.4515
	step [16/255], loss=23.5652
	step [17/255], loss=23.6876
	step [18/255], loss=25.0598
	step [19/255], loss=23.2279
	step [20/255], loss=23.8544
	step [21/255], loss=23.9304
	step [22/255], loss=23.2383
	step [23/255], loss=25.8306
	step [24/255], loss=22.2272
	step [25/255], loss=23.5336
	step [26/255], loss=22.8296
	step [27/255], loss=26.6653
	step [28/255], loss=24.5715
	step [29/255], loss=23.5688
	step [30/255], loss=23.9628
	step [31/255], loss=23.7803
	step [32/255], loss=22.8939
	step [33/255], loss=24.7249
	step [34/255], loss=23.6832
	step [35/255], loss=24.3582
	step [36/255], loss=23.8492
	step [37/255], loss=24.9123
	step [38/255], loss=23.7760
	step [39/255], loss=25.9524
	step [40/255], loss=23.8681
	step [41/255], loss=23.9477
	step [42/255], loss=24.8480
	step [43/255], loss=22.9195
	step [44/255], loss=21.7573
	step [45/255], loss=23.4497
	step [46/255], loss=22.0445
	step [47/255], loss=22.2293
	step [48/255], loss=25.7250
	step [49/255], loss=23.4122
	step [50/255], loss=22.4358
	step [51/255], loss=22.7812
	step [52/255], loss=24.5408
	step [53/255], loss=22.9559
	step [54/255], loss=24.8626
	step [55/255], loss=24.0871
	step [56/255], loss=22.8044
	step [57/255], loss=23.8304
	step [58/255], loss=22.3574
	step [59/255], loss=22.3740
	step [60/255], loss=22.9046
	step [61/255], loss=23.3244
	step [62/255], loss=21.7382
	step [63/255], loss=24.3302
	step [64/255], loss=23.0849
	step [65/255], loss=23.2653
	step [66/255], loss=25.9469
	step [67/255], loss=22.8609
	step [68/255], loss=21.7653
	step [69/255], loss=21.6938
	step [70/255], loss=22.5790
	step [71/255], loss=23.1692
	step [72/255], loss=22.7640
	step [73/255], loss=23.3458
	step [74/255], loss=24.0602
	step [75/255], loss=22.5779
	step [76/255], loss=20.5567
	step [77/255], loss=23.3721
	step [78/255], loss=22.2487
	step [79/255], loss=24.5592
	step [80/255], loss=24.0118
	step [81/255], loss=21.3604
	step [82/255], loss=25.3289
	step [83/255], loss=22.9365
	step [84/255], loss=25.2853
	step [85/255], loss=21.6774
	step [86/255], loss=23.4387
	step [87/255], loss=23.5406
	step [88/255], loss=23.7390
	step [89/255], loss=21.1853
	step [90/255], loss=24.3070
	step [91/255], loss=22.4566
	step [92/255], loss=20.7825
	step [93/255], loss=25.4187
	step [94/255], loss=23.0859
	step [95/255], loss=22.2965
	step [96/255], loss=22.4795
	step [97/255], loss=23.6416
	step [98/255], loss=21.9326
	step [99/255], loss=21.6091
	step [100/255], loss=23.6401
	step [101/255], loss=27.3268
	step [102/255], loss=23.5142
	step [103/255], loss=22.3154
	step [104/255], loss=20.8068
	step [105/255], loss=22.7717
	step [106/255], loss=20.7910
	step [107/255], loss=23.8689
	step [108/255], loss=20.8139
	step [109/255], loss=22.5506
	step [110/255], loss=21.6302
	step [111/255], loss=20.5879
	step [112/255], loss=24.3870
	step [113/255], loss=24.1595
	step [114/255], loss=23.3490
	step [115/255], loss=22.3507
	step [116/255], loss=22.9385
	step [117/255], loss=21.7438
	step [118/255], loss=24.0100
	step [119/255], loss=21.4496
	step [120/255], loss=23.3425
	step [121/255], loss=21.9823
	step [122/255], loss=22.4886
	step [123/255], loss=20.4089
	step [124/255], loss=22.5996
	step [125/255], loss=21.4648
	step [126/255], loss=21.2718
	step [127/255], loss=22.8545
	step [128/255], loss=21.1131
	step [129/255], loss=20.5204
	step [130/255], loss=20.4427
	step [131/255], loss=22.7919
	step [132/255], loss=23.8688
	step [133/255], loss=21.0161
	step [134/255], loss=23.2243
	step [135/255], loss=19.4693
	step [136/255], loss=20.8799
	step [137/255], loss=22.0522
	step [138/255], loss=21.8508
	step [139/255], loss=23.1866
	step [140/255], loss=20.1170
	step [141/255], loss=20.7864
	step [142/255], loss=21.8265
	step [143/255], loss=23.4759
	step [144/255], loss=23.5386
	step [145/255], loss=20.8091
	step [146/255], loss=22.0493
	step [147/255], loss=20.9812
	step [148/255], loss=24.8065
	step [149/255], loss=20.4495
	step [150/255], loss=23.1580
	step [151/255], loss=22.2777
	step [152/255], loss=19.7941
	step [153/255], loss=21.8993
	step [154/255], loss=20.2924
	step [155/255], loss=20.3743
	step [156/255], loss=22.5635
	step [157/255], loss=22.7482
	step [158/255], loss=20.8888
	step [159/255], loss=23.8791
	step [160/255], loss=20.9930
	step [161/255], loss=21.3298
	step [162/255], loss=21.9519
	step [163/255], loss=20.5501
	step [164/255], loss=20.2968
	step [165/255], loss=21.5297
	step [166/255], loss=22.1176
	step [167/255], loss=21.3095
	step [168/255], loss=19.5677
	step [169/255], loss=23.2351
	step [170/255], loss=20.0786
	step [171/255], loss=22.4972
	step [172/255], loss=21.1606
	step [173/255], loss=20.3742
	step [174/255], loss=24.0287
	step [175/255], loss=20.4401
	step [176/255], loss=19.7026
	step [177/255], loss=20.5722
	step [178/255], loss=19.8699
	step [179/255], loss=24.0559
	step [180/255], loss=19.5922
	step [181/255], loss=21.5102
	step [182/255], loss=20.7206
	step [183/255], loss=21.5994
	step [184/255], loss=21.6892
	step [185/255], loss=20.3139
	step [186/255], loss=21.6274
	step [187/255], loss=20.5986
	step [188/255], loss=21.1243
	step [189/255], loss=19.9614
	step [190/255], loss=20.6728
	step [191/255], loss=21.9129
	step [192/255], loss=20.1791
	step [193/255], loss=22.5666
	step [194/255], loss=21.2106
	step [195/255], loss=21.8682
	step [196/255], loss=22.2284
	step [197/255], loss=23.7239
	step [198/255], loss=19.8687
	step [199/255], loss=20.6785
	step [200/255], loss=20.4869
	step [201/255], loss=20.3116
	step [202/255], loss=21.0370
	step [203/255], loss=20.8544
	step [204/255], loss=21.0447
	step [205/255], loss=22.8602
	step [206/255], loss=19.0747
	step [207/255], loss=19.2066
	step [208/255], loss=20.8632
	step [209/255], loss=21.9247
	step [210/255], loss=21.4750
	step [211/255], loss=22.1174
	step [212/255], loss=18.9887
	step [213/255], loss=19.6309
	step [214/255], loss=20.7919
	step [215/255], loss=19.2619
	step [216/255], loss=22.9669
	step [217/255], loss=18.8490
	step [218/255], loss=19.4461
	step [219/255], loss=21.7124
	step [220/255], loss=20.6560
	step [221/255], loss=21.3634
	step [222/255], loss=18.0606
	step [223/255], loss=19.8014
	step [224/255], loss=23.0851
	step [225/255], loss=21.6736
	step [226/255], loss=22.3368
	step [227/255], loss=22.3379
	step [228/255], loss=22.5798
	step [229/255], loss=22.7885
	step [230/255], loss=21.1276
	step [231/255], loss=20.0294
	step [232/255], loss=22.1768
	step [233/255], loss=20.2197
	step [234/255], loss=20.6368
	step [235/255], loss=20.5026
	step [236/255], loss=19.6765
	step [237/255], loss=19.6880
	step [238/255], loss=21.2772
	step [239/255], loss=21.3304
	step [240/255], loss=19.3591
	step [241/255], loss=19.7058
	step [242/255], loss=20.3975
	step [243/255], loss=19.8218
	step [244/255], loss=19.3461
	step [245/255], loss=18.5172
	step [246/255], loss=19.3201
	step [247/255], loss=20.8408
	step [248/255], loss=19.8966
	step [249/255], loss=18.7521
	step [250/255], loss=20.3309
	step [251/255], loss=18.2144
	step [252/255], loss=21.5167
	step [253/255], loss=19.3394
	step [254/255], loss=18.0476
	step [255/255], loss=18.7423
	Evaluating
	loss=0.0842, precision=0.1517, recall=0.9925, f1=0.6385
Training epoch 5
	step [1/255], loss=18.8264
	step [2/255], loss=20.0446
	step [3/255], loss=18.0854
	step [4/255], loss=18.4608
	step [5/255], loss=18.8955
	step [6/255], loss=19.4187
	step [7/255], loss=19.4434
	step [8/255], loss=18.4589
	step [9/255], loss=19.0690
	step [10/255], loss=21.5465
	step [11/255], loss=20.1563
	step [12/255], loss=20.5031
	step [13/255], loss=19.1864
	step [14/255], loss=19.9511
	step [15/255], loss=19.2063
	step [16/255], loss=19.6403
	step [17/255], loss=22.0032
	step [18/255], loss=21.8509
	step [19/255], loss=18.7500
	step [20/255], loss=19.0346
	step [21/255], loss=18.6659
	step [22/255], loss=20.5288
	step [23/255], loss=20.4063
	step [24/255], loss=19.4524
	step [25/255], loss=21.5476
	step [26/255], loss=20.0633
	step [27/255], loss=20.5745
	step [28/255], loss=20.1183
	step [29/255], loss=17.5670
	step [30/255], loss=19.4653
	step [31/255], loss=23.2096
	step [32/255], loss=21.4989
	step [33/255], loss=20.0718
	step [34/255], loss=19.4646
	step [35/255], loss=18.7372
	step [36/255], loss=21.0639
	step [37/255], loss=18.4522
	step [38/255], loss=22.3446
	step [39/255], loss=18.6373
	step [40/255], loss=19.1725
	step [41/255], loss=20.5470
	step [42/255], loss=18.0409
	step [43/255], loss=21.0285
	step [44/255], loss=18.1426
	step [45/255], loss=17.4034
	step [46/255], loss=18.8500
	step [47/255], loss=19.0383
	step [48/255], loss=20.3996
	step [49/255], loss=22.3227
	step [50/255], loss=19.7841
	step [51/255], loss=18.8792
	step [52/255], loss=20.0276
	step [53/255], loss=20.6665
	step [54/255], loss=18.0575
	step [55/255], loss=18.0105
	step [56/255], loss=17.9561
	step [57/255], loss=19.9795
	step [58/255], loss=19.6534
	step [59/255], loss=17.0965
	step [60/255], loss=18.6731
	step [61/255], loss=20.9616
	step [62/255], loss=20.7379
	step [63/255], loss=18.5446
	step [64/255], loss=20.3734
	step [65/255], loss=17.9871
	step [66/255], loss=18.4871
	step [67/255], loss=17.3208
	step [68/255], loss=18.1127
	step [69/255], loss=20.8177
	step [70/255], loss=18.8870
	step [71/255], loss=15.6343
	step [72/255], loss=18.3471
	step [73/255], loss=17.2056
	step [74/255], loss=19.9528
	step [75/255], loss=21.6724
	step [76/255], loss=19.3946
	step [77/255], loss=18.5572
	step [78/255], loss=17.1796
	step [79/255], loss=21.1803
	step [80/255], loss=17.8119
	step [81/255], loss=19.9659
	step [82/255], loss=17.6720
	step [83/255], loss=18.2048
	step [84/255], loss=19.2064
	step [85/255], loss=19.6987
	step [86/255], loss=18.3458
	step [87/255], loss=20.9364
	step [88/255], loss=20.1137
	step [89/255], loss=21.5417
	step [90/255], loss=19.7383
	step [91/255], loss=18.6505
	step [92/255], loss=19.6839
	step [93/255], loss=18.9075
	step [94/255], loss=18.2995
	step [95/255], loss=16.4444
	step [96/255], loss=16.9036
	step [97/255], loss=19.2759
	step [98/255], loss=18.8916
	step [99/255], loss=19.6430
	step [100/255], loss=17.0527
	step [101/255], loss=18.5023
	step [102/255], loss=18.2285
	step [103/255], loss=17.9360
	step [104/255], loss=19.9803
	step [105/255], loss=18.8949
	step [106/255], loss=16.7795
	step [107/255], loss=16.5296
	step [108/255], loss=17.9328
	step [109/255], loss=18.8429
	step [110/255], loss=17.9968
	step [111/255], loss=21.1662
	step [112/255], loss=18.1975
	step [113/255], loss=16.7895
	step [114/255], loss=19.0615
	step [115/255], loss=15.5650
	step [116/255], loss=17.7753
	step [117/255], loss=18.1774
	step [118/255], loss=17.0499
	step [119/255], loss=18.4933
	step [120/255], loss=16.6303
	step [121/255], loss=17.9246
	step [122/255], loss=17.4285
	step [123/255], loss=18.7906
	step [124/255], loss=18.8052
	step [125/255], loss=18.4662
	step [126/255], loss=19.1523
	step [127/255], loss=17.5245
	step [128/255], loss=20.2331
	step [129/255], loss=16.7411
	step [130/255], loss=16.6649
	step [131/255], loss=16.6449
	step [132/255], loss=19.5974
	step [133/255], loss=16.6753
	step [134/255], loss=21.4905
	step [135/255], loss=17.7635
	step [136/255], loss=18.7617
	step [137/255], loss=16.8960
	step [138/255], loss=20.1290
	step [139/255], loss=17.5932
	step [140/255], loss=19.7784
	step [141/255], loss=17.1355
	step [142/255], loss=17.4410
	step [143/255], loss=18.7713
	step [144/255], loss=19.1956
	step [145/255], loss=18.7120
	step [146/255], loss=18.2374
	step [147/255], loss=16.5518
	step [148/255], loss=16.7706
	step [149/255], loss=17.2237
	step [150/255], loss=17.6564
	step [151/255], loss=17.3407
	step [152/255], loss=16.9539
	step [153/255], loss=17.2251
	step [154/255], loss=16.6679
	step [155/255], loss=17.3253
	step [156/255], loss=17.5458
	step [157/255], loss=16.2290
	step [158/255], loss=19.2366
	step [159/255], loss=17.9996
	step [160/255], loss=18.1309
	step [161/255], loss=17.6632
	step [162/255], loss=17.2534
	step [163/255], loss=17.3996
	step [164/255], loss=17.2447
	step [165/255], loss=17.1181
	step [166/255], loss=16.9549
	step [167/255], loss=18.7545
	step [168/255], loss=17.8980
	step [169/255], loss=18.1198
	step [170/255], loss=19.4787
	step [171/255], loss=18.4592
	step [172/255], loss=16.4300
	step [173/255], loss=19.1795
	step [174/255], loss=20.1224
	step [175/255], loss=16.4473
	step [176/255], loss=17.6432
	step [177/255], loss=17.7543
	step [178/255], loss=16.8151
	step [179/255], loss=18.1439
	step [180/255], loss=17.5332
	step [181/255], loss=17.5026
	step [182/255], loss=16.4822
	step [183/255], loss=16.3014
	step [184/255], loss=18.4877
	step [185/255], loss=17.1062
	step [186/255], loss=17.7985
	step [187/255], loss=15.2216
	step [188/255], loss=18.6169
	step [189/255], loss=18.5053
	step [190/255], loss=18.1031
	step [191/255], loss=18.2387
	step [192/255], loss=16.6648
	step [193/255], loss=16.5293
	step [194/255], loss=17.7147
	step [195/255], loss=16.5156
	step [196/255], loss=18.7381
	step [197/255], loss=18.5280
	step [198/255], loss=18.8857
	step [199/255], loss=16.6182
	step [200/255], loss=16.2403
	step [201/255], loss=18.7546
	step [202/255], loss=16.6562
	step [203/255], loss=14.8507
	step [204/255], loss=15.9187
	step [205/255], loss=17.6996
	step [206/255], loss=15.7526
	step [207/255], loss=19.2330
	step [208/255], loss=16.2534
	step [209/255], loss=19.3770
	step [210/255], loss=16.9645
	step [211/255], loss=18.5535
	step [212/255], loss=16.9276
	step [213/255], loss=17.4375
	step [214/255], loss=17.5870
	step [215/255], loss=16.3221
	step [216/255], loss=19.9318
	step [217/255], loss=16.8990
	step [218/255], loss=17.6702
	step [219/255], loss=16.3693
	step [220/255], loss=16.1745
	step [221/255], loss=17.4700
	step [222/255], loss=16.0216
	step [223/255], loss=17.2143
	step [224/255], loss=17.2519
	step [225/255], loss=17.8191
	step [226/255], loss=16.3783
	step [227/255], loss=17.5703
	step [228/255], loss=15.1399
	step [229/255], loss=16.4429
	step [230/255], loss=15.7835
	step [231/255], loss=15.1075
	step [232/255], loss=18.0828
	step [233/255], loss=16.2666
	step [234/255], loss=17.9179
	step [235/255], loss=18.0514
	step [236/255], loss=17.9282
	step [237/255], loss=16.4344
	step [238/255], loss=19.4138
	step [239/255], loss=16.9943
	step [240/255], loss=20.3613
	step [241/255], loss=16.8525
	step [242/255], loss=18.7967
	step [243/255], loss=17.2536
	step [244/255], loss=17.0871
	step [245/255], loss=16.3257
	step [246/255], loss=15.4913
	step [247/255], loss=19.1056
	step [248/255], loss=17.0625
	step [249/255], loss=13.8442
	step [250/255], loss=18.4643
	step [251/255], loss=14.9468
	step [252/255], loss=15.2085
	step [253/255], loss=21.5509
	step [254/255], loss=17.2680
	step [255/255], loss=14.4933
	Evaluating
	loss=0.0675, precision=0.1247, recall=0.9940, f1=0.5857
Training epoch 6
	step [1/255], loss=17.3177
	step [2/255], loss=15.6048
	step [3/255], loss=16.0565
	step [4/255], loss=19.1665
	step [5/255], loss=15.5585
	step [6/255], loss=15.7934
	step [7/255], loss=16.5947
	step [8/255], loss=15.3567
	step [9/255], loss=15.3417
	step [10/255], loss=16.1020
	step [11/255], loss=17.2915
	step [12/255], loss=17.0228
	step [13/255], loss=18.6892
	step [14/255], loss=18.6108
	step [15/255], loss=14.7714
	step [16/255], loss=15.9876
	step [17/255], loss=16.0289
	step [18/255], loss=16.7477
	step [19/255], loss=16.0905
	step [20/255], loss=16.0659
	step [21/255], loss=16.1525
	step [22/255], loss=19.5689
	step [23/255], loss=17.9328
	step [24/255], loss=15.6069
	step [25/255], loss=18.3021
	step [26/255], loss=15.5591
	step [27/255], loss=15.3508
	step [28/255], loss=15.5200
	step [29/255], loss=17.6354
	step [30/255], loss=18.0212
	step [31/255], loss=16.6646
	step [32/255], loss=16.2855
	step [33/255], loss=14.9895
	step [34/255], loss=16.2136
	step [35/255], loss=16.9702
	step [36/255], loss=15.6061
	step [37/255], loss=13.6718
	step [38/255], loss=14.1218
	step [39/255], loss=16.6609
	step [40/255], loss=17.8606
	step [41/255], loss=15.1679
	step [42/255], loss=16.2802
	step [43/255], loss=17.4529
	step [44/255], loss=14.5295
	step [45/255], loss=15.5154
	step [46/255], loss=16.2722
	step [47/255], loss=18.2446
	step [48/255], loss=15.8097
	step [49/255], loss=17.3438
	step [50/255], loss=15.4397
	step [51/255], loss=14.3926
	step [52/255], loss=18.4074
	step [53/255], loss=17.1535
	step [54/255], loss=17.4427
	step [55/255], loss=16.5790
	step [56/255], loss=16.6857
	step [57/255], loss=16.2013
	step [58/255], loss=15.9999
	step [59/255], loss=15.1444
	step [60/255], loss=17.4470
	step [61/255], loss=14.9424
	step [62/255], loss=16.7480
	step [63/255], loss=16.4020
	step [64/255], loss=16.6858
	step [65/255], loss=15.2305
	step [66/255], loss=19.0136
	step [67/255], loss=14.2185
	step [68/255], loss=16.6098
	step [69/255], loss=16.6676
	step [70/255], loss=15.7354
	step [71/255], loss=14.7280
	step [72/255], loss=16.3510
	step [73/255], loss=16.9706
	step [74/255], loss=15.4756
	step [75/255], loss=15.2214
	step [76/255], loss=15.2170
	step [77/255], loss=16.2672
	step [78/255], loss=15.4819
	step [79/255], loss=14.0984
	step [80/255], loss=16.9895
	step [81/255], loss=14.5345
	step [82/255], loss=14.8997
	step [83/255], loss=16.5910
	step [84/255], loss=16.0035
	step [85/255], loss=13.9237
	step [86/255], loss=17.3697
	step [87/255], loss=16.5589
	step [88/255], loss=16.4356
	step [89/255], loss=16.0316
	step [90/255], loss=16.5487
	step [91/255], loss=17.0611
	step [92/255], loss=15.9133
	step [93/255], loss=18.1281
	step [94/255], loss=16.3785
	step [95/255], loss=16.7235
	step [96/255], loss=14.9383
	step [97/255], loss=15.1572
	step [98/255], loss=14.5779
	step [99/255], loss=15.5441
	step [100/255], loss=14.2485
	step [101/255], loss=15.1405
	step [102/255], loss=15.1443
	step [103/255], loss=17.7589
	step [104/255], loss=18.4168
	step [105/255], loss=14.0223
	step [106/255], loss=14.5971
	step [107/255], loss=14.8385
	step [108/255], loss=16.3920
	step [109/255], loss=15.9522
	step [110/255], loss=16.4003
	step [111/255], loss=15.9305
	step [112/255], loss=16.1333
	step [113/255], loss=18.2273
	step [114/255], loss=15.2791
	step [115/255], loss=20.1681
	step [116/255], loss=15.5627
	step [117/255], loss=13.9185
	step [118/255], loss=13.2185
	step [119/255], loss=16.4520
	step [120/255], loss=16.2435
	step [121/255], loss=15.6836
	step [122/255], loss=13.4358
	step [123/255], loss=18.4480
	step [124/255], loss=16.5162
	step [125/255], loss=15.3027
	step [126/255], loss=15.4352
	step [127/255], loss=15.6718
	step [128/255], loss=15.7269
	step [129/255], loss=17.4489
	step [130/255], loss=16.6601
	step [131/255], loss=16.5108
	step [132/255], loss=15.0765
	step [133/255], loss=15.1395
	step [134/255], loss=13.2135
	step [135/255], loss=19.3456
	step [136/255], loss=14.8487
	step [137/255], loss=14.4711
	step [138/255], loss=16.2767
	step [139/255], loss=14.2849
	step [140/255], loss=15.2421
	step [141/255], loss=17.1745
	step [142/255], loss=17.5825
	step [143/255], loss=15.0337
	step [144/255], loss=15.6857
	step [145/255], loss=16.3063
	step [146/255], loss=16.0027
	step [147/255], loss=13.2614
	step [148/255], loss=15.1196
	step [149/255], loss=15.2463
	step [150/255], loss=14.8089
	step [151/255], loss=12.6159
	step [152/255], loss=15.7893
	step [153/255], loss=14.8003
	step [154/255], loss=12.4919
	step [155/255], loss=18.0272
	step [156/255], loss=18.1919
	step [157/255], loss=16.0619
	step [158/255], loss=14.7054
	step [159/255], loss=19.1089
	step [160/255], loss=15.4533
	step [161/255], loss=16.1351
	step [162/255], loss=15.5890
	step [163/255], loss=16.3243
	step [164/255], loss=16.4149
	step [165/255], loss=19.3331
	step [166/255], loss=15.8209
	step [167/255], loss=13.4452
	step [168/255], loss=14.5034
	step [169/255], loss=14.7804
	step [170/255], loss=17.3213
	step [171/255], loss=13.0988
	step [172/255], loss=16.8597
	step [173/255], loss=14.8302
	step [174/255], loss=13.7779
	step [175/255], loss=14.8981
	step [176/255], loss=14.0946
	step [177/255], loss=16.6640
	step [178/255], loss=13.4626
	step [179/255], loss=13.9767
	step [180/255], loss=16.1778
	step [181/255], loss=18.4077
	step [182/255], loss=15.5058
	step [183/255], loss=14.3816
	step [184/255], loss=18.0409
	step [185/255], loss=15.1251
	step [186/255], loss=15.1980
	step [187/255], loss=17.9497
	step [188/255], loss=15.2704
	step [189/255], loss=16.7865
	step [190/255], loss=15.2502
	step [191/255], loss=14.2816
	step [192/255], loss=15.6016
	step [193/255], loss=14.3271
	step [194/255], loss=15.3493
	step [195/255], loss=14.1642
	step [196/255], loss=14.7506
	step [197/255], loss=15.8384
	step [198/255], loss=15.2131
	step [199/255], loss=13.8508
	step [200/255], loss=15.0163
	step [201/255], loss=13.8062
	step [202/255], loss=16.3408
	step [203/255], loss=16.8816
	step [204/255], loss=16.4308
	step [205/255], loss=16.4542
	step [206/255], loss=15.8518
	step [207/255], loss=14.9541
	step [208/255], loss=16.2503
	step [209/255], loss=14.1965
	step [210/255], loss=13.8304
	step [211/255], loss=14.7859
	step [212/255], loss=14.9712
	step [213/255], loss=19.3789
	step [214/255], loss=14.1948
	step [215/255], loss=13.0693
	step [216/255], loss=17.0264
	step [217/255], loss=14.0371
	step [218/255], loss=13.6567
	step [219/255], loss=12.3525
	step [220/255], loss=13.6514
	step [221/255], loss=15.0903
	step [222/255], loss=20.8768
	step [223/255], loss=14.5305
	step [224/255], loss=15.5661
	step [225/255], loss=15.0793
	step [226/255], loss=13.2457
	step [227/255], loss=14.9008
	step [228/255], loss=14.2123
	step [229/255], loss=13.6917
	step [230/255], loss=13.2760
	step [231/255], loss=14.2319
	step [232/255], loss=18.0563
	step [233/255], loss=14.0559
	step [234/255], loss=19.4525
	step [235/255], loss=15.3548
	step [236/255], loss=15.9510
	step [237/255], loss=14.0463
	step [238/255], loss=16.4787
	step [239/255], loss=15.4790
	step [240/255], loss=15.7014
	step [241/255], loss=16.1173
	step [242/255], loss=14.8009
	step [243/255], loss=16.4198
	step [244/255], loss=13.1338
	step [245/255], loss=13.6569
	step [246/255], loss=15.0185
	step [247/255], loss=15.6442
	step [248/255], loss=13.7751
	step [249/255], loss=16.2340
	step [250/255], loss=16.2522
	step [251/255], loss=14.3476
	step [252/255], loss=15.0152
	step [253/255], loss=14.3206
	step [254/255], loss=13.2627
	step [255/255], loss=11.3215
	Evaluating
	loss=0.0565, precision=0.1454, recall=0.9937, f1=0.6277
Training epoch 7
	step [1/255], loss=16.7250
	step [2/255], loss=14.0050
	step [3/255], loss=13.1135
	step [4/255], loss=14.9791
	step [5/255], loss=14.6963
	step [6/255], loss=14.6639
	step [7/255], loss=17.2586
	step [8/255], loss=15.1135
	step [9/255], loss=15.6449
	step [10/255], loss=12.9942
	step [11/255], loss=15.4079
	step [12/255], loss=13.5774
	step [13/255], loss=15.5209
	step [14/255], loss=14.8104
	step [15/255], loss=14.9389
	step [16/255], loss=16.7990
	step [17/255], loss=14.4578
	step [18/255], loss=14.4225
	step [19/255], loss=14.8981
	step [20/255], loss=13.3437
	step [21/255], loss=14.4313
	step [22/255], loss=14.4047
	step [23/255], loss=14.3928
	step [24/255], loss=13.2182
	step [25/255], loss=12.8523
	step [26/255], loss=14.7317
	step [27/255], loss=14.3428
	step [28/255], loss=14.6101
	step [29/255], loss=13.6165
	step [30/255], loss=14.7285
	step [31/255], loss=15.4121
	step [32/255], loss=13.2931
	step [33/255], loss=14.6511
	step [34/255], loss=17.6903
	step [35/255], loss=14.4259
	step [36/255], loss=12.7647
	step [37/255], loss=16.2879
	step [38/255], loss=12.1380
	step [39/255], loss=17.3254
	step [40/255], loss=16.9310
	step [41/255], loss=14.7823
	step [42/255], loss=15.9550
	step [43/255], loss=14.1435
	step [44/255], loss=18.2576
	step [45/255], loss=15.2779
	step [46/255], loss=13.5398
	step [47/255], loss=13.7296
	step [48/255], loss=14.5014
	step [49/255], loss=13.3006
	step [50/255], loss=13.6616
	step [51/255], loss=13.1838
	step [52/255], loss=12.3855
	step [53/255], loss=13.3734
	step [54/255], loss=13.5207
	step [55/255], loss=12.3396
	step [56/255], loss=13.7702
	step [57/255], loss=13.1391
	step [58/255], loss=13.2326
	step [59/255], loss=14.8295
	step [60/255], loss=12.1633
	step [61/255], loss=13.0092
	step [62/255], loss=14.9622
	step [63/255], loss=13.2694
	step [64/255], loss=12.8858
	step [65/255], loss=13.9794
	step [66/255], loss=13.3658
	step [67/255], loss=14.7234
	step [68/255], loss=13.0977
	step [69/255], loss=14.4716
	step [70/255], loss=14.3059
	step [71/255], loss=13.3342
	step [72/255], loss=14.2465
	step [73/255], loss=13.3482
	step [74/255], loss=14.7625
	step [75/255], loss=15.7402
	step [76/255], loss=14.8259
	step [77/255], loss=15.2979
	step [78/255], loss=15.8363
	step [79/255], loss=15.5266
	step [80/255], loss=14.0403
	step [81/255], loss=12.4928
	step [82/255], loss=15.3831
	step [83/255], loss=12.9915
	step [84/255], loss=14.5487
	step [85/255], loss=13.5598
	step [86/255], loss=13.1364
	step [87/255], loss=13.3303
	step [88/255], loss=12.3567
	step [89/255], loss=14.0768
	step [90/255], loss=15.6237
	step [91/255], loss=12.9660
	step [92/255], loss=15.5213
	step [93/255], loss=16.3022
	step [94/255], loss=14.5688
	step [95/255], loss=16.6030
	step [96/255], loss=13.8163
	step [97/255], loss=12.9190
	step [98/255], loss=16.6813
	step [99/255], loss=14.6113
	step [100/255], loss=17.2496
	step [101/255], loss=12.4712
	step [102/255], loss=13.3599
	step [103/255], loss=15.4983
	step [104/255], loss=12.8251
	step [105/255], loss=14.8429
	step [106/255], loss=11.1569
	step [107/255], loss=15.4776
	step [108/255], loss=16.2933
	step [109/255], loss=15.5034
	step [110/255], loss=14.5311
	step [111/255], loss=13.5610
	step [112/255], loss=13.2185
	step [113/255], loss=13.0140
	step [114/255], loss=13.4869
	step [115/255], loss=15.0409
	step [116/255], loss=12.8838
	step [117/255], loss=14.0889
	step [118/255], loss=14.0195
	step [119/255], loss=16.2629
	step [120/255], loss=14.8576
	step [121/255], loss=13.8194
	step [122/255], loss=15.6306
	step [123/255], loss=14.0966
	step [124/255], loss=15.3898
	step [125/255], loss=15.4149
	step [126/255], loss=14.1247
	step [127/255], loss=16.3912
	step [128/255], loss=13.5366
	step [129/255], loss=13.1623
	step [130/255], loss=15.6005
	step [131/255], loss=16.3561
	step [132/255], loss=12.8325
	step [133/255], loss=12.6790
	step [134/255], loss=13.4873
	step [135/255], loss=12.2272
	step [136/255], loss=11.5249
	step [137/255], loss=13.2005
	step [138/255], loss=13.3650
	step [139/255], loss=15.1715
	step [140/255], loss=16.3111
	step [141/255], loss=16.0569
	step [142/255], loss=15.7852
	step [143/255], loss=14.4672
	step [144/255], loss=14.0479
	step [145/255], loss=13.3072
	step [146/255], loss=13.8290
	step [147/255], loss=13.6414
	step [148/255], loss=13.4828
	step [149/255], loss=16.1065
	step [150/255], loss=13.4944
	step [151/255], loss=11.8505
	step [152/255], loss=14.3873
	step [153/255], loss=14.4602
	step [154/255], loss=13.8160
	step [155/255], loss=15.2763
	step [156/255], loss=14.6608
	step [157/255], loss=15.8485
	step [158/255], loss=16.1858
	step [159/255], loss=12.4643
	step [160/255], loss=12.4795
	step [161/255], loss=13.6731
	step [162/255], loss=13.5811
	step [163/255], loss=11.5015
	step [164/255], loss=14.1212
	step [165/255], loss=14.4048
	step [166/255], loss=13.7068
	step [167/255], loss=15.7521
	step [168/255], loss=16.6178
	step [169/255], loss=13.9579
	step [170/255], loss=12.7639
	step [171/255], loss=13.1729
	step [172/255], loss=13.3903
	step [173/255], loss=14.8358
	step [174/255], loss=14.7720
	step [175/255], loss=15.1094
	step [176/255], loss=13.5617
	step [177/255], loss=12.0898
	step [178/255], loss=15.5620
	step [179/255], loss=13.8065
	step [180/255], loss=13.4263
	step [181/255], loss=12.1619
	step [182/255], loss=14.8347
	step [183/255], loss=12.4900
	step [184/255], loss=14.5595
	step [185/255], loss=12.8167
	step [186/255], loss=14.9071
	step [187/255], loss=14.1426
	step [188/255], loss=14.9397
	step [189/255], loss=15.6884
	step [190/255], loss=12.1483
	step [191/255], loss=14.0117
	step [192/255], loss=15.3639
	step [193/255], loss=13.4083
	step [194/255], loss=14.8129
	step [195/255], loss=13.4940
	step [196/255], loss=13.2916
	step [197/255], loss=13.7582
	step [198/255], loss=14.3712
	step [199/255], loss=13.4959
	step [200/255], loss=14.0112
	step [201/255], loss=14.8100
	step [202/255], loss=12.6160
	step [203/255], loss=16.4833
	step [204/255], loss=15.0853
	step [205/255], loss=13.3693
	step [206/255], loss=12.2263
	step [207/255], loss=13.7298
	step [208/255], loss=12.9014
	step [209/255], loss=11.4312
	step [210/255], loss=16.0365
	step [211/255], loss=12.8509
	step [212/255], loss=16.3372
	step [213/255], loss=13.1678
	step [214/255], loss=11.1490
	step [215/255], loss=13.2683
	step [216/255], loss=13.5910
	step [217/255], loss=10.2183
	step [218/255], loss=15.1836
	step [219/255], loss=10.8514
	step [220/255], loss=11.2106
	step [221/255], loss=13.2428
	step [222/255], loss=13.0300
	step [223/255], loss=12.1508
	step [224/255], loss=12.4106
	step [225/255], loss=13.0509
	step [226/255], loss=10.8497
	step [227/255], loss=11.8824
	step [228/255], loss=16.2327
	step [229/255], loss=10.7480
	step [230/255], loss=16.2440
	step [231/255], loss=13.3915
	step [232/255], loss=15.4391
	step [233/255], loss=12.9600
	step [234/255], loss=12.4485
	step [235/255], loss=13.9539
	step [236/255], loss=15.0487
	step [237/255], loss=15.2055
	step [238/255], loss=13.7551
	step [239/255], loss=14.5989
	step [240/255], loss=13.4552
	step [241/255], loss=12.5118
	step [242/255], loss=12.4935
	step [243/255], loss=11.1128
	step [244/255], loss=13.8046
	step [245/255], loss=14.1098
	step [246/255], loss=12.7057
	step [247/255], loss=15.0013
	step [248/255], loss=15.8270
	step [249/255], loss=14.4163
	step [250/255], loss=12.4752
	step [251/255], loss=12.7584
	step [252/255], loss=12.5403
	step [253/255], loss=13.8910
	step [254/255], loss=13.4793
	step [255/255], loss=8.8741
	Evaluating
	loss=0.0476, precision=0.1491, recall=0.9935, f1=0.6343
Training epoch 8
	step [1/255], loss=15.1089
	step [2/255], loss=12.3782
	step [3/255], loss=13.3938
	step [4/255], loss=12.2777
	step [5/255], loss=14.4266
	step [6/255], loss=13.1192
	step [7/255], loss=12.4163
	step [8/255], loss=14.1883
	step [9/255], loss=12.7424
	step [10/255], loss=12.5151
	step [11/255], loss=12.7186
	step [12/255], loss=12.3662
	step [13/255], loss=11.8156
	step [14/255], loss=13.2521
	step [15/255], loss=12.9120
	step [16/255], loss=12.3466
	step [17/255], loss=11.9909
	step [18/255], loss=17.5131
	step [19/255], loss=11.5703
	step [20/255], loss=14.6129
	step [21/255], loss=13.0157
	step [22/255], loss=12.7919
	step [23/255], loss=15.6560
	step [24/255], loss=11.4276
	step [25/255], loss=13.1031
	step [26/255], loss=12.0875
	step [27/255], loss=13.4384
	step [28/255], loss=13.7516
	step [29/255], loss=14.9530
	step [30/255], loss=12.7143
	step [31/255], loss=11.4612
	step [32/255], loss=13.6799
	step [33/255], loss=11.3548
	step [34/255], loss=15.1904
	step [35/255], loss=12.6266
	step [36/255], loss=11.6875
	step [37/255], loss=12.3504
	step [38/255], loss=14.4671
	step [39/255], loss=14.1043
	step [40/255], loss=13.3637
	step [41/255], loss=13.4494
	step [42/255], loss=14.5766
	step [43/255], loss=12.6293
	step [44/255], loss=12.5489
	step [45/255], loss=12.1379
	step [46/255], loss=11.7603
	step [47/255], loss=12.0956
	step [48/255], loss=13.1529
	step [49/255], loss=14.8890
	step [50/255], loss=13.1813
	step [51/255], loss=13.7247
	step [52/255], loss=14.2709
	step [53/255], loss=12.6525
	step [54/255], loss=11.2703
	step [55/255], loss=11.1649
	step [56/255], loss=13.7180
	step [57/255], loss=12.8218
	step [58/255], loss=14.6345
	step [59/255], loss=14.5534
	step [60/255], loss=11.0175
	step [61/255], loss=13.3394
	step [62/255], loss=11.4420
	step [63/255], loss=15.3569
	step [64/255], loss=13.5135
	step [65/255], loss=12.6361
	step [66/255], loss=13.3004
	step [67/255], loss=16.8473
	step [68/255], loss=12.5003
	step [69/255], loss=11.7034
	step [70/255], loss=12.5301
	step [71/255], loss=11.6335
	step [72/255], loss=17.0642
	step [73/255], loss=12.7341
	step [74/255], loss=13.5468
	step [75/255], loss=12.9167
	step [76/255], loss=12.5017
	step [77/255], loss=10.9782
	step [78/255], loss=14.2968
	step [79/255], loss=12.9663
	step [80/255], loss=13.8249
	step [81/255], loss=11.2766
	step [82/255], loss=13.2471
	step [83/255], loss=10.6059
	step [84/255], loss=13.2332
	step [85/255], loss=11.2504
	step [86/255], loss=13.9436
	step [87/255], loss=13.8196
	step [88/255], loss=14.4305
	step [89/255], loss=11.0960
	step [90/255], loss=11.7640
	step [91/255], loss=14.2713
	step [92/255], loss=14.2745
	step [93/255], loss=14.0802
	step [94/255], loss=14.8501
	step [95/255], loss=13.3342
	step [96/255], loss=15.2166
	step [97/255], loss=13.1900
	step [98/255], loss=12.6464
	step [99/255], loss=12.3755
	step [100/255], loss=11.5334
	step [101/255], loss=12.9152
	step [102/255], loss=10.4847
	step [103/255], loss=11.9821
	step [104/255], loss=10.6226
	step [105/255], loss=14.1313
	step [106/255], loss=12.1645
	step [107/255], loss=17.0456
	step [108/255], loss=11.4074
	step [109/255], loss=12.6551
	step [110/255], loss=13.6956
	step [111/255], loss=15.4921
	step [112/255], loss=11.7070
	step [113/255], loss=12.5294
	step [114/255], loss=13.7516
	step [115/255], loss=12.7358
	step [116/255], loss=12.4589
	step [117/255], loss=13.9577
	step [118/255], loss=11.0963
	step [119/255], loss=10.4475
	step [120/255], loss=13.7511
	step [121/255], loss=11.7829
	step [122/255], loss=14.5139
	step [123/255], loss=16.9242
	step [124/255], loss=14.7594
	step [125/255], loss=13.1427
	step [126/255], loss=11.3224
	step [127/255], loss=16.3324
	step [128/255], loss=12.1055
	step [129/255], loss=12.9684
	step [130/255], loss=13.8492
	step [131/255], loss=11.9546
	step [132/255], loss=13.8134
	step [133/255], loss=14.4860
	step [134/255], loss=11.5060
	step [135/255], loss=10.4555
	step [136/255], loss=11.4628
	step [137/255], loss=12.3196
	step [138/255], loss=11.9942
	step [139/255], loss=13.2229
	step [140/255], loss=13.4095
	step [141/255], loss=12.3203
	step [142/255], loss=13.1368
	step [143/255], loss=11.6941
	step [144/255], loss=10.8521
	step [145/255], loss=13.7892
	step [146/255], loss=13.5394
	step [147/255], loss=11.3158
	step [148/255], loss=10.5873
	step [149/255], loss=9.5049
	step [150/255], loss=10.5287
	step [151/255], loss=13.0107
	step [152/255], loss=12.7454
	step [153/255], loss=11.7590
	step [154/255], loss=10.5183
	step [155/255], loss=13.4140
	step [156/255], loss=13.0287
	step [157/255], loss=13.0403
	step [158/255], loss=10.1618
	step [159/255], loss=11.7360
	step [160/255], loss=12.5661
	step [161/255], loss=10.5790
	step [162/255], loss=10.4461
	step [163/255], loss=13.2993
	step [164/255], loss=13.1994
	step [165/255], loss=15.2480
	step [166/255], loss=10.6229
	step [167/255], loss=15.4764
	step [168/255], loss=12.2563
	step [169/255], loss=11.4454
	step [170/255], loss=12.9485
	step [171/255], loss=10.8691
	step [172/255], loss=11.1236
	step [173/255], loss=12.2212
	step [174/255], loss=14.4939
	step [175/255], loss=10.4378
	step [176/255], loss=13.8331
	step [177/255], loss=15.5424
	step [178/255], loss=14.3477
	step [179/255], loss=12.6534
	step [180/255], loss=15.1977
	step [181/255], loss=12.5092
	step [182/255], loss=14.5155
	step [183/255], loss=14.4546
	step [184/255], loss=12.6655
	step [185/255], loss=13.7996
	step [186/255], loss=13.7936
	step [187/255], loss=11.7320
	step [188/255], loss=13.3395
	step [189/255], loss=10.8068
	step [190/255], loss=13.1422
	step [191/255], loss=16.9583
	step [192/255], loss=11.3866
	step [193/255], loss=14.3209
	step [194/255], loss=14.1142
	step [195/255], loss=11.4341
	step [196/255], loss=13.2241
	step [197/255], loss=13.4532
	step [198/255], loss=12.6232
	step [199/255], loss=13.4206
	step [200/255], loss=12.2442
	step [201/255], loss=12.6901
	step [202/255], loss=15.1573
	step [203/255], loss=14.1062
	step [204/255], loss=15.3854
	step [205/255], loss=14.4598
	step [206/255], loss=11.1485
	step [207/255], loss=11.4998
	step [208/255], loss=11.6487
	step [209/255], loss=14.7431
	step [210/255], loss=10.6465
	step [211/255], loss=14.2034
	step [212/255], loss=10.8995
	step [213/255], loss=11.3612
	step [214/255], loss=12.1191
	step [215/255], loss=13.2700
	step [216/255], loss=13.3327
	step [217/255], loss=11.0957
	step [218/255], loss=11.5439
	step [219/255], loss=11.7012
	step [220/255], loss=13.9752
	step [221/255], loss=11.6298
	step [222/255], loss=11.3346
	step [223/255], loss=11.0377
	step [224/255], loss=12.6400
	step [225/255], loss=13.8615
	step [226/255], loss=13.5742
	step [227/255], loss=13.5611
	step [228/255], loss=12.2401
	step [229/255], loss=12.3907
	step [230/255], loss=12.1734
	step [231/255], loss=12.4186
	step [232/255], loss=13.3545
	step [233/255], loss=12.5290
	step [234/255], loss=12.2330
	step [235/255], loss=13.0495
	step [236/255], loss=11.7067
	step [237/255], loss=11.9510
	step [238/255], loss=11.1971
	step [239/255], loss=11.1986
	step [240/255], loss=11.1622
	step [241/255], loss=10.8863
	step [242/255], loss=11.8126
	step [243/255], loss=14.1968
	step [244/255], loss=12.0682
	step [245/255], loss=13.1733
	step [246/255], loss=13.5882
	step [247/255], loss=11.0988
	step [248/255], loss=11.6145
	step [249/255], loss=13.1475
	step [250/255], loss=12.1018
	step [251/255], loss=13.2716
	step [252/255], loss=11.6762
	step [253/255], loss=10.1014
	step [254/255], loss=11.2470
	step [255/255], loss=9.0890
	Evaluating
	loss=0.0391, precision=0.1876, recall=0.9919, f1=0.6943
saving model as: 0_saved_model.pth
Training epoch 9
	step [1/255], loss=12.3326
	step [2/255], loss=11.4346
	step [3/255], loss=12.3347
	step [4/255], loss=11.6210
	step [5/255], loss=12.9524
	step [6/255], loss=12.0176
	step [7/255], loss=12.3244
	step [8/255], loss=12.1584
	step [9/255], loss=12.0100
	step [10/255], loss=10.7170
	step [11/255], loss=10.2119
	step [12/255], loss=13.7586
	step [13/255], loss=11.0338
	step [14/255], loss=9.1741
	step [15/255], loss=10.6715
	step [16/255], loss=12.1162
	step [17/255], loss=10.9565
	step [18/255], loss=13.4407
	step [19/255], loss=11.0943
	step [20/255], loss=12.5563
	step [21/255], loss=17.3298
	step [22/255], loss=10.9171
	step [23/255], loss=11.4402
	step [24/255], loss=13.9548
	step [25/255], loss=14.0345
	step [26/255], loss=12.5191
	step [27/255], loss=13.0935
	step [28/255], loss=10.3617
	step [29/255], loss=13.1959
	step [30/255], loss=13.2198
	step [31/255], loss=11.8830
	step [32/255], loss=11.4137
	step [33/255], loss=13.7204
	step [34/255], loss=12.2129
	step [35/255], loss=11.4887
	step [36/255], loss=11.7927
	step [37/255], loss=11.5255
	step [38/255], loss=11.3064
	step [39/255], loss=11.4710
	step [40/255], loss=10.6736
	step [41/255], loss=13.7087
	step [42/255], loss=10.3726
	step [43/255], loss=11.1714
	step [44/255], loss=12.2357
	step [45/255], loss=12.3135
	step [46/255], loss=13.7362
	step [47/255], loss=11.0046
	step [48/255], loss=13.6646
	step [49/255], loss=11.4168
	step [50/255], loss=10.8788
	step [51/255], loss=12.7299
	step [52/255], loss=11.5919
	step [53/255], loss=12.8899
	step [54/255], loss=12.8199
	step [55/255], loss=11.1718
	step [56/255], loss=12.1829
	step [57/255], loss=11.1487
	step [58/255], loss=12.3491
	step [59/255], loss=11.9284
	step [60/255], loss=11.3520
	step [61/255], loss=10.1874
	step [62/255], loss=12.1507
	step [63/255], loss=11.2198
	step [64/255], loss=12.0670
	step [65/255], loss=10.3263
	step [66/255], loss=9.9597
	step [67/255], loss=12.4046
	step [68/255], loss=11.0772
	step [69/255], loss=13.5163
	step [70/255], loss=13.9301
	step [71/255], loss=14.4481
	step [72/255], loss=12.4088
	step [73/255], loss=10.8918
	step [74/255], loss=11.4396
	step [75/255], loss=14.9219
	step [76/255], loss=12.4514
	step [77/255], loss=11.3500
	step [78/255], loss=10.3128
	step [79/255], loss=12.6103
	step [80/255], loss=14.9651
	step [81/255], loss=9.9410
	step [82/255], loss=10.0135
	step [83/255], loss=9.0949
	step [84/255], loss=12.2081
	step [85/255], loss=16.4959
	step [86/255], loss=12.7115
	step [87/255], loss=11.1308
	step [88/255], loss=11.2968
	step [89/255], loss=11.9100
	step [90/255], loss=12.2242
	step [91/255], loss=13.7835
	step [92/255], loss=11.6703
	step [93/255], loss=14.0890
	step [94/255], loss=12.7311
	step [95/255], loss=10.1323
	step [96/255], loss=11.2976
	step [97/255], loss=11.2779
	step [98/255], loss=13.4823
	step [99/255], loss=11.2056
	step [100/255], loss=15.8490
	step [101/255], loss=9.1825
	step [102/255], loss=11.5159
	step [103/255], loss=10.3344
	step [104/255], loss=11.5193
	step [105/255], loss=11.5742
	step [106/255], loss=13.1832
	step [107/255], loss=13.4205
	step [108/255], loss=11.6551
	step [109/255], loss=11.8584
	step [110/255], loss=9.8284
	step [111/255], loss=11.1119
	step [112/255], loss=13.0271
	step [113/255], loss=11.2341
	step [114/255], loss=11.9046
	step [115/255], loss=10.9907
	step [116/255], loss=12.1850
	step [117/255], loss=12.5050
	step [118/255], loss=9.6352
	step [119/255], loss=12.0719
	step [120/255], loss=12.5122
	step [121/255], loss=12.7383
	step [122/255], loss=11.2538
	step [123/255], loss=12.2917
	step [124/255], loss=11.9591
	step [125/255], loss=13.4433
	step [126/255], loss=12.1463
	step [127/255], loss=10.2382
	step [128/255], loss=11.9625
	step [129/255], loss=12.0126
	step [130/255], loss=12.2025
	step [131/255], loss=11.4265
	step [132/255], loss=9.5902
	step [133/255], loss=10.8924
	step [134/255], loss=12.2683
	step [135/255], loss=12.7842
	step [136/255], loss=10.3717
	step [137/255], loss=12.1495
	step [138/255], loss=9.4485
	step [139/255], loss=12.0011
	step [140/255], loss=11.0022
	step [141/255], loss=11.1106
	step [142/255], loss=10.2481
	step [143/255], loss=13.2636
	step [144/255], loss=11.7262
	step [145/255], loss=10.8248
	step [146/255], loss=12.0123
	step [147/255], loss=10.9068
	step [148/255], loss=10.2910
	step [149/255], loss=11.3956
	step [150/255], loss=11.4792
	step [151/255], loss=11.3524
	step [152/255], loss=12.9332
	step [153/255], loss=11.8440
	step [154/255], loss=11.2112
	step [155/255], loss=12.0544
	step [156/255], loss=15.4800
	step [157/255], loss=12.6830
	step [158/255], loss=10.5058
	step [159/255], loss=10.4711
	step [160/255], loss=14.4090
	step [161/255], loss=11.3362
	step [162/255], loss=11.4641
	step [163/255], loss=10.3184
	step [164/255], loss=11.4922
	step [165/255], loss=13.8883
	step [166/255], loss=13.3815
	step [167/255], loss=10.8495
	step [168/255], loss=11.4811
	step [169/255], loss=10.8485
	step [170/255], loss=12.0983
	step [171/255], loss=12.9543
	step [172/255], loss=11.2746
	step [173/255], loss=11.3644
	step [174/255], loss=15.6547
	step [175/255], loss=12.9326
	step [176/255], loss=10.4990
	step [177/255], loss=9.4258
	step [178/255], loss=8.6994
	step [179/255], loss=9.9907
	step [180/255], loss=11.3359
	step [181/255], loss=12.9386
	step [182/255], loss=10.0048
	step [183/255], loss=10.4147
	step [184/255], loss=11.0231
	step [185/255], loss=8.6455
	step [186/255], loss=10.9731
	step [187/255], loss=10.8403
	step [188/255], loss=9.5768
	step [189/255], loss=10.0443
	step [190/255], loss=9.7452
	step [191/255], loss=11.0435
	step [192/255], loss=9.9847
	step [193/255], loss=12.4499
	step [194/255], loss=11.7270
	step [195/255], loss=11.6066
	step [196/255], loss=10.0445
	step [197/255], loss=11.2067
	step [198/255], loss=12.1180
	step [199/255], loss=14.3195
	step [200/255], loss=13.2205
	step [201/255], loss=12.7422
	step [202/255], loss=10.3055
	step [203/255], loss=11.2095
	step [204/255], loss=11.5171
	step [205/255], loss=9.1220
	step [206/255], loss=12.4089
	step [207/255], loss=12.7936
	step [208/255], loss=11.0974
	step [209/255], loss=10.3871
	step [210/255], loss=11.0390
	step [211/255], loss=11.6431
	step [212/255], loss=12.7630
	step [213/255], loss=11.7289
	step [214/255], loss=14.0242
	step [215/255], loss=11.9609
	step [216/255], loss=11.8613
	step [217/255], loss=11.1281
	step [218/255], loss=12.1247
	step [219/255], loss=10.4962
	step [220/255], loss=9.8909
	step [221/255], loss=11.5377
	step [222/255], loss=13.4163
	step [223/255], loss=12.5577
	step [224/255], loss=10.7697
	step [225/255], loss=11.9994
	step [226/255], loss=12.7533
	step [227/255], loss=11.5071
	step [228/255], loss=10.1871
	step [229/255], loss=10.4715
	step [230/255], loss=13.1164
	step [231/255], loss=10.8849
	step [232/255], loss=14.0586
	step [233/255], loss=10.6929
	step [234/255], loss=12.2590
	step [235/255], loss=13.1050
	step [236/255], loss=11.8847
	step [237/255], loss=11.3679
	step [238/255], loss=10.5042
	step [239/255], loss=11.4578
	step [240/255], loss=12.9711
	step [241/255], loss=13.2215
	step [242/255], loss=9.0007
	step [243/255], loss=10.3994
	step [244/255], loss=10.5286
	step [245/255], loss=11.6505
	step [246/255], loss=12.5582
	step [247/255], loss=8.9500
	step [248/255], loss=11.9412
	step [249/255], loss=10.0921
	step [250/255], loss=14.2340
	step [251/255], loss=11.7537
	step [252/255], loss=13.1365
	step [253/255], loss=12.1197
	step [254/255], loss=10.1842
	step [255/255], loss=11.4966
	Evaluating
	loss=0.0444, precision=0.1354, recall=0.9932, f1=0.6080
Training epoch 10
	step [1/255], loss=9.7873
	step [2/255], loss=11.5581
	step [3/255], loss=11.2457
	step [4/255], loss=12.4764
	step [5/255], loss=10.9483
	step [6/255], loss=12.3738
	step [7/255], loss=12.3316
	step [8/255], loss=10.6555
	step [9/255], loss=8.6471
	step [10/255], loss=12.2307
	step [11/255], loss=11.7388
	step [12/255], loss=11.9646
	step [13/255], loss=13.4995
	step [14/255], loss=12.8566
	step [15/255], loss=11.6308
	step [16/255], loss=9.9603
	step [17/255], loss=11.1542
	step [18/255], loss=11.5772
	step [19/255], loss=11.2985
	step [20/255], loss=11.1795
	step [21/255], loss=12.6307
	step [22/255], loss=12.2621
	step [23/255], loss=9.7670
	step [24/255], loss=14.1625
	step [25/255], loss=9.3377
	step [26/255], loss=10.0614
	step [27/255], loss=11.9928
	step [28/255], loss=8.6577
	step [29/255], loss=13.8524
	step [30/255], loss=13.2423
	step [31/255], loss=10.7362
	step [32/255], loss=12.2781
	step [33/255], loss=10.4685
	step [34/255], loss=12.4176
	step [35/255], loss=10.9568
	step [36/255], loss=13.4205
	step [37/255], loss=11.5239
	step [38/255], loss=10.7103
	step [39/255], loss=11.0431
	step [40/255], loss=11.3843
	step [41/255], loss=11.4884
	step [42/255], loss=11.3403
	step [43/255], loss=11.9605
	step [44/255], loss=11.7490
	step [45/255], loss=10.7954
	step [46/255], loss=10.2828
	step [47/255], loss=10.2320
	step [48/255], loss=11.4642
	step [49/255], loss=10.5948
	step [50/255], loss=11.0759
	step [51/255], loss=8.8338
	step [52/255], loss=11.5167
	step [53/255], loss=10.2411
	step [54/255], loss=9.7963
	step [55/255], loss=11.0379
	step [56/255], loss=12.5081
	step [57/255], loss=10.2941
	step [58/255], loss=10.4125
	step [59/255], loss=11.1793
	step [60/255], loss=10.7952
	step [61/255], loss=9.9630
	step [62/255], loss=11.0901
	step [63/255], loss=9.7719
	step [64/255], loss=12.7311
	step [65/255], loss=13.4731
	step [66/255], loss=11.3087
	step [67/255], loss=12.1954
	step [68/255], loss=9.1747
	step [69/255], loss=11.7807
	step [70/255], loss=11.2886
	step [71/255], loss=10.9497
	step [72/255], loss=10.4518
	step [73/255], loss=11.4792
	step [74/255], loss=11.2738
	step [75/255], loss=9.5921
	step [76/255], loss=8.9707
	step [77/255], loss=11.8155
	step [78/255], loss=13.7836
	step [79/255], loss=11.2642
	step [80/255], loss=9.9513
	step [81/255], loss=9.4059
	step [82/255], loss=10.3239
	step [83/255], loss=10.4643
	step [84/255], loss=10.8508
	step [85/255], loss=11.1712
	step [86/255], loss=11.2397
	step [87/255], loss=10.9104
	step [88/255], loss=11.5261
	step [89/255], loss=10.0718
	step [90/255], loss=12.5908
	step [91/255], loss=11.2507
	step [92/255], loss=8.8607
	step [93/255], loss=10.1206
	step [94/255], loss=10.1912
	step [95/255], loss=10.6128
	step [96/255], loss=12.6381
	step [97/255], loss=11.4828
	step [98/255], loss=9.1903
	step [99/255], loss=10.3060
	step [100/255], loss=13.3820
	step [101/255], loss=12.9510
	step [102/255], loss=13.2050
	step [103/255], loss=11.4721
	step [104/255], loss=11.9203
	step [105/255], loss=11.0440
	step [106/255], loss=12.3641
	step [107/255], loss=11.6727
	step [108/255], loss=10.9140
	step [109/255], loss=12.3501
	step [110/255], loss=11.1765
	step [111/255], loss=10.0429
	step [112/255], loss=10.8042
	step [113/255], loss=9.0270
	step [114/255], loss=10.5343
	step [115/255], loss=9.6547
	step [116/255], loss=13.1675
	step [117/255], loss=12.8949
	step [118/255], loss=13.3192
	step [119/255], loss=11.0669
	step [120/255], loss=11.8342
	step [121/255], loss=10.6331
	step [122/255], loss=10.2478
	step [123/255], loss=10.6752
	step [124/255], loss=10.8137
	step [125/255], loss=9.4680
	step [126/255], loss=8.8707
	step [127/255], loss=12.0559
	step [128/255], loss=11.0842
	step [129/255], loss=13.0810
	step [130/255], loss=10.7205
	step [131/255], loss=11.0714
	step [132/255], loss=11.8777
	step [133/255], loss=11.1514
	step [134/255], loss=12.3153
	step [135/255], loss=12.3102
	step [136/255], loss=9.7853
	step [137/255], loss=10.6974
	step [138/255], loss=10.4371
	step [139/255], loss=11.4658
	step [140/255], loss=11.1463
	step [141/255], loss=9.4629
	step [142/255], loss=9.9033
	step [143/255], loss=9.4202
	step [144/255], loss=10.0586
	step [145/255], loss=11.6699
	step [146/255], loss=12.9622
	step [147/255], loss=10.8109
	step [148/255], loss=9.6607
	step [149/255], loss=10.2624
	step [150/255], loss=11.5797
	step [151/255], loss=12.6675
	step [152/255], loss=11.5144
	step [153/255], loss=10.6628
	step [154/255], loss=11.6474
	step [155/255], loss=11.1864
	step [156/255], loss=8.8134
	step [157/255], loss=11.4804
	step [158/255], loss=9.3591
	step [159/255], loss=10.1640
	step [160/255], loss=9.1893
	step [161/255], loss=9.5926
	step [162/255], loss=9.8465
	step [163/255], loss=10.5383
	step [164/255], loss=10.8560
	step [165/255], loss=12.9392
	step [166/255], loss=9.8975
	step [167/255], loss=10.6439
	step [168/255], loss=9.5000
	step [169/255], loss=10.0484
	step [170/255], loss=12.1044
	step [171/255], loss=9.6240
	step [172/255], loss=10.7790
	step [173/255], loss=10.4381
	step [174/255], loss=11.2939
	step [175/255], loss=10.6416
	step [176/255], loss=10.8377
	step [177/255], loss=11.3403
	step [178/255], loss=11.8756
	step [179/255], loss=11.0829
	step [180/255], loss=11.9014
	step [181/255], loss=10.5730
	step [182/255], loss=11.3602
	step [183/255], loss=10.1414
	step [184/255], loss=10.5253
	step [185/255], loss=9.8849
	step [186/255], loss=12.1223
	step [187/255], loss=9.5749
	step [188/255], loss=12.2640
	step [189/255], loss=8.7470
	step [190/255], loss=10.6853
	step [191/255], loss=9.8928
	step [192/255], loss=11.7785
	step [193/255], loss=10.4850
	step [194/255], loss=10.0041
	step [195/255], loss=10.3226
	step [196/255], loss=10.6982
	step [197/255], loss=10.4451
	step [198/255], loss=11.3916
	step [199/255], loss=8.9396
	step [200/255], loss=13.0702
	step [201/255], loss=10.0149
	step [202/255], loss=11.8022
	step [203/255], loss=9.8044
	step [204/255], loss=12.8015
	step [205/255], loss=11.3648
	step [206/255], loss=9.6564
	step [207/255], loss=11.8799
	step [208/255], loss=12.1358
	step [209/255], loss=11.2349
	step [210/255], loss=9.9706
	step [211/255], loss=10.5209
	step [212/255], loss=10.1845
	step [213/255], loss=10.2680
	step [214/255], loss=10.3381
	step [215/255], loss=11.3477
	step [216/255], loss=11.5930
	step [217/255], loss=9.2750
	step [218/255], loss=10.5852
	step [219/255], loss=9.8984
	step [220/255], loss=12.8566
	step [221/255], loss=11.4658
	step [222/255], loss=10.1007
	step [223/255], loss=12.4183
	step [224/255], loss=9.9675
	step [225/255], loss=9.0814
	step [226/255], loss=14.9508
	step [227/255], loss=12.3831
	step [228/255], loss=13.0250
	step [229/255], loss=13.6622
	step [230/255], loss=11.0927
	step [231/255], loss=10.3373
	step [232/255], loss=10.1389
	step [233/255], loss=10.8590
	step [234/255], loss=8.8567
	step [235/255], loss=14.4046
	step [236/255], loss=11.0486
	step [237/255], loss=9.7114
	step [238/255], loss=10.9157
	step [239/255], loss=11.0268
	step [240/255], loss=13.9187
	step [241/255], loss=11.5944
	step [242/255], loss=10.6798
	step [243/255], loss=9.2975
	step [244/255], loss=11.5034
	step [245/255], loss=10.8193
	step [246/255], loss=11.0332
	step [247/255], loss=11.5604
	step [248/255], loss=13.5393
	step [249/255], loss=9.7296
	step [250/255], loss=11.3601
	step [251/255], loss=13.1489
	step [252/255], loss=9.8441
	step [253/255], loss=11.9269
	step [254/255], loss=10.9978
	step [255/255], loss=7.2538
	Evaluating
	loss=0.0400, precision=0.1322, recall=0.9945, f1=0.6019
Training epoch 11
	step [1/255], loss=13.0250
	step [2/255], loss=11.3830
	step [3/255], loss=9.9495
	step [4/255], loss=9.4826
	step [5/255], loss=10.3711
	step [6/255], loss=11.2064
	step [7/255], loss=9.5195
	step [8/255], loss=9.0741
	step [9/255], loss=11.4587
	step [10/255], loss=11.8257
	step [11/255], loss=11.0304
	step [12/255], loss=10.2666
	step [13/255], loss=9.7865
	step [14/255], loss=10.1441
	step [15/255], loss=8.2350
	step [16/255], loss=10.1478
	step [17/255], loss=9.4651
	step [18/255], loss=11.4006
	step [19/255], loss=11.7483
	step [20/255], loss=12.1916
	step [21/255], loss=10.2571
	step [22/255], loss=8.5872
	step [23/255], loss=10.2349
	step [24/255], loss=11.6800
	step [25/255], loss=11.3337
	step [26/255], loss=9.9779
	step [27/255], loss=8.9734
	step [28/255], loss=10.3296
	step [29/255], loss=8.8703
	step [30/255], loss=11.1584
	step [31/255], loss=10.0825
	step [32/255], loss=10.1028
	step [33/255], loss=9.9573
	step [34/255], loss=12.3536
	step [35/255], loss=9.2654
	step [36/255], loss=10.8664
	step [37/255], loss=10.5757
	step [38/255], loss=10.1554
	step [39/255], loss=10.1250
	step [40/255], loss=9.6715
	step [41/255], loss=11.4996
	step [42/255], loss=10.0978
	step [43/255], loss=10.4091
	step [44/255], loss=9.7897
	step [45/255], loss=10.3976
	step [46/255], loss=7.9108
	step [47/255], loss=12.0533
	step [48/255], loss=10.3570
	step [49/255], loss=11.1838
	step [50/255], loss=10.3356
	step [51/255], loss=10.4918
	step [52/255], loss=9.5788
	step [53/255], loss=9.7016
	step [54/255], loss=11.1691
	step [55/255], loss=11.2253
	step [56/255], loss=8.9546
	step [57/255], loss=10.9315
	step [58/255], loss=10.1214
	step [59/255], loss=11.0079
	step [60/255], loss=8.2155
	step [61/255], loss=10.5729
	step [62/255], loss=13.3411
	step [63/255], loss=11.2102
	step [64/255], loss=11.3572
	step [65/255], loss=10.1953
	step [66/255], loss=11.3086
	step [67/255], loss=10.1879
	step [68/255], loss=12.1177
	step [69/255], loss=10.7279
	step [70/255], loss=11.0060
	step [71/255], loss=12.4330
	step [72/255], loss=10.3374
	step [73/255], loss=12.8255
	step [74/255], loss=10.7105
	step [75/255], loss=12.8448
	step [76/255], loss=10.3605
	step [77/255], loss=8.6690
	step [78/255], loss=10.2754
	step [79/255], loss=8.1690
	step [80/255], loss=12.9316
	step [81/255], loss=16.4611
	step [82/255], loss=9.4504
	step [83/255], loss=12.4873
	step [84/255], loss=8.9925
	step [85/255], loss=10.9534
	step [86/255], loss=9.8415
	step [87/255], loss=12.8088
	step [88/255], loss=10.6126
	step [89/255], loss=11.4242
	step [90/255], loss=12.2891
	step [91/255], loss=11.4040
	step [92/255], loss=9.1356
	step [93/255], loss=9.7425
	step [94/255], loss=11.2049
	step [95/255], loss=10.9649
	step [96/255], loss=9.4711
	step [97/255], loss=9.5524
	step [98/255], loss=12.5011
	step [99/255], loss=10.2338
	step [100/255], loss=9.8165
	step [101/255], loss=10.1790
	step [102/255], loss=11.0302
	step [103/255], loss=9.8642
	step [104/255], loss=9.2783
	step [105/255], loss=10.3347
	step [106/255], loss=9.8890
	step [107/255], loss=11.8806
	step [108/255], loss=9.9554
	step [109/255], loss=12.5805
	step [110/255], loss=12.8382
	step [111/255], loss=11.2740
	step [112/255], loss=9.7016
	step [113/255], loss=9.5354
	step [114/255], loss=11.0970
	step [115/255], loss=9.9190
	step [116/255], loss=9.5693
	step [117/255], loss=11.3776
	step [118/255], loss=9.3212
	step [119/255], loss=10.1772
	step [120/255], loss=13.1407
	step [121/255], loss=10.0015
	step [122/255], loss=11.2571
	step [123/255], loss=12.0637
	step [124/255], loss=10.0060
	step [125/255], loss=10.7853
	step [126/255], loss=10.1547
	step [127/255], loss=12.4828
	step [128/255], loss=11.3015
	step [129/255], loss=9.6003
	step [130/255], loss=10.1020
	step [131/255], loss=11.2999
	step [132/255], loss=10.3455
	step [133/255], loss=9.8983
	step [134/255], loss=10.5792
	step [135/255], loss=8.1988
	step [136/255], loss=9.8653
	step [137/255], loss=10.5782
	step [138/255], loss=9.2310
	step [139/255], loss=11.3234
	step [140/255], loss=11.9735
	step [141/255], loss=9.3578
	step [142/255], loss=9.5533
	step [143/255], loss=9.6721
	step [144/255], loss=8.6467
	step [145/255], loss=9.7859
	step [146/255], loss=8.2110
	step [147/255], loss=13.1895
	step [148/255], loss=11.4220
	step [149/255], loss=10.8184
	step [150/255], loss=11.8758
	step [151/255], loss=9.1669
	step [152/255], loss=9.7604
	step [153/255], loss=8.1311
	step [154/255], loss=13.4639
	step [155/255], loss=13.2411
	step [156/255], loss=10.0798
	step [157/255], loss=8.2208
	step [158/255], loss=10.7272
	step [159/255], loss=9.3460
	step [160/255], loss=10.5394
	step [161/255], loss=9.0732
	step [162/255], loss=12.6263
	step [163/255], loss=9.1490
	step [164/255], loss=9.3765
	step [165/255], loss=10.3633
	step [166/255], loss=10.9024
	step [167/255], loss=13.0836
	step [168/255], loss=10.6181
	step [169/255], loss=10.3204
	step [170/255], loss=12.1234
	step [171/255], loss=9.5891
	step [172/255], loss=12.4838
	step [173/255], loss=10.5226
	step [174/255], loss=10.5878
	step [175/255], loss=11.0503
	step [176/255], loss=11.7503
	step [177/255], loss=10.8246
	step [178/255], loss=9.7853
	step [179/255], loss=11.8421
	step [180/255], loss=7.3662
	step [181/255], loss=9.4908
	step [182/255], loss=9.8656
	step [183/255], loss=9.8148
	step [184/255], loss=8.3131
	step [185/255], loss=9.7288
	step [186/255], loss=9.8373
	step [187/255], loss=9.6447
	step [188/255], loss=10.5127
	step [189/255], loss=9.1746
	step [190/255], loss=8.8947
	step [191/255], loss=12.8018
	step [192/255], loss=10.5521
	step [193/255], loss=12.7627
	step [194/255], loss=8.4003
	step [195/255], loss=10.8661
	step [196/255], loss=11.1112
	step [197/255], loss=10.2075
	step [198/255], loss=10.5937
	step [199/255], loss=8.2746
	step [200/255], loss=9.8904
	step [201/255], loss=10.6670
	step [202/255], loss=9.2032
	step [203/255], loss=8.6592
	step [204/255], loss=12.2301
	step [205/255], loss=8.9428
	step [206/255], loss=11.1677
	step [207/255], loss=10.1000
	step [208/255], loss=9.8137
	step [209/255], loss=9.1677
	step [210/255], loss=10.0601
	step [211/255], loss=11.2327
	step [212/255], loss=10.8718
	step [213/255], loss=9.5140
	step [214/255], loss=8.9941
	step [215/255], loss=10.9234
	step [216/255], loss=9.9347
	step [217/255], loss=8.1712
	step [218/255], loss=9.0041
	step [219/255], loss=10.6026
	step [220/255], loss=8.9207
	step [221/255], loss=12.2271
	step [222/255], loss=9.7721
	step [223/255], loss=8.9329
	step [224/255], loss=9.8249
	step [225/255], loss=8.4921
	step [226/255], loss=11.4032
	step [227/255], loss=8.7772
	step [228/255], loss=9.5468
	step [229/255], loss=8.2886
	step [230/255], loss=11.9884
	step [231/255], loss=9.4215
	step [232/255], loss=11.1973
	step [233/255], loss=10.4731
	step [234/255], loss=10.6577
	step [235/255], loss=9.1728
	step [236/255], loss=9.0351
	step [237/255], loss=9.4710
	step [238/255], loss=9.9561
	step [239/255], loss=9.8532
	step [240/255], loss=10.5643
	step [241/255], loss=11.4063
	step [242/255], loss=10.1784
	step [243/255], loss=9.0512
	step [244/255], loss=11.3953
	step [245/255], loss=10.4372
	step [246/255], loss=9.4255
	step [247/255], loss=9.0054
	step [248/255], loss=11.4600
	step [249/255], loss=10.0158
	step [250/255], loss=7.8006
	step [251/255], loss=10.0577
	step [252/255], loss=8.4013
	step [253/255], loss=10.8376
	step [254/255], loss=12.9315
	step [255/255], loss=9.8503
	Evaluating
	loss=0.0408, precision=0.1177, recall=0.9949, f1=0.5702
Training epoch 12
	step [1/255], loss=10.2512
	step [2/255], loss=10.6891
	step [3/255], loss=10.8624
	step [4/255], loss=10.0547
	step [5/255], loss=10.4107
	step [6/255], loss=10.2776
	step [7/255], loss=7.0870
	step [8/255], loss=9.8381
	step [9/255], loss=7.6648
	step [10/255], loss=10.8879
	step [11/255], loss=9.9151
	step [12/255], loss=12.1038
	step [13/255], loss=8.6764
	step [14/255], loss=9.9504
	step [15/255], loss=10.1231
	step [16/255], loss=10.2648
	step [17/255], loss=7.1411
	step [18/255], loss=12.5940
	step [19/255], loss=11.9569
	step [20/255], loss=11.5544
	step [21/255], loss=9.5263
	step [22/255], loss=9.6408
	step [23/255], loss=9.8012
	step [24/255], loss=10.0535
	step [25/255], loss=12.0166
	step [26/255], loss=9.1131
	step [27/255], loss=13.0968
	step [28/255], loss=11.0560
	step [29/255], loss=10.2027
	step [30/255], loss=9.2666
	step [31/255], loss=8.5226
	step [32/255], loss=9.3572
	step [33/255], loss=12.9486
	step [34/255], loss=11.8625
	step [35/255], loss=11.3748
	step [36/255], loss=11.7558
	step [37/255], loss=9.9318
	step [38/255], loss=10.0576
	step [39/255], loss=10.2118
	step [40/255], loss=10.7279
	step [41/255], loss=8.4603
	step [42/255], loss=9.4937
	step [43/255], loss=13.6302
	step [44/255], loss=10.8862
	step [45/255], loss=9.9449
	step [46/255], loss=8.0012
	step [47/255], loss=13.0070
	step [48/255], loss=10.0726
	step [49/255], loss=11.3516
	step [50/255], loss=11.7918
	step [51/255], loss=10.3477
	step [52/255], loss=7.8780
	step [53/255], loss=9.6987
	step [54/255], loss=9.4671
	step [55/255], loss=11.0606
	step [56/255], loss=9.6235
	step [57/255], loss=9.1981
	step [58/255], loss=13.0743
	step [59/255], loss=10.8288
	step [60/255], loss=9.9226
	step [61/255], loss=9.9330
	step [62/255], loss=8.2444
	step [63/255], loss=9.7336
	step [64/255], loss=10.1136
	step [65/255], loss=9.4234
	step [66/255], loss=10.0105
	step [67/255], loss=11.3986
	step [68/255], loss=12.6457
	step [69/255], loss=10.3195
	step [70/255], loss=8.8711
	step [71/255], loss=9.7608
	step [72/255], loss=8.9424
	step [73/255], loss=11.1119
	step [74/255], loss=11.0370
	step [75/255], loss=7.9801
	step [76/255], loss=8.8963
	step [77/255], loss=8.6362
	step [78/255], loss=10.4098
	step [79/255], loss=11.1759
	step [80/255], loss=9.1134
	step [81/255], loss=8.2782
	step [82/255], loss=9.1590
	step [83/255], loss=9.8385
	step [84/255], loss=11.5298
	step [85/255], loss=8.3323
	step [86/255], loss=12.5049
	step [87/255], loss=8.8291
	step [88/255], loss=10.3414
	step [89/255], loss=7.9724
	step [90/255], loss=9.8400
	step [91/255], loss=9.7100
	step [92/255], loss=11.4246
	step [93/255], loss=10.1920
	step [94/255], loss=9.9010
	step [95/255], loss=9.5565
	step [96/255], loss=7.5168
	step [97/255], loss=8.3226
	step [98/255], loss=9.7945
	step [99/255], loss=9.6911
	step [100/255], loss=10.5611
	step [101/255], loss=9.8503
	step [102/255], loss=7.7596
	step [103/255], loss=10.1904
	step [104/255], loss=9.0678
	step [105/255], loss=11.0315
	step [106/255], loss=10.0921
	step [107/255], loss=8.4613
	step [108/255], loss=11.1676
	step [109/255], loss=9.8887
	step [110/255], loss=9.4188
	step [111/255], loss=9.9082
	step [112/255], loss=12.5231
	step [113/255], loss=11.2030
	step [114/255], loss=9.4175
	step [115/255], loss=9.6394
	step [116/255], loss=9.4298
	step [117/255], loss=10.7059
	step [118/255], loss=11.1507
	step [119/255], loss=11.0722
	step [120/255], loss=9.8550
	step [121/255], loss=9.2002
	step [122/255], loss=9.0363
	step [123/255], loss=8.2444
	step [124/255], loss=10.6332
	step [125/255], loss=12.6055
	step [126/255], loss=10.1276
	step [127/255], loss=13.0906
	step [128/255], loss=8.6200
	step [129/255], loss=9.9410
	step [130/255], loss=9.7420
	step [131/255], loss=12.3388
	step [132/255], loss=10.9299
	step [133/255], loss=9.9737
	step [134/255], loss=10.8628
	step [135/255], loss=10.9078
	step [136/255], loss=10.7946
	step [137/255], loss=9.9735
	step [138/255], loss=9.5545
	step [139/255], loss=9.7063
	step [140/255], loss=8.4706
	step [141/255], loss=10.5526
	step [142/255], loss=9.8015
	step [143/255], loss=11.0399
	step [144/255], loss=11.0224
	step [145/255], loss=9.3909
	step [146/255], loss=8.6972
	step [147/255], loss=9.5472
	step [148/255], loss=9.2656
	step [149/255], loss=11.2130
	step [150/255], loss=14.6054
	step [151/255], loss=7.9362
	step [152/255], loss=8.4745
	step [153/255], loss=8.9495
	step [154/255], loss=9.7366
	step [155/255], loss=8.5656
	step [156/255], loss=10.6428
	step [157/255], loss=9.9215
	step [158/255], loss=9.3525
	step [159/255], loss=9.9954
	step [160/255], loss=9.0211
	step [161/255], loss=16.7876
	step [162/255], loss=8.4350
	step [163/255], loss=8.4099
	step [164/255], loss=7.9094
	step [165/255], loss=11.0851
	step [166/255], loss=7.0017
	step [167/255], loss=9.7204
	step [168/255], loss=10.5840
	step [169/255], loss=9.7390
	step [170/255], loss=9.3855
	step [171/255], loss=10.3210
	step [172/255], loss=10.2874
	step [173/255], loss=11.4170
	step [174/255], loss=8.9861
	step [175/255], loss=9.3791
	step [176/255], loss=11.1812
	step [177/255], loss=10.2212
	step [178/255], loss=11.3333
	step [179/255], loss=9.6165
	step [180/255], loss=11.2035
	step [181/255], loss=7.9896
	step [182/255], loss=10.5317
	step [183/255], loss=10.0305
	step [184/255], loss=10.6569
	step [185/255], loss=10.4302
	step [186/255], loss=8.7297
	step [187/255], loss=9.2773
	step [188/255], loss=8.6837
	step [189/255], loss=8.5610
	step [190/255], loss=8.8758
	step [191/255], loss=9.2815
	step [192/255], loss=8.0991
	step [193/255], loss=11.8279
	step [194/255], loss=10.0063
	step [195/255], loss=10.0867
	step [196/255], loss=10.2944
	step [197/255], loss=8.4262
	step [198/255], loss=8.5613
	step [199/255], loss=7.2281
	step [200/255], loss=10.2494
	step [201/255], loss=9.8268
	step [202/255], loss=10.8496
	step [203/255], loss=9.8767
	step [204/255], loss=9.2939
	step [205/255], loss=10.7269
	step [206/255], loss=8.7318
	step [207/255], loss=9.5310
	step [208/255], loss=9.1657
	step [209/255], loss=8.8961
	step [210/255], loss=10.0744
	step [211/255], loss=8.6699
	step [212/255], loss=10.4069
	step [213/255], loss=9.6464
	step [214/255], loss=9.4857
	step [215/255], loss=11.1424
	step [216/255], loss=8.5236
	step [217/255], loss=10.3254
	step [218/255], loss=12.8165
	step [219/255], loss=7.4978
	step [220/255], loss=9.1113
	step [221/255], loss=10.8691
	step [222/255], loss=10.2275
	step [223/255], loss=9.0345
	step [224/255], loss=10.1572
	step [225/255], loss=9.1241
	step [226/255], loss=7.4420
	step [227/255], loss=11.2390
	step [228/255], loss=9.7256
	step [229/255], loss=14.1782
	step [230/255], loss=8.4832
	step [231/255], loss=11.6883
	step [232/255], loss=8.0972
	step [233/255], loss=8.9365
	step [234/255], loss=10.3779
	step [235/255], loss=8.4506
	step [236/255], loss=8.5072
	step [237/255], loss=10.1486
	step [238/255], loss=9.8535
	step [239/255], loss=9.0708
	step [240/255], loss=11.8684
	step [241/255], loss=8.3861
	step [242/255], loss=8.9339
	step [243/255], loss=10.2600
	step [244/255], loss=9.3314
	step [245/255], loss=9.3877
	step [246/255], loss=9.5801
	step [247/255], loss=8.2639
	step [248/255], loss=10.5265
	step [249/255], loss=8.9668
	step [250/255], loss=7.7604
	step [251/255], loss=10.7097
	step [252/255], loss=9.0666
	step [253/255], loss=9.1438
	step [254/255], loss=10.1375
	step [255/255], loss=7.7574
	Evaluating
	loss=0.0312, precision=0.1763, recall=0.9923, f1=0.6784
Training epoch 13
	step [1/255], loss=8.3327
	step [2/255], loss=8.8232
	step [3/255], loss=9.7825
	step [4/255], loss=11.2393
	step [5/255], loss=11.0096
	step [6/255], loss=9.3710
	step [7/255], loss=10.2712
	step [8/255], loss=9.7939
	step [9/255], loss=9.8009
	step [10/255], loss=10.5651
	step [11/255], loss=9.1213
	step [12/255], loss=11.1390
	step [13/255], loss=8.5478
	step [14/255], loss=10.4694
	step [15/255], loss=10.0540
	step [16/255], loss=8.3905
	step [17/255], loss=7.7786
	step [18/255], loss=9.5032
	step [19/255], loss=10.2381
	step [20/255], loss=10.9972
	step [21/255], loss=11.0008
	step [22/255], loss=8.6313
	step [23/255], loss=10.4256
	step [24/255], loss=9.2416
	step [25/255], loss=9.9190
	step [26/255], loss=10.2927
	step [27/255], loss=10.2816
	step [28/255], loss=8.7157
	step [29/255], loss=9.7417
	step [30/255], loss=10.6790
	step [31/255], loss=10.8776
	step [32/255], loss=10.1347
	step [33/255], loss=10.4555
	step [34/255], loss=10.6968
	step [35/255], loss=9.2607
	step [36/255], loss=8.6079
	step [37/255], loss=8.0583
	step [38/255], loss=8.5536
	step [39/255], loss=7.0008
	step [40/255], loss=9.1911
	step [41/255], loss=7.6520
	step [42/255], loss=8.8163
	step [43/255], loss=8.8701
	step [44/255], loss=12.2906
	step [45/255], loss=11.5479
	step [46/255], loss=8.5592
	step [47/255], loss=9.1069
	step [48/255], loss=9.2732
	step [49/255], loss=12.2236
	step [50/255], loss=9.4821
	step [51/255], loss=8.5138
	step [52/255], loss=10.6928
	step [53/255], loss=8.0595
	step [54/255], loss=10.8491
	step [55/255], loss=8.5234
	step [56/255], loss=8.6665
	step [57/255], loss=9.6225
	step [58/255], loss=7.2007
	step [59/255], loss=9.7453
	step [60/255], loss=10.5491
	step [61/255], loss=8.8685
	step [62/255], loss=8.8398
	step [63/255], loss=9.4079
	step [64/255], loss=9.7659
	step [65/255], loss=8.9845
	step [66/255], loss=8.2482
	step [67/255], loss=10.4110
	step [68/255], loss=8.0166
	step [69/255], loss=9.0366
	step [70/255], loss=13.2079
	step [71/255], loss=8.0865
	step [72/255], loss=9.8964
	step [73/255], loss=10.2170
	step [74/255], loss=8.3781
	step [75/255], loss=9.1065
	step [76/255], loss=9.6526
	step [77/255], loss=14.5027
	step [78/255], loss=10.1210
	step [79/255], loss=9.9554
	step [80/255], loss=7.6467
	step [81/255], loss=8.5358
	step [82/255], loss=9.1923
	step [83/255], loss=10.2164
	step [84/255], loss=9.2188
	step [85/255], loss=7.7857
	step [86/255], loss=10.3705
	step [87/255], loss=7.5396
	step [88/255], loss=9.3193
	step [89/255], loss=9.1445
	step [90/255], loss=8.4462
	step [91/255], loss=7.3883
	step [92/255], loss=10.0687
	step [93/255], loss=11.8807
	step [94/255], loss=7.9731
	step [95/255], loss=11.4378
	step [96/255], loss=11.2339
	step [97/255], loss=10.7411
	step [98/255], loss=10.7069
	step [99/255], loss=10.3364
	step [100/255], loss=7.8609
	step [101/255], loss=10.7432
	step [102/255], loss=9.0728
	step [103/255], loss=7.5343
	step [104/255], loss=9.0275
	step [105/255], loss=9.0287
	step [106/255], loss=7.7977
	step [107/255], loss=10.3003
	step [108/255], loss=7.6237
	step [109/255], loss=8.8206
	step [110/255], loss=8.7211
	step [111/255], loss=11.5455
	step [112/255], loss=10.0468
	step [113/255], loss=10.6900
	step [114/255], loss=11.0788
	step [115/255], loss=8.4334
	step [116/255], loss=7.8591
	step [117/255], loss=8.6977
	step [118/255], loss=8.4039
	step [119/255], loss=9.8345
	step [120/255], loss=8.8631
	step [121/255], loss=13.2120
	step [122/255], loss=10.8180
	step [123/255], loss=10.0927
	step [124/255], loss=8.4440
	step [125/255], loss=10.6969
	step [126/255], loss=10.5718
	step [127/255], loss=7.5549
	step [128/255], loss=10.1258
	step [129/255], loss=8.0996
	step [130/255], loss=8.0532
	step [131/255], loss=8.4686
	step [132/255], loss=8.8780
	step [133/255], loss=9.0020
	step [134/255], loss=7.9783
	step [135/255], loss=9.2904
	step [136/255], loss=10.0730
	step [137/255], loss=7.5781
	step [138/255], loss=9.6854
	step [139/255], loss=8.6494
	step [140/255], loss=9.4849
	step [141/255], loss=7.4269
	step [142/255], loss=7.2890
	step [143/255], loss=10.4588
	step [144/255], loss=8.5563
	step [145/255], loss=7.7629
	step [146/255], loss=10.7056
	step [147/255], loss=9.5945
	step [148/255], loss=8.3542
	step [149/255], loss=8.1188
	step [150/255], loss=10.4997
	step [151/255], loss=7.7385
	step [152/255], loss=7.9344
	step [153/255], loss=11.1157
	step [154/255], loss=8.6699
	step [155/255], loss=9.3616
	step [156/255], loss=7.7050
	step [157/255], loss=8.7204
	step [158/255], loss=7.6748
	step [159/255], loss=9.3882
	step [160/255], loss=11.8836
	step [161/255], loss=8.8224
	step [162/255], loss=10.5897
	step [163/255], loss=11.6868
	step [164/255], loss=8.3736
	step [165/255], loss=10.4635
	step [166/255], loss=8.1478
	step [167/255], loss=8.9989
	step [168/255], loss=8.9859
	step [169/255], loss=9.5250
	step [170/255], loss=9.3547
	step [171/255], loss=12.2096
	step [172/255], loss=9.1151
	step [173/255], loss=9.6605
	step [174/255], loss=11.1764
	step [175/255], loss=10.7308
	step [176/255], loss=10.0706
	step [177/255], loss=9.5613
	step [178/255], loss=8.8219
	step [179/255], loss=10.2336
	step [180/255], loss=9.3551
	step [181/255], loss=8.9199
	step [182/255], loss=10.9761
	step [183/255], loss=10.1190
	step [184/255], loss=8.6549
	step [185/255], loss=10.0575
	step [186/255], loss=8.1261
	step [187/255], loss=8.3739
	step [188/255], loss=9.5044
	step [189/255], loss=8.6173
	step [190/255], loss=9.3798
	step [191/255], loss=8.4270
	step [192/255], loss=10.6116
	step [193/255], loss=8.0311
	step [194/255], loss=7.0271
	step [195/255], loss=11.2716
	step [196/255], loss=12.9417
	step [197/255], loss=10.8852
	step [198/255], loss=9.8096
	step [199/255], loss=9.5452
	step [200/255], loss=9.6276
	step [201/255], loss=9.3454
	step [202/255], loss=11.3575
	step [203/255], loss=9.8017
	step [204/255], loss=9.0455
	step [205/255], loss=14.4066
	step [206/255], loss=9.0664
	step [207/255], loss=9.4802
	step [208/255], loss=11.8419
	step [209/255], loss=8.8878
	step [210/255], loss=8.8248
	step [211/255], loss=9.9095
	step [212/255], loss=8.3029
	step [213/255], loss=8.5307
	step [214/255], loss=7.6676
	step [215/255], loss=10.8269
	step [216/255], loss=11.2693
	step [217/255], loss=7.2000
	step [218/255], loss=8.8117
	step [219/255], loss=8.3952
	step [220/255], loss=10.1804
	step [221/255], loss=7.7941
	step [222/255], loss=8.4833
	step [223/255], loss=8.0467
	step [224/255], loss=9.5537
	step [225/255], loss=10.0630
	step [226/255], loss=8.5838
	step [227/255], loss=8.0000
	step [228/255], loss=10.3880
	step [229/255], loss=9.8615
	step [230/255], loss=9.9430
	step [231/255], loss=9.2143
	step [232/255], loss=10.2413
	step [233/255], loss=9.7723
	step [234/255], loss=9.3289
	step [235/255], loss=8.5566
	step [236/255], loss=8.7189
	step [237/255], loss=8.6186
	step [238/255], loss=9.3524
	step [239/255], loss=12.3610
	step [240/255], loss=10.8408
	step [241/255], loss=9.7206
	step [242/255], loss=12.2872
	step [243/255], loss=9.4224
	step [244/255], loss=12.5630
	step [245/255], loss=10.3875
	step [246/255], loss=9.2782
	step [247/255], loss=9.0822
	step [248/255], loss=10.8228
	step [249/255], loss=10.5035
	step [250/255], loss=9.0616
	step [251/255], loss=7.8683
	step [252/255], loss=8.3964
	step [253/255], loss=10.0495
	step [254/255], loss=8.5510
	step [255/255], loss=9.9638
	Evaluating
	loss=0.0275, precision=0.1902, recall=0.9919, f1=0.6978
saving model as: 0_saved_model.pth
Training epoch 14
	step [1/255], loss=8.6676
	step [2/255], loss=8.9379
	step [3/255], loss=7.6672
	step [4/255], loss=8.1415
	step [5/255], loss=7.9760
	step [6/255], loss=13.0486
	step [7/255], loss=11.8461
	step [8/255], loss=8.3196
	step [9/255], loss=7.6973
	step [10/255], loss=8.9171
	step [11/255], loss=9.4152
	step [12/255], loss=8.5694
	step [13/255], loss=8.4769
	step [14/255], loss=10.6094
	step [15/255], loss=9.5959
	step [16/255], loss=10.2441
	step [17/255], loss=10.8509
	step [18/255], loss=10.8894
	step [19/255], loss=9.2272
	step [20/255], loss=8.1905
	step [21/255], loss=9.5750
	step [22/255], loss=8.2960
	step [23/255], loss=10.0925
	step [24/255], loss=8.3925
	step [25/255], loss=9.4416
	step [26/255], loss=10.0119
	step [27/255], loss=8.3843
	step [28/255], loss=8.7898
	step [29/255], loss=9.4490
	step [30/255], loss=6.2596
	step [31/255], loss=7.6971
	step [32/255], loss=9.7580
	step [33/255], loss=11.1526
	step [34/255], loss=7.8716
	step [35/255], loss=8.7918
	step [36/255], loss=6.5730
	step [37/255], loss=10.3675
	step [38/255], loss=9.5269
	step [39/255], loss=9.6238
	step [40/255], loss=9.1868
	step [41/255], loss=9.2131
	step [42/255], loss=9.2839
	step [43/255], loss=13.7415
	step [44/255], loss=8.8955
	step [45/255], loss=7.7750
	step [46/255], loss=7.2194
	step [47/255], loss=9.9326
	step [48/255], loss=10.1968
	step [49/255], loss=10.0782
	step [50/255], loss=8.4151
	step [51/255], loss=9.3776
	step [52/255], loss=10.2609
	step [53/255], loss=10.1367
	step [54/255], loss=8.1953
	step [55/255], loss=10.0095
	step [56/255], loss=9.3096
	step [57/255], loss=9.6214
	step [58/255], loss=9.2664
	step [59/255], loss=12.8915
	step [60/255], loss=10.0900
	step [61/255], loss=9.6619
	step [62/255], loss=11.6632
	step [63/255], loss=10.0184
	step [64/255], loss=9.0122
	step [65/255], loss=7.6400
	step [66/255], loss=7.7878
	step [67/255], loss=12.1486
	step [68/255], loss=11.4558
	step [69/255], loss=9.2986
	step [70/255], loss=9.4105
	step [71/255], loss=8.3797
	step [72/255], loss=9.9685
	step [73/255], loss=9.3869
	step [74/255], loss=11.8240
	step [75/255], loss=8.5188
	step [76/255], loss=11.0590
	step [77/255], loss=8.3281
	step [78/255], loss=7.6900
	step [79/255], loss=9.5593
	step [80/255], loss=8.4724
	step [81/255], loss=8.4554
	step [82/255], loss=7.6093
	step [83/255], loss=7.6560
	step [84/255], loss=8.1876
	step [85/255], loss=9.5854
	step [86/255], loss=11.6285
	step [87/255], loss=8.4527
	step [88/255], loss=7.7029
	step [89/255], loss=8.6809
	step [90/255], loss=7.6247
	step [91/255], loss=9.6348
	step [92/255], loss=8.4191
	step [93/255], loss=7.7855
	step [94/255], loss=8.6635
	step [95/255], loss=9.5306
	step [96/255], loss=11.5491
	step [97/255], loss=10.7760
	step [98/255], loss=8.8328
	step [99/255], loss=10.1023
	step [100/255], loss=9.0890
	step [101/255], loss=9.8848
	step [102/255], loss=8.8776
	step [103/255], loss=8.4883
	step [104/255], loss=8.6725
	step [105/255], loss=8.8088
	step [106/255], loss=10.6118
	step [107/255], loss=8.3686
	step [108/255], loss=9.3124
	step [109/255], loss=7.3237
	step [110/255], loss=7.9202
	step [111/255], loss=8.9886
	step [112/255], loss=8.3907
	step [113/255], loss=9.2682
	step [114/255], loss=10.0923
	step [115/255], loss=7.5523
	step [116/255], loss=8.9375
	step [117/255], loss=7.4773
	step [118/255], loss=8.4546
	step [119/255], loss=8.8028
	step [120/255], loss=8.9475
	step [121/255], loss=8.3274
	step [122/255], loss=8.0443
	step [123/255], loss=7.6434
	step [124/255], loss=10.3709
	step [125/255], loss=7.6530
	step [126/255], loss=12.1541
	step [127/255], loss=10.3547
	step [128/255], loss=10.2180
	step [129/255], loss=9.7223
	step [130/255], loss=10.8967
	step [131/255], loss=9.0726
	step [132/255], loss=8.5339
	step [133/255], loss=8.6740
	step [134/255], loss=7.0834
	step [135/255], loss=10.4408
	step [136/255], loss=7.4355
	step [137/255], loss=9.5513
	step [138/255], loss=7.8733
	step [139/255], loss=8.9533
	step [140/255], loss=10.1581
	step [141/255], loss=9.7130
	step [142/255], loss=8.7839
	step [143/255], loss=10.8579
	step [144/255], loss=9.5814
	step [145/255], loss=9.3901
	step [146/255], loss=7.6806
	step [147/255], loss=7.3241
	step [148/255], loss=9.3429
	step [149/255], loss=11.0221
	step [150/255], loss=9.9768
	step [151/255], loss=10.0530
	step [152/255], loss=9.3565
	step [153/255], loss=8.9911
	step [154/255], loss=7.9103
	step [155/255], loss=7.1609
	step [156/255], loss=8.2371
	step [157/255], loss=9.3263
	step [158/255], loss=9.9949
	step [159/255], loss=7.7851
	step [160/255], loss=11.0786
	step [161/255], loss=6.4911
	step [162/255], loss=8.3559
	step [163/255], loss=8.0962
	step [164/255], loss=9.6986
	step [165/255], loss=8.1485
	step [166/255], loss=8.8059
	step [167/255], loss=9.3123
	step [168/255], loss=8.7953
	step [169/255], loss=9.7117
	step [170/255], loss=8.4381
	step [171/255], loss=8.7409
	step [172/255], loss=9.8669
	step [173/255], loss=10.0731
	step [174/255], loss=7.9888
	step [175/255], loss=7.8029
	step [176/255], loss=9.9175
	step [177/255], loss=7.9940
	step [178/255], loss=7.4533
	step [179/255], loss=11.0046
	step [180/255], loss=7.8490
	step [181/255], loss=10.3582
	step [182/255], loss=10.5245
	step [183/255], loss=9.2297
	step [184/255], loss=11.9066
	step [185/255], loss=8.5919
	step [186/255], loss=6.6084
	step [187/255], loss=7.4476
	step [188/255], loss=6.6183
	step [189/255], loss=8.4555
	step [190/255], loss=11.2227
	step [191/255], loss=8.7520
	step [192/255], loss=8.8850
	step [193/255], loss=9.7791
	step [194/255], loss=8.5683
	step [195/255], loss=9.7643
	step [196/255], loss=8.5199
	step [197/255], loss=9.7567
	step [198/255], loss=8.6309
	step [199/255], loss=10.8659
	step [200/255], loss=10.3466
	step [201/255], loss=9.9764
	step [202/255], loss=7.9085
	step [203/255], loss=9.0285
	step [204/255], loss=9.3637
	step [205/255], loss=10.0983
	step [206/255], loss=9.6166
	step [207/255], loss=9.9993
	step [208/255], loss=8.1018
	step [209/255], loss=8.7311
	step [210/255], loss=6.6286
	step [211/255], loss=10.5416
	step [212/255], loss=7.6938
	step [213/255], loss=9.8797
	step [214/255], loss=9.8795
	step [215/255], loss=10.1700
	step [216/255], loss=8.6621
	step [217/255], loss=11.6223
	step [218/255], loss=9.4023
	step [219/255], loss=9.0822
	step [220/255], loss=10.1176
	step [221/255], loss=7.9827
	step [222/255], loss=8.9497
	step [223/255], loss=9.6061
	step [224/255], loss=10.4509
	step [225/255], loss=9.8347
	step [226/255], loss=9.5016
	step [227/255], loss=12.6316
	step [228/255], loss=8.3623
	step [229/255], loss=9.8826
	step [230/255], loss=7.7932
	step [231/255], loss=10.2652
	step [232/255], loss=9.3517
	step [233/255], loss=11.6347
	step [234/255], loss=8.5038
	step [235/255], loss=9.2259
	step [236/255], loss=9.6009
	step [237/255], loss=9.4221
	step [238/255], loss=8.1083
	step [239/255], loss=7.1739
	step [240/255], loss=8.4386
	step [241/255], loss=9.9286
	step [242/255], loss=8.6727
	step [243/255], loss=6.8544
	step [244/255], loss=7.5906
	step [245/255], loss=7.9997
	step [246/255], loss=9.8206
	step [247/255], loss=9.7875
	step [248/255], loss=8.2980
	step [249/255], loss=9.1575
	step [250/255], loss=8.9610
	step [251/255], loss=8.7949
	step [252/255], loss=9.0062
	step [253/255], loss=8.6168
	step [254/255], loss=7.1013
	step [255/255], loss=6.8188
	Evaluating
	loss=0.0276, precision=0.1743, recall=0.9926, f1=0.6755
Training epoch 15
	step [1/255], loss=9.6168
	step [2/255], loss=8.0951
	step [3/255], loss=9.0310
	step [4/255], loss=11.2668
	step [5/255], loss=9.4311
	step [6/255], loss=7.4646
	step [7/255], loss=7.7249
	step [8/255], loss=9.3399
	step [9/255], loss=8.0079
	step [10/255], loss=8.0420
	step [11/255], loss=10.5102
	step [12/255], loss=9.8976
	step [13/255], loss=10.4410
	step [14/255], loss=9.0442
	step [15/255], loss=8.7203
	step [16/255], loss=8.0692
	step [17/255], loss=7.7241
	step [18/255], loss=6.9080
	step [19/255], loss=9.4180
	step [20/255], loss=7.3578
	step [21/255], loss=7.4890
	step [22/255], loss=7.0513
	step [23/255], loss=11.3156
	step [24/255], loss=11.1452
	step [25/255], loss=12.0268
	step [26/255], loss=6.8472
	step [27/255], loss=9.3616
	step [28/255], loss=8.3993
	step [29/255], loss=8.2518
	step [30/255], loss=9.5738
	step [31/255], loss=10.0400
	step [32/255], loss=8.2843
	step [33/255], loss=8.3290
	step [34/255], loss=8.3824
	step [35/255], loss=11.2777
	step [36/255], loss=8.6458
	step [37/255], loss=8.1246
	step [38/255], loss=9.0162
	step [39/255], loss=8.8007
	step [40/255], loss=7.1512
	step [41/255], loss=8.1603
	step [42/255], loss=8.7796
	step [43/255], loss=8.5978
	step [44/255], loss=9.0761
	step [45/255], loss=8.4485
	step [46/255], loss=7.7153
	step [47/255], loss=8.6426
	step [48/255], loss=8.8642
	step [49/255], loss=9.2600
	step [50/255], loss=7.5343
	step [51/255], loss=9.4713
	step [52/255], loss=11.7844
	step [53/255], loss=8.5400
	step [54/255], loss=8.1214
	step [55/255], loss=8.2904
	step [56/255], loss=8.7849
	step [57/255], loss=8.6653
	step [58/255], loss=7.1776
	step [59/255], loss=9.1818
	step [60/255], loss=10.6960
	step [61/255], loss=9.1887
	step [62/255], loss=10.2363
	step [63/255], loss=9.0911
	step [64/255], loss=10.1930
	step [65/255], loss=10.8312
	step [66/255], loss=7.2292
	step [67/255], loss=9.7507
	step [68/255], loss=9.4270
	step [69/255], loss=8.5672
	step [70/255], loss=10.1580
	step [71/255], loss=11.8341
	step [72/255], loss=6.7490
	step [73/255], loss=8.1243
	step [74/255], loss=13.8547
	step [75/255], loss=8.5995
	step [76/255], loss=7.5813
	step [77/255], loss=6.6817
	step [78/255], loss=7.9349
	step [79/255], loss=6.8664
	step [80/255], loss=7.1280
	step [81/255], loss=9.8429
	step [82/255], loss=8.5353
	step [83/255], loss=10.4251
	step [84/255], loss=9.4548
	step [85/255], loss=9.3023
	step [86/255], loss=8.0216
	step [87/255], loss=10.3859
	step [88/255], loss=8.3887
	step [89/255], loss=10.6171
	step [90/255], loss=9.2676
	step [91/255], loss=8.4044
	step [92/255], loss=12.4727
	step [93/255], loss=9.2245
	step [94/255], loss=10.6561
	step [95/255], loss=8.2235
	step [96/255], loss=7.8901
	step [97/255], loss=7.6916
	step [98/255], loss=9.3054
	step [99/255], loss=9.5758
	step [100/255], loss=10.3745
	step [101/255], loss=9.2144
	step [102/255], loss=8.1646
	step [103/255], loss=7.8398
	step [104/255], loss=10.6535
	step [105/255], loss=6.7181
	step [106/255], loss=8.3689
	step [107/255], loss=6.8611
	step [108/255], loss=7.8660
	step [109/255], loss=10.0777
	step [110/255], loss=8.6131
	step [111/255], loss=8.2526
	step [112/255], loss=7.5172
	step [113/255], loss=10.3040
	step [114/255], loss=8.4244
	step [115/255], loss=10.8045
	step [116/255], loss=8.2336
	step [117/255], loss=6.7560
	step [118/255], loss=7.5087
	step [119/255], loss=10.1217
	step [120/255], loss=9.6059
	step [121/255], loss=6.9397
	step [122/255], loss=10.1305
	step [123/255], loss=9.6438
	step [124/255], loss=7.9837
	step [125/255], loss=6.8109
	step [126/255], loss=7.0389
	step [127/255], loss=9.3047
	step [128/255], loss=8.7627
	step [129/255], loss=9.3720
	step [130/255], loss=8.3475
	step [131/255], loss=8.2439
	step [132/255], loss=8.0468
	step [133/255], loss=8.9953
	step [134/255], loss=11.0178
	step [135/255], loss=8.0599
	step [136/255], loss=7.7315
	step [137/255], loss=8.0943
	step [138/255], loss=6.5216
	step [139/255], loss=7.0269
	step [140/255], loss=6.9670
	step [141/255], loss=9.1409
	step [142/255], loss=9.9999
	step [143/255], loss=11.8255
	step [144/255], loss=8.2444
	step [145/255], loss=10.0185
	step [146/255], loss=8.8584
	step [147/255], loss=10.5664
	step [148/255], loss=8.6404
	step [149/255], loss=8.0400
	step [150/255], loss=10.5023
	step [151/255], loss=9.0639
	step [152/255], loss=10.2045
	step [153/255], loss=9.5952
	step [154/255], loss=8.7373
	step [155/255], loss=6.3300
	step [156/255], loss=8.7802
	step [157/255], loss=8.0140
	step [158/255], loss=7.8018
	step [159/255], loss=8.7077
	step [160/255], loss=9.5848
	step [161/255], loss=10.2007
	step [162/255], loss=7.5846
	step [163/255], loss=7.5472
	step [164/255], loss=9.1529
	step [165/255], loss=7.1944
	step [166/255], loss=7.6389
	step [167/255], loss=10.7242
	step [168/255], loss=11.3229
	step [169/255], loss=9.4514
	step [170/255], loss=7.5988
	step [171/255], loss=10.0407
	step [172/255], loss=5.8499
	step [173/255], loss=8.1188
	step [174/255], loss=11.1159
	step [175/255], loss=7.3815
	step [176/255], loss=7.8940
	step [177/255], loss=8.7169
	step [178/255], loss=6.9159
	step [179/255], loss=10.3513
	step [180/255], loss=9.1263
	step [181/255], loss=8.4549
	step [182/255], loss=10.3538
	step [183/255], loss=8.2177
	step [184/255], loss=9.6831
	step [185/255], loss=8.8826
	step [186/255], loss=8.0922
	step [187/255], loss=8.4340
	step [188/255], loss=8.4017
	step [189/255], loss=12.8700
	step [190/255], loss=8.3789
	step [191/255], loss=7.7176
	step [192/255], loss=8.6784
	step [193/255], loss=7.6496
	step [194/255], loss=7.3192
	step [195/255], loss=9.2069
	step [196/255], loss=8.0208
	step [197/255], loss=8.4688
	step [198/255], loss=9.6382
	step [199/255], loss=8.9557
	step [200/255], loss=10.3359
	step [201/255], loss=7.8809
	step [202/255], loss=8.8360
	step [203/255], loss=10.8865
	step [204/255], loss=8.2788
	step [205/255], loss=8.3833
	step [206/255], loss=7.2818
	step [207/255], loss=9.9467
	step [208/255], loss=9.7780
	step [209/255], loss=9.4300
	step [210/255], loss=8.6727
	step [211/255], loss=9.3246
	step [212/255], loss=8.0962
	step [213/255], loss=7.8861
	step [214/255], loss=7.5699
	step [215/255], loss=10.0907
	step [216/255], loss=7.3668
	step [217/255], loss=10.7361
	step [218/255], loss=7.4922
	step [219/255], loss=8.0823
	step [220/255], loss=9.7176
	step [221/255], loss=8.7092
	step [222/255], loss=6.5902
	step [223/255], loss=9.5054
	step [224/255], loss=9.5737
	step [225/255], loss=8.2982
	step [226/255], loss=11.2049
	step [227/255], loss=8.6758
	step [228/255], loss=7.4436
	step [229/255], loss=10.2111
	step [230/255], loss=8.4367
	step [231/255], loss=6.8590
	step [232/255], loss=9.3439
	step [233/255], loss=10.2876
	step [234/255], loss=8.3821
	step [235/255], loss=8.4874
	step [236/255], loss=8.7445
	step [237/255], loss=8.4578
	step [238/255], loss=7.3465
	step [239/255], loss=7.8467
	step [240/255], loss=7.0888
	step [241/255], loss=9.0718
	step [242/255], loss=6.7020
	step [243/255], loss=8.7699
	step [244/255], loss=10.7534
	step [245/255], loss=9.0722
	step [246/255], loss=10.6866
	step [247/255], loss=8.4436
	step [248/255], loss=9.3126
	step [249/255], loss=7.5965
	step [250/255], loss=7.9919
	step [251/255], loss=9.7571
	step [252/255], loss=9.3514
	step [253/255], loss=8.8566
	step [254/255], loss=7.9853
	step [255/255], loss=8.7472
	Evaluating
	loss=0.0267, precision=0.1709, recall=0.9929, f1=0.6704
Training epoch 16
	step [1/255], loss=6.9727
	step [2/255], loss=8.0503
	step [3/255], loss=8.7435
	step [4/255], loss=9.3185
	step [5/255], loss=9.0619
	step [6/255], loss=8.0773
	step [7/255], loss=6.4248
	step [8/255], loss=8.2273
	step [9/255], loss=7.2584
	step [10/255], loss=8.8193
	step [11/255], loss=6.6650
	step [12/255], loss=8.1491
	step [13/255], loss=11.1283
	step [14/255], loss=8.5378
	step [15/255], loss=9.0509
	step [16/255], loss=9.7579
	step [17/255], loss=7.2909
	step [18/255], loss=7.7358
	step [19/255], loss=7.9254
	step [20/255], loss=8.2319
	step [21/255], loss=7.4314
	step [22/255], loss=10.4965
	step [23/255], loss=7.7381
	step [24/255], loss=11.9787
	step [25/255], loss=8.5750
	step [26/255], loss=5.9313
	step [27/255], loss=8.7540
	step [28/255], loss=7.7949
	step [29/255], loss=8.2186
	step [30/255], loss=7.5808
	step [31/255], loss=8.4990
	step [32/255], loss=9.1657
	step [33/255], loss=6.2839
	step [34/255], loss=9.4194
	step [35/255], loss=7.8469
	step [36/255], loss=8.3803
	step [37/255], loss=10.9186
	step [38/255], loss=10.4153
	step [39/255], loss=8.8580
	step [40/255], loss=7.6428
	step [41/255], loss=7.9670
	step [42/255], loss=9.5166
	step [43/255], loss=8.1523
	step [44/255], loss=8.2932
	step [45/255], loss=7.9673
	step [46/255], loss=8.2472
	step [47/255], loss=8.4230
	step [48/255], loss=6.8576
	step [49/255], loss=10.5929
	step [50/255], loss=7.7240
	step [51/255], loss=9.0655
	step [52/255], loss=6.9047
	step [53/255], loss=10.8697
	step [54/255], loss=8.8247
	step [55/255], loss=9.9806
	step [56/255], loss=11.0542
	step [57/255], loss=8.1274
	step [58/255], loss=8.1961
	step [59/255], loss=6.5367
	step [60/255], loss=8.7733
	step [61/255], loss=8.9928
	step [62/255], loss=10.1403
	step [63/255], loss=8.5550
	step [64/255], loss=9.5112
	step [65/255], loss=8.0506
	step [66/255], loss=6.3953
	step [67/255], loss=9.2841
	step [68/255], loss=8.7086
	step [69/255], loss=9.8622
	step [70/255], loss=9.4823
	step [71/255], loss=8.5846
	step [72/255], loss=9.3366
	step [73/255], loss=8.2352
	step [74/255], loss=9.1741
	step [75/255], loss=7.7692
	step [76/255], loss=8.5595
	step [77/255], loss=10.4091
	step [78/255], loss=9.8804
	step [79/255], loss=9.9037
	step [80/255], loss=9.2487
	step [81/255], loss=7.5336
	step [82/255], loss=8.0555
	step [83/255], loss=7.7151
	step [84/255], loss=10.6018
	step [85/255], loss=12.2773
	step [86/255], loss=9.2196
	step [87/255], loss=7.9761
	step [88/255], loss=9.3158
	step [89/255], loss=7.2782
	step [90/255], loss=7.5664
	step [91/255], loss=8.5990
	step [92/255], loss=8.3798
	step [93/255], loss=9.4268
	step [94/255], loss=7.8104
	step [95/255], loss=9.9717
	step [96/255], loss=9.7069
	step [97/255], loss=9.9373
	step [98/255], loss=7.0770
	step [99/255], loss=8.9726
	step [100/255], loss=8.1754
	step [101/255], loss=8.3354
	step [102/255], loss=7.2199
	step [103/255], loss=6.6825
	step [104/255], loss=10.3202
	step [105/255], loss=9.0133
	step [106/255], loss=9.1775
	step [107/255], loss=10.4626
	step [108/255], loss=8.8204
	step [109/255], loss=8.1218
	step [110/255], loss=8.8501
	step [111/255], loss=8.1172
	step [112/255], loss=8.2066
	step [113/255], loss=8.1723
	step [114/255], loss=11.4167
	step [115/255], loss=8.2190
	step [116/255], loss=7.3307
	step [117/255], loss=7.3080
	step [118/255], loss=7.4750
	step [119/255], loss=9.0029
	step [120/255], loss=7.2642
	step [121/255], loss=7.6830
	step [122/255], loss=7.5323
	step [123/255], loss=8.7504
	step [124/255], loss=6.7130
	step [125/255], loss=6.2281
	step [126/255], loss=8.6131
	step [127/255], loss=9.3339
	step [128/255], loss=8.3039
	step [129/255], loss=9.3176
	step [130/255], loss=9.5607
	step [131/255], loss=7.6257
	step [132/255], loss=7.4956
	step [133/255], loss=11.5040
	step [134/255], loss=8.5290
	step [135/255], loss=8.0014
	step [136/255], loss=9.7486
	step [137/255], loss=9.1725
	step [138/255], loss=8.9289
	step [139/255], loss=7.3394
	step [140/255], loss=6.9576
	step [141/255], loss=6.6634
	step [142/255], loss=9.2661
	step [143/255], loss=10.9915
	step [144/255], loss=9.0901
	step [145/255], loss=12.0885
	step [146/255], loss=9.5081
	step [147/255], loss=8.5125
	step [148/255], loss=8.8458
	step [149/255], loss=6.3617
	step [150/255], loss=9.1730
	step [151/255], loss=9.1572
	step [152/255], loss=7.7187
	step [153/255], loss=8.6626
	step [154/255], loss=9.1760
	step [155/255], loss=10.6844
	step [156/255], loss=7.1801
	step [157/255], loss=7.6086
	step [158/255], loss=6.3938
	step [159/255], loss=7.2142
	step [160/255], loss=8.3785
	step [161/255], loss=10.4132
	step [162/255], loss=8.0566
	step [163/255], loss=7.1988
	step [164/255], loss=9.9818
	step [165/255], loss=9.3072
	step [166/255], loss=8.2475
	step [167/255], loss=9.0313
	step [168/255], loss=8.4011
	step [169/255], loss=6.9927
	step [170/255], loss=9.2085
	step [171/255], loss=7.0020
	step [172/255], loss=9.6341
	step [173/255], loss=9.4222
	step [174/255], loss=8.0112
	step [175/255], loss=8.2517
	step [176/255], loss=7.2661
	step [177/255], loss=8.7997
	step [178/255], loss=8.3479
	step [179/255], loss=7.0036
	step [180/255], loss=8.9871
	step [181/255], loss=7.0757
	step [182/255], loss=7.4590
	step [183/255], loss=6.2576
	step [184/255], loss=7.5696
	step [185/255], loss=8.2392
	step [186/255], loss=8.3921
	step [187/255], loss=7.0148
	step [188/255], loss=8.7038
	step [189/255], loss=9.2846
	step [190/255], loss=7.4969
	step [191/255], loss=7.5001
	step [192/255], loss=7.4180
	step [193/255], loss=9.1436
	step [194/255], loss=9.7305
	step [195/255], loss=6.8060
	step [196/255], loss=7.5825
	step [197/255], loss=8.5299
	step [198/255], loss=10.7903
	step [199/255], loss=9.3943
	step [200/255], loss=10.0688
	step [201/255], loss=10.0464
	step [202/255], loss=9.5489
	step [203/255], loss=10.4162
	step [204/255], loss=8.7274
	step [205/255], loss=8.1364
	step [206/255], loss=7.9551
	step [207/255], loss=8.8824
	step [208/255], loss=7.7530
	step [209/255], loss=7.5737
	step [210/255], loss=8.7436
	step [211/255], loss=6.9110
	step [212/255], loss=10.3186
	step [213/255], loss=8.2144
	step [214/255], loss=6.5751
	step [215/255], loss=7.8375
	step [216/255], loss=9.6887
	step [217/255], loss=7.4234
	step [218/255], loss=6.3441
	step [219/255], loss=9.2094
	step [220/255], loss=7.4549
	step [221/255], loss=7.9460
	step [222/255], loss=7.4972
	step [223/255], loss=11.0982
	step [224/255], loss=8.2821
	step [225/255], loss=9.4591
	step [226/255], loss=7.1168
	step [227/255], loss=7.4693
	step [228/255], loss=10.5708
	step [229/255], loss=6.4012
	step [230/255], loss=9.1849
	step [231/255], loss=10.3861
	step [232/255], loss=8.7698
	step [233/255], loss=9.4802
	step [234/255], loss=10.3405
	step [235/255], loss=7.9756
	step [236/255], loss=8.0172
	step [237/255], loss=8.4669
	step [238/255], loss=11.0208
	step [239/255], loss=8.3284
	step [240/255], loss=8.2256
	step [241/255], loss=8.2140
	step [242/255], loss=8.6805
	step [243/255], loss=8.5138
	step [244/255], loss=7.3311
	step [245/255], loss=7.5069
	step [246/255], loss=7.8012
	step [247/255], loss=11.6971
	step [248/255], loss=9.0117
	step [249/255], loss=8.3466
	step [250/255], loss=10.6147
	step [251/255], loss=8.4722
	step [252/255], loss=9.3397
	step [253/255], loss=7.5839
	step [254/255], loss=9.1995
	step [255/255], loss=6.8069
	Evaluating
	loss=0.0290, precision=0.1534, recall=0.9928, f1=0.6417
Training epoch 17
	step [1/255], loss=8.0009
	step [2/255], loss=7.7653
	step [3/255], loss=8.8130
	step [4/255], loss=8.0633
	step [5/255], loss=8.1751
	step [6/255], loss=8.4517
	step [7/255], loss=6.7928
	step [8/255], loss=8.7706
	step [9/255], loss=9.9652
	step [10/255], loss=8.0781
	step [11/255], loss=8.0331
	step [12/255], loss=7.3305
	step [13/255], loss=8.6742
	step [14/255], loss=9.2060
	step [15/255], loss=9.7477
	step [16/255], loss=7.6981
	step [17/255], loss=8.5719
	step [18/255], loss=6.8301
	step [19/255], loss=8.5429
	step [20/255], loss=8.6501
	step [21/255], loss=9.6445
	step [22/255], loss=8.2771
	step [23/255], loss=7.6787
	step [24/255], loss=6.2093
	step [25/255], loss=7.7517
	step [26/255], loss=8.6436
	step [27/255], loss=8.4803
	step [28/255], loss=9.0207
	step [29/255], loss=11.3093
	step [30/255], loss=8.5648
	step [31/255], loss=7.6246
	step [32/255], loss=7.4808
	step [33/255], loss=7.8293
	step [34/255], loss=7.4324
	step [35/255], loss=8.8085
	step [36/255], loss=8.9026
	step [37/255], loss=7.8284
	step [38/255], loss=9.4460
	step [39/255], loss=9.5543
	step [40/255], loss=6.8726
	step [41/255], loss=7.9839
	step [42/255], loss=6.7515
	step [43/255], loss=7.3070
	step [44/255], loss=10.0247
	step [45/255], loss=7.6275
	step [46/255], loss=6.8403
	step [47/255], loss=7.3557
	step [48/255], loss=8.6111
	step [49/255], loss=7.8843
	step [50/255], loss=7.8168
	step [51/255], loss=9.1307
	step [52/255], loss=9.7849
	step [53/255], loss=7.7942
	step [54/255], loss=9.3519
	step [55/255], loss=10.4132
	step [56/255], loss=9.0308
	step [57/255], loss=7.8192
	step [58/255], loss=8.5633
	step [59/255], loss=10.0981
	step [60/255], loss=8.7418
	step [61/255], loss=7.8766
	step [62/255], loss=10.2463
	step [63/255], loss=8.8628
	step [64/255], loss=8.5423
	step [65/255], loss=8.9944
	step [66/255], loss=9.5537
	step [67/255], loss=6.3414
	step [68/255], loss=8.4494
	step [69/255], loss=7.8058
	step [70/255], loss=7.1970
	step [71/255], loss=10.7340
	step [72/255], loss=6.8804
	step [73/255], loss=9.2727
	step [74/255], loss=9.5298
	step [75/255], loss=7.9919
	step [76/255], loss=7.6462
	step [77/255], loss=7.1419
	step [78/255], loss=7.6671
	step [79/255], loss=7.8182
	step [80/255], loss=8.6424
	step [81/255], loss=8.1066
	step [82/255], loss=6.5948
	step [83/255], loss=9.3505
	step [84/255], loss=8.9469
	step [85/255], loss=9.6507
	step [86/255], loss=7.3288
	step [87/255], loss=7.0010
	step [88/255], loss=6.6539
	step [89/255], loss=8.4074
	step [90/255], loss=8.2093
	step [91/255], loss=8.3415
	step [92/255], loss=8.3286
	step [93/255], loss=7.2207
	step [94/255], loss=7.4312
	step [95/255], loss=8.8038
	step [96/255], loss=6.5981
	step [97/255], loss=7.8999
	step [98/255], loss=7.0910
	step [99/255], loss=9.2596
	step [100/255], loss=7.6191
	step [101/255], loss=8.7315
	step [102/255], loss=11.3429
	step [103/255], loss=9.8863
	step [104/255], loss=6.7565
	step [105/255], loss=7.6613
	step [106/255], loss=8.5426
	step [107/255], loss=8.4483
	step [108/255], loss=7.3060
	step [109/255], loss=7.7804
	step [110/255], loss=8.4272
	step [111/255], loss=9.3997
	step [112/255], loss=8.6833
	step [113/255], loss=8.3407
	step [114/255], loss=8.7060
	step [115/255], loss=8.0792
	step [116/255], loss=5.8864
	step [117/255], loss=9.3678
	step [118/255], loss=7.1759
	step [119/255], loss=8.8028
	step [120/255], loss=6.6323
	step [121/255], loss=7.7819
	step [122/255], loss=6.9170
	step [123/255], loss=8.1923
	step [124/255], loss=8.6823
	step [125/255], loss=6.2072
	step [126/255], loss=9.5966
	step [127/255], loss=7.7502
	step [128/255], loss=7.1036
	step [129/255], loss=9.4842
	step [130/255], loss=7.9943
	step [131/255], loss=9.4992
	step [132/255], loss=7.7627
	step [133/255], loss=7.4198
	step [134/255], loss=6.3683
	step [135/255], loss=6.7796
	step [136/255], loss=7.4858
	step [137/255], loss=9.1746
	step [138/255], loss=8.7522
	step [139/255], loss=10.4976
	step [140/255], loss=10.0418
	step [141/255], loss=8.5056
	step [142/255], loss=8.4813
	step [143/255], loss=7.1348
	step [144/255], loss=7.6591
	step [145/255], loss=8.8724
	step [146/255], loss=10.5781
	step [147/255], loss=10.7846
	step [148/255], loss=8.2174
	step [149/255], loss=8.2937
	step [150/255], loss=7.1471
	step [151/255], loss=9.7723
	step [152/255], loss=9.0728
	step [153/255], loss=8.5604
	step [154/255], loss=7.7032
	step [155/255], loss=6.4555
	step [156/255], loss=7.4364
	step [157/255], loss=9.1145
	step [158/255], loss=6.9056
	step [159/255], loss=8.8882
	step [160/255], loss=6.0873
	step [161/255], loss=6.8965
	step [162/255], loss=7.7807
	step [163/255], loss=8.6475
	step [164/255], loss=6.6382
	step [165/255], loss=9.4305
	step [166/255], loss=7.8549
	step [167/255], loss=7.6301
	step [168/255], loss=6.8689
	step [169/255], loss=8.3740
	step [170/255], loss=8.2403
	step [171/255], loss=10.1021
	step [172/255], loss=8.3826
	step [173/255], loss=9.4116
	step [174/255], loss=8.5741
	step [175/255], loss=7.4260
	step [176/255], loss=10.6993
	step [177/255], loss=7.6710
	step [178/255], loss=8.8411
	step [179/255], loss=8.0225
	step [180/255], loss=6.7629
	step [181/255], loss=6.7694
	step [182/255], loss=9.0201
	step [183/255], loss=7.9506
	step [184/255], loss=10.1877
	step [185/255], loss=9.4405
	step [186/255], loss=9.3649
	step [187/255], loss=7.4516
	step [188/255], loss=7.6056
	step [189/255], loss=8.3279
	step [190/255], loss=7.5363
	step [191/255], loss=8.6942
	step [192/255], loss=9.8839
	step [193/255], loss=7.0630
	step [194/255], loss=7.1776
	step [195/255], loss=8.1699
	step [196/255], loss=7.2984
	step [197/255], loss=9.5424
	step [198/255], loss=7.8633
	step [199/255], loss=10.2375
	step [200/255], loss=7.1557
	step [201/255], loss=9.1543
	step [202/255], loss=7.7293
	step [203/255], loss=8.3112
	step [204/255], loss=10.0988
	step [205/255], loss=8.9674
	step [206/255], loss=7.4783
	step [207/255], loss=5.9140
	step [208/255], loss=10.4512
	step [209/255], loss=8.5271
	step [210/255], loss=6.9838
	step [211/255], loss=9.1286
	step [212/255], loss=7.4956
	step [213/255], loss=6.0441
	step [214/255], loss=8.1190
	step [215/255], loss=7.6412
	step [216/255], loss=6.9440
	step [217/255], loss=7.6839
	step [218/255], loss=9.9567
	step [219/255], loss=8.6283
	step [220/255], loss=5.7528
	step [221/255], loss=7.2674
	step [222/255], loss=8.9790
	step [223/255], loss=8.7347
	step [224/255], loss=7.6892
	step [225/255], loss=6.8791
	step [226/255], loss=8.9958
	step [227/255], loss=7.8363
	step [228/255], loss=7.8394
	step [229/255], loss=10.1532
	step [230/255], loss=7.0494
	step [231/255], loss=7.8475
	step [232/255], loss=9.0633
	step [233/255], loss=8.3153
	step [234/255], loss=7.1134
	step [235/255], loss=6.4792
	step [236/255], loss=6.7015
	step [237/255], loss=8.0854
	step [238/255], loss=9.0492
	step [239/255], loss=8.6775
	step [240/255], loss=8.8835
	step [241/255], loss=7.2279
	step [242/255], loss=8.8029
	step [243/255], loss=7.7183
	step [244/255], loss=8.5921
	step [245/255], loss=9.1010
	step [246/255], loss=7.4952
	step [247/255], loss=8.7259
	step [248/255], loss=7.0333
	step [249/255], loss=7.1084
	step [250/255], loss=8.0925
	step [251/255], loss=6.9854
	step [252/255], loss=9.2020
	step [253/255], loss=5.3193
	step [254/255], loss=7.6954
	step [255/255], loss=6.0673
	Evaluating
	loss=0.0248, precision=0.1856, recall=0.9918, f1=0.6914
Training epoch 18
	step [1/255], loss=10.5890
	step [2/255], loss=9.1960
	step [3/255], loss=7.4102
	step [4/255], loss=7.7553
	step [5/255], loss=9.6166
	step [6/255], loss=10.8880
	step [7/255], loss=8.1259
	step [8/255], loss=6.7900
	step [9/255], loss=11.5687
	step [10/255], loss=8.2541
	step [11/255], loss=6.0114
	step [12/255], loss=7.3854
	step [13/255], loss=9.7971
	step [14/255], loss=8.4896
	step [15/255], loss=7.5455
	step [16/255], loss=7.1451
	step [17/255], loss=7.7397
	step [18/255], loss=8.7570
	step [19/255], loss=7.6247
	step [20/255], loss=9.0704
	step [21/255], loss=8.1289
	step [22/255], loss=7.3257
	step [23/255], loss=6.5346
	step [24/255], loss=6.8598
	step [25/255], loss=7.7293
	step [26/255], loss=6.8317
	step [27/255], loss=7.5167
	step [28/255], loss=6.9171
	step [29/255], loss=6.7798
	step [30/255], loss=6.7435
	step [31/255], loss=8.7079
	step [32/255], loss=9.3938
	step [33/255], loss=9.4544
	step [34/255], loss=7.8988
	step [35/255], loss=7.9425
	step [36/255], loss=9.4424
	step [37/255], loss=6.8647
	step [38/255], loss=8.7034
	step [39/255], loss=9.2575
	step [40/255], loss=6.6205
	step [41/255], loss=7.6347
	step [42/255], loss=7.7935
	step [43/255], loss=10.5837
	step [44/255], loss=10.8868
	step [45/255], loss=6.4353
	step [46/255], loss=6.8601
	step [47/255], loss=8.9638
	step [48/255], loss=7.7670
	step [49/255], loss=7.4410
	step [50/255], loss=8.6470
	step [51/255], loss=8.2290
	step [52/255], loss=8.8027
	step [53/255], loss=10.2169
	step [54/255], loss=6.6875
	step [55/255], loss=7.3681
	step [56/255], loss=8.1567
	step [57/255], loss=7.9866
	step [58/255], loss=8.5702
	step [59/255], loss=11.2767
	step [60/255], loss=5.8359
	step [61/255], loss=7.9664
	step [62/255], loss=8.1105
	step [63/255], loss=6.9980
	step [64/255], loss=7.0178
	step [65/255], loss=8.8588
	step [66/255], loss=8.3508
	step [67/255], loss=9.5795
	step [68/255], loss=6.6107
	step [69/255], loss=6.3804
	step [70/255], loss=9.6732
	step [71/255], loss=9.4477
	step [72/255], loss=6.6403
	step [73/255], loss=6.2686
	step [74/255], loss=7.2716
	step [75/255], loss=8.4026
	step [76/255], loss=6.4250
	step [77/255], loss=9.2297
	step [78/255], loss=9.5218
	step [79/255], loss=8.5889
	step [80/255], loss=9.3892
	step [81/255], loss=7.6752
	step [82/255], loss=6.7225
	step [83/255], loss=7.0126
	step [84/255], loss=6.2778
	step [85/255], loss=8.9919
	step [86/255], loss=9.0078
	step [87/255], loss=6.7549
	step [88/255], loss=5.7925
	step [89/255], loss=7.4773
	step [90/255], loss=8.5769
	step [91/255], loss=7.6180
	step [92/255], loss=7.6942
	step [93/255], loss=5.8545
	step [94/255], loss=8.3399
	step [95/255], loss=7.8319
	step [96/255], loss=8.3961
	step [97/255], loss=9.5506
	step [98/255], loss=7.5074
	step [99/255], loss=6.6894
	step [100/255], loss=8.6578
	step [101/255], loss=9.6755
	step [102/255], loss=6.8288
	step [103/255], loss=6.3798
	step [104/255], loss=9.3152
	step [105/255], loss=8.1358
	step [106/255], loss=7.1084
	step [107/255], loss=9.4803
	step [108/255], loss=7.0988
	step [109/255], loss=8.0555
	step [110/255], loss=7.7891
	step [111/255], loss=7.1018
	step [112/255], loss=8.4754
	step [113/255], loss=7.2161
	step [114/255], loss=11.4657
	step [115/255], loss=8.9367
	step [116/255], loss=8.8004
	step [117/255], loss=7.6337
	step [118/255], loss=7.8937
	step [119/255], loss=8.1772
	step [120/255], loss=7.7759
	step [121/255], loss=7.3368
	step [122/255], loss=8.2194
	step [123/255], loss=7.6089
	step [124/255], loss=6.6464
	step [125/255], loss=9.0097
	step [126/255], loss=8.6519
	step [127/255], loss=8.2639
	step [128/255], loss=8.2554
	step [129/255], loss=8.6488
	step [130/255], loss=7.7157
	step [131/255], loss=8.4817
	step [132/255], loss=6.5550
	step [133/255], loss=7.5756
	step [134/255], loss=11.4795
	step [135/255], loss=8.9535
	step [136/255], loss=7.6827
	step [137/255], loss=9.7223
	step [138/255], loss=7.0808
	step [139/255], loss=7.2505
	step [140/255], loss=6.1716
	step [141/255], loss=8.5502
	step [142/255], loss=7.1867
	step [143/255], loss=6.4220
	step [144/255], loss=7.3318
	step [145/255], loss=8.9870
	step [146/255], loss=9.9421
	step [147/255], loss=6.8280
	step [148/255], loss=8.1134
	step [149/255], loss=7.8905
	step [150/255], loss=8.6763
	step [151/255], loss=8.1919
	step [152/255], loss=8.7815
	step [153/255], loss=7.7964
	step [154/255], loss=6.6312
	step [155/255], loss=7.5483
	step [156/255], loss=6.8876
	step [157/255], loss=7.9740
	step [158/255], loss=9.7027
	step [159/255], loss=8.9330
	step [160/255], loss=9.0629
	step [161/255], loss=7.9924
	step [162/255], loss=7.2888
	step [163/255], loss=6.6554
	step [164/255], loss=7.7526
	step [165/255], loss=9.0170
	step [166/255], loss=7.8372
	step [167/255], loss=7.5986
	step [168/255], loss=8.6105
	step [169/255], loss=7.4933
	step [170/255], loss=5.8892
	step [171/255], loss=7.6191
	step [172/255], loss=8.3548
	step [173/255], loss=8.6635
	step [174/255], loss=5.2166
	step [175/255], loss=9.5539
	step [176/255], loss=7.4006
	step [177/255], loss=9.3887
	step [178/255], loss=7.9372
	step [179/255], loss=7.4917
	step [180/255], loss=9.5304
	step [181/255], loss=7.3639
	step [182/255], loss=8.7569
	step [183/255], loss=7.5672
	step [184/255], loss=6.5947
	step [185/255], loss=7.6771
	step [186/255], loss=7.4193
	step [187/255], loss=6.6828
	step [188/255], loss=6.5608
	step [189/255], loss=10.3750
	step [190/255], loss=7.2870
	step [191/255], loss=7.6567
	step [192/255], loss=7.6293
	step [193/255], loss=8.6183
	step [194/255], loss=6.1765
	step [195/255], loss=7.1155
	step [196/255], loss=8.0153
	step [197/255], loss=7.8395
	step [198/255], loss=10.1452
	step [199/255], loss=9.0584
	step [200/255], loss=7.5707
	step [201/255], loss=7.6995
	step [202/255], loss=6.9203
	step [203/255], loss=6.4814
	step [204/255], loss=7.3412
	step [205/255], loss=8.4843
	step [206/255], loss=8.1890
	step [207/255], loss=8.1666
	step [208/255], loss=7.8070
	step [209/255], loss=7.5223
	step [210/255], loss=5.6576
	step [211/255], loss=8.1279
	step [212/255], loss=7.7277
	step [213/255], loss=7.3571
	step [214/255], loss=9.0802
	step [215/255], loss=8.5152
	step [216/255], loss=8.1466
	step [217/255], loss=7.5859
	step [218/255], loss=6.8949
	step [219/255], loss=8.0991
	step [220/255], loss=7.6933
	step [221/255], loss=8.6846
	step [222/255], loss=5.9815
	step [223/255], loss=6.5590
	step [224/255], loss=7.4587
	step [225/255], loss=6.1525
	step [226/255], loss=10.3583
	step [227/255], loss=7.5667
	step [228/255], loss=8.2992
	step [229/255], loss=9.4114
	step [230/255], loss=7.9284
	step [231/255], loss=7.1501
	step [232/255], loss=6.6666
	step [233/255], loss=6.0073
	step [234/255], loss=5.8049
	step [235/255], loss=7.2706
	step [236/255], loss=8.2607
	step [237/255], loss=7.0517
	step [238/255], loss=6.9266
	step [239/255], loss=10.2202
	step [240/255], loss=9.7427
	step [241/255], loss=7.1872
	step [242/255], loss=7.6648
	step [243/255], loss=6.4525
	step [244/255], loss=8.1785
	step [245/255], loss=7.2822
	step [246/255], loss=7.5301
	step [247/255], loss=9.1181
	step [248/255], loss=9.1364
	step [249/255], loss=7.8988
	step [250/255], loss=8.5667
	step [251/255], loss=7.1477
	step [252/255], loss=8.1314
	step [253/255], loss=8.8417
	step [254/255], loss=7.4247
	step [255/255], loss=6.5655
	Evaluating
	loss=0.0287, precision=0.1536, recall=0.9941, f1=0.6425
Training epoch 19
	step [1/255], loss=7.7221
	step [2/255], loss=7.2304
	step [3/255], loss=5.9075
	step [4/255], loss=7.8215
	step [5/255], loss=5.9150
	step [6/255], loss=8.4765
	step [7/255], loss=6.7367
	step [8/255], loss=6.2790
	step [9/255], loss=7.4234
	step [10/255], loss=8.9773
	step [11/255], loss=5.9539
	step [12/255], loss=8.1235
	step [13/255], loss=6.7421
	step [14/255], loss=7.0917
	step [15/255], loss=7.8927
	step [16/255], loss=7.2527
	step [17/255], loss=7.5747
	step [18/255], loss=8.0075
	step [19/255], loss=6.7532
	step [20/255], loss=8.0983
	step [21/255], loss=7.2348
	step [22/255], loss=8.4620
	step [23/255], loss=7.3037
	step [24/255], loss=8.6789
	step [25/255], loss=7.3450
	step [26/255], loss=10.0593
	step [27/255], loss=8.1832
	step [28/255], loss=7.0186
	step [29/255], loss=7.1824
	step [30/255], loss=7.4193
	step [31/255], loss=6.1972
	step [32/255], loss=7.2839
	step [33/255], loss=8.1626
	step [34/255], loss=9.4932
	step [35/255], loss=9.6414
	step [36/255], loss=8.0855
	step [37/255], loss=7.1306
	step [38/255], loss=7.3755
	step [39/255], loss=8.3070
	step [40/255], loss=10.1796
	step [41/255], loss=7.5565
	step [42/255], loss=7.1327
	step [43/255], loss=7.6986
	step [44/255], loss=6.6278
	step [45/255], loss=7.0064
	step [46/255], loss=7.1830
	step [47/255], loss=6.4904
	step [48/255], loss=8.2312
	step [49/255], loss=8.3785
	step [50/255], loss=9.0634
	step [51/255], loss=7.9891
	step [52/255], loss=7.2571
	step [53/255], loss=7.2043
	step [54/255], loss=6.2505
	step [55/255], loss=6.6512
	step [56/255], loss=6.8814
	step [57/255], loss=7.7332
	step [58/255], loss=7.9205
	step [59/255], loss=8.9209
	step [60/255], loss=7.3864
	step [61/255], loss=6.2168
	step [62/255], loss=8.9656
	step [63/255], loss=7.2960
	step [64/255], loss=7.6757
	step [65/255], loss=6.1705
	step [66/255], loss=6.8468
	step [67/255], loss=6.6021
	step [68/255], loss=7.4651
	step [69/255], loss=9.1370
	step [70/255], loss=7.5620
	step [71/255], loss=9.1894
	step [72/255], loss=5.4630
	step [73/255], loss=6.1939
	step [74/255], loss=7.2950
	step [75/255], loss=6.3320
	step [76/255], loss=6.9412
	step [77/255], loss=6.6217
	step [78/255], loss=7.2586
	step [79/255], loss=8.1930
	step [80/255], loss=6.3066
	step [81/255], loss=5.9491
	step [82/255], loss=7.6923
	step [83/255], loss=6.0117
	step [84/255], loss=8.4276
	step [85/255], loss=9.1151
	step [86/255], loss=5.8606
	step [87/255], loss=9.5259
	step [88/255], loss=7.2608
	step [89/255], loss=7.8018
	step [90/255], loss=8.1220
	step [91/255], loss=9.1696
	step [92/255], loss=7.2122
	step [93/255], loss=6.6063
	step [94/255], loss=9.3355
	step [95/255], loss=6.6099
	step [96/255], loss=9.3275
	step [97/255], loss=8.1919
	step [98/255], loss=5.6907
	step [99/255], loss=9.0198
	step [100/255], loss=10.4936
	step [101/255], loss=6.6814
	step [102/255], loss=7.2647
	step [103/255], loss=7.5722
	step [104/255], loss=8.0747
	step [105/255], loss=9.1379
	step [106/255], loss=8.1432
	step [107/255], loss=7.9472
	step [108/255], loss=8.3686
	step [109/255], loss=9.8473
	step [110/255], loss=5.7505
	step [111/255], loss=8.4346
	step [112/255], loss=8.2566
	step [113/255], loss=9.1157
	step [114/255], loss=7.5715
	step [115/255], loss=7.7001
	step [116/255], loss=7.1791
	step [117/255], loss=7.6101
	step [118/255], loss=9.2563
	step [119/255], loss=9.2313
	step [120/255], loss=7.7784
	step [121/255], loss=10.0992
	step [122/255], loss=6.7891
	step [123/255], loss=8.2628
	step [124/255], loss=8.8444
	step [125/255], loss=9.1221
	step [126/255], loss=7.0394
	step [127/255], loss=8.2362
	step [128/255], loss=9.3259
	step [129/255], loss=9.0753
	step [130/255], loss=8.5879
	step [131/255], loss=8.8492
	step [132/255], loss=6.8481
	step [133/255], loss=6.5143
	step [134/255], loss=7.1914
	step [135/255], loss=6.2386
	step [136/255], loss=9.9582
	step [137/255], loss=7.2579
	step [138/255], loss=7.0169
	step [139/255], loss=7.2636
	step [140/255], loss=7.3636
	step [141/255], loss=6.2996
	step [142/255], loss=6.4710
	step [143/255], loss=10.2560
	step [144/255], loss=7.4638
	step [145/255], loss=6.3873
	step [146/255], loss=7.9419
	step [147/255], loss=7.1514
	step [148/255], loss=7.9040
	step [149/255], loss=8.8089
	step [150/255], loss=8.0647
	step [151/255], loss=8.5416
	step [152/255], loss=7.6923
	step [153/255], loss=7.4905
	step [154/255], loss=7.9547
	step [155/255], loss=9.2406
	step [156/255], loss=7.0383
	step [157/255], loss=7.2692
	step [158/255], loss=7.4311
	step [159/255], loss=7.9302
	step [160/255], loss=6.9795
	step [161/255], loss=7.4517
	step [162/255], loss=6.9767
	step [163/255], loss=8.4949
	step [164/255], loss=6.6630
	step [165/255], loss=8.7809
	step [166/255], loss=6.9608
	step [167/255], loss=7.3713
	step [168/255], loss=7.1906
	step [169/255], loss=6.4638
	step [170/255], loss=8.3957
	step [171/255], loss=6.3442
	step [172/255], loss=7.6713
	step [173/255], loss=7.1269
	step [174/255], loss=5.7131
	step [175/255], loss=9.2178
	step [176/255], loss=8.0228
	step [177/255], loss=7.6929
	step [178/255], loss=7.4392
	step [179/255], loss=8.5568
	step [180/255], loss=8.2301
	step [181/255], loss=7.8359
	step [182/255], loss=9.0454
	step [183/255], loss=8.1173
	step [184/255], loss=9.0654
	step [185/255], loss=8.8184
	step [186/255], loss=7.9267
	step [187/255], loss=8.5725
	step [188/255], loss=7.8069
	step [189/255], loss=8.4308
	step [190/255], loss=10.3183
	step [191/255], loss=8.2379
	step [192/255], loss=6.8265
	step [193/255], loss=7.8181
	step [194/255], loss=9.0732
	step [195/255], loss=7.3115
	step [196/255], loss=7.0456
	step [197/255], loss=7.2740
	step [198/255], loss=7.3610
	step [199/255], loss=6.6060
	step [200/255], loss=9.0136
	step [201/255], loss=7.0867
	step [202/255], loss=8.4996
	step [203/255], loss=8.9905
	step [204/255], loss=6.2654
	step [205/255], loss=8.5534
	step [206/255], loss=8.7121
	step [207/255], loss=7.9443
	step [208/255], loss=10.1973
	step [209/255], loss=7.6773
	step [210/255], loss=9.3241
	step [211/255], loss=6.5662
	step [212/255], loss=10.3560
	step [213/255], loss=8.9976
	step [214/255], loss=7.6511
	step [215/255], loss=6.6175
	step [216/255], loss=8.9355
	step [217/255], loss=7.0144
	step [218/255], loss=7.2932
	step [219/255], loss=6.6477
	step [220/255], loss=7.0237
	step [221/255], loss=5.4354
	step [222/255], loss=7.9369
	step [223/255], loss=9.4293
	step [224/255], loss=7.3155
	step [225/255], loss=8.3754
	step [226/255], loss=6.1337
	step [227/255], loss=6.7748
	step [228/255], loss=7.0975
	step [229/255], loss=8.5136
	step [230/255], loss=7.8226
	step [231/255], loss=8.1836
	step [232/255], loss=5.8780
	step [233/255], loss=6.9443
	step [234/255], loss=10.2172
	step [235/255], loss=6.6778
	step [236/255], loss=7.2007
	step [237/255], loss=7.2491
	step [238/255], loss=6.6270
	step [239/255], loss=7.5491
	step [240/255], loss=8.6679
	step [241/255], loss=7.1096
	step [242/255], loss=8.3610
	step [243/255], loss=7.0653
	step [244/255], loss=8.2687
	step [245/255], loss=10.2854
	step [246/255], loss=5.1046
	step [247/255], loss=7.9180
	step [248/255], loss=6.7152
	step [249/255], loss=8.3186
	step [250/255], loss=7.1580
	step [251/255], loss=7.5330
	step [252/255], loss=8.0463
	step [253/255], loss=8.0512
	step [254/255], loss=6.9193
	step [255/255], loss=4.4531
	Evaluating
	loss=0.0289, precision=0.1440, recall=0.9939, f1=0.6251
Training epoch 20
	step [1/255], loss=8.3911
	step [2/255], loss=7.4217
	step [3/255], loss=5.9096
	step [4/255], loss=7.4441
	step [5/255], loss=6.8397
	step [6/255], loss=7.9209
	step [7/255], loss=7.6116
	step [8/255], loss=7.3676
	step [9/255], loss=8.9452
	step [10/255], loss=7.0423
	step [11/255], loss=7.0719
	step [12/255], loss=6.0014
	step [13/255], loss=5.2399
	step [14/255], loss=9.1601
	step [15/255], loss=6.9238
	step [16/255], loss=9.4577
	step [17/255], loss=6.5164
	step [18/255], loss=8.0823
	step [19/255], loss=9.4868
	step [20/255], loss=7.3894
	step [21/255], loss=7.8111
	step [22/255], loss=6.7119
	step [23/255], loss=5.7109
	step [24/255], loss=10.6477
	step [25/255], loss=7.7502
	step [26/255], loss=7.3399
	step [27/255], loss=7.2848
	step [28/255], loss=7.8656
	step [29/255], loss=7.9211
	step [30/255], loss=6.8885
	step [31/255], loss=7.2216
	step [32/255], loss=7.5022
	step [33/255], loss=6.2227
	step [34/255], loss=8.5908
	step [35/255], loss=7.4407
	step [36/255], loss=6.0629
	step [37/255], loss=8.1238
	step [38/255], loss=9.0492
	step [39/255], loss=7.5550
	step [40/255], loss=7.5556
	step [41/255], loss=8.2045
	step [42/255], loss=8.1814
	step [43/255], loss=7.5182
	step [44/255], loss=6.7900
	step [45/255], loss=5.2815
	step [46/255], loss=6.7993
	step [47/255], loss=6.9848
	step [48/255], loss=7.9608
	step [49/255], loss=8.8050
	step [50/255], loss=5.9083
	step [51/255], loss=8.7712
	step [52/255], loss=6.7587
	step [53/255], loss=7.0869
	step [54/255], loss=7.8768
	step [55/255], loss=6.7154
	step [56/255], loss=6.7304
	step [57/255], loss=7.8087
	step [58/255], loss=7.6529
	step [59/255], loss=6.1451
	step [60/255], loss=9.9838
	step [61/255], loss=7.5078
	step [62/255], loss=6.4083
	step [63/255], loss=7.6450
	step [64/255], loss=8.5083
	step [65/255], loss=8.2930
	step [66/255], loss=7.9753
	step [67/255], loss=9.0967
	step [68/255], loss=8.9037
	step [69/255], loss=11.1843
	step [70/255], loss=8.2378
	step [71/255], loss=7.3444
	step [72/255], loss=5.6722
	step [73/255], loss=8.0725
	step [74/255], loss=7.2082
	step [75/255], loss=5.7609
	step [76/255], loss=7.6654
	step [77/255], loss=9.1360
	step [78/255], loss=5.5894
	step [79/255], loss=7.9346
	step [80/255], loss=5.4818
	step [81/255], loss=7.2561
	step [82/255], loss=6.9980
	step [83/255], loss=8.3135
	step [84/255], loss=7.2295
	step [85/255], loss=13.6369
	step [86/255], loss=8.1024
	step [87/255], loss=6.7162
	step [88/255], loss=8.2822
	step [89/255], loss=7.6065
	step [90/255], loss=8.6249
	step [91/255], loss=6.7999
	step [92/255], loss=10.7788
	step [93/255], loss=6.1907
	step [94/255], loss=7.6089
	step [95/255], loss=6.3770
	step [96/255], loss=8.0272
	step [97/255], loss=9.3906
	step [98/255], loss=7.5901
	step [99/255], loss=6.4008
	step [100/255], loss=7.3802
	step [101/255], loss=7.4897
	step [102/255], loss=8.0505
	step [103/255], loss=9.1121
	step [104/255], loss=7.7863
	step [105/255], loss=8.0680
	step [106/255], loss=8.9629
	step [107/255], loss=8.6889
	step [108/255], loss=8.9171
	step [109/255], loss=8.9165
	step [110/255], loss=6.8934
	step [111/255], loss=8.5261
	step [112/255], loss=9.9580
	step [113/255], loss=7.6550
	step [114/255], loss=6.8576
	step [115/255], loss=9.2196
	step [116/255], loss=6.7990
	step [117/255], loss=8.6851
	step [118/255], loss=7.4354
	step [119/255], loss=7.3432
	step [120/255], loss=7.7339
	step [121/255], loss=8.2638
	step [122/255], loss=7.2531
	step [123/255], loss=7.7916
	step [124/255], loss=8.2572
	step [125/255], loss=7.6861
	step [126/255], loss=8.8383
	step [127/255], loss=6.8466
	step [128/255], loss=9.8675
	step [129/255], loss=6.2659
	step [130/255], loss=7.2957
	step [131/255], loss=7.0272
	step [132/255], loss=7.1189
	step [133/255], loss=6.1435
	step [134/255], loss=8.0545
	step [135/255], loss=7.8744
	step [136/255], loss=7.2798
	step [137/255], loss=8.2953
	step [138/255], loss=5.9920
	step [139/255], loss=6.4560
	step [140/255], loss=6.1861
	step [141/255], loss=7.7183
	step [142/255], loss=8.7695
	step [143/255], loss=6.5835
	step [144/255], loss=6.8726
	step [145/255], loss=6.9252
	step [146/255], loss=7.8180
	step [147/255], loss=7.9450
	step [148/255], loss=7.7321
	step [149/255], loss=8.2189
	step [150/255], loss=7.3016
	step [151/255], loss=7.3308
	step [152/255], loss=6.2510
	step [153/255], loss=8.9057
	step [154/255], loss=6.5118
	step [155/255], loss=7.3637
	step [156/255], loss=8.8613
	step [157/255], loss=5.5625
	step [158/255], loss=7.9430
	step [159/255], loss=7.3078
	step [160/255], loss=8.3116
	step [161/255], loss=8.2223
	step [162/255], loss=6.7995
	step [163/255], loss=8.4683
	step [164/255], loss=6.8840
	step [165/255], loss=7.1951
	step [166/255], loss=7.8211
	step [167/255], loss=8.3306
	step [168/255], loss=7.0333
	step [169/255], loss=7.2523
	step [170/255], loss=6.6527
	step [171/255], loss=6.5111
	step [172/255], loss=7.0660
	step [173/255], loss=7.3000
	step [174/255], loss=8.0481
	step [175/255], loss=7.4295
	step [176/255], loss=7.3725
	step [177/255], loss=8.8371
	step [178/255], loss=6.8071
	step [179/255], loss=7.5780
	step [180/255], loss=8.0565
	step [181/255], loss=8.2280
	step [182/255], loss=6.4231
	step [183/255], loss=5.3324
	step [184/255], loss=7.8041
	step [185/255], loss=7.1994
	step [186/255], loss=8.6863
	step [187/255], loss=6.5248
	step [188/255], loss=7.1616
	step [189/255], loss=8.4293
	step [190/255], loss=7.4883
	step [191/255], loss=7.2988
	step [192/255], loss=7.8040
	step [193/255], loss=8.4115
	step [194/255], loss=6.6184
	step [195/255], loss=7.1887
	step [196/255], loss=7.2908
	step [197/255], loss=7.9997
	step [198/255], loss=6.6575
	step [199/255], loss=6.4046
	step [200/255], loss=9.2168
	step [201/255], loss=7.2560
	step [202/255], loss=7.1929
	step [203/255], loss=6.7891
	step [204/255], loss=6.6135
	step [205/255], loss=9.0964
	step [206/255], loss=7.9672
	step [207/255], loss=7.0391
	step [208/255], loss=8.5293
	step [209/255], loss=6.8579
	step [210/255], loss=7.7145
	step [211/255], loss=8.0406
	step [212/255], loss=6.5302
	step [213/255], loss=6.7366
	step [214/255], loss=6.8386
	step [215/255], loss=9.4222
	step [216/255], loss=6.9091
	step [217/255], loss=7.4366
	step [218/255], loss=6.9004
	step [219/255], loss=5.7342
	step [220/255], loss=8.1385
	step [221/255], loss=6.7384
	step [222/255], loss=7.6745
	step [223/255], loss=6.4636
	step [224/255], loss=9.0050
	step [225/255], loss=5.9825
	step [226/255], loss=6.3120
	step [227/255], loss=5.1870
	step [228/255], loss=6.3989
	step [229/255], loss=8.6178
	step [230/255], loss=9.4213
	step [231/255], loss=7.5120
	step [232/255], loss=7.7704
	step [233/255], loss=9.1923
	step [234/255], loss=5.0569
	step [235/255], loss=5.6635
	step [236/255], loss=6.8611
	step [237/255], loss=7.6927
	step [238/255], loss=7.0929
	step [239/255], loss=8.7377
	step [240/255], loss=8.4299
	step [241/255], loss=6.8053
	step [242/255], loss=6.3583
	step [243/255], loss=6.9776
	step [244/255], loss=7.0061
	step [245/255], loss=4.7495
	step [246/255], loss=9.4044
	step [247/255], loss=6.1982
	step [248/255], loss=8.6253
	step [249/255], loss=6.1968
	step [250/255], loss=6.6645
	step [251/255], loss=6.5264
	step [252/255], loss=6.3424
	step [253/255], loss=7.8385
	step [254/255], loss=8.0546
	step [255/255], loss=6.7610
	Evaluating
	loss=0.0230, precision=0.1891, recall=0.9920, f1=0.6963
Training epoch 21
	step [1/255], loss=7.7608
	step [2/255], loss=6.6589
	step [3/255], loss=6.8540
	step [4/255], loss=7.4041
	step [5/255], loss=8.4909
	step [6/255], loss=8.4957
	step [7/255], loss=8.0072
	step [8/255], loss=7.7262
	step [9/255], loss=6.7494
	step [10/255], loss=6.4216
	step [11/255], loss=6.3810
	step [12/255], loss=7.6482
	step [13/255], loss=7.9274
	step [14/255], loss=6.1972
	step [15/255], loss=6.4916
	step [16/255], loss=8.7336
	step [17/255], loss=7.0886
	step [18/255], loss=7.5736
	step [19/255], loss=7.6707
	step [20/255], loss=6.9859
	step [21/255], loss=8.4793
	step [22/255], loss=6.3105
	step [23/255], loss=6.6005
	step [24/255], loss=8.9469
	step [25/255], loss=7.9152
	step [26/255], loss=6.8609
	step [27/255], loss=6.2837
	step [28/255], loss=7.0306
	step [29/255], loss=7.5884
	step [30/255], loss=6.5782
	step [31/255], loss=9.2474
	step [32/255], loss=8.3711
	step [33/255], loss=7.9682
	step [34/255], loss=6.6174
	step [35/255], loss=7.4337
	step [36/255], loss=6.9236
	step [37/255], loss=6.8670
	step [38/255], loss=7.7408
	step [39/255], loss=5.6726
	step [40/255], loss=7.1309
	step [41/255], loss=6.5582
	step [42/255], loss=6.6366
	step [43/255], loss=7.3053
	step [44/255], loss=6.3403
	step [45/255], loss=6.6017
	step [46/255], loss=8.1425
	step [47/255], loss=6.6148
	step [48/255], loss=6.1235
	step [49/255], loss=6.0054
	step [50/255], loss=5.4956
	step [51/255], loss=7.5548
	step [52/255], loss=7.3723
	step [53/255], loss=6.3052
	step [54/255], loss=6.5454
	step [55/255], loss=6.6647
	step [56/255], loss=9.3734
	step [57/255], loss=6.0854
	step [58/255], loss=6.4538
	step [59/255], loss=7.4085
	step [60/255], loss=7.8651
	step [61/255], loss=6.1686
	step [62/255], loss=7.3669
	step [63/255], loss=7.6580
	step [64/255], loss=6.9699
	step [65/255], loss=8.0088
	step [66/255], loss=6.9424
	step [67/255], loss=6.9800
	step [68/255], loss=8.4778
	step [69/255], loss=7.7695
	step [70/255], loss=6.7737
	step [71/255], loss=8.2279
	step [72/255], loss=7.2545
	step [73/255], loss=6.6671
	step [74/255], loss=5.9429
	step [75/255], loss=6.3976
	step [76/255], loss=6.6995
	step [77/255], loss=6.8104
	step [78/255], loss=10.2344
	step [79/255], loss=6.5135
	step [80/255], loss=8.1497
	step [81/255], loss=6.7847
	step [82/255], loss=6.4471
	step [83/255], loss=6.2294
	step [84/255], loss=5.4307
	step [85/255], loss=7.4234
	step [86/255], loss=7.4561
	step [87/255], loss=7.1817
	step [88/255], loss=5.6446
	step [89/255], loss=8.5146
	step [90/255], loss=8.5401
	step [91/255], loss=7.8966
	step [92/255], loss=7.1421
	step [93/255], loss=5.9781
	step [94/255], loss=6.8343
	step [95/255], loss=6.8839
	step [96/255], loss=6.8960
	step [97/255], loss=7.1214
	step [98/255], loss=8.2668
	step [99/255], loss=9.5797
	step [100/255], loss=7.5769
	step [101/255], loss=7.2106
	step [102/255], loss=6.5418
	step [103/255], loss=7.1631
	step [104/255], loss=6.3412
	step [105/255], loss=7.0838
	step [106/255], loss=6.2065
	step [107/255], loss=6.1857
	step [108/255], loss=6.9275
	step [109/255], loss=10.4774
	step [110/255], loss=7.8434
	step [111/255], loss=6.0996
	step [112/255], loss=7.0305
	step [113/255], loss=7.2344
	step [114/255], loss=7.3013
	step [115/255], loss=6.5612
	step [116/255], loss=8.8047
	step [117/255], loss=5.0561
	step [118/255], loss=7.5895
	step [119/255], loss=7.8106
	step [120/255], loss=6.8149
	step [121/255], loss=8.1569
	step [122/255], loss=6.5701
	step [123/255], loss=6.8419
	step [124/255], loss=7.7953
	step [125/255], loss=5.8080
	step [126/255], loss=7.3642
	step [127/255], loss=6.1825
	step [128/255], loss=7.2181
	step [129/255], loss=7.5303
	step [130/255], loss=6.3076
	step [131/255], loss=6.7186
	step [132/255], loss=8.1392
	step [133/255], loss=6.3180
	step [134/255], loss=6.2412
	step [135/255], loss=8.2274
	step [136/255], loss=5.0095
	step [137/255], loss=5.4692
	step [138/255], loss=6.3731
	step [139/255], loss=8.6278
	step [140/255], loss=9.0714
	step [141/255], loss=6.8834
	step [142/255], loss=7.6156
	step [143/255], loss=8.7299
	step [144/255], loss=6.8656
	step [145/255], loss=7.1574
	step [146/255], loss=11.1372
	step [147/255], loss=5.3780
	step [148/255], loss=8.0746
	step [149/255], loss=7.6271
	step [150/255], loss=6.1045
	step [151/255], loss=6.9150
	step [152/255], loss=6.6326
	step [153/255], loss=6.3349
	step [154/255], loss=5.8906
	step [155/255], loss=6.8025
	step [156/255], loss=7.4683
	step [157/255], loss=6.7483
	step [158/255], loss=6.9433
	step [159/255], loss=7.1457
	step [160/255], loss=6.4471
	step [161/255], loss=5.5428
	step [162/255], loss=6.1502
	step [163/255], loss=6.2934
	step [164/255], loss=9.4767
	step [165/255], loss=7.5661
	step [166/255], loss=7.8410
	step [167/255], loss=5.8628
	step [168/255], loss=7.6727
	step [169/255], loss=8.0799
	step [170/255], loss=8.0414
	step [171/255], loss=10.0556
	step [172/255], loss=6.1580
	step [173/255], loss=8.7277
	step [174/255], loss=7.6783
	step [175/255], loss=8.2596
	step [176/255], loss=8.9964
	step [177/255], loss=8.0436
	step [178/255], loss=8.4739
	step [179/255], loss=6.9659
	step [180/255], loss=7.0854
	step [181/255], loss=7.0729
	step [182/255], loss=7.2412
	step [183/255], loss=7.1185
	step [184/255], loss=8.3209
	step [185/255], loss=5.6502
	step [186/255], loss=7.7037
	step [187/255], loss=6.4076
	step [188/255], loss=7.6203
	step [189/255], loss=8.4176
	step [190/255], loss=6.7674
	step [191/255], loss=6.8762
	step [192/255], loss=9.0662
	step [193/255], loss=6.2734
	step [194/255], loss=6.1763
	step [195/255], loss=5.8945
	step [196/255], loss=6.9133
	step [197/255], loss=7.6520
	step [198/255], loss=8.7300
	step [199/255], loss=7.4420
	step [200/255], loss=5.8252
	step [201/255], loss=6.1698
	step [202/255], loss=7.0823
	step [203/255], loss=10.0607
	step [204/255], loss=9.2207
	step [205/255], loss=6.2152
	step [206/255], loss=6.4299
	step [207/255], loss=7.5758
	step [208/255], loss=6.4338
	step [209/255], loss=7.8849
	step [210/255], loss=7.3901
	step [211/255], loss=7.2360
	step [212/255], loss=7.8908
	step [213/255], loss=7.9062
	step [214/255], loss=7.6004
	step [215/255], loss=8.1742
	step [216/255], loss=8.2412
	step [217/255], loss=6.6862
	step [218/255], loss=6.9610
	step [219/255], loss=7.3407
	step [220/255], loss=7.0779
	step [221/255], loss=5.5533
	step [222/255], loss=5.6157
	step [223/255], loss=7.8777
	step [224/255], loss=7.8867
	step [225/255], loss=6.5407
	step [226/255], loss=6.4409
	step [227/255], loss=7.5459
	step [228/255], loss=8.4227
	step [229/255], loss=5.9723
	step [230/255], loss=6.2651
	step [231/255], loss=7.5800
	step [232/255], loss=8.8078
	step [233/255], loss=7.6100
	step [234/255], loss=8.6359
	step [235/255], loss=6.6086
	step [236/255], loss=7.9257
	step [237/255], loss=7.8461
	step [238/255], loss=6.8857
	step [239/255], loss=7.5026
	step [240/255], loss=8.6793
	step [241/255], loss=5.8773
	step [242/255], loss=6.9703
	step [243/255], loss=7.6167
	step [244/255], loss=6.7621
	step [245/255], loss=6.8110
	step [246/255], loss=6.8530
	step [247/255], loss=6.6708
	step [248/255], loss=5.7850
	step [249/255], loss=7.3291
	step [250/255], loss=9.7825
	step [251/255], loss=6.3656
	step [252/255], loss=7.3454
	step [253/255], loss=7.5920
	step [254/255], loss=10.0597
	step [255/255], loss=4.7893
	Evaluating
	loss=0.0254, precision=0.1626, recall=0.9928, f1=0.6572
Training epoch 22
	step [1/255], loss=7.5502
	step [2/255], loss=7.5226
	step [3/255], loss=7.5074
	step [4/255], loss=6.8507
	step [5/255], loss=8.1983
	step [6/255], loss=5.9152
	step [7/255], loss=6.1313
	step [8/255], loss=8.7904
	step [9/255], loss=6.6855
	step [10/255], loss=6.9808
	step [11/255], loss=5.9490
	step [12/255], loss=7.7772
	step [13/255], loss=5.3751
	step [14/255], loss=7.2902
	step [15/255], loss=6.4133
	step [16/255], loss=6.2696
	step [17/255], loss=5.5052
	step [18/255], loss=7.7631
	step [19/255], loss=8.1996
	step [20/255], loss=6.2262
	step [21/255], loss=5.7383
	step [22/255], loss=7.2435
	step [23/255], loss=7.5633
	step [24/255], loss=6.7191
	step [25/255], loss=6.4100
	step [26/255], loss=5.8130
	step [27/255], loss=7.6409
	step [28/255], loss=6.3552
	step [29/255], loss=7.8563
	step [30/255], loss=7.6848
	step [31/255], loss=7.3887
	step [32/255], loss=6.8804
	step [33/255], loss=8.3340
	step [34/255], loss=6.8097
	step [35/255], loss=8.5119
	step [36/255], loss=5.6298
	step [37/255], loss=6.5590
	step [38/255], loss=7.3029
	step [39/255], loss=6.6363
	step [40/255], loss=8.2709
	step [41/255], loss=6.2940
	step [42/255], loss=8.0936
	step [43/255], loss=4.9405
	step [44/255], loss=7.5985
	step [45/255], loss=6.5001
	step [46/255], loss=6.0263
	step [47/255], loss=7.5573
	step [48/255], loss=7.5501
	step [49/255], loss=6.4145
	step [50/255], loss=5.9643
	step [51/255], loss=5.5659
	step [52/255], loss=8.0725
	step [53/255], loss=6.8901
	step [54/255], loss=5.4614
	step [55/255], loss=5.5731
	step [56/255], loss=6.3467
	step [57/255], loss=9.1355
	step [58/255], loss=6.8177
	step [59/255], loss=6.7075
	step [60/255], loss=6.5269
	step [61/255], loss=8.3659
	step [62/255], loss=7.4019
	step [63/255], loss=8.1924
	step [64/255], loss=7.1489
	step [65/255], loss=7.4346
	step [66/255], loss=7.2837
	step [67/255], loss=5.8854
	step [68/255], loss=7.2204
	step [69/255], loss=9.2138
	step [70/255], loss=9.3863
	step [71/255], loss=6.4085
	step [72/255], loss=7.3783
	step [73/255], loss=6.5238
	step [74/255], loss=7.0912
	step [75/255], loss=6.6759
	step [76/255], loss=6.8608
	step [77/255], loss=9.6846
	step [78/255], loss=7.4336
	step [79/255], loss=6.9393
	step [80/255], loss=6.5544
	step [81/255], loss=8.0692
	step [82/255], loss=8.8496
	step [83/255], loss=5.7861
	step [84/255], loss=6.3801
	step [85/255], loss=8.3702
	step [86/255], loss=8.3044
	step [87/255], loss=6.1516
	step [88/255], loss=7.0835
	step [89/255], loss=6.2337
	step [90/255], loss=7.3355
	step [91/255], loss=5.4145
	step [92/255], loss=8.3694
	step [93/255], loss=6.3722
	step [94/255], loss=4.8459
	step [95/255], loss=6.7017
	step [96/255], loss=6.8344
	step [97/255], loss=6.4721
	step [98/255], loss=6.4853
	step [99/255], loss=8.1212
	step [100/255], loss=7.8860
	step [101/255], loss=8.0591
	step [102/255], loss=8.7907
	step [103/255], loss=7.4819
	step [104/255], loss=6.5797
	step [105/255], loss=6.1958
	step [106/255], loss=7.3095
	step [107/255], loss=7.8782
	step [108/255], loss=6.1867
	step [109/255], loss=6.9750
	step [110/255], loss=8.0846
	step [111/255], loss=7.0398
	step [112/255], loss=6.3204
	step [113/255], loss=6.9211
	step [114/255], loss=7.8220
	step [115/255], loss=8.1133
	step [116/255], loss=5.9985
	step [117/255], loss=6.7412
	step [118/255], loss=8.2994
	step [119/255], loss=7.5521
	step [120/255], loss=7.1459
	step [121/255], loss=6.9718
	step [122/255], loss=6.8093
	step [123/255], loss=7.2178
	step [124/255], loss=8.8160
	step [125/255], loss=7.3854
	step [126/255], loss=6.4713
	step [127/255], loss=8.3451
	step [128/255], loss=7.4125
	step [129/255], loss=5.8442
	step [130/255], loss=7.4865
	step [131/255], loss=7.5040
	step [132/255], loss=7.0955
	step [133/255], loss=6.0370
	step [134/255], loss=7.5812
	step [135/255], loss=9.1291
	step [136/255], loss=7.2193
	step [137/255], loss=7.4071
	step [138/255], loss=6.1532
	step [139/255], loss=7.2244
	step [140/255], loss=7.6505
	step [141/255], loss=7.5006
	step [142/255], loss=7.9392
	step [143/255], loss=6.9626
	step [144/255], loss=7.5328
	step [145/255], loss=5.6974
	step [146/255], loss=7.3309
	step [147/255], loss=6.1532
	step [148/255], loss=7.8801
	step [149/255], loss=5.6916
	step [150/255], loss=6.3295
	step [151/255], loss=6.5898
	step [152/255], loss=6.7605
	step [153/255], loss=7.2030
	step [154/255], loss=7.7788
	step [155/255], loss=8.2766
	step [156/255], loss=6.2709
	step [157/255], loss=6.5436
	step [158/255], loss=7.5092
	step [159/255], loss=7.5578
	step [160/255], loss=7.7257
	step [161/255], loss=7.3342
	step [162/255], loss=7.5942
	step [163/255], loss=7.3797
	step [164/255], loss=8.4809
	step [165/255], loss=6.1882
	step [166/255], loss=4.6828
	step [167/255], loss=6.0442
	step [168/255], loss=6.5520
	step [169/255], loss=8.2970
	step [170/255], loss=7.1734
	step [171/255], loss=6.3448
	step [172/255], loss=9.1966
	step [173/255], loss=7.1059
	step [174/255], loss=6.4089
	step [175/255], loss=7.6520
	step [176/255], loss=7.8765
	step [177/255], loss=9.1180
	step [178/255], loss=8.4923
	step [179/255], loss=7.3223
	step [180/255], loss=6.2729
	step [181/255], loss=7.0514
	step [182/255], loss=8.5113
	step [183/255], loss=5.6214
	step [184/255], loss=8.2870
	step [185/255], loss=6.1070
	step [186/255], loss=8.6123
	step [187/255], loss=8.7142
	step [188/255], loss=6.9661
	step [189/255], loss=7.2837
	step [190/255], loss=6.6233
	step [191/255], loss=6.4038
	step [192/255], loss=6.5801
	step [193/255], loss=8.5069
	step [194/255], loss=6.1189
	step [195/255], loss=8.0191
	step [196/255], loss=6.9005
	step [197/255], loss=7.4845
	step [198/255], loss=6.0685
	step [199/255], loss=6.8755
	step [200/255], loss=5.5980
	step [201/255], loss=7.3472
	step [202/255], loss=8.4847
	step [203/255], loss=6.4822
	step [204/255], loss=5.4733
	step [205/255], loss=8.2095
	step [206/255], loss=6.3307
	step [207/255], loss=6.5301
	step [208/255], loss=6.1220
	step [209/255], loss=6.5176
	step [210/255], loss=6.9416
	step [211/255], loss=6.7814
	step [212/255], loss=7.9771
	step [213/255], loss=5.5234
	step [214/255], loss=7.2042
	step [215/255], loss=6.8526
	step [216/255], loss=6.9864
	step [217/255], loss=7.7691
	step [218/255], loss=7.3662
	step [219/255], loss=6.2083
	step [220/255], loss=6.7372
	step [221/255], loss=6.8875
	step [222/255], loss=5.7587
	step [223/255], loss=8.0831
	step [224/255], loss=7.5182
	step [225/255], loss=6.2238
	step [226/255], loss=7.1633
	step [227/255], loss=7.4002
	step [228/255], loss=5.7959
	step [229/255], loss=7.0809
	step [230/255], loss=8.0772
	step [231/255], loss=6.4755
	step [232/255], loss=5.9475
	step [233/255], loss=6.9817
	step [234/255], loss=6.9181
	step [235/255], loss=7.8337
	step [236/255], loss=6.2233
	step [237/255], loss=7.3536
	step [238/255], loss=7.2368
	step [239/255], loss=6.0494
	step [240/255], loss=6.7304
	step [241/255], loss=5.9665
	step [242/255], loss=6.1180
	step [243/255], loss=8.0450
	step [244/255], loss=6.9888
	step [245/255], loss=8.9882
	step [246/255], loss=5.5205
	step [247/255], loss=6.7959
	step [248/255], loss=7.4434
	step [249/255], loss=7.5794
	step [250/255], loss=8.9338
	step [251/255], loss=8.4349
	step [252/255], loss=6.5430
	step [253/255], loss=7.3377
	step [254/255], loss=6.1776
	step [255/255], loss=4.3166
	Evaluating
	loss=0.0204, precision=0.2062, recall=0.9914, f1=0.7180
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/255], loss=6.4335
	step [2/255], loss=6.7346
	step [3/255], loss=6.5035
	step [4/255], loss=6.5993
	step [5/255], loss=8.1520
	step [6/255], loss=8.2711
	step [7/255], loss=5.4895
	step [8/255], loss=6.2305
	step [9/255], loss=6.0646
	step [10/255], loss=7.3056
	step [11/255], loss=5.7298
	step [12/255], loss=6.2736
	step [13/255], loss=7.8112
	step [14/255], loss=5.8907
	step [15/255], loss=6.4792
	step [16/255], loss=6.6657
	step [17/255], loss=8.3396
	step [18/255], loss=5.9279
	step [19/255], loss=5.9801
	step [20/255], loss=8.9432
	step [21/255], loss=7.4329
	step [22/255], loss=7.1418
	step [23/255], loss=5.7453
	step [24/255], loss=5.7831
	step [25/255], loss=5.6945
	step [26/255], loss=6.9898
	step [27/255], loss=6.9810
	step [28/255], loss=6.3303
	step [29/255], loss=8.5898
	step [30/255], loss=5.7016
	step [31/255], loss=6.7447
	step [32/255], loss=5.8877
	step [33/255], loss=6.6789
	step [34/255], loss=8.7556
	step [35/255], loss=7.9367
	step [36/255], loss=5.1919
	step [37/255], loss=5.8790
	step [38/255], loss=6.2735
	step [39/255], loss=6.9410
	step [40/255], loss=5.4004
	step [41/255], loss=7.3855
	step [42/255], loss=7.6512
	step [43/255], loss=7.1143
	step [44/255], loss=9.3252
	step [45/255], loss=5.8612
	step [46/255], loss=6.3658
	step [47/255], loss=6.0610
	step [48/255], loss=5.6251
	step [49/255], loss=6.1724
	step [50/255], loss=7.7809
	step [51/255], loss=5.7225
	step [52/255], loss=6.4182
	step [53/255], loss=8.0183
	step [54/255], loss=8.7051
	step [55/255], loss=6.9216
	step [56/255], loss=6.2909
	step [57/255], loss=8.1881
	step [58/255], loss=6.4354
	step [59/255], loss=7.8957
	step [60/255], loss=7.5069
	step [61/255], loss=8.7763
	step [62/255], loss=8.2072
	step [63/255], loss=6.6358
	step [64/255], loss=6.1897
	step [65/255], loss=6.8838
	step [66/255], loss=6.3260
	step [67/255], loss=6.6405
	step [68/255], loss=6.3471
	step [69/255], loss=6.6467
	step [70/255], loss=6.5569
	step [71/255], loss=5.9765
	step [72/255], loss=6.3167
	step [73/255], loss=8.5852
	step [74/255], loss=6.5753
	step [75/255], loss=8.3636
	step [76/255], loss=7.6378
	step [77/255], loss=7.9779
	step [78/255], loss=5.5063
	step [79/255], loss=6.2140
	step [80/255], loss=6.2449
	step [81/255], loss=5.9379
	step [82/255], loss=6.8324
	step [83/255], loss=8.0268
	step [84/255], loss=7.4558
	step [85/255], loss=7.4493
	step [86/255], loss=6.5181
	step [87/255], loss=6.1512
	step [88/255], loss=6.1774
	step [89/255], loss=7.2561
	step [90/255], loss=6.9667
	step [91/255], loss=6.0383
	step [92/255], loss=7.9627
	step [93/255], loss=7.3647
	step [94/255], loss=7.7970
	step [95/255], loss=6.4555
	step [96/255], loss=6.3132
	step [97/255], loss=6.3311
	step [98/255], loss=7.0728
	step [99/255], loss=6.3060
	step [100/255], loss=6.0048
	step [101/255], loss=6.0527
	step [102/255], loss=9.2383
	step [103/255], loss=8.0275
	step [104/255], loss=7.5438
	step [105/255], loss=6.4263
	step [106/255], loss=7.7559
	step [107/255], loss=7.2023
	step [108/255], loss=6.5405
	step [109/255], loss=6.5323
	step [110/255], loss=7.1162
	step [111/255], loss=9.3720
	step [112/255], loss=6.6544
	step [113/255], loss=7.3503
	step [114/255], loss=5.8749
	step [115/255], loss=7.8100
	step [116/255], loss=8.7235
	step [117/255], loss=9.7148
	step [118/255], loss=7.1343
	step [119/255], loss=7.6447
	step [120/255], loss=7.1251
	step [121/255], loss=6.8647
	step [122/255], loss=8.5942
	step [123/255], loss=7.5098
	step [124/255], loss=6.3941
	step [125/255], loss=4.7640
	step [126/255], loss=8.1858
	step [127/255], loss=7.8277
	step [128/255], loss=6.8811
	step [129/255], loss=7.6297
	step [130/255], loss=6.5363
	step [131/255], loss=5.3371
	step [132/255], loss=8.2257
	step [133/255], loss=6.7225
	step [134/255], loss=7.3498
	step [135/255], loss=6.7822
	step [136/255], loss=7.2267
	step [137/255], loss=5.7976
	step [138/255], loss=5.9279
	step [139/255], loss=6.7010
	step [140/255], loss=6.6389
	step [141/255], loss=7.1398
	step [142/255], loss=6.0827
	step [143/255], loss=5.8948
	step [144/255], loss=7.5528
	step [145/255], loss=9.7842
	step [146/255], loss=7.1663
	step [147/255], loss=5.5212
	step [148/255], loss=6.2033
	step [149/255], loss=5.1986
	step [150/255], loss=6.1114
	step [151/255], loss=7.6368
	step [152/255], loss=6.7842
	step [153/255], loss=9.2354
	step [154/255], loss=6.2575
	step [155/255], loss=6.4997
	step [156/255], loss=7.2349
	step [157/255], loss=6.4324
	step [158/255], loss=5.9556
	step [159/255], loss=5.4537
	step [160/255], loss=6.3757
	step [161/255], loss=7.5601
	step [162/255], loss=9.4734
	step [163/255], loss=7.2412
	step [164/255], loss=6.2284
	step [165/255], loss=7.1485
	step [166/255], loss=5.9420
	step [167/255], loss=6.4048
	step [168/255], loss=7.9085
	step [169/255], loss=5.4442
	step [170/255], loss=8.5127
	step [171/255], loss=7.1672
	step [172/255], loss=6.0197
	step [173/255], loss=8.0786
	step [174/255], loss=5.9704
	step [175/255], loss=6.2532
	step [176/255], loss=8.1182
	step [177/255], loss=6.3157
	step [178/255], loss=6.2033
	step [179/255], loss=6.8508
	step [180/255], loss=6.8651
	step [181/255], loss=7.5042
	step [182/255], loss=6.1535
	step [183/255], loss=6.5740
	step [184/255], loss=6.7612
	step [185/255], loss=7.1732
	step [186/255], loss=6.1716
	step [187/255], loss=6.8491
	step [188/255], loss=6.9717
	step [189/255], loss=5.4882
	step [190/255], loss=7.7131
	step [191/255], loss=6.9131
	step [192/255], loss=7.9988
	step [193/255], loss=6.0345
	step [194/255], loss=8.4100
	step [195/255], loss=8.0033
	step [196/255], loss=5.7044
	step [197/255], loss=5.1655
	step [198/255], loss=7.2493
	step [199/255], loss=7.2472
	step [200/255], loss=4.7339
	step [201/255], loss=5.8352
	step [202/255], loss=5.4882
	step [203/255], loss=5.4530
	step [204/255], loss=8.2858
	step [205/255], loss=6.2327
	step [206/255], loss=6.5283
	step [207/255], loss=8.5321
	step [208/255], loss=7.8625
	step [209/255], loss=6.1666
	step [210/255], loss=6.4835
	step [211/255], loss=7.3409
	step [212/255], loss=6.0459
	step [213/255], loss=5.1959
	step [214/255], loss=6.7490
	step [215/255], loss=5.8999
	step [216/255], loss=7.2768
	step [217/255], loss=5.3831
	step [218/255], loss=5.9266
	step [219/255], loss=5.2004
	step [220/255], loss=8.4570
	step [221/255], loss=7.9533
	step [222/255], loss=6.8036
	step [223/255], loss=9.1698
	step [224/255], loss=5.6659
	step [225/255], loss=7.8483
	step [226/255], loss=7.8231
	step [227/255], loss=7.9380
	step [228/255], loss=8.3862
	step [229/255], loss=7.7428
	step [230/255], loss=7.0581
	step [231/255], loss=7.3932
	step [232/255], loss=6.8269
	step [233/255], loss=6.8116
	step [234/255], loss=7.6878
	step [235/255], loss=5.7978
	step [236/255], loss=6.8369
	step [237/255], loss=5.8711
	step [238/255], loss=7.0525
	step [239/255], loss=6.7370
	step [240/255], loss=5.8708
	step [241/255], loss=8.1261
	step [242/255], loss=6.9060
	step [243/255], loss=6.5113
	step [244/255], loss=7.8924
	step [245/255], loss=8.2005
	step [246/255], loss=7.0546
	step [247/255], loss=7.2580
	step [248/255], loss=6.9332
	step [249/255], loss=6.1350
	step [250/255], loss=6.5938
	step [251/255], loss=6.5971
	step [252/255], loss=5.7859
	step [253/255], loss=5.9837
	step [254/255], loss=9.4279
	step [255/255], loss=6.0712
	Evaluating
	loss=0.0243, precision=0.1815, recall=0.9925, f1=0.6860
Training epoch 24
	step [1/255], loss=6.1722
	step [2/255], loss=7.1276
	step [3/255], loss=6.4054
	step [4/255], loss=6.8952
	step [5/255], loss=6.4188
	step [6/255], loss=7.2627
	step [7/255], loss=6.0030
	step [8/255], loss=6.9390
	step [9/255], loss=5.0782
	step [10/255], loss=5.4171
	step [11/255], loss=6.2557
	step [12/255], loss=4.7766
	step [13/255], loss=7.1781
	step [14/255], loss=5.3036
	step [15/255], loss=6.0031
	step [16/255], loss=5.9875
	step [17/255], loss=8.4921
	step [18/255], loss=6.6199
	step [19/255], loss=7.2214
	step [20/255], loss=7.3896
	step [21/255], loss=8.0369
	step [22/255], loss=6.2828
	step [23/255], loss=7.1096
	step [24/255], loss=5.9144
	step [25/255], loss=6.6709
	step [26/255], loss=7.6839
	step [27/255], loss=6.5171
	step [28/255], loss=4.9863
	step [29/255], loss=7.3136
	step [30/255], loss=6.8487
	step [31/255], loss=5.9321
	step [32/255], loss=6.4815
	step [33/255], loss=6.4242
	step [34/255], loss=7.3820
	step [35/255], loss=6.6900
	step [36/255], loss=8.2501
	step [37/255], loss=7.2023
	step [38/255], loss=5.9813
	step [39/255], loss=6.5992
	step [40/255], loss=5.8349
	step [41/255], loss=6.8045
	step [42/255], loss=5.6774
	step [43/255], loss=6.7854
	step [44/255], loss=6.2900
	step [45/255], loss=5.3513
	step [46/255], loss=5.6719
	step [47/255], loss=8.4823
	step [48/255], loss=5.7831
	step [49/255], loss=6.3677
	step [50/255], loss=8.6037
	step [51/255], loss=6.2259
	step [52/255], loss=6.1475
	step [53/255], loss=7.7851
	step [54/255], loss=6.0803
	step [55/255], loss=9.1549
	step [56/255], loss=6.6697
	step [57/255], loss=7.6325
	step [58/255], loss=5.6557
	step [59/255], loss=5.9202
	step [60/255], loss=7.1070
	step [61/255], loss=6.6161
	step [62/255], loss=9.6094
	step [63/255], loss=9.5432
	step [64/255], loss=5.7244
	step [65/255], loss=7.7544
	step [66/255], loss=5.7487
	step [67/255], loss=7.1172
	step [68/255], loss=6.1213
	step [69/255], loss=7.8095
	step [70/255], loss=9.1102
	step [71/255], loss=7.7021
	step [72/255], loss=6.0496
	step [73/255], loss=5.7858
	step [74/255], loss=5.3425
	step [75/255], loss=7.5931
	step [76/255], loss=4.8700
	step [77/255], loss=6.4666
	step [78/255], loss=8.2203
	step [79/255], loss=6.1299
	step [80/255], loss=5.7550
	step [81/255], loss=5.9097
	step [82/255], loss=7.4637
	step [83/255], loss=5.6330
	step [84/255], loss=6.2032
	step [85/255], loss=7.5975
	step [86/255], loss=6.5821
	step [87/255], loss=7.0177
	step [88/255], loss=6.3646
	step [89/255], loss=6.7870
	step [90/255], loss=6.5164
	step [91/255], loss=6.2982
	step [92/255], loss=5.6632
	step [93/255], loss=5.8623
	step [94/255], loss=6.9689
	step [95/255], loss=8.6041
	step [96/255], loss=10.9280
	step [97/255], loss=6.5683
	step [98/255], loss=6.1552
	step [99/255], loss=9.2422
	step [100/255], loss=6.3095
	step [101/255], loss=6.6025
	step [102/255], loss=6.5019
	step [103/255], loss=9.7066
	step [104/255], loss=8.0125
	step [105/255], loss=7.7943
	step [106/255], loss=8.5451
	step [107/255], loss=6.7819
	step [108/255], loss=7.2208
	step [109/255], loss=7.4971
	step [110/255], loss=7.9109
	step [111/255], loss=6.1836
	step [112/255], loss=7.6732
	step [113/255], loss=5.5066
	step [114/255], loss=6.3887
	step [115/255], loss=6.7209
	step [116/255], loss=9.7836
	step [117/255], loss=6.3002
	step [118/255], loss=5.7013
	step [119/255], loss=7.0451
	step [120/255], loss=5.4815
	step [121/255], loss=6.2144
	step [122/255], loss=7.6746
	step [123/255], loss=6.0919
	step [124/255], loss=5.9342
	step [125/255], loss=7.4631
	step [126/255], loss=6.5336
	step [127/255], loss=6.8846
	step [128/255], loss=8.3565
	step [129/255], loss=6.8751
	step [130/255], loss=5.6489
	step [131/255], loss=6.0786
	step [132/255], loss=6.1767
	step [133/255], loss=6.1817
	step [134/255], loss=7.1331
	step [135/255], loss=5.8582
	step [136/255], loss=7.1809
	step [137/255], loss=7.4089
	step [138/255], loss=6.8619
	step [139/255], loss=6.0271
	step [140/255], loss=6.3909
	step [141/255], loss=6.7308
	step [142/255], loss=5.6707
	step [143/255], loss=6.9707
	step [144/255], loss=6.3833
	step [145/255], loss=6.9911
	step [146/255], loss=8.0513
	step [147/255], loss=6.2935
	step [148/255], loss=5.2123
	step [149/255], loss=6.5647
	step [150/255], loss=8.2079
	step [151/255], loss=7.4096
	step [152/255], loss=5.9003
	step [153/255], loss=7.1313
	step [154/255], loss=6.8032
	step [155/255], loss=5.8330
	step [156/255], loss=5.6032
	step [157/255], loss=6.3946
	step [158/255], loss=8.3260
	step [159/255], loss=6.6251
	step [160/255], loss=7.1557
	step [161/255], loss=5.9454
	step [162/255], loss=5.4402
	step [163/255], loss=6.2092
	step [164/255], loss=6.0039
	step [165/255], loss=6.0305
	step [166/255], loss=7.9592
	step [167/255], loss=5.0824
	step [168/255], loss=5.5769
	step [169/255], loss=5.7296
	step [170/255], loss=5.6992
	step [171/255], loss=6.5228
	step [172/255], loss=7.0767
	step [173/255], loss=9.0289
	step [174/255], loss=5.5238
	step [175/255], loss=5.7790
	step [176/255], loss=6.8493
	step [177/255], loss=5.6226
	step [178/255], loss=6.4318
	step [179/255], loss=6.0337
	step [180/255], loss=7.2868
	step [181/255], loss=7.0556
	step [182/255], loss=8.0583
	step [183/255], loss=6.5324
	step [184/255], loss=6.6785
	step [185/255], loss=6.6789
	step [186/255], loss=7.4558
	step [187/255], loss=5.7937
	step [188/255], loss=6.0325
	step [189/255], loss=6.2713
	step [190/255], loss=5.9955
	step [191/255], loss=6.4669
	step [192/255], loss=5.7611
	step [193/255], loss=7.8802
	step [194/255], loss=8.0546
	step [195/255], loss=5.6013
	step [196/255], loss=7.0580
	step [197/255], loss=4.8934
	step [198/255], loss=6.4424
	step [199/255], loss=8.5006
	step [200/255], loss=6.5019
	step [201/255], loss=5.6213
	step [202/255], loss=6.7525
	step [203/255], loss=6.9432
	step [204/255], loss=6.2536
	step [205/255], loss=7.2815
	step [206/255], loss=6.7498
	step [207/255], loss=5.0392
	step [208/255], loss=6.4889
	step [209/255], loss=4.7740
	step [210/255], loss=8.5746
	step [211/255], loss=6.2616
	step [212/255], loss=5.8488
	step [213/255], loss=6.7054
	step [214/255], loss=5.0832
	step [215/255], loss=5.4202
	step [216/255], loss=7.2695
	step [217/255], loss=7.1574
	step [218/255], loss=5.9215
	step [219/255], loss=8.5783
	step [220/255], loss=6.2492
	step [221/255], loss=6.6976
	step [222/255], loss=6.0915
	step [223/255], loss=6.5433
	step [224/255], loss=7.1212
	step [225/255], loss=4.8958
	step [226/255], loss=5.2381
	step [227/255], loss=5.9712
	step [228/255], loss=6.3228
	step [229/255], loss=6.2601
	step [230/255], loss=5.1161
	step [231/255], loss=5.8945
	step [232/255], loss=9.7877
	step [233/255], loss=9.1662
	step [234/255], loss=6.1624
	step [235/255], loss=6.7008
	step [236/255], loss=9.0865
	step [237/255], loss=8.2617
	step [238/255], loss=5.5223
	step [239/255], loss=6.9512
	step [240/255], loss=7.8518
	step [241/255], loss=7.0142
	step [242/255], loss=6.4686
	step [243/255], loss=7.9712
	step [244/255], loss=7.6310
	step [245/255], loss=8.6688
	step [246/255], loss=5.9946
	step [247/255], loss=5.9590
	step [248/255], loss=6.6505
	step [249/255], loss=8.3784
	step [250/255], loss=6.0860
	step [251/255], loss=6.7526
	step [252/255], loss=6.4384
	step [253/255], loss=5.5435
	step [254/255], loss=7.8699
	step [255/255], loss=6.1678
	Evaluating
	loss=0.0208, precision=0.1953, recall=0.9920, f1=0.7046
Training epoch 25
	step [1/255], loss=6.3910
	step [2/255], loss=8.9640
	step [3/255], loss=4.9168
	step [4/255], loss=5.8368
	step [5/255], loss=6.3173
	step [6/255], loss=6.7177
	step [7/255], loss=7.0452
	step [8/255], loss=5.0227
	step [9/255], loss=7.2163
	step [10/255], loss=5.7776
	step [11/255], loss=7.7191
	step [12/255], loss=6.4556
	step [13/255], loss=5.3943
	step [14/255], loss=6.4501
	step [15/255], loss=5.8979
	step [16/255], loss=6.6030
	step [17/255], loss=7.3811
	step [18/255], loss=5.4935
	step [19/255], loss=5.2352
	step [20/255], loss=7.6815
	step [21/255], loss=5.9638
	step [22/255], loss=6.6334
	step [23/255], loss=6.1810
	step [24/255], loss=6.0687
	step [25/255], loss=8.2109
	step [26/255], loss=5.4772
	step [27/255], loss=9.0258
	step [28/255], loss=6.3015
	step [29/255], loss=7.5049
	step [30/255], loss=7.6135
	step [31/255], loss=6.1733
	step [32/255], loss=5.2375
	step [33/255], loss=5.5935
	step [34/255], loss=7.5697
	step [35/255], loss=7.5649
	step [36/255], loss=5.0028
	step [37/255], loss=5.7583
	step [38/255], loss=4.4690
	step [39/255], loss=6.0976
	step [40/255], loss=6.5163
	step [41/255], loss=6.4432
	step [42/255], loss=6.6808
	step [43/255], loss=4.3642
	step [44/255], loss=5.5219
	step [45/255], loss=4.6941
	step [46/255], loss=7.7497
	step [47/255], loss=7.1334
	step [48/255], loss=6.7941
	step [49/255], loss=6.5241
	step [50/255], loss=8.4709
	step [51/255], loss=6.7452
	step [52/255], loss=5.0952
	step [53/255], loss=7.3399
	step [54/255], loss=6.7483
	step [55/255], loss=6.6529
	step [56/255], loss=5.9094
	step [57/255], loss=4.6186
	step [58/255], loss=6.3526
	step [59/255], loss=6.9587
	step [60/255], loss=4.9950
	step [61/255], loss=6.3625
	step [62/255], loss=5.8809
	step [63/255], loss=6.7915
	step [64/255], loss=8.4807
	step [65/255], loss=5.6372
	step [66/255], loss=7.6637
	step [67/255], loss=7.0554
	step [68/255], loss=6.5663
	step [69/255], loss=5.5115
	step [70/255], loss=6.4298
	step [71/255], loss=6.4139
	step [72/255], loss=6.9746
	step [73/255], loss=6.6306
	step [74/255], loss=6.5646
	step [75/255], loss=6.6502
	step [76/255], loss=6.5983
	step [77/255], loss=5.5768
	step [78/255], loss=6.4982
	step [79/255], loss=6.0784
	step [80/255], loss=4.9777
	step [81/255], loss=6.7074
	step [82/255], loss=6.5449
	step [83/255], loss=7.7346
	step [84/255], loss=5.8097
	step [85/255], loss=7.0936
	step [86/255], loss=5.4577
	step [87/255], loss=4.9879
	step [88/255], loss=6.0420
	step [89/255], loss=6.1478
	step [90/255], loss=7.8862
	step [91/255], loss=8.1426
	step [92/255], loss=8.4442
	step [93/255], loss=7.2558
	step [94/255], loss=5.4996
	step [95/255], loss=7.8752
	step [96/255], loss=6.3470
	step [97/255], loss=6.9500
	step [98/255], loss=5.4271
	step [99/255], loss=5.2434
	step [100/255], loss=8.2649
	step [101/255], loss=6.7522
	step [102/255], loss=6.2003
	step [103/255], loss=7.1547
	step [104/255], loss=7.5150
	step [105/255], loss=5.5553
	step [106/255], loss=6.1424
	step [107/255], loss=5.8860
	step [108/255], loss=5.3088
	step [109/255], loss=6.2062
	step [110/255], loss=5.6167
	step [111/255], loss=5.2168
	step [112/255], loss=5.9629
	step [113/255], loss=5.8961
	step [114/255], loss=6.0062
	step [115/255], loss=5.3174
	step [116/255], loss=5.9714
	step [117/255], loss=6.5080
	step [118/255], loss=5.3042
	step [119/255], loss=7.2253
	step [120/255], loss=5.3908
	step [121/255], loss=6.0744
	step [122/255], loss=6.1208
	step [123/255], loss=4.7122
	step [124/255], loss=6.1187
	step [125/255], loss=7.6553
	step [126/255], loss=5.8429
	step [127/255], loss=7.5159
	step [128/255], loss=6.6986
	step [129/255], loss=6.6760
	step [130/255], loss=7.2147
	step [131/255], loss=7.5543
	step [132/255], loss=5.8915
	step [133/255], loss=6.4437
	step [134/255], loss=5.7570
	step [135/255], loss=5.9617
	step [136/255], loss=5.3801
	step [137/255], loss=6.9711
	step [138/255], loss=4.7042
	step [139/255], loss=7.8946
	step [140/255], loss=6.5417
	step [141/255], loss=6.2931
	step [142/255], loss=7.8760
	step [143/255], loss=7.5253
	step [144/255], loss=7.8072
	step [145/255], loss=6.0450
	step [146/255], loss=6.5752
	step [147/255], loss=5.7872
	step [148/255], loss=5.4736
	step [149/255], loss=7.6590
	step [150/255], loss=5.4765
	step [151/255], loss=6.6544
	step [152/255], loss=8.1974
	step [153/255], loss=4.9476
	step [154/255], loss=7.2529
	step [155/255], loss=5.9133
	step [156/255], loss=6.2438
	step [157/255], loss=5.5914
	step [158/255], loss=4.9973
	step [159/255], loss=6.1985
	step [160/255], loss=6.1011
	step [161/255], loss=7.1474
	step [162/255], loss=4.1883
	step [163/255], loss=6.6035
	step [164/255], loss=6.3077
	step [165/255], loss=6.3425
	step [166/255], loss=6.0266
	step [167/255], loss=6.3253
	step [168/255], loss=6.8512
	step [169/255], loss=6.2263
	step [170/255], loss=5.4733
	step [171/255], loss=5.9515
	step [172/255], loss=6.4991
	step [173/255], loss=5.7948
	step [174/255], loss=7.3478
	step [175/255], loss=6.6542
	step [176/255], loss=6.5208
	step [177/255], loss=6.8010
	step [178/255], loss=6.5849
	step [179/255], loss=5.6425
	step [180/255], loss=6.1082
	step [181/255], loss=8.6365
	step [182/255], loss=6.3117
	step [183/255], loss=6.5590
	step [184/255], loss=6.2326
	step [185/255], loss=5.7587
	step [186/255], loss=5.9437
	step [187/255], loss=5.8434
	step [188/255], loss=6.3769
	step [189/255], loss=10.0818
	step [190/255], loss=9.6100
	step [191/255], loss=6.5050
	step [192/255], loss=7.0354
	step [193/255], loss=7.2924
	step [194/255], loss=5.8385
	step [195/255], loss=5.8054
	step [196/255], loss=7.5141
	step [197/255], loss=6.0746
	step [198/255], loss=7.0727
	step [199/255], loss=5.8331
	step [200/255], loss=6.9340
	step [201/255], loss=7.8288
	step [202/255], loss=8.6651
	step [203/255], loss=6.6978
	step [204/255], loss=6.9527
	step [205/255], loss=7.6588
	step [206/255], loss=6.0971
	step [207/255], loss=6.2898
	step [208/255], loss=7.0294
	step [209/255], loss=8.3950
	step [210/255], loss=7.6288
	step [211/255], loss=8.4311
	step [212/255], loss=8.1756
	step [213/255], loss=6.5068
	step [214/255], loss=7.1305
	step [215/255], loss=7.5790
	step [216/255], loss=6.1567
	step [217/255], loss=5.9433
	step [218/255], loss=6.5710
	step [219/255], loss=6.8152
	step [220/255], loss=5.7746
	step [221/255], loss=6.7013
	step [222/255], loss=5.7650
	step [223/255], loss=7.1867
	step [224/255], loss=6.7074
	step [225/255], loss=7.0453
	step [226/255], loss=5.8095
	step [227/255], loss=5.7260
	step [228/255], loss=8.7239
	step [229/255], loss=7.0642
	step [230/255], loss=7.4721
	step [231/255], loss=8.4218
	step [232/255], loss=6.6327
	step [233/255], loss=8.7926
	step [234/255], loss=7.1493
	step [235/255], loss=6.4622
	step [236/255], loss=7.1720
	step [237/255], loss=7.0043
	step [238/255], loss=8.9335
	step [239/255], loss=6.1287
	step [240/255], loss=7.2498
	step [241/255], loss=6.1736
	step [242/255], loss=6.6780
	step [243/255], loss=7.0313
	step [244/255], loss=7.3283
	step [245/255], loss=5.5088
	step [246/255], loss=7.9209
	step [247/255], loss=6.5067
	step [248/255], loss=7.1885
	step [249/255], loss=6.0042
	step [250/255], loss=7.1258
	step [251/255], loss=6.1060
	step [252/255], loss=5.5241
	step [253/255], loss=5.3291
	step [254/255], loss=6.0033
	step [255/255], loss=6.0480
	Evaluating
	loss=0.0195, precision=0.2117, recall=0.9922, f1=0.7249
saving model as: 0_saved_model.pth
Training epoch 26
	step [1/255], loss=5.3021
	step [2/255], loss=6.3373
	step [3/255], loss=6.3500
	step [4/255], loss=4.0523
	step [5/255], loss=6.6643
	step [6/255], loss=5.6542
	step [7/255], loss=6.4636
	step [8/255], loss=4.8878
	step [9/255], loss=6.2631
	step [10/255], loss=6.6993
	step [11/255], loss=6.1974
	step [12/255], loss=7.4499
	step [13/255], loss=5.4014
	step [14/255], loss=7.8631
	step [15/255], loss=6.7534
	step [16/255], loss=6.5260
	step [17/255], loss=5.3531
	step [18/255], loss=5.7026
	step [19/255], loss=5.7853
	step [20/255], loss=5.7844
	step [21/255], loss=6.5170
	step [22/255], loss=6.3550
	step [23/255], loss=6.5903
	step [24/255], loss=5.4629
	step [25/255], loss=6.2327
	step [26/255], loss=5.6500
	step [27/255], loss=5.7964
	step [28/255], loss=7.7262
	step [29/255], loss=6.3108
	step [30/255], loss=7.5716
	step [31/255], loss=5.3326
	step [32/255], loss=5.7601
	step [33/255], loss=8.0185
	step [34/255], loss=5.8714
	step [35/255], loss=6.2004
	step [36/255], loss=6.8857
	step [37/255], loss=7.6280
	step [38/255], loss=6.0473
	step [39/255], loss=5.7715
	step [40/255], loss=5.0313
	step [41/255], loss=6.3271
	step [42/255], loss=6.7929
	step [43/255], loss=7.7403
	step [44/255], loss=5.6967
	step [45/255], loss=7.6569
	step [46/255], loss=6.6593
	step [47/255], loss=6.0044
	step [48/255], loss=6.4829
	step [49/255], loss=7.0136
	step [50/255], loss=8.7839
	step [51/255], loss=5.5705
	step [52/255], loss=5.7193
	step [53/255], loss=5.7544
	step [54/255], loss=4.8850
	step [55/255], loss=4.8301
	step [56/255], loss=6.4934
	step [57/255], loss=6.1042
	step [58/255], loss=6.7141
	step [59/255], loss=7.2321
	step [60/255], loss=6.5635
	step [61/255], loss=6.0845
	step [62/255], loss=6.0133
	step [63/255], loss=8.8170
	step [64/255], loss=7.2944
	step [65/255], loss=5.6871
	step [66/255], loss=7.1658
	step [67/255], loss=6.3293
	step [68/255], loss=6.5596
	step [69/255], loss=5.6100
	step [70/255], loss=6.8528
	step [71/255], loss=4.9930
	step [72/255], loss=6.3205
	step [73/255], loss=6.7225
	step [74/255], loss=6.7616
	step [75/255], loss=5.2383
	step [76/255], loss=6.0257
	step [77/255], loss=5.4980
	step [78/255], loss=6.4904
	step [79/255], loss=6.8031
	step [80/255], loss=5.1006
	step [81/255], loss=7.4349
	step [82/255], loss=7.0556
	step [83/255], loss=7.3961
	step [84/255], loss=4.9552
	step [85/255], loss=6.9742
	step [86/255], loss=4.5514
	step [87/255], loss=8.0782
	step [88/255], loss=6.5745
	step [89/255], loss=6.4941
	step [90/255], loss=6.3230
	step [91/255], loss=6.1743
	step [92/255], loss=7.5741
	step [93/255], loss=4.9349
	step [94/255], loss=7.9293
	step [95/255], loss=6.6092
	step [96/255], loss=6.8127
	step [97/255], loss=6.2118
	step [98/255], loss=7.1249
	step [99/255], loss=5.4617
	step [100/255], loss=8.7754
	step [101/255], loss=6.6226
	step [102/255], loss=5.9066
	step [103/255], loss=5.7035
	step [104/255], loss=7.1972
	step [105/255], loss=7.3547
	step [106/255], loss=6.3319
	step [107/255], loss=6.6469
	step [108/255], loss=5.6492
	step [109/255], loss=7.2013
	step [110/255], loss=5.2460
	step [111/255], loss=5.3077
	step [112/255], loss=5.8879
	step [113/255], loss=5.2774
	step [114/255], loss=6.7996
	step [115/255], loss=5.5565
	step [116/255], loss=6.1214
	step [117/255], loss=6.2256
	step [118/255], loss=5.9943
	step [119/255], loss=8.4878
	step [120/255], loss=6.0781
	step [121/255], loss=5.9294
	step [122/255], loss=6.2104
	step [123/255], loss=5.6610
	step [124/255], loss=5.7086
	step [125/255], loss=6.6893
	step [126/255], loss=5.7409
	step [127/255], loss=5.3260
	step [128/255], loss=6.0281
	step [129/255], loss=5.9611
	step [130/255], loss=5.1798
	step [131/255], loss=6.5902
	step [132/255], loss=8.4405
	step [133/255], loss=7.4535
	step [134/255], loss=4.7447
	step [135/255], loss=6.6196
	step [136/255], loss=5.2291
	step [137/255], loss=5.2531
	step [138/255], loss=5.8084
	step [139/255], loss=5.6736
	step [140/255], loss=6.4733
	step [141/255], loss=7.1991
	step [142/255], loss=5.5994
	step [143/255], loss=5.6966
	step [144/255], loss=6.1646
	step [145/255], loss=6.6574
	step [146/255], loss=6.4986
	step [147/255], loss=9.0692
	step [148/255], loss=5.6888
	step [149/255], loss=5.5307
	step [150/255], loss=6.7512
	step [151/255], loss=5.8928
	step [152/255], loss=6.0197
	step [153/255], loss=5.9798
	step [154/255], loss=7.1101
	step [155/255], loss=6.1722
	step [156/255], loss=6.7743
	step [157/255], loss=8.5277
	step [158/255], loss=6.1905
	step [159/255], loss=7.5723
	step [160/255], loss=5.6895
	step [161/255], loss=7.0687
	step [162/255], loss=7.7127
	step [163/255], loss=6.8689
	step [164/255], loss=6.6399
	step [165/255], loss=4.5413
	step [166/255], loss=7.2642
	step [167/255], loss=6.3356
	step [168/255], loss=7.0576
	step [169/255], loss=5.6363
	step [170/255], loss=5.7457
	step [171/255], loss=7.9364
	step [172/255], loss=5.7194
	step [173/255], loss=5.6470
	step [174/255], loss=5.6173
	step [175/255], loss=7.1660
	step [176/255], loss=7.4258
	step [177/255], loss=6.2864
	step [178/255], loss=6.3808
	step [179/255], loss=5.5816
	step [180/255], loss=5.4227
	step [181/255], loss=5.5018
	step [182/255], loss=5.4453
	step [183/255], loss=6.0426
	step [184/255], loss=6.5347
	step [185/255], loss=5.4239
	step [186/255], loss=6.0179
	step [187/255], loss=5.4653
	step [188/255], loss=7.0920
	step [189/255], loss=6.7976
	step [190/255], loss=8.9228
	step [191/255], loss=5.7276
	step [192/255], loss=6.2152
	step [193/255], loss=6.8765
	step [194/255], loss=6.1101
	step [195/255], loss=6.5609
	step [196/255], loss=5.5287
	step [197/255], loss=7.2421
	step [198/255], loss=6.5868
	step [199/255], loss=8.8691
	step [200/255], loss=6.1282
	step [201/255], loss=7.2843
	step [202/255], loss=6.1625
	step [203/255], loss=7.2484
	step [204/255], loss=7.4631
	step [205/255], loss=5.5986
	step [206/255], loss=5.2372
	step [207/255], loss=8.1253
	step [208/255], loss=5.1310
	step [209/255], loss=6.0524
	step [210/255], loss=6.0356
	step [211/255], loss=8.9604
	step [212/255], loss=6.6632
	step [213/255], loss=6.3627
	step [214/255], loss=6.0958
	step [215/255], loss=6.0592
	step [216/255], loss=5.3938
	step [217/255], loss=9.3634
	step [218/255], loss=7.2306
	step [219/255], loss=6.6647
	step [220/255], loss=6.2041
	step [221/255], loss=5.5229
	step [222/255], loss=5.7899
	step [223/255], loss=5.2196
	step [224/255], loss=5.8121
	step [225/255], loss=5.6514
	step [226/255], loss=6.0222
	step [227/255], loss=6.7689
	step [228/255], loss=4.5302
	step [229/255], loss=9.4079
	step [230/255], loss=7.3026
	step [231/255], loss=6.5227
	step [232/255], loss=6.8657
	step [233/255], loss=6.0487
	step [234/255], loss=7.4211
	step [235/255], loss=6.5718
	step [236/255], loss=6.1323
	step [237/255], loss=5.5663
	step [238/255], loss=5.8040
	step [239/255], loss=7.6240
	step [240/255], loss=6.5637
	step [241/255], loss=6.0120
	step [242/255], loss=6.5490
	step [243/255], loss=4.5733
	step [244/255], loss=7.3200
	step [245/255], loss=5.8796
	step [246/255], loss=6.1124
	step [247/255], loss=5.7702
	step [248/255], loss=7.7107
	step [249/255], loss=4.3633
	step [250/255], loss=7.0657
	step [251/255], loss=5.5209
	step [252/255], loss=5.7552
	step [253/255], loss=6.2190
	step [254/255], loss=6.4788
	step [255/255], loss=5.1656
	Evaluating
	loss=0.0210, precision=0.1908, recall=0.9925, f1=0.6989
Training epoch 27
	step [1/255], loss=4.6306
	step [2/255], loss=6.1643
	step [3/255], loss=5.6567
	step [4/255], loss=8.1439
	step [5/255], loss=5.8050
	step [6/255], loss=7.5843
	step [7/255], loss=5.8255
	step [8/255], loss=7.2349
	step [9/255], loss=5.8256
	step [10/255], loss=5.7218
	step [11/255], loss=5.9671
	step [12/255], loss=5.9313
	step [13/255], loss=6.4897
	step [14/255], loss=7.1901
	step [15/255], loss=5.7669
	step [16/255], loss=6.7327
	step [17/255], loss=9.1722
	step [18/255], loss=5.6092
	step [19/255], loss=5.4589
	step [20/255], loss=5.2718
	step [21/255], loss=6.3805
	step [22/255], loss=6.1586
	step [23/255], loss=5.7573
	step [24/255], loss=4.6848
	step [25/255], loss=6.9886
	step [26/255], loss=5.8604
	step [27/255], loss=6.0290
	step [28/255], loss=5.6974
	step [29/255], loss=6.5918
	step [30/255], loss=6.8950
	step [31/255], loss=6.7378
	step [32/255], loss=7.2203
	step [33/255], loss=6.2405
	step [34/255], loss=5.6621
	step [35/255], loss=5.3320
	step [36/255], loss=6.9988
	step [37/255], loss=7.4334
	step [38/255], loss=6.4033
	step [39/255], loss=6.0228
	step [40/255], loss=5.1655
	step [41/255], loss=7.9130
	step [42/255], loss=5.5534
	step [43/255], loss=5.7270
	step [44/255], loss=6.4227
	step [45/255], loss=5.6852
	step [46/255], loss=4.9561
	step [47/255], loss=4.9574
	step [48/255], loss=6.7268
	step [49/255], loss=6.0224
	step [50/255], loss=5.7261
	step [51/255], loss=6.5470
	step [52/255], loss=5.4519
	step [53/255], loss=7.3231
	step [54/255], loss=5.2829
	step [55/255], loss=6.4137
	step [56/255], loss=6.9115
	step [57/255], loss=4.9733
	step [58/255], loss=6.5047
	step [59/255], loss=5.0687
	step [60/255], loss=6.7606
	step [61/255], loss=5.1778
	step [62/255], loss=5.0876
	step [63/255], loss=7.2843
	step [64/255], loss=6.0483
	step [65/255], loss=6.5600
	step [66/255], loss=5.6268
	step [67/255], loss=5.0435
	step [68/255], loss=6.5348
	step [69/255], loss=6.1433
	step [70/255], loss=5.9747
	step [71/255], loss=6.0716
	step [72/255], loss=4.6811
	step [73/255], loss=6.4009
	step [74/255], loss=5.1261
	step [75/255], loss=6.7574
	step [76/255], loss=4.6231
	step [77/255], loss=8.2233
	step [78/255], loss=6.6981
	step [79/255], loss=5.1509
	step [80/255], loss=5.2610
	step [81/255], loss=6.9150
	step [82/255], loss=5.2539
	step [83/255], loss=7.1613
	step [84/255], loss=6.8037
	step [85/255], loss=5.7793
	step [86/255], loss=4.9376
	step [87/255], loss=5.3887
	step [88/255], loss=5.1757
	step [89/255], loss=5.8486
	step [90/255], loss=6.9571
	step [91/255], loss=6.1010
	step [92/255], loss=5.4919
	step [93/255], loss=5.6947
	step [94/255], loss=6.6956
	step [95/255], loss=6.9017
	step [96/255], loss=7.3041
	step [97/255], loss=5.1579
	step [98/255], loss=5.7767
	step [99/255], loss=6.3848
	step [100/255], loss=6.9524
	step [101/255], loss=5.9134
	step [102/255], loss=6.8246
	step [103/255], loss=8.2481
	step [104/255], loss=6.6151
	step [105/255], loss=6.3483
	step [106/255], loss=4.5910
	step [107/255], loss=5.8340
	step [108/255], loss=6.1655
	step [109/255], loss=5.9045
	step [110/255], loss=5.6971
	step [111/255], loss=7.8588
	step [112/255], loss=8.9281
	step [113/255], loss=5.8733
	step [114/255], loss=5.1126
	step [115/255], loss=6.1236
	step [116/255], loss=5.9305
	step [117/255], loss=6.4281
	step [118/255], loss=6.6621
	step [119/255], loss=5.7189
	step [120/255], loss=5.8651
	step [121/255], loss=4.2993
	step [122/255], loss=6.4850
	step [123/255], loss=7.7049
	step [124/255], loss=5.5840
	step [125/255], loss=6.1871
	step [126/255], loss=7.7110
	step [127/255], loss=7.3738
	step [128/255], loss=5.5997
	step [129/255], loss=6.2064
	step [130/255], loss=5.9305
	step [131/255], loss=5.9318
	step [132/255], loss=6.6191
	step [133/255], loss=7.1316
	step [134/255], loss=5.6723
	step [135/255], loss=5.5502
	step [136/255], loss=5.5658
	step [137/255], loss=6.3936
	step [138/255], loss=6.8257
	step [139/255], loss=7.3028
	step [140/255], loss=5.4396
	step [141/255], loss=6.0589
	step [142/255], loss=6.5894
	step [143/255], loss=6.7985
	step [144/255], loss=7.0224
	step [145/255], loss=6.7707
	step [146/255], loss=5.4543
	step [147/255], loss=5.1327
	step [148/255], loss=5.0689
	step [149/255], loss=7.6109
	step [150/255], loss=5.0603
	step [151/255], loss=6.3253
	step [152/255], loss=7.1645
	step [153/255], loss=6.4555
	step [154/255], loss=8.6339
	step [155/255], loss=6.2319
	step [156/255], loss=6.7005
	step [157/255], loss=8.1165
	step [158/255], loss=6.1396
	step [159/255], loss=7.1496
	step [160/255], loss=5.4243
	step [161/255], loss=5.9227
	step [162/255], loss=5.5029
	step [163/255], loss=5.3634
	step [164/255], loss=8.6826
	step [165/255], loss=6.0486
	step [166/255], loss=6.6409
	step [167/255], loss=6.9835
	step [168/255], loss=7.4326
	step [169/255], loss=6.1547
	step [170/255], loss=5.2580
	step [171/255], loss=7.1625
	step [172/255], loss=6.0238
	step [173/255], loss=7.0620
	step [174/255], loss=6.8836
	step [175/255], loss=6.0576
	step [176/255], loss=6.6402
	step [177/255], loss=6.0675
	step [178/255], loss=5.9360
	step [179/255], loss=6.9565
	step [180/255], loss=5.9360
	step [181/255], loss=6.7506
	step [182/255], loss=6.8823
	step [183/255], loss=4.0298
	step [184/255], loss=6.4060
	step [185/255], loss=5.7351
	step [186/255], loss=6.0840
	step [187/255], loss=6.5820
	step [188/255], loss=8.9356
	step [189/255], loss=5.8193
	step [190/255], loss=5.0897
	step [191/255], loss=4.9968
	step [192/255], loss=8.0959
	step [193/255], loss=5.1874
	step [194/255], loss=5.2295
	step [195/255], loss=6.4320
	step [196/255], loss=7.7364
	step [197/255], loss=5.5191
	step [198/255], loss=5.3384
	step [199/255], loss=6.2167
	step [200/255], loss=5.3835
	step [201/255], loss=6.9437
	step [202/255], loss=5.4253
	step [203/255], loss=5.5960
	step [204/255], loss=5.8553
	step [205/255], loss=5.6906
	step [206/255], loss=5.9718
	step [207/255], loss=6.7123
	step [208/255], loss=7.9952
	step [209/255], loss=5.4794
	step [210/255], loss=7.6476
	step [211/255], loss=6.6787
	step [212/255], loss=4.5787
	step [213/255], loss=5.8723
	step [214/255], loss=6.3866
	step [215/255], loss=5.6119
	step [216/255], loss=5.5909
	step [217/255], loss=6.5450
	step [218/255], loss=5.6480
	step [219/255], loss=7.4572
	step [220/255], loss=6.3642
	step [221/255], loss=6.3787
	step [222/255], loss=5.8461
	step [223/255], loss=5.7023
	step [224/255], loss=4.8669
	step [225/255], loss=6.6756
	step [226/255], loss=5.4906
	step [227/255], loss=6.7392
	step [228/255], loss=5.9554
	step [229/255], loss=4.7922
	step [230/255], loss=5.8537
	step [231/255], loss=6.6987
	step [232/255], loss=6.1742
	step [233/255], loss=6.9169
	step [234/255], loss=6.3435
	step [235/255], loss=5.7249
	step [236/255], loss=6.4562
	step [237/255], loss=7.2241
	step [238/255], loss=5.7618
	step [239/255], loss=6.8129
	step [240/255], loss=5.2082
	step [241/255], loss=6.5457
	step [242/255], loss=5.9968
	step [243/255], loss=6.8502
	step [244/255], loss=5.1123
	step [245/255], loss=6.2031
	step [246/255], loss=6.9892
	step [247/255], loss=5.9008
	step [248/255], loss=5.6675
	step [249/255], loss=4.7061
	step [250/255], loss=6.7508
	step [251/255], loss=5.9596
	step [252/255], loss=6.6744
	step [253/255], loss=5.8941
	step [254/255], loss=5.8358
	step [255/255], loss=4.6419
	Evaluating
	loss=0.0195, precision=0.2083, recall=0.9917, f1=0.7207
Training epoch 28
	step [1/255], loss=5.2584
	step [2/255], loss=5.6703
	step [3/255], loss=5.4957
	step [4/255], loss=7.7305
	step [5/255], loss=5.7529
	step [6/255], loss=4.5692
	step [7/255], loss=5.4277
	step [8/255], loss=7.8428
	step [9/255], loss=6.5771
	step [10/255], loss=5.2574
	step [11/255], loss=6.4257
	step [12/255], loss=6.0265
	step [13/255], loss=7.6102
	step [14/255], loss=5.5694
	step [15/255], loss=6.7769
	step [16/255], loss=6.4667
	step [17/255], loss=6.0046
	step [18/255], loss=6.0500
	step [19/255], loss=6.5038
	step [20/255], loss=5.5031
	step [21/255], loss=4.9501
	step [22/255], loss=6.0036
	step [23/255], loss=6.6835
	step [24/255], loss=7.2501
	step [25/255], loss=6.3249
	step [26/255], loss=6.6353
	step [27/255], loss=5.9357
	step [28/255], loss=6.9307
	step [29/255], loss=5.3896
	step [30/255], loss=5.9934
	step [31/255], loss=5.1994
	step [32/255], loss=6.5213
	step [33/255], loss=5.1826
	step [34/255], loss=6.5320
	step [35/255], loss=6.6155
	step [36/255], loss=5.0009
	step [37/255], loss=6.5339
	step [38/255], loss=5.4337
	step [39/255], loss=5.8996
	step [40/255], loss=5.9941
	step [41/255], loss=5.0172
	step [42/255], loss=6.2022
	step [43/255], loss=6.2748
	step [44/255], loss=5.7424
	step [45/255], loss=5.8932
	step [46/255], loss=6.8785
	step [47/255], loss=5.6095
	step [48/255], loss=5.1825
	step [49/255], loss=6.7639
	step [50/255], loss=4.3294
	step [51/255], loss=5.6932
	step [52/255], loss=5.3119
	step [53/255], loss=5.8448
	step [54/255], loss=5.8830
	step [55/255], loss=5.4960
	step [56/255], loss=6.1258
	step [57/255], loss=5.6530
	step [58/255], loss=4.9806
	step [59/255], loss=6.7902
	step [60/255], loss=5.6118
	step [61/255], loss=6.9216
	step [62/255], loss=6.8336
	step [63/255], loss=5.9646
	step [64/255], loss=5.1372
	step [65/255], loss=5.0855
	step [66/255], loss=6.2251
	step [67/255], loss=9.2315
	step [68/255], loss=5.0793
	step [69/255], loss=5.3673
	step [70/255], loss=4.2680
	step [71/255], loss=4.3246
	step [72/255], loss=7.3155
	step [73/255], loss=5.1137
	step [74/255], loss=6.1916
	step [75/255], loss=5.1753
	step [76/255], loss=4.6535
	step [77/255], loss=5.5288
	step [78/255], loss=6.3841
	step [79/255], loss=4.5537
	step [80/255], loss=5.5650
	step [81/255], loss=6.1544
	step [82/255], loss=5.3500
	step [83/255], loss=5.3554
	step [84/255], loss=5.3107
	step [85/255], loss=5.3331
	step [86/255], loss=5.7329
	step [87/255], loss=5.6082
	step [88/255], loss=4.9756
	step [89/255], loss=5.6068
	step [90/255], loss=6.4181
	step [91/255], loss=5.6932
	step [92/255], loss=6.3457
	step [93/255], loss=7.0666
	step [94/255], loss=6.2394
	step [95/255], loss=5.8900
	step [96/255], loss=5.4244
	step [97/255], loss=5.1812
	step [98/255], loss=5.5152
	step [99/255], loss=6.1320
	step [100/255], loss=6.0544
	step [101/255], loss=5.4961
	step [102/255], loss=6.8050
	step [103/255], loss=5.8512
	step [104/255], loss=6.1527
	step [105/255], loss=7.3878
	step [106/255], loss=7.3735
	step [107/255], loss=7.0335
	step [108/255], loss=4.8810
	step [109/255], loss=5.7641
	step [110/255], loss=7.5960
	step [111/255], loss=6.7104
	step [112/255], loss=6.3605
	step [113/255], loss=5.5267
	step [114/255], loss=6.0489
	step [115/255], loss=4.5746
	step [116/255], loss=4.9220
	step [117/255], loss=5.0998
	step [118/255], loss=6.0661
	step [119/255], loss=6.4680
	step [120/255], loss=6.4506
	step [121/255], loss=5.2822
	step [122/255], loss=7.4251
	step [123/255], loss=5.2312
	step [124/255], loss=6.8294
	step [125/255], loss=5.6754
	step [126/255], loss=5.1749
	step [127/255], loss=7.0965
	step [128/255], loss=6.2827
	step [129/255], loss=5.1467
	step [130/255], loss=4.9425
	step [131/255], loss=6.7918
	step [132/255], loss=5.2015
	step [133/255], loss=6.7317
	step [134/255], loss=6.3003
	step [135/255], loss=6.2972
	step [136/255], loss=5.3955
	step [137/255], loss=5.2500
	step [138/255], loss=5.2485
	step [139/255], loss=6.3277
	step [140/255], loss=6.5436
	step [141/255], loss=5.0293
	step [142/255], loss=5.5928
	step [143/255], loss=6.0483
	step [144/255], loss=5.2232
	step [145/255], loss=6.5716
	step [146/255], loss=6.8177
	step [147/255], loss=5.0443
	step [148/255], loss=5.2402
	step [149/255], loss=6.0228
	step [150/255], loss=5.0178
	step [151/255], loss=6.4706
	step [152/255], loss=6.1988
	step [153/255], loss=7.3606
	step [154/255], loss=5.8154
	step [155/255], loss=6.7874
	step [156/255], loss=5.1764
	step [157/255], loss=4.8064
	step [158/255], loss=7.1734
	step [159/255], loss=7.6459
	step [160/255], loss=7.0482
	step [161/255], loss=6.1131
	step [162/255], loss=5.7871
	step [163/255], loss=5.6207
	step [164/255], loss=6.4292
	step [165/255], loss=5.8523
	step [166/255], loss=7.2254
	step [167/255], loss=6.9494
	step [168/255], loss=4.6684
	step [169/255], loss=5.9756
	step [170/255], loss=5.9839
	step [171/255], loss=7.4140
	step [172/255], loss=5.8497
	step [173/255], loss=5.2181
	step [174/255], loss=5.9275
	step [175/255], loss=5.8135
	step [176/255], loss=5.8390
	step [177/255], loss=5.9465
	step [178/255], loss=5.7944
	step [179/255], loss=5.5316
	step [180/255], loss=7.1664
	step [181/255], loss=6.5538
	step [182/255], loss=7.0398
	step [183/255], loss=7.1443
	step [184/255], loss=4.9075
	step [185/255], loss=5.9947
	step [186/255], loss=6.7547
	step [187/255], loss=5.9689
	step [188/255], loss=5.7162
	step [189/255], loss=5.3499
	step [190/255], loss=6.0609
	step [191/255], loss=7.9752
	step [192/255], loss=7.0905
	step [193/255], loss=6.0079
	step [194/255], loss=5.4289
	step [195/255], loss=7.0254
	step [196/255], loss=7.5876
	step [197/255], loss=5.1410
	step [198/255], loss=6.2494
	step [199/255], loss=7.2845
	step [200/255], loss=8.0432
	step [201/255], loss=6.2521
	step [202/255], loss=6.4877
	step [203/255], loss=5.6124
	step [204/255], loss=5.7242
	step [205/255], loss=5.0750
	step [206/255], loss=6.6499
	step [207/255], loss=7.2601
	step [208/255], loss=5.4435
	step [209/255], loss=7.4871
	step [210/255], loss=4.1716
	step [211/255], loss=6.4011
	step [212/255], loss=5.5453
	step [213/255], loss=5.5460
	step [214/255], loss=5.6888
	step [215/255], loss=5.3770
	step [216/255], loss=4.7215
	step [217/255], loss=4.6517
	step [218/255], loss=7.4774
	step [219/255], loss=6.0031
	step [220/255], loss=5.1971
	step [221/255], loss=8.1856
	step [222/255], loss=5.2192
	step [223/255], loss=6.1378
	step [224/255], loss=6.8899
	step [225/255], loss=5.5005
	step [226/255], loss=6.2530
	step [227/255], loss=6.7928
	step [228/255], loss=6.8843
	step [229/255], loss=5.3473
	step [230/255], loss=6.5479
	step [231/255], loss=7.2514
	step [232/255], loss=5.3410
	step [233/255], loss=5.9325
	step [234/255], loss=7.0699
	step [235/255], loss=6.0452
	step [236/255], loss=5.7698
	step [237/255], loss=6.6794
	step [238/255], loss=5.6808
	step [239/255], loss=6.8188
	step [240/255], loss=5.9064
	step [241/255], loss=6.2122
	step [242/255], loss=6.9449
	step [243/255], loss=6.4837
	step [244/255], loss=6.9352
	step [245/255], loss=6.6537
	step [246/255], loss=5.8912
	step [247/255], loss=6.2537
	step [248/255], loss=5.3464
	step [249/255], loss=5.7964
	step [250/255], loss=6.0118
	step [251/255], loss=5.8061
	step [252/255], loss=4.9664
	step [253/255], loss=7.0231
	step [254/255], loss=7.1269
	step [255/255], loss=4.1733
	Evaluating
	loss=0.0233, precision=0.1753, recall=0.9931, f1=0.6772
Training epoch 29
	step [1/255], loss=5.5371
	step [2/255], loss=5.1667
	step [3/255], loss=4.5141
	step [4/255], loss=6.9984
	step [5/255], loss=5.4713
	step [6/255], loss=4.7682
	step [7/255], loss=7.5065
	step [8/255], loss=6.5264
	step [9/255], loss=5.5021
	step [10/255], loss=4.9428
	step [11/255], loss=5.7226
	step [12/255], loss=5.6123
	step [13/255], loss=4.7770
	step [14/255], loss=7.5638
	step [15/255], loss=7.3386
	step [16/255], loss=5.4548
	step [17/255], loss=6.1359
	step [18/255], loss=6.5740
	step [19/255], loss=5.5710
	step [20/255], loss=6.3907
	step [21/255], loss=5.8996
	step [22/255], loss=6.2572
	step [23/255], loss=5.6008
	step [24/255], loss=5.7908
	step [25/255], loss=6.2956
	step [26/255], loss=4.7815
	step [27/255], loss=5.5804
	step [28/255], loss=4.5869
	step [29/255], loss=5.1991
	step [30/255], loss=5.0908
	step [31/255], loss=5.8001
	step [32/255], loss=5.8482
	step [33/255], loss=6.0393
	step [34/255], loss=6.9602
	step [35/255], loss=6.2531
	step [36/255], loss=7.9742
	step [37/255], loss=5.5986
	step [38/255], loss=5.9793
	step [39/255], loss=4.9306
	step [40/255], loss=6.1496
	step [41/255], loss=5.5373
	step [42/255], loss=6.0207
	step [43/255], loss=5.2752
	step [44/255], loss=5.2436
	step [45/255], loss=6.5306
	step [46/255], loss=5.6124
	step [47/255], loss=6.3748
	step [48/255], loss=5.9034
	step [49/255], loss=4.5849
	step [50/255], loss=5.2867
	step [51/255], loss=5.6308
	step [52/255], loss=5.7204
	step [53/255], loss=4.9218
	step [54/255], loss=5.5008
	step [55/255], loss=5.8672
	step [56/255], loss=6.8953
	step [57/255], loss=5.4387
	step [58/255], loss=4.7032
	step [59/255], loss=5.4388
	step [60/255], loss=6.9276
	step [61/255], loss=5.5255
	step [62/255], loss=6.0845
	step [63/255], loss=6.1656
	step [64/255], loss=6.1444
	step [65/255], loss=4.8070
	step [66/255], loss=5.9957
	step [67/255], loss=7.1521
	step [68/255], loss=5.4339
	step [69/255], loss=5.9643
	step [70/255], loss=6.2967
	step [71/255], loss=6.2029
	step [72/255], loss=7.4083
	step [73/255], loss=8.4922
	step [74/255], loss=7.4255
	step [75/255], loss=5.7830
	step [76/255], loss=7.1345
	step [77/255], loss=5.4099
	step [78/255], loss=5.5888
	step [79/255], loss=6.8969
	step [80/255], loss=7.5530
	step [81/255], loss=6.4561
	step [82/255], loss=4.9901
	step [83/255], loss=7.3653
	step [84/255], loss=4.0735
	step [85/255], loss=6.0936
	step [86/255], loss=5.8875
	step [87/255], loss=5.9418
	step [88/255], loss=6.1761
	step [89/255], loss=6.3462
	step [90/255], loss=5.7448
	step [91/255], loss=4.2786
	step [92/255], loss=4.7521
	step [93/255], loss=5.9885
	step [94/255], loss=5.5068
	step [95/255], loss=5.3010
	step [96/255], loss=6.7071
	step [97/255], loss=5.0356
	step [98/255], loss=5.7224
	step [99/255], loss=5.8802
	step [100/255], loss=5.5960
	step [101/255], loss=6.1238
	step [102/255], loss=7.1122
	step [103/255], loss=4.8052
	step [104/255], loss=5.9438
	step [105/255], loss=4.6784
	step [106/255], loss=5.5434
	step [107/255], loss=5.0907
	step [108/255], loss=6.5486
	step [109/255], loss=6.5855
	step [110/255], loss=6.3310
	step [111/255], loss=5.7059
	step [112/255], loss=7.0924
	step [113/255], loss=5.4647
	step [114/255], loss=4.7889
	step [115/255], loss=5.6315
	step [116/255], loss=5.5428
	step [117/255], loss=4.9276
	step [118/255], loss=6.0783
	step [119/255], loss=7.1778
	step [120/255], loss=6.3234
	step [121/255], loss=6.1936
	step [122/255], loss=5.9358
	step [123/255], loss=5.1364
	step [124/255], loss=6.4405
	step [125/255], loss=5.8048
	step [126/255], loss=6.1758
	step [127/255], loss=6.7384
	step [128/255], loss=6.1994
	step [129/255], loss=3.9093
	step [130/255], loss=5.6935
	step [131/255], loss=4.2425
	step [132/255], loss=6.6002
	step [133/255], loss=4.7076
	step [134/255], loss=4.9361
	step [135/255], loss=5.1117
	step [136/255], loss=5.6357
	step [137/255], loss=6.5091
	step [138/255], loss=5.3317
	step [139/255], loss=5.4557
	step [140/255], loss=6.7956
	step [141/255], loss=4.8667
	step [142/255], loss=6.0968
	step [143/255], loss=5.9941
	step [144/255], loss=6.1388
	step [145/255], loss=5.1308
	step [146/255], loss=5.1819
	step [147/255], loss=7.4039
	step [148/255], loss=5.3241
	step [149/255], loss=5.7086
	step [150/255], loss=7.6709
	step [151/255], loss=6.4802
	step [152/255], loss=4.9518
	step [153/255], loss=6.6249
	step [154/255], loss=5.6029
	step [155/255], loss=4.7148
	step [156/255], loss=6.5966
	step [157/255], loss=5.2803
	step [158/255], loss=5.4893
	step [159/255], loss=6.6884
	step [160/255], loss=6.1016
	step [161/255], loss=5.9235
	step [162/255], loss=5.7020
	step [163/255], loss=6.2556
	step [164/255], loss=4.7580
	step [165/255], loss=5.8029
	step [166/255], loss=6.4728
	step [167/255], loss=5.1144
	step [168/255], loss=5.0420
	step [169/255], loss=6.4803
	step [170/255], loss=6.6056
	step [171/255], loss=5.3841
	step [172/255], loss=5.7526
	step [173/255], loss=5.9922
	step [174/255], loss=5.7232
	step [175/255], loss=5.9107
	step [176/255], loss=6.9236
	step [177/255], loss=5.6168
	step [178/255], loss=5.4257
	step [179/255], loss=6.0229
	step [180/255], loss=5.0745
	step [181/255], loss=5.1662
	step [182/255], loss=6.8446
	step [183/255], loss=6.3851
	step [184/255], loss=4.7731
	step [185/255], loss=5.4802
	step [186/255], loss=7.6659
	step [187/255], loss=5.7427
	step [188/255], loss=6.2285
	step [189/255], loss=6.1920
	step [190/255], loss=7.1616
	step [191/255], loss=5.8251
	step [192/255], loss=6.4966
	step [193/255], loss=6.7433
	step [194/255], loss=6.1810
	step [195/255], loss=7.6503
	step [196/255], loss=5.8921
	step [197/255], loss=4.8552
	step [198/255], loss=5.4465
	step [199/255], loss=5.0625
	step [200/255], loss=6.5852
	step [201/255], loss=6.3131
	step [202/255], loss=6.2527
	step [203/255], loss=6.1072
	step [204/255], loss=5.8577
	step [205/255], loss=5.8580
	step [206/255], loss=4.7594
	step [207/255], loss=5.9217
	step [208/255], loss=4.6008
	step [209/255], loss=6.3960
	step [210/255], loss=5.3136
	step [211/255], loss=5.2212
	step [212/255], loss=5.6158
	step [213/255], loss=5.4799
	step [214/255], loss=6.7465
	step [215/255], loss=6.9593
	step [216/255], loss=5.9363
	step [217/255], loss=4.6812
	step [218/255], loss=5.2128
	step [219/255], loss=6.9346
	step [220/255], loss=5.6393
	step [221/255], loss=5.7998
	step [222/255], loss=5.2289
	step [223/255], loss=5.8364
	step [224/255], loss=6.2711
	step [225/255], loss=6.1899
	step [226/255], loss=6.1834
	step [227/255], loss=5.5207
	step [228/255], loss=7.2172
	step [229/255], loss=5.0106
	step [230/255], loss=5.2851
	step [231/255], loss=5.8678
	step [232/255], loss=5.6924
	step [233/255], loss=5.3220
	step [234/255], loss=5.4368
	step [235/255], loss=4.7978
	step [236/255], loss=6.0565
	step [237/255], loss=6.3634
	step [238/255], loss=6.9053
	step [239/255], loss=5.1215
	step [240/255], loss=6.2524
	step [241/255], loss=6.2502
	step [242/255], loss=7.6818
	step [243/255], loss=4.6978
	step [244/255], loss=5.0262
	step [245/255], loss=7.3263
	step [246/255], loss=5.1355
	step [247/255], loss=5.5818
	step [248/255], loss=6.8492
	step [249/255], loss=5.8201
	step [250/255], loss=5.8156
	step [251/255], loss=5.8856
	step [252/255], loss=6.1348
	step [253/255], loss=6.4289
	step [254/255], loss=6.7960
	step [255/255], loss=5.4363
	Evaluating
	loss=0.0181, precision=0.2176, recall=0.9916, f1=0.7314
saving model as: 0_saved_model.pth
Training epoch 30
	step [1/255], loss=5.0711
	step [2/255], loss=5.7488
	step [3/255], loss=5.5931
	step [4/255], loss=6.1358
	step [5/255], loss=4.5846
	step [6/255], loss=5.1649
	step [7/255], loss=5.8571
	step [8/255], loss=4.2656
	step [9/255], loss=5.7877
	step [10/255], loss=6.3596
	step [11/255], loss=4.8290
	step [12/255], loss=7.7732
	step [13/255], loss=7.1139
	step [14/255], loss=5.5223
	step [15/255], loss=5.4128
	step [16/255], loss=6.9351
	step [17/255], loss=4.9223
	step [18/255], loss=4.8575
	step [19/255], loss=5.5146
	step [20/255], loss=5.8091
	step [21/255], loss=5.8403
	step [22/255], loss=5.2094
	step [23/255], loss=5.3533
	step [24/255], loss=6.5210
	step [25/255], loss=5.6624
	step [26/255], loss=5.1311
	step [27/255], loss=5.2956
	step [28/255], loss=5.0880
	step [29/255], loss=6.9370
	step [30/255], loss=6.2806
	step [31/255], loss=4.8227
	step [32/255], loss=8.2637
	step [33/255], loss=5.3055
	step [34/255], loss=5.1734
	step [35/255], loss=6.1712
	step [36/255], loss=5.5541
	step [37/255], loss=6.3835
	step [38/255], loss=5.9995
	step [39/255], loss=7.6048
	step [40/255], loss=4.8964
	step [41/255], loss=5.8597
	step [42/255], loss=5.8529
	step [43/255], loss=6.4387
	step [44/255], loss=4.5681
	step [45/255], loss=6.6016
	step [46/255], loss=5.8840
	step [47/255], loss=5.3118
	step [48/255], loss=5.0635
	step [49/255], loss=4.7222
	step [50/255], loss=6.3541
	step [51/255], loss=6.1113
	step [52/255], loss=5.5066
	step [53/255], loss=7.7000
	step [54/255], loss=5.4620
	step [55/255], loss=5.0250
	step [56/255], loss=5.6444
	step [57/255], loss=5.6646
	step [58/255], loss=5.1816
	step [59/255], loss=7.3409
	step [60/255], loss=5.8834
	step [61/255], loss=5.5327
	step [62/255], loss=4.4268
	step [63/255], loss=5.8058
	step [64/255], loss=4.9126
	step [65/255], loss=6.8215
	step [66/255], loss=4.4678
	step [67/255], loss=6.3818
	step [68/255], loss=5.4809
	step [69/255], loss=5.8772
	step [70/255], loss=6.2814
	step [71/255], loss=6.8115
	step [72/255], loss=6.1404
	step [73/255], loss=6.2454
	step [74/255], loss=4.9896
	step [75/255], loss=5.1596
	step [76/255], loss=5.7180
	step [77/255], loss=4.7058
	step [78/255], loss=5.0008
	step [79/255], loss=6.4571
	step [80/255], loss=5.7000
	step [81/255], loss=7.2759
	step [82/255], loss=6.6665
	step [83/255], loss=6.1220
	step [84/255], loss=5.4794
	step [85/255], loss=6.4365
	step [86/255], loss=5.5926
	step [87/255], loss=6.3695
	step [88/255], loss=5.2652
	step [89/255], loss=5.2897
	step [90/255], loss=6.1150
	step [91/255], loss=5.6938
	step [92/255], loss=7.4945
	step [93/255], loss=4.9065
	step [94/255], loss=5.8072
	step [95/255], loss=5.5625
	step [96/255], loss=5.3066
	step [97/255], loss=7.1939
	step [98/255], loss=7.0629
	step [99/255], loss=5.0482
	step [100/255], loss=5.0399
	step [101/255], loss=6.1774
	step [102/255], loss=5.4359
	step [103/255], loss=6.3724
	step [104/255], loss=5.7015
	step [105/255], loss=8.2462
	step [106/255], loss=5.2457
	step [107/255], loss=5.0364
	step [108/255], loss=5.6452
	step [109/255], loss=6.3157
	step [110/255], loss=6.6302
	step [111/255], loss=5.4369
	step [112/255], loss=4.6711
	step [113/255], loss=4.9335
	step [114/255], loss=4.8892
	step [115/255], loss=5.9357
	step [116/255], loss=5.6873
	step [117/255], loss=6.1956
	step [118/255], loss=6.4406
	step [119/255], loss=6.8682
	step [120/255], loss=5.5004
	step [121/255], loss=6.4280
	step [122/255], loss=5.4333
	step [123/255], loss=5.5097
	step [124/255], loss=5.9935
	step [125/255], loss=5.0665
	step [126/255], loss=5.9188
	step [127/255], loss=6.0856
	step [128/255], loss=6.0676
	step [129/255], loss=4.6203
	step [130/255], loss=4.8290
	step [131/255], loss=5.0413
	step [132/255], loss=4.9610
	step [133/255], loss=4.9749
	step [134/255], loss=4.5410
	step [135/255], loss=7.0694
	step [136/255], loss=5.5288
	step [137/255], loss=7.0491
	step [138/255], loss=5.2215
	step [139/255], loss=5.2898
	step [140/255], loss=4.7207
	step [141/255], loss=5.8685
	step [142/255], loss=4.9912
	step [143/255], loss=7.6585
	step [144/255], loss=4.9456
	step [145/255], loss=5.0887
	step [146/255], loss=5.2643
	step [147/255], loss=7.2892
	step [148/255], loss=5.1752
	step [149/255], loss=5.8780
	step [150/255], loss=5.5816
	step [151/255], loss=6.8047
	step [152/255], loss=6.3883
	step [153/255], loss=5.2542
	step [154/255], loss=5.4102
	step [155/255], loss=5.6418
	step [156/255], loss=6.0133
	step [157/255], loss=5.1346
	step [158/255], loss=4.6957
	step [159/255], loss=5.7077
	step [160/255], loss=6.5917
	step [161/255], loss=5.2891
	step [162/255], loss=5.1781
	step [163/255], loss=6.0937
	step [164/255], loss=5.4035
	step [165/255], loss=6.7208
	step [166/255], loss=4.6596
	step [167/255], loss=7.0095
	step [168/255], loss=6.6136
	step [169/255], loss=6.7032
	step [170/255], loss=5.1675
	step [171/255], loss=4.8451
	step [172/255], loss=5.7532
	step [173/255], loss=5.5932
	step [174/255], loss=6.2299
	step [175/255], loss=6.2604
	step [176/255], loss=6.5844
	step [177/255], loss=7.0025
	step [178/255], loss=6.3815
	step [179/255], loss=6.2064
	step [180/255], loss=6.5031
	step [181/255], loss=6.7232
	step [182/255], loss=5.1374
	step [183/255], loss=4.9905
	step [184/255], loss=6.4544
	step [185/255], loss=5.6383
	step [186/255], loss=6.0262
	step [187/255], loss=5.6628
	step [188/255], loss=6.2198
	step [189/255], loss=7.2645
	step [190/255], loss=6.6017
	step [191/255], loss=6.7565
	step [192/255], loss=5.1193
	step [193/255], loss=4.7204
	step [194/255], loss=6.6784
	step [195/255], loss=6.3542
	step [196/255], loss=7.0562
	step [197/255], loss=5.9327
	step [198/255], loss=6.0896
	step [199/255], loss=5.8975
	step [200/255], loss=4.9168
	step [201/255], loss=6.3893
	step [202/255], loss=5.8039
	step [203/255], loss=6.6455
	step [204/255], loss=5.0688
	step [205/255], loss=4.7112
	step [206/255], loss=6.1895
	step [207/255], loss=5.6325
	step [208/255], loss=6.9293
	step [209/255], loss=7.1726
	step [210/255], loss=4.8254
	step [211/255], loss=4.8530
	step [212/255], loss=5.8959
	step [213/255], loss=7.8414
	step [214/255], loss=6.5767
	step [215/255], loss=6.5389
	step [216/255], loss=4.5027
	step [217/255], loss=6.0425
	step [218/255], loss=5.3777
	step [219/255], loss=5.8799
	step [220/255], loss=7.0398
	step [221/255], loss=4.8722
	step [222/255], loss=4.9386
	step [223/255], loss=5.8947
	step [224/255], loss=5.8502
	step [225/255], loss=6.4017
	step [226/255], loss=8.8105
	step [227/255], loss=6.1740
	step [228/255], loss=5.8428
	step [229/255], loss=6.7961
	step [230/255], loss=6.0351
	step [231/255], loss=7.5753
	step [232/255], loss=7.6536
	step [233/255], loss=5.9913
	step [234/255], loss=7.3995
	step [235/255], loss=5.5856
	step [236/255], loss=5.4318
	step [237/255], loss=5.3567
	step [238/255], loss=5.8850
	step [239/255], loss=5.6663
	step [240/255], loss=6.5276
	step [241/255], loss=5.2081
	step [242/255], loss=5.0325
	step [243/255], loss=6.3080
	step [244/255], loss=5.4048
	step [245/255], loss=6.8469
	step [246/255], loss=5.7407
	step [247/255], loss=5.3573
	step [248/255], loss=5.6900
	step [249/255], loss=5.3554
	step [250/255], loss=4.9700
	step [251/255], loss=5.4263
	step [252/255], loss=6.0347
	step [253/255], loss=4.8276
	step [254/255], loss=5.9887
	step [255/255], loss=4.2941
	Evaluating
	loss=0.0215, precision=0.1940, recall=0.9923, f1=0.7031
Training finished
best_f1: 0.7314109767584174
directing: X rim_enhanced: False test_id 1
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2707 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_two/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/191], loss=2046.7047
	step [2/191], loss=1468.3666
	step [3/191], loss=966.4210
	step [4/191], loss=881.8928
	step [5/191], loss=632.8103
	step [6/191], loss=641.6945
	step [7/191], loss=541.2433
	step [8/191], loss=431.1371
	step [9/191], loss=384.3413
	step [10/191], loss=309.3544
	step [11/191], loss=296.7299
	step [12/191], loss=245.4149
	step [13/191], loss=230.1623
	step [14/191], loss=208.1863
	step [15/191], loss=193.9578
	step [16/191], loss=176.2493
	step [17/191], loss=164.9275
	step [18/191], loss=162.6223
	step [19/191], loss=149.7624
	step [20/191], loss=140.3741
	step [21/191], loss=133.9912
	step [22/191], loss=125.1577
	step [23/191], loss=124.8530
	step [24/191], loss=122.8237
	step [25/191], loss=115.9773
	step [26/191], loss=120.5850
	step [27/191], loss=115.4328
	step [28/191], loss=115.7651
	step [29/191], loss=109.8504
	step [30/191], loss=113.6302
	step [31/191], loss=103.2648
	step [32/191], loss=108.3908
	step [33/191], loss=106.1899
	step [34/191], loss=107.9819
	step [35/191], loss=101.3465
	step [36/191], loss=98.3960
	step [37/191], loss=101.6603
	step [38/191], loss=103.7798
	step [39/191], loss=99.1764
	step [40/191], loss=105.0532
	step [41/191], loss=98.9646
	step [42/191], loss=96.5386
	step [43/191], loss=97.6892
	step [44/191], loss=95.2155
	step [45/191], loss=96.0915
	step [46/191], loss=93.8155
	step [47/191], loss=95.4382
	step [48/191], loss=95.8912
	step [49/191], loss=93.5411
	step [50/191], loss=95.2945
	step [51/191], loss=92.5088
	step [52/191], loss=93.5228
	step [53/191], loss=93.2834
	step [54/191], loss=92.8579
	step [55/191], loss=91.3480
	step [56/191], loss=89.7805
	step [57/191], loss=92.8805
	step [58/191], loss=92.2425
	step [59/191], loss=92.9141
	step [60/191], loss=92.0128
	step [61/191], loss=89.0376
	step [62/191], loss=91.0775
	step [63/191], loss=87.8949
	step [64/191], loss=88.9403
	step [65/191], loss=88.8375
	step [66/191], loss=87.4706
	step [67/191], loss=87.3674
	step [68/191], loss=86.1290
	step [69/191], loss=88.2923
	step [70/191], loss=86.2970
	step [71/191], loss=86.7073
	step [72/191], loss=87.3678
	step [73/191], loss=86.0657
	step [74/191], loss=87.1102
	step [75/191], loss=85.7591
	step [76/191], loss=87.9347
	step [77/191], loss=86.8405
	step [78/191], loss=83.7453
	step [79/191], loss=85.2269
	step [80/191], loss=85.9815
	step [81/191], loss=84.8394
	step [82/191], loss=87.6879
	step [83/191], loss=86.0663
	step [84/191], loss=84.2400
	step [85/191], loss=84.2309
	step [86/191], loss=84.2371
	step [87/191], loss=82.6515
	step [88/191], loss=83.4778
	step [89/191], loss=85.3761
	step [90/191], loss=82.8483
	step [91/191], loss=82.6096
	step [92/191], loss=83.2903
	step [93/191], loss=82.3715
	step [94/191], loss=81.6412
	step [95/191], loss=82.2967
	step [96/191], loss=80.4918
	step [97/191], loss=82.5390
	step [98/191], loss=80.3095
	step [99/191], loss=81.6112
	step [100/191], loss=80.2938
	step [101/191], loss=80.7816
	step [102/191], loss=82.4700
	step [103/191], loss=80.1321
	step [104/191], loss=82.1277
	step [105/191], loss=79.5846
	step [106/191], loss=80.6487
	step [107/191], loss=77.7017
	step [108/191], loss=81.3870
	step [109/191], loss=80.4558
	step [110/191], loss=80.3903
	step [111/191], loss=77.8017
	step [112/191], loss=80.0758
	step [113/191], loss=77.8548
	step [114/191], loss=77.6537
	step [115/191], loss=79.9881
	step [116/191], loss=77.0939
	step [117/191], loss=77.9113
	step [118/191], loss=77.6315
	step [119/191], loss=79.1113
	step [120/191], loss=77.8135
	step [121/191], loss=79.0818
	step [122/191], loss=76.3163
	step [123/191], loss=77.5318
	step [124/191], loss=77.7336
	step [125/191], loss=78.1689
	step [126/191], loss=78.2072
	step [127/191], loss=76.7769
	step [128/191], loss=76.8251
	step [129/191], loss=77.2964
	step [130/191], loss=75.4247
	step [131/191], loss=74.7788
	step [132/191], loss=74.5459
	step [133/191], loss=77.5331
	step [134/191], loss=75.2411
	step [135/191], loss=75.3135
	step [136/191], loss=75.4433
	step [137/191], loss=75.4956
	step [138/191], loss=72.6793
	step [139/191], loss=75.4485
	step [140/191], loss=73.4058
	step [141/191], loss=76.5237
	step [142/191], loss=73.9923
	step [143/191], loss=74.6165
	step [144/191], loss=76.6044
	step [145/191], loss=74.4775
	step [146/191], loss=76.3161
	step [147/191], loss=77.1557
	step [148/191], loss=72.8062
	step [149/191], loss=74.7658
	step [150/191], loss=71.7931
	step [151/191], loss=74.6981
	step [152/191], loss=71.6768
	step [153/191], loss=73.7615
	step [154/191], loss=73.7069
	step [155/191], loss=72.6076
	step [156/191], loss=72.4756
	step [157/191], loss=71.9035
	step [158/191], loss=72.2130
	step [159/191], loss=71.8435
	step [160/191], loss=71.6241
	step [161/191], loss=71.6181
	step [162/191], loss=72.9535
	step [163/191], loss=72.0589
	step [164/191], loss=70.7509
	step [165/191], loss=70.0929
	step [166/191], loss=68.8157
	step [167/191], loss=68.6041
	step [168/191], loss=71.0445
	step [169/191], loss=69.7695
	step [170/191], loss=70.5890
	step [171/191], loss=69.9498
	step [172/191], loss=69.2428
	step [173/191], loss=69.7607
	step [174/191], loss=70.5974
	step [175/191], loss=69.6240
	step [176/191], loss=67.9495
	step [177/191], loss=68.9943
	step [178/191], loss=68.5233
	step [179/191], loss=68.3866
	step [180/191], loss=67.5691
	step [181/191], loss=69.0478
	step [182/191], loss=66.8234
	step [183/191], loss=68.5725
	step [184/191], loss=68.3148
	step [185/191], loss=67.3979
	step [186/191], loss=68.6077
	step [187/191], loss=71.0477
	step [188/191], loss=67.9739
	step [189/191], loss=71.2708
	step [190/191], loss=67.7129
	step [191/191], loss=31.1244
	Evaluating
	loss=0.3189, precision=0.1421, recall=0.9858, f1=0.6185
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/191], loss=69.5634
	step [2/191], loss=68.6889
	step [3/191], loss=67.2468
	step [4/191], loss=68.6471
	step [5/191], loss=67.2863
	step [6/191], loss=67.7621
	step [7/191], loss=66.8686
	step [8/191], loss=67.1987
	step [9/191], loss=66.0984
	step [10/191], loss=66.6306
	step [11/191], loss=66.2254
	step [12/191], loss=66.6955
	step [13/191], loss=70.1140
	step [14/191], loss=67.1796
	step [15/191], loss=66.5511
	step [16/191], loss=66.6052
	step [17/191], loss=64.5371
	step [18/191], loss=65.8855
	step [19/191], loss=65.2913
	step [20/191], loss=66.8221
	step [21/191], loss=64.1697
	step [22/191], loss=63.7472
	step [23/191], loss=64.0809
	step [24/191], loss=65.5169
	step [25/191], loss=67.1999
	step [26/191], loss=65.3030
	step [27/191], loss=65.0200
	step [28/191], loss=65.5317
	step [29/191], loss=65.2047
	step [30/191], loss=66.0256
	step [31/191], loss=65.7800
	step [32/191], loss=66.3093
	step [33/191], loss=63.8899
	step [34/191], loss=65.8687
	step [35/191], loss=63.8890
	step [36/191], loss=64.5673
	step [37/191], loss=65.4059
	step [38/191], loss=64.8474
	step [39/191], loss=65.7691
	step [40/191], loss=64.0476
	step [41/191], loss=62.9462
	step [42/191], loss=63.1809
	step [43/191], loss=66.5945
	step [44/191], loss=65.7737
	step [45/191], loss=64.5594
	step [46/191], loss=62.8021
	step [47/191], loss=64.4345
	step [48/191], loss=66.3325
	step [49/191], loss=63.1835
	step [50/191], loss=65.6075
	step [51/191], loss=62.3282
	step [52/191], loss=62.9454
	step [53/191], loss=61.4759
	step [54/191], loss=61.6615
	step [55/191], loss=62.9632
	step [56/191], loss=63.2943
	step [57/191], loss=63.2171
	step [58/191], loss=62.4804
	step [59/191], loss=63.8552
	step [60/191], loss=61.9625
	step [61/191], loss=61.7915
	step [62/191], loss=62.3950
	step [63/191], loss=61.4709
	step [64/191], loss=62.9169
	step [65/191], loss=63.4339
	step [66/191], loss=61.9400
	step [67/191], loss=61.3902
	step [68/191], loss=61.6872
	step [69/191], loss=61.2258
	step [70/191], loss=60.8485
	step [71/191], loss=61.0969
	step [72/191], loss=62.0937
	step [73/191], loss=61.4940
	step [74/191], loss=60.1572
	step [75/191], loss=60.8047
	step [76/191], loss=62.1729
	step [77/191], loss=60.1220
	step [78/191], loss=63.6401
	step [79/191], loss=59.8030
	step [80/191], loss=59.1479
	step [81/191], loss=62.5138
	step [82/191], loss=60.9210
	step [83/191], loss=59.4718
	step [84/191], loss=60.0307
	step [85/191], loss=61.5705
	step [86/191], loss=62.4807
	step [87/191], loss=60.1956
	step [88/191], loss=61.7593
	step [89/191], loss=58.4470
	step [90/191], loss=58.6134
	step [91/191], loss=59.9750
	step [92/191], loss=59.1880
	step [93/191], loss=59.2822
	step [94/191], loss=59.4733
	step [95/191], loss=59.7117
	step [96/191], loss=60.4855
	step [97/191], loss=60.6411
	step [98/191], loss=60.0772
	step [99/191], loss=59.0315
	step [100/191], loss=60.4073
	step [101/191], loss=59.0600
	step [102/191], loss=61.4105
	step [103/191], loss=60.4525
	step [104/191], loss=56.9051
	step [105/191], loss=57.4246
	step [106/191], loss=60.1263
	step [107/191], loss=58.0163
	step [108/191], loss=60.2729
	step [109/191], loss=58.0621
	step [110/191], loss=56.8041
	step [111/191], loss=57.6302
	step [112/191], loss=56.7266
	step [113/191], loss=56.9952
	step [114/191], loss=58.3685
	step [115/191], loss=56.7032
	step [116/191], loss=57.5905
	step [117/191], loss=59.9751
	step [118/191], loss=56.8091
	step [119/191], loss=57.8497
	step [120/191], loss=58.1278
	step [121/191], loss=58.4510
	step [122/191], loss=56.5850
	step [123/191], loss=57.6689
	step [124/191], loss=56.0060
	step [125/191], loss=58.2667
	step [126/191], loss=56.4668
	step [127/191], loss=57.1935
	step [128/191], loss=56.5296
	step [129/191], loss=58.8468
	step [130/191], loss=56.0790
	step [131/191], loss=56.6640
	step [132/191], loss=58.0222
	step [133/191], loss=57.4340
	step [134/191], loss=54.4920
	step [135/191], loss=56.3629
	step [136/191], loss=55.6881
	step [137/191], loss=58.1698
	step [138/191], loss=55.6096
	step [139/191], loss=55.8393
	step [140/191], loss=56.1650
	step [141/191], loss=55.8632
	step [142/191], loss=56.0214
	step [143/191], loss=58.0064
	step [144/191], loss=54.1987
	step [145/191], loss=57.2948
	step [146/191], loss=57.0328
	step [147/191], loss=58.0557
	step [148/191], loss=56.0519
	step [149/191], loss=53.6509
	step [150/191], loss=56.7968
	step [151/191], loss=56.1217
	step [152/191], loss=55.8330
	step [153/191], loss=56.6884
	step [154/191], loss=56.5347
	step [155/191], loss=58.6445
	step [156/191], loss=56.5602
	step [157/191], loss=55.2439
	step [158/191], loss=54.9714
	step [159/191], loss=55.7237
	step [160/191], loss=54.4933
	step [161/191], loss=55.6384
	step [162/191], loss=53.3716
	step [163/191], loss=53.5650
	step [164/191], loss=55.3503
	step [165/191], loss=55.5898
	step [166/191], loss=53.9134
	step [167/191], loss=54.2259
	step [168/191], loss=55.1667
	step [169/191], loss=54.2547
	step [170/191], loss=54.0194
	step [171/191], loss=54.2588
	step [172/191], loss=53.5812
	step [173/191], loss=55.6026
	step [174/191], loss=55.0927
	step [175/191], loss=53.9445
	step [176/191], loss=53.7172
	step [177/191], loss=55.5541
	step [178/191], loss=53.6399
	step [179/191], loss=52.8994
	step [180/191], loss=53.9592
	step [181/191], loss=56.6565
	step [182/191], loss=56.9883
	step [183/191], loss=55.2864
	step [184/191], loss=58.0489
	step [185/191], loss=55.4838
	step [186/191], loss=53.9596
	step [187/191], loss=54.5416
	step [188/191], loss=53.5782
	step [189/191], loss=54.4364
	step [190/191], loss=56.9707
	step [191/191], loss=23.0734
	Evaluating
	loss=0.2384, precision=0.2064, recall=0.9835, f1=0.7145
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/191], loss=53.2297
	step [2/191], loss=51.9112
	step [3/191], loss=52.7961
	step [4/191], loss=53.8332
	step [5/191], loss=52.8319
	step [6/191], loss=51.3459
	step [7/191], loss=51.6112
	step [8/191], loss=52.1035
	step [9/191], loss=52.6176
	step [10/191], loss=51.9431
	step [11/191], loss=53.6907
	step [12/191], loss=52.6874
	step [13/191], loss=51.8334
	step [14/191], loss=53.5948
	step [15/191], loss=53.4003
	step [16/191], loss=52.5259
	step [17/191], loss=52.7193
	step [18/191], loss=50.4988
	step [19/191], loss=52.7731
	step [20/191], loss=52.7836
	step [21/191], loss=50.3782
	step [22/191], loss=51.9898
	step [23/191], loss=53.0302
	step [24/191], loss=53.8346
	step [25/191], loss=50.9329
	step [26/191], loss=51.8111
	step [27/191], loss=52.2196
	step [28/191], loss=50.5405
	step [29/191], loss=49.9808
	step [30/191], loss=53.8718
	step [31/191], loss=50.4785
	step [32/191], loss=52.8211
	step [33/191], loss=51.2421
	step [34/191], loss=49.4841
	step [35/191], loss=50.0661
	step [36/191], loss=50.0043
	step [37/191], loss=52.5040
	step [38/191], loss=50.7840
	step [39/191], loss=50.9691
	step [40/191], loss=52.6081
	step [41/191], loss=50.6095
	step [42/191], loss=50.1469
	step [43/191], loss=51.0737
	step [44/191], loss=52.1091
	step [45/191], loss=50.5139
	step [46/191], loss=51.1646
	step [47/191], loss=50.7582
	step [48/191], loss=49.6595
	step [49/191], loss=50.7956
	step [50/191], loss=50.9500
	step [51/191], loss=49.8543
	step [52/191], loss=50.6343
	step [53/191], loss=50.9827
	step [54/191], loss=52.1960
	step [55/191], loss=51.4136
	step [56/191], loss=49.9537
	step [57/191], loss=49.0755
	step [58/191], loss=50.3554
	step [59/191], loss=51.0480
	step [60/191], loss=48.3306
	step [61/191], loss=51.2232
	step [62/191], loss=50.0198
	step [63/191], loss=50.6851
	step [64/191], loss=50.5124
	step [65/191], loss=51.5956
	step [66/191], loss=49.7368
	step [67/191], loss=50.1103
	step [68/191], loss=49.3741
	step [69/191], loss=49.7036
	step [70/191], loss=48.9832
	step [71/191], loss=49.1758
	step [72/191], loss=49.4652
	step [73/191], loss=50.9208
	step [74/191], loss=51.4107
	step [75/191], loss=49.0009
	step [76/191], loss=49.3263
	step [77/191], loss=50.8025
	step [78/191], loss=49.4275
	step [79/191], loss=49.1534
	step [80/191], loss=47.6737
	step [81/191], loss=49.0853
	step [82/191], loss=49.7514
	step [83/191], loss=49.8571
	step [84/191], loss=49.1830
	step [85/191], loss=47.8845
	step [86/191], loss=50.0394
	step [87/191], loss=52.6764
	step [88/191], loss=47.9170
	step [89/191], loss=49.6704
	step [90/191], loss=49.1656
	step [91/191], loss=48.1573
	step [92/191], loss=48.3889
	step [93/191], loss=49.4173
	step [94/191], loss=47.8854
	step [95/191], loss=47.8701
	step [96/191], loss=48.6768
	step [97/191], loss=47.6471
	step [98/191], loss=50.6828
	step [99/191], loss=47.2538
	step [100/191], loss=47.9326
	step [101/191], loss=46.9949
	step [102/191], loss=48.4863
	step [103/191], loss=47.7083
	step [104/191], loss=47.3893
	step [105/191], loss=48.4540
	step [106/191], loss=48.3344
	step [107/191], loss=46.8274
	step [108/191], loss=48.1846
	step [109/191], loss=47.6695
	step [110/191], loss=47.6327
	step [111/191], loss=47.2047
	step [112/191], loss=49.2990
	step [113/191], loss=47.9676
	step [114/191], loss=47.4479
	step [115/191], loss=49.3706
	step [116/191], loss=46.5877
	step [117/191], loss=48.3208
	step [118/191], loss=45.9595
	step [119/191], loss=47.0066
	step [120/191], loss=47.5424
	step [121/191], loss=45.3626
	step [122/191], loss=45.8417
	step [123/191], loss=47.5748
	step [124/191], loss=46.3855
	step [125/191], loss=46.6417
	step [126/191], loss=46.3513
	step [127/191], loss=46.9062
	step [128/191], loss=46.3632
	step [129/191], loss=47.4912
	step [130/191], loss=46.0995
	step [131/191], loss=46.2690
	step [132/191], loss=46.9123
	step [133/191], loss=48.3873
	step [134/191], loss=47.5267
	step [135/191], loss=48.5666
	step [136/191], loss=47.9368
	step [137/191], loss=47.0505
	step [138/191], loss=46.1027
	step [139/191], loss=44.8765
	step [140/191], loss=46.7805
	step [141/191], loss=43.9576
	step [142/191], loss=44.6663
	step [143/191], loss=46.3281
	step [144/191], loss=47.7325
	step [145/191], loss=46.4197
	step [146/191], loss=45.5981
	step [147/191], loss=48.0267
	step [148/191], loss=46.1235
	step [149/191], loss=45.4287
	step [150/191], loss=45.8470
	step [151/191], loss=46.7440
	step [152/191], loss=44.8509
	step [153/191], loss=45.2449
	step [154/191], loss=46.2832
	step [155/191], loss=46.8949
	step [156/191], loss=46.8175
	step [157/191], loss=45.5884
	step [158/191], loss=44.5261
	step [159/191], loss=45.0680
	step [160/191], loss=46.7573
	step [161/191], loss=43.7625
	step [162/191], loss=46.4781
	step [163/191], loss=45.9251
	step [164/191], loss=43.6612
	step [165/191], loss=44.1099
	step [166/191], loss=43.8891
	step [167/191], loss=46.4641
	step [168/191], loss=44.8043
	step [169/191], loss=45.0461
	step [170/191], loss=46.4182
	step [171/191], loss=46.4199
	step [172/191], loss=48.8394
	step [173/191], loss=43.9138
	step [174/191], loss=46.7492
	step [175/191], loss=47.0690
	step [176/191], loss=44.6724
	step [177/191], loss=44.3382
	step [178/191], loss=44.3089
	step [179/191], loss=45.6351
	step [180/191], loss=45.3318
	step [181/191], loss=46.9994
	step [182/191], loss=44.9946
	step [183/191], loss=46.0200
	step [184/191], loss=47.0713
	step [185/191], loss=43.6536
	step [186/191], loss=44.1659
	step [187/191], loss=44.8153
	step [188/191], loss=44.3180
	step [189/191], loss=46.6362
	step [190/191], loss=44.4858
	step [191/191], loss=19.6365
	Evaluating
	loss=0.1929, precision=0.2347, recall=0.9822, f1=0.7449
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/191], loss=44.1960
	step [2/191], loss=44.0959
	step [3/191], loss=45.7460
	step [4/191], loss=43.0325
	step [5/191], loss=46.6435
	step [6/191], loss=44.3855
	step [7/191], loss=42.0723
	step [8/191], loss=45.4706
	step [9/191], loss=43.8542
	step [10/191], loss=42.9931
	step [11/191], loss=43.3927
	step [12/191], loss=45.4821
	step [13/191], loss=42.8978
	step [14/191], loss=44.6040
	step [15/191], loss=42.4852
	step [16/191], loss=41.9120
	step [17/191], loss=42.4402
	step [18/191], loss=42.7811
	step [19/191], loss=43.5989
	step [20/191], loss=43.8265
	step [21/191], loss=43.7506
	step [22/191], loss=43.4269
	step [23/191], loss=42.0892
	step [24/191], loss=41.8754
	step [25/191], loss=44.6434
	step [26/191], loss=44.1968
	step [27/191], loss=46.3487
	step [28/191], loss=45.2137
	step [29/191], loss=44.7755
	step [30/191], loss=42.8992
	step [31/191], loss=43.1905
	step [32/191], loss=43.4854
	step [33/191], loss=42.4210
	step [34/191], loss=44.6756
	step [35/191], loss=44.3946
	step [36/191], loss=43.3127
	step [37/191], loss=42.1759
	step [38/191], loss=41.8923
	step [39/191], loss=41.6758
	step [40/191], loss=41.9596
	step [41/191], loss=42.5338
	step [42/191], loss=43.7948
	step [43/191], loss=41.9660
	step [44/191], loss=43.6279
	step [45/191], loss=41.8972
	step [46/191], loss=41.8534
	step [47/191], loss=41.1564
	step [48/191], loss=43.6240
	step [49/191], loss=44.0131
	step [50/191], loss=44.0434
	step [51/191], loss=41.8590
	step [52/191], loss=41.4806
	step [53/191], loss=42.9552
	step [54/191], loss=41.4843
	step [55/191], loss=42.2974
	step [56/191], loss=43.5414
	step [57/191], loss=41.4822
	step [58/191], loss=41.7811
	step [59/191], loss=41.9028
	step [60/191], loss=39.6634
	step [61/191], loss=41.2002
	step [62/191], loss=44.2814
	step [63/191], loss=40.9379
	step [64/191], loss=42.9315
	step [65/191], loss=44.6266
	step [66/191], loss=40.2635
	step [67/191], loss=40.7825
	step [68/191], loss=42.4596
	step [69/191], loss=40.1591
	step [70/191], loss=43.0982
	step [71/191], loss=43.6283
	step [72/191], loss=41.4830
	step [73/191], loss=40.0122
	step [74/191], loss=42.3711
	step [75/191], loss=41.5472
	step [76/191], loss=41.4616
	step [77/191], loss=42.6732
	step [78/191], loss=40.3258
	step [79/191], loss=40.9025
	step [80/191], loss=40.2809
	step [81/191], loss=42.6103
	step [82/191], loss=42.3117
	step [83/191], loss=42.4872
	step [84/191], loss=40.6650
	step [85/191], loss=41.2215
	step [86/191], loss=40.3469
	step [87/191], loss=40.8672
	step [88/191], loss=39.6892
	step [89/191], loss=41.3055
	step [90/191], loss=42.6668
	step [91/191], loss=41.6383
	step [92/191], loss=43.0828
	step [93/191], loss=40.0202
	step [94/191], loss=40.0565
	step [95/191], loss=39.0070
	step [96/191], loss=40.6018
	step [97/191], loss=42.0175
	step [98/191], loss=39.7052
	step [99/191], loss=39.2508
	step [100/191], loss=38.9722
	step [101/191], loss=42.3292
	step [102/191], loss=40.3597
	step [103/191], loss=40.0127
	step [104/191], loss=39.8800
	step [105/191], loss=38.8940
	step [106/191], loss=40.1584
	step [107/191], loss=38.5979
	step [108/191], loss=42.3789
	step [109/191], loss=39.0069
	step [110/191], loss=40.2198
	step [111/191], loss=39.1166
	step [112/191], loss=42.0062
	step [113/191], loss=40.3691
	step [114/191], loss=39.0458
	step [115/191], loss=38.3504
	step [116/191], loss=38.3762
	step [117/191], loss=38.6893
	step [118/191], loss=40.3191
	step [119/191], loss=39.3592
	step [120/191], loss=39.0942
	step [121/191], loss=38.8734
	step [122/191], loss=37.4900
	step [123/191], loss=40.3695
	step [124/191], loss=36.9705
	step [125/191], loss=38.2986
	step [126/191], loss=39.3174
	step [127/191], loss=38.8711
	step [128/191], loss=40.4658
	step [129/191], loss=39.8469
	step [130/191], loss=37.9917
	step [131/191], loss=40.1930
	step [132/191], loss=37.2274
	step [133/191], loss=41.3860
	step [134/191], loss=39.9277
	step [135/191], loss=38.1541
	step [136/191], loss=38.2374
	step [137/191], loss=37.9206
	step [138/191], loss=38.6718
	step [139/191], loss=39.5131
	step [140/191], loss=37.5004
	step [141/191], loss=38.7100
	step [142/191], loss=40.5180
	step [143/191], loss=40.5982
	step [144/191], loss=38.0126
	step [145/191], loss=39.1015
	step [146/191], loss=39.0502
	step [147/191], loss=37.8370
	step [148/191], loss=37.9889
	step [149/191], loss=38.3278
	step [150/191], loss=39.6070
	step [151/191], loss=38.1847
	step [152/191], loss=39.3426
	step [153/191], loss=38.3504
	step [154/191], loss=41.5202
	step [155/191], loss=38.2720
	step [156/191], loss=37.6973
	step [157/191], loss=38.9705
	step [158/191], loss=38.7859
	step [159/191], loss=37.0897
	step [160/191], loss=37.7350
	step [161/191], loss=37.9313
	step [162/191], loss=37.9739
	step [163/191], loss=37.4115
	step [164/191], loss=38.0595
	step [165/191], loss=38.1794
	step [166/191], loss=37.5407
	step [167/191], loss=37.1412
	step [168/191], loss=37.9874
	step [169/191], loss=37.7312
	step [170/191], loss=37.3669
	step [171/191], loss=36.9116
	step [172/191], loss=38.3537
	step [173/191], loss=38.1216
	step [174/191], loss=36.3557
	step [175/191], loss=37.8511
	step [176/191], loss=36.0334
	step [177/191], loss=37.7795
	step [178/191], loss=36.9882
	step [179/191], loss=39.6849
	step [180/191], loss=36.2891
	step [181/191], loss=37.8724
	step [182/191], loss=36.9706
	step [183/191], loss=37.3997
	step [184/191], loss=38.2642
	step [185/191], loss=37.8064
	step [186/191], loss=38.1415
	step [187/191], loss=36.1559
	step [188/191], loss=37.2091
	step [189/191], loss=38.2871
	step [190/191], loss=35.6735
	step [191/191], loss=16.6562
	Evaluating
	loss=0.1632, precision=0.2048, recall=0.9846, f1=0.7131
Training epoch 5
	step [1/191], loss=38.8004
	step [2/191], loss=39.2805
	step [3/191], loss=36.9523
	step [4/191], loss=35.9462
	step [5/191], loss=38.1500
	step [6/191], loss=35.4632
	step [7/191], loss=35.9089
	step [8/191], loss=37.1013
	step [9/191], loss=36.4016
	step [10/191], loss=37.0113
	step [11/191], loss=35.3007
	step [12/191], loss=36.9808
	step [13/191], loss=36.9338
	step [14/191], loss=35.7495
	step [15/191], loss=36.2273
	step [16/191], loss=34.9094
	step [17/191], loss=36.2115
	step [18/191], loss=35.3035
	step [19/191], loss=38.9844
	step [20/191], loss=35.7530
	step [21/191], loss=36.6344
	step [22/191], loss=37.0352
	step [23/191], loss=34.7763
	step [24/191], loss=37.2684
	step [25/191], loss=35.4273
	step [26/191], loss=34.8116
	step [27/191], loss=37.1241
	step [28/191], loss=37.8969
	step [29/191], loss=38.0238
	step [30/191], loss=34.9160
	step [31/191], loss=37.7146
	step [32/191], loss=34.4629
	step [33/191], loss=36.2120
	step [34/191], loss=38.5867
	step [35/191], loss=36.3447
	step [36/191], loss=37.1491
	step [37/191], loss=35.7455
	step [38/191], loss=35.0269
	step [39/191], loss=35.7364
	step [40/191], loss=35.3200
	step [41/191], loss=36.2506
	step [42/191], loss=35.2320
	step [43/191], loss=33.4373
	step [44/191], loss=35.8973
	step [45/191], loss=34.9983
	step [46/191], loss=36.0856
	step [47/191], loss=34.7904
	step [48/191], loss=38.0206
	step [49/191], loss=37.3878
	step [50/191], loss=36.0710
	step [51/191], loss=37.8201
	step [52/191], loss=34.9004
	step [53/191], loss=34.8617
	step [54/191], loss=35.2862
	step [55/191], loss=35.6080
	step [56/191], loss=36.9303
	step [57/191], loss=35.3284
	step [58/191], loss=34.2276
	step [59/191], loss=35.4262
	step [60/191], loss=35.6996
	step [61/191], loss=35.7808
	step [62/191], loss=35.4931
	step [63/191], loss=35.6047
	step [64/191], loss=35.0035
	step [65/191], loss=36.4666
	step [66/191], loss=33.2170
	step [67/191], loss=35.4464
	step [68/191], loss=35.1570
	step [69/191], loss=36.5982
	step [70/191], loss=36.2405
	step [71/191], loss=35.0201
	step [72/191], loss=36.1741
	step [73/191], loss=33.8165
	step [74/191], loss=36.0928
	step [75/191], loss=33.8782
	step [76/191], loss=36.6605
	step [77/191], loss=35.7922
	step [78/191], loss=33.7768
	step [79/191], loss=35.2197
	step [80/191], loss=32.7165
	step [81/191], loss=35.4357
	step [82/191], loss=34.1855
	step [83/191], loss=35.4966
	step [84/191], loss=32.5677
	step [85/191], loss=34.9514
	step [86/191], loss=33.5870
	step [87/191], loss=35.2653
	step [88/191], loss=33.5872
	step [89/191], loss=35.2634
	step [90/191], loss=33.6346
	step [91/191], loss=35.6089
	step [92/191], loss=35.2577
	step [93/191], loss=34.3465
	step [94/191], loss=32.7535
	step [95/191], loss=33.6278
	step [96/191], loss=34.0848
	step [97/191], loss=35.4373
	step [98/191], loss=33.3066
	step [99/191], loss=34.6813
	step [100/191], loss=34.0953
	step [101/191], loss=34.2007
	step [102/191], loss=34.9333
	step [103/191], loss=34.7486
	step [104/191], loss=34.7480
	step [105/191], loss=34.3698
	step [106/191], loss=34.3529
	step [107/191], loss=32.9070
	step [108/191], loss=33.3937
	step [109/191], loss=34.8221
	step [110/191], loss=34.5543
	step [111/191], loss=32.2021
	step [112/191], loss=34.3757
	step [113/191], loss=33.1625
	step [114/191], loss=33.4658
	step [115/191], loss=33.1822
	step [116/191], loss=32.6420
	step [117/191], loss=31.5356
	step [118/191], loss=32.8994
	step [119/191], loss=34.4099
	step [120/191], loss=34.0604
	step [121/191], loss=32.7127
	step [122/191], loss=34.4965
	step [123/191], loss=32.6565
	step [124/191], loss=31.1264
	step [125/191], loss=31.4627
	step [126/191], loss=32.6606
	step [127/191], loss=34.7491
	step [128/191], loss=32.2944
	step [129/191], loss=34.7199
	step [130/191], loss=34.6560
	step [131/191], loss=34.0133
	step [132/191], loss=32.2281
	step [133/191], loss=32.5103
	step [134/191], loss=34.5204
	step [135/191], loss=33.0631
	step [136/191], loss=33.6196
	step [137/191], loss=33.0252
	step [138/191], loss=33.0064
	step [139/191], loss=32.7148
	step [140/191], loss=34.0524
	step [141/191], loss=33.2709
	step [142/191], loss=33.5209
	step [143/191], loss=31.8175
	step [144/191], loss=35.5103
	step [145/191], loss=33.0889
	step [146/191], loss=31.9054
	step [147/191], loss=34.1816
	step [148/191], loss=32.3056
	step [149/191], loss=31.4880
	step [150/191], loss=36.7701
	step [151/191], loss=33.5094
	step [152/191], loss=32.1453
	step [153/191], loss=34.3709
	step [154/191], loss=32.3940
	step [155/191], loss=33.1563
	step [156/191], loss=33.7873
	step [157/191], loss=34.8006
	step [158/191], loss=33.3502
	step [159/191], loss=32.7822
	step [160/191], loss=31.2575
	step [161/191], loss=33.4709
	step [162/191], loss=34.6806
	step [163/191], loss=30.4267
	step [164/191], loss=31.8106
	step [165/191], loss=31.9958
	step [166/191], loss=31.1138
	step [167/191], loss=31.3916
	step [168/191], loss=31.6376
	step [169/191], loss=33.3036
	step [170/191], loss=34.0698
	step [171/191], loss=34.7227
	step [172/191], loss=32.6623
	step [173/191], loss=31.3561
	step [174/191], loss=33.0671
	step [175/191], loss=31.5804
	step [176/191], loss=32.9307
	step [177/191], loss=30.9727
	step [178/191], loss=33.4065
	step [179/191], loss=30.4811
	step [180/191], loss=34.1792
	step [181/191], loss=32.8639
	step [182/191], loss=31.9506
	step [183/191], loss=31.8176
	step [184/191], loss=30.2644
	step [185/191], loss=33.0670
	step [186/191], loss=32.3312
	step [187/191], loss=31.4070
	step [188/191], loss=32.3110
	step [189/191], loss=32.0752
	step [190/191], loss=30.7736
	step [191/191], loss=13.9983
	Evaluating
	loss=0.1379, precision=0.1841, recall=0.9858, f1=0.6868
Training epoch 6
	step [1/191], loss=32.0014
	step [2/191], loss=31.7196
	step [3/191], loss=31.1513
	step [4/191], loss=31.9959
	step [5/191], loss=31.1564
	step [6/191], loss=31.5413
	step [7/191], loss=31.1928
	step [8/191], loss=32.1457
	step [9/191], loss=32.0096
	step [10/191], loss=30.6663
	step [11/191], loss=30.3459
	step [12/191], loss=30.5659
	step [13/191], loss=34.6433
	step [14/191], loss=31.3652
	step [15/191], loss=29.5113
	step [16/191], loss=29.8676
	step [17/191], loss=28.7901
	step [18/191], loss=30.5134
	step [19/191], loss=29.8531
	step [20/191], loss=30.7198
	step [21/191], loss=29.9132
	step [22/191], loss=32.5694
	step [23/191], loss=30.9846
	step [24/191], loss=30.9324
	step [25/191], loss=31.9794
	step [26/191], loss=30.8039
	step [27/191], loss=31.1138
	step [28/191], loss=29.1498
	step [29/191], loss=30.2252
	step [30/191], loss=31.2599
	step [31/191], loss=31.6425
	step [32/191], loss=30.6107
	step [33/191], loss=29.8491
	step [34/191], loss=29.6255
	step [35/191], loss=31.1774
	step [36/191], loss=31.2086
	step [37/191], loss=30.7342
	step [38/191], loss=30.3239
	step [39/191], loss=33.6163
	step [40/191], loss=31.7113
	step [41/191], loss=30.1529
	step [42/191], loss=28.8914
	step [43/191], loss=29.1006
	step [44/191], loss=31.0093
	step [45/191], loss=30.1791
	step [46/191], loss=31.8733
	step [47/191], loss=32.2562
	step [48/191], loss=29.5426
	step [49/191], loss=29.7690
	step [50/191], loss=29.3017
	step [51/191], loss=28.3416
	step [52/191], loss=31.4089
	step [53/191], loss=28.1839
	step [54/191], loss=30.7135
	step [55/191], loss=29.9173
	step [56/191], loss=30.7799
	step [57/191], loss=32.4476
	step [58/191], loss=31.1274
	step [59/191], loss=30.5638
	step [60/191], loss=28.8606
	step [61/191], loss=28.1486
	step [62/191], loss=29.9376
	step [63/191], loss=31.0124
	step [64/191], loss=30.5724
	step [65/191], loss=27.9889
	step [66/191], loss=29.8762
	step [67/191], loss=29.6800
	step [68/191], loss=28.9223
	step [69/191], loss=30.7545
	step [70/191], loss=30.0048
	step [71/191], loss=30.6406
	step [72/191], loss=32.2338
	step [73/191], loss=27.6159
	step [74/191], loss=30.7839
	step [75/191], loss=29.0741
	step [76/191], loss=30.1406
	step [77/191], loss=29.7720
	step [78/191], loss=30.2579
	step [79/191], loss=28.0092
	step [80/191], loss=29.9009
	step [81/191], loss=29.2541
	step [82/191], loss=30.6241
	step [83/191], loss=28.8513
	step [84/191], loss=28.2170
	step [85/191], loss=31.7187
	step [86/191], loss=28.2358
	step [87/191], loss=28.2273
	step [88/191], loss=29.1472
	step [89/191], loss=30.4248
	step [90/191], loss=28.8054
	step [91/191], loss=27.8970
	step [92/191], loss=29.2253
	step [93/191], loss=30.2414
	step [94/191], loss=29.3577
	step [95/191], loss=28.8887
	step [96/191], loss=28.9044
	step [97/191], loss=29.1434
	step [98/191], loss=28.0598
	step [99/191], loss=29.7495
	step [100/191], loss=30.6864
	step [101/191], loss=26.8211
	step [102/191], loss=28.3302
	step [103/191], loss=28.8757
	step [104/191], loss=30.0970
	step [105/191], loss=29.1974
	step [106/191], loss=28.9056
	step [107/191], loss=29.2538
	step [108/191], loss=29.8922
	step [109/191], loss=28.2972
	step [110/191], loss=27.0411
	step [111/191], loss=29.4982
	step [112/191], loss=28.4499
	step [113/191], loss=29.7083
	step [114/191], loss=27.9525
	step [115/191], loss=28.8890
	step [116/191], loss=28.9701
	step [117/191], loss=29.2217
	step [118/191], loss=27.4221
	step [119/191], loss=30.0059
	step [120/191], loss=30.6925
	step [121/191], loss=29.6870
	step [122/191], loss=28.6107
	step [123/191], loss=31.8024
	step [124/191], loss=27.9527
	step [125/191], loss=29.3489
	step [126/191], loss=26.0638
	step [127/191], loss=29.8236
	step [128/191], loss=29.6009
	step [129/191], loss=29.5842
	step [130/191], loss=29.7631
	step [131/191], loss=27.8117
	step [132/191], loss=27.0471
	step [133/191], loss=29.2050
	step [134/191], loss=27.3933
	step [135/191], loss=28.6208
	step [136/191], loss=27.0226
	step [137/191], loss=28.6215
	step [138/191], loss=27.8126
	step [139/191], loss=26.7488
	step [140/191], loss=26.6041
	step [141/191], loss=27.3896
	step [142/191], loss=28.0325
	step [143/191], loss=27.1060
	step [144/191], loss=28.5938
	step [145/191], loss=28.0574
	step [146/191], loss=26.9940
	step [147/191], loss=27.9633
	step [148/191], loss=28.2257
	step [149/191], loss=30.3520
	step [150/191], loss=27.4773
	step [151/191], loss=28.3895
	step [152/191], loss=28.8282
	step [153/191], loss=26.2178
	step [154/191], loss=28.3305
	step [155/191], loss=28.2604
	step [156/191], loss=28.2476
	step [157/191], loss=28.8230
	step [158/191], loss=26.0651
	step [159/191], loss=28.5409
	step [160/191], loss=30.1702
	step [161/191], loss=28.2681
	step [162/191], loss=28.6668
	step [163/191], loss=28.9807
	step [164/191], loss=25.9800
	step [165/191], loss=27.3317
	step [166/191], loss=28.6880
	step [167/191], loss=28.1153
	step [168/191], loss=29.5309
	step [169/191], loss=26.8376
	step [170/191], loss=26.2878
	step [171/191], loss=28.6157
	step [172/191], loss=27.2728
	step [173/191], loss=28.0764
	step [174/191], loss=27.2131
	step [175/191], loss=28.9991
	step [176/191], loss=27.4230
	step [177/191], loss=29.0755
	step [178/191], loss=27.6937
	step [179/191], loss=26.3152
	step [180/191], loss=28.1356
	step [181/191], loss=26.7190
	step [182/191], loss=27.3966
	step [183/191], loss=30.1628
	step [184/191], loss=27.8223
	step [185/191], loss=26.3137
	step [186/191], loss=28.9486
	step [187/191], loss=26.7025
	step [188/191], loss=27.7894
	step [189/191], loss=27.0588
	step [190/191], loss=27.4522
	step [191/191], loss=12.2896
	Evaluating
	loss=0.1175, precision=0.1919, recall=0.9862, f1=0.6975
Training epoch 7
	step [1/191], loss=27.1902
	step [2/191], loss=27.7680
	step [3/191], loss=25.8986
	step [4/191], loss=25.4225
	step [5/191], loss=26.4777
	step [6/191], loss=28.4038
	step [7/191], loss=26.4622
	step [8/191], loss=26.8454
	step [9/191], loss=29.0197
	step [10/191], loss=27.1493
	step [11/191], loss=27.2263
	step [12/191], loss=26.9357
	step [13/191], loss=25.8297
	step [14/191], loss=28.1639
	step [15/191], loss=25.8041
	step [16/191], loss=28.9632
	step [17/191], loss=28.1471
	step [18/191], loss=25.9681
	step [19/191], loss=26.4798
	step [20/191], loss=25.3166
	step [21/191], loss=26.9682
	step [22/191], loss=27.8352
	step [23/191], loss=25.8626
	step [24/191], loss=26.1651
	step [25/191], loss=27.5132
	step [26/191], loss=26.9799
	step [27/191], loss=24.5974
	step [28/191], loss=26.3776
	step [29/191], loss=29.5665
	step [30/191], loss=28.4896
	step [31/191], loss=25.9713
	step [32/191], loss=25.8862
	step [33/191], loss=28.2218
	step [34/191], loss=27.1483
	step [35/191], loss=26.5942
	step [36/191], loss=27.1037
	step [37/191], loss=26.6108
	step [38/191], loss=27.8362
	step [39/191], loss=27.0754
	step [40/191], loss=26.5521
	step [41/191], loss=27.2376
	step [42/191], loss=26.1158
	step [43/191], loss=26.0013
	step [44/191], loss=25.6742
	step [45/191], loss=25.8201
	step [46/191], loss=25.5355
	step [47/191], loss=26.2014
	step [48/191], loss=24.9792
	step [49/191], loss=25.9025
	step [50/191], loss=25.8779
	step [51/191], loss=27.0103
	step [52/191], loss=26.8854
	step [53/191], loss=26.0512
	step [54/191], loss=25.2501
	step [55/191], loss=24.9955
	step [56/191], loss=24.7075
	step [57/191], loss=28.7306
	step [58/191], loss=24.7237
	step [59/191], loss=24.1108
	step [60/191], loss=24.4215
	step [61/191], loss=23.7774
	step [62/191], loss=25.9614
	step [63/191], loss=25.2347
	step [64/191], loss=28.2792
	step [65/191], loss=27.5371
	step [66/191], loss=25.4408
	step [67/191], loss=26.0807
	step [68/191], loss=26.3774
	step [69/191], loss=24.1354
	step [70/191], loss=26.7955
	step [71/191], loss=25.3126
	step [72/191], loss=27.3221
	step [73/191], loss=27.9645
	step [74/191], loss=26.9820
	step [75/191], loss=25.9733
	step [76/191], loss=25.8502
	step [77/191], loss=25.6080
	step [78/191], loss=27.1494
	step [79/191], loss=27.0425
	step [80/191], loss=27.2431
	step [81/191], loss=23.7842
	step [82/191], loss=25.9015
	step [83/191], loss=25.2123
	step [84/191], loss=25.2758
	step [85/191], loss=24.3054
	step [86/191], loss=27.2303
	step [87/191], loss=25.9126
	step [88/191], loss=23.2175
	step [89/191], loss=23.6314
	step [90/191], loss=25.5046
	step [91/191], loss=24.4240
	step [92/191], loss=26.6548
	step [93/191], loss=25.0602
	step [94/191], loss=27.4512
	step [95/191], loss=24.2500
	step [96/191], loss=27.2650
	step [97/191], loss=24.7864
	step [98/191], loss=25.7066
	step [99/191], loss=24.8992
	step [100/191], loss=25.0182
	step [101/191], loss=24.7434
	step [102/191], loss=24.0394
	step [103/191], loss=26.6012
	step [104/191], loss=25.3127
	step [105/191], loss=25.6621
	step [106/191], loss=25.5187
	step [107/191], loss=23.6343
	step [108/191], loss=25.3191
	step [109/191], loss=25.1250
	step [110/191], loss=24.7189
	step [111/191], loss=25.8503
	step [112/191], loss=25.5975
	step [113/191], loss=24.9118
	step [114/191], loss=24.1961
	step [115/191], loss=24.7877
	step [116/191], loss=25.4434
	step [117/191], loss=23.8128
	step [118/191], loss=24.4816
	step [119/191], loss=24.2615
	step [120/191], loss=23.6614
	step [121/191], loss=24.2506
	step [122/191], loss=24.3034
	step [123/191], loss=24.3686
	step [124/191], loss=22.9576
	step [125/191], loss=25.0820
	step [126/191], loss=23.9290
	step [127/191], loss=25.8030
	step [128/191], loss=25.2667
	step [129/191], loss=24.9854
	step [130/191], loss=23.7195
	step [131/191], loss=23.6515
	step [132/191], loss=23.4602
	step [133/191], loss=25.9113
	step [134/191], loss=24.1045
	step [135/191], loss=26.3161
	step [136/191], loss=25.2660
	step [137/191], loss=24.7460
	step [138/191], loss=23.2741
	step [139/191], loss=25.6074
	step [140/191], loss=24.8354
	step [141/191], loss=24.3913
	step [142/191], loss=25.0700
	step [143/191], loss=25.6007
	step [144/191], loss=25.3471
	step [145/191], loss=23.6217
	step [146/191], loss=25.4486
	step [147/191], loss=24.1700
	step [148/191], loss=24.8359
	step [149/191], loss=28.2107
	step [150/191], loss=25.9147
	step [151/191], loss=25.1938
	step [152/191], loss=23.1589
	step [153/191], loss=24.0694
	step [154/191], loss=24.2094
	step [155/191], loss=24.0803
	step [156/191], loss=22.3773
	step [157/191], loss=23.7061
	step [158/191], loss=23.3529
	step [159/191], loss=25.5393
	step [160/191], loss=25.7297
	step [161/191], loss=25.7332
	step [162/191], loss=24.0803
	step [163/191], loss=23.7129
	step [164/191], loss=26.6753
	step [165/191], loss=24.2226
	step [166/191], loss=23.2212
	step [167/191], loss=24.3108
	step [168/191], loss=24.6019
	step [169/191], loss=24.9582
	step [170/191], loss=24.3086
	step [171/191], loss=23.5454
	step [172/191], loss=21.9056
	step [173/191], loss=25.5504
	step [174/191], loss=23.7507
	step [175/191], loss=22.7329
	step [176/191], loss=24.1542
	step [177/191], loss=25.9105
	step [178/191], loss=22.9829
	step [179/191], loss=23.3808
	step [180/191], loss=24.5961
	step [181/191], loss=24.8549
	step [182/191], loss=24.5839
	step [183/191], loss=22.8855
	step [184/191], loss=24.2215
	step [185/191], loss=23.4297
	step [186/191], loss=24.5469
	step [187/191], loss=23.1320
	step [188/191], loss=23.4547
	step [189/191], loss=23.8517
	step [190/191], loss=22.7226
	step [191/191], loss=10.4245
	Evaluating
	loss=0.0972, precision=0.1987, recall=0.9855, f1=0.7060
Training epoch 8
	step [1/191], loss=23.4547
	step [2/191], loss=23.6568
	step [3/191], loss=22.4537
	step [4/191], loss=23.2589
	step [5/191], loss=22.8983
	step [6/191], loss=21.1873
	step [7/191], loss=24.3632
	step [8/191], loss=23.9886
	step [9/191], loss=26.4877
	step [10/191], loss=24.3440
	step [11/191], loss=23.8100
	step [12/191], loss=23.3857
	step [13/191], loss=22.7945
	step [14/191], loss=23.2523
	step [15/191], loss=25.2795
	step [16/191], loss=23.8416
	step [17/191], loss=23.6929
	step [18/191], loss=22.9145
	step [19/191], loss=25.7067
	step [20/191], loss=24.0724
	step [21/191], loss=23.7469
	step [22/191], loss=23.3117
	step [23/191], loss=22.1162
	step [24/191], loss=23.9722
	step [25/191], loss=22.5570
	step [26/191], loss=24.2260
	step [27/191], loss=23.3747
	step [28/191], loss=24.4096
	step [29/191], loss=22.8106
	step [30/191], loss=24.5840
	step [31/191], loss=21.9270
	step [32/191], loss=24.9274
	step [33/191], loss=22.6056
	step [34/191], loss=26.1433
	step [35/191], loss=24.9601
	step [36/191], loss=22.3480
	step [37/191], loss=23.9376
	step [38/191], loss=23.6948
	step [39/191], loss=24.7670
	step [40/191], loss=23.6238
	step [41/191], loss=22.8637
	step [42/191], loss=24.5455
	step [43/191], loss=22.2761
	step [44/191], loss=21.7565
	step [45/191], loss=25.8440
	step [46/191], loss=24.5621
	step [47/191], loss=21.8916
	step [48/191], loss=26.0700
	step [49/191], loss=24.1698
	step [50/191], loss=23.4357
	step [51/191], loss=23.1137
	step [52/191], loss=22.6904
	step [53/191], loss=24.2204
	step [54/191], loss=25.9406
	step [55/191], loss=23.0672
	step [56/191], loss=23.2742
	step [57/191], loss=22.7102
	step [58/191], loss=23.9433
	step [59/191], loss=22.2329
	step [60/191], loss=23.9133
	step [61/191], loss=23.9750
	step [62/191], loss=24.3693
	step [63/191], loss=22.1015
	step [64/191], loss=23.7071
	step [65/191], loss=22.1653
	step [66/191], loss=25.2432
	step [67/191], loss=23.1330
	step [68/191], loss=21.1997
	step [69/191], loss=21.3184
	step [70/191], loss=22.0860
	step [71/191], loss=23.4756
	step [72/191], loss=20.7164
	step [73/191], loss=23.7941
	step [74/191], loss=24.5853
	step [75/191], loss=21.0249
	step [76/191], loss=22.2780
	step [77/191], loss=22.2150
	step [78/191], loss=22.3408
	step [79/191], loss=21.1698
	step [80/191], loss=20.7920
	step [81/191], loss=20.2874
	step [82/191], loss=21.7018
	step [83/191], loss=22.9150
	step [84/191], loss=21.6380
	step [85/191], loss=22.1751
	step [86/191], loss=21.2646
	step [87/191], loss=22.0439
	step [88/191], loss=24.1479
	step [89/191], loss=22.6910
	step [90/191], loss=19.8236
	step [91/191], loss=23.0306
	step [92/191], loss=20.2141
	step [93/191], loss=22.3983
	step [94/191], loss=22.2294
	step [95/191], loss=19.5292
	step [96/191], loss=23.1514
	step [97/191], loss=23.8231
	step [98/191], loss=22.5248
	step [99/191], loss=21.5321
	step [100/191], loss=22.0028
	step [101/191], loss=22.0581
	step [102/191], loss=22.1046
	step [103/191], loss=22.8115
	step [104/191], loss=22.1506
	step [105/191], loss=21.2276
	step [106/191], loss=21.3791
	step [107/191], loss=21.8222
	step [108/191], loss=21.4508
	step [109/191], loss=19.8985
	step [110/191], loss=22.1182
	step [111/191], loss=23.3118
	step [112/191], loss=20.5346
	step [113/191], loss=21.6369
	step [114/191], loss=23.4814
	step [115/191], loss=20.5490
	step [116/191], loss=21.2232
	step [117/191], loss=20.3444
	step [118/191], loss=22.1359
	step [119/191], loss=22.2142
	step [120/191], loss=20.4838
	step [121/191], loss=20.6964
	step [122/191], loss=22.9044
	step [123/191], loss=20.9436
	step [124/191], loss=21.5966
	step [125/191], loss=20.3610
	step [126/191], loss=21.4302
	step [127/191], loss=20.6887
	step [128/191], loss=22.9002
	step [129/191], loss=21.8826
	step [130/191], loss=21.6299
	step [131/191], loss=21.0582
	step [132/191], loss=20.2178
	step [133/191], loss=20.1429
	step [134/191], loss=22.2828
	step [135/191], loss=21.1003
	step [136/191], loss=25.3909
	step [137/191], loss=24.1831
	step [138/191], loss=20.6487
	step [139/191], loss=19.7443
	step [140/191], loss=19.6465
	step [141/191], loss=21.0638
	step [142/191], loss=21.3808
	step [143/191], loss=23.1383
	step [144/191], loss=23.9346
	step [145/191], loss=22.1415
	step [146/191], loss=21.7776
	step [147/191], loss=21.8110
	step [148/191], loss=22.7900
	step [149/191], loss=20.8384
	step [150/191], loss=23.8308
	step [151/191], loss=21.9856
	step [152/191], loss=20.9149
	step [153/191], loss=21.6864
	step [154/191], loss=22.3018
	step [155/191], loss=20.8998
	step [156/191], loss=22.1549
	step [157/191], loss=21.3567
	step [158/191], loss=23.7433
	step [159/191], loss=20.1978
	step [160/191], loss=21.9055
	step [161/191], loss=19.9444
	step [162/191], loss=20.0499
	step [163/191], loss=21.4980
	step [164/191], loss=22.0441
	step [165/191], loss=23.1238
	step [166/191], loss=20.7720
	step [167/191], loss=21.8246
	step [168/191], loss=21.4453
	step [169/191], loss=21.6099
	step [170/191], loss=20.7508
	step [171/191], loss=21.6966
	step [172/191], loss=23.6345
	step [173/191], loss=20.4197
	step [174/191], loss=20.9298
	step [175/191], loss=24.2783
	step [176/191], loss=21.1389
	step [177/191], loss=20.8172
	step [178/191], loss=22.2276
	step [179/191], loss=22.4981
	step [180/191], loss=20.2041
	step [181/191], loss=21.3700
	step [182/191], loss=21.9671
	step [183/191], loss=21.6003
	step [184/191], loss=20.7433
	step [185/191], loss=21.4552
	step [186/191], loss=19.3466
	step [187/191], loss=21.7282
	step [188/191], loss=22.3146
	step [189/191], loss=20.0681
	step [190/191], loss=23.3198
	step [191/191], loss=9.6532
	Evaluating
	loss=0.0941, precision=0.1472, recall=0.9879, f1=0.6288
Training epoch 9
	step [1/191], loss=20.2376
	step [2/191], loss=22.1430
	step [3/191], loss=21.1606
	step [4/191], loss=19.8936
	step [5/191], loss=22.5188
	step [6/191], loss=19.5462
	step [7/191], loss=20.8352
	step [8/191], loss=21.4823
	step [9/191], loss=18.5846
	step [10/191], loss=20.0212
	step [11/191], loss=18.9086
	step [12/191], loss=21.1785
	step [13/191], loss=20.0117
	step [14/191], loss=21.1885
	step [15/191], loss=20.5082
	step [16/191], loss=18.7504
	step [17/191], loss=21.0867
	step [18/191], loss=25.1735
	step [19/191], loss=20.6504
	step [20/191], loss=20.7846
	step [21/191], loss=21.1180
	step [22/191], loss=19.3205
	step [23/191], loss=19.8957
	step [24/191], loss=20.5813
	step [25/191], loss=19.9756
	step [26/191], loss=19.8914
	step [27/191], loss=21.7720
	step [28/191], loss=20.1004
	step [29/191], loss=20.7401
	step [30/191], loss=20.1893
	step [31/191], loss=22.7507
	step [32/191], loss=20.2727
	step [33/191], loss=19.7151
	step [34/191], loss=20.8719
	step [35/191], loss=20.9597
	step [36/191], loss=22.3256
	step [37/191], loss=20.1381
	step [38/191], loss=20.9667
	step [39/191], loss=21.7284
	step [40/191], loss=20.5705
	step [41/191], loss=19.4155
	step [42/191], loss=20.3620
	step [43/191], loss=23.1307
	step [44/191], loss=21.2863
	step [45/191], loss=20.9380
	step [46/191], loss=20.3458
	step [47/191], loss=20.3991
	step [48/191], loss=20.0195
	step [49/191], loss=21.5152
	step [50/191], loss=22.0788
	step [51/191], loss=22.5152
	step [52/191], loss=20.6201
	step [53/191], loss=21.5360
	step [54/191], loss=20.8958
	step [55/191], loss=19.4574
	step [56/191], loss=23.0864
	step [57/191], loss=18.9708
	step [58/191], loss=20.0656
	step [59/191], loss=19.0988
	step [60/191], loss=19.5157
	step [61/191], loss=20.1818
	step [62/191], loss=21.0708
	step [63/191], loss=20.1287
	step [64/191], loss=18.9634
	step [65/191], loss=20.0364
	step [66/191], loss=20.0484
	step [67/191], loss=20.2595
	step [68/191], loss=21.6595
	step [69/191], loss=19.9927
	step [70/191], loss=21.0203
	step [71/191], loss=21.7619
	step [72/191], loss=20.3721
	step [73/191], loss=19.6765
	step [74/191], loss=19.6313
	step [75/191], loss=20.8410
	step [76/191], loss=22.0240
	step [77/191], loss=19.4599
	step [78/191], loss=21.1717
	step [79/191], loss=19.3685
	step [80/191], loss=20.5660
	step [81/191], loss=19.9484
	step [82/191], loss=19.9811
	step [83/191], loss=18.6400
	step [84/191], loss=21.0384
	step [85/191], loss=19.6621
	step [86/191], loss=20.5700
	step [87/191], loss=19.2981
	step [88/191], loss=19.9034
	step [89/191], loss=21.6418
	step [90/191], loss=21.5899
	step [91/191], loss=20.6713
	step [92/191], loss=20.5163
	step [93/191], loss=20.4610
	step [94/191], loss=19.2837
	step [95/191], loss=21.3355
	step [96/191], loss=19.7667
	step [97/191], loss=18.8237
	step [98/191], loss=20.2297
	step [99/191], loss=20.5190
	step [100/191], loss=21.6055
	step [101/191], loss=19.3238
	step [102/191], loss=20.9120
	step [103/191], loss=21.9538
	step [104/191], loss=19.8084
	step [105/191], loss=19.4146
	step [106/191], loss=19.4594
	step [107/191], loss=20.0505
	step [108/191], loss=18.7020
	step [109/191], loss=19.1667
	step [110/191], loss=19.4300
	step [111/191], loss=19.5991
	step [112/191], loss=20.7607
	step [113/191], loss=18.7009
	step [114/191], loss=19.0278
	step [115/191], loss=18.5844
	step [116/191], loss=18.5114
	step [117/191], loss=19.5061
	step [118/191], loss=20.6645
	step [119/191], loss=18.1949
	step [120/191], loss=18.2725
	step [121/191], loss=18.3775
	step [122/191], loss=18.6029
	step [123/191], loss=19.2828
	step [124/191], loss=19.2463
	step [125/191], loss=20.1403
	step [126/191], loss=21.3505
	step [127/191], loss=19.7555
	step [128/191], loss=19.6044
	step [129/191], loss=17.8157
	step [130/191], loss=20.1218
	step [131/191], loss=18.0742
	step [132/191], loss=17.6973
	step [133/191], loss=20.0588
	step [134/191], loss=18.0198
	step [135/191], loss=20.7882
	step [136/191], loss=18.8744
	step [137/191], loss=19.1224
	step [138/191], loss=19.3895
	step [139/191], loss=19.7136
	step [140/191], loss=18.8445
	step [141/191], loss=20.8553
	step [142/191], loss=20.1873
	step [143/191], loss=19.1453
	step [144/191], loss=18.1753
	step [145/191], loss=18.8348
	step [146/191], loss=18.8447
	step [147/191], loss=17.9330
	step [148/191], loss=19.1237
	step [149/191], loss=19.1866
	step [150/191], loss=18.9087
	step [151/191], loss=20.7044
	step [152/191], loss=17.5188
	step [153/191], loss=19.0222
	step [154/191], loss=17.2855
	step [155/191], loss=19.0667
	step [156/191], loss=17.9919
	step [157/191], loss=17.4834
	step [158/191], loss=20.6926
	step [159/191], loss=17.6430
	step [160/191], loss=18.9350
	step [161/191], loss=19.4282
	step [162/191], loss=18.9677
	step [163/191], loss=16.6265
	step [164/191], loss=18.3050
	step [165/191], loss=18.1474
	step [166/191], loss=16.8919
	step [167/191], loss=20.5831
	step [168/191], loss=18.9281
	step [169/191], loss=18.3412
	step [170/191], loss=19.2555
	step [171/191], loss=19.7797
	step [172/191], loss=17.5124
	step [173/191], loss=18.9915
	step [174/191], loss=18.2861
	step [175/191], loss=21.0555
	step [176/191], loss=17.9810
	step [177/191], loss=19.2029
	step [178/191], loss=19.3311
	step [179/191], loss=18.8791
	step [180/191], loss=19.6647
	step [181/191], loss=18.2348
	step [182/191], loss=19.6073
	step [183/191], loss=22.3317
	step [184/191], loss=18.9889
	step [185/191], loss=18.3714
	step [186/191], loss=18.7440
	step [187/191], loss=16.8386
	step [188/191], loss=19.8551
	step [189/191], loss=17.3691
	step [190/191], loss=18.3144
	step [191/191], loss=8.4609
	Evaluating
	loss=0.0702, precision=0.2466, recall=0.9828, f1=0.7568
saving model as: 1_saved_model.pth
Training epoch 10
	step [1/191], loss=18.3792
	step [2/191], loss=19.6900
	step [3/191], loss=18.4460
	step [4/191], loss=18.7056
	step [5/191], loss=18.1038
	step [6/191], loss=19.2461
	step [7/191], loss=19.3956
	step [8/191], loss=19.3014
	step [9/191], loss=18.6801
	step [10/191], loss=18.7499
	step [11/191], loss=18.1544
	step [12/191], loss=20.3080
	step [13/191], loss=19.3639
	step [14/191], loss=16.8361
	step [15/191], loss=19.4736
	step [16/191], loss=18.7400
	step [17/191], loss=20.1127
	step [18/191], loss=18.9836
	step [19/191], loss=17.8532
	step [20/191], loss=18.9362
	step [21/191], loss=18.3809
	step [22/191], loss=19.3466
	step [23/191], loss=17.1461
	step [24/191], loss=17.2326
	step [25/191], loss=17.8917
	step [26/191], loss=21.5495
	step [27/191], loss=20.0662
	step [28/191], loss=20.6048
	step [29/191], loss=18.5360
	step [30/191], loss=19.1247
	step [31/191], loss=17.1428
	step [32/191], loss=20.0418
	step [33/191], loss=18.2271
	step [34/191], loss=18.2393
	step [35/191], loss=16.9162
	step [36/191], loss=18.4055
	step [37/191], loss=18.4686
	step [38/191], loss=18.4952
	step [39/191], loss=18.9429
	step [40/191], loss=18.0831
	step [41/191], loss=18.8745
	step [42/191], loss=17.6583
	step [43/191], loss=17.8688
	step [44/191], loss=18.5057
	step [45/191], loss=18.6407
	step [46/191], loss=18.5275
	step [47/191], loss=18.8139
	step [48/191], loss=17.9639
	step [49/191], loss=16.1368
	step [50/191], loss=18.2660
	step [51/191], loss=18.0427
	step [52/191], loss=18.7259
	step [53/191], loss=17.7587
	step [54/191], loss=17.8096
	step [55/191], loss=16.3770
	step [56/191], loss=19.7759
	step [57/191], loss=18.3928
	step [58/191], loss=17.2292
	step [59/191], loss=17.9182
	step [60/191], loss=18.9667
	step [61/191], loss=16.2178
	step [62/191], loss=16.5530
	step [63/191], loss=17.5138
	step [64/191], loss=17.2114
	step [65/191], loss=19.5293
	step [66/191], loss=18.2617
	step [67/191], loss=18.8020
	step [68/191], loss=17.5266
	step [69/191], loss=18.2701
	step [70/191], loss=19.8549
	step [71/191], loss=19.5617
	step [72/191], loss=17.7858
	step [73/191], loss=16.5281
	step [74/191], loss=17.5143
	step [75/191], loss=18.6070
	step [76/191], loss=18.4888
	step [77/191], loss=18.1655
	step [78/191], loss=19.1747
	step [79/191], loss=18.9815
	step [80/191], loss=18.1459
	step [81/191], loss=19.4635
	step [82/191], loss=17.6387
	step [83/191], loss=18.3905
	step [84/191], loss=18.6793
	step [85/191], loss=17.6821
	step [86/191], loss=19.1191
	step [87/191], loss=20.1407
	step [88/191], loss=18.8180
	step [89/191], loss=18.2356
	step [90/191], loss=17.5955
	step [91/191], loss=18.9477
	step [92/191], loss=18.7010
	step [93/191], loss=18.7794
	step [94/191], loss=15.9339
	step [95/191], loss=19.1490
	step [96/191], loss=19.1729
	step [97/191], loss=17.2363
	step [98/191], loss=16.2858
	step [99/191], loss=19.3985
	step [100/191], loss=16.5110
	step [101/191], loss=19.4672
	step [102/191], loss=19.1973
	step [103/191], loss=15.6753
	step [104/191], loss=20.0501
	step [105/191], loss=18.0010
	step [106/191], loss=17.8903
	step [107/191], loss=18.0806
	step [108/191], loss=18.9867
	step [109/191], loss=18.6592
	step [110/191], loss=17.9185
	step [111/191], loss=16.7356
	step [112/191], loss=17.5401
	step [113/191], loss=17.0526
	step [114/191], loss=15.8446
	step [115/191], loss=17.7703
	step [116/191], loss=17.5993
	step [117/191], loss=18.4668
	step [118/191], loss=16.2268
	step [119/191], loss=17.7936
	step [120/191], loss=18.0302
	step [121/191], loss=19.9583
	step [122/191], loss=16.6178
	step [123/191], loss=17.6239
	step [124/191], loss=16.9820
	step [125/191], loss=16.8582
	step [126/191], loss=16.3249
	step [127/191], loss=18.6043
	step [128/191], loss=17.6129
	step [129/191], loss=17.7899
	step [130/191], loss=17.4736
	step [131/191], loss=17.5928
	step [132/191], loss=17.0609
	step [133/191], loss=18.9299
	step [134/191], loss=16.6176
	step [135/191], loss=19.7587
	step [136/191], loss=18.0328
	step [137/191], loss=17.9562
	step [138/191], loss=16.1027
	step [139/191], loss=21.0968
	step [140/191], loss=16.4725
	step [141/191], loss=15.5582
	step [142/191], loss=19.2681
	step [143/191], loss=18.4464
	step [144/191], loss=17.7886
	step [145/191], loss=19.8344
	step [146/191], loss=15.5327
	step [147/191], loss=17.1080
	step [148/191], loss=17.1064
	step [149/191], loss=16.0677
	step [150/191], loss=17.6923
	step [151/191], loss=17.1697
	step [152/191], loss=18.2974
	step [153/191], loss=19.8962
	step [154/191], loss=19.9723
	step [155/191], loss=17.6560
	step [156/191], loss=17.0322
	step [157/191], loss=19.0238
	step [158/191], loss=18.1165
	step [159/191], loss=16.6935
	step [160/191], loss=16.2020
	step [161/191], loss=15.4329
	step [162/191], loss=17.4026
	step [163/191], loss=15.8097
	step [164/191], loss=16.8009
	step [165/191], loss=18.4031
	step [166/191], loss=16.6413
	step [167/191], loss=17.4196
	step [168/191], loss=16.6418
	step [169/191], loss=16.7388
	step [170/191], loss=16.4153
	step [171/191], loss=17.3959
	step [172/191], loss=15.2605
	step [173/191], loss=15.9482
	step [174/191], loss=16.6555
	step [175/191], loss=17.7753
	step [176/191], loss=17.5364
	step [177/191], loss=19.5436
	step [178/191], loss=17.4812
	step [179/191], loss=17.6469
	step [180/191], loss=17.8238
	step [181/191], loss=16.2781
	step [182/191], loss=18.2672
	step [183/191], loss=16.1661
	step [184/191], loss=16.0565
	step [185/191], loss=19.0888
	step [186/191], loss=17.4850
	step [187/191], loss=19.1030
	step [188/191], loss=16.7517
	step [189/191], loss=16.4362
	step [190/191], loss=18.1413
	step [191/191], loss=7.9900
	Evaluating
	loss=0.0687, precision=0.1885, recall=0.9847, f1=0.6922
Training epoch 11
	step [1/191], loss=16.2276
	step [2/191], loss=17.2980
	step [3/191], loss=16.6809
	step [4/191], loss=16.4852
	step [5/191], loss=16.2651
	step [6/191], loss=17.3155
	step [7/191], loss=17.2745
	step [8/191], loss=17.5398
	step [9/191], loss=18.0427
	step [10/191], loss=16.9372
	step [11/191], loss=16.5091
	step [12/191], loss=16.8417
	step [13/191], loss=16.9887
	step [14/191], loss=17.3342
	step [15/191], loss=16.3044
	step [16/191], loss=18.1997
	step [17/191], loss=15.8054
	step [18/191], loss=15.6958
	step [19/191], loss=15.7349
	step [20/191], loss=18.2337
	step [21/191], loss=16.7853
	step [22/191], loss=16.6099
	step [23/191], loss=18.0375
	step [24/191], loss=17.3359
	step [25/191], loss=17.3740
	step [26/191], loss=15.4727
	step [27/191], loss=15.9915
	step [28/191], loss=17.3077
	step [29/191], loss=16.8196
	step [30/191], loss=15.8704
	step [31/191], loss=15.2283
	step [32/191], loss=15.9874
	step [33/191], loss=16.6507
	step [34/191], loss=16.5061
	step [35/191], loss=15.9684
	step [36/191], loss=17.1441
	step [37/191], loss=17.3709
	step [38/191], loss=15.4836
	step [39/191], loss=15.4373
	step [40/191], loss=17.1102
	step [41/191], loss=15.4707
	step [42/191], loss=15.3226
	step [43/191], loss=16.9528
	step [44/191], loss=16.2906
	step [45/191], loss=16.7841
	step [46/191], loss=16.5485
	step [47/191], loss=17.1891
	step [48/191], loss=15.6119
	step [49/191], loss=15.7718
	step [50/191], loss=17.0435
	step [51/191], loss=16.7778
	step [52/191], loss=17.9674
	step [53/191], loss=15.5380
	step [54/191], loss=15.6022
	step [55/191], loss=15.9031
	step [56/191], loss=16.8896
	step [57/191], loss=17.2559
	step [58/191], loss=15.6752
	step [59/191], loss=16.9588
	step [60/191], loss=16.6288
	step [61/191], loss=16.2660
	step [62/191], loss=17.2743
	step [63/191], loss=17.2360
	step [64/191], loss=18.0582
	step [65/191], loss=16.0145
	step [66/191], loss=16.8395
	step [67/191], loss=14.8803
	step [68/191], loss=14.3043
	step [69/191], loss=17.6601
	step [70/191], loss=18.2558
	step [71/191], loss=15.0117
	step [72/191], loss=18.3479
	step [73/191], loss=17.1044
	step [74/191], loss=15.8174
	step [75/191], loss=19.0556
	step [76/191], loss=15.4212
	step [77/191], loss=16.4496
	step [78/191], loss=16.3052
	step [79/191], loss=16.3872
	step [80/191], loss=16.1493
	step [81/191], loss=17.6745
	step [82/191], loss=15.6119
	step [83/191], loss=16.9265
	step [84/191], loss=15.6325
	step [85/191], loss=18.2469
	step [86/191], loss=15.7898
	step [87/191], loss=15.9300
	step [88/191], loss=16.2501
	step [89/191], loss=18.2450
	step [90/191], loss=17.0993
	step [91/191], loss=15.6363
	step [92/191], loss=19.3202
	step [93/191], loss=15.8134
	step [94/191], loss=17.4550
	step [95/191], loss=17.7654
	step [96/191], loss=16.1139
	step [97/191], loss=15.7662
	step [98/191], loss=16.0130
	step [99/191], loss=14.6088
	step [100/191], loss=15.4248
	step [101/191], loss=14.6591
	step [102/191], loss=15.7856
	step [103/191], loss=14.9212
	step [104/191], loss=16.4842
	step [105/191], loss=17.3187
	step [106/191], loss=15.8187
	step [107/191], loss=15.2535
	step [108/191], loss=16.2677
	step [109/191], loss=18.4025
	step [110/191], loss=16.4244
	step [111/191], loss=15.5679
	step [112/191], loss=16.2507
	step [113/191], loss=16.4400
	step [114/191], loss=15.2528
	step [115/191], loss=17.2188
	step [116/191], loss=15.8021
	step [117/191], loss=19.1984
	step [118/191], loss=16.0981
	step [119/191], loss=14.7973
	step [120/191], loss=15.3550
	step [121/191], loss=14.5912
	step [122/191], loss=16.3436
	step [123/191], loss=17.2581
	step [124/191], loss=17.1086
	step [125/191], loss=14.4472
	step [126/191], loss=15.7778
	step [127/191], loss=15.8496
	step [128/191], loss=16.3526
	step [129/191], loss=14.4023
	step [130/191], loss=14.1207
	step [131/191], loss=16.5224
	step [132/191], loss=15.9544
	step [133/191], loss=15.6368
	step [134/191], loss=16.9347
	step [135/191], loss=15.6379
	step [136/191], loss=17.1284
	step [137/191], loss=15.9149
	step [138/191], loss=15.5811
	step [139/191], loss=16.3141
	step [140/191], loss=16.2031
	step [141/191], loss=17.8172
	step [142/191], loss=15.1042
	step [143/191], loss=16.8611
	step [144/191], loss=16.3189
	step [145/191], loss=14.8877
	step [146/191], loss=15.5043
	step [147/191], loss=15.1201
	step [148/191], loss=17.5752
	step [149/191], loss=15.7668
	step [150/191], loss=14.5374
	step [151/191], loss=18.4861
	step [152/191], loss=15.6323
	step [153/191], loss=15.3237
	step [154/191], loss=15.2846
	step [155/191], loss=17.9968
	step [156/191], loss=16.9394
	step [157/191], loss=16.5302
	step [158/191], loss=16.0124
	step [159/191], loss=16.4307
	step [160/191], loss=15.4092
	step [161/191], loss=14.7050
	step [162/191], loss=16.3077
	step [163/191], loss=16.6547
	step [164/191], loss=16.6019
	step [165/191], loss=15.6140
	step [166/191], loss=15.3639
	step [167/191], loss=14.3285
	step [168/191], loss=15.7578
	step [169/191], loss=15.6334
	step [170/191], loss=15.8700
	step [171/191], loss=16.9140
	step [172/191], loss=14.2095
	step [173/191], loss=15.9738
	step [174/191], loss=15.8620
	step [175/191], loss=16.0426
	step [176/191], loss=15.2609
	step [177/191], loss=15.2589
	step [178/191], loss=15.8187
	step [179/191], loss=17.3521
	step [180/191], loss=14.5269
	step [181/191], loss=15.5713
	step [182/191], loss=16.1219
	step [183/191], loss=14.9579
	step [184/191], loss=13.8575
	step [185/191], loss=15.2649
	step [186/191], loss=12.9884
	step [187/191], loss=15.4802
	step [188/191], loss=14.1480
	step [189/191], loss=16.2826
	step [190/191], loss=13.8327
	step [191/191], loss=8.4098
	Evaluating
	loss=0.0648, precision=0.1692, recall=0.9865, f1=0.6651
Training epoch 12
	step [1/191], loss=15.7912
	step [2/191], loss=16.3272
	step [3/191], loss=15.5819
	step [4/191], loss=15.0467
	step [5/191], loss=15.8000
	step [6/191], loss=14.3620
	step [7/191], loss=14.1968
	step [8/191], loss=14.0624
	step [9/191], loss=14.7744
	step [10/191], loss=15.0850
	step [11/191], loss=16.9178
	step [12/191], loss=14.4905
	step [13/191], loss=14.8561
	step [14/191], loss=16.3444
	step [15/191], loss=16.0477
	step [16/191], loss=15.5321
	step [17/191], loss=15.1561
	step [18/191], loss=16.8785
	step [19/191], loss=15.5603
	step [20/191], loss=15.4004
	step [21/191], loss=14.7718
	step [22/191], loss=16.5600
	step [23/191], loss=16.4753
	step [24/191], loss=15.3544
	step [25/191], loss=15.1238
	step [26/191], loss=17.0075
	step [27/191], loss=14.3791
	step [28/191], loss=14.9799
	step [29/191], loss=16.0149
	step [30/191], loss=14.2041
	step [31/191], loss=13.7966
	step [32/191], loss=16.2347
	step [33/191], loss=16.1551
	step [34/191], loss=15.4703
	step [35/191], loss=14.3985
	step [36/191], loss=16.1255
	step [37/191], loss=16.8459
	step [38/191], loss=14.8738
	step [39/191], loss=15.3851
	step [40/191], loss=14.1200
	step [41/191], loss=16.2649
	step [42/191], loss=16.1568
	step [43/191], loss=14.5643
	step [44/191], loss=14.3502
	step [45/191], loss=15.3604
	step [46/191], loss=14.1332
	step [47/191], loss=15.4257
	step [48/191], loss=14.4404
	step [49/191], loss=17.0203
	step [50/191], loss=13.2274
	step [51/191], loss=15.8954
	step [52/191], loss=16.7303
	step [53/191], loss=13.2220
	step [54/191], loss=13.1984
	step [55/191], loss=15.1029
	step [56/191], loss=16.7241
	step [57/191], loss=14.6219
	step [58/191], loss=14.4946
	step [59/191], loss=14.6249
	step [60/191], loss=14.9617
	step [61/191], loss=15.2469
	step [62/191], loss=15.9648
	step [63/191], loss=16.1872
	step [64/191], loss=14.1792
	step [65/191], loss=13.1929
	step [66/191], loss=15.9158
	step [67/191], loss=15.0082
	step [68/191], loss=16.2204
	step [69/191], loss=15.7285
	step [70/191], loss=13.9415
	step [71/191], loss=15.1642
	step [72/191], loss=15.6055
	step [73/191], loss=14.8571
	step [74/191], loss=14.9068
	step [75/191], loss=18.4688
	step [76/191], loss=15.4167
	step [77/191], loss=15.6926
	step [78/191], loss=15.0221
	step [79/191], loss=16.0356
	step [80/191], loss=16.1951
	step [81/191], loss=14.0807
	step [82/191], loss=13.7913
	step [83/191], loss=14.2881
	step [84/191], loss=14.1254
	step [85/191], loss=15.6867
	step [86/191], loss=13.7274
	step [87/191], loss=17.4900
	step [88/191], loss=15.8965
	step [89/191], loss=15.2889
	step [90/191], loss=15.2736
	step [91/191], loss=13.3242
	step [92/191], loss=14.7121
	step [93/191], loss=14.3953
	step [94/191], loss=13.5572
	step [95/191], loss=14.2352
	step [96/191], loss=14.6283
	step [97/191], loss=15.4071
	step [98/191], loss=13.5621
	step [99/191], loss=13.8943
	step [100/191], loss=14.1515
	step [101/191], loss=15.7355
	step [102/191], loss=12.1658
	step [103/191], loss=14.5556
	step [104/191], loss=17.2118
	step [105/191], loss=14.9123
	step [106/191], loss=15.1020
	step [107/191], loss=18.7509
	step [108/191], loss=14.6698
	step [109/191], loss=13.3123
	step [110/191], loss=15.7683
	step [111/191], loss=14.8832
	step [112/191], loss=15.6258
	step [113/191], loss=14.4229
	step [114/191], loss=13.9972
	step [115/191], loss=14.7260
	step [116/191], loss=15.4550
	step [117/191], loss=14.8180
	step [118/191], loss=15.8452
	step [119/191], loss=14.9074
	step [120/191], loss=15.4610
	step [121/191], loss=16.5876
	step [122/191], loss=13.9794
	step [123/191], loss=14.4751
	step [124/191], loss=15.1210
	step [125/191], loss=13.8722
	step [126/191], loss=14.8371
	step [127/191], loss=13.5787
	step [128/191], loss=14.9713
	step [129/191], loss=14.4693
	step [130/191], loss=13.8723
	step [131/191], loss=13.9013
	step [132/191], loss=13.4527
	step [133/191], loss=14.8375
	step [134/191], loss=15.1303
	step [135/191], loss=13.2332
	step [136/191], loss=12.4731
	step [137/191], loss=12.9274
	step [138/191], loss=15.3768
	step [139/191], loss=15.3004
	step [140/191], loss=13.6889
	step [141/191], loss=11.7193
	step [142/191], loss=14.6150
	step [143/191], loss=14.2767
	step [144/191], loss=14.7786
	step [145/191], loss=14.8763
	step [146/191], loss=16.0296
	step [147/191], loss=13.8901
	step [148/191], loss=13.5005
	step [149/191], loss=14.8926
	step [150/191], loss=13.3823
	step [151/191], loss=14.8341
	step [152/191], loss=12.7480
	step [153/191], loss=13.0720
	step [154/191], loss=13.6757
	step [155/191], loss=14.1778
	step [156/191], loss=16.0473
	step [157/191], loss=12.8346
	step [158/191], loss=14.4343
	step [159/191], loss=13.9961
	step [160/191], loss=14.3054
	step [161/191], loss=15.0906
	step [162/191], loss=14.3444
	step [163/191], loss=14.9108
	step [164/191], loss=12.8420
	step [165/191], loss=14.6496
	step [166/191], loss=15.3412
	step [167/191], loss=13.2888
	step [168/191], loss=15.4108
	step [169/191], loss=14.2690
	step [170/191], loss=14.2707
	step [171/191], loss=15.7850
	step [172/191], loss=14.3167
	step [173/191], loss=13.4475
	step [174/191], loss=16.0850
	step [175/191], loss=14.4208
	step [176/191], loss=14.4847
	step [177/191], loss=15.4195
	step [178/191], loss=16.2057
	step [179/191], loss=13.7336
	step [180/191], loss=15.6540
	step [181/191], loss=15.7173
	step [182/191], loss=13.8333
	step [183/191], loss=15.4600
	step [184/191], loss=14.4387
	step [185/191], loss=16.5408
	step [186/191], loss=13.9195
	step [187/191], loss=14.8105
	step [188/191], loss=14.7384
	step [189/191], loss=14.3801
	step [190/191], loss=16.0952
	step [191/191], loss=6.1581
	Evaluating
	loss=0.0515, precision=0.2175, recall=0.9849, f1=0.7280
Training epoch 13
	step [1/191], loss=13.2694
	step [2/191], loss=11.6362
	step [3/191], loss=12.9751
	step [4/191], loss=14.4933
	step [5/191], loss=13.6917
	step [6/191], loss=13.4625
	step [7/191], loss=13.7017
	step [8/191], loss=14.2402
	step [9/191], loss=14.3028
	step [10/191], loss=17.4696
	step [11/191], loss=12.4226
	step [12/191], loss=14.1700
	step [13/191], loss=14.9151
	step [14/191], loss=14.0686
	step [15/191], loss=15.6408
	step [16/191], loss=13.9184
	step [17/191], loss=14.4678
	step [18/191], loss=13.5426
	step [19/191], loss=13.7543
	step [20/191], loss=13.7331
	step [21/191], loss=14.4231
	step [22/191], loss=13.9622
	step [23/191], loss=12.8371
	step [24/191], loss=13.9404
	step [25/191], loss=14.1896
	step [26/191], loss=14.5619
	step [27/191], loss=13.5500
	step [28/191], loss=14.4928
	step [29/191], loss=13.5413
	step [30/191], loss=13.3868
	step [31/191], loss=13.4607
	step [32/191], loss=14.1378
	step [33/191], loss=14.4012
	step [34/191], loss=14.0746
	step [35/191], loss=15.6822
	step [36/191], loss=15.0446
	step [37/191], loss=15.3915
	step [38/191], loss=15.1887
	step [39/191], loss=13.2742
	step [40/191], loss=14.6436
	step [41/191], loss=14.1964
	step [42/191], loss=12.0835
	step [43/191], loss=13.4571
	step [44/191], loss=15.0699
	step [45/191], loss=14.1685
	step [46/191], loss=12.6653
	step [47/191], loss=14.7775
	step [48/191], loss=14.9032
	step [49/191], loss=14.2555
	step [50/191], loss=11.6621
	step [51/191], loss=14.4874
	step [52/191], loss=13.8132
	step [53/191], loss=14.0438
	step [54/191], loss=13.9471
	step [55/191], loss=13.2233
	step [56/191], loss=13.1608
	step [57/191], loss=14.1064
	step [58/191], loss=14.1996
	step [59/191], loss=12.3132
	step [60/191], loss=13.0200
	step [61/191], loss=15.3400
	step [62/191], loss=12.3965
	step [63/191], loss=12.9701
	step [64/191], loss=14.5708
	step [65/191], loss=13.5502
	step [66/191], loss=14.1541
	step [67/191], loss=13.7784
	step [68/191], loss=14.9954
	step [69/191], loss=13.7253
	step [70/191], loss=14.1429
	step [71/191], loss=12.4802
	step [72/191], loss=13.6812
	step [73/191], loss=14.5734
	step [74/191], loss=12.9259
	step [75/191], loss=14.4742
	step [76/191], loss=15.0735
	step [77/191], loss=12.8826
	step [78/191], loss=13.7262
	step [79/191], loss=13.7338
	step [80/191], loss=14.4842
	step [81/191], loss=14.3805
	step [82/191], loss=16.2962
	step [83/191], loss=13.2076
	step [84/191], loss=15.6526
	step [85/191], loss=15.1623
	step [86/191], loss=13.7260
	step [87/191], loss=13.5288
	step [88/191], loss=13.4013
	step [89/191], loss=15.1563
	step [90/191], loss=14.5171
	step [91/191], loss=14.9710
	step [92/191], loss=14.3343
	step [93/191], loss=14.2406
	step [94/191], loss=13.3792
	step [95/191], loss=14.2908
	step [96/191], loss=12.4362
	step [97/191], loss=12.1281
	step [98/191], loss=12.4830
	step [99/191], loss=13.3716
	step [100/191], loss=13.7410
	step [101/191], loss=13.8073
	step [102/191], loss=13.5704
	step [103/191], loss=14.1978
	step [104/191], loss=15.6302
	step [105/191], loss=12.8873
	step [106/191], loss=14.8940
	step [107/191], loss=14.4321
	step [108/191], loss=13.2670
	step [109/191], loss=14.2539
	step [110/191], loss=13.4446
	step [111/191], loss=12.4048
	step [112/191], loss=12.0688
	step [113/191], loss=13.5657
	step [114/191], loss=12.2743
	step [115/191], loss=15.9981
	step [116/191], loss=11.7760
	step [117/191], loss=13.1175
	step [118/191], loss=13.6501
	step [119/191], loss=12.6356
	step [120/191], loss=13.4774
	step [121/191], loss=12.6496
	step [122/191], loss=12.5178
	step [123/191], loss=12.3132
	step [124/191], loss=13.2002
	step [125/191], loss=14.0289
	step [126/191], loss=11.2834
	step [127/191], loss=14.2740
	step [128/191], loss=12.9586
	step [129/191], loss=12.5490
	step [130/191], loss=12.6988
	step [131/191], loss=12.8648
	step [132/191], loss=12.7579
	step [133/191], loss=13.4061
	step [134/191], loss=12.8857
	step [135/191], loss=12.5068
	step [136/191], loss=13.5885
	step [137/191], loss=12.1041
	step [138/191], loss=14.7651
	step [139/191], loss=13.8353
	step [140/191], loss=13.9572
	step [141/191], loss=13.9278
	step [142/191], loss=13.9259
	step [143/191], loss=14.1601
	step [144/191], loss=13.9085
	step [145/191], loss=12.6315
	step [146/191], loss=14.7397
	step [147/191], loss=13.1269
	step [148/191], loss=15.3910
	step [149/191], loss=11.8223
	step [150/191], loss=13.2325
	step [151/191], loss=13.4057
	step [152/191], loss=13.7951
	step [153/191], loss=13.0391
	step [154/191], loss=12.8101
	step [155/191], loss=15.9957
	step [156/191], loss=11.7673
	step [157/191], loss=11.3706
	step [158/191], loss=12.8978
	step [159/191], loss=13.2494
	step [160/191], loss=11.6173
	step [161/191], loss=12.5886
	step [162/191], loss=13.6117
	step [163/191], loss=14.4401
	step [164/191], loss=14.1822
	step [165/191], loss=14.1568
	step [166/191], loss=14.7054
	step [167/191], loss=12.5172
	step [168/191], loss=12.1524
	step [169/191], loss=15.8115
	step [170/191], loss=14.1251
	step [171/191], loss=12.6207
	step [172/191], loss=13.7299
	step [173/191], loss=12.8275
	step [174/191], loss=13.1052
	step [175/191], loss=13.6817
	step [176/191], loss=12.0788
	step [177/191], loss=12.6884
	step [178/191], loss=16.4697
	step [179/191], loss=13.8174
	step [180/191], loss=13.4995
	step [181/191], loss=12.0764
	step [182/191], loss=14.6271
	step [183/191], loss=13.5327
	step [184/191], loss=12.1455
	step [185/191], loss=13.4937
	step [186/191], loss=12.0653
	step [187/191], loss=13.8371
	step [188/191], loss=13.0380
	step [189/191], loss=12.5621
	step [190/191], loss=13.4878
	step [191/191], loss=6.7513
	Evaluating
	loss=0.0505, precision=0.2086, recall=0.9857, f1=0.7182
Training epoch 14
	step [1/191], loss=14.8967
	step [2/191], loss=16.2127
	step [3/191], loss=13.5646
	step [4/191], loss=13.8875
	step [5/191], loss=13.1024
	step [6/191], loss=13.7227
	step [7/191], loss=13.2376
	step [8/191], loss=12.3132
	step [9/191], loss=12.9193
	step [10/191], loss=12.3877
	step [11/191], loss=12.6263
	step [12/191], loss=12.4591
	step [13/191], loss=17.6078
	step [14/191], loss=14.4879
	step [15/191], loss=14.6546
	step [16/191], loss=14.1024
	step [17/191], loss=14.2335
	step [18/191], loss=13.2145
	step [19/191], loss=13.4485
	step [20/191], loss=14.8536
	step [21/191], loss=12.2718
	step [22/191], loss=13.3706
	step [23/191], loss=14.7276
	step [24/191], loss=11.1769
	step [25/191], loss=13.5108
	step [26/191], loss=12.4570
	step [27/191], loss=11.4485
	step [28/191], loss=11.4957
	step [29/191], loss=13.3261
	step [30/191], loss=11.8796
	step [31/191], loss=12.8833
	step [32/191], loss=12.6717
	step [33/191], loss=13.3739
	step [34/191], loss=13.3812
	step [35/191], loss=12.1966
	step [36/191], loss=13.9010
	step [37/191], loss=12.7239
	step [38/191], loss=12.8529
	step [39/191], loss=11.9622
	step [40/191], loss=13.1236
	step [41/191], loss=14.2081
	step [42/191], loss=13.3275
	step [43/191], loss=12.8475
	step [44/191], loss=12.2038
	step [45/191], loss=12.7729
	step [46/191], loss=12.1156
	step [47/191], loss=11.6602
	step [48/191], loss=13.3975
	step [49/191], loss=12.1179
	step [50/191], loss=12.0780
	step [51/191], loss=12.6175
	step [52/191], loss=12.3583
	step [53/191], loss=13.4306
	step [54/191], loss=15.7388
	step [55/191], loss=14.0025
	step [56/191], loss=12.8892
	step [57/191], loss=13.6227
	step [58/191], loss=12.3391
	step [59/191], loss=12.9097
	step [60/191], loss=13.2140
	step [61/191], loss=12.1154
	step [62/191], loss=13.4965
	step [63/191], loss=11.7307
	step [64/191], loss=13.8892
	step [65/191], loss=15.4346
	step [66/191], loss=13.9056
	step [67/191], loss=11.7951
	step [68/191], loss=11.8095
	step [69/191], loss=12.8138
	step [70/191], loss=12.8027
	step [71/191], loss=12.1552
	step [72/191], loss=12.0502
	step [73/191], loss=13.2471
	step [74/191], loss=13.1387
	step [75/191], loss=11.5301
	step [76/191], loss=13.8706
	step [77/191], loss=13.7921
	step [78/191], loss=12.2860
	step [79/191], loss=13.2473
	step [80/191], loss=12.3137
	step [81/191], loss=12.2613
	step [82/191], loss=13.7675
	step [83/191], loss=11.8204
	step [84/191], loss=12.7782
	step [85/191], loss=13.1742
	step [86/191], loss=12.5520
	step [87/191], loss=12.2718
	step [88/191], loss=12.6045
	step [89/191], loss=12.7202
	step [90/191], loss=12.0894
	step [91/191], loss=12.3264
	step [92/191], loss=13.0715
	step [93/191], loss=11.8804
	step [94/191], loss=13.6283
	step [95/191], loss=11.7342
	step [96/191], loss=12.8317
	step [97/191], loss=12.0509
	step [98/191], loss=13.2261
	step [99/191], loss=13.5372
	step [100/191], loss=12.2295
	step [101/191], loss=11.5474
	step [102/191], loss=13.5515
	step [103/191], loss=13.5462
	step [104/191], loss=10.7331
	step [105/191], loss=13.5576
	step [106/191], loss=11.5581
	step [107/191], loss=12.3186
	step [108/191], loss=11.1685
	step [109/191], loss=11.2853
	step [110/191], loss=13.3744
	step [111/191], loss=12.1491
	step [112/191], loss=13.2383
	step [113/191], loss=12.4408
	step [114/191], loss=12.0308
	step [115/191], loss=11.7536
	step [116/191], loss=12.7224
	step [117/191], loss=13.3134
	step [118/191], loss=11.4182
	step [119/191], loss=12.6722
	step [120/191], loss=12.2453
	step [121/191], loss=10.5819
	step [122/191], loss=12.2196
	step [123/191], loss=14.0975
	step [124/191], loss=11.9180
	step [125/191], loss=12.0286
	step [126/191], loss=12.0150
	step [127/191], loss=12.3447
	step [128/191], loss=11.8728
	step [129/191], loss=11.0896
	step [130/191], loss=13.8195
	step [131/191], loss=14.0920
	step [132/191], loss=14.9159
	step [133/191], loss=13.2944
	step [134/191], loss=14.1705
	step [135/191], loss=12.8282
	step [136/191], loss=11.1777
	step [137/191], loss=13.6287
	step [138/191], loss=11.6885
	step [139/191], loss=13.2290
	step [140/191], loss=12.0393
	step [141/191], loss=13.3013
	step [142/191], loss=13.3930
	step [143/191], loss=14.1220
	step [144/191], loss=13.0059
	step [145/191], loss=13.0983
	step [146/191], loss=13.1578
	step [147/191], loss=13.8417
	step [148/191], loss=12.7310
	step [149/191], loss=13.0770
	step [150/191], loss=12.0688
	step [151/191], loss=12.7536
	step [152/191], loss=12.2914
	step [153/191], loss=11.6279
	step [154/191], loss=13.3181
	step [155/191], loss=12.9931
	step [156/191], loss=11.8920
	step [157/191], loss=12.9296
	step [158/191], loss=13.1439
	step [159/191], loss=11.7185
	step [160/191], loss=12.2137
	step [161/191], loss=13.7342
	step [162/191], loss=12.0286
	step [163/191], loss=11.3159
	step [164/191], loss=11.9089
	step [165/191], loss=11.8638
	step [166/191], loss=10.2523
	step [167/191], loss=10.8155
	step [168/191], loss=9.6041
	step [169/191], loss=11.2744
	step [170/191], loss=10.8138
	step [171/191], loss=11.6576
	step [172/191], loss=11.7618
	step [173/191], loss=10.9424
	step [174/191], loss=12.4251
	step [175/191], loss=13.2558
	step [176/191], loss=11.4815
	step [177/191], loss=11.8184
	step [178/191], loss=13.6286
	step [179/191], loss=12.6123
	step [180/191], loss=11.1946
	step [181/191], loss=15.3252
	step [182/191], loss=13.9712
	step [183/191], loss=13.0787
	step [184/191], loss=11.4581
	step [185/191], loss=11.2721
	step [186/191], loss=10.5912
	step [187/191], loss=11.7242
	step [188/191], loss=11.6487
	step [189/191], loss=12.5885
	step [190/191], loss=11.6028
	step [191/191], loss=4.9957
	Evaluating
	loss=0.0426, precision=0.2407, recall=0.9820, f1=0.7508
Training epoch 15
	step [1/191], loss=12.5493
	step [2/191], loss=12.4751
	step [3/191], loss=14.0374
	step [4/191], loss=13.7346
	step [5/191], loss=10.9620
	step [6/191], loss=11.7189
	step [7/191], loss=13.6753
	step [8/191], loss=11.6527
	step [9/191], loss=12.5711
	step [10/191], loss=11.8793
	step [11/191], loss=11.9633
	step [12/191], loss=13.1844
	step [13/191], loss=13.1932
	step [14/191], loss=12.8632
	step [15/191], loss=11.4639
	step [16/191], loss=12.0997
	step [17/191], loss=11.0698
	step [18/191], loss=11.3401
	step [19/191], loss=12.0067
	step [20/191], loss=12.8572
	step [21/191], loss=14.0782
	step [22/191], loss=10.6342
	step [23/191], loss=10.6045
	step [24/191], loss=12.9225
	step [25/191], loss=11.0593
	step [26/191], loss=11.2082
	step [27/191], loss=11.3962
	step [28/191], loss=14.4461
	step [29/191], loss=12.2195
	step [30/191], loss=11.6732
	step [31/191], loss=12.6922
	step [32/191], loss=11.5157
	step [33/191], loss=11.9036
	step [34/191], loss=12.5195
	step [35/191], loss=11.5569
	step [36/191], loss=12.3828
	step [37/191], loss=12.2687
	step [38/191], loss=11.6903
	step [39/191], loss=10.2674
	step [40/191], loss=12.8484
	step [41/191], loss=11.0561
	step [42/191], loss=11.0692
	step [43/191], loss=11.1343
	step [44/191], loss=12.2138
	step [45/191], loss=11.5942
	step [46/191], loss=11.9487
	step [47/191], loss=11.3018
	step [48/191], loss=14.9331
	step [49/191], loss=11.3711
	step [50/191], loss=10.9015
	step [51/191], loss=11.0117
	step [52/191], loss=13.3054
	step [53/191], loss=12.6929
	step [54/191], loss=12.5221
	step [55/191], loss=12.2396
	step [56/191], loss=9.9548
	step [57/191], loss=11.3831
	step [58/191], loss=12.5172
	step [59/191], loss=11.1042
	step [60/191], loss=11.9278
	step [61/191], loss=11.4516
	step [62/191], loss=12.3043
	step [63/191], loss=12.1819
	step [64/191], loss=11.4500
	step [65/191], loss=10.3743
	step [66/191], loss=9.9342
	step [67/191], loss=11.2707
	step [68/191], loss=12.2196
	step [69/191], loss=12.9290
	step [70/191], loss=9.9228
	step [71/191], loss=10.7752
	step [72/191], loss=10.8126
	step [73/191], loss=10.8728
	step [74/191], loss=11.8409
	step [75/191], loss=13.1572
	step [76/191], loss=13.3706
	step [77/191], loss=12.1752
	step [78/191], loss=12.3232
	step [79/191], loss=11.8202
	step [80/191], loss=12.2245
	step [81/191], loss=11.0718
	step [82/191], loss=11.8012
	step [83/191], loss=10.6577
	step [84/191], loss=11.7279
	step [85/191], loss=13.9691
	step [86/191], loss=11.0401
	step [87/191], loss=10.5752
	step [88/191], loss=11.7597
	step [89/191], loss=10.8219
	step [90/191], loss=12.6166
	step [91/191], loss=10.6101
	step [92/191], loss=9.9887
	step [93/191], loss=11.5294
	step [94/191], loss=12.3251
	step [95/191], loss=11.9078
	step [96/191], loss=12.4031
	step [97/191], loss=11.8858
	step [98/191], loss=12.0561
	step [99/191], loss=11.9848
	step [100/191], loss=12.6408
	step [101/191], loss=11.9182
	step [102/191], loss=11.0114
	step [103/191], loss=11.0827
	step [104/191], loss=10.2950
	step [105/191], loss=11.6301
	step [106/191], loss=10.5260
	step [107/191], loss=10.9012
	step [108/191], loss=12.0775
	step [109/191], loss=10.5136
	step [110/191], loss=14.2124
	step [111/191], loss=10.9466
	step [112/191], loss=11.9424
	step [113/191], loss=12.6259
	step [114/191], loss=13.2142
	step [115/191], loss=11.6461
	step [116/191], loss=12.4773
	step [117/191], loss=11.2130
	step [118/191], loss=12.8387
	step [119/191], loss=11.3136
	step [120/191], loss=12.6378
	step [121/191], loss=12.5380
	step [122/191], loss=12.1239
	step [123/191], loss=11.8770
	step [124/191], loss=9.7486
	step [125/191], loss=12.9868
	step [126/191], loss=13.3343
	step [127/191], loss=12.4576
	step [128/191], loss=11.6578
	step [129/191], loss=10.6955
	step [130/191], loss=10.9650
	step [131/191], loss=10.7026
	step [132/191], loss=10.7411
	step [133/191], loss=11.7790
	step [134/191], loss=13.6480
	step [135/191], loss=12.6857
	step [136/191], loss=12.4028
	step [137/191], loss=10.9468
	step [138/191], loss=11.8681
	step [139/191], loss=10.0689
	step [140/191], loss=11.0791
	step [141/191], loss=11.4579
	step [142/191], loss=10.9123
	step [143/191], loss=12.6974
	step [144/191], loss=13.6455
	step [145/191], loss=11.8326
	step [146/191], loss=10.8657
	step [147/191], loss=15.4745
	step [148/191], loss=11.5463
	step [149/191], loss=11.0065
	step [150/191], loss=11.4245
	step [151/191], loss=11.5054
	step [152/191], loss=11.3053
	step [153/191], loss=13.1583
	step [154/191], loss=11.4808
	step [155/191], loss=10.6077
	step [156/191], loss=12.2822
	step [157/191], loss=11.0937
	step [158/191], loss=10.6637
	step [159/191], loss=13.2682
	step [160/191], loss=10.8388
	step [161/191], loss=12.0735
	step [162/191], loss=10.3692
	step [163/191], loss=13.1584
	step [164/191], loss=11.0321
	step [165/191], loss=10.9104
	step [166/191], loss=11.8991
	step [167/191], loss=9.6108
	step [168/191], loss=10.3577
	step [169/191], loss=10.5127
	step [170/191], loss=10.9241
	step [171/191], loss=12.0444
	step [172/191], loss=9.6232
	step [173/191], loss=11.6936
	step [174/191], loss=10.6647
	step [175/191], loss=12.0524
	step [176/191], loss=11.4581
	step [177/191], loss=11.1198
	step [178/191], loss=11.1795
	step [179/191], loss=10.9291
	step [180/191], loss=10.6280
	step [181/191], loss=12.1972
	step [182/191], loss=12.0834
	step [183/191], loss=10.8300
	step [184/191], loss=11.1498
	step [185/191], loss=11.8189
	step [186/191], loss=10.6636
	step [187/191], loss=12.0719
	step [188/191], loss=9.7925
	step [189/191], loss=10.9183
	step [190/191], loss=10.9200
	step [191/191], loss=4.6537
	Evaluating
	loss=0.0459, precision=0.1964, recall=0.9851, f1=0.7029
Training epoch 16
	step [1/191], loss=12.8947
	step [2/191], loss=10.9415
	step [3/191], loss=12.4002
	step [4/191], loss=10.2063
	step [5/191], loss=11.4921
	step [6/191], loss=11.6768
	step [7/191], loss=11.9064
	step [8/191], loss=13.1762
	step [9/191], loss=11.0030
	step [10/191], loss=10.8954
	step [11/191], loss=10.7970
	step [12/191], loss=12.5544
	step [13/191], loss=11.4686
	step [14/191], loss=12.2217
	step [15/191], loss=9.5827
	step [16/191], loss=11.4208
	step [17/191], loss=11.5898
	step [18/191], loss=11.6766
	step [19/191], loss=10.4108
	step [20/191], loss=11.0247
	step [21/191], loss=10.9796
	step [22/191], loss=9.8415
	step [23/191], loss=11.3201
	step [24/191], loss=12.7312
	step [25/191], loss=12.0744
	step [26/191], loss=10.7404
	step [27/191], loss=11.0943
	step [28/191], loss=10.6698
	step [29/191], loss=10.1312
	step [30/191], loss=9.9975
	step [31/191], loss=10.0609
	step [32/191], loss=12.5503
	step [33/191], loss=11.9919
	step [34/191], loss=11.9912
	step [35/191], loss=12.0847
	step [36/191], loss=10.9572
	step [37/191], loss=10.6292
	step [38/191], loss=11.2420
	step [39/191], loss=9.9089
	step [40/191], loss=11.2712
	step [41/191], loss=12.2051
	step [42/191], loss=10.2737
	step [43/191], loss=11.4370
	step [44/191], loss=10.4036
	step [45/191], loss=8.9771
	step [46/191], loss=9.8746
	step [47/191], loss=12.1831
	step [48/191], loss=12.7609
	step [49/191], loss=10.7198
	step [50/191], loss=10.9914
	step [51/191], loss=9.6248
	step [52/191], loss=11.8076
	step [53/191], loss=11.0338
	step [54/191], loss=13.6436
	step [55/191], loss=11.9632
	step [56/191], loss=10.0646
	step [57/191], loss=11.0886
	step [58/191], loss=11.3242
	step [59/191], loss=10.5790
	step [60/191], loss=10.8493
	step [61/191], loss=11.8613
	step [62/191], loss=11.3941
	step [63/191], loss=10.4975
	step [64/191], loss=11.0764
	step [65/191], loss=12.5612
	step [66/191], loss=11.9289
	step [67/191], loss=10.9910
	step [68/191], loss=13.1135
	step [69/191], loss=11.6059
	step [70/191], loss=12.7441
	step [71/191], loss=10.9052
	step [72/191], loss=11.1964
	step [73/191], loss=10.6923
	step [74/191], loss=11.9634
	step [75/191], loss=13.0023
	step [76/191], loss=10.0118
	step [77/191], loss=9.6742
	step [78/191], loss=10.2614
	step [79/191], loss=10.9858
	step [80/191], loss=10.4991
	step [81/191], loss=10.7467
	step [82/191], loss=10.7350
	step [83/191], loss=11.9011
	step [84/191], loss=11.2497
	step [85/191], loss=11.2624
	step [86/191], loss=10.5426
	step [87/191], loss=11.2381
	step [88/191], loss=11.9443
	step [89/191], loss=10.5462
	step [90/191], loss=11.7652
	step [91/191], loss=10.6622
	step [92/191], loss=11.3937
	step [93/191], loss=11.0508
	step [94/191], loss=9.8255
	step [95/191], loss=10.9575
	step [96/191], loss=10.3955
	step [97/191], loss=10.6146
	step [98/191], loss=10.9121
	step [99/191], loss=10.2668
	step [100/191], loss=9.6223
	step [101/191], loss=13.0785
	step [102/191], loss=11.6629
	step [103/191], loss=10.3274
	step [104/191], loss=9.0349
	step [105/191], loss=10.2687
	step [106/191], loss=10.4234
	step [107/191], loss=13.9735
	step [108/191], loss=10.4007
	step [109/191], loss=11.4026
	step [110/191], loss=12.9255
	step [111/191], loss=11.8443
	step [112/191], loss=11.4952
	step [113/191], loss=10.2031
	step [114/191], loss=10.4435
	step [115/191], loss=11.6049
	step [116/191], loss=10.1784
	step [117/191], loss=10.3458
	step [118/191], loss=10.6190
	step [119/191], loss=10.3391
	step [120/191], loss=9.7908
	step [121/191], loss=11.6442
	step [122/191], loss=12.1278
	step [123/191], loss=10.5305
	step [124/191], loss=11.6428
	step [125/191], loss=12.5301
	step [126/191], loss=10.3164
	step [127/191], loss=11.8015
	step [128/191], loss=11.1553
	step [129/191], loss=9.2836
	step [130/191], loss=9.6386
	step [131/191], loss=11.5886
	step [132/191], loss=10.0991
	step [133/191], loss=12.6114
	step [134/191], loss=11.2959
	step [135/191], loss=10.6881
	step [136/191], loss=10.5594
	step [137/191], loss=8.5614
	step [138/191], loss=10.7093
	step [139/191], loss=10.6671
	step [140/191], loss=11.3046
	step [141/191], loss=11.1261
	step [142/191], loss=10.8335
	step [143/191], loss=10.3297
	step [144/191], loss=11.2571
	step [145/191], loss=10.0947
	step [146/191], loss=10.2006
	step [147/191], loss=10.6983
	step [148/191], loss=12.1487
	step [149/191], loss=11.5776
	step [150/191], loss=11.2826
	step [151/191], loss=10.8138
	step [152/191], loss=10.3879
	step [153/191], loss=11.6576
	step [154/191], loss=9.8795
	step [155/191], loss=11.5275
	step [156/191], loss=9.4181
	step [157/191], loss=10.3856
	step [158/191], loss=9.7924
	step [159/191], loss=10.3585
	step [160/191], loss=9.7686
	step [161/191], loss=11.6112
	step [162/191], loss=10.6948
	step [163/191], loss=11.1996
	step [164/191], loss=11.0152
	step [165/191], loss=12.4390
	step [166/191], loss=11.7456
	step [167/191], loss=10.6933
	step [168/191], loss=11.9172
	step [169/191], loss=9.5389
	step [170/191], loss=11.8698
	step [171/191], loss=9.6837
	step [172/191], loss=10.6872
	step [173/191], loss=10.1427
	step [174/191], loss=13.2706
	step [175/191], loss=9.9848
	step [176/191], loss=8.1566
	step [177/191], loss=13.6494
	step [178/191], loss=10.4977
	step [179/191], loss=10.9962
	step [180/191], loss=10.2032
	step [181/191], loss=10.3896
	step [182/191], loss=10.9719
	step [183/191], loss=10.9912
	step [184/191], loss=9.7298
	step [185/191], loss=10.1468
	step [186/191], loss=11.1793
	step [187/191], loss=9.5158
	step [188/191], loss=11.7668
	step [189/191], loss=8.8666
	step [190/191], loss=10.7989
	step [191/191], loss=4.8963
	Evaluating
	loss=0.0340, precision=0.2698, recall=0.9809, f1=0.7763
saving model as: 1_saved_model.pth
Training epoch 17
	step [1/191], loss=9.6638
	step [2/191], loss=11.7371
	step [3/191], loss=11.1706
	step [4/191], loss=10.6598
	step [5/191], loss=9.5372
	step [6/191], loss=10.0064
	step [7/191], loss=11.1407
	step [8/191], loss=10.8019
	step [9/191], loss=9.5460
	step [10/191], loss=11.3289
	step [11/191], loss=9.4485
	step [12/191], loss=9.1934
	step [13/191], loss=9.4853
	step [14/191], loss=10.5434
	step [15/191], loss=10.7279
	step [16/191], loss=9.7235
	step [17/191], loss=11.3038
	step [18/191], loss=8.2021
	step [19/191], loss=10.5765
	step [20/191], loss=9.3199
	step [21/191], loss=10.8305
	step [22/191], loss=9.2481
	step [23/191], loss=11.6347
	step [24/191], loss=10.3038
	step [25/191], loss=9.4659
	step [26/191], loss=10.9581
	step [27/191], loss=11.6073
	step [28/191], loss=10.3107
	step [29/191], loss=10.6994
	step [30/191], loss=9.9652
	step [31/191], loss=10.2878
	step [32/191], loss=8.9356
	step [33/191], loss=12.7796
	step [34/191], loss=10.1372
	step [35/191], loss=13.5180
	step [36/191], loss=11.2312
	step [37/191], loss=9.6323
	step [38/191], loss=10.0375
	step [39/191], loss=9.9177
	step [40/191], loss=10.6926
	step [41/191], loss=11.0075
	step [42/191], loss=9.2761
	step [43/191], loss=11.5809
	step [44/191], loss=11.0776
	step [45/191], loss=10.6381
	step [46/191], loss=9.6572
	step [47/191], loss=9.7780
	step [48/191], loss=11.5602
	step [49/191], loss=10.1357
	step [50/191], loss=9.9816
	step [51/191], loss=10.3869
	step [52/191], loss=10.1655
	step [53/191], loss=8.9812
	step [54/191], loss=10.0276
	step [55/191], loss=10.0313
	step [56/191], loss=10.7502
	step [57/191], loss=10.7931
	step [58/191], loss=10.2656
	step [59/191], loss=10.3879
	step [60/191], loss=10.0063
	step [61/191], loss=11.5954
	step [62/191], loss=10.2771
	step [63/191], loss=10.8466
	step [64/191], loss=8.5834
	step [65/191], loss=9.5707
	step [66/191], loss=11.7880
	step [67/191], loss=9.8919
	step [68/191], loss=11.6961
	step [69/191], loss=10.4421
	step [70/191], loss=10.0465
	step [71/191], loss=10.5346
	step [72/191], loss=8.5981
	step [73/191], loss=8.4797
	step [74/191], loss=12.1156
	step [75/191], loss=9.6873
	step [76/191], loss=11.5267
	step [77/191], loss=9.0104
	step [78/191], loss=11.3639
	step [79/191], loss=9.8943
	step [80/191], loss=9.2429
	step [81/191], loss=9.7461
	step [82/191], loss=9.2474
	step [83/191], loss=10.5611
	step [84/191], loss=10.2205
	step [85/191], loss=10.7935
	step [86/191], loss=11.2066
	step [87/191], loss=11.6071
	step [88/191], loss=10.1041
	step [89/191], loss=11.1169
	step [90/191], loss=10.7280
	step [91/191], loss=9.7025
	step [92/191], loss=10.0571
	step [93/191], loss=11.0568
	step [94/191], loss=9.6742
	step [95/191], loss=10.1323
	step [96/191], loss=9.3635
	step [97/191], loss=13.0150
	step [98/191], loss=11.0849
	step [99/191], loss=10.2335
	step [100/191], loss=9.1942
	step [101/191], loss=9.8580
	step [102/191], loss=10.4911
	step [103/191], loss=10.5438
	step [104/191], loss=12.8864
	step [105/191], loss=10.4444
	step [106/191], loss=10.2538
	step [107/191], loss=11.5388
	step [108/191], loss=10.8140
	step [109/191], loss=10.1658
	step [110/191], loss=10.8376
	step [111/191], loss=10.4480
	step [112/191], loss=9.5238
	step [113/191], loss=10.4576
	step [114/191], loss=10.2421
	step [115/191], loss=9.9434
	step [116/191], loss=9.3948
	step [117/191], loss=12.1444
	step [118/191], loss=11.8561
	step [119/191], loss=10.2528
	step [120/191], loss=12.7898
	step [121/191], loss=10.6910
	step [122/191], loss=9.8237
	step [123/191], loss=9.2489
	step [124/191], loss=9.7300
	step [125/191], loss=10.8165
	step [126/191], loss=11.0553
	step [127/191], loss=10.3075
	step [128/191], loss=9.4697
	step [129/191], loss=10.2914
	step [130/191], loss=12.2729
	step [131/191], loss=9.7601
	step [132/191], loss=10.2632
	step [133/191], loss=9.3406
	step [134/191], loss=9.8636
	step [135/191], loss=9.6791
	step [136/191], loss=12.3313
	step [137/191], loss=11.8428
	step [138/191], loss=11.5848
	step [139/191], loss=9.5934
	step [140/191], loss=9.7771
	step [141/191], loss=10.7078
	step [142/191], loss=8.8484
	step [143/191], loss=10.9922
	step [144/191], loss=11.2095
	step [145/191], loss=10.8391
	step [146/191], loss=11.8340
	step [147/191], loss=10.0322
	step [148/191], loss=9.2580
	step [149/191], loss=10.1508
	step [150/191], loss=9.6910
	step [151/191], loss=9.6014
	step [152/191], loss=10.3845
	step [153/191], loss=12.0083
	step [154/191], loss=10.0598
	step [155/191], loss=11.1281
	step [156/191], loss=9.2738
	step [157/191], loss=11.2411
	step [158/191], loss=8.4658
	step [159/191], loss=11.0337
	step [160/191], loss=11.3010
	step [161/191], loss=11.1163
	step [162/191], loss=11.0351
	step [163/191], loss=11.6379
	step [164/191], loss=9.3865
	step [165/191], loss=11.3270
	step [166/191], loss=10.4691
	step [167/191], loss=10.8711
	step [168/191], loss=9.1712
	step [169/191], loss=10.7939
	step [170/191], loss=9.6750
	step [171/191], loss=8.8473
	step [172/191], loss=10.4811
	step [173/191], loss=9.4220
	step [174/191], loss=10.9609
	step [175/191], loss=11.3290
	step [176/191], loss=9.7893
	step [177/191], loss=12.2424
	step [178/191], loss=10.5309
	step [179/191], loss=10.0100
	step [180/191], loss=9.4469
	step [181/191], loss=10.5420
	step [182/191], loss=9.9744
	step [183/191], loss=9.5723
	step [184/191], loss=10.0011
	step [185/191], loss=10.1227
	step [186/191], loss=9.8382
	step [187/191], loss=9.9473
	step [188/191], loss=10.8514
	step [189/191], loss=11.0561
	step [190/191], loss=10.3274
	step [191/191], loss=4.1230
	Evaluating
	loss=0.0399, precision=0.2151, recall=0.9850, f1=0.7254
Training epoch 18
	step [1/191], loss=9.3145
	step [2/191], loss=10.3888
	step [3/191], loss=10.5284
	step [4/191], loss=12.1385
	step [5/191], loss=8.9909
	step [6/191], loss=9.1316
	step [7/191], loss=9.0292
	step [8/191], loss=10.6736
	step [9/191], loss=9.4561
	step [10/191], loss=9.9387
	step [11/191], loss=9.6409
	step [12/191], loss=9.0596
	step [13/191], loss=11.2813
	step [14/191], loss=9.6533
	step [15/191], loss=9.5531
	step [16/191], loss=10.1663
	step [17/191], loss=10.8223
	step [18/191], loss=8.7009
	step [19/191], loss=10.7988
	step [20/191], loss=10.7091
	step [21/191], loss=10.0748
	step [22/191], loss=9.4428
	step [23/191], loss=10.2852
	step [24/191], loss=9.1414
	step [25/191], loss=9.8537
	step [26/191], loss=9.0324
	step [27/191], loss=12.0273
	step [28/191], loss=9.5892
	step [29/191], loss=9.6172
	step [30/191], loss=9.0025
	step [31/191], loss=10.4551
	step [32/191], loss=9.5119
	step [33/191], loss=10.1022
	step [34/191], loss=11.1621
	step [35/191], loss=11.2305
	step [36/191], loss=9.2879
	step [37/191], loss=9.5121
	step [38/191], loss=9.8621
	step [39/191], loss=10.6380
	step [40/191], loss=9.6854
	step [41/191], loss=9.3132
	step [42/191], loss=10.2303
	step [43/191], loss=10.2624
	step [44/191], loss=9.8099
	step [45/191], loss=10.7208
	step [46/191], loss=10.4238
	step [47/191], loss=9.1385
	step [48/191], loss=11.0592
	step [49/191], loss=9.3224
	step [50/191], loss=9.5568
	step [51/191], loss=10.6683
	step [52/191], loss=9.5018
	step [53/191], loss=8.5948
	step [54/191], loss=9.1942
	step [55/191], loss=8.6834
	step [56/191], loss=10.1411
	step [57/191], loss=8.9177
	step [58/191], loss=9.7063
	step [59/191], loss=9.5991
	step [60/191], loss=9.6329
	step [61/191], loss=11.3951
	step [62/191], loss=10.6453
	step [63/191], loss=9.8417
	step [64/191], loss=12.2221
	step [65/191], loss=8.0929
	step [66/191], loss=9.3514
	step [67/191], loss=9.8427
	step [68/191], loss=9.6161
	step [69/191], loss=9.9352
	step [70/191], loss=9.6836
	step [71/191], loss=9.4753
	step [72/191], loss=10.3875
	step [73/191], loss=10.5802
	step [74/191], loss=10.8544
	step [75/191], loss=10.2596
	step [76/191], loss=9.1261
	step [77/191], loss=11.1532
	step [78/191], loss=9.2557
	step [79/191], loss=10.6054
	step [80/191], loss=9.5831
	step [81/191], loss=9.4421
	step [82/191], loss=9.4056
	step [83/191], loss=9.4611
	step [84/191], loss=9.4460
	step [85/191], loss=11.1788
	step [86/191], loss=11.0012
	step [87/191], loss=12.1082
	step [88/191], loss=9.6617
	step [89/191], loss=9.6574
	step [90/191], loss=9.6288
	step [91/191], loss=10.0215
	step [92/191], loss=10.1236
	step [93/191], loss=10.2785
	step [94/191], loss=11.3963
	step [95/191], loss=8.9125
	step [96/191], loss=8.8498
	step [97/191], loss=10.3175
	step [98/191], loss=10.9411
	step [99/191], loss=10.7257
	step [100/191], loss=10.2095
	step [101/191], loss=8.1714
	step [102/191], loss=10.3343
	step [103/191], loss=9.4187
	step [104/191], loss=8.8476
	step [105/191], loss=11.0108
	step [106/191], loss=8.0354
	step [107/191], loss=10.6568
	step [108/191], loss=8.8255
	step [109/191], loss=9.2100
	step [110/191], loss=8.4008
	step [111/191], loss=9.8702
	step [112/191], loss=10.2101
	step [113/191], loss=9.6395
	step [114/191], loss=10.0020
	step [115/191], loss=10.1109
	step [116/191], loss=12.2445
	step [117/191], loss=9.7736
	step [118/191], loss=9.9915
	step [119/191], loss=10.5138
	step [120/191], loss=10.2193
	step [121/191], loss=9.2526
	step [122/191], loss=9.3141
	step [123/191], loss=10.1385
	step [124/191], loss=9.0232
	step [125/191], loss=9.9529
	step [126/191], loss=9.2685
	step [127/191], loss=9.1305
	step [128/191], loss=8.3638
	step [129/191], loss=9.4650
	step [130/191], loss=9.1107
	step [131/191], loss=8.7856
	step [132/191], loss=9.7208
	step [133/191], loss=8.9240
	step [134/191], loss=9.4177
	step [135/191], loss=7.9195
	step [136/191], loss=8.7278
	step [137/191], loss=9.7420
	step [138/191], loss=9.6930
	step [139/191], loss=10.7705
	step [140/191], loss=8.9856
	step [141/191], loss=12.3587
	step [142/191], loss=9.6278
	step [143/191], loss=9.4635
	step [144/191], loss=10.3204
	step [145/191], loss=9.5901
	step [146/191], loss=8.9979
	step [147/191], loss=9.2704
	step [148/191], loss=9.4890
	step [149/191], loss=8.8498
	step [150/191], loss=11.5077
	step [151/191], loss=8.5374
	step [152/191], loss=10.7691
	step [153/191], loss=9.4368
	step [154/191], loss=8.3122
	step [155/191], loss=10.8749
	step [156/191], loss=9.1209
	step [157/191], loss=8.8584
	step [158/191], loss=8.5410
	step [159/191], loss=10.4032
	step [160/191], loss=9.4856
	step [161/191], loss=8.9183
	step [162/191], loss=9.5726
	step [163/191], loss=7.6604
	step [164/191], loss=9.9181
	step [165/191], loss=8.4180
	step [166/191], loss=10.0924
	step [167/191], loss=9.0022
	step [168/191], loss=8.5915
	step [169/191], loss=10.6297
	step [170/191], loss=9.6584
	step [171/191], loss=9.8246
	step [172/191], loss=8.9000
	step [173/191], loss=9.3271
	step [174/191], loss=9.1768
	step [175/191], loss=9.3681
	step [176/191], loss=8.8733
	step [177/191], loss=8.2826
	step [178/191], loss=11.1484
	step [179/191], loss=10.1150
	step [180/191], loss=9.5421
	step [181/191], loss=8.0376
	step [182/191], loss=9.5574
	step [183/191], loss=9.2649
	step [184/191], loss=9.4930
	step [185/191], loss=9.7419
	step [186/191], loss=8.8553
	step [187/191], loss=11.1634
	step [188/191], loss=9.8459
	step [189/191], loss=9.8723
	step [190/191], loss=9.7714
	step [191/191], loss=4.4076
	Evaluating
	loss=0.0356, precision=0.2207, recall=0.9834, f1=0.7309
Training epoch 19
	step [1/191], loss=10.5693
	step [2/191], loss=9.1419
	step [3/191], loss=8.5255
	step [4/191], loss=9.3192
	step [5/191], loss=9.2380
	step [6/191], loss=8.2527
	step [7/191], loss=11.7267
	step [8/191], loss=9.9295
	step [9/191], loss=8.4340
	step [10/191], loss=7.8187
	step [11/191], loss=8.6075
	step [12/191], loss=10.0736
	step [13/191], loss=9.5569
	step [14/191], loss=10.4945
	step [15/191], loss=9.3046
	step [16/191], loss=9.1776
	step [17/191], loss=10.3533
	step [18/191], loss=9.8116
	step [19/191], loss=10.5633
	step [20/191], loss=9.7252
	step [21/191], loss=9.8184
	step [22/191], loss=8.9041
	step [23/191], loss=10.0230
	step [24/191], loss=8.4296
	step [25/191], loss=9.3833
	step [26/191], loss=9.6263
	step [27/191], loss=7.5688
	step [28/191], loss=10.1560
	step [29/191], loss=9.0277
	step [30/191], loss=9.7964
	step [31/191], loss=9.6314
	step [32/191], loss=8.0411
	step [33/191], loss=9.0766
	step [34/191], loss=10.0420
	step [35/191], loss=10.6699
	step [36/191], loss=9.4083
	step [37/191], loss=9.6064
	step [38/191], loss=8.3566
	step [39/191], loss=8.7721
	step [40/191], loss=7.0787
	step [41/191], loss=9.8888
	step [42/191], loss=9.7458
	step [43/191], loss=8.0656
	step [44/191], loss=9.3503
	step [45/191], loss=8.8701
	step [46/191], loss=8.2412
	step [47/191], loss=9.8485
	step [48/191], loss=10.0279
	step [49/191], loss=8.6065
	step [50/191], loss=10.6421
	step [51/191], loss=8.2291
	step [52/191], loss=9.2102
	step [53/191], loss=8.5929
	step [54/191], loss=8.9113
	step [55/191], loss=7.7543
	step [56/191], loss=12.6953
	step [57/191], loss=9.4162
	step [58/191], loss=8.6253
	step [59/191], loss=11.1147
	step [60/191], loss=10.2116
	step [61/191], loss=9.2962
	step [62/191], loss=8.9049
	step [63/191], loss=10.0184
	step [64/191], loss=9.1341
	step [65/191], loss=10.5054
	step [66/191], loss=9.2176
	step [67/191], loss=10.0623
	step [68/191], loss=9.8239
	step [69/191], loss=8.5023
	step [70/191], loss=9.1941
	step [71/191], loss=9.1799
	step [72/191], loss=8.7913
	step [73/191], loss=8.5929
	step [74/191], loss=8.9445
	step [75/191], loss=8.4795
	step [76/191], loss=9.4129
	step [77/191], loss=9.8758
	step [78/191], loss=10.5782
	step [79/191], loss=8.8022
	step [80/191], loss=8.5169
	step [81/191], loss=10.0606
	step [82/191], loss=9.8139
	step [83/191], loss=9.6683
	step [84/191], loss=9.1379
	step [85/191], loss=8.4951
	step [86/191], loss=9.7556
	step [87/191], loss=9.2238
	step [88/191], loss=8.7001
	step [89/191], loss=9.5657
	step [90/191], loss=9.0694
	step [91/191], loss=9.6086
	step [92/191], loss=9.2357
	step [93/191], loss=8.3354
	step [94/191], loss=10.1360
	step [95/191], loss=10.2022
	step [96/191], loss=8.9187
	step [97/191], loss=9.3509
	step [98/191], loss=8.9364
	step [99/191], loss=9.2182
	step [100/191], loss=8.7968
	step [101/191], loss=10.9164
	step [102/191], loss=9.2081
	step [103/191], loss=9.0668
	step [104/191], loss=7.4808
	step [105/191], loss=8.6062
	step [106/191], loss=9.1577
	step [107/191], loss=8.3338
	step [108/191], loss=8.6323
	step [109/191], loss=9.1768
	step [110/191], loss=9.1867
	step [111/191], loss=9.7803
	step [112/191], loss=8.0943
	step [113/191], loss=9.5139
	step [114/191], loss=9.6378
	step [115/191], loss=7.9295
	step [116/191], loss=8.8006
	step [117/191], loss=8.8807
	step [118/191], loss=8.8386
	step [119/191], loss=8.3776
	step [120/191], loss=8.3752
	step [121/191], loss=9.3831
	step [122/191], loss=9.2743
	step [123/191], loss=10.2691
	step [124/191], loss=9.5734
	step [125/191], loss=9.1419
	step [126/191], loss=10.4990
	step [127/191], loss=8.8528
	step [128/191], loss=8.2739
	step [129/191], loss=9.3136
	step [130/191], loss=8.2296
	step [131/191], loss=9.8074
	step [132/191], loss=8.8486
	step [133/191], loss=9.7238
	step [134/191], loss=10.0365
	step [135/191], loss=8.2841
	step [136/191], loss=8.8149
	step [137/191], loss=10.0568
	step [138/191], loss=8.8947
	step [139/191], loss=9.6885
	step [140/191], loss=9.3183
	step [141/191], loss=8.9847
	step [142/191], loss=8.5916
	step [143/191], loss=9.3420
	step [144/191], loss=10.3403
	step [145/191], loss=8.8729
	step [146/191], loss=8.8362
	step [147/191], loss=9.0377
	step [148/191], loss=8.6556
	step [149/191], loss=11.0950
	step [150/191], loss=8.1693
	step [151/191], loss=9.5979
	step [152/191], loss=9.0750
	step [153/191], loss=8.5763
	step [154/191], loss=8.4014
	step [155/191], loss=9.0811
	step [156/191], loss=8.0068
	step [157/191], loss=8.4178
	step [158/191], loss=8.4819
	step [159/191], loss=9.1517
	step [160/191], loss=9.9023
	step [161/191], loss=9.0444
	step [162/191], loss=8.9010
	step [163/191], loss=9.0474
	step [164/191], loss=8.4357
	step [165/191], loss=8.8134
	step [166/191], loss=11.1925
	step [167/191], loss=9.7935
	step [168/191], loss=9.2626
	step [169/191], loss=9.3948
	step [170/191], loss=10.5715
	step [171/191], loss=9.9487
	step [172/191], loss=8.8827
	step [173/191], loss=10.5019
	step [174/191], loss=9.6601
	step [175/191], loss=9.0455
	step [176/191], loss=9.8177
	step [177/191], loss=9.4828
	step [178/191], loss=7.9436
	step [179/191], loss=8.5676
	step [180/191], loss=8.8269
	step [181/191], loss=8.3608
	step [182/191], loss=8.7894
	step [183/191], loss=7.8673
	step [184/191], loss=8.8340
	step [185/191], loss=9.4983
	step [186/191], loss=10.1444
	step [187/191], loss=8.8729
	step [188/191], loss=7.9650
	step [189/191], loss=8.7409
	step [190/191], loss=8.7116
	step [191/191], loss=3.5788
	Evaluating
	loss=0.0305, precision=0.2560, recall=0.9821, f1=0.7651
Training epoch 20
	step [1/191], loss=9.8819
	step [2/191], loss=8.5973
	step [3/191], loss=9.0656
	step [4/191], loss=7.5962
	step [5/191], loss=8.5264
	step [6/191], loss=9.5776
	step [7/191], loss=10.5676
	step [8/191], loss=10.8239
	step [9/191], loss=7.9693
	step [10/191], loss=9.8834
	step [11/191], loss=9.5282
	step [12/191], loss=8.6357
	step [13/191], loss=10.0908
	step [14/191], loss=9.4337
	step [15/191], loss=9.4381
	step [16/191], loss=8.5709
	step [17/191], loss=8.7031
	step [18/191], loss=8.4309
	step [19/191], loss=9.6160
	step [20/191], loss=9.9902
	step [21/191], loss=9.0409
	step [22/191], loss=7.7980
	step [23/191], loss=8.6736
	step [24/191], loss=8.4600
	step [25/191], loss=7.7405
	step [26/191], loss=9.0092
	step [27/191], loss=9.6181
	step [28/191], loss=10.0008
	step [29/191], loss=9.8513
	step [30/191], loss=9.7086
	step [31/191], loss=8.3119
	step [32/191], loss=9.8846
	step [33/191], loss=7.5078
	step [34/191], loss=8.9870
	step [35/191], loss=10.4078
	step [36/191], loss=9.1017
	step [37/191], loss=8.7942
	step [38/191], loss=8.0461
	step [39/191], loss=9.3419
	step [40/191], loss=7.7622
	step [41/191], loss=10.1991
	step [42/191], loss=8.4560
	step [43/191], loss=8.0312
	step [44/191], loss=8.1364
	step [45/191], loss=6.9746
	step [46/191], loss=8.5635
	step [47/191], loss=8.0165
	step [48/191], loss=8.2380
	step [49/191], loss=8.6248
	step [50/191], loss=8.6011
	step [51/191], loss=10.2208
	step [52/191], loss=9.5752
	step [53/191], loss=9.5411
	step [54/191], loss=10.2436
	step [55/191], loss=9.1483
	step [56/191], loss=9.5949
	step [57/191], loss=8.6730
	step [58/191], loss=10.6053
	step [59/191], loss=7.8736
	step [60/191], loss=8.3442
	step [61/191], loss=9.0632
	step [62/191], loss=10.1670
	step [63/191], loss=11.1509
	step [64/191], loss=8.4204
	step [65/191], loss=8.4951
	step [66/191], loss=8.5326
	step [67/191], loss=8.8442
	step [68/191], loss=7.8816
	step [69/191], loss=8.3440
	step [70/191], loss=7.7007
	step [71/191], loss=8.1247
	step [72/191], loss=7.8708
	step [73/191], loss=8.5358
	step [74/191], loss=8.7200
	step [75/191], loss=9.1280
	step [76/191], loss=8.2249
	step [77/191], loss=8.2988
	step [78/191], loss=7.8942
	step [79/191], loss=8.5494
	step [80/191], loss=7.6647
	step [81/191], loss=8.6547
	step [82/191], loss=8.5580
	step [83/191], loss=9.1461
	step [84/191], loss=8.9405
	step [85/191], loss=10.9413
	step [86/191], loss=8.5149
	step [87/191], loss=8.9419
	step [88/191], loss=9.3508
	step [89/191], loss=8.8784
	step [90/191], loss=8.0498
	step [91/191], loss=10.7513
	step [92/191], loss=9.2022
	step [93/191], loss=9.3413
	step [94/191], loss=8.5984
	step [95/191], loss=10.6353
	step [96/191], loss=7.9505
	step [97/191], loss=8.9391
	step [98/191], loss=8.0726
	step [99/191], loss=10.2321
	step [100/191], loss=7.0908
	step [101/191], loss=7.3491
	step [102/191], loss=8.0779
	step [103/191], loss=8.8566
	step [104/191], loss=8.1468
	step [105/191], loss=8.6211
	step [106/191], loss=9.9385
	step [107/191], loss=7.2992
	step [108/191], loss=9.8396
	step [109/191], loss=8.1081
	step [110/191], loss=8.2965
	step [111/191], loss=9.1520
	step [112/191], loss=8.4770
	step [113/191], loss=7.9997
	step [114/191], loss=8.6255
	step [115/191], loss=8.7500
	step [116/191], loss=9.4242
	step [117/191], loss=7.8290
	step [118/191], loss=8.3927
	step [119/191], loss=7.4614
	step [120/191], loss=7.7616
	step [121/191], loss=8.3284
	step [122/191], loss=9.7182
	step [123/191], loss=8.3682
	step [124/191], loss=10.2394
	step [125/191], loss=8.7143
	step [126/191], loss=10.2237
	step [127/191], loss=7.0865
	step [128/191], loss=7.7143
	step [129/191], loss=8.6818
	step [130/191], loss=8.2406
	step [131/191], loss=9.0820
	step [132/191], loss=8.7844
	step [133/191], loss=10.3280
	step [134/191], loss=10.6231
	step [135/191], loss=7.9987
	step [136/191], loss=7.8199
	step [137/191], loss=7.8334
	step [138/191], loss=7.4120
	step [139/191], loss=8.5357
	step [140/191], loss=8.8570
	step [141/191], loss=7.3929
	step [142/191], loss=8.8539
	step [143/191], loss=7.9845
	step [144/191], loss=8.9575
	step [145/191], loss=8.4200
	step [146/191], loss=8.3460
	step [147/191], loss=8.4446
	step [148/191], loss=7.7053
	step [149/191], loss=9.0295
	step [150/191], loss=8.1908
	step [151/191], loss=7.3746
	step [152/191], loss=8.1231
	step [153/191], loss=8.4477
	step [154/191], loss=10.1061
	step [155/191], loss=8.3012
	step [156/191], loss=9.5640
	step [157/191], loss=10.2405
	step [158/191], loss=8.6697
	step [159/191], loss=8.1547
	step [160/191], loss=8.5122
	step [161/191], loss=9.7644
	step [162/191], loss=8.4261
	step [163/191], loss=9.4237
	step [164/191], loss=8.2072
	step [165/191], loss=8.7113
	step [166/191], loss=11.3220
	step [167/191], loss=11.1888
	step [168/191], loss=12.5417
	step [169/191], loss=9.3386
	step [170/191], loss=9.2304
	step [171/191], loss=9.0748
	step [172/191], loss=9.0244
	step [173/191], loss=8.5615
	step [174/191], loss=8.6871
	step [175/191], loss=8.1589
	step [176/191], loss=7.8715
	step [177/191], loss=7.5861
	step [178/191], loss=11.7269
	step [179/191], loss=8.8757
	step [180/191], loss=9.4627
	step [181/191], loss=9.1240
	step [182/191], loss=8.0113
	step [183/191], loss=9.1827
	step [184/191], loss=9.8985
	step [185/191], loss=9.0128
	step [186/191], loss=7.7685
	step [187/191], loss=8.6702
	step [188/191], loss=8.5412
	step [189/191], loss=8.1296
	step [190/191], loss=9.1397
	step [191/191], loss=4.2572
	Evaluating
	loss=0.0280, precision=0.2659, recall=0.9808, f1=0.7730
Training epoch 21
	step [1/191], loss=7.6798
	step [2/191], loss=8.3273
	step [3/191], loss=7.8357
	step [4/191], loss=9.0584
	step [5/191], loss=11.1466
	step [6/191], loss=9.3442
	step [7/191], loss=10.6460
	step [8/191], loss=11.1209
	step [9/191], loss=11.3727
	step [10/191], loss=10.1078
	step [11/191], loss=9.5037
	step [12/191], loss=10.7285
	step [13/191], loss=9.0460
	step [14/191], loss=8.7304
	step [15/191], loss=9.9520
	step [16/191], loss=9.8259
	step [17/191], loss=8.7994
	step [18/191], loss=9.0824
	step [19/191], loss=9.5724
	step [20/191], loss=8.2151
	step [21/191], loss=11.1087
	step [22/191], loss=10.4533
	step [23/191], loss=7.8872
	step [24/191], loss=9.8483
	step [25/191], loss=10.9892
	step [26/191], loss=9.4172
	step [27/191], loss=8.5078
	step [28/191], loss=8.5007
	step [29/191], loss=9.8049
	step [30/191], loss=9.1411
	step [31/191], loss=9.0756
	step [32/191], loss=9.2716
	step [33/191], loss=8.8139
	step [34/191], loss=8.8978
	step [35/191], loss=7.7340
	step [36/191], loss=7.5687
	step [37/191], loss=9.0947
	step [38/191], loss=8.2081
	step [39/191], loss=9.9831
	step [40/191], loss=8.7965
	step [41/191], loss=8.3768
	step [42/191], loss=8.5213
	step [43/191], loss=9.8512
	step [44/191], loss=9.7595
	step [45/191], loss=8.2675
	step [46/191], loss=8.2023
	step [47/191], loss=8.6711
	step [48/191], loss=9.1871
	step [49/191], loss=10.2650
	step [50/191], loss=8.8677
	step [51/191], loss=9.9190
	step [52/191], loss=8.2961
	step [53/191], loss=8.4243
	step [54/191], loss=9.0458
	step [55/191], loss=9.4945
	step [56/191], loss=7.5424
	step [57/191], loss=9.6050
	step [58/191], loss=7.6237
	step [59/191], loss=9.4801
	step [60/191], loss=9.5093
	step [61/191], loss=8.2415
	step [62/191], loss=9.5060
	step [63/191], loss=8.2639
	step [64/191], loss=8.9300
	step [65/191], loss=9.1505
	step [66/191], loss=8.5081
	step [67/191], loss=7.8860
	step [68/191], loss=8.1565
	step [69/191], loss=7.8016
	step [70/191], loss=7.8715
	step [71/191], loss=9.0149
	step [72/191], loss=8.7103
	step [73/191], loss=8.6224
	step [74/191], loss=10.9051
	step [75/191], loss=10.5498
	step [76/191], loss=8.9458
	step [77/191], loss=8.6073
	step [78/191], loss=9.2500
	step [79/191], loss=10.0837
	step [80/191], loss=11.5762
	step [81/191], loss=7.8899
	step [82/191], loss=8.4008
	step [83/191], loss=7.4077
	step [84/191], loss=10.6370
	step [85/191], loss=8.2530
	step [86/191], loss=9.4857
	step [87/191], loss=9.6275
	step [88/191], loss=10.1511
	step [89/191], loss=8.2366
	step [90/191], loss=8.2883
	step [91/191], loss=7.9352
	step [92/191], loss=9.6544
	step [93/191], loss=9.9605
	step [94/191], loss=9.3999
	step [95/191], loss=8.0537
	step [96/191], loss=8.5348
	step [97/191], loss=8.2674
	step [98/191], loss=8.6326
	step [99/191], loss=7.9502
	step [100/191], loss=7.9407
	step [101/191], loss=8.9532
	step [102/191], loss=8.1293
	step [103/191], loss=8.0557
	step [104/191], loss=9.1937
	step [105/191], loss=8.8832
	step [106/191], loss=7.2008
	step [107/191], loss=9.3145
	step [108/191], loss=10.6995
	step [109/191], loss=8.6587
	step [110/191], loss=8.8189
	step [111/191], loss=8.9044
	step [112/191], loss=9.1593
	step [113/191], loss=8.2895
	step [114/191], loss=8.3311
	step [115/191], loss=8.3751
	step [116/191], loss=7.8034
	step [117/191], loss=7.7947
	step [118/191], loss=9.5631
	step [119/191], loss=8.3371
	step [120/191], loss=7.6656
	step [121/191], loss=7.4387
	step [122/191], loss=8.6647
	step [123/191], loss=9.5847
	step [124/191], loss=7.7522
	step [125/191], loss=7.9327
	step [126/191], loss=9.0325
	step [127/191], loss=7.0426
	step [128/191], loss=7.3276
	step [129/191], loss=7.8466
	step [130/191], loss=7.3463
	step [131/191], loss=8.8068
	step [132/191], loss=8.4174
	step [133/191], loss=8.3955
	step [134/191], loss=9.6508
	step [135/191], loss=7.7891
	step [136/191], loss=8.1031
	step [137/191], loss=10.2010
	step [138/191], loss=9.0816
	step [139/191], loss=7.7464
	step [140/191], loss=8.5914
	step [141/191], loss=8.7420
	step [142/191], loss=6.6801
	step [143/191], loss=8.2351
	step [144/191], loss=10.2695
	step [145/191], loss=8.1267
	step [146/191], loss=7.2728
	step [147/191], loss=9.4114
	step [148/191], loss=8.5621
	step [149/191], loss=8.2659
	step [150/191], loss=6.8298
	step [151/191], loss=7.0372
	step [152/191], loss=7.4829
	step [153/191], loss=8.2486
	step [154/191], loss=7.8425
	step [155/191], loss=8.4137
	step [156/191], loss=9.3269
	step [157/191], loss=7.7809
	step [158/191], loss=8.2453
	step [159/191], loss=8.8266
	step [160/191], loss=8.7950
	step [161/191], loss=7.6500
	step [162/191], loss=8.0910
	step [163/191], loss=9.1654
	step [164/191], loss=8.4229
	step [165/191], loss=9.3388
	step [166/191], loss=8.8176
	step [167/191], loss=9.0703
	step [168/191], loss=8.0458
	step [169/191], loss=7.6925
	step [170/191], loss=10.4380
	step [171/191], loss=9.9706
	step [172/191], loss=7.9211
	step [173/191], loss=8.3159
	step [174/191], loss=8.6214
	step [175/191], loss=7.2190
	step [176/191], loss=9.5405
	step [177/191], loss=7.6268
	step [178/191], loss=9.1966
	step [179/191], loss=7.2243
	step [180/191], loss=8.2022
	step [181/191], loss=8.2582
	step [182/191], loss=8.6363
	step [183/191], loss=10.0070
	step [184/191], loss=8.7569
	step [185/191], loss=7.6052
	step [186/191], loss=7.9898
	step [187/191], loss=7.2018
	step [188/191], loss=9.0357
	step [189/191], loss=9.2107
	step [190/191], loss=10.0591
	step [191/191], loss=3.4160
	Evaluating
	loss=0.0266, precision=0.2695, recall=0.9813, f1=0.7762
Training epoch 22
	step [1/191], loss=7.7173
	step [2/191], loss=8.4540
	step [3/191], loss=6.9434
	step [4/191], loss=8.7839
	step [5/191], loss=9.0676
	step [6/191], loss=8.1033
	step [7/191], loss=8.2095
	step [8/191], loss=9.3931
	step [9/191], loss=9.4719
	step [10/191], loss=8.6629
	step [11/191], loss=8.7885
	step [12/191], loss=8.3472
	step [13/191], loss=9.1464
	step [14/191], loss=8.7519
	step [15/191], loss=8.4950
	step [16/191], loss=7.5484
	step [17/191], loss=8.0757
	step [18/191], loss=8.1291
	step [19/191], loss=9.1485
	step [20/191], loss=6.8290
	step [21/191], loss=10.2931
	step [22/191], loss=7.4025
	step [23/191], loss=8.3882
	step [24/191], loss=8.8058
	step [25/191], loss=7.5495
	step [26/191], loss=6.9227
	step [27/191], loss=6.7909
	step [28/191], loss=8.7230
	step [29/191], loss=8.1840
	step [30/191], loss=7.2926
	step [31/191], loss=7.9843
	step [32/191], loss=8.1054
	step [33/191], loss=9.1807
	step [34/191], loss=7.9497
	step [35/191], loss=9.5684
	step [36/191], loss=8.7264
	step [37/191], loss=8.4952
	step [38/191], loss=8.0969
	step [39/191], loss=8.5735
	step [40/191], loss=8.2914
	step [41/191], loss=7.7117
	step [42/191], loss=6.8097
	step [43/191], loss=7.9615
	step [44/191], loss=8.5458
	step [45/191], loss=7.2479
	step [46/191], loss=8.0695
	step [47/191], loss=7.8293
	step [48/191], loss=7.9898
	step [49/191], loss=8.2303
	step [50/191], loss=9.4464
	step [51/191], loss=9.8343
	step [52/191], loss=9.3639
	step [53/191], loss=7.9421
	step [54/191], loss=8.3977
	step [55/191], loss=6.6205
	step [56/191], loss=7.5364
	step [57/191], loss=8.1504
	step [58/191], loss=7.8679
	step [59/191], loss=7.7482
	step [60/191], loss=8.2969
	step [61/191], loss=7.8202
	step [62/191], loss=7.5644
	step [63/191], loss=8.7085
	step [64/191], loss=7.1554
	step [65/191], loss=10.2174
	step [66/191], loss=7.6881
	step [67/191], loss=7.4038
	step [68/191], loss=7.9169
	step [69/191], loss=8.2215
	step [70/191], loss=8.4398
	step [71/191], loss=8.6623
	step [72/191], loss=8.6200
	step [73/191], loss=9.5396
	step [74/191], loss=7.8823
	step [75/191], loss=7.5079
	step [76/191], loss=7.8816
	step [77/191], loss=7.2633
	step [78/191], loss=9.3389
	step [79/191], loss=8.2414
	step [80/191], loss=7.9818
	step [81/191], loss=10.1032
	step [82/191], loss=7.7458
	step [83/191], loss=7.8951
	step [84/191], loss=7.8723
	step [85/191], loss=7.4954
	step [86/191], loss=8.5986
	step [87/191], loss=7.9300
	step [88/191], loss=7.7836
	step [89/191], loss=8.3643
	step [90/191], loss=7.8354
	step [91/191], loss=7.4785
	step [92/191], loss=9.2196
	step [93/191], loss=7.4871
	step [94/191], loss=7.6953
	step [95/191], loss=7.0806
	step [96/191], loss=8.1423
	step [97/191], loss=8.1113
	step [98/191], loss=7.4884
	step [99/191], loss=7.9286
	step [100/191], loss=7.8157
	step [101/191], loss=7.1903
	step [102/191], loss=6.9036
	step [103/191], loss=8.5200
	step [104/191], loss=10.2359
	step [105/191], loss=8.1101
	step [106/191], loss=6.4816
	step [107/191], loss=8.1971
	step [108/191], loss=8.3865
	step [109/191], loss=8.2691
	step [110/191], loss=9.6640
	step [111/191], loss=9.0653
	step [112/191], loss=8.2150
	step [113/191], loss=7.5554
	step [114/191], loss=9.6996
	step [115/191], loss=7.5506
	step [116/191], loss=7.6883
	step [117/191], loss=8.2832
	step [118/191], loss=7.3591
	step [119/191], loss=8.3056
	step [120/191], loss=7.7281
	step [121/191], loss=7.3539
	step [122/191], loss=8.2956
	step [123/191], loss=9.3304
	step [124/191], loss=7.2780
	step [125/191], loss=7.9132
	step [126/191], loss=7.5474
	step [127/191], loss=8.1425
	step [128/191], loss=7.2894
	step [129/191], loss=7.6829
	step [130/191], loss=7.5306
	step [131/191], loss=8.7637
	step [132/191], loss=8.2270
	step [133/191], loss=6.7074
	step [134/191], loss=7.8576
	step [135/191], loss=8.9311
	step [136/191], loss=8.3273
	step [137/191], loss=7.2240
	step [138/191], loss=7.4186
	step [139/191], loss=8.7264
	step [140/191], loss=8.6074
	step [141/191], loss=7.1502
	step [142/191], loss=8.4039
	step [143/191], loss=7.4273
	step [144/191], loss=7.8167
	step [145/191], loss=8.3056
	step [146/191], loss=7.6756
	step [147/191], loss=6.0932
	step [148/191], loss=8.0875
	step [149/191], loss=7.4837
	step [150/191], loss=7.4297
	step [151/191], loss=7.1645
	step [152/191], loss=7.3161
	step [153/191], loss=7.0186
	step [154/191], loss=8.2368
	step [155/191], loss=8.0078
	step [156/191], loss=7.5152
	step [157/191], loss=7.0995
	step [158/191], loss=6.9471
	step [159/191], loss=7.3731
	step [160/191], loss=8.4923
	step [161/191], loss=6.9214
	step [162/191], loss=6.5813
	step [163/191], loss=8.3541
	step [164/191], loss=6.7252
	step [165/191], loss=8.0101
	step [166/191], loss=8.9112
	step [167/191], loss=6.9504
	step [168/191], loss=8.2097
	step [169/191], loss=6.6218
	step [170/191], loss=7.7888
	step [171/191], loss=7.6547
	step [172/191], loss=8.1418
	step [173/191], loss=8.8269
	step [174/191], loss=9.3869
	step [175/191], loss=7.5361
	step [176/191], loss=9.1078
	step [177/191], loss=8.7842
	step [178/191], loss=7.5749
	step [179/191], loss=7.3260
	step [180/191], loss=7.8417
	step [181/191], loss=7.0069
	step [182/191], loss=8.4583
	step [183/191], loss=8.7235
	step [184/191], loss=7.0663
	step [185/191], loss=7.9569
	step [186/191], loss=7.3520
	step [187/191], loss=9.0129
	step [188/191], loss=7.7742
	step [189/191], loss=8.8447
	step [190/191], loss=9.9814
	step [191/191], loss=3.2314
	Evaluating
	loss=0.0295, precision=0.2329, recall=0.9836, f1=0.7438
Training epoch 23
	step [1/191], loss=8.4383
	step [2/191], loss=7.3355
	step [3/191], loss=7.7912
	step [4/191], loss=6.8866
	step [5/191], loss=8.5399
	step [6/191], loss=8.3990
	step [7/191], loss=7.6242
	step [8/191], loss=7.8445
	step [9/191], loss=6.7828
	step [10/191], loss=8.2399
	step [11/191], loss=7.5470
	step [12/191], loss=7.2263
	step [13/191], loss=7.5406
	step [14/191], loss=9.0194
	step [15/191], loss=8.0097
	step [16/191], loss=8.0077
	step [17/191], loss=8.1685
	step [18/191], loss=7.9484
	step [19/191], loss=9.0751
	step [20/191], loss=7.9184
	step [21/191], loss=9.5584
	step [22/191], loss=8.6965
	step [23/191], loss=8.1986
	step [24/191], loss=9.1523
	step [25/191], loss=8.4586
	step [26/191], loss=8.0061
	step [27/191], loss=6.8565
	step [28/191], loss=9.1566
	step [29/191], loss=7.6139
	step [30/191], loss=7.6931
	step [31/191], loss=7.8469
	step [32/191], loss=7.6498
	step [33/191], loss=9.7756
	step [34/191], loss=7.3769
	step [35/191], loss=8.3584
	step [36/191], loss=6.4483
	step [37/191], loss=8.6747
	step [38/191], loss=7.4168
	step [39/191], loss=7.0788
	step [40/191], loss=8.1512
	step [41/191], loss=6.9170
	step [42/191], loss=6.7823
	step [43/191], loss=6.6552
	step [44/191], loss=8.1773
	step [45/191], loss=6.7382
	step [46/191], loss=7.3884
	step [47/191], loss=7.6417
	step [48/191], loss=8.3437
	step [49/191], loss=7.3247
	step [50/191], loss=6.6808
	step [51/191], loss=7.9163
	step [52/191], loss=8.3765
	step [53/191], loss=7.2270
	step [54/191], loss=6.7854
	step [55/191], loss=7.2925
	step [56/191], loss=8.0522
	step [57/191], loss=7.9813
	step [58/191], loss=6.6834
	step [59/191], loss=7.3537
	step [60/191], loss=8.0537
	step [61/191], loss=6.7815
	step [62/191], loss=7.9925
	step [63/191], loss=7.5565
	step [64/191], loss=8.2054
	step [65/191], loss=8.1936
	step [66/191], loss=7.7655
	step [67/191], loss=6.5795
	step [68/191], loss=8.2184
	step [69/191], loss=8.3020
	step [70/191], loss=9.5142
	step [71/191], loss=6.9912
	step [72/191], loss=9.0306
	step [73/191], loss=7.7027
	step [74/191], loss=7.8782
	step [75/191], loss=7.2572
	step [76/191], loss=8.1561
	step [77/191], loss=7.8640
	step [78/191], loss=7.2291
	step [79/191], loss=10.2443
	step [80/191], loss=8.0383
	step [81/191], loss=7.1339
	step [82/191], loss=8.3793
	step [83/191], loss=7.1084
	step [84/191], loss=8.1554
	step [85/191], loss=7.4300
	step [86/191], loss=7.9881
	step [87/191], loss=7.7542
	step [88/191], loss=7.8954
	step [89/191], loss=7.7071
	step [90/191], loss=9.1749
	step [91/191], loss=7.4571
	step [92/191], loss=8.0894
	step [93/191], loss=7.5961
	step [94/191], loss=7.3987
	step [95/191], loss=7.9148
	step [96/191], loss=6.5997
	step [97/191], loss=6.8378
	step [98/191], loss=8.4913
	step [99/191], loss=7.6985
	step [100/191], loss=7.7967
	step [101/191], loss=7.4889
	step [102/191], loss=7.8233
	step [103/191], loss=8.5697
	step [104/191], loss=6.7895
	step [105/191], loss=7.7363
	step [106/191], loss=6.3199
	step [107/191], loss=6.8337
	step [108/191], loss=7.6317
	step [109/191], loss=8.8065
	step [110/191], loss=8.4029
	step [111/191], loss=7.0086
	step [112/191], loss=7.7391
	step [113/191], loss=7.8173
	step [114/191], loss=7.5053
	step [115/191], loss=8.8466
	step [116/191], loss=7.1036
	step [117/191], loss=6.6804
	step [118/191], loss=6.8256
	step [119/191], loss=8.1895
	step [120/191], loss=8.2994
	step [121/191], loss=7.6072
	step [122/191], loss=8.1585
	step [123/191], loss=6.8026
	step [124/191], loss=8.1620
	step [125/191], loss=6.4713
	step [126/191], loss=8.0708
	step [127/191], loss=7.8524
	step [128/191], loss=6.2971
	step [129/191], loss=6.2537
	step [130/191], loss=7.3218
	step [131/191], loss=7.1272
	step [132/191], loss=9.0653
	step [133/191], loss=7.6843
	step [134/191], loss=9.1312
	step [135/191], loss=8.0520
	step [136/191], loss=8.1803
	step [137/191], loss=6.3423
	step [138/191], loss=7.9720
	step [139/191], loss=7.0711
	step [140/191], loss=7.9362
	step [141/191], loss=8.4581
	step [142/191], loss=6.7347
	step [143/191], loss=6.7921
	step [144/191], loss=8.1622
	step [145/191], loss=7.1713
	step [146/191], loss=6.7574
	step [147/191], loss=8.8514
	step [148/191], loss=6.5188
	step [149/191], loss=7.2761
	step [150/191], loss=7.2010
	step [151/191], loss=8.1971
	step [152/191], loss=9.7879
	step [153/191], loss=7.8674
	step [154/191], loss=6.9052
	step [155/191], loss=7.7777
	step [156/191], loss=8.4363
	step [157/191], loss=8.6349
	step [158/191], loss=7.7312
	step [159/191], loss=8.2470
	step [160/191], loss=6.5920
	step [161/191], loss=7.6416
	step [162/191], loss=7.0696
	step [163/191], loss=6.3366
	step [164/191], loss=6.8832
	step [165/191], loss=7.7069
	step [166/191], loss=8.0385
	step [167/191], loss=7.4378
	step [168/191], loss=8.4417
	step [169/191], loss=6.7284
	step [170/191], loss=6.4820
	step [171/191], loss=7.2047
	step [172/191], loss=6.9060
	step [173/191], loss=7.2917
	step [174/191], loss=7.9697
	step [175/191], loss=8.5719
	step [176/191], loss=7.5463
	step [177/191], loss=7.5037
	step [178/191], loss=6.7184
	step [179/191], loss=6.4142
	step [180/191], loss=6.5313
	step [181/191], loss=7.1752
	step [182/191], loss=7.5260
	step [183/191], loss=8.1593
	step [184/191], loss=9.8984
	step [185/191], loss=7.8392
	step [186/191], loss=8.3500
	step [187/191], loss=8.1308
	step [188/191], loss=8.4837
	step [189/191], loss=8.4196
	step [190/191], loss=7.4588
	step [191/191], loss=3.4561
	Evaluating
	loss=0.0307, precision=0.2260, recall=0.9834, f1=0.7365
Training epoch 24
	step [1/191], loss=6.2519
	step [2/191], loss=7.2977
	step [3/191], loss=7.0365
	step [4/191], loss=8.0927
	step [5/191], loss=8.0677
	step [6/191], loss=7.2109
	step [7/191], loss=6.7772
	step [8/191], loss=6.7163
	step [9/191], loss=7.3191
	step [10/191], loss=7.3455
	step [11/191], loss=8.4184
	step [12/191], loss=8.5920
	step [13/191], loss=8.0893
	step [14/191], loss=6.8740
	step [15/191], loss=8.1214
	step [16/191], loss=7.4600
	step [17/191], loss=7.3215
	step [18/191], loss=7.2811
	step [19/191], loss=8.6082
	step [20/191], loss=6.7773
	step [21/191], loss=7.2411
	step [22/191], loss=7.3272
	step [23/191], loss=8.6149
	step [24/191], loss=6.9884
	step [25/191], loss=7.7606
	step [26/191], loss=7.1195
	step [27/191], loss=7.2418
	step [28/191], loss=6.9754
	step [29/191], loss=6.8193
	step [30/191], loss=8.2739
	step [31/191], loss=8.9403
	step [32/191], loss=8.4267
	step [33/191], loss=7.0134
	step [34/191], loss=5.9769
	step [35/191], loss=7.1157
	step [36/191], loss=7.6073
	step [37/191], loss=6.6366
	step [38/191], loss=8.1784
	step [39/191], loss=8.5911
	step [40/191], loss=6.7212
	step [41/191], loss=7.9965
	step [42/191], loss=6.2984
	step [43/191], loss=7.2072
	step [44/191], loss=7.0168
	step [45/191], loss=7.7068
	step [46/191], loss=7.5501
	step [47/191], loss=8.5430
	step [48/191], loss=7.3528
	step [49/191], loss=7.2774
	step [50/191], loss=6.8242
	step [51/191], loss=6.6114
	step [52/191], loss=7.6428
	step [53/191], loss=7.2225
	step [54/191], loss=7.4855
	step [55/191], loss=7.3316
	step [56/191], loss=8.6088
	step [57/191], loss=6.5563
	step [58/191], loss=7.6617
	step [59/191], loss=9.0198
	step [60/191], loss=7.6135
	step [61/191], loss=6.6153
	step [62/191], loss=6.4192
	step [63/191], loss=7.8471
	step [64/191], loss=6.8400
	step [65/191], loss=7.1622
	step [66/191], loss=7.7233
	step [67/191], loss=7.0381
	step [68/191], loss=5.8882
	step [69/191], loss=6.6223
	step [70/191], loss=7.6203
	step [71/191], loss=8.4837
	step [72/191], loss=8.4716
	step [73/191], loss=8.3463
	step [74/191], loss=8.1261
	step [75/191], loss=6.4860
	step [76/191], loss=7.3402
	step [77/191], loss=7.7104
	step [78/191], loss=7.5439
	step [79/191], loss=7.7860
	step [80/191], loss=8.0069
	step [81/191], loss=8.3355
	step [82/191], loss=7.0159
	step [83/191], loss=7.3555
	step [84/191], loss=7.7059
	step [85/191], loss=8.9205
	step [86/191], loss=7.5788
	step [87/191], loss=7.1221
	step [88/191], loss=7.5583
	step [89/191], loss=7.1889
	step [90/191], loss=7.4119
	step [91/191], loss=8.1989
	step [92/191], loss=7.9348
	step [93/191], loss=7.0899
	step [94/191], loss=7.4843
	step [95/191], loss=8.6569
	step [96/191], loss=5.4792
	step [97/191], loss=7.9988
	step [98/191], loss=6.4457
	step [99/191], loss=9.6142
	step [100/191], loss=7.6528
	step [101/191], loss=6.6339
	step [102/191], loss=8.2091
	step [103/191], loss=8.4030
	step [104/191], loss=8.0381
	step [105/191], loss=7.6185
	step [106/191], loss=6.6447
	step [107/191], loss=7.3591
	step [108/191], loss=7.5103
	step [109/191], loss=8.4644
	step [110/191], loss=7.0490
	step [111/191], loss=7.3667
	step [112/191], loss=6.4374
	step [113/191], loss=7.4424
	step [114/191], loss=7.1334
	step [115/191], loss=10.1020
	step [116/191], loss=7.2947
	step [117/191], loss=7.1375
	step [118/191], loss=7.6925
	step [119/191], loss=8.8052
	step [120/191], loss=6.9656
	step [121/191], loss=7.1695
	step [122/191], loss=8.1751
	step [123/191], loss=6.9710
	step [124/191], loss=6.3504
	step [125/191], loss=7.4960
	step [126/191], loss=7.7948
	step [127/191], loss=7.5447
	step [128/191], loss=8.0119
	step [129/191], loss=7.0684
	step [130/191], loss=8.9040
	step [131/191], loss=7.3759
	step [132/191], loss=8.0774
	step [133/191], loss=7.4074
	step [134/191], loss=6.7327
	step [135/191], loss=7.8116
	step [136/191], loss=6.9616
	step [137/191], loss=7.6479
	step [138/191], loss=7.7217
	step [139/191], loss=7.2624
	step [140/191], loss=6.8241
	step [141/191], loss=7.4445
	step [142/191], loss=6.9318
	step [143/191], loss=7.5860
	step [144/191], loss=7.3926
	step [145/191], loss=6.5569
	step [146/191], loss=8.1475
	step [147/191], loss=7.7361
	step [148/191], loss=8.0238
	step [149/191], loss=6.9984
	step [150/191], loss=7.0868
	step [151/191], loss=7.8359
	step [152/191], loss=6.6056
	step [153/191], loss=8.6367
	step [154/191], loss=7.4916
	step [155/191], loss=6.5206
	step [156/191], loss=7.5122
	step [157/191], loss=5.2617
	step [158/191], loss=6.3407
	step [159/191], loss=6.2094
	step [160/191], loss=7.3410
	step [161/191], loss=5.7924
	step [162/191], loss=6.9604
	step [163/191], loss=7.7415
	step [164/191], loss=6.8728
	step [165/191], loss=6.6348
	step [166/191], loss=7.3644
	step [167/191], loss=7.2730
	step [168/191], loss=6.7881
	step [169/191], loss=7.4927
	step [170/191], loss=7.6586
	step [171/191], loss=8.2414
	step [172/191], loss=7.1478
	step [173/191], loss=6.5859
	step [174/191], loss=6.6287
	step [175/191], loss=6.4799
	step [176/191], loss=7.2645
	step [177/191], loss=7.0287
	step [178/191], loss=6.4698
	step [179/191], loss=7.9516
	step [180/191], loss=8.2434
	step [181/191], loss=8.1611
	step [182/191], loss=6.6169
	step [183/191], loss=6.6109
	step [184/191], loss=6.8025
	step [185/191], loss=7.2041
	step [186/191], loss=7.2030
	step [187/191], loss=5.9210
	step [188/191], loss=8.9317
	step [189/191], loss=7.7754
	step [190/191], loss=6.6810
	step [191/191], loss=3.4097
	Evaluating
	loss=0.0253, precision=0.2589, recall=0.9814, f1=0.7673
Training epoch 25
	step [1/191], loss=6.8491
	step [2/191], loss=7.4832
	step [3/191], loss=7.6416
	step [4/191], loss=7.4514
	step [5/191], loss=7.1449
	step [6/191], loss=8.3059
	step [7/191], loss=6.7397
	step [8/191], loss=7.8676
	step [9/191], loss=6.9972
	step [10/191], loss=6.5620
	step [11/191], loss=6.4776
	step [12/191], loss=7.0220
	step [13/191], loss=8.6179
	step [14/191], loss=6.1185
	step [15/191], loss=7.7986
	step [16/191], loss=9.0265
	step [17/191], loss=6.4939
	step [18/191], loss=7.3142
	step [19/191], loss=8.2574
	step [20/191], loss=6.5708
	step [21/191], loss=6.2174
	step [22/191], loss=6.1939
	step [23/191], loss=8.7544
	step [24/191], loss=6.6653
	step [25/191], loss=8.1528
	step [26/191], loss=6.5610
	step [27/191], loss=7.7613
	step [28/191], loss=7.7663
	step [29/191], loss=6.5883
	step [30/191], loss=7.9456
	step [31/191], loss=6.6231
	step [32/191], loss=8.0837
	step [33/191], loss=8.4734
	step [34/191], loss=7.3403
	step [35/191], loss=7.2806
	step [36/191], loss=7.8524
	step [37/191], loss=7.0837
	step [38/191], loss=7.8758
	step [39/191], loss=7.4311
	step [40/191], loss=6.4871
	step [41/191], loss=6.5940
	step [42/191], loss=7.2514
	step [43/191], loss=7.0115
	step [44/191], loss=6.9136
	step [45/191], loss=7.6004
	step [46/191], loss=8.3205
	step [47/191], loss=8.0008
	step [48/191], loss=6.3199
	step [49/191], loss=6.4916
	step [50/191], loss=7.0923
	step [51/191], loss=6.7529
	step [52/191], loss=6.6841
	step [53/191], loss=8.0219
	step [54/191], loss=6.2563
	step [55/191], loss=7.1496
	step [56/191], loss=6.4086
	step [57/191], loss=7.4852
	step [58/191], loss=7.5677
	step [59/191], loss=7.7407
	step [60/191], loss=8.7041
	step [61/191], loss=7.8582
	step [62/191], loss=9.8906
	step [63/191], loss=7.2429
	step [64/191], loss=6.9454
	step [65/191], loss=7.1073
	step [66/191], loss=6.1746
	step [67/191], loss=6.9283
	step [68/191], loss=7.3547
	step [69/191], loss=5.3621
	step [70/191], loss=6.5620
	step [71/191], loss=7.6159
	step [72/191], loss=7.8306
	step [73/191], loss=7.4906
	step [74/191], loss=7.3182
	step [75/191], loss=7.0005
	step [76/191], loss=7.4090
	step [77/191], loss=8.2526
	step [78/191], loss=8.1151
	step [79/191], loss=6.8234
	step [80/191], loss=7.5805
	step [81/191], loss=7.3982
	step [82/191], loss=6.9470
	step [83/191], loss=7.1583
	step [84/191], loss=7.3734
	step [85/191], loss=7.1883
	step [86/191], loss=7.1640
	step [87/191], loss=6.4533
	step [88/191], loss=7.2307
	step [89/191], loss=8.0140
	step [90/191], loss=7.1122
	step [91/191], loss=6.9843
	step [92/191], loss=6.4533
	step [93/191], loss=7.2610
	step [94/191], loss=6.2119
	step [95/191], loss=7.2208
	step [96/191], loss=5.9203
	step [97/191], loss=6.5641
	step [98/191], loss=7.2795
	step [99/191], loss=6.9145
	step [100/191], loss=6.7316
	step [101/191], loss=8.3629
	step [102/191], loss=7.8495
	step [103/191], loss=7.2556
	step [104/191], loss=6.9754
	step [105/191], loss=7.5393
	step [106/191], loss=6.6167
	step [107/191], loss=7.2038
	step [108/191], loss=7.0925
	step [109/191], loss=8.6506
	step [110/191], loss=8.2221
	step [111/191], loss=7.7959
	step [112/191], loss=7.8377
	step [113/191], loss=6.8812
	step [114/191], loss=7.3834
	step [115/191], loss=7.6780
	step [116/191], loss=6.7506
	step [117/191], loss=5.8781
	step [118/191], loss=5.8628
	step [119/191], loss=7.0945
	step [120/191], loss=6.5687
	step [121/191], loss=7.4468
	step [122/191], loss=6.9683
	step [123/191], loss=6.8593
	step [124/191], loss=7.4814
	step [125/191], loss=7.2977
	step [126/191], loss=6.2331
	step [127/191], loss=5.9581
	step [128/191], loss=6.4310
	step [129/191], loss=6.5790
	step [130/191], loss=8.2568
	step [131/191], loss=6.8542
	step [132/191], loss=8.1550
	step [133/191], loss=5.9712
	step [134/191], loss=7.0215
	step [135/191], loss=6.5168
	step [136/191], loss=6.4863
	step [137/191], loss=6.5152
	step [138/191], loss=7.1774
	step [139/191], loss=7.2567
	step [140/191], loss=6.1421
	step [141/191], loss=6.9644
	step [142/191], loss=5.5801
	step [143/191], loss=8.1678
	step [144/191], loss=8.3025
	step [145/191], loss=6.7870
	step [146/191], loss=9.0352
	step [147/191], loss=6.1694
	step [148/191], loss=7.3542
	step [149/191], loss=6.1716
	step [150/191], loss=8.1274
	step [151/191], loss=7.1652
	step [152/191], loss=6.2690
	step [153/191], loss=8.1438
	step [154/191], loss=7.6175
	step [155/191], loss=6.2962
	step [156/191], loss=7.3941
	step [157/191], loss=7.1418
	step [158/191], loss=7.4791
	step [159/191], loss=6.4521
	step [160/191], loss=6.8075
	step [161/191], loss=6.2766
	step [162/191], loss=6.1801
	step [163/191], loss=8.7727
	step [164/191], loss=6.8587
	step [165/191], loss=8.4232
	step [166/191], loss=7.1623
	step [167/191], loss=6.9363
	step [168/191], loss=6.5942
	step [169/191], loss=7.0816
	step [170/191], loss=6.8733
	step [171/191], loss=6.6668
	step [172/191], loss=7.2772
	step [173/191], loss=8.1848
	step [174/191], loss=7.1361
	step [175/191], loss=6.7033
	step [176/191], loss=7.3471
	step [177/191], loss=8.1352
	step [178/191], loss=7.5760
	step [179/191], loss=7.0651
	step [180/191], loss=7.8299
	step [181/191], loss=7.2105
	step [182/191], loss=7.3759
	step [183/191], loss=6.6450
	step [184/191], loss=6.7972
	step [185/191], loss=6.7542
	step [186/191], loss=6.4661
	step [187/191], loss=7.1451
	step [188/191], loss=7.7249
	step [189/191], loss=5.9846
	step [190/191], loss=7.4500
	step [191/191], loss=3.2956
	Evaluating
	loss=0.0223, precision=0.2870, recall=0.9797, f1=0.7892
saving model as: 1_saved_model.pth
Training epoch 26
	step [1/191], loss=7.8493
	step [2/191], loss=8.8606
	step [3/191], loss=6.2929
	step [4/191], loss=5.7202
	step [5/191], loss=6.0122
	step [6/191], loss=9.1240
	step [7/191], loss=6.2450
	step [8/191], loss=7.5340
	step [9/191], loss=6.9147
	step [10/191], loss=7.1516
	step [11/191], loss=8.3532
	step [12/191], loss=9.0659
	step [13/191], loss=7.2451
	step [14/191], loss=7.1260
	step [15/191], loss=6.6382
	step [16/191], loss=7.4319
	step [17/191], loss=6.6262
	step [18/191], loss=6.5187
	step [19/191], loss=7.0744
	step [20/191], loss=6.9131
	step [21/191], loss=6.8455
	step [22/191], loss=6.3873
	step [23/191], loss=5.3202
	step [24/191], loss=6.7536
	step [25/191], loss=7.1194
	step [26/191], loss=7.7438
	step [27/191], loss=5.9595
	step [28/191], loss=5.8434
	step [29/191], loss=7.6681
	step [30/191], loss=7.6898
	step [31/191], loss=6.8164
	step [32/191], loss=6.0764
	step [33/191], loss=8.1992
	step [34/191], loss=6.6994
	step [35/191], loss=8.3179
	step [36/191], loss=6.4925
	step [37/191], loss=7.1074
	step [38/191], loss=6.9186
	step [39/191], loss=7.6659
	step [40/191], loss=6.8578
	step [41/191], loss=7.7774
	step [42/191], loss=6.6819
	step [43/191], loss=6.3258
	step [44/191], loss=6.7583
	step [45/191], loss=6.7801
	step [46/191], loss=6.4030
	step [47/191], loss=6.1692
	step [48/191], loss=7.0883
	step [49/191], loss=6.3684
	step [50/191], loss=7.1819
	step [51/191], loss=6.6595
	step [52/191], loss=7.2887
	step [53/191], loss=7.0119
	step [54/191], loss=6.4754
	step [55/191], loss=6.8508
	step [56/191], loss=6.1274
	step [57/191], loss=6.2327
	step [58/191], loss=6.5105
	step [59/191], loss=5.5662
	step [60/191], loss=9.9539
	step [61/191], loss=6.9600
	step [62/191], loss=6.4509
	step [63/191], loss=7.3660
	step [64/191], loss=6.2941
	step [65/191], loss=7.0959
	step [66/191], loss=7.4426
	step [67/191], loss=6.4482
	step [68/191], loss=7.1257
	step [69/191], loss=6.8133
	step [70/191], loss=7.7639
	step [71/191], loss=6.5444
	step [72/191], loss=6.6356
	step [73/191], loss=6.9063
	step [74/191], loss=6.3321
	step [75/191], loss=7.6367
	step [76/191], loss=6.7810
	step [77/191], loss=8.0708
	step [78/191], loss=7.2229
	step [79/191], loss=6.4633
	step [80/191], loss=7.5787
	step [81/191], loss=7.2996
	step [82/191], loss=7.1144
	step [83/191], loss=6.3727
	step [84/191], loss=7.0115
	step [85/191], loss=6.5218
	step [86/191], loss=6.7275
	step [87/191], loss=7.2459
	step [88/191], loss=7.8361
	step [89/191], loss=6.9608
	step [90/191], loss=6.5721
	step [91/191], loss=6.9842
	step [92/191], loss=6.0151
	step [93/191], loss=5.4544
	step [94/191], loss=6.2024
	step [95/191], loss=6.2691
	step [96/191], loss=6.7198
	step [97/191], loss=6.8455
	step [98/191], loss=5.7580
	step [99/191], loss=7.0943
	step [100/191], loss=7.4937
	step [101/191], loss=6.2139
	step [102/191], loss=7.6740
	step [103/191], loss=6.4660
	step [104/191], loss=6.2624
	step [105/191], loss=6.1438
	step [106/191], loss=7.9438
	step [107/191], loss=6.8091
	step [108/191], loss=5.5550
	step [109/191], loss=7.4271
	step [110/191], loss=6.9853
	step [111/191], loss=6.1985
	step [112/191], loss=7.1103
	step [113/191], loss=6.8583
	step [114/191], loss=5.8187
	step [115/191], loss=6.1372
	step [116/191], loss=6.8723
	step [117/191], loss=5.1102
	step [118/191], loss=6.9261
	step [119/191], loss=8.6619
	step [120/191], loss=6.8768
	step [121/191], loss=6.6813
	step [122/191], loss=6.1868
	step [123/191], loss=7.2566
	step [124/191], loss=6.6900
	step [125/191], loss=6.0172
	step [126/191], loss=6.4172
	step [127/191], loss=8.0908
	step [128/191], loss=8.1746
	step [129/191], loss=6.4010
	step [130/191], loss=7.9391
	step [131/191], loss=7.1477
	step [132/191], loss=6.3720
	step [133/191], loss=6.5548
	step [134/191], loss=7.8236
	step [135/191], loss=7.1382
	step [136/191], loss=5.8709
	step [137/191], loss=6.1258
	step [138/191], loss=7.5626
	step [139/191], loss=7.6999
	step [140/191], loss=7.2622
	step [141/191], loss=6.2825
	step [142/191], loss=6.3412
	step [143/191], loss=7.6179
	step [144/191], loss=6.7958
	step [145/191], loss=6.8490
	step [146/191], loss=6.8761
	step [147/191], loss=6.3104
	step [148/191], loss=6.5236
	step [149/191], loss=6.4428
	step [150/191], loss=6.4030
	step [151/191], loss=6.6363
	step [152/191], loss=8.0837
	step [153/191], loss=6.4876
	step [154/191], loss=8.3050
	step [155/191], loss=6.4603
	step [156/191], loss=7.3742
	step [157/191], loss=7.8771
	step [158/191], loss=6.9579
	step [159/191], loss=6.8698
	step [160/191], loss=7.2861
	step [161/191], loss=6.5364
	step [162/191], loss=6.5111
	step [163/191], loss=6.6524
	step [164/191], loss=7.0210
	step [165/191], loss=5.9898
	step [166/191], loss=8.3138
	step [167/191], loss=7.5603
	step [168/191], loss=7.7967
	step [169/191], loss=7.1470
	step [170/191], loss=6.4185
	step [171/191], loss=7.4752
	step [172/191], loss=6.1976
	step [173/191], loss=6.9134
	step [174/191], loss=5.9983
	step [175/191], loss=5.9645
	step [176/191], loss=6.0655
	step [177/191], loss=7.7454
	step [178/191], loss=7.8905
	step [179/191], loss=6.6844
	step [180/191], loss=6.6762
	step [181/191], loss=7.0434
	step [182/191], loss=6.8420
	step [183/191], loss=5.9702
	step [184/191], loss=6.6437
	step [185/191], loss=5.7150
	step [186/191], loss=6.5288
	step [187/191], loss=7.3789
	step [188/191], loss=6.6876
	step [189/191], loss=7.0622
	step [190/191], loss=7.1625
	step [191/191], loss=3.1169
	Evaluating
	loss=0.0241, precision=0.2823, recall=0.9797, f1=0.7856
Training epoch 27
	step [1/191], loss=7.1485
	step [2/191], loss=6.2771
	step [3/191], loss=7.6282
	step [4/191], loss=6.8780
	step [5/191], loss=6.9052
	step [6/191], loss=7.1114
	step [7/191], loss=7.3932
	step [8/191], loss=6.3937
	step [9/191], loss=7.2805
	step [10/191], loss=5.8386
	step [11/191], loss=7.3121
	step [12/191], loss=6.3952
	step [13/191], loss=7.4817
	step [14/191], loss=7.1415
	step [15/191], loss=7.0011
	step [16/191], loss=7.1151
	step [17/191], loss=5.7199
	step [18/191], loss=7.3837
	step [19/191], loss=7.0602
	step [20/191], loss=6.7121
	step [21/191], loss=6.3125
	step [22/191], loss=5.4676
	step [23/191], loss=6.8453
	step [24/191], loss=6.8055
	step [25/191], loss=7.5471
	step [26/191], loss=7.6747
	step [27/191], loss=7.0725
	step [28/191], loss=7.3579
	step [29/191], loss=6.7619
	step [30/191], loss=7.7486
	step [31/191], loss=6.8729
	step [32/191], loss=6.3526
	step [33/191], loss=6.2797
	step [34/191], loss=5.7334
	step [35/191], loss=6.1866
	step [36/191], loss=7.7260
	step [37/191], loss=6.5215
	step [38/191], loss=7.1207
	step [39/191], loss=6.5469
	step [40/191], loss=6.4456
	step [41/191], loss=5.7084
	step [42/191], loss=6.6434
	step [43/191], loss=7.2823
	step [44/191], loss=6.3606
	step [45/191], loss=6.5151
	step [46/191], loss=6.1734
	step [47/191], loss=7.5278
	step [48/191], loss=6.9335
	step [49/191], loss=7.3515
	step [50/191], loss=6.5213
	step [51/191], loss=6.8681
	step [52/191], loss=6.5713
	step [53/191], loss=7.7908
	step [54/191], loss=5.9595
	step [55/191], loss=7.3745
	step [56/191], loss=6.5359
	step [57/191], loss=6.6778
	step [58/191], loss=5.7042
	step [59/191], loss=7.6807
	step [60/191], loss=5.8154
	step [61/191], loss=7.1080
	step [62/191], loss=5.4764
	step [63/191], loss=6.5906
	step [64/191], loss=6.1781
	step [65/191], loss=6.7800
	step [66/191], loss=6.4034
	step [67/191], loss=6.7201
	step [68/191], loss=7.0374
	step [69/191], loss=6.5023
	step [70/191], loss=6.7844
	step [71/191], loss=6.5090
	step [72/191], loss=6.0054
	step [73/191], loss=6.7292
	step [74/191], loss=6.2144
	step [75/191], loss=7.2947
	step [76/191], loss=5.8235
	step [77/191], loss=8.1870
	step [78/191], loss=6.2432
	step [79/191], loss=6.0248
	step [80/191], loss=6.3953
	step [81/191], loss=7.2660
	step [82/191], loss=6.7761
	step [83/191], loss=6.7781
	step [84/191], loss=7.8509
	step [85/191], loss=6.5132
	step [86/191], loss=8.9467
	step [87/191], loss=6.0232
	step [88/191], loss=6.7405
	step [89/191], loss=7.2316
	step [90/191], loss=6.6225
	step [91/191], loss=6.4333
	step [92/191], loss=7.9852
	step [93/191], loss=7.3314
	step [94/191], loss=5.7781
	step [95/191], loss=7.3960
	step [96/191], loss=6.6779
	step [97/191], loss=7.1988
	step [98/191], loss=6.9898
	step [99/191], loss=5.8821
	step [100/191], loss=7.4675
	step [101/191], loss=6.3140
	step [102/191], loss=6.2714
	step [103/191], loss=6.7929
	step [104/191], loss=7.2853
	step [105/191], loss=4.9706
	step [106/191], loss=6.5962
	step [107/191], loss=5.2343
	step [108/191], loss=6.5077
	step [109/191], loss=6.0379
	step [110/191], loss=7.2693
	step [111/191], loss=7.3412
	step [112/191], loss=6.1465
	step [113/191], loss=6.7942
	step [114/191], loss=6.7417
	step [115/191], loss=7.4254
	step [116/191], loss=5.6947
	step [117/191], loss=7.3258
	step [118/191], loss=7.5832
	step [119/191], loss=6.9036
	step [120/191], loss=6.2548
	step [121/191], loss=6.9245
	step [122/191], loss=5.9096
	step [123/191], loss=6.6654
	step [124/191], loss=6.8691
	step [125/191], loss=7.0809
	step [126/191], loss=7.1444
	step [127/191], loss=7.4744
	step [128/191], loss=6.8880
	step [129/191], loss=5.4502
	step [130/191], loss=5.8328
	step [131/191], loss=7.7725
	step [132/191], loss=8.1935
	step [133/191], loss=7.2939
	step [134/191], loss=6.0885
	step [135/191], loss=6.5987
	step [136/191], loss=6.7139
	step [137/191], loss=6.7704
	step [138/191], loss=6.4274
	step [139/191], loss=5.7463
	step [140/191], loss=7.1469
	step [141/191], loss=7.2876
	step [142/191], loss=5.4504
	step [143/191], loss=7.1348
	step [144/191], loss=6.6707
	step [145/191], loss=7.1770
	step [146/191], loss=7.7992
	step [147/191], loss=6.8611
	step [148/191], loss=6.9086
	step [149/191], loss=5.9278
	step [150/191], loss=6.9207
	step [151/191], loss=6.7547
	step [152/191], loss=6.8235
	step [153/191], loss=7.3176
	step [154/191], loss=6.0510
	step [155/191], loss=7.1515
	step [156/191], loss=6.2772
	step [157/191], loss=6.9323
	step [158/191], loss=6.4012
	step [159/191], loss=6.4992
	step [160/191], loss=5.6645
	step [161/191], loss=6.5222
	step [162/191], loss=7.2089
	step [163/191], loss=5.9105
	step [164/191], loss=7.5534
	step [165/191], loss=8.1686
	step [166/191], loss=7.1744
	step [167/191], loss=6.0356
	step [168/191], loss=6.1762
	step [169/191], loss=7.8323
	step [170/191], loss=5.7716
	step [171/191], loss=6.5763
	step [172/191], loss=6.2680
	step [173/191], loss=7.8508
	step [174/191], loss=6.9865
	step [175/191], loss=5.8316
	step [176/191], loss=6.8953
	step [177/191], loss=5.4194
	step [178/191], loss=5.6237
	step [179/191], loss=7.4539
	step [180/191], loss=6.2618
	step [181/191], loss=5.7511
	step [182/191], loss=7.5033
	step [183/191], loss=7.7350
	step [184/191], loss=6.4076
	step [185/191], loss=6.6297
	step [186/191], loss=6.3451
	step [187/191], loss=6.6077
	step [188/191], loss=6.5273
	step [189/191], loss=7.3396
	step [190/191], loss=5.6330
	step [191/191], loss=3.2245
	Evaluating
	loss=0.0240, precision=0.2761, recall=0.9801, f1=0.7810
Training epoch 28
	step [1/191], loss=5.8721
	step [2/191], loss=6.0549
	step [3/191], loss=7.1817
	step [4/191], loss=6.9106
	step [5/191], loss=7.7503
	step [6/191], loss=5.9757
	step [7/191], loss=7.3824
	step [8/191], loss=6.6843
	step [9/191], loss=4.9600
	step [10/191], loss=6.1080
	step [11/191], loss=6.8357
	step [12/191], loss=6.2071
	step [13/191], loss=6.6462
	step [14/191], loss=6.3742
	step [15/191], loss=7.0278
	step [16/191], loss=7.0532
	step [17/191], loss=7.4648
	step [18/191], loss=7.0760
	step [19/191], loss=7.0220
	step [20/191], loss=8.1497
	step [21/191], loss=5.7721
	step [22/191], loss=6.7503
	step [23/191], loss=6.1544
	step [24/191], loss=6.8993
	step [25/191], loss=5.7137
	step [26/191], loss=6.3845
	step [27/191], loss=6.3249
	step [28/191], loss=5.7977
	step [29/191], loss=6.8514
	step [30/191], loss=6.4254
	step [31/191], loss=5.1790
	step [32/191], loss=7.0086
	step [33/191], loss=6.6373
	step [34/191], loss=6.2757
	step [35/191], loss=5.7202
	step [36/191], loss=6.4533
	step [37/191], loss=5.4138
	step [38/191], loss=5.4997
	step [39/191], loss=5.6102
	step [40/191], loss=5.9487
	step [41/191], loss=8.7850
	step [42/191], loss=6.9931
	step [43/191], loss=5.7783
	step [44/191], loss=7.1746
	step [45/191], loss=6.4222
	step [46/191], loss=5.7058
	step [47/191], loss=6.2202
	step [48/191], loss=6.4992
	step [49/191], loss=5.5267
	step [50/191], loss=5.8605
	step [51/191], loss=7.0621
	step [52/191], loss=5.8693
	step [53/191], loss=5.3817
	step [54/191], loss=6.4136
	step [55/191], loss=7.7668
	step [56/191], loss=6.4491
	step [57/191], loss=6.7565
	step [58/191], loss=8.0840
	step [59/191], loss=5.4274
	step [60/191], loss=6.6424
	step [61/191], loss=8.4219
	step [62/191], loss=6.8719
	step [63/191], loss=6.6381
	step [64/191], loss=7.0019
	step [65/191], loss=6.7379
	step [66/191], loss=5.8978
	step [67/191], loss=7.1167
	step [68/191], loss=6.7869
	step [69/191], loss=7.4757
	step [70/191], loss=6.4753
	step [71/191], loss=7.7461
	step [72/191], loss=8.4573
	step [73/191], loss=6.9002
	step [74/191], loss=7.6657
	step [75/191], loss=6.4692
	step [76/191], loss=6.8235
	step [77/191], loss=6.5972
	step [78/191], loss=6.6481
	step [79/191], loss=6.9450
	step [80/191], loss=6.6796
	step [81/191], loss=7.3094
	step [82/191], loss=6.9623
	step [83/191], loss=7.4974
	step [84/191], loss=6.5631
	step [85/191], loss=6.1746
	step [86/191], loss=6.2243
	step [87/191], loss=7.1453
	step [88/191], loss=6.2349
	step [89/191], loss=6.9582
	step [90/191], loss=7.5110
	step [91/191], loss=6.0924
	step [92/191], loss=7.1555
	step [93/191], loss=5.2688
	step [94/191], loss=8.1779
	step [95/191], loss=6.7663
	step [96/191], loss=5.9645
	step [97/191], loss=6.5046
	step [98/191], loss=7.2403
	step [99/191], loss=6.5098
	step [100/191], loss=6.2244
	step [101/191], loss=6.7476
	step [102/191], loss=7.5360
	step [103/191], loss=7.5721
	step [104/191], loss=5.8004
	step [105/191], loss=6.0978
	step [106/191], loss=6.8988
	step [107/191], loss=6.2298
	step [108/191], loss=5.8201
	step [109/191], loss=6.5511
	step [110/191], loss=7.2702
	step [111/191], loss=6.5195
	step [112/191], loss=5.1774
	step [113/191], loss=5.5052
	step [114/191], loss=6.7493
	step [115/191], loss=5.1357
	step [116/191], loss=7.0976
	step [117/191], loss=7.0037
	step [118/191], loss=5.4150
	step [119/191], loss=5.1425
	step [120/191], loss=8.0168
	step [121/191], loss=6.0756
	step [122/191], loss=6.1934
	step [123/191], loss=6.6977
	step [124/191], loss=6.1178
	step [125/191], loss=6.4726
	step [126/191], loss=7.0408
	step [127/191], loss=6.0794
	step [128/191], loss=6.6370
	step [129/191], loss=6.2256
	step [130/191], loss=6.6418
	step [131/191], loss=6.4121
	step [132/191], loss=7.0953
	step [133/191], loss=5.0311
	step [134/191], loss=5.8436
	step [135/191], loss=6.9237
	step [136/191], loss=7.5050
	step [137/191], loss=6.2461
	step [138/191], loss=6.6728
	step [139/191], loss=5.7696
	step [140/191], loss=5.8582
	step [141/191], loss=5.7742
	step [142/191], loss=5.9297
	step [143/191], loss=6.2221
	step [144/191], loss=7.4973
	step [145/191], loss=6.0377
	step [146/191], loss=7.3603
	step [147/191], loss=6.0481
	step [148/191], loss=6.9986
	step [149/191], loss=6.5930
	step [150/191], loss=5.9320
	step [151/191], loss=6.2612
	step [152/191], loss=6.1634
	step [153/191], loss=5.7185
	step [154/191], loss=5.4887
	step [155/191], loss=6.5374
	step [156/191], loss=7.1636
	step [157/191], loss=7.1793
	step [158/191], loss=6.7165
	step [159/191], loss=5.4232
	step [160/191], loss=5.9740
	step [161/191], loss=5.9571
	step [162/191], loss=6.8626
	step [163/191], loss=5.7968
	step [164/191], loss=6.5527
	step [165/191], loss=7.4465
	step [166/191], loss=5.8786
	step [167/191], loss=7.0630
	step [168/191], loss=6.5203
	step [169/191], loss=5.3401
	step [170/191], loss=6.6125
	step [171/191], loss=7.1974
	step [172/191], loss=5.7129
	step [173/191], loss=6.5980
	step [174/191], loss=7.5930
	step [175/191], loss=6.1274
	step [176/191], loss=8.3540
	step [177/191], loss=6.9619
	step [178/191], loss=6.5226
	step [179/191], loss=5.6332
	step [180/191], loss=6.0932
	step [181/191], loss=8.1945
	step [182/191], loss=6.6158
	step [183/191], loss=7.1029
	step [184/191], loss=8.2226
	step [185/191], loss=6.5678
	step [186/191], loss=6.9167
	step [187/191], loss=6.3802
	step [188/191], loss=6.3089
	step [189/191], loss=6.6703
	step [190/191], loss=5.7949
	step [191/191], loss=2.8592
	Evaluating
	loss=0.0204, precision=0.3076, recall=0.9791, f1=0.8037
saving model as: 1_saved_model.pth
Training epoch 29
	step [1/191], loss=5.6969
	step [2/191], loss=7.5301
	step [3/191], loss=6.8818
	step [4/191], loss=6.5071
	step [5/191], loss=6.7819
	step [6/191], loss=6.5886
	step [7/191], loss=5.7828
	step [8/191], loss=6.9195
	step [9/191], loss=6.1590
	step [10/191], loss=7.4121
	step [11/191], loss=6.3310
	step [12/191], loss=6.7274
	step [13/191], loss=6.6231
	step [14/191], loss=7.1566
	step [15/191], loss=8.3864
	step [16/191], loss=5.7898
	step [17/191], loss=7.2896
	step [18/191], loss=5.8541
	step [19/191], loss=6.4385
	step [20/191], loss=5.6019
	step [21/191], loss=5.2460
	step [22/191], loss=6.2964
	step [23/191], loss=5.5921
	step [24/191], loss=5.3444
	step [25/191], loss=6.6137
	step [26/191], loss=6.7821
	step [27/191], loss=6.2198
	step [28/191], loss=6.5019
	step [29/191], loss=6.8378
	step [30/191], loss=6.9067
	step [31/191], loss=6.0860
	step [32/191], loss=6.9785
	step [33/191], loss=5.6805
	step [34/191], loss=7.3825
	step [35/191], loss=7.3279
	step [36/191], loss=5.8107
	step [37/191], loss=6.8637
	step [38/191], loss=6.1511
	step [39/191], loss=5.2451
	step [40/191], loss=8.2827
	step [41/191], loss=5.8884
	step [42/191], loss=6.6670
	step [43/191], loss=5.5660
	step [44/191], loss=6.6556
	step [45/191], loss=5.0062
	step [46/191], loss=8.2745
	step [47/191], loss=6.5268
	step [48/191], loss=6.0844
	step [49/191], loss=5.5882
	step [50/191], loss=5.4592
	step [51/191], loss=6.6278
	step [52/191], loss=5.6339
	step [53/191], loss=5.8946
	step [54/191], loss=5.9596
	step [55/191], loss=7.4699
	step [56/191], loss=6.8288
	step [57/191], loss=5.8110
	step [58/191], loss=5.6445
	step [59/191], loss=5.6826
	step [60/191], loss=5.8335
	step [61/191], loss=5.2006
	step [62/191], loss=6.3257
	step [63/191], loss=7.6333
	step [64/191], loss=6.0392
	step [65/191], loss=6.1547
	step [66/191], loss=5.7669
	step [67/191], loss=7.1827
	step [68/191], loss=5.9211
	step [69/191], loss=6.8867
	step [70/191], loss=7.5728
	step [71/191], loss=6.5840
	step [72/191], loss=6.7253
	step [73/191], loss=7.0809
	step [74/191], loss=7.0998
	step [75/191], loss=5.4102
	step [76/191], loss=6.5232
	step [77/191], loss=6.4557
	step [78/191], loss=6.2262
	step [79/191], loss=6.6758
	step [80/191], loss=5.6501
	step [81/191], loss=6.9393
	step [82/191], loss=7.1961
	step [83/191], loss=6.6242
	step [84/191], loss=6.0913
	step [85/191], loss=8.0041
	step [86/191], loss=7.7203
	step [87/191], loss=5.7644
	step [88/191], loss=7.2181
	step [89/191], loss=7.0803
	step [90/191], loss=5.8386
	step [91/191], loss=5.8656
	step [92/191], loss=6.7928
	step [93/191], loss=5.7118
	step [94/191], loss=6.3783
	step [95/191], loss=6.7799
	step [96/191], loss=5.8528
	step [97/191], loss=6.1256
	step [98/191], loss=5.7662
	step [99/191], loss=5.6082
	step [100/191], loss=6.7874
	step [101/191], loss=7.5853
	step [102/191], loss=6.0936
	step [103/191], loss=8.3391
	step [104/191], loss=6.4292
	step [105/191], loss=6.2680
	step [106/191], loss=6.6583
	step [107/191], loss=6.5371
	step [108/191], loss=5.8057
	step [109/191], loss=6.1837
	step [110/191], loss=5.9134
	step [111/191], loss=5.8625
	step [112/191], loss=5.3093
	step [113/191], loss=6.4103
	step [114/191], loss=6.1972
	step [115/191], loss=5.3741
	step [116/191], loss=5.7817
	step [117/191], loss=5.6169
	step [118/191], loss=6.9661
	step [119/191], loss=5.5975
	step [120/191], loss=5.2664
	step [121/191], loss=6.9328
	step [122/191], loss=7.3518
	step [123/191], loss=5.1029
	step [124/191], loss=6.8124
	step [125/191], loss=6.4308
	step [126/191], loss=5.7573
	step [127/191], loss=6.6134
	step [128/191], loss=5.6235
	step [129/191], loss=7.5177
	step [130/191], loss=6.9253
	step [131/191], loss=7.8199
	step [132/191], loss=5.9258
	step [133/191], loss=7.6175
	step [134/191], loss=6.0668
	step [135/191], loss=6.1535
	step [136/191], loss=6.0592
	step [137/191], loss=6.4551
	step [138/191], loss=6.4808
	step [139/191], loss=6.8259
	step [140/191], loss=6.6433
	step [141/191], loss=6.0528
	step [142/191], loss=5.9569
	step [143/191], loss=6.0791
	step [144/191], loss=7.7035
	step [145/191], loss=5.4047
	step [146/191], loss=5.6979
	step [147/191], loss=7.0138
	step [148/191], loss=6.1310
	step [149/191], loss=5.8143
	step [150/191], loss=6.7593
	step [151/191], loss=6.1088
	step [152/191], loss=5.9665
	step [153/191], loss=6.0472
	step [154/191], loss=7.3169
	step [155/191], loss=5.8908
	step [156/191], loss=5.5878
	step [157/191], loss=5.4938
	step [158/191], loss=7.0178
	step [159/191], loss=5.8520
	step [160/191], loss=5.3202
	step [161/191], loss=6.0782
	step [162/191], loss=6.3926
	step [163/191], loss=6.2373
	step [164/191], loss=5.8444
	step [165/191], loss=6.2698
	step [166/191], loss=6.4913
	step [167/191], loss=5.1351
	step [168/191], loss=5.5037
	step [169/191], loss=5.2602
	step [170/191], loss=5.4362
	step [171/191], loss=5.1235
	step [172/191], loss=7.0462
	step [173/191], loss=5.5505
	step [174/191], loss=6.4727
	step [175/191], loss=6.4191
	step [176/191], loss=6.2350
	step [177/191], loss=6.5730
	step [178/191], loss=5.6036
	step [179/191], loss=7.5310
	step [180/191], loss=5.2677
	step [181/191], loss=6.8052
	step [182/191], loss=5.7321
	step [183/191], loss=6.7971
	step [184/191], loss=6.7320
	step [185/191], loss=6.2436
	step [186/191], loss=6.8790
	step [187/191], loss=6.3709
	step [188/191], loss=6.1571
	step [189/191], loss=6.5352
	step [190/191], loss=5.9777
	step [191/191], loss=2.7803
	Evaluating
	loss=0.0234, precision=0.2641, recall=0.9811, f1=0.7716
Training epoch 30
	step [1/191], loss=6.4830
	step [2/191], loss=7.0427
	step [3/191], loss=6.6652
	step [4/191], loss=5.8667
	step [5/191], loss=6.5411
	step [6/191], loss=6.7325
	step [7/191], loss=8.5707
	step [8/191], loss=6.9196
	step [9/191], loss=7.2997
	step [10/191], loss=6.1075
	step [11/191], loss=6.5574
	step [12/191], loss=6.6737
	step [13/191], loss=6.4544
	step [14/191], loss=6.1160
	step [15/191], loss=5.5657
	step [16/191], loss=7.1383
	step [17/191], loss=5.8737
	step [18/191], loss=5.7715
	step [19/191], loss=6.7546
	step [20/191], loss=5.8023
	step [21/191], loss=6.1404
	step [22/191], loss=5.3967
	step [23/191], loss=5.6184
	step [24/191], loss=5.3991
	step [25/191], loss=6.0687
	step [26/191], loss=5.9434
	step [27/191], loss=6.2114
	step [28/191], loss=6.1847
	step [29/191], loss=5.9730
	step [30/191], loss=5.8423
	step [31/191], loss=6.1386
	step [32/191], loss=6.6370
	step [33/191], loss=6.2706
	step [34/191], loss=6.2793
	step [35/191], loss=6.3952
	step [36/191], loss=6.6457
	step [37/191], loss=6.4073
	step [38/191], loss=5.9898
	step [39/191], loss=7.1022
	step [40/191], loss=6.0730
	step [41/191], loss=6.9879
	step [42/191], loss=6.4835
	step [43/191], loss=6.7253
	step [44/191], loss=5.7518
	step [45/191], loss=4.6284
	step [46/191], loss=6.9419
	step [47/191], loss=6.1797
	step [48/191], loss=5.0382
	step [49/191], loss=5.9745
	step [50/191], loss=5.6145
	step [51/191], loss=5.5790
	step [52/191], loss=5.8340
	step [53/191], loss=6.7281
	step [54/191], loss=5.5176
	step [55/191], loss=6.9037
	step [56/191], loss=7.1886
	step [57/191], loss=5.6202
	step [58/191], loss=7.6494
	step [59/191], loss=6.5855
	step [60/191], loss=5.3034
	step [61/191], loss=5.8215
	step [62/191], loss=6.2597
	step [63/191], loss=6.4020
	step [64/191], loss=5.5139
	step [65/191], loss=6.9351
	step [66/191], loss=5.4505
	step [67/191], loss=5.9809
	step [68/191], loss=5.9073
	step [69/191], loss=5.6762
	step [70/191], loss=6.4100
	step [71/191], loss=6.4361
	step [72/191], loss=5.7564
	step [73/191], loss=5.6298
	step [74/191], loss=7.1522
	step [75/191], loss=6.0733
	step [76/191], loss=6.1775
	step [77/191], loss=6.5087
	step [78/191], loss=6.3504
	step [79/191], loss=6.9569
	step [80/191], loss=5.6366
	step [81/191], loss=6.1313
	step [82/191], loss=6.6176
	step [83/191], loss=6.1012
	step [84/191], loss=5.7838
	step [85/191], loss=5.5550
	step [86/191], loss=5.9771
	step [87/191], loss=6.9435
	step [88/191], loss=7.1949
	step [89/191], loss=7.1359
	step [90/191], loss=7.2558
	step [91/191], loss=6.0798
	step [92/191], loss=5.7218
	step [93/191], loss=6.8632
	step [94/191], loss=5.6444
	step [95/191], loss=5.7748
	step [96/191], loss=5.7447
	step [97/191], loss=5.4278
	step [98/191], loss=6.2321
	step [99/191], loss=6.2103
	step [100/191], loss=7.3646
	step [101/191], loss=5.0312
	step [102/191], loss=6.6792
	step [103/191], loss=6.7148
	step [104/191], loss=7.4360
	step [105/191], loss=6.8187
	step [106/191], loss=5.4096
	step [107/191], loss=7.0815
	step [108/191], loss=6.0902
	step [109/191], loss=5.8673
	step [110/191], loss=6.6468
	step [111/191], loss=7.0915
	step [112/191], loss=5.5000
	step [113/191], loss=5.7424
	step [114/191], loss=6.1564
	step [115/191], loss=6.0631
	step [116/191], loss=5.8351
	step [117/191], loss=4.8625
	step [118/191], loss=6.8488
	step [119/191], loss=8.2055
	step [120/191], loss=7.8991
	step [121/191], loss=5.1542
	step [122/191], loss=6.1142
	step [123/191], loss=5.6522
	step [124/191], loss=5.7410
	step [125/191], loss=6.3374
	step [126/191], loss=6.9423
	step [127/191], loss=5.8262
	step [128/191], loss=5.5751
	step [129/191], loss=5.5650
	step [130/191], loss=6.4380
	step [131/191], loss=6.9527
	step [132/191], loss=6.9291
	step [133/191], loss=6.8067
	step [134/191], loss=5.9892
	step [135/191], loss=6.4277
	step [136/191], loss=6.3977
	step [137/191], loss=6.3900
	step [138/191], loss=7.9017
	step [139/191], loss=5.5107
	step [140/191], loss=6.6570
	step [141/191], loss=5.1861
	step [142/191], loss=5.6262
	step [143/191], loss=6.5228
	step [144/191], loss=7.0280
	step [145/191], loss=6.2295
	step [146/191], loss=5.9706
	step [147/191], loss=5.5082
	step [148/191], loss=5.6336
	step [149/191], loss=5.4037
	step [150/191], loss=5.7753
	step [151/191], loss=6.0907
	step [152/191], loss=7.8307
	step [153/191], loss=5.6452
	step [154/191], loss=5.4747
	step [155/191], loss=5.5501
	step [156/191], loss=5.8022
	step [157/191], loss=6.3708
	step [158/191], loss=5.7172
	step [159/191], loss=6.3837
	step [160/191], loss=5.0497
	step [161/191], loss=5.4790
	step [162/191], loss=6.2634
	step [163/191], loss=5.9636
	step [164/191], loss=6.8554
	step [165/191], loss=6.0353
	step [166/191], loss=6.8398
	step [167/191], loss=6.2496
	step [168/191], loss=5.4178
	step [169/191], loss=6.7699
	step [170/191], loss=6.1781
	step [171/191], loss=4.9490
	step [172/191], loss=6.0711
	step [173/191], loss=5.8986
	step [174/191], loss=6.4047
	step [175/191], loss=7.1662
	step [176/191], loss=7.2381
	step [177/191], loss=6.5064
	step [178/191], loss=6.7536
	step [179/191], loss=5.5048
	step [180/191], loss=5.9642
	step [181/191], loss=6.7249
	step [182/191], loss=6.3477
	step [183/191], loss=6.2987
	step [184/191], loss=6.0186
	step [185/191], loss=7.6696
	step [186/191], loss=5.9373
	step [187/191], loss=5.4092
	step [188/191], loss=6.4674
	step [189/191], loss=5.1065
	step [190/191], loss=7.1084
	step [191/191], loss=2.9516
	Evaluating
	loss=0.0196, precision=0.3084, recall=0.9785, f1=0.8038
saving model as: 1_saved_model.pth
Training finished
best_f1: 0.803792887639729
directing: Y rim_enhanced: False test_id 1
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15614 # image files with weight 15579
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4469 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_two/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/325], loss=359.4248
	step [2/325], loss=257.8473
	step [3/325], loss=197.0797
	step [4/325], loss=170.9888
	step [5/325], loss=164.7858
	step [6/325], loss=158.7522
	step [7/325], loss=154.6249
	step [8/325], loss=152.0326
	step [9/325], loss=150.5262
	step [10/325], loss=147.1402
	step [11/325], loss=146.1699
	step [12/325], loss=144.9284
	step [13/325], loss=142.3680
	step [14/325], loss=141.7208
	step [15/325], loss=137.9874
	step [16/325], loss=138.1372
	step [17/325], loss=132.0087
	step [18/325], loss=132.2942
	step [19/325], loss=130.9841
	step [20/325], loss=130.7005
	step [21/325], loss=128.2155
	step [22/325], loss=127.2650
	step [23/325], loss=126.4716
	step [24/325], loss=122.0920
	step [25/325], loss=120.2626
	step [26/325], loss=120.9189
	step [27/325], loss=120.0730
	step [28/325], loss=115.7546
	step [29/325], loss=116.5627
	step [30/325], loss=113.8565
	step [31/325], loss=112.2444
	step [32/325], loss=109.6669
	step [33/325], loss=110.6394
	step [34/325], loss=111.0357
	step [35/325], loss=109.7145
	step [36/325], loss=108.2291
	step [37/325], loss=106.8739
	step [38/325], loss=102.2692
	step [39/325], loss=106.9104
	step [40/325], loss=108.1031
	step [41/325], loss=101.7562
	step [42/325], loss=103.5400
	step [43/325], loss=100.5237
	step [44/325], loss=99.9440
	step [45/325], loss=97.2163
	step [46/325], loss=99.1408
	step [47/325], loss=101.9967
	step [48/325], loss=96.8484
	step [49/325], loss=95.9164
	step [50/325], loss=95.0599
	step [51/325], loss=94.7885
	step [52/325], loss=95.3937
	step [53/325], loss=93.3527
	step [54/325], loss=95.3137
	step [55/325], loss=92.1950
	step [56/325], loss=91.8189
	step [57/325], loss=89.8327
	step [58/325], loss=91.5094
	step [59/325], loss=90.6195
	step [60/325], loss=90.4175
	step [61/325], loss=92.8823
	step [62/325], loss=88.7223
	step [63/325], loss=90.7805
	step [64/325], loss=88.0383
	step [65/325], loss=88.4246
	step [66/325], loss=88.6795
	step [67/325], loss=89.2873
	step [68/325], loss=91.7361
	step [69/325], loss=86.5555
	step [70/325], loss=87.5399
	step [71/325], loss=90.1091
	step [72/325], loss=84.6760
	step [73/325], loss=85.1165
	step [74/325], loss=89.9669
	step [75/325], loss=87.2046
	step [76/325], loss=85.5471
	step [77/325], loss=84.3325
	step [78/325], loss=85.4154
	step [79/325], loss=84.2307
	step [80/325], loss=87.2802
	step [81/325], loss=83.5733
	step [82/325], loss=87.5102
	step [83/325], loss=85.7922
	step [84/325], loss=84.8257
	step [85/325], loss=84.6185
	step [86/325], loss=80.8428
	step [87/325], loss=86.5561
	step [88/325], loss=82.4514
	step [89/325], loss=84.9348
	step [90/325], loss=84.4051
	step [91/325], loss=83.0570
	step [92/325], loss=82.5116
	step [93/325], loss=82.5529
	step [94/325], loss=83.0140
	step [95/325], loss=78.0623
	step [96/325], loss=79.2364
	step [97/325], loss=82.2644
	step [98/325], loss=81.2319
	step [99/325], loss=80.6053
	step [100/325], loss=78.1714
	step [101/325], loss=80.9076
	step [102/325], loss=79.9708
	step [103/325], loss=79.4468
	step [104/325], loss=78.7413
	step [105/325], loss=79.9486
	step [106/325], loss=81.7386
	step [107/325], loss=83.3967
	step [108/325], loss=78.5891
	step [109/325], loss=77.4397
	step [110/325], loss=80.3566
	step [111/325], loss=82.1279
	step [112/325], loss=78.1188
	step [113/325], loss=78.8470
	step [114/325], loss=77.4520
	step [115/325], loss=78.7770
	step [116/325], loss=78.0927
	step [117/325], loss=77.2717
	step [118/325], loss=76.6000
	step [119/325], loss=78.5007
	step [120/325], loss=77.8186
	step [121/325], loss=79.6064
	step [122/325], loss=77.9103
	step [123/325], loss=76.7561
	step [124/325], loss=77.3419
	step [125/325], loss=75.2175
	step [126/325], loss=78.8751
	step [127/325], loss=78.9009
	step [128/325], loss=76.8120
	step [129/325], loss=78.5410
	step [130/325], loss=75.0886
	step [131/325], loss=79.2953
	step [132/325], loss=74.7565
	step [133/325], loss=74.5604
	step [134/325], loss=75.4657
	step [135/325], loss=77.9569
	step [136/325], loss=77.2161
	step [137/325], loss=80.6586
	step [138/325], loss=78.4073
	step [139/325], loss=74.7916
	step [140/325], loss=74.7382
	step [141/325], loss=74.5719
	step [142/325], loss=76.3478
	step [143/325], loss=76.1936
	step [144/325], loss=74.0408
	step [145/325], loss=73.5406
	step [146/325], loss=76.0813
	step [147/325], loss=78.7020
	step [148/325], loss=75.9177
	step [149/325], loss=75.1264
	step [150/325], loss=72.0775
	step [151/325], loss=74.9409
	step [152/325], loss=74.6978
	step [153/325], loss=72.4080
	step [154/325], loss=73.9717
	step [155/325], loss=75.8393
	step [156/325], loss=72.7799
	step [157/325], loss=75.3573
	step [158/325], loss=73.2267
	step [159/325], loss=73.4950
	step [160/325], loss=72.7794
	step [161/325], loss=71.3478
	step [162/325], loss=69.8470
	step [163/325], loss=76.3468
	step [164/325], loss=75.2626
	step [165/325], loss=71.3151
	step [166/325], loss=72.0282
	step [167/325], loss=71.0448
	step [168/325], loss=73.5462
	step [169/325], loss=70.9707
	step [170/325], loss=71.0247
	step [171/325], loss=72.4521
	step [172/325], loss=72.7799
	step [173/325], loss=70.1437
	step [174/325], loss=72.8357
	step [175/325], loss=70.5679
	step [176/325], loss=72.0320
	step [177/325], loss=69.8154
	step [178/325], loss=72.5900
	step [179/325], loss=69.4839
	step [180/325], loss=71.5878
	step [181/325], loss=71.8048
	step [182/325], loss=71.3031
	step [183/325], loss=71.2088
	step [184/325], loss=70.0502
	step [185/325], loss=71.1051
	step [186/325], loss=69.1980
	step [187/325], loss=69.0394
	step [188/325], loss=74.1721
	step [189/325], loss=72.1829
	step [190/325], loss=70.3199
	step [191/325], loss=71.4163
	step [192/325], loss=69.9337
	step [193/325], loss=68.1383
	step [194/325], loss=69.1032
	step [195/325], loss=69.6140
	step [196/325], loss=70.0951
	step [197/325], loss=69.0042
	step [198/325], loss=68.9565
	step [199/325], loss=70.7896
	step [200/325], loss=69.8129
	step [201/325], loss=69.7595
	step [202/325], loss=68.1511
	step [203/325], loss=69.8280
	step [204/325], loss=67.1543
	step [205/325], loss=69.6517
	step [206/325], loss=67.5210
	step [207/325], loss=70.0382
	step [208/325], loss=69.0286
	step [209/325], loss=69.1841
	step [210/325], loss=67.5017
	step [211/325], loss=68.0445
	step [212/325], loss=70.0727
	step [213/325], loss=68.0034
	step [214/325], loss=66.7180
	step [215/325], loss=67.8346
	step [216/325], loss=68.9910
	step [217/325], loss=66.3773
	step [218/325], loss=67.4000
	step [219/325], loss=66.7136
	step [220/325], loss=68.7305
	step [221/325], loss=67.9364
	step [222/325], loss=67.6008
	step [223/325], loss=68.3328
	step [224/325], loss=65.5537
	step [225/325], loss=70.0606
	step [226/325], loss=67.1896
	step [227/325], loss=68.0429
	step [228/325], loss=66.2024
	step [229/325], loss=65.9945
	step [230/325], loss=69.0878
	step [231/325], loss=68.5838
	step [232/325], loss=65.0496
	step [233/325], loss=66.3078
	step [234/325], loss=65.4243
	step [235/325], loss=67.0355
	step [236/325], loss=68.6978
	step [237/325], loss=65.7214
	step [238/325], loss=68.1212
	step [239/325], loss=64.7936
	step [240/325], loss=66.6384
	step [241/325], loss=65.7417
	step [242/325], loss=66.0079
	step [243/325], loss=66.1680
	step [244/325], loss=65.0717
	step [245/325], loss=66.2893
	step [246/325], loss=65.3446
	step [247/325], loss=67.5637
	step [248/325], loss=66.3114
	step [249/325], loss=65.8165
	step [250/325], loss=62.1773
	step [251/325], loss=64.0905
	step [252/325], loss=64.7014
	step [253/325], loss=66.6417
	step [254/325], loss=67.0001
	step [255/325], loss=63.4703
	step [256/325], loss=64.6186
	step [257/325], loss=64.3900
	step [258/325], loss=63.3067
	step [259/325], loss=64.2258
	step [260/325], loss=64.6840
	step [261/325], loss=64.1087
	step [262/325], loss=63.2329
	step [263/325], loss=63.2173
	step [264/325], loss=66.4245
	step [265/325], loss=64.8698
	step [266/325], loss=64.2818
	step [267/325], loss=63.3145
	step [268/325], loss=64.1283
	step [269/325], loss=66.4979
	step [270/325], loss=64.7655
	step [271/325], loss=64.3316
	step [272/325], loss=63.9576
	step [273/325], loss=62.7498
	step [274/325], loss=65.0249
	step [275/325], loss=61.0495
	step [276/325], loss=62.4805
	step [277/325], loss=64.2688
	step [278/325], loss=65.8901
	step [279/325], loss=64.2789
	step [280/325], loss=62.8260
	step [281/325], loss=64.6134
	step [282/325], loss=63.8843
	step [283/325], loss=65.2624
	step [284/325], loss=63.2804
	step [285/325], loss=61.1833
	step [286/325], loss=61.1216
	step [287/325], loss=64.2029
	step [288/325], loss=66.1490
	step [289/325], loss=62.6972
	step [290/325], loss=60.7894
	step [291/325], loss=62.9426
	step [292/325], loss=64.4146
	step [293/325], loss=63.4758
	step [294/325], loss=63.2707
	step [295/325], loss=64.1961
	step [296/325], loss=62.0245
	step [297/325], loss=61.4247
	step [298/325], loss=63.3769
	step [299/325], loss=61.1375
	step [300/325], loss=61.8135
	step [301/325], loss=60.7191
	step [302/325], loss=61.8457
	step [303/325], loss=61.3112
	step [304/325], loss=62.2114
	step [305/325], loss=59.5878
	step [306/325], loss=59.9364
	step [307/325], loss=61.1981
	step [308/325], loss=60.1102
	step [309/325], loss=61.3350
	step [310/325], loss=60.4946
	step [311/325], loss=61.4106
	step [312/325], loss=60.2418
	step [313/325], loss=60.2602
	step [314/325], loss=61.6818
	step [315/325], loss=62.7365
	step [316/325], loss=60.3784
	step [317/325], loss=59.5419
	step [318/325], loss=60.8117
	step [319/325], loss=60.9792
	step [320/325], loss=60.3877
	step [321/325], loss=59.5395
	step [322/325], loss=59.3178
	step [323/325], loss=60.6524
	step [324/325], loss=60.9952
	step [325/325], loss=31.5749
	Evaluating
	loss=0.2882, precision=0.1842, recall=0.9865, f1=0.6872
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/325], loss=61.2985
...training function Done
