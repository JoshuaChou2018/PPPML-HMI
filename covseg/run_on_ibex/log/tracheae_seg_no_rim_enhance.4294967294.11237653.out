Loading anaconda...
...Anaconda env loaded
directing: X rim_enhanced: False test_id 0
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9414 # image files with weight 9372
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2468 # image files with weight 2460
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9372
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/147], loss=201.4378
	step [2/147], loss=185.6799
	step [3/147], loss=179.5513
	step [4/147], loss=172.7563
	step [5/147], loss=168.9954
	step [6/147], loss=165.6880
	step [7/147], loss=163.3442
	step [8/147], loss=158.3102
	step [9/147], loss=156.2409
	step [10/147], loss=151.7191
	step [11/147], loss=147.9644
	step [12/147], loss=145.1236
	step [13/147], loss=143.2777
	step [14/147], loss=139.8140
	step [15/147], loss=137.8041
	step [16/147], loss=136.9622
	step [17/147], loss=135.0016
	step [18/147], loss=132.5502
	step [19/147], loss=132.0563
	step [20/147], loss=129.9306
	step [21/147], loss=126.4046
	step [22/147], loss=124.0362
	step [23/147], loss=123.5942
	step [24/147], loss=120.0465
	step [25/147], loss=121.8392
	step [26/147], loss=119.3767
	step [27/147], loss=117.4008
	step [28/147], loss=119.3459
	step [29/147], loss=115.0476
	step [30/147], loss=113.8258
	step [31/147], loss=114.0203
	step [32/147], loss=112.2195
	step [33/147], loss=109.3174
	step [34/147], loss=109.0522
	step [35/147], loss=112.7550
	step [36/147], loss=107.3396
	step [37/147], loss=106.3410
	step [38/147], loss=105.6647
	step [39/147], loss=106.1789
	step [40/147], loss=106.1609
	step [41/147], loss=105.0297
	step [42/147], loss=103.1213
	step [43/147], loss=102.4519
	step [44/147], loss=102.5825
	step [45/147], loss=101.2531
	step [46/147], loss=100.5259
	step [47/147], loss=100.6401
	step [48/147], loss=100.6985
	step [49/147], loss=99.3408
	step [50/147], loss=98.2085
	step [51/147], loss=96.5103
	step [52/147], loss=96.2654
	step [53/147], loss=96.2501
	step [54/147], loss=94.8063
	step [55/147], loss=93.6944
	step [56/147], loss=92.5855
	step [57/147], loss=93.2709
	step [58/147], loss=95.3573
	step [59/147], loss=93.6533
	step [60/147], loss=91.4709
	step [61/147], loss=91.3960
	step [62/147], loss=93.3095
	step [63/147], loss=92.4395
	step [64/147], loss=92.0316
	step [65/147], loss=92.0577
	step [66/147], loss=90.6525
	step [67/147], loss=88.3478
	step [68/147], loss=90.1733
	step [69/147], loss=87.5853
	step [70/147], loss=90.1168
	step [71/147], loss=88.2827
	step [72/147], loss=91.3996
	step [73/147], loss=87.5382
	step [74/147], loss=88.4354
	step [75/147], loss=86.1080
	step [76/147], loss=86.0967
	step [77/147], loss=85.7840
	step [78/147], loss=86.9579
	step [79/147], loss=87.9729
	step [80/147], loss=85.3101
	step [81/147], loss=85.5304
	step [82/147], loss=84.3611
	step [83/147], loss=87.3730
	step [84/147], loss=85.0353
	step [85/147], loss=84.5889
	step [86/147], loss=84.1568
	step [87/147], loss=86.1179
	step [88/147], loss=83.7386
	step [89/147], loss=83.0417
	step [90/147], loss=84.2355
	step [91/147], loss=83.3939
	step [92/147], loss=83.1883
	step [93/147], loss=81.5360
	step [94/147], loss=83.2845
	step [95/147], loss=85.0158
	step [96/147], loss=82.0313
	step [97/147], loss=82.3652
	step [98/147], loss=81.0572
	step [99/147], loss=79.8613
	step [100/147], loss=81.3125
	step [101/147], loss=80.4593
	step [102/147], loss=79.6488
	step [103/147], loss=81.2393
	step [104/147], loss=79.4707
	step [105/147], loss=79.1936
	step [106/147], loss=77.4407
	step [107/147], loss=81.7296
	step [108/147], loss=80.2386
	step [109/147], loss=78.5848
	step [110/147], loss=79.7012
	step [111/147], loss=78.0683
	step [112/147], loss=78.6867
	step [113/147], loss=80.4831
	step [114/147], loss=79.8081
	step [115/147], loss=75.6270
	step [116/147], loss=78.9508
	step [117/147], loss=78.4091
	step [118/147], loss=76.6213
	step [119/147], loss=77.5753
	step [120/147], loss=77.7246
	step [121/147], loss=76.7182
	step [122/147], loss=78.0422
	step [123/147], loss=75.9824
	step [124/147], loss=77.4847
	step [125/147], loss=76.7178
	step [126/147], loss=77.9319
	step [127/147], loss=76.1019
	step [128/147], loss=75.7226
	step [129/147], loss=75.3168
	step [130/147], loss=76.6582
	step [131/147], loss=78.1466
	step [132/147], loss=74.6714
	step [133/147], loss=76.3088
	step [134/147], loss=75.9140
	step [135/147], loss=76.5789
	step [136/147], loss=78.7002
	step [137/147], loss=74.3814
	step [138/147], loss=76.0478
	step [139/147], loss=74.3255
	step [140/147], loss=73.6073
	step [141/147], loss=76.0837
	step [142/147], loss=73.6538
	step [143/147], loss=75.3607
	step [144/147], loss=74.6953
	step [145/147], loss=73.4627
	step [146/147], loss=73.2251
	step [147/147], loss=34.4300
	Evaluating
	loss=0.2628, precision=0.1984, recall=0.9919, f1=0.3306
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/147], loss=73.5936
	step [2/147], loss=71.2970
	step [3/147], loss=75.4485
	step [4/147], loss=72.8702
	step [5/147], loss=71.7878
	step [6/147], loss=73.7837
	step [7/147], loss=75.1958
	step [8/147], loss=74.4378
	step [9/147], loss=74.2790
	step [10/147], loss=71.7762
	step [11/147], loss=73.5507
	step [12/147], loss=70.5067
	step [13/147], loss=71.1606
	step [14/147], loss=70.8382
	step [15/147], loss=72.0937
	step [16/147], loss=71.3545
	step [17/147], loss=73.3211
	step [18/147], loss=70.0384
	step [19/147], loss=70.7791
	step [20/147], loss=72.2145
	step [21/147], loss=73.5364
	step [22/147], loss=70.5819
	step [23/147], loss=70.4672
	step [24/147], loss=71.0977
	step [25/147], loss=73.4038
	step [26/147], loss=70.0995
	step [27/147], loss=71.2341
	step [28/147], loss=70.8926
	step [29/147], loss=73.1186
	step [30/147], loss=70.9809
	step [31/147], loss=70.2699
	step [32/147], loss=68.6506
	step [33/147], loss=69.5682
	step [34/147], loss=70.2970
	step [35/147], loss=68.7505
	step [36/147], loss=70.9360
	step [37/147], loss=68.9880
	step [38/147], loss=69.4627
	step [39/147], loss=70.2744
	step [40/147], loss=68.0099
	step [41/147], loss=66.9715
	step [42/147], loss=69.8746
	step [43/147], loss=68.9608
	step [44/147], loss=68.7423
	step [45/147], loss=69.6609
	step [46/147], loss=67.6939
	step [47/147], loss=69.5141
	step [48/147], loss=68.5205
	step [49/147], loss=65.9280
	step [50/147], loss=67.5384
	step [51/147], loss=69.3578
	step [52/147], loss=67.7455
	step [53/147], loss=66.4276
	step [54/147], loss=68.8996
	step [55/147], loss=68.3607
	step [56/147], loss=68.8846
	step [57/147], loss=68.2155
	step [58/147], loss=69.0157
	step [59/147], loss=67.4515
	step [60/147], loss=66.6874
	step [61/147], loss=65.0877
	step [62/147], loss=65.6509
	step [63/147], loss=64.5643
	step [64/147], loss=67.3862
	step [65/147], loss=67.7454
	step [66/147], loss=68.0520
	step [67/147], loss=66.4369
	step [68/147], loss=65.9299
	step [69/147], loss=65.5868
	step [70/147], loss=65.2526
	step [71/147], loss=68.0211
	step [72/147], loss=65.6375
	step [73/147], loss=65.0402
	step [74/147], loss=64.7015
	step [75/147], loss=65.1850
	step [76/147], loss=65.0075
	step [77/147], loss=66.7955
	step [78/147], loss=66.3728
	step [79/147], loss=65.5654
	step [80/147], loss=63.8964
	step [81/147], loss=65.4648
	step [82/147], loss=64.7158
	step [83/147], loss=63.9734
	step [84/147], loss=67.7478
	step [85/147], loss=63.2628
	step [86/147], loss=62.7710
	step [87/147], loss=65.1178
	step [88/147], loss=65.0834
	step [89/147], loss=63.6217
	step [90/147], loss=63.8470
	step [91/147], loss=63.1509
	step [92/147], loss=63.8691
	step [93/147], loss=62.3141
	step [94/147], loss=63.5088
	step [95/147], loss=64.3917
	step [96/147], loss=63.3980
	step [97/147], loss=62.0506
	step [98/147], loss=62.9822
	step [99/147], loss=61.3135
	step [100/147], loss=61.1985
	step [101/147], loss=63.4049
	step [102/147], loss=61.2078
	step [103/147], loss=60.3044
	step [104/147], loss=61.3039
	step [105/147], loss=60.8081
	step [106/147], loss=62.6789
	step [107/147], loss=60.7479
	step [108/147], loss=60.4265
	step [109/147], loss=61.6557
	step [110/147], loss=61.0132
	step [111/147], loss=60.0995
	step [112/147], loss=60.2054
	step [113/147], loss=62.1800
	step [114/147], loss=59.8780
	step [115/147], loss=59.9383
	step [116/147], loss=60.9454
	step [117/147], loss=61.3456
	step [118/147], loss=59.6869
	step [119/147], loss=60.1915
	step [120/147], loss=59.8273
	step [121/147], loss=59.5661
	step [122/147], loss=60.2118
	step [123/147], loss=58.7018
	step [124/147], loss=57.5213
	step [125/147], loss=57.7651
	step [126/147], loss=58.2634
	step [127/147], loss=60.2478
	step [128/147], loss=60.2836
	step [129/147], loss=60.7151
	step [130/147], loss=57.1135
	step [131/147], loss=58.6702
	step [132/147], loss=58.4146
	step [133/147], loss=57.7473
	step [134/147], loss=58.6108
	step [135/147], loss=57.1576
	step [136/147], loss=58.1921
	step [137/147], loss=57.7886
	step [138/147], loss=58.2274
	step [139/147], loss=58.1401
	step [140/147], loss=58.7368
	step [141/147], loss=58.3442
	step [142/147], loss=56.5373
	step [143/147], loss=57.6152
	step [144/147], loss=58.4339
	step [145/147], loss=57.9355
	step [146/147], loss=57.0944
	step [147/147], loss=24.6258
	Evaluating
	loss=0.2057, precision=0.1757, recall=0.9931, f1=0.2986
Training epoch 3
	step [1/147], loss=58.4898
	step [2/147], loss=56.4390
	step [3/147], loss=55.8868
	step [4/147], loss=55.9543
	step [5/147], loss=55.6718
	step [6/147], loss=55.4634
	step [7/147], loss=56.9222
	step [8/147], loss=55.4622
	step [9/147], loss=56.2644
	step [10/147], loss=55.5480
	step [11/147], loss=54.7972
	step [12/147], loss=55.6137
	step [13/147], loss=56.3000
	step [14/147], loss=54.9441
	step [15/147], loss=53.7952
	step [16/147], loss=57.4717
	step [17/147], loss=54.4362
	step [18/147], loss=55.5892
	step [19/147], loss=54.1099
	step [20/147], loss=53.9640
	step [21/147], loss=53.7814
	step [22/147], loss=56.5159
	step [23/147], loss=53.6758
	step [24/147], loss=53.8269
	step [25/147], loss=54.4476
	step [26/147], loss=52.5487
	step [27/147], loss=52.0169
	step [28/147], loss=55.6849
	step [29/147], loss=51.8524
	step [30/147], loss=53.1847
	step [31/147], loss=56.4780
	step [32/147], loss=53.0518
	step [33/147], loss=53.1034
	step [34/147], loss=54.1186
	step [35/147], loss=53.8480
	step [36/147], loss=53.4529
	step [37/147], loss=51.5318
	step [38/147], loss=52.2764
	step [39/147], loss=53.1330
	step [40/147], loss=53.0595
	step [41/147], loss=53.3086
	step [42/147], loss=55.0269
	step [43/147], loss=51.1984
	step [44/147], loss=52.9587
	step [45/147], loss=53.0810
	step [46/147], loss=54.7695
	step [47/147], loss=53.8475
	step [48/147], loss=50.7256
	step [49/147], loss=54.1577
	step [50/147], loss=50.7255
	step [51/147], loss=52.4117
	step [52/147], loss=51.2478
	step [53/147], loss=51.8996
	step [54/147], loss=49.9058
	step [55/147], loss=50.5149
	step [56/147], loss=51.1324
	step [57/147], loss=49.4988
	step [58/147], loss=53.0138
	step [59/147], loss=52.8746
	step [60/147], loss=51.4201
	step [61/147], loss=50.7639
	step [62/147], loss=53.6602
	step [63/147], loss=51.6990
	step [64/147], loss=49.6311
	step [65/147], loss=50.3133
	step [66/147], loss=50.7767
	step [67/147], loss=49.4863
	step [68/147], loss=50.7788
	step [69/147], loss=50.9279
	step [70/147], loss=51.0145
	step [71/147], loss=51.6973
	step [72/147], loss=49.5211
	step [73/147], loss=50.7325
	step [74/147], loss=48.6663
	step [75/147], loss=49.9095
	step [76/147], loss=48.6376
	step [77/147], loss=49.3891
	step [78/147], loss=49.8342
	step [79/147], loss=51.6331
	step [80/147], loss=51.2217
	step [81/147], loss=51.0664
	step [82/147], loss=49.5679
	step [83/147], loss=49.2793
	step [84/147], loss=49.5466
	step [85/147], loss=49.3601
	step [86/147], loss=48.3584
	step [87/147], loss=48.0653
	step [88/147], loss=49.8019
	step [89/147], loss=49.0925
	step [90/147], loss=50.7354
	step [91/147], loss=47.2176
	step [92/147], loss=50.5563
	step [93/147], loss=47.4198
	step [94/147], loss=44.4604
	step [95/147], loss=50.1305
	step [96/147], loss=47.0305
	step [97/147], loss=49.6604
	step [98/147], loss=49.6174
	step [99/147], loss=47.6668
	step [100/147], loss=47.0771
	step [101/147], loss=46.2068
	step [102/147], loss=49.2390
	step [103/147], loss=49.2372
	step [104/147], loss=47.7391
	step [105/147], loss=48.7295
	step [106/147], loss=48.6909
	step [107/147], loss=49.0305
	step [108/147], loss=47.6081
	step [109/147], loss=46.1584
	step [110/147], loss=48.0261
	step [111/147], loss=46.6343
	step [112/147], loss=49.1503
	step [113/147], loss=46.1837
	step [114/147], loss=45.9475
	step [115/147], loss=45.5241
	step [116/147], loss=45.6172
	step [117/147], loss=47.3103
	step [118/147], loss=44.9547
	step [119/147], loss=47.7480
	step [120/147], loss=46.0011
	step [121/147], loss=47.9094
	step [122/147], loss=47.3452
	step [123/147], loss=45.2257
	step [124/147], loss=46.8164
	step [125/147], loss=46.3769
	step [126/147], loss=46.1030
	step [127/147], loss=45.8373
	step [128/147], loss=47.0337
	step [129/147], loss=44.3030
	step [130/147], loss=47.0461
	step [131/147], loss=44.7213
	step [132/147], loss=46.2565
	step [133/147], loss=47.1620
	step [134/147], loss=44.4926
	step [135/147], loss=45.5253
	step [136/147], loss=49.1935
	step [137/147], loss=44.3027
	step [138/147], loss=47.7584
	step [139/147], loss=44.9361
	step [140/147], loss=45.7968
	step [141/147], loss=43.5574
	step [142/147], loss=46.3227
	step [143/147], loss=44.6908
	step [144/147], loss=46.8067
	step [145/147], loss=45.2812
	step [146/147], loss=43.5639
	step [147/147], loss=18.2754
	Evaluating
	loss=0.1499, precision=0.1833, recall=0.9931, f1=0.3095
Training epoch 4
	step [1/147], loss=42.2905
	step [2/147], loss=42.8030
	step [3/147], loss=44.0292
	step [4/147], loss=44.3334
	step [5/147], loss=44.6313
	step [6/147], loss=43.7573
	step [7/147], loss=43.4661
	step [8/147], loss=44.3481
	step [9/147], loss=48.1908
	step [10/147], loss=43.9479
	step [11/147], loss=42.5636
	step [12/147], loss=45.5646
	step [13/147], loss=43.3875
	step [14/147], loss=43.3318
	step [15/147], loss=42.5357
	step [16/147], loss=44.1033
	step [17/147], loss=46.8370
	step [18/147], loss=43.3444
	step [19/147], loss=43.8974
	step [20/147], loss=44.0014
	step [21/147], loss=44.0061
	step [22/147], loss=42.3696
	step [23/147], loss=43.2887
	step [24/147], loss=43.0034
	step [25/147], loss=44.7229
	step [26/147], loss=43.5525
	step [27/147], loss=43.7616
	step [28/147], loss=41.3359
	step [29/147], loss=41.2983
	step [30/147], loss=44.7885
	step [31/147], loss=44.7808
	step [32/147], loss=43.4693
	step [33/147], loss=43.2047
	step [34/147], loss=43.8792
	step [35/147], loss=43.2688
	step [36/147], loss=41.8862
	step [37/147], loss=45.6511
	step [38/147], loss=44.4735
	step [39/147], loss=43.4133
	step [40/147], loss=41.3130
	step [41/147], loss=43.9695
	step [42/147], loss=44.4930
	step [43/147], loss=43.3528
	step [44/147], loss=42.4803
	step [45/147], loss=44.9505
	step [46/147], loss=42.5097
	step [47/147], loss=40.4231
	step [48/147], loss=43.0766
	step [49/147], loss=42.9379
	step [50/147], loss=41.2485
	step [51/147], loss=41.7528
	step [52/147], loss=43.2765
	step [53/147], loss=46.3721
	step [54/147], loss=41.6458
	step [55/147], loss=40.8802
	step [56/147], loss=39.6106
	step [57/147], loss=41.5631
	step [58/147], loss=41.3568
	step [59/147], loss=40.2738
	step [60/147], loss=42.6030
	step [61/147], loss=42.3612
	step [62/147], loss=39.3934
	step [63/147], loss=40.9563
	step [64/147], loss=40.8781
	step [65/147], loss=40.8731
	step [66/147], loss=40.0070
	step [67/147], loss=41.9488
	step [68/147], loss=42.0479
	step [69/147], loss=52.1467
	step [70/147], loss=42.9324
	step [71/147], loss=41.3185
	step [72/147], loss=43.7291
	step [73/147], loss=45.9851
	step [74/147], loss=47.1936
	step [75/147], loss=49.4231
	step [76/147], loss=46.2118
	step [77/147], loss=41.9665
	step [78/147], loss=41.6584
	step [79/147], loss=41.9965
	step [80/147], loss=41.1907
	step [81/147], loss=42.9293
	step [82/147], loss=41.2517
	step [83/147], loss=41.1913
	step [84/147], loss=42.0241
	step [85/147], loss=41.0664
	step [86/147], loss=44.3928
	step [87/147], loss=41.0054
	step [88/147], loss=44.4080
	step [89/147], loss=40.9969
	step [90/147], loss=42.1295
	step [91/147], loss=42.3934
	step [92/147], loss=41.4455
	step [93/147], loss=39.3172
	step [94/147], loss=43.4919
	step [95/147], loss=39.3908
	step [96/147], loss=41.3343
	step [97/147], loss=40.7448
	step [98/147], loss=43.3423
	step [99/147], loss=41.6541
	step [100/147], loss=40.9774
	step [101/147], loss=40.4155
	step [102/147], loss=41.4502
	step [103/147], loss=40.7641
	step [104/147], loss=39.8891
	step [105/147], loss=41.3072
	step [106/147], loss=40.2623
	step [107/147], loss=42.2949
	step [108/147], loss=39.0811
	step [109/147], loss=38.4101
	step [110/147], loss=38.8516
	step [111/147], loss=40.7970
	step [112/147], loss=41.9253
	step [113/147], loss=38.4536
	step [114/147], loss=38.9027
	step [115/147], loss=37.8467
	step [116/147], loss=41.7017
	step [117/147], loss=40.5403
	step [118/147], loss=40.9014
	step [119/147], loss=40.2675
	step [120/147], loss=40.2626
	step [121/147], loss=39.1801
	step [122/147], loss=41.4192
	step [123/147], loss=38.2992
	step [124/147], loss=39.7526
	step [125/147], loss=37.3585
	step [126/147], loss=39.2133
	step [127/147], loss=38.4081
	step [128/147], loss=39.4280
	step [129/147], loss=37.6548
	step [130/147], loss=43.3855
	step [131/147], loss=36.6145
	step [132/147], loss=41.0693
	step [133/147], loss=40.2388
	step [134/147], loss=39.0803
	step [135/147], loss=39.0493
	step [136/147], loss=38.2568
	step [137/147], loss=38.7872
	step [138/147], loss=37.3520
	step [139/147], loss=38.8104
	step [140/147], loss=38.3263
	step [141/147], loss=40.8042
	step [142/147], loss=38.1888
	step [143/147], loss=38.5450
	step [144/147], loss=36.3109
	step [145/147], loss=40.2358
	step [146/147], loss=39.3983
	step [147/147], loss=17.9691
	Evaluating
	loss=0.1490, precision=0.1600, recall=0.9935, f1=0.2756
Training epoch 5
	step [1/147], loss=40.2257
	step [2/147], loss=38.5235
	step [3/147], loss=38.5533
	step [4/147], loss=37.1155
	step [5/147], loss=38.5654
	step [6/147], loss=38.4101
	step [7/147], loss=39.0852
	step [8/147], loss=39.1021
	step [9/147], loss=41.1527
	step [10/147], loss=37.4795
	step [11/147], loss=37.7914
	step [12/147], loss=39.7878
	step [13/147], loss=39.2993
	step [14/147], loss=36.6869
	step [15/147], loss=39.3710
	step [16/147], loss=39.0687
	step [17/147], loss=38.7505
	step [18/147], loss=40.7182
	step [19/147], loss=36.4047
	step [20/147], loss=38.8729
	step [21/147], loss=36.8238
	step [22/147], loss=37.1069
	step [23/147], loss=38.0348
	step [24/147], loss=36.0689
	step [25/147], loss=37.2575
	step [26/147], loss=34.8985
	step [27/147], loss=37.2861
	step [28/147], loss=35.4368
	step [29/147], loss=38.9833
	step [30/147], loss=36.2678
	step [31/147], loss=36.4690
	step [32/147], loss=36.0288
	step [33/147], loss=35.9697
	step [34/147], loss=36.2137
	step [35/147], loss=35.8850
	step [36/147], loss=36.6518
	step [37/147], loss=38.3749
	step [38/147], loss=37.3975
	step [39/147], loss=37.4208
	step [40/147], loss=37.1669
	step [41/147], loss=35.0363
	step [42/147], loss=36.2590
	step [43/147], loss=35.7594
	step [44/147], loss=36.4763
	step [45/147], loss=35.9021
	step [46/147], loss=34.8957
	step [47/147], loss=37.4170
	step [48/147], loss=36.2350
	step [49/147], loss=33.8946
	step [50/147], loss=38.2182
	step [51/147], loss=35.4777
	step [52/147], loss=34.3453
	step [53/147], loss=34.6421
	step [54/147], loss=38.7224
	step [55/147], loss=36.0013
	step [56/147], loss=36.3788
	step [57/147], loss=34.5538
	step [58/147], loss=35.9079
	step [59/147], loss=40.6984
	step [60/147], loss=36.7237
	step [61/147], loss=35.7426
	step [62/147], loss=38.1017
	step [63/147], loss=35.1809
	step [64/147], loss=36.4367
	step [65/147], loss=36.9641
	step [66/147], loss=34.5298
	step [67/147], loss=36.7688
	step [68/147], loss=34.8288
	step [69/147], loss=36.6430
	step [70/147], loss=36.3915
	step [71/147], loss=34.0725
	step [72/147], loss=35.4467
	step [73/147], loss=38.8079
	step [74/147], loss=34.5495
	step [75/147], loss=35.7996
	step [76/147], loss=35.2594
	step [77/147], loss=35.9194
	step [78/147], loss=34.6558
	step [79/147], loss=36.4861
	step [80/147], loss=35.3643
	step [81/147], loss=36.3078
	step [82/147], loss=36.3067
	step [83/147], loss=34.8896
	step [84/147], loss=33.8552
	step [85/147], loss=35.1526
	step [86/147], loss=33.1322
	step [87/147], loss=35.6600
	step [88/147], loss=35.0824
	step [89/147], loss=34.9586
	step [90/147], loss=35.0615
	step [91/147], loss=33.9060
	step [92/147], loss=35.4143
	step [93/147], loss=36.0434
	step [94/147], loss=34.4876
	step [95/147], loss=34.4027
	step [96/147], loss=33.6671
	step [97/147], loss=34.3542
	step [98/147], loss=35.1023
	step [99/147], loss=34.2616
	step [100/147], loss=37.4898
	step [101/147], loss=35.5348
	step [102/147], loss=34.8907
	step [103/147], loss=33.3279
	step [104/147], loss=35.8810
	step [105/147], loss=31.8670
	step [106/147], loss=34.0435
	step [107/147], loss=34.8138
	step [108/147], loss=34.2156
	step [109/147], loss=33.7537
	step [110/147], loss=37.8210
	step [111/147], loss=34.3509
	step [112/147], loss=34.8325
	step [113/147], loss=32.5669
	step [114/147], loss=33.0396
	step [115/147], loss=36.3122
	step [116/147], loss=34.4474
	step [117/147], loss=32.1348
	step [118/147], loss=34.8251
	step [119/147], loss=33.1761
	step [120/147], loss=36.0448
	step [121/147], loss=35.4885
	step [122/147], loss=33.7641
	step [123/147], loss=32.8565
	step [124/147], loss=33.6225
	step [125/147], loss=32.2548
	step [126/147], loss=34.1125
	step [127/147], loss=34.6523
	step [128/147], loss=32.2954
	step [129/147], loss=34.2929
	step [130/147], loss=34.1119
	step [131/147], loss=35.4731
	step [132/147], loss=33.5195
	step [133/147], loss=33.9270
	step [134/147], loss=34.0130
	step [135/147], loss=35.0665
	step [136/147], loss=35.7190
	step [137/147], loss=34.1836
	step [138/147], loss=30.9017
	step [139/147], loss=31.3787
	step [140/147], loss=30.7609
	step [141/147], loss=37.8144
	step [142/147], loss=33.6986
	step [143/147], loss=33.5835
	step [144/147], loss=32.2982
	step [145/147], loss=34.8630
	step [146/147], loss=32.3634
	step [147/147], loss=16.4560
	Evaluating
	loss=0.1135, precision=0.1530, recall=0.9937, f1=0.2652
Training epoch 6
	step [1/147], loss=33.7273
	step [2/147], loss=32.7811
	step [3/147], loss=33.6698
	step [4/147], loss=33.7914
	step [5/147], loss=33.2345
	step [6/147], loss=34.8716
	step [7/147], loss=32.6212
	step [8/147], loss=33.2979
	step [9/147], loss=30.9593
	step [10/147], loss=33.2714
	step [11/147], loss=34.5338
	step [12/147], loss=32.3863
	step [13/147], loss=33.3828
	step [14/147], loss=31.1500
	step [15/147], loss=33.3106
	step [16/147], loss=33.8524
	step [17/147], loss=32.9266
	step [18/147], loss=32.1865
	step [19/147], loss=35.0908
	step [20/147], loss=29.0758
	step [21/147], loss=34.1890
	step [22/147], loss=32.5240
	step [23/147], loss=32.5041
	step [24/147], loss=32.3020
	step [25/147], loss=32.0736
	step [26/147], loss=31.1013
	step [27/147], loss=31.8582
	step [28/147], loss=33.6847
	step [29/147], loss=32.2193
	step [30/147], loss=34.3269
	step [31/147], loss=30.5213
	step [32/147], loss=30.9504
	step [33/147], loss=31.4290
	step [34/147], loss=31.0451
	step [35/147], loss=32.4299
	step [36/147], loss=30.1358
	step [37/147], loss=30.7814
	step [38/147], loss=29.9974
	step [39/147], loss=34.8037
	step [40/147], loss=29.8906
	step [41/147], loss=29.8969
	step [42/147], loss=33.8343
	step [43/147], loss=32.4711
	step [44/147], loss=31.0279
	step [45/147], loss=29.2288
	step [46/147], loss=30.8033
	step [47/147], loss=29.9807
	step [48/147], loss=31.7265
	step [49/147], loss=30.8006
	step [50/147], loss=32.5263
	step [51/147], loss=33.1741
	step [52/147], loss=32.1703
	step [53/147], loss=31.5528
	step [54/147], loss=30.1736
	step [55/147], loss=31.9893
	step [56/147], loss=30.7709
	step [57/147], loss=31.9027
	step [58/147], loss=31.8648
	step [59/147], loss=30.9136
	step [60/147], loss=31.2129
	step [61/147], loss=31.6818
	step [62/147], loss=29.2768
	step [63/147], loss=29.1011
	step [64/147], loss=32.6656
	step [65/147], loss=31.0123
	step [66/147], loss=30.2696
	step [67/147], loss=31.8907
	step [68/147], loss=32.5216
	step [69/147], loss=28.9896
	step [70/147], loss=32.2885
	step [71/147], loss=30.6267
	step [72/147], loss=34.6300
	step [73/147], loss=29.3433
	step [74/147], loss=28.7357
	step [75/147], loss=33.5034
	step [76/147], loss=33.4856
	step [77/147], loss=30.2509
	step [78/147], loss=30.3846
	step [79/147], loss=31.1904
	step [80/147], loss=30.1692
	step [81/147], loss=29.9161
	step [82/147], loss=30.3168
	step [83/147], loss=28.7204
	step [84/147], loss=30.4260
	step [85/147], loss=30.2255
	step [86/147], loss=28.0497
	step [87/147], loss=29.2014
	step [88/147], loss=31.4162
	step [89/147], loss=28.3628
	step [90/147], loss=31.3247
	step [91/147], loss=33.0766
	step [92/147], loss=31.1370
	step [93/147], loss=30.2612
	step [94/147], loss=28.3875
	step [95/147], loss=29.7613
	step [96/147], loss=28.8544
	step [97/147], loss=30.2357
	step [98/147], loss=29.8399
	step [99/147], loss=29.8515
	step [100/147], loss=30.8814
	step [101/147], loss=29.0961
	step [102/147], loss=28.9895
	step [103/147], loss=29.1629
	step [104/147], loss=27.1773
	step [105/147], loss=31.1298
	step [106/147], loss=32.0392
	step [107/147], loss=30.0646
	step [108/147], loss=33.2706
	step [109/147], loss=30.6493
	step [110/147], loss=31.4289
	step [111/147], loss=29.8687
	step [112/147], loss=30.1966
	step [113/147], loss=29.6206
	step [114/147], loss=30.1332
	step [115/147], loss=29.7059
	step [116/147], loss=31.6223
	step [117/147], loss=32.0523
	step [118/147], loss=30.7068
	step [119/147], loss=30.1173
	step [120/147], loss=32.5757
	step [121/147], loss=30.2448
	step [122/147], loss=29.2750
	step [123/147], loss=30.1397
	step [124/147], loss=29.9123
	step [125/147], loss=28.8742
	step [126/147], loss=31.2740
	step [127/147], loss=31.3662
	step [128/147], loss=30.1202
	step [129/147], loss=28.0240
	step [130/147], loss=30.2675
	step [131/147], loss=30.3468
	step [132/147], loss=29.8534
	step [133/147], loss=30.0313
	step [134/147], loss=30.2519
	step [135/147], loss=28.5678
	step [136/147], loss=30.8596
	step [137/147], loss=30.5061
	step [138/147], loss=28.6048
	step [139/147], loss=28.7387
	step [140/147], loss=29.4298
	step [141/147], loss=29.3069
	step [142/147], loss=28.8969
	step [143/147], loss=29.5576
	step [144/147], loss=29.1541
	step [145/147], loss=31.2170
	step [146/147], loss=27.8742
	step [147/147], loss=11.9205
	Evaluating
	loss=0.0889, precision=0.1757, recall=0.9936, f1=0.2986
Training epoch 7
	step [1/147], loss=29.5123
	step [2/147], loss=28.5781
	step [3/147], loss=29.3279
	step [4/147], loss=29.1165
	step [5/147], loss=28.9148
	step [6/147], loss=29.6393
	step [7/147], loss=32.2910
	step [8/147], loss=28.1405
	step [9/147], loss=26.6892
	step [10/147], loss=28.5977
	step [11/147], loss=27.1157
	step [12/147], loss=28.9728
	step [13/147], loss=27.3346
	step [14/147], loss=29.3968
	step [15/147], loss=26.6623
	step [16/147], loss=28.0644
	step [17/147], loss=28.5643
	step [18/147], loss=28.9794
	step [19/147], loss=31.0063
	step [20/147], loss=30.5367
	step [21/147], loss=28.0496
	step [22/147], loss=27.9866
	step [23/147], loss=28.2601
	step [24/147], loss=30.9316
	step [25/147], loss=28.7623
	step [26/147], loss=28.6619
	step [27/147], loss=29.0958
	step [28/147], loss=28.7313
	step [29/147], loss=30.0315
	step [30/147], loss=28.5335
	step [31/147], loss=27.3819
	step [32/147], loss=24.8853
	step [33/147], loss=27.1589
	step [34/147], loss=29.7679
	step [35/147], loss=32.5981
	step [36/147], loss=29.8471
	step [37/147], loss=29.0969
	step [38/147], loss=30.8036
	step [39/147], loss=29.0263
	step [40/147], loss=28.1893
	step [41/147], loss=27.3080
	step [42/147], loss=26.9975
	step [43/147], loss=26.2102
	step [44/147], loss=26.4826
	step [45/147], loss=28.9960
	step [46/147], loss=29.7201
	step [47/147], loss=26.6684
	step [48/147], loss=29.5279
	step [49/147], loss=31.3983
	step [50/147], loss=28.3938
	step [51/147], loss=26.4095
	step [52/147], loss=29.1496
	step [53/147], loss=26.6870
	step [54/147], loss=30.2014
	step [55/147], loss=30.3711
	step [56/147], loss=29.7308
	step [57/147], loss=26.8904
	step [58/147], loss=28.3456
	step [59/147], loss=29.2360
	step [60/147], loss=27.7796
	step [61/147], loss=26.5307
	step [62/147], loss=28.1515
	step [63/147], loss=25.9497
	step [64/147], loss=28.0632
	step [65/147], loss=27.9492
	step [66/147], loss=26.3335
	step [67/147], loss=28.1613
	step [68/147], loss=29.5942
	step [69/147], loss=27.2796
	step [70/147], loss=27.0641
	step [71/147], loss=25.1304
	step [72/147], loss=27.5570
	step [73/147], loss=25.4640
	step [74/147], loss=28.2163
	step [75/147], loss=28.8735
	step [76/147], loss=27.0212
	step [77/147], loss=26.7330
	step [78/147], loss=28.8056
	step [79/147], loss=27.4444
	step [80/147], loss=26.1432
	step [81/147], loss=27.0876
	step [82/147], loss=26.5810
	step [83/147], loss=28.0247
	step [84/147], loss=27.2830
	step [85/147], loss=28.5037
	step [86/147], loss=28.5855
	step [87/147], loss=27.9282
	step [88/147], loss=26.2394
	step [89/147], loss=25.4338
	step [90/147], loss=27.7210
	step [91/147], loss=27.5841
	step [92/147], loss=25.6439
	step [93/147], loss=28.8374
	step [94/147], loss=27.6509
	step [95/147], loss=28.1750
	step [96/147], loss=29.2684
	step [97/147], loss=29.5634
	step [98/147], loss=25.4183
	step [99/147], loss=27.8876
	step [100/147], loss=27.0568
	step [101/147], loss=27.6773
	step [102/147], loss=26.3285
	step [103/147], loss=30.5112
	step [104/147], loss=27.7900
	step [105/147], loss=29.4513
	step [106/147], loss=26.6005
	step [107/147], loss=26.5328
	step [108/147], loss=27.1068
	step [109/147], loss=26.1981
	step [110/147], loss=25.5671
	step [111/147], loss=28.9483
	step [112/147], loss=27.6964
	step [113/147], loss=25.4168
	step [114/147], loss=23.9001
	step [115/147], loss=28.0390
	step [116/147], loss=26.2776
	step [117/147], loss=24.2708
	step [118/147], loss=25.7268
	step [119/147], loss=25.4062
	step [120/147], loss=29.4472
	step [121/147], loss=25.6298
	step [122/147], loss=24.3752
	step [123/147], loss=26.3920
	step [124/147], loss=25.8872
	step [125/147], loss=27.0130
	step [126/147], loss=28.3304
	step [127/147], loss=28.1394
	step [128/147], loss=25.0573
	step [129/147], loss=25.8827
	step [130/147], loss=26.0605
	step [131/147], loss=25.2373
	step [132/147], loss=27.3431
	step [133/147], loss=26.0825
	step [134/147], loss=27.7571
	step [135/147], loss=26.3965
	step [136/147], loss=25.8369
	step [137/147], loss=28.1046
	step [138/147], loss=27.3079
	step [139/147], loss=26.1542
	step [140/147], loss=25.7868
	step [141/147], loss=25.9412
	step [142/147], loss=28.1259
	step [143/147], loss=24.8367
	step [144/147], loss=24.8245
	step [145/147], loss=25.1133
	step [146/147], loss=25.5645
	step [147/147], loss=13.2573
	Evaluating
	loss=0.0805, precision=0.1754, recall=0.9931, f1=0.2982
Training epoch 8
	step [1/147], loss=24.9997
	step [2/147], loss=26.6572
	step [3/147], loss=26.1222
	step [4/147], loss=25.2949
	step [5/147], loss=28.8107
	step [6/147], loss=24.8406
	step [7/147], loss=25.1749
	step [8/147], loss=25.9036
	step [9/147], loss=29.4260
	step [10/147], loss=24.7109
	step [11/147], loss=27.6141
	step [12/147], loss=25.2025
	step [13/147], loss=25.1671
	step [14/147], loss=25.9336
	step [15/147], loss=25.6922
	step [16/147], loss=24.0597
	step [17/147], loss=23.8696
	step [18/147], loss=25.3258
	step [19/147], loss=26.8229
	step [20/147], loss=26.0119
	step [21/147], loss=23.9747
	step [22/147], loss=25.0123
	step [23/147], loss=27.6935
	step [24/147], loss=26.8566
	step [25/147], loss=25.7538
	step [26/147], loss=24.9808
	step [27/147], loss=26.7545
	step [28/147], loss=27.3607
	step [29/147], loss=25.3100
	step [30/147], loss=26.7889
	step [31/147], loss=24.6040
	step [32/147], loss=25.1904
	step [33/147], loss=28.9803
	step [34/147], loss=26.3052
	step [35/147], loss=24.9021
	step [36/147], loss=25.0586
	step [37/147], loss=25.7221
	step [38/147], loss=24.2192
	step [39/147], loss=26.5827
	step [40/147], loss=27.2378
	step [41/147], loss=25.5407
	step [42/147], loss=25.0558
	step [43/147], loss=24.5124
	step [44/147], loss=24.2901
	step [45/147], loss=26.0604
	step [46/147], loss=23.8643
	step [47/147], loss=26.1165
	step [48/147], loss=26.5143
	step [49/147], loss=25.3338
	step [50/147], loss=25.7805
	step [51/147], loss=26.2256
	step [52/147], loss=28.3192
	step [53/147], loss=24.8984
	step [54/147], loss=25.5440
	step [55/147], loss=25.8726
	step [56/147], loss=24.5487
	step [57/147], loss=25.8587
	step [58/147], loss=25.5451
	step [59/147], loss=26.1364
	step [60/147], loss=25.4176
	step [61/147], loss=25.5666
	step [62/147], loss=24.2184
	step [63/147], loss=25.6268
	step [64/147], loss=26.8854
	step [65/147], loss=25.5032
	step [66/147], loss=23.8134
	step [67/147], loss=23.9992
	step [68/147], loss=27.3310
	step [69/147], loss=27.5251
	step [70/147], loss=27.1583
	step [71/147], loss=24.4657
	step [72/147], loss=25.4258
	step [73/147], loss=24.6080
	step [74/147], loss=24.2474
	step [75/147], loss=25.3608
	step [76/147], loss=25.8403
	step [77/147], loss=24.1613
	step [78/147], loss=24.5633
	step [79/147], loss=26.0806
	step [80/147], loss=26.0280
	step [81/147], loss=25.4387
	step [82/147], loss=28.2050
	step [83/147], loss=23.6411
	step [84/147], loss=22.8962
	step [85/147], loss=24.3215
	step [86/147], loss=21.4220
	step [87/147], loss=23.2065
	step [88/147], loss=22.1165
	step [89/147], loss=27.2567
	step [90/147], loss=24.1144
	step [91/147], loss=24.7500
	step [92/147], loss=24.4610
	step [93/147], loss=26.8485
	step [94/147], loss=22.8853
	step [95/147], loss=24.6047
	step [96/147], loss=28.1658
	step [97/147], loss=22.5958
	step [98/147], loss=21.8027
	step [99/147], loss=24.1852
	step [100/147], loss=25.5942
	step [101/147], loss=24.7479
	step [102/147], loss=25.2367
	step [103/147], loss=24.3989
	step [104/147], loss=26.2096
	step [105/147], loss=24.7930
	step [106/147], loss=25.6663
	step [107/147], loss=24.0207
	step [108/147], loss=25.0145
	step [109/147], loss=23.6920
	step [110/147], loss=24.4364
	step [111/147], loss=26.7071
	step [112/147], loss=22.5438
	step [113/147], loss=24.6571
	step [114/147], loss=25.7261
	step [115/147], loss=25.9394
	step [116/147], loss=23.0881
	step [117/147], loss=25.3425
	step [118/147], loss=26.2289
	step [119/147], loss=23.3023
	step [120/147], loss=25.8898
	step [121/147], loss=25.9386
	step [122/147], loss=26.0250
	step [123/147], loss=24.5768
	step [124/147], loss=22.2798
	step [125/147], loss=25.5194
	step [126/147], loss=25.2717
	step [127/147], loss=25.0933
	step [128/147], loss=22.6523
	step [129/147], loss=22.7819
	step [130/147], loss=23.8856
	step [131/147], loss=25.0013
	step [132/147], loss=23.3289
	step [133/147], loss=23.8593
	step [134/147], loss=22.5800
	step [135/147], loss=22.6971
	step [136/147], loss=24.9207
	step [137/147], loss=24.9040
	step [138/147], loss=23.0087
	step [139/147], loss=26.0188
	step [140/147], loss=23.0951
	step [141/147], loss=25.7860
	step [142/147], loss=23.7018
	step [143/147], loss=24.9803
	step [144/147], loss=24.1026
	step [145/147], loss=22.4746
	step [146/147], loss=25.7570
	step [147/147], loss=9.7182
	Evaluating
	loss=0.0707, precision=0.1651, recall=0.9938, f1=0.2832
Training epoch 9
	step [1/147], loss=25.4953
	step [2/147], loss=24.8436
	step [3/147], loss=26.3199
	step [4/147], loss=23.9194
	step [5/147], loss=23.8154
	step [6/147], loss=22.2007
	step [7/147], loss=24.9751
	step [8/147], loss=23.2581
	step [9/147], loss=22.1469
	step [10/147], loss=24.2958
	step [11/147], loss=23.2445
	step [12/147], loss=23.5149
	step [13/147], loss=22.4126
	step [14/147], loss=23.7653
	step [15/147], loss=23.5084
	step [16/147], loss=24.1232
	step [17/147], loss=26.4664
	step [18/147], loss=23.3576
	step [19/147], loss=26.5026
	step [20/147], loss=24.3140
	step [21/147], loss=26.2402
	step [22/147], loss=25.6956
	step [23/147], loss=24.5477
	step [24/147], loss=23.6373
	step [25/147], loss=23.5969
	step [26/147], loss=24.6743
	step [27/147], loss=24.5981
	step [28/147], loss=21.2158
	step [29/147], loss=24.5189
	step [30/147], loss=25.8774
	step [31/147], loss=25.7464
	step [32/147], loss=24.0853
	step [33/147], loss=24.2221
	step [34/147], loss=26.8074
	step [35/147], loss=22.4300
	step [36/147], loss=23.0043
	step [37/147], loss=21.7805
	step [38/147], loss=24.1420
	step [39/147], loss=22.0273
	step [40/147], loss=23.8552
	step [41/147], loss=23.6025
	step [42/147], loss=24.7038
	step [43/147], loss=23.0923
	step [44/147], loss=25.1230
	step [45/147], loss=21.8236
	step [46/147], loss=22.4134
	step [47/147], loss=22.9099
	step [48/147], loss=23.1066
	step [49/147], loss=23.4496
	step [50/147], loss=23.5218
	step [51/147], loss=23.5145
	step [52/147], loss=22.5566
	step [53/147], loss=25.3665
	step [54/147], loss=22.1271
	step [55/147], loss=22.5354
	step [56/147], loss=23.0133
	step [57/147], loss=24.0416
	step [58/147], loss=21.7029
	step [59/147], loss=20.6219
	step [60/147], loss=21.3829
	step [61/147], loss=21.5427
	step [62/147], loss=22.6995
	step [63/147], loss=20.5622
	step [64/147], loss=23.6929
	step [65/147], loss=25.1171
	step [66/147], loss=23.4716
	step [67/147], loss=24.1475
	step [68/147], loss=22.4070
	step [69/147], loss=24.0789
	step [70/147], loss=24.0917
	step [71/147], loss=24.8520
	step [72/147], loss=23.1830
	step [73/147], loss=23.3955
	step [74/147], loss=21.8113
	step [75/147], loss=26.1630
	step [76/147], loss=22.0710
	step [77/147], loss=21.7777
	step [78/147], loss=22.0372
	step [79/147], loss=24.0538
	step [80/147], loss=20.8212
	step [81/147], loss=23.6275
	step [82/147], loss=25.2034
	step [83/147], loss=20.3415
	step [84/147], loss=25.6292
	step [85/147], loss=24.8517
	step [86/147], loss=20.4506
	step [87/147], loss=23.1008
	step [88/147], loss=23.2451
	step [89/147], loss=23.4618
	step [90/147], loss=23.3911
	step [91/147], loss=23.6899
	step [92/147], loss=22.6528
	step [93/147], loss=23.2662
	step [94/147], loss=22.1347
	step [95/147], loss=25.4193
	step [96/147], loss=22.1606
	step [97/147], loss=22.7960
	step [98/147], loss=21.5873
	step [99/147], loss=25.2469
	step [100/147], loss=20.3800
	step [101/147], loss=23.1070
	step [102/147], loss=22.8297
	step [103/147], loss=22.2836
	step [104/147], loss=23.2660
	step [105/147], loss=23.0931
	step [106/147], loss=22.2701
	step [107/147], loss=25.9847
	step [108/147], loss=22.6380
	step [109/147], loss=21.1673
	step [110/147], loss=21.2002
	step [111/147], loss=20.1188
	step [112/147], loss=24.3370
	step [113/147], loss=22.9798
	step [114/147], loss=20.4640
	step [115/147], loss=22.5831
	step [116/147], loss=23.2536
	step [117/147], loss=23.9276
	step [118/147], loss=21.8279
	step [119/147], loss=23.7733
	step [120/147], loss=21.6808
	step [121/147], loss=18.9906
	step [122/147], loss=20.9434
	step [123/147], loss=22.0132
	step [124/147], loss=20.4472
	step [125/147], loss=22.1830
	step [126/147], loss=24.3512
	step [127/147], loss=23.9044
	step [128/147], loss=21.3150
	step [129/147], loss=22.6207
	step [130/147], loss=21.4208
	step [131/147], loss=20.7103
	step [132/147], loss=23.6028
	step [133/147], loss=20.1991
	step [134/147], loss=22.2282
	step [135/147], loss=22.6947
	step [136/147], loss=24.2237
	step [137/147], loss=22.0958
	step [138/147], loss=23.6335
	step [139/147], loss=25.4939
	step [140/147], loss=25.2567
	step [141/147], loss=24.0452
	step [142/147], loss=22.2259
	step [143/147], loss=24.8342
	step [144/147], loss=19.6087
	step [145/147], loss=22.7778
	step [146/147], loss=22.1322
	step [147/147], loss=9.8647
	Evaluating
	loss=0.0620, precision=0.1849, recall=0.9936, f1=0.3117
Training epoch 10
	step [1/147], loss=22.6104
	step [2/147], loss=23.7493
	step [3/147], loss=24.5944
	step [4/147], loss=23.7413
	step [5/147], loss=22.0108
	step [6/147], loss=21.8417
	step [7/147], loss=19.9785
	step [8/147], loss=21.6564
	step [9/147], loss=23.1135
	step [10/147], loss=21.5016
	step [11/147], loss=21.4422
	step [12/147], loss=22.5810
	step [13/147], loss=21.7531
	step [14/147], loss=22.8882
	step [15/147], loss=23.1594
	step [16/147], loss=22.1277
	step [17/147], loss=20.1744
	step [18/147], loss=23.6622
	step [19/147], loss=22.0774
	step [20/147], loss=19.6481
	step [21/147], loss=21.6754
	step [22/147], loss=21.8980
	step [23/147], loss=23.5738
	step [24/147], loss=22.8242
	step [25/147], loss=20.3136
	step [26/147], loss=23.8496
	step [27/147], loss=22.2859
	step [28/147], loss=19.0972
	step [29/147], loss=21.1256
	step [30/147], loss=23.5299
	step [31/147], loss=22.2086
	step [32/147], loss=19.5879
	step [33/147], loss=22.8758
	step [34/147], loss=23.7316
	step [35/147], loss=22.4626
	step [36/147], loss=19.9968
	step [37/147], loss=20.4996
	step [38/147], loss=21.5263
	step [39/147], loss=21.2809
	step [40/147], loss=21.8813
	step [41/147], loss=23.2120
	step [42/147], loss=24.0144
	step [43/147], loss=20.9569
	step [44/147], loss=23.5989
	step [45/147], loss=21.8568
	step [46/147], loss=23.4235
	step [47/147], loss=24.0996
	step [48/147], loss=21.4606
	step [49/147], loss=22.4405
	step [50/147], loss=19.2580
	step [51/147], loss=19.6512
	step [52/147], loss=19.2107
	step [53/147], loss=22.3351
	step [54/147], loss=21.0301
	step [55/147], loss=24.0710
	step [56/147], loss=20.5808
	step [57/147], loss=21.5533
	step [58/147], loss=22.9084
	step [59/147], loss=23.8452
	step [60/147], loss=20.2814
	step [61/147], loss=21.2029
	step [62/147], loss=21.7096
	step [63/147], loss=21.7908
	step [64/147], loss=20.1505
	step [65/147], loss=23.7604
	step [66/147], loss=22.0136
	step [67/147], loss=24.6640
	step [68/147], loss=21.0916
	step [69/147], loss=20.7325
	step [70/147], loss=19.8694
	step [71/147], loss=20.7874
	step [72/147], loss=20.6785
	step [73/147], loss=22.8435
	step [74/147], loss=22.7468
	step [75/147], loss=21.5536
	step [76/147], loss=23.6265
	step [77/147], loss=21.8696
	step [78/147], loss=22.0880
	step [79/147], loss=22.9692
	step [80/147], loss=20.9315
	step [81/147], loss=21.8945
	step [82/147], loss=21.6914
	step [83/147], loss=24.2036
	step [84/147], loss=23.5137
	step [85/147], loss=22.2857
	step [86/147], loss=20.9806
	step [87/147], loss=21.6730
	step [88/147], loss=20.3531
	step [89/147], loss=21.6805
	step [90/147], loss=20.8116
	step [91/147], loss=22.7131
	step [92/147], loss=21.8153
	step [93/147], loss=22.2419
	step [94/147], loss=22.4989
	step [95/147], loss=21.7992
	step [96/147], loss=19.4145
	step [97/147], loss=20.8737
	step [98/147], loss=21.2228
	step [99/147], loss=19.7261
	step [100/147], loss=24.7787
	step [101/147], loss=21.9240
	step [102/147], loss=19.2351
	step [103/147], loss=22.9050
	step [104/147], loss=20.1224
	step [105/147], loss=20.4934
	step [106/147], loss=21.2058
	step [107/147], loss=21.4069
	step [108/147], loss=21.6979
	step [109/147], loss=20.9369
	step [110/147], loss=21.5845
	step [111/147], loss=18.9368
	step [112/147], loss=20.6473
	step [113/147], loss=18.3639
	step [114/147], loss=21.0799
	step [115/147], loss=19.8556
	step [116/147], loss=20.5675
	step [117/147], loss=21.1505
	step [118/147], loss=21.9105
	step [119/147], loss=19.1226
	step [120/147], loss=20.5358
	step [121/147], loss=19.7080
	step [122/147], loss=21.4964
	step [123/147], loss=19.7642
	step [124/147], loss=21.5879
	step [125/147], loss=21.3960
	step [126/147], loss=22.1594
	step [127/147], loss=21.2763
	step [128/147], loss=22.9023
	step [129/147], loss=20.0255
	step [130/147], loss=21.5139
	step [131/147], loss=17.3625
	step [132/147], loss=22.7711
	step [133/147], loss=18.9141
	step [134/147], loss=22.0637
	step [135/147], loss=19.7948
	step [136/147], loss=19.9011
	step [137/147], loss=21.6984
	step [138/147], loss=21.1044
	step [139/147], loss=19.3096
	step [140/147], loss=22.4175
	step [141/147], loss=22.1278
	step [142/147], loss=19.2435
	step [143/147], loss=24.5317
	step [144/147], loss=20.6800
	step [145/147], loss=24.2773
	step [146/147], loss=21.2735
	step [147/147], loss=12.6720
	Evaluating
	loss=0.0685, precision=0.1265, recall=0.9950, f1=0.2245
Training epoch 11
	step [1/147], loss=19.5947
	step [2/147], loss=20.0658
	step [3/147], loss=22.3202
	step [4/147], loss=22.7057
	step [5/147], loss=20.4573
	step [6/147], loss=21.6333
	step [7/147], loss=22.0550
	step [8/147], loss=20.3288
	step [9/147], loss=19.1521
	step [10/147], loss=21.6490
	step [11/147], loss=19.1461
	step [12/147], loss=20.8272
	step [13/147], loss=22.3151
	step [14/147], loss=21.4471
	step [15/147], loss=19.7348
	step [16/147], loss=20.9067
	step [17/147], loss=21.8589
	step [18/147], loss=21.5042
	step [19/147], loss=18.9231
	step [20/147], loss=22.0549
	step [21/147], loss=20.7984
	step [22/147], loss=19.1732
	step [23/147], loss=23.5649
	step [24/147], loss=20.9902
	step [25/147], loss=19.8319
	step [26/147], loss=21.4948
	step [27/147], loss=19.9800
	step [28/147], loss=23.6691
	step [29/147], loss=22.4176
	step [30/147], loss=22.2464
	step [31/147], loss=18.7081
	step [32/147], loss=20.9023
	step [33/147], loss=21.2843
	step [34/147], loss=18.1627
	step [35/147], loss=19.8340
	step [36/147], loss=19.8953
	step [37/147], loss=21.4092
	step [38/147], loss=19.6925
	step [39/147], loss=21.5147
	step [40/147], loss=25.9997
	step [41/147], loss=20.1188
	step [42/147], loss=20.8186
	step [43/147], loss=20.9857
	step [44/147], loss=20.6541
	step [45/147], loss=20.6069
	step [46/147], loss=17.9705
	step [47/147], loss=17.4397
	step [48/147], loss=21.5869
	step [49/147], loss=18.6954
	step [50/147], loss=21.2784
	step [51/147], loss=19.7832
	step [52/147], loss=20.5294
	step [53/147], loss=19.5040
	step [54/147], loss=18.9735
	step [55/147], loss=19.1420
	step [56/147], loss=19.2369
	step [57/147], loss=23.6504
	step [58/147], loss=19.4019
	step [59/147], loss=18.4090
	step [60/147], loss=20.9847
	step [61/147], loss=22.3964
	step [62/147], loss=20.3916
	step [63/147], loss=18.5941
	step [64/147], loss=21.9883
	step [65/147], loss=20.5507
	step [66/147], loss=20.6774
	step [67/147], loss=20.0468
	step [68/147], loss=20.8172
	step [69/147], loss=19.9340
	step [70/147], loss=20.1885
	step [71/147], loss=19.8589
	step [72/147], loss=22.8138
	step [73/147], loss=21.5516
	step [74/147], loss=22.3119
	step [75/147], loss=19.0424
	step [76/147], loss=19.7038
	step [77/147], loss=18.6333
	step [78/147], loss=23.6923
	step [79/147], loss=19.1633
	step [80/147], loss=19.1164
	step [81/147], loss=19.8474
	step [82/147], loss=22.3237
	step [83/147], loss=19.1425
	step [84/147], loss=18.8405
	step [85/147], loss=19.5981
	step [86/147], loss=19.7635
	step [87/147], loss=19.2424
	step [88/147], loss=19.6028
	step [89/147], loss=19.6989
	step [90/147], loss=22.1746
	step [91/147], loss=21.0956
	step [92/147], loss=20.6043
	step [93/147], loss=20.0076
	step [94/147], loss=19.3992
	step [95/147], loss=20.1247
	step [96/147], loss=22.6070
	step [97/147], loss=21.3930
	step [98/147], loss=19.1287
	step [99/147], loss=19.5869
	step [100/147], loss=19.8959
	step [101/147], loss=20.5256
	step [102/147], loss=20.0789
	step [103/147], loss=20.1150
	step [104/147], loss=18.7975
	step [105/147], loss=19.5386
	step [106/147], loss=19.5559
	step [107/147], loss=18.9366
	step [108/147], loss=20.9540
	step [109/147], loss=19.9448
	step [110/147], loss=21.0502
	step [111/147], loss=20.0968
	step [112/147], loss=21.2165
	step [113/147], loss=19.2768
	step [114/147], loss=17.5784
	step [115/147], loss=17.8486
	step [116/147], loss=20.1220
	step [117/147], loss=18.7615
	step [118/147], loss=19.5812
	step [119/147], loss=18.3908
	step [120/147], loss=19.7603
	step [121/147], loss=19.4008
	step [122/147], loss=19.9894
	step [123/147], loss=21.2627
	step [124/147], loss=18.7986
	step [125/147], loss=19.4725
	step [126/147], loss=19.4850
	step [127/147], loss=18.8494
	step [128/147], loss=20.4537
	step [129/147], loss=21.2621
	step [130/147], loss=17.7270
	step [131/147], loss=20.9178
	step [132/147], loss=19.5006
	step [133/147], loss=19.8598
	step [134/147], loss=17.7062
	step [135/147], loss=21.7348
	step [136/147], loss=18.9272
	step [137/147], loss=23.3325
	step [138/147], loss=19.3202
	step [139/147], loss=19.1371
	step [140/147], loss=19.9383
	step [141/147], loss=21.9559
	step [142/147], loss=18.2474
	step [143/147], loss=18.8607
	step [144/147], loss=20.0949
	step [145/147], loss=19.3755
	step [146/147], loss=18.8068
	step [147/147], loss=8.5144
	Evaluating
	loss=0.0490, precision=0.2075, recall=0.9927, f1=0.3433
saving model as: 0_saved_model.pth
Training epoch 12
	step [1/147], loss=18.6856
	step [2/147], loss=18.5626
	step [3/147], loss=19.5004
	step [4/147], loss=19.9589
	step [5/147], loss=19.9440
	step [6/147], loss=20.3550
	step [7/147], loss=18.4257
	step [8/147], loss=19.5485
	step [9/147], loss=20.8348
	step [10/147], loss=18.5627
	step [11/147], loss=21.0688
	step [12/147], loss=19.5453
	step [13/147], loss=17.5464
	step [14/147], loss=19.5147
	step [15/147], loss=19.0422
	step [16/147], loss=20.5136
	step [17/147], loss=18.9951
	step [18/147], loss=17.8817
	step [19/147], loss=18.5457
	step [20/147], loss=20.5731
	step [21/147], loss=19.7590
	step [22/147], loss=18.1669
	step [23/147], loss=22.3856
	step [24/147], loss=19.0076
	step [25/147], loss=22.3061
	step [26/147], loss=18.4655
	step [27/147], loss=20.7989
	step [28/147], loss=18.2482
	step [29/147], loss=22.3697
	step [30/147], loss=20.9688
	step [31/147], loss=20.4809
	step [32/147], loss=18.9253
	step [33/147], loss=21.6646
	step [34/147], loss=19.5265
	step [35/147], loss=18.5055
	step [36/147], loss=21.4004
	step [37/147], loss=20.4696
	step [38/147], loss=17.3488
	step [39/147], loss=22.4767
	step [40/147], loss=20.6347
	step [41/147], loss=18.6418
	step [42/147], loss=18.6525
	step [43/147], loss=20.0577
	step [44/147], loss=18.4702
	step [45/147], loss=20.0472
	step [46/147], loss=19.9055
	step [47/147], loss=21.6569
	step [48/147], loss=19.4141
	step [49/147], loss=20.8986
	step [50/147], loss=19.0455
	step [51/147], loss=19.7822
	step [52/147], loss=18.5261
	step [53/147], loss=23.3508
	step [54/147], loss=20.6031
	step [55/147], loss=19.3938
	step [56/147], loss=20.8762
	step [57/147], loss=19.3792
	step [58/147], loss=20.0881
	step [59/147], loss=18.8673
	step [60/147], loss=22.5201
	step [61/147], loss=17.5000
	step [62/147], loss=18.5176
	step [63/147], loss=18.7478
	step [64/147], loss=19.9722
	step [65/147], loss=18.7158
	step [66/147], loss=18.9818
	step [67/147], loss=19.6823
	step [68/147], loss=17.4595
	step [69/147], loss=16.3384
	step [70/147], loss=17.8024
	step [71/147], loss=18.5970
	step [72/147], loss=20.0600
	step [73/147], loss=16.3942
	step [74/147], loss=20.5532
	step [75/147], loss=16.9600
	step [76/147], loss=18.1161
	step [77/147], loss=18.5208
	step [78/147], loss=19.3418
	step [79/147], loss=19.0440
	step [80/147], loss=18.9529
	step [81/147], loss=19.0440
	step [82/147], loss=17.4807
	step [83/147], loss=18.8292
	step [84/147], loss=22.4936
	step [85/147], loss=20.3917
	step [86/147], loss=22.1034
	step [87/147], loss=17.2723
	step [88/147], loss=20.7489
	step [89/147], loss=16.9471
	step [90/147], loss=22.9015
	step [91/147], loss=19.9950
	step [92/147], loss=20.0727
	step [93/147], loss=20.7210
	step [94/147], loss=18.6957
	step [95/147], loss=17.9241
	step [96/147], loss=19.1617
	step [97/147], loss=16.3157
	step [98/147], loss=19.4519
	step [99/147], loss=19.8915
	step [100/147], loss=18.1196
	step [101/147], loss=21.6446
	step [102/147], loss=16.7343
	step [103/147], loss=18.8287
	step [104/147], loss=21.0495
	step [105/147], loss=18.5427
	step [106/147], loss=22.1386
	step [107/147], loss=19.6163
	step [108/147], loss=19.9012
	step [109/147], loss=22.6535
	step [110/147], loss=18.7598
	step [111/147], loss=20.7136
	step [112/147], loss=18.3006
	step [113/147], loss=19.9552
	step [114/147], loss=20.0051
	step [115/147], loss=18.5788
	step [116/147], loss=20.4899
	step [117/147], loss=19.8038
	step [118/147], loss=23.1247
	step [119/147], loss=15.8761
	step [120/147], loss=18.8209
	step [121/147], loss=19.4865
	step [122/147], loss=19.4717
	step [123/147], loss=18.1912
	step [124/147], loss=20.5694
	step [125/147], loss=18.4179
	step [126/147], loss=18.2482
	step [127/147], loss=21.0078
	step [128/147], loss=19.2682
	step [129/147], loss=20.0816
	step [130/147], loss=18.2401
	step [131/147], loss=19.9658
	step [132/147], loss=20.2027
	step [133/147], loss=18.3286
	step [134/147], loss=17.8396
	step [135/147], loss=18.6766
	step [136/147], loss=18.9591
	step [137/147], loss=21.2121
	step [138/147], loss=17.7857
	step [139/147], loss=17.7426
	step [140/147], loss=20.3348
	step [141/147], loss=18.3926
	step [142/147], loss=18.2785
	step [143/147], loss=18.3365
	step [144/147], loss=19.8311
	step [145/147], loss=18.5936
	step [146/147], loss=18.3531
	step [147/147], loss=7.7022
	Evaluating
	loss=0.0500, precision=0.1698, recall=0.9941, f1=0.2901
Training epoch 13
	step [1/147], loss=17.8403
	step [2/147], loss=19.7233
	step [3/147], loss=16.9953
	step [4/147], loss=20.3372
	step [5/147], loss=20.4090
	step [6/147], loss=21.2228
	step [7/147], loss=20.2451
	step [8/147], loss=20.0441
	step [9/147], loss=18.4480
	step [10/147], loss=17.5977
	step [11/147], loss=18.6530
	step [12/147], loss=17.3140
	step [13/147], loss=16.6508
	step [14/147], loss=16.3621
	step [15/147], loss=18.9626
	step [16/147], loss=19.0160
	step [17/147], loss=19.9554
	step [18/147], loss=18.9020
	step [19/147], loss=17.3231
	step [20/147], loss=18.1910
	step [21/147], loss=17.4435
	step [22/147], loss=17.3592
	step [23/147], loss=17.8717
	step [24/147], loss=16.6488
	step [25/147], loss=17.2339
	step [26/147], loss=19.6918
	step [27/147], loss=19.7741
	step [28/147], loss=17.5493
	step [29/147], loss=18.0111
	step [30/147], loss=18.3031
	step [31/147], loss=19.1630
	step [32/147], loss=19.4964
	step [33/147], loss=18.0779
	step [34/147], loss=17.8225
	step [35/147], loss=19.4327
	step [36/147], loss=22.3363
	step [37/147], loss=20.3488
	step [38/147], loss=19.7923
	step [39/147], loss=19.1106
	step [40/147], loss=20.9001
	step [41/147], loss=20.7145
	step [42/147], loss=18.2490
	step [43/147], loss=18.8193
	step [44/147], loss=18.5508
	step [45/147], loss=18.4663
	step [46/147], loss=20.5400
	step [47/147], loss=19.9449
	step [48/147], loss=18.7404
	step [49/147], loss=17.8816
	step [50/147], loss=21.5147
	step [51/147], loss=18.0862
	step [52/147], loss=19.0745
	step [53/147], loss=19.7830
	step [54/147], loss=17.2490
	step [55/147], loss=18.5914
	step [56/147], loss=18.6945
	step [57/147], loss=20.3464
	step [58/147], loss=18.2911
	step [59/147], loss=18.1394
	step [60/147], loss=17.6511
	step [61/147], loss=19.0305
	step [62/147], loss=19.2291
	step [63/147], loss=18.3195
	step [64/147], loss=17.9582
	step [65/147], loss=16.1521
	step [66/147], loss=18.7959
	step [67/147], loss=21.5439
	step [68/147], loss=17.3599
	step [69/147], loss=17.2985
	step [70/147], loss=16.9547
	step [71/147], loss=16.9794
	step [72/147], loss=16.2558
	step [73/147], loss=18.4127
	step [74/147], loss=19.6727
	step [75/147], loss=17.5831
	step [76/147], loss=17.4344
	step [77/147], loss=18.5757
	step [78/147], loss=17.2900
	step [79/147], loss=17.5413
	step [80/147], loss=17.8035
	step [81/147], loss=17.7598
	step [82/147], loss=19.5739
	step [83/147], loss=15.8207
	step [84/147], loss=18.8226
	step [85/147], loss=16.8967
	step [86/147], loss=17.2679
	step [87/147], loss=16.5586
	step [88/147], loss=15.1177
	step [89/147], loss=17.7048
	step [90/147], loss=19.7583
	step [91/147], loss=16.3732
	step [92/147], loss=16.9832
	step [93/147], loss=21.5160
	step [94/147], loss=17.8774
	step [95/147], loss=17.3824
	step [96/147], loss=20.5612
	step [97/147], loss=17.5812
	step [98/147], loss=17.6666
	step [99/147], loss=18.5475
	step [100/147], loss=17.4314
	step [101/147], loss=16.1086
	step [102/147], loss=18.7137
	step [103/147], loss=16.9874
	step [104/147], loss=18.3528
	step [105/147], loss=19.0005
	step [106/147], loss=16.0511
	step [107/147], loss=18.3812
	step [108/147], loss=21.8774
	step [109/147], loss=18.2552
	step [110/147], loss=19.7433
	step [111/147], loss=17.8711
	step [112/147], loss=17.4098
	step [113/147], loss=15.9224
	step [114/147], loss=16.7050
	step [115/147], loss=20.3211
	step [116/147], loss=20.5999
	step [117/147], loss=16.8821
	step [118/147], loss=18.1023
	step [119/147], loss=17.2810
	step [120/147], loss=18.2716
	step [121/147], loss=17.9268
	step [122/147], loss=17.4803
	step [123/147], loss=17.8158
	step [124/147], loss=20.0971
	step [125/147], loss=17.8817
	step [126/147], loss=16.2053
	step [127/147], loss=21.1478
	step [128/147], loss=19.3027
	step [129/147], loss=19.3654
	step [130/147], loss=15.7101
	step [131/147], loss=16.5715
	step [132/147], loss=19.8782
	step [133/147], loss=20.0987
	step [134/147], loss=20.0027
	step [135/147], loss=16.2337
	step [136/147], loss=17.8902
	step [137/147], loss=18.5193
	step [138/147], loss=17.5876
	step [139/147], loss=22.2492
	step [140/147], loss=18.8653
	step [141/147], loss=15.5355
	step [142/147], loss=17.5801
	step [143/147], loss=19.2165
	step [144/147], loss=17.9285
	step [145/147], loss=19.7295
	step [146/147], loss=17.8512
	step [147/147], loss=7.6366
	Evaluating
	loss=0.0501, precision=0.1534, recall=0.9942, f1=0.2658
Training epoch 14
	step [1/147], loss=20.5776
	step [2/147], loss=16.5552
	step [3/147], loss=17.5680
	step [4/147], loss=16.7058
	step [5/147], loss=20.6485
	step [6/147], loss=19.7283
	step [7/147], loss=18.2634
	step [8/147], loss=17.7697
	step [9/147], loss=18.3341
	step [10/147], loss=16.5987
	step [11/147], loss=16.8961
	step [12/147], loss=18.2329
	step [13/147], loss=15.9603
	step [14/147], loss=20.5808
	step [15/147], loss=15.4612
	step [16/147], loss=20.8197
	step [17/147], loss=19.1023
	step [18/147], loss=16.7069
	step [19/147], loss=16.4810
	step [20/147], loss=17.9353
	step [21/147], loss=17.4528
	step [22/147], loss=16.8314
	step [23/147], loss=16.5775
	step [24/147], loss=17.6393
	step [25/147], loss=18.5046
	step [26/147], loss=19.5126
	step [27/147], loss=17.4377
	step [28/147], loss=19.7091
	step [29/147], loss=18.9842
	step [30/147], loss=18.8144
	step [31/147], loss=17.9226
	step [32/147], loss=16.7399
	step [33/147], loss=16.9898
	step [34/147], loss=17.1821
	step [35/147], loss=16.5087
	step [36/147], loss=18.3561
	step [37/147], loss=18.0124
	step [38/147], loss=19.8850
	step [39/147], loss=19.4813
	step [40/147], loss=16.4364
	step [41/147], loss=17.0155
	step [42/147], loss=18.0262
	step [43/147], loss=17.4656
	step [44/147], loss=20.3222
	step [45/147], loss=18.1069
	step [46/147], loss=18.1612
	step [47/147], loss=15.8909
	step [48/147], loss=17.9155
	step [49/147], loss=17.4525
	step [50/147], loss=20.6958
	step [51/147], loss=16.7587
	step [52/147], loss=16.4334
	step [53/147], loss=17.1015
	step [54/147], loss=16.9257
	step [55/147], loss=15.3998
	step [56/147], loss=17.0498
	step [57/147], loss=16.4764
	step [58/147], loss=18.2429
	step [59/147], loss=15.4887
	step [60/147], loss=19.5253
	step [61/147], loss=17.2060
	step [62/147], loss=18.8574
	step [63/147], loss=14.8663
	step [64/147], loss=15.3164
	step [65/147], loss=18.1819
	step [66/147], loss=18.2256
	step [67/147], loss=17.8538
	step [68/147], loss=16.9475
	step [69/147], loss=18.4256
	step [70/147], loss=18.3598
	step [71/147], loss=19.1831
	step [72/147], loss=18.6650
	step [73/147], loss=19.1387
	step [74/147], loss=17.4755
	step [75/147], loss=17.6682
	step [76/147], loss=17.0183
	step [77/147], loss=17.8397
	step [78/147], loss=18.8700
	step [79/147], loss=17.6372
	step [80/147], loss=22.3268
	step [81/147], loss=16.6296
	step [82/147], loss=18.5440
	step [83/147], loss=21.6763
	step [84/147], loss=15.4533
	step [85/147], loss=17.4002
	step [86/147], loss=19.6347
	step [87/147], loss=16.2136
	step [88/147], loss=18.4588
	step [89/147], loss=17.6317
	step [90/147], loss=17.1582
	step [91/147], loss=18.6531
	step [92/147], loss=17.4704
	step [93/147], loss=17.6307
	step [94/147], loss=21.6389
	step [95/147], loss=16.1873
	step [96/147], loss=18.0140
	step [97/147], loss=17.0003
	step [98/147], loss=17.0169
	step [99/147], loss=16.8970
	step [100/147], loss=17.5011
	step [101/147], loss=17.5012
	step [102/147], loss=19.9078
	step [103/147], loss=17.6277
	step [104/147], loss=18.2556
	step [105/147], loss=16.8482
	step [106/147], loss=17.6403
	step [107/147], loss=18.0755
	step [108/147], loss=16.9448
	step [109/147], loss=15.3944
	step [110/147], loss=15.7030
	step [111/147], loss=15.9939
	step [112/147], loss=15.4687
	step [113/147], loss=16.6571
	step [114/147], loss=16.0868
	step [115/147], loss=20.9014
	step [116/147], loss=15.4693
	step [117/147], loss=20.5672
	step [118/147], loss=17.7824
	step [119/147], loss=14.6193
	step [120/147], loss=18.7230
	step [121/147], loss=17.0038
	step [122/147], loss=16.9403
	step [123/147], loss=17.7695
	step [124/147], loss=18.2749
	step [125/147], loss=16.7634
	step [126/147], loss=16.4584
	step [127/147], loss=15.5687
	step [128/147], loss=18.3106
	step [129/147], loss=18.1971
	step [130/147], loss=17.1517
	step [131/147], loss=16.2977
	step [132/147], loss=16.5314
	step [133/147], loss=14.9634
	step [134/147], loss=15.8862
	step [135/147], loss=15.7905
	step [136/147], loss=17.8654
	step [137/147], loss=16.0276
	step [138/147], loss=16.2296
	step [139/147], loss=15.9232
	step [140/147], loss=17.3965
	step [141/147], loss=17.0821
	step [142/147], loss=18.2096
	step [143/147], loss=18.4913
	step [144/147], loss=16.6198
	step [145/147], loss=17.4100
	step [146/147], loss=19.2969
	step [147/147], loss=7.2687
	Evaluating
	loss=0.0462, precision=0.1678, recall=0.9942, f1=0.2872
Training epoch 15
	step [1/147], loss=18.3637
	step [2/147], loss=17.6262
	step [3/147], loss=18.5752
	step [4/147], loss=17.7497
	step [5/147], loss=16.1229
	step [6/147], loss=16.3765
	step [7/147], loss=18.1907
	step [8/147], loss=20.1621
	step [9/147], loss=16.0516
	step [10/147], loss=15.8362
	step [11/147], loss=17.1446
	step [12/147], loss=17.1270
	step [13/147], loss=17.1536
	step [14/147], loss=16.4444
	step [15/147], loss=16.2275
	step [16/147], loss=17.9535
	step [17/147], loss=19.2434
	step [18/147], loss=16.6803
	step [19/147], loss=16.9058
	step [20/147], loss=17.7469
	step [21/147], loss=17.5123
	step [22/147], loss=17.2790
	step [23/147], loss=16.3114
	step [24/147], loss=15.6741
	step [25/147], loss=18.8538
	step [26/147], loss=19.0956
	step [27/147], loss=17.0629
	step [28/147], loss=16.0321
	step [29/147], loss=16.5454
	step [30/147], loss=15.8953
	step [31/147], loss=16.5966
	step [32/147], loss=15.8262
	step [33/147], loss=16.2625
	step [34/147], loss=18.2952
	step [35/147], loss=14.8159
	step [36/147], loss=18.5980
	step [37/147], loss=15.2321
	step [38/147], loss=17.8046
	step [39/147], loss=17.6672
	step [40/147], loss=19.2047
	step [41/147], loss=17.8123
	step [42/147], loss=15.4525
	step [43/147], loss=17.2269
	step [44/147], loss=16.8308
	step [45/147], loss=14.5513
	step [46/147], loss=15.2530
	step [47/147], loss=17.8374
	step [48/147], loss=20.7438
	step [49/147], loss=15.0895
	step [50/147], loss=16.6129
	step [51/147], loss=16.9746
	step [52/147], loss=18.3045
	step [53/147], loss=19.7079
	step [54/147], loss=17.5669
	step [55/147], loss=18.5635
	step [56/147], loss=15.4472
	step [57/147], loss=17.9200
	step [58/147], loss=16.8354
	step [59/147], loss=17.7120
	step [60/147], loss=17.4198
	step [61/147], loss=17.0090
	step [62/147], loss=18.0326
	step [63/147], loss=16.6173
	step [64/147], loss=15.8370
	step [65/147], loss=16.4546
	step [66/147], loss=15.3914
	step [67/147], loss=15.1817
	step [68/147], loss=15.7931
	step [69/147], loss=14.9700
	step [70/147], loss=17.6673
	step [71/147], loss=15.9997
	step [72/147], loss=18.4416
	step [73/147], loss=14.8863
	step [74/147], loss=14.2678
	step [75/147], loss=18.9647
	step [76/147], loss=16.9735
	step [77/147], loss=17.8160
	step [78/147], loss=16.7697
	step [79/147], loss=17.1381
	step [80/147], loss=16.0719
	step [81/147], loss=18.0318
	step [82/147], loss=14.7376
	step [83/147], loss=17.3409
	step [84/147], loss=17.4991
	step [85/147], loss=15.8957
	step [86/147], loss=17.0549
	step [87/147], loss=15.8157
	step [88/147], loss=16.1341
	step [89/147], loss=16.5691
	step [90/147], loss=19.0117
	step [91/147], loss=18.5095
	step [92/147], loss=17.3270
	step [93/147], loss=17.4355
	step [94/147], loss=16.8442
	step [95/147], loss=20.1861
	step [96/147], loss=16.9521
	step [97/147], loss=15.3591
	step [98/147], loss=16.9630
	step [99/147], loss=17.5126
	step [100/147], loss=17.7206
	step [101/147], loss=17.4106
	step [102/147], loss=15.4315
	step [103/147], loss=15.9326
	step [104/147], loss=16.4673
	step [105/147], loss=16.5146
	step [106/147], loss=18.3296
	step [107/147], loss=15.8267
	step [108/147], loss=16.9635
	step [109/147], loss=15.6486
	step [110/147], loss=16.3104
	step [111/147], loss=15.4487
	step [112/147], loss=18.1042
	step [113/147], loss=17.3309
	step [114/147], loss=15.2448
	step [115/147], loss=16.0829
	step [116/147], loss=16.4753
	step [117/147], loss=17.5456
	step [118/147], loss=18.0340
	step [119/147], loss=16.7564
	step [120/147], loss=16.8179
	step [121/147], loss=16.3854
	step [122/147], loss=17.9482
	step [123/147], loss=18.4918
	step [124/147], loss=15.9815
	step [125/147], loss=14.3263
	step [126/147], loss=20.0116
	step [127/147], loss=15.3755
	step [128/147], loss=20.1937
	step [129/147], loss=15.9463
	step [130/147], loss=14.8782
	step [131/147], loss=15.6851
	step [132/147], loss=18.9829
	step [133/147], loss=15.6601
	step [134/147], loss=16.6809
	step [135/147], loss=18.1657
	step [136/147], loss=15.3273
	step [137/147], loss=15.4375
	step [138/147], loss=17.8073
	step [139/147], loss=13.7768
	step [140/147], loss=15.9901
	step [141/147], loss=17.1343
	step [142/147], loss=17.9047
	step [143/147], loss=13.9623
	step [144/147], loss=15.3392
	step [145/147], loss=15.9206
	step [146/147], loss=17.0552
	step [147/147], loss=7.2249
	Evaluating
	loss=0.0460, precision=0.1592, recall=0.9944, f1=0.2744
Training epoch 16
	step [1/147], loss=16.9217
	step [2/147], loss=15.7409
	step [3/147], loss=15.3946
	step [4/147], loss=16.8754
	step [5/147], loss=17.5380
	step [6/147], loss=15.0528
	step [7/147], loss=18.0618
	step [8/147], loss=16.2543
	step [9/147], loss=18.9171
	step [10/147], loss=16.1634
	step [11/147], loss=15.4387
	step [12/147], loss=20.8323
	step [13/147], loss=16.8266
	step [14/147], loss=15.7498
	step [15/147], loss=16.4689
	step [16/147], loss=16.5603
	step [17/147], loss=17.3728
	step [18/147], loss=14.1704
	step [19/147], loss=19.6568
	step [20/147], loss=17.8141
	step [21/147], loss=16.2053
	step [22/147], loss=13.6645
	step [23/147], loss=16.2630
	step [24/147], loss=14.3268
	step [25/147], loss=16.3753
	step [26/147], loss=17.0349
	step [27/147], loss=16.9567
	step [28/147], loss=18.5363
	step [29/147], loss=16.9995
	step [30/147], loss=17.2028
	step [31/147], loss=15.7122
	step [32/147], loss=17.9399
	step [33/147], loss=15.0322
	step [34/147], loss=15.7600
	step [35/147], loss=17.3017
	step [36/147], loss=16.6961
	step [37/147], loss=15.9308
	step [38/147], loss=14.6504
	step [39/147], loss=15.1283
	step [40/147], loss=15.2912
	step [41/147], loss=15.2042
	step [42/147], loss=15.1411
	step [43/147], loss=16.3833
	step [44/147], loss=14.7768
	step [45/147], loss=17.9185
	step [46/147], loss=12.7250
	step [47/147], loss=16.6677
	step [48/147], loss=19.7314
	step [49/147], loss=15.2201
	step [50/147], loss=16.1601
	step [51/147], loss=16.9447
	step [52/147], loss=15.3933
	step [53/147], loss=16.9116
	step [54/147], loss=16.1325
	step [55/147], loss=15.3717
	step [56/147], loss=15.1592
	step [57/147], loss=15.2817
	step [58/147], loss=17.9991
	step [59/147], loss=16.6460
	step [60/147], loss=16.2913
	step [61/147], loss=14.8558
	step [62/147], loss=18.0793
	step [63/147], loss=15.1844
	step [64/147], loss=15.1581
	step [65/147], loss=17.2391
	step [66/147], loss=16.6468
	step [67/147], loss=16.3027
	step [68/147], loss=16.3437
	step [69/147], loss=15.6141
	step [70/147], loss=16.3112
	step [71/147], loss=15.3205
	step [72/147], loss=15.6446
	step [73/147], loss=16.5263
	step [74/147], loss=16.8086
	step [75/147], loss=14.7145
	step [76/147], loss=15.8042
	step [77/147], loss=14.8074
	step [78/147], loss=18.6973
	step [79/147], loss=14.1833
	step [80/147], loss=15.9330
	step [81/147], loss=15.8850
	step [82/147], loss=16.0757
	step [83/147], loss=17.1599
	step [84/147], loss=16.6253
	step [85/147], loss=15.2765
	step [86/147], loss=14.6549
	step [87/147], loss=14.0340
	step [88/147], loss=14.3505
	step [89/147], loss=19.3951
	step [90/147], loss=16.9283
	step [91/147], loss=17.8715
	step [92/147], loss=16.0664
	step [93/147], loss=18.5055
	step [94/147], loss=15.4452
	step [95/147], loss=17.1672
	step [96/147], loss=16.6630
	step [97/147], loss=14.6564
	step [98/147], loss=15.0290
	step [99/147], loss=15.0436
	step [100/147], loss=16.4832
	step [101/147], loss=13.8999
	step [102/147], loss=18.4077
	step [103/147], loss=16.7312
	step [104/147], loss=16.7547
	step [105/147], loss=16.7712
	step [106/147], loss=16.6607
	step [107/147], loss=16.9626
	step [108/147], loss=14.9780
	step [109/147], loss=16.9803
	step [110/147], loss=16.7471
	step [111/147], loss=15.2551
	step [112/147], loss=17.8583
	step [113/147], loss=13.9556
	step [114/147], loss=16.7493
	step [115/147], loss=17.4324
	step [116/147], loss=18.3387
	step [117/147], loss=16.3092
	step [118/147], loss=16.2052
	step [119/147], loss=17.5416
	step [120/147], loss=16.5345
	step [121/147], loss=15.7174
	step [122/147], loss=15.6005
	step [123/147], loss=15.7375
	step [124/147], loss=15.5517
	step [125/147], loss=13.0751
	step [126/147], loss=16.0112
	step [127/147], loss=16.6790
	step [128/147], loss=15.4608
	step [129/147], loss=17.6233
	step [130/147], loss=15.7527
	step [131/147], loss=15.1518
	step [132/147], loss=15.7863
	step [133/147], loss=18.2378
	step [134/147], loss=15.6071
	step [135/147], loss=15.3908
	step [136/147], loss=16.9135
	step [137/147], loss=15.7984
	step [138/147], loss=16.9592
	step [139/147], loss=15.0729
	step [140/147], loss=15.5901
	step [141/147], loss=15.6613
	step [142/147], loss=16.2813
	step [143/147], loss=17.8322
	step [144/147], loss=18.5804
	step [145/147], loss=16.0287
	step [146/147], loss=17.0054
	step [147/147], loss=7.1338
	Evaluating
	loss=0.0504, precision=0.1338, recall=0.9956, f1=0.2360
Training epoch 17
	step [1/147], loss=15.8365
	step [2/147], loss=19.6506
	step [3/147], loss=15.6404
	step [4/147], loss=16.3139
	step [5/147], loss=16.2684
	step [6/147], loss=16.2217
	step [7/147], loss=15.5102
	step [8/147], loss=16.8218
	step [9/147], loss=15.4258
	step [10/147], loss=16.3055
	step [11/147], loss=17.7773
	step [12/147], loss=14.3906
	step [13/147], loss=13.5439
	step [14/147], loss=14.5348
	step [15/147], loss=16.3493
	step [16/147], loss=16.5116
	step [17/147], loss=17.1967
	step [18/147], loss=14.4121
	step [19/147], loss=12.2240
	step [20/147], loss=17.3963
	step [21/147], loss=15.8843
	step [22/147], loss=14.6726
	step [23/147], loss=16.8745
	step [24/147], loss=14.4689
	step [25/147], loss=15.6882
	step [26/147], loss=15.3909
	step [27/147], loss=18.1800
	step [28/147], loss=15.8143
	step [29/147], loss=15.8044
	step [30/147], loss=13.5657
	step [31/147], loss=14.2204
	step [32/147], loss=16.6601
	step [33/147], loss=15.5238
	step [34/147], loss=17.5282
	step [35/147], loss=17.1021
	step [36/147], loss=16.9920
	step [37/147], loss=16.9101
	step [38/147], loss=16.2188
	step [39/147], loss=16.3033
	step [40/147], loss=17.2244
	step [41/147], loss=18.0826
	step [42/147], loss=16.2343
	step [43/147], loss=14.7107
	step [44/147], loss=15.6871
	step [45/147], loss=16.6545
	step [46/147], loss=14.9382
	step [47/147], loss=14.1947
	step [48/147], loss=13.9826
	step [49/147], loss=15.8346
	step [50/147], loss=17.6627
	step [51/147], loss=14.4673
	step [52/147], loss=16.7418
	step [53/147], loss=14.9423
	step [54/147], loss=15.5439
	step [55/147], loss=13.9724
	step [56/147], loss=14.6590
	step [57/147], loss=15.0618
	step [58/147], loss=15.9035
	step [59/147], loss=14.5631
	step [60/147], loss=19.4310
	step [61/147], loss=13.0995
	step [62/147], loss=15.3065
	step [63/147], loss=14.5701
	step [64/147], loss=14.4480
	step [65/147], loss=14.3456
	step [66/147], loss=16.1455
	step [67/147], loss=15.9576
	step [68/147], loss=15.6685
	step [69/147], loss=14.5954
	step [70/147], loss=15.3902
	step [71/147], loss=16.2634
	step [72/147], loss=15.0021
	step [73/147], loss=14.8412
	step [74/147], loss=14.7294
	step [75/147], loss=15.0353
	step [76/147], loss=13.3794
	step [77/147], loss=14.6088
	step [78/147], loss=16.0910
	step [79/147], loss=17.6832
	step [80/147], loss=17.7309
	step [81/147], loss=13.9303
	step [82/147], loss=16.7563
	step [83/147], loss=16.8233
	step [84/147], loss=16.9924
	step [85/147], loss=15.3631
	step [86/147], loss=15.2294
	step [87/147], loss=14.9175
	step [88/147], loss=17.1964
	step [89/147], loss=15.8370
	step [90/147], loss=13.7540
	step [91/147], loss=19.7474
	step [92/147], loss=15.9547
	step [93/147], loss=17.5681
	step [94/147], loss=13.0279
	step [95/147], loss=15.7910
	step [96/147], loss=16.2888
	step [97/147], loss=18.4461
	step [98/147], loss=14.0031
	step [99/147], loss=15.7486
	step [100/147], loss=18.6994
	step [101/147], loss=16.5054
	step [102/147], loss=15.7182
	step [103/147], loss=16.5966
	step [104/147], loss=17.5989
	step [105/147], loss=15.5649
	step [106/147], loss=15.2324
	step [107/147], loss=16.3213
	step [108/147], loss=16.0043
	step [109/147], loss=16.0213
	step [110/147], loss=17.9075
	step [111/147], loss=13.8673
	step [112/147], loss=13.7520
	step [113/147], loss=16.9772
	step [114/147], loss=14.1392
	step [115/147], loss=16.5103
	step [116/147], loss=14.9525
	step [117/147], loss=14.9655
	step [118/147], loss=14.8463
	step [119/147], loss=15.7392
	step [120/147], loss=14.2539
	step [121/147], loss=15.2692
	step [122/147], loss=14.8052
	step [123/147], loss=19.0059
	step [124/147], loss=16.6532
	step [125/147], loss=14.4218
	step [126/147], loss=16.0754
	step [127/147], loss=14.8988
	step [128/147], loss=14.6776
	step [129/147], loss=15.0379
	step [130/147], loss=14.2578
	step [131/147], loss=13.3706
	step [132/147], loss=16.5852
	step [133/147], loss=16.0605
	step [134/147], loss=15.0899
	step [135/147], loss=15.7666
	step [136/147], loss=15.6000
	step [137/147], loss=14.9990
	step [138/147], loss=16.3702
	step [139/147], loss=14.5174
	step [140/147], loss=15.8487
	step [141/147], loss=16.4173
	step [142/147], loss=14.9641
	step [143/147], loss=18.3522
	step [144/147], loss=14.6606
	step [145/147], loss=15.4174
	step [146/147], loss=14.6333
	step [147/147], loss=6.9818
	Evaluating
	loss=0.0408, precision=0.1751, recall=0.9941, f1=0.2978
Training epoch 18
	step [1/147], loss=13.8947
	step [2/147], loss=13.2449
	step [3/147], loss=14.6305
	step [4/147], loss=18.6345
	step [5/147], loss=15.3762
	step [6/147], loss=12.8121
	step [7/147], loss=14.4279
	step [8/147], loss=14.0914
	step [9/147], loss=15.1561
	step [10/147], loss=15.0435
	step [11/147], loss=14.8394
	step [12/147], loss=13.2743
	step [13/147], loss=17.8746
	step [14/147], loss=14.1975
	step [15/147], loss=14.5803
	step [16/147], loss=14.4634
	step [17/147], loss=17.1220
	step [18/147], loss=15.1456
	step [19/147], loss=17.7159
	step [20/147], loss=14.4068
	step [21/147], loss=15.2254
	step [22/147], loss=16.6511
	step [23/147], loss=15.8305
	step [24/147], loss=16.0178
	step [25/147], loss=14.6100
	step [26/147], loss=15.3812
	step [27/147], loss=16.8475
	step [28/147], loss=15.0730
	step [29/147], loss=18.3215
	step [30/147], loss=14.7597
	step [31/147], loss=15.7366
	step [32/147], loss=15.0471
	step [33/147], loss=16.1244
	step [34/147], loss=16.5894
	step [35/147], loss=15.0597
	step [36/147], loss=17.2703
	step [37/147], loss=15.3803
	step [38/147], loss=13.3537
	step [39/147], loss=13.3754
	step [40/147], loss=15.3443
	step [41/147], loss=13.4296
	step [42/147], loss=17.8064
	step [43/147], loss=14.1825
	step [44/147], loss=18.8353
	step [45/147], loss=15.5178
	step [46/147], loss=15.0455
	step [47/147], loss=14.7239
	step [48/147], loss=14.4606
	step [49/147], loss=14.7392
	step [50/147], loss=15.4172
	step [51/147], loss=15.9148
	step [52/147], loss=15.7547
	step [53/147], loss=14.0728
	step [54/147], loss=14.5301
	step [55/147], loss=16.6207
	step [56/147], loss=15.8232
	step [57/147], loss=15.0896
	step [58/147], loss=15.1363
	step [59/147], loss=15.0245
	step [60/147], loss=15.8704
	step [61/147], loss=15.3073
	step [62/147], loss=13.6739
	step [63/147], loss=13.9271
	step [64/147], loss=15.0241
	step [65/147], loss=15.1738
	step [66/147], loss=12.5139
	step [67/147], loss=15.1064
	step [68/147], loss=15.2067
	step [69/147], loss=14.5223
	step [70/147], loss=15.7047
	step [71/147], loss=15.5334
	step [72/147], loss=15.6990
	step [73/147], loss=14.6705
	step [74/147], loss=14.8210
	step [75/147], loss=15.1165
	step [76/147], loss=16.7307
	step [77/147], loss=16.7614
	step [78/147], loss=17.5724
	step [79/147], loss=13.6499
	step [80/147], loss=16.6365
	step [81/147], loss=17.6107
	step [82/147], loss=13.8083
	step [83/147], loss=16.9973
	step [84/147], loss=15.0898
	step [85/147], loss=14.6461
	step [86/147], loss=15.7244
	step [87/147], loss=15.1036
	step [88/147], loss=16.1762
	step [89/147], loss=13.8384
	step [90/147], loss=16.7178
	step [91/147], loss=17.3783
	step [92/147], loss=14.4730
	step [93/147], loss=14.4675
	step [94/147], loss=14.4519
	step [95/147], loss=16.2115
	step [96/147], loss=16.3460
	step [97/147], loss=14.7501
	step [98/147], loss=14.0755
	step [99/147], loss=14.7480
	step [100/147], loss=15.3003
	step [101/147], loss=16.4316
	step [102/147], loss=15.7962
	step [103/147], loss=16.2261
	step [104/147], loss=14.5413
	step [105/147], loss=14.8770
	step [106/147], loss=15.6180
	step [107/147], loss=15.1121
	step [108/147], loss=16.1680
	step [109/147], loss=14.6220
	step [110/147], loss=15.7312
	step [111/147], loss=14.6059
	step [112/147], loss=14.2497
	step [113/147], loss=14.4815
	step [114/147], loss=15.8481
	step [115/147], loss=14.3250
	step [116/147], loss=14.5287
	step [117/147], loss=16.4157
	step [118/147], loss=15.9638
	step [119/147], loss=12.5855
	step [120/147], loss=16.2674
	step [121/147], loss=14.6127
	step [122/147], loss=14.8534
	step [123/147], loss=13.7149
	step [124/147], loss=15.5742
	step [125/147], loss=17.0410
	step [126/147], loss=14.4675
	step [127/147], loss=15.4608
	step [128/147], loss=12.4808
	step [129/147], loss=14.4127
	step [130/147], loss=13.7308
	step [131/147], loss=14.5965
	step [132/147], loss=13.6384
	step [133/147], loss=12.9215
	step [134/147], loss=15.4221
	step [135/147], loss=17.0602
	step [136/147], loss=11.7050
	step [137/147], loss=13.7838
	step [138/147], loss=14.5791
	step [139/147], loss=13.8519
	step [140/147], loss=16.1067
	step [141/147], loss=15.6267
	step [142/147], loss=13.7124
	step [143/147], loss=14.1110
	step [144/147], loss=14.5361
	step [145/147], loss=14.8773
	step [146/147], loss=12.8198
	step [147/147], loss=8.1775
	Evaluating
	loss=0.0426, precision=0.1540, recall=0.9945, f1=0.2667
Training epoch 19
	step [1/147], loss=12.3408
	step [2/147], loss=15.6869
	step [3/147], loss=14.2669
	step [4/147], loss=13.4385
	step [5/147], loss=13.7983
	step [6/147], loss=14.5008
	step [7/147], loss=13.8120
	step [8/147], loss=14.0875
	step [9/147], loss=15.2572
	step [10/147], loss=15.6251
	step [11/147], loss=14.1568
	step [12/147], loss=14.6226
	step [13/147], loss=16.0135
	step [14/147], loss=16.7966
	step [15/147], loss=13.8419
	step [16/147], loss=15.2749
	step [17/147], loss=13.7949
	step [18/147], loss=15.7510
	step [19/147], loss=14.1605
	step [20/147], loss=15.6733
	step [21/147], loss=14.0752
	step [22/147], loss=13.8059
	step [23/147], loss=14.2798
	step [24/147], loss=14.3488
	step [25/147], loss=12.1450
	step [26/147], loss=14.7247
	step [27/147], loss=13.2762
	step [28/147], loss=14.2908
	step [29/147], loss=13.9125
	step [30/147], loss=14.9170
	step [31/147], loss=14.7777
	step [32/147], loss=14.2018
	step [33/147], loss=14.9831
	step [34/147], loss=13.7369
	step [35/147], loss=16.4618
	step [36/147], loss=13.6945
	step [37/147], loss=14.2278
	step [38/147], loss=18.0829
	step [39/147], loss=15.8461
	step [40/147], loss=14.5428
	step [41/147], loss=15.6976
	step [42/147], loss=15.2686
	step [43/147], loss=14.9854
	step [44/147], loss=13.0559
	step [45/147], loss=13.9718
	step [46/147], loss=16.9114
	step [47/147], loss=14.3793
	step [48/147], loss=15.7848
	step [49/147], loss=14.3219
	step [50/147], loss=13.5063
	step [51/147], loss=14.6757
	step [52/147], loss=15.2112
	step [53/147], loss=13.0814
	step [54/147], loss=16.5456
	step [55/147], loss=14.4646
	step [56/147], loss=14.4892
	step [57/147], loss=14.2528
	step [58/147], loss=15.4000
	step [59/147], loss=15.4562
	step [60/147], loss=14.7489
	step [61/147], loss=15.6906
	step [62/147], loss=13.6862
	step [63/147], loss=15.1884
	step [64/147], loss=16.5915
	step [65/147], loss=14.3372
	step [66/147], loss=11.5435
	step [67/147], loss=15.3144
	step [68/147], loss=14.0493
	step [69/147], loss=14.6351
	step [70/147], loss=14.0168
	step [71/147], loss=11.9373
	step [72/147], loss=17.5832
	step [73/147], loss=14.1089
	step [74/147], loss=13.1814
	step [75/147], loss=16.0300
	step [76/147], loss=16.2032
	step [77/147], loss=14.2644
	step [78/147], loss=14.2180
	step [79/147], loss=16.0326
	step [80/147], loss=13.1038
	step [81/147], loss=15.1338
	step [82/147], loss=14.3642
	step [83/147], loss=14.0618
	step [84/147], loss=14.7104
	step [85/147], loss=13.6209
	step [86/147], loss=13.9233
	step [87/147], loss=14.7885
	step [88/147], loss=15.5106
	step [89/147], loss=13.5975
	step [90/147], loss=15.5556
	step [91/147], loss=13.9027
	step [92/147], loss=16.1904
	step [93/147], loss=14.0922
	step [94/147], loss=15.7975
	step [95/147], loss=13.9697
	step [96/147], loss=14.6561
	step [97/147], loss=14.7129
	step [98/147], loss=14.8768
	step [99/147], loss=14.9404
	step [100/147], loss=16.8872
	step [101/147], loss=14.6529
	step [102/147], loss=16.1287
	step [103/147], loss=15.6702
	step [104/147], loss=17.4474
	step [105/147], loss=14.7408
	step [106/147], loss=14.7721
	step [107/147], loss=13.5269
	step [108/147], loss=16.5122
	step [109/147], loss=14.0672
	step [110/147], loss=14.4340
	step [111/147], loss=14.4205
	step [112/147], loss=13.4126
	step [113/147], loss=14.7847
	step [114/147], loss=13.7706
	step [115/147], loss=14.0072
	step [116/147], loss=17.1057
	step [117/147], loss=12.7979
	step [118/147], loss=12.1905
	step [119/147], loss=13.6264
	step [120/147], loss=13.8319
	step [121/147], loss=15.1532
	step [122/147], loss=14.0465
	step [123/147], loss=14.3437
	step [124/147], loss=16.9610
	step [125/147], loss=15.3986
	step [126/147], loss=15.3701
	step [127/147], loss=14.5327
	step [128/147], loss=13.5339
	step [129/147], loss=13.8654
	step [130/147], loss=14.9367
	step [131/147], loss=12.6144
	step [132/147], loss=13.0730
	step [133/147], loss=15.9458
	step [134/147], loss=14.1570
	step [135/147], loss=14.9478
	step [136/147], loss=14.9789
	step [137/147], loss=14.3994
	step [138/147], loss=13.1515
	step [139/147], loss=11.9480
	step [140/147], loss=17.1315
	step [141/147], loss=15.6214
	step [142/147], loss=15.4890
	step [143/147], loss=11.5292
	step [144/147], loss=13.3676
	step [145/147], loss=13.0249
	step [146/147], loss=16.2620
	step [147/147], loss=7.2900
	Evaluating
	loss=0.0404, precision=0.1638, recall=0.9940, f1=0.2813
Training epoch 20
	step [1/147], loss=13.4964
	step [2/147], loss=12.3853
	step [3/147], loss=13.8351
	step [4/147], loss=12.1558
	step [5/147], loss=15.0536
	step [6/147], loss=13.8972
	step [7/147], loss=14.7228
	step [8/147], loss=12.7649
	step [9/147], loss=12.2980
	step [10/147], loss=14.0035
	step [11/147], loss=12.9514
	step [12/147], loss=14.9346
	step [13/147], loss=14.5281
	step [14/147], loss=15.4146
	step [15/147], loss=15.1333
	step [16/147], loss=14.1559
	step [17/147], loss=16.6161
	step [18/147], loss=14.4748
	step [19/147], loss=13.2592
	step [20/147], loss=12.1914
	step [21/147], loss=14.2170
	step [22/147], loss=14.0393
	step [23/147], loss=14.7673
	step [24/147], loss=12.4340
	step [25/147], loss=11.5089
	step [26/147], loss=14.5633
	step [27/147], loss=15.2457
	step [28/147], loss=15.8723
	step [29/147], loss=14.5120
	step [30/147], loss=14.0645
	step [31/147], loss=14.8379
	step [32/147], loss=14.4915
	step [33/147], loss=13.7351
	step [34/147], loss=13.3451
	step [35/147], loss=15.3925
	step [36/147], loss=14.1704
	step [37/147], loss=13.9848
	step [38/147], loss=16.2488
	step [39/147], loss=14.1625
	step [40/147], loss=13.6882
	step [41/147], loss=14.7759
	step [42/147], loss=13.4802
	step [43/147], loss=15.2184
	step [44/147], loss=13.2683
	step [45/147], loss=16.0442
	step [46/147], loss=15.3725
	step [47/147], loss=13.4003
	step [48/147], loss=14.2364
	step [49/147], loss=12.7936
	step [50/147], loss=15.2287
	step [51/147], loss=12.5984
	step [52/147], loss=13.8585
	step [53/147], loss=13.8670
	step [54/147], loss=14.1175
	step [55/147], loss=13.0351
	step [56/147], loss=11.6178
	step [57/147], loss=14.2111
	step [58/147], loss=16.1422
	step [59/147], loss=12.8201
	step [60/147], loss=15.4977
	step [61/147], loss=14.7243
	step [62/147], loss=13.6885
	step [63/147], loss=12.1182
	step [64/147], loss=14.9087
	step [65/147], loss=14.5451
	step [66/147], loss=13.7517
	step [67/147], loss=14.9865
	step [68/147], loss=13.3713
	step [69/147], loss=13.3230
	step [70/147], loss=13.7128
	step [71/147], loss=14.8018
	step [72/147], loss=13.5495
	step [73/147], loss=13.7702
	step [74/147], loss=17.6131
	step [75/147], loss=16.3157
	step [76/147], loss=16.0257
	step [77/147], loss=15.4053
	step [78/147], loss=16.5412
	step [79/147], loss=15.0738
	step [80/147], loss=13.6615
	step [81/147], loss=15.6768
	step [82/147], loss=14.6986
	step [83/147], loss=14.1726
	step [84/147], loss=14.7077
	step [85/147], loss=16.1662
	step [86/147], loss=15.6540
	step [87/147], loss=15.3715
	step [88/147], loss=14.1716
	step [89/147], loss=13.8080
	step [90/147], loss=14.7982
	step [91/147], loss=13.4556
	step [92/147], loss=13.1935
	step [93/147], loss=13.2635
	step [94/147], loss=13.3423
	step [95/147], loss=13.4474
	step [96/147], loss=15.0067
	step [97/147], loss=12.2992
	step [98/147], loss=13.0209
	step [99/147], loss=13.9064
	step [100/147], loss=14.6912
	step [101/147], loss=14.3405
	step [102/147], loss=13.0933
	step [103/147], loss=13.8912
	step [104/147], loss=14.6499
	step [105/147], loss=15.9997
	step [106/147], loss=14.1942
	step [107/147], loss=12.4927
	step [108/147], loss=14.8939
	step [109/147], loss=11.3371
	step [110/147], loss=10.9826
	step [111/147], loss=14.8872
	step [112/147], loss=14.9849
	step [113/147], loss=13.0203
	step [114/147], loss=14.2636
	step [115/147], loss=12.5118
	step [116/147], loss=13.8555
	step [117/147], loss=13.6104
	step [118/147], loss=13.7309
	step [119/147], loss=12.1435
	step [120/147], loss=14.2442
	step [121/147], loss=13.4352
	step [122/147], loss=12.6395
	step [123/147], loss=15.0419
	step [124/147], loss=12.7762
	step [125/147], loss=13.0448
	step [126/147], loss=14.1175
	step [127/147], loss=12.9563
	step [128/147], loss=17.5562
	step [129/147], loss=15.2570
	step [130/147], loss=14.9926
	step [131/147], loss=13.8768
	step [132/147], loss=13.4142
	step [133/147], loss=13.9473
	step [134/147], loss=12.5122
	step [135/147], loss=15.4555
	step [136/147], loss=13.8555
	step [137/147], loss=16.7703
	step [138/147], loss=17.1006
	step [139/147], loss=13.3298
	step [140/147], loss=15.5857
	step [141/147], loss=14.1647
	step [142/147], loss=13.1989
	step [143/147], loss=12.2705
	step [144/147], loss=11.7309
	step [145/147], loss=12.8005
	step [146/147], loss=12.1469
	step [147/147], loss=5.8133
	Evaluating
	loss=0.0326, precision=0.2030, recall=0.9930, f1=0.3371
Training epoch 21
	step [1/147], loss=14.2274
	step [2/147], loss=12.9256
	step [3/147], loss=13.9350
	step [4/147], loss=12.8101
	step [5/147], loss=11.4841
	step [6/147], loss=15.3361
	step [7/147], loss=12.3907
	step [8/147], loss=14.9397
	step [9/147], loss=13.5460
	step [10/147], loss=14.7974
	step [11/147], loss=12.5832
	step [12/147], loss=12.5081
	step [13/147], loss=14.2741
	step [14/147], loss=14.0366
	step [15/147], loss=13.2584
	step [16/147], loss=12.8875
	step [17/147], loss=14.9075
	step [18/147], loss=12.5799
	step [19/147], loss=12.7353
	step [20/147], loss=12.6291
	step [21/147], loss=12.9115
	step [22/147], loss=12.8655
	step [23/147], loss=13.6201
	step [24/147], loss=14.3164
	step [25/147], loss=13.7655
	step [26/147], loss=12.5041
	step [27/147], loss=12.8268
	step [28/147], loss=12.1249
	step [29/147], loss=12.9467
	step [30/147], loss=13.1080
	step [31/147], loss=12.1395
	step [32/147], loss=13.3794
	step [33/147], loss=13.8179
	step [34/147], loss=11.8907
	step [35/147], loss=12.7084
	step [36/147], loss=13.7073
	step [37/147], loss=11.9733
	step [38/147], loss=13.6142
	step [39/147], loss=12.8536
	step [40/147], loss=14.5384
	step [41/147], loss=14.6437
	step [42/147], loss=12.9131
	step [43/147], loss=13.7979
	step [44/147], loss=12.2550
	step [45/147], loss=15.9518
	step [46/147], loss=14.3408
	step [47/147], loss=14.6358
	step [48/147], loss=13.2387
	step [49/147], loss=12.1522
	step [50/147], loss=12.9780
	step [51/147], loss=13.2093
	step [52/147], loss=15.5685
	step [53/147], loss=12.0119
	step [54/147], loss=11.5580
	step [55/147], loss=13.6607
	step [56/147], loss=14.6693
	step [57/147], loss=12.3341
	step [58/147], loss=13.7206
	step [59/147], loss=14.7145
	step [60/147], loss=13.0022
	step [61/147], loss=12.4738
	step [62/147], loss=12.1222
	step [63/147], loss=14.3046
	step [64/147], loss=15.1997
	step [65/147], loss=15.1935
	step [66/147], loss=15.8608
	step [67/147], loss=13.5717
	step [68/147], loss=14.6005
	step [69/147], loss=13.5729
	step [70/147], loss=13.7437
	step [71/147], loss=15.5375
	step [72/147], loss=13.7963
	step [73/147], loss=12.0249
	step [74/147], loss=14.6278
	step [75/147], loss=15.3657
	step [76/147], loss=12.1902
	step [77/147], loss=15.2371
	step [78/147], loss=14.4649
	step [79/147], loss=13.9181
	step [80/147], loss=14.4254
	step [81/147], loss=12.0053
	step [82/147], loss=15.0436
	step [83/147], loss=14.9729
	step [84/147], loss=15.5122
	step [85/147], loss=13.8679
	step [86/147], loss=13.3532
	step [87/147], loss=14.1122
	step [88/147], loss=13.9169
	step [89/147], loss=12.2008
	step [90/147], loss=14.0119
	step [91/147], loss=13.2292
	step [92/147], loss=13.8677
	step [93/147], loss=12.5798
	step [94/147], loss=14.9404
	step [95/147], loss=14.4564
	step [96/147], loss=12.0942
	step [97/147], loss=12.9325
	step [98/147], loss=14.8762
	step [99/147], loss=14.8201
	step [100/147], loss=12.0791
	step [101/147], loss=13.6820
	step [102/147], loss=14.2057
	step [103/147], loss=12.8283
	step [104/147], loss=13.5554
	step [105/147], loss=13.5743
	step [106/147], loss=13.8721
	step [107/147], loss=13.3309
	step [108/147], loss=12.5039
	step [109/147], loss=13.3460
	step [110/147], loss=13.3666
	step [111/147], loss=15.3176
	step [112/147], loss=14.0686
	step [113/147], loss=13.5044
	step [114/147], loss=14.1754
	step [115/147], loss=13.4299
	step [116/147], loss=13.7780
	step [117/147], loss=13.6226
	step [118/147], loss=12.3760
	step [119/147], loss=12.9300
	step [120/147], loss=12.7186
	step [121/147], loss=12.5063
	step [122/147], loss=14.6559
	step [123/147], loss=12.9327
	step [124/147], loss=14.2685
	step [125/147], loss=12.2571
	step [126/147], loss=12.9976
	step [127/147], loss=12.5450
	step [128/147], loss=13.5694
	step [129/147], loss=12.6742
	step [130/147], loss=12.2501
	step [131/147], loss=13.6954
	step [132/147], loss=14.2951
	step [133/147], loss=11.8913
	step [134/147], loss=12.8461
	step [135/147], loss=15.7386
	step [136/147], loss=11.7213
	step [137/147], loss=12.8269
	step [138/147], loss=14.8310
	step [139/147], loss=11.7809
	step [140/147], loss=11.9334
	step [141/147], loss=12.1818
	step [142/147], loss=13.6234
	step [143/147], loss=14.7769
	step [144/147], loss=12.9516
	step [145/147], loss=13.0453
	step [146/147], loss=14.1717
	step [147/147], loss=7.2611
	Evaluating
	loss=0.0418, precision=0.1493, recall=0.9943, f1=0.2596
Training epoch 22
	step [1/147], loss=15.1547
	step [2/147], loss=12.3396
	step [3/147], loss=14.9656
	step [4/147], loss=13.0938
	step [5/147], loss=14.8757
	step [6/147], loss=12.6762
	step [7/147], loss=13.6231
	step [8/147], loss=15.8466
	step [9/147], loss=12.3411
	step [10/147], loss=13.3058
	step [11/147], loss=12.8265
	step [12/147], loss=14.2832
	step [13/147], loss=13.0287
	step [14/147], loss=12.4050
	step [15/147], loss=12.8137
	step [16/147], loss=12.9431
	step [17/147], loss=13.8236
	step [18/147], loss=12.9134
	step [19/147], loss=10.9158
	step [20/147], loss=14.1235
	step [21/147], loss=11.5624
	step [22/147], loss=14.3698
	step [23/147], loss=13.3748
	step [24/147], loss=12.1441
	step [25/147], loss=12.6802
	step [26/147], loss=12.9104
	step [27/147], loss=10.7031
	step [28/147], loss=12.0600
	step [29/147], loss=12.5195
	step [30/147], loss=14.9415
	step [31/147], loss=13.2924
	step [32/147], loss=12.6000
	step [33/147], loss=13.3195
	step [34/147], loss=16.5877
	step [35/147], loss=12.2846
	step [36/147], loss=12.7239
	step [37/147], loss=13.0919
	step [38/147], loss=12.2117
	step [39/147], loss=11.8163
	step [40/147], loss=12.5753
	step [41/147], loss=11.9668
	step [42/147], loss=14.3060
	step [43/147], loss=15.2190
	step [44/147], loss=13.5316
	step [45/147], loss=13.4494
	step [46/147], loss=14.4798
	step [47/147], loss=12.7484
	step [48/147], loss=13.1014
	step [49/147], loss=12.7144
	step [50/147], loss=12.1218
	step [51/147], loss=12.4055
	step [52/147], loss=12.1933
	step [53/147], loss=13.6398
	step [54/147], loss=12.1767
	step [55/147], loss=15.4214
	step [56/147], loss=13.4033
	step [57/147], loss=12.8428
	step [58/147], loss=12.1723
	step [59/147], loss=14.0256
	step [60/147], loss=14.4165
	step [61/147], loss=14.2272
	step [62/147], loss=12.0720
	step [63/147], loss=12.4673
	step [64/147], loss=14.6198
	step [65/147], loss=14.1705
	step [66/147], loss=15.4326
	step [67/147], loss=14.2435
	step [68/147], loss=13.3088
	step [69/147], loss=12.8720
	step [70/147], loss=14.5396
	step [71/147], loss=13.2397
	step [72/147], loss=15.5441
	step [73/147], loss=12.7276
	step [74/147], loss=11.7985
	step [75/147], loss=15.1496
	step [76/147], loss=12.8644
	step [77/147], loss=12.5104
	step [78/147], loss=14.0281
	step [79/147], loss=11.7444
	step [80/147], loss=12.4837
	step [81/147], loss=14.2378
	step [82/147], loss=14.1123
	step [83/147], loss=13.2909
	step [84/147], loss=12.4990
	step [85/147], loss=13.0286
	step [86/147], loss=11.4078
	step [87/147], loss=14.6915
	step [88/147], loss=11.8935
	step [89/147], loss=13.5161
	step [90/147], loss=13.5035
	step [91/147], loss=13.1632
	step [92/147], loss=15.2431
	step [93/147], loss=12.4085
	step [94/147], loss=12.7062
	step [95/147], loss=13.1784
	step [96/147], loss=12.8533
	step [97/147], loss=13.5579
	step [98/147], loss=12.8232
	step [99/147], loss=12.5462
	step [100/147], loss=15.0989
	step [101/147], loss=13.6889
	step [102/147], loss=11.3307
	step [103/147], loss=12.3107
	step [104/147], loss=12.0402
	step [105/147], loss=13.7032
	step [106/147], loss=12.0947
	step [107/147], loss=13.9764
	step [108/147], loss=12.6213
	step [109/147], loss=11.4855
	step [110/147], loss=12.0220
	step [111/147], loss=12.4063
	step [112/147], loss=11.7484
	step [113/147], loss=14.5520
	step [114/147], loss=11.0113
	step [115/147], loss=13.8413
	step [116/147], loss=11.7929
	step [117/147], loss=11.6698
	step [118/147], loss=11.4830
	step [119/147], loss=12.0437
	step [120/147], loss=12.3975
	step [121/147], loss=13.9011
	step [122/147], loss=11.2745
	step [123/147], loss=14.3671
	step [124/147], loss=13.0634
	step [125/147], loss=12.6159
	step [126/147], loss=12.3294
	step [127/147], loss=13.9356
	step [128/147], loss=12.4905
	step [129/147], loss=13.5587
	step [130/147], loss=13.7994
	step [131/147], loss=13.3475
	step [132/147], loss=13.4438
	step [133/147], loss=14.2337
	step [134/147], loss=14.0629
	step [135/147], loss=12.4036
	step [136/147], loss=13.6198
	step [137/147], loss=12.8078
	step [138/147], loss=13.5254
	step [139/147], loss=13.7892
	step [140/147], loss=12.6274
	step [141/147], loss=11.3162
	step [142/147], loss=12.8374
	step [143/147], loss=12.5910
	step [144/147], loss=14.1518
	step [145/147], loss=13.8950
	step [146/147], loss=13.6080
	step [147/147], loss=7.0162
	Evaluating
	loss=0.0387, precision=0.1588, recall=0.9942, f1=0.2738
Training epoch 23
	step [1/147], loss=11.3640
	step [2/147], loss=11.8818
	step [3/147], loss=12.7398
	step [4/147], loss=14.2934
	step [5/147], loss=11.7739
	step [6/147], loss=13.3371
	step [7/147], loss=15.3142
	step [8/147], loss=12.6013
	step [9/147], loss=11.2796
	step [10/147], loss=11.3334
	step [11/147], loss=11.2127
	step [12/147], loss=12.0687
	step [13/147], loss=11.4135
	step [14/147], loss=13.4600
	step [15/147], loss=11.7645
	step [16/147], loss=12.6785
	step [17/147], loss=12.7093
	step [18/147], loss=12.5353
	step [19/147], loss=15.1407
	step [20/147], loss=13.2919
	step [21/147], loss=13.0429
	step [22/147], loss=14.4301
	step [23/147], loss=12.3953
	step [24/147], loss=12.4869
	step [25/147], loss=12.6755
	step [26/147], loss=12.5375
	step [27/147], loss=11.5461
	step [28/147], loss=12.1972
	step [29/147], loss=12.6589
	step [30/147], loss=11.3082
	step [31/147], loss=12.7697
	step [32/147], loss=14.4619
	step [33/147], loss=13.5425
	step [34/147], loss=11.9907
	step [35/147], loss=13.1326
	step [36/147], loss=13.9236
	step [37/147], loss=12.7793
	step [38/147], loss=13.3734
	step [39/147], loss=12.5213
	step [40/147], loss=12.7345
	step [41/147], loss=13.5308
	step [42/147], loss=13.0128
	step [43/147], loss=16.2862
	step [44/147], loss=11.5238
	step [45/147], loss=13.4703
	step [46/147], loss=12.5761
	step [47/147], loss=11.8630
	step [48/147], loss=12.7326
	step [49/147], loss=12.1556
	step [50/147], loss=12.0277
	step [51/147], loss=13.8890
	step [52/147], loss=11.7247
	step [53/147], loss=13.6986
	step [54/147], loss=12.0615
	step [55/147], loss=11.3322
	step [56/147], loss=14.1462
	step [57/147], loss=12.1842
	step [58/147], loss=14.7159
	step [59/147], loss=13.0515
	step [60/147], loss=12.5470
	step [61/147], loss=13.2849
	step [62/147], loss=13.9033
	step [63/147], loss=11.1764
	step [64/147], loss=12.0586
	step [65/147], loss=13.8850
	step [66/147], loss=12.0547
	step [67/147], loss=13.7243
	step [68/147], loss=13.2396
	step [69/147], loss=14.2058
	step [70/147], loss=12.6932
	step [71/147], loss=12.8518
	step [72/147], loss=13.0764
	step [73/147], loss=11.2174
	step [74/147], loss=13.1459
	step [75/147], loss=12.7378
	step [76/147], loss=11.8336
	step [77/147], loss=15.6061
	step [78/147], loss=13.0060
	step [79/147], loss=10.9770
	step [80/147], loss=10.5860
	step [81/147], loss=12.1179
	step [82/147], loss=13.6038
	step [83/147], loss=13.0864
	step [84/147], loss=12.5834
	step [85/147], loss=12.8982
	step [86/147], loss=11.9645
	step [87/147], loss=12.1266
	step [88/147], loss=12.2413
	step [89/147], loss=14.4235
	step [90/147], loss=12.5403
	step [91/147], loss=11.3424
	step [92/147], loss=11.5187
	step [93/147], loss=11.8069
	step [94/147], loss=12.0181
	step [95/147], loss=11.3976
	step [96/147], loss=13.5594
	step [97/147], loss=12.6563
	step [98/147], loss=12.2360
	step [99/147], loss=11.4462
	step [100/147], loss=15.8928
	step [101/147], loss=13.5097
	step [102/147], loss=11.8864
	step [103/147], loss=12.9010
	step [104/147], loss=13.5348
	step [105/147], loss=13.7492
	step [106/147], loss=14.6464
	step [107/147], loss=11.2504
	step [108/147], loss=11.7133
	step [109/147], loss=11.9311
	step [110/147], loss=13.4045
	step [111/147], loss=10.8598
	step [112/147], loss=12.7608
	step [113/147], loss=12.7871
	step [114/147], loss=13.5905
	step [115/147], loss=13.1428
	step [116/147], loss=12.6274
	step [117/147], loss=11.4009
	step [118/147], loss=13.9444
	step [119/147], loss=13.9131
	step [120/147], loss=11.9884
	step [121/147], loss=12.5006
	step [122/147], loss=13.8875
	step [123/147], loss=12.7760
	step [124/147], loss=12.6762
	step [125/147], loss=13.1324
	step [126/147], loss=12.0846
	step [127/147], loss=12.4836
	step [128/147], loss=11.6349
	step [129/147], loss=12.1984
	step [130/147], loss=12.4687
	step [131/147], loss=11.5998
	step [132/147], loss=12.6049
	step [133/147], loss=13.0549
	step [134/147], loss=11.8163
	step [135/147], loss=13.2620
	step [136/147], loss=12.2908
	step [137/147], loss=11.1155
	step [138/147], loss=11.8698
	step [139/147], loss=13.5186
	step [140/147], loss=11.1394
	step [141/147], loss=11.9007
	step [142/147], loss=13.9969
	step [143/147], loss=15.0795
	step [144/147], loss=13.7231
	step [145/147], loss=11.1601
	step [146/147], loss=11.6652
	step [147/147], loss=7.7861
	Evaluating
	loss=0.0342, precision=0.1892, recall=0.9936, f1=0.3179
Training epoch 24
	step [1/147], loss=14.3628
	step [2/147], loss=11.1333
	step [3/147], loss=11.8201
	step [4/147], loss=13.7585
	step [5/147], loss=10.8355
	step [6/147], loss=14.0585
	step [7/147], loss=13.9642
	step [8/147], loss=12.2835
	step [9/147], loss=12.3321
	step [10/147], loss=14.4413
	step [11/147], loss=13.5851
	step [12/147], loss=11.8979
	step [13/147], loss=13.2959
	step [14/147], loss=13.3275
	step [15/147], loss=12.7442
	step [16/147], loss=12.5291
	step [17/147], loss=12.8547
	step [18/147], loss=11.5331
	step [19/147], loss=12.3195
	step [20/147], loss=12.5465
	step [21/147], loss=11.1028
	step [22/147], loss=14.2376
	step [23/147], loss=11.7143
	step [24/147], loss=11.9028
	step [25/147], loss=11.7197
	step [26/147], loss=13.7059
	step [27/147], loss=13.6008
	step [28/147], loss=13.8091
	step [29/147], loss=12.6127
	step [30/147], loss=12.4474
	step [31/147], loss=10.3476
	step [32/147], loss=12.3089
	step [33/147], loss=12.0570
	step [34/147], loss=12.4286
	step [35/147], loss=11.4121
	step [36/147], loss=10.4015
	step [37/147], loss=12.4173
	step [38/147], loss=14.1644
	step [39/147], loss=11.6387
	step [40/147], loss=13.1183
	step [41/147], loss=13.0463
	step [42/147], loss=14.2840
	step [43/147], loss=10.4203
	step [44/147], loss=11.6673
	step [45/147], loss=11.7870
	step [46/147], loss=12.1941
	step [47/147], loss=12.4429
	step [48/147], loss=13.6185
	step [49/147], loss=12.2032
	step [50/147], loss=10.9452
	step [51/147], loss=12.4047
	step [52/147], loss=12.2652
	step [53/147], loss=12.1698
	step [54/147], loss=12.6660
	step [55/147], loss=11.9309
	step [56/147], loss=14.6668
	step [57/147], loss=12.4360
	step [58/147], loss=13.0894
	step [59/147], loss=12.6627
	step [60/147], loss=12.3692
	step [61/147], loss=11.6198
	step [62/147], loss=10.9564
	step [63/147], loss=11.4943
	step [64/147], loss=12.4703
	step [65/147], loss=10.4000
	step [66/147], loss=11.7118
	step [67/147], loss=11.1216
	step [68/147], loss=12.6999
	step [69/147], loss=12.5064
	step [70/147], loss=12.0673
	step [71/147], loss=9.8569
	step [72/147], loss=13.3127
	step [73/147], loss=12.0235
	step [74/147], loss=11.2910
	step [75/147], loss=11.5502
	step [76/147], loss=13.3724
	step [77/147], loss=10.2369
	step [78/147], loss=12.9485
	step [79/147], loss=13.4711
	step [80/147], loss=11.9778
	step [81/147], loss=12.9099
	step [82/147], loss=10.8138
	step [83/147], loss=12.3243
	step [84/147], loss=12.7521
	step [85/147], loss=12.9841
	step [86/147], loss=11.4262
	step [87/147], loss=11.7628
	step [88/147], loss=12.7778
	step [89/147], loss=13.2943
	step [90/147], loss=11.4206
	step [91/147], loss=12.6946
	step [92/147], loss=12.2173
	step [93/147], loss=13.0364
	step [94/147], loss=11.9002
	step [95/147], loss=12.1916
	step [96/147], loss=12.7500
	step [97/147], loss=13.3209
	step [98/147], loss=12.9525
	step [99/147], loss=10.7619
	step [100/147], loss=12.6683
	step [101/147], loss=11.4985
	step [102/147], loss=11.2928
	step [103/147], loss=14.0476
	step [104/147], loss=12.4712
	step [105/147], loss=11.1319
	step [106/147], loss=11.8123
	step [107/147], loss=10.2785
	step [108/147], loss=13.0016
	step [109/147], loss=13.3825
	step [110/147], loss=15.0426
	step [111/147], loss=12.5475
	step [112/147], loss=13.6677
	step [113/147], loss=13.6032
	step [114/147], loss=12.7236
	step [115/147], loss=11.3442
	step [116/147], loss=9.8412
	step [117/147], loss=11.5874
	step [118/147], loss=11.2144
	step [119/147], loss=12.9960
	step [120/147], loss=12.6686
	step [121/147], loss=10.9317
	step [122/147], loss=11.0021
	step [123/147], loss=14.4362
	step [124/147], loss=13.6864
	step [125/147], loss=12.4684
	step [126/147], loss=11.3338
	step [127/147], loss=11.6444
	step [128/147], loss=13.0891
	step [129/147], loss=12.8364
	step [130/147], loss=12.2760
	step [131/147], loss=13.6919
	step [132/147], loss=12.4910
	step [133/147], loss=11.6832
	step [134/147], loss=12.0910
	step [135/147], loss=15.5895
	step [136/147], loss=13.5834
	step [137/147], loss=12.3727
	step [138/147], loss=11.8237
	step [139/147], loss=12.1430
	step [140/147], loss=12.1580
	step [141/147], loss=10.4855
	step [142/147], loss=10.9380
	step [143/147], loss=11.5667
	step [144/147], loss=13.4799
	step [145/147], loss=12.9498
	step [146/147], loss=13.8358
	step [147/147], loss=5.3845
	Evaluating
	loss=0.0314, precision=0.1968, recall=0.9933, f1=0.3285
Training epoch 25
	step [1/147], loss=11.3056
	step [2/147], loss=12.7139
	step [3/147], loss=12.9501
	step [4/147], loss=13.3744
	step [5/147], loss=11.5584
	step [6/147], loss=13.6739
	step [7/147], loss=11.4380
	step [8/147], loss=13.5765
	step [9/147], loss=11.6259
	step [10/147], loss=12.0086
	step [11/147], loss=10.4621
	step [12/147], loss=12.1728
	step [13/147], loss=11.1560
	step [14/147], loss=10.4331
	step [15/147], loss=12.0422
	step [16/147], loss=12.7615
	step [17/147], loss=11.5795
	step [18/147], loss=12.0831
	step [19/147], loss=12.2510
	step [20/147], loss=11.5471
	step [21/147], loss=10.8011
	step [22/147], loss=12.5143
	step [23/147], loss=13.4451
	step [24/147], loss=9.6085
	step [25/147], loss=11.4667
	step [26/147], loss=12.1776
	step [27/147], loss=11.7828
	step [28/147], loss=11.2954
	step [29/147], loss=12.8403
	step [30/147], loss=10.2715
	step [31/147], loss=10.0287
	step [32/147], loss=11.6550
	step [33/147], loss=11.1894
	step [34/147], loss=12.8257
	step [35/147], loss=11.6145
	step [36/147], loss=8.9541
	step [37/147], loss=11.2848
	step [38/147], loss=11.0365
	step [39/147], loss=11.7529
	step [40/147], loss=11.5658
	step [41/147], loss=10.5752
	step [42/147], loss=11.3876
	step [43/147], loss=10.3534
	step [44/147], loss=12.1603
	step [45/147], loss=12.2529
	step [46/147], loss=12.5298
	step [47/147], loss=10.6457
	step [48/147], loss=10.9126
	step [49/147], loss=13.8924
	step [50/147], loss=10.5068
	step [51/147], loss=13.2653
	step [52/147], loss=10.8597
	step [53/147], loss=12.1917
	step [54/147], loss=11.0316
	step [55/147], loss=11.2263
	step [56/147], loss=11.4451
	step [57/147], loss=12.0011
	step [58/147], loss=12.1389
	step [59/147], loss=10.5428
	step [60/147], loss=12.1017
	step [61/147], loss=13.0700
	step [62/147], loss=10.6889
	step [63/147], loss=12.0994
	step [64/147], loss=11.4000
	step [65/147], loss=11.5341
	step [66/147], loss=11.3786
	step [67/147], loss=11.2318
	step [68/147], loss=13.9957
	step [69/147], loss=12.0499
	step [70/147], loss=12.5491
	step [71/147], loss=12.1844
	step [72/147], loss=12.1854
	step [73/147], loss=12.8210
	step [74/147], loss=13.2317
	step [75/147], loss=12.5781
	step [76/147], loss=9.7813
	step [77/147], loss=13.2539
	step [78/147], loss=11.9874
	step [79/147], loss=11.1438
	step [80/147], loss=11.8647
	step [81/147], loss=12.4687
	step [82/147], loss=9.9529
	step [83/147], loss=10.5634
	step [84/147], loss=11.6794
	step [85/147], loss=11.1307
	step [86/147], loss=11.2590
	step [87/147], loss=10.1759
	step [88/147], loss=12.2516
	step [89/147], loss=11.0416
	step [90/147], loss=10.3716
	step [91/147], loss=11.0863
	step [92/147], loss=11.3642
	step [93/147], loss=12.3584
	step [94/147], loss=12.6529
	step [95/147], loss=13.7876
	step [96/147], loss=12.9547
	step [97/147], loss=12.4498
	step [98/147], loss=11.9674
	step [99/147], loss=11.0506
	step [100/147], loss=11.1146
	step [101/147], loss=12.8225
	step [102/147], loss=13.3946
	step [103/147], loss=14.4689
	step [104/147], loss=12.0917
	step [105/147], loss=12.2766
	step [106/147], loss=13.4842
	step [107/147], loss=11.6995
	step [108/147], loss=13.2750
	step [109/147], loss=12.3919
	step [110/147], loss=10.4045
	step [111/147], loss=12.7421
	step [112/147], loss=12.8393
	step [113/147], loss=10.6982
	step [114/147], loss=13.7115
	step [115/147], loss=10.3755
	step [116/147], loss=11.5351
	step [117/147], loss=10.6679
	step [118/147], loss=11.6001
	step [119/147], loss=11.6220
	step [120/147], loss=11.0114
	step [121/147], loss=11.9219
	step [122/147], loss=12.0048
	step [123/147], loss=11.8411
	step [124/147], loss=11.5938
	step [125/147], loss=9.0704
	step [126/147], loss=10.5181
	step [127/147], loss=12.8945
	step [128/147], loss=12.7242
	step [129/147], loss=11.9684
	step [130/147], loss=13.2560
	step [131/147], loss=12.3520
	step [132/147], loss=11.5429
	step [133/147], loss=12.8767
	step [134/147], loss=12.3683
	step [135/147], loss=12.8765
	step [136/147], loss=11.5537
	step [137/147], loss=12.2055
	step [138/147], loss=12.7972
	step [139/147], loss=11.7946
	step [140/147], loss=11.4373
	step [141/147], loss=10.0553
	step [142/147], loss=11.3096
	step [143/147], loss=12.6336
	step [144/147], loss=11.0808
	step [145/147], loss=12.7095
	step [146/147], loss=13.4379
	step [147/147], loss=6.0244
	Evaluating
	loss=0.0353, precision=0.1655, recall=0.9942, f1=0.2837
Training epoch 26
	step [1/147], loss=10.5263
	step [2/147], loss=11.7845
	step [3/147], loss=12.0695
	step [4/147], loss=11.0182
	step [5/147], loss=10.7680
	step [6/147], loss=11.7361
	step [7/147], loss=14.4899
	step [8/147], loss=11.4570
	step [9/147], loss=12.7852
	step [10/147], loss=12.1688
	step [11/147], loss=11.3187
	step [12/147], loss=11.1120
	step [13/147], loss=11.3391
	step [14/147], loss=12.6506
	step [15/147], loss=11.1469
	step [16/147], loss=12.3489
	step [17/147], loss=13.9243
	step [18/147], loss=12.2951
	step [19/147], loss=13.9081
	step [20/147], loss=12.2171
	step [21/147], loss=13.2594
	step [22/147], loss=10.6401
	step [23/147], loss=11.4657
	step [24/147], loss=10.1858
	step [25/147], loss=14.2460
	step [26/147], loss=12.4251
	step [27/147], loss=11.5837
	step [28/147], loss=12.5035
	step [29/147], loss=11.2326
	step [30/147], loss=11.5789
	step [31/147], loss=12.7048
	step [32/147], loss=12.5211
	step [33/147], loss=10.9423
	step [34/147], loss=11.8699
	step [35/147], loss=11.2925
	step [36/147], loss=13.5963
	step [37/147], loss=12.0600
	step [38/147], loss=10.4206
	step [39/147], loss=11.1233
	step [40/147], loss=11.5489
	step [41/147], loss=11.3045
	step [42/147], loss=11.5196
	step [43/147], loss=10.9681
	step [44/147], loss=10.0543
	step [45/147], loss=11.5967
	step [46/147], loss=11.0190
	step [47/147], loss=12.0266
	step [48/147], loss=11.5641
	step [49/147], loss=11.5470
	step [50/147], loss=11.8871
	step [51/147], loss=10.0697
	step [52/147], loss=11.8650
	step [53/147], loss=11.5456
	step [54/147], loss=11.9960
	step [55/147], loss=11.9409
	step [56/147], loss=11.3619
	step [57/147], loss=11.5796
	step [58/147], loss=12.2244
	step [59/147], loss=9.8898
	step [60/147], loss=10.2868
	step [61/147], loss=11.5608
	step [62/147], loss=11.5191
	step [63/147], loss=12.0186
	step [64/147], loss=13.8416
	step [65/147], loss=12.4246
	step [66/147], loss=11.0165
	step [67/147], loss=13.0014
	step [68/147], loss=11.1362
	step [69/147], loss=12.2046
	step [70/147], loss=9.8248
	step [71/147], loss=12.2644
	step [72/147], loss=11.5948
	step [73/147], loss=12.5675
	step [74/147], loss=12.4802
	step [75/147], loss=10.8027
	step [76/147], loss=11.6634
	step [77/147], loss=12.0144
	step [78/147], loss=11.6667
	step [79/147], loss=12.4236
	step [80/147], loss=10.8547
	step [81/147], loss=11.7710
	step [82/147], loss=12.4867
	step [83/147], loss=10.8492
	step [84/147], loss=11.2026
	step [85/147], loss=10.3943
	step [86/147], loss=11.6875
	step [87/147], loss=11.2589
	step [88/147], loss=11.8418
	step [89/147], loss=12.3082
	step [90/147], loss=12.8443
	step [91/147], loss=12.2861
	step [92/147], loss=12.8351
	step [93/147], loss=13.1039
	step [94/147], loss=11.3448
	step [95/147], loss=12.2437
	step [96/147], loss=11.4607
	step [97/147], loss=10.8635
	step [98/147], loss=13.2874
	step [99/147], loss=9.6598
	step [100/147], loss=11.9007
	step [101/147], loss=9.9646
	step [102/147], loss=11.4437
	step [103/147], loss=12.7763
	step [104/147], loss=11.9133
	step [105/147], loss=9.8765
	step [106/147], loss=11.6608
	step [107/147], loss=11.7731
	step [108/147], loss=11.0621
	step [109/147], loss=11.7436
	step [110/147], loss=13.6800
	step [111/147], loss=11.0946
	step [112/147], loss=10.7080
	step [113/147], loss=11.5439
	step [114/147], loss=11.2962
	step [115/147], loss=11.2337
	step [116/147], loss=10.2249
	step [117/147], loss=9.8242
	step [118/147], loss=10.0617
	step [119/147], loss=12.5554
	step [120/147], loss=10.0091
	step [121/147], loss=12.4932
	step [122/147], loss=12.5868
	step [123/147], loss=13.2029
	step [124/147], loss=11.0585
	step [125/147], loss=10.4868
	step [126/147], loss=11.1274
	step [127/147], loss=11.7069
	step [128/147], loss=10.7013
	step [129/147], loss=13.0648
	step [130/147], loss=11.8677
	step [131/147], loss=11.0375
	step [132/147], loss=12.5407
	step [133/147], loss=11.9883
	step [134/147], loss=11.5277
	step [135/147], loss=12.3837
	step [136/147], loss=11.2940
	step [137/147], loss=11.4796
	step [138/147], loss=11.2773
	step [139/147], loss=11.0085
	step [140/147], loss=12.1722
	step [141/147], loss=8.7623
	step [142/147], loss=10.6947
	step [143/147], loss=12.2053
	step [144/147], loss=11.7539
	step [145/147], loss=12.4631
	step [146/147], loss=10.8816
	step [147/147], loss=4.6421
	Evaluating
	loss=0.0355, precision=0.1751, recall=0.9940, f1=0.2977
Training epoch 27
	step [1/147], loss=12.5623
	step [2/147], loss=11.0605
	step [3/147], loss=11.8380
	step [4/147], loss=12.0506
	step [5/147], loss=10.9574
	step [6/147], loss=11.9685
	step [7/147], loss=12.4190
	step [8/147], loss=11.1521
	step [9/147], loss=10.4557
	step [10/147], loss=11.8313
	step [11/147], loss=9.4143
	step [12/147], loss=10.8188
	step [13/147], loss=10.0892
	step [14/147], loss=11.2277
	step [15/147], loss=11.4978
	step [16/147], loss=9.2929
	step [17/147], loss=11.6375
	step [18/147], loss=12.5465
	step [19/147], loss=11.7482
	step [20/147], loss=10.4114
	step [21/147], loss=11.4103
	step [22/147], loss=12.2745
	step [23/147], loss=12.2132
	step [24/147], loss=11.2862
	step [25/147], loss=12.3807
	step [26/147], loss=11.0738
	step [27/147], loss=11.8748
	step [28/147], loss=10.9686
	step [29/147], loss=11.4329
	step [30/147], loss=11.6910
	step [31/147], loss=11.0512
	step [32/147], loss=12.0727
	step [33/147], loss=11.4443
	step [34/147], loss=10.3714
	step [35/147], loss=10.0449
	step [36/147], loss=10.3340
	step [37/147], loss=12.4520
	step [38/147], loss=12.1659
	step [39/147], loss=11.3598
	step [40/147], loss=11.0244
	step [41/147], loss=11.2551
	step [42/147], loss=12.0173
	step [43/147], loss=11.3742
	step [44/147], loss=9.8308
	step [45/147], loss=10.0278
	step [46/147], loss=11.1913
	step [47/147], loss=11.2743
	step [48/147], loss=10.6077
	step [49/147], loss=12.1164
	step [50/147], loss=11.7308
	step [51/147], loss=13.9343
	step [52/147], loss=10.2764
	step [53/147], loss=11.5993
	step [54/147], loss=10.7960
	step [55/147], loss=10.5171
	step [56/147], loss=11.6599
	step [57/147], loss=10.5964
	step [58/147], loss=11.1984
	step [59/147], loss=10.8963
	step [60/147], loss=13.7046
	step [61/147], loss=9.5986
	step [62/147], loss=11.2888
	step [63/147], loss=11.4870
	step [64/147], loss=10.3037
	step [65/147], loss=10.8715
	step [66/147], loss=12.0052
	step [67/147], loss=10.7982
	step [68/147], loss=11.3541
	step [69/147], loss=10.2467
	step [70/147], loss=12.1381
	step [71/147], loss=10.9213
	step [72/147], loss=11.0512
	step [73/147], loss=12.2731
	step [74/147], loss=9.5273
	step [75/147], loss=12.3143
	step [76/147], loss=11.7609
	step [77/147], loss=11.0024
	step [78/147], loss=11.6247
	step [79/147], loss=11.2767
	step [80/147], loss=11.6641
	step [81/147], loss=11.3900
	step [82/147], loss=9.9582
	step [83/147], loss=10.1199
	step [84/147], loss=10.8904
	step [85/147], loss=11.2572
	step [86/147], loss=11.0307
	step [87/147], loss=11.6322
	step [88/147], loss=9.8503
	step [89/147], loss=12.5575
	step [90/147], loss=11.4841
	step [91/147], loss=10.5199
	step [92/147], loss=10.6963
	step [93/147], loss=10.5468
	step [94/147], loss=10.3873
	step [95/147], loss=10.2945
	step [96/147], loss=12.4295
	step [97/147], loss=10.9652
	step [98/147], loss=10.2981
	step [99/147], loss=10.6945
	step [100/147], loss=9.7008
	step [101/147], loss=10.8389
	step [102/147], loss=11.6868
	step [103/147], loss=11.1592
	step [104/147], loss=11.2743
	step [105/147], loss=11.2461
	step [106/147], loss=12.6955
	step [107/147], loss=10.4889
	step [108/147], loss=11.6777
	step [109/147], loss=12.0933
	step [110/147], loss=12.9235
	step [111/147], loss=12.3739
	step [112/147], loss=10.7928
	step [113/147], loss=11.9856
	step [114/147], loss=10.9270
	step [115/147], loss=10.6877
	step [116/147], loss=10.5380
	step [117/147], loss=11.7697
	step [118/147], loss=10.0999
	step [119/147], loss=11.9688
	step [120/147], loss=10.7854
	step [121/147], loss=10.4937
	step [122/147], loss=9.3857
	step [123/147], loss=10.1199
	step [124/147], loss=11.3264
	step [125/147], loss=11.0351
	step [126/147], loss=12.2240
	step [127/147], loss=10.2389
	step [128/147], loss=11.1197
	step [129/147], loss=10.6830
	step [130/147], loss=10.4732
	step [131/147], loss=9.1716
	step [132/147], loss=11.3685
	step [133/147], loss=11.0342
	step [134/147], loss=11.1511
	step [135/147], loss=13.7392
	step [136/147], loss=12.4620
	step [137/147], loss=10.4895
	step [138/147], loss=11.3551
	step [139/147], loss=9.6015
	step [140/147], loss=10.5860
	step [141/147], loss=10.5922
	step [142/147], loss=13.3748
	step [143/147], loss=11.3025
	step [144/147], loss=10.5736
	step [145/147], loss=11.2897
	step [146/147], loss=12.1258
	step [147/147], loss=5.5561
	Evaluating
	loss=0.0323, precision=0.1911, recall=0.9938, f1=0.3205
Training epoch 28
	step [1/147], loss=10.6736
	step [2/147], loss=11.1021
	step [3/147], loss=10.8695
	step [4/147], loss=11.3550
	step [5/147], loss=10.7797
	step [6/147], loss=9.3596
	step [7/147], loss=11.4641
	step [8/147], loss=9.2926
	step [9/147], loss=9.1806
	step [10/147], loss=10.8933
	step [11/147], loss=12.2559
	step [12/147], loss=11.7201
	step [13/147], loss=10.8553
	step [14/147], loss=11.8010
	step [15/147], loss=10.4965
	step [16/147], loss=10.7040
	step [17/147], loss=11.0849
	step [18/147], loss=10.0635
	step [19/147], loss=11.8256
	step [20/147], loss=8.9400
	step [21/147], loss=10.3933
	step [22/147], loss=11.8922
	step [23/147], loss=10.6834
	step [24/147], loss=9.2107
	step [25/147], loss=10.7745
	step [26/147], loss=11.7946
	step [27/147], loss=8.9780
	step [28/147], loss=11.3575
	step [29/147], loss=11.1902
	step [30/147], loss=10.9945
	step [31/147], loss=10.2875
	step [32/147], loss=11.7764
	step [33/147], loss=10.3600
	step [34/147], loss=11.4973
	step [35/147], loss=11.6061
	step [36/147], loss=10.1976
	step [37/147], loss=9.4769
	step [38/147], loss=9.5032
	step [39/147], loss=12.5969
	step [40/147], loss=11.8440
	step [41/147], loss=9.9219
	step [42/147], loss=10.3388
	step [43/147], loss=10.3578
	step [44/147], loss=10.7713
	step [45/147], loss=10.8475
	step [46/147], loss=11.0717
	step [47/147], loss=10.4578
	step [48/147], loss=9.4722
	step [49/147], loss=9.7001
	step [50/147], loss=8.9464
	step [51/147], loss=10.9922
	step [52/147], loss=12.5083
	step [53/147], loss=11.2269
	step [54/147], loss=9.4571
	step [55/147], loss=12.4006
	step [56/147], loss=9.6397
	step [57/147], loss=11.3906
	step [58/147], loss=11.2935
	step [59/147], loss=11.5071
	step [60/147], loss=10.0166
	step [61/147], loss=10.6522
	step [62/147], loss=11.1048
	step [63/147], loss=10.6986
	step [64/147], loss=9.4977
	step [65/147], loss=9.9072
	step [66/147], loss=12.4815
	step [67/147], loss=10.3670
	step [68/147], loss=10.5751
	step [69/147], loss=9.9757
	step [70/147], loss=10.0725
	step [71/147], loss=11.7690
	step [72/147], loss=12.0169
	step [73/147], loss=11.7968
	step [74/147], loss=11.4135
	step [75/147], loss=10.2090
	step [76/147], loss=11.5852
	step [77/147], loss=10.6539
	step [78/147], loss=10.5119
	step [79/147], loss=10.1522
	step [80/147], loss=9.8615
	step [81/147], loss=10.0149
	step [82/147], loss=12.2175
	step [83/147], loss=10.3129
	step [84/147], loss=9.9377
	step [85/147], loss=11.2417
	step [86/147], loss=9.8053
	step [87/147], loss=10.2040
	step [88/147], loss=11.1930
	step [89/147], loss=9.7085
	step [90/147], loss=9.7280
	step [91/147], loss=11.6520
	step [92/147], loss=9.8117
	step [93/147], loss=11.3944
	step [94/147], loss=11.5430
	step [95/147], loss=9.6255
	step [96/147], loss=11.4539
	step [97/147], loss=10.6649
	step [98/147], loss=11.9369
	step [99/147], loss=12.3584
	step [100/147], loss=11.9141
	step [101/147], loss=9.8537
	step [102/147], loss=11.3532
	step [103/147], loss=11.1900
	step [104/147], loss=10.8147
	step [105/147], loss=12.3214
	step [106/147], loss=12.1882
	step [107/147], loss=12.5221
	step [108/147], loss=10.9695
	step [109/147], loss=12.8933
	step [110/147], loss=9.5575
	step [111/147], loss=12.1181
	step [112/147], loss=11.2917
	step [113/147], loss=11.4854
	step [114/147], loss=11.3872
	step [115/147], loss=11.0296
	step [116/147], loss=11.6636
	step [117/147], loss=10.7491
	step [118/147], loss=10.9009
	step [119/147], loss=11.1250
	step [120/147], loss=11.1331
	step [121/147], loss=10.6648
	step [122/147], loss=10.6688
	step [123/147], loss=12.6376
	step [124/147], loss=10.6670
	step [125/147], loss=13.0796
	step [126/147], loss=11.0361
	step [127/147], loss=11.0185
	step [128/147], loss=11.1777
	step [129/147], loss=10.3206
	step [130/147], loss=12.8520
	step [131/147], loss=10.1912
	step [132/147], loss=10.7061
	step [133/147], loss=12.6982
	step [134/147], loss=11.7224
	step [135/147], loss=11.8054
	step [136/147], loss=10.9996
	step [137/147], loss=10.6099
	step [138/147], loss=11.1643
	step [139/147], loss=10.9428
	step [140/147], loss=11.7454
	step [141/147], loss=10.0491
	step [142/147], loss=10.0178
	step [143/147], loss=11.1429
	step [144/147], loss=12.5357
	step [145/147], loss=10.8677
	step [146/147], loss=9.6424
	step [147/147], loss=3.6460
	Evaluating
	loss=0.0277, precision=0.2099, recall=0.9932, f1=0.3466
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/147], loss=10.0785
	step [2/147], loss=9.7362
	step [3/147], loss=9.6027
	step [4/147], loss=11.7805
	step [5/147], loss=9.8151
	step [6/147], loss=11.0113
	step [7/147], loss=11.4081
	step [8/147], loss=10.2395
	step [9/147], loss=9.7995
	step [10/147], loss=9.3282
	step [11/147], loss=10.5358
	step [12/147], loss=11.9978
	step [13/147], loss=10.4820
	step [14/147], loss=9.9812
	step [15/147], loss=9.4032
	step [16/147], loss=10.8134
	step [17/147], loss=10.1572
	step [18/147], loss=12.9402
	step [19/147], loss=11.7024
	step [20/147], loss=10.9179
	step [21/147], loss=10.1577
	step [22/147], loss=10.1081
	step [23/147], loss=10.2319
	step [24/147], loss=11.0433
	step [25/147], loss=11.2822
	step [26/147], loss=8.5513
	step [27/147], loss=11.8147
	step [28/147], loss=9.7912
	step [29/147], loss=11.8535
	step [30/147], loss=10.3544
	step [31/147], loss=9.7665
	step [32/147], loss=10.3511
	step [33/147], loss=11.4173
	step [34/147], loss=10.6592
	step [35/147], loss=10.0609
	step [36/147], loss=10.8966
	step [37/147], loss=11.1227
	step [38/147], loss=11.3783
	step [39/147], loss=10.2463
	step [40/147], loss=9.8731
	step [41/147], loss=13.5822
	step [42/147], loss=10.3372
	step [43/147], loss=10.8701
	step [44/147], loss=10.4179
	step [45/147], loss=9.5166
	step [46/147], loss=10.3152
	step [47/147], loss=12.2336
	step [48/147], loss=10.6711
	step [49/147], loss=9.9499
	step [50/147], loss=9.2458
	step [51/147], loss=12.4233
	step [52/147], loss=9.3655
	step [53/147], loss=9.3039
	step [54/147], loss=11.9691
	step [55/147], loss=9.9541
	step [56/147], loss=10.3900
	step [57/147], loss=11.1554
	step [58/147], loss=12.7807
	step [59/147], loss=10.5618
	step [60/147], loss=12.2758
	step [61/147], loss=9.6207
	step [62/147], loss=9.3570
	step [63/147], loss=11.1631
	step [64/147], loss=9.6200
	step [65/147], loss=11.6773
	step [66/147], loss=9.8088
	step [67/147], loss=13.5137
	step [68/147], loss=10.5413
	step [69/147], loss=11.0367
	step [70/147], loss=10.4323
	step [71/147], loss=12.3064
	step [72/147], loss=11.3295
	step [73/147], loss=9.9287
	step [74/147], loss=10.5216
	step [75/147], loss=10.3191
	step [76/147], loss=9.7388
	step [77/147], loss=9.0389
	step [78/147], loss=11.2284
	step [79/147], loss=9.7640
	step [80/147], loss=11.5472
	step [81/147], loss=10.7504
	step [82/147], loss=10.3523
	step [83/147], loss=10.9566
	step [84/147], loss=10.3511
	step [85/147], loss=8.3397
	step [86/147], loss=11.1077
	step [87/147], loss=11.5710
	step [88/147], loss=11.7695
	step [89/147], loss=9.7249
	step [90/147], loss=10.0909
	step [91/147], loss=10.1781
	step [92/147], loss=10.9966
	step [93/147], loss=10.0369
	step [94/147], loss=9.4151
	step [95/147], loss=10.6614
	step [96/147], loss=11.5392
	step [97/147], loss=9.9459
	step [98/147], loss=9.8409
	step [99/147], loss=9.1886
	step [100/147], loss=11.3754
	step [101/147], loss=10.5790
	step [102/147], loss=10.3386
	step [103/147], loss=11.0518
	step [104/147], loss=10.7206
	step [105/147], loss=9.7249
	step [106/147], loss=10.0799
	step [107/147], loss=9.5811
	step [108/147], loss=11.2249
	step [109/147], loss=9.3136
	step [110/147], loss=11.8508
	step [111/147], loss=10.2233
	step [112/147], loss=10.7554
	step [113/147], loss=9.2618
	step [114/147], loss=9.8875
	step [115/147], loss=9.3043
	step [116/147], loss=9.9862
	step [117/147], loss=11.4001
	step [118/147], loss=8.2732
	step [119/147], loss=10.1291
	step [120/147], loss=10.4818
	step [121/147], loss=9.7227
	step [122/147], loss=11.6035
	step [123/147], loss=10.7582
	step [124/147], loss=9.6097
	step [125/147], loss=9.9023
	step [126/147], loss=9.8150
	step [127/147], loss=11.3836
	step [128/147], loss=11.0906
	step [129/147], loss=10.1551
	step [130/147], loss=10.1494
	step [131/147], loss=12.4082
	step [132/147], loss=11.2073
	step [133/147], loss=9.6834
	step [134/147], loss=10.3632
	step [135/147], loss=12.7742
	step [136/147], loss=10.0334
	step [137/147], loss=10.1999
	step [138/147], loss=11.6521
	step [139/147], loss=8.9746
	step [140/147], loss=10.8543
	step [141/147], loss=10.9041
	step [142/147], loss=9.7132
	step [143/147], loss=11.4254
	step [144/147], loss=9.8921
	step [145/147], loss=10.7711
	step [146/147], loss=10.5189
	step [147/147], loss=4.5331
	Evaluating
	loss=0.0240, precision=0.2340, recall=0.9918, f1=0.3787
saving model as: 0_saved_model.pth
Training epoch 30
	step [1/147], loss=9.7194
	step [2/147], loss=9.3169
	step [3/147], loss=10.7791
	step [4/147], loss=10.7518
	step [5/147], loss=10.1705
	step [6/147], loss=10.3075
	step [7/147], loss=9.0665
	step [8/147], loss=10.0942
	step [9/147], loss=10.1812
	step [10/147], loss=8.5254
	step [11/147], loss=9.0551
	step [12/147], loss=10.3595
	step [13/147], loss=9.7315
	step [14/147], loss=10.5840
	step [15/147], loss=10.4509
	step [16/147], loss=9.4361
	step [17/147], loss=10.5389
	step [18/147], loss=10.5209
	step [19/147], loss=9.0171
	step [20/147], loss=9.6103
	step [21/147], loss=9.0231
	step [22/147], loss=10.8458
	step [23/147], loss=10.3235
	step [24/147], loss=10.1291
	step [25/147], loss=11.3245
	step [26/147], loss=10.5471
	step [27/147], loss=11.8555
	step [28/147], loss=10.1541
	step [29/147], loss=10.1687
	step [30/147], loss=11.6386
	step [31/147], loss=10.2255
	step [32/147], loss=9.9373
	step [33/147], loss=10.1064
	step [34/147], loss=10.8011
	step [35/147], loss=9.4086
	step [36/147], loss=9.8635
	step [37/147], loss=10.8394
	step [38/147], loss=8.4308
	step [39/147], loss=10.3500
	step [40/147], loss=10.0484
	step [41/147], loss=10.2990
	step [42/147], loss=11.3988
	step [43/147], loss=9.8492
	step [44/147], loss=12.8175
	step [45/147], loss=9.8026
	step [46/147], loss=10.4039
	step [47/147], loss=10.6697
	step [48/147], loss=7.9891
	step [49/147], loss=12.1632
	step [50/147], loss=10.4702
	step [51/147], loss=9.5919
	step [52/147], loss=9.8811
	step [53/147], loss=11.9347
	step [54/147], loss=10.8788
	step [55/147], loss=10.5598
	step [56/147], loss=11.4687
	step [57/147], loss=9.0795
	step [58/147], loss=10.6441
	step [59/147], loss=11.6204
	step [60/147], loss=10.8019
	step [61/147], loss=10.0115
	step [62/147], loss=10.6724
	step [63/147], loss=9.2132
	step [64/147], loss=11.2674
	step [65/147], loss=11.6216
	step [66/147], loss=10.5531
	step [67/147], loss=10.4159
	step [68/147], loss=12.0457
	step [69/147], loss=9.7647
	step [70/147], loss=9.2068
	step [71/147], loss=10.4721
	step [72/147], loss=10.5429
	step [73/147], loss=9.8456
	step [74/147], loss=10.3161
	step [75/147], loss=10.5533
	step [76/147], loss=10.5246
	step [77/147], loss=10.6303
	step [78/147], loss=9.3338
	step [79/147], loss=11.4099
	step [80/147], loss=10.7670
	step [81/147], loss=11.2089
	step [82/147], loss=14.1739
	step [83/147], loss=8.8109
	step [84/147], loss=8.8780
	step [85/147], loss=8.8325
	step [86/147], loss=9.4968
	step [87/147], loss=10.4616
	step [88/147], loss=9.6093
	step [89/147], loss=11.2957
	step [90/147], loss=10.4505
	step [91/147], loss=9.5182
	step [92/147], loss=10.7758
	step [93/147], loss=11.6378
	step [94/147], loss=9.3677
	step [95/147], loss=9.9301
	step [96/147], loss=10.4241
	step [97/147], loss=11.2835
	step [98/147], loss=9.8564
	step [99/147], loss=9.7706
	step [100/147], loss=8.4240
	step [101/147], loss=9.9836
	step [102/147], loss=10.2424
	step [103/147], loss=9.0053
	step [104/147], loss=9.2848
	step [105/147], loss=8.2186
	step [106/147], loss=9.1046
	step [107/147], loss=9.2415
	step [108/147], loss=9.1083
	step [109/147], loss=11.0781
	step [110/147], loss=10.1200
	step [111/147], loss=10.1104
	step [112/147], loss=9.4562
	step [113/147], loss=9.8148
	step [114/147], loss=11.8531
	step [115/147], loss=11.0416
	step [116/147], loss=10.0673
	step [117/147], loss=9.2022
	step [118/147], loss=8.4586
	step [119/147], loss=9.6375
	step [120/147], loss=10.1720
	step [121/147], loss=13.8763
	step [122/147], loss=10.0780
	step [123/147], loss=9.9051
	step [124/147], loss=10.8981
	step [125/147], loss=9.5085
	step [126/147], loss=11.7790
	step [127/147], loss=11.5213
	step [128/147], loss=11.1183
	step [129/147], loss=10.2936
	step [130/147], loss=9.1726
	step [131/147], loss=11.5804
	step [132/147], loss=9.9455
	step [133/147], loss=10.7974
	step [134/147], loss=10.4444
	step [135/147], loss=10.6520
	step [136/147], loss=9.5142
	step [137/147], loss=10.4556
	step [138/147], loss=9.4353
	step [139/147], loss=11.7310
	step [140/147], loss=12.1968
	step [141/147], loss=11.5611
	step [142/147], loss=10.0202
	step [143/147], loss=9.7392
	step [144/147], loss=9.2677
	step [145/147], loss=11.2200
	step [146/147], loss=9.7445
	step [147/147], loss=3.5395
	Evaluating
	loss=0.0276, precision=0.1996, recall=0.9932, f1=0.3324
Training finished
best_f1: 0.3786798745169475
directing: Y rim_enhanced: False test_id 0
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15956 # image files with weight 15917
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4127 # image files with weight 4113
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15917
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/249], loss=767.6510
	step [2/249], loss=529.5880
	step [3/249], loss=254.9048
	step [4/249], loss=192.9329
	step [5/249], loss=167.8714
	step [6/249], loss=148.4320
	step [7/249], loss=149.3413
	step [8/249], loss=140.8333
	step [9/249], loss=139.6256
	step [10/249], loss=137.4385
	step [11/249], loss=137.1715
	step [12/249], loss=136.2386
	step [13/249], loss=135.7954
	step [14/249], loss=133.3169
	step [15/249], loss=132.7267
	step [16/249], loss=133.6272
	step [17/249], loss=130.5316
	step [18/249], loss=130.0053
	step [19/249], loss=129.9531
	step [20/249], loss=131.2039
	step [21/249], loss=127.9886
	step [22/249], loss=126.0084
	step [23/249], loss=124.8404
	step [24/249], loss=123.9310
	step [25/249], loss=125.6142
	step [26/249], loss=122.5031
	step [27/249], loss=119.8615
	step [28/249], loss=120.8162
	step [29/249], loss=119.2878
	step [30/249], loss=119.7310
	step [31/249], loss=119.9093
	step [32/249], loss=115.5613
	step [33/249], loss=117.5919
	step [34/249], loss=116.1845
	step [35/249], loss=116.3165
	step [36/249], loss=115.4742
	step [37/249], loss=112.6619
	step [38/249], loss=114.1307
	step [39/249], loss=111.3360
	step [40/249], loss=112.0500
	step [41/249], loss=109.5026
	step [42/249], loss=115.3026
	step [43/249], loss=113.4866
	step [44/249], loss=104.8200
	step [45/249], loss=105.7398
	step [46/249], loss=107.3416
	step [47/249], loss=111.3553
	step [48/249], loss=107.7047
	step [49/249], loss=107.0469
	step [50/249], loss=105.0712
	step [51/249], loss=106.0995
	step [52/249], loss=103.1201
	step [53/249], loss=105.1655
	step [54/249], loss=105.2469
	step [55/249], loss=107.6520
	step [56/249], loss=103.4020
	step [57/249], loss=100.0096
	step [58/249], loss=100.7840
	step [59/249], loss=100.0102
	step [60/249], loss=100.9924
	step [61/249], loss=102.7272
	step [62/249], loss=101.1997
	step [63/249], loss=99.2404
	step [64/249], loss=100.4246
	step [65/249], loss=97.8993
	step [66/249], loss=101.1397
	step [67/249], loss=97.7563
	step [68/249], loss=97.3915
	step [69/249], loss=96.1750
	step [70/249], loss=96.5548
	step [71/249], loss=94.0672
	step [72/249], loss=98.8227
	step [73/249], loss=95.9989
	step [74/249], loss=94.6391
	step [75/249], loss=94.8840
	step [76/249], loss=95.4848
	step [77/249], loss=95.6471
	step [78/249], loss=94.6414
	step [79/249], loss=93.5506
	step [80/249], loss=94.2075
	step [81/249], loss=92.2992
	step [82/249], loss=96.4277
	step [83/249], loss=93.7050
	step [84/249], loss=90.3059
	step [85/249], loss=88.5292
	step [86/249], loss=92.2850
	step [87/249], loss=90.9159
	step [88/249], loss=89.9967
	step [89/249], loss=88.8943
	step [90/249], loss=88.8541
	step [91/249], loss=89.7433
	step [92/249], loss=92.0834
	step [93/249], loss=89.1360
	step [94/249], loss=88.9975
	step [95/249], loss=84.9302
	step [96/249], loss=90.8594
	step [97/249], loss=93.5707
	step [98/249], loss=85.3757
	step [99/249], loss=87.8356
	step [100/249], loss=87.8607
	step [101/249], loss=86.4409
	step [102/249], loss=83.9724
	step [103/249], loss=84.2912
	step [104/249], loss=86.0008
	step [105/249], loss=86.0925
	step [106/249], loss=83.7736
	step [107/249], loss=86.2901
	step [108/249], loss=84.4128
	step [109/249], loss=81.7964
	step [110/249], loss=83.6089
	step [111/249], loss=84.1594
	step [112/249], loss=84.0527
	step [113/249], loss=83.7893
	step [114/249], loss=82.0182
	step [115/249], loss=82.1434
	step [116/249], loss=83.3664
	step [117/249], loss=80.4617
	step [118/249], loss=81.7193
	step [119/249], loss=82.4772
	step [120/249], loss=83.3111
	step [121/249], loss=83.0923
	step [122/249], loss=84.3395
	step [123/249], loss=82.0765
	step [124/249], loss=79.3985
	step [125/249], loss=82.3381
	step [126/249], loss=81.8385
	step [127/249], loss=79.0959
	step [128/249], loss=83.4863
	step [129/249], loss=79.0320
	step [130/249], loss=78.8184
	step [131/249], loss=81.3593
	step [132/249], loss=81.8268
	step [133/249], loss=76.9826
	step [134/249], loss=77.7386
	step [135/249], loss=77.3121
	step [136/249], loss=79.8302
	step [137/249], loss=79.1507
	step [138/249], loss=80.3941
	step [139/249], loss=79.5510
	step [140/249], loss=76.4491
	step [141/249], loss=75.9185
	step [142/249], loss=79.0098
	step [143/249], loss=75.8101
	step [144/249], loss=77.4725
	step [145/249], loss=76.2226
	step [146/249], loss=76.6275
	step [147/249], loss=75.8273
	step [148/249], loss=74.8434
	step [149/249], loss=74.7590
	step [150/249], loss=75.3472
	step [151/249], loss=76.7468
	step [152/249], loss=74.6936
	step [153/249], loss=72.7557
	step [154/249], loss=75.1027
	step [155/249], loss=74.2698
	step [156/249], loss=75.0814
	step [157/249], loss=76.8034
	step [158/249], loss=77.2613
	step [159/249], loss=73.2879
	step [160/249], loss=70.8033
	step [161/249], loss=71.0635
	step [162/249], loss=75.0058
	step [163/249], loss=75.3086
	step [164/249], loss=72.0436
	step [165/249], loss=73.9475
	step [166/249], loss=71.7727
	step [167/249], loss=69.4740
	step [168/249], loss=70.1126
	step [169/249], loss=73.1876
	step [170/249], loss=74.8497
	step [171/249], loss=71.3959
	step [172/249], loss=70.9359
	step [173/249], loss=71.2208
	step [174/249], loss=72.1846
	step [175/249], loss=68.4467
	step [176/249], loss=74.0514
	step [177/249], loss=71.4984
	step [178/249], loss=68.7022
	step [179/249], loss=69.4386
	step [180/249], loss=70.3071
	step [181/249], loss=69.0900
	step [182/249], loss=72.7697
	step [183/249], loss=68.2115
	step [184/249], loss=71.5917
	step [185/249], loss=71.1937
	step [186/249], loss=71.3833
	step [187/249], loss=72.9406
	step [188/249], loss=67.8423
	step [189/249], loss=71.8388
	step [190/249], loss=69.2945
	step [191/249], loss=71.0424
	step [192/249], loss=69.8225
	step [193/249], loss=70.5226
	step [194/249], loss=65.4257
	step [195/249], loss=69.9660
	step [196/249], loss=72.7166
	step [197/249], loss=72.2051
	step [198/249], loss=65.8232
	step [199/249], loss=65.3392
	step [200/249], loss=66.2111
	step [201/249], loss=68.1082
	step [202/249], loss=67.0318
	step [203/249], loss=67.5110
	step [204/249], loss=68.1174
	step [205/249], loss=67.4947
	step [206/249], loss=66.6245
	step [207/249], loss=64.9962
	step [208/249], loss=69.6143
	step [209/249], loss=65.5852
	step [210/249], loss=64.2041
	step [211/249], loss=63.9611
	step [212/249], loss=65.1171
	step [213/249], loss=64.8449
	step [214/249], loss=66.1535
	step [215/249], loss=67.0859
	step [216/249], loss=66.6067
	step [217/249], loss=65.6945
	step [218/249], loss=65.3377
	step [219/249], loss=67.0405
	step [220/249], loss=67.9377
	step [221/249], loss=65.4239
	step [222/249], loss=66.9882
	step [223/249], loss=66.0091
	step [224/249], loss=64.1702
	step [225/249], loss=63.9076
	step [226/249], loss=63.5712
	step [227/249], loss=64.8342
	step [228/249], loss=66.3049
	step [229/249], loss=63.5602
	step [230/249], loss=61.5187
	step [231/249], loss=65.8120
	step [232/249], loss=66.2095
	step [233/249], loss=63.6492
	step [234/249], loss=67.0501
	step [235/249], loss=64.0036
	step [236/249], loss=63.5947
	step [237/249], loss=62.5459
	step [238/249], loss=62.8309
	step [239/249], loss=65.4679
	step [240/249], loss=65.2064
	step [241/249], loss=64.0531
	step [242/249], loss=64.3885
	step [243/249], loss=64.2570
	step [244/249], loss=63.8494
	step [245/249], loss=64.6182
	step [246/249], loss=63.1569
	step [247/249], loss=62.4812
	step [248/249], loss=60.9009
	step [249/249], loss=44.6369
	Evaluating
	loss=0.2239, precision=0.1408, recall=0.9934, f1=0.2466
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/249], loss=61.7922
	step [2/249], loss=62.1358
	step [3/249], loss=64.4292
	step [4/249], loss=60.0454
	step [5/249], loss=61.0093
	step [6/249], loss=62.7957
	step [7/249], loss=61.8975
	step [8/249], loss=61.4603
	step [9/249], loss=61.4380
	step [10/249], loss=66.3122
	step [11/249], loss=64.1269
	step [12/249], loss=60.2563
	step [13/249], loss=63.5322
	step [14/249], loss=61.8322
	step [15/249], loss=61.7434
	step [16/249], loss=63.3728
	step [17/249], loss=60.7044
	step [18/249], loss=61.6375
	step [19/249], loss=63.5997
	step [20/249], loss=60.0804
	step [21/249], loss=60.3831
	step [22/249], loss=61.3881
	step [23/249], loss=63.0887
	step [24/249], loss=61.7370
	step [25/249], loss=58.8579
	step [26/249], loss=61.1399
	step [27/249], loss=61.2623
	step [28/249], loss=59.8367
	step [29/249], loss=60.1521
	step [30/249], loss=60.9610
	step [31/249], loss=59.3202
	step [32/249], loss=61.2152
	step [33/249], loss=57.9405
	step [34/249], loss=60.8784
	step [35/249], loss=57.9505
	step [36/249], loss=64.2637
	step [37/249], loss=59.7858
	step [38/249], loss=60.0051
	step [39/249], loss=57.8606
	step [40/249], loss=60.8248
	step [41/249], loss=57.0754
	step [42/249], loss=57.9784
	step [43/249], loss=61.2400
	step [44/249], loss=58.8965
	step [45/249], loss=60.8911
	step [46/249], loss=60.5094
	step [47/249], loss=57.2457
	step [48/249], loss=60.9867
	step [49/249], loss=58.5771
	step [50/249], loss=58.8414
	step [51/249], loss=60.4279
	step [52/249], loss=56.4255
	step [53/249], loss=58.4855
	step [54/249], loss=55.5884
	step [55/249], loss=56.3317
	step [56/249], loss=57.2175
	step [57/249], loss=58.0397
	step [58/249], loss=56.4815
	step [59/249], loss=56.3950
	step [60/249], loss=57.5735
	step [61/249], loss=57.7056
	step [62/249], loss=55.6307
	step [63/249], loss=57.2036
	step [64/249], loss=58.8579
	step [65/249], loss=56.2681
	step [66/249], loss=56.6051
	step [67/249], loss=55.4191
	step [68/249], loss=57.0644
	step [69/249], loss=56.0604
	step [70/249], loss=56.0315
	step [71/249], loss=53.1652
	step [72/249], loss=57.9898
	step [73/249], loss=58.8443
	step [74/249], loss=52.9265
	step [75/249], loss=57.6896
	step [76/249], loss=55.1381
	step [77/249], loss=58.4545
	step [78/249], loss=55.9705
	step [79/249], loss=55.9930
	step [80/249], loss=55.4896
	step [81/249], loss=54.9214
	step [82/249], loss=54.8873
	step [83/249], loss=56.7976
	step [84/249], loss=55.9414
	step [85/249], loss=55.9731
	step [86/249], loss=53.9087
	step [87/249], loss=53.1102
	step [88/249], loss=55.2957
	step [89/249], loss=56.9662
	step [90/249], loss=54.6745
	step [91/249], loss=53.9174
	step [92/249], loss=56.1583
	step [93/249], loss=55.7341
	step [94/249], loss=56.3212
	step [95/249], loss=53.6626
	step [96/249], loss=56.1019
	step [97/249], loss=53.5341
	step [98/249], loss=51.9601
	step [99/249], loss=52.3018
	step [100/249], loss=57.1179
	step [101/249], loss=54.3497
	step [102/249], loss=57.0200
	step [103/249], loss=52.5934
	step [104/249], loss=55.7563
	step [105/249], loss=54.5421
	step [106/249], loss=53.5358
	step [107/249], loss=55.2629
	step [108/249], loss=51.8226
	step [109/249], loss=53.8647
	step [110/249], loss=54.7970
	step [111/249], loss=54.0598
	step [112/249], loss=52.7153
	step [113/249], loss=53.0268
	step [114/249], loss=52.9506
	step [115/249], loss=52.5195
	step [116/249], loss=51.9713
	step [117/249], loss=54.3326
	step [118/249], loss=52.6258
	step [119/249], loss=54.1915
	step [120/249], loss=52.7646
	step [121/249], loss=52.0694
	step [122/249], loss=53.8381
	step [123/249], loss=52.4529
	step [124/249], loss=52.6500
	step [125/249], loss=51.9799
	step [126/249], loss=54.3381
	step [127/249], loss=51.1547
	step [128/249], loss=53.1842
	step [129/249], loss=51.7259
	step [130/249], loss=53.4468
	step [131/249], loss=53.7872
	step [132/249], loss=52.7204
	step [133/249], loss=50.1395
	step [134/249], loss=50.6966
	step [135/249], loss=49.0914
	step [136/249], loss=51.3511
	step [137/249], loss=49.8299
	step [138/249], loss=52.0979
	step [139/249], loss=49.6874
	step [140/249], loss=51.7474
	step [141/249], loss=51.3251
	step [142/249], loss=52.4183
	step [143/249], loss=49.8701
	step [144/249], loss=50.7951
	step [145/249], loss=50.2763
	step [146/249], loss=48.9088
	step [147/249], loss=51.1831
	step [148/249], loss=47.1033
	step [149/249], loss=49.9877
	step [150/249], loss=50.7107
	step [151/249], loss=48.5042
	step [152/249], loss=49.2183
	step [153/249], loss=48.9303
	step [154/249], loss=48.6638
	step [155/249], loss=51.5784
	step [156/249], loss=49.4691
	step [157/249], loss=50.8108
	step [158/249], loss=48.4550
	step [159/249], loss=47.8905
	step [160/249], loss=50.0959
	step [161/249], loss=48.8793
	step [162/249], loss=49.8046
	step [163/249], loss=49.9267
	step [164/249], loss=49.2451
	step [165/249], loss=50.7232
	step [166/249], loss=47.2929
	step [167/249], loss=47.4805
	step [168/249], loss=48.6098
	step [169/249], loss=49.5256
	step [170/249], loss=50.5971
	step [171/249], loss=47.9001
	step [172/249], loss=47.6728
	step [173/249], loss=49.2895
	step [174/249], loss=49.4822
	step [175/249], loss=49.4371
	step [176/249], loss=47.8705
	step [177/249], loss=46.7536
	step [178/249], loss=45.4438
	step [179/249], loss=49.1138
	step [180/249], loss=48.0694
	step [181/249], loss=47.0028
	step [182/249], loss=48.0845
	step [183/249], loss=48.4019
	step [184/249], loss=47.0725
	step [185/249], loss=49.8343
	step [186/249], loss=50.5370
	step [187/249], loss=48.2621
	step [188/249], loss=49.0465
	step [189/249], loss=46.3860
	step [190/249], loss=49.1689
	step [191/249], loss=46.2434
	step [192/249], loss=49.1155
	step [193/249], loss=50.4488
	step [194/249], loss=47.7811
	step [195/249], loss=52.9729
	step [196/249], loss=48.2281
	step [197/249], loss=46.8272
	step [198/249], loss=48.0987
	step [199/249], loss=44.5863
	step [200/249], loss=46.4061
	step [201/249], loss=50.7907
	step [202/249], loss=45.3018
	step [203/249], loss=46.1366
	step [204/249], loss=48.3165
	step [205/249], loss=44.9537
	step [206/249], loss=47.4184
	step [207/249], loss=46.2870
	step [208/249], loss=47.3354
	step [209/249], loss=44.9561
	step [210/249], loss=47.1428
	step [211/249], loss=49.5709
	step [212/249], loss=49.1358
	step [213/249], loss=48.3836
	step [214/249], loss=47.7960
	step [215/249], loss=47.3598
	step [216/249], loss=50.3498
	step [217/249], loss=46.9568
	step [218/249], loss=45.3539
	step [219/249], loss=47.1286
	step [220/249], loss=45.6610
	step [221/249], loss=45.0527
	step [222/249], loss=47.3205
	step [223/249], loss=44.5480
	step [224/249], loss=46.2096
	step [225/249], loss=43.1294
	step [226/249], loss=47.8702
	step [227/249], loss=47.5692
	step [228/249], loss=45.5199
	step [229/249], loss=48.2657
	step [230/249], loss=45.9540
	step [231/249], loss=45.1112
	step [232/249], loss=44.6500
	step [233/249], loss=46.0427
	step [234/249], loss=42.9377
	step [235/249], loss=46.5167
	step [236/249], loss=44.8221
	step [237/249], loss=43.6074
	step [238/249], loss=45.4686
	step [239/249], loss=44.1888
	step [240/249], loss=46.6548
	step [241/249], loss=44.7857
	step [242/249], loss=42.8546
	step [243/249], loss=44.2379
	step [244/249], loss=44.6367
	step [245/249], loss=44.8123
	step [246/249], loss=44.7184
	step [247/249], loss=42.5177
	step [248/249], loss=44.7560
	step [249/249], loss=30.5666
	Evaluating
	loss=0.1572, precision=0.1335, recall=0.9935, f1=0.2354
Training epoch 3
	step [1/249], loss=44.0037
	step [2/249], loss=44.8602
	step [3/249], loss=42.5940
	step [4/249], loss=44.6522
	step [5/249], loss=44.6303
	step [6/249], loss=43.6116
	step [7/249], loss=41.7450
	step [8/249], loss=43.9449
	step [9/249], loss=45.1596
	step [10/249], loss=42.0250
	step [11/249], loss=43.0338
	step [12/249], loss=45.8977
	step [13/249], loss=43.6802
	step [14/249], loss=41.9715
	step [15/249], loss=42.6057
	step [16/249], loss=44.1954
	step [17/249], loss=43.4919
	step [18/249], loss=46.0784
	step [19/249], loss=45.7349
	step [20/249], loss=43.2528
	step [21/249], loss=44.2761
	step [22/249], loss=43.4768
	step [23/249], loss=44.1142
	step [24/249], loss=45.9709
	step [25/249], loss=43.8596
	step [26/249], loss=42.2049
	step [27/249], loss=47.6489
	step [28/249], loss=44.3219
	step [29/249], loss=43.9678
	step [30/249], loss=44.1773
	step [31/249], loss=45.1959
	step [32/249], loss=41.9260
	step [33/249], loss=45.3003
	step [34/249], loss=42.7882
	step [35/249], loss=42.9007
	step [36/249], loss=44.3948
	step [37/249], loss=40.1987
	step [38/249], loss=42.5053
	step [39/249], loss=42.1674
	step [40/249], loss=44.7982
	step [41/249], loss=40.8272
	step [42/249], loss=44.8160
	step [43/249], loss=43.1959
	step [44/249], loss=40.1443
	step [45/249], loss=41.8452
	step [46/249], loss=43.5091
	step [47/249], loss=42.4478
	step [48/249], loss=40.5241
	step [49/249], loss=42.9455
	step [50/249], loss=41.0184
	step [51/249], loss=43.8985
	step [52/249], loss=43.6443
	step [53/249], loss=41.3193
	step [54/249], loss=43.1910
	step [55/249], loss=43.7420
	step [56/249], loss=41.4683
	step [57/249], loss=42.3586
	step [58/249], loss=43.5419
	step [59/249], loss=42.9223
	step [60/249], loss=43.0373
	step [61/249], loss=42.9170
	step [62/249], loss=40.9633
	step [63/249], loss=39.6356
	step [64/249], loss=40.5849
	step [65/249], loss=41.2982
	step [66/249], loss=39.5962
	step [67/249], loss=42.1400
	step [68/249], loss=41.2461
	step [69/249], loss=38.6246
	step [70/249], loss=39.3903
	step [71/249], loss=41.5049
	step [72/249], loss=39.2926
	step [73/249], loss=40.4725
	step [74/249], loss=42.8273
	step [75/249], loss=42.8263
	step [76/249], loss=39.8813
	step [77/249], loss=42.1414
	step [78/249], loss=39.1162
	step [79/249], loss=42.2290
	step [80/249], loss=39.5787
	step [81/249], loss=41.0952
	step [82/249], loss=38.5979
	step [83/249], loss=41.0008
	step [84/249], loss=40.2493
	step [85/249], loss=40.5002
	step [86/249], loss=37.2942
	step [87/249], loss=43.1245
	step [88/249], loss=40.9981
	step [89/249], loss=40.4894
	step [90/249], loss=39.2111
	step [91/249], loss=39.9045
	step [92/249], loss=38.9523
	step [93/249], loss=38.7151
	step [94/249], loss=41.1159
	step [95/249], loss=41.2292
	step [96/249], loss=38.7873
	step [97/249], loss=37.6774
	step [98/249], loss=40.6514
	step [99/249], loss=39.8335
	step [100/249], loss=39.1545
	step [101/249], loss=41.5834
	step [102/249], loss=39.4419
	step [103/249], loss=39.7380
	step [104/249], loss=40.5525
	step [105/249], loss=40.4073
	step [106/249], loss=42.2680
	step [107/249], loss=40.0824
	step [108/249], loss=40.2460
	step [109/249], loss=37.1973
	step [110/249], loss=39.3735
	step [111/249], loss=42.3698
	step [112/249], loss=37.8989
	step [113/249], loss=39.8780
	step [114/249], loss=39.4127
	step [115/249], loss=38.5370
	step [116/249], loss=38.5615
	step [117/249], loss=37.8559
	step [118/249], loss=38.9134
	step [119/249], loss=39.3743
	step [120/249], loss=38.6782
	step [121/249], loss=38.8859
	step [122/249], loss=38.9109
	step [123/249], loss=36.2252
	step [124/249], loss=37.6809
	step [125/249], loss=40.0941
	step [126/249], loss=38.5252
	step [127/249], loss=38.3048
	step [128/249], loss=38.1294
	step [129/249], loss=37.9220
	step [130/249], loss=41.0579
	step [131/249], loss=39.2489
	step [132/249], loss=37.2740
	step [133/249], loss=37.2697
	step [134/249], loss=39.0882
	step [135/249], loss=36.8262
	step [136/249], loss=39.1270
	step [137/249], loss=37.5593
	step [138/249], loss=36.5263
	step [139/249], loss=39.1863
	step [140/249], loss=35.8366
	step [141/249], loss=36.7321
	step [142/249], loss=38.4895
	step [143/249], loss=39.3941
	step [144/249], loss=38.8156
	step [145/249], loss=36.3808
	step [146/249], loss=36.9557
	step [147/249], loss=34.3868
	step [148/249], loss=36.7682
	step [149/249], loss=35.6971
	step [150/249], loss=36.4954
	step [151/249], loss=37.8862
	step [152/249], loss=38.1924
	step [153/249], loss=36.4343
	step [154/249], loss=37.1646
	step [155/249], loss=38.1281
	step [156/249], loss=36.9575
	step [157/249], loss=38.7578
	step [158/249], loss=36.8728
	step [159/249], loss=36.7737
	step [160/249], loss=37.2151
	step [161/249], loss=37.1606
	step [162/249], loss=35.8153
	step [163/249], loss=37.5668
	step [164/249], loss=38.9844
	step [165/249], loss=36.0795
	step [166/249], loss=39.0856
	step [167/249], loss=39.0840
	step [168/249], loss=37.8415
	step [169/249], loss=36.3700
	step [170/249], loss=37.2531
	step [171/249], loss=35.9767
	step [172/249], loss=35.5459
	step [173/249], loss=36.7795
	step [174/249], loss=36.1416
	step [175/249], loss=39.4412
	step [176/249], loss=36.7449
	step [177/249], loss=36.1132
	step [178/249], loss=39.5584
	step [179/249], loss=37.2395
	step [180/249], loss=37.6896
	step [181/249], loss=34.5534
	step [182/249], loss=36.6451
	step [183/249], loss=36.7474
	step [184/249], loss=38.6021
	step [185/249], loss=36.4706
	step [186/249], loss=34.8509
	step [187/249], loss=36.2214
	step [188/249], loss=36.6017
	step [189/249], loss=42.3899
	step [190/249], loss=38.5282
	step [191/249], loss=36.9339
	step [192/249], loss=34.8285
	step [193/249], loss=34.5045
	step [194/249], loss=36.2903
	step [195/249], loss=36.5251
	step [196/249], loss=36.4136
	step [197/249], loss=37.9832
	step [198/249], loss=37.0032
	step [199/249], loss=35.6811
	step [200/249], loss=36.9831
	step [201/249], loss=36.2152
	step [202/249], loss=36.9517
	step [203/249], loss=34.7240
	step [204/249], loss=33.4904
	step [205/249], loss=35.6914
	step [206/249], loss=37.7156
	step [207/249], loss=35.0847
	step [208/249], loss=35.0525
	step [209/249], loss=34.1423
	step [210/249], loss=41.8235
	step [211/249], loss=36.1140
	step [212/249], loss=37.7946
	step [213/249], loss=34.4659
	step [214/249], loss=34.7534
	step [215/249], loss=34.8605
	step [216/249], loss=36.9230
	step [217/249], loss=34.9444
	step [218/249], loss=33.7524
	step [219/249], loss=35.4037
	step [220/249], loss=34.7809
	step [221/249], loss=34.2779
	step [222/249], loss=33.4985
	step [223/249], loss=35.4890
	step [224/249], loss=36.2813
	step [225/249], loss=34.4530
	step [226/249], loss=34.2874
	step [227/249], loss=34.0307
	step [228/249], loss=37.4284
	step [229/249], loss=35.0366
	step [230/249], loss=35.8286
	step [231/249], loss=36.0866
	step [232/249], loss=36.1817
	step [233/249], loss=33.9471
	step [234/249], loss=34.9031
	step [235/249], loss=34.9668
	step [236/249], loss=33.4871
	step [237/249], loss=35.8005
	step [238/249], loss=38.5239
	step [239/249], loss=36.1646
	step [240/249], loss=34.7848
	step [241/249], loss=32.9484
	step [242/249], loss=35.4746
	step [243/249], loss=34.3593
	step [244/249], loss=35.3385
	step [245/249], loss=34.1421
	step [246/249], loss=35.7148
	step [247/249], loss=34.1944
	step [248/249], loss=33.3500
	step [249/249], loss=23.5868
	Evaluating
	loss=0.1138, precision=0.1510, recall=0.9935, f1=0.2622
saving model as: 0_saved_model.pth
Training epoch 4
	step [1/249], loss=32.5615
	step [2/249], loss=37.1029
	step [3/249], loss=32.8372
	step [4/249], loss=32.6803
	step [5/249], loss=32.1855
	step [6/249], loss=31.8329
	step [7/249], loss=31.5042
	step [8/249], loss=36.0691
	step [9/249], loss=33.7552
	step [10/249], loss=35.2082
	step [11/249], loss=33.8702
	step [12/249], loss=32.4644
	step [13/249], loss=33.9297
	step [14/249], loss=32.5042
	step [15/249], loss=33.8590
	step [16/249], loss=32.8336
	step [17/249], loss=34.4794
	step [18/249], loss=34.8879
	step [19/249], loss=33.8052
	step [20/249], loss=33.2946
	step [21/249], loss=32.7382
	step [22/249], loss=32.8487
	step [23/249], loss=31.2458
	step [24/249], loss=35.4214
	step [25/249], loss=37.8173
	step [26/249], loss=34.2087
	step [27/249], loss=34.3032
	step [28/249], loss=33.1943
	step [29/249], loss=33.8942
	step [30/249], loss=33.2786
	step [31/249], loss=34.8893
	step [32/249], loss=33.5815
	step [33/249], loss=31.5969
	step [34/249], loss=34.1360
	step [35/249], loss=32.4213
	step [36/249], loss=34.7348
	step [37/249], loss=33.1203
	step [38/249], loss=32.7548
	step [39/249], loss=31.4214
	step [40/249], loss=29.6608
	step [41/249], loss=32.8724
	step [42/249], loss=33.7243
	step [43/249], loss=31.9830
	step [44/249], loss=33.6105
	step [45/249], loss=32.2637
	step [46/249], loss=33.6534
	step [47/249], loss=30.3107
	step [48/249], loss=31.0359
	step [49/249], loss=33.0526
	step [50/249], loss=31.8448
	step [51/249], loss=36.0708
	step [52/249], loss=34.4606
	step [53/249], loss=32.5058
	step [54/249], loss=33.9205
	step [55/249], loss=30.9488
	step [56/249], loss=30.5150
	step [57/249], loss=33.3508
	step [58/249], loss=30.7335
	step [59/249], loss=33.6152
	step [60/249], loss=32.3303
	step [61/249], loss=31.9907
	step [62/249], loss=30.2884
	step [63/249], loss=34.6695
	step [64/249], loss=33.4922
	step [65/249], loss=31.7081
	step [66/249], loss=32.1520
	step [67/249], loss=32.5812
	step [68/249], loss=33.2699
	step [69/249], loss=29.7634
	step [70/249], loss=33.4695
	step [71/249], loss=34.5221
	step [72/249], loss=33.0116
	step [73/249], loss=31.5315
	step [74/249], loss=32.9830
	step [75/249], loss=32.6539
	step [76/249], loss=31.4136
	step [77/249], loss=31.0451
	step [78/249], loss=31.7238
	step [79/249], loss=30.0006
	step [80/249], loss=33.0276
	step [81/249], loss=30.7140
	step [82/249], loss=32.7042
	step [83/249], loss=31.4968
	step [84/249], loss=30.4681
	step [85/249], loss=31.7329
	step [86/249], loss=29.9379
	step [87/249], loss=30.4249
	step [88/249], loss=33.3678
	step [89/249], loss=29.5334
	step [90/249], loss=32.3504
	step [91/249], loss=30.8043
	step [92/249], loss=28.8713
	step [93/249], loss=32.0956
	step [94/249], loss=32.7605
	step [95/249], loss=30.5031
	step [96/249], loss=30.6692
	step [97/249], loss=32.9572
	step [98/249], loss=32.9786
	step [99/249], loss=31.5141
	step [100/249], loss=30.1587
	step [101/249], loss=30.4925
	step [102/249], loss=31.4313
	step [103/249], loss=31.3081
	step [104/249], loss=29.2310
	step [105/249], loss=31.7680
	step [106/249], loss=33.4601
	step [107/249], loss=32.1579
	step [108/249], loss=30.9344
	step [109/249], loss=31.0546
	step [110/249], loss=34.6735
	step [111/249], loss=29.8258
	step [112/249], loss=30.6105
	step [113/249], loss=31.1353
	step [114/249], loss=28.6892
	step [115/249], loss=31.0690
	step [116/249], loss=30.0754
	step [117/249], loss=30.2362
	step [118/249], loss=27.9148
	step [119/249], loss=34.0446
	step [120/249], loss=32.2820
	step [121/249], loss=33.4960
	step [122/249], loss=30.2794
	step [123/249], loss=32.0592
	step [124/249], loss=31.9808
	step [125/249], loss=34.3674
	step [126/249], loss=30.0507
	step [127/249], loss=28.9668
	step [128/249], loss=31.8249
	step [129/249], loss=33.1618
	step [130/249], loss=32.2720
	step [131/249], loss=29.8265
	step [132/249], loss=31.1170
	step [133/249], loss=29.1097
	step [134/249], loss=28.7978
	step [135/249], loss=31.3321
	step [136/249], loss=29.1070
	step [137/249], loss=33.1646
	step [138/249], loss=29.7163
	step [139/249], loss=29.3657
	step [140/249], loss=28.6485
	step [141/249], loss=30.0385
	step [142/249], loss=29.1481
	step [143/249], loss=31.7839
	step [144/249], loss=29.6213
	step [145/249], loss=29.1290
	step [146/249], loss=29.7008
	step [147/249], loss=28.5983
	step [148/249], loss=28.7333
	step [149/249], loss=29.6955
	step [150/249], loss=31.2053
	step [151/249], loss=30.0832
	step [152/249], loss=28.9201
	step [153/249], loss=29.2364
	step [154/249], loss=28.9044
	step [155/249], loss=28.5525
	step [156/249], loss=27.3368
	step [157/249], loss=32.4584
	step [158/249], loss=29.0565
	step [159/249], loss=30.7632
	step [160/249], loss=29.6555
	step [161/249], loss=27.0378
	step [162/249], loss=29.1119
	step [163/249], loss=28.3581
	step [164/249], loss=28.5671
	step [165/249], loss=30.7249
	step [166/249], loss=32.0844
	step [167/249], loss=29.9151
	step [168/249], loss=27.6702
	step [169/249], loss=30.2462
	step [170/249], loss=29.7271
	step [171/249], loss=30.1642
	step [172/249], loss=30.8324
	step [173/249], loss=27.8179
	step [174/249], loss=27.8681
	step [175/249], loss=28.0497
	step [176/249], loss=29.1471
	step [177/249], loss=27.9351
	step [178/249], loss=27.5840
	step [179/249], loss=28.3083
	step [180/249], loss=29.8824
	step [181/249], loss=30.1955
	step [182/249], loss=29.4404
	step [183/249], loss=31.6369
	step [184/249], loss=27.7532
	step [185/249], loss=31.7467
	step [186/249], loss=30.0987
	step [187/249], loss=31.5809
	step [188/249], loss=28.1598
	step [189/249], loss=29.7287
	step [190/249], loss=27.2394
	step [191/249], loss=32.0229
	step [192/249], loss=30.0499
	step [193/249], loss=29.2458
	step [194/249], loss=28.0132
	step [195/249], loss=27.9185
	step [196/249], loss=28.3212
	step [197/249], loss=30.2368
	step [198/249], loss=27.9789
	step [199/249], loss=30.2787
	step [200/249], loss=28.4554
	step [201/249], loss=28.4447
	step [202/249], loss=29.4713
	step [203/249], loss=26.2941
	step [204/249], loss=27.7013
	step [205/249], loss=30.4078
	step [206/249], loss=29.4561
	step [207/249], loss=26.5348
	step [208/249], loss=27.8864
	step [209/249], loss=29.9094
	step [210/249], loss=28.7508
	step [211/249], loss=28.4270
	step [212/249], loss=29.0466
	step [213/249], loss=27.4897
	step [214/249], loss=27.4082
	step [215/249], loss=29.6591
	step [216/249], loss=27.3663
	step [217/249], loss=28.7533
	step [218/249], loss=28.4608
	step [219/249], loss=28.1416
	step [220/249], loss=26.8003
	step [221/249], loss=29.8491
	step [222/249], loss=27.3988
	step [223/249], loss=25.4195
	step [224/249], loss=32.6194
	step [225/249], loss=27.8906
	step [226/249], loss=27.8489
	step [227/249], loss=28.9077
	step [228/249], loss=29.1426
	step [229/249], loss=26.9206
	step [230/249], loss=30.2566
	step [231/249], loss=26.5788
	step [232/249], loss=28.7935
	step [233/249], loss=27.7906
	step [234/249], loss=28.7768
	step [235/249], loss=26.7209
	step [236/249], loss=30.0885
	step [237/249], loss=29.4770
	step [238/249], loss=28.7597
	step [239/249], loss=26.2286
	step [240/249], loss=27.1042
	step [241/249], loss=28.8591
	step [242/249], loss=27.8652
	step [243/249], loss=29.8737
	step [244/249], loss=28.4579
	step [245/249], loss=28.7016
	step [246/249], loss=29.6360
	step [247/249], loss=27.7771
	step [248/249], loss=28.3608
	step [249/249], loss=20.4120
	Evaluating
	loss=0.0906, precision=0.1596, recall=0.9935, f1=0.2751
saving model as: 0_saved_model.pth
Training epoch 5
	step [1/249], loss=27.8593
	step [2/249], loss=30.0367
	step [3/249], loss=30.3387
	step [4/249], loss=24.5292
	step [5/249], loss=27.7607
	step [6/249], loss=26.7430
	step [7/249], loss=29.1935
	step [8/249], loss=27.3876
	step [9/249], loss=28.3911
	step [10/249], loss=27.7473
	step [11/249], loss=27.3356
	step [12/249], loss=26.8773
	step [13/249], loss=26.1780
	step [14/249], loss=25.3867
	step [15/249], loss=27.1714
	step [16/249], loss=26.0127
	step [17/249], loss=25.0620
	step [18/249], loss=27.5351
	step [19/249], loss=30.3523
	step [20/249], loss=24.7827
	step [21/249], loss=26.7526
	step [22/249], loss=30.4185
	step [23/249], loss=27.7512
	step [24/249], loss=27.5575
	step [25/249], loss=29.5460
	step [26/249], loss=25.9922
	step [27/249], loss=25.2616
	step [28/249], loss=25.4080
	step [29/249], loss=26.8400
	step [30/249], loss=28.2071
	step [31/249], loss=26.7968
	step [32/249], loss=30.6096
	step [33/249], loss=28.7573
	step [34/249], loss=26.3924
	step [35/249], loss=26.2599
	step [36/249], loss=25.7568
	step [37/249], loss=24.3145
	step [38/249], loss=28.2519
	step [39/249], loss=24.7057
	step [40/249], loss=28.3759
	step [41/249], loss=26.7252
	step [42/249], loss=28.6654
	step [43/249], loss=24.1108
	step [44/249], loss=26.9783
	step [45/249], loss=24.6756
	step [46/249], loss=26.7627
	step [47/249], loss=28.9949
	step [48/249], loss=26.6577
	step [49/249], loss=27.0384
	step [50/249], loss=26.3654
	step [51/249], loss=27.2735
	step [52/249], loss=24.4905
	step [53/249], loss=26.0962
	step [54/249], loss=25.0591
	step [55/249], loss=26.4422
	step [56/249], loss=27.1790
	step [57/249], loss=26.7611
	step [58/249], loss=26.2162
	step [59/249], loss=25.6389
	step [60/249], loss=25.2529
	step [61/249], loss=30.7794
	step [62/249], loss=27.2490
	step [63/249], loss=28.0862
	step [64/249], loss=25.1352
	step [65/249], loss=28.9239
	step [66/249], loss=26.4900
	step [67/249], loss=26.0377
	step [68/249], loss=27.4123
	step [69/249], loss=24.3579
	step [70/249], loss=25.6133
	step [71/249], loss=28.7494
	step [72/249], loss=30.2095
	step [73/249], loss=25.7536
	step [74/249], loss=24.7466
	step [75/249], loss=28.2174
	step [76/249], loss=26.4836
	step [77/249], loss=25.5130
	step [78/249], loss=28.9744
	step [79/249], loss=25.4832
	step [80/249], loss=27.2022
	step [81/249], loss=24.5625
	step [82/249], loss=25.2861
	step [83/249], loss=25.3636
	step [84/249], loss=27.1565
	step [85/249], loss=25.1544
	step [86/249], loss=25.8760
	step [87/249], loss=22.3142
	step [88/249], loss=26.1929
	step [89/249], loss=25.9353
	step [90/249], loss=28.7847
	step [91/249], loss=25.1496
	step [92/249], loss=23.6349
	step [93/249], loss=24.5067
	step [94/249], loss=23.5818
	step [95/249], loss=27.1991
	step [96/249], loss=26.2818
	step [97/249], loss=25.2093
	step [98/249], loss=26.6245
	step [99/249], loss=23.5290
	step [100/249], loss=24.0282
	step [101/249], loss=26.8845
	step [102/249], loss=26.9052
	step [103/249], loss=25.7081
	step [104/249], loss=25.8115
	step [105/249], loss=24.7602
	step [106/249], loss=26.4197
	step [107/249], loss=23.5356
	step [108/249], loss=23.6805
	step [109/249], loss=24.9368
	step [110/249], loss=26.3047
	step [111/249], loss=25.6790
	step [112/249], loss=25.6236
	step [113/249], loss=26.5030
	step [114/249], loss=26.4323
	step [115/249], loss=24.2385
	step [116/249], loss=24.9135
	step [117/249], loss=24.6173
	step [118/249], loss=23.8338
	step [119/249], loss=22.6413
	step [120/249], loss=25.8888
	step [121/249], loss=24.7663
	step [122/249], loss=25.0835
	step [123/249], loss=24.3088
	step [124/249], loss=24.8919
	step [125/249], loss=22.6994
	step [126/249], loss=23.7109
	step [127/249], loss=25.9643
	step [128/249], loss=23.5368
	step [129/249], loss=23.6872
	step [130/249], loss=23.6980
	step [131/249], loss=22.1088
	step [132/249], loss=27.3984
	step [133/249], loss=24.7226
	step [134/249], loss=25.0529
	step [135/249], loss=24.5819
	step [136/249], loss=25.3190
	step [137/249], loss=23.9192
	step [138/249], loss=23.5112
	step [139/249], loss=24.8414
	step [140/249], loss=26.5910
	step [141/249], loss=23.7887
	step [142/249], loss=25.8778
	step [143/249], loss=25.9719
	step [144/249], loss=25.3663
	step [145/249], loss=24.9327
	step [146/249], loss=24.0838
	step [147/249], loss=26.5966
	step [148/249], loss=23.6292
	step [149/249], loss=24.9952
	step [150/249], loss=23.3827
	step [151/249], loss=23.5736
	step [152/249], loss=25.9505
	step [153/249], loss=26.0024
	step [154/249], loss=23.3426
	step [155/249], loss=23.8129
	step [156/249], loss=28.6539
	step [157/249], loss=23.1156
	step [158/249], loss=25.3617
	step [159/249], loss=27.1872
	step [160/249], loss=25.6428
	step [161/249], loss=26.3148
	step [162/249], loss=21.7010
	step [163/249], loss=23.2576
	step [164/249], loss=27.2687
	step [165/249], loss=27.2669
	step [166/249], loss=23.9552
	step [167/249], loss=24.4753
	step [168/249], loss=23.7771
	step [169/249], loss=23.7478
	step [170/249], loss=22.8414
	step [171/249], loss=24.6887
	step [172/249], loss=25.6865
	step [173/249], loss=26.2414
	step [174/249], loss=23.2006
	step [175/249], loss=24.1708
	step [176/249], loss=24.1627
	step [177/249], loss=24.7613
	step [178/249], loss=23.6265
	step [179/249], loss=23.1626
	step [180/249], loss=23.2666
	step [181/249], loss=23.2557
	step [182/249], loss=23.2182
	step [183/249], loss=26.0606
	step [184/249], loss=27.2012
	step [185/249], loss=23.3662
	step [186/249], loss=22.8511
	step [187/249], loss=24.6385
	step [188/249], loss=24.0237
	step [189/249], loss=24.3403
	step [190/249], loss=25.1169
	step [191/249], loss=22.6863
	step [192/249], loss=24.7180
	step [193/249], loss=26.4445
	step [194/249], loss=23.3328
	step [195/249], loss=23.6247
	step [196/249], loss=25.0320
	step [197/249], loss=23.9842
	step [198/249], loss=25.6008
	step [199/249], loss=23.1068
	step [200/249], loss=24.6281
	step [201/249], loss=25.0383
	step [202/249], loss=22.7025
	step [203/249], loss=23.0251
	step [204/249], loss=22.2202
	step [205/249], loss=22.8154
	step [206/249], loss=23.2955
	step [207/249], loss=24.1552
	step [208/249], loss=27.4901
	step [209/249], loss=21.8968
	step [210/249], loss=21.9460
	step [211/249], loss=23.1686
	step [212/249], loss=25.7955
	step [213/249], loss=26.9105
	step [214/249], loss=26.0566
	step [215/249], loss=23.0629
	step [216/249], loss=24.0714
	step [217/249], loss=23.3290
	step [218/249], loss=23.0840
	step [219/249], loss=26.9973
	step [220/249], loss=22.1033
	step [221/249], loss=25.1882
	step [222/249], loss=23.3244
	step [223/249], loss=23.0973
	step [224/249], loss=22.6196
	step [225/249], loss=23.4885
	step [226/249], loss=25.8155
	step [227/249], loss=25.5975
	step [228/249], loss=23.4912
	step [229/249], loss=24.6548
	step [230/249], loss=22.1729
	step [231/249], loss=21.9068
	step [232/249], loss=22.8730
	step [233/249], loss=22.3511
	step [234/249], loss=23.6480
	step [235/249], loss=24.0693
	step [236/249], loss=23.1437
	step [237/249], loss=22.9384
	step [238/249], loss=24.1111
	step [239/249], loss=22.6301
	step [240/249], loss=23.8931
	step [241/249], loss=23.8866
	step [242/249], loss=26.4004
	step [243/249], loss=23.0707
	step [244/249], loss=22.3329
	step [245/249], loss=23.7312
	step [246/249], loss=20.6025
	step [247/249], loss=26.1214
	step [248/249], loss=21.5917
	step [249/249], loss=18.2891
	Evaluating
	loss=0.0743, precision=0.1457, recall=0.9941, f1=0.2542
Training epoch 6
	step [1/249], loss=22.4660
	step [2/249], loss=23.8306
	step [3/249], loss=20.9753
	step [4/249], loss=25.6736
	step [5/249], loss=23.9738
	step [6/249], loss=21.5820
	step [7/249], loss=22.1504
	step [8/249], loss=21.9404
	step [9/249], loss=23.0869
	step [10/249], loss=27.2846
	step [11/249], loss=23.1035
	step [12/249], loss=22.1287
	step [13/249], loss=25.0255
	step [14/249], loss=21.7462
	step [15/249], loss=23.4325
	step [16/249], loss=24.8932
	step [17/249], loss=26.1435
	step [18/249], loss=21.9461
	step [19/249], loss=22.4142
	step [20/249], loss=23.6671
	step [21/249], loss=20.7388
	step [22/249], loss=24.4703
	step [23/249], loss=24.8556
	step [24/249], loss=22.2537
	step [25/249], loss=22.8938
	step [26/249], loss=21.6942
	step [27/249], loss=22.6749
	step [28/249], loss=21.1113
	step [29/249], loss=20.8403
	step [30/249], loss=22.5201
	step [31/249], loss=22.2027
	step [32/249], loss=23.4499
	step [33/249], loss=21.4179
	step [34/249], loss=21.6548
	step [35/249], loss=22.5228
	step [36/249], loss=20.4153
	step [37/249], loss=23.7072
	step [38/249], loss=20.6262
	step [39/249], loss=21.3548
	step [40/249], loss=22.2312
	step [41/249], loss=24.4051
	step [42/249], loss=22.0260
	step [43/249], loss=23.9271
	step [44/249], loss=20.9244
	step [45/249], loss=20.6409
	step [46/249], loss=21.8113
	step [47/249], loss=20.9790
	step [48/249], loss=23.0885
	step [49/249], loss=22.2222
	step [50/249], loss=24.3341
	step [51/249], loss=23.2170
	step [52/249], loss=22.0513
	step [53/249], loss=23.6247
	step [54/249], loss=22.4244
	step [55/249], loss=23.1361
	step [56/249], loss=22.9067
	step [57/249], loss=21.6271
	step [58/249], loss=23.7239
	step [59/249], loss=22.1384
	step [60/249], loss=21.6992
	step [61/249], loss=21.0434
	step [62/249], loss=21.2488
	step [63/249], loss=20.7650
	step [64/249], loss=20.6480
	step [65/249], loss=22.2430
	step [66/249], loss=21.3996
	step [67/249], loss=23.9899
	step [68/249], loss=20.9446
	step [69/249], loss=21.0204
	step [70/249], loss=22.8825
	step [71/249], loss=23.5856
	step [72/249], loss=22.5110
	step [73/249], loss=23.4386
	step [74/249], loss=22.6375
	step [75/249], loss=20.0643
	step [76/249], loss=20.5775
	step [77/249], loss=22.5198
	step [78/249], loss=23.3200
	step [79/249], loss=22.1621
	step [80/249], loss=26.0253
	step [81/249], loss=22.0797
	step [82/249], loss=21.7598
	step [83/249], loss=23.8524
	step [84/249], loss=21.8807
	step [85/249], loss=21.9098
	step [86/249], loss=23.1646
	step [87/249], loss=21.9579
	step [88/249], loss=19.8187
	step [89/249], loss=22.7162
	step [90/249], loss=21.1934
	step [91/249], loss=21.2065
	step [92/249], loss=22.2711
	step [93/249], loss=20.5786
	step [94/249], loss=22.7799
	step [95/249], loss=21.3698
	step [96/249], loss=23.1140
	step [97/249], loss=22.3984
	step [98/249], loss=21.6179
	step [99/249], loss=20.0927
	step [100/249], loss=22.2621
	step [101/249], loss=22.2496
	step [102/249], loss=22.6321
	step [103/249], loss=19.1841
	step [104/249], loss=22.2938
	step [105/249], loss=19.2837
	step [106/249], loss=20.6513
	step [107/249], loss=22.6866
	step [108/249], loss=21.7136
	step [109/249], loss=21.4922
	step [110/249], loss=19.4366
	step [111/249], loss=19.6871
	step [112/249], loss=22.7482
	step [113/249], loss=21.2671
	step [114/249], loss=24.7841
	step [115/249], loss=20.6684
	step [116/249], loss=22.0693
	step [117/249], loss=20.5487
	step [118/249], loss=23.2450
	step [119/249], loss=20.7013
	step [120/249], loss=21.1072
	step [121/249], loss=20.6428
	step [122/249], loss=20.2566
	step [123/249], loss=19.1911
	step [124/249], loss=22.4657
	step [125/249], loss=22.8005
	step [126/249], loss=19.4160
	step [127/249], loss=19.7566
	step [128/249], loss=20.3619
	step [129/249], loss=19.3030
	step [130/249], loss=21.2247
	step [131/249], loss=19.4045
	step [132/249], loss=20.6110
	step [133/249], loss=22.8389
	step [134/249], loss=20.2763
	step [135/249], loss=23.7216
	step [136/249], loss=21.9064
	step [137/249], loss=21.2636
	step [138/249], loss=21.9644
	step [139/249], loss=23.9557
	step [140/249], loss=22.6500
	step [141/249], loss=18.5477
	step [142/249], loss=20.0150
	step [143/249], loss=19.9660
	step [144/249], loss=21.4259
	step [145/249], loss=22.2248
	step [146/249], loss=25.0787
	step [147/249], loss=21.1227
	step [148/249], loss=22.1544
	step [149/249], loss=21.3662
	step [150/249], loss=21.7889
	step [151/249], loss=21.5112
	step [152/249], loss=23.1772
	step [153/249], loss=20.5970
	step [154/249], loss=22.5315
	step [155/249], loss=20.9747
	step [156/249], loss=21.6006
	step [157/249], loss=19.6735
	step [158/249], loss=19.5557
	step [159/249], loss=21.4870
	step [160/249], loss=20.5818
	step [161/249], loss=21.3879
	step [162/249], loss=22.5999
	step [163/249], loss=19.2273
	step [164/249], loss=21.0913
	step [165/249], loss=20.4345
	step [166/249], loss=22.9464
	step [167/249], loss=20.1755
	step [168/249], loss=20.7750
	step [169/249], loss=24.7150
	step [170/249], loss=19.5143
	step [171/249], loss=19.4933
	step [172/249], loss=20.9174
	step [173/249], loss=20.9007
	step [174/249], loss=17.5885
	step [175/249], loss=21.3679
	step [176/249], loss=22.4224
	step [177/249], loss=18.9959
	step [178/249], loss=19.5156
	step [179/249], loss=20.9958
	step [180/249], loss=20.2848
	step [181/249], loss=20.4482
	step [182/249], loss=19.0018
	step [183/249], loss=22.2310
	step [184/249], loss=21.5871
	step [185/249], loss=22.4631
	step [186/249], loss=18.4759
	step [187/249], loss=19.4132
	step [188/249], loss=20.7578
	step [189/249], loss=18.5985
	step [190/249], loss=23.8205
	step [191/249], loss=21.3785
	step [192/249], loss=19.4453
	step [193/249], loss=21.6806
	step [194/249], loss=21.2282
	step [195/249], loss=19.5133
	step [196/249], loss=20.6440
	step [197/249], loss=21.1479
	step [198/249], loss=20.0839
	step [199/249], loss=22.6304
	step [200/249], loss=20.9041
	step [201/249], loss=20.0138
	step [202/249], loss=19.0934
	step [203/249], loss=21.7758
	step [204/249], loss=19.9454
	step [205/249], loss=21.3343
	step [206/249], loss=22.6292
	step [207/249], loss=21.9395
	step [208/249], loss=20.1332
	step [209/249], loss=19.1884
	step [210/249], loss=21.9564
	step [211/249], loss=19.4654
	step [212/249], loss=19.9053
	step [213/249], loss=20.1777
	step [214/249], loss=23.3915
	step [215/249], loss=21.3074
	step [216/249], loss=22.0219
	step [217/249], loss=20.3749
	step [218/249], loss=25.4972
	step [219/249], loss=20.6789
	step [220/249], loss=18.0523
	step [221/249], loss=19.0497
	step [222/249], loss=19.1300
	step [223/249], loss=20.1037
	step [224/249], loss=22.5292
	step [225/249], loss=23.9196
	step [226/249], loss=19.1073
	step [227/249], loss=20.2330
	step [228/249], loss=21.2761
	step [229/249], loss=21.1507
	step [230/249], loss=20.8667
	step [231/249], loss=20.5373
	step [232/249], loss=19.0892
	step [233/249], loss=20.5074
	step [234/249], loss=21.3977
	step [235/249], loss=22.4192
	step [236/249], loss=22.2030
	step [237/249], loss=21.2096
	step [238/249], loss=19.4873
	step [239/249], loss=18.8825
	step [240/249], loss=19.8554
	step [241/249], loss=17.9413
	step [242/249], loss=21.6741
	step [243/249], loss=18.5316
	step [244/249], loss=19.4945
	step [245/249], loss=21.0235
	step [246/249], loss=21.0681
	step [247/249], loss=21.3435
	step [248/249], loss=24.0237
	step [249/249], loss=13.8522
	Evaluating
	loss=0.0611, precision=0.1317, recall=0.9941, f1=0.2326
Training epoch 7
	step [1/249], loss=19.7869
	step [2/249], loss=22.8970
	step [3/249], loss=20.1136
	step [4/249], loss=22.1654
	step [5/249], loss=19.5559
	step [6/249], loss=19.3753
	step [7/249], loss=20.9540
	step [8/249], loss=19.7065
	step [9/249], loss=18.9246
	step [10/249], loss=19.1513
	step [11/249], loss=19.4704
	step [12/249], loss=20.0110
	step [13/249], loss=20.0351
	step [14/249], loss=24.0151
	step [15/249], loss=21.9878
	step [16/249], loss=18.3133
	step [17/249], loss=19.6590
	step [18/249], loss=19.1492
	step [19/249], loss=20.5114
	step [20/249], loss=19.4115
	step [21/249], loss=23.1102
	step [22/249], loss=21.5060
	step [23/249], loss=20.1128
	step [24/249], loss=18.1869
	step [25/249], loss=20.5779
	step [26/249], loss=17.3434
	step [27/249], loss=20.3280
	step [28/249], loss=21.2315
	step [29/249], loss=16.8632
	step [30/249], loss=17.2769
	step [31/249], loss=18.4311
	step [32/249], loss=17.7617
	step [33/249], loss=19.8008
	step [34/249], loss=19.6216
	step [35/249], loss=19.8179
	step [36/249], loss=21.4955
	step [37/249], loss=21.3214
	step [38/249], loss=20.7852
	step [39/249], loss=21.1958
	step [40/249], loss=18.6638
	step [41/249], loss=22.7738
	step [42/249], loss=21.3356
	step [43/249], loss=22.2575
	step [44/249], loss=17.2878
	step [45/249], loss=17.9499
	step [46/249], loss=19.7168
	step [47/249], loss=19.7374
	step [48/249], loss=20.6006
	step [49/249], loss=17.9885
	step [50/249], loss=16.4698
	step [51/249], loss=20.2966
	step [52/249], loss=21.6832
	step [53/249], loss=20.6623
	step [54/249], loss=19.2642
	step [55/249], loss=20.8604
	step [56/249], loss=17.8669
	step [57/249], loss=19.0837
	step [58/249], loss=19.0677
	step [59/249], loss=20.4038
	step [60/249], loss=18.4128
	step [61/249], loss=17.9711
	step [62/249], loss=17.7764
	step [63/249], loss=16.5881
	step [64/249], loss=22.6305
	step [65/249], loss=19.2044
	step [66/249], loss=20.7466
	step [67/249], loss=19.9095
	step [68/249], loss=19.2865
	step [69/249], loss=16.6589
	step [70/249], loss=19.2271
	step [71/249], loss=19.2401
	step [72/249], loss=18.0055
	step [73/249], loss=20.9136
	step [74/249], loss=19.4403
	step [75/249], loss=19.1003
	step [76/249], loss=18.2161
	step [77/249], loss=20.6539
	step [78/249], loss=18.5181
	step [79/249], loss=17.8072
	step [80/249], loss=24.1672
	step [81/249], loss=17.2611
	step [82/249], loss=19.1200
	step [83/249], loss=19.3401
	step [84/249], loss=19.0219
	step [85/249], loss=19.8969
	step [86/249], loss=18.3449
	step [87/249], loss=18.7094
	step [88/249], loss=18.9960
	step [89/249], loss=17.2813
	step [90/249], loss=20.4987
	step [91/249], loss=18.5248
	step [92/249], loss=20.8303
	step [93/249], loss=17.3479
	step [94/249], loss=17.1736
	step [95/249], loss=17.7219
	step [96/249], loss=20.9500
	step [97/249], loss=18.6209
	step [98/249], loss=20.8786
	step [99/249], loss=19.4267
	step [100/249], loss=19.6535
	step [101/249], loss=19.3322
	step [102/249], loss=18.9378
	step [103/249], loss=17.2601
	step [104/249], loss=18.4953
	step [105/249], loss=18.6149
	step [106/249], loss=20.3769
	step [107/249], loss=18.8170
	step [108/249], loss=18.2688
	step [109/249], loss=21.1323
	step [110/249], loss=18.2242
	step [111/249], loss=18.4102
	step [112/249], loss=18.5951
	step [113/249], loss=18.8244
	step [114/249], loss=20.5651
	step [115/249], loss=18.5013
	step [116/249], loss=16.7983
	step [117/249], loss=21.5821
	step [118/249], loss=17.7815
	step [119/249], loss=18.1848
	step [120/249], loss=17.2361
	step [121/249], loss=19.2211
	step [122/249], loss=19.9038
	step [123/249], loss=19.2904
	step [124/249], loss=18.4327
	step [125/249], loss=19.6522
	step [126/249], loss=16.4267
	step [127/249], loss=16.8894
	step [128/249], loss=17.9514
	step [129/249], loss=18.4826
	step [130/249], loss=18.7454
	step [131/249], loss=21.2667
	step [132/249], loss=17.3870
	step [133/249], loss=20.1859
	step [134/249], loss=20.6292
	step [135/249], loss=17.4595
	step [136/249], loss=18.3086
	step [137/249], loss=20.3492
	step [138/249], loss=19.6365
	step [139/249], loss=20.0200
	step [140/249], loss=20.2987
	step [141/249], loss=16.4208
	step [142/249], loss=20.4098
	step [143/249], loss=18.3679
	step [144/249], loss=19.6444
	step [145/249], loss=17.7590
	step [146/249], loss=19.9837
	step [147/249], loss=21.0353
	step [148/249], loss=17.8482
	step [149/249], loss=17.7257
	step [150/249], loss=18.0178
	step [151/249], loss=21.5734
	step [152/249], loss=20.6427
	step [153/249], loss=18.5112
	step [154/249], loss=15.2022
	step [155/249], loss=17.4191
	step [156/249], loss=19.5915
	step [157/249], loss=17.6242
	step [158/249], loss=16.8450
	step [159/249], loss=23.5945
	step [160/249], loss=17.1455
	step [161/249], loss=18.1137
	step [162/249], loss=16.5608
	step [163/249], loss=17.2993
	step [164/249], loss=20.0671
	step [165/249], loss=18.8086
	step [166/249], loss=19.4153
	step [167/249], loss=18.9961
	step [168/249], loss=16.7164
	step [169/249], loss=18.0780
	step [170/249], loss=17.2930
	step [171/249], loss=18.3048
	step [172/249], loss=17.1945
	step [173/249], loss=19.6477
	step [174/249], loss=22.1116
	step [175/249], loss=17.8926
	step [176/249], loss=16.4019
	step [177/249], loss=20.3458
	step [178/249], loss=17.3085
	step [179/249], loss=18.8879
	step [180/249], loss=17.1135
	step [181/249], loss=18.6314
	step [182/249], loss=16.9035
	step [183/249], loss=16.6116
	step [184/249], loss=18.4785
	step [185/249], loss=17.9125
	step [186/249], loss=17.0786
	step [187/249], loss=19.9406
	step [188/249], loss=15.1577
	step [189/249], loss=16.5225
	step [190/249], loss=17.7790
	step [191/249], loss=18.8084
	step [192/249], loss=16.7374
	step [193/249], loss=17.6734
	step [194/249], loss=17.9813
	step [195/249], loss=16.8064
	step [196/249], loss=16.3560
	step [197/249], loss=19.3729
	step [198/249], loss=17.0662
	step [199/249], loss=16.9929
	step [200/249], loss=18.8666
	step [201/249], loss=16.4208
	step [202/249], loss=17.8939
	step [203/249], loss=17.3780
	step [204/249], loss=18.8146
	step [205/249], loss=18.3788
	step [206/249], loss=18.0141
	step [207/249], loss=19.4142
	step [208/249], loss=16.1973
	step [209/249], loss=17.9206
	step [210/249], loss=17.4380
	step [211/249], loss=16.5723
	step [212/249], loss=15.8657
	step [213/249], loss=19.4262
	step [214/249], loss=16.5589
	step [215/249], loss=18.4905
	step [216/249], loss=15.8593
	step [217/249], loss=17.1404
	step [218/249], loss=18.2982
	step [219/249], loss=18.8691
	step [220/249], loss=15.7084
	step [221/249], loss=20.6460
	step [222/249], loss=16.1569
	step [223/249], loss=17.3343
	step [224/249], loss=18.2373
	step [225/249], loss=17.3256
	step [226/249], loss=19.0928
	step [227/249], loss=19.7187
	step [228/249], loss=19.0336
	step [229/249], loss=15.7887
	step [230/249], loss=17.8633
	step [231/249], loss=17.5458
	step [232/249], loss=18.6924
	step [233/249], loss=17.9213
	step [234/249], loss=17.1775
	step [235/249], loss=18.8766
	step [236/249], loss=17.5455
	step [237/249], loss=18.5755
	step [238/249], loss=17.4219
	step [239/249], loss=18.8384
	step [240/249], loss=18.2684
	step [241/249], loss=22.0566
	step [242/249], loss=17.3870
	step [243/249], loss=15.8771
	step [244/249], loss=17.1458
	step [245/249], loss=19.3241
	step [246/249], loss=17.2151
	step [247/249], loss=17.0434
	step [248/249], loss=19.4662
	step [249/249], loss=12.8274
	Evaluating
	loss=0.0530, precision=0.1343, recall=0.9944, f1=0.2366
Training epoch 8
	step [1/249], loss=21.5340
	step [2/249], loss=19.1017
	step [3/249], loss=15.8833
	step [4/249], loss=18.5611
	step [5/249], loss=16.3865
	step [6/249], loss=17.2174
	step [7/249], loss=17.3790
	step [8/249], loss=17.5186
	step [9/249], loss=17.4443
	step [10/249], loss=16.9124
	step [11/249], loss=20.8551
	step [12/249], loss=18.5087
	step [13/249], loss=18.4311
	step [14/249], loss=15.7353
	step [15/249], loss=18.7740
	step [16/249], loss=19.3261
	step [17/249], loss=17.0940
	step [18/249], loss=15.9912
	step [19/249], loss=16.6790
	step [20/249], loss=15.5262
	step [21/249], loss=16.5158
	step [22/249], loss=17.9723
	step [23/249], loss=15.9905
	step [24/249], loss=20.4879
	step [25/249], loss=17.7449
	step [26/249], loss=17.5153
	step [27/249], loss=18.6207
	step [28/249], loss=18.5772
	step [29/249], loss=17.1578
	step [30/249], loss=18.2325
	step [31/249], loss=17.3493
	step [32/249], loss=18.0045
	step [33/249], loss=18.8399
	step [34/249], loss=16.1099
	step [35/249], loss=16.7367
	step [36/249], loss=17.2277
	step [37/249], loss=18.0602
	step [38/249], loss=18.8826
	step [39/249], loss=17.9554
	step [40/249], loss=16.2169
	step [41/249], loss=18.4688
	step [42/249], loss=15.3912
	step [43/249], loss=17.2655
	step [44/249], loss=15.8703
	step [45/249], loss=19.2446
	step [46/249], loss=19.3225
	step [47/249], loss=17.1112
	step [48/249], loss=19.0068
	step [49/249], loss=18.1415
	step [50/249], loss=16.2589
	step [51/249], loss=16.7977
	step [52/249], loss=17.4183
	step [53/249], loss=16.7032
	step [54/249], loss=19.5718
	step [55/249], loss=14.8145
	step [56/249], loss=16.5575
	step [57/249], loss=17.4991
	step [58/249], loss=18.8045
	step [59/249], loss=16.2427
	step [60/249], loss=17.3158
	step [61/249], loss=14.8853
	step [62/249], loss=16.2035
	step [63/249], loss=17.0235
	step [64/249], loss=17.5088
	step [65/249], loss=18.1075
	step [66/249], loss=16.2427
	step [67/249], loss=18.3113
	step [68/249], loss=18.5137
	step [69/249], loss=16.6733
	step [70/249], loss=20.0728
	step [71/249], loss=16.6222
	step [72/249], loss=18.6123
	step [73/249], loss=16.1871
	step [74/249], loss=14.8649
	step [75/249], loss=16.4300
	step [76/249], loss=17.6892
	step [77/249], loss=16.2243
	step [78/249], loss=15.9577
	step [79/249], loss=18.4824
	step [80/249], loss=17.7323
	step [81/249], loss=15.7406
	step [82/249], loss=16.2526
	step [83/249], loss=18.0824
	step [84/249], loss=16.9098
	step [85/249], loss=16.1181
	step [86/249], loss=18.4987
	step [87/249], loss=17.7872
	step [88/249], loss=15.9371
	step [89/249], loss=15.9839
	step [90/249], loss=16.1721
	step [91/249], loss=18.5466
	step [92/249], loss=15.2402
	step [93/249], loss=18.7020
	step [94/249], loss=19.9758
	step [95/249], loss=14.5889
	step [96/249], loss=17.7788
	step [97/249], loss=17.3821
	step [98/249], loss=14.3807
	step [99/249], loss=18.0527
	step [100/249], loss=19.8069
	step [101/249], loss=16.0758
	step [102/249], loss=18.5521
	step [103/249], loss=15.8197
	step [104/249], loss=15.5686
	step [105/249], loss=16.4294
	step [106/249], loss=17.7667
	step [107/249], loss=17.4728
	step [108/249], loss=15.5876
	step [109/249], loss=15.9372
	step [110/249], loss=15.6354
	step [111/249], loss=15.8752
	step [112/249], loss=17.4205
	step [113/249], loss=14.4873
	step [114/249], loss=17.5802
	step [115/249], loss=17.4843
	step [116/249], loss=17.4803
	step [117/249], loss=15.9919
	step [118/249], loss=17.8072
	step [119/249], loss=17.3535
	step [120/249], loss=16.6373
	step [121/249], loss=15.8648
	step [122/249], loss=17.4559
	step [123/249], loss=16.4536
	step [124/249], loss=19.6678
	step [125/249], loss=16.4648
	step [126/249], loss=16.6302
	step [127/249], loss=16.3937
	step [128/249], loss=16.7767
	step [129/249], loss=17.3030
	step [130/249], loss=17.0318
	step [131/249], loss=16.2751
	step [132/249], loss=18.2086
	step [133/249], loss=15.7911
	step [134/249], loss=16.7265
	step [135/249], loss=16.4190
	step [136/249], loss=16.9052
	step [137/249], loss=15.9761
	step [138/249], loss=17.5564
	step [139/249], loss=16.9000
	step [140/249], loss=15.4443
	step [141/249], loss=17.7655
	step [142/249], loss=16.8062
	step [143/249], loss=16.9854
	step [144/249], loss=17.3048
	step [145/249], loss=15.1350
	step [146/249], loss=16.9274
	step [147/249], loss=16.6383
	step [148/249], loss=14.8195
	step [149/249], loss=19.7989
	step [150/249], loss=15.0899
	step [151/249], loss=17.1189
	step [152/249], loss=15.1851
	step [153/249], loss=17.8111
	step [154/249], loss=16.5199
	step [155/249], loss=15.4922
	step [156/249], loss=18.0982
	step [157/249], loss=16.6145
	step [158/249], loss=15.0294
	step [159/249], loss=16.0163
	step [160/249], loss=16.5784
	step [161/249], loss=16.4560
	step [162/249], loss=15.0435
	step [163/249], loss=16.2680
	step [164/249], loss=17.0932
	step [165/249], loss=17.2689
	step [166/249], loss=15.8900
	step [167/249], loss=18.7756
	step [168/249], loss=18.4304
	step [169/249], loss=14.9855
	step [170/249], loss=19.9805
	step [171/249], loss=17.7541
	step [172/249], loss=16.6050
	step [173/249], loss=17.4242
	step [174/249], loss=17.5156
	step [175/249], loss=16.8346
	step [176/249], loss=16.9943
	step [177/249], loss=15.4592
	step [178/249], loss=16.6671
	step [179/249], loss=18.4963
	step [180/249], loss=16.8356
	step [181/249], loss=15.5708
	step [182/249], loss=15.8374
	step [183/249], loss=17.0328
	step [184/249], loss=14.5553
	step [185/249], loss=15.9559
	step [186/249], loss=16.2148
	step [187/249], loss=14.5666
	step [188/249], loss=14.1825
	step [189/249], loss=17.7380
	step [190/249], loss=16.4903
	step [191/249], loss=16.1465
	step [192/249], loss=17.8484
	step [193/249], loss=14.2788
	step [194/249], loss=16.4319
	step [195/249], loss=18.5936
	step [196/249], loss=19.3963
	step [197/249], loss=17.7920
	step [198/249], loss=15.5243
	step [199/249], loss=16.9728
	step [200/249], loss=13.6957
	step [201/249], loss=13.9032
	step [202/249], loss=13.7883
	step [203/249], loss=15.6747
	step [204/249], loss=14.6786
	step [205/249], loss=15.2054
	step [206/249], loss=17.3810
	step [207/249], loss=19.1177
	step [208/249], loss=15.6438
	step [209/249], loss=16.1125
	step [210/249], loss=18.5285
	step [211/249], loss=15.8941
	step [212/249], loss=17.6648
	step [213/249], loss=17.6442
	step [214/249], loss=17.2633
	step [215/249], loss=16.6298
	step [216/249], loss=14.8534
	step [217/249], loss=19.5853
	step [218/249], loss=15.6874
	step [219/249], loss=16.4701
	step [220/249], loss=15.3768
	step [221/249], loss=17.1840
	step [222/249], loss=15.7690
	step [223/249], loss=15.3718
	step [224/249], loss=14.8178
	step [225/249], loss=16.5772
	step [226/249], loss=15.5027
	step [227/249], loss=15.1409
	step [228/249], loss=17.9953
	step [229/249], loss=16.7697
	step [230/249], loss=17.2966
	step [231/249], loss=15.6904
	step [232/249], loss=18.3270
	step [233/249], loss=16.0605
	step [234/249], loss=18.0150
	step [235/249], loss=17.8355
	step [236/249], loss=14.9865
	step [237/249], loss=15.3287
	step [238/249], loss=16.1077
	step [239/249], loss=14.8813
	step [240/249], loss=16.2983
	step [241/249], loss=16.0460
	step [242/249], loss=17.3482
	step [243/249], loss=14.4424
	step [244/249], loss=17.2832
	step [245/249], loss=17.0412
	step [246/249], loss=14.7552
	step [247/249], loss=17.7045
	step [248/249], loss=19.6973
	step [249/249], loss=12.0090
	Evaluating
	loss=0.0532, precision=0.1026, recall=0.9951, f1=0.1860
Training epoch 9
	step [1/249], loss=16.7196
	step [2/249], loss=19.1535
	step [3/249], loss=14.8096
	step [4/249], loss=15.0255
	step [5/249], loss=14.1145
	step [6/249], loss=17.0993
	step [7/249], loss=18.0139
	step [8/249], loss=16.2264
	step [9/249], loss=14.1359
	step [10/249], loss=15.5019
	step [11/249], loss=16.6368
	step [12/249], loss=16.8319
	step [13/249], loss=13.7823
	step [14/249], loss=17.0832
	step [15/249], loss=16.0843
	step [16/249], loss=15.0848
	step [17/249], loss=15.7972
	step [18/249], loss=19.4283
	step [19/249], loss=14.0613
	step [20/249], loss=15.0092
	step [21/249], loss=16.5884
	step [22/249], loss=14.5949
	step [23/249], loss=15.1089
	step [24/249], loss=14.1222
	step [25/249], loss=17.6780
	step [26/249], loss=15.4294
	step [27/249], loss=18.0206
	step [28/249], loss=16.3589
	step [29/249], loss=16.6813
	step [30/249], loss=15.0053
	step [31/249], loss=14.9681
	step [32/249], loss=14.1473
	step [33/249], loss=14.2782
	step [34/249], loss=15.1052
	step [35/249], loss=14.6155
	step [36/249], loss=15.0500
	step [37/249], loss=14.6901
	step [38/249], loss=15.0638
	step [39/249], loss=14.5354
	step [40/249], loss=17.4777
	step [41/249], loss=15.8105
	step [42/249], loss=19.3054
	step [43/249], loss=18.3603
	step [44/249], loss=14.0639
	step [45/249], loss=17.8012
	step [46/249], loss=17.1009
	step [47/249], loss=16.4014
	step [48/249], loss=14.4663
	step [49/249], loss=15.7874
	step [50/249], loss=14.4145
	step [51/249], loss=14.9100
	step [52/249], loss=14.7177
	step [53/249], loss=15.6103
	step [54/249], loss=16.3861
	step [55/249], loss=16.4312
	step [56/249], loss=19.6486
	step [57/249], loss=16.2588
	step [58/249], loss=15.9294
	step [59/249], loss=18.1954
	step [60/249], loss=13.7402
	step [61/249], loss=14.0855
	step [62/249], loss=14.6630
	step [63/249], loss=16.5356
	step [64/249], loss=15.5409
	step [65/249], loss=15.2018
	step [66/249], loss=16.2407
	step [67/249], loss=23.3003
	step [68/249], loss=16.0230
	step [69/249], loss=14.3390
	step [70/249], loss=14.7371
	step [71/249], loss=14.8728
	step [72/249], loss=14.9850
	step [73/249], loss=16.4629
	step [74/249], loss=16.4720
	step [75/249], loss=14.9324
	step [76/249], loss=15.5915
	step [77/249], loss=15.5615
	step [78/249], loss=17.3070
	step [79/249], loss=14.2563
	step [80/249], loss=16.1327
	step [81/249], loss=16.2907
	step [82/249], loss=17.8796
	step [83/249], loss=14.9368
	step [84/249], loss=16.1796
	step [85/249], loss=16.2028
	step [86/249], loss=15.4284
	step [87/249], loss=17.2816
	step [88/249], loss=15.5940
	step [89/249], loss=17.3106
	step [90/249], loss=15.2220
	step [91/249], loss=15.9460
	step [92/249], loss=14.3753
	step [93/249], loss=15.2066
	step [94/249], loss=14.8622
	step [95/249], loss=13.5012
	step [96/249], loss=14.3469
	step [97/249], loss=16.4117
	step [98/249], loss=17.4607
	step [99/249], loss=17.2048
	step [100/249], loss=14.8581
	step [101/249], loss=16.5059
	step [102/249], loss=16.4025
	step [103/249], loss=14.4179
	step [104/249], loss=14.0688
	step [105/249], loss=16.1112
	step [106/249], loss=15.4115
	step [107/249], loss=14.5795
	step [108/249], loss=17.2513
	step [109/249], loss=14.9149
	step [110/249], loss=14.7488
	step [111/249], loss=15.1978
	step [112/249], loss=13.8424
	step [113/249], loss=15.4411
	step [114/249], loss=14.0603
	step [115/249], loss=15.0316
	step [116/249], loss=15.1292
	step [117/249], loss=16.5263
	step [118/249], loss=15.6758
	step [119/249], loss=15.3874
	step [120/249], loss=14.6815
	step [121/249], loss=14.4465
	step [122/249], loss=13.5554
	step [123/249], loss=15.7686
	step [124/249], loss=15.3045
	step [125/249], loss=12.5418
	step [126/249], loss=14.0047
	step [127/249], loss=15.3470
	step [128/249], loss=15.7183
	step [129/249], loss=13.6623
	step [130/249], loss=16.6304
	step [131/249], loss=17.4016
	step [132/249], loss=13.9453
	step [133/249], loss=16.6032
	step [134/249], loss=14.0997
	step [135/249], loss=16.3104
	step [136/249], loss=13.7701
	step [137/249], loss=14.5994
	step [138/249], loss=14.8417
	step [139/249], loss=14.1149
	step [140/249], loss=15.9886
	step [141/249], loss=14.8477
	step [142/249], loss=14.9624
	step [143/249], loss=17.3622
	step [144/249], loss=15.0581
	step [145/249], loss=12.9643
	step [146/249], loss=12.6960
	step [147/249], loss=17.5786
	step [148/249], loss=14.6538
	step [149/249], loss=14.2314
	step [150/249], loss=14.7808
	step [151/249], loss=13.2757
	step [152/249], loss=14.8536
	step [153/249], loss=15.1396
	step [154/249], loss=14.4414
	step [155/249], loss=13.4316
	step [156/249], loss=15.7844
	step [157/249], loss=15.3702
	step [158/249], loss=17.8682
	step [159/249], loss=16.6003
	step [160/249], loss=14.9123
	step [161/249], loss=16.4227
	step [162/249], loss=15.8395
	step [163/249], loss=14.2763
	step [164/249], loss=15.2520
	step [165/249], loss=16.2075
	step [166/249], loss=14.2763
	step [167/249], loss=15.0544
	step [168/249], loss=15.9214
	step [169/249], loss=14.5078
	step [170/249], loss=15.8291
	step [171/249], loss=13.5513
	step [172/249], loss=16.2439
	step [173/249], loss=14.4243
	step [174/249], loss=13.5584
	step [175/249], loss=14.1461
	step [176/249], loss=13.9073
	step [177/249], loss=13.6991
	step [178/249], loss=14.8586
	step [179/249], loss=13.2782
	step [180/249], loss=17.3011
	step [181/249], loss=13.8562
	step [182/249], loss=14.2446
	step [183/249], loss=15.1240
	step [184/249], loss=15.2322
	step [185/249], loss=12.7816
	step [186/249], loss=15.9528
	step [187/249], loss=16.3581
	step [188/249], loss=13.7210
	step [189/249], loss=13.2913
	step [190/249], loss=14.4198
	step [191/249], loss=13.3755
	step [192/249], loss=13.4942
	step [193/249], loss=16.6587
	step [194/249], loss=12.8673
	step [195/249], loss=14.3185
	step [196/249], loss=15.6852
	step [197/249], loss=14.2246
	step [198/249], loss=18.1835
	step [199/249], loss=15.4323
	step [200/249], loss=14.3184
	step [201/249], loss=15.3215
	step [202/249], loss=13.9421
	step [203/249], loss=15.3135
	step [204/249], loss=15.5316
	step [205/249], loss=17.0532
	step [206/249], loss=14.2609
	step [207/249], loss=15.0156
	step [208/249], loss=13.5711
	step [209/249], loss=15.0393
	step [210/249], loss=14.6208
	step [211/249], loss=16.8775
	step [212/249], loss=15.0488
	step [213/249], loss=14.8038
	step [214/249], loss=15.9563
	step [215/249], loss=13.4698
	step [216/249], loss=14.3574
	step [217/249], loss=13.4542
	step [218/249], loss=14.6329
	step [219/249], loss=12.9332
	step [220/249], loss=14.4641
	step [221/249], loss=13.2258
	step [222/249], loss=15.7707
	step [223/249], loss=15.9774
	step [224/249], loss=14.2030
	step [225/249], loss=14.8188
	step [226/249], loss=16.8297
	step [227/249], loss=13.2541
	step [228/249], loss=14.7555
	step [229/249], loss=15.7899
	step [230/249], loss=12.6573
	step [231/249], loss=14.0205
	step [232/249], loss=14.3375
	step [233/249], loss=14.0116
	step [234/249], loss=15.5542
	step [235/249], loss=18.4979
	step [236/249], loss=16.0312
	step [237/249], loss=14.9573
	step [238/249], loss=14.5499
	step [239/249], loss=16.4218
	step [240/249], loss=17.5478
	step [241/249], loss=13.4101
	step [242/249], loss=14.2698
	step [243/249], loss=15.7739
	step [244/249], loss=16.8540
	step [245/249], loss=14.5883
	step [246/249], loss=16.1272
	step [247/249], loss=14.5946
	step [248/249], loss=12.1644
	step [249/249], loss=10.7526
	Evaluating
	loss=0.0410, precision=0.1425, recall=0.9940, f1=0.2493
Training epoch 10
	step [1/249], loss=17.3698
	step [2/249], loss=15.5458
	step [3/249], loss=13.1550
	step [4/249], loss=16.1734
	step [5/249], loss=15.3352
	step [6/249], loss=16.7669
	step [7/249], loss=14.0687
	step [8/249], loss=13.2715
	step [9/249], loss=16.2375
	step [10/249], loss=13.4709
	step [11/249], loss=15.3491
	step [12/249], loss=14.2197
	step [13/249], loss=14.7881
	step [14/249], loss=12.7639
	step [15/249], loss=14.8705
	step [16/249], loss=12.5485
	step [17/249], loss=15.9059
	step [18/249], loss=14.5736
	step [19/249], loss=13.3824
	step [20/249], loss=13.7895
	step [21/249], loss=16.3551
	step [22/249], loss=13.2603
	step [23/249], loss=16.3198
	step [24/249], loss=14.7489
	step [25/249], loss=15.2495
	step [26/249], loss=13.7082
	step [27/249], loss=14.3432
	step [28/249], loss=16.2715
	step [29/249], loss=15.4130
	step [30/249], loss=13.6666
	step [31/249], loss=12.6524
	step [32/249], loss=13.8588
	step [33/249], loss=14.9580
	step [34/249], loss=15.5516
	step [35/249], loss=14.7356
	step [36/249], loss=13.5273
	step [37/249], loss=12.5715
	step [38/249], loss=14.7501
	step [39/249], loss=12.6739
	step [40/249], loss=13.2988
	step [41/249], loss=12.9965
	step [42/249], loss=12.8891
	step [43/249], loss=14.0753
	step [44/249], loss=14.1391
	step [45/249], loss=13.3722
	step [46/249], loss=17.3571
	step [47/249], loss=12.8971
	step [48/249], loss=14.2802
	step [49/249], loss=13.0796
	step [50/249], loss=12.9153
	step [51/249], loss=13.6032
	step [52/249], loss=14.8855
	step [53/249], loss=15.4327
	step [54/249], loss=17.2726
	step [55/249], loss=14.8170
	step [56/249], loss=13.8717
	step [57/249], loss=14.4573
	step [58/249], loss=14.7650
	step [59/249], loss=15.4414
	step [60/249], loss=16.3864
	step [61/249], loss=13.4175
	step [62/249], loss=14.8526
	step [63/249], loss=14.8038
	step [64/249], loss=14.9202
	step [65/249], loss=14.3182
	step [66/249], loss=11.8477
	step [67/249], loss=14.6734
	step [68/249], loss=14.1444
	step [69/249], loss=13.0805
	step [70/249], loss=14.3066
	step [71/249], loss=13.2755
	step [72/249], loss=15.5561
	step [73/249], loss=15.6896
	step [74/249], loss=15.2755
	step [75/249], loss=12.6606
	step [76/249], loss=14.0387
	step [77/249], loss=13.2898
	step [78/249], loss=12.4282
	step [79/249], loss=13.1987
	step [80/249], loss=14.8505
	step [81/249], loss=16.2378
	step [82/249], loss=13.8527
	step [83/249], loss=16.6290
	step [84/249], loss=13.2378
	step [85/249], loss=14.5418
	step [86/249], loss=12.3411
	step [87/249], loss=13.8068
	step [88/249], loss=14.5409
	step [89/249], loss=13.4364
	step [90/249], loss=13.8406
	step [91/249], loss=12.8357
	step [92/249], loss=13.8747
	step [93/249], loss=13.6675
	step [94/249], loss=14.4820
	step [95/249], loss=12.7714
	step [96/249], loss=14.7488
	step [97/249], loss=14.7318
	step [98/249], loss=14.2337
	step [99/249], loss=14.7449
	step [100/249], loss=15.2424
	step [101/249], loss=13.2804
	step [102/249], loss=14.5485
	step [103/249], loss=14.2333
	step [104/249], loss=14.4321
	step [105/249], loss=14.6341
	step [106/249], loss=15.3953
	step [107/249], loss=13.9617
	step [108/249], loss=14.3028
	step [109/249], loss=16.2914
	step [110/249], loss=12.1654
	step [111/249], loss=16.4233
	step [112/249], loss=16.0912
	step [113/249], loss=14.7386
	step [114/249], loss=12.1324
	step [115/249], loss=13.3467
	step [116/249], loss=14.1810
	step [117/249], loss=13.8227
	step [118/249], loss=15.1353
	step [119/249], loss=14.5357
	step [120/249], loss=14.5228
	step [121/249], loss=14.4105
	step [122/249], loss=13.3274
	step [123/249], loss=14.2348
	step [124/249], loss=14.7484
	step [125/249], loss=11.6306
	step [126/249], loss=14.6330
	step [127/249], loss=12.6173
	step [128/249], loss=12.7797
	step [129/249], loss=12.6937
	step [130/249], loss=14.9249
	step [131/249], loss=17.7128
	step [132/249], loss=13.2466
	step [133/249], loss=14.1770
	step [134/249], loss=15.4994
	step [135/249], loss=12.5634
	step [136/249], loss=16.2660
	step [137/249], loss=15.8143
	step [138/249], loss=12.5445
	step [139/249], loss=12.9998
	step [140/249], loss=12.1264
	step [141/249], loss=17.0360
	step [142/249], loss=13.4055
	step [143/249], loss=13.9870
	step [144/249], loss=16.5328
	step [145/249], loss=13.2815
	step [146/249], loss=14.9516
	step [147/249], loss=13.0606
	step [148/249], loss=12.6021
	step [149/249], loss=14.2952
	step [150/249], loss=12.6081
	step [151/249], loss=15.2530
	step [152/249], loss=13.3853
	step [153/249], loss=11.9934
	step [154/249], loss=14.2112
	step [155/249], loss=14.4777
	step [156/249], loss=12.8068
	step [157/249], loss=14.7585
	step [158/249], loss=12.7407
	step [159/249], loss=15.5682
	step [160/249], loss=12.8329
	step [161/249], loss=13.5800
	step [162/249], loss=16.5108
	step [163/249], loss=11.1301
	step [164/249], loss=17.8402
	step [165/249], loss=19.2010
	step [166/249], loss=15.5424
	step [167/249], loss=13.7379
	step [168/249], loss=13.7964
	step [169/249], loss=13.5575
	step [170/249], loss=15.2160
	step [171/249], loss=14.1852
	step [172/249], loss=13.4867
	step [173/249], loss=12.8970
	step [174/249], loss=12.8224
	step [175/249], loss=16.5366
	step [176/249], loss=12.8944
	step [177/249], loss=12.5841
	step [178/249], loss=13.0590
	step [179/249], loss=12.6567
	step [180/249], loss=13.6339
	step [181/249], loss=14.8982
	step [182/249], loss=12.0726
	step [183/249], loss=17.5119
	step [184/249], loss=15.4999
	step [185/249], loss=12.2216
	step [186/249], loss=15.4573
	step [187/249], loss=11.6834
	step [188/249], loss=14.7425
	step [189/249], loss=14.2210
	step [190/249], loss=12.3790
	step [191/249], loss=13.9095
	step [192/249], loss=13.2523
	step [193/249], loss=13.7417
	step [194/249], loss=14.4222
	step [195/249], loss=13.4115
	step [196/249], loss=14.6200
	step [197/249], loss=14.9804
	step [198/249], loss=13.4971
	step [199/249], loss=15.0313
	step [200/249], loss=13.6252
	step [201/249], loss=14.6383
	step [202/249], loss=15.9109
	step [203/249], loss=13.0494
	step [204/249], loss=13.0895
	step [205/249], loss=14.2642
	step [206/249], loss=12.7289
	step [207/249], loss=11.8468
	step [208/249], loss=13.2291
	step [209/249], loss=15.4565
	step [210/249], loss=16.4401
	step [211/249], loss=13.5228
	step [212/249], loss=14.1118
	step [213/249], loss=11.8918
	step [214/249], loss=14.1550
	step [215/249], loss=12.4692
	step [216/249], loss=13.1244
	step [217/249], loss=14.4380
	step [218/249], loss=14.1443
	step [219/249], loss=12.9628
	step [220/249], loss=13.2081
	step [221/249], loss=13.7536
	step [222/249], loss=15.6619
	step [223/249], loss=10.5015
	step [224/249], loss=13.3422
	step [225/249], loss=13.7132
	step [226/249], loss=15.9469
	step [227/249], loss=12.1308
	step [228/249], loss=13.7665
	step [229/249], loss=12.7454
	step [230/249], loss=11.0532
	step [231/249], loss=13.8022
	step [232/249], loss=15.0443
	step [233/249], loss=15.7296
	step [234/249], loss=12.4686
	step [235/249], loss=10.8696
	step [236/249], loss=14.3529
	step [237/249], loss=14.5557
	step [238/249], loss=13.7185
	step [239/249], loss=14.2651
	step [240/249], loss=13.1865
	step [241/249], loss=12.8523
	step [242/249], loss=15.4254
	step [243/249], loss=13.5126
	step [244/249], loss=15.1345
	step [245/249], loss=11.6927
	step [246/249], loss=13.1863
	step [247/249], loss=15.8040
	step [248/249], loss=13.3737
	step [249/249], loss=9.3646
	Evaluating
	loss=0.0389, precision=0.1372, recall=0.9944, f1=0.2411
Training epoch 11
	step [1/249], loss=15.4031
	step [2/249], loss=15.0508
	step [3/249], loss=12.5234
	step [4/249], loss=11.8996
	step [5/249], loss=13.9045
	step [6/249], loss=14.9936
	step [7/249], loss=12.8542
	step [8/249], loss=14.5440
	step [9/249], loss=15.1972
	step [10/249], loss=13.5920
	step [11/249], loss=12.4617
	step [12/249], loss=13.8382
	step [13/249], loss=10.6341
	step [14/249], loss=15.7705
	step [15/249], loss=11.9983
	step [16/249], loss=14.3913
	step [17/249], loss=14.7337
	step [18/249], loss=12.3145
	step [19/249], loss=13.2134
	step [20/249], loss=12.7251
	step [21/249], loss=15.0646
	step [22/249], loss=13.1218
	step [23/249], loss=12.0316
	step [24/249], loss=11.7271
	step [25/249], loss=10.6562
	step [26/249], loss=13.1576
	step [27/249], loss=11.1088
	step [28/249], loss=13.1252
	step [29/249], loss=11.1016
	step [30/249], loss=10.2630
	step [31/249], loss=14.4737
	step [32/249], loss=14.5823
	step [33/249], loss=12.2844
	step [34/249], loss=14.8816
	step [35/249], loss=12.6267
	step [36/249], loss=11.1799
	step [37/249], loss=15.2525
	step [38/249], loss=14.6264
	step [39/249], loss=15.2756
	step [40/249], loss=12.6154
	step [41/249], loss=13.3869
	step [42/249], loss=14.2380
	step [43/249], loss=12.6531
	step [44/249], loss=13.7308
	step [45/249], loss=13.6861
	step [46/249], loss=14.1826
	step [47/249], loss=13.7583
	step [48/249], loss=13.8596
	step [49/249], loss=13.9003
	step [50/249], loss=12.2766
	step [51/249], loss=13.8412
	step [52/249], loss=13.9338
	step [53/249], loss=12.8332
	step [54/249], loss=12.9569
	step [55/249], loss=12.1298
	step [56/249], loss=16.2690
	step [57/249], loss=13.5182
	step [58/249], loss=12.1618
	step [59/249], loss=14.2563
	step [60/249], loss=13.1789
	step [61/249], loss=11.6601
	step [62/249], loss=13.5110
	step [63/249], loss=12.5738
	step [64/249], loss=10.9928
	step [65/249], loss=12.7081
	step [66/249], loss=13.3749
	step [67/249], loss=13.5311
	step [68/249], loss=11.3958
	step [69/249], loss=13.8905
	step [70/249], loss=13.2297
	step [71/249], loss=12.0221
	step [72/249], loss=11.9185
	step [73/249], loss=13.6917
	step [74/249], loss=13.7852
	step [75/249], loss=13.8830
	step [76/249], loss=12.3896
	step [77/249], loss=10.5505
	step [78/249], loss=12.9511
	step [79/249], loss=15.5817
	step [80/249], loss=13.9454
	step [81/249], loss=14.6168
	step [82/249], loss=15.3293
	step [83/249], loss=13.6341
	step [84/249], loss=12.8841
	step [85/249], loss=12.9066
	step [86/249], loss=12.8731
	step [87/249], loss=15.0652
	step [88/249], loss=12.0408
	step [89/249], loss=11.6963
	step [90/249], loss=15.4352
	step [91/249], loss=13.6958
	step [92/249], loss=12.1328
	step [93/249], loss=14.2745
	step [94/249], loss=14.5775
	step [95/249], loss=13.1033
	step [96/249], loss=13.6449
	step [97/249], loss=12.1024
	step [98/249], loss=12.3297
	step [99/249], loss=13.8682
	step [100/249], loss=12.6398
	step [101/249], loss=13.8396
	step [102/249], loss=16.3887
	step [103/249], loss=13.8448
	step [104/249], loss=12.5130
	step [105/249], loss=13.1938
	step [106/249], loss=13.8340
	step [107/249], loss=13.3751
	step [108/249], loss=13.6778
	step [109/249], loss=12.7608
	step [110/249], loss=11.9880
	step [111/249], loss=12.6326
	step [112/249], loss=13.8850
	step [113/249], loss=13.8378
	step [114/249], loss=13.5757
	step [115/249], loss=12.0006
	step [116/249], loss=11.2473
	step [117/249], loss=12.7095
	step [118/249], loss=12.8071
	step [119/249], loss=11.3241
	step [120/249], loss=13.2745
	step [121/249], loss=12.7689
	step [122/249], loss=14.2374
	step [123/249], loss=13.6185
	step [124/249], loss=14.7040
	step [125/249], loss=11.9176
	step [126/249], loss=10.8598
	step [127/249], loss=13.9815
	step [128/249], loss=13.1500
	step [129/249], loss=12.5197
	step [130/249], loss=14.3332
	step [131/249], loss=11.7153
	step [132/249], loss=15.2829
	step [133/249], loss=14.4325
	step [134/249], loss=15.9260
	step [135/249], loss=13.1056
	step [136/249], loss=11.0793
	step [137/249], loss=11.9977
	step [138/249], loss=14.0528
	step [139/249], loss=13.3205
	step [140/249], loss=16.0845
	step [141/249], loss=12.4967
	step [142/249], loss=10.9457
	step [143/249], loss=15.5418
	step [144/249], loss=12.9103
	step [145/249], loss=16.5043
	step [146/249], loss=11.1112
	step [147/249], loss=13.2970
	step [148/249], loss=10.5717
	step [149/249], loss=11.7169
	step [150/249], loss=12.8753
	step [151/249], loss=13.6266
	step [152/249], loss=12.4064
	step [153/249], loss=14.2261
	step [154/249], loss=11.7032
	step [155/249], loss=12.2116
	step [156/249], loss=12.6241
	step [157/249], loss=13.7756
	step [158/249], loss=10.6940
	step [159/249], loss=13.2835
	step [160/249], loss=11.1432
	step [161/249], loss=10.2444
	step [162/249], loss=11.7963
	step [163/249], loss=14.4952
	step [164/249], loss=13.1674
	step [165/249], loss=12.9055
	step [166/249], loss=13.7826
	step [167/249], loss=12.4461
	step [168/249], loss=11.6490
	step [169/249], loss=14.5744
	step [170/249], loss=11.8386
	step [171/249], loss=14.7977
	step [172/249], loss=12.1482
	step [173/249], loss=14.5610
	step [174/249], loss=13.2646
	step [175/249], loss=13.6260
	step [176/249], loss=12.6409
	step [177/249], loss=14.6037
	step [178/249], loss=12.2098
	step [179/249], loss=11.1942
	step [180/249], loss=14.4380
	step [181/249], loss=13.3469
	step [182/249], loss=12.0547
	step [183/249], loss=14.9858
	step [184/249], loss=15.6832
	step [185/249], loss=12.8293
	step [186/249], loss=10.3277
	step [187/249], loss=14.0698
	step [188/249], loss=13.2690
	step [189/249], loss=11.1614
	step [190/249], loss=11.0330
	step [191/249], loss=12.3758
	step [192/249], loss=10.4358
	step [193/249], loss=13.8167
	step [194/249], loss=14.7150
	step [195/249], loss=13.5185
	step [196/249], loss=11.8200
	step [197/249], loss=10.9961
	step [198/249], loss=12.7311
	step [199/249], loss=10.6076
	step [200/249], loss=12.6684
	step [201/249], loss=15.2499
	step [202/249], loss=11.7095
	step [203/249], loss=11.2211
	step [204/249], loss=12.6285
	step [205/249], loss=11.3502
	step [206/249], loss=11.8187
	step [207/249], loss=12.7339
	step [208/249], loss=12.7505
	step [209/249], loss=13.9503
	step [210/249], loss=13.1911
	step [211/249], loss=13.3829
	step [212/249], loss=12.4800
	step [213/249], loss=12.0846
	step [214/249], loss=12.2580
	step [215/249], loss=14.4443
	step [216/249], loss=11.8880
	step [217/249], loss=13.0889
	step [218/249], loss=14.3141
	step [219/249], loss=11.3415
	step [220/249], loss=14.0030
	step [221/249], loss=12.2119
	step [222/249], loss=11.8691
	step [223/249], loss=11.1524
	step [224/249], loss=12.3897
	step [225/249], loss=12.8203
	step [226/249], loss=13.4711
	step [227/249], loss=13.7047
	step [228/249], loss=11.2889
	step [229/249], loss=10.7964
	step [230/249], loss=13.6650
	step [231/249], loss=12.0005
	step [232/249], loss=12.6782
	step [233/249], loss=11.6487
	step [234/249], loss=13.3566
	step [235/249], loss=11.5427
	step [236/249], loss=13.3691
	step [237/249], loss=11.2525
	step [238/249], loss=16.2120
	step [239/249], loss=11.1306
	step [240/249], loss=12.7548
	step [241/249], loss=13.3017
	step [242/249], loss=12.9563
	step [243/249], loss=11.7836
	step [244/249], loss=12.6326
	step [245/249], loss=12.1357
	step [246/249], loss=11.2415
	step [247/249], loss=11.1329
	step [248/249], loss=11.4266
	step [249/249], loss=9.1878
	Evaluating
	loss=0.0355, precision=0.1408, recall=0.9943, f1=0.2467
Training epoch 12
	step [1/249], loss=12.2787
	step [2/249], loss=13.8546
	step [3/249], loss=13.9307
	step [4/249], loss=12.7771
	step [5/249], loss=10.8020
	step [6/249], loss=12.8170
	step [7/249], loss=13.2948
	step [8/249], loss=11.4441
	step [9/249], loss=11.6302
	step [10/249], loss=13.5124
	step [11/249], loss=11.5709
	step [12/249], loss=11.3147
	step [13/249], loss=11.7029
	step [14/249], loss=15.6299
	step [15/249], loss=14.5492
	step [16/249], loss=10.9211
	step [17/249], loss=12.7077
	step [18/249], loss=13.9342
	step [19/249], loss=12.6679
	step [20/249], loss=10.6929
	step [21/249], loss=13.6925
	step [22/249], loss=12.6005
	step [23/249], loss=12.0183
	step [24/249], loss=12.8081
	step [25/249], loss=11.8339
	step [26/249], loss=11.4855
	step [27/249], loss=13.8783
	step [28/249], loss=12.1867
	step [29/249], loss=12.1184
	step [30/249], loss=13.5183
	step [31/249], loss=13.3233
	step [32/249], loss=14.0253
	step [33/249], loss=13.1267
	step [34/249], loss=11.9075
	step [35/249], loss=12.4525
	step [36/249], loss=11.3578
	step [37/249], loss=11.6719
	step [38/249], loss=13.4611
	step [39/249], loss=11.0199
	step [40/249], loss=10.9949
	step [41/249], loss=11.3232
	step [42/249], loss=11.6480
	step [43/249], loss=14.1579
	step [44/249], loss=11.9546
	step [45/249], loss=11.5996
	step [46/249], loss=13.1908
	step [47/249], loss=11.8976
	step [48/249], loss=11.4321
	step [49/249], loss=11.5227
	step [50/249], loss=12.3499
	step [51/249], loss=11.7717
	step [52/249], loss=11.0642
	step [53/249], loss=12.4602
	step [54/249], loss=13.3004
	step [55/249], loss=11.0324
	step [56/249], loss=12.0581
	step [57/249], loss=12.3789
	step [58/249], loss=11.4288
	step [59/249], loss=11.4079
	step [60/249], loss=13.0858
	step [61/249], loss=13.9818
	step [62/249], loss=10.1711
	step [63/249], loss=11.7982
	step [64/249], loss=11.6629
	step [65/249], loss=11.9559
	step [66/249], loss=11.9071
	step [67/249], loss=12.9349
	step [68/249], loss=12.8781
	step [69/249], loss=14.8807
	step [70/249], loss=11.8393
	step [71/249], loss=11.6405
	step [72/249], loss=13.1218
	step [73/249], loss=12.3894
	step [74/249], loss=12.8872
	step [75/249], loss=12.4123
	step [76/249], loss=11.5984
	step [77/249], loss=11.7666
	step [78/249], loss=11.3859
	step [79/249], loss=14.1325
	step [80/249], loss=11.7943
	step [81/249], loss=10.3230
	step [82/249], loss=13.3016
	step [83/249], loss=11.4213
	step [84/249], loss=10.2305
	step [85/249], loss=11.5946
	step [86/249], loss=12.6374
	step [87/249], loss=11.2791
	step [88/249], loss=12.4976
	step [89/249], loss=13.2643
	step [90/249], loss=11.9509
	step [91/249], loss=10.9549
	step [92/249], loss=10.6861
	step [93/249], loss=12.2122
	step [94/249], loss=13.2155
	step [95/249], loss=11.3079
	step [96/249], loss=11.3680
	step [97/249], loss=12.9278
	step [98/249], loss=11.9922
	step [99/249], loss=10.4124
	step [100/249], loss=13.0750
	step [101/249], loss=15.0647
	step [102/249], loss=11.5391
	step [103/249], loss=11.4865
	step [104/249], loss=9.9808
	step [105/249], loss=13.0509
	step [106/249], loss=11.1161
	step [107/249], loss=13.8679
	step [108/249], loss=12.1474
	step [109/249], loss=14.1532
	step [110/249], loss=12.1096
	step [111/249], loss=11.4816
	step [112/249], loss=15.2663
	step [113/249], loss=13.1141
	step [114/249], loss=12.7981
	step [115/249], loss=11.7741
	step [116/249], loss=14.1060
	step [117/249], loss=10.3788
	step [118/249], loss=12.2950
	step [119/249], loss=12.3248
	step [120/249], loss=10.7301
	step [121/249], loss=12.8947
	step [122/249], loss=14.1169
	step [123/249], loss=12.2882
	step [124/249], loss=13.7406
	step [125/249], loss=10.6734
	step [126/249], loss=9.6702
	step [127/249], loss=13.4382
	step [128/249], loss=11.3439
	step [129/249], loss=14.4804
	step [130/249], loss=9.4214
	step [131/249], loss=11.2750
	step [132/249], loss=11.2641
	step [133/249], loss=11.6533
	step [134/249], loss=12.4717
	step [135/249], loss=13.0281
	step [136/249], loss=10.9378
	step [137/249], loss=11.3437
	step [138/249], loss=12.7574
	step [139/249], loss=10.2065
	step [140/249], loss=8.4689
	step [141/249], loss=11.8265
	step [142/249], loss=12.4678
	step [143/249], loss=11.2884
	step [144/249], loss=13.9335
	step [145/249], loss=10.7465
	step [146/249], loss=14.8121
	step [147/249], loss=13.2112
	step [148/249], loss=12.7017
	step [149/249], loss=12.5996
	step [150/249], loss=10.2752
	step [151/249], loss=13.6563
	step [152/249], loss=12.0421
	step [153/249], loss=12.7254
	step [154/249], loss=12.2541
	step [155/249], loss=12.2243
	step [156/249], loss=12.8771
	step [157/249], loss=11.0607
	step [158/249], loss=14.3416
	step [159/249], loss=11.6044
	step [160/249], loss=10.5004
	step [161/249], loss=13.5958
	step [162/249], loss=11.2399
	step [163/249], loss=10.6617
	step [164/249], loss=11.1586
	step [165/249], loss=12.6240
	step [166/249], loss=11.9548
	step [167/249], loss=11.4614
	step [168/249], loss=12.7988
	step [169/249], loss=11.8613
	step [170/249], loss=11.5598
	step [171/249], loss=12.7162
	step [172/249], loss=11.7078
	step [173/249], loss=10.9034
	step [174/249], loss=11.0354
	step [175/249], loss=14.1516
	step [176/249], loss=12.2260
	step [177/249], loss=13.0695
	step [178/249], loss=11.3747
	step [179/249], loss=11.6326
	step [180/249], loss=10.3970
	step [181/249], loss=14.1607
	step [182/249], loss=13.4585
	step [183/249], loss=13.1975
	step [184/249], loss=10.6449
	step [185/249], loss=10.0421
	step [186/249], loss=11.4741
	step [187/249], loss=11.7004
	step [188/249], loss=13.3495
	step [189/249], loss=13.5331
	step [190/249], loss=13.0594
	step [191/249], loss=11.2867
	step [192/249], loss=12.4506
	step [193/249], loss=11.9401
	step [194/249], loss=11.5905
	step [195/249], loss=11.7254
	step [196/249], loss=11.8693
	step [197/249], loss=11.3825
	step [198/249], loss=8.1198
	step [199/249], loss=10.7889
	step [200/249], loss=12.2000
	step [201/249], loss=12.8156
	step [202/249], loss=13.2358
	step [203/249], loss=12.7346
	step [204/249], loss=14.3436
	step [205/249], loss=12.5915
	step [206/249], loss=11.1400
	step [207/249], loss=11.1930
	step [208/249], loss=12.2847
	step [209/249], loss=15.0591
	step [210/249], loss=13.0281
	step [211/249], loss=12.0826
	step [212/249], loss=12.0586
	step [213/249], loss=11.5955
	step [214/249], loss=11.9855
	step [215/249], loss=9.5134
	step [216/249], loss=9.7784
	step [217/249], loss=14.2135
	step [218/249], loss=11.6243
	step [219/249], loss=13.2242
	step [220/249], loss=10.7334
	step [221/249], loss=11.3062
	step [222/249], loss=10.8484
	step [223/249], loss=11.9119
	step [224/249], loss=11.3125
	step [225/249], loss=13.7990
	step [226/249], loss=12.4141
	step [227/249], loss=10.0596
	step [228/249], loss=11.5887
	step [229/249], loss=11.2611
	step [230/249], loss=11.7779
	step [231/249], loss=11.0088
	step [232/249], loss=11.9135
	step [233/249], loss=12.1946
	step [234/249], loss=12.8406
	step [235/249], loss=13.5925
	step [236/249], loss=13.6034
	step [237/249], loss=12.2201
	step [238/249], loss=10.4348
	step [239/249], loss=10.4903
	step [240/249], loss=12.6517
	step [241/249], loss=11.3571
	step [242/249], loss=13.0523
	step [243/249], loss=9.2795
	step [244/249], loss=11.7230
	step [245/249], loss=11.9938
	step [246/249], loss=12.1562
	step [247/249], loss=13.5287
	step [248/249], loss=11.4864
	step [249/249], loss=9.1658
	Evaluating
	loss=0.0320, precision=0.1478, recall=0.9946, f1=0.2574
Training epoch 13
	step [1/249], loss=11.6004
	step [2/249], loss=12.3663
	step [3/249], loss=10.9189
	step [4/249], loss=9.9457
	step [5/249], loss=14.4157
	step [6/249], loss=12.5428
	step [7/249], loss=12.2534
	step [8/249], loss=10.9908
	step [9/249], loss=10.2027
	step [10/249], loss=10.3452
	step [11/249], loss=11.1029
	step [12/249], loss=12.7480
	step [13/249], loss=12.8619
	step [14/249], loss=11.6137
	step [15/249], loss=12.2794
	step [16/249], loss=11.4073
	step [17/249], loss=14.5989
	step [18/249], loss=10.5855
	step [19/249], loss=11.2034
	step [20/249], loss=12.6605
	step [21/249], loss=12.3016
	step [22/249], loss=14.6759
	step [23/249], loss=10.1673
	step [24/249], loss=9.4272
	step [25/249], loss=11.8274
	step [26/249], loss=11.3830
	step [27/249], loss=11.3194
	step [28/249], loss=10.1992
	step [29/249], loss=11.4773
	step [30/249], loss=11.5005
	step [31/249], loss=12.5402
	step [32/249], loss=9.9416
	step [33/249], loss=11.0502
	step [34/249], loss=12.0819
	step [35/249], loss=12.5101
	step [36/249], loss=9.3432
	step [37/249], loss=11.0039
	step [38/249], loss=14.4975
	step [39/249], loss=11.6769
	step [40/249], loss=11.9561
	step [41/249], loss=11.2642
	step [42/249], loss=9.8322
	step [43/249], loss=11.5251
	step [44/249], loss=12.2049
	step [45/249], loss=11.0153
	step [46/249], loss=12.2212
	step [47/249], loss=10.8795
	step [48/249], loss=11.6512
	step [49/249], loss=11.3771
	step [50/249], loss=10.3373
	step [51/249], loss=11.4747
	step [52/249], loss=9.3739
	step [53/249], loss=13.6686
	step [54/249], loss=10.0464
	step [55/249], loss=10.7567
	step [56/249], loss=12.4995
	step [57/249], loss=11.4065
	step [58/249], loss=13.9949
	step [59/249], loss=10.4464
	step [60/249], loss=10.3886
	step [61/249], loss=14.7010
	step [62/249], loss=10.6639
	step [63/249], loss=10.8784
	step [64/249], loss=13.1857
	step [65/249], loss=12.1780
	step [66/249], loss=10.2844
	step [67/249], loss=11.0081
	step [68/249], loss=11.3301
	step [69/249], loss=11.6360
	step [70/249], loss=10.3807
	step [71/249], loss=12.1281
	step [72/249], loss=9.8662
	step [73/249], loss=14.2494
	step [74/249], loss=11.6992
	step [75/249], loss=10.7118
	step [76/249], loss=10.6264
	step [77/249], loss=10.9686
	step [78/249], loss=10.4203
	step [79/249], loss=10.5228
	step [80/249], loss=9.4703
	step [81/249], loss=13.4833
	step [82/249], loss=10.0076
	step [83/249], loss=12.5430
	step [84/249], loss=11.7357
	step [85/249], loss=9.4264
	step [86/249], loss=10.7748
	step [87/249], loss=13.5133
	step [88/249], loss=13.7011
	step [89/249], loss=11.3134
	step [90/249], loss=12.1867
	step [91/249], loss=11.8563
	step [92/249], loss=11.3074
	step [93/249], loss=13.2986
	step [94/249], loss=11.3558
	step [95/249], loss=9.9932
	step [96/249], loss=13.4767
	step [97/249], loss=12.8761
	step [98/249], loss=9.1167
	step [99/249], loss=11.2400
	step [100/249], loss=11.3829
	step [101/249], loss=12.1846
	step [102/249], loss=10.4719
	step [103/249], loss=12.0668
	step [104/249], loss=12.9257
	step [105/249], loss=11.3638
	step [106/249], loss=11.9735
	step [107/249], loss=12.2081
	step [108/249], loss=13.7051
	step [109/249], loss=11.8354
	step [110/249], loss=10.5108
	step [111/249], loss=11.7889
	step [112/249], loss=10.8070
	step [113/249], loss=12.8495
	step [114/249], loss=12.0705
	step [115/249], loss=11.6556
	step [116/249], loss=10.8063
	step [117/249], loss=12.5103
	step [118/249], loss=10.8092
	step [119/249], loss=11.3053
	step [120/249], loss=13.3304
	step [121/249], loss=10.4964
	step [122/249], loss=12.0249
	step [123/249], loss=11.4565
	step [124/249], loss=11.2423
	step [125/249], loss=12.7324
	step [126/249], loss=11.5545
	step [127/249], loss=10.6556
	step [128/249], loss=11.2339
	step [129/249], loss=10.7467
	step [130/249], loss=10.2849
	step [131/249], loss=11.3523
	step [132/249], loss=11.1133
	step [133/249], loss=9.3294
	step [134/249], loss=11.9044
	step [135/249], loss=12.7156
	step [136/249], loss=10.7316
	step [137/249], loss=10.3273
	step [138/249], loss=10.9033
	step [139/249], loss=12.6364
	step [140/249], loss=10.2656
	step [141/249], loss=10.9796
	step [142/249], loss=11.8507
	step [143/249], loss=12.0938
	step [144/249], loss=10.7649
	step [145/249], loss=11.3644
	step [146/249], loss=13.3437
	step [147/249], loss=10.0271
	step [148/249], loss=11.3410
	step [149/249], loss=14.2365
	step [150/249], loss=10.9116
	step [151/249], loss=10.4858
	step [152/249], loss=11.8426
	step [153/249], loss=11.3479
	step [154/249], loss=9.3000
	step [155/249], loss=9.6579
	step [156/249], loss=9.4859
	step [157/249], loss=12.1894
	step [158/249], loss=11.4017
	step [159/249], loss=11.2393
	step [160/249], loss=11.9211
	step [161/249], loss=11.3070
	step [162/249], loss=10.2084
	step [163/249], loss=10.5575
	step [164/249], loss=14.5273
	step [165/249], loss=12.5076
	step [166/249], loss=10.6114
	step [167/249], loss=10.3546
	step [168/249], loss=12.2042
	step [169/249], loss=11.3678
	step [170/249], loss=10.5513
	step [171/249], loss=9.4540
	step [172/249], loss=11.0038
	step [173/249], loss=12.9660
	step [174/249], loss=12.2044
	step [175/249], loss=13.0007
	step [176/249], loss=10.5862
	step [177/249], loss=10.7065
	step [178/249], loss=12.6219
	step [179/249], loss=10.0466
	step [180/249], loss=11.0250
	step [181/249], loss=9.6812
	step [182/249], loss=10.7660
	step [183/249], loss=12.1581
	step [184/249], loss=10.4748
	step [185/249], loss=10.6930
	step [186/249], loss=11.6151
	step [187/249], loss=10.8747
	step [188/249], loss=7.9882
	step [189/249], loss=11.3149
	step [190/249], loss=9.6141
	step [191/249], loss=8.7044
	step [192/249], loss=10.9687
	step [193/249], loss=9.3234
	step [194/249], loss=10.7014
	step [195/249], loss=11.1285
	step [196/249], loss=12.7957
	step [197/249], loss=10.8036
	step [198/249], loss=13.6720
	step [199/249], loss=10.5488
	step [200/249], loss=12.2076
	step [201/249], loss=10.4398
	step [202/249], loss=12.6650
	step [203/249], loss=10.3503
	step [204/249], loss=10.8552
	step [205/249], loss=11.5797
	step [206/249], loss=11.1227
	step [207/249], loss=14.3660
	step [208/249], loss=11.4283
	step [209/249], loss=9.2008
	step [210/249], loss=9.9074
	step [211/249], loss=10.0772
	step [212/249], loss=12.5394
	step [213/249], loss=10.2668
	step [214/249], loss=11.9368
	step [215/249], loss=10.0630
	step [216/249], loss=13.7899
	step [217/249], loss=9.2707
	step [218/249], loss=9.5727
	step [219/249], loss=11.8084
	step [220/249], loss=11.5815
	step [221/249], loss=13.5333
	step [222/249], loss=10.5608
	step [223/249], loss=10.8958
	step [224/249], loss=9.3458
	step [225/249], loss=8.9879
	step [226/249], loss=13.7352
	step [227/249], loss=8.4643
	step [228/249], loss=10.4834
	step [229/249], loss=10.5589
	step [230/249], loss=10.6773
	step [231/249], loss=11.8419
	step [232/249], loss=12.2661
	step [233/249], loss=10.8091
	step [234/249], loss=11.1592
	step [235/249], loss=12.2325
	step [236/249], loss=12.0510
	step [237/249], loss=12.5636
	step [238/249], loss=10.8475
	step [239/249], loss=11.0295
	step [240/249], loss=11.6621
	step [241/249], loss=13.3374
	step [242/249], loss=11.7016
	step [243/249], loss=10.6912
	step [244/249], loss=10.5387
	step [245/249], loss=13.8043
	step [246/249], loss=12.2805
	step [247/249], loss=14.8026
	step [248/249], loss=9.9682
	step [249/249], loss=10.0919
	Evaluating
	loss=0.0319, precision=0.1312, recall=0.9948, f1=0.2319
Training epoch 14
	step [1/249], loss=12.4728
	step [2/249], loss=12.4647
	step [3/249], loss=11.4736
	step [4/249], loss=9.6254
	step [5/249], loss=11.2902
	step [6/249], loss=11.8045
	step [7/249], loss=12.1350
	step [8/249], loss=10.4573
	step [9/249], loss=11.1143
	step [10/249], loss=12.1852
	step [11/249], loss=10.9585
	step [12/249], loss=11.1935
	step [13/249], loss=10.6229
	step [14/249], loss=9.6236
	step [15/249], loss=10.3973
	step [16/249], loss=9.8618
	step [17/249], loss=13.4105
	step [18/249], loss=10.6846
	step [19/249], loss=12.0462
	step [20/249], loss=9.3621
	step [21/249], loss=11.4369
	step [22/249], loss=13.0211
	step [23/249], loss=11.1144
	step [24/249], loss=11.5989
	step [25/249], loss=11.0673
	step [26/249], loss=10.2405
	step [27/249], loss=11.5861
	step [28/249], loss=10.7376
	step [29/249], loss=11.8626
	step [30/249], loss=13.7502
	step [31/249], loss=11.1147
	step [32/249], loss=11.3808
	step [33/249], loss=13.0402
	step [34/249], loss=10.5818
	step [35/249], loss=10.5742
	step [36/249], loss=11.1602
	step [37/249], loss=11.1785
	step [38/249], loss=10.4475
	step [39/249], loss=12.0905
	step [40/249], loss=10.4971
	step [41/249], loss=11.7672
	step [42/249], loss=12.4515
	step [43/249], loss=10.6887
	step [44/249], loss=10.0652
	step [45/249], loss=11.5393
	step [46/249], loss=10.8500
	step [47/249], loss=10.7727
	step [48/249], loss=10.1979
	step [49/249], loss=10.8110
	step [50/249], loss=9.9746
	step [51/249], loss=11.7772
	step [52/249], loss=10.7413
	step [53/249], loss=10.2149
	step [54/249], loss=10.5782
	step [55/249], loss=8.6903
	step [56/249], loss=8.9877
	step [57/249], loss=11.5724
	step [58/249], loss=10.6558
	step [59/249], loss=9.0058
	step [60/249], loss=11.5928
	step [61/249], loss=12.6472
	step [62/249], loss=10.1829
	step [63/249], loss=11.8717
	step [64/249], loss=9.2584
	step [65/249], loss=11.4564
	step [66/249], loss=9.2613
	step [67/249], loss=13.1275
	step [68/249], loss=12.5398
	step [69/249], loss=11.1101
	step [70/249], loss=10.4336
	step [71/249], loss=10.4201
	step [72/249], loss=9.9774
	step [73/249], loss=9.6601
	step [74/249], loss=10.5345
	step [75/249], loss=9.6287
	step [76/249], loss=12.5469
	step [77/249], loss=10.7306
	step [78/249], loss=8.1518
	step [79/249], loss=11.4327
	step [80/249], loss=12.2619
	step [81/249], loss=10.3866
	step [82/249], loss=12.9326
	step [83/249], loss=13.1425
	step [84/249], loss=12.4811
	step [85/249], loss=11.6494
	step [86/249], loss=9.3803
	step [87/249], loss=10.6150
	step [88/249], loss=8.3273
	step [89/249], loss=11.6063
	step [90/249], loss=11.3786
	step [91/249], loss=10.1479
	step [92/249], loss=9.6987
	step [93/249], loss=11.0610
	step [94/249], loss=10.3984
	step [95/249], loss=8.9417
	step [96/249], loss=11.6549
	step [97/249], loss=11.0263
	step [98/249], loss=10.1762
	step [99/249], loss=11.0900
	step [100/249], loss=11.0268
	step [101/249], loss=13.0096
	step [102/249], loss=10.5400
	step [103/249], loss=9.0963
	step [104/249], loss=8.8566
	step [105/249], loss=10.4496
	step [106/249], loss=11.7126
	step [107/249], loss=10.9301
	step [108/249], loss=10.2711
	step [109/249], loss=10.7806
	step [110/249], loss=9.9048
	step [111/249], loss=14.0378
	step [112/249], loss=11.0670
	step [113/249], loss=11.3925
	step [114/249], loss=9.3675
	step [115/249], loss=9.6820
	step [116/249], loss=11.9352
	step [117/249], loss=10.0742
	step [118/249], loss=9.3686
	step [119/249], loss=10.9669
	step [120/249], loss=11.5343
	step [121/249], loss=10.1825
	step [122/249], loss=9.5733
	step [123/249], loss=11.8004
	step [124/249], loss=11.3222
	step [125/249], loss=9.3313
	step [126/249], loss=10.2987
	step [127/249], loss=12.3035
	step [128/249], loss=10.9192
	step [129/249], loss=10.6256
	step [130/249], loss=10.3227
	step [131/249], loss=8.7930
	step [132/249], loss=9.9626
	step [133/249], loss=12.0864
	step [134/249], loss=11.5868
	step [135/249], loss=11.5828
	step [136/249], loss=9.3858
	step [137/249], loss=11.0682
	step [138/249], loss=8.6245
	step [139/249], loss=12.6214
	step [140/249], loss=13.0080
	step [141/249], loss=8.7327
	step [142/249], loss=11.9865
	step [143/249], loss=15.1172
	step [144/249], loss=9.2792
	step [145/249], loss=9.5658
	step [146/249], loss=10.9315
	step [147/249], loss=10.8412
	step [148/249], loss=10.5895
	step [149/249], loss=8.5806
	step [150/249], loss=10.5912
	step [151/249], loss=10.0901
	step [152/249], loss=10.2488
	step [153/249], loss=11.9177
	step [154/249], loss=9.6619
	step [155/249], loss=8.8912
	step [156/249], loss=12.1978
	step [157/249], loss=9.5476
	step [158/249], loss=12.0933
	step [159/249], loss=8.2563
	step [160/249], loss=10.2531
	step [161/249], loss=10.6174
	step [162/249], loss=9.8606
	step [163/249], loss=10.1497
	step [164/249], loss=11.3720
	step [165/249], loss=11.8609
	step [166/249], loss=11.1160
	step [167/249], loss=10.4326
	step [168/249], loss=9.5450
	step [169/249], loss=11.3323
	step [170/249], loss=9.6725
	step [171/249], loss=12.4042
	step [172/249], loss=10.7022
	step [173/249], loss=11.2303
	step [174/249], loss=11.0607
	step [175/249], loss=12.4885
	step [176/249], loss=10.0670
	step [177/249], loss=11.0900
	step [178/249], loss=11.4516
	step [179/249], loss=9.7910
	step [180/249], loss=10.6286
	step [181/249], loss=11.2386
	step [182/249], loss=11.0337
	step [183/249], loss=9.6775
	step [184/249], loss=10.2542
	step [185/249], loss=8.2173
	step [186/249], loss=10.1673
	step [187/249], loss=11.4126
	step [188/249], loss=11.2872
	step [189/249], loss=10.2188
	step [190/249], loss=13.0410
	step [191/249], loss=13.4215
	step [192/249], loss=9.5700
	step [193/249], loss=10.4532
	step [194/249], loss=9.7385
	step [195/249], loss=11.0565
	step [196/249], loss=11.4545
	step [197/249], loss=10.3859
	step [198/249], loss=11.2352
	step [199/249], loss=8.9571
	step [200/249], loss=13.5836
	step [201/249], loss=10.2621
	step [202/249], loss=10.0733
	step [203/249], loss=8.0691
	step [204/249], loss=9.6166
	step [205/249], loss=9.7595
	step [206/249], loss=9.5888
	step [207/249], loss=10.2942
	step [208/249], loss=9.1632
	step [209/249], loss=12.7069
	step [210/249], loss=8.6461
	step [211/249], loss=10.3634
	step [212/249], loss=8.6743
	step [213/249], loss=9.8810
	step [214/249], loss=9.8394
	step [215/249], loss=9.1202
	step [216/249], loss=9.2212
	step [217/249], loss=10.4757
	step [218/249], loss=10.4857
	step [219/249], loss=10.2416
	step [220/249], loss=9.7455
	step [221/249], loss=10.7468
	step [222/249], loss=14.7089
	step [223/249], loss=11.9493
	step [224/249], loss=10.9857
	step [225/249], loss=12.9386
	step [226/249], loss=10.3565
	step [227/249], loss=10.3565
	step [228/249], loss=10.6230
	step [229/249], loss=9.5085
	step [230/249], loss=10.9535
	step [231/249], loss=10.8375
	step [232/249], loss=9.8009
	step [233/249], loss=12.6974
	step [234/249], loss=10.3484
	step [235/249], loss=7.9401
	step [236/249], loss=10.0632
	step [237/249], loss=12.6251
	step [238/249], loss=8.2344
	step [239/249], loss=11.3866
	step [240/249], loss=9.4939
	step [241/249], loss=12.9182
	step [242/249], loss=11.5077
	step [243/249], loss=9.8062
	step [244/249], loss=8.7390
	step [245/249], loss=11.5395
	step [246/249], loss=9.5179
	step [247/249], loss=11.7920
	step [248/249], loss=8.5902
	step [249/249], loss=6.8099
	Evaluating
	loss=0.0290, precision=0.1533, recall=0.9936, f1=0.2656
Training epoch 15
	step [1/249], loss=10.8427
	step [2/249], loss=10.0421
	step [3/249], loss=7.6578
	step [4/249], loss=10.6862
	step [5/249], loss=11.1227
	step [6/249], loss=8.3937
	step [7/249], loss=13.5736
	step [8/249], loss=10.8410
	step [9/249], loss=8.7906
	step [10/249], loss=11.9140
	step [11/249], loss=11.8106
	step [12/249], loss=11.1464
	step [13/249], loss=12.7031
	step [14/249], loss=9.6650
	step [15/249], loss=9.0111
	step [16/249], loss=8.9525
	step [17/249], loss=8.7862
	step [18/249], loss=9.8671
	step [19/249], loss=8.3130
	step [20/249], loss=12.6254
	step [21/249], loss=10.5394
	step [22/249], loss=9.4105
	step [23/249], loss=10.7233
	step [24/249], loss=11.7591
	step [25/249], loss=9.9806
	step [26/249], loss=10.5245
	step [27/249], loss=11.0933
	step [28/249], loss=10.8876
	step [29/249], loss=9.7833
	step [30/249], loss=8.1640
	step [31/249], loss=10.7521
	step [32/249], loss=10.1061
	step [33/249], loss=10.5628
	step [34/249], loss=11.0617
	step [35/249], loss=8.7676
	step [36/249], loss=10.1763
	step [37/249], loss=10.2168
	step [38/249], loss=8.3773
	step [39/249], loss=11.7490
	step [40/249], loss=10.8866
	step [41/249], loss=12.0991
	step [42/249], loss=8.0417
	step [43/249], loss=11.1702
	step [44/249], loss=11.0857
	step [45/249], loss=12.2404
	step [46/249], loss=9.1074
	step [47/249], loss=9.9541
	step [48/249], loss=10.7613
	step [49/249], loss=11.4584
	step [50/249], loss=10.7539
	step [51/249], loss=11.9870
	step [52/249], loss=10.3295
	step [53/249], loss=9.2031
	step [54/249], loss=9.4549
	step [55/249], loss=10.0132
	step [56/249], loss=10.1029
	step [57/249], loss=10.2629
	step [58/249], loss=12.0151
	step [59/249], loss=9.4956
	step [60/249], loss=8.0346
	step [61/249], loss=11.8783
	step [62/249], loss=10.8753
	step [63/249], loss=10.6928
	step [64/249], loss=12.5786
	step [65/249], loss=11.8151
	step [66/249], loss=10.1220
	step [67/249], loss=11.5784
	step [68/249], loss=8.9853
	step [69/249], loss=12.5706
	step [70/249], loss=11.0640
	step [71/249], loss=10.6308
	step [72/249], loss=10.6154
	step [73/249], loss=10.6621
	step [74/249], loss=11.2922
	step [75/249], loss=11.7366
	step [76/249], loss=9.9936
	step [77/249], loss=9.8815
	step [78/249], loss=10.3038
	step [79/249], loss=8.7690
	step [80/249], loss=10.7582
	step [81/249], loss=9.2652
	step [82/249], loss=8.4316
	step [83/249], loss=10.5761
	step [84/249], loss=10.5308
	step [85/249], loss=9.2665
	step [86/249], loss=11.0629
	step [87/249], loss=10.7306
	step [88/249], loss=11.2189
	step [89/249], loss=9.4326
	step [90/249], loss=9.9558
	step [91/249], loss=11.1173
	step [92/249], loss=11.2361
	step [93/249], loss=10.5449
	step [94/249], loss=8.9307
	step [95/249], loss=11.2049
	step [96/249], loss=9.7717
	step [97/249], loss=9.3132
	step [98/249], loss=10.6890
	step [99/249], loss=12.0178
	step [100/249], loss=8.9699
	step [101/249], loss=10.0212
	step [102/249], loss=11.2094
	step [103/249], loss=10.8394
	step [104/249], loss=9.5982
	step [105/249], loss=10.8768
	step [106/249], loss=11.1204
	step [107/249], loss=11.0175
	step [108/249], loss=10.7240
	step [109/249], loss=9.7745
	step [110/249], loss=10.3683
	step [111/249], loss=8.3703
	step [112/249], loss=8.6413
	step [113/249], loss=9.2483
	step [114/249], loss=11.1777
	step [115/249], loss=9.5631
	step [116/249], loss=9.1410
	step [117/249], loss=9.7440
	step [118/249], loss=8.2848
	step [119/249], loss=9.4502
	step [120/249], loss=9.1346
	step [121/249], loss=9.8665
	step [122/249], loss=14.9662
	step [123/249], loss=10.8530
	step [124/249], loss=11.3108
	step [125/249], loss=8.4014
	step [126/249], loss=9.5175
	step [127/249], loss=10.6702
	step [128/249], loss=11.4259
	step [129/249], loss=9.8336
	step [130/249], loss=9.2603
	step [131/249], loss=10.6693
	step [132/249], loss=8.7021
	step [133/249], loss=10.2437
	step [134/249], loss=8.4666
	step [135/249], loss=11.0466
	step [136/249], loss=9.0997
	step [137/249], loss=10.2951
	step [138/249], loss=8.7685
	step [139/249], loss=10.8605
	step [140/249], loss=7.9629
	step [141/249], loss=9.3063
	step [142/249], loss=10.1254
	step [143/249], loss=11.5884
	step [144/249], loss=11.6315
	step [145/249], loss=11.4388
	step [146/249], loss=10.1551
	step [147/249], loss=9.6734
	step [148/249], loss=8.9501
	step [149/249], loss=8.1321
	step [150/249], loss=11.6286
	step [151/249], loss=8.2252
	step [152/249], loss=9.9610
	step [153/249], loss=11.3781
	step [154/249], loss=11.7459
	step [155/249], loss=9.9021
	step [156/249], loss=9.1441
	step [157/249], loss=10.3765
	step [158/249], loss=9.8569
	step [159/249], loss=9.9596
	step [160/249], loss=8.7173
	step [161/249], loss=11.5867
	step [162/249], loss=10.7361
	step [163/249], loss=10.2768
	step [164/249], loss=8.8760
	step [165/249], loss=10.7970
	step [166/249], loss=9.6323
	step [167/249], loss=11.7951
	step [168/249], loss=8.7021
	step [169/249], loss=10.1933
	step [170/249], loss=9.2615
	step [171/249], loss=9.6585
	step [172/249], loss=11.7464
	step [173/249], loss=9.7913
	step [174/249], loss=10.2739
	step [175/249], loss=10.4043
	step [176/249], loss=11.2137
	step [177/249], loss=9.2644
	step [178/249], loss=12.0904
	step [179/249], loss=9.5866
	step [180/249], loss=10.6026
	step [181/249], loss=10.4311
	step [182/249], loss=10.1914
	step [183/249], loss=8.0097
	step [184/249], loss=10.6821
	step [185/249], loss=8.5785
	step [186/249], loss=8.5468
	step [187/249], loss=9.7540
	step [188/249], loss=9.8862
	step [189/249], loss=10.6939
	step [190/249], loss=11.0232
	step [191/249], loss=10.9064
	step [192/249], loss=9.4435
	step [193/249], loss=10.0626
	step [194/249], loss=9.3806
	step [195/249], loss=9.6322
	step [196/249], loss=9.3318
	step [197/249], loss=10.4283
	step [198/249], loss=12.1490
	step [199/249], loss=7.7214
	step [200/249], loss=9.0034
	step [201/249], loss=9.0655
	step [202/249], loss=10.0881
	step [203/249], loss=9.8047
	step [204/249], loss=9.8268
	step [205/249], loss=9.8942
	step [206/249], loss=11.2786
	step [207/249], loss=10.5925
	step [208/249], loss=9.2693
	step [209/249], loss=9.9083
	step [210/249], loss=9.6927
	step [211/249], loss=9.9198
	step [212/249], loss=9.7184
	step [213/249], loss=10.2019
	step [214/249], loss=11.1907
	step [215/249], loss=10.5039
	step [216/249], loss=10.3918
	step [217/249], loss=9.9738
	step [218/249], loss=8.3143
	step [219/249], loss=8.4735
	step [220/249], loss=7.7284
	step [221/249], loss=8.7611
	step [222/249], loss=10.8660
	step [223/249], loss=11.1794
	step [224/249], loss=10.0277
	step [225/249], loss=9.0427
	step [226/249], loss=11.1240
	step [227/249], loss=10.0603
	step [228/249], loss=8.6806
	step [229/249], loss=8.9010
	step [230/249], loss=11.2049
	step [231/249], loss=8.8782
	step [232/249], loss=10.2454
	step [233/249], loss=10.0872
	step [234/249], loss=10.1431
	step [235/249], loss=10.5970
	step [236/249], loss=9.9129
	step [237/249], loss=10.4860
	step [238/249], loss=10.0654
	step [239/249], loss=9.4325
	step [240/249], loss=9.4295
	step [241/249], loss=10.9522
	step [242/249], loss=9.2833
	step [243/249], loss=7.4451
	step [244/249], loss=11.1168
	step [245/249], loss=9.7173
	step [246/249], loss=10.5903
	step [247/249], loss=8.8212
	step [248/249], loss=9.4679
	step [249/249], loss=6.1718
	Evaluating
	loss=0.0265, precision=0.1567, recall=0.9941, f1=0.2707
Training epoch 16
	step [1/249], loss=9.7449
	step [2/249], loss=9.8758
	step [3/249], loss=9.3017
	step [4/249], loss=9.3112
	step [5/249], loss=8.5711
	step [6/249], loss=7.5254
	step [7/249], loss=9.7784
	step [8/249], loss=9.1894
	step [9/249], loss=9.1834
	step [10/249], loss=8.8726
	step [11/249], loss=8.7492
	step [12/249], loss=8.3212
	step [13/249], loss=8.4326
	step [14/249], loss=10.0471
	step [15/249], loss=9.9276
	step [16/249], loss=8.9648
	step [17/249], loss=10.1306
	step [18/249], loss=9.9955
	step [19/249], loss=9.0684
	step [20/249], loss=8.5285
	step [21/249], loss=9.0966
	step [22/249], loss=9.3851
	step [23/249], loss=10.6133
	step [24/249], loss=9.3948
	step [25/249], loss=9.2205
	step [26/249], loss=10.4487
	step [27/249], loss=9.4606
	step [28/249], loss=10.4768
	step [29/249], loss=7.4642
	step [30/249], loss=11.0911
	step [31/249], loss=9.4892
	step [32/249], loss=9.3139
	step [33/249], loss=7.9456
	step [34/249], loss=8.4775
	step [35/249], loss=10.3166
	step [36/249], loss=9.2288
	step [37/249], loss=8.3937
	step [38/249], loss=9.7028
	step [39/249], loss=9.8708
	step [40/249], loss=13.4647
	step [41/249], loss=10.6622
	step [42/249], loss=9.4036
	step [43/249], loss=12.1318
	step [44/249], loss=11.3055
	step [45/249], loss=10.4105
	step [46/249], loss=12.4874
	step [47/249], loss=9.7081
	step [48/249], loss=10.2358
	step [49/249], loss=9.5614
	step [50/249], loss=8.6381
	step [51/249], loss=11.7782
	step [52/249], loss=8.1381
	step [53/249], loss=9.4385
	step [54/249], loss=9.4777
	step [55/249], loss=7.9426
	step [56/249], loss=9.6238
	step [57/249], loss=10.6264
	step [58/249], loss=9.3377
	step [59/249], loss=10.4772
	step [60/249], loss=12.1627
	step [61/249], loss=10.3573
	step [62/249], loss=10.6525
	step [63/249], loss=9.3410
	step [64/249], loss=9.3495
	step [65/249], loss=11.6096
	step [66/249], loss=8.4871
	step [67/249], loss=8.8746
	step [68/249], loss=9.0035
	step [69/249], loss=11.3018
	step [70/249], loss=9.6305
	step [71/249], loss=8.6900
	step [72/249], loss=7.7300
	step [73/249], loss=12.1954
	step [74/249], loss=11.5145
	step [75/249], loss=10.0255
	step [76/249], loss=11.1609
	step [77/249], loss=10.9170
	step [78/249], loss=9.2173
	step [79/249], loss=9.2429
	step [80/249], loss=8.3628
	step [81/249], loss=9.2958
	step [82/249], loss=11.5802
	step [83/249], loss=9.7618
	step [84/249], loss=8.9248
	step [85/249], loss=8.7821
	step [86/249], loss=10.8420
	step [87/249], loss=10.7088
	step [88/249], loss=10.7270
	step [89/249], loss=9.5242
	step [90/249], loss=9.3387
	step [91/249], loss=9.3053
	step [92/249], loss=10.1450
	step [93/249], loss=8.7044
	step [94/249], loss=10.3608
	step [95/249], loss=9.2879
	step [96/249], loss=11.3328
	step [97/249], loss=8.9767
	step [98/249], loss=8.5328
	step [99/249], loss=9.9228
	step [100/249], loss=10.8913
	step [101/249], loss=10.0448
	step [102/249], loss=8.9007
	step [103/249], loss=8.7569
	step [104/249], loss=9.4741
	step [105/249], loss=9.3814
	step [106/249], loss=10.3091
	step [107/249], loss=10.4400
	step [108/249], loss=8.3278
	step [109/249], loss=9.3102
	step [110/249], loss=10.7328
	step [111/249], loss=9.6330
	step [112/249], loss=10.0156
	step [113/249], loss=9.6306
	step [114/249], loss=10.1609
	step [115/249], loss=8.5376
	step [116/249], loss=9.1122
	step [117/249], loss=8.7418
	step [118/249], loss=10.5386
	step [119/249], loss=9.5453
	step [120/249], loss=9.0196
	step [121/249], loss=10.0907
	step [122/249], loss=9.8921
	step [123/249], loss=8.0597
	step [124/249], loss=9.5882
	step [125/249], loss=6.9165
	step [126/249], loss=8.2426
	step [127/249], loss=12.2538
	step [128/249], loss=8.6574
	step [129/249], loss=9.9935
	step [130/249], loss=7.9021
	step [131/249], loss=7.8940
	step [132/249], loss=10.8797
	step [133/249], loss=9.7556
	step [134/249], loss=10.7682
	step [135/249], loss=7.5153
	step [136/249], loss=9.7523
	step [137/249], loss=9.6074
	step [138/249], loss=12.8673
	step [139/249], loss=9.1800
	step [140/249], loss=10.2279
	step [141/249], loss=10.3319
	step [142/249], loss=11.9753
	step [143/249], loss=9.7605
	step [144/249], loss=12.9353
	step [145/249], loss=8.4765
	step [146/249], loss=9.4841
	step [147/249], loss=9.0873
	step [148/249], loss=10.5518
	step [149/249], loss=8.4993
	step [150/249], loss=9.9726
	step [151/249], loss=9.4037
	step [152/249], loss=8.7465
	step [153/249], loss=10.5474
	step [154/249], loss=8.6052
	step [155/249], loss=8.3218
	step [156/249], loss=9.2033
	step [157/249], loss=8.2068
	step [158/249], loss=9.8682
	step [159/249], loss=11.4651
	step [160/249], loss=10.2954
	step [161/249], loss=10.8297
	step [162/249], loss=8.8302
	step [163/249], loss=7.7008
	step [164/249], loss=9.8629
	step [165/249], loss=9.9206
	step [166/249], loss=9.7691
	step [167/249], loss=10.2941
	step [168/249], loss=8.0236
	step [169/249], loss=8.4302
	step [170/249], loss=8.3938
	step [171/249], loss=10.4126
	step [172/249], loss=11.5617
	step [173/249], loss=9.9532
	step [174/249], loss=9.6016
	step [175/249], loss=10.9956
	step [176/249], loss=10.0033
	step [177/249], loss=8.6724
	step [178/249], loss=8.2037
	step [179/249], loss=8.9185
	step [180/249], loss=8.7742
	step [181/249], loss=10.1941
	step [182/249], loss=9.2994
	step [183/249], loss=10.0881
	step [184/249], loss=9.4724
	step [185/249], loss=10.5740
	step [186/249], loss=7.8629
	step [187/249], loss=7.9740
	step [188/249], loss=10.7153
	step [189/249], loss=9.2962
	step [190/249], loss=8.3184
	step [191/249], loss=8.5403
	step [192/249], loss=9.0999
	step [193/249], loss=8.3123
	step [194/249], loss=10.1383
	step [195/249], loss=9.1328
	step [196/249], loss=9.7009
	step [197/249], loss=9.5396
	step [198/249], loss=10.6120
	step [199/249], loss=10.3716
	step [200/249], loss=8.7023
	step [201/249], loss=10.1433
	step [202/249], loss=10.6918
	step [203/249], loss=8.5660
	step [204/249], loss=7.7890
	step [205/249], loss=10.4426
	step [206/249], loss=10.8407
	step [207/249], loss=9.3384
	step [208/249], loss=9.0099
	step [209/249], loss=8.9986
	step [210/249], loss=9.5196
	step [211/249], loss=9.3195
	step [212/249], loss=10.0076
	step [213/249], loss=9.1349
	step [214/249], loss=8.8485
	step [215/249], loss=9.1522
	step [216/249], loss=9.7484
	step [217/249], loss=11.3432
	step [218/249], loss=9.2525
	step [219/249], loss=9.6018
	step [220/249], loss=8.4277
	step [221/249], loss=8.9945
	step [222/249], loss=9.5069
	step [223/249], loss=8.3249
	step [224/249], loss=9.1950
	step [225/249], loss=8.3628
	step [226/249], loss=9.2149
	step [227/249], loss=11.6325
	step [228/249], loss=7.7006
	step [229/249], loss=10.7073
	step [230/249], loss=8.7106
	step [231/249], loss=10.5424
	step [232/249], loss=9.5932
	step [233/249], loss=9.4750
	step [234/249], loss=9.9425
	step [235/249], loss=10.0244
	step [236/249], loss=10.2614
	step [237/249], loss=9.2648
	step [238/249], loss=8.5566
	step [239/249], loss=10.2005
	step [240/249], loss=10.2107
	step [241/249], loss=10.8651
	step [242/249], loss=7.2360
	step [243/249], loss=9.4962
	step [244/249], loss=8.3358
	step [245/249], loss=9.3673
	step [246/249], loss=9.6088
	step [247/249], loss=8.7214
	step [248/249], loss=10.2652
	step [249/249], loss=5.9797
	Evaluating
	loss=0.0262, precision=0.1566, recall=0.9940, f1=0.2705
Training epoch 17
	step [1/249], loss=8.4293
	step [2/249], loss=7.7996
	step [3/249], loss=10.2428
	step [4/249], loss=9.0969
	step [5/249], loss=8.0045
	step [6/249], loss=8.8399
	step [7/249], loss=8.6763
	step [8/249], loss=7.9805
	step [9/249], loss=7.7171
	step [10/249], loss=8.3568
	step [11/249], loss=10.4291
	step [12/249], loss=9.3189
	step [13/249], loss=10.2699
	step [14/249], loss=9.8061
	step [15/249], loss=8.9616
	step [16/249], loss=8.5594
	step [17/249], loss=7.3889
	step [18/249], loss=8.6494
	step [19/249], loss=9.4710
	step [20/249], loss=9.4718
	step [21/249], loss=9.0795
	step [22/249], loss=9.6912
	step [23/249], loss=9.3767
	step [24/249], loss=9.1539
	step [25/249], loss=8.3259
	step [26/249], loss=9.7423
	step [27/249], loss=9.9735
	step [28/249], loss=9.8809
	step [29/249], loss=9.5284
	step [30/249], loss=9.8912
	step [31/249], loss=8.5508
	step [32/249], loss=10.5879
	step [33/249], loss=8.5971
	step [34/249], loss=9.4743
	step [35/249], loss=9.0114
	step [36/249], loss=8.7545
	step [37/249], loss=8.4764
	step [38/249], loss=10.2107
	step [39/249], loss=12.1266
	step [40/249], loss=9.0129
	step [41/249], loss=8.5255
	step [42/249], loss=8.7774
	step [43/249], loss=9.6924
	step [44/249], loss=10.4047
	step [45/249], loss=8.2409
	step [46/249], loss=9.9254
	step [47/249], loss=9.6723
	step [48/249], loss=9.4014
	step [49/249], loss=8.2329
	step [50/249], loss=9.2575
	step [51/249], loss=9.9429
	step [52/249], loss=8.2732
	step [53/249], loss=8.3013
	step [54/249], loss=9.0588
	step [55/249], loss=8.8898
	step [56/249], loss=9.5665
	step [57/249], loss=9.5742
	step [58/249], loss=12.9126
	step [59/249], loss=7.6306
	step [60/249], loss=9.1534
	step [61/249], loss=7.6531
	step [62/249], loss=10.4390
	step [63/249], loss=9.3741
	step [64/249], loss=10.6106
	step [65/249], loss=9.2814
	step [66/249], loss=8.9106
	step [67/249], loss=9.0950
	step [68/249], loss=10.6906
	step [69/249], loss=8.5972
	step [70/249], loss=9.3811
	step [71/249], loss=8.1162
	step [72/249], loss=10.1784
	step [73/249], loss=9.9367
	step [74/249], loss=8.4215
	step [75/249], loss=9.4555
	step [76/249], loss=9.9138
	step [77/249], loss=9.5022
	step [78/249], loss=9.3268
	step [79/249], loss=7.6881
	step [80/249], loss=8.5059
	step [81/249], loss=8.0040
	step [82/249], loss=9.4278
	step [83/249], loss=8.9019
	step [84/249], loss=8.4034
	step [85/249], loss=11.8643
	step [86/249], loss=9.6327
	step [87/249], loss=7.8600
	step [88/249], loss=9.8423
	step [89/249], loss=9.1329
	step [90/249], loss=9.8363
	step [91/249], loss=8.3030
	step [92/249], loss=9.6817
	step [93/249], loss=10.6586
	step [94/249], loss=9.4719
	step [95/249], loss=8.8140
	step [96/249], loss=11.0037
	step [97/249], loss=7.3338
	step [98/249], loss=7.4625
	step [99/249], loss=9.3899
	step [100/249], loss=9.9630
	step [101/249], loss=7.5162
	step [102/249], loss=8.3123
	step [103/249], loss=9.2638
	step [104/249], loss=7.6134
	step [105/249], loss=7.9483
	step [106/249], loss=8.7353
	step [107/249], loss=10.7632
	step [108/249], loss=9.8983
	step [109/249], loss=8.2147
	step [110/249], loss=7.8910
	step [111/249], loss=9.0225
	step [112/249], loss=9.6038
	step [113/249], loss=10.9542
	step [114/249], loss=6.9331
	step [115/249], loss=7.3007
	step [116/249], loss=8.3059
	step [117/249], loss=8.8894
	step [118/249], loss=12.1343
	step [119/249], loss=10.6704
	step [120/249], loss=7.8353
	step [121/249], loss=9.0423
	step [122/249], loss=11.4128
	step [123/249], loss=9.9607
	step [124/249], loss=10.2302
	step [125/249], loss=9.8327
	step [126/249], loss=11.0106
	step [127/249], loss=9.0294
	step [128/249], loss=9.3938
	step [129/249], loss=8.9073
	step [130/249], loss=9.3432
	step [131/249], loss=9.0282
	step [132/249], loss=8.6712
	step [133/249], loss=9.9290
	step [134/249], loss=8.7754
	step [135/249], loss=10.6780
	step [136/249], loss=9.2104
	step [137/249], loss=8.8997
	step [138/249], loss=7.2090
	step [139/249], loss=11.1956
	step [140/249], loss=8.9127
	step [141/249], loss=8.3596
	step [142/249], loss=10.5845
	step [143/249], loss=9.5146
	step [144/249], loss=7.8945
	step [145/249], loss=9.4021
	step [146/249], loss=9.6021
	step [147/249], loss=10.4410
	step [148/249], loss=7.0216
	step [149/249], loss=8.3330
	step [150/249], loss=7.5508
	step [151/249], loss=7.4461
	step [152/249], loss=9.3402
	step [153/249], loss=7.9938
	step [154/249], loss=8.0210
	step [155/249], loss=8.1418
	step [156/249], loss=10.4997
	step [157/249], loss=9.7889
	step [158/249], loss=9.0332
	step [159/249], loss=10.0762
	step [160/249], loss=10.2042
	step [161/249], loss=8.0339
	step [162/249], loss=9.3470
	step [163/249], loss=8.1880
	step [164/249], loss=8.3010
	step [165/249], loss=10.6668
	step [166/249], loss=7.9244
	step [167/249], loss=9.8209
	step [168/249], loss=9.9128
	step [169/249], loss=8.5722
	step [170/249], loss=10.2193
	step [171/249], loss=9.1523
	step [172/249], loss=9.4678
	step [173/249], loss=8.9228
	step [174/249], loss=9.5452
	step [175/249], loss=10.2531
	step [176/249], loss=9.2167
	step [177/249], loss=10.3478
	step [178/249], loss=7.9485
	step [179/249], loss=8.5987
	step [180/249], loss=9.7408
	step [181/249], loss=8.6860
	step [182/249], loss=9.5320
	step [183/249], loss=8.8458
	step [184/249], loss=12.1259
	step [185/249], loss=7.7341
	step [186/249], loss=9.9537
	step [187/249], loss=8.6034
	step [188/249], loss=9.1607
	step [189/249], loss=10.0303
	step [190/249], loss=10.3185
	step [191/249], loss=8.7749
	step [192/249], loss=10.4738
	step [193/249], loss=8.7113
	step [194/249], loss=6.7959
	step [195/249], loss=8.2970
	step [196/249], loss=9.5986
	step [197/249], loss=8.9023
	step [198/249], loss=9.8855
	step [199/249], loss=8.0509
	step [200/249], loss=7.6309
	step [201/249], loss=7.1900
	step [202/249], loss=9.3170
	step [203/249], loss=9.3803
	step [204/249], loss=10.4598
	step [205/249], loss=8.5436
	step [206/249], loss=9.7342
	step [207/249], loss=9.6422
	step [208/249], loss=9.5003
	step [209/249], loss=8.1911
	step [210/249], loss=8.2612
	step [211/249], loss=8.3649
	step [212/249], loss=8.8777
	step [213/249], loss=9.1884
	step [214/249], loss=8.6247
	step [215/249], loss=10.8676
	step [216/249], loss=10.6458
	step [217/249], loss=8.6456
	step [218/249], loss=7.3322
	step [219/249], loss=9.9563
	step [220/249], loss=11.6167
	step [221/249], loss=9.7799
	step [222/249], loss=8.6680
	step [223/249], loss=9.8299
	step [224/249], loss=9.2781
	step [225/249], loss=10.2329
	step [226/249], loss=11.3910
	step [227/249], loss=9.3545
	step [228/249], loss=9.9736
	step [229/249], loss=11.2745
	step [230/249], loss=7.7127
	step [231/249], loss=10.8166
	step [232/249], loss=9.6366
	step [233/249], loss=9.3981
	step [234/249], loss=12.8611
	step [235/249], loss=8.4500
	step [236/249], loss=7.8690
	step [237/249], loss=8.5223
	step [238/249], loss=9.0222
	step [239/249], loss=9.8102
	step [240/249], loss=9.3496
	step [241/249], loss=8.8720
	step [242/249], loss=7.9902
	step [243/249], loss=9.6890
	step [244/249], loss=9.1631
	step [245/249], loss=7.3352
	step [246/249], loss=9.0007
	step [247/249], loss=9.9078
	step [248/249], loss=7.4587
	step [249/249], loss=7.4237
	Evaluating
	loss=0.0260, precision=0.1557, recall=0.9940, f1=0.2692
Training epoch 18
	step [1/249], loss=8.2922
	step [2/249], loss=9.1320
	step [3/249], loss=9.6416
	step [4/249], loss=9.5551
	step [5/249], loss=9.5146
	step [6/249], loss=9.3786
	step [7/249], loss=9.2553
	step [8/249], loss=8.1074
	step [9/249], loss=7.4712
	step [10/249], loss=10.4946
	step [11/249], loss=9.8001
	step [12/249], loss=9.4462
	step [13/249], loss=10.6721
	step [14/249], loss=9.1147
	step [15/249], loss=9.8023
	step [16/249], loss=9.0637
	step [17/249], loss=8.6274
	step [18/249], loss=7.8548
	step [19/249], loss=10.1678
	step [20/249], loss=8.7940
	step [21/249], loss=8.1746
	step [22/249], loss=8.8304
	step [23/249], loss=7.3590
	step [24/249], loss=7.0430
	step [25/249], loss=8.1109
	step [26/249], loss=10.7750
	step [27/249], loss=10.0279
	step [28/249], loss=8.4635
	step [29/249], loss=6.2721
	step [30/249], loss=10.8644
	step [31/249], loss=8.5252
	step [32/249], loss=9.6091
	step [33/249], loss=9.3933
	step [34/249], loss=8.3908
	step [35/249], loss=8.7759
	step [36/249], loss=8.6673
	step [37/249], loss=9.3817
	step [38/249], loss=10.0394
	step [39/249], loss=7.5995
	step [40/249], loss=8.7135
	step [41/249], loss=5.9569
	step [42/249], loss=8.5465
	step [43/249], loss=10.1065
	step [44/249], loss=7.1823
	step [45/249], loss=8.5152
	step [46/249], loss=9.0248
	step [47/249], loss=9.6503
	step [48/249], loss=10.1624
	step [49/249], loss=6.3146
	step [50/249], loss=9.6425
	step [51/249], loss=9.7168
	step [52/249], loss=9.7842
	step [53/249], loss=8.6818
	step [54/249], loss=8.5925
	step [55/249], loss=7.7351
	step [56/249], loss=9.1726
	step [57/249], loss=9.0248
	step [58/249], loss=8.1264
	step [59/249], loss=8.2515
	step [60/249], loss=8.8921
	step [61/249], loss=7.2104
	step [62/249], loss=8.3310
	step [63/249], loss=8.6770
	step [64/249], loss=9.8783
	step [65/249], loss=7.6953
	step [66/249], loss=8.5213
	step [67/249], loss=8.4587
	step [68/249], loss=6.5893
	step [69/249], loss=6.3441
	step [70/249], loss=8.5193
	step [71/249], loss=8.3444
	step [72/249], loss=10.3109
	step [73/249], loss=8.3582
	step [74/249], loss=8.4827
	step [75/249], loss=9.1288
	step [76/249], loss=9.5161
	step [77/249], loss=9.1887
	step [78/249], loss=9.3769
	step [79/249], loss=10.2494
	step [80/249], loss=7.8126
	step [81/249], loss=9.1274
	step [82/249], loss=8.7278
	step [83/249], loss=10.5708
	step [84/249], loss=9.9066
	step [85/249], loss=8.5572
	step [86/249], loss=7.8407
	step [87/249], loss=8.2990
	step [88/249], loss=8.8227
	step [89/249], loss=8.0152
	step [90/249], loss=8.8339
	step [91/249], loss=10.5139
	step [92/249], loss=9.1433
	step [93/249], loss=12.1555
	step [94/249], loss=7.7992
	step [95/249], loss=9.4930
	step [96/249], loss=10.9884
	step [97/249], loss=7.4266
	step [98/249], loss=7.8515
	step [99/249], loss=6.8477
	step [100/249], loss=7.6488
	step [101/249], loss=8.2373
	step [102/249], loss=8.6436
	step [103/249], loss=7.6765
	step [104/249], loss=8.1179
	step [105/249], loss=8.2283
	step [106/249], loss=8.1962
	step [107/249], loss=7.7935
	step [108/249], loss=7.1127
	step [109/249], loss=7.4758
	step [110/249], loss=9.1352
	step [111/249], loss=7.6907
	step [112/249], loss=8.7776
	step [113/249], loss=7.8288
	step [114/249], loss=8.7645
	step [115/249], loss=9.7347
	step [116/249], loss=9.7884
	step [117/249], loss=9.2888
	step [118/249], loss=7.6891
	step [119/249], loss=9.1334
	step [120/249], loss=8.2462
	step [121/249], loss=11.9510
	step [122/249], loss=8.5379
	step [123/249], loss=8.3730
	step [124/249], loss=8.9354
	step [125/249], loss=8.9293
	step [126/249], loss=9.8059
	step [127/249], loss=9.2954
	step [128/249], loss=7.4068
	step [129/249], loss=7.3373
	step [130/249], loss=11.5130
	step [131/249], loss=8.4646
	step [132/249], loss=8.1342
	step [133/249], loss=6.5764
	step [134/249], loss=7.8139
	step [135/249], loss=9.0059
	step [136/249], loss=8.4900
	step [137/249], loss=9.5139
	step [138/249], loss=8.3367
	step [139/249], loss=8.7537
	step [140/249], loss=9.5507
	step [141/249], loss=8.4567
	step [142/249], loss=9.7239
	step [143/249], loss=10.0386
	step [144/249], loss=8.2530
	step [145/249], loss=8.1938
	step [146/249], loss=9.6828
	step [147/249], loss=8.0323
	step [148/249], loss=7.9045
	step [149/249], loss=7.9534
	step [150/249], loss=8.5087
	step [151/249], loss=9.2898
	step [152/249], loss=8.1744
	step [153/249], loss=9.9097
	step [154/249], loss=6.4088
	step [155/249], loss=7.8811
	step [156/249], loss=11.8050
	step [157/249], loss=7.8678
	step [158/249], loss=9.1819
	step [159/249], loss=7.4063
	step [160/249], loss=8.4798
	step [161/249], loss=8.6447
	step [162/249], loss=9.0905
	step [163/249], loss=7.6676
	step [164/249], loss=8.3521
	step [165/249], loss=8.0729
	step [166/249], loss=11.0671
	step [167/249], loss=7.4628
	step [168/249], loss=6.3623
	step [169/249], loss=8.2672
	step [170/249], loss=8.0251
	step [171/249], loss=8.6635
	step [172/249], loss=8.7958
	step [173/249], loss=9.3720
	step [174/249], loss=9.1765
	step [175/249], loss=8.0959
	step [176/249], loss=7.6347
	step [177/249], loss=8.3256
	step [178/249], loss=9.0106
	step [179/249], loss=7.7088
	step [180/249], loss=7.1488
	step [181/249], loss=12.6242
	step [182/249], loss=8.6145
	step [183/249], loss=7.5796
	step [184/249], loss=8.0773
	step [185/249], loss=9.1050
	step [186/249], loss=8.1239
	step [187/249], loss=8.3234
	step [188/249], loss=8.8472
	step [189/249], loss=6.6916
	step [190/249], loss=8.7286
	step [191/249], loss=7.1847
	step [192/249], loss=8.9248
	step [193/249], loss=8.7272
	step [194/249], loss=7.9069
	step [195/249], loss=7.3692
	step [196/249], loss=9.1795
	step [197/249], loss=9.1413
	step [198/249], loss=7.2822
	step [199/249], loss=11.3753
	step [200/249], loss=8.3068
	step [201/249], loss=7.3917
	step [202/249], loss=8.7664
	step [203/249], loss=8.6409
	step [204/249], loss=9.7237
	step [205/249], loss=6.8502
	step [206/249], loss=7.4935
	step [207/249], loss=7.9711
	step [208/249], loss=8.1621
	step [209/249], loss=7.2633
	step [210/249], loss=8.4821
	step [211/249], loss=8.4038
	step [212/249], loss=8.4393
	step [213/249], loss=8.6069
	step [214/249], loss=9.6753
	step [215/249], loss=8.3300
	step [216/249], loss=9.6169
	step [217/249], loss=7.6209
	step [218/249], loss=9.1905
	step [219/249], loss=9.2767
	step [220/249], loss=8.3358
	step [221/249], loss=9.7335
	step [222/249], loss=8.3413
	step [223/249], loss=8.9178
	step [224/249], loss=8.5501
	step [225/249], loss=8.3565
	step [226/249], loss=6.3563
	step [227/249], loss=8.0596
	step [228/249], loss=8.7401
	step [229/249], loss=10.5080
	step [230/249], loss=7.2260
	step [231/249], loss=8.3240
	step [232/249], loss=7.5616
	step [233/249], loss=8.3508
	step [234/249], loss=10.2193
	step [235/249], loss=9.3318
	step [236/249], loss=9.6082
	step [237/249], loss=8.1401
	step [238/249], loss=7.7461
	step [239/249], loss=8.9359
	step [240/249], loss=10.0695
	step [241/249], loss=10.2198
	step [242/249], loss=7.0914
	step [243/249], loss=8.0597
	step [244/249], loss=7.4463
	step [245/249], loss=9.5655
	step [246/249], loss=8.6272
	step [247/249], loss=8.6973
	step [248/249], loss=9.1427
	step [249/249], loss=7.5536
	Evaluating
	loss=0.0232, precision=0.1723, recall=0.9934, f1=0.2937
saving model as: 0_saved_model.pth
Training epoch 19
	step [1/249], loss=8.8993
	step [2/249], loss=7.1506
	step [3/249], loss=8.5285
	step [4/249], loss=8.1420
	step [5/249], loss=6.9794
	step [6/249], loss=8.7823
	step [7/249], loss=8.3496
	step [8/249], loss=8.3583
	step [9/249], loss=8.7277
	step [10/249], loss=8.0558
	step [11/249], loss=6.6837
	step [12/249], loss=9.1857
	step [13/249], loss=8.2511
	step [14/249], loss=10.2386
	step [15/249], loss=8.3190
	step [16/249], loss=7.1723
	step [17/249], loss=8.2577
	step [18/249], loss=8.5389
	step [19/249], loss=7.7195
	step [20/249], loss=7.8175
	step [21/249], loss=8.8677
	step [22/249], loss=8.6950
	step [23/249], loss=8.0354
	step [24/249], loss=7.7521
	step [25/249], loss=7.6006
	step [26/249], loss=9.4422
	step [27/249], loss=9.8039
	step [28/249], loss=7.4969
	step [29/249], loss=6.3560
	step [30/249], loss=9.3406
	step [31/249], loss=8.3514
	step [32/249], loss=8.3914
	step [33/249], loss=8.3430
	step [34/249], loss=8.9095
	step [35/249], loss=11.1690
	step [36/249], loss=8.2345
	step [37/249], loss=8.5076
	step [38/249], loss=8.7515
	step [39/249], loss=9.7425
	step [40/249], loss=8.6471
	step [41/249], loss=8.9994
	step [42/249], loss=7.8175
	step [43/249], loss=8.2678
	step [44/249], loss=8.4926
	step [45/249], loss=8.0489
	step [46/249], loss=7.9786
	step [47/249], loss=7.5958
	step [48/249], loss=9.1774
	step [49/249], loss=9.1395
	step [50/249], loss=8.7075
	step [51/249], loss=7.8390
	step [52/249], loss=7.8466
	step [53/249], loss=9.1287
	step [54/249], loss=7.9495
	step [55/249], loss=8.6508
	step [56/249], loss=7.5620
	step [57/249], loss=9.4971
	step [58/249], loss=8.8303
	step [59/249], loss=7.9145
	step [60/249], loss=8.4551
	step [61/249], loss=7.7107
	step [62/249], loss=7.3861
	step [63/249], loss=8.4986
	step [64/249], loss=8.4730
	step [65/249], loss=10.5138
	step [66/249], loss=8.8026
	step [67/249], loss=7.8094
	step [68/249], loss=7.6483
	step [69/249], loss=7.8625
	step [70/249], loss=8.5153
	step [71/249], loss=8.7416
	step [72/249], loss=6.6815
	step [73/249], loss=9.2457
	step [74/249], loss=6.4969
	step [75/249], loss=9.0981
	step [76/249], loss=10.0322
	step [77/249], loss=7.1016
	step [78/249], loss=9.4364
	step [79/249], loss=8.2629
	step [80/249], loss=8.2124
	step [81/249], loss=7.5595
	step [82/249], loss=7.6161
	step [83/249], loss=9.7100
	step [84/249], loss=7.7133
	step [85/249], loss=11.4926
	step [86/249], loss=8.4224
	step [87/249], loss=7.9525
	step [88/249], loss=10.0907
	step [89/249], loss=8.7843
	step [90/249], loss=7.2427
	step [91/249], loss=7.7012
	step [92/249], loss=7.6621
	step [93/249], loss=7.9632
	step [94/249], loss=8.6033
	step [95/249], loss=9.6400
	step [96/249], loss=9.1313
	step [97/249], loss=9.0632
	step [98/249], loss=8.2120
	step [99/249], loss=10.5887
	step [100/249], loss=9.3907
	step [101/249], loss=8.2115
	step [102/249], loss=7.4666
	step [103/249], loss=8.1090
	step [104/249], loss=7.7874
	step [105/249], loss=7.2005
	step [106/249], loss=8.2159
	step [107/249], loss=10.7588
	step [108/249], loss=7.7338
	step [109/249], loss=8.4526
	step [110/249], loss=6.5981
	step [111/249], loss=8.0497
	step [112/249], loss=8.8429
	step [113/249], loss=9.9011
	step [114/249], loss=7.8999
	step [115/249], loss=7.4597
	step [116/249], loss=7.5036
	step [117/249], loss=8.1423
	step [118/249], loss=7.8427
	step [119/249], loss=8.0214
	step [120/249], loss=8.1144
	step [121/249], loss=8.2415
	step [122/249], loss=7.4920
	step [123/249], loss=8.5759
	step [124/249], loss=7.6053
	step [125/249], loss=10.4935
	step [126/249], loss=8.5886
	step [127/249], loss=8.8300
	step [128/249], loss=7.9674
	step [129/249], loss=8.0903
	step [130/249], loss=8.1212
	step [131/249], loss=8.7051
	step [132/249], loss=11.3686
	step [133/249], loss=7.2011
	step [134/249], loss=11.7555
	step [135/249], loss=10.1419
	step [136/249], loss=7.4158
	step [137/249], loss=10.2183
	step [138/249], loss=8.9924
	step [139/249], loss=7.5528
	step [140/249], loss=9.1908
	step [141/249], loss=7.7478
	step [142/249], loss=8.0986
	step [143/249], loss=8.1571
	step [144/249], loss=7.2604
	step [145/249], loss=9.3027
	step [146/249], loss=8.9404
	step [147/249], loss=7.2436
	step [148/249], loss=7.4204
	step [149/249], loss=7.3233
	step [150/249], loss=9.2630
	step [151/249], loss=8.8479
	step [152/249], loss=9.9928
	step [153/249], loss=7.2165
	step [154/249], loss=8.2619
	step [155/249], loss=8.5527
	step [156/249], loss=7.0493
	step [157/249], loss=7.8123
	step [158/249], loss=8.3180
	step [159/249], loss=10.6580
	step [160/249], loss=8.6817
	step [161/249], loss=8.7047
	step [162/249], loss=9.5801
	step [163/249], loss=9.1520
	step [164/249], loss=6.9182
	step [165/249], loss=6.8402
	step [166/249], loss=9.2795
	step [167/249], loss=7.2741
	step [168/249], loss=7.6424
	step [169/249], loss=8.7247
	step [170/249], loss=7.4606
	step [171/249], loss=7.2742
	step [172/249], loss=9.2992
	step [173/249], loss=8.8255
	step [174/249], loss=7.5597
	step [175/249], loss=7.5485
	step [176/249], loss=7.4536
	step [177/249], loss=8.8793
	step [178/249], loss=8.7911
	step [179/249], loss=9.0640
	step [180/249], loss=8.0241
	step [181/249], loss=9.1593
	step [182/249], loss=7.6618
	step [183/249], loss=8.4205
	step [184/249], loss=7.1652
	step [185/249], loss=9.5403
	step [186/249], loss=9.9905
	step [187/249], loss=8.7376
	step [188/249], loss=9.4853
	step [189/249], loss=9.8527
	step [190/249], loss=9.3113
	step [191/249], loss=7.7527
	step [192/249], loss=7.4908
	step [193/249], loss=7.0814
	step [194/249], loss=7.9920
	step [195/249], loss=7.5569
	step [196/249], loss=9.6707
	step [197/249], loss=7.4832
	step [198/249], loss=7.8312
	step [199/249], loss=8.2703
	step [200/249], loss=6.8879
	step [201/249], loss=8.2250
	step [202/249], loss=6.9113
	step [203/249], loss=7.5402
	step [204/249], loss=7.4753
	step [205/249], loss=8.1865
	step [206/249], loss=9.1173
	step [207/249], loss=7.4372
	step [208/249], loss=7.2680
	step [209/249], loss=8.6982
	step [210/249], loss=9.7120
	step [211/249], loss=9.2960
	step [212/249], loss=7.1126
	step [213/249], loss=8.2088
	step [214/249], loss=8.1554
	step [215/249], loss=7.8880
	step [216/249], loss=9.1496
	step [217/249], loss=7.2200
	step [218/249], loss=9.2124
	step [219/249], loss=9.3781
	step [220/249], loss=7.2929
	step [221/249], loss=7.6949
	step [222/249], loss=7.1186
	step [223/249], loss=8.0685
	step [224/249], loss=8.0631
	step [225/249], loss=7.5240
	step [226/249], loss=10.2360
	step [227/249], loss=6.4856
	step [228/249], loss=10.1447
	step [229/249], loss=6.9122
	step [230/249], loss=9.8328
	step [231/249], loss=9.7027
	step [232/249], loss=7.4306
	step [233/249], loss=7.6122
	step [234/249], loss=7.1830
	step [235/249], loss=8.1044
	step [236/249], loss=7.4072
	step [237/249], loss=8.5289
	step [238/249], loss=9.3195
	step [239/249], loss=7.8158
	step [240/249], loss=7.6966
	step [241/249], loss=7.5448
	step [242/249], loss=9.7509
	step [243/249], loss=6.8449
	step [244/249], loss=7.9049
	step [245/249], loss=8.3590
	step [246/249], loss=8.0180
	step [247/249], loss=7.4622
	step [248/249], loss=8.0655
	step [249/249], loss=6.2372
	Evaluating
	loss=0.0234, precision=0.1670, recall=0.9936, f1=0.2860
Training epoch 20
	step [1/249], loss=9.0219
	step [2/249], loss=6.2230
	step [3/249], loss=9.5710
	step [4/249], loss=8.1122
	step [5/249], loss=9.3492
	step [6/249], loss=8.6943
	step [7/249], loss=9.4691
	step [8/249], loss=6.7408
	step [9/249], loss=7.9842
	step [10/249], loss=8.3339
	step [11/249], loss=8.4506
	step [12/249], loss=7.4801
	step [13/249], loss=8.2490
	step [14/249], loss=6.8418
	step [15/249], loss=8.5738
	step [16/249], loss=8.2895
	step [17/249], loss=7.7786
	step [18/249], loss=6.2903
	step [19/249], loss=6.5011
	step [20/249], loss=9.6548
	step [21/249], loss=8.6275
	step [22/249], loss=7.6887
	step [23/249], loss=6.8791
	step [24/249], loss=9.8057
	step [25/249], loss=7.1882
	step [26/249], loss=8.3160
	step [27/249], loss=7.0994
	step [28/249], loss=8.4250
	step [29/249], loss=7.5087
	step [30/249], loss=8.9267
	step [31/249], loss=8.1780
	step [32/249], loss=6.7898
	step [33/249], loss=8.8037
	step [34/249], loss=6.5520
	step [35/249], loss=7.5740
	step [36/249], loss=8.7742
	step [37/249], loss=7.6596
	step [38/249], loss=7.0046
	step [39/249], loss=7.5918
	step [40/249], loss=8.1876
	step [41/249], loss=9.7059
	step [42/249], loss=9.5519
	step [43/249], loss=7.8144
	step [44/249], loss=7.8817
	step [45/249], loss=8.2653
	step [46/249], loss=8.1763
	step [47/249], loss=7.5174
	step [48/249], loss=8.7828
	step [49/249], loss=8.3802
	step [50/249], loss=6.8007
	step [51/249], loss=7.6047
	step [52/249], loss=8.9887
	step [53/249], loss=8.6760
	step [54/249], loss=9.3440
	step [55/249], loss=8.5153
	step [56/249], loss=8.1790
	step [57/249], loss=9.9478
	step [58/249], loss=9.5609
	step [59/249], loss=9.0680
	step [60/249], loss=6.4056
	step [61/249], loss=8.2224
	step [62/249], loss=8.1695
	step [63/249], loss=8.1490
	step [64/249], loss=7.3177
	step [65/249], loss=8.0450
	step [66/249], loss=7.4885
	step [67/249], loss=11.2515
	step [68/249], loss=7.8610
	step [69/249], loss=8.2905
	step [70/249], loss=9.7693
	step [71/249], loss=6.9934
	step [72/249], loss=8.4702
	step [73/249], loss=7.8893
	step [74/249], loss=8.9385
	step [75/249], loss=7.1181
	step [76/249], loss=6.8006
	step [77/249], loss=7.9998
	step [78/249], loss=8.2189
	step [79/249], loss=7.2616
	step [80/249], loss=6.6842
	step [81/249], loss=7.8125
	step [82/249], loss=8.5177
	step [83/249], loss=7.9304
	step [84/249], loss=7.3084
	step [85/249], loss=6.9809
	step [86/249], loss=8.2769
	step [87/249], loss=9.7237
	step [88/249], loss=7.3060
	step [89/249], loss=7.9917
	step [90/249], loss=8.6007
	step [91/249], loss=8.6712
	step [92/249], loss=7.3132
	step [93/249], loss=8.2887
	step [94/249], loss=10.6284
	step [95/249], loss=7.0577
	step [96/249], loss=8.2351
	step [97/249], loss=8.1850
	step [98/249], loss=9.2114
	step [99/249], loss=8.2731
	step [100/249], loss=9.3178
	step [101/249], loss=7.8301
	step [102/249], loss=8.8693
	step [103/249], loss=8.3672
	step [104/249], loss=7.6735
	step [105/249], loss=7.9213
	step [106/249], loss=8.3562
	step [107/249], loss=7.8326
	step [108/249], loss=7.0978
	step [109/249], loss=8.0975
	step [110/249], loss=8.1641
	step [111/249], loss=8.3905
	step [112/249], loss=7.8417
	step [113/249], loss=9.1789
	step [114/249], loss=8.6586
	step [115/249], loss=6.9456
	step [116/249], loss=6.6130
	step [117/249], loss=8.0177
	step [118/249], loss=9.3698
	step [119/249], loss=8.7906
	step [120/249], loss=8.2514
	step [121/249], loss=6.8819
	step [122/249], loss=6.5736
	step [123/249], loss=7.0015
	step [124/249], loss=8.5688
	step [125/249], loss=8.2767
	step [126/249], loss=8.5776
	step [127/249], loss=8.5446
	step [128/249], loss=8.6374
	step [129/249], loss=7.2700
	step [130/249], loss=7.4335
	step [131/249], loss=9.5150
	step [132/249], loss=9.8361
	step [133/249], loss=6.8395
	step [134/249], loss=6.9841
	step [135/249], loss=8.9805
	step [136/249], loss=7.1806
	step [137/249], loss=8.8509
	step [138/249], loss=7.7885
	step [139/249], loss=9.1229
	step [140/249], loss=8.9776
	step [141/249], loss=8.0393
	step [142/249], loss=7.5205
	step [143/249], loss=6.3413
	step [144/249], loss=7.5487
	step [145/249], loss=9.3888
	step [146/249], loss=7.5005
	step [147/249], loss=7.1824
	step [148/249], loss=8.0064
	step [149/249], loss=7.6905
	step [150/249], loss=6.8760
	step [151/249], loss=8.3254
	step [152/249], loss=8.9482
	step [153/249], loss=8.4236
	step [154/249], loss=7.3328
	step [155/249], loss=6.6714
	step [156/249], loss=7.7950
	step [157/249], loss=8.4393
	step [158/249], loss=6.6104
	step [159/249], loss=7.3523
	step [160/249], loss=7.4922
	step [161/249], loss=8.0147
	step [162/249], loss=8.4961
	step [163/249], loss=8.6518
	step [164/249], loss=8.3012
	step [165/249], loss=9.2509
	step [166/249], loss=8.0713
	step [167/249], loss=7.4149
	step [168/249], loss=7.9759
	step [169/249], loss=9.9390
	step [170/249], loss=7.9642
	step [171/249], loss=8.4445
	step [172/249], loss=7.7731
	step [173/249], loss=8.9513
	step [174/249], loss=7.1742
	step [175/249], loss=8.1198
	step [176/249], loss=7.7874
	step [177/249], loss=10.7872
	step [178/249], loss=8.1160
	step [179/249], loss=9.6878
	step [180/249], loss=8.5574
	step [181/249], loss=8.0123
	step [182/249], loss=8.2722
	step [183/249], loss=6.5676
	step [184/249], loss=7.0714
	step [185/249], loss=8.1670
	step [186/249], loss=9.0908
	step [187/249], loss=8.5891
	step [188/249], loss=8.3817
	step [189/249], loss=8.8941
	step [190/249], loss=7.0102
	step [191/249], loss=7.4242
	step [192/249], loss=8.5898
	step [193/249], loss=7.7187
	step [194/249], loss=7.4550
	step [195/249], loss=7.2710
	step [196/249], loss=7.2916
	step [197/249], loss=7.3579
	step [198/249], loss=8.6660
	step [199/249], loss=10.2741
	step [200/249], loss=7.5138
	step [201/249], loss=7.7725
	step [202/249], loss=7.8148
	step [203/249], loss=8.4432
	step [204/249], loss=6.4286
	step [205/249], loss=8.4787
	step [206/249], loss=7.5959
	step [207/249], loss=6.9212
	step [208/249], loss=7.4041
	step [209/249], loss=8.4072
	step [210/249], loss=8.0227
	step [211/249], loss=7.7559
	step [212/249], loss=9.1364
	step [213/249], loss=8.9038
	step [214/249], loss=9.1418
	step [215/249], loss=8.2394
	step [216/249], loss=7.9737
	step [217/249], loss=8.7220
	step [218/249], loss=7.8526
	step [219/249], loss=6.6711
	step [220/249], loss=8.2403
	step [221/249], loss=7.8583
	step [222/249], loss=6.9857
	step [223/249], loss=6.8545
	step [224/249], loss=6.7630
	step [225/249], loss=8.0371
	step [226/249], loss=7.9435
	step [227/249], loss=8.3781
	step [228/249], loss=5.6699
	step [229/249], loss=9.4856
	step [230/249], loss=9.4870
	step [231/249], loss=8.4069
	step [232/249], loss=8.0552
	step [233/249], loss=8.7424
	step [234/249], loss=7.2457
	step [235/249], loss=7.3345
	step [236/249], loss=8.7452
	step [237/249], loss=7.1824
	step [238/249], loss=7.0666
	step [239/249], loss=6.7704
	step [240/249], loss=8.8026
	step [241/249], loss=7.5377
	step [242/249], loss=8.6930
	step [243/249], loss=7.2518
	step [244/249], loss=7.3951
	step [245/249], loss=7.0343
	step [246/249], loss=8.9940
	step [247/249], loss=6.8387
	step [248/249], loss=9.7586
	step [249/249], loss=4.8755
	Evaluating
	loss=0.0217, precision=0.1727, recall=0.9941, f1=0.2942
saving model as: 0_saved_model.pth
Training epoch 21
	step [1/249], loss=7.0907
	step [2/249], loss=8.1094
	step [3/249], loss=8.3580
	step [4/249], loss=9.6735
	step [5/249], loss=7.4355
	step [6/249], loss=7.2015
	step [7/249], loss=7.5654
	step [8/249], loss=7.7559
	step [9/249], loss=8.5095
	step [10/249], loss=8.9595
	step [11/249], loss=7.5496
	step [12/249], loss=7.3076
	step [13/249], loss=7.4780
	step [14/249], loss=8.5509
	step [15/249], loss=6.6967
	step [16/249], loss=7.3323
	step [17/249], loss=8.4951
	step [18/249], loss=9.0436
	step [19/249], loss=8.1259
	step [20/249], loss=6.9606
	step [21/249], loss=8.0868
	step [22/249], loss=8.8737
	step [23/249], loss=9.1391
	step [24/249], loss=7.5465
	step [25/249], loss=8.2269
	step [26/249], loss=8.4543
	step [27/249], loss=7.2052
	step [28/249], loss=7.4810
	step [29/249], loss=7.0777
	step [30/249], loss=7.0009
	step [31/249], loss=8.4215
	step [32/249], loss=10.8706
	step [33/249], loss=7.1651
	step [34/249], loss=6.6554
	step [35/249], loss=7.8245
	step [36/249], loss=9.4508
	step [37/249], loss=8.6762
	step [38/249], loss=7.7130
	step [39/249], loss=7.0808
	step [40/249], loss=7.6476
	step [41/249], loss=7.9018
	step [42/249], loss=7.9897
	step [43/249], loss=6.7713
	step [44/249], loss=7.3507
	step [45/249], loss=8.3213
	step [46/249], loss=7.6890
	step [47/249], loss=8.6125
	step [48/249], loss=6.3360
	step [49/249], loss=10.1104
	step [50/249], loss=9.4386
	step [51/249], loss=7.5085
	step [52/249], loss=7.2889
	step [53/249], loss=11.2981
	step [54/249], loss=7.0983
	step [55/249], loss=6.7473
	step [56/249], loss=7.1286
	step [57/249], loss=7.8177
	step [58/249], loss=6.7406
	step [59/249], loss=9.1790
	step [60/249], loss=7.3257
	step [61/249], loss=7.0783
	step [62/249], loss=8.3956
	step [63/249], loss=7.3078
	step [64/249], loss=7.9860
	step [65/249], loss=6.8482
	step [66/249], loss=10.2282
	step [67/249], loss=8.5745
	step [68/249], loss=7.4433
	step [69/249], loss=7.9258
	step [70/249], loss=6.5458
	step [71/249], loss=8.0326
	step [72/249], loss=8.1385
	step [73/249], loss=6.7265
	step [74/249], loss=6.9260
	step [75/249], loss=6.3356
	step [76/249], loss=7.7801
	step [77/249], loss=7.3044
	step [78/249], loss=7.4302
	step [79/249], loss=7.7848
	step [80/249], loss=6.8662
	step [81/249], loss=7.3499
	step [82/249], loss=8.9803
	step [83/249], loss=7.2633
	step [84/249], loss=8.8352
	step [85/249], loss=7.8191
	step [86/249], loss=7.2855
	step [87/249], loss=8.1293
	step [88/249], loss=6.9354
	step [89/249], loss=7.6361
	step [90/249], loss=6.5953
	step [91/249], loss=8.3740
	step [92/249], loss=7.4077
	step [93/249], loss=8.3735
	step [94/249], loss=8.5430
	step [95/249], loss=6.6988
	step [96/249], loss=7.2584
	step [97/249], loss=7.4580
	step [98/249], loss=7.8187
	step [99/249], loss=7.5531
	step [100/249], loss=7.5257
	step [101/249], loss=8.2486
	step [102/249], loss=7.6798
	step [103/249], loss=8.6920
	step [104/249], loss=8.1774
	step [105/249], loss=6.3945
	step [106/249], loss=7.9897
	step [107/249], loss=7.6973
	step [108/249], loss=7.7639
	step [109/249], loss=8.7934
	step [110/249], loss=7.6660
	step [111/249], loss=7.4996
	step [112/249], loss=6.8655
	step [113/249], loss=7.2159
	step [114/249], loss=7.3669
	step [115/249], loss=7.3286
	step [116/249], loss=7.1286
	step [117/249], loss=7.1574
	step [118/249], loss=6.6704
	step [119/249], loss=7.4857
	step [120/249], loss=7.9899
	step [121/249], loss=7.3606
	step [122/249], loss=9.8540
	step [123/249], loss=7.9700
	step [124/249], loss=7.7142
	step [125/249], loss=7.6723
	step [126/249], loss=7.4136
	step [127/249], loss=5.7430
	step [128/249], loss=7.1198
	step [129/249], loss=7.8063
	step [130/249], loss=6.6805
	step [131/249], loss=7.9424
	step [132/249], loss=8.1246
	step [133/249], loss=7.9045
	step [134/249], loss=7.1898
	step [135/249], loss=7.8623
	step [136/249], loss=7.2425
	step [137/249], loss=6.6782
	step [138/249], loss=10.1512
	step [139/249], loss=9.1837
	step [140/249], loss=7.2676
	step [141/249], loss=6.7431
	step [142/249], loss=7.0448
	step [143/249], loss=9.6576
	step [144/249], loss=6.7159
	step [145/249], loss=7.9057
	step [146/249], loss=7.8298
	step [147/249], loss=6.7941
	step [148/249], loss=8.4829
	step [149/249], loss=8.1384
	step [150/249], loss=7.0214
	step [151/249], loss=6.8114
	step [152/249], loss=8.0894
	step [153/249], loss=8.0345
	step [154/249], loss=6.6568
	step [155/249], loss=5.9847
	step [156/249], loss=9.7077
	step [157/249], loss=8.1463
	step [158/249], loss=7.5256
	step [159/249], loss=7.3032
	step [160/249], loss=5.9100
	step [161/249], loss=10.1256
	step [162/249], loss=6.1474
	step [163/249], loss=7.6343
	step [164/249], loss=7.7133
	step [165/249], loss=7.1146
	step [166/249], loss=9.2018
	step [167/249], loss=7.1256
	step [168/249], loss=8.1112
	step [169/249], loss=8.4140
	step [170/249], loss=7.6840
	step [171/249], loss=7.4747
	step [172/249], loss=8.1636
	step [173/249], loss=6.3133
	step [174/249], loss=6.5307
	step [175/249], loss=5.8782
	step [176/249], loss=8.0838
	step [177/249], loss=7.7428
	step [178/249], loss=6.9377
	step [179/249], loss=7.7305
	step [180/249], loss=8.4304
	step [181/249], loss=6.1233
	step [182/249], loss=7.6648
	step [183/249], loss=8.8336
	step [184/249], loss=9.3889
	step [185/249], loss=7.5748
	step [186/249], loss=6.6459
	step [187/249], loss=7.0759
	step [188/249], loss=7.2276
	step [189/249], loss=8.1229
	step [190/249], loss=6.0153
	step [191/249], loss=10.2068
	step [192/249], loss=8.9668
	step [193/249], loss=7.4983
	step [194/249], loss=6.6606
	step [195/249], loss=7.3423
	step [196/249], loss=8.2222
	step [197/249], loss=8.2655
	step [198/249], loss=7.8433
	step [199/249], loss=7.3328
	step [200/249], loss=7.3406
	step [201/249], loss=8.2357
	step [202/249], loss=8.0157
	step [203/249], loss=9.6034
	step [204/249], loss=7.7349
	step [205/249], loss=7.0930
	step [206/249], loss=7.5068
	step [207/249], loss=7.4346
	step [208/249], loss=6.5957
	step [209/249], loss=7.0860
	step [210/249], loss=6.9575
	step [211/249], loss=6.5203
	step [212/249], loss=7.0233
	step [213/249], loss=7.6525
	step [214/249], loss=9.2540
	step [215/249], loss=7.3824
	step [216/249], loss=7.8666
	step [217/249], loss=7.6414
	step [218/249], loss=7.7566
	step [219/249], loss=7.1966
	step [220/249], loss=7.3896
	step [221/249], loss=7.2794
	step [222/249], loss=7.4356
	step [223/249], loss=9.6312
	step [224/249], loss=7.2841
	step [225/249], loss=7.1048
	step [226/249], loss=8.0400
	step [227/249], loss=6.6473
	step [228/249], loss=7.9000
	step [229/249], loss=6.5188
	step [230/249], loss=5.7166
	step [231/249], loss=9.0909
	step [232/249], loss=6.8238
	step [233/249], loss=6.2283
	step [234/249], loss=9.7699
	step [235/249], loss=9.0279
	step [236/249], loss=7.7706
	step [237/249], loss=8.1116
	step [238/249], loss=7.7795
	step [239/249], loss=7.8415
	step [240/249], loss=8.9213
	step [241/249], loss=8.4945
	step [242/249], loss=6.3946
	step [243/249], loss=7.6851
	step [244/249], loss=7.9681
	step [245/249], loss=9.1037
	step [246/249], loss=8.7857
	step [247/249], loss=8.8617
	step [248/249], loss=8.0312
	step [249/249], loss=5.1046
	Evaluating
	loss=0.0221, precision=0.1704, recall=0.9936, f1=0.2910
Training epoch 22
	step [1/249], loss=8.4666
	step [2/249], loss=5.8270
	step [3/249], loss=7.7527
	step [4/249], loss=8.0834
	step [5/249], loss=8.2518
	step [6/249], loss=7.9234
	step [7/249], loss=7.0863
	step [8/249], loss=6.2785
	step [9/249], loss=6.6688
	step [10/249], loss=7.9116
	step [11/249], loss=8.3618
	step [12/249], loss=7.6830
	step [13/249], loss=7.8439
	step [14/249], loss=7.1242
	step [15/249], loss=8.2874
	step [16/249], loss=7.9703
	step [17/249], loss=9.8535
	step [18/249], loss=8.6544
	step [19/249], loss=6.6324
	step [20/249], loss=7.8355
	step [21/249], loss=7.1013
	step [22/249], loss=7.4187
	step [23/249], loss=8.8714
	step [24/249], loss=6.2940
	step [25/249], loss=5.3468
	step [26/249], loss=6.2840
	step [27/249], loss=7.6019
	step [28/249], loss=6.3531
	step [29/249], loss=6.7117
	step [30/249], loss=7.7447
	step [31/249], loss=9.4393
	step [32/249], loss=7.3044
	step [33/249], loss=7.9554
	step [34/249], loss=7.1153
	step [35/249], loss=8.9971
	step [36/249], loss=6.2956
	step [37/249], loss=7.8173
	step [38/249], loss=6.5002
	step [39/249], loss=8.5357
	step [40/249], loss=8.3373
	step [41/249], loss=9.2045
	step [42/249], loss=8.3271
	step [43/249], loss=7.4453
	step [44/249], loss=7.6698
	step [45/249], loss=8.7436
	step [46/249], loss=8.1915
	step [47/249], loss=8.2938
	step [48/249], loss=8.4527
	step [49/249], loss=7.8620
	step [50/249], loss=7.4653
	step [51/249], loss=6.4679
	step [52/249], loss=9.5544
	step [53/249], loss=7.2553
	step [54/249], loss=9.0898
	step [55/249], loss=6.0549
	step [56/249], loss=7.4608
	step [57/249], loss=9.2521
	step [58/249], loss=9.5716
	step [59/249], loss=8.3460
	step [60/249], loss=7.8427
	step [61/249], loss=7.8160
	step [62/249], loss=6.5807
	step [63/249], loss=7.6152
	step [64/249], loss=7.0540
	step [65/249], loss=5.7600
	step [66/249], loss=8.3551
	step [67/249], loss=7.3986
	step [68/249], loss=5.9638
	step [69/249], loss=7.7474
	step [70/249], loss=9.8464
	step [71/249], loss=8.8531
	step [72/249], loss=7.3829
	step [73/249], loss=8.1938
	step [74/249], loss=8.9011
	step [75/249], loss=7.2839
	step [76/249], loss=8.9676
	step [77/249], loss=6.9739
	step [78/249], loss=9.6421
	step [79/249], loss=6.7237
	step [80/249], loss=6.4484
	step [81/249], loss=6.1585
	step [82/249], loss=7.4095
	step [83/249], loss=8.7846
	step [84/249], loss=10.3686
	step [85/249], loss=6.6831
	step [86/249], loss=7.9046
	step [87/249], loss=7.7749
	step [88/249], loss=7.6771
	step [89/249], loss=7.8608
	step [90/249], loss=9.0051
	step [91/249], loss=6.6689
	step [92/249], loss=7.1528
	step [93/249], loss=7.3492
	step [94/249], loss=7.3525
	step [95/249], loss=7.8643
	step [96/249], loss=6.3071
	step [97/249], loss=12.6825
	step [98/249], loss=5.8927
	step [99/249], loss=8.7907
	step [100/249], loss=7.5681
	step [101/249], loss=5.8226
	step [102/249], loss=7.9531
	step [103/249], loss=10.2362
	step [104/249], loss=7.0608
	step [105/249], loss=8.3971
	step [106/249], loss=6.3321
	step [107/249], loss=7.9947
	step [108/249], loss=6.3448
	step [109/249], loss=7.4102
	step [110/249], loss=7.2491
	step [111/249], loss=10.1351
	step [112/249], loss=7.5934
	step [113/249], loss=8.7230
	step [114/249], loss=6.9832
	step [115/249], loss=9.0726
	step [116/249], loss=5.9599
	step [117/249], loss=7.8617
	step [118/249], loss=6.5673
	step [119/249], loss=7.3966
	step [120/249], loss=6.2019
	step [121/249], loss=6.5008
	step [122/249], loss=7.6192
	step [123/249], loss=7.3959
	step [124/249], loss=7.6724
	step [125/249], loss=6.0760
	step [126/249], loss=7.9437
	step [127/249], loss=8.0656
	step [128/249], loss=7.8213
	step [129/249], loss=7.9763
	step [130/249], loss=6.1232
	step [131/249], loss=8.3073
	step [132/249], loss=8.8381
	step [133/249], loss=7.9598
	step [134/249], loss=7.5548
	step [135/249], loss=8.3098
	step [136/249], loss=7.5698
	step [137/249], loss=6.7325
	step [138/249], loss=8.1113
	step [139/249], loss=7.2382
	step [140/249], loss=7.3829
	step [141/249], loss=6.4577
	step [142/249], loss=8.7043
	step [143/249], loss=9.0818
	step [144/249], loss=7.4358
	step [145/249], loss=6.8267
	step [146/249], loss=8.8414
	step [147/249], loss=8.1100
	step [148/249], loss=6.4305
	step [149/249], loss=8.3122
	step [150/249], loss=6.7367
	step [151/249], loss=6.1972
	step [152/249], loss=6.8693
	step [153/249], loss=6.8126
	step [154/249], loss=8.5235
	step [155/249], loss=6.9294
	step [156/249], loss=8.5106
	step [157/249], loss=6.7600
	step [158/249], loss=6.3535
	step [159/249], loss=6.9935
	step [160/249], loss=6.5544
	step [161/249], loss=8.4812
	step [162/249], loss=7.7590
	step [163/249], loss=7.1480
	step [164/249], loss=7.1752
	step [165/249], loss=6.9286
	step [166/249], loss=7.8887
	step [167/249], loss=7.0601
	step [168/249], loss=6.2971
	step [169/249], loss=8.0667
	step [170/249], loss=7.5848
	step [171/249], loss=7.2074
	step [172/249], loss=5.7084
	step [173/249], loss=6.2358
	step [174/249], loss=7.0388
	step [175/249], loss=5.6868
	step [176/249], loss=8.3254
	step [177/249], loss=7.7352
	step [178/249], loss=7.1720
	step [179/249], loss=7.7337
	step [180/249], loss=6.4976
	step [181/249], loss=8.8044
	step [182/249], loss=8.3079
	step [183/249], loss=7.7681
	step [184/249], loss=8.8429
	step [185/249], loss=7.9477
	step [186/249], loss=6.0309
	step [187/249], loss=8.1529
	step [188/249], loss=7.4460
	step [189/249], loss=7.1281
	step [190/249], loss=7.4552
	step [191/249], loss=6.9302
	step [192/249], loss=7.9087
	step [193/249], loss=8.5777
	step [194/249], loss=7.4424
	step [195/249], loss=7.5370
	step [196/249], loss=7.4517
	step [197/249], loss=6.7846
	step [198/249], loss=7.2023
	step [199/249], loss=7.2455
	step [200/249], loss=6.4666
	step [201/249], loss=7.1986
	step [202/249], loss=7.6624
	step [203/249], loss=7.9178
	step [204/249], loss=7.4807
	step [205/249], loss=6.9884
	step [206/249], loss=5.9008
	step [207/249], loss=7.3314
	step [208/249], loss=7.2761
	step [209/249], loss=9.1029
	step [210/249], loss=8.6623
	step [211/249], loss=5.8413
	step [212/249], loss=6.7581
	step [213/249], loss=7.3019
	step [214/249], loss=7.7454
	step [215/249], loss=8.0830
	step [216/249], loss=6.8557
	step [217/249], loss=7.2524
	step [218/249], loss=6.8690
	step [219/249], loss=7.4690
	step [220/249], loss=6.7807
	step [221/249], loss=8.1021
	step [222/249], loss=7.8392
	step [223/249], loss=7.5266
	step [224/249], loss=6.7430
	step [225/249], loss=7.1772
	step [226/249], loss=8.5438
	step [227/249], loss=7.2973
	step [228/249], loss=5.8891
	step [229/249], loss=6.3531
	step [230/249], loss=6.9674
	step [231/249], loss=6.7059
	step [232/249], loss=8.3505
	step [233/249], loss=7.3197
	step [234/249], loss=7.7074
	step [235/249], loss=7.9990
	step [236/249], loss=7.0928
	step [237/249], loss=7.2619
	step [238/249], loss=7.2863
	step [239/249], loss=8.0971
	step [240/249], loss=8.1557
	step [241/249], loss=8.1305
	step [242/249], loss=7.9181
	step [243/249], loss=8.4574
	step [244/249], loss=6.2420
	step [245/249], loss=7.5750
	step [246/249], loss=6.9442
	step [247/249], loss=7.5270
	step [248/249], loss=6.3538
	step [249/249], loss=3.9108
	Evaluating
	loss=0.0221, precision=0.1730, recall=0.9939, f1=0.2947
saving model as: 0_saved_model.pth
Training epoch 23
	step [1/249], loss=6.1282
	step [2/249], loss=6.9952
	step [3/249], loss=7.3086
	step [4/249], loss=8.4817
	step [5/249], loss=6.8800
	step [6/249], loss=5.6649
	step [7/249], loss=8.1990
	step [8/249], loss=6.3461
	step [9/249], loss=8.3403
	step [10/249], loss=7.3264
	step [11/249], loss=6.3731
	step [12/249], loss=6.1529
	step [13/249], loss=6.8254
	step [14/249], loss=6.9444
	step [15/249], loss=6.3939
	step [16/249], loss=7.1807
	step [17/249], loss=7.3308
	step [18/249], loss=6.8985
	step [19/249], loss=8.5417
	step [20/249], loss=6.9470
	step [21/249], loss=6.9707
	step [22/249], loss=6.8866
	step [23/249], loss=7.1786
	step [24/249], loss=6.5824
	step [25/249], loss=7.6591
	step [26/249], loss=8.0195
	step [27/249], loss=8.4510
	step [28/249], loss=7.1429
	step [29/249], loss=6.5478
	step [30/249], loss=5.3450
	step [31/249], loss=7.1722
	step [32/249], loss=8.4959
	step [33/249], loss=6.1126
	step [34/249], loss=6.8581
	step [35/249], loss=7.8853
	step [36/249], loss=6.6781
	step [37/249], loss=7.7237
	step [38/249], loss=5.9742
	step [39/249], loss=7.5271
	step [40/249], loss=5.0713
	step [41/249], loss=8.3724
	step [42/249], loss=8.8031
	step [43/249], loss=8.0994
	step [44/249], loss=7.7577
	step [45/249], loss=6.3030
	step [46/249], loss=6.0346
	step [47/249], loss=6.6923
	step [48/249], loss=8.1944
	step [49/249], loss=7.4380
	step [50/249], loss=6.6923
	step [51/249], loss=7.0900
	step [52/249], loss=6.5455
	step [53/249], loss=7.0650
	step [54/249], loss=9.3733
	step [55/249], loss=6.8070
	step [56/249], loss=6.9326
	step [57/249], loss=7.9225
	step [58/249], loss=5.9663
	step [59/249], loss=7.2532
	step [60/249], loss=7.6701
	step [61/249], loss=6.0323
	step [62/249], loss=6.4614
	step [63/249], loss=9.1440
	step [64/249], loss=7.4082
	step [65/249], loss=6.8067
	step [66/249], loss=7.0264
	step [67/249], loss=7.5682
	step [68/249], loss=6.5760
	step [69/249], loss=8.0073
	step [70/249], loss=5.9199
	step [71/249], loss=7.5008
	step [72/249], loss=8.0978
	step [73/249], loss=7.8206
	step [74/249], loss=9.3883
	step [75/249], loss=7.5792
	step [76/249], loss=7.0585
	step [77/249], loss=7.4245
	step [78/249], loss=7.6003
	step [79/249], loss=6.1984
	step [80/249], loss=7.6895
	step [81/249], loss=7.6665
	step [82/249], loss=6.8292
	step [83/249], loss=6.5544
	step [84/249], loss=7.3698
	step [85/249], loss=9.1536
	step [86/249], loss=6.8225
	step [87/249], loss=8.1448
	step [88/249], loss=6.7560
	step [89/249], loss=6.4334
	step [90/249], loss=6.8305
	step [91/249], loss=6.7225
	step [92/249], loss=6.9365
	step [93/249], loss=7.0970
	step [94/249], loss=8.5246
	step [95/249], loss=7.8754
	step [96/249], loss=6.4865
	step [97/249], loss=6.7480
	step [98/249], loss=6.9215
	step [99/249], loss=6.3871
	step [100/249], loss=8.8171
	step [101/249], loss=6.9003
	step [102/249], loss=6.0921
	step [103/249], loss=7.2224
	step [104/249], loss=7.4562
	step [105/249], loss=6.7105
	step [106/249], loss=8.3500
	step [107/249], loss=6.3465
	step [108/249], loss=7.7504
	step [109/249], loss=8.1467
	step [110/249], loss=6.7200
	step [111/249], loss=6.4640
	step [112/249], loss=7.6550
	step [113/249], loss=7.9349
	step [114/249], loss=8.0425
	step [115/249], loss=8.0429
	step [116/249], loss=7.6234
	step [117/249], loss=6.7409
	step [118/249], loss=6.5380
	step [119/249], loss=6.6289
	step [120/249], loss=8.6409
	step [121/249], loss=5.9335
	step [122/249], loss=6.0985
	step [123/249], loss=6.7539
	step [124/249], loss=6.5957
	step [125/249], loss=7.5152
	step [126/249], loss=6.4775
	step [127/249], loss=10.4103
	step [128/249], loss=6.3908
	step [129/249], loss=6.8714
	step [130/249], loss=7.0594
	step [131/249], loss=7.2151
	step [132/249], loss=7.1444
	step [133/249], loss=6.2579
	step [134/249], loss=8.1974
	step [135/249], loss=6.4688
	step [136/249], loss=7.2261
	step [137/249], loss=6.8990
	step [138/249], loss=6.3271
	step [139/249], loss=7.8082
	step [140/249], loss=6.8545
	step [141/249], loss=7.0710
	step [142/249], loss=8.0635
	step [143/249], loss=7.0346
	step [144/249], loss=6.6115
	step [145/249], loss=6.4587
	step [146/249], loss=6.4965
	step [147/249], loss=8.3036
	step [148/249], loss=6.4505
	step [149/249], loss=7.5831
	step [150/249], loss=6.9013
	step [151/249], loss=6.6283
	step [152/249], loss=6.5329
	step [153/249], loss=6.6473
	step [154/249], loss=7.9009
	step [155/249], loss=6.7029
	step [156/249], loss=7.1236
	step [157/249], loss=7.5009
	step [158/249], loss=6.3204
	step [159/249], loss=7.7191
	step [160/249], loss=8.5272
	step [161/249], loss=6.7611
	step [162/249], loss=7.7361
	step [163/249], loss=7.2470
	step [164/249], loss=8.2288
	step [165/249], loss=6.9214
	step [166/249], loss=8.2203
	step [167/249], loss=6.5042
	step [168/249], loss=7.3695
	step [169/249], loss=7.1151
	step [170/249], loss=7.6304
	step [171/249], loss=7.1836
	step [172/249], loss=7.5548
	step [173/249], loss=6.9805
	step [174/249], loss=6.6693
	step [175/249], loss=6.9436
	step [176/249], loss=6.5724
	step [177/249], loss=8.1981
	step [178/249], loss=7.7770
	step [179/249], loss=7.1739
	step [180/249], loss=7.4621
	step [181/249], loss=8.3812
	step [182/249], loss=8.0130
	step [183/249], loss=6.8748
	step [184/249], loss=8.2563
	step [185/249], loss=8.5049
	step [186/249], loss=7.6803
	step [187/249], loss=7.1971
	step [188/249], loss=8.4111
	step [189/249], loss=7.0332
	step [190/249], loss=6.8143
	step [191/249], loss=8.4894
	step [192/249], loss=10.0633
	step [193/249], loss=7.4397
	step [194/249], loss=8.4113
	step [195/249], loss=8.9873
	step [196/249], loss=6.5463
	step [197/249], loss=6.3922
	step [198/249], loss=7.0144
	step [199/249], loss=8.2588
	step [200/249], loss=6.6380
	step [201/249], loss=7.6401
	step [202/249], loss=6.8075
	step [203/249], loss=6.7844
	step [204/249], loss=6.3196
	step [205/249], loss=6.8196
	step [206/249], loss=8.3968
	step [207/249], loss=6.9167
	step [208/249], loss=7.5920
	step [209/249], loss=6.2510
	step [210/249], loss=6.8080
	step [211/249], loss=5.7695
	step [212/249], loss=7.5846
	step [213/249], loss=6.7721
	step [214/249], loss=6.6149
	step [215/249], loss=9.2769
	step [216/249], loss=6.4468
	step [217/249], loss=6.8442
	step [218/249], loss=8.9019
	step [219/249], loss=7.2454
	step [220/249], loss=6.0951
	step [221/249], loss=8.1428
	step [222/249], loss=8.3642
	step [223/249], loss=7.2186
	step [224/249], loss=9.0705
	step [225/249], loss=6.2707
	step [226/249], loss=7.3273
	step [227/249], loss=6.6667
	step [228/249], loss=7.1010
	step [229/249], loss=6.8399
	step [230/249], loss=7.9192
	step [231/249], loss=7.5845
	step [232/249], loss=7.9071
	step [233/249], loss=8.0301
	step [234/249], loss=7.1118
	step [235/249], loss=6.9865
	step [236/249], loss=6.0959
	step [237/249], loss=6.9488
	step [238/249], loss=7.0966
	step [239/249], loss=10.4365
	step [240/249], loss=6.8677
	step [241/249], loss=5.8489
	step [242/249], loss=7.2749
	step [243/249], loss=7.1561
	step [244/249], loss=7.2756
	step [245/249], loss=7.8476
	step [246/249], loss=5.7237
	step [247/249], loss=5.7283
	step [248/249], loss=6.6042
	step [249/249], loss=6.2437
	Evaluating
	loss=0.0185, precision=0.1996, recall=0.9925, f1=0.3323
saving model as: 0_saved_model.pth
Training epoch 24
	step [1/249], loss=7.0106
	step [2/249], loss=7.6326
	step [3/249], loss=6.7178
	step [4/249], loss=5.5427
	step [5/249], loss=6.1485
	step [6/249], loss=6.9999
	step [7/249], loss=7.0711
	step [8/249], loss=7.3108
	step [9/249], loss=5.7991
	step [10/249], loss=6.1561
	step [11/249], loss=7.2368
	step [12/249], loss=7.1583
	step [13/249], loss=6.8913
	step [14/249], loss=6.0459
	step [15/249], loss=7.2109
	step [16/249], loss=7.6881
	step [17/249], loss=6.6643
	step [18/249], loss=6.8237
	step [19/249], loss=7.6417
	step [20/249], loss=7.0405
	step [21/249], loss=7.7390
	step [22/249], loss=7.1977
	step [23/249], loss=6.7489
	step [24/249], loss=8.9622
	step [25/249], loss=6.8441
	step [26/249], loss=6.9781
	step [27/249], loss=6.8194
	step [28/249], loss=7.3122
	step [29/249], loss=7.2070
	step [30/249], loss=8.0212
	step [31/249], loss=6.4619
	step [32/249], loss=7.3147
	step [33/249], loss=6.0236
	step [34/249], loss=7.9524
	step [35/249], loss=7.9469
	step [36/249], loss=6.9291
	step [37/249], loss=6.7148
	step [38/249], loss=9.1721
	step [39/249], loss=7.7836
	step [40/249], loss=6.7324
	step [41/249], loss=5.9891
	step [42/249], loss=5.7993
	step [43/249], loss=7.1317
	step [44/249], loss=7.6354
	step [45/249], loss=7.8548
	step [46/249], loss=7.9403
	step [47/249], loss=6.4425
	step [48/249], loss=7.8359
	step [49/249], loss=8.3997
	step [50/249], loss=7.0244
	step [51/249], loss=7.0770
	step [52/249], loss=6.4878
	step [53/249], loss=7.3440
	step [54/249], loss=7.1862
	step [55/249], loss=6.6720
	step [56/249], loss=8.0359
	step [57/249], loss=8.2329
	step [58/249], loss=7.2779
	step [59/249], loss=6.5937
	step [60/249], loss=6.9675
	step [61/249], loss=6.6362
	step [62/249], loss=6.6014
	step [63/249], loss=7.3047
	step [64/249], loss=6.4140
	step [65/249], loss=6.5989
	step [66/249], loss=7.0634
	step [67/249], loss=7.3002
	step [68/249], loss=6.5070
	step [69/249], loss=6.8363
	step [70/249], loss=6.8430
	step [71/249], loss=8.4648
	step [72/249], loss=7.2459
	step [73/249], loss=7.7961
	step [74/249], loss=6.7727
	step [75/249], loss=9.5929
	step [76/249], loss=7.8810
	step [77/249], loss=5.6843
	step [78/249], loss=8.7998
	step [79/249], loss=6.6866
	step [80/249], loss=5.6606
	step [81/249], loss=6.4708
	step [82/249], loss=9.0892
	step [83/249], loss=7.0152
	step [84/249], loss=6.9421
	step [85/249], loss=7.1783
	step [86/249], loss=7.1450
	step [87/249], loss=6.9258
	step [88/249], loss=6.8161
	step [89/249], loss=8.0007
	step [90/249], loss=7.1396
	step [91/249], loss=6.9566
	step [92/249], loss=5.6413
	step [93/249], loss=8.3326
	step [94/249], loss=7.5367
	step [95/249], loss=7.5636
	step [96/249], loss=7.3086
	step [97/249], loss=7.0427
	step [98/249], loss=7.3523
	step [99/249], loss=8.3069
	step [100/249], loss=6.5746
	step [101/249], loss=7.8238
	step [102/249], loss=6.6777
	step [103/249], loss=7.1110
	step [104/249], loss=6.9010
	step [105/249], loss=6.3924
	step [106/249], loss=7.0175
	step [107/249], loss=8.3121
	step [108/249], loss=9.2417
	step [109/249], loss=6.9070
	step [110/249], loss=5.8067
	step [111/249], loss=7.0787
	step [112/249], loss=6.6780
	step [113/249], loss=7.0420
	step [114/249], loss=8.1062
	step [115/249], loss=7.1229
	step [116/249], loss=8.7299
	step [117/249], loss=8.1695
	step [118/249], loss=9.2564
	step [119/249], loss=7.5765
	step [120/249], loss=6.5389
	step [121/249], loss=7.1985
	step [122/249], loss=7.2898
	step [123/249], loss=6.2234
	step [124/249], loss=8.1557
	step [125/249], loss=7.1031
	step [126/249], loss=6.8334
	step [127/249], loss=6.5143
	step [128/249], loss=6.7595
	step [129/249], loss=7.5435
	step [130/249], loss=6.2869
	step [131/249], loss=6.8825
	step [132/249], loss=6.7078
	step [133/249], loss=5.6660
	step [134/249], loss=8.4297
	step [135/249], loss=7.5623
	step [136/249], loss=6.9365
	step [137/249], loss=7.3083
	step [138/249], loss=7.3887
	step [139/249], loss=6.6079
	step [140/249], loss=7.5674
	step [141/249], loss=7.5110
	step [142/249], loss=7.2042
	step [143/249], loss=6.4650
	step [144/249], loss=8.2416
	step [145/249], loss=7.0897
	step [146/249], loss=7.6117
	step [147/249], loss=6.1793
	step [148/249], loss=7.2767
	step [149/249], loss=6.4924
	step [150/249], loss=8.1233
	step [151/249], loss=6.2698
	step [152/249], loss=6.7589
	step [153/249], loss=5.5847
	step [154/249], loss=8.6913
	step [155/249], loss=6.4487
	step [156/249], loss=6.2036
	step [157/249], loss=7.2575
	step [158/249], loss=7.1100
	step [159/249], loss=6.9793
	step [160/249], loss=7.4368
	step [161/249], loss=5.8345
	step [162/249], loss=10.5723
	step [163/249], loss=6.4079
	step [164/249], loss=7.4669
	step [165/249], loss=6.0170
	step [166/249], loss=6.6849
	step [167/249], loss=6.3727
	step [168/249], loss=6.0481
	step [169/249], loss=7.1451
	step [170/249], loss=6.3853
	step [171/249], loss=6.9796
	step [172/249], loss=8.6933
	step [173/249], loss=5.9017
	step [174/249], loss=6.6683
	step [175/249], loss=7.5015
	step [176/249], loss=7.7261
	step [177/249], loss=8.5812
	step [178/249], loss=7.1289
	step [179/249], loss=6.1792
	step [180/249], loss=6.9207
	step [181/249], loss=8.1868
	step [182/249], loss=7.2906
	step [183/249], loss=6.0475
	step [184/249], loss=6.7754
	step [185/249], loss=6.2143
	step [186/249], loss=8.5051
	step [187/249], loss=6.5715
	step [188/249], loss=5.6666
	step [189/249], loss=5.8748
	step [190/249], loss=6.3126
	step [191/249], loss=6.1924
	step [192/249], loss=8.5942
	step [193/249], loss=6.2573
	step [194/249], loss=5.6913
	step [195/249], loss=6.6580
	step [196/249], loss=7.1082
	step [197/249], loss=7.6076
	step [198/249], loss=7.1466
	step [199/249], loss=6.5130
	step [200/249], loss=7.1099
	step [201/249], loss=6.0536
	step [202/249], loss=8.0056
	step [203/249], loss=8.1635
	step [204/249], loss=6.5225
	step [205/249], loss=6.9469
	step [206/249], loss=6.0542
	step [207/249], loss=6.9303
	step [208/249], loss=6.3745
	step [209/249], loss=7.0356
	step [210/249], loss=6.5356
	step [211/249], loss=6.9327
	step [212/249], loss=4.9599
	step [213/249], loss=7.3022
	step [214/249], loss=6.4323
	step [215/249], loss=6.7410
	step [216/249], loss=5.7208
	step [217/249], loss=6.7283
	step [218/249], loss=8.1341
	step [219/249], loss=5.9654
	step [220/249], loss=7.3586
	step [221/249], loss=7.7222
	step [222/249], loss=7.3712
	step [223/249], loss=7.4714
	step [224/249], loss=7.3689
	step [225/249], loss=6.2685
	step [226/249], loss=6.9854
	step [227/249], loss=8.2271
	step [228/249], loss=6.9617
	step [229/249], loss=6.6680
	step [230/249], loss=8.3404
	step [231/249], loss=7.0806
	step [232/249], loss=8.8322
	step [233/249], loss=6.9372
	step [234/249], loss=6.0533
	step [235/249], loss=6.8972
	step [236/249], loss=6.3531
	step [237/249], loss=9.2498
	step [238/249], loss=8.7708
	step [239/249], loss=6.0214
	step [240/249], loss=5.7868
	step [241/249], loss=6.6585
	step [242/249], loss=7.0151
	step [243/249], loss=7.2954
	step [244/249], loss=10.7872
	step [245/249], loss=5.9073
	step [246/249], loss=7.5804
	step [247/249], loss=6.0449
	step [248/249], loss=6.5288
	step [249/249], loss=3.7455
	Evaluating
	loss=0.0187, precision=0.1955, recall=0.9925, f1=0.3267
Training epoch 25
	step [1/249], loss=6.9535
	step [2/249], loss=6.3151
	step [3/249], loss=6.6307
	step [4/249], loss=6.6278
	step [5/249], loss=6.7767
	step [6/249], loss=6.3145
	step [7/249], loss=5.1493
	step [8/249], loss=8.5412
	step [9/249], loss=6.9730
	step [10/249], loss=6.6124
	step [11/249], loss=7.0114
	step [12/249], loss=6.2772
	step [13/249], loss=6.1995
	step [14/249], loss=6.3010
	step [15/249], loss=6.1260
	step [16/249], loss=6.7241
	step [17/249], loss=5.3305
	step [18/249], loss=6.6818
	step [19/249], loss=6.4775
	step [20/249], loss=6.2967
	step [21/249], loss=7.1569
	step [22/249], loss=6.7824
	step [23/249], loss=6.5421
	step [24/249], loss=6.0960
	step [25/249], loss=5.3038
	step [26/249], loss=7.6479
	step [27/249], loss=5.8291
	step [28/249], loss=6.7261
	step [29/249], loss=6.2410
	step [30/249], loss=6.3163
	step [31/249], loss=8.1989
	step [32/249], loss=7.1379
	step [33/249], loss=7.1658
	step [34/249], loss=6.8807
	step [35/249], loss=7.3750
	step [36/249], loss=7.4077
	step [37/249], loss=7.0270
	step [38/249], loss=6.3507
	step [39/249], loss=7.2325
	step [40/249], loss=7.4494
	step [41/249], loss=7.9778
	step [42/249], loss=7.5659
	step [43/249], loss=8.1436
	step [44/249], loss=6.5703
	step [45/249], loss=7.4219
	step [46/249], loss=6.4697
	step [47/249], loss=7.5767
	step [48/249], loss=9.1792
	step [49/249], loss=7.1547
	step [50/249], loss=8.2145
	step [51/249], loss=6.0656
	step [52/249], loss=6.4084
	step [53/249], loss=5.7939
	step [54/249], loss=5.6861
	step [55/249], loss=6.2257
	step [56/249], loss=6.4687
	step [57/249], loss=9.3373
	step [58/249], loss=7.7018
	step [59/249], loss=7.1960
	step [60/249], loss=7.9926
	step [61/249], loss=7.3937
	step [62/249], loss=7.4123
	step [63/249], loss=8.5939
	step [64/249], loss=7.7033
	step [65/249], loss=7.3054
	step [66/249], loss=7.6224
	step [67/249], loss=6.9389
	step [68/249], loss=7.0533
	step [69/249], loss=6.5987
	step [70/249], loss=7.6575
	step [71/249], loss=8.4256
	step [72/249], loss=7.5791
	step [73/249], loss=6.0721
	step [74/249], loss=6.4281
	step [75/249], loss=6.6132
	step [76/249], loss=5.4019
	step [77/249], loss=7.3688
	step [78/249], loss=8.1279
	step [79/249], loss=8.1745
	step [80/249], loss=7.4839
	step [81/249], loss=6.8874
	step [82/249], loss=6.4586
	step [83/249], loss=6.7785
	step [84/249], loss=6.2213
	step [85/249], loss=5.2171
	step [86/249], loss=6.0354
	step [87/249], loss=7.1274
	step [88/249], loss=7.0061
	step [89/249], loss=6.4697
	step [90/249], loss=6.3058
	step [91/249], loss=7.1819
	step [92/249], loss=7.0775
	step [93/249], loss=6.9161
	step [94/249], loss=6.2713
	step [95/249], loss=6.2117
	step [96/249], loss=7.2367
	step [97/249], loss=8.7362
	step [98/249], loss=5.3151
	step [99/249], loss=5.6399
	step [100/249], loss=6.8505
	step [101/249], loss=7.9918
	step [102/249], loss=6.6483
	step [103/249], loss=7.4625
	step [104/249], loss=6.7484
	step [105/249], loss=7.1820
	step [106/249], loss=6.0763
	step [107/249], loss=6.8981
	step [108/249], loss=7.7926
	step [109/249], loss=6.1162
	step [110/249], loss=8.6528
	step [111/249], loss=7.0779
	step [112/249], loss=6.2914
	step [113/249], loss=6.6090
	step [114/249], loss=7.4088
	step [115/249], loss=5.4537
	step [116/249], loss=5.9440
	step [117/249], loss=6.1798
	step [118/249], loss=6.6277
	step [119/249], loss=6.4484
	step [120/249], loss=8.4330
	step [121/249], loss=7.3372
	step [122/249], loss=7.2260
	step [123/249], loss=7.1997
	step [124/249], loss=9.9886
	step [125/249], loss=7.8157
	step [126/249], loss=6.4173
	step [127/249], loss=6.7572
	step [128/249], loss=7.3801
	step [129/249], loss=6.8562
	step [130/249], loss=7.0413
	step [131/249], loss=6.5010
	step [132/249], loss=8.6063
	step [133/249], loss=7.6222
	step [134/249], loss=7.9911
	step [135/249], loss=5.0670
	step [136/249], loss=5.5432
	step [137/249], loss=6.7140
	step [138/249], loss=6.8426
	step [139/249], loss=6.2530
	step [140/249], loss=7.3239
	step [141/249], loss=6.6855
	step [142/249], loss=5.4480
	step [143/249], loss=6.3646
	step [144/249], loss=7.1168
	step [145/249], loss=6.2971
	step [146/249], loss=6.9709
	step [147/249], loss=6.8485
	step [148/249], loss=6.4157
	step [149/249], loss=6.7314
	step [150/249], loss=7.3733
	step [151/249], loss=6.4402
	step [152/249], loss=8.7718
	step [153/249], loss=8.0472
	step [154/249], loss=6.5392
	step [155/249], loss=6.3559
	step [156/249], loss=6.5082
	step [157/249], loss=6.1325
	step [158/249], loss=8.0341
	step [159/249], loss=5.4113
	step [160/249], loss=5.4672
	step [161/249], loss=7.5755
	step [162/249], loss=7.9383
	step [163/249], loss=6.5193
	step [164/249], loss=5.8355
	step [165/249], loss=6.9855
	step [166/249], loss=7.6500
	step [167/249], loss=7.1434
	step [168/249], loss=5.9894
	step [169/249], loss=7.1480
	step [170/249], loss=7.5649
	step [171/249], loss=7.1488
	step [172/249], loss=5.7458
	step [173/249], loss=6.6123
	step [174/249], loss=7.7983
	step [175/249], loss=7.2104
	step [176/249], loss=6.6693
	step [177/249], loss=5.8221
	step [178/249], loss=7.1298
	step [179/249], loss=6.4951
	step [180/249], loss=8.7865
	step [181/249], loss=6.5814
	step [182/249], loss=5.8148
	step [183/249], loss=7.0853
	step [184/249], loss=5.3345
	step [185/249], loss=5.8179
	step [186/249], loss=7.6885
	step [187/249], loss=6.7822
	step [188/249], loss=6.7342
	step [189/249], loss=5.9819
	step [190/249], loss=7.5356
	step [191/249], loss=6.5276
	step [192/249], loss=6.3942
	step [193/249], loss=5.6079
	step [194/249], loss=7.5168
	step [195/249], loss=5.9424
	step [196/249], loss=6.5595
	step [197/249], loss=6.7755
	step [198/249], loss=6.6911
	step [199/249], loss=6.7238
	step [200/249], loss=6.1583
	step [201/249], loss=6.6150
	step [202/249], loss=7.1853
	step [203/249], loss=6.0458
	step [204/249], loss=5.7704
	step [205/249], loss=7.4438
	step [206/249], loss=7.5022
	step [207/249], loss=9.6802
	step [208/249], loss=7.1589
	step [209/249], loss=6.3310
	step [210/249], loss=6.0559
	step [211/249], loss=7.6315
	step [212/249], loss=7.1908
	step [213/249], loss=5.8355
	step [214/249], loss=6.2932
	step [215/249], loss=5.6694
	step [216/249], loss=6.2740
	step [217/249], loss=6.2541
	step [218/249], loss=7.9239
	step [219/249], loss=5.0161
	step [220/249], loss=6.9278
	step [221/249], loss=5.8917
	step [222/249], loss=7.8561
	step [223/249], loss=7.0882
	step [224/249], loss=6.1741
	step [225/249], loss=5.5245
	step [226/249], loss=9.5744
	step [227/249], loss=6.7987
	step [228/249], loss=6.0205
	step [229/249], loss=9.3144
	step [230/249], loss=7.0310
	step [231/249], loss=6.8478
	step [232/249], loss=7.3468
	step [233/249], loss=6.9486
	step [234/249], loss=6.6107
	step [235/249], loss=7.3545
	step [236/249], loss=6.8270
	step [237/249], loss=6.8727
	step [238/249], loss=6.7173
	step [239/249], loss=6.5458
	step [240/249], loss=6.2329
	step [241/249], loss=7.1733
	step [242/249], loss=5.9214
	step [243/249], loss=4.8444
	step [244/249], loss=6.5718
	step [245/249], loss=5.8843
	step [246/249], loss=5.2902
	step [247/249], loss=6.1839
	step [248/249], loss=6.6509
	step [249/249], loss=6.4302
	Evaluating
	loss=0.0196, precision=0.1840, recall=0.9931, f1=0.3105
Training epoch 26
	step [1/249], loss=7.3711
	step [2/249], loss=6.3400
	step [3/249], loss=6.4595
	step [4/249], loss=8.2993
	step [5/249], loss=6.7044
	step [6/249], loss=7.1213
	step [7/249], loss=5.4599
	step [8/249], loss=6.1568
	step [9/249], loss=6.3982
	step [10/249], loss=6.0564
	step [11/249], loss=6.0696
	step [12/249], loss=6.4999
	step [13/249], loss=6.3706
	step [14/249], loss=6.6324
	step [15/249], loss=7.8816
	step [16/249], loss=4.9401
	step [17/249], loss=6.4486
	step [18/249], loss=6.6620
	step [19/249], loss=6.5395
	step [20/249], loss=6.8060
	step [21/249], loss=8.5327
	step [22/249], loss=7.4003
	step [23/249], loss=7.0493
	step [24/249], loss=5.4837
	step [25/249], loss=7.0634
	step [26/249], loss=6.7059
	step [27/249], loss=7.1659
	step [28/249], loss=8.0595
	step [29/249], loss=6.4756
	step [30/249], loss=7.0416
	step [31/249], loss=5.9830
	step [32/249], loss=5.3795
	step [33/249], loss=5.3779
	step [34/249], loss=7.5876
	step [35/249], loss=7.6947
	step [36/249], loss=6.4577
	step [37/249], loss=7.1961
	step [38/249], loss=5.3528
	step [39/249], loss=7.8506
	step [40/249], loss=5.2825
	step [41/249], loss=7.2867
	step [42/249], loss=5.7681
	step [43/249], loss=6.3300
	step [44/249], loss=6.1126
	step [45/249], loss=5.8360
	step [46/249], loss=6.5985
	step [47/249], loss=7.0057
	step [48/249], loss=6.7578
	step [49/249], loss=6.7325
	step [50/249], loss=6.5857
	step [51/249], loss=6.7268
	step [52/249], loss=5.0064
	step [53/249], loss=6.3151
	step [54/249], loss=7.0361
	step [55/249], loss=6.5188
	step [56/249], loss=5.8317
	step [57/249], loss=6.1651
	step [58/249], loss=7.9422
	step [59/249], loss=5.7123
	step [60/249], loss=5.6472
	step [61/249], loss=7.5042
	step [62/249], loss=7.0341
	step [63/249], loss=6.2195
	step [64/249], loss=6.8462
	step [65/249], loss=5.6259
	step [66/249], loss=7.1212
	step [67/249], loss=5.9880
	step [68/249], loss=5.8676
	step [69/249], loss=7.7397
	step [70/249], loss=5.4108
	step [71/249], loss=6.7474
	step [72/249], loss=8.0206
	step [73/249], loss=6.0768
	step [74/249], loss=5.7211
	step [75/249], loss=6.7435
	step [76/249], loss=6.9850
	step [77/249], loss=10.6996
	step [78/249], loss=7.4588
	step [79/249], loss=7.2311
	step [80/249], loss=6.7473
	step [81/249], loss=6.4672
	step [82/249], loss=6.8141
	step [83/249], loss=8.1967
	step [84/249], loss=7.1013
	step [85/249], loss=5.0970
	step [86/249], loss=5.6821
	step [87/249], loss=8.9019
	step [88/249], loss=6.4333
	step [89/249], loss=6.1048
	step [90/249], loss=6.3051
	step [91/249], loss=6.4759
	step [92/249], loss=7.7608
	step [93/249], loss=6.8308
	step [94/249], loss=8.4095
	step [95/249], loss=7.2897
	step [96/249], loss=6.4552
	step [97/249], loss=5.7580
	step [98/249], loss=5.2898
	step [99/249], loss=6.1425
	step [100/249], loss=6.3062
	step [101/249], loss=6.8532
	step [102/249], loss=6.8914
	step [103/249], loss=6.2588
	step [104/249], loss=7.3104
	step [105/249], loss=6.8412
	step [106/249], loss=8.3514
	step [107/249], loss=6.5205
	step [108/249], loss=6.7753
	step [109/249], loss=5.9347
	step [110/249], loss=7.2371
	step [111/249], loss=5.8344
	step [112/249], loss=6.6984
	step [113/249], loss=5.7036
	step [114/249], loss=7.4827
	step [115/249], loss=7.1745
	step [116/249], loss=6.7952
	step [117/249], loss=8.1559
	step [118/249], loss=6.3566
	step [119/249], loss=5.5444
	step [120/249], loss=6.8407
	step [121/249], loss=6.4749
	step [122/249], loss=7.1277
	step [123/249], loss=7.0246
	step [124/249], loss=5.5137
	step [125/249], loss=7.4621
	step [126/249], loss=6.2079
	step [127/249], loss=6.5961
	step [128/249], loss=7.1214
	step [129/249], loss=9.0128
	step [130/249], loss=6.2110
	step [131/249], loss=5.5221
	step [132/249], loss=7.2186
	step [133/249], loss=6.4531
	step [134/249], loss=7.8682
	step [135/249], loss=7.9305
	step [136/249], loss=6.8782
	step [137/249], loss=6.6927
	step [138/249], loss=5.5895
	step [139/249], loss=6.8547
	step [140/249], loss=5.8758
	step [141/249], loss=6.0661
	step [142/249], loss=5.6781
	step [143/249], loss=5.6942
	step [144/249], loss=7.5570
	step [145/249], loss=6.5146
	step [146/249], loss=6.2472
	step [147/249], loss=6.0994
	step [148/249], loss=6.5682
	step [149/249], loss=7.3667
	step [150/249], loss=7.0173
	step [151/249], loss=6.4351
	step [152/249], loss=6.9253
	step [153/249], loss=5.9715
	step [154/249], loss=6.6674
	step [155/249], loss=6.1867
	step [156/249], loss=6.2760
	step [157/249], loss=7.1406
	step [158/249], loss=6.3277
	step [159/249], loss=5.6839
	step [160/249], loss=7.3697
	step [161/249], loss=5.9891
	step [162/249], loss=7.4986
	step [163/249], loss=6.8182
	step [164/249], loss=6.6811
	step [165/249], loss=6.0354
	step [166/249], loss=5.6521
	step [167/249], loss=6.3705
	step [168/249], loss=6.3674
	step [169/249], loss=7.2450
	step [170/249], loss=7.0714
	step [171/249], loss=5.7420
	step [172/249], loss=5.5614
	step [173/249], loss=7.5967
	step [174/249], loss=8.1321
	step [175/249], loss=7.8441
	step [176/249], loss=6.3278
	step [177/249], loss=6.9898
	step [178/249], loss=6.4025
	step [179/249], loss=6.9484
	step [180/249], loss=5.6128
	step [181/249], loss=8.1514
	step [182/249], loss=6.1522
	step [183/249], loss=6.9606
	step [184/249], loss=5.6365
	step [185/249], loss=5.9044
	step [186/249], loss=5.6625
	step [187/249], loss=5.0531
	step [188/249], loss=6.5908
	step [189/249], loss=8.8343
	step [190/249], loss=5.2283
	step [191/249], loss=6.2820
	step [192/249], loss=6.3407
	step [193/249], loss=6.5882
	step [194/249], loss=7.1565
	step [195/249], loss=5.8918
	step [196/249], loss=6.6678
	step [197/249], loss=7.9148
	step [198/249], loss=5.1638
	step [199/249], loss=6.3190
	step [200/249], loss=5.9596
	step [201/249], loss=7.2175
	step [202/249], loss=5.3949
	step [203/249], loss=7.3605
	step [204/249], loss=7.5449
	step [205/249], loss=5.7767
	step [206/249], loss=8.2920
	step [207/249], loss=7.4887
	step [208/249], loss=7.1439
	step [209/249], loss=7.2618
	step [210/249], loss=6.4395
	step [211/249], loss=6.3501
	step [212/249], loss=6.9496
	step [213/249], loss=7.6438
	step [214/249], loss=6.4328
	step [215/249], loss=6.3275
	step [216/249], loss=6.1705
	step [217/249], loss=7.1777
	step [218/249], loss=5.2644
	step [219/249], loss=7.7600
	step [220/249], loss=7.5761
	step [221/249], loss=6.3380
	step [222/249], loss=6.1656
	step [223/249], loss=6.7289
	step [224/249], loss=5.4849
	step [225/249], loss=8.5170
	step [226/249], loss=6.5373
	step [227/249], loss=6.4220
	step [228/249], loss=5.6818
	step [229/249], loss=5.2493
	step [230/249], loss=6.6956
	step [231/249], loss=5.6001
	step [232/249], loss=6.3518
	step [233/249], loss=6.0237
	step [234/249], loss=6.6759
	step [235/249], loss=7.6389
	step [236/249], loss=6.3845
	step [237/249], loss=5.9541
	step [238/249], loss=4.9387
	step [239/249], loss=6.5038
	step [240/249], loss=4.8367
	step [241/249], loss=6.0121
	step [242/249], loss=7.6728
	step [243/249], loss=7.9732
	step [244/249], loss=5.9191
	step [245/249], loss=4.7511
	step [246/249], loss=6.9028
	step [247/249], loss=6.4456
	step [248/249], loss=6.2268
	step [249/249], loss=5.2335
	Evaluating
	loss=0.0186, precision=0.1913, recall=0.9930, f1=0.3208
Training epoch 27
	step [1/249], loss=5.6394
	step [2/249], loss=7.9591
	step [3/249], loss=6.8738
	step [4/249], loss=6.2483
	step [5/249], loss=6.3128
	step [6/249], loss=6.5225
	step [7/249], loss=5.7084
	step [8/249], loss=5.8512
	step [9/249], loss=7.2313
	step [10/249], loss=7.5800
	step [11/249], loss=7.0489
	step [12/249], loss=5.6285
	step [13/249], loss=6.8992
	step [14/249], loss=6.8408
	step [15/249], loss=6.8735
	step [16/249], loss=5.8100
	step [17/249], loss=5.5614
	step [18/249], loss=5.5972
	step [19/249], loss=7.0011
	step [20/249], loss=7.3844
	step [21/249], loss=7.1275
	step [22/249], loss=6.7734
	step [23/249], loss=6.5742
	step [24/249], loss=8.5278
	step [25/249], loss=7.9152
	step [26/249], loss=6.5756
	step [27/249], loss=6.5806
	step [28/249], loss=6.2668
	step [29/249], loss=5.9917
	step [30/249], loss=6.7202
	step [31/249], loss=6.9770
	step [32/249], loss=6.9004
	step [33/249], loss=6.9011
	step [34/249], loss=7.1933
	step [35/249], loss=6.2256
	step [36/249], loss=6.0206
	step [37/249], loss=6.4830
	step [38/249], loss=7.2923
	step [39/249], loss=6.4932
	step [40/249], loss=5.4351
	step [41/249], loss=5.3119
	step [42/249], loss=5.4442
	step [43/249], loss=6.5189
	step [44/249], loss=5.5658
	step [45/249], loss=6.8662
	step [46/249], loss=5.8644
	step [47/249], loss=6.7858
	step [48/249], loss=7.9907
	step [49/249], loss=5.4482
	step [50/249], loss=6.7037
	step [51/249], loss=6.5880
	step [52/249], loss=7.1648
	step [53/249], loss=7.8084
	step [54/249], loss=6.5874
	step [55/249], loss=5.7186
	step [56/249], loss=6.1250
	step [57/249], loss=6.7301
	step [58/249], loss=6.7730
	step [59/249], loss=7.2986
	step [60/249], loss=6.6630
	step [61/249], loss=6.2889
	step [62/249], loss=5.5189
	step [63/249], loss=5.7157
	step [64/249], loss=5.8601
	step [65/249], loss=5.8274
	step [66/249], loss=6.5434
	step [67/249], loss=5.8361
	step [68/249], loss=5.7761
	step [69/249], loss=5.7329
	step [70/249], loss=6.3561
	step [71/249], loss=6.5159
	step [72/249], loss=5.7610
	step [73/249], loss=4.7498
	step [74/249], loss=6.2580
	step [75/249], loss=6.5687
	step [76/249], loss=5.5986
	step [77/249], loss=6.6097
	step [78/249], loss=6.4629
	step [79/249], loss=5.2343
	step [80/249], loss=7.3090
	step [81/249], loss=6.6519
	step [82/249], loss=5.5216
	step [83/249], loss=7.3501
	step [84/249], loss=6.0934
	step [85/249], loss=7.2630
	step [86/249], loss=6.6569
	step [87/249], loss=7.4555
	step [88/249], loss=7.1549
	step [89/249], loss=6.6746
	step [90/249], loss=5.7074
	step [91/249], loss=5.5426
	step [92/249], loss=7.0131
	step [93/249], loss=6.3394
	step [94/249], loss=7.0725
	step [95/249], loss=6.0120
	step [96/249], loss=7.0262
	step [97/249], loss=6.7166
	step [98/249], loss=6.5082
	step [99/249], loss=6.7403
	step [100/249], loss=4.8950
	step [101/249], loss=6.4197
	step [102/249], loss=6.5560
	step [103/249], loss=6.8702
	step [104/249], loss=7.0776
	step [105/249], loss=7.6026
	step [106/249], loss=7.3087
	step [107/249], loss=5.2971
	step [108/249], loss=7.1516
	step [109/249], loss=6.1816
	step [110/249], loss=7.1882
	step [111/249], loss=5.6887
	step [112/249], loss=5.9568
	step [113/249], loss=7.0527
	step [114/249], loss=6.9906
	step [115/249], loss=7.3006
	step [116/249], loss=5.8906
	step [117/249], loss=6.4567
	step [118/249], loss=5.8922
	step [119/249], loss=6.2911
	step [120/249], loss=6.5169
	step [121/249], loss=5.8653
	step [122/249], loss=4.8440
	step [123/249], loss=4.7446
	step [124/249], loss=6.8871
	step [125/249], loss=7.0972
	step [126/249], loss=6.3940
	step [127/249], loss=8.0726
	step [128/249], loss=7.5368
	step [129/249], loss=5.7395
	step [130/249], loss=5.6452
	step [131/249], loss=6.2968
	step [132/249], loss=6.9764
	step [133/249], loss=6.9531
	step [134/249], loss=6.9027
	step [135/249], loss=5.6352
	step [136/249], loss=5.7321
	step [137/249], loss=6.8524
	step [138/249], loss=7.2982
	step [139/249], loss=7.3639
	step [140/249], loss=5.9250
	step [141/249], loss=7.3275
	step [142/249], loss=6.2203
	step [143/249], loss=6.7246
	step [144/249], loss=5.8172
	step [145/249], loss=6.3351
	step [146/249], loss=7.8870
	step [147/249], loss=6.3044
	step [148/249], loss=7.4945
	step [149/249], loss=5.9732
	step [150/249], loss=6.2926
	step [151/249], loss=5.9498
	step [152/249], loss=4.9330
	step [153/249], loss=5.5076
	step [154/249], loss=7.9329
	step [155/249], loss=6.2315
	step [156/249], loss=6.2367
	step [157/249], loss=6.6808
	step [158/249], loss=6.3532
	step [159/249], loss=6.5994
	step [160/249], loss=6.4802
	step [161/249], loss=5.5950
	step [162/249], loss=7.2637
	step [163/249], loss=6.1762
	step [164/249], loss=5.8325
	step [165/249], loss=6.3319
	step [166/249], loss=6.3893
	step [167/249], loss=7.8342
	step [168/249], loss=7.1660
	step [169/249], loss=6.8387
	step [170/249], loss=6.8390
	step [171/249], loss=7.7775
	step [172/249], loss=5.6933
	step [173/249], loss=6.2544
	step [174/249], loss=6.8588
	step [175/249], loss=6.3705
	step [176/249], loss=6.4608
	step [177/249], loss=5.1120
	step [178/249], loss=7.5020
	step [179/249], loss=6.6360
	step [180/249], loss=6.5547
	step [181/249], loss=4.7148
	step [182/249], loss=5.7114
	step [183/249], loss=7.3316
	step [184/249], loss=5.9977
	step [185/249], loss=6.0665
	step [186/249], loss=6.2717
	step [187/249], loss=6.8048
	step [188/249], loss=5.0574
	step [189/249], loss=4.9781
	step [190/249], loss=7.0101
	step [191/249], loss=6.3605
	step [192/249], loss=5.7614
	step [193/249], loss=6.1120
	step [194/249], loss=5.4172
	step [195/249], loss=4.9558
	step [196/249], loss=6.6220
	step [197/249], loss=6.5748
	step [198/249], loss=7.0397
	step [199/249], loss=6.1495
	step [200/249], loss=5.9471
	step [201/249], loss=5.6521
	step [202/249], loss=7.1021
	step [203/249], loss=7.2716
	step [204/249], loss=6.2467
	step [205/249], loss=6.3257
	step [206/249], loss=5.5714
	step [207/249], loss=6.2346
	step [208/249], loss=7.2856
	step [209/249], loss=6.7894
	step [210/249], loss=6.8318
	step [211/249], loss=7.5047
	step [212/249], loss=6.1494
	step [213/249], loss=7.0879
	step [214/249], loss=7.2406
	step [215/249], loss=6.8812
	step [216/249], loss=6.2458
	step [217/249], loss=5.2618
	step [218/249], loss=5.7374
	step [219/249], loss=6.6579
	step [220/249], loss=7.1796
	step [221/249], loss=6.8099
	step [222/249], loss=6.4369
	step [223/249], loss=7.2347
	step [224/249], loss=5.9189
	step [225/249], loss=5.4682
	step [226/249], loss=6.2596
	step [227/249], loss=8.0475
	step [228/249], loss=6.3319
	step [229/249], loss=8.0059
	step [230/249], loss=5.8442
	step [231/249], loss=5.6704
	step [232/249], loss=7.0920
	step [233/249], loss=5.9888
	step [234/249], loss=8.4297
	step [235/249], loss=6.4371
	step [236/249], loss=5.1062
	step [237/249], loss=5.9943
	step [238/249], loss=7.4404
	step [239/249], loss=6.0512
	step [240/249], loss=7.4673
	step [241/249], loss=6.5127
	step [242/249], loss=6.5262
	step [243/249], loss=6.9466
	step [244/249], loss=6.9664
	step [245/249], loss=6.5717
	step [246/249], loss=6.0086
	step [247/249], loss=7.5658
	step [248/249], loss=7.7542
	step [249/249], loss=4.7946
	Evaluating
	loss=0.0195, precision=0.1828, recall=0.9930, f1=0.3088
Training epoch 28
	step [1/249], loss=5.6129
	step [2/249], loss=5.9612
	step [3/249], loss=6.6321
	step [4/249], loss=6.7826
	step [5/249], loss=6.9156
	step [6/249], loss=6.0279
	step [7/249], loss=5.0727
	step [8/249], loss=6.1851
	step [9/249], loss=5.7457
	step [10/249], loss=7.8173
	step [11/249], loss=6.0891
	step [12/249], loss=6.5395
	step [13/249], loss=7.0863
	step [14/249], loss=5.9220
	step [15/249], loss=6.0023
	step [16/249], loss=6.4761
	step [17/249], loss=7.8153
	step [18/249], loss=5.7177
	step [19/249], loss=5.5765
	step [20/249], loss=5.9781
	step [21/249], loss=5.2015
	step [22/249], loss=7.8018
	step [23/249], loss=6.2584
	step [24/249], loss=5.8928
	step [25/249], loss=7.6258
	step [26/249], loss=6.2688
	step [27/249], loss=5.9958
	step [28/249], loss=5.5524
	step [29/249], loss=6.2032
	step [30/249], loss=6.9992
	step [31/249], loss=6.6245
	step [32/249], loss=6.6835
	step [33/249], loss=7.4178
	step [34/249], loss=4.5049
	step [35/249], loss=6.8347
	step [36/249], loss=5.9604
	step [37/249], loss=7.4417
	step [38/249], loss=5.6672
	step [39/249], loss=5.0765
	step [40/249], loss=6.5523
	step [41/249], loss=6.6407
	step [42/249], loss=7.5554
	step [43/249], loss=6.7705
	step [44/249], loss=7.9669
	step [45/249], loss=4.8691
	step [46/249], loss=6.4105
	step [47/249], loss=7.6078
	step [48/249], loss=5.7515
	step [49/249], loss=4.7622
	step [50/249], loss=6.9127
	step [51/249], loss=5.7928
	step [52/249], loss=5.7104
	step [53/249], loss=5.2625
	step [54/249], loss=6.8910
	step [55/249], loss=5.7120
	step [56/249], loss=5.0591
	step [57/249], loss=6.3951
	step [58/249], loss=6.7742
	step [59/249], loss=4.8971
	step [60/249], loss=5.8253
	step [61/249], loss=6.2689
	step [62/249], loss=5.8452
	step [63/249], loss=6.8226
	step [64/249], loss=6.2000
	step [65/249], loss=6.6225
	step [66/249], loss=7.0129
	step [67/249], loss=4.6201
	step [68/249], loss=6.5428
	step [69/249], loss=6.8139
	step [70/249], loss=6.1198
	step [71/249], loss=7.1085
	step [72/249], loss=7.2682
	step [73/249], loss=6.0292
	step [74/249], loss=6.0989
	step [75/249], loss=7.7167
	step [76/249], loss=5.9802
	step [77/249], loss=6.7180
	step [78/249], loss=4.5345
	step [79/249], loss=6.0548
	step [80/249], loss=7.1052
	step [81/249], loss=6.4732
	step [82/249], loss=5.6133
	step [83/249], loss=5.1460
	step [84/249], loss=5.6203
	step [85/249], loss=5.6470
	step [86/249], loss=6.7679
	step [87/249], loss=5.1538
	step [88/249], loss=5.1979
	step [89/249], loss=6.1925
	step [90/249], loss=4.9726
	step [91/249], loss=5.7844
	step [92/249], loss=6.5223
	step [93/249], loss=5.9440
	step [94/249], loss=6.1189
	step [95/249], loss=4.7640
	step [96/249], loss=5.8932
	step [97/249], loss=6.6884
	step [98/249], loss=6.2100
	step [99/249], loss=5.9943
	step [100/249], loss=5.3290
	step [101/249], loss=6.4035
	step [102/249], loss=5.8914
	step [103/249], loss=7.8739
	step [104/249], loss=6.6410
	step [105/249], loss=5.9168
	step [106/249], loss=7.3299
	step [107/249], loss=5.9070
	step [108/249], loss=5.9501
	step [109/249], loss=6.1561
	step [110/249], loss=6.0816
	step [111/249], loss=7.3878
	step [112/249], loss=5.6346
	step [113/249], loss=6.8872
	step [114/249], loss=5.2015
	step [115/249], loss=7.7977
	step [116/249], loss=8.2100
	step [117/249], loss=5.5489
	step [118/249], loss=5.8074
	step [119/249], loss=6.6388
	step [120/249], loss=6.4265
	step [121/249], loss=6.4457
	step [122/249], loss=6.7424
	step [123/249], loss=5.8844
	step [124/249], loss=6.3343
	step [125/249], loss=6.0981
	step [126/249], loss=7.9411
	step [127/249], loss=5.0330
	step [128/249], loss=6.3296
	step [129/249], loss=5.9917
	step [130/249], loss=5.9138
	step [131/249], loss=6.2982
	step [132/249], loss=6.7837
	step [133/249], loss=5.9013
	step [134/249], loss=7.0059
	step [135/249], loss=6.4764
	step [136/249], loss=5.6087
	step [137/249], loss=6.7408
	step [138/249], loss=6.2782
	step [139/249], loss=6.1454
	step [140/249], loss=6.5545
	step [141/249], loss=5.6090
	step [142/249], loss=5.7768
	step [143/249], loss=6.2976
	step [144/249], loss=5.6721
	step [145/249], loss=6.8527
	step [146/249], loss=6.0570
	step [147/249], loss=6.2059
	step [148/249], loss=6.0159
	step [149/249], loss=6.0286
	step [150/249], loss=5.8267
	step [151/249], loss=6.8631
	step [152/249], loss=7.5343
	step [153/249], loss=7.3184
	step [154/249], loss=5.9693
	step [155/249], loss=6.2625
	step [156/249], loss=5.7764
	step [157/249], loss=6.1320
	step [158/249], loss=5.7504
	step [159/249], loss=6.5099
	step [160/249], loss=5.7429
	step [161/249], loss=6.6902
	step [162/249], loss=6.1057
	step [163/249], loss=6.3749
	step [164/249], loss=6.3523
	step [165/249], loss=7.8062
	step [166/249], loss=6.2455
	step [167/249], loss=7.4233
	step [168/249], loss=6.5156
	step [169/249], loss=6.3405
	step [170/249], loss=7.3703
	step [171/249], loss=7.2446
	step [172/249], loss=8.4999
	step [173/249], loss=5.6663
	step [174/249], loss=6.0584
	step [175/249], loss=6.1725
	step [176/249], loss=5.3680
	step [177/249], loss=6.6860
	step [178/249], loss=7.8105
	step [179/249], loss=5.6526
	step [180/249], loss=6.9188
	step [181/249], loss=6.2884
	step [182/249], loss=7.6723
	step [183/249], loss=5.2499
	step [184/249], loss=6.8357
	step [185/249], loss=5.4649
	step [186/249], loss=7.0081
	step [187/249], loss=7.7075
	step [188/249], loss=7.0501
	step [189/249], loss=6.5579
	step [190/249], loss=5.5753
	step [191/249], loss=5.6138
	step [192/249], loss=6.0364
	step [193/249], loss=5.7917
	step [194/249], loss=5.8052
	step [195/249], loss=6.0373
	step [196/249], loss=6.2255
	step [197/249], loss=6.4648
	step [198/249], loss=6.7372
	step [199/249], loss=5.7844
	step [200/249], loss=6.1884
	step [201/249], loss=5.2552
	step [202/249], loss=6.2053
	step [203/249], loss=6.1059
	step [204/249], loss=5.0503
	step [205/249], loss=6.4109
	step [206/249], loss=6.0308
	step [207/249], loss=6.0128
	step [208/249], loss=7.3052
	step [209/249], loss=5.9424
	step [210/249], loss=7.3668
	step [211/249], loss=6.0847
	step [212/249], loss=6.0257
	step [213/249], loss=7.2552
	step [214/249], loss=5.8405
	step [215/249], loss=6.8540
	step [216/249], loss=7.9921
	step [217/249], loss=6.0542
	step [218/249], loss=8.0447
	step [219/249], loss=7.0689
	step [220/249], loss=4.5953
	step [221/249], loss=7.6179
	step [222/249], loss=7.1116
	step [223/249], loss=7.0324
	step [224/249], loss=5.7395
	step [225/249], loss=7.3333
	step [226/249], loss=7.2477
	step [227/249], loss=6.1409
	step [228/249], loss=7.7259
	step [229/249], loss=8.2693
	step [230/249], loss=5.1321
	step [231/249], loss=6.6581
	step [232/249], loss=6.1823
	step [233/249], loss=6.2920
	step [234/249], loss=5.2499
	step [235/249], loss=6.6543
	step [236/249], loss=5.5002
	step [237/249], loss=6.4363
	step [238/249], loss=5.6808
	step [239/249], loss=6.7725
	step [240/249], loss=7.2328
	step [241/249], loss=5.6818
	step [242/249], loss=5.0769
	step [243/249], loss=7.8262
	step [244/249], loss=6.0692
	step [245/249], loss=5.4487
	step [246/249], loss=6.3072
	step [247/249], loss=5.1786
	step [248/249], loss=6.6673
	step [249/249], loss=4.9072
	Evaluating
	loss=0.0172, precision=0.2023, recall=0.9922, f1=0.3361
saving model as: 0_saved_model.pth
Training epoch 29
	step [1/249], loss=5.9740
	step [2/249], loss=5.8586
	step [3/249], loss=5.5568
	step [4/249], loss=5.3366
	step [5/249], loss=6.3899
	step [6/249], loss=5.7830
	step [7/249], loss=5.8345
	step [8/249], loss=5.6004
	step [9/249], loss=7.1893
	step [10/249], loss=5.9163
	step [11/249], loss=5.2654
	step [12/249], loss=5.7346
	step [13/249], loss=5.7050
	step [14/249], loss=6.0456
	step [15/249], loss=6.1769
	step [16/249], loss=5.6117
	step [17/249], loss=5.5540
	step [18/249], loss=7.4561
	step [19/249], loss=6.2166
	step [20/249], loss=6.1153
	step [21/249], loss=6.1974
	step [22/249], loss=6.9887
	step [23/249], loss=5.9466
	step [24/249], loss=5.7645
	step [25/249], loss=5.5056
	step [26/249], loss=5.6262
	step [27/249], loss=7.4057
	step [28/249], loss=6.2755
	step [29/249], loss=5.7181
	step [30/249], loss=6.0091
	step [31/249], loss=6.6646
	step [32/249], loss=5.3237
	step [33/249], loss=5.8853
	step [34/249], loss=5.6837
	step [35/249], loss=5.9072
	step [36/249], loss=6.2383
	step [37/249], loss=5.6823
	step [38/249], loss=5.3469
	step [39/249], loss=5.5073
	step [40/249], loss=5.7317
	step [41/249], loss=5.7834
	step [42/249], loss=5.5073
	step [43/249], loss=5.6045
	step [44/249], loss=6.7631
	step [45/249], loss=5.3391
	step [46/249], loss=5.8612
	step [47/249], loss=5.6701
	step [48/249], loss=7.0899
	step [49/249], loss=6.1161
	step [50/249], loss=6.8574
	step [51/249], loss=5.5832
	step [52/249], loss=5.5856
	step [53/249], loss=6.8315
	step [54/249], loss=6.1425
	step [55/249], loss=6.1283
	step [56/249], loss=6.0106
	step [57/249], loss=5.2781
	step [58/249], loss=7.4779
	step [59/249], loss=6.6785
	step [60/249], loss=7.3340
	step [61/249], loss=6.0201
	step [62/249], loss=6.2671
	step [63/249], loss=4.4908
	step [64/249], loss=5.1678
	step [65/249], loss=6.4879
	step [66/249], loss=6.5601
	step [67/249], loss=6.3980
	step [68/249], loss=5.6446
	step [69/249], loss=5.9463
	step [70/249], loss=5.5594
	step [71/249], loss=5.3946
	step [72/249], loss=6.5074
	step [73/249], loss=5.6062
	step [74/249], loss=6.3615
	step [75/249], loss=6.6690
	step [76/249], loss=5.9306
	step [77/249], loss=6.6390
	step [78/249], loss=8.2617
	step [79/249], loss=5.1575
	step [80/249], loss=5.9254
	step [81/249], loss=5.8046
	step [82/249], loss=6.7033
	step [83/249], loss=6.6242
	step [84/249], loss=6.9589
	step [85/249], loss=6.3354
	step [86/249], loss=9.7918
	step [87/249], loss=6.6214
	step [88/249], loss=8.3643
	step [89/249], loss=6.4153
	step [90/249], loss=6.6896
	step [91/249], loss=5.7800
	step [92/249], loss=4.5771
	step [93/249], loss=8.5688
	step [94/249], loss=4.6706
	step [95/249], loss=5.7663
	step [96/249], loss=5.9595
	step [97/249], loss=6.0186
	step [98/249], loss=6.5898
	step [99/249], loss=5.8135
	step [100/249], loss=5.0741
	step [101/249], loss=6.5425
	step [102/249], loss=5.0067
	step [103/249], loss=6.4386
	step [104/249], loss=6.8142
	step [105/249], loss=6.3915
	step [106/249], loss=7.1444
	step [107/249], loss=5.8213
	step [108/249], loss=6.0590
	step [109/249], loss=4.9641
	step [110/249], loss=6.2624
	step [111/249], loss=6.2927
	step [112/249], loss=6.8303
	step [113/249], loss=6.8384
	step [114/249], loss=6.0519
	step [115/249], loss=5.2529
	step [116/249], loss=5.5460
	step [117/249], loss=7.5102
	step [118/249], loss=8.4354
	step [119/249], loss=5.7652
	step [120/249], loss=6.5665
	step [121/249], loss=6.4507
	step [122/249], loss=7.2904
	step [123/249], loss=5.7767
	step [124/249], loss=6.8796
	step [125/249], loss=5.7758
	step [126/249], loss=6.8695
	step [127/249], loss=6.7115
	step [128/249], loss=6.1266
	step [129/249], loss=5.6675
	step [130/249], loss=7.7682
	step [131/249], loss=6.6312
	step [132/249], loss=6.5380
	step [133/249], loss=6.4265
	step [134/249], loss=5.9090
	step [135/249], loss=6.2212
	step [136/249], loss=5.8547
	step [137/249], loss=6.6214
	step [138/249], loss=7.7289
	step [139/249], loss=5.8673
	step [140/249], loss=6.3186
	step [141/249], loss=5.9886
	step [142/249], loss=5.5789
	step [143/249], loss=5.2553
	step [144/249], loss=5.7454
	step [145/249], loss=5.0945
	step [146/249], loss=6.3038
	step [147/249], loss=6.6738
	step [148/249], loss=6.8151
	step [149/249], loss=5.3508
	step [150/249], loss=5.6041
	step [151/249], loss=8.0810
	step [152/249], loss=7.3951
	step [153/249], loss=6.2749
	step [154/249], loss=6.3601
	step [155/249], loss=7.0490
	step [156/249], loss=6.3158
	step [157/249], loss=5.7885
	step [158/249], loss=5.3830
	step [159/249], loss=5.4677
	step [160/249], loss=5.9271
	step [161/249], loss=6.2573
	step [162/249], loss=6.5295
	step [163/249], loss=5.2319
	step [164/249], loss=6.1819
	step [165/249], loss=6.5289
	step [166/249], loss=5.4656
	step [167/249], loss=5.7685
	step [168/249], loss=4.8713
	step [169/249], loss=6.6239
	step [170/249], loss=6.5468
	step [171/249], loss=6.6410
	step [172/249], loss=5.9299
	step [173/249], loss=6.5925
	step [174/249], loss=5.1023
	step [175/249], loss=5.5435
	step [176/249], loss=8.0587
	step [177/249], loss=5.6870
	step [178/249], loss=5.0340
	step [179/249], loss=6.6836
	step [180/249], loss=6.3829
	step [181/249], loss=8.5030
	step [182/249], loss=6.4962
	step [183/249], loss=5.6771
	step [184/249], loss=7.4889
	step [185/249], loss=5.7240
	step [186/249], loss=4.8074
	step [187/249], loss=7.1281
	step [188/249], loss=5.5743
	step [189/249], loss=6.1833
	step [190/249], loss=5.8575
	step [191/249], loss=5.9937
	step [192/249], loss=5.0701
	step [193/249], loss=6.6040
	step [194/249], loss=5.6289
	step [195/249], loss=6.3302
	step [196/249], loss=6.4926
	step [197/249], loss=5.7061
	step [198/249], loss=6.0540
	step [199/249], loss=6.9589
	step [200/249], loss=6.9069
	step [201/249], loss=5.6430
	step [202/249], loss=6.2506
	step [203/249], loss=6.1198
	step [204/249], loss=6.8512
	step [205/249], loss=6.5824
	step [206/249], loss=6.2962
	step [207/249], loss=7.1870
	step [208/249], loss=6.0218
	step [209/249], loss=5.5029
	step [210/249], loss=6.7322
	step [211/249], loss=5.4150
	step [212/249], loss=5.4807
	step [213/249], loss=6.4706
	step [214/249], loss=5.4622
	step [215/249], loss=5.0020
	step [216/249], loss=6.3674
	step [217/249], loss=4.9956
	step [218/249], loss=5.0045
	step [219/249], loss=7.2784
	step [220/249], loss=7.4681
	step [221/249], loss=7.5897
	step [222/249], loss=5.3831
	step [223/249], loss=6.1646
	step [224/249], loss=4.9019
	step [225/249], loss=5.4426
	step [226/249], loss=5.3446
	step [227/249], loss=5.6745
	step [228/249], loss=7.8303
	step [229/249], loss=5.3017
	step [230/249], loss=5.9245
	step [231/249], loss=8.7175
	step [232/249], loss=6.4179
	step [233/249], loss=6.6940
	step [234/249], loss=7.1036
	step [235/249], loss=5.6114
	step [236/249], loss=5.5175
	step [237/249], loss=6.8556
	step [238/249], loss=5.4656
	step [239/249], loss=7.2660
	step [240/249], loss=7.3999
	step [241/249], loss=6.6244
	step [242/249], loss=4.7056
	step [243/249], loss=6.6453
	step [244/249], loss=6.7480
	step [245/249], loss=6.4289
	step [246/249], loss=5.0382
	step [247/249], loss=5.1221
	step [248/249], loss=6.9820
	step [249/249], loss=4.6943
	Evaluating
	loss=0.0190, precision=0.1895, recall=0.9930, f1=0.3182
Training epoch 30
	step [1/249], loss=5.4787
	step [2/249], loss=6.7464
	step [3/249], loss=5.4861
	step [4/249], loss=7.5051
	step [5/249], loss=4.9497
	step [6/249], loss=6.7329
	step [7/249], loss=5.9080
	step [8/249], loss=7.3115
	step [9/249], loss=5.9979
	step [10/249], loss=4.8541
	step [11/249], loss=5.4567
	step [12/249], loss=5.4888
	step [13/249], loss=5.8692
	step [14/249], loss=6.0078
	step [15/249], loss=4.6178
	step [16/249], loss=4.9363
	step [17/249], loss=6.5124
	step [18/249], loss=6.1385
	step [19/249], loss=5.5444
	step [20/249], loss=5.9570
	step [21/249], loss=6.5290
	step [22/249], loss=5.3815
	step [23/249], loss=6.0236
	step [24/249], loss=5.4373
	step [25/249], loss=5.9189
	step [26/249], loss=5.9252
	step [27/249], loss=5.3919
	step [28/249], loss=5.4384
	step [29/249], loss=6.0995
	step [30/249], loss=6.1400
	step [31/249], loss=6.0031
	step [32/249], loss=5.6289
	step [33/249], loss=5.6316
	step [34/249], loss=6.5751
	step [35/249], loss=5.6874
	step [36/249], loss=5.7421
	step [37/249], loss=4.8315
	step [38/249], loss=5.9954
	step [39/249], loss=6.2678
	step [40/249], loss=6.9211
	step [41/249], loss=6.6637
	step [42/249], loss=5.1470
	step [43/249], loss=5.9465
	step [44/249], loss=6.1573
	step [45/249], loss=6.5370
	step [46/249], loss=6.1294
	step [47/249], loss=6.4353
	step [48/249], loss=5.7755
	step [49/249], loss=5.5867
	step [50/249], loss=5.5937
	step [51/249], loss=4.8265
	step [52/249], loss=6.9510
	step [53/249], loss=6.7113
	step [54/249], loss=4.1832
	step [55/249], loss=5.9232
	step [56/249], loss=6.4525
	step [57/249], loss=5.6401
	step [58/249], loss=5.3640
	step [59/249], loss=6.3808
	step [60/249], loss=7.2435
	step [61/249], loss=6.6350
	step [62/249], loss=5.6800
	step [63/249], loss=6.0001
	step [64/249], loss=6.6160
	step [65/249], loss=5.9559
	step [66/249], loss=6.0655
	step [67/249], loss=5.7705
	step [68/249], loss=5.8613
	step [69/249], loss=6.3556
	step [70/249], loss=6.9335
	step [71/249], loss=8.3534
	step [72/249], loss=6.8136
	step [73/249], loss=6.2587
	step [74/249], loss=4.4151
	step [75/249], loss=6.6726
	step [76/249], loss=6.1044
	step [77/249], loss=6.6206
	step [78/249], loss=5.0229
	step [79/249], loss=5.1969
	step [80/249], loss=6.0908
	step [81/249], loss=5.3354
	step [82/249], loss=6.4067
	step [83/249], loss=5.6961
	step [84/249], loss=4.9282
	step [85/249], loss=6.2023
	step [86/249], loss=7.1460
	step [87/249], loss=6.9203
	step [88/249], loss=6.7173
	step [89/249], loss=6.0801
	step [90/249], loss=6.1914
	step [91/249], loss=5.1747
	step [92/249], loss=6.7901
	step [93/249], loss=5.3099
	step [94/249], loss=5.7505
	step [95/249], loss=6.2428
	step [96/249], loss=5.5591
	step [97/249], loss=5.2438
	step [98/249], loss=5.5420
	step [99/249], loss=6.0525
	step [100/249], loss=5.5336
	step [101/249], loss=5.6424
	step [102/249], loss=5.6077
	step [103/249], loss=4.6358
	step [104/249], loss=5.6502
	step [105/249], loss=6.5082
	step [106/249], loss=6.1212
	step [107/249], loss=7.4681
	step [108/249], loss=5.6307
	step [109/249], loss=5.4375
	step [110/249], loss=5.2852
	step [111/249], loss=7.4096
	step [112/249], loss=5.1923
	step [113/249], loss=5.5179
	step [114/249], loss=5.6048
	step [115/249], loss=6.3707
	step [116/249], loss=5.6275
	step [117/249], loss=5.4704
	step [118/249], loss=5.7655
	step [119/249], loss=5.8287
	step [120/249], loss=6.0669
	step [121/249], loss=5.1605
	step [122/249], loss=5.2171
	step [123/249], loss=6.4512
	step [124/249], loss=5.9617
	step [125/249], loss=5.4660
	step [126/249], loss=5.5528
	step [127/249], loss=5.7632
	step [128/249], loss=5.8866
	step [129/249], loss=5.5200
	step [130/249], loss=5.7730
	step [131/249], loss=6.2802
	step [132/249], loss=5.3130
	step [133/249], loss=5.4581
	step [134/249], loss=5.0259
	step [135/249], loss=5.7956
	step [136/249], loss=5.2071
	step [137/249], loss=5.8041
	step [138/249], loss=7.0847
	step [139/249], loss=7.7212
	step [140/249], loss=6.1041
	step [141/249], loss=5.3125
	step [142/249], loss=6.3365
	step [143/249], loss=5.4755
	step [144/249], loss=6.2457
	step [145/249], loss=5.4245
	step [146/249], loss=5.9881
	step [147/249], loss=4.7063
	step [148/249], loss=7.1273
	step [149/249], loss=4.6127
	step [150/249], loss=5.2313
	step [151/249], loss=6.4824
	step [152/249], loss=7.2106
	step [153/249], loss=6.1301
	step [154/249], loss=7.4495
	step [155/249], loss=6.6287
	step [156/249], loss=5.2926
	step [157/249], loss=6.8051
	step [158/249], loss=6.0288
	step [159/249], loss=6.3236
	step [160/249], loss=6.1519
	step [161/249], loss=5.9764
	step [162/249], loss=6.4081
	step [163/249], loss=5.2797
	step [164/249], loss=5.5562
	step [165/249], loss=5.3729
	step [166/249], loss=4.8029
	step [167/249], loss=4.1856
	step [168/249], loss=6.6202
	step [169/249], loss=6.4581
	step [170/249], loss=5.4555
	step [171/249], loss=6.4553
	step [172/249], loss=5.7120
	step [173/249], loss=5.3677
	step [174/249], loss=5.7509
	step [175/249], loss=6.9485
	step [176/249], loss=6.3199
	step [177/249], loss=7.3871
	step [178/249], loss=4.9081
	step [179/249], loss=5.3691
	step [180/249], loss=6.0174
	step [181/249], loss=6.6336
	step [182/249], loss=7.0352
	step [183/249], loss=6.1720
	step [184/249], loss=6.3527
	step [185/249], loss=6.1205
	step [186/249], loss=4.9390
	step [187/249], loss=5.9079
	step [188/249], loss=6.1279
	step [189/249], loss=6.3930
	step [190/249], loss=7.5915
	step [191/249], loss=6.0792
	step [192/249], loss=6.2578
	step [193/249], loss=5.8297
	step [194/249], loss=6.5185
	step [195/249], loss=5.2696
	step [196/249], loss=6.1909
	step [197/249], loss=6.3594
	step [198/249], loss=7.5276
	step [199/249], loss=5.5068
	step [200/249], loss=5.2627
	step [201/249], loss=7.4794
	step [202/249], loss=6.4695
	step [203/249], loss=6.5479
	step [204/249], loss=5.2104
	step [205/249], loss=4.7835
	step [206/249], loss=4.7903
	step [207/249], loss=6.0541
	step [208/249], loss=5.9065
	step [209/249], loss=6.1853
	step [210/249], loss=7.1609
	step [211/249], loss=5.7112
	step [212/249], loss=7.0477
	step [213/249], loss=5.3083
	step [214/249], loss=5.3172
	step [215/249], loss=6.8722
	step [216/249], loss=6.6987
	step [217/249], loss=6.5819
	step [218/249], loss=5.6586
	step [219/249], loss=6.9851
	step [220/249], loss=7.1407
	step [221/249], loss=4.9638
	step [222/249], loss=7.2354
	step [223/249], loss=5.6363
	step [224/249], loss=6.0259
	step [225/249], loss=6.1720
	step [226/249], loss=6.3624
	step [227/249], loss=6.7179
	step [228/249], loss=5.6269
	step [229/249], loss=5.8929
	step [230/249], loss=5.8973
	step [231/249], loss=5.4406
	step [232/249], loss=5.3322
	step [233/249], loss=5.0819
	step [234/249], loss=6.1824
	step [235/249], loss=6.3612
	step [236/249], loss=5.4120
	step [237/249], loss=6.6422
	step [238/249], loss=6.1646
	step [239/249], loss=6.3865
	step [240/249], loss=8.9249
	step [241/249], loss=5.7006
	step [242/249], loss=6.1181
	step [243/249], loss=9.0294
	step [244/249], loss=6.3827
	step [245/249], loss=7.7912
	step [246/249], loss=6.2008
	step [247/249], loss=5.8546
	step [248/249], loss=7.8044
	step [249/249], loss=3.9081
	Evaluating
	loss=0.0180, precision=0.1902, recall=0.9927, f1=0.3192
Training finished
best_f1: 0.33607242949987653
directing: Z rim_enhanced: False test_id 0
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12263 # image files with weight 12232
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3256 # image files with weight 3252
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12232
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/192], loss=326.9948
	step [2/192], loss=264.7426
	step [3/192], loss=219.4819
	step [4/192], loss=210.9692
	step [5/192], loss=190.2082
	step [6/192], loss=188.0526
	step [7/192], loss=182.1472
	step [8/192], loss=177.8064
	step [9/192], loss=174.0516
	step [10/192], loss=172.7446
	step [11/192], loss=169.8229
	step [12/192], loss=169.2134
	step [13/192], loss=167.0586
	step [14/192], loss=163.8808
	step [15/192], loss=162.5744
	step [16/192], loss=161.3061
	step [17/192], loss=160.2974
	step [18/192], loss=157.7064
	step [19/192], loss=154.9131
	step [20/192], loss=152.6548
	step [21/192], loss=152.7650
	step [22/192], loss=149.5470
	step [23/192], loss=148.5818
	step [24/192], loss=146.6867
	step [25/192], loss=144.2466
	step [26/192], loss=144.6999
	step [27/192], loss=140.2166
	step [28/192], loss=139.9489
	step [29/192], loss=139.6236
	step [30/192], loss=136.2017
	step [31/192], loss=135.2093
	step [32/192], loss=133.6918
	step [33/192], loss=131.2219
	step [34/192], loss=128.9014
	step [35/192], loss=128.2211
	step [36/192], loss=128.5154
	step [37/192], loss=127.6036
	step [38/192], loss=127.2058
	step [39/192], loss=121.9945
	step [40/192], loss=123.5902
	step [41/192], loss=123.7126
	step [42/192], loss=121.8226
	step [43/192], loss=119.7859
	step [44/192], loss=118.9883
	step [45/192], loss=115.5522
	step [46/192], loss=114.5551
	step [47/192], loss=113.2628
	step [48/192], loss=115.8138
	step [49/192], loss=114.6438
	step [50/192], loss=113.3838
	step [51/192], loss=111.9150
	step [52/192], loss=110.0033
	step [53/192], loss=110.6843
	step [54/192], loss=107.5011
	step [55/192], loss=107.8821
	step [56/192], loss=105.9177
	step [57/192], loss=108.7430
	step [58/192], loss=105.4045
	step [59/192], loss=104.4687
	step [60/192], loss=104.4748
	step [61/192], loss=102.9525
	step [62/192], loss=103.3892
	step [63/192], loss=101.7592
	step [64/192], loss=102.2379
	step [65/192], loss=100.1269
	step [66/192], loss=100.1381
	step [67/192], loss=100.5156
	step [68/192], loss=98.2760
	step [69/192], loss=98.5952
	step [70/192], loss=98.2953
	step [71/192], loss=97.9928
	step [72/192], loss=97.1682
	step [73/192], loss=98.2864
	step [74/192], loss=97.3674
	step [75/192], loss=96.6581
	step [76/192], loss=96.4999
	step [77/192], loss=94.3854
	step [78/192], loss=93.2990
	step [79/192], loss=93.3593
	step [80/192], loss=97.0487
	step [81/192], loss=94.9794
	step [82/192], loss=94.9934
	step [83/192], loss=92.0736
	step [84/192], loss=94.3995
	step [85/192], loss=91.8541
	step [86/192], loss=94.8861
	step [87/192], loss=91.8429
	step [88/192], loss=90.2309
	step [89/192], loss=94.0078
	step [90/192], loss=88.9342
	step [91/192], loss=93.1882
	step [92/192], loss=90.8653
	step [93/192], loss=91.1839
	step [94/192], loss=90.5964
	step [95/192], loss=91.1209
	step [96/192], loss=89.8946
	step [97/192], loss=90.3790
	step [98/192], loss=89.6770
	step [99/192], loss=87.9947
	step [100/192], loss=87.9544
	step [101/192], loss=87.0409
	step [102/192], loss=86.4313
	step [103/192], loss=88.6916
	step [104/192], loss=85.4856
	step [105/192], loss=84.9306
	step [106/192], loss=89.2100
	step [107/192], loss=86.5989
	step [108/192], loss=88.3601
	step [109/192], loss=87.0560
	step [110/192], loss=85.2672
	step [111/192], loss=85.3009
	step [112/192], loss=87.0561
	step [113/192], loss=87.8374
	step [114/192], loss=85.4711
	step [115/192], loss=87.8484
	step [116/192], loss=84.9315
	step [117/192], loss=84.0788
	step [118/192], loss=84.1833
	step [119/192], loss=86.5574
	step [120/192], loss=84.1301
	step [121/192], loss=83.0775
	step [122/192], loss=83.3045
	step [123/192], loss=82.7064
	step [124/192], loss=83.6983
	step [125/192], loss=84.7379
	step [126/192], loss=83.3471
	step [127/192], loss=82.9451
	step [128/192], loss=80.6027
	step [129/192], loss=83.6129
	step [130/192], loss=82.6473
	step [131/192], loss=82.4954
	step [132/192], loss=84.8727
	step [133/192], loss=83.1334
	step [134/192], loss=81.1472
	step [135/192], loss=85.9914
	step [136/192], loss=83.8234
	step [137/192], loss=84.5791
	step [138/192], loss=79.7499
	step [139/192], loss=84.5604
	step [140/192], loss=80.8365
	step [141/192], loss=81.1692
	step [142/192], loss=82.0104
	step [143/192], loss=80.5670
	step [144/192], loss=81.4789
	step [145/192], loss=81.2131
	step [146/192], loss=79.8774
	step [147/192], loss=80.7207
	step [148/192], loss=81.0403
	step [149/192], loss=81.4746
	step [150/192], loss=81.0847
	step [151/192], loss=77.9814
	step [152/192], loss=79.9712
	step [153/192], loss=77.0237
	step [154/192], loss=78.9335
	step [155/192], loss=81.1357
	step [156/192], loss=81.6763
	step [157/192], loss=78.6336
	step [158/192], loss=78.0185
	step [159/192], loss=77.2242
	step [160/192], loss=78.5836
	step [161/192], loss=81.4711
	step [162/192], loss=76.8411
	step [163/192], loss=80.1797
	step [164/192], loss=77.9837
	step [165/192], loss=77.6703
	step [166/192], loss=78.6253
	step [167/192], loss=78.9013
	step [168/192], loss=79.1284
	step [169/192], loss=80.6141
	step [170/192], loss=76.4138
	step [171/192], loss=77.3723
	step [172/192], loss=77.9799
	step [173/192], loss=76.8666
	step [174/192], loss=77.8806
	step [175/192], loss=77.7849
	step [176/192], loss=77.1233
	step [177/192], loss=79.2358
	step [178/192], loss=75.7166
	step [179/192], loss=74.8839
	step [180/192], loss=76.1851
	step [181/192], loss=76.2451
	step [182/192], loss=78.5953
	step [183/192], loss=76.1231
	step [184/192], loss=75.5862
	step [185/192], loss=78.4963
	step [186/192], loss=75.9660
	step [187/192], loss=76.5532
	step [188/192], loss=73.8543
	step [189/192], loss=74.0469
	step [190/192], loss=74.5846
	step [191/192], loss=76.4317
	step [192/192], loss=9.5905
	Evaluating
	loss=0.2713, precision=0.1870, recall=0.9930, f1=0.3147
saving model as: 0_saved_model.pth
Training epoch 2
	step [1/192], loss=74.6818
	step [2/192], loss=74.8807
	step [3/192], loss=75.7057
	step [4/192], loss=73.8326
	step [5/192], loss=74.9495
	step [6/192], loss=75.3480
	step [7/192], loss=73.4408
	step [8/192], loss=74.9679
	step [9/192], loss=72.1344
	step [10/192], loss=75.6951
	step [11/192], loss=72.7509
	step [12/192], loss=74.5467
	step [13/192], loss=76.0237
	step [14/192], loss=73.2986
	step [15/192], loss=76.6100
	step [16/192], loss=70.8145
	step [17/192], loss=73.0375
	step [18/192], loss=71.7244
	step [19/192], loss=76.7668
	step [20/192], loss=74.2222
	step [21/192], loss=73.8719
	step [22/192], loss=74.1812
	step [23/192], loss=71.1344
	step [24/192], loss=71.3406
	step [25/192], loss=73.2925
	step [26/192], loss=70.0184
	step [27/192], loss=73.4229
	step [28/192], loss=72.6338
	step [29/192], loss=72.6828
	step [30/192], loss=70.2650
	step [31/192], loss=73.3442
	step [32/192], loss=71.7862
	step [33/192], loss=71.7058
	step [34/192], loss=70.4876
	step [35/192], loss=70.9715
	step [36/192], loss=72.1201
	step [37/192], loss=70.4501
	step [38/192], loss=71.5166
	step [39/192], loss=69.3135
	step [40/192], loss=72.0118
	step [41/192], loss=69.6524
	step [42/192], loss=71.1370
	step [43/192], loss=70.3215
	step [44/192], loss=70.4673
	step [45/192], loss=69.4655
	step [46/192], loss=73.1859
	step [47/192], loss=70.4934
	step [48/192], loss=69.2301
	step [49/192], loss=68.3311
	step [50/192], loss=70.9289
	step [51/192], loss=68.3116
	step [52/192], loss=66.9130
	step [53/192], loss=70.7496
	step [54/192], loss=70.1438
	step [55/192], loss=70.7968
	step [56/192], loss=70.9188
	step [57/192], loss=70.2016
	step [58/192], loss=70.6040
	step [59/192], loss=67.9368
	step [60/192], loss=69.7177
	step [61/192], loss=71.3590
	step [62/192], loss=69.0152
	step [63/192], loss=68.5339
	step [64/192], loss=70.6908
	step [65/192], loss=68.5364
	step [66/192], loss=69.9390
	step [67/192], loss=68.0029
	step [68/192], loss=65.5950
	step [69/192], loss=69.9245
	step [70/192], loss=67.5862
	step [71/192], loss=67.7152
	step [72/192], loss=64.9594
	step [73/192], loss=66.8898
	step [74/192], loss=67.8526
	step [75/192], loss=70.3030
	step [76/192], loss=69.4170
	step [77/192], loss=68.5217
	step [78/192], loss=66.2610
	step [79/192], loss=65.3997
	step [80/192], loss=66.5715
	step [81/192], loss=68.5677
	step [82/192], loss=69.2160
	step [83/192], loss=66.5892
	step [84/192], loss=65.8161
	step [85/192], loss=66.8613
	step [86/192], loss=64.0645
	step [87/192], loss=65.2051
	step [88/192], loss=66.1420
	step [89/192], loss=67.6274
	step [90/192], loss=67.5005
	step [91/192], loss=64.3496
	step [92/192], loss=64.2885
	step [93/192], loss=65.3355
	step [94/192], loss=63.1677
	step [95/192], loss=65.6550
	step [96/192], loss=63.9749
	step [97/192], loss=64.4621
	step [98/192], loss=64.9395
	step [99/192], loss=64.6612
	step [100/192], loss=64.4576
	step [101/192], loss=63.6074
	step [102/192], loss=65.2882
	step [103/192], loss=65.7906
	step [104/192], loss=67.3914
	step [105/192], loss=65.9126
	step [106/192], loss=68.5833
	step [107/192], loss=66.4384
	step [108/192], loss=61.8331
	step [109/192], loss=63.7103
	step [110/192], loss=62.9333
	step [111/192], loss=64.7778
	step [112/192], loss=67.2783
	step [113/192], loss=62.1475
	step [114/192], loss=63.1514
	step [115/192], loss=62.5595
	step [116/192], loss=63.9468
	step [117/192], loss=60.7756
	step [118/192], loss=63.2689
	step [119/192], loss=63.5498
	step [120/192], loss=65.0968
	step [121/192], loss=63.8335
	step [122/192], loss=65.2711
	step [123/192], loss=62.2103
	step [124/192], loss=62.6266
	step [125/192], loss=61.8029
	step [126/192], loss=62.1303
	step [127/192], loss=61.8154
	step [128/192], loss=66.0670
	step [129/192], loss=61.5640
	step [130/192], loss=63.3949
	step [131/192], loss=62.0072
	step [132/192], loss=63.2504
	step [133/192], loss=62.2070
	step [134/192], loss=63.4274
	step [135/192], loss=60.2967
	step [136/192], loss=63.5752
	step [137/192], loss=61.6449
	step [138/192], loss=62.9838
	step [139/192], loss=62.6783
	step [140/192], loss=60.9898
	step [141/192], loss=59.5785
	step [142/192], loss=59.9604
	step [143/192], loss=60.9287
	step [144/192], loss=61.1734
	step [145/192], loss=63.6634
	step [146/192], loss=60.5657
	step [147/192], loss=62.7774
	step [148/192], loss=59.7636
	step [149/192], loss=59.7497
	step [150/192], loss=59.8588
	step [151/192], loss=61.6162
	step [152/192], loss=62.0057
	step [153/192], loss=62.4970
	step [154/192], loss=59.7116
	step [155/192], loss=60.4413
	step [156/192], loss=58.2759
	step [157/192], loss=59.7467
	step [158/192], loss=61.4274
	step [159/192], loss=62.0119
	step [160/192], loss=61.2500
	step [161/192], loss=57.2648
	step [162/192], loss=60.5011
	step [163/192], loss=60.3956
	step [164/192], loss=60.4624
	step [165/192], loss=57.9165
	step [166/192], loss=56.9682
	step [167/192], loss=60.1966
	step [168/192], loss=60.1922
	step [169/192], loss=58.0704
	step [170/192], loss=57.0293
	step [171/192], loss=59.5675
	step [172/192], loss=55.9619
	step [173/192], loss=59.9100
	step [174/192], loss=60.6722
	step [175/192], loss=57.4147
	step [176/192], loss=57.5676
	step [177/192], loss=60.0953
	step [178/192], loss=59.0419
	step [179/192], loss=57.8851
	step [180/192], loss=57.3835
	step [181/192], loss=57.8404
	step [182/192], loss=57.5117
	step [183/192], loss=57.9240
	step [184/192], loss=58.3426
	step [185/192], loss=58.8950
	step [186/192], loss=54.2295
	step [187/192], loss=57.9978
	step [188/192], loss=57.9547
	step [189/192], loss=57.0549
	step [190/192], loss=58.1737
	step [191/192], loss=58.0232
	step [192/192], loss=7.3412
	Evaluating
	loss=0.2073, precision=0.1691, recall=0.9938, f1=0.2891
Training epoch 3
	step [1/192], loss=56.9446
	step [2/192], loss=57.2512
	step [3/192], loss=59.2425
	step [4/192], loss=55.7474
	step [5/192], loss=54.7503
	step [6/192], loss=56.2993
	step [7/192], loss=56.8717
	step [8/192], loss=56.5166
	step [9/192], loss=56.2699
	step [10/192], loss=55.8937
	step [11/192], loss=56.7082
	step [12/192], loss=57.0797
	step [13/192], loss=55.5007
	step [14/192], loss=55.8525
	step [15/192], loss=54.0099
	step [16/192], loss=55.1938
	step [17/192], loss=54.9334
	step [18/192], loss=55.7271
	step [19/192], loss=54.6515
	step [20/192], loss=54.6316
	step [21/192], loss=56.0387
	step [22/192], loss=54.0890
	step [23/192], loss=54.1820
	step [24/192], loss=57.1233
	step [25/192], loss=54.4374
	step [26/192], loss=57.5082
	step [27/192], loss=55.5863
	step [28/192], loss=55.3179
	step [29/192], loss=54.9491
	step [30/192], loss=54.7280
	step [31/192], loss=52.0140
	step [32/192], loss=55.3730
	step [33/192], loss=54.1565
	step [34/192], loss=52.3487
	step [35/192], loss=54.8110
	step [36/192], loss=54.2550
	step [37/192], loss=53.2485
	step [38/192], loss=54.6238
	step [39/192], loss=55.7838
	step [40/192], loss=57.5113
	step [41/192], loss=53.0281
	step [42/192], loss=54.0544
	step [43/192], loss=53.9067
	step [44/192], loss=54.8762
	step [45/192], loss=55.9134
	step [46/192], loss=55.0301
	step [47/192], loss=53.5704
	step [48/192], loss=51.4629
	step [49/192], loss=55.1172
	step [50/192], loss=52.9593
	step [51/192], loss=53.0842
	step [52/192], loss=54.7227
	step [53/192], loss=54.3681
	step [54/192], loss=54.2481
	step [55/192], loss=53.5728
	step [56/192], loss=52.9976
	step [57/192], loss=53.6076
	step [58/192], loss=51.5225
	step [59/192], loss=52.4803
	step [60/192], loss=53.6370
	step [61/192], loss=52.8759
	step [62/192], loss=52.7219
	step [63/192], loss=56.8662
	step [64/192], loss=53.7701
	step [65/192], loss=52.2537
	step [66/192], loss=50.5466
	step [67/192], loss=52.7279
	step [68/192], loss=55.2393
	step [69/192], loss=53.7348
	step [70/192], loss=51.5276
	step [71/192], loss=52.4397
	step [72/192], loss=52.7132
	step [73/192], loss=51.3589
	step [74/192], loss=52.3046
	step [75/192], loss=51.1388
	step [76/192], loss=49.3438
	step [77/192], loss=51.4598
	step [78/192], loss=51.8284
	step [79/192], loss=50.0491
	step [80/192], loss=50.6529
	step [81/192], loss=49.6725
	step [82/192], loss=50.7814
	step [83/192], loss=51.2334
	step [84/192], loss=50.6990
	step [85/192], loss=49.5775
	step [86/192], loss=50.9050
	step [87/192], loss=49.6451
	step [88/192], loss=51.4484
	step [89/192], loss=51.2026
	step [90/192], loss=51.4839
	step [91/192], loss=48.2770
	step [92/192], loss=51.3435
	step [93/192], loss=53.4875
	step [94/192], loss=51.9775
	step [95/192], loss=50.5189
	step [96/192], loss=50.9431
	step [97/192], loss=50.3210
	step [98/192], loss=49.8248
	step [99/192], loss=47.4687
	step [100/192], loss=50.3162
	step [101/192], loss=48.7550
	step [102/192], loss=47.4212
	step [103/192], loss=49.5220
	step [104/192], loss=53.4447
	step [105/192], loss=52.2568
	step [106/192], loss=50.2394
	step [107/192], loss=47.9020
	step [108/192], loss=50.9309
	step [109/192], loss=48.8049
	step [110/192], loss=50.0001
	step [111/192], loss=49.7428
	step [112/192], loss=50.6586
	step [113/192], loss=50.1686
	step [114/192], loss=47.5537
	step [115/192], loss=49.4027
	step [116/192], loss=49.2520
	step [117/192], loss=49.0744
	step [118/192], loss=49.4586
	step [119/192], loss=49.7974
	step [120/192], loss=50.1006
	step [121/192], loss=50.8078
	step [122/192], loss=50.8305
	step [123/192], loss=50.6469
	step [124/192], loss=46.9566
	step [125/192], loss=48.6700
	step [126/192], loss=49.4705
	step [127/192], loss=47.5264
	step [128/192], loss=47.8486
	step [129/192], loss=46.9308
	step [130/192], loss=47.6138
	step [131/192], loss=47.9398
	step [132/192], loss=50.8908
	step [133/192], loss=48.2936
	step [134/192], loss=48.5594
	step [135/192], loss=47.6974
	step [136/192], loss=51.5384
	step [137/192], loss=49.4436
	step [138/192], loss=50.7149
	step [139/192], loss=45.5962
	step [140/192], loss=46.9549
	step [141/192], loss=47.1589
	step [142/192], loss=47.9902
	step [143/192], loss=49.7652
	step [144/192], loss=47.7634
	step [145/192], loss=48.4693
	step [146/192], loss=46.4977
	step [147/192], loss=48.2356
	step [148/192], loss=48.0036
	step [149/192], loss=45.2467
	step [150/192], loss=47.3013
	step [151/192], loss=47.6669
	step [152/192], loss=48.3055
	step [153/192], loss=45.8847
	step [154/192], loss=46.5524
	step [155/192], loss=48.0790
	step [156/192], loss=47.5861
	step [157/192], loss=45.7429
	step [158/192], loss=45.8494
	step [159/192], loss=46.3564
	step [160/192], loss=47.6815
	step [161/192], loss=47.1183
	step [162/192], loss=46.6026
	step [163/192], loss=45.5980
	step [164/192], loss=48.8500
	step [165/192], loss=45.1802
	step [166/192], loss=49.2715
	step [167/192], loss=46.7966
	step [168/192], loss=45.0573
	step [169/192], loss=47.0452
	step [170/192], loss=46.2194
	step [171/192], loss=46.2780
	step [172/192], loss=44.4206
	step [173/192], loss=45.2066
	step [174/192], loss=46.3671
	step [175/192], loss=46.3717
	step [176/192], loss=44.4319
	step [177/192], loss=49.2477
	step [178/192], loss=44.8330
	step [179/192], loss=46.2212
	step [180/192], loss=44.9460
	step [181/192], loss=44.8984
	step [182/192], loss=44.9317
	step [183/192], loss=44.7589
	step [184/192], loss=46.0141
	step [185/192], loss=44.7890
	step [186/192], loss=47.7410
	step [187/192], loss=44.6463
	step [188/192], loss=44.5794
	step [189/192], loss=44.2320
	step [190/192], loss=45.7670
	step [191/192], loss=45.4557
	step [192/192], loss=5.4349
	Evaluating
	loss=0.1544, precision=0.1630, recall=0.9939, f1=0.2800
Training epoch 4
	step [1/192], loss=44.8234
	step [2/192], loss=44.5138
	step [3/192], loss=45.9789
	step [4/192], loss=46.5294
	step [5/192], loss=46.0467
	step [6/192], loss=43.6467
	step [7/192], loss=46.0058
	step [8/192], loss=44.6553
	step [9/192], loss=43.0213
	step [10/192], loss=45.1668
	step [11/192], loss=46.6928
	step [12/192], loss=43.8854
	step [13/192], loss=42.0712
	step [14/192], loss=43.3520
	step [15/192], loss=46.8067
	step [16/192], loss=43.9522
	step [17/192], loss=44.7129
	step [18/192], loss=44.5190
	step [19/192], loss=44.8479
	step [20/192], loss=44.1865
	step [21/192], loss=46.4671
	step [22/192], loss=43.9975
	step [23/192], loss=44.6279
	step [24/192], loss=43.2666
	step [25/192], loss=43.8999
	step [26/192], loss=44.1186
	step [27/192], loss=42.3211
	step [28/192], loss=46.4624
	step [29/192], loss=43.9028
	step [30/192], loss=46.0782
	step [31/192], loss=45.4543
	step [32/192], loss=42.3747
	step [33/192], loss=43.3218
	step [34/192], loss=42.4042
	step [35/192], loss=43.2545
	step [36/192], loss=43.1026
	step [37/192], loss=41.5584
	step [38/192], loss=42.5592
	step [39/192], loss=41.3115
	step [40/192], loss=45.0506
	step [41/192], loss=43.0235
	step [42/192], loss=44.3251
	step [43/192], loss=43.7789
	step [44/192], loss=45.6861
	step [45/192], loss=44.3466
	step [46/192], loss=44.1621
	step [47/192], loss=44.4999
	step [48/192], loss=43.1736
	step [49/192], loss=40.2833
	step [50/192], loss=42.6018
	step [51/192], loss=41.5555
	step [52/192], loss=43.5911
	step [53/192], loss=43.2196
	step [54/192], loss=41.4957
	step [55/192], loss=43.2613
	step [56/192], loss=41.4662
	step [57/192], loss=43.3828
	step [58/192], loss=43.1065
	step [59/192], loss=41.9296
	step [60/192], loss=42.1935
	step [61/192], loss=42.4227
	step [62/192], loss=41.2471
	step [63/192], loss=42.2299
	step [64/192], loss=43.5305
	step [65/192], loss=41.9996
	step [66/192], loss=44.7993
	step [67/192], loss=43.3889
	step [68/192], loss=42.5278
	step [69/192], loss=39.8413
	step [70/192], loss=42.5604
	step [71/192], loss=41.5379
	step [72/192], loss=39.9834
	step [73/192], loss=40.6591
	step [74/192], loss=38.6788
	step [75/192], loss=39.6664
	step [76/192], loss=40.3867
	step [77/192], loss=39.3848
	step [78/192], loss=37.5721
	step [79/192], loss=40.1352
	step [80/192], loss=42.3121
	step [81/192], loss=39.1559
	step [82/192], loss=44.3519
	step [83/192], loss=39.1902
	step [84/192], loss=41.8329
	step [85/192], loss=40.5658
	step [86/192], loss=40.5680
	step [87/192], loss=40.3783
	step [88/192], loss=40.6460
	step [89/192], loss=40.9798
	step [90/192], loss=39.6859
	step [91/192], loss=41.4619
	step [92/192], loss=39.4161
	step [93/192], loss=42.2515
	step [94/192], loss=38.0737
	step [95/192], loss=41.2499
	step [96/192], loss=41.0132
	step [97/192], loss=42.0560
	step [98/192], loss=40.2293
	step [99/192], loss=38.6218
	step [100/192], loss=39.1057
	step [101/192], loss=41.7826
	step [102/192], loss=40.1108
	step [103/192], loss=39.4263
	step [104/192], loss=40.1676
	step [105/192], loss=39.9485
	step [106/192], loss=39.8365
	step [107/192], loss=39.5611
	step [108/192], loss=40.1084
	step [109/192], loss=40.0419
	step [110/192], loss=41.7628
	step [111/192], loss=38.2996
	step [112/192], loss=41.3256
	step [113/192], loss=39.6606
	step [114/192], loss=39.9287
	step [115/192], loss=38.2000
	step [116/192], loss=39.3185
	step [117/192], loss=38.2833
	step [118/192], loss=39.5155
	step [119/192], loss=40.0515
	step [120/192], loss=40.5315
	step [121/192], loss=38.8258
	step [122/192], loss=41.2918
	step [123/192], loss=38.2658
	step [124/192], loss=38.6929
	step [125/192], loss=39.6933
	step [126/192], loss=37.6421
	step [127/192], loss=40.8584
	step [128/192], loss=40.5143
	step [129/192], loss=40.4834
	step [130/192], loss=37.5422
	step [131/192], loss=39.3127
	step [132/192], loss=39.9992
	step [133/192], loss=40.2392
	step [134/192], loss=40.5256
	step [135/192], loss=40.8875
	step [136/192], loss=38.5374
	step [137/192], loss=39.0633
	step [138/192], loss=39.6639
	step [139/192], loss=37.5930
	step [140/192], loss=37.7881
	step [141/192], loss=38.7103
	step [142/192], loss=38.9136
	step [143/192], loss=37.1885
	step [144/192], loss=39.1045
	step [145/192], loss=38.5106
	step [146/192], loss=37.1898
	step [147/192], loss=40.7742
	step [148/192], loss=37.6727
	step [149/192], loss=40.5820
	step [150/192], loss=38.7565
	step [151/192], loss=42.7659
	step [152/192], loss=39.5956
	step [153/192], loss=40.2070
	step [154/192], loss=39.5526
	step [155/192], loss=38.4104
	step [156/192], loss=39.1158
	step [157/192], loss=38.8521
	step [158/192], loss=37.4015
	step [159/192], loss=39.1406
	step [160/192], loss=39.3859
	step [161/192], loss=38.8765
	step [162/192], loss=39.4920
	step [163/192], loss=36.7310
	step [164/192], loss=35.1227
	step [165/192], loss=38.0609
	step [166/192], loss=37.5858
	step [167/192], loss=37.8855
	step [168/192], loss=37.9089
	step [169/192], loss=37.1007
	step [170/192], loss=40.3641
	step [171/192], loss=37.3520
	step [172/192], loss=35.9073
	step [173/192], loss=38.7202
	step [174/192], loss=36.3404
	step [175/192], loss=35.5255
	step [176/192], loss=38.6932
	step [177/192], loss=40.2466
	step [178/192], loss=35.4302
	step [179/192], loss=37.6020
	step [180/192], loss=37.9998
	step [181/192], loss=39.1143
	step [182/192], loss=37.6648
	step [183/192], loss=37.0520
	step [184/192], loss=35.3490
	step [185/192], loss=36.4593
	step [186/192], loss=37.2129
	step [187/192], loss=35.8668
	step [188/192], loss=37.9857
	step [189/192], loss=39.3527
	step [190/192], loss=36.2317
	step [191/192], loss=35.9002
	step [192/192], loss=4.5817
	Evaluating
	loss=0.1273, precision=0.1727, recall=0.9940, f1=0.2943
Training epoch 5
	step [1/192], loss=36.9913
	step [2/192], loss=34.4603
	step [3/192], loss=36.1020
	step [4/192], loss=35.1180
	step [5/192], loss=35.4057
	step [6/192], loss=37.9458
	step [7/192], loss=37.4103
	step [8/192], loss=39.3107
	step [9/192], loss=36.7531
	step [10/192], loss=36.6337
	step [11/192], loss=38.1188
	step [12/192], loss=38.7695
	step [13/192], loss=36.4543
	step [14/192], loss=36.9608
	step [15/192], loss=36.1793
	step [16/192], loss=36.2659
	step [17/192], loss=37.7417
	step [18/192], loss=38.2976
	step [19/192], loss=36.1036
	step [20/192], loss=36.2760
	step [21/192], loss=35.9671
	step [22/192], loss=35.4065
	step [23/192], loss=38.9392
	step [24/192], loss=37.8503
	step [25/192], loss=34.9516
	step [26/192], loss=33.7915
	step [27/192], loss=36.5173
	step [28/192], loss=37.1479
	step [29/192], loss=38.7316
	step [30/192], loss=35.4977
	step [31/192], loss=35.9227
	step [32/192], loss=36.8020
	step [33/192], loss=39.7563
	step [34/192], loss=34.2567
	step [35/192], loss=35.9895
	step [36/192], loss=34.0596
	step [37/192], loss=34.6225
	step [38/192], loss=35.2559
	step [39/192], loss=33.6157
	step [40/192], loss=34.6299
	step [41/192], loss=34.4987
	step [42/192], loss=36.1626
	step [43/192], loss=38.1523
	step [44/192], loss=37.0080
	step [45/192], loss=34.8624
	step [46/192], loss=34.0397
	step [47/192], loss=34.2714
	step [48/192], loss=34.8169
	step [49/192], loss=32.7588
	step [50/192], loss=36.5136
	step [51/192], loss=33.6606
	step [52/192], loss=37.8895
	step [53/192], loss=35.5105
	step [54/192], loss=34.0724
	step [55/192], loss=36.8331
	step [56/192], loss=34.9095
	step [57/192], loss=33.7233
	step [58/192], loss=34.6596
	step [59/192], loss=33.9446
	step [60/192], loss=35.2339
	step [61/192], loss=34.3664
	step [62/192], loss=36.0078
	step [63/192], loss=34.7236
	step [64/192], loss=34.7204
	step [65/192], loss=34.0806
	step [66/192], loss=33.8991
	step [67/192], loss=33.3551
	step [68/192], loss=34.3443
	step [69/192], loss=33.8574
	step [70/192], loss=35.0807
	step [71/192], loss=34.2023
	step [72/192], loss=34.4026
	step [73/192], loss=36.7828
	step [74/192], loss=34.3920
	step [75/192], loss=34.1253
	step [76/192], loss=33.1221
	step [77/192], loss=32.8131
	step [78/192], loss=34.0425
	step [79/192], loss=35.0477
	step [80/192], loss=36.3265
	step [81/192], loss=33.9082
	step [82/192], loss=35.6473
	step [83/192], loss=33.3367
	step [84/192], loss=34.2434
	step [85/192], loss=32.5663
	step [86/192], loss=33.5267
	step [87/192], loss=33.0768
	step [88/192], loss=36.0517
	step [89/192], loss=33.3905
	step [90/192], loss=33.3006
	step [91/192], loss=32.0142
	step [92/192], loss=33.6675
	step [93/192], loss=35.0011
	step [94/192], loss=33.3677
	step [95/192], loss=32.8842
	step [96/192], loss=33.1259
	step [97/192], loss=32.5167
	step [98/192], loss=33.4627
	step [99/192], loss=33.0354
	step [100/192], loss=37.6609
	step [101/192], loss=32.1043
	step [102/192], loss=35.8695
	step [103/192], loss=34.5170
	step [104/192], loss=34.4535
	step [105/192], loss=32.4954
	step [106/192], loss=32.2414
	step [107/192], loss=33.3116
	step [108/192], loss=32.8351
	step [109/192], loss=35.0995
	step [110/192], loss=34.5181
	step [111/192], loss=32.9963
	step [112/192], loss=34.4975
	step [113/192], loss=33.7911
	step [114/192], loss=31.0652
	step [115/192], loss=35.2122
	step [116/192], loss=32.2099
	step [117/192], loss=33.3382
	step [118/192], loss=31.2865
	step [119/192], loss=31.5793
	step [120/192], loss=31.9891
	step [121/192], loss=32.7962
	step [122/192], loss=32.4788
	step [123/192], loss=34.2071
	step [124/192], loss=31.7705
	step [125/192], loss=32.1168
	step [126/192], loss=34.4091
	step [127/192], loss=35.3077
	step [128/192], loss=33.5559
	step [129/192], loss=33.5641
	step [130/192], loss=32.4448
	step [131/192], loss=34.2208
	step [132/192], loss=33.9038
	step [133/192], loss=31.2346
	step [134/192], loss=32.8412
	step [135/192], loss=29.9881
	step [136/192], loss=32.5486
	step [137/192], loss=31.2058
	step [138/192], loss=30.8698
	step [139/192], loss=32.0142
	step [140/192], loss=30.7859
	step [141/192], loss=30.4261
	step [142/192], loss=31.8636
	step [143/192], loss=34.3586
	step [144/192], loss=33.4202
	step [145/192], loss=34.5796
	step [146/192], loss=31.3663
	step [147/192], loss=31.6511
	step [148/192], loss=32.0230
	step [149/192], loss=32.7876
	step [150/192], loss=32.9688
	step [151/192], loss=31.8532
	step [152/192], loss=30.0090
	step [153/192], loss=32.3777
	step [154/192], loss=32.6090
	step [155/192], loss=34.6448
	step [156/192], loss=33.8726
	step [157/192], loss=30.7627
	step [158/192], loss=32.5937
	step [159/192], loss=32.3540
	step [160/192], loss=33.5670
	step [161/192], loss=32.4843
	step [162/192], loss=33.4226
	step [163/192], loss=31.7902
	step [164/192], loss=33.8343
	step [165/192], loss=32.4262
	step [166/192], loss=30.9794
	step [167/192], loss=32.0202
	step [168/192], loss=30.7685
	step [169/192], loss=31.8807
	step [170/192], loss=32.3795
	step [171/192], loss=33.1300
	step [172/192], loss=33.6972
	step [173/192], loss=32.7048
	step [174/192], loss=31.4200
	step [175/192], loss=29.3179
	step [176/192], loss=32.5562
	step [177/192], loss=30.5064
	step [178/192], loss=31.6726
	step [179/192], loss=33.3696
	step [180/192], loss=31.7368
	step [181/192], loss=29.8374
	step [182/192], loss=30.3318
	step [183/192], loss=30.6746
	step [184/192], loss=31.8380
	step [185/192], loss=35.2278
	step [186/192], loss=29.4980
	step [187/192], loss=29.0881
	step [188/192], loss=30.6948
	step [189/192], loss=31.0861
	step [190/192], loss=33.1942
	step [191/192], loss=30.0528
	step [192/192], loss=3.3786
	Evaluating
	loss=0.1025, precision=0.1821, recall=0.9936, f1=0.3078
Training epoch 6
	step [1/192], loss=33.0723
	step [2/192], loss=31.6850
	step [3/192], loss=32.6808
	step [4/192], loss=32.1231
	step [5/192], loss=32.9046
	step [6/192], loss=31.3373
	step [7/192], loss=30.6911
	step [8/192], loss=29.1909
	step [9/192], loss=31.6618
	step [10/192], loss=32.0503
	step [11/192], loss=32.1143
	step [12/192], loss=30.0170
	step [13/192], loss=32.2639
	step [14/192], loss=31.3416
	step [15/192], loss=30.7847
	step [16/192], loss=29.6085
	step [17/192], loss=30.2196
	step [18/192], loss=30.1736
	step [19/192], loss=29.9615
	step [20/192], loss=30.6011
	step [21/192], loss=30.8006
	step [22/192], loss=32.5857
	step [23/192], loss=31.1731
	step [24/192], loss=33.2437
	step [25/192], loss=28.7439
	step [26/192], loss=31.2223
	step [27/192], loss=29.2573
	step [28/192], loss=30.8308
	step [29/192], loss=27.7993
	step [30/192], loss=32.2818
	step [31/192], loss=30.2343
	step [32/192], loss=29.7608
	step [33/192], loss=30.4701
	step [34/192], loss=29.5813
	step [35/192], loss=29.9067
	step [36/192], loss=32.2128
	step [37/192], loss=29.8467
	step [38/192], loss=29.0815
	step [39/192], loss=29.7089
	step [40/192], loss=28.1493
	step [41/192], loss=29.5985
	step [42/192], loss=29.4439
	step [43/192], loss=29.5168
	step [44/192], loss=28.8440
	step [45/192], loss=28.9617
	step [46/192], loss=30.1430
	step [47/192], loss=30.4252
	step [48/192], loss=31.5523
	step [49/192], loss=27.1575
	step [50/192], loss=28.8688
	step [51/192], loss=30.7528
	step [52/192], loss=27.3113
	step [53/192], loss=28.4731
	step [54/192], loss=30.8001
	step [55/192], loss=29.4312
	step [56/192], loss=30.7678
	step [57/192], loss=29.6747
	step [58/192], loss=32.0357
	step [59/192], loss=31.2525
	step [60/192], loss=28.1055
	step [61/192], loss=31.4601
	step [62/192], loss=29.1972
	step [63/192], loss=30.3927
	step [64/192], loss=31.9890
	step [65/192], loss=30.0905
	step [66/192], loss=27.7933
	step [67/192], loss=29.1994
	step [68/192], loss=30.0729
	step [69/192], loss=29.5380
	step [70/192], loss=31.6593
	step [71/192], loss=28.2635
	step [72/192], loss=30.2182
	step [73/192], loss=26.9969
	step [74/192], loss=29.0695
	step [75/192], loss=28.3881
	step [76/192], loss=28.0411
	step [77/192], loss=26.6896
	step [78/192], loss=29.4010
	step [79/192], loss=26.9428
	step [80/192], loss=29.7862
	step [81/192], loss=29.9616
	step [82/192], loss=28.2359
	step [83/192], loss=30.6921
	step [84/192], loss=26.6448
	step [85/192], loss=28.4404
	step [86/192], loss=30.0460
	step [87/192], loss=31.2978
	step [88/192], loss=28.8283
	step [89/192], loss=27.2535
	step [90/192], loss=27.7296
	step [91/192], loss=29.1029
	step [92/192], loss=28.0118
	step [93/192], loss=29.3251
	step [94/192], loss=30.1725
	step [95/192], loss=29.5723
	step [96/192], loss=28.7386
	step [97/192], loss=29.9714
	step [98/192], loss=29.3116
	step [99/192], loss=28.7684
	step [100/192], loss=27.6500
	step [101/192], loss=27.8776
	step [102/192], loss=28.5161
	step [103/192], loss=29.4348
	step [104/192], loss=27.3195
	step [105/192], loss=28.8490
	step [106/192], loss=28.1705
	step [107/192], loss=28.4170
	step [108/192], loss=28.6761
	step [109/192], loss=28.6760
	step [110/192], loss=29.7570
	step [111/192], loss=30.1268
	step [112/192], loss=29.1099
	step [113/192], loss=29.8190
	step [114/192], loss=30.6696
	step [115/192], loss=27.3059
	step [116/192], loss=27.0876
	step [117/192], loss=29.7348
	step [118/192], loss=29.9194
	step [119/192], loss=28.8509
	step [120/192], loss=28.3359
	step [121/192], loss=26.8647
	step [122/192], loss=28.7901
	step [123/192], loss=28.6687
	step [124/192], loss=29.8413
	step [125/192], loss=28.8987
	step [126/192], loss=29.6754
	step [127/192], loss=27.7959
	step [128/192], loss=27.5745
	step [129/192], loss=28.0058
	step [130/192], loss=24.9227
	step [131/192], loss=27.6046
	step [132/192], loss=27.5073
	step [133/192], loss=28.6504
	step [134/192], loss=25.6574
	step [135/192], loss=28.6803
	step [136/192], loss=25.1802
	step [137/192], loss=27.9924
	step [138/192], loss=27.5200
	step [139/192], loss=29.1155
	step [140/192], loss=28.9734
	step [141/192], loss=27.4517
	step [142/192], loss=27.3267
	step [143/192], loss=28.7983
	step [144/192], loss=28.9756
	step [145/192], loss=26.7810
	step [146/192], loss=27.6421
	step [147/192], loss=27.5738
	step [148/192], loss=28.4857
	step [149/192], loss=25.9622
	step [150/192], loss=27.8240
	step [151/192], loss=26.3903
	step [152/192], loss=24.6579
	step [153/192], loss=28.8615
	step [154/192], loss=25.9461
	step [155/192], loss=25.8481
	step [156/192], loss=28.8801
	step [157/192], loss=29.6965
	step [158/192], loss=26.3430
	step [159/192], loss=28.6625
	step [160/192], loss=25.4291
	step [161/192], loss=28.0223
	step [162/192], loss=25.4367
	step [163/192], loss=25.9288
	step [164/192], loss=26.0964
	step [165/192], loss=30.7652
	step [166/192], loss=27.1124
	step [167/192], loss=28.0939
	step [168/192], loss=25.2567
	step [169/192], loss=27.7244
	step [170/192], loss=26.7381
	step [171/192], loss=27.0372
	step [172/192], loss=26.0116
	step [173/192], loss=26.3481
	step [174/192], loss=25.8482
	step [175/192], loss=29.0657
	step [176/192], loss=29.9694
	step [177/192], loss=26.0916
	step [178/192], loss=25.0149
	step [179/192], loss=29.3171
	step [180/192], loss=26.8523
	step [181/192], loss=26.0977
	step [182/192], loss=27.7311
	step [183/192], loss=26.7677
	step [184/192], loss=25.3519
	step [185/192], loss=27.7818
	step [186/192], loss=25.9452
	step [187/192], loss=27.1359
	step [188/192], loss=27.9661
	step [189/192], loss=26.4867
	step [190/192], loss=27.3182
	step [191/192], loss=27.6265
	step [192/192], loss=4.5163
	Evaluating
	loss=0.0846, precision=0.1544, recall=0.9943, f1=0.2673
Training epoch 7
	step [1/192], loss=28.3109
	step [2/192], loss=26.5334
	step [3/192], loss=27.7813
	step [4/192], loss=25.4904
	step [5/192], loss=24.6984
	step [6/192], loss=23.9313
	step [7/192], loss=25.4648
	step [8/192], loss=26.7897
	step [9/192], loss=25.9476
	step [10/192], loss=25.1577
	step [11/192], loss=28.0255
	step [12/192], loss=28.0195
	step [13/192], loss=25.8790
	step [14/192], loss=24.6303
	step [15/192], loss=25.7205
	step [16/192], loss=25.9848
	step [17/192], loss=26.0956
	step [18/192], loss=25.8969
	step [19/192], loss=24.4263
	step [20/192], loss=24.0079
	step [21/192], loss=26.2764
	step [22/192], loss=24.9019
	step [23/192], loss=24.9087
	step [24/192], loss=27.9579
	step [25/192], loss=24.3042
	step [26/192], loss=25.5707
	step [27/192], loss=27.8700
	step [28/192], loss=25.1856
	step [29/192], loss=24.6060
	step [30/192], loss=26.1682
	step [31/192], loss=27.4630
	step [32/192], loss=29.1915
	step [33/192], loss=24.8852
	step [34/192], loss=26.0725
	step [35/192], loss=26.5310
	step [36/192], loss=24.8882
	step [37/192], loss=27.0641
	step [38/192], loss=27.3062
	step [39/192], loss=28.1711
	step [40/192], loss=26.4542
	step [41/192], loss=23.9347
	step [42/192], loss=30.8102
	step [43/192], loss=24.5674
	step [44/192], loss=27.9551
	step [45/192], loss=25.4925
	step [46/192], loss=24.6623
	step [47/192], loss=25.2769
	step [48/192], loss=24.2665
	step [49/192], loss=27.5790
	step [50/192], loss=26.9873
	step [51/192], loss=27.2623
	step [52/192], loss=26.6242
	step [53/192], loss=25.9632
	step [54/192], loss=27.0747
	step [55/192], loss=26.0623
	step [56/192], loss=26.4563
	step [57/192], loss=24.6165
	step [58/192], loss=24.2489
	step [59/192], loss=25.5953
	step [60/192], loss=25.8676
	step [61/192], loss=27.2696
	step [62/192], loss=24.8907
	step [63/192], loss=24.8602
	step [64/192], loss=26.4686
	step [65/192], loss=23.2448
	step [66/192], loss=25.0406
	step [67/192], loss=27.9685
	step [68/192], loss=24.1249
	step [69/192], loss=24.9070
	step [70/192], loss=24.3535
	step [71/192], loss=23.7710
	step [72/192], loss=24.0314
	step [73/192], loss=25.0915
	step [74/192], loss=27.5734
	step [75/192], loss=27.0507
	step [76/192], loss=23.7434
	step [77/192], loss=25.1309
	step [78/192], loss=23.1824
	step [79/192], loss=26.6956
	step [80/192], loss=25.7488
	step [81/192], loss=27.7267
	step [82/192], loss=25.3748
	step [83/192], loss=23.5361
	step [84/192], loss=26.1431
	step [85/192], loss=25.1739
	step [86/192], loss=25.1814
	step [87/192], loss=26.9647
	step [88/192], loss=24.1010
	step [89/192], loss=24.4359
	step [90/192], loss=26.5947
	step [91/192], loss=26.1220
	step [92/192], loss=27.3947
	step [93/192], loss=24.3915
	step [94/192], loss=22.5045
	step [95/192], loss=24.3002
	step [96/192], loss=21.3888
	step [97/192], loss=25.5516
	step [98/192], loss=24.9786
	step [99/192], loss=24.9519
	step [100/192], loss=25.7248
	step [101/192], loss=24.9768
	step [102/192], loss=24.3077
	step [103/192], loss=23.8776
	step [104/192], loss=26.1148
	step [105/192], loss=24.5843
	step [106/192], loss=26.7491
	step [107/192], loss=25.0109
	step [108/192], loss=24.7164
	step [109/192], loss=26.7330
	step [110/192], loss=26.2640
	step [111/192], loss=26.3004
	step [112/192], loss=24.9474
	step [113/192], loss=25.5134
	step [114/192], loss=26.6436
	step [115/192], loss=26.8735
	step [116/192], loss=25.4656
	step [117/192], loss=23.3655
	step [118/192], loss=25.4623
	step [119/192], loss=24.1923
	step [120/192], loss=25.3008
	step [121/192], loss=22.2782
	step [122/192], loss=24.2737
	step [123/192], loss=24.9881
	step [124/192], loss=24.1277
	step [125/192], loss=23.2217
	step [126/192], loss=26.4197
	step [127/192], loss=24.2132
	step [128/192], loss=25.8590
	step [129/192], loss=22.6651
	step [130/192], loss=26.2704
	step [131/192], loss=24.4484
	step [132/192], loss=23.5460
	step [133/192], loss=24.6095
	step [134/192], loss=21.9486
	step [135/192], loss=26.3260
	step [136/192], loss=25.7498
	step [137/192], loss=24.5107
	step [138/192], loss=26.5012
	step [139/192], loss=23.4848
	step [140/192], loss=23.6866
	step [141/192], loss=22.2922
	step [142/192], loss=25.0031
	step [143/192], loss=27.7259
	step [144/192], loss=24.3543
	step [145/192], loss=25.9531
	step [146/192], loss=24.6917
	step [147/192], loss=23.7505
	step [148/192], loss=25.9617
	step [149/192], loss=24.2360
	step [150/192], loss=22.9026
	step [151/192], loss=23.7862
	step [152/192], loss=24.0531
	step [153/192], loss=23.5657
	step [154/192], loss=27.2273
	step [155/192], loss=22.1772
	step [156/192], loss=24.8281
	step [157/192], loss=22.5386
	step [158/192], loss=26.5286
	step [159/192], loss=24.8903
	step [160/192], loss=23.8540
	step [161/192], loss=28.1716
	step [162/192], loss=23.1161
	step [163/192], loss=24.5851
	step [164/192], loss=23.5633
	step [165/192], loss=24.0760
	step [166/192], loss=21.4714
	step [167/192], loss=24.5546
	step [168/192], loss=23.9514
	step [169/192], loss=23.5929
	step [170/192], loss=23.7722
	step [171/192], loss=23.6075
	step [172/192], loss=27.4641
	step [173/192], loss=23.6473
	step [174/192], loss=23.0892
	step [175/192], loss=21.8012
	step [176/192], loss=23.1653
	step [177/192], loss=22.6960
	step [178/192], loss=23.2170
	step [179/192], loss=20.6946
	step [180/192], loss=23.5211
	step [181/192], loss=23.0257
	step [182/192], loss=26.2579
	step [183/192], loss=23.1984
	step [184/192], loss=23.6006
	step [185/192], loss=24.1560
	step [186/192], loss=23.8035
	step [187/192], loss=23.5819
	step [188/192], loss=24.1098
	step [189/192], loss=23.1066
	step [190/192], loss=22.6242
	step [191/192], loss=23.1693
	step [192/192], loss=3.5855
	Evaluating
	loss=0.0738, precision=0.1645, recall=0.9944, f1=0.2824
Training epoch 8
	step [1/192], loss=22.6361
	step [2/192], loss=23.4809
	step [3/192], loss=22.9409
	step [4/192], loss=23.3254
	step [5/192], loss=24.5656
	step [6/192], loss=22.7472
	step [7/192], loss=23.7461
	step [8/192], loss=24.3637
	step [9/192], loss=24.1602
	step [10/192], loss=25.5303
	step [11/192], loss=23.3729
	step [12/192], loss=22.7641
	step [13/192], loss=22.6207
	step [14/192], loss=24.3672
	step [15/192], loss=21.8278
	step [16/192], loss=24.1353
	step [17/192], loss=24.4966
	step [18/192], loss=28.6303
	step [19/192], loss=22.8801
	step [20/192], loss=22.4057
	step [21/192], loss=21.9643
	step [22/192], loss=21.9559
	step [23/192], loss=23.4857
	step [24/192], loss=24.0792
	step [25/192], loss=22.1503
	step [26/192], loss=21.8120
	step [27/192], loss=22.8089
	step [28/192], loss=24.8442
	step [29/192], loss=23.2206
	step [30/192], loss=22.7530
	step [31/192], loss=23.0014
	step [32/192], loss=23.6330
	step [33/192], loss=21.5732
	step [34/192], loss=22.6169
	step [35/192], loss=21.3997
	step [36/192], loss=21.9004
	step [37/192], loss=21.3676
	step [38/192], loss=23.5804
	step [39/192], loss=24.7913
	step [40/192], loss=23.7842
	step [41/192], loss=23.5740
	step [42/192], loss=22.0250
	step [43/192], loss=22.8000
	step [44/192], loss=23.9011
	step [45/192], loss=25.1086
	step [46/192], loss=23.7692
	step [47/192], loss=23.1911
	step [48/192], loss=22.0573
	step [49/192], loss=21.9608
	step [50/192], loss=20.8013
	step [51/192], loss=23.2247
	step [52/192], loss=22.4175
	step [53/192], loss=21.4341
	step [54/192], loss=23.5249
	step [55/192], loss=25.4097
	step [56/192], loss=24.8953
	step [57/192], loss=20.6420
	step [58/192], loss=23.3352
	step [59/192], loss=21.9712
	step [60/192], loss=20.3655
	step [61/192], loss=22.0700
	step [62/192], loss=22.1569
	step [63/192], loss=21.9581
	step [64/192], loss=22.0569
	step [65/192], loss=23.1743
	step [66/192], loss=24.5401
	step [67/192], loss=25.1401
	step [68/192], loss=20.5482
	step [69/192], loss=23.3426
	step [70/192], loss=20.9312
	step [71/192], loss=23.6500
	step [72/192], loss=20.9944
	step [73/192], loss=21.9986
	step [74/192], loss=22.0388
	step [75/192], loss=23.9891
	step [76/192], loss=25.1145
	step [77/192], loss=24.7919
	step [78/192], loss=22.1483
	step [79/192], loss=20.5429
	step [80/192], loss=21.9223
	step [81/192], loss=21.3763
	step [82/192], loss=21.5221
	step [83/192], loss=22.8657
	step [84/192], loss=21.8080
	step [85/192], loss=22.0221
	step [86/192], loss=25.1645
	step [87/192], loss=20.4918
	step [88/192], loss=22.0064
	step [89/192], loss=20.5731
	step [90/192], loss=23.0724
	step [91/192], loss=21.0010
	step [92/192], loss=23.3023
	step [93/192], loss=22.3174
	step [94/192], loss=21.7052
	step [95/192], loss=21.8035
	step [96/192], loss=21.2706
	step [97/192], loss=22.0647
	step [98/192], loss=22.5200
	step [99/192], loss=20.7404
	step [100/192], loss=22.8351
	step [101/192], loss=22.0057
	step [102/192], loss=20.9614
	step [103/192], loss=20.8244
	step [104/192], loss=22.4088
	step [105/192], loss=23.4547
	step [106/192], loss=22.7305
	step [107/192], loss=21.0315
	step [108/192], loss=21.2642
	step [109/192], loss=21.9981
	step [110/192], loss=21.7138
	step [111/192], loss=21.6291
	step [112/192], loss=19.6308
	step [113/192], loss=19.6160
	step [114/192], loss=26.0164
	step [115/192], loss=21.7560
	step [116/192], loss=22.0555
	step [117/192], loss=20.7479
	step [118/192], loss=19.9529
	step [119/192], loss=21.1004
	step [120/192], loss=23.4807
	step [121/192], loss=19.4610
	step [122/192], loss=22.2490
	step [123/192], loss=23.3318
	step [124/192], loss=22.3955
	step [125/192], loss=20.6221
	step [126/192], loss=21.1632
	step [127/192], loss=20.4438
	step [128/192], loss=22.7578
	step [129/192], loss=20.4835
	step [130/192], loss=23.8465
	step [131/192], loss=21.6286
	step [132/192], loss=24.4543
	step [133/192], loss=22.3850
	step [134/192], loss=20.7241
	step [135/192], loss=22.9282
	step [136/192], loss=19.9252
	step [137/192], loss=23.3128
	step [138/192], loss=21.5373
	step [139/192], loss=21.3088
	step [140/192], loss=22.8507
	step [141/192], loss=20.4556
	step [142/192], loss=20.7532
	step [143/192], loss=21.0717
	step [144/192], loss=20.8101
	step [145/192], loss=19.8036
	step [146/192], loss=26.3449
	step [147/192], loss=21.9697
	step [148/192], loss=20.0336
	step [149/192], loss=20.3707
	step [150/192], loss=23.1783
	step [151/192], loss=21.7354
	step [152/192], loss=20.4497
	step [153/192], loss=21.4110
	step [154/192], loss=20.6178
	step [155/192], loss=21.6285
	step [156/192], loss=21.6915
	step [157/192], loss=22.4298
	step [158/192], loss=19.5804
	step [159/192], loss=19.6521
	step [160/192], loss=20.6115
	step [161/192], loss=20.5192
	step [162/192], loss=23.1143
	step [163/192], loss=21.1764
	step [164/192], loss=22.5824
	step [165/192], loss=22.6501
	step [166/192], loss=21.4450
	step [167/192], loss=19.4026
	step [168/192], loss=18.8471
	step [169/192], loss=21.4935
	step [170/192], loss=22.1111
	step [171/192], loss=21.7565
	step [172/192], loss=20.1227
	step [173/192], loss=23.6732
	step [174/192], loss=25.1592
	step [175/192], loss=21.1247
	step [176/192], loss=21.5675
	step [177/192], loss=21.9866
	step [178/192], loss=20.1759
	step [179/192], loss=21.3361
	step [180/192], loss=20.1159
	step [181/192], loss=22.9139
	step [182/192], loss=22.9350
	step [183/192], loss=20.9640
	step [184/192], loss=20.2150
	step [185/192], loss=22.1211
	step [186/192], loss=21.2992
	step [187/192], loss=20.8827
	step [188/192], loss=20.5486
	step [189/192], loss=18.0284
	step [190/192], loss=20.8869
	step [191/192], loss=18.8248
	step [192/192], loss=2.3876
	Evaluating
	loss=0.0630, precision=0.1819, recall=0.9938, f1=0.3076
Training epoch 9
	step [1/192], loss=21.6074
	step [2/192], loss=20.6888
	step [3/192], loss=21.5942
	step [4/192], loss=19.6589
	step [5/192], loss=20.7968
	step [6/192], loss=23.0038
	step [7/192], loss=22.3704
	step [8/192], loss=21.0132
	step [9/192], loss=21.2244
	step [10/192], loss=20.0406
	step [11/192], loss=24.5848
	step [12/192], loss=20.9828
	step [13/192], loss=21.5773
	step [14/192], loss=19.9592
	step [15/192], loss=20.0570
	step [16/192], loss=20.2571
	step [17/192], loss=20.2789
	step [18/192], loss=18.2267
	step [19/192], loss=21.2260
	step [20/192], loss=19.8564
	step [21/192], loss=20.6685
	step [22/192], loss=20.3038
	step [23/192], loss=20.7564
	step [24/192], loss=20.7738
	step [25/192], loss=20.7498
	step [26/192], loss=20.8982
	step [27/192], loss=22.6930
	step [28/192], loss=21.2481
	step [29/192], loss=20.3642
	step [30/192], loss=21.2985
	step [31/192], loss=20.9543
	step [32/192], loss=21.9990
	step [33/192], loss=18.5205
	step [34/192], loss=18.3155
	step [35/192], loss=20.4705
	step [36/192], loss=22.3283
	step [37/192], loss=20.3021
	step [38/192], loss=20.9632
	step [39/192], loss=17.8763
	step [40/192], loss=20.2822
	step [41/192], loss=20.1970
	step [42/192], loss=19.0297
	step [43/192], loss=19.1545
	step [44/192], loss=21.7657
	step [45/192], loss=20.9032
	step [46/192], loss=21.4861
	step [47/192], loss=21.5278
	step [48/192], loss=21.8528
	step [49/192], loss=20.1855
	step [50/192], loss=20.3964
	step [51/192], loss=21.5825
	step [52/192], loss=19.0793
	step [53/192], loss=19.7669
	step [54/192], loss=19.1914
	step [55/192], loss=20.8654
	step [56/192], loss=18.9403
	step [57/192], loss=19.0172
	step [58/192], loss=19.7697
	step [59/192], loss=20.0101
	step [60/192], loss=19.6865
	step [61/192], loss=19.0120
	step [62/192], loss=18.4504
	step [63/192], loss=22.3146
	step [64/192], loss=22.0591
	step [65/192], loss=21.3755
	step [66/192], loss=20.8783
	step [67/192], loss=24.3227
	step [68/192], loss=18.6683
	step [69/192], loss=21.0880
	step [70/192], loss=20.5682
	step [71/192], loss=21.8655
	step [72/192], loss=19.3849
	step [73/192], loss=19.1161
	step [74/192], loss=18.2133
	step [75/192], loss=22.5514
	step [76/192], loss=20.2909
	step [77/192], loss=19.3686
	step [78/192], loss=20.1398
	step [79/192], loss=18.5489
	step [80/192], loss=20.3239
	step [81/192], loss=21.2812
	step [82/192], loss=19.6838
	step [83/192], loss=19.6028
	step [84/192], loss=22.1713
	step [85/192], loss=18.9797
	step [86/192], loss=19.2247
	step [87/192], loss=18.9029
	step [88/192], loss=20.9254
	step [89/192], loss=17.8588
	step [90/192], loss=22.1398
	step [91/192], loss=20.5143
	step [92/192], loss=19.8136
	step [93/192], loss=19.3527
	step [94/192], loss=18.7929
	step [95/192], loss=19.6877
	step [96/192], loss=21.9225
	step [97/192], loss=16.9562
	step [98/192], loss=18.9789
	step [99/192], loss=19.5597
	step [100/192], loss=20.0784
	step [101/192], loss=18.6579
	step [102/192], loss=18.2442
	step [103/192], loss=18.1841
	step [104/192], loss=18.3450
	step [105/192], loss=17.4895
	step [106/192], loss=18.4511
	step [107/192], loss=19.2626
	step [108/192], loss=20.4832
	step [109/192], loss=19.5948
	step [110/192], loss=17.9503
	step [111/192], loss=19.0108
	step [112/192], loss=20.1144
	step [113/192], loss=19.7194
	step [114/192], loss=18.6860
	step [115/192], loss=18.9152
	step [116/192], loss=21.1112
	step [117/192], loss=19.9971
	step [118/192], loss=17.1956
	step [119/192], loss=18.0250
	step [120/192], loss=19.9740
	step [121/192], loss=23.2085
	step [122/192], loss=20.0682
	step [123/192], loss=19.2646
	step [124/192], loss=18.5593
	step [125/192], loss=19.4936
	step [126/192], loss=18.0535
	step [127/192], loss=17.7877
	step [128/192], loss=20.6447
	step [129/192], loss=21.3263
	step [130/192], loss=19.4910
	step [131/192], loss=17.0671
	step [132/192], loss=18.0136
	step [133/192], loss=21.5329
	step [134/192], loss=20.2539
	step [135/192], loss=21.2551
	step [136/192], loss=19.3222
	step [137/192], loss=18.3899
	step [138/192], loss=21.1118
	step [139/192], loss=21.0579
	step [140/192], loss=20.8396
	step [141/192], loss=21.1218
	step [142/192], loss=19.3351
	step [143/192], loss=18.1191
	step [144/192], loss=20.7291
	step [145/192], loss=17.8917
	step [146/192], loss=18.8834
	step [147/192], loss=21.3458
	step [148/192], loss=18.6680
	step [149/192], loss=20.5675
	step [150/192], loss=20.3964
	step [151/192], loss=17.7353
	step [152/192], loss=20.3272
	step [153/192], loss=18.1671
	step [154/192], loss=20.5361
	step [155/192], loss=19.2529
	step [156/192], loss=19.3058
	step [157/192], loss=19.6040
	step [158/192], loss=21.6708
	step [159/192], loss=20.0481
	step [160/192], loss=20.1988
	step [161/192], loss=19.5837
	step [162/192], loss=18.3815
	step [163/192], loss=19.8572
	step [164/192], loss=18.2168
	step [165/192], loss=19.7699
	step [166/192], loss=20.7427
	step [167/192], loss=17.2102
	step [168/192], loss=18.2228
	step [169/192], loss=19.4204
	step [170/192], loss=19.8705
	step [171/192], loss=18.4221
	step [172/192], loss=16.7087
	step [173/192], loss=20.7402
	step [174/192], loss=17.8878
	step [175/192], loss=19.7544
	step [176/192], loss=19.2137
	step [177/192], loss=19.6749
	step [178/192], loss=18.3855
	step [179/192], loss=19.5231
	step [180/192], loss=20.4809
	step [181/192], loss=20.7898
	step [182/192], loss=21.1035
	step [183/192], loss=19.1770
	step [184/192], loss=18.5246
	step [185/192], loss=17.6639
	step [186/192], loss=19.6592
	step [187/192], loss=19.4344
	step [188/192], loss=19.2485
	step [189/192], loss=18.5059
	step [190/192], loss=18.7830
	step [191/192], loss=20.7274
	step [192/192], loss=2.1496
	Evaluating
	loss=0.0575, precision=0.1669, recall=0.9946, f1=0.2858
Training epoch 10
	step [1/192], loss=17.9928
	step [2/192], loss=20.4863
	step [3/192], loss=16.5508
	step [4/192], loss=19.2135
	step [5/192], loss=17.1396
	step [6/192], loss=19.3027
	step [7/192], loss=19.8180
	step [8/192], loss=20.2777
	step [9/192], loss=18.4146
	step [10/192], loss=18.4806
	step [11/192], loss=17.5138
	step [12/192], loss=17.0722
	step [13/192], loss=16.3622
	step [14/192], loss=19.2154
	step [15/192], loss=17.4321
	step [16/192], loss=20.2910
	step [17/192], loss=19.2012
	step [18/192], loss=19.3616
	step [19/192], loss=20.1239
	step [20/192], loss=18.0293
	step [21/192], loss=20.8474
	step [22/192], loss=19.5049
	step [23/192], loss=18.1889
	step [24/192], loss=19.1631
	step [25/192], loss=19.4880
	step [26/192], loss=18.7949
	step [27/192], loss=20.8820
	step [28/192], loss=17.6349
	step [29/192], loss=16.1543
	step [30/192], loss=19.2540
	step [31/192], loss=17.1008
	step [32/192], loss=16.8858
	step [33/192], loss=19.1646
	step [34/192], loss=18.3601
	step [35/192], loss=19.0360
	step [36/192], loss=17.7083
	step [37/192], loss=17.2737
	step [38/192], loss=18.8725
	step [39/192], loss=20.2220
	step [40/192], loss=19.6007
	step [41/192], loss=24.4878
	step [42/192], loss=18.4661
	step [43/192], loss=19.0531
	step [44/192], loss=18.9908
	step [45/192], loss=16.8212
	step [46/192], loss=17.4120
	step [47/192], loss=17.3229
	step [48/192], loss=17.0868
	step [49/192], loss=20.4728
	step [50/192], loss=20.0146
	step [51/192], loss=17.2839
	step [52/192], loss=23.9160
	step [53/192], loss=18.2486
	step [54/192], loss=16.0720
	step [55/192], loss=16.4753
	step [56/192], loss=18.8295
	step [57/192], loss=15.7479
	step [58/192], loss=17.6390
	step [59/192], loss=18.1995
	step [60/192], loss=19.7644
	step [61/192], loss=17.1882
	step [62/192], loss=18.9411
	step [63/192], loss=17.9597
	step [64/192], loss=20.1584
	step [65/192], loss=19.0505
	step [66/192], loss=18.0611
	step [67/192], loss=16.3029
	step [68/192], loss=17.0950
	step [69/192], loss=16.1969
	step [70/192], loss=18.3448
	step [71/192], loss=19.1113
	step [72/192], loss=19.0678
	step [73/192], loss=20.4034
	step [74/192], loss=15.4592
	step [75/192], loss=15.7735
	step [76/192], loss=20.0390
	step [77/192], loss=19.9591
	step [78/192], loss=17.0215
	step [79/192], loss=19.2856
	step [80/192], loss=18.1173
	step [81/192], loss=18.2781
	step [82/192], loss=17.5402
	step [83/192], loss=16.2449
	step [84/192], loss=17.8021
	step [85/192], loss=18.6605
	step [86/192], loss=17.8795
	step [87/192], loss=19.6908
	step [88/192], loss=17.8108
	step [89/192], loss=19.9155
	step [90/192], loss=19.0360
	step [91/192], loss=18.0429
	step [92/192], loss=17.7839
	step [93/192], loss=17.9025
	step [94/192], loss=18.9027
	step [95/192], loss=17.8860
	step [96/192], loss=17.3992
	step [97/192], loss=16.4518
	step [98/192], loss=22.5923
	step [99/192], loss=18.2102
	step [100/192], loss=19.3096
	step [101/192], loss=16.6444
	step [102/192], loss=19.1903
	step [103/192], loss=18.9729
	step [104/192], loss=16.6302
	step [105/192], loss=18.2668
	step [106/192], loss=17.5625
	step [107/192], loss=17.7689
	step [108/192], loss=15.7899
	step [109/192], loss=17.7917
	step [110/192], loss=19.2559
	step [111/192], loss=18.4326
	step [112/192], loss=19.4359
	step [113/192], loss=20.7669
	step [114/192], loss=18.9059
	step [115/192], loss=18.3860
	step [116/192], loss=15.9373
	step [117/192], loss=19.6010
	step [118/192], loss=18.6430
	step [119/192], loss=17.3300
	step [120/192], loss=19.8956
	step [121/192], loss=17.7314
	step [122/192], loss=17.5671
	step [123/192], loss=17.7275
	step [124/192], loss=15.8624
	step [125/192], loss=18.6571
	step [126/192], loss=19.2137
	step [127/192], loss=17.3740
	step [128/192], loss=18.6151
	step [129/192], loss=18.8451
	step [130/192], loss=16.3200
	step [131/192], loss=17.7033
	step [132/192], loss=17.1681
	step [133/192], loss=15.4267
	step [134/192], loss=19.9734
	step [135/192], loss=16.9096
	step [136/192], loss=17.4899
	step [137/192], loss=20.0002
	step [138/192], loss=16.2703
	step [139/192], loss=17.4989
	step [140/192], loss=18.3146
	step [141/192], loss=18.0141
	step [142/192], loss=19.2911
	step [143/192], loss=15.8912
	step [144/192], loss=17.2644
	step [145/192], loss=19.4389
	step [146/192], loss=18.5876
	step [147/192], loss=19.4618
	step [148/192], loss=18.3927
	step [149/192], loss=18.1846
	step [150/192], loss=15.0110
	step [151/192], loss=18.9868
	step [152/192], loss=18.8596
	step [153/192], loss=19.6516
	step [154/192], loss=19.7846
	step [155/192], loss=15.3851
	step [156/192], loss=20.3908
	step [157/192], loss=15.6328
	step [158/192], loss=17.8600
	step [159/192], loss=17.9645
	step [160/192], loss=16.5201
	step [161/192], loss=18.0666
	step [162/192], loss=17.2685
	step [163/192], loss=17.3427
	step [164/192], loss=20.5063
	step [165/192], loss=19.7049
	step [166/192], loss=18.8356
	step [167/192], loss=17.0039
	step [168/192], loss=17.6349
	step [169/192], loss=15.0108
	step [170/192], loss=17.5753
	step [171/192], loss=17.8020
	step [172/192], loss=17.7739
	step [173/192], loss=17.9386
	step [174/192], loss=20.4656
	step [175/192], loss=17.2486
	step [176/192], loss=15.1146
	step [177/192], loss=17.0768
	step [178/192], loss=16.0490
	step [179/192], loss=18.3079
	step [180/192], loss=19.1168
	step [181/192], loss=16.7615
	step [182/192], loss=17.7546
	step [183/192], loss=17.3931
	step [184/192], loss=18.3198
	step [185/192], loss=16.9641
	step [186/192], loss=16.4576
	step [187/192], loss=22.3454
	step [188/192], loss=15.8463
	step [189/192], loss=17.6937
	step [190/192], loss=17.4956
	step [191/192], loss=18.3247
	step [192/192], loss=2.0818
	Evaluating
	loss=0.0515, precision=0.1650, recall=0.9939, f1=0.2830
Training epoch 11
	step [1/192], loss=17.9273
	step [2/192], loss=18.3090
	step [3/192], loss=17.7811
	step [4/192], loss=16.7046
	step [5/192], loss=18.7022
	step [6/192], loss=17.5517
	step [7/192], loss=17.9942
	step [8/192], loss=18.4603
	step [9/192], loss=16.1739
	step [10/192], loss=18.2917
	step [11/192], loss=15.1666
	step [12/192], loss=17.0600
	step [13/192], loss=16.6450
	step [14/192], loss=17.3730
	step [15/192], loss=17.7086
	step [16/192], loss=16.9281
	step [17/192], loss=17.8090
	step [18/192], loss=16.0437
	step [19/192], loss=17.7880
	step [20/192], loss=17.2041
	step [21/192], loss=18.0702
	step [22/192], loss=18.3354
	step [23/192], loss=14.6055
	step [24/192], loss=16.6714
	step [25/192], loss=15.7318
	step [26/192], loss=18.5291
	step [27/192], loss=14.5354
	step [28/192], loss=18.1201
	step [29/192], loss=15.9880
	step [30/192], loss=15.6512
	step [31/192], loss=16.3538
	step [32/192], loss=18.3796
	step [33/192], loss=16.4868
	step [34/192], loss=16.8372
	step [35/192], loss=19.5753
	step [36/192], loss=16.0387
	step [37/192], loss=16.4612
	step [38/192], loss=16.7207
	step [39/192], loss=18.6804
	step [40/192], loss=16.2016
	step [41/192], loss=15.0992
	step [42/192], loss=18.1006
	step [43/192], loss=17.2726
	step [44/192], loss=16.1195
	step [45/192], loss=17.3907
	step [46/192], loss=17.4799
	step [47/192], loss=14.5506
	step [48/192], loss=16.1261
	step [49/192], loss=16.2476
	step [50/192], loss=16.5067
	step [51/192], loss=15.5592
	step [52/192], loss=20.4014
	step [53/192], loss=17.3198
	step [54/192], loss=17.0110
	step [55/192], loss=18.5596
	step [56/192], loss=16.1904
	step [57/192], loss=16.5476
	step [58/192], loss=16.2076
	step [59/192], loss=15.7679
	step [60/192], loss=16.1001
	step [61/192], loss=19.0012
	step [62/192], loss=15.2538
	step [63/192], loss=16.9475
	step [64/192], loss=17.5942
	step [65/192], loss=17.4458
	step [66/192], loss=14.9405
	step [67/192], loss=16.0991
	step [68/192], loss=15.1981
	step [69/192], loss=15.3513
	step [70/192], loss=18.1830
	step [71/192], loss=20.9045
	step [72/192], loss=16.4715
	step [73/192], loss=16.4313
	step [74/192], loss=17.4070
	step [75/192], loss=16.3048
	step [76/192], loss=18.1391
	step [77/192], loss=18.0698
	step [78/192], loss=15.8476
	step [79/192], loss=15.9581
	step [80/192], loss=16.8416
	step [81/192], loss=14.8028
	step [82/192], loss=18.7977
	step [83/192], loss=17.7243
	step [84/192], loss=18.0585
	step [85/192], loss=17.5520
	step [86/192], loss=15.9766
	step [87/192], loss=17.9863
	step [88/192], loss=14.8965
	step [89/192], loss=17.1329
	step [90/192], loss=18.3577
	step [91/192], loss=15.0448
	step [92/192], loss=16.1671
	step [93/192], loss=18.5244
	step [94/192], loss=17.6507
	step [95/192], loss=15.4513
	step [96/192], loss=14.2804
	step [97/192], loss=15.4214
	step [98/192], loss=16.7978
	step [99/192], loss=15.0930
	step [100/192], loss=17.4887
	step [101/192], loss=17.8330
	step [102/192], loss=14.5661
	step [103/192], loss=13.0718
	step [104/192], loss=17.7267
	step [105/192], loss=15.7659
	step [106/192], loss=17.1425
	step [107/192], loss=16.6967
	step [108/192], loss=14.9228
	step [109/192], loss=17.2205
	step [110/192], loss=17.1790
	step [111/192], loss=16.8727
	step [112/192], loss=16.1055
	step [113/192], loss=17.6800
	step [114/192], loss=18.4867
	step [115/192], loss=20.4030
	step [116/192], loss=18.0588
	step [117/192], loss=17.6794
	step [118/192], loss=17.0397
	step [119/192], loss=17.6783
	step [120/192], loss=16.2873
	step [121/192], loss=17.2585
	step [122/192], loss=17.2167
	step [123/192], loss=16.7152
	step [124/192], loss=17.0095
	step [125/192], loss=14.7855
	step [126/192], loss=16.6684
	step [127/192], loss=16.9134
	step [128/192], loss=15.9639
	step [129/192], loss=14.6037
	step [130/192], loss=15.6171
	step [131/192], loss=15.4302
	step [132/192], loss=14.9442
	step [133/192], loss=15.9085
	step [134/192], loss=17.6095
	step [135/192], loss=15.5214
	step [136/192], loss=18.6139
	step [137/192], loss=16.5493
	step [138/192], loss=16.3351
	step [139/192], loss=16.8449
	step [140/192], loss=15.6090
	step [141/192], loss=17.8215
	step [142/192], loss=16.7129
	step [143/192], loss=14.9462
	step [144/192], loss=13.5855
	step [145/192], loss=15.0735
	step [146/192], loss=18.1959
	step [147/192], loss=15.4340
	step [148/192], loss=15.9767
	step [149/192], loss=19.2439
	step [150/192], loss=16.6944
	step [151/192], loss=17.4239
	step [152/192], loss=15.7177
	step [153/192], loss=15.3875
	step [154/192], loss=18.5169
	step [155/192], loss=17.1855
	step [156/192], loss=14.5046
	step [157/192], loss=15.8326
	step [158/192], loss=15.6176
	step [159/192], loss=14.9275
	step [160/192], loss=15.3670
	step [161/192], loss=14.8940
	step [162/192], loss=16.8979
	step [163/192], loss=14.3404
	step [164/192], loss=17.3576
	step [165/192], loss=15.1263
	step [166/192], loss=16.0279
	step [167/192], loss=16.2844
	step [168/192], loss=16.6497
	step [169/192], loss=16.8428
	step [170/192], loss=15.5827
	step [171/192], loss=16.5355
	step [172/192], loss=15.1907
	step [173/192], loss=17.3052
	step [174/192], loss=16.7585
	step [175/192], loss=15.9635
	step [176/192], loss=15.5820
	step [177/192], loss=15.5225
	step [178/192], loss=15.4651
	step [179/192], loss=15.1858
	step [180/192], loss=15.3542
	step [181/192], loss=14.4152
	step [182/192], loss=15.2470
	step [183/192], loss=16.5225
	step [184/192], loss=16.8211
	step [185/192], loss=17.9066
	step [186/192], loss=14.1043
	step [187/192], loss=16.9552
	step [188/192], loss=18.0185
	step [189/192], loss=16.6833
	step [190/192], loss=17.2217
	step [191/192], loss=15.6476
	step [192/192], loss=2.9686
	Evaluating
	loss=0.0468, precision=0.1658, recall=0.9943, f1=0.2841
Training epoch 12
	step [1/192], loss=14.3868
	step [2/192], loss=14.2296
	step [3/192], loss=16.2098
	step [4/192], loss=15.0058
	step [5/192], loss=15.7470
	step [6/192], loss=15.2333
	step [7/192], loss=17.0573
	step [8/192], loss=16.2099
	step [9/192], loss=18.6528
	step [10/192], loss=16.9264
	step [11/192], loss=16.1902
	step [12/192], loss=16.9183
	step [13/192], loss=15.3725
	step [14/192], loss=15.5583
	step [15/192], loss=17.1267
	step [16/192], loss=14.5740
	step [17/192], loss=17.1624
	step [18/192], loss=14.3399
	step [19/192], loss=15.1772
	step [20/192], loss=15.0267
	step [21/192], loss=14.4884
	step [22/192], loss=17.2615
	step [23/192], loss=16.4093
	step [24/192], loss=16.8354
	step [25/192], loss=15.2780
	step [26/192], loss=15.6362
	step [27/192], loss=12.9749
	step [28/192], loss=17.1002
	step [29/192], loss=16.5076
	step [30/192], loss=16.7233
	step [31/192], loss=17.4146
	step [32/192], loss=14.9514
	step [33/192], loss=17.3212
	step [34/192], loss=15.5858
	step [35/192], loss=15.7413
	step [36/192], loss=14.8924
	step [37/192], loss=16.1814
	step [38/192], loss=18.7124
	step [39/192], loss=14.2929
	step [40/192], loss=17.9885
	step [41/192], loss=15.5091
	step [42/192], loss=13.6482
	step [43/192], loss=14.0567
	step [44/192], loss=15.9289
	step [45/192], loss=16.6042
	step [46/192], loss=14.7567
	step [47/192], loss=14.3127
	step [48/192], loss=15.6991
	step [49/192], loss=13.5945
	step [50/192], loss=13.6400
	step [51/192], loss=15.4620
	step [52/192], loss=15.8336
	step [53/192], loss=14.4064
	step [54/192], loss=15.6791
	step [55/192], loss=13.8932
	step [56/192], loss=14.6254
	step [57/192], loss=12.5145
	step [58/192], loss=15.7335
	step [59/192], loss=14.7302
	step [60/192], loss=17.3795
	step [61/192], loss=15.4365
	step [62/192], loss=15.4813
	step [63/192], loss=13.7795
	step [64/192], loss=15.8522
	step [65/192], loss=15.9615
	step [66/192], loss=16.1532
	step [67/192], loss=17.9764
	step [68/192], loss=16.9466
	step [69/192], loss=14.3860
	step [70/192], loss=15.5501
	step [71/192], loss=15.2042
	step [72/192], loss=14.7014
	step [73/192], loss=15.2367
	step [74/192], loss=16.3634
	step [75/192], loss=14.3369
	step [76/192], loss=14.7299
	step [77/192], loss=14.9555
	step [78/192], loss=15.0409
	step [79/192], loss=15.5891
	step [80/192], loss=14.7181
	step [81/192], loss=17.0636
	step [82/192], loss=15.4023
	step [83/192], loss=16.3038
	step [84/192], loss=14.8994
	step [85/192], loss=14.9585
	step [86/192], loss=15.4864
	step [87/192], loss=17.2183
	step [88/192], loss=14.9690
	step [89/192], loss=15.3738
	step [90/192], loss=17.5791
	step [91/192], loss=15.8895
	step [92/192], loss=14.8312
	step [93/192], loss=17.4581
	step [94/192], loss=14.5364
	step [95/192], loss=15.0699
	step [96/192], loss=14.3495
	step [97/192], loss=15.4232
	step [98/192], loss=16.6208
	step [99/192], loss=15.2853
	step [100/192], loss=17.4880
	step [101/192], loss=16.5798
	step [102/192], loss=13.4086
	step [103/192], loss=13.6814
	step [104/192], loss=15.5183
	step [105/192], loss=16.7350
	step [106/192], loss=16.3198
	step [107/192], loss=17.1670
	step [108/192], loss=17.0849
	step [109/192], loss=15.8797
	step [110/192], loss=16.3697
	step [111/192], loss=12.1939
	step [112/192], loss=14.3808
	step [113/192], loss=16.4321
	step [114/192], loss=17.6029
	step [115/192], loss=15.4266
	step [116/192], loss=14.5343
	step [117/192], loss=14.8544
	step [118/192], loss=14.5249
	step [119/192], loss=13.8152
	step [120/192], loss=14.9128
	step [121/192], loss=17.3174
	step [122/192], loss=15.9345
	step [123/192], loss=15.2924
	step [124/192], loss=16.9281
	step [125/192], loss=16.8223
	step [126/192], loss=16.2135
	step [127/192], loss=15.8566
	step [128/192], loss=16.0240
	step [129/192], loss=15.1047
	step [130/192], loss=14.2902
	step [131/192], loss=17.8178
	step [132/192], loss=15.2226
	step [133/192], loss=16.6387
	step [134/192], loss=17.3375
	step [135/192], loss=15.6092
	step [136/192], loss=12.9483
	step [137/192], loss=17.3870
	step [138/192], loss=13.9668
	step [139/192], loss=16.1476
	step [140/192], loss=15.8788
	step [141/192], loss=15.7776
	step [142/192], loss=15.0296
	step [143/192], loss=16.7731
	step [144/192], loss=13.0526
	step [145/192], loss=14.5265
	step [146/192], loss=16.1877
	step [147/192], loss=16.5066
	step [148/192], loss=13.7410
	step [149/192], loss=14.3566
	step [150/192], loss=13.9782
	step [151/192], loss=15.5938
	step [152/192], loss=15.5984
	step [153/192], loss=13.5989
	step [154/192], loss=14.3325
	step [155/192], loss=17.4611
	step [156/192], loss=13.6194
	step [157/192], loss=14.7898
	step [158/192], loss=14.9592
	step [159/192], loss=15.2896
	step [160/192], loss=14.5377
	step [161/192], loss=15.1122
	step [162/192], loss=12.9425
	step [163/192], loss=16.0243
	step [164/192], loss=14.1828
	step [165/192], loss=16.6338
	step [166/192], loss=14.6738
	step [167/192], loss=16.7648
	step [168/192], loss=18.1775
	step [169/192], loss=14.2100
	step [170/192], loss=14.7185
	step [171/192], loss=14.2862
	step [172/192], loss=14.9721
	step [173/192], loss=16.3970
	step [174/192], loss=14.7675
	step [175/192], loss=13.1378
	step [176/192], loss=18.6409
	step [177/192], loss=14.3887
	step [178/192], loss=14.7011
	step [179/192], loss=15.8052
	step [180/192], loss=16.1536
	step [181/192], loss=14.1318
	step [182/192], loss=14.9888
	step [183/192], loss=13.2638
	step [184/192], loss=13.8576
	step [185/192], loss=16.8327
	step [186/192], loss=13.4354
	step [187/192], loss=16.9771
	step [188/192], loss=16.5396
	step [189/192], loss=16.5231
	step [190/192], loss=15.5557
	step [191/192], loss=16.8416
	step [192/192], loss=2.4469
	Evaluating
	loss=0.0423, precision=0.1845, recall=0.9937, f1=0.3112
Training epoch 13
	step [1/192], loss=16.4236
	step [2/192], loss=14.7304
	step [3/192], loss=14.4791
	step [4/192], loss=14.9895
	step [5/192], loss=13.9207
	step [6/192], loss=14.2198
	step [7/192], loss=16.6438
	step [8/192], loss=15.5086
	step [9/192], loss=13.7520
	step [10/192], loss=15.1146
	step [11/192], loss=14.4618
	step [12/192], loss=13.6350
	step [13/192], loss=15.0613
	step [14/192], loss=16.3700
	step [15/192], loss=13.9046
	step [16/192], loss=15.1665
	step [17/192], loss=15.7653
	step [18/192], loss=14.0942
	step [19/192], loss=15.1762
	step [20/192], loss=13.1584
	step [21/192], loss=12.5200
	step [22/192], loss=14.9481
	step [23/192], loss=14.3517
	step [24/192], loss=14.4427
	step [25/192], loss=14.1502
	step [26/192], loss=16.8042
	step [27/192], loss=15.6789
	step [28/192], loss=13.0897
	step [29/192], loss=15.9982
	step [30/192], loss=14.2096
	step [31/192], loss=13.8404
	step [32/192], loss=13.7104
	step [33/192], loss=15.9817
	step [34/192], loss=16.3674
	step [35/192], loss=13.0619
	step [36/192], loss=14.0240
	step [37/192], loss=15.4617
	step [38/192], loss=12.9722
	step [39/192], loss=15.4373
	step [40/192], loss=15.2667
	step [41/192], loss=14.0283
	step [42/192], loss=13.6035
	step [43/192], loss=15.7012
	step [44/192], loss=13.3626
	step [45/192], loss=14.6360
	step [46/192], loss=12.4066
	step [47/192], loss=14.9715
	step [48/192], loss=14.9504
	step [49/192], loss=13.7642
	step [50/192], loss=14.6262
	step [51/192], loss=15.4847
	step [52/192], loss=16.4307
	step [53/192], loss=14.6352
	step [54/192], loss=16.4876
	step [55/192], loss=14.0239
	step [56/192], loss=14.5199
	step [57/192], loss=17.3826
	step [58/192], loss=18.0143
	step [59/192], loss=15.5181
	step [60/192], loss=14.5384
	step [61/192], loss=15.4283
	step [62/192], loss=13.4757
	step [63/192], loss=14.5497
	step [64/192], loss=15.8703
	step [65/192], loss=15.1685
	step [66/192], loss=13.1349
	step [67/192], loss=13.4075
	step [68/192], loss=16.6271
	step [69/192], loss=16.2783
	step [70/192], loss=12.6975
	step [71/192], loss=13.2494
	step [72/192], loss=13.1819
	step [73/192], loss=14.5293
	step [74/192], loss=14.0940
	step [75/192], loss=16.0235
	step [76/192], loss=13.9622
	step [77/192], loss=15.2336
	step [78/192], loss=14.6160
	step [79/192], loss=16.3965
	step [80/192], loss=13.9205
	step [81/192], loss=13.2025
	step [82/192], loss=15.4523
	step [83/192], loss=13.2664
	step [84/192], loss=14.6435
	step [85/192], loss=13.9300
	step [86/192], loss=12.6864
	step [87/192], loss=12.9563
	step [88/192], loss=15.2414
	step [89/192], loss=13.0864
	step [90/192], loss=15.6244
	step [91/192], loss=12.6101
	step [92/192], loss=12.8464
	step [93/192], loss=15.2641
	step [94/192], loss=14.5771
	step [95/192], loss=14.2091
	step [96/192], loss=13.7717
	step [97/192], loss=14.5385
	step [98/192], loss=14.1946
	step [99/192], loss=16.5618
	step [100/192], loss=14.6821
	step [101/192], loss=15.4533
	step [102/192], loss=14.8740
	step [103/192], loss=15.7532
	step [104/192], loss=14.5420
	step [105/192], loss=15.1354
	step [106/192], loss=12.8289
	step [107/192], loss=15.6461
	step [108/192], loss=14.0961
	step [109/192], loss=16.7614
	step [110/192], loss=13.2003
	step [111/192], loss=17.3813
	step [112/192], loss=17.7549
	step [113/192], loss=12.2535
	step [114/192], loss=14.1824
	step [115/192], loss=14.3438
	step [116/192], loss=15.1756
	step [117/192], loss=16.2370
	step [118/192], loss=13.5892
	step [119/192], loss=14.0530
	step [120/192], loss=14.3608
	step [121/192], loss=14.2129
	step [122/192], loss=12.5810
	step [123/192], loss=13.5533
	step [124/192], loss=14.3512
	step [125/192], loss=12.5217
	step [126/192], loss=14.3862
	step [127/192], loss=15.4517
	step [128/192], loss=13.7716
	step [129/192], loss=14.4503
	step [130/192], loss=14.9748
	step [131/192], loss=13.7785
	step [132/192], loss=14.1170
	step [133/192], loss=15.1856
	step [134/192], loss=12.9653
	step [135/192], loss=12.8396
	step [136/192], loss=15.9012
	step [137/192], loss=13.1387
	step [138/192], loss=15.2030
	step [139/192], loss=16.5974
	step [140/192], loss=17.7538
	step [141/192], loss=13.5796
	step [142/192], loss=13.5963
	step [143/192], loss=13.1614
	step [144/192], loss=17.9072
	step [145/192], loss=14.6082
	step [146/192], loss=15.8943
	step [147/192], loss=12.5959
	step [148/192], loss=12.8633
	step [149/192], loss=13.6417
	step [150/192], loss=14.5609
	step [151/192], loss=12.7702
	step [152/192], loss=12.2896
	step [153/192], loss=12.5919
	step [154/192], loss=15.3629
	step [155/192], loss=15.4532
	step [156/192], loss=13.3632
	step [157/192], loss=16.1490
	step [158/192], loss=14.2864
	step [159/192], loss=12.8477
	step [160/192], loss=13.4563
	step [161/192], loss=13.3329
	step [162/192], loss=13.4828
	step [163/192], loss=15.4917
	step [164/192], loss=16.7803
	step [165/192], loss=14.1611
	step [166/192], loss=11.7968
	step [167/192], loss=14.4296
	step [168/192], loss=14.1926
	step [169/192], loss=14.6402
	step [170/192], loss=13.7812
	step [171/192], loss=14.9328
	step [172/192], loss=14.8699
	step [173/192], loss=14.2754
	step [174/192], loss=13.6346
	step [175/192], loss=13.6883
	step [176/192], loss=14.4265
	step [177/192], loss=14.9933
	step [178/192], loss=14.0391
	step [179/192], loss=16.0927
	step [180/192], loss=13.5019
	step [181/192], loss=13.0225
	step [182/192], loss=13.4804
	step [183/192], loss=13.9534
	step [184/192], loss=13.8525
	step [185/192], loss=14.6773
	step [186/192], loss=15.7435
	step [187/192], loss=12.1780
	step [188/192], loss=14.6545
	step [189/192], loss=14.9390
	step [190/192], loss=15.7563
	step [191/192], loss=15.5638
	step [192/192], loss=2.1983
	Evaluating
	loss=0.0403, precision=0.1631, recall=0.9941, f1=0.2803
Training epoch 14
	step [1/192], loss=14.2530
	step [2/192], loss=16.8298
	step [3/192], loss=11.2880
	step [4/192], loss=14.7970
	step [5/192], loss=17.2711
	step [6/192], loss=11.9251
	step [7/192], loss=14.8023
	step [8/192], loss=14.6142
	step [9/192], loss=14.1496
	step [10/192], loss=13.2097
	step [11/192], loss=12.9237
	step [12/192], loss=11.8647
	step [13/192], loss=14.2994
	step [14/192], loss=14.9217
	step [15/192], loss=13.8488
	step [16/192], loss=12.4933
	step [17/192], loss=12.7568
	step [18/192], loss=12.0536
	step [19/192], loss=13.6816
	step [20/192], loss=13.5699
	step [21/192], loss=15.3834
	step [22/192], loss=12.7848
	step [23/192], loss=14.6459
	step [24/192], loss=13.3206
	step [25/192], loss=13.7100
	step [26/192], loss=13.7437
	step [27/192], loss=13.2156
	step [28/192], loss=15.9483
	step [29/192], loss=12.3642
	step [30/192], loss=14.3828
	step [31/192], loss=13.6690
	step [32/192], loss=13.9230
	step [33/192], loss=12.2921
	step [34/192], loss=16.8831
	step [35/192], loss=12.4549
	step [36/192], loss=13.8719
	step [37/192], loss=12.7655
	step [38/192], loss=12.8866
	step [39/192], loss=12.6599
	step [40/192], loss=11.9316
	step [41/192], loss=15.5434
	step [42/192], loss=13.9001
	step [43/192], loss=12.3073
	step [44/192], loss=13.6476
	step [45/192], loss=13.1905
	step [46/192], loss=12.5618
	step [47/192], loss=14.1184
	step [48/192], loss=12.9575
	step [49/192], loss=12.4473
	step [50/192], loss=13.6327
	step [51/192], loss=12.7422
	step [52/192], loss=12.0410
	step [53/192], loss=14.5618
	step [54/192], loss=14.6276
	step [55/192], loss=14.3785
	step [56/192], loss=14.7526
	step [57/192], loss=13.5086
	step [58/192], loss=13.9517
	step [59/192], loss=12.4611
	step [60/192], loss=16.1065
	step [61/192], loss=16.3992
	step [62/192], loss=12.8483
	step [63/192], loss=14.9947
	step [64/192], loss=12.9934
	step [65/192], loss=12.2963
	step [66/192], loss=13.9107
	step [67/192], loss=12.8141
	step [68/192], loss=14.2690
	step [69/192], loss=15.1918
	step [70/192], loss=13.1002
	step [71/192], loss=12.6634
	step [72/192], loss=13.8403
	step [73/192], loss=13.4008
	step [74/192], loss=14.1712
	step [75/192], loss=13.5521
	step [76/192], loss=11.0543
	step [77/192], loss=14.8242
	step [78/192], loss=14.6657
	step [79/192], loss=12.7257
	step [80/192], loss=13.6957
	step [81/192], loss=12.6782
	step [82/192], loss=14.8705
	step [83/192], loss=13.0999
	step [84/192], loss=13.2102
	step [85/192], loss=12.8724
	step [86/192], loss=13.9674
	step [87/192], loss=13.0331
	step [88/192], loss=13.5092
	step [89/192], loss=12.3311
	step [90/192], loss=13.3372
	step [91/192], loss=12.1851
	step [92/192], loss=13.1302
	step [93/192], loss=13.4362
	step [94/192], loss=13.8976
	step [95/192], loss=11.9237
	step [96/192], loss=13.5052
	step [97/192], loss=14.6290
	step [98/192], loss=12.6860
	step [99/192], loss=12.1474
	step [100/192], loss=14.0627
	step [101/192], loss=12.0342
	step [102/192], loss=12.4661
	step [103/192], loss=14.5014
	step [104/192], loss=17.0738
	step [105/192], loss=15.4824
	step [106/192], loss=13.6447
	step [107/192], loss=12.8689
	step [108/192], loss=12.9847
	step [109/192], loss=13.9303
	step [110/192], loss=13.6548
	step [111/192], loss=15.4007
	step [112/192], loss=12.8152
	step [113/192], loss=15.6318
	step [114/192], loss=15.4795
	step [115/192], loss=12.9244
	step [116/192], loss=14.6177
	step [117/192], loss=14.4309
	step [118/192], loss=13.1866
	step [119/192], loss=15.1593
	step [120/192], loss=12.8793
	step [121/192], loss=13.0805
	step [122/192], loss=14.1592
	step [123/192], loss=13.7773
	step [124/192], loss=14.6890
	step [125/192], loss=13.4186
	step [126/192], loss=14.3256
	step [127/192], loss=11.9339
	step [128/192], loss=13.5839
	step [129/192], loss=11.4745
	step [130/192], loss=13.2745
	step [131/192], loss=13.7399
	step [132/192], loss=12.2059
	step [133/192], loss=15.7413
	step [134/192], loss=11.7705
	step [135/192], loss=14.4733
	step [136/192], loss=10.8686
	step [137/192], loss=11.6927
	step [138/192], loss=13.8524
	step [139/192], loss=15.3761
	step [140/192], loss=13.0568
	step [141/192], loss=14.1866
	step [142/192], loss=13.2314
	step [143/192], loss=12.3490
	step [144/192], loss=14.2541
	step [145/192], loss=11.4181
	step [146/192], loss=13.5170
	step [147/192], loss=14.3430
	step [148/192], loss=13.0868
	step [149/192], loss=15.4402
	step [150/192], loss=13.7045
	step [151/192], loss=12.3558
	step [152/192], loss=13.3846
	step [153/192], loss=14.2298
	step [154/192], loss=12.8639
	step [155/192], loss=12.3342
	step [156/192], loss=14.0195
	step [157/192], loss=14.6719
	step [158/192], loss=15.6865
	step [159/192], loss=12.8906
	step [160/192], loss=13.8447
	step [161/192], loss=12.8424
	step [162/192], loss=13.2874
	step [163/192], loss=13.2940
	step [164/192], loss=13.4546
	step [165/192], loss=12.6435
	step [166/192], loss=15.6286
	step [167/192], loss=14.2979
	step [168/192], loss=12.4946
	step [169/192], loss=13.4917
	step [170/192], loss=11.9209
	step [171/192], loss=15.6931
	step [172/192], loss=13.4208
	step [173/192], loss=13.0811
	step [174/192], loss=13.4192
	step [175/192], loss=13.2353
	step [176/192], loss=11.0779
	step [177/192], loss=11.9951
	step [178/192], loss=13.1402
	step [179/192], loss=14.1209
	step [180/192], loss=14.2258
	step [181/192], loss=14.2833
	step [182/192], loss=12.2157
	step [183/192], loss=13.1327
	step [184/192], loss=12.2692
	step [185/192], loss=14.0796
	step [186/192], loss=15.8527
	step [187/192], loss=12.3539
	step [188/192], loss=14.3142
	step [189/192], loss=12.2572
	step [190/192], loss=12.2507
	step [191/192], loss=12.1792
	step [192/192], loss=2.7806
	Evaluating
	loss=0.0356, precision=0.1901, recall=0.9935, f1=0.3192
saving model as: 0_saved_model.pth
Training epoch 15
	step [1/192], loss=12.5153
	step [2/192], loss=13.0063
	step [3/192], loss=12.1662
	step [4/192], loss=13.7479
	step [5/192], loss=14.3282
	step [6/192], loss=13.1791
	step [7/192], loss=12.6400
	step [8/192], loss=12.1129
	step [9/192], loss=12.1928
	step [10/192], loss=14.3460
	step [11/192], loss=13.7222
	step [12/192], loss=13.9439
	step [13/192], loss=15.6719
	step [14/192], loss=14.2922
	step [15/192], loss=11.8931
	step [16/192], loss=13.0889
	step [17/192], loss=13.2298
	step [18/192], loss=12.8036
	step [19/192], loss=11.9764
	step [20/192], loss=11.4556
	step [21/192], loss=15.2420
	step [22/192], loss=11.6978
	step [23/192], loss=12.2514
	step [24/192], loss=15.0758
	step [25/192], loss=11.8801
	step [26/192], loss=12.7124
	step [27/192], loss=14.4818
	step [28/192], loss=13.4999
	step [29/192], loss=12.3005
	step [30/192], loss=14.6735
	step [31/192], loss=12.2857
	step [32/192], loss=12.3266
	step [33/192], loss=13.1716
	step [34/192], loss=13.8319
	step [35/192], loss=11.4399
	step [36/192], loss=12.2505
	step [37/192], loss=18.5248
	step [38/192], loss=10.9511
	step [39/192], loss=12.1898
	step [40/192], loss=11.8305
	step [41/192], loss=13.9443
	step [42/192], loss=13.6035
	step [43/192], loss=13.4869
	step [44/192], loss=12.6552
	step [45/192], loss=13.1974
	step [46/192], loss=13.7965
	step [47/192], loss=12.8671
	step [48/192], loss=12.4206
	step [49/192], loss=10.8101
	step [50/192], loss=11.4615
	step [51/192], loss=13.3074
	step [52/192], loss=12.7359
	step [53/192], loss=10.9321
	step [54/192], loss=12.0379
	step [55/192], loss=12.0576
	step [56/192], loss=12.6868
	step [57/192], loss=14.4642
	step [58/192], loss=11.8241
	step [59/192], loss=11.6940
	step [60/192], loss=11.7311
	step [61/192], loss=11.8967
	step [62/192], loss=11.5435
	step [63/192], loss=10.3649
	step [64/192], loss=11.1847
	step [65/192], loss=13.9191
	step [66/192], loss=11.4539
	step [67/192], loss=12.7849
	step [68/192], loss=13.0009
	step [69/192], loss=13.2093
	step [70/192], loss=11.6194
	step [71/192], loss=14.0487
	step [72/192], loss=11.7681
	step [73/192], loss=9.9519
	step [74/192], loss=11.9184
	step [75/192], loss=13.4129
	step [76/192], loss=14.5857
	step [77/192], loss=12.7526
	step [78/192], loss=14.0991
	step [79/192], loss=12.6191
	step [80/192], loss=13.2363
	step [81/192], loss=13.5460
	step [82/192], loss=12.7835
	step [83/192], loss=12.8634
	step [84/192], loss=11.2100
	step [85/192], loss=12.5503
	step [86/192], loss=11.6439
	step [87/192], loss=15.1218
	step [88/192], loss=13.1275
	step [89/192], loss=13.8746
	step [90/192], loss=12.8620
	step [91/192], loss=13.7412
	step [92/192], loss=12.4650
	step [93/192], loss=11.5723
	step [94/192], loss=14.2510
	step [95/192], loss=12.9874
	step [96/192], loss=12.2304
	step [97/192], loss=12.3957
	step [98/192], loss=12.3743
	step [99/192], loss=13.2806
	step [100/192], loss=11.7999
	step [101/192], loss=11.1744
	step [102/192], loss=13.0025
	step [103/192], loss=16.2742
	step [104/192], loss=14.1800
	step [105/192], loss=12.7488
	step [106/192], loss=12.3183
	step [107/192], loss=14.2206
	step [108/192], loss=13.4250
	step [109/192], loss=11.7602
	step [110/192], loss=12.2588
	step [111/192], loss=13.3978
	step [112/192], loss=12.3397
	step [113/192], loss=11.6863
	step [114/192], loss=12.1006
	step [115/192], loss=14.7565
	step [116/192], loss=14.4103
	step [117/192], loss=12.3806
	step [118/192], loss=13.6766
	step [119/192], loss=13.3258
	step [120/192], loss=12.2755
	step [121/192], loss=13.2147
	step [122/192], loss=12.1700
	step [123/192], loss=13.8928
	step [124/192], loss=16.4650
	step [125/192], loss=12.6061
	step [126/192], loss=14.4331
	step [127/192], loss=11.3834
	step [128/192], loss=11.6688
	step [129/192], loss=12.9515
	step [130/192], loss=11.5961
	step [131/192], loss=11.6986
	step [132/192], loss=12.6553
	step [133/192], loss=13.2981
	step [134/192], loss=13.9117
	step [135/192], loss=14.0732
	step [136/192], loss=12.6100
	step [137/192], loss=13.9306
	step [138/192], loss=10.5186
	step [139/192], loss=11.8100
	step [140/192], loss=12.5705
	step [141/192], loss=13.5253
	step [142/192], loss=11.0116
	step [143/192], loss=11.4492
	step [144/192], loss=12.4077
	step [145/192], loss=13.2994
	step [146/192], loss=13.2370
	step [147/192], loss=11.5347
	step [148/192], loss=10.6671
	step [149/192], loss=11.6994
	step [150/192], loss=13.2059
	step [151/192], loss=13.8428
	step [152/192], loss=12.2645
	step [153/192], loss=12.2928
	step [154/192], loss=11.7643
	step [155/192], loss=11.7803
	step [156/192], loss=10.8191
	step [157/192], loss=13.7921
	step [158/192], loss=13.4608
	step [159/192], loss=10.3031
	step [160/192], loss=15.3993
	step [161/192], loss=12.2403
	step [162/192], loss=12.9801
	step [163/192], loss=13.4051
	step [164/192], loss=15.7286
	step [165/192], loss=12.1381
	step [166/192], loss=13.1293
	step [167/192], loss=12.0849
	step [168/192], loss=11.1424
	step [169/192], loss=14.6153
	step [170/192], loss=12.3264
	step [171/192], loss=12.3854
	step [172/192], loss=13.1758
	step [173/192], loss=12.7152
	step [174/192], loss=12.0883
	step [175/192], loss=15.0034
	step [176/192], loss=13.1025
	step [177/192], loss=12.7230
	step [178/192], loss=12.5750
	step [179/192], loss=11.3781
	step [180/192], loss=12.0396
	step [181/192], loss=16.4000
	step [182/192], loss=11.8861
	step [183/192], loss=14.5548
	step [184/192], loss=10.6864
	step [185/192], loss=11.8696
	step [186/192], loss=13.8705
	step [187/192], loss=11.8644
	step [188/192], loss=13.1683
	step [189/192], loss=12.5608
	step [190/192], loss=13.5527
	step [191/192], loss=10.2839
	step [192/192], loss=1.4350
	Evaluating
	loss=0.0345, precision=0.1812, recall=0.9940, f1=0.3065
Training epoch 16
	step [1/192], loss=11.8490
	step [2/192], loss=12.7874
	step [3/192], loss=12.1134
	step [4/192], loss=12.5140
	step [5/192], loss=14.1636
	step [6/192], loss=12.3410
	step [7/192], loss=10.8165
	step [8/192], loss=11.6717
	step [9/192], loss=11.4253
	step [10/192], loss=12.4126
	step [11/192], loss=11.1666
	step [12/192], loss=10.2095
	step [13/192], loss=11.1303
	step [14/192], loss=13.1836
	step [15/192], loss=11.3307
	step [16/192], loss=12.4715
	step [17/192], loss=11.9756
	step [18/192], loss=11.6518
	step [19/192], loss=11.5389
	step [20/192], loss=12.3864
	step [21/192], loss=12.0843
	step [22/192], loss=12.0591
	step [23/192], loss=11.6306
	step [24/192], loss=13.1044
	step [25/192], loss=10.7220
	step [26/192], loss=12.8931
	step [27/192], loss=11.4990
	step [28/192], loss=11.2863
	step [29/192], loss=14.3643
	step [30/192], loss=14.9325
	step [31/192], loss=10.6636
	step [32/192], loss=14.3253
	step [33/192], loss=14.8818
	step [34/192], loss=12.3325
	step [35/192], loss=13.0148
	step [36/192], loss=14.1865
	step [37/192], loss=14.0876
	step [38/192], loss=12.9231
	step [39/192], loss=11.2690
	step [40/192], loss=16.1712
	step [41/192], loss=12.6547
	step [42/192], loss=11.6408
	step [43/192], loss=10.5360
	step [44/192], loss=14.0476
	step [45/192], loss=10.7316
	step [46/192], loss=11.7103
	step [47/192], loss=11.8688
	step [48/192], loss=13.7803
	step [49/192], loss=13.0812
	step [50/192], loss=12.8243
	step [51/192], loss=11.3000
	step [52/192], loss=12.2582
	step [53/192], loss=13.5871
	step [54/192], loss=13.6929
	step [55/192], loss=13.6142
	step [56/192], loss=12.0947
	step [57/192], loss=9.0088
	step [58/192], loss=10.5305
	step [59/192], loss=9.9776
	step [60/192], loss=10.1394
	step [61/192], loss=12.8262
	step [62/192], loss=13.8786
	step [63/192], loss=11.2407
	step [64/192], loss=10.5147
	step [65/192], loss=13.8158
	step [66/192], loss=13.3196
	step [67/192], loss=12.4044
	step [68/192], loss=13.0995
	step [69/192], loss=11.5415
	step [70/192], loss=11.4139
	step [71/192], loss=11.7144
	step [72/192], loss=12.4507
	step [73/192], loss=10.3482
	step [74/192], loss=9.9910
	step [75/192], loss=13.3867
	step [76/192], loss=13.6436
	step [77/192], loss=13.3617
	step [78/192], loss=13.3584
	step [79/192], loss=14.0543
	step [80/192], loss=11.0243
	step [81/192], loss=12.3398
	step [82/192], loss=11.7252
	step [83/192], loss=12.0923
	step [84/192], loss=12.5165
	step [85/192], loss=13.8871
	step [86/192], loss=14.3355
	step [87/192], loss=12.3614
	step [88/192], loss=12.5465
	step [89/192], loss=12.2994
	step [90/192], loss=11.2881
	step [91/192], loss=10.6433
	step [92/192], loss=12.6236
	step [93/192], loss=12.1975
	step [94/192], loss=10.1396
	step [95/192], loss=12.4464
	step [96/192], loss=10.5847
	step [97/192], loss=11.3168
	step [98/192], loss=13.0705
	step [99/192], loss=12.3165
	step [100/192], loss=12.3807
	step [101/192], loss=13.3369
	step [102/192], loss=13.1420
	step [103/192], loss=10.6435
	step [104/192], loss=14.5099
	step [105/192], loss=13.4798
	step [106/192], loss=15.3988
	step [107/192], loss=15.3537
	step [108/192], loss=12.3893
	step [109/192], loss=11.1848
	step [110/192], loss=11.6947
	step [111/192], loss=13.0517
	step [112/192], loss=13.2514
	step [113/192], loss=11.8222
	step [114/192], loss=12.1096
	step [115/192], loss=12.4039
	step [116/192], loss=13.1689
	step [117/192], loss=11.4699
	step [118/192], loss=9.5695
	step [119/192], loss=11.4496
	step [120/192], loss=11.8539
	step [121/192], loss=12.1747
	step [122/192], loss=11.5074
	step [123/192], loss=11.0746
	step [124/192], loss=12.5408
	step [125/192], loss=11.5898
	step [126/192], loss=10.8359
	step [127/192], loss=12.6370
	step [128/192], loss=12.2374
	step [129/192], loss=13.5704
	step [130/192], loss=12.7072
	step [131/192], loss=10.2950
	step [132/192], loss=12.3193
	step [133/192], loss=10.4935
	step [134/192], loss=11.2533
	step [135/192], loss=11.1788
	step [136/192], loss=13.5371
	step [137/192], loss=11.2309
	step [138/192], loss=11.5202
	step [139/192], loss=12.3069
	step [140/192], loss=12.1426
	step [141/192], loss=11.3507
	step [142/192], loss=12.2061
	step [143/192], loss=11.4721
	step [144/192], loss=12.2860
	step [145/192], loss=12.3897
	step [146/192], loss=11.5397
	step [147/192], loss=10.1290
	step [148/192], loss=13.0267
	step [149/192], loss=12.2588
	step [150/192], loss=11.9045
	step [151/192], loss=11.3229
	step [152/192], loss=10.6670
	step [153/192], loss=13.0912
	step [154/192], loss=9.3033
	step [155/192], loss=11.7420
	step [156/192], loss=12.7632
	step [157/192], loss=12.5557
	step [158/192], loss=12.6297
	step [159/192], loss=15.4354
	step [160/192], loss=12.8032
	step [161/192], loss=12.4214
	step [162/192], loss=13.5727
	step [163/192], loss=12.5532
	step [164/192], loss=13.6334
	step [165/192], loss=10.5743
	step [166/192], loss=10.8989
	step [167/192], loss=10.5523
	step [168/192], loss=11.5268
	step [169/192], loss=14.3618
	step [170/192], loss=13.1726
	step [171/192], loss=10.7080
	step [172/192], loss=10.2931
	step [173/192], loss=10.4333
	step [174/192], loss=13.5929
	step [175/192], loss=11.3067
	step [176/192], loss=11.2046
	step [177/192], loss=12.8587
	step [178/192], loss=9.9551
	step [179/192], loss=12.3975
	step [180/192], loss=11.7211
	step [181/192], loss=10.2043
	step [182/192], loss=12.5300
	step [183/192], loss=13.2553
	step [184/192], loss=10.0417
	step [185/192], loss=12.4190
	step [186/192], loss=13.3702
	step [187/192], loss=13.9520
	step [188/192], loss=11.1895
	step [189/192], loss=11.0370
	step [190/192], loss=10.8920
	step [191/192], loss=10.7823
	step [192/192], loss=1.5375
	Evaluating
	loss=0.0320, precision=0.1776, recall=0.9937, f1=0.3013
Training epoch 17
	step [1/192], loss=10.4992
	step [2/192], loss=12.2624
	step [3/192], loss=13.1125
	step [4/192], loss=10.5633
	step [5/192], loss=11.6654
	step [6/192], loss=11.8224
	step [7/192], loss=10.0610
	step [8/192], loss=11.9019
	step [9/192], loss=12.1905
	step [10/192], loss=11.9237
	step [11/192], loss=9.3476
	step [12/192], loss=10.4623
	step [13/192], loss=10.5690
	step [14/192], loss=12.4296
	step [15/192], loss=9.2913
	step [16/192], loss=13.5184
	step [17/192], loss=9.8459
	step [18/192], loss=9.2784
	step [19/192], loss=12.6868
	step [20/192], loss=12.0227
	step [21/192], loss=10.3365
	step [22/192], loss=9.5355
	step [23/192], loss=10.7139
	step [24/192], loss=13.7438
	step [25/192], loss=10.9236
	step [26/192], loss=10.8105
	step [27/192], loss=10.5396
	step [28/192], loss=14.2286
	step [29/192], loss=12.1043
	step [30/192], loss=11.1459
	step [31/192], loss=12.1183
	step [32/192], loss=11.8108
	step [33/192], loss=13.5662
	step [34/192], loss=10.1512
	step [35/192], loss=10.5936
	step [36/192], loss=10.7398
	step [37/192], loss=13.1899
	step [38/192], loss=11.2905
	step [39/192], loss=10.0665
	step [40/192], loss=11.1712
	step [41/192], loss=11.4209
	step [42/192], loss=10.7989
	step [43/192], loss=11.6867
	step [44/192], loss=11.6891
	step [45/192], loss=12.5293
	step [46/192], loss=10.9781
	step [47/192], loss=11.1936
	step [48/192], loss=10.8192
	step [49/192], loss=11.3508
	step [50/192], loss=11.4739
	step [51/192], loss=10.6445
	step [52/192], loss=12.5841
	step [53/192], loss=13.1894
	step [54/192], loss=13.9449
	step [55/192], loss=9.9512
	step [56/192], loss=11.0510
	step [57/192], loss=12.2915
	step [58/192], loss=14.0155
	step [59/192], loss=12.6857
	step [60/192], loss=12.4180
	step [61/192], loss=11.9505
	step [62/192], loss=11.4233
	step [63/192], loss=11.2472
	step [64/192], loss=12.0484
	step [65/192], loss=12.2791
	step [66/192], loss=9.0343
	step [67/192], loss=10.4483
	step [68/192], loss=12.1278
	step [69/192], loss=12.4289
	step [70/192], loss=10.5368
	step [71/192], loss=12.1070
	step [72/192], loss=11.3140
	step [73/192], loss=11.8549
	step [74/192], loss=10.6419
	step [75/192], loss=11.0798
	step [76/192], loss=13.3363
	step [77/192], loss=9.3856
	step [78/192], loss=11.8333
	step [79/192], loss=12.0580
	step [80/192], loss=10.8599
	step [81/192], loss=9.7595
	step [82/192], loss=11.8698
	step [83/192], loss=10.8002
	step [84/192], loss=11.9085
	step [85/192], loss=12.4579
	step [86/192], loss=12.4230
	step [87/192], loss=10.7347
	step [88/192], loss=11.9907
	step [89/192], loss=10.0808
	step [90/192], loss=11.6864
	step [91/192], loss=11.4593
	step [92/192], loss=12.2527
	step [93/192], loss=12.0619
	step [94/192], loss=10.7884
	step [95/192], loss=10.0965
	step [96/192], loss=10.2806
	step [97/192], loss=11.5635
	step [98/192], loss=12.7399
	step [99/192], loss=13.4868
	step [100/192], loss=13.9351
	step [101/192], loss=11.3457
	step [102/192], loss=12.8290
	step [103/192], loss=12.3666
	step [104/192], loss=12.9445
	step [105/192], loss=10.9022
	step [106/192], loss=10.4265
	step [107/192], loss=12.2672
	step [108/192], loss=11.5827
	step [109/192], loss=12.1726
	step [110/192], loss=13.7248
	step [111/192], loss=13.6158
	step [112/192], loss=11.4358
	step [113/192], loss=10.5796
	step [114/192], loss=12.3990
	step [115/192], loss=14.5619
	step [116/192], loss=11.4599
	step [117/192], loss=11.0207
	step [118/192], loss=9.4640
	step [119/192], loss=11.2523
	step [120/192], loss=11.1520
	step [121/192], loss=9.5354
	step [122/192], loss=10.3461
	step [123/192], loss=11.4079
	step [124/192], loss=11.7319
	step [125/192], loss=11.0690
	step [126/192], loss=14.0119
	step [127/192], loss=11.1287
	step [128/192], loss=13.8605
	step [129/192], loss=12.6185
	step [130/192], loss=10.7891
	step [131/192], loss=10.7558
	step [132/192], loss=11.8411
	step [133/192], loss=14.4739
	step [134/192], loss=12.5777
	step [135/192], loss=11.5603
	step [136/192], loss=11.9822
	step [137/192], loss=11.0088
	step [138/192], loss=11.4488
	step [139/192], loss=12.4848
	step [140/192], loss=12.2407
	step [141/192], loss=9.6216
	step [142/192], loss=11.9169
	step [143/192], loss=11.2259
	step [144/192], loss=10.0404
	step [145/192], loss=12.5012
	step [146/192], loss=10.8306
	step [147/192], loss=10.7499
	step [148/192], loss=9.3618
	step [149/192], loss=9.6577
	step [150/192], loss=10.6982
	step [151/192], loss=11.2693
	step [152/192], loss=10.7661
	step [153/192], loss=12.5681
	step [154/192], loss=10.9481
	step [155/192], loss=10.8506
	step [156/192], loss=8.7726
	step [157/192], loss=12.2782
	step [158/192], loss=11.6877
	step [159/192], loss=12.0750
	step [160/192], loss=11.9663
	step [161/192], loss=10.5900
	step [162/192], loss=11.7896
	step [163/192], loss=10.7059
	step [164/192], loss=10.5611
	step [165/192], loss=12.1935
	step [166/192], loss=12.3755
	step [167/192], loss=10.2673
	step [168/192], loss=12.0726
	step [169/192], loss=12.0377
	step [170/192], loss=10.7325
	step [171/192], loss=12.2803
	step [172/192], loss=13.2370
	step [173/192], loss=12.4919
	step [174/192], loss=13.3083
	step [175/192], loss=13.2457
	step [176/192], loss=10.7462
	step [177/192], loss=11.3940
	step [178/192], loss=11.2607
	step [179/192], loss=14.5703
	step [180/192], loss=10.6763
	step [181/192], loss=10.8264
	step [182/192], loss=11.8067
	step [183/192], loss=9.4535
	step [184/192], loss=14.1190
	step [185/192], loss=11.2398
	step [186/192], loss=11.1418
	step [187/192], loss=10.1729
	step [188/192], loss=11.0423
	step [189/192], loss=9.8362
	step [190/192], loss=10.9353
	step [191/192], loss=9.7628
	step [192/192], loss=1.5305
	Evaluating
	loss=0.0305, precision=0.1852, recall=0.9932, f1=0.3122
Training epoch 18
	step [1/192], loss=10.5835
	step [2/192], loss=10.7164
	step [3/192], loss=12.1098
	step [4/192], loss=12.5122
	step [5/192], loss=10.2356
	step [6/192], loss=10.2950
	step [7/192], loss=8.5375
	step [8/192], loss=12.2612
	step [9/192], loss=11.4532
	step [10/192], loss=11.5543
	step [11/192], loss=11.0131
	step [12/192], loss=11.7068
	step [13/192], loss=10.7658
	step [14/192], loss=10.5220
	step [15/192], loss=9.8429
	step [16/192], loss=11.1041
	step [17/192], loss=10.8373
	step [18/192], loss=11.3389
	step [19/192], loss=10.9250
	step [20/192], loss=9.9754
	step [21/192], loss=8.4917
	step [22/192], loss=9.7656
	step [23/192], loss=9.2509
	step [24/192], loss=12.2311
	step [25/192], loss=10.5295
	step [26/192], loss=11.0193
	step [27/192], loss=12.8527
	step [28/192], loss=10.1043
	step [29/192], loss=10.1302
	step [30/192], loss=10.7559
	step [31/192], loss=9.7848
	step [32/192], loss=10.5594
	step [33/192], loss=10.1770
	step [34/192], loss=9.4967
	step [35/192], loss=9.0281
	step [36/192], loss=10.4600
	step [37/192], loss=13.0774
	step [38/192], loss=11.5395
	step [39/192], loss=9.5389
	step [40/192], loss=11.0812
	step [41/192], loss=11.4097
	step [42/192], loss=11.9366
	step [43/192], loss=12.5594
	step [44/192], loss=11.0022
	step [45/192], loss=10.1543
	step [46/192], loss=10.8028
	step [47/192], loss=14.4889
	step [48/192], loss=9.8433
	step [49/192], loss=10.9596
	step [50/192], loss=11.0881
	step [51/192], loss=12.0685
	step [52/192], loss=12.1147
	step [53/192], loss=11.2621
	step [54/192], loss=10.7276
	step [55/192], loss=9.4378
	step [56/192], loss=10.8887
	step [57/192], loss=10.8413
	step [58/192], loss=12.9371
	step [59/192], loss=13.9281
	step [60/192], loss=11.9909
	step [61/192], loss=10.2434
	step [62/192], loss=10.2422
	step [63/192], loss=11.7055
	step [64/192], loss=9.4054
	step [65/192], loss=9.9339
	step [66/192], loss=9.4166
	step [67/192], loss=9.7249
	step [68/192], loss=10.7709
	step [69/192], loss=9.5982
	step [70/192], loss=11.7691
	step [71/192], loss=10.2599
	step [72/192], loss=11.9112
	step [73/192], loss=12.6132
	step [74/192], loss=10.7498
	step [75/192], loss=11.4915
	step [76/192], loss=10.8961
	step [77/192], loss=11.1170
	step [78/192], loss=11.4301
	step [79/192], loss=11.0650
	step [80/192], loss=11.1541
	step [81/192], loss=9.5390
	step [82/192], loss=11.1263
	step [83/192], loss=10.5193
	step [84/192], loss=10.7962
	step [85/192], loss=10.7179
	step [86/192], loss=11.6315
	step [87/192], loss=10.7956
	step [88/192], loss=9.3812
	step [89/192], loss=11.5652
	step [90/192], loss=9.2336
	step [91/192], loss=10.9151
	step [92/192], loss=9.3117
	step [93/192], loss=11.4008
	step [94/192], loss=10.4932
	step [95/192], loss=11.5075
	step [96/192], loss=10.7814
	step [97/192], loss=10.4686
	step [98/192], loss=11.4657
	step [99/192], loss=11.0890
	step [100/192], loss=10.8731
	step [101/192], loss=10.5695
	step [102/192], loss=11.3107
	step [103/192], loss=12.5890
	step [104/192], loss=10.9892
	step [105/192], loss=10.4577
	step [106/192], loss=12.0296
	step [107/192], loss=11.4515
	step [108/192], loss=12.0901
	step [109/192], loss=11.9973
	step [110/192], loss=10.0332
	step [111/192], loss=10.8538
	step [112/192], loss=9.3122
	step [113/192], loss=10.5614
	step [114/192], loss=10.2268
	step [115/192], loss=8.2697
	step [116/192], loss=10.3468
	step [117/192], loss=11.1700
	step [118/192], loss=9.4036
	step [119/192], loss=13.2223
	step [120/192], loss=10.1510
	step [121/192], loss=9.9950
	step [122/192], loss=14.3253
	step [123/192], loss=10.2795
	step [124/192], loss=9.5576
	step [125/192], loss=12.9005
	step [126/192], loss=11.1888
	step [127/192], loss=10.2499
	step [128/192], loss=9.1462
	step [129/192], loss=13.1734
	step [130/192], loss=10.3420
	step [131/192], loss=8.8180
	step [132/192], loss=14.8406
	step [133/192], loss=12.8577
	step [134/192], loss=9.8216
	step [135/192], loss=11.2080
	step [136/192], loss=9.8689
	step [137/192], loss=9.7581
	step [138/192], loss=9.9023
	step [139/192], loss=10.8265
	step [140/192], loss=10.6865
	step [141/192], loss=10.2144
	step [142/192], loss=10.7790
	step [143/192], loss=12.2558
	step [144/192], loss=10.6464
	step [145/192], loss=10.8842
	step [146/192], loss=10.1509
	step [147/192], loss=11.7960
	step [148/192], loss=11.6296
	step [149/192], loss=11.1847
	step [150/192], loss=10.1051
	step [151/192], loss=11.5317
	step [152/192], loss=8.6711
	step [153/192], loss=11.4049
	step [154/192], loss=9.3213
	step [155/192], loss=14.7616
	step [156/192], loss=10.8148
	step [157/192], loss=11.3847
	step [158/192], loss=11.0415
	step [159/192], loss=10.4278
	step [160/192], loss=9.4245
	step [161/192], loss=8.7866
	step [162/192], loss=8.7794
	step [163/192], loss=11.8029
	step [164/192], loss=12.3604
	step [165/192], loss=13.8182
	step [166/192], loss=11.5039
	step [167/192], loss=9.6269
	step [168/192], loss=10.5886
	step [169/192], loss=11.2012
	step [170/192], loss=10.9135
	step [171/192], loss=11.4923
	step [172/192], loss=10.9681
	step [173/192], loss=12.5686
	step [174/192], loss=11.7517
	step [175/192], loss=11.4399
	step [176/192], loss=10.9447
	step [177/192], loss=11.0806
	step [178/192], loss=11.2436
	step [179/192], loss=10.7289
	step [180/192], loss=9.9672
	step [181/192], loss=10.1650
	step [182/192], loss=11.1096
	step [183/192], loss=12.3252
	step [184/192], loss=9.2633
	step [185/192], loss=10.4853
	step [186/192], loss=11.0748
	step [187/192], loss=9.8604
	step [188/192], loss=10.9653
	step [189/192], loss=9.5722
	step [190/192], loss=10.9511
	step [191/192], loss=10.6940
	step [192/192], loss=1.0937
	Evaluating
	loss=0.0303, precision=0.1738, recall=0.9938, f1=0.2959
Training epoch 19
	step [1/192], loss=10.4195
	step [2/192], loss=9.2902
	step [3/192], loss=11.7592
	step [4/192], loss=11.4644
	step [5/192], loss=9.5024
	step [6/192], loss=9.0769
	step [7/192], loss=10.0625
	step [8/192], loss=10.3559
	step [9/192], loss=10.0709
	step [10/192], loss=11.2127
	step [11/192], loss=9.7989
	step [12/192], loss=13.3851
	step [13/192], loss=9.1071
	step [14/192], loss=9.9562
	step [15/192], loss=10.6565
	step [16/192], loss=10.1452
	step [17/192], loss=10.7292
	step [18/192], loss=10.3602
	step [19/192], loss=7.6458
	step [20/192], loss=9.8615
	step [21/192], loss=11.6356
	step [22/192], loss=10.8760
	step [23/192], loss=11.8476
	step [24/192], loss=11.0004
	step [25/192], loss=10.8760
	step [26/192], loss=10.8384
	step [27/192], loss=9.9296
	step [28/192], loss=9.0032
	step [29/192], loss=10.1115
	step [30/192], loss=11.0877
	step [31/192], loss=11.0375
	step [32/192], loss=8.0811
	step [33/192], loss=8.6753
	step [34/192], loss=10.9328
	step [35/192], loss=11.5722
	step [36/192], loss=10.6034
	step [37/192], loss=9.6741
	step [38/192], loss=11.0532
	step [39/192], loss=11.0539
	step [40/192], loss=11.6272
	step [41/192], loss=9.7427
	step [42/192], loss=10.4862
	step [43/192], loss=10.8526
	step [44/192], loss=9.6512
	step [45/192], loss=12.0669
	step [46/192], loss=9.6883
	step [47/192], loss=10.8030
	step [48/192], loss=10.6723
	step [49/192], loss=10.6899
	step [50/192], loss=10.7456
	step [51/192], loss=8.6457
	step [52/192], loss=13.5461
	step [53/192], loss=10.5948
	step [54/192], loss=9.1567
	step [55/192], loss=10.0121
	step [56/192], loss=12.2600
	step [57/192], loss=10.8191
	step [58/192], loss=12.1908
	step [59/192], loss=12.3115
	step [60/192], loss=11.9278
	step [61/192], loss=10.5666
	step [62/192], loss=10.9226
	step [63/192], loss=10.6148
	step [64/192], loss=9.5253
	step [65/192], loss=9.8561
	step [66/192], loss=10.4282
	step [67/192], loss=11.8790
	step [68/192], loss=9.2193
	step [69/192], loss=11.2367
	step [70/192], loss=9.3193
	step [71/192], loss=11.1939
	step [72/192], loss=10.1646
	step [73/192], loss=9.4498
	step [74/192], loss=12.9235
	step [75/192], loss=10.2437
	step [76/192], loss=10.8428
	step [77/192], loss=12.7450
	step [78/192], loss=11.8033
	step [79/192], loss=11.5941
	step [80/192], loss=9.9715
	step [81/192], loss=10.7114
	step [82/192], loss=9.5942
	step [83/192], loss=11.7533
	step [84/192], loss=11.5528
	step [85/192], loss=9.1509
	step [86/192], loss=11.9966
	step [87/192], loss=10.4130
	step [88/192], loss=10.6153
	step [89/192], loss=9.8230
	step [90/192], loss=10.9341
	step [91/192], loss=10.7320
	step [92/192], loss=8.4315
	step [93/192], loss=9.4161
	step [94/192], loss=9.1874
	step [95/192], loss=9.8052
	step [96/192], loss=10.8238
	step [97/192], loss=8.8674
	step [98/192], loss=13.1054
	step [99/192], loss=11.6977
	step [100/192], loss=13.9895
	step [101/192], loss=9.4406
	step [102/192], loss=11.7540
	step [103/192], loss=10.9625
	step [104/192], loss=8.9845
	step [105/192], loss=10.2597
	step [106/192], loss=8.9585
	step [107/192], loss=9.8846
	step [108/192], loss=9.2974
	step [109/192], loss=9.2127
	step [110/192], loss=11.6234
	step [111/192], loss=8.9688
	step [112/192], loss=11.4594
	step [113/192], loss=15.0384
	step [114/192], loss=10.2325
	step [115/192], loss=12.2760
	step [116/192], loss=10.5522
	step [117/192], loss=11.4452
	step [118/192], loss=11.3584
	step [119/192], loss=10.9077
	step [120/192], loss=11.1973
	step [121/192], loss=9.0361
	step [122/192], loss=11.3437
	step [123/192], loss=12.4112
	step [124/192], loss=9.3218
	step [125/192], loss=11.2406
	step [126/192], loss=9.5533
	step [127/192], loss=10.0353
	step [128/192], loss=9.8411
	step [129/192], loss=11.9291
	step [130/192], loss=9.1279
	step [131/192], loss=10.1046
	step [132/192], loss=10.1914
	step [133/192], loss=9.4664
	step [134/192], loss=11.4846
	step [135/192], loss=7.5249
	step [136/192], loss=10.2953
	step [137/192], loss=11.6116
	step [138/192], loss=11.2948
	step [139/192], loss=9.2692
	step [140/192], loss=12.1030
	step [141/192], loss=10.7028
	step [142/192], loss=10.3771
	step [143/192], loss=10.9526
	step [144/192], loss=9.7680
	step [145/192], loss=9.4034
	step [146/192], loss=12.0125
	step [147/192], loss=9.9856
	step [148/192], loss=10.4091
	step [149/192], loss=8.2246
	step [150/192], loss=10.2213
	step [151/192], loss=11.7162
	step [152/192], loss=10.4666
	step [153/192], loss=9.1593
	step [154/192], loss=10.2268
	step [155/192], loss=9.3818
	step [156/192], loss=11.4962
	step [157/192], loss=10.3146
	step [158/192], loss=9.5054
	step [159/192], loss=12.6174
	step [160/192], loss=8.5782
	step [161/192], loss=11.0005
	step [162/192], loss=9.9467
	step [163/192], loss=11.6900
	step [164/192], loss=9.9892
	step [165/192], loss=12.1415
	step [166/192], loss=9.8829
	step [167/192], loss=12.4513
	step [168/192], loss=11.5483
	step [169/192], loss=10.2965
	step [170/192], loss=8.1124
	step [171/192], loss=10.5957
	step [172/192], loss=9.7566
	step [173/192], loss=11.1436
	step [174/192], loss=9.0676
	step [175/192], loss=8.8794
	step [176/192], loss=13.6934
	step [177/192], loss=11.3036
	step [178/192], loss=11.1026
	step [179/192], loss=10.3338
	step [180/192], loss=10.7145
	step [181/192], loss=9.9671
	step [182/192], loss=10.0383
	step [183/192], loss=9.8589
	step [184/192], loss=10.2896
	step [185/192], loss=9.7067
	step [186/192], loss=11.3990
	step [187/192], loss=11.5212
	step [188/192], loss=8.3652
	step [189/192], loss=9.7443
	step [190/192], loss=10.0496
	step [191/192], loss=10.9502
	step [192/192], loss=1.7573
	Evaluating
	loss=0.0305, precision=0.1672, recall=0.9944, f1=0.2862
Training epoch 20
	step [1/192], loss=11.9777
	step [2/192], loss=9.2655
	step [3/192], loss=9.6479
	step [4/192], loss=10.1582
	step [5/192], loss=10.7307
	step [6/192], loss=9.0680
	step [7/192], loss=10.1337
	step [8/192], loss=11.1629
	step [9/192], loss=9.9892
	step [10/192], loss=9.4477
	step [11/192], loss=10.3883
	step [12/192], loss=10.5669
	step [13/192], loss=9.5565
	step [14/192], loss=9.0151
	step [15/192], loss=9.8387
	step [16/192], loss=8.8258
	step [17/192], loss=8.8943
	step [18/192], loss=9.5004
	step [19/192], loss=9.9569
	step [20/192], loss=10.4399
	step [21/192], loss=10.8603
	step [22/192], loss=8.6336
	step [23/192], loss=8.9016
	step [24/192], loss=8.8232
	step [25/192], loss=9.5821
	step [26/192], loss=12.0925
	step [27/192], loss=10.1614
	step [28/192], loss=10.7766
	step [29/192], loss=11.3117
	step [30/192], loss=10.1088
	step [31/192], loss=11.4291
	step [32/192], loss=11.7593
	step [33/192], loss=10.8520
	step [34/192], loss=9.0991
	step [35/192], loss=10.8716
	step [36/192], loss=9.7835
	step [37/192], loss=9.8832
	step [38/192], loss=9.4640
	step [39/192], loss=10.6003
	step [40/192], loss=8.0232
	step [41/192], loss=11.5217
	step [42/192], loss=10.1290
	step [43/192], loss=9.7960
	step [44/192], loss=9.0978
	step [45/192], loss=11.0088
	step [46/192], loss=11.6131
	step [47/192], loss=8.9304
	step [48/192], loss=11.7022
	step [49/192], loss=12.1542
	step [50/192], loss=13.1437
	step [51/192], loss=11.2003
	step [52/192], loss=9.4530
	step [53/192], loss=9.3707
	step [54/192], loss=10.3199
	step [55/192], loss=11.3176
	step [56/192], loss=11.6954
	step [57/192], loss=10.3797
	step [58/192], loss=10.4329
	step [59/192], loss=13.0184
	step [60/192], loss=11.6875
	step [61/192], loss=10.9391
	step [62/192], loss=9.1972
	step [63/192], loss=10.2749
	step [64/192], loss=10.2028
	step [65/192], loss=9.2894
	step [66/192], loss=10.8938
	step [67/192], loss=11.9144
	step [68/192], loss=12.0903
	step [69/192], loss=10.5851
	step [70/192], loss=9.3283
	step [71/192], loss=11.3324
	step [72/192], loss=10.7994
	step [73/192], loss=10.7017
	step [74/192], loss=9.6489
	step [75/192], loss=9.1950
	step [76/192], loss=9.5616
	step [77/192], loss=8.8925
	step [78/192], loss=9.5516
	step [79/192], loss=11.2484
	step [80/192], loss=11.4547
	step [81/192], loss=11.5667
	step [82/192], loss=9.5923
	step [83/192], loss=10.7764
	step [84/192], loss=9.3609
	step [85/192], loss=8.9310
	step [86/192], loss=9.7788
	step [87/192], loss=9.6859
	step [88/192], loss=11.4490
	step [89/192], loss=9.6730
	step [90/192], loss=9.0286
	step [91/192], loss=11.2630
	step [92/192], loss=10.2696
	step [93/192], loss=10.6932
	step [94/192], loss=8.8547
	step [95/192], loss=10.6413
	step [96/192], loss=9.4283
	step [97/192], loss=8.5224
	step [98/192], loss=10.8324
	step [99/192], loss=9.5981
	step [100/192], loss=10.7633
	step [101/192], loss=10.8140
	step [102/192], loss=8.8684
	step [103/192], loss=9.8431
	step [104/192], loss=9.5678
	step [105/192], loss=10.9260
	step [106/192], loss=10.9810
	step [107/192], loss=9.5985
	step [108/192], loss=11.0435
	step [109/192], loss=10.0116
	step [110/192], loss=10.9764
	step [111/192], loss=9.2681
	step [112/192], loss=10.5896
	step [113/192], loss=9.8239
	step [114/192], loss=8.7780
	step [115/192], loss=9.6815
	step [116/192], loss=8.9364
	step [117/192], loss=8.2744
	step [118/192], loss=9.3124
	step [119/192], loss=11.4679
	step [120/192], loss=10.4550
	step [121/192], loss=11.9290
	step [122/192], loss=10.7885
	step [123/192], loss=9.8981
	step [124/192], loss=10.4174
	step [125/192], loss=9.4894
	step [126/192], loss=9.3575
	step [127/192], loss=9.0779
	step [128/192], loss=11.4035
	step [129/192], loss=10.9962
	step [130/192], loss=8.4828
	step [131/192], loss=12.0885
	step [132/192], loss=8.1753
	step [133/192], loss=9.3826
	step [134/192], loss=9.8514
	step [135/192], loss=10.9279
	step [136/192], loss=9.6299
	step [137/192], loss=11.0261
	step [138/192], loss=10.7870
	step [139/192], loss=11.2627
	step [140/192], loss=10.3032
	step [141/192], loss=10.4835
	step [142/192], loss=10.2873
	step [143/192], loss=9.0712
	step [144/192], loss=10.6005
	step [145/192], loss=10.1573
	step [146/192], loss=11.0610
	step [147/192], loss=9.3386
	step [148/192], loss=9.2919
	step [149/192], loss=9.2560
	step [150/192], loss=9.0288
	step [151/192], loss=10.1289
	step [152/192], loss=10.3698
	step [153/192], loss=7.6312
	step [154/192], loss=9.6751
	step [155/192], loss=12.4627
	step [156/192], loss=9.3135
	step [157/192], loss=10.6157
	step [158/192], loss=9.1138
	step [159/192], loss=11.2353
	step [160/192], loss=10.3216
	step [161/192], loss=9.2028
	step [162/192], loss=9.1180
	step [163/192], loss=7.7311
	step [164/192], loss=10.6354
	step [165/192], loss=10.7163
	step [166/192], loss=11.5156
	step [167/192], loss=10.2417
	step [168/192], loss=10.7391
	step [169/192], loss=8.7558
	step [170/192], loss=9.0842
	step [171/192], loss=9.9284
	step [172/192], loss=8.7121
	step [173/192], loss=10.5543
	step [174/192], loss=10.1858
	step [175/192], loss=9.8588
	step [176/192], loss=9.6965
	step [177/192], loss=9.0333
	step [178/192], loss=9.6095
	step [179/192], loss=12.3447
	step [180/192], loss=10.3763
	step [181/192], loss=10.9434
	step [182/192], loss=12.0130
	step [183/192], loss=11.2644
	step [184/192], loss=12.6473
	step [185/192], loss=10.3141
	step [186/192], loss=12.3031
	step [187/192], loss=9.2162
	step [188/192], loss=8.8339
	step [189/192], loss=9.9733
	step [190/192], loss=12.6229
	step [191/192], loss=8.2601
	step [192/192], loss=1.5539
	Evaluating
	loss=0.0270, precision=0.1903, recall=0.9936, f1=0.3194
saving model as: 0_saved_model.pth
Training epoch 21
	step [1/192], loss=8.7431
	step [2/192], loss=11.0157
	step [3/192], loss=9.9481
	step [4/192], loss=8.8879
	step [5/192], loss=11.0887
	step [6/192], loss=9.9065
	step [7/192], loss=9.5445
	step [8/192], loss=9.2810
	step [9/192], loss=8.4192
	step [10/192], loss=11.2426
	step [11/192], loss=10.0386
	step [12/192], loss=10.8598
	step [13/192], loss=7.3602
	step [14/192], loss=8.9835
	step [15/192], loss=7.7754
	step [16/192], loss=11.4649
	step [17/192], loss=8.9515
	step [18/192], loss=9.3873
	step [19/192], loss=10.4047
	step [20/192], loss=10.0897
	step [21/192], loss=7.7223
	step [22/192], loss=8.9570
	step [23/192], loss=12.0695
	step [24/192], loss=12.0169
	step [25/192], loss=11.2341
	step [26/192], loss=9.8836
	step [27/192], loss=8.4347
	step [28/192], loss=9.3909
	step [29/192], loss=9.3701
	step [30/192], loss=9.9873
	step [31/192], loss=8.9660
	step [32/192], loss=10.7661
	step [33/192], loss=12.8927
	step [34/192], loss=8.2852
	step [35/192], loss=9.7537
	step [36/192], loss=8.9361
	step [37/192], loss=9.6680
	step [38/192], loss=10.8884
	step [39/192], loss=11.1831
	step [40/192], loss=8.7942
	step [41/192], loss=9.4372
	step [42/192], loss=9.6874
	step [43/192], loss=8.5484
	step [44/192], loss=12.4114
	step [45/192], loss=11.1957
	step [46/192], loss=11.1039
	step [47/192], loss=10.8677
	step [48/192], loss=10.5840
	step [49/192], loss=11.6413
	step [50/192], loss=9.5056
	step [51/192], loss=10.7406
	step [52/192], loss=8.9258
	step [53/192], loss=9.2820
	step [54/192], loss=10.5991
	step [55/192], loss=10.1811
	step [56/192], loss=9.6944
	step [57/192], loss=8.6914
	step [58/192], loss=8.5809
	step [59/192], loss=11.3424
	step [60/192], loss=13.5464
	step [61/192], loss=8.7372
	step [62/192], loss=9.4918
	step [63/192], loss=8.6497
	step [64/192], loss=9.8140
	step [65/192], loss=8.4016
	step [66/192], loss=9.7034
	step [67/192], loss=10.4300
	step [68/192], loss=10.3074
	step [69/192], loss=10.8990
	step [70/192], loss=8.8191
	step [71/192], loss=10.2060
	step [72/192], loss=9.0523
	step [73/192], loss=9.9063
	step [74/192], loss=9.4915
	step [75/192], loss=9.3025
	step [76/192], loss=9.1825
	step [77/192], loss=9.4522
	step [78/192], loss=10.0384
	step [79/192], loss=9.5085
	step [80/192], loss=11.1199
	step [81/192], loss=9.4709
	step [82/192], loss=8.2420
	step [83/192], loss=9.7586
	step [84/192], loss=10.2593
	step [85/192], loss=9.9190
	step [86/192], loss=7.5143
	step [87/192], loss=9.6827
	step [88/192], loss=9.5143
	step [89/192], loss=7.8899
	step [90/192], loss=9.7641
	step [91/192], loss=10.0307
	step [92/192], loss=9.1163
	step [93/192], loss=8.4254
	step [94/192], loss=9.3214
	step [95/192], loss=9.0529
	step [96/192], loss=10.5312
	step [97/192], loss=8.9412
	step [98/192], loss=9.4184
	step [99/192], loss=10.2942
	step [100/192], loss=11.1977
	step [101/192], loss=10.2667
	step [102/192], loss=7.8688
	step [103/192], loss=8.7901
	step [104/192], loss=10.1558
	step [105/192], loss=10.3630
	step [106/192], loss=10.6023
	step [107/192], loss=9.2902
	step [108/192], loss=8.8166
	step [109/192], loss=8.0205
	step [110/192], loss=10.8952
	step [111/192], loss=9.0916
	step [112/192], loss=10.1496
	step [113/192], loss=8.2548
	step [114/192], loss=10.3659
	step [115/192], loss=10.5567
	step [116/192], loss=10.4585
	step [117/192], loss=12.2148
	step [118/192], loss=9.7572
	step [119/192], loss=12.9233
	step [120/192], loss=10.4443
	step [121/192], loss=8.7150
	step [122/192], loss=9.8111
	step [123/192], loss=10.2126
	step [124/192], loss=7.7823
	step [125/192], loss=10.0069
	step [126/192], loss=11.2096
	step [127/192], loss=12.5668
	step [128/192], loss=10.0189
	step [129/192], loss=8.3371
	step [130/192], loss=9.2730
	step [131/192], loss=8.9333
	step [132/192], loss=9.6337
	step [133/192], loss=9.4678
	step [134/192], loss=11.6470
	step [135/192], loss=9.1420
	step [136/192], loss=8.0061
	step [137/192], loss=8.3685
	step [138/192], loss=9.1903
	step [139/192], loss=9.9541
	step [140/192], loss=9.1841
	step [141/192], loss=10.4834
	step [142/192], loss=10.3719
	step [143/192], loss=10.9220
	step [144/192], loss=9.3581
	step [145/192], loss=9.4023
	step [146/192], loss=7.5887
	step [147/192], loss=8.7349
	step [148/192], loss=7.2567
	step [149/192], loss=7.5547
	step [150/192], loss=9.7979
	step [151/192], loss=11.4587
	step [152/192], loss=10.7335
	step [153/192], loss=9.3088
	step [154/192], loss=7.9244
	step [155/192], loss=10.8920
	step [156/192], loss=10.9377
	step [157/192], loss=9.3735
	step [158/192], loss=8.7256
	step [159/192], loss=8.8130
	step [160/192], loss=10.3050
	step [161/192], loss=10.6640
	step [162/192], loss=11.0494
	step [163/192], loss=9.6846
	step [164/192], loss=9.2880
	step [165/192], loss=9.4163
	step [166/192], loss=8.9323
	step [167/192], loss=8.2905
	step [168/192], loss=9.5779
	step [169/192], loss=10.4552
	step [170/192], loss=11.9133
	step [171/192], loss=10.2126
	step [172/192], loss=9.2277
	step [173/192], loss=8.5380
	step [174/192], loss=7.6289
	step [175/192], loss=10.2308
	step [176/192], loss=7.8900
	step [177/192], loss=9.7483
	step [178/192], loss=9.6627
	step [179/192], loss=7.8841
	step [180/192], loss=8.9330
	step [181/192], loss=8.2346
	step [182/192], loss=12.2287
	step [183/192], loss=9.6992
	step [184/192], loss=9.4776
	step [185/192], loss=8.2810
	step [186/192], loss=8.8963
	step [187/192], loss=9.9906
	step [188/192], loss=10.1439
	step [189/192], loss=9.8460
	step [190/192], loss=8.6524
	step [191/192], loss=10.5328
	step [192/192], loss=2.2668
	Evaluating
	loss=0.0258, precision=0.1999, recall=0.9931, f1=0.3328
saving model as: 0_saved_model.pth
Training epoch 22
	step [1/192], loss=8.5567
	step [2/192], loss=7.7299
	step [3/192], loss=8.3423
	step [4/192], loss=9.3613
	step [5/192], loss=9.1086
	step [6/192], loss=11.7973
	step [7/192], loss=8.5295
	step [8/192], loss=10.9016
	step [9/192], loss=9.8896
	step [10/192], loss=9.9368
	step [11/192], loss=10.7494
	step [12/192], loss=9.1482
	step [13/192], loss=9.9263
	step [14/192], loss=9.9562
	step [15/192], loss=8.1464
	step [16/192], loss=9.4141
	step [17/192], loss=10.2472
	step [18/192], loss=9.0723
	step [19/192], loss=10.2265
	step [20/192], loss=10.8486
	step [21/192], loss=9.1080
	step [22/192], loss=8.7382
	step [23/192], loss=8.3141
	step [24/192], loss=10.3280
	step [25/192], loss=8.2551
	step [26/192], loss=8.1340
	step [27/192], loss=9.5581
	step [28/192], loss=7.5771
	step [29/192], loss=8.3354
	step [30/192], loss=9.6517
	step [31/192], loss=9.5944
	step [32/192], loss=7.7363
	step [33/192], loss=8.7987
	step [34/192], loss=8.8954
	step [35/192], loss=8.7753
	step [36/192], loss=9.1923
	step [37/192], loss=10.0083
	step [38/192], loss=9.4953
	step [39/192], loss=9.4107
	step [40/192], loss=11.0151
	step [41/192], loss=9.0660
	step [42/192], loss=8.7008
	step [43/192], loss=11.2729
	step [44/192], loss=9.3878
	step [45/192], loss=7.2312
	step [46/192], loss=9.0613
	step [47/192], loss=8.2156
	step [48/192], loss=9.0013
	step [49/192], loss=8.7245
	step [50/192], loss=10.1713
	step [51/192], loss=9.5907
	step [52/192], loss=9.8147
	step [53/192], loss=10.4613
	step [54/192], loss=9.4468
	step [55/192], loss=9.6424
	step [56/192], loss=8.6289
	step [57/192], loss=9.5790
	step [58/192], loss=10.6636
	step [59/192], loss=9.8145
	step [60/192], loss=10.1064
	step [61/192], loss=10.0431
	step [62/192], loss=9.4512
	step [63/192], loss=9.3402
	step [64/192], loss=8.8793
	step [65/192], loss=10.0837
	step [66/192], loss=10.1772
	step [67/192], loss=9.3604
	step [68/192], loss=10.2284
	step [69/192], loss=9.9965
	step [70/192], loss=10.4505
	step [71/192], loss=9.0772
	step [72/192], loss=10.0913
	step [73/192], loss=8.9160
	step [74/192], loss=8.6828
	step [75/192], loss=10.1934
	step [76/192], loss=8.4642
	step [77/192], loss=8.1596
	step [78/192], loss=12.4632
	step [79/192], loss=9.5495
	step [80/192], loss=10.0665
	step [81/192], loss=8.9847
	step [82/192], loss=8.3858
	step [83/192], loss=10.4057
	step [84/192], loss=8.5852
	step [85/192], loss=10.4615
	step [86/192], loss=9.6278
	step [87/192], loss=8.9955
	step [88/192], loss=7.6832
	step [89/192], loss=8.9759
	step [90/192], loss=9.3216
	step [91/192], loss=7.9106
	step [92/192], loss=7.8723
	step [93/192], loss=8.9872
	step [94/192], loss=11.2482
	step [95/192], loss=8.5976
	step [96/192], loss=8.3030
	step [97/192], loss=10.1399
	step [98/192], loss=9.8450
	step [99/192], loss=9.6330
	step [100/192], loss=7.8522
	step [101/192], loss=9.4063
	step [102/192], loss=10.9896
	step [103/192], loss=9.0608
	step [104/192], loss=11.7617
	step [105/192], loss=10.0711
	step [106/192], loss=8.5366
	step [107/192], loss=9.1007
	step [108/192], loss=11.6237
	step [109/192], loss=9.3061
	step [110/192], loss=10.3121
	step [111/192], loss=8.7412
	step [112/192], loss=9.4307
	step [113/192], loss=10.1607
	step [114/192], loss=10.0149
	step [115/192], loss=9.2977
	step [116/192], loss=7.9471
	step [117/192], loss=10.0351
	step [118/192], loss=8.7293
	step [119/192], loss=10.2102
	step [120/192], loss=8.7652
	step [121/192], loss=8.9089
	step [122/192], loss=10.0906
	step [123/192], loss=8.6120
	step [124/192], loss=10.2857
	step [125/192], loss=8.0699
	step [126/192], loss=10.2339
	step [127/192], loss=10.6081
	step [128/192], loss=9.2327
	step [129/192], loss=9.9399
	step [130/192], loss=9.7553
	step [131/192], loss=11.4215
	step [132/192], loss=9.7959
	step [133/192], loss=8.9360
	step [134/192], loss=7.9922
	step [135/192], loss=10.2063
	step [136/192], loss=8.3223
	step [137/192], loss=10.7883
	step [138/192], loss=9.5111
	step [139/192], loss=8.4401
	step [140/192], loss=8.0220
	step [141/192], loss=8.3017
	step [142/192], loss=10.6544
	step [143/192], loss=10.2550
	step [144/192], loss=8.2986
	step [145/192], loss=7.8925
	step [146/192], loss=7.9356
	step [147/192], loss=9.3407
	step [148/192], loss=7.6648
	step [149/192], loss=8.7803
	step [150/192], loss=10.1731
	step [151/192], loss=10.1212
	step [152/192], loss=9.8800
	step [153/192], loss=9.5030
	step [154/192], loss=8.9061
	step [155/192], loss=9.2676
	step [156/192], loss=10.6140
	step [157/192], loss=10.0413
	step [158/192], loss=8.4838
	step [159/192], loss=7.9165
	step [160/192], loss=10.1753
	step [161/192], loss=8.3227
	step [162/192], loss=10.9858
	step [163/192], loss=9.7388
	step [164/192], loss=8.9006
	step [165/192], loss=8.6821
	step [166/192], loss=9.4932
	step [167/192], loss=10.1501
	step [168/192], loss=8.6845
	step [169/192], loss=7.6294
	step [170/192], loss=8.3259
	step [171/192], loss=8.5496
	step [172/192], loss=9.9396
	step [173/192], loss=10.4480
	step [174/192], loss=9.5056
	step [175/192], loss=9.9854
	step [176/192], loss=9.1498
	step [177/192], loss=9.3808
	step [178/192], loss=9.9727
	step [179/192], loss=11.6898
	step [180/192], loss=7.2365
	step [181/192], loss=10.9049
	step [182/192], loss=7.3940
	step [183/192], loss=9.2634
	step [184/192], loss=8.2450
	step [185/192], loss=9.0579
	step [186/192], loss=9.5430
	step [187/192], loss=9.3585
	step [188/192], loss=8.4140
	step [189/192], loss=8.6870
	step [190/192], loss=7.9420
	step [191/192], loss=9.1952
	step [192/192], loss=0.9515
	Evaluating
	loss=0.0265, precision=0.1800, recall=0.9933, f1=0.3048
Training epoch 23
	step [1/192], loss=9.2798
	step [2/192], loss=9.1636
	step [3/192], loss=7.3507
	step [4/192], loss=8.0870
	step [5/192], loss=7.6950
	step [6/192], loss=8.3294
	step [7/192], loss=6.7021
	step [8/192], loss=9.0218
	step [9/192], loss=9.6366
	step [10/192], loss=8.5268
	step [11/192], loss=9.3076
	step [12/192], loss=8.2412
	step [13/192], loss=9.0137
	step [14/192], loss=9.7207
	step [15/192], loss=8.3822
	step [16/192], loss=8.6917
	step [17/192], loss=9.6129
	step [18/192], loss=9.4006
	step [19/192], loss=10.0289
	step [20/192], loss=8.6322
	step [21/192], loss=8.2898
	step [22/192], loss=8.8373
	step [23/192], loss=9.7950
	step [24/192], loss=7.1940
	step [25/192], loss=10.2196
	step [26/192], loss=7.9293
	step [27/192], loss=8.6763
	step [28/192], loss=8.5662
	step [29/192], loss=7.3324
	step [30/192], loss=8.9674
	step [31/192], loss=9.8930
	step [32/192], loss=8.7569
	step [33/192], loss=7.9860
	step [34/192], loss=9.7745
	step [35/192], loss=10.5018
	step [36/192], loss=10.6163
	step [37/192], loss=9.2772
	step [38/192], loss=11.2198
	step [39/192], loss=8.7423
	step [40/192], loss=9.5289
	step [41/192], loss=7.0773
	step [42/192], loss=11.3018
	step [43/192], loss=10.2184
	step [44/192], loss=10.7418
	step [45/192], loss=8.4198
	step [46/192], loss=10.4924
	step [47/192], loss=10.2867
	step [48/192], loss=9.4706
	step [49/192], loss=7.4814
	step [50/192], loss=9.7537
	step [51/192], loss=8.0360
	step [52/192], loss=8.3741
	step [53/192], loss=9.8502
	step [54/192], loss=7.7750
	step [55/192], loss=7.9478
	step [56/192], loss=9.8976
	step [57/192], loss=7.9481
	step [58/192], loss=10.1585
	step [59/192], loss=9.6434
	step [60/192], loss=10.5739
	step [61/192], loss=9.3479
	step [62/192], loss=10.1926
	step [63/192], loss=8.5048
	step [64/192], loss=10.6495
	step [65/192], loss=8.1269
	step [66/192], loss=8.9900
	step [67/192], loss=8.2905
	step [68/192], loss=8.3844
	step [69/192], loss=8.9373
	step [70/192], loss=8.0570
	step [71/192], loss=7.8681
	step [72/192], loss=9.6819
	step [73/192], loss=8.9538
	step [74/192], loss=7.9843
	step [75/192], loss=8.3915
	step [76/192], loss=7.8972
	step [77/192], loss=8.9472
	step [78/192], loss=9.0377
	step [79/192], loss=8.6636
	step [80/192], loss=9.2341
	step [81/192], loss=8.9941
	step [82/192], loss=9.8730
	step [83/192], loss=10.1736
	step [84/192], loss=8.2925
	step [85/192], loss=7.8840
	step [86/192], loss=9.9490
	step [87/192], loss=11.9848
	step [88/192], loss=10.8292
	step [89/192], loss=8.8295
	step [90/192], loss=8.9931
	step [91/192], loss=13.2884
	step [92/192], loss=8.6387
	step [93/192], loss=9.8718
	step [94/192], loss=9.4549
	step [95/192], loss=10.6375
	step [96/192], loss=8.0479
	step [97/192], loss=9.2486
	step [98/192], loss=10.2069
	step [99/192], loss=8.4445
	step [100/192], loss=9.4476
	step [101/192], loss=10.3288
	step [102/192], loss=8.7572
	step [103/192], loss=8.4194
	step [104/192], loss=6.8613
	step [105/192], loss=9.3298
	step [106/192], loss=9.6600
	step [107/192], loss=8.3496
	step [108/192], loss=7.2300
	step [109/192], loss=9.1965
	step [110/192], loss=8.0603
	step [111/192], loss=9.0975
	step [112/192], loss=8.1724
	step [113/192], loss=10.1147
	step [114/192], loss=9.0426
	step [115/192], loss=11.0005
	step [116/192], loss=8.6816
	step [117/192], loss=7.9654
	step [118/192], loss=11.3675
	step [119/192], loss=9.4204
	step [120/192], loss=10.4124
	step [121/192], loss=8.7408
	step [122/192], loss=10.7845
	step [123/192], loss=8.4379
	step [124/192], loss=7.8706
	step [125/192], loss=10.0168
	step [126/192], loss=7.2843
	step [127/192], loss=8.4514
	step [128/192], loss=9.4392
	step [129/192], loss=9.2968
	step [130/192], loss=9.4326
	step [131/192], loss=7.7914
	step [132/192], loss=8.6008
	step [133/192], loss=7.1853
	step [134/192], loss=8.5421
	step [135/192], loss=10.1617
	step [136/192], loss=8.4268
	step [137/192], loss=9.3287
	step [138/192], loss=7.4631
	step [139/192], loss=9.5867
	step [140/192], loss=7.1950
	step [141/192], loss=8.2602
	step [142/192], loss=9.7831
	step [143/192], loss=7.7136
	step [144/192], loss=10.8015
	step [145/192], loss=8.6965
	step [146/192], loss=8.0026
	step [147/192], loss=11.8440
	step [148/192], loss=8.3577
	step [149/192], loss=9.4412
	step [150/192], loss=9.6674
	step [151/192], loss=8.8259
	step [152/192], loss=7.7637
	step [153/192], loss=7.1600
	step [154/192], loss=8.1497
	step [155/192], loss=8.9293
	step [156/192], loss=9.1117
	step [157/192], loss=9.0313
	step [158/192], loss=8.0478
	step [159/192], loss=7.7583
	step [160/192], loss=8.9378
	step [161/192], loss=10.2602
	step [162/192], loss=8.0431
	step [163/192], loss=9.9719
	step [164/192], loss=8.6496
	step [165/192], loss=11.5005
	step [166/192], loss=8.8316
	step [167/192], loss=7.7550
	step [168/192], loss=10.4167
	step [169/192], loss=8.2109
	step [170/192], loss=9.9394
	step [171/192], loss=8.7507
	step [172/192], loss=8.0139
	step [173/192], loss=7.7724
	step [174/192], loss=8.5345
	step [175/192], loss=9.4718
	step [176/192], loss=8.7570
	step [177/192], loss=10.3881
	step [178/192], loss=10.2056
	step [179/192], loss=8.6687
	step [180/192], loss=8.1356
	step [181/192], loss=10.6632
	step [182/192], loss=9.1056
	step [183/192], loss=9.4064
	step [184/192], loss=9.5246
	step [185/192], loss=10.6920
	step [186/192], loss=8.6772
	step [187/192], loss=9.7139
	step [188/192], loss=10.5891
	step [189/192], loss=9.3939
	step [190/192], loss=7.7106
	step [191/192], loss=7.6658
	step [192/192], loss=1.1579
	Evaluating
	loss=0.0266, precision=0.1892, recall=0.9932, f1=0.3178
Training epoch 24
	step [1/192], loss=10.1558
	step [2/192], loss=9.6166
	step [3/192], loss=8.9545
	step [4/192], loss=9.5216
	step [5/192], loss=9.0605
	step [6/192], loss=9.3915
	step [7/192], loss=8.2320
	step [8/192], loss=10.2314
	step [9/192], loss=8.5816
	step [10/192], loss=8.4454
	step [11/192], loss=9.4437
	step [12/192], loss=8.7293
	step [13/192], loss=8.9210
	step [14/192], loss=8.6414
	step [15/192], loss=8.0729
	step [16/192], loss=7.8102
	step [17/192], loss=9.6028
	step [18/192], loss=8.5731
	step [19/192], loss=9.2825
	step [20/192], loss=9.2685
	step [21/192], loss=8.5613
	step [22/192], loss=8.6183
	step [23/192], loss=9.1776
	step [24/192], loss=9.5509
	step [25/192], loss=8.5754
	step [26/192], loss=8.5319
	step [27/192], loss=8.9426
	step [28/192], loss=9.1643
	step [29/192], loss=8.3164
	step [30/192], loss=9.6560
	step [31/192], loss=8.5369
	step [32/192], loss=8.7392
	step [33/192], loss=7.8762
	step [34/192], loss=7.7063
	step [35/192], loss=7.7119
	step [36/192], loss=8.9903
	step [37/192], loss=10.4714
	step [38/192], loss=8.3646
	step [39/192], loss=7.7245
	step [40/192], loss=7.9242
	step [41/192], loss=9.4794
	step [42/192], loss=9.5571
	step [43/192], loss=9.1341
	step [44/192], loss=7.4969
	step [45/192], loss=7.9608
	step [46/192], loss=9.8259
	step [47/192], loss=8.9292
	step [48/192], loss=8.2190
	step [49/192], loss=9.4507
	step [50/192], loss=10.7923
	step [51/192], loss=9.0772
	step [52/192], loss=5.9938
	step [53/192], loss=9.3090
	step [54/192], loss=10.4747
	step [55/192], loss=9.0510
	step [56/192], loss=9.0068
	step [57/192], loss=9.2927
	step [58/192], loss=8.0317
	step [59/192], loss=8.2208
	step [60/192], loss=9.3415
	step [61/192], loss=10.2879
	step [62/192], loss=7.3548
	step [63/192], loss=8.2775
	step [64/192], loss=7.9390
	step [65/192], loss=9.8637
	step [66/192], loss=7.0217
	step [67/192], loss=9.2105
	step [68/192], loss=8.1704
	step [69/192], loss=7.6795
	step [70/192], loss=8.5963
	step [71/192], loss=7.3201
	step [72/192], loss=8.4086
	step [73/192], loss=9.6139
	step [74/192], loss=8.0608
	step [75/192], loss=7.9930
	step [76/192], loss=10.7785
	step [77/192], loss=7.0896
	step [78/192], loss=8.8170
	step [79/192], loss=10.6283
	step [80/192], loss=9.0566
	step [81/192], loss=9.1917
	step [82/192], loss=8.7104
	step [83/192], loss=10.1999
	step [84/192], loss=8.5724
	step [85/192], loss=9.4849
	step [86/192], loss=7.0104
	step [87/192], loss=8.0695
	step [88/192], loss=9.1693
	step [89/192], loss=9.4537
	step [90/192], loss=8.3300
	step [91/192], loss=9.0865
	step [92/192], loss=8.6046
	step [93/192], loss=7.9312
	step [94/192], loss=7.9560
	step [95/192], loss=8.9990
	step [96/192], loss=8.4875
	step [97/192], loss=10.0410
	step [98/192], loss=9.2560
	step [99/192], loss=9.8844
	step [100/192], loss=9.8709
	step [101/192], loss=8.5762
	step [102/192], loss=8.9590
	step [103/192], loss=8.0275
	step [104/192], loss=9.5638
	step [105/192], loss=9.2794
	step [106/192], loss=8.5826
	step [107/192], loss=7.9776
	step [108/192], loss=7.6918
	step [109/192], loss=10.0628
	step [110/192], loss=9.2309
	step [111/192], loss=10.5080
	step [112/192], loss=8.9521
	step [113/192], loss=8.3558
	step [114/192], loss=8.7869
	step [115/192], loss=11.2683
	step [116/192], loss=7.3064
	step [117/192], loss=9.9732
	step [118/192], loss=8.9979
	step [119/192], loss=7.3473
	step [120/192], loss=9.3846
	step [121/192], loss=8.5034
	step [122/192], loss=8.1117
	step [123/192], loss=6.9455
	step [124/192], loss=8.0001
	step [125/192], loss=8.8105
	step [126/192], loss=9.1985
	step [127/192], loss=9.3919
	step [128/192], loss=7.3706
	step [129/192], loss=8.7874
	step [130/192], loss=10.9053
	step [131/192], loss=8.6604
	step [132/192], loss=9.8714
	step [133/192], loss=7.3423
	step [134/192], loss=8.2536
	step [135/192], loss=8.1532
	step [136/192], loss=8.5369
	step [137/192], loss=9.0735
	step [138/192], loss=7.8323
	step [139/192], loss=8.5833
	step [140/192], loss=9.3160
	step [141/192], loss=9.1803
	step [142/192], loss=9.4866
	step [143/192], loss=8.3018
	step [144/192], loss=10.8144
	step [145/192], loss=7.9471
	step [146/192], loss=9.9516
	step [147/192], loss=7.1583
	step [148/192], loss=8.3328
	step [149/192], loss=8.4769
	step [150/192], loss=6.3410
	step [151/192], loss=7.6471
	step [152/192], loss=8.3903
	step [153/192], loss=7.5854
	step [154/192], loss=8.1582
	step [155/192], loss=8.5519
	step [156/192], loss=7.7455
	step [157/192], loss=9.3815
	step [158/192], loss=8.8872
	step [159/192], loss=8.4337
	step [160/192], loss=7.4320
	step [161/192], loss=7.1655
	step [162/192], loss=10.5157
	step [163/192], loss=6.5258
	step [164/192], loss=9.9003
	step [165/192], loss=8.7303
	step [166/192], loss=10.7693
	step [167/192], loss=9.8993
	step [168/192], loss=8.7218
	step [169/192], loss=8.6047
	step [170/192], loss=9.2442
	step [171/192], loss=10.0201
	step [172/192], loss=8.8874
	step [173/192], loss=8.0974
	step [174/192], loss=8.5962
	step [175/192], loss=8.2336
	step [176/192], loss=7.7997
	step [177/192], loss=8.8502
	step [178/192], loss=10.4748
	step [179/192], loss=8.2421
	step [180/192], loss=9.0469
	step [181/192], loss=8.7363
	step [182/192], loss=9.4117
	step [183/192], loss=11.3726
	step [184/192], loss=9.8170
	step [185/192], loss=8.0543
	step [186/192], loss=10.1709
	step [187/192], loss=8.9942
	step [188/192], loss=9.3075
	step [189/192], loss=7.7402
	step [190/192], loss=8.6485
	step [191/192], loss=9.7014
	step [192/192], loss=0.7513
	Evaluating
	loss=0.0254, precision=0.1938, recall=0.9935, f1=0.3243
Training epoch 25
	step [1/192], loss=8.1950
	step [2/192], loss=8.1532
	step [3/192], loss=10.7415
	step [4/192], loss=7.6002
	step [5/192], loss=8.4505
	step [6/192], loss=8.1477
	step [7/192], loss=7.0986
	step [8/192], loss=9.3176
	step [9/192], loss=9.5928
	step [10/192], loss=8.1998
	step [11/192], loss=9.1931
	step [12/192], loss=10.3239
	step [13/192], loss=8.9234
	step [14/192], loss=9.5704
	step [15/192], loss=8.4387
	step [16/192], loss=7.6675
	step [17/192], loss=8.5675
	step [18/192], loss=10.4595
	step [19/192], loss=7.4677
	step [20/192], loss=8.5262
	step [21/192], loss=8.7248
	step [22/192], loss=7.9294
	step [23/192], loss=7.9421
	step [24/192], loss=8.5176
	step [25/192], loss=8.8906
	step [26/192], loss=9.4421
	step [27/192], loss=9.0963
	step [28/192], loss=9.8352
	step [29/192], loss=7.1634
	step [30/192], loss=8.0219
	step [31/192], loss=8.5264
	step [32/192], loss=8.9351
	step [33/192], loss=9.1080
	step [34/192], loss=8.5409
	step [35/192], loss=8.4564
	step [36/192], loss=6.8090
	step [37/192], loss=9.3535
	step [38/192], loss=8.5299
	step [39/192], loss=8.9579
	step [40/192], loss=7.5019
	step [41/192], loss=11.4664
	step [42/192], loss=8.4827
	step [43/192], loss=9.5616
	step [44/192], loss=7.8289
	step [45/192], loss=8.2468
	step [46/192], loss=9.4563
	step [47/192], loss=7.6183
	step [48/192], loss=8.6438
	step [49/192], loss=9.4841
	step [50/192], loss=8.0494
	step [51/192], loss=7.5475
	step [52/192], loss=8.9178
	step [53/192], loss=8.4688
	step [54/192], loss=8.2494
	step [55/192], loss=8.3075
	step [56/192], loss=7.9503
	step [57/192], loss=7.9358
	step [58/192], loss=8.6201
	step [59/192], loss=9.1606
	step [60/192], loss=7.7487
	step [61/192], loss=6.6572
	step [62/192], loss=9.5165
	step [63/192], loss=8.3192
	step [64/192], loss=8.2574
	step [65/192], loss=8.9915
	step [66/192], loss=7.8553
	step [67/192], loss=7.5934
	step [68/192], loss=9.1341
	step [69/192], loss=8.4269
	step [70/192], loss=7.1627
	step [71/192], loss=8.9311
	step [72/192], loss=9.5508
	step [73/192], loss=7.7657
	step [74/192], loss=8.4448
	step [75/192], loss=7.8433
	step [76/192], loss=11.3057
	step [77/192], loss=7.7717
	step [78/192], loss=6.9481
	step [79/192], loss=8.9913
	step [80/192], loss=8.7143
	step [81/192], loss=9.3448
	step [82/192], loss=7.2630
	step [83/192], loss=9.1818
	step [84/192], loss=7.9117
	step [85/192], loss=8.4175
	step [86/192], loss=7.3834
	step [87/192], loss=8.3098
	step [88/192], loss=9.6679
	step [89/192], loss=10.5584
	step [90/192], loss=8.9640
	step [91/192], loss=9.4088
	step [92/192], loss=9.1325
	step [93/192], loss=10.2861
	step [94/192], loss=8.8631
	step [95/192], loss=8.2657
	step [96/192], loss=8.0886
	step [97/192], loss=9.2424
	step [98/192], loss=7.9594
	step [99/192], loss=8.5598
	step [100/192], loss=7.7095
	step [101/192], loss=8.4575
	step [102/192], loss=7.6262
	step [103/192], loss=9.6026
	step [104/192], loss=9.5980
	step [105/192], loss=9.7252
	step [106/192], loss=7.7805
	step [107/192], loss=8.2528
	step [108/192], loss=9.4329
	step [109/192], loss=8.9471
	step [110/192], loss=8.0577
	step [111/192], loss=7.4328
	step [112/192], loss=9.5759
	step [113/192], loss=11.0329
	step [114/192], loss=8.4864
	step [115/192], loss=10.0108
	step [116/192], loss=7.9525
	step [117/192], loss=7.5965
	step [118/192], loss=7.8841
	step [119/192], loss=10.5320
	step [120/192], loss=6.8820
	step [121/192], loss=9.4616
	step [122/192], loss=9.1197
	step [123/192], loss=9.4884
	step [124/192], loss=8.4679
	step [125/192], loss=7.2180
	step [126/192], loss=7.5586
	step [127/192], loss=8.3507
	step [128/192], loss=8.2306
	step [129/192], loss=6.4664
	step [130/192], loss=7.3545
	step [131/192], loss=9.0518
	step [132/192], loss=8.2438
	step [133/192], loss=8.6469
	step [134/192], loss=8.0543
	step [135/192], loss=7.6903
	step [136/192], loss=8.7034
	step [137/192], loss=7.3581
	step [138/192], loss=7.6689
	step [139/192], loss=8.2090
	step [140/192], loss=6.8713
	step [141/192], loss=7.7204
	step [142/192], loss=9.4888
	step [143/192], loss=9.4350
	step [144/192], loss=7.2904
	step [145/192], loss=8.4914
	step [146/192], loss=6.7783
	step [147/192], loss=7.2023
	step [148/192], loss=8.3963
	step [149/192], loss=8.4898
	step [150/192], loss=8.0118
	step [151/192], loss=6.4597
	step [152/192], loss=9.5419
	step [153/192], loss=7.6280
	step [154/192], loss=8.3567
	step [155/192], loss=8.0339
	step [156/192], loss=7.6973
	step [157/192], loss=6.2236
	step [158/192], loss=7.1102
	step [159/192], loss=9.3547
	step [160/192], loss=10.0786
	step [161/192], loss=8.2669
	step [162/192], loss=7.7518
	step [163/192], loss=7.8038
	step [164/192], loss=7.7422
	step [165/192], loss=8.1429
	step [166/192], loss=10.7914
	step [167/192], loss=9.6510
	step [168/192], loss=8.7982
	step [169/192], loss=8.6103
	step [170/192], loss=10.3682
	step [171/192], loss=7.9147
	step [172/192], loss=6.8733
	step [173/192], loss=7.7486
	step [174/192], loss=7.7008
	step [175/192], loss=7.2718
	step [176/192], loss=7.7474
	step [177/192], loss=7.2304
	step [178/192], loss=10.7149
	step [179/192], loss=8.9045
	step [180/192], loss=8.5375
	step [181/192], loss=7.7199
	step [182/192], loss=8.4119
	step [183/192], loss=8.3094
	step [184/192], loss=8.8209
	step [185/192], loss=12.9096
	step [186/192], loss=8.5821
	step [187/192], loss=7.9717
	step [188/192], loss=8.9232
	step [189/192], loss=6.6339
	step [190/192], loss=7.2370
	step [191/192], loss=10.0685
	step [192/192], loss=1.6178
	Evaluating
	loss=0.0231, precision=0.2053, recall=0.9924, f1=0.3402
saving model as: 0_saved_model.pth
Training epoch 26
	step [1/192], loss=10.8688
	step [2/192], loss=6.9143
	step [3/192], loss=6.7894
	step [4/192], loss=8.2112
	step [5/192], loss=8.3118
	step [6/192], loss=7.5407
	step [7/192], loss=7.1623
	step [8/192], loss=6.6984
	step [9/192], loss=7.1686
	step [10/192], loss=6.9184
	step [11/192], loss=7.6285
	step [12/192], loss=7.7459
	step [13/192], loss=7.3866
	step [14/192], loss=7.9412
	step [15/192], loss=8.2876
	step [16/192], loss=8.3661
	step [17/192], loss=7.7240
	step [18/192], loss=7.2288
	step [19/192], loss=8.0748
	step [20/192], loss=6.5289
	step [21/192], loss=7.7444
	step [22/192], loss=7.7671
	step [23/192], loss=10.1781
	step [24/192], loss=7.0963
	step [25/192], loss=8.2504
	step [26/192], loss=8.3051
	step [27/192], loss=6.3927
	step [28/192], loss=6.9625
	step [29/192], loss=6.7814
	step [30/192], loss=8.3945
	step [31/192], loss=7.3888
	step [32/192], loss=7.3551
	step [33/192], loss=9.0997
	step [34/192], loss=6.0965
	step [35/192], loss=6.7199
	step [36/192], loss=8.3244
	step [37/192], loss=8.0296
	step [38/192], loss=6.1595
	step [39/192], loss=7.5350
	step [40/192], loss=8.6574
	step [41/192], loss=7.2275
	step [42/192], loss=7.3109
	step [43/192], loss=10.4006
	step [44/192], loss=8.2605
	step [45/192], loss=9.5499
	step [46/192], loss=8.7197
	step [47/192], loss=7.7787
	step [48/192], loss=10.4567
	step [49/192], loss=7.6028
	step [50/192], loss=9.1567
	step [51/192], loss=8.7318
	step [52/192], loss=7.2046
	step [53/192], loss=8.3924
	step [54/192], loss=7.0089
	step [55/192], loss=8.1364
	step [56/192], loss=9.4775
	step [57/192], loss=10.4237
	step [58/192], loss=9.0615
	step [59/192], loss=8.7946
	step [60/192], loss=6.6942
	step [61/192], loss=7.1230
	step [62/192], loss=8.0074
	step [63/192], loss=7.4711
	step [64/192], loss=8.8110
	step [65/192], loss=8.4484
	step [66/192], loss=8.8299
	step [67/192], loss=7.3145
	step [68/192], loss=9.2256
	step [69/192], loss=8.7126
	step [70/192], loss=9.7821
	step [71/192], loss=8.0927
	step [72/192], loss=6.4503
	step [73/192], loss=7.4016
	step [74/192], loss=7.1837
	step [75/192], loss=9.7987
	step [76/192], loss=7.2557
	step [77/192], loss=7.8776
	step [78/192], loss=7.4627
	step [79/192], loss=8.2590
	step [80/192], loss=6.7687
	step [81/192], loss=10.7978
	step [82/192], loss=7.6492
	step [83/192], loss=11.0433
	step [84/192], loss=8.4640
	step [85/192], loss=7.9877
	step [86/192], loss=8.1223
	step [87/192], loss=7.6860
	step [88/192], loss=8.7518
	step [89/192], loss=7.8989
	step [90/192], loss=7.5575
	step [91/192], loss=6.9322
	step [92/192], loss=8.8978
	step [93/192], loss=7.4395
	step [94/192], loss=8.5544
	step [95/192], loss=9.6910
	step [96/192], loss=8.1338
	step [97/192], loss=7.6935
	step [98/192], loss=7.4609
	step [99/192], loss=8.3765
	step [100/192], loss=9.3496
	step [101/192], loss=7.0820
	step [102/192], loss=8.6959
	step [103/192], loss=7.7416
	step [104/192], loss=7.6442
	step [105/192], loss=8.6546
	step [106/192], loss=7.6947
	step [107/192], loss=8.0617
	step [108/192], loss=8.2318
	step [109/192], loss=8.6071
	step [110/192], loss=10.2249
	step [111/192], loss=7.9947
	step [112/192], loss=7.4959
	step [113/192], loss=8.9471
	step [114/192], loss=8.4789
	step [115/192], loss=7.9542
	step [116/192], loss=8.5675
	step [117/192], loss=7.3220
	step [118/192], loss=7.1060
	step [119/192], loss=9.7868
	step [120/192], loss=8.6607
	step [121/192], loss=9.1492
	step [122/192], loss=8.1385
	step [123/192], loss=7.1873
	step [124/192], loss=7.2199
	step [125/192], loss=7.9695
	step [126/192], loss=7.1377
	step [127/192], loss=8.8941
	step [128/192], loss=8.7923
	step [129/192], loss=8.5841
	step [130/192], loss=7.5353
	step [131/192], loss=9.4648
	step [132/192], loss=8.5051
	step [133/192], loss=9.4334
	step [134/192], loss=8.0807
	step [135/192], loss=9.6431
	step [136/192], loss=7.9324
	step [137/192], loss=8.4656
	step [138/192], loss=7.7879
	step [139/192], loss=8.0274
	step [140/192], loss=6.8962
	step [141/192], loss=7.8620
	step [142/192], loss=8.4103
	step [143/192], loss=9.4546
	step [144/192], loss=6.8131
	step [145/192], loss=8.9900
	step [146/192], loss=8.2116
	step [147/192], loss=8.1846
	step [148/192], loss=8.1197
	step [149/192], loss=8.2560
	step [150/192], loss=9.0781
	step [151/192], loss=8.0013
	step [152/192], loss=8.3796
	step [153/192], loss=7.9927
	step [154/192], loss=6.2237
	step [155/192], loss=7.6817
	step [156/192], loss=9.1109
	step [157/192], loss=7.1296
	step [158/192], loss=9.6688
	step [159/192], loss=8.3901
	step [160/192], loss=10.0597
	step [161/192], loss=7.6830
	step [162/192], loss=9.4227
	step [163/192], loss=8.9390
	step [164/192], loss=8.6570
	step [165/192], loss=8.9182
	step [166/192], loss=8.7979
	step [167/192], loss=7.8532
	step [168/192], loss=8.8508
	step [169/192], loss=8.3853
	step [170/192], loss=7.6219
	step [171/192], loss=8.8218
	step [172/192], loss=7.2609
	step [173/192], loss=5.9213
	step [174/192], loss=6.5511
	step [175/192], loss=6.4587
	step [176/192], loss=8.5283
	step [177/192], loss=9.3457
	step [178/192], loss=8.5675
	step [179/192], loss=9.2381
	step [180/192], loss=9.0697
	step [181/192], loss=9.2836
	step [182/192], loss=7.4373
	step [183/192], loss=8.5874
	step [184/192], loss=10.6031
	step [185/192], loss=9.0393
	step [186/192], loss=10.0109
	step [187/192], loss=9.1538
	step [188/192], loss=6.0291
	step [189/192], loss=9.3736
	step [190/192], loss=8.3027
	step [191/192], loss=8.2690
	step [192/192], loss=1.5534
	Evaluating
	loss=0.0229, precision=0.2085, recall=0.9926, f1=0.3446
saving model as: 0_saved_model.pth
Training epoch 27
	step [1/192], loss=6.8548
	step [2/192], loss=7.5314
	step [3/192], loss=7.8702
	step [4/192], loss=9.3713
	step [5/192], loss=8.8957
	step [6/192], loss=6.4868
	step [7/192], loss=9.6356
	step [8/192], loss=8.0519
	step [9/192], loss=7.0891
	step [10/192], loss=8.3235
	step [11/192], loss=7.0398
	step [12/192], loss=8.8224
	step [13/192], loss=8.7087
	step [14/192], loss=7.9209
	step [15/192], loss=9.4218
	step [16/192], loss=7.6956
	step [17/192], loss=6.7238
	step [18/192], loss=6.6977
	step [19/192], loss=9.0533
	step [20/192], loss=8.2079
	step [21/192], loss=7.4125
	step [22/192], loss=10.0411
	step [23/192], loss=6.5938
	step [24/192], loss=7.5752
	step [25/192], loss=8.2474
	step [26/192], loss=8.1981
	step [27/192], loss=8.0162
	step [28/192], loss=7.8604
	step [29/192], loss=7.5708
	step [30/192], loss=7.6276
	step [31/192], loss=7.1484
	step [32/192], loss=7.1930
	step [33/192], loss=7.4643
	step [34/192], loss=8.8255
	step [35/192], loss=6.3869
	step [36/192], loss=8.7015
	step [37/192], loss=8.2302
	step [38/192], loss=7.0130
	step [39/192], loss=7.8666
	step [40/192], loss=7.7032
	step [41/192], loss=9.4077
	step [42/192], loss=9.9557
	step [43/192], loss=6.0649
	step [44/192], loss=9.3228
	step [45/192], loss=7.8507
	step [46/192], loss=7.9442
	step [47/192], loss=10.6155
	step [48/192], loss=7.6380
	step [49/192], loss=7.1272
	step [50/192], loss=10.0274
	step [51/192], loss=7.9640
	step [52/192], loss=8.0420
	step [53/192], loss=7.3299
	step [54/192], loss=7.6510
	step [55/192], loss=7.0153
	step [56/192], loss=8.4152
	step [57/192], loss=11.2934
	step [58/192], loss=8.7401
	step [59/192], loss=8.6250
	step [60/192], loss=8.4252
	step [61/192], loss=8.6730
	step [62/192], loss=9.3477
	step [63/192], loss=9.2426
	step [64/192], loss=9.5544
	step [65/192], loss=7.2711
	step [66/192], loss=9.8928
	step [67/192], loss=8.1357
	step [68/192], loss=9.0510
	step [69/192], loss=7.3773
	step [70/192], loss=7.4612
	step [71/192], loss=7.3709
	step [72/192], loss=7.1804
	step [73/192], loss=8.7826
	step [74/192], loss=7.2486
	step [75/192], loss=7.9182
	step [76/192], loss=7.3458
	step [77/192], loss=8.8226
	step [78/192], loss=8.3971
	step [79/192], loss=6.4414
	step [80/192], loss=6.8919
	step [81/192], loss=7.5895
	step [82/192], loss=7.2691
	step [83/192], loss=8.1465
	step [84/192], loss=7.9645
	step [85/192], loss=7.2359
	step [86/192], loss=7.7628
	step [87/192], loss=9.3563
	step [88/192], loss=8.1249
	step [89/192], loss=8.2797
	step [90/192], loss=9.3631
	step [91/192], loss=7.6924
	step [92/192], loss=8.0652
	step [93/192], loss=8.4747
	step [94/192], loss=6.5312
	step [95/192], loss=8.1498
	step [96/192], loss=7.3002
	step [97/192], loss=8.3283
	step [98/192], loss=8.8432
	step [99/192], loss=6.6208
	step [100/192], loss=8.8331
	step [101/192], loss=7.8247
	step [102/192], loss=7.7639
	step [103/192], loss=8.4261
	step [104/192], loss=8.4733
	step [105/192], loss=7.9078
	step [106/192], loss=7.7170
	step [107/192], loss=6.3863
	step [108/192], loss=7.7719
	step [109/192], loss=6.7573
	step [110/192], loss=8.3228
	step [111/192], loss=9.4578
	step [112/192], loss=8.4208
	step [113/192], loss=7.2779
	step [114/192], loss=6.4474
	step [115/192], loss=6.8519
	step [116/192], loss=7.7933
	step [117/192], loss=8.2496
	step [118/192], loss=6.5887
	step [119/192], loss=8.0607
	step [120/192], loss=6.0940
	step [121/192], loss=6.7816
	step [122/192], loss=7.8675
	step [123/192], loss=8.9225
	step [124/192], loss=7.5100
	step [125/192], loss=7.9774
	step [126/192], loss=7.9877
	step [127/192], loss=7.5105
	step [128/192], loss=6.9514
	step [129/192], loss=7.1300
	step [130/192], loss=7.0427
	step [131/192], loss=9.3915
	step [132/192], loss=8.0095
	step [133/192], loss=8.6854
	step [134/192], loss=9.2215
	step [135/192], loss=9.0784
	step [136/192], loss=7.5071
	step [137/192], loss=6.9063
	step [138/192], loss=7.7602
	step [139/192], loss=8.7147
	step [140/192], loss=8.8626
	step [141/192], loss=5.8185
	step [142/192], loss=7.3857
	step [143/192], loss=8.2279
	step [144/192], loss=6.1632
	step [145/192], loss=8.3637
	step [146/192], loss=7.6040
	step [147/192], loss=9.0881
	step [148/192], loss=8.6359
	step [149/192], loss=6.0335
	step [150/192], loss=7.1291
	step [151/192], loss=7.2199
	step [152/192], loss=7.5225
	step [153/192], loss=6.7404
	step [154/192], loss=6.8965
	step [155/192], loss=7.0649
	step [156/192], loss=6.3789
	step [157/192], loss=5.6225
	step [158/192], loss=7.7495
	step [159/192], loss=8.3867
	step [160/192], loss=8.4596
	step [161/192], loss=8.2412
	step [162/192], loss=8.4267
	step [163/192], loss=8.8331
	step [164/192], loss=8.4513
	step [165/192], loss=9.4914
	step [166/192], loss=9.9937
	step [167/192], loss=6.9819
	step [168/192], loss=7.3112
	step [169/192], loss=9.0649
	step [170/192], loss=9.0573
	step [171/192], loss=8.8327
	step [172/192], loss=7.8567
	step [173/192], loss=7.6522
	step [174/192], loss=10.2708
	step [175/192], loss=7.8969
	step [176/192], loss=7.9040
	step [177/192], loss=9.3997
	step [178/192], loss=8.0417
	step [179/192], loss=7.8414
	step [180/192], loss=9.2801
	step [181/192], loss=6.9758
	step [182/192], loss=10.4480
	step [183/192], loss=8.0582
	step [184/192], loss=7.3685
	step [185/192], loss=8.3330
	step [186/192], loss=7.0604
	step [187/192], loss=8.7647
	step [188/192], loss=7.1579
	step [189/192], loss=7.0634
	step [190/192], loss=8.0187
	step [191/192], loss=8.3946
	step [192/192], loss=1.2353
	Evaluating
	loss=0.0229, precision=0.2055, recall=0.9930, f1=0.3405
Training epoch 28
	step [1/192], loss=6.4196
	step [2/192], loss=8.4659
	step [3/192], loss=8.4885
	step [4/192], loss=6.7907
	step [5/192], loss=7.8874
	step [6/192], loss=6.5049
	step [7/192], loss=5.6423
	step [8/192], loss=7.7253
	step [9/192], loss=6.5614
	step [10/192], loss=6.7369
	step [11/192], loss=6.6142
	step [12/192], loss=8.3591
	step [13/192], loss=7.7088
	step [14/192], loss=9.5552
	step [15/192], loss=7.7654
	step [16/192], loss=6.4745
	step [17/192], loss=6.6311
	step [18/192], loss=10.3224
	step [19/192], loss=7.8979
	step [20/192], loss=7.0747
	step [21/192], loss=7.0702
	step [22/192], loss=8.7361
	step [23/192], loss=7.7433
	step [24/192], loss=7.3760
	step [25/192], loss=7.8745
	step [26/192], loss=9.2972
	step [27/192], loss=7.2388
	step [28/192], loss=8.6937
	step [29/192], loss=7.9216
	step [30/192], loss=7.6682
	step [31/192], loss=8.0753
	step [32/192], loss=6.9057
	step [33/192], loss=7.1433
	step [34/192], loss=6.5892
	step [35/192], loss=8.2675
	step [36/192], loss=5.9303
	step [37/192], loss=8.9844
	step [38/192], loss=9.1211
	step [39/192], loss=8.2599
	step [40/192], loss=8.4209
	step [41/192], loss=6.4852
	step [42/192], loss=7.7647
	step [43/192], loss=7.9973
	step [44/192], loss=6.8906
	step [45/192], loss=8.3313
	step [46/192], loss=7.2392
	step [47/192], loss=8.7426
	step [48/192], loss=8.5841
	step [49/192], loss=9.0547
	step [50/192], loss=7.5167
	step [51/192], loss=8.2307
	step [52/192], loss=7.9086
	step [53/192], loss=6.9439
	step [54/192], loss=7.6353
	step [55/192], loss=8.9227
	step [56/192], loss=7.2018
	step [57/192], loss=6.8970
	step [58/192], loss=8.0553
	step [59/192], loss=8.2061
	step [60/192], loss=6.6193
	step [61/192], loss=8.4507
	step [62/192], loss=6.7402
	step [63/192], loss=8.6447
	step [64/192], loss=8.7403
	step [65/192], loss=8.1756
	step [66/192], loss=9.4497
	step [67/192], loss=6.0864
	step [68/192], loss=7.7830
	step [69/192], loss=9.7194
	step [70/192], loss=7.6241
	step [71/192], loss=8.0800
	step [72/192], loss=7.9399
	step [73/192], loss=6.1593
	step [74/192], loss=6.8879
	step [75/192], loss=8.7600
	step [76/192], loss=7.8315
	step [77/192], loss=7.5287
	step [78/192], loss=6.6885
	step [79/192], loss=8.4231
	step [80/192], loss=7.4730
	step [81/192], loss=6.9415
	step [82/192], loss=8.4306
	step [83/192], loss=5.5172
	step [84/192], loss=7.3920
	step [85/192], loss=7.0389
	step [86/192], loss=7.4067
	step [87/192], loss=7.7429
	step [88/192], loss=7.1018
	step [89/192], loss=7.7897
	step [90/192], loss=9.9446
	step [91/192], loss=8.3296
	step [92/192], loss=7.7978
	step [93/192], loss=6.5844
	step [94/192], loss=8.0013
	step [95/192], loss=6.9887
	step [96/192], loss=8.0712
	step [97/192], loss=6.4108
	step [98/192], loss=8.4156
	step [99/192], loss=7.0294
	step [100/192], loss=7.2787
	step [101/192], loss=7.2383
	step [102/192], loss=7.1913
	step [103/192], loss=7.5249
	step [104/192], loss=7.2489
	step [105/192], loss=8.6594
	step [106/192], loss=7.3379
	step [107/192], loss=6.4752
	step [108/192], loss=9.2957
	step [109/192], loss=7.8081
	step [110/192], loss=8.7546
	step [111/192], loss=6.5057
	step [112/192], loss=8.3311
	step [113/192], loss=9.8281
	step [114/192], loss=7.7385
	step [115/192], loss=7.9360
	step [116/192], loss=6.8258
	step [117/192], loss=8.6177
	step [118/192], loss=8.4391
	step [119/192], loss=8.7293
	step [120/192], loss=7.8731
	step [121/192], loss=7.3876
	step [122/192], loss=7.0870
	step [123/192], loss=8.2012
	step [124/192], loss=7.4398
	step [125/192], loss=6.7769
	step [126/192], loss=8.0540
	step [127/192], loss=8.8131
	step [128/192], loss=8.2737
	step [129/192], loss=6.8421
	step [130/192], loss=6.9414
	step [131/192], loss=8.4285
	step [132/192], loss=5.5486
	step [133/192], loss=8.1212
	step [134/192], loss=8.1436
	step [135/192], loss=6.9506
	step [136/192], loss=7.7523
	step [137/192], loss=7.5353
	step [138/192], loss=8.5237
	step [139/192], loss=7.8084
	step [140/192], loss=7.4150
	step [141/192], loss=9.0913
	step [142/192], loss=7.6524
	step [143/192], loss=6.7687
	step [144/192], loss=7.3367
	step [145/192], loss=7.8568
	step [146/192], loss=6.8960
	step [147/192], loss=7.2696
	step [148/192], loss=6.9215
	step [149/192], loss=7.6783
	step [150/192], loss=7.6004
	step [151/192], loss=8.6726
	step [152/192], loss=5.7794
	step [153/192], loss=7.6517
	step [154/192], loss=7.3122
	step [155/192], loss=6.2909
	step [156/192], loss=8.2818
	step [157/192], loss=8.4294
	step [158/192], loss=7.7315
	step [159/192], loss=7.2931
	step [160/192], loss=8.4878
	step [161/192], loss=7.5588
	step [162/192], loss=8.3198
	step [163/192], loss=7.5158
	step [164/192], loss=7.6166
	step [165/192], loss=8.8674
	step [166/192], loss=6.9720
	step [167/192], loss=7.9038
	step [168/192], loss=7.1110
	step [169/192], loss=6.2649
	step [170/192], loss=7.7604
	step [171/192], loss=6.7109
	step [172/192], loss=7.5861
	step [173/192], loss=8.3971
	step [174/192], loss=6.8429
	step [175/192], loss=8.3915
	step [176/192], loss=7.0717
	step [177/192], loss=9.0683
	step [178/192], loss=8.6493
	step [179/192], loss=7.3718
	step [180/192], loss=8.1876
	step [181/192], loss=9.3964
	step [182/192], loss=8.5845
	step [183/192], loss=7.5201
	step [184/192], loss=6.9353
	step [185/192], loss=7.5869
	step [186/192], loss=7.3679
	step [187/192], loss=9.0152
	step [188/192], loss=7.8298
	step [189/192], loss=8.1542
	step [190/192], loss=8.9514
	step [191/192], loss=8.0345
	step [192/192], loss=1.3332
	Evaluating
	loss=0.0237, precision=0.2003, recall=0.9929, f1=0.3334
Training epoch 29
	step [1/192], loss=9.7339
	step [2/192], loss=8.6134
	step [3/192], loss=8.9001
	step [4/192], loss=8.2764
	step [5/192], loss=7.6495
	step [6/192], loss=6.8655
	step [7/192], loss=8.7169
	step [8/192], loss=8.1011
	step [9/192], loss=6.9258
	step [10/192], loss=7.3058
	step [11/192], loss=9.1862
	step [12/192], loss=8.5210
	step [13/192], loss=7.3681
	step [14/192], loss=8.1816
	step [15/192], loss=7.6277
	step [16/192], loss=7.2410
	step [17/192], loss=7.0306
	step [18/192], loss=7.0938
	step [19/192], loss=8.2457
	step [20/192], loss=7.2226
	step [21/192], loss=6.9669
	step [22/192], loss=6.4007
	step [23/192], loss=6.1991
	step [24/192], loss=6.7172
	step [25/192], loss=6.3997
	step [26/192], loss=6.5353
	step [27/192], loss=9.5690
	step [28/192], loss=7.4824
	step [29/192], loss=7.0944
	step [30/192], loss=8.4856
	step [31/192], loss=7.4958
	step [32/192], loss=8.3943
	step [33/192], loss=6.9415
	step [34/192], loss=6.7409
	step [35/192], loss=8.1327
	step [36/192], loss=7.2327
	step [37/192], loss=8.0592
	step [38/192], loss=6.8270
	step [39/192], loss=6.3620
	step [40/192], loss=6.6655
	step [41/192], loss=6.7122
	step [42/192], loss=6.9059
	step [43/192], loss=8.5241
	step [44/192], loss=9.1297
	step [45/192], loss=7.0450
	step [46/192], loss=6.2152
	step [47/192], loss=8.1126
	step [48/192], loss=6.7434
	step [49/192], loss=7.6022
	step [50/192], loss=6.4748
	step [51/192], loss=8.7353
	step [52/192], loss=9.3260
	step [53/192], loss=8.3688
	step [54/192], loss=8.9353
	step [55/192], loss=8.6089
	step [56/192], loss=7.5082
	step [57/192], loss=7.5960
	step [58/192], loss=7.3011
	step [59/192], loss=6.3414
	step [60/192], loss=8.5815
	step [61/192], loss=6.5134
	step [62/192], loss=6.2574
	step [63/192], loss=7.3462
	step [64/192], loss=8.2791
	step [65/192], loss=8.9921
	step [66/192], loss=6.6946
	step [67/192], loss=8.6471
	step [68/192], loss=7.3921
	step [69/192], loss=6.6575
	step [70/192], loss=7.4928
	step [71/192], loss=8.0632
	step [72/192], loss=6.9214
	step [73/192], loss=7.2864
	step [74/192], loss=7.7764
	step [75/192], loss=8.2475
	step [76/192], loss=8.7386
	step [77/192], loss=7.4093
	step [78/192], loss=7.6277
	step [79/192], loss=7.5947
	step [80/192], loss=8.3559
	step [81/192], loss=8.6665
	step [82/192], loss=6.6590
	step [83/192], loss=6.7112
	step [84/192], loss=7.0744
	step [85/192], loss=7.4051
	step [86/192], loss=6.7583
	step [87/192], loss=9.1850
	step [88/192], loss=8.0593
	step [89/192], loss=6.6577
	step [90/192], loss=6.7874
	step [91/192], loss=10.4543
	step [92/192], loss=6.5674
	step [93/192], loss=8.0176
	step [94/192], loss=6.7471
	step [95/192], loss=7.9486
	step [96/192], loss=7.2299
	step [97/192], loss=8.4440
	step [98/192], loss=7.2601
	step [99/192], loss=6.8283
	step [100/192], loss=6.4067
	step [101/192], loss=7.6057
	step [102/192], loss=6.7385
	step [103/192], loss=7.6538
	step [104/192], loss=7.1701
	step [105/192], loss=7.4197
	step [106/192], loss=6.9139
	step [107/192], loss=6.9207
	step [108/192], loss=6.8771
	step [109/192], loss=9.1248
	step [110/192], loss=7.1898
	step [111/192], loss=8.2215
	step [112/192], loss=7.9585
	step [113/192], loss=9.3482
	step [114/192], loss=7.3121
	step [115/192], loss=7.0124
	step [116/192], loss=7.5944
	step [117/192], loss=6.5901
	step [118/192], loss=6.5986
	step [119/192], loss=8.8571
	step [120/192], loss=8.0673
	step [121/192], loss=6.5595
	step [122/192], loss=7.5605
	step [123/192], loss=8.3481
	step [124/192], loss=10.8055
	step [125/192], loss=7.1614
	step [126/192], loss=7.1462
	step [127/192], loss=7.7004
	step [128/192], loss=8.9632
	step [129/192], loss=6.6878
	step [130/192], loss=6.4331
	step [131/192], loss=7.1825
	step [132/192], loss=7.6741
	step [133/192], loss=8.0698
	step [134/192], loss=6.9659
	step [135/192], loss=6.6363
	step [136/192], loss=7.9937
	step [137/192], loss=6.7118
	step [138/192], loss=8.0033
	step [139/192], loss=7.6873
	step [140/192], loss=7.7701
	step [141/192], loss=9.2332
	step [142/192], loss=7.8711
	step [143/192], loss=7.8454
	step [144/192], loss=7.6450
	step [145/192], loss=7.8061
	step [146/192], loss=6.9869
	step [147/192], loss=6.4847
	step [148/192], loss=7.7015
	step [149/192], loss=6.4669
	step [150/192], loss=7.5244
	step [151/192], loss=9.9437
	step [152/192], loss=6.6970
	step [153/192], loss=6.5555
	step [154/192], loss=8.7845
	step [155/192], loss=7.7914
	step [156/192], loss=6.1727
	step [157/192], loss=6.5657
	step [158/192], loss=7.8326
	step [159/192], loss=8.1641
	step [160/192], loss=7.8298
	step [161/192], loss=7.2597
	step [162/192], loss=7.0465
	step [163/192], loss=7.8670
	step [164/192], loss=7.3468
	step [165/192], loss=7.2105
	step [166/192], loss=8.2721
	step [167/192], loss=6.3583
	step [168/192], loss=11.3600
	step [169/192], loss=8.5671
	step [170/192], loss=7.6413
	step [171/192], loss=6.4645
	step [172/192], loss=8.2000
	step [173/192], loss=8.0678
	step [174/192], loss=6.9142
	step [175/192], loss=7.8521
	step [176/192], loss=7.7025
	step [177/192], loss=8.0083
	step [178/192], loss=9.1984
	step [179/192], loss=7.1188
	step [180/192], loss=7.4308
	step [181/192], loss=7.1047
	step [182/192], loss=6.7020
	step [183/192], loss=6.0081
	step [184/192], loss=7.6606
	step [185/192], loss=7.4387
	step [186/192], loss=7.8658
	step [187/192], loss=7.8163
	step [188/192], loss=7.1669
	step [189/192], loss=5.6386
	step [190/192], loss=8.3592
	step [191/192], loss=7.6161
	step [192/192], loss=1.1884
	Evaluating
	loss=0.0209, precision=0.2073, recall=0.9924, f1=0.3430
Training epoch 30
	step [1/192], loss=6.6379
	step [2/192], loss=7.0270
	step [3/192], loss=7.8143
	step [4/192], loss=7.1328
	step [5/192], loss=5.8031
	step [6/192], loss=7.1805
	step [7/192], loss=6.3823
	step [8/192], loss=8.4986
	step [9/192], loss=6.7287
	step [10/192], loss=7.7042
	step [11/192], loss=7.4455
	step [12/192], loss=8.0882
	step [13/192], loss=7.2131
	step [14/192], loss=7.5583
	step [15/192], loss=6.6039
	step [16/192], loss=7.0748
	step [17/192], loss=6.1608
	step [18/192], loss=7.7581
	step [19/192], loss=8.2304
	step [20/192], loss=6.5668
	step [21/192], loss=6.6112
	step [22/192], loss=7.8035
	step [23/192], loss=7.2244
	step [24/192], loss=8.8109
	step [25/192], loss=6.3702
	step [26/192], loss=7.5215
	step [27/192], loss=7.4659
	step [28/192], loss=6.8346
	step [29/192], loss=6.4879
	step [30/192], loss=7.5815
	step [31/192], loss=9.3922
	step [32/192], loss=6.5737
	step [33/192], loss=6.4181
	step [34/192], loss=6.5265
	step [35/192], loss=5.8069
	step [36/192], loss=6.2996
	step [37/192], loss=6.7589
	step [38/192], loss=8.1047
	step [39/192], loss=7.8127
	step [40/192], loss=8.4351
	step [41/192], loss=7.4926
	step [42/192], loss=7.3328
	step [43/192], loss=6.9953
	step [44/192], loss=7.0077
	step [45/192], loss=7.3058
	step [46/192], loss=7.8862
	step [47/192], loss=6.6820
	step [48/192], loss=9.7434
	step [49/192], loss=6.9746
	step [50/192], loss=8.0578
	step [51/192], loss=9.5223
	step [52/192], loss=7.1239
	step [53/192], loss=6.5285
	step [54/192], loss=5.8449
	step [55/192], loss=7.9263
	step [56/192], loss=8.9614
	step [57/192], loss=7.4123
	step [58/192], loss=7.6327
	step [59/192], loss=7.6508
	step [60/192], loss=7.0366
	step [61/192], loss=6.7006
	step [62/192], loss=7.4650
	step [63/192], loss=6.8479
	step [64/192], loss=6.8004
	step [65/192], loss=7.8653
	step [66/192], loss=9.5558
	step [67/192], loss=7.0454
	step [68/192], loss=7.1800
	step [69/192], loss=8.5966
	step [70/192], loss=8.7405
	step [71/192], loss=8.8579
	step [72/192], loss=8.2302
	step [73/192], loss=6.6794
	step [74/192], loss=8.1210
	step [75/192], loss=8.5329
	step [76/192], loss=6.9858
	step [77/192], loss=7.1067
	step [78/192], loss=5.9287
	step [79/192], loss=6.7336
	step [80/192], loss=6.9994
	step [81/192], loss=8.1151
	step [82/192], loss=7.9561
	step [83/192], loss=7.3841
	step [84/192], loss=7.7498
	step [85/192], loss=7.4740
	step [86/192], loss=6.9784
	step [87/192], loss=6.8444
	step [88/192], loss=7.0495
	step [89/192], loss=7.1513
	step [90/192], loss=7.0857
	step [91/192], loss=7.0720
	step [92/192], loss=6.8183
	step [93/192], loss=8.1282
	step [94/192], loss=7.8570
	step [95/192], loss=6.6289
	step [96/192], loss=6.5569
	step [97/192], loss=7.8382
	step [98/192], loss=6.6435
	step [99/192], loss=7.2888
	step [100/192], loss=6.5994
	step [101/192], loss=10.1053
	step [102/192], loss=7.3624
	step [103/192], loss=7.4711
	step [104/192], loss=7.6192
	step [105/192], loss=7.2461
	step [106/192], loss=7.8406
	step [107/192], loss=7.6108
	step [108/192], loss=7.1554
	step [109/192], loss=5.6783
	step [110/192], loss=6.7343
	step [111/192], loss=5.7764
	step [112/192], loss=7.8888
	step [113/192], loss=7.3556
	step [114/192], loss=8.0967
	step [115/192], loss=7.2701
	step [116/192], loss=6.3830
	step [117/192], loss=7.4469
	step [118/192], loss=5.7173
	step [119/192], loss=8.0675
	step [120/192], loss=6.3666
	step [121/192], loss=7.7763
	step [122/192], loss=9.1000
	step [123/192], loss=7.5106
	step [124/192], loss=8.7493
	step [125/192], loss=7.1511
	step [126/192], loss=8.4118
	step [127/192], loss=6.4336
	step [128/192], loss=8.5781
	step [129/192], loss=7.2276
	step [130/192], loss=7.5383
	step [131/192], loss=7.0126
	step [132/192], loss=6.1601
	step [133/192], loss=7.5104
	step [134/192], loss=8.3909
	step [135/192], loss=7.4253
	step [136/192], loss=8.2853
	step [137/192], loss=6.7401
	step [138/192], loss=7.7462
	step [139/192], loss=6.2250
	step [140/192], loss=7.6376
	step [141/192], loss=10.9663
	step [142/192], loss=6.3950
	step [143/192], loss=7.3837
	step [144/192], loss=8.4225
	step [145/192], loss=6.9323
	step [146/192], loss=7.6051
	step [147/192], loss=6.5421
	step [148/192], loss=7.0631
	step [149/192], loss=6.8887
	step [150/192], loss=6.5820
	step [151/192], loss=7.8253
	step [152/192], loss=7.4925
	step [153/192], loss=7.3648
	step [154/192], loss=6.5175
	step [155/192], loss=7.3785
	step [156/192], loss=7.6188
	step [157/192], loss=7.7350
	step [158/192], loss=6.7263
	step [159/192], loss=7.6677
	step [160/192], loss=6.6004
	step [161/192], loss=7.5965
	step [162/192], loss=9.0615
	step [163/192], loss=7.4031
	step [164/192], loss=7.4469
	step [165/192], loss=6.5368
	step [166/192], loss=6.3695
	step [167/192], loss=8.4516
	step [168/192], loss=9.2627
	step [169/192], loss=6.4770
	step [170/192], loss=7.6316
	step [171/192], loss=7.1082
	step [172/192], loss=7.3917
	step [173/192], loss=7.2838
	step [174/192], loss=7.6396
	step [175/192], loss=7.0360
	step [176/192], loss=7.6439
	step [177/192], loss=7.9949
	step [178/192], loss=6.5421
	step [179/192], loss=7.8146
	step [180/192], loss=6.2828
	step [181/192], loss=6.6097
	step [182/192], loss=7.0505
	step [183/192], loss=8.2491
	step [184/192], loss=7.3441
	step [185/192], loss=7.5750
	step [186/192], loss=6.1996
	step [187/192], loss=8.1257
	step [188/192], loss=8.0135
	step [189/192], loss=6.6422
	step [190/192], loss=6.4593
	step [191/192], loss=6.7114
	step [192/192], loss=2.2335
	Evaluating
	loss=0.0205, precision=0.2152, recall=0.9921, f1=0.3537
saving model as: 0_saved_model.pth
Training finished
best_f1: 0.3536888012843017
directing: X rim_enhanced: False test_id 1
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9175 # image files with weight 9141
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2707 # image files with weight 2691
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9141
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/143], loss=388.1206
	step [2/143], loss=267.0596
	step [3/143], loss=229.9972
	step [4/143], loss=218.6886
	step [5/143], loss=215.2418
	step [6/143], loss=210.7752
	step [7/143], loss=207.1481
	step [8/143], loss=205.2930
	step [9/143], loss=203.1306
	step [10/143], loss=201.2356
	step [11/143], loss=201.7614
	step [12/143], loss=197.7551
	step [13/143], loss=195.2444
	step [14/143], loss=194.4847
	step [15/143], loss=191.1777
	step [16/143], loss=189.2822
	step [17/143], loss=189.1113
	step [18/143], loss=187.6693
	step [19/143], loss=184.7211
	step [20/143], loss=185.5751
	step [21/143], loss=181.2062
	step [22/143], loss=179.7679
	step [23/143], loss=180.6345
	step [24/143], loss=176.6058
	step [25/143], loss=174.8909
	step [26/143], loss=176.1671
	step [27/143], loss=173.1469
	step [28/143], loss=170.5914
	step [29/143], loss=168.9185
	step [30/143], loss=170.0905
	step [31/143], loss=166.1595
	step [32/143], loss=166.7819
	step [33/143], loss=163.7382
	step [34/143], loss=160.4809
	step [35/143], loss=161.5962
	step [36/143], loss=162.0044
	step [37/143], loss=159.5726
	step [38/143], loss=159.8731
	step [39/143], loss=158.6083
	step [40/143], loss=155.7528
	step [41/143], loss=156.8625
	step [42/143], loss=154.9180
	step [43/143], loss=156.1859
	step [44/143], loss=154.1751
	step [45/143], loss=152.4707
	step [46/143], loss=150.8583
	step [47/143], loss=152.4745
	step [48/143], loss=149.8689
	step [49/143], loss=147.8836
	step [50/143], loss=149.1203
	step [51/143], loss=147.7560
	step [52/143], loss=147.5844
	step [53/143], loss=147.9097
	step [54/143], loss=146.0366
	step [55/143], loss=144.4016
	step [56/143], loss=144.0365
	step [57/143], loss=144.0904
	step [58/143], loss=145.7345
	step [59/143], loss=142.9263
	step [60/143], loss=143.1508
	step [61/143], loss=141.6506
	step [62/143], loss=139.4098
	step [63/143], loss=140.2365
	step [64/143], loss=138.6223
	step [65/143], loss=140.5656
	step [66/143], loss=140.6183
	step [67/143], loss=138.6187
	step [68/143], loss=138.1090
	step [69/143], loss=138.9289
	step [70/143], loss=137.9145
	step [71/143], loss=137.6438
	step [72/143], loss=137.9381
	step [73/143], loss=137.9476
	step [74/143], loss=137.3422
	step [75/143], loss=133.5333
	step [76/143], loss=135.8694
	step [77/143], loss=135.3873
	step [78/143], loss=135.2548
	step [79/143], loss=135.1262
	step [80/143], loss=132.9697
	step [81/143], loss=132.9329
	step [82/143], loss=134.5418
	step [83/143], loss=133.8390
	step [84/143], loss=132.6719
	step [85/143], loss=133.1659
	step [86/143], loss=133.4560
	step [87/143], loss=131.2964
	step [88/143], loss=131.0443
	step [89/143], loss=130.2283
	step [90/143], loss=130.1922
	step [91/143], loss=130.0865
	step [92/143], loss=131.3197
	step [93/143], loss=128.5291
	step [94/143], loss=129.9399
	step [95/143], loss=129.4523
	step [96/143], loss=128.1337
	step [97/143], loss=129.5562
	step [98/143], loss=126.8245
	step [99/143], loss=125.8800
	step [100/143], loss=127.0148
	step [101/143], loss=127.1569
	step [102/143], loss=126.2568
	step [103/143], loss=127.6430
	step [104/143], loss=125.3918
	step [105/143], loss=125.9412
	step [106/143], loss=126.8823
	step [107/143], loss=123.3048
	step [108/143], loss=123.0940
	step [109/143], loss=124.8032
	step [110/143], loss=123.2725
	step [111/143], loss=125.2667
	step [112/143], loss=123.0651
	step [113/143], loss=122.7357
	step [114/143], loss=122.6590
	step [115/143], loss=122.2044
	step [116/143], loss=122.0171
	step [117/143], loss=121.7152
	step [118/143], loss=122.1249
	step [119/143], loss=122.6298
	step [120/143], loss=121.1455
	step [121/143], loss=119.8763
	step [122/143], loss=121.4700
	step [123/143], loss=120.8549
	step [124/143], loss=118.5821
	step [125/143], loss=118.2292
	step [126/143], loss=120.7558
	step [127/143], loss=118.9317
	step [128/143], loss=119.2448
	step [129/143], loss=120.9705
	step [130/143], loss=119.1577
	step [131/143], loss=119.7543
	step [132/143], loss=118.9101
	step [133/143], loss=117.4936
	step [134/143], loss=119.1808
	step [135/143], loss=116.4656
	step [136/143], loss=117.9284
	step [137/143], loss=116.0165
	step [138/143], loss=116.7821
	step [139/143], loss=117.3749
	step [140/143], loss=116.0285
	step [141/143], loss=114.6995
	step [142/143], loss=114.2367
	step [143/143], loss=94.5925
	Evaluating
	loss=0.4335, precision=0.1133, recall=0.9922, f1=0.2034
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/143], loss=115.2184
	step [2/143], loss=113.2527
	step [3/143], loss=116.6133
	step [4/143], loss=114.2977
	step [5/143], loss=115.2404
	step [6/143], loss=114.6784
	step [7/143], loss=115.5198
	step [8/143], loss=112.3801
	step [9/143], loss=113.6859
	step [10/143], loss=114.1821
	step [11/143], loss=113.3513
	step [12/143], loss=113.2190
	step [13/143], loss=111.7688
	step [14/143], loss=111.9117
	step [15/143], loss=111.0023
	step [16/143], loss=113.8102
	step [17/143], loss=112.3929
	step [18/143], loss=110.6465
	step [19/143], loss=111.2030
	step [20/143], loss=112.5559
	step [21/143], loss=110.2223
	step [22/143], loss=109.2027
	step [23/143], loss=110.3102
	step [24/143], loss=111.7625
	step [25/143], loss=110.1748
	step [26/143], loss=108.6844
	step [27/143], loss=110.5398
	step [28/143], loss=110.6225
	step [29/143], loss=108.4244
	step [30/143], loss=107.7520
	step [31/143], loss=106.6158
	step [32/143], loss=108.7532
	step [33/143], loss=109.1897
	step [34/143], loss=107.9531
	step [35/143], loss=108.4852
	step [36/143], loss=109.9471
	step [37/143], loss=107.6799
	step [38/143], loss=106.0696
	step [39/143], loss=106.9194
	step [40/143], loss=107.1717
	step [41/143], loss=105.5739
	step [42/143], loss=106.8426
	step [43/143], loss=107.9624
	step [44/143], loss=105.6816
	step [45/143], loss=105.8366
	step [46/143], loss=105.4137
	step [47/143], loss=106.1400
	step [48/143], loss=103.8189
	step [49/143], loss=103.9168
	step [50/143], loss=104.7953
	step [51/143], loss=104.7160
	step [52/143], loss=103.7620
	step [53/143], loss=104.2859
	step [54/143], loss=104.5605
	step [55/143], loss=105.4982
	step [56/143], loss=102.7656
	step [57/143], loss=101.8015
	step [58/143], loss=102.9388
	step [59/143], loss=101.7162
	step [60/143], loss=103.3153
	step [61/143], loss=104.8127
	step [62/143], loss=103.0524
	step [63/143], loss=102.8088
	step [64/143], loss=102.2724
	step [65/143], loss=101.1540
	step [66/143], loss=102.2651
	step [67/143], loss=101.2892
	step [68/143], loss=101.3322
	step [69/143], loss=101.9793
	step [70/143], loss=102.3585
	step [71/143], loss=100.5473
	step [72/143], loss=100.5813
	step [73/143], loss=100.9451
	step [74/143], loss=100.7219
	step [75/143], loss=100.1978
	step [76/143], loss=98.8668
	step [77/143], loss=101.8713
	step [78/143], loss=100.4168
	step [79/143], loss=99.2209
	step [80/143], loss=98.6770
	step [81/143], loss=99.3941
	step [82/143], loss=97.7828
	step [83/143], loss=99.6232
	step [84/143], loss=99.5115
	step [85/143], loss=97.4745
	step [86/143], loss=97.5756
	step [87/143], loss=98.1263
	step [88/143], loss=96.8877
	step [89/143], loss=96.5726
	step [90/143], loss=98.8035
	step [91/143], loss=97.2424
	step [92/143], loss=97.8251
	step [93/143], loss=98.0392
	step [94/143], loss=97.1404
	step [95/143], loss=96.2285
	step [96/143], loss=98.6243
	step [97/143], loss=94.5660
	step [98/143], loss=95.9788
	step [99/143], loss=96.1801
	step [100/143], loss=94.4876
	step [101/143], loss=94.2183
	step [102/143], loss=96.1452
	step [103/143], loss=97.3050
	step [104/143], loss=96.2573
	step [105/143], loss=93.3980
	step [106/143], loss=93.2449
	step [107/143], loss=95.1008
	step [108/143], loss=93.7691
	step [109/143], loss=95.9733
	step [110/143], loss=94.4356
	step [111/143], loss=94.1669
	step [112/143], loss=94.5515
	step [113/143], loss=93.3294
	step [114/143], loss=93.3950
	step [115/143], loss=92.1187
	step [116/143], loss=92.1920
	step [117/143], loss=93.8646
	step [118/143], loss=93.9914
	step [119/143], loss=92.0094
	step [120/143], loss=93.2413
	step [121/143], loss=93.1860
	step [122/143], loss=93.9178
	step [123/143], loss=92.3820
	step [124/143], loss=91.3987
	step [125/143], loss=91.5953
	step [126/143], loss=94.4284
	step [127/143], loss=91.9542
	step [128/143], loss=91.6164
	step [129/143], loss=91.2966
	step [130/143], loss=90.9992
	step [131/143], loss=90.0421
	step [132/143], loss=90.8586
	step [133/143], loss=92.2938
	step [134/143], loss=90.1522
	step [135/143], loss=89.3320
	step [136/143], loss=89.6021
	step [137/143], loss=90.3516
	step [138/143], loss=89.2123
	step [139/143], loss=88.0888
	step [140/143], loss=91.8876
	step [141/143], loss=88.8254
	step [142/143], loss=90.6830
	step [143/143], loss=74.2506
	Evaluating
	loss=0.3281, precision=0.1883, recall=0.9922, f1=0.3165
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/143], loss=89.6207
	step [2/143], loss=87.3745
	step [3/143], loss=90.6493
	step [4/143], loss=90.3716
	step [5/143], loss=88.0021
	step [6/143], loss=88.7351
	step [7/143], loss=89.0939
	step [8/143], loss=87.5081
	step [9/143], loss=86.9540
	step [10/143], loss=87.6514
	step [11/143], loss=86.5739
	step [12/143], loss=88.9561
	step [13/143], loss=87.7616
	step [14/143], loss=87.5397
	step [15/143], loss=85.3862
	step [16/143], loss=86.9411
	step [17/143], loss=85.7878
	step [18/143], loss=86.5258
	step [19/143], loss=89.2163
	step [20/143], loss=86.3803
	step [21/143], loss=85.5156
	step [22/143], loss=84.1622
	step [23/143], loss=85.3421
	step [24/143], loss=84.0925
	step [25/143], loss=85.0305
	step [26/143], loss=85.1627
	step [27/143], loss=83.5366
	step [28/143], loss=82.8851
	step [29/143], loss=85.7234
	step [30/143], loss=83.6144
	step [31/143], loss=86.1504
	step [32/143], loss=83.1389
	step [33/143], loss=84.3417
	step [34/143], loss=84.1654
	step [35/143], loss=82.9567
	step [36/143], loss=83.7960
	step [37/143], loss=82.8818
	step [38/143], loss=84.9914
	step [39/143], loss=84.9429
	step [40/143], loss=83.3959
	step [41/143], loss=82.1401
	step [42/143], loss=83.3288
	step [43/143], loss=84.3920
	step [44/143], loss=83.4625
	step [45/143], loss=82.6162
	step [46/143], loss=82.0043
	step [47/143], loss=81.1739
	step [48/143], loss=82.9077
	step [49/143], loss=85.2894
	step [50/143], loss=80.7747
	step [51/143], loss=81.5453
	step [52/143], loss=83.2856
	step [53/143], loss=81.2823
	step [54/143], loss=79.2571
	step [55/143], loss=82.8594
	step [56/143], loss=81.3152
	step [57/143], loss=79.4737
	step [58/143], loss=80.9301
	step [59/143], loss=82.1849
	step [60/143], loss=81.9715
	step [61/143], loss=82.4479
	step [62/143], loss=82.8498
	step [63/143], loss=79.8893
	step [64/143], loss=79.4467
	step [65/143], loss=79.7357
	step [66/143], loss=81.3113
	step [67/143], loss=79.9399
	step [68/143], loss=83.5269
	step [69/143], loss=79.5156
	step [70/143], loss=80.0237
	step [71/143], loss=80.4501
	step [72/143], loss=78.2102
	step [73/143], loss=79.0121
	step [74/143], loss=78.5174
	step [75/143], loss=78.3252
	step [76/143], loss=78.7617
	step [77/143], loss=78.8970
	step [78/143], loss=80.6629
	step [79/143], loss=77.3334
	step [80/143], loss=79.1566
	step [81/143], loss=77.4527
	step [82/143], loss=78.3041
	step [83/143], loss=78.3733
	step [84/143], loss=81.8565
	step [85/143], loss=77.4070
	step [86/143], loss=79.0413
	step [87/143], loss=80.4590
	step [88/143], loss=77.4806
	step [89/143], loss=76.3146
	step [90/143], loss=77.2753
	step [91/143], loss=78.5165
	step [92/143], loss=76.1924
	step [93/143], loss=76.0799
	step [94/143], loss=76.8817
	step [95/143], loss=75.9512
	step [96/143], loss=75.5774
	step [97/143], loss=74.0444
	step [98/143], loss=76.7623
	step [99/143], loss=78.3121
	step [100/143], loss=76.0056
	step [101/143], loss=77.8949
	step [102/143], loss=76.1760
	step [103/143], loss=75.9272
	step [104/143], loss=76.4322
	step [105/143], loss=75.8956
	step [106/143], loss=75.0367
	step [107/143], loss=74.6802
	step [108/143], loss=73.4935
	step [109/143], loss=76.3084
	step [110/143], loss=74.2585
	step [111/143], loss=73.6001
	step [112/143], loss=74.7364
	step [113/143], loss=74.9394
	step [114/143], loss=74.0884
	step [115/143], loss=73.1336
	step [116/143], loss=72.2509
	step [117/143], loss=74.4379
	step [118/143], loss=73.7659
	step [119/143], loss=74.5360
	step [120/143], loss=74.9309
	step [121/143], loss=75.0856
	step [122/143], loss=73.2121
	step [123/143], loss=73.4403
	step [124/143], loss=73.7063
	step [125/143], loss=74.2274
	step [126/143], loss=72.6920
	step [127/143], loss=70.7408
	step [128/143], loss=73.3582
	step [129/143], loss=71.2268
	step [130/143], loss=72.4420
	step [131/143], loss=72.0141
	step [132/143], loss=72.1389
	step [133/143], loss=71.3150
	step [134/143], loss=71.1804
	step [135/143], loss=71.0624
	step [136/143], loss=72.1805
	step [137/143], loss=71.3859
	step [138/143], loss=73.9378
	step [139/143], loss=73.1830
	step [140/143], loss=72.3791
	step [141/143], loss=71.4408
	step [142/143], loss=73.4303
	step [143/143], loss=59.0328
	Evaluating
	loss=0.2559, precision=0.2093, recall=0.9915, f1=0.3456
saving model as: 1_saved_model.pth
Training epoch 4
	step [1/143], loss=71.7199
	step [2/143], loss=70.1843
	step [3/143], loss=70.3223
	step [4/143], loss=70.6815
	step [5/143], loss=68.9263
	step [6/143], loss=69.4845
	step [7/143], loss=72.2570
	step [8/143], loss=73.0912
	step [9/143], loss=70.8779
	step [10/143], loss=70.2657
	step [11/143], loss=71.4171
	step [12/143], loss=71.2097
	step [13/143], loss=68.9657
	step [14/143], loss=71.3253
	step [15/143], loss=68.0179
	step [16/143], loss=69.4990
	step [17/143], loss=69.5016
	step [18/143], loss=70.8484
	step [19/143], loss=70.8465
	step [20/143], loss=70.3435
	step [21/143], loss=69.4138
	step [22/143], loss=67.5799
	step [23/143], loss=67.9801
	step [24/143], loss=67.9761
	step [25/143], loss=67.6893
	step [26/143], loss=70.5041
	step [27/143], loss=70.8784
	step [28/143], loss=66.2164
	step [29/143], loss=68.6815
	step [30/143], loss=68.9195
	step [31/143], loss=68.8663
	step [32/143], loss=65.6469
	step [33/143], loss=68.0256
	step [34/143], loss=68.6790
	step [35/143], loss=68.5188
	step [36/143], loss=69.6932
	step [37/143], loss=69.3299
	step [38/143], loss=66.8725
	step [39/143], loss=67.6993
	step [40/143], loss=66.7593
	step [41/143], loss=68.9463
	step [42/143], loss=67.4015
	step [43/143], loss=65.8448
	step [44/143], loss=66.3977
	step [45/143], loss=67.2887
	step [46/143], loss=67.2837
	step [47/143], loss=65.9018
	step [48/143], loss=65.6956
	step [49/143], loss=65.7583
	step [50/143], loss=68.6223
	step [51/143], loss=65.7537
	step [52/143], loss=66.9731
	step [53/143], loss=65.4896
	step [54/143], loss=64.9259
	step [55/143], loss=66.3388
	step [56/143], loss=66.5207
	step [57/143], loss=65.4838
	step [58/143], loss=64.7795
	step [59/143], loss=64.9449
	step [60/143], loss=64.3090
	step [61/143], loss=64.1718
	step [62/143], loss=63.7431
	step [63/143], loss=64.8854
	step [64/143], loss=66.1329
	step [65/143], loss=64.9072
	step [66/143], loss=62.7256
	step [67/143], loss=64.0300
	step [68/143], loss=64.8811
	step [69/143], loss=64.4185
	step [70/143], loss=65.0215
	step [71/143], loss=65.0736
	step [72/143], loss=63.5716
	step [73/143], loss=63.2504
	step [74/143], loss=62.8344
	step [75/143], loss=66.5544
	step [76/143], loss=63.9822
	step [77/143], loss=63.5609
	step [78/143], loss=63.1592
	step [79/143], loss=61.5954
	step [80/143], loss=62.7367
	step [81/143], loss=64.1744
	step [82/143], loss=63.8011
	step [83/143], loss=62.8275
	step [84/143], loss=61.7802
	step [85/143], loss=62.2106
	step [86/143], loss=62.9492
	step [87/143], loss=63.4672
	step [88/143], loss=62.1244
	step [89/143], loss=64.5254
	step [90/143], loss=62.3213
	step [91/143], loss=61.4513
	step [92/143], loss=63.3927
	step [93/143], loss=62.8966
	step [94/143], loss=61.9048
	step [95/143], loss=62.9046
	step [96/143], loss=61.0247
	step [97/143], loss=62.6081
	step [98/143], loss=60.9269
	step [99/143], loss=61.0458
	step [100/143], loss=61.9350
	step [101/143], loss=61.3587
	step [102/143], loss=61.0130
	step [103/143], loss=63.3071
	step [104/143], loss=61.3215
	step [105/143], loss=61.2884
	step [106/143], loss=60.8351
	step [107/143], loss=61.0188
	step [108/143], loss=61.4124
	step [109/143], loss=60.7219
	step [110/143], loss=60.4701
	step [111/143], loss=60.5994
	step [112/143], loss=62.0824
	step [113/143], loss=62.4842
	step [114/143], loss=59.5217
	step [115/143], loss=59.8236
	step [116/143], loss=60.1512
	step [117/143], loss=58.8712
	step [118/143], loss=61.5329
	step [119/143], loss=58.5874
	step [120/143], loss=59.5509
	step [121/143], loss=58.7559
	step [122/143], loss=59.6540
	step [123/143], loss=61.0919
	step [124/143], loss=61.5304
	step [125/143], loss=59.1928
	step [126/143], loss=59.4585
	step [127/143], loss=60.6379
	step [128/143], loss=60.1173
	step [129/143], loss=59.2060
	step [130/143], loss=56.1360
	step [131/143], loss=58.6590
	step [132/143], loss=59.4979
	step [133/143], loss=57.7464
	step [134/143], loss=57.6525
	step [135/143], loss=58.1565
	step [136/143], loss=59.1143
	step [137/143], loss=57.7086
	step [138/143], loss=57.4927
	step [139/143], loss=57.1288
	step [140/143], loss=57.2965
	step [141/143], loss=56.7562
	step [142/143], loss=58.2728
	step [143/143], loss=48.3613
	Evaluating
	loss=0.2005, precision=0.2337, recall=0.9905, f1=0.3782
saving model as: 1_saved_model.pth
Training epoch 5
	step [1/143], loss=57.2361
	step [2/143], loss=59.2683
	step [3/143], loss=56.4290
	step [4/143], loss=58.0203
	step [5/143], loss=57.1052
	step [6/143], loss=58.1620
	step [7/143], loss=58.0551
	step [8/143], loss=56.3814
	step [9/143], loss=56.6398
	step [10/143], loss=56.2643
	step [11/143], loss=57.6943
	step [12/143], loss=58.5547
	step [13/143], loss=57.5452
	step [14/143], loss=57.2623
	step [15/143], loss=56.4595
	step [16/143], loss=56.2544
	step [17/143], loss=59.2452
	step [18/143], loss=55.4980
	step [19/143], loss=57.2284
	step [20/143], loss=56.7662
	step [21/143], loss=55.9236
	step [22/143], loss=55.2291
	step [23/143], loss=55.1505
	step [24/143], loss=55.4333
	step [25/143], loss=58.2823
	step [26/143], loss=57.2274
	step [27/143], loss=57.9078
	step [28/143], loss=54.5254
	step [29/143], loss=56.5707
	step [30/143], loss=53.9734
	step [31/143], loss=54.0850
	step [32/143], loss=55.3718
	step [33/143], loss=54.0645
	step [34/143], loss=55.5185
	step [35/143], loss=54.2654
	step [36/143], loss=55.7524
	step [37/143], loss=52.6837
	step [38/143], loss=53.7477
	step [39/143], loss=56.0433
	step [40/143], loss=55.2761
	step [41/143], loss=53.9093
	step [42/143], loss=54.8477
	step [43/143], loss=55.0922
	step [44/143], loss=53.8749
	step [45/143], loss=54.2103
	step [46/143], loss=51.5806
	step [47/143], loss=53.8693
	step [48/143], loss=53.0938
	step [49/143], loss=56.6263
	step [50/143], loss=53.7023
	step [51/143], loss=54.7990
	step [52/143], loss=53.9545
	step [53/143], loss=53.8470
	step [54/143], loss=52.2654
	step [55/143], loss=53.7381
	step [56/143], loss=54.0583
	step [57/143], loss=53.6935
	step [58/143], loss=52.8222
	step [59/143], loss=53.9943
	step [60/143], loss=52.6410
	step [61/143], loss=52.0224
	step [62/143], loss=53.9299
	step [63/143], loss=52.0862
	step [64/143], loss=52.6620
	step [65/143], loss=52.8167
	step [66/143], loss=52.7391
	step [67/143], loss=54.4218
	step [68/143], loss=54.6561
	step [69/143], loss=54.2034
	step [70/143], loss=50.5199
	step [71/143], loss=51.2875
	step [72/143], loss=52.1862
	step [73/143], loss=50.8347
	step [74/143], loss=51.2447
	step [75/143], loss=51.4045
	step [76/143], loss=54.0408
	step [77/143], loss=52.4077
	step [78/143], loss=52.8794
	step [79/143], loss=51.8120
	step [80/143], loss=53.4641
	step [81/143], loss=52.3842
	step [82/143], loss=51.7325
	step [83/143], loss=49.9141
	step [84/143], loss=50.5130
	step [85/143], loss=50.7459
	step [86/143], loss=55.9680
	step [87/143], loss=52.9203
	step [88/143], loss=54.3017
	step [89/143], loss=51.4684
	step [90/143], loss=50.8475
	step [91/143], loss=53.7509
	step [92/143], loss=51.6536
	step [93/143], loss=52.7323
	step [94/143], loss=51.6563
	step [95/143], loss=50.8343
	step [96/143], loss=51.2926
	step [97/143], loss=50.5665
	step [98/143], loss=52.7301
	step [99/143], loss=49.5776
	step [100/143], loss=49.6846
	step [101/143], loss=49.4239
	step [102/143], loss=49.2450
	step [103/143], loss=48.4438
	step [104/143], loss=52.9655
	step [105/143], loss=47.9039
	step [106/143], loss=50.1643
	step [107/143], loss=50.7599
	step [108/143], loss=50.5294
	step [109/143], loss=49.0896
	step [110/143], loss=49.2576
	step [111/143], loss=50.2260
	step [112/143], loss=49.6212
	step [113/143], loss=49.2870
	step [114/143], loss=49.7213
	step [115/143], loss=48.2375
	step [116/143], loss=51.8927
	step [117/143], loss=51.0243
	step [118/143], loss=49.1819
	step [119/143], loss=52.5866
	step [120/143], loss=48.6246
	step [121/143], loss=51.9824
	step [122/143], loss=49.2670
	step [123/143], loss=50.3678
	step [124/143], loss=48.6628
	step [125/143], loss=50.1502
	step [126/143], loss=48.6671
	step [127/143], loss=48.8000
	step [128/143], loss=46.8977
	step [129/143], loss=49.5787
	step [130/143], loss=47.0663
	step [131/143], loss=48.2050
	step [132/143], loss=47.4463
	step [133/143], loss=48.7599
	step [134/143], loss=48.5519
	step [135/143], loss=47.3915
	step [136/143], loss=49.6598
	step [137/143], loss=49.5762
	step [138/143], loss=46.6040
	step [139/143], loss=47.1574
	step [140/143], loss=46.9215
	step [141/143], loss=47.7651
	step [142/143], loss=47.3841
	step [143/143], loss=39.7057
	Evaluating
	loss=0.1582, precision=0.2416, recall=0.9900, f1=0.3884
saving model as: 1_saved_model.pth
Training epoch 6
	step [1/143], loss=47.1814
	step [2/143], loss=47.5409
	step [3/143], loss=46.7925
	step [4/143], loss=45.9050
	step [5/143], loss=47.9929
	step [6/143], loss=47.6062
	step [7/143], loss=49.9128
	step [8/143], loss=46.1237
	step [9/143], loss=46.0661
	step [10/143], loss=47.9411
	step [11/143], loss=46.1052
	step [12/143], loss=48.0357
	step [13/143], loss=46.7222
	step [14/143], loss=46.1561
	step [15/143], loss=48.1994
	step [16/143], loss=47.8708
	step [17/143], loss=45.1535
	step [18/143], loss=48.0087
	step [19/143], loss=45.2693
	step [20/143], loss=49.3273
	step [21/143], loss=46.2215
	step [22/143], loss=46.3000
	step [23/143], loss=47.0631
	step [24/143], loss=46.8950
	step [25/143], loss=47.0735
	step [26/143], loss=45.8232
	step [27/143], loss=45.9333
	step [28/143], loss=44.3900
	step [29/143], loss=45.0071
	step [30/143], loss=44.7576
	step [31/143], loss=46.5667
	step [32/143], loss=44.3766
	step [33/143], loss=44.7170
	step [34/143], loss=44.2786
	step [35/143], loss=45.4828
	step [36/143], loss=44.2240
	step [37/143], loss=46.9000
	step [38/143], loss=45.4264
	step [39/143], loss=45.0901
	step [40/143], loss=44.2025
	step [41/143], loss=43.6166
	step [42/143], loss=44.9619
	step [43/143], loss=44.3412
	step [44/143], loss=42.9698
	step [45/143], loss=44.1568
	step [46/143], loss=44.7633
	step [47/143], loss=45.1366
	step [48/143], loss=43.5635
	step [49/143], loss=46.6134
	step [50/143], loss=44.2865
	step [51/143], loss=44.8562
	step [52/143], loss=42.9956
	step [53/143], loss=45.6647
	step [54/143], loss=43.7867
	step [55/143], loss=47.3966
	step [56/143], loss=45.9087
	step [57/143], loss=44.7887
	step [58/143], loss=44.7080
	step [59/143], loss=41.7001
	step [60/143], loss=43.5153
	step [61/143], loss=43.4195
	step [62/143], loss=44.6390
	step [63/143], loss=45.1536
	step [64/143], loss=43.8174
	step [65/143], loss=42.4124
	step [66/143], loss=47.0558
	step [67/143], loss=45.5465
	step [68/143], loss=44.0505
	step [69/143], loss=41.6637
	step [70/143], loss=43.4035
	step [71/143], loss=44.3895
	step [72/143], loss=44.6604
	step [73/143], loss=41.3686
	step [74/143], loss=44.2987
	step [75/143], loss=42.6049
	step [76/143], loss=41.2259
	step [77/143], loss=43.8716
	step [78/143], loss=43.5103
	step [79/143], loss=43.3825
	step [80/143], loss=42.5548
	step [81/143], loss=45.5334
	step [82/143], loss=42.7251
	step [83/143], loss=44.5163
	step [84/143], loss=43.4175
	step [85/143], loss=43.9165
	step [86/143], loss=41.1097
	step [87/143], loss=44.6530
	step [88/143], loss=43.6553
	step [89/143], loss=45.5217
	step [90/143], loss=43.8559
	step [91/143], loss=41.9187
	step [92/143], loss=41.2612
	step [93/143], loss=42.1017
	step [94/143], loss=43.8988
	step [95/143], loss=41.1435
	step [96/143], loss=40.1950
	step [97/143], loss=41.0922
	step [98/143], loss=41.9845
	step [99/143], loss=40.3255
	step [100/143], loss=42.5708
	step [101/143], loss=42.7027
	step [102/143], loss=40.7366
	step [103/143], loss=40.8930
	step [104/143], loss=40.9017
	step [105/143], loss=40.9464
	step [106/143], loss=41.3766
	step [107/143], loss=40.0454
	step [108/143], loss=40.6843
	step [109/143], loss=41.5714
	step [110/143], loss=39.6163
	step [111/143], loss=41.9962
	step [112/143], loss=41.1871
	step [113/143], loss=40.7114
	step [114/143], loss=40.7220
	step [115/143], loss=40.6003
	step [116/143], loss=42.1980
	step [117/143], loss=40.5887
	step [118/143], loss=40.3791
	step [119/143], loss=43.1222
	step [120/143], loss=40.3197
	step [121/143], loss=39.1265
	step [122/143], loss=42.0702
	step [123/143], loss=42.4060
	step [124/143], loss=39.5022
	step [125/143], loss=43.2281
	step [126/143], loss=44.0397
	step [127/143], loss=41.0656
	step [128/143], loss=40.2114
	step [129/143], loss=40.9529
	step [130/143], loss=39.5571
	step [131/143], loss=39.4664
	step [132/143], loss=40.4705
	step [133/143], loss=44.0725
	step [134/143], loss=38.2791
	step [135/143], loss=39.2986
	step [136/143], loss=42.1795
	step [137/143], loss=40.2674
	step [138/143], loss=39.6235
	step [139/143], loss=41.5649
	step [140/143], loss=38.6117
	step [141/143], loss=38.6034
	step [142/143], loss=38.3529
	step [143/143], loss=33.5178
	Evaluating
	loss=0.1315, precision=0.2304, recall=0.9906, f1=0.3738
Training epoch 7
	step [1/143], loss=39.5948
	step [2/143], loss=39.3263
	step [3/143], loss=39.4835
	step [4/143], loss=40.1298
	step [5/143], loss=41.6794
	step [6/143], loss=39.1478
	step [7/143], loss=41.2601
	step [8/143], loss=42.0746
	step [9/143], loss=38.8347
	step [10/143], loss=41.2760
	step [11/143], loss=39.6160
	step [12/143], loss=40.4224
	step [13/143], loss=39.5260
	step [14/143], loss=39.0429
	step [15/143], loss=37.5560
	step [16/143], loss=41.2400
	step [17/143], loss=38.1832
	step [18/143], loss=40.0387
	step [19/143], loss=39.6476
	step [20/143], loss=39.8119
	step [21/143], loss=38.9203
	step [22/143], loss=40.1427
	step [23/143], loss=39.1021
	step [24/143], loss=39.6204
	step [25/143], loss=39.1225
	step [26/143], loss=38.9446
	step [27/143], loss=40.8401
	step [28/143], loss=38.1715
	step [29/143], loss=37.9232
	step [30/143], loss=40.9472
	step [31/143], loss=37.8544
	step [32/143], loss=38.9892
	step [33/143], loss=37.1582
	step [34/143], loss=37.6845
	step [35/143], loss=38.5572
	step [36/143], loss=40.1793
	step [37/143], loss=37.6769
	step [38/143], loss=38.2334
	step [39/143], loss=36.8549
	step [40/143], loss=37.8610
	step [41/143], loss=40.1729
	step [42/143], loss=37.1592
	step [43/143], loss=37.6343
	step [44/143], loss=37.8605
	step [45/143], loss=36.9011
	step [46/143], loss=36.3701
	step [47/143], loss=36.8832
	step [48/143], loss=39.2172
	step [49/143], loss=37.7357
	step [50/143], loss=36.7238
	step [51/143], loss=36.5745
	step [52/143], loss=36.8132
	step [53/143], loss=35.8395
	step [54/143], loss=40.5606
	step [55/143], loss=39.4576
	step [56/143], loss=36.9421
	step [57/143], loss=40.0439
	step [58/143], loss=36.7062
	step [59/143], loss=37.2547
	step [60/143], loss=37.2978
	step [61/143], loss=37.2514
	step [62/143], loss=37.5791
	step [63/143], loss=38.2011
	step [64/143], loss=34.4194
	step [65/143], loss=38.1686
	step [66/143], loss=37.2585
	step [67/143], loss=36.9609
	step [68/143], loss=38.5027
	step [69/143], loss=35.5097
	step [70/143], loss=39.3757
	step [71/143], loss=38.1404
	step [72/143], loss=35.9666
	step [73/143], loss=35.6358
	step [74/143], loss=35.4793
	step [75/143], loss=35.2093
	step [76/143], loss=35.8143
	step [77/143], loss=38.2852
	step [78/143], loss=37.7026
	step [79/143], loss=36.5128
	step [80/143], loss=38.3167
	step [81/143], loss=34.9509
	step [82/143], loss=40.2884
	step [83/143], loss=36.2404
	step [84/143], loss=39.1949
	step [85/143], loss=34.9810
	step [86/143], loss=38.3674
	step [87/143], loss=35.6283
	step [88/143], loss=37.9700
	step [89/143], loss=36.3776
	step [90/143], loss=34.1859
	step [91/143], loss=34.5941
	step [92/143], loss=36.8338
	step [93/143], loss=35.5209
	step [94/143], loss=37.4793
	step [95/143], loss=35.3180
	step [96/143], loss=34.7762
	step [97/143], loss=36.4619
	step [98/143], loss=37.0559
	step [99/143], loss=36.0034
	step [100/143], loss=36.0333
	step [101/143], loss=35.4774
	step [102/143], loss=36.3968
	step [103/143], loss=36.5049
	step [104/143], loss=34.2606
	step [105/143], loss=36.2090
	step [106/143], loss=36.3281
	step [107/143], loss=35.9828
	step [108/143], loss=35.3807
	step [109/143], loss=34.4734
	step [110/143], loss=35.9958
	step [111/143], loss=37.1462
	step [112/143], loss=35.4203
	step [113/143], loss=32.9579
	step [114/143], loss=35.6139
	step [115/143], loss=34.5871
	step [116/143], loss=33.4843
	step [117/143], loss=33.5710
	step [118/143], loss=34.9712
	step [119/143], loss=35.6099
	step [120/143], loss=36.3196
	step [121/143], loss=37.2947
	step [122/143], loss=36.6699
	step [123/143], loss=31.8421
	step [124/143], loss=34.0738
	step [125/143], loss=35.3158
	step [126/143], loss=35.3061
	step [127/143], loss=35.4999
	step [128/143], loss=35.0845
	step [129/143], loss=34.6052
	step [130/143], loss=35.6052
	step [131/143], loss=34.9928
	step [132/143], loss=33.9010
	step [133/143], loss=34.0216
	step [134/143], loss=35.1903
	step [135/143], loss=34.1123
	step [136/143], loss=33.6077
	step [137/143], loss=35.1781
	step [138/143], loss=35.8043
	step [139/143], loss=33.1033
	step [140/143], loss=34.2836
	step [141/143], loss=33.3717
	step [142/143], loss=33.1523
	step [143/143], loss=28.4378
	Evaluating
	loss=0.1129, precision=0.1945, recall=0.9925, f1=0.3252
Training epoch 8
	step [1/143], loss=36.2983
	step [2/143], loss=32.3400
	step [3/143], loss=32.9580
	step [4/143], loss=35.3152
	step [5/143], loss=35.1120
	step [6/143], loss=33.3653
	step [7/143], loss=33.4835
	step [8/143], loss=35.0301
	step [9/143], loss=34.4449
	step [10/143], loss=33.3553
	step [11/143], loss=33.8100
	step [12/143], loss=33.4506
	step [13/143], loss=34.7252
	step [14/143], loss=34.5185
	step [15/143], loss=34.4232
	step [16/143], loss=33.7688
	step [17/143], loss=32.5019
	step [18/143], loss=31.7205
	step [19/143], loss=35.6252
	step [20/143], loss=32.6563
	step [21/143], loss=35.4624
	step [22/143], loss=33.4031
	step [23/143], loss=34.0405
	step [24/143], loss=33.4339
	step [25/143], loss=33.7323
	step [26/143], loss=31.9181
	step [27/143], loss=32.4226
	step [28/143], loss=34.0307
	step [29/143], loss=33.2752
	step [30/143], loss=33.6232
	step [31/143], loss=34.4849
	step [32/143], loss=33.4805
	step [33/143], loss=32.7666
	step [34/143], loss=32.6597
	step [35/143], loss=34.2506
	step [36/143], loss=34.9236
	step [37/143], loss=33.7452
	step [38/143], loss=31.9778
	step [39/143], loss=31.9538
	step [40/143], loss=32.1081
	step [41/143], loss=31.7011
	step [42/143], loss=33.2638
	step [43/143], loss=31.4936
	step [44/143], loss=34.4111
	step [45/143], loss=33.1412
	step [46/143], loss=32.7734
	step [47/143], loss=35.0367
	step [48/143], loss=30.7621
	step [49/143], loss=32.2613
	step [50/143], loss=35.1163
	step [51/143], loss=32.1004
	step [52/143], loss=31.7698
	step [53/143], loss=33.7097
	step [54/143], loss=32.3848
	step [55/143], loss=33.2814
	step [56/143], loss=33.0228
	step [57/143], loss=34.0079
	step [58/143], loss=30.7274
	step [59/143], loss=34.5877
	step [60/143], loss=36.4031
	step [61/143], loss=32.3828
	step [62/143], loss=33.3743
	step [63/143], loss=33.5654
	step [64/143], loss=32.4809
	step [65/143], loss=32.7514
	step [66/143], loss=32.5607
	step [67/143], loss=35.0300
	step [68/143], loss=32.2263
	step [69/143], loss=33.9309
	step [70/143], loss=30.8848
	step [71/143], loss=32.4250
	step [72/143], loss=31.3081
	step [73/143], loss=31.8385
	step [74/143], loss=33.2185
	step [75/143], loss=31.4112
	step [76/143], loss=29.8556
	step [77/143], loss=30.0246
	step [78/143], loss=31.0662
	step [79/143], loss=30.7027
	step [80/143], loss=34.0310
	step [81/143], loss=32.5614
	step [82/143], loss=31.4283
	step [83/143], loss=32.1316
	step [84/143], loss=31.5855
	step [85/143], loss=30.4874
	step [86/143], loss=31.5727
	step [87/143], loss=31.4275
	step [88/143], loss=31.9733
	step [89/143], loss=32.3441
	step [90/143], loss=31.9149
	step [91/143], loss=30.0495
	step [92/143], loss=31.2619
	step [93/143], loss=32.3028
	step [94/143], loss=31.2757
	step [95/143], loss=31.6210
	step [96/143], loss=32.3037
	step [97/143], loss=33.0532
	step [98/143], loss=30.9466
	step [99/143], loss=32.0169
	step [100/143], loss=32.7397
	step [101/143], loss=30.4789
	step [102/143], loss=29.6204
	step [103/143], loss=32.7246
	step [104/143], loss=30.8362
	step [105/143], loss=30.5004
	step [106/143], loss=32.8888
	step [107/143], loss=32.6934
	step [108/143], loss=30.4804
	step [109/143], loss=30.5367
	step [110/143], loss=29.2906
	step [111/143], loss=30.8824
	step [112/143], loss=30.4583
	step [113/143], loss=31.5613
	step [114/143], loss=29.9944
	step [115/143], loss=30.2458
	step [116/143], loss=29.4162
	step [117/143], loss=31.0681
	step [118/143], loss=28.1600
	step [119/143], loss=32.0940
	step [120/143], loss=30.3118
	step [121/143], loss=31.0800
	step [122/143], loss=32.0288
	step [123/143], loss=31.4768
	step [124/143], loss=30.0283
	step [125/143], loss=29.9933
	step [126/143], loss=30.8654
	step [127/143], loss=28.9866
	step [128/143], loss=29.8963
	step [129/143], loss=29.5753
	step [130/143], loss=30.8046
	step [131/143], loss=29.1405
	step [132/143], loss=29.8946
	step [133/143], loss=30.0076
	step [134/143], loss=29.5571
	step [135/143], loss=31.2602
	step [136/143], loss=30.3652
	step [137/143], loss=28.5211
	step [138/143], loss=29.2935
	step [139/143], loss=30.7854
	step [140/143], loss=30.5675
	step [141/143], loss=29.8083
	step [142/143], loss=31.3742
	step [143/143], loss=24.7043
	Evaluating
	loss=0.0961, precision=0.2172, recall=0.9915, f1=0.3564
Training epoch 9
	step [1/143], loss=30.8578
	step [2/143], loss=28.4917
	step [3/143], loss=29.8719
	step [4/143], loss=30.9958
	step [5/143], loss=27.0070
	step [6/143], loss=28.0380
	step [7/143], loss=27.9518
	step [8/143], loss=33.0226
	step [9/143], loss=29.7943
	step [10/143], loss=29.3291
	step [11/143], loss=29.1004
	step [12/143], loss=27.6472
	step [13/143], loss=29.5632
	step [14/143], loss=29.0891
	step [15/143], loss=31.7558
	step [16/143], loss=30.9568
	step [17/143], loss=29.8563
	step [18/143], loss=30.7654
	step [19/143], loss=30.9481
	step [20/143], loss=30.4787
	step [21/143], loss=30.5343
	step [22/143], loss=28.7196
	step [23/143], loss=30.9472
	step [24/143], loss=29.5401
	step [25/143], loss=29.5111
	step [26/143], loss=30.6996
	step [27/143], loss=27.9882
	step [28/143], loss=29.5498
	step [29/143], loss=31.2408
	step [30/143], loss=33.3036
	step [31/143], loss=30.6823
	step [32/143], loss=29.8981
	step [33/143], loss=28.2639
	step [34/143], loss=29.1346
	step [35/143], loss=28.9198
	step [36/143], loss=30.0854
	step [37/143], loss=30.4158
	step [38/143], loss=29.4868
	step [39/143], loss=28.7932
	step [40/143], loss=27.8611
	step [41/143], loss=30.7280
	step [42/143], loss=29.4381
	step [43/143], loss=28.7197
	step [44/143], loss=28.4767
	step [45/143], loss=29.2562
	step [46/143], loss=30.2026
	step [47/143], loss=27.8033
	step [48/143], loss=29.5180
	step [49/143], loss=27.9932
	step [50/143], loss=29.8360
	step [51/143], loss=28.6404
	step [52/143], loss=31.6248
	step [53/143], loss=31.7160
	step [54/143], loss=27.3179
	step [55/143], loss=28.1636
	step [56/143], loss=29.3109
	step [57/143], loss=27.4537
	step [58/143], loss=28.7018
	step [59/143], loss=29.9130
	step [60/143], loss=28.8739
	step [61/143], loss=26.5892
	step [62/143], loss=28.9990
	step [63/143], loss=27.0353
	step [64/143], loss=26.1586
	step [65/143], loss=28.8869
	step [66/143], loss=30.4825
	step [67/143], loss=26.5721
	step [68/143], loss=29.8328
	step [69/143], loss=28.0341
	step [70/143], loss=28.6532
	step [71/143], loss=26.8717
	step [72/143], loss=29.2866
	step [73/143], loss=29.2357
	step [74/143], loss=27.3868
	step [75/143], loss=29.1580
	step [76/143], loss=28.9016
	step [77/143], loss=27.5544
	step [78/143], loss=27.7401
	step [79/143], loss=27.8226
	step [80/143], loss=29.6010
	step [81/143], loss=29.8146
	step [82/143], loss=27.9206
	step [83/143], loss=28.3535
	step [84/143], loss=27.9063
	step [85/143], loss=27.4713
	step [86/143], loss=28.4957
	step [87/143], loss=27.1766
	step [88/143], loss=28.0881
	step [89/143], loss=26.3140
	step [90/143], loss=29.1206
	step [91/143], loss=27.8903
	step [92/143], loss=27.2380
	step [93/143], loss=29.0596
	step [94/143], loss=26.4189
	step [95/143], loss=27.7050
	step [96/143], loss=28.6105
	step [97/143], loss=26.6504
	step [98/143], loss=28.6472
	step [99/143], loss=27.0568
	step [100/143], loss=27.7178
	step [101/143], loss=27.9924
	step [102/143], loss=27.9939
	step [103/143], loss=27.1423
	step [104/143], loss=28.2614
	step [105/143], loss=29.3651
	step [106/143], loss=29.3782
	step [107/143], loss=27.9707
	step [108/143], loss=27.7612
	step [109/143], loss=28.3974
	step [110/143], loss=27.5345
	step [111/143], loss=27.7616
	step [112/143], loss=28.7219
	step [113/143], loss=28.2759
	step [114/143], loss=27.6611
	step [115/143], loss=28.6753
	step [116/143], loss=26.7237
	step [117/143], loss=27.8819
	step [118/143], loss=28.4741
	step [119/143], loss=26.0679
	step [120/143], loss=28.6706
	step [121/143], loss=27.1637
	step [122/143], loss=27.8895
	step [123/143], loss=27.5533
	step [124/143], loss=29.7364
	step [125/143], loss=27.7817
	step [126/143], loss=28.0182
	step [127/143], loss=25.6130
	step [128/143], loss=27.2889
	step [129/143], loss=28.0176
	step [130/143], loss=27.0527
	step [131/143], loss=28.6103
	step [132/143], loss=25.6057
	step [133/143], loss=26.4601
	step [134/143], loss=27.2980
	step [135/143], loss=28.0569
	step [136/143], loss=26.9639
	step [137/143], loss=26.4984
	step [138/143], loss=28.9038
	step [139/143], loss=27.0371
	step [140/143], loss=27.7332
	step [141/143], loss=25.5846
	step [142/143], loss=26.2858
	step [143/143], loss=22.0762
	Evaluating
	loss=0.0802, precision=0.2310, recall=0.9899, f1=0.3746
Training epoch 10
	step [1/143], loss=26.0525
	step [2/143], loss=25.4818
	step [3/143], loss=27.8266
	step [4/143], loss=26.6892
	step [5/143], loss=25.1785
	step [6/143], loss=26.2448
	step [7/143], loss=23.9189
	step [8/143], loss=24.9671
	step [9/143], loss=23.9826
	step [10/143], loss=29.3084
	step [11/143], loss=27.2687
	step [12/143], loss=25.2645
	step [13/143], loss=26.1054
	step [14/143], loss=25.5720
	step [15/143], loss=25.0757
	step [16/143], loss=26.2757
	step [17/143], loss=27.4672
	step [18/143], loss=29.6222
	step [19/143], loss=24.6509
	step [20/143], loss=25.2775
	step [21/143], loss=24.7944
	step [22/143], loss=26.5078
	step [23/143], loss=24.7312
	step [24/143], loss=28.4064
	step [25/143], loss=26.5499
	step [26/143], loss=24.9671
	step [27/143], loss=25.9152
	step [28/143], loss=26.0277
	step [29/143], loss=23.5996
	step [30/143], loss=25.8239
	step [31/143], loss=26.7345
	step [32/143], loss=28.5400
	step [33/143], loss=25.3274
	step [34/143], loss=26.6310
	step [35/143], loss=25.1473
	step [36/143], loss=24.6082
	step [37/143], loss=26.4826
	step [38/143], loss=23.8107
	step [39/143], loss=25.3155
	step [40/143], loss=25.4182
	step [41/143], loss=26.9902
	step [42/143], loss=24.3979
	step [43/143], loss=24.1475
	step [44/143], loss=25.0664
	step [45/143], loss=24.9683
	step [46/143], loss=24.5888
	step [47/143], loss=25.4567
	step [48/143], loss=27.0407
	step [49/143], loss=27.2748
	step [50/143], loss=27.1347
	step [51/143], loss=27.2034
	step [52/143], loss=26.5307
	step [53/143], loss=24.8043
	step [54/143], loss=26.4709
	step [55/143], loss=27.3599
	step [56/143], loss=26.0476
	step [57/143], loss=25.5249
	step [58/143], loss=26.4956
	step [59/143], loss=25.9268
	step [60/143], loss=26.1986
	step [61/143], loss=27.5333
	step [62/143], loss=27.3084
	step [63/143], loss=26.2983
	step [64/143], loss=26.2509
	step [65/143], loss=24.4190
	step [66/143], loss=25.7957
	step [67/143], loss=26.7948
	step [68/143], loss=25.9497
	step [69/143], loss=24.8901
	step [70/143], loss=26.0781
	step [71/143], loss=25.2490
	step [72/143], loss=26.5270
	step [73/143], loss=26.9457
	step [74/143], loss=30.3358
	step [75/143], loss=25.9652
	step [76/143], loss=23.8557
	step [77/143], loss=26.7940
	step [78/143], loss=23.5416
	step [79/143], loss=28.0165
	step [80/143], loss=26.1876
	step [81/143], loss=23.1532
	step [82/143], loss=26.6226
	step [83/143], loss=27.4081
	step [84/143], loss=26.3178
	step [85/143], loss=27.1274
	step [86/143], loss=25.0064
	step [87/143], loss=27.1583
	step [88/143], loss=26.5561
	step [89/143], loss=25.4794
	step [90/143], loss=27.9773
	step [91/143], loss=25.8563
	step [92/143], loss=23.8483
	step [93/143], loss=27.4835
	step [94/143], loss=25.3434
	step [95/143], loss=24.9595
	step [96/143], loss=24.2427
	step [97/143], loss=25.2676
	step [98/143], loss=27.7382
	step [99/143], loss=25.7178
	step [100/143], loss=25.4666
	step [101/143], loss=24.1566
	step [102/143], loss=23.8259
	step [103/143], loss=25.8654
	step [104/143], loss=23.8845
	step [105/143], loss=25.8948
	step [106/143], loss=22.6486
	step [107/143], loss=23.8138
	step [108/143], loss=25.9252
	step [109/143], loss=24.6383
	step [110/143], loss=24.3995
	step [111/143], loss=22.8796
	step [112/143], loss=22.8776
	step [113/143], loss=26.2650
	step [114/143], loss=24.1629
	step [115/143], loss=23.9425
	step [116/143], loss=25.1446
	step [117/143], loss=24.0774
	step [118/143], loss=25.9906
	step [119/143], loss=24.5907
	step [120/143], loss=23.6462
	step [121/143], loss=25.7845
	step [122/143], loss=25.1745
	step [123/143], loss=24.4965
	step [124/143], loss=23.8788
	step [125/143], loss=26.2882
	step [126/143], loss=22.6386
	step [127/143], loss=24.8120
	step [128/143], loss=23.9013
	step [129/143], loss=26.9625
	step [130/143], loss=25.0244
	step [131/143], loss=22.5157
	step [132/143], loss=26.2437
	step [133/143], loss=28.0694
	step [134/143], loss=27.4112
	step [135/143], loss=24.8222
	step [136/143], loss=27.1075
	step [137/143], loss=25.7224
	step [138/143], loss=24.0921
	step [139/143], loss=23.1470
	step [140/143], loss=24.9195
	step [141/143], loss=23.8508
	step [142/143], loss=28.2142
	step [143/143], loss=19.0667
	Evaluating
	loss=0.0711, precision=0.2269, recall=0.9906, f1=0.3692
Training epoch 11
	step [1/143], loss=24.1503
	step [2/143], loss=25.3128
	step [3/143], loss=24.4335
	step [4/143], loss=25.9912
	step [5/143], loss=26.3008
	step [6/143], loss=23.4267
	step [7/143], loss=25.3936
	step [8/143], loss=24.6326
	step [9/143], loss=24.3229
	step [10/143], loss=21.7612
	step [11/143], loss=25.6703
	step [12/143], loss=25.9788
	step [13/143], loss=22.7881
	step [14/143], loss=24.1724
	step [15/143], loss=23.9464
	step [16/143], loss=24.0575
	step [17/143], loss=23.4240
	step [18/143], loss=24.2829
	step [19/143], loss=23.9965
	step [20/143], loss=23.6595
	step [21/143], loss=23.9774
	step [22/143], loss=24.0259
	step [23/143], loss=25.5003
	step [24/143], loss=23.4190
	step [25/143], loss=22.8186
	step [26/143], loss=22.8911
	step [27/143], loss=24.2773
	step [28/143], loss=25.4779
	step [29/143], loss=25.4460
	step [30/143], loss=28.3626
	step [31/143], loss=23.6695
	step [32/143], loss=23.9927
	step [33/143], loss=23.2669
	step [34/143], loss=23.9952
	step [35/143], loss=22.2388
	step [36/143], loss=27.5475
	step [37/143], loss=24.4127
	step [38/143], loss=24.7410
	step [39/143], loss=23.9222
	step [40/143], loss=23.3252
	step [41/143], loss=23.7928
	step [42/143], loss=23.5577
	step [43/143], loss=24.3215
	step [44/143], loss=23.4309
	step [45/143], loss=23.9733
	step [46/143], loss=23.2540
	step [47/143], loss=22.4700
	step [48/143], loss=23.7964
	step [49/143], loss=26.0204
	step [50/143], loss=26.7251
	step [51/143], loss=25.9531
	step [52/143], loss=24.0322
	step [53/143], loss=23.8753
	step [54/143], loss=21.9854
	step [55/143], loss=23.3873
	step [56/143], loss=24.0894
	step [57/143], loss=23.8546
	step [58/143], loss=22.1880
	step [59/143], loss=24.8657
	step [60/143], loss=23.1517
	step [61/143], loss=23.6070
	step [62/143], loss=21.6240
	step [63/143], loss=24.3524
	step [64/143], loss=22.1405
	step [65/143], loss=26.0856
	step [66/143], loss=25.0696
	step [67/143], loss=22.4706
	step [68/143], loss=21.9687
	step [69/143], loss=24.1269
	step [70/143], loss=24.9921
	step [71/143], loss=22.6183
	step [72/143], loss=22.1659
	step [73/143], loss=26.5740
	step [74/143], loss=24.0299
	step [75/143], loss=23.3778
	step [76/143], loss=23.9340
	step [77/143], loss=21.6499
	step [78/143], loss=22.8377
	step [79/143], loss=23.6736
	step [80/143], loss=22.9761
	step [81/143], loss=23.8608
	step [82/143], loss=24.3710
	step [83/143], loss=23.0497
	step [84/143], loss=21.8315
	step [85/143], loss=23.8304
	step [86/143], loss=22.4869
	step [87/143], loss=22.4890
	step [88/143], loss=23.5592
	step [89/143], loss=24.4050
	step [90/143], loss=23.9774
	step [91/143], loss=22.4818
	step [92/143], loss=23.4552
	step [93/143], loss=21.5679
	step [94/143], loss=22.7374
	step [95/143], loss=22.6688
	step [96/143], loss=22.5590
	step [97/143], loss=25.9742
	step [98/143], loss=25.2073
	step [99/143], loss=20.7039
	step [100/143], loss=25.2539
	step [101/143], loss=23.9035
	step [102/143], loss=22.9734
	step [103/143], loss=22.4542
	step [104/143], loss=23.0593
	step [105/143], loss=23.9833
	step [106/143], loss=22.7475
	step [107/143], loss=21.9631
	step [108/143], loss=24.5964
	step [109/143], loss=22.2525
	step [110/143], loss=24.4577
	step [111/143], loss=22.0907
	step [112/143], loss=23.0882
	step [113/143], loss=22.3316
	step [114/143], loss=22.5808
	step [115/143], loss=25.3742
	step [116/143], loss=24.2884
	step [117/143], loss=22.8893
	step [118/143], loss=23.3847
	step [119/143], loss=25.1671
	step [120/143], loss=23.8018
	step [121/143], loss=22.7587
	step [122/143], loss=24.1058
	step [123/143], loss=21.9753
	step [124/143], loss=21.6481
	step [125/143], loss=23.8102
	step [126/143], loss=21.4060
	step [127/143], loss=23.3532
	step [128/143], loss=22.9705
	step [129/143], loss=23.2173
	step [130/143], loss=21.3943
	step [131/143], loss=23.8502
	step [132/143], loss=21.7805
	step [133/143], loss=21.9284
	step [134/143], loss=22.1211
	step [135/143], loss=23.2830
	step [136/143], loss=21.7279
	step [137/143], loss=23.2378
	step [138/143], loss=21.0265
	step [139/143], loss=21.2552
	step [140/143], loss=21.1137
	step [141/143], loss=21.9497
	step [142/143], loss=22.5098
	step [143/143], loss=18.4708
	Evaluating
	loss=0.0772, precision=0.1472, recall=0.9934, f1=0.2564
Training epoch 12
	step [1/143], loss=24.8267
	step [2/143], loss=21.7651
	step [3/143], loss=20.6286
	step [4/143], loss=21.1095
	step [5/143], loss=21.7236
	step [6/143], loss=21.9815
	step [7/143], loss=21.6224
	step [8/143], loss=20.5016
	step [9/143], loss=21.6653
	step [10/143], loss=21.4754
	step [11/143], loss=24.7090
	step [12/143], loss=23.3024
	step [13/143], loss=22.3834
	step [14/143], loss=23.4843
	step [15/143], loss=21.4505
	step [16/143], loss=21.3921
	step [17/143], loss=24.3812
	step [18/143], loss=20.4746
	step [19/143], loss=22.1303
	step [20/143], loss=23.4076
	step [21/143], loss=24.2703
	step [22/143], loss=21.5431
	step [23/143], loss=21.5475
	step [24/143], loss=21.3969
	step [25/143], loss=23.3218
	step [26/143], loss=21.8986
	step [27/143], loss=19.4260
	step [28/143], loss=21.8891
	step [29/143], loss=21.4566
	step [30/143], loss=19.5778
	step [31/143], loss=20.1729
	step [32/143], loss=21.8531
	step [33/143], loss=22.3898
	step [34/143], loss=23.7913
	step [35/143], loss=23.1910
	step [36/143], loss=22.8435
	step [37/143], loss=24.5715
	step [38/143], loss=22.3968
	step [39/143], loss=21.4651
	step [40/143], loss=22.3502
	step [41/143], loss=22.0374
	step [42/143], loss=21.4246
	step [43/143], loss=22.6618
	step [44/143], loss=21.9484
	step [45/143], loss=21.4494
	step [46/143], loss=22.3089
	step [47/143], loss=22.9875
	step [48/143], loss=20.4628
	step [49/143], loss=21.8134
	step [50/143], loss=21.0125
	step [51/143], loss=20.1059
	step [52/143], loss=21.7130
	step [53/143], loss=23.4016
	step [54/143], loss=22.0791
	step [55/143], loss=21.3001
	step [56/143], loss=20.4205
	step [57/143], loss=21.0484
	step [58/143], loss=23.8054
	step [59/143], loss=22.8662
	step [60/143], loss=22.6536
	step [61/143], loss=24.5480
	step [62/143], loss=22.4654
	step [63/143], loss=22.1719
	step [64/143], loss=21.5524
	step [65/143], loss=20.8196
	step [66/143], loss=20.0293
	step [67/143], loss=22.9016
	step [68/143], loss=21.5799
	step [69/143], loss=22.4630
	step [70/143], loss=21.6771
	step [71/143], loss=21.3540
	step [72/143], loss=20.1172
	step [73/143], loss=20.7006
	step [74/143], loss=22.2239
	step [75/143], loss=23.7161
	step [76/143], loss=19.9875
	step [77/143], loss=22.4364
	step [78/143], loss=20.8907
	step [79/143], loss=22.4806
	step [80/143], loss=19.3802
	step [81/143], loss=21.0633
	step [82/143], loss=22.3720
	step [83/143], loss=22.2309
	step [84/143], loss=18.4154
	step [85/143], loss=22.3792
	step [86/143], loss=23.0034
	step [87/143], loss=20.2632
	step [88/143], loss=21.5812
	step [89/143], loss=22.7257
	step [90/143], loss=21.2224
	step [91/143], loss=20.7907
	step [92/143], loss=22.6776
	step [93/143], loss=20.6265
	step [94/143], loss=20.5210
	step [95/143], loss=20.5350
	step [96/143], loss=22.0074
	step [97/143], loss=20.3182
	step [98/143], loss=21.1399
	step [99/143], loss=21.3208
	step [100/143], loss=22.6495
	step [101/143], loss=20.9427
	step [102/143], loss=21.6850
	step [103/143], loss=21.3953
	step [104/143], loss=21.4294
	step [105/143], loss=20.1957
	step [106/143], loss=21.6885
	step [107/143], loss=22.3296
	step [108/143], loss=22.2957
	step [109/143], loss=21.1122
	step [110/143], loss=20.4239
	step [111/143], loss=22.9681
	step [112/143], loss=21.5818
	step [113/143], loss=25.5707
	step [114/143], loss=22.7156
	step [115/143], loss=20.6554
	step [116/143], loss=21.4744
	step [117/143], loss=19.6126
	step [118/143], loss=21.3977
	step [119/143], loss=20.5884
	step [120/143], loss=22.1264
	step [121/143], loss=18.7184
	step [122/143], loss=20.0383
	step [123/143], loss=21.9114
	step [124/143], loss=21.3712
	step [125/143], loss=23.7618
	step [126/143], loss=23.7592
	step [127/143], loss=20.8646
	step [128/143], loss=20.9924
	step [129/143], loss=20.2847
	step [130/143], loss=20.1141
	step [131/143], loss=20.1432
	step [132/143], loss=21.9666
	step [133/143], loss=21.6994
	step [134/143], loss=20.4421
	step [135/143], loss=21.6739
	step [136/143], loss=20.1266
	step [137/143], loss=21.6361
	step [138/143], loss=20.4367
	step [139/143], loss=21.4339
	step [140/143], loss=19.8126
	step [141/143], loss=22.6712
	step [142/143], loss=22.3797
	step [143/143], loss=16.8465
	Evaluating
	loss=0.0551, precision=0.2415, recall=0.9900, f1=0.3883
Training epoch 13
	step [1/143], loss=19.9323
	step [2/143], loss=22.4161
	step [3/143], loss=21.5515
	step [4/143], loss=19.9395
	step [5/143], loss=20.8020
	step [6/143], loss=22.3634
	step [7/143], loss=22.6884
	step [8/143], loss=20.0443
	step [9/143], loss=19.9504
	step [10/143], loss=21.8970
	step [11/143], loss=19.9894
	step [12/143], loss=22.1793
	step [13/143], loss=19.8188
	step [14/143], loss=22.3317
	step [15/143], loss=21.7220
	step [16/143], loss=21.8642
	step [17/143], loss=18.8148
	step [18/143], loss=21.4174
	step [19/143], loss=21.0813
	step [20/143], loss=20.2466
	step [21/143], loss=21.2725
	step [22/143], loss=20.4437
	step [23/143], loss=19.6764
	step [24/143], loss=19.5233
	step [25/143], loss=20.5272
	step [26/143], loss=21.1934
	step [27/143], loss=19.9887
	step [28/143], loss=21.7981
	step [29/143], loss=21.0706
	step [30/143], loss=18.3105
	step [31/143], loss=19.2128
	step [32/143], loss=22.0088
	step [33/143], loss=20.2759
	step [34/143], loss=21.3538
	step [35/143], loss=21.8785
	step [36/143], loss=20.6445
	step [37/143], loss=21.1785
	step [38/143], loss=20.3981
	step [39/143], loss=20.4858
	step [40/143], loss=17.4164
	step [41/143], loss=20.2030
	step [42/143], loss=22.0197
	step [43/143], loss=21.7797
	step [44/143], loss=21.6836
	step [45/143], loss=19.7474
	step [46/143], loss=20.2767
	step [47/143], loss=20.2311
	step [48/143], loss=21.0375
	step [49/143], loss=20.6433
	step [50/143], loss=19.7084
	step [51/143], loss=20.9220
	step [52/143], loss=20.5269
	step [53/143], loss=19.1178
	step [54/143], loss=21.0652
	step [55/143], loss=19.6019
	step [56/143], loss=21.8472
	step [57/143], loss=20.7072
	step [58/143], loss=21.1886
	step [59/143], loss=19.9801
	step [60/143], loss=18.6182
	step [61/143], loss=19.8076
	step [62/143], loss=19.3022
	step [63/143], loss=22.5506
	step [64/143], loss=19.0931
	step [65/143], loss=20.0172
	step [66/143], loss=19.2070
	step [67/143], loss=18.0766
	step [68/143], loss=20.2216
	step [69/143], loss=21.3963
	step [70/143], loss=22.4631
	step [71/143], loss=20.5622
	step [72/143], loss=19.4498
	step [73/143], loss=19.4966
	step [74/143], loss=20.2566
	step [75/143], loss=17.7681
	step [76/143], loss=20.4795
	step [77/143], loss=22.2256
	step [78/143], loss=20.7593
	step [79/143], loss=19.1356
	step [80/143], loss=20.2271
	step [81/143], loss=19.1811
	step [82/143], loss=20.8182
	step [83/143], loss=19.9552
	step [84/143], loss=19.3200
	step [85/143], loss=19.8671
	step [86/143], loss=18.0884
	step [87/143], loss=18.9223
	step [88/143], loss=18.6522
	step [89/143], loss=21.5108
	step [90/143], loss=19.5260
	step [91/143], loss=20.2817
	step [92/143], loss=20.1273
	step [93/143], loss=20.7135
	step [94/143], loss=21.0474
	step [95/143], loss=20.4787
	step [96/143], loss=21.9160
	step [97/143], loss=18.0363
	step [98/143], loss=19.5112
	step [99/143], loss=20.3942
	step [100/143], loss=19.5275
	step [101/143], loss=18.9237
	step [102/143], loss=20.7298
	step [103/143], loss=20.6764
	step [104/143], loss=21.3236
	step [105/143], loss=19.1949
	step [106/143], loss=19.6303
	step [107/143], loss=18.6083
	step [108/143], loss=19.1741
	step [109/143], loss=16.7815
	step [110/143], loss=18.9371
	step [111/143], loss=18.5176
	step [112/143], loss=20.3311
	step [113/143], loss=19.3485
	step [114/143], loss=19.7917
	step [115/143], loss=20.0886
	step [116/143], loss=18.5211
	step [117/143], loss=19.0024
	step [118/143], loss=19.8606
	step [119/143], loss=20.5561
	step [120/143], loss=18.9273
	step [121/143], loss=18.4468
	step [122/143], loss=19.4937
	step [123/143], loss=20.4021
	step [124/143], loss=19.1208
	step [125/143], loss=20.4262
	step [126/143], loss=19.8743
	step [127/143], loss=20.1112
	step [128/143], loss=20.3144
	step [129/143], loss=19.5990
	step [130/143], loss=20.1656
	step [131/143], loss=20.5617
	step [132/143], loss=21.9418
	step [133/143], loss=18.1015
	step [134/143], loss=21.8778
	step [135/143], loss=18.7163
	step [136/143], loss=19.7068
	step [137/143], loss=20.5165
	step [138/143], loss=18.7579
	step [139/143], loss=19.9686
	step [140/143], loss=19.7122
	step [141/143], loss=19.3869
	step [142/143], loss=19.2444
	step [143/143], loss=17.8071
	Evaluating
	loss=0.0518, precision=0.2274, recall=0.9899, f1=0.3698
Training epoch 14
	step [1/143], loss=18.9201
	step [2/143], loss=19.9569
	step [3/143], loss=19.5594
	step [4/143], loss=18.3874
	step [5/143], loss=19.0821
	step [6/143], loss=19.0950
	step [7/143], loss=17.0257
	step [8/143], loss=21.2087
	step [9/143], loss=19.3533
	step [10/143], loss=19.4842
	step [11/143], loss=18.4431
	step [12/143], loss=20.2344
	step [13/143], loss=19.8482
	step [14/143], loss=19.0416
	step [15/143], loss=18.4698
	step [16/143], loss=21.2964
	step [17/143], loss=19.1472
	step [18/143], loss=18.7600
	step [19/143], loss=20.6341
	step [20/143], loss=18.7703
	step [21/143], loss=17.0040
	step [22/143], loss=18.4935
	step [23/143], loss=18.7001
	step [24/143], loss=20.8622
	step [25/143], loss=19.7224
	step [26/143], loss=18.9612
	step [27/143], loss=19.8865
	step [28/143], loss=20.9734
	step [29/143], loss=19.5045
	step [30/143], loss=18.9834
	step [31/143], loss=20.4841
	step [32/143], loss=18.5193
	step [33/143], loss=21.2556
	step [34/143], loss=19.6193
	step [35/143], loss=17.5169
	step [36/143], loss=20.0661
	step [37/143], loss=18.6749
	step [38/143], loss=18.4939
	step [39/143], loss=19.9898
	step [40/143], loss=19.5974
	step [41/143], loss=19.8978
	step [42/143], loss=19.0605
	step [43/143], loss=18.5599
	step [44/143], loss=19.3960
	step [45/143], loss=18.3732
	step [46/143], loss=21.9104
	step [47/143], loss=19.9074
	step [48/143], loss=18.3164
	step [49/143], loss=22.3498
	step [50/143], loss=18.1061
	step [51/143], loss=16.8566
	step [52/143], loss=19.4520
	step [53/143], loss=17.9492
	step [54/143], loss=18.1142
	step [55/143], loss=20.1704
	step [56/143], loss=17.1459
	step [57/143], loss=18.1267
	step [58/143], loss=18.7315
	step [59/143], loss=21.5784
	step [60/143], loss=18.5552
	step [61/143], loss=18.6559
	step [62/143], loss=20.3966
	step [63/143], loss=17.2759
	step [64/143], loss=19.0750
	step [65/143], loss=19.8360
	step [66/143], loss=21.1262
	step [67/143], loss=18.5342
	step [68/143], loss=20.2273
	step [69/143], loss=17.0303
	step [70/143], loss=17.9644
	step [71/143], loss=18.5621
	step [72/143], loss=19.0280
	step [73/143], loss=17.3273
	step [74/143], loss=17.2202
	step [75/143], loss=18.0970
	step [76/143], loss=20.2408
	step [77/143], loss=20.1631
	step [78/143], loss=18.5921
	step [79/143], loss=17.9103
	step [80/143], loss=18.0557
	step [81/143], loss=18.5868
	step [82/143], loss=19.5819
	step [83/143], loss=17.9954
	step [84/143], loss=19.6462
	step [85/143], loss=17.9981
	step [86/143], loss=17.3545
	step [87/143], loss=16.4911
	step [88/143], loss=17.6732
	step [89/143], loss=18.2105
	step [90/143], loss=18.1450
	step [91/143], loss=18.8659
	step [92/143], loss=20.4085
	step [93/143], loss=19.2719
	step [94/143], loss=17.5748
	step [95/143], loss=18.9530
	step [96/143], loss=18.1818
	step [97/143], loss=18.3103
	step [98/143], loss=18.0893
	step [99/143], loss=19.0570
	step [100/143], loss=18.2084
	step [101/143], loss=18.3193
	step [102/143], loss=17.6536
	step [103/143], loss=18.8741
	step [104/143], loss=17.9084
	step [105/143], loss=17.6491
	step [106/143], loss=18.9447
	step [107/143], loss=19.1057
	step [108/143], loss=17.2903
	step [109/143], loss=16.8539
	step [110/143], loss=17.6368
	step [111/143], loss=20.8833
	step [112/143], loss=18.1893
	step [113/143], loss=18.8520
	step [114/143], loss=22.9139
	step [115/143], loss=20.0057
	step [116/143], loss=19.4104
	step [117/143], loss=19.6371
	step [118/143], loss=18.6039
	step [119/143], loss=18.5175
	step [120/143], loss=18.9645
	step [121/143], loss=19.5964
	step [122/143], loss=17.3545
	step [123/143], loss=18.0422
	step [124/143], loss=15.7363
	step [125/143], loss=18.4296
	step [126/143], loss=18.8409
	step [127/143], loss=17.8546
	step [128/143], loss=19.4547
	step [129/143], loss=15.8957
	step [130/143], loss=15.8828
	step [131/143], loss=17.7786
	step [132/143], loss=17.6364
	step [133/143], loss=19.1126
	step [134/143], loss=20.0062
	step [135/143], loss=18.8860
	step [136/143], loss=20.4306
	step [137/143], loss=17.2670
	step [138/143], loss=17.6972
	step [139/143], loss=19.5396
	step [140/143], loss=19.4246
	step [141/143], loss=18.2409
	step [142/143], loss=20.5699
	step [143/143], loss=14.2895
	Evaluating
	loss=0.0509, precision=0.2212, recall=0.9909, f1=0.3616
Training epoch 15
	step [1/143], loss=18.3405
	step [2/143], loss=17.3544
	step [3/143], loss=17.6471
	step [4/143], loss=19.1381
	step [5/143], loss=18.6815
	step [6/143], loss=18.0386
	step [7/143], loss=18.3777
	step [8/143], loss=18.2709
	step [9/143], loss=17.5973
	step [10/143], loss=18.4754
	step [11/143], loss=17.9734
	step [12/143], loss=18.1693
	step [13/143], loss=20.0538
	step [14/143], loss=18.7890
	step [15/143], loss=21.5528
	step [16/143], loss=18.3236
	step [17/143], loss=16.2472
	step [18/143], loss=18.0157
	step [19/143], loss=20.0365
	step [20/143], loss=18.7997
	step [21/143], loss=18.9410
	step [22/143], loss=18.2724
	step [23/143], loss=19.2951
	step [24/143], loss=17.3909
	step [25/143], loss=17.3649
	step [26/143], loss=19.4125
	step [27/143], loss=18.8776
	step [28/143], loss=17.9420
	step [29/143], loss=16.4752
	step [30/143], loss=16.5559
	step [31/143], loss=17.4377
	step [32/143], loss=18.3177
	step [33/143], loss=17.1097
	step [34/143], loss=17.5129
	step [35/143], loss=18.1402
	step [36/143], loss=17.8414
	step [37/143], loss=17.5709
	step [38/143], loss=17.0867
	step [39/143], loss=16.6142
	step [40/143], loss=17.1557
	step [41/143], loss=18.8516
	step [42/143], loss=18.7064
	step [43/143], loss=18.3153
	step [44/143], loss=16.1575
	step [45/143], loss=17.1011
	step [46/143], loss=18.7667
	step [47/143], loss=17.0712
	step [48/143], loss=17.5723
	step [49/143], loss=16.9317
	step [50/143], loss=17.9314
	step [51/143], loss=16.8116
	step [52/143], loss=17.3430
	step [53/143], loss=18.4918
	step [54/143], loss=20.1230
	step [55/143], loss=16.8740
	step [56/143], loss=16.9394
	step [57/143], loss=18.7693
	step [58/143], loss=18.0300
	step [59/143], loss=18.3549
	step [60/143], loss=19.3759
	step [61/143], loss=15.7931
	step [62/143], loss=16.5641
	step [63/143], loss=17.7393
	step [64/143], loss=18.9562
	step [65/143], loss=17.1086
	step [66/143], loss=16.7071
	step [67/143], loss=18.8389
	step [68/143], loss=18.3269
	step [69/143], loss=16.6532
	step [70/143], loss=17.4206
	step [71/143], loss=17.6788
	step [72/143], loss=17.1689
	step [73/143], loss=16.1258
	step [74/143], loss=19.3774
	step [75/143], loss=17.8808
	step [76/143], loss=20.4988
	step [77/143], loss=18.3219
	step [78/143], loss=17.8868
	step [79/143], loss=18.5458
	step [80/143], loss=16.5215
	step [81/143], loss=17.6293
	step [82/143], loss=17.5648
	step [83/143], loss=15.8648
	step [84/143], loss=16.9604
	step [85/143], loss=17.9877
	step [86/143], loss=17.7669
	step [87/143], loss=15.8615
	step [88/143], loss=18.2032
	step [89/143], loss=19.0058
	step [90/143], loss=16.3339
	step [91/143], loss=17.6041
	step [92/143], loss=18.4321
	step [93/143], loss=17.2353
	step [94/143], loss=16.0841
	step [95/143], loss=17.1485
	step [96/143], loss=17.1083
	step [97/143], loss=17.1996
	step [98/143], loss=16.7961
	step [99/143], loss=17.9205
	step [100/143], loss=18.6110
	step [101/143], loss=16.4033
	step [102/143], loss=16.0149
	step [103/143], loss=17.4712
	step [104/143], loss=16.0494
	step [105/143], loss=16.0868
	step [106/143], loss=17.3871
	step [107/143], loss=17.4012
	step [108/143], loss=21.2658
	step [109/143], loss=19.3779
	step [110/143], loss=16.9911
	step [111/143], loss=18.9193
	step [112/143], loss=17.6337
	step [113/143], loss=18.1403
	step [114/143], loss=17.4266
	step [115/143], loss=16.3392
	step [116/143], loss=15.1819
	step [117/143], loss=16.5588
	step [118/143], loss=19.5537
	step [119/143], loss=15.6808
	step [120/143], loss=18.1121
	step [121/143], loss=17.3545
	step [122/143], loss=15.7363
	step [123/143], loss=16.5240
	step [124/143], loss=15.8800
	step [125/143], loss=18.1012
	step [126/143], loss=18.5619
	step [127/143], loss=16.7430
	step [128/143], loss=17.6443
	step [129/143], loss=18.4752
	step [130/143], loss=19.7520
	step [131/143], loss=18.5401
	step [132/143], loss=18.9786
	step [133/143], loss=18.1418
	step [134/143], loss=21.6644
	step [135/143], loss=16.6227
	step [136/143], loss=17.6231
	step [137/143], loss=15.8500
	step [138/143], loss=16.8902
	step [139/143], loss=16.1105
	step [140/143], loss=16.8441
	step [141/143], loss=17.0338
	step [142/143], loss=16.4037
	step [143/143], loss=12.4349
	Evaluating
	loss=0.0450, precision=0.2297, recall=0.9902, f1=0.3729
Training epoch 16
	step [1/143], loss=19.0818
	step [2/143], loss=16.5083
	step [3/143], loss=16.8717
	step [4/143], loss=18.5402
	step [5/143], loss=17.6906
	step [6/143], loss=18.1629
	step [7/143], loss=16.5917
	step [8/143], loss=18.0486
	step [9/143], loss=17.0235
	step [10/143], loss=15.8161
	step [11/143], loss=18.1990
	step [12/143], loss=18.1735
	step [13/143], loss=19.0112
	step [14/143], loss=17.1635
	step [15/143], loss=16.8531
	step [16/143], loss=18.3151
	step [17/143], loss=16.4882
	step [18/143], loss=17.4650
	step [19/143], loss=16.1295
	step [20/143], loss=17.6985
	step [21/143], loss=17.6551
	step [22/143], loss=20.0773
	step [23/143], loss=17.4863
	step [24/143], loss=17.7536
	step [25/143], loss=16.3748
	step [26/143], loss=18.1365
	step [27/143], loss=16.5224
	step [28/143], loss=16.4473
	step [29/143], loss=16.9109
	step [30/143], loss=16.3046
	step [31/143], loss=14.8772
	step [32/143], loss=18.9626
	step [33/143], loss=16.7014
	step [34/143], loss=18.6761
	step [35/143], loss=17.0445
	step [36/143], loss=17.4259
	step [37/143], loss=17.1193
	step [38/143], loss=16.6141
	step [39/143], loss=18.6118
	step [40/143], loss=17.4789
	step [41/143], loss=16.0203
	step [42/143], loss=16.7986
	step [43/143], loss=16.1059
	step [44/143], loss=15.7980
	step [45/143], loss=17.9546
	step [46/143], loss=15.5415
	step [47/143], loss=16.8726
	step [48/143], loss=16.9612
	step [49/143], loss=16.0701
	step [50/143], loss=17.9014
	step [51/143], loss=15.2736
	step [52/143], loss=18.3481
	step [53/143], loss=20.8057
	step [54/143], loss=16.8508
	step [55/143], loss=17.3460
	step [56/143], loss=16.3048
	step [57/143], loss=16.4546
	step [58/143], loss=17.8441
	step [59/143], loss=16.8754
	step [60/143], loss=17.4704
	step [61/143], loss=17.3314
	step [62/143], loss=19.7991
	step [63/143], loss=17.3037
	step [64/143], loss=16.8399
	step [65/143], loss=17.1538
	step [66/143], loss=15.3946
	step [67/143], loss=17.3604
	step [68/143], loss=17.5983
	step [69/143], loss=17.4411
	step [70/143], loss=18.5188
	step [71/143], loss=17.2484
	step [72/143], loss=15.2867
	step [73/143], loss=16.9818
	step [74/143], loss=15.4482
	step [75/143], loss=15.9967
	step [76/143], loss=15.2655
	step [77/143], loss=16.7780
	step [78/143], loss=16.1136
	step [79/143], loss=17.6960
	step [80/143], loss=16.0532
	step [81/143], loss=15.6743
	step [82/143], loss=16.4965
	step [83/143], loss=17.4753
	step [84/143], loss=18.3419
	step [85/143], loss=14.9187
	step [86/143], loss=16.3973
	step [87/143], loss=16.4926
	step [88/143], loss=14.5817
	step [89/143], loss=16.5001
	step [90/143], loss=18.3475
	step [91/143], loss=15.3195
	step [92/143], loss=17.6073
	step [93/143], loss=18.2980
	step [94/143], loss=17.0686
	step [95/143], loss=15.2510
	step [96/143], loss=16.7476
	step [97/143], loss=16.5818
	step [98/143], loss=16.2731
	step [99/143], loss=14.4326
	step [100/143], loss=15.8131
	step [101/143], loss=17.1921
	step [102/143], loss=16.4421
	step [103/143], loss=18.2154
	step [104/143], loss=17.4302
	step [105/143], loss=16.5283
	step [106/143], loss=16.1454
	step [107/143], loss=16.1422
	step [108/143], loss=17.4280
	step [109/143], loss=14.4630
	step [110/143], loss=18.4810
	step [111/143], loss=16.2720
	step [112/143], loss=15.9600
	step [113/143], loss=16.9655
	step [114/143], loss=16.5850
	step [115/143], loss=15.3723
	step [116/143], loss=17.8127
	step [117/143], loss=15.1308
	step [118/143], loss=16.0922
	step [119/143], loss=16.8038
	step [120/143], loss=16.3490
	step [121/143], loss=15.2332
	step [122/143], loss=15.1657
	step [123/143], loss=16.3871
	step [124/143], loss=15.2951
	step [125/143], loss=17.3743
	step [126/143], loss=15.7579
	step [127/143], loss=16.2122
	step [128/143], loss=17.5401
	step [129/143], loss=15.9435
	step [130/143], loss=16.2601
	step [131/143], loss=14.6372
	step [132/143], loss=17.2574
	step [133/143], loss=16.7373
	step [134/143], loss=15.9978
	step [135/143], loss=15.4860
	step [136/143], loss=16.4399
	step [137/143], loss=18.4176
	step [138/143], loss=16.3574
	step [139/143], loss=15.6819
	step [140/143], loss=17.0770
	step [141/143], loss=16.8956
	step [142/143], loss=16.3472
	step [143/143], loss=13.5490
	Evaluating
	loss=0.0403, precision=0.2552, recall=0.9887, f1=0.4057
saving model as: 1_saved_model.pth
Training epoch 17
	step [1/143], loss=16.8144
	step [2/143], loss=15.8019
	step [3/143], loss=14.7241
	step [4/143], loss=15.0034
	step [5/143], loss=17.2192
	step [6/143], loss=15.9555
	step [7/143], loss=18.2141
	step [8/143], loss=14.5511
	step [9/143], loss=17.4369
	step [10/143], loss=15.9782
	step [11/143], loss=16.1468
	step [12/143], loss=17.2105
	step [13/143], loss=18.2055
	step [14/143], loss=15.8012
	step [15/143], loss=15.1955
	step [16/143], loss=16.9744
	step [17/143], loss=14.6349
	step [18/143], loss=15.2043
	step [19/143], loss=14.8132
	step [20/143], loss=19.5292
	step [21/143], loss=14.5257
	step [22/143], loss=14.9588
	step [23/143], loss=14.8438
	step [24/143], loss=16.1537
	step [25/143], loss=15.7005
	step [26/143], loss=15.4650
	step [27/143], loss=17.6854
	step [28/143], loss=16.1469
	step [29/143], loss=17.0550
	step [30/143], loss=15.4636
	step [31/143], loss=17.9209
	step [32/143], loss=16.3169
	step [33/143], loss=15.5901
	step [34/143], loss=17.1441
	step [35/143], loss=17.3564
	step [36/143], loss=14.2934
	step [37/143], loss=14.8256
	step [38/143], loss=14.8823
	step [39/143], loss=16.9241
	step [40/143], loss=14.4607
	step [41/143], loss=20.7512
	step [42/143], loss=14.5740
	step [43/143], loss=16.9212
	step [44/143], loss=16.1947
	step [45/143], loss=14.4161
	step [46/143], loss=17.0129
	step [47/143], loss=17.3293
	step [48/143], loss=15.3732
	step [49/143], loss=16.5656
	step [50/143], loss=17.3927
	step [51/143], loss=16.8920
	step [52/143], loss=15.3894
	step [53/143], loss=16.6722
	step [54/143], loss=13.8287
	step [55/143], loss=15.9029
	step [56/143], loss=14.0032
	step [57/143], loss=15.7081
	step [58/143], loss=14.4957
	step [59/143], loss=14.6441
	step [60/143], loss=15.6000
	step [61/143], loss=16.8374
	step [62/143], loss=15.8242
	step [63/143], loss=17.5825
	step [64/143], loss=16.2249
	step [65/143], loss=16.5706
	step [66/143], loss=16.0228
	step [67/143], loss=15.7457
	step [68/143], loss=17.4503
	step [69/143], loss=16.3384
	step [70/143], loss=16.3166
	step [71/143], loss=14.7172
	step [72/143], loss=15.5739
	step [73/143], loss=17.3022
	step [74/143], loss=16.5301
	step [75/143], loss=15.0761
	step [76/143], loss=14.5994
	step [77/143], loss=19.1661
	step [78/143], loss=16.0977
	step [79/143], loss=13.6445
	step [80/143], loss=14.6595
	step [81/143], loss=14.8527
	step [82/143], loss=16.6325
	step [83/143], loss=16.1126
	step [84/143], loss=14.5895
	step [85/143], loss=15.3647
	step [86/143], loss=15.4890
	step [87/143], loss=14.4530
	step [88/143], loss=14.1491
	step [89/143], loss=14.7585
	step [90/143], loss=15.8990
	step [91/143], loss=15.0211
	step [92/143], loss=16.5516
	step [93/143], loss=18.1225
	step [94/143], loss=14.6070
	step [95/143], loss=18.1469
	step [96/143], loss=14.1312
	step [97/143], loss=17.6317
	step [98/143], loss=16.5699
	step [99/143], loss=15.8855
	step [100/143], loss=14.9044
	step [101/143], loss=16.0181
	step [102/143], loss=18.7925
	step [103/143], loss=15.8595
	step [104/143], loss=15.3721
	step [105/143], loss=16.9558
	step [106/143], loss=15.6593
	step [107/143], loss=16.1687
	step [108/143], loss=16.0813
	step [109/143], loss=15.8147
	step [110/143], loss=17.6419
	step [111/143], loss=16.7165
	step [112/143], loss=15.2528
	step [113/143], loss=13.3928
	step [114/143], loss=17.4186
	step [115/143], loss=16.1402
	step [116/143], loss=15.0100
	step [117/143], loss=15.9969
	step [118/143], loss=18.5111
	step [119/143], loss=16.0744
	step [120/143], loss=16.0208
	step [121/143], loss=16.6272
	step [122/143], loss=15.9619
	step [123/143], loss=14.7351
	step [124/143], loss=17.5818
	step [125/143], loss=14.7888
	step [126/143], loss=14.1513
	step [127/143], loss=14.9307
	step [128/143], loss=14.3980
	step [129/143], loss=17.3342
	step [130/143], loss=16.8049
	step [131/143], loss=15.9905
	step [132/143], loss=14.9893
	step [133/143], loss=16.0467
	step [134/143], loss=16.4982
	step [135/143], loss=16.3738
	step [136/143], loss=17.0110
	step [137/143], loss=17.6387
	step [138/143], loss=15.0060
	step [139/143], loss=16.9987
	step [140/143], loss=17.4769
	step [141/143], loss=16.9261
	step [142/143], loss=17.1752
	step [143/143], loss=12.8060
	Evaluating
	loss=0.0488, precision=0.2104, recall=0.9902, f1=0.3471
Training epoch 18
	step [1/143], loss=17.7055
	step [2/143], loss=13.8464
	step [3/143], loss=15.4312
	step [4/143], loss=14.2729
	step [5/143], loss=16.1172
	step [6/143], loss=16.4164
	step [7/143], loss=18.3297
	step [8/143], loss=17.7073
	step [9/143], loss=16.4743
	step [10/143], loss=15.0803
	step [11/143], loss=16.1067
	step [12/143], loss=15.7439
	step [13/143], loss=14.4889
	step [14/143], loss=14.3688
	step [15/143], loss=13.8933
	step [16/143], loss=15.1427
	step [17/143], loss=16.8428
	step [18/143], loss=15.9313
	step [19/143], loss=14.9799
	step [20/143], loss=17.4619
	step [21/143], loss=14.6010
	step [22/143], loss=17.1005
	step [23/143], loss=13.7540
	step [24/143], loss=15.2464
	step [25/143], loss=13.2933
	step [26/143], loss=15.0241
	step [27/143], loss=14.9469
	step [28/143], loss=14.4810
	step [29/143], loss=19.0552
	step [30/143], loss=14.4679
	step [31/143], loss=14.9439
	step [32/143], loss=17.4511
	step [33/143], loss=14.8827
	step [34/143], loss=15.3938
	step [35/143], loss=14.1594
	step [36/143], loss=16.8299
	step [37/143], loss=15.5955
	step [38/143], loss=14.4283
	step [39/143], loss=14.5489
	step [40/143], loss=15.9565
	step [41/143], loss=15.1503
	step [42/143], loss=16.6586
	step [43/143], loss=14.7827
	step [44/143], loss=15.4122
	step [45/143], loss=16.3514
	step [46/143], loss=16.0441
	step [47/143], loss=16.1917
	step [48/143], loss=15.1421
	step [49/143], loss=13.9953
	step [50/143], loss=15.5457
	step [51/143], loss=15.7821
	step [52/143], loss=13.4441
	step [53/143], loss=16.4641
	step [54/143], loss=15.8085
	step [55/143], loss=15.0670
	step [56/143], loss=16.7094
	step [57/143], loss=15.6483
	step [58/143], loss=14.7206
	step [59/143], loss=14.7091
	step [60/143], loss=17.5175
	step [61/143], loss=14.8917
	step [62/143], loss=16.6648
	step [63/143], loss=15.5145
	step [64/143], loss=18.2945
	step [65/143], loss=15.2271
	step [66/143], loss=14.6173
	step [67/143], loss=13.2801
	step [68/143], loss=16.6354
	step [69/143], loss=15.4008
	step [70/143], loss=16.7184
	step [71/143], loss=12.4985
	step [72/143], loss=16.4761
	step [73/143], loss=14.8368
	step [74/143], loss=14.8361
	step [75/143], loss=14.4888
	step [76/143], loss=15.0139
	step [77/143], loss=15.3181
	step [78/143], loss=16.4819
	step [79/143], loss=15.5050
	step [80/143], loss=15.1234
	step [81/143], loss=17.3973
	step [82/143], loss=15.0787
	step [83/143], loss=12.5015
	step [84/143], loss=16.4476
	step [85/143], loss=13.9547
	step [86/143], loss=17.3279
	step [87/143], loss=15.6150
	step [88/143], loss=15.3058
	step [89/143], loss=15.2771
	step [90/143], loss=16.4702
	step [91/143], loss=14.7213
	step [92/143], loss=16.4136
	step [93/143], loss=15.0962
	step [94/143], loss=17.4621
	step [95/143], loss=14.9095
	step [96/143], loss=13.9988
	step [97/143], loss=15.1461
	step [98/143], loss=15.1037
	step [99/143], loss=14.4915
	step [100/143], loss=16.7382
	step [101/143], loss=15.2521
	step [102/143], loss=15.3014
	step [103/143], loss=15.5389
	step [104/143], loss=15.2409
	step [105/143], loss=15.4621
	step [106/143], loss=15.1457
	step [107/143], loss=15.9435
	step [108/143], loss=16.3147
	step [109/143], loss=14.5754
	step [110/143], loss=14.0814
	step [111/143], loss=17.1352
	step [112/143], loss=13.6656
	step [113/143], loss=15.7321
	step [114/143], loss=16.6033
	step [115/143], loss=14.3349
	step [116/143], loss=16.0793
	step [117/143], loss=16.1139
	step [118/143], loss=15.4285
	step [119/143], loss=13.5901
	step [120/143], loss=16.0531
	step [121/143], loss=16.0963
	step [122/143], loss=12.8079
	step [123/143], loss=16.5249
	step [124/143], loss=13.6323
	step [125/143], loss=14.9497
	step [126/143], loss=14.6054
	step [127/143], loss=14.4263
	step [128/143], loss=13.2338
	step [129/143], loss=14.9319
	step [130/143], loss=13.6213
	step [131/143], loss=14.4052
	step [132/143], loss=16.1302
	step [133/143], loss=15.3108
	step [134/143], loss=15.4436
	step [135/143], loss=13.6134
	step [136/143], loss=14.2782
	step [137/143], loss=14.7379
	step [138/143], loss=15.0102
	step [139/143], loss=14.2366
	step [140/143], loss=15.7734
	step [141/143], loss=13.3593
	step [142/143], loss=15.4058
	step [143/143], loss=11.6720
	Evaluating
	loss=0.0415, precision=0.2107, recall=0.9905, f1=0.3475
Training epoch 19
	step [1/143], loss=13.5243
	step [2/143], loss=14.7172
	step [3/143], loss=15.4646
	step [4/143], loss=14.1771
	step [5/143], loss=14.5540
	step [6/143], loss=14.9646
	step [7/143], loss=13.9615
	step [8/143], loss=13.2556
	step [9/143], loss=13.5090
	step [10/143], loss=12.5171
	step [11/143], loss=12.6528
	step [12/143], loss=13.8462
	step [13/143], loss=16.1845
	step [14/143], loss=15.8657
	step [15/143], loss=15.1888
	step [16/143], loss=15.1663
	step [17/143], loss=14.3804
	step [18/143], loss=15.1388
	step [19/143], loss=14.0774
	step [20/143], loss=15.9155
	step [21/143], loss=16.1017
	step [22/143], loss=16.2590
	step [23/143], loss=14.4279
	step [24/143], loss=15.1111
	step [25/143], loss=14.6185
	step [26/143], loss=13.4360
	step [27/143], loss=14.2047
	step [28/143], loss=13.1980
	step [29/143], loss=13.7677
	step [30/143], loss=14.6492
	step [31/143], loss=16.7930
	step [32/143], loss=14.2026
	step [33/143], loss=15.3916
	step [34/143], loss=15.3484
	step [35/143], loss=15.3261
	step [36/143], loss=15.2677
	step [37/143], loss=15.1806
	step [38/143], loss=14.2066
	step [39/143], loss=15.0171
	step [40/143], loss=14.8309
	step [41/143], loss=14.1942
	step [42/143], loss=15.7947
	step [43/143], loss=14.1144
	step [44/143], loss=15.4614
	step [45/143], loss=14.4765
	step [46/143], loss=14.2485
	step [47/143], loss=15.3822
	step [48/143], loss=14.3769
	step [49/143], loss=14.0954
	step [50/143], loss=15.9404
	step [51/143], loss=14.1924
	step [52/143], loss=14.4741
	step [53/143], loss=15.3889
	step [54/143], loss=13.9754
	step [55/143], loss=16.2993
	step [56/143], loss=14.8995
	step [57/143], loss=16.5038
	step [58/143], loss=14.3973
	step [59/143], loss=14.5074
	step [60/143], loss=14.7389
	step [61/143], loss=14.2100
	step [62/143], loss=17.6435
	step [63/143], loss=13.9066
	step [64/143], loss=15.0772
	step [65/143], loss=13.3512
	step [66/143], loss=15.5612
	step [67/143], loss=14.6136
	step [68/143], loss=14.2776
	step [69/143], loss=16.2224
	step [70/143], loss=12.1684
	step [71/143], loss=14.8215
	step [72/143], loss=14.6159
	step [73/143], loss=14.3502
	step [74/143], loss=16.1436
	step [75/143], loss=14.2753
	step [76/143], loss=14.9913
	step [77/143], loss=14.3142
	step [78/143], loss=14.8865
	step [79/143], loss=14.1836
	step [80/143], loss=14.9876
	step [81/143], loss=15.7140
	step [82/143], loss=13.9223
	step [83/143], loss=13.9615
	step [84/143], loss=15.1189
	step [85/143], loss=14.4436
	step [86/143], loss=13.2359
	step [87/143], loss=14.7323
	step [88/143], loss=14.8938
	step [89/143], loss=15.1337
	step [90/143], loss=15.6147
	step [91/143], loss=14.2399
	step [92/143], loss=14.2079
	step [93/143], loss=14.2211
	step [94/143], loss=14.8380
	step [95/143], loss=14.4622
	step [96/143], loss=15.4686
	step [97/143], loss=13.5610
	step [98/143], loss=14.1919
	step [99/143], loss=13.6911
	step [100/143], loss=15.7046
	step [101/143], loss=16.1166
	step [102/143], loss=13.5476
	step [103/143], loss=15.4523
	step [104/143], loss=13.5476
	step [105/143], loss=15.3859
	step [106/143], loss=16.5420
	step [107/143], loss=14.3376
	step [108/143], loss=13.8553
	step [109/143], loss=12.6011
	step [110/143], loss=14.1399
	step [111/143], loss=15.0556
	step [112/143], loss=14.3745
	step [113/143], loss=14.6277
	step [114/143], loss=15.2359
	step [115/143], loss=15.4368
	step [116/143], loss=13.0312
	step [117/143], loss=13.4812
	step [118/143], loss=15.2349
	step [119/143], loss=15.1486
	step [120/143], loss=13.9103
	step [121/143], loss=14.8868
	step [122/143], loss=15.5167
	step [123/143], loss=16.0295
	step [124/143], loss=13.7721
	step [125/143], loss=15.7786
	step [126/143], loss=13.7702
	step [127/143], loss=15.9006
	step [128/143], loss=15.3795
	step [129/143], loss=14.6384
	step [130/143], loss=13.5745
	step [131/143], loss=15.9887
	step [132/143], loss=13.1539
	step [133/143], loss=11.9559
	step [134/143], loss=15.3722
	step [135/143], loss=14.2506
	step [136/143], loss=14.5494
	step [137/143], loss=14.1935
	step [138/143], loss=14.3079
	step [139/143], loss=15.5541
	step [140/143], loss=13.8870
	step [141/143], loss=14.7996
	step [142/143], loss=15.2720
	step [143/143], loss=12.3428
	Evaluating
	loss=0.0320, precision=0.2698, recall=0.9873, f1=0.4238
saving model as: 1_saved_model.pth
Training epoch 20
	step [1/143], loss=13.2855
	step [2/143], loss=12.6697
	step [3/143], loss=16.3043
	step [4/143], loss=14.1522
	step [5/143], loss=14.0190
	step [6/143], loss=15.0033
	step [7/143], loss=13.7440
	step [8/143], loss=12.9555
	step [9/143], loss=16.0538
	step [10/143], loss=16.0578
	step [11/143], loss=14.4112
	step [12/143], loss=15.1098
	step [13/143], loss=14.1037
	step [14/143], loss=14.7379
	step [15/143], loss=14.1445
	step [16/143], loss=14.5266
	step [17/143], loss=13.3831
	step [18/143], loss=13.4213
	step [19/143], loss=13.3644
	step [20/143], loss=13.5135
	step [21/143], loss=14.6748
	step [22/143], loss=14.3499
	step [23/143], loss=14.2252
	step [24/143], loss=13.7198
	step [25/143], loss=13.4327
	step [26/143], loss=16.1104
	step [27/143], loss=12.5732
	step [28/143], loss=13.7934
	step [29/143], loss=13.9414
	step [30/143], loss=13.6098
	step [31/143], loss=12.3017
	step [32/143], loss=12.5924
	step [33/143], loss=14.3486
	step [34/143], loss=13.0920
	step [35/143], loss=13.0573
	step [36/143], loss=15.8349
	step [37/143], loss=15.6546
	step [38/143], loss=13.1011
	step [39/143], loss=12.4706
	step [40/143], loss=14.1760
	step [41/143], loss=15.4962
	step [42/143], loss=13.9695
	step [43/143], loss=15.3557
	step [44/143], loss=14.0248
	step [45/143], loss=15.2488
	step [46/143], loss=13.5740
	step [47/143], loss=14.7638
	step [48/143], loss=13.8992
	step [49/143], loss=15.3891
	step [50/143], loss=13.9422
	step [51/143], loss=13.0173
	step [52/143], loss=16.2013
	step [53/143], loss=13.6149
	step [54/143], loss=14.9307
	step [55/143], loss=13.7841
	step [56/143], loss=13.7053
	step [57/143], loss=15.6860
	step [58/143], loss=16.8492
	step [59/143], loss=14.1970
	step [60/143], loss=14.7507
	step [61/143], loss=13.4953
	step [62/143], loss=13.0736
	step [63/143], loss=14.0626
	step [64/143], loss=13.8814
	step [65/143], loss=13.8215
	step [66/143], loss=11.6430
	step [67/143], loss=13.2105
	step [68/143], loss=14.3930
	step [69/143], loss=12.5051
	step [70/143], loss=12.4392
	step [71/143], loss=14.8979
	step [72/143], loss=14.4153
	step [73/143], loss=14.3215
	step [74/143], loss=12.7898
	step [75/143], loss=12.9140
	step [76/143], loss=12.4476
	step [77/143], loss=13.3943
	step [78/143], loss=12.9485
	step [79/143], loss=14.7264
	step [80/143], loss=13.6955
	step [81/143], loss=13.1650
	step [82/143], loss=14.3152
	step [83/143], loss=12.8750
	step [84/143], loss=14.0871
	step [85/143], loss=14.4175
	step [86/143], loss=13.4536
	step [87/143], loss=13.9688
	step [88/143], loss=13.0753
	step [89/143], loss=13.7703
	step [90/143], loss=14.6433
	step [91/143], loss=12.9620
	step [92/143], loss=12.5884
	step [93/143], loss=14.4069
	step [94/143], loss=14.1442
	step [95/143], loss=14.1650
	step [96/143], loss=13.4673
	step [97/143], loss=13.7552
	step [98/143], loss=14.2558
	step [99/143], loss=13.5156
	step [100/143], loss=12.8055
	step [101/143], loss=14.9442
	step [102/143], loss=12.4534
	step [103/143], loss=14.8583
	step [104/143], loss=12.5399
	step [105/143], loss=12.6719
	step [106/143], loss=14.4860
	step [107/143], loss=16.1497
	step [108/143], loss=13.7849
	step [109/143], loss=12.3334
	step [110/143], loss=15.3396
	step [111/143], loss=14.0851
	step [112/143], loss=13.3408
	step [113/143], loss=14.9964
	step [114/143], loss=16.1781
	step [115/143], loss=13.5898
	step [116/143], loss=16.8369
	step [117/143], loss=15.7862
	step [118/143], loss=12.7656
	step [119/143], loss=13.7846
	step [120/143], loss=15.6142
	step [121/143], loss=13.6651
	step [122/143], loss=13.4283
	step [123/143], loss=14.3514
	step [124/143], loss=11.8877
	step [125/143], loss=13.2018
	step [126/143], loss=13.7342
	step [127/143], loss=13.0724
	step [128/143], loss=14.5754
	step [129/143], loss=13.8797
	step [130/143], loss=11.8595
	step [131/143], loss=12.7424
	step [132/143], loss=14.1931
	step [133/143], loss=12.7972
	step [134/143], loss=15.0379
	step [135/143], loss=13.5548
	step [136/143], loss=12.5638
	step [137/143], loss=12.3233
	step [138/143], loss=15.3539
	step [139/143], loss=14.0286
	step [140/143], loss=13.1074
	step [141/143], loss=15.3571
	step [142/143], loss=14.7855
	step [143/143], loss=11.6252
	Evaluating
	loss=0.0401, precision=0.1981, recall=0.9914, f1=0.3302
Training epoch 21
	step [1/143], loss=15.1951
	step [2/143], loss=12.0807
	step [3/143], loss=14.5654
	step [4/143], loss=13.2701
	step [5/143], loss=12.2926
	step [6/143], loss=14.3751
	step [7/143], loss=14.8784
	step [8/143], loss=13.7456
	step [9/143], loss=13.5459
	step [10/143], loss=13.1417
	step [11/143], loss=10.9260
	step [12/143], loss=13.9388
	step [13/143], loss=13.6157
	step [14/143], loss=13.7129
	step [15/143], loss=13.0101
	step [16/143], loss=13.6836
	step [17/143], loss=12.6180
	step [18/143], loss=14.7017
	step [19/143], loss=12.5346
	step [20/143], loss=13.0137
	step [21/143], loss=13.0225
	step [22/143], loss=14.5542
	step [23/143], loss=13.9045
	step [24/143], loss=13.2887
	step [25/143], loss=13.0371
	step [26/143], loss=13.2762
	step [27/143], loss=13.8365
	step [28/143], loss=14.0221
	step [29/143], loss=11.2300
	step [30/143], loss=15.0842
	step [31/143], loss=13.0026
	step [32/143], loss=13.0023
	step [33/143], loss=14.1461
	step [34/143], loss=13.0259
	step [35/143], loss=12.2511
	step [36/143], loss=13.3934
	step [37/143], loss=15.0885
	step [38/143], loss=12.4950
	step [39/143], loss=11.9462
	step [40/143], loss=13.2241
	step [41/143], loss=13.2717
	step [42/143], loss=12.5821
	step [43/143], loss=12.2743
	step [44/143], loss=13.9128
	step [45/143], loss=13.9151
	step [46/143], loss=12.5711
	step [47/143], loss=13.4931
	step [48/143], loss=13.4371
	step [49/143], loss=12.1241
	step [50/143], loss=12.6382
	step [51/143], loss=12.7788
	step [52/143], loss=12.5903
	step [53/143], loss=13.4998
	step [54/143], loss=13.0894
	step [55/143], loss=13.2800
	step [56/143], loss=14.1562
	step [57/143], loss=14.1767
	step [58/143], loss=13.8902
	step [59/143], loss=17.1043
	step [60/143], loss=13.5134
	step [61/143], loss=13.9110
	step [62/143], loss=13.4350
	step [63/143], loss=13.6429
	step [64/143], loss=15.1764
	step [65/143], loss=12.7847
	step [66/143], loss=13.3367
	step [67/143], loss=13.5057
	step [68/143], loss=13.6240
	step [69/143], loss=14.4347
	step [70/143], loss=12.7238
	step [71/143], loss=13.6218
	step [72/143], loss=12.0871
	step [73/143], loss=12.6335
	step [74/143], loss=11.6631
	step [75/143], loss=12.0442
	step [76/143], loss=13.7971
	step [77/143], loss=12.4270
	step [78/143], loss=12.4653
	step [79/143], loss=10.5765
	step [80/143], loss=14.5560
	step [81/143], loss=13.2587
	step [82/143], loss=15.2029
	step [83/143], loss=11.7573
	step [84/143], loss=13.2886
	step [85/143], loss=13.3427
	step [86/143], loss=13.1723
	step [87/143], loss=13.0516
	step [88/143], loss=11.9280
	step [89/143], loss=11.7858
	step [90/143], loss=14.2846
	step [91/143], loss=13.1532
	step [92/143], loss=13.7323
	step [93/143], loss=13.8160
	step [94/143], loss=14.4456
	step [95/143], loss=13.8790
	step [96/143], loss=13.2927
	step [97/143], loss=13.7007
	step [98/143], loss=13.2274
	step [99/143], loss=14.4112
	step [100/143], loss=13.6529
	step [101/143], loss=14.1101
	step [102/143], loss=10.8339
	step [103/143], loss=13.4034
	step [104/143], loss=12.8047
	step [105/143], loss=15.3207
	step [106/143], loss=12.8970
	step [107/143], loss=13.4316
	step [108/143], loss=12.8775
	step [109/143], loss=15.3617
	step [110/143], loss=13.4423
	step [111/143], loss=12.5699
	step [112/143], loss=14.4978
	step [113/143], loss=14.6388
	step [114/143], loss=12.1713
	step [115/143], loss=12.2123
	step [116/143], loss=12.9626
	step [117/143], loss=14.5902
	step [118/143], loss=11.0460
	step [119/143], loss=12.2032
	step [120/143], loss=12.8244
	step [121/143], loss=13.1394
	step [122/143], loss=13.3889
	step [123/143], loss=13.0306
	step [124/143], loss=13.1678
	step [125/143], loss=13.4591
	step [126/143], loss=12.5117
	step [127/143], loss=14.8993
	step [128/143], loss=12.8838
	step [129/143], loss=14.9038
	step [130/143], loss=13.7889
	step [131/143], loss=14.1025
	step [132/143], loss=11.9142
	step [133/143], loss=11.7364
	step [134/143], loss=12.9930
	step [135/143], loss=13.5112
	step [136/143], loss=14.1922
	step [137/143], loss=11.6018
	step [138/143], loss=12.2507
	step [139/143], loss=13.0180
	step [140/143], loss=14.2880
	step [141/143], loss=12.9137
	step [142/143], loss=14.1760
	step [143/143], loss=9.4637
	Evaluating
	loss=0.0362, precision=0.2134, recall=0.9902, f1=0.3512
Training epoch 22
	step [1/143], loss=12.1829
	step [2/143], loss=14.1710
	step [3/143], loss=13.4347
	step [4/143], loss=14.5356
	step [5/143], loss=13.7374
	step [6/143], loss=12.1175
	step [7/143], loss=14.1329
	step [8/143], loss=14.0318
	step [9/143], loss=11.6983
	step [10/143], loss=11.9917
	step [11/143], loss=12.3680
	step [12/143], loss=14.2089
	step [13/143], loss=13.5498
	step [14/143], loss=10.9744
	step [15/143], loss=13.4916
	step [16/143], loss=12.8343
	step [17/143], loss=13.2706
	step [18/143], loss=14.1793
	step [19/143], loss=15.5181
	step [20/143], loss=13.7345
	step [21/143], loss=10.9905
	step [22/143], loss=13.4479
	step [23/143], loss=11.6743
	step [24/143], loss=13.5132
	step [25/143], loss=12.4029
	step [26/143], loss=12.7712
	step [27/143], loss=11.7775
	step [28/143], loss=13.7574
	step [29/143], loss=13.2834
	step [30/143], loss=12.5504
	step [31/143], loss=14.1811
	step [32/143], loss=12.2004
	step [33/143], loss=12.3195
	step [34/143], loss=14.9729
	step [35/143], loss=15.0382
	step [36/143], loss=12.1321
	step [37/143], loss=14.3300
	step [38/143], loss=13.9869
	step [39/143], loss=12.0382
	step [40/143], loss=13.2674
	step [41/143], loss=12.5472
	step [42/143], loss=12.4911
	step [43/143], loss=13.5634
	step [44/143], loss=11.9856
	step [45/143], loss=13.0068
	step [46/143], loss=12.0661
	step [47/143], loss=10.7708
	step [48/143], loss=11.5948
	step [49/143], loss=12.2800
	step [50/143], loss=15.6574
	step [51/143], loss=11.2160
	step [52/143], loss=10.6352
	step [53/143], loss=14.9148
	step [54/143], loss=12.9804
	step [55/143], loss=14.1396
	step [56/143], loss=12.5981
	step [57/143], loss=14.3033
	step [58/143], loss=12.3559
	step [59/143], loss=12.2795
	step [60/143], loss=12.8173
	step [61/143], loss=14.4366
	step [62/143], loss=14.0115
	step [63/143], loss=13.1336
	step [64/143], loss=11.6663
	step [65/143], loss=12.5622
	step [66/143], loss=12.5786
	step [67/143], loss=11.9079
	step [68/143], loss=13.8075
	step [69/143], loss=12.4557
	step [70/143], loss=12.4789
	step [71/143], loss=11.9478
	step [72/143], loss=13.7884
	step [73/143], loss=12.2913
	step [74/143], loss=14.4978
	step [75/143], loss=11.7210
	step [76/143], loss=11.8100
	step [77/143], loss=11.1220
	step [78/143], loss=12.4491
	step [79/143], loss=12.5726
	step [80/143], loss=12.4992
	step [81/143], loss=16.4402
	step [82/143], loss=13.3875
	step [83/143], loss=12.7954
	step [84/143], loss=11.9826
	step [85/143], loss=11.9078
	step [86/143], loss=11.5755
	step [87/143], loss=12.8360
	step [88/143], loss=13.4334
	step [89/143], loss=13.7036
	step [90/143], loss=11.7950
	step [91/143], loss=11.9784
	step [92/143], loss=12.7218
	step [93/143], loss=12.8920
	step [94/143], loss=11.8013
	step [95/143], loss=14.5271
	step [96/143], loss=12.9521
	step [97/143], loss=11.9804
	step [98/143], loss=14.6956
	step [99/143], loss=12.3388
	step [100/143], loss=12.8762
	step [101/143], loss=11.7616
	step [102/143], loss=11.8560
	step [103/143], loss=12.4541
	step [104/143], loss=10.6418
	step [105/143], loss=13.3911
	step [106/143], loss=15.3573
	step [107/143], loss=15.2858
	step [108/143], loss=13.1300
	step [109/143], loss=12.8779
	step [110/143], loss=14.2934
	step [111/143], loss=13.5331
	step [112/143], loss=12.0684
	step [113/143], loss=13.0411
	step [114/143], loss=13.9234
	step [115/143], loss=13.9019
	step [116/143], loss=12.5811
	step [117/143], loss=13.0793
	step [118/143], loss=14.1523
	step [119/143], loss=13.5607
	step [120/143], loss=11.9274
	step [121/143], loss=12.3756
	step [122/143], loss=12.6651
	step [123/143], loss=11.2489
	step [124/143], loss=12.8632
	step [125/143], loss=13.6349
	step [126/143], loss=12.2579
	step [127/143], loss=13.8243
	step [128/143], loss=13.2829
	step [129/143], loss=11.6860
	step [130/143], loss=12.9628
	step [131/143], loss=14.2329
	step [132/143], loss=11.7871
	step [133/143], loss=11.8523
	step [134/143], loss=12.9233
	step [135/143], loss=12.1121
	step [136/143], loss=11.2910
	step [137/143], loss=14.4189
	step [138/143], loss=11.5821
	step [139/143], loss=11.3106
	step [140/143], loss=14.6456
	step [141/143], loss=13.5479
	step [142/143], loss=15.8089
	step [143/143], loss=10.0808
	Evaluating
	loss=0.0289, precision=0.2715, recall=0.9880, f1=0.4260
saving model as: 1_saved_model.pth
Training epoch 23
	step [1/143], loss=13.8342
	step [2/143], loss=13.2238
	step [3/143], loss=11.5343
	step [4/143], loss=10.8347
	step [5/143], loss=11.2073
	step [6/143], loss=12.8143
	step [7/143], loss=12.8071
	step [8/143], loss=13.2866
	step [9/143], loss=13.4175
	step [10/143], loss=13.6513
	step [11/143], loss=12.6173
	step [12/143], loss=14.3094
	step [13/143], loss=12.4995
	step [14/143], loss=12.1752
	step [15/143], loss=10.2233
	step [16/143], loss=11.0589
	step [17/143], loss=12.0264
	step [18/143], loss=12.5646
	step [19/143], loss=14.2408
	step [20/143], loss=11.0845
	step [21/143], loss=12.9076
	step [22/143], loss=13.2885
	step [23/143], loss=10.8902
	step [24/143], loss=12.3643
	step [25/143], loss=11.5895
	step [26/143], loss=11.7264
	step [27/143], loss=12.0008
	step [28/143], loss=12.4216
	step [29/143], loss=10.8470
	step [30/143], loss=10.8371
	step [31/143], loss=15.1185
	step [32/143], loss=12.3725
	step [33/143], loss=11.4951
	step [34/143], loss=12.0461
	step [35/143], loss=13.4476
	step [36/143], loss=12.9392
	step [37/143], loss=12.8626
	step [38/143], loss=14.4885
	step [39/143], loss=12.1011
	step [40/143], loss=11.0645
	step [41/143], loss=13.8380
	step [42/143], loss=14.5323
	step [43/143], loss=12.2723
	step [44/143], loss=12.1346
	step [45/143], loss=10.9035
	step [46/143], loss=13.0582
	step [47/143], loss=10.9846
	step [48/143], loss=12.3934
	step [49/143], loss=10.9750
	step [50/143], loss=14.0084
	step [51/143], loss=13.6194
	step [52/143], loss=13.2216
	step [53/143], loss=11.6205
	step [54/143], loss=13.0042
	step [55/143], loss=11.5062
	step [56/143], loss=11.7076
	step [57/143], loss=12.1573
	step [58/143], loss=12.9266
	step [59/143], loss=13.1703
	step [60/143], loss=12.0278
	step [61/143], loss=12.4170
	step [62/143], loss=12.0985
	step [63/143], loss=12.3796
	step [64/143], loss=11.8132
	step [65/143], loss=12.5342
	step [66/143], loss=11.2830
	step [67/143], loss=12.0431
	step [68/143], loss=12.2825
	step [69/143], loss=11.4059
	step [70/143], loss=11.7333
	step [71/143], loss=13.1953
	step [72/143], loss=11.2800
	step [73/143], loss=12.1961
	step [74/143], loss=10.7925
	step [75/143], loss=12.4713
	step [76/143], loss=14.8518
	step [77/143], loss=12.2988
	step [78/143], loss=12.0316
	step [79/143], loss=10.9167
	step [80/143], loss=12.4271
	step [81/143], loss=13.5014
	step [82/143], loss=9.9964
	step [83/143], loss=13.3415
	step [84/143], loss=10.9899
	step [85/143], loss=11.3116
	step [86/143], loss=12.4815
	step [87/143], loss=11.9630
	step [88/143], loss=11.4505
	step [89/143], loss=13.8136
	step [90/143], loss=14.4613
	step [91/143], loss=12.0016
	step [92/143], loss=12.5100
	step [93/143], loss=11.2044
	step [94/143], loss=11.8910
	step [95/143], loss=12.7250
	step [96/143], loss=11.7272
	step [97/143], loss=11.2510
	step [98/143], loss=13.1727
	step [99/143], loss=13.4061
	step [100/143], loss=11.3240
	step [101/143], loss=12.4803
	step [102/143], loss=11.5655
	step [103/143], loss=11.3521
	step [104/143], loss=14.4703
	step [105/143], loss=12.0563
	step [106/143], loss=11.9823
	step [107/143], loss=12.4200
	step [108/143], loss=15.0180
	step [109/143], loss=12.6293
	step [110/143], loss=12.3026
	step [111/143], loss=12.2589
	step [112/143], loss=12.4452
	step [113/143], loss=13.8485
	step [114/143], loss=13.2050
	step [115/143], loss=11.8765
	step [116/143], loss=10.7200
	step [117/143], loss=11.4341
	step [118/143], loss=12.7723
	step [119/143], loss=13.4982
	step [120/143], loss=11.8446
	step [121/143], loss=14.1009
	step [122/143], loss=10.9372
	step [123/143], loss=13.3150
	step [124/143], loss=11.5038
	step [125/143], loss=12.9669
	step [126/143], loss=12.0240
	step [127/143], loss=13.0060
	step [128/143], loss=13.3674
	step [129/143], loss=11.1472
	step [130/143], loss=12.8141
	step [131/143], loss=13.1380
	step [132/143], loss=12.1410
	step [133/143], loss=11.3027
	step [134/143], loss=11.2687
	step [135/143], loss=14.6074
	step [136/143], loss=11.4752
	step [137/143], loss=10.5233
	step [138/143], loss=14.0636
	step [139/143], loss=12.2880
	step [140/143], loss=13.8664
	step [141/143], loss=12.8187
	step [142/143], loss=12.9008
	step [143/143], loss=12.8527
	Evaluating
	loss=0.0319, precision=0.2367, recall=0.9888, f1=0.3820
Training epoch 24
	step [1/143], loss=11.2297
	step [2/143], loss=11.0353
	step [3/143], loss=10.7836
	step [4/143], loss=13.2238
	step [5/143], loss=13.3377
	step [6/143], loss=12.5609
	step [7/143], loss=11.1222
	step [8/143], loss=13.0042
	step [9/143], loss=11.5924
	step [10/143], loss=11.3820
	step [11/143], loss=12.7584
	step [12/143], loss=11.4847
	step [13/143], loss=11.5399
	step [14/143], loss=11.6237
	step [15/143], loss=10.2725
	step [16/143], loss=12.3555
	step [17/143], loss=11.9050
	step [18/143], loss=12.3252
	step [19/143], loss=14.4990
	step [20/143], loss=11.8890
	step [21/143], loss=11.1124
	step [22/143], loss=11.7718
	step [23/143], loss=12.3285
	step [24/143], loss=12.4094
	step [25/143], loss=10.8497
	step [26/143], loss=12.2150
	step [27/143], loss=13.9133
	step [28/143], loss=13.7605
	step [29/143], loss=12.1468
	step [30/143], loss=13.3130
	step [31/143], loss=10.8119
	step [32/143], loss=11.5406
	step [33/143], loss=10.4611
	step [34/143], loss=13.3428
	step [35/143], loss=10.9561
	step [36/143], loss=12.8495
	step [37/143], loss=11.9063
	step [38/143], loss=12.2844
	step [39/143], loss=11.5733
	step [40/143], loss=11.8459
	step [41/143], loss=12.0115
	step [42/143], loss=12.0367
	step [43/143], loss=13.3925
	step [44/143], loss=11.5791
	step [45/143], loss=11.3260
	step [46/143], loss=11.6127
	step [47/143], loss=11.5044
	step [48/143], loss=11.5149
	step [49/143], loss=11.8141
	step [50/143], loss=11.9223
	step [51/143], loss=13.4725
	step [52/143], loss=10.5982
	step [53/143], loss=12.1643
	step [54/143], loss=11.2118
	step [55/143], loss=12.3477
	step [56/143], loss=10.1028
	step [57/143], loss=14.1546
	step [58/143], loss=12.0717
	step [59/143], loss=12.7604
	step [60/143], loss=12.0702
	step [61/143], loss=11.8644
	step [62/143], loss=11.5394
	step [63/143], loss=11.4519
	step [64/143], loss=13.4393
	step [65/143], loss=11.0849
	step [66/143], loss=12.9690
	step [67/143], loss=12.9434
	step [68/143], loss=11.8402
	step [69/143], loss=12.0162
	step [70/143], loss=10.5432
	step [71/143], loss=11.9848
	step [72/143], loss=13.9942
	step [73/143], loss=11.4965
	step [74/143], loss=10.8998
	step [75/143], loss=11.6180
	step [76/143], loss=11.0647
	step [77/143], loss=10.5523
	step [78/143], loss=10.4115
	step [79/143], loss=11.6198
	step [80/143], loss=14.8179
	step [81/143], loss=11.6543
	step [82/143], loss=13.1779
	step [83/143], loss=12.9544
	step [84/143], loss=9.7699
	step [85/143], loss=13.6447
	step [86/143], loss=11.0229
	step [87/143], loss=8.9372
	step [88/143], loss=12.6269
	step [89/143], loss=9.6467
	step [90/143], loss=12.4769
	step [91/143], loss=9.8617
	step [92/143], loss=10.6001
	step [93/143], loss=9.9918
	step [94/143], loss=11.3480
	step [95/143], loss=12.2480
	step [96/143], loss=11.8814
	step [97/143], loss=12.5181
	step [98/143], loss=12.0115
	step [99/143], loss=13.7098
	step [100/143], loss=11.7776
	step [101/143], loss=12.3664
	step [102/143], loss=13.0376
	step [103/143], loss=10.6986
	step [104/143], loss=11.9441
	step [105/143], loss=9.9979
	step [106/143], loss=10.5645
	step [107/143], loss=12.8490
	step [108/143], loss=11.5174
	step [109/143], loss=11.6393
	step [110/143], loss=12.4162
	step [111/143], loss=11.5114
	step [112/143], loss=11.7708
	step [113/143], loss=12.0467
	step [114/143], loss=10.2674
	step [115/143], loss=12.7903
	step [116/143], loss=11.6702
	step [117/143], loss=11.2678
	step [118/143], loss=11.0032
	step [119/143], loss=10.4449
	step [120/143], loss=11.0370
	step [121/143], loss=12.0252
	step [122/143], loss=11.8510
	step [123/143], loss=15.1741
	step [124/143], loss=12.4416
	step [125/143], loss=10.7941
	step [126/143], loss=11.0993
	step [127/143], loss=11.7701
	step [128/143], loss=11.1327
	step [129/143], loss=11.2255
	step [130/143], loss=10.7897
	step [131/143], loss=11.7470
	step [132/143], loss=11.6230
	step [133/143], loss=12.3647
	step [134/143], loss=12.0855
	step [135/143], loss=10.6915
	step [136/143], loss=13.0429
	step [137/143], loss=11.7007
	step [138/143], loss=10.6069
	step [139/143], loss=12.4775
	step [140/143], loss=12.6246
	step [141/143], loss=13.2596
	step [142/143], loss=11.4504
	step [143/143], loss=8.7019
	Evaluating
	loss=0.0320, precision=0.2302, recall=0.9890, f1=0.3735
Training epoch 25
	step [1/143], loss=11.5824
	step [2/143], loss=10.9874
	step [3/143], loss=12.6276
	step [4/143], loss=11.4069
	step [5/143], loss=11.4867
	step [6/143], loss=10.5178
	step [7/143], loss=10.8102
	step [8/143], loss=12.1364
	step [9/143], loss=10.8684
	step [10/143], loss=11.3771
	step [11/143], loss=9.9558
	step [12/143], loss=11.1148
	step [13/143], loss=10.2834
	step [14/143], loss=11.5098
	step [15/143], loss=12.0574
	step [16/143], loss=11.6278
	step [17/143], loss=11.1337
	step [18/143], loss=10.6993
	step [19/143], loss=10.0723
	step [20/143], loss=11.8237
	step [21/143], loss=13.6059
	step [22/143], loss=10.1159
	step [23/143], loss=10.0965
	step [24/143], loss=11.0128
	step [25/143], loss=10.2789
	step [26/143], loss=13.6974
	step [27/143], loss=10.2590
	step [28/143], loss=10.6377
	step [29/143], loss=11.9469
	step [30/143], loss=11.8967
	step [31/143], loss=10.6807
	step [32/143], loss=11.3466
	step [33/143], loss=13.1720
	step [34/143], loss=11.0662
	step [35/143], loss=11.1172
	step [36/143], loss=13.1921
	step [37/143], loss=11.6026
	step [38/143], loss=11.0986
	step [39/143], loss=12.9987
	step [40/143], loss=12.1764
	step [41/143], loss=10.3502
	step [42/143], loss=12.6434
	step [43/143], loss=11.9591
	step [44/143], loss=12.1415
	step [45/143], loss=10.7488
	step [46/143], loss=10.8193
	step [47/143], loss=11.5194
	step [48/143], loss=10.8769
	step [49/143], loss=10.9084
	step [50/143], loss=10.5169
	step [51/143], loss=12.3663
	step [52/143], loss=10.9860
	step [53/143], loss=10.5184
	step [54/143], loss=11.0761
	step [55/143], loss=11.3368
	step [56/143], loss=10.6279
	step [57/143], loss=11.9530
	step [58/143], loss=11.1283
	step [59/143], loss=13.3760
	step [60/143], loss=12.1310
	step [61/143], loss=11.8064
	step [62/143], loss=10.2482
	step [63/143], loss=11.2611
	step [64/143], loss=12.2131
	step [65/143], loss=12.0205
	step [66/143], loss=11.1129
	step [67/143], loss=11.0730
	step [68/143], loss=10.1758
	step [69/143], loss=10.4923
	step [70/143], loss=11.3421
	step [71/143], loss=12.9582
	step [72/143], loss=11.6145
	step [73/143], loss=11.2694
	step [74/143], loss=11.6066
	step [75/143], loss=10.2270
	step [76/143], loss=12.2774
	step [77/143], loss=13.1487
	step [78/143], loss=9.8915
	step [79/143], loss=11.8500
	step [80/143], loss=12.0239
	step [81/143], loss=12.1538
	step [82/143], loss=12.5136
	step [83/143], loss=12.0014
	step [84/143], loss=10.9103
	step [85/143], loss=11.6149
	step [86/143], loss=13.0967
	step [87/143], loss=10.9487
	step [88/143], loss=11.2669
	step [89/143], loss=11.2882
	step [90/143], loss=10.2755
	step [91/143], loss=11.9208
	step [92/143], loss=11.2334
	step [93/143], loss=11.7023
	step [94/143], loss=12.6585
	step [95/143], loss=11.0478
	step [96/143], loss=10.0188
	step [97/143], loss=12.7301
	step [98/143], loss=12.5086
	step [99/143], loss=11.2332
	step [100/143], loss=10.6901
	step [101/143], loss=11.6921
	step [102/143], loss=11.9049
	step [103/143], loss=9.8350
	step [104/143], loss=12.0677
	step [105/143], loss=11.1270
	step [106/143], loss=10.1431
	step [107/143], loss=11.3636
	step [108/143], loss=11.6550
	step [109/143], loss=11.2278
	step [110/143], loss=12.1486
	step [111/143], loss=11.5366
	step [112/143], loss=10.8402
	step [113/143], loss=9.3581
	step [114/143], loss=12.5910
	step [115/143], loss=12.2531
	step [116/143], loss=9.9588
	step [117/143], loss=11.9582
	step [118/143], loss=12.0577
	step [119/143], loss=10.1543
	step [120/143], loss=12.9355
	step [121/143], loss=11.4300
	step [122/143], loss=11.1688
	step [123/143], loss=12.5580
	step [124/143], loss=10.6405
	step [125/143], loss=11.8504
	step [126/143], loss=11.1665
	step [127/143], loss=10.5657
	step [128/143], loss=11.3276
	step [129/143], loss=10.9737
	step [130/143], loss=11.1480
	step [131/143], loss=12.2277
	step [132/143], loss=12.0506
	step [133/143], loss=10.5104
	step [134/143], loss=10.8493
	step [135/143], loss=11.0587
	step [136/143], loss=11.4596
	step [137/143], loss=11.2331
	step [138/143], loss=12.9484
	step [139/143], loss=11.3107
	step [140/143], loss=10.9603
	step [141/143], loss=10.3552
	step [142/143], loss=11.7967
	step [143/143], loss=8.9230
	Evaluating
	loss=0.0301, precision=0.2473, recall=0.9889, f1=0.3956
Training epoch 26
	step [1/143], loss=10.7152
	step [2/143], loss=13.0971
	step [3/143], loss=10.7383
	step [4/143], loss=13.0340
	step [5/143], loss=12.2241
	step [6/143], loss=11.3170
	step [7/143], loss=10.1874
	step [8/143], loss=9.8135
	step [9/143], loss=11.6274
	step [10/143], loss=11.0833
	step [11/143], loss=10.0812
	step [12/143], loss=9.6275
	step [13/143], loss=11.1842
	step [14/143], loss=10.6075
	step [15/143], loss=10.4697
	step [16/143], loss=11.4295
	step [17/143], loss=10.1855
	step [18/143], loss=10.5893
	step [19/143], loss=10.6911
	step [20/143], loss=10.7970
	step [21/143], loss=10.6314
	step [22/143], loss=12.1737
	step [23/143], loss=11.2838
	step [24/143], loss=9.6803
	step [25/143], loss=10.9394
	step [26/143], loss=11.2366
	step [27/143], loss=12.6388
	step [28/143], loss=11.5982
	step [29/143], loss=12.8620
	step [30/143], loss=10.3913
	step [31/143], loss=11.2898
	step [32/143], loss=10.7894
	step [33/143], loss=10.0618
	step [34/143], loss=10.4089
	step [35/143], loss=10.8851
	step [36/143], loss=10.6054
	step [37/143], loss=12.7032
	step [38/143], loss=10.7339
	step [39/143], loss=10.3917
	step [40/143], loss=11.4401
	step [41/143], loss=10.3064
	step [42/143], loss=10.1896
	step [43/143], loss=10.6130
	step [44/143], loss=11.5691
	step [45/143], loss=12.0427
	step [46/143], loss=11.5106
	step [47/143], loss=9.5108
	step [48/143], loss=11.6567
	step [49/143], loss=11.2484
	step [50/143], loss=9.9012
	step [51/143], loss=11.4979
	step [52/143], loss=10.8041
	step [53/143], loss=12.7968
	step [54/143], loss=10.3306
	step [55/143], loss=12.5871
	step [56/143], loss=11.2972
	step [57/143], loss=10.0880
	step [58/143], loss=12.1230
	step [59/143], loss=11.8954
	step [60/143], loss=10.8965
	step [61/143], loss=9.8128
	step [62/143], loss=11.1583
	step [63/143], loss=12.0568
	step [64/143], loss=12.2601
	step [65/143], loss=10.1161
	step [66/143], loss=11.2698
	step [67/143], loss=10.6645
	step [68/143], loss=10.5659
	step [69/143], loss=10.1717
	step [70/143], loss=10.6078
	step [71/143], loss=11.9096
	step [72/143], loss=10.6400
	step [73/143], loss=10.2974
	step [74/143], loss=9.5823
	step [75/143], loss=8.8463
	step [76/143], loss=10.9259
	step [77/143], loss=11.1449
	step [78/143], loss=9.4914
	step [79/143], loss=12.4593
	step [80/143], loss=10.9542
	step [81/143], loss=10.5533
	step [82/143], loss=11.1339
	step [83/143], loss=11.7305
	step [84/143], loss=11.6494
	step [85/143], loss=10.8305
	step [86/143], loss=11.0212
	step [87/143], loss=11.1609
	step [88/143], loss=10.3076
	step [89/143], loss=10.3609
	step [90/143], loss=10.3405
	step [91/143], loss=9.9992
	step [92/143], loss=11.1471
	step [93/143], loss=11.0725
	step [94/143], loss=8.8951
	step [95/143], loss=10.7186
	step [96/143], loss=12.3386
	step [97/143], loss=10.5531
	step [98/143], loss=10.5105
	step [99/143], loss=10.7493
	step [100/143], loss=11.5692
	step [101/143], loss=12.5111
	step [102/143], loss=10.5288
	step [103/143], loss=11.3315
	step [104/143], loss=11.9993
	step [105/143], loss=12.2763
	step [106/143], loss=10.7902
	step [107/143], loss=12.1399
	step [108/143], loss=12.9215
	step [109/143], loss=10.9834
	step [110/143], loss=11.2203
	step [111/143], loss=10.2571
	step [112/143], loss=11.7709
	step [113/143], loss=12.4155
	step [114/143], loss=9.9359
	step [115/143], loss=10.0569
	step [116/143], loss=13.3011
	step [117/143], loss=12.2273
	step [118/143], loss=11.2220
	step [119/143], loss=11.4194
	step [120/143], loss=13.2083
	step [121/143], loss=12.3446
	step [122/143], loss=10.9862
	step [123/143], loss=10.2723
	step [124/143], loss=9.9757
	step [125/143], loss=12.7011
	step [126/143], loss=11.6438
	step [127/143], loss=11.9101
	step [128/143], loss=10.9271
	step [129/143], loss=9.0907
	step [130/143], loss=11.1222
	step [131/143], loss=13.2348
	step [132/143], loss=9.0850
	step [133/143], loss=10.8296
	step [134/143], loss=13.9131
	step [135/143], loss=11.7070
	step [136/143], loss=9.6230
	step [137/143], loss=10.4268
	step [138/143], loss=12.2895
	step [139/143], loss=11.1677
	step [140/143], loss=10.1894
	step [141/143], loss=11.4778
	step [142/143], loss=11.7950
	step [143/143], loss=8.4014
	Evaluating
	loss=0.0310, precision=0.2322, recall=0.9893, f1=0.3762
Training epoch 27
	step [1/143], loss=10.0653
	step [2/143], loss=10.3993
	step [3/143], loss=11.0306
	step [4/143], loss=11.9328
	step [5/143], loss=11.7421
	step [6/143], loss=10.3427
	step [7/143], loss=10.4007
	step [8/143], loss=10.6050
	step [9/143], loss=10.3522
	step [10/143], loss=10.9967
	step [11/143], loss=11.6420
	step [12/143], loss=9.7928
	step [13/143], loss=9.9950
	step [14/143], loss=10.2693
	step [15/143], loss=11.0459
	step [16/143], loss=9.1275
	step [17/143], loss=11.1979
	step [18/143], loss=9.4022
	step [19/143], loss=9.4444
	step [20/143], loss=10.8974
	step [21/143], loss=9.8152
	step [22/143], loss=9.9221
	step [23/143], loss=10.3170
	step [24/143], loss=11.3899
	step [25/143], loss=10.3316
	step [26/143], loss=11.9195
	step [27/143], loss=10.1026
	step [28/143], loss=9.5120
	step [29/143], loss=10.9356
	step [30/143], loss=10.5127
	step [31/143], loss=8.9548
	step [32/143], loss=9.6648
	step [33/143], loss=10.5702
	step [34/143], loss=11.1599
	step [35/143], loss=11.2610
	step [36/143], loss=10.1470
	step [37/143], loss=9.0633
	step [38/143], loss=10.0795
	step [39/143], loss=10.0115
	step [40/143], loss=9.9472
	step [41/143], loss=9.7042
	step [42/143], loss=10.7760
	step [43/143], loss=10.4096
	step [44/143], loss=12.0384
	step [45/143], loss=10.7883
	step [46/143], loss=12.7933
	step [47/143], loss=11.3473
	step [48/143], loss=10.8494
	step [49/143], loss=8.6218
	step [50/143], loss=11.4417
	step [51/143], loss=11.3176
	step [52/143], loss=11.9014
	step [53/143], loss=9.7319
	step [54/143], loss=13.2199
	step [55/143], loss=10.5355
	step [56/143], loss=10.6708
	step [57/143], loss=11.1622
	step [58/143], loss=10.7408
	step [59/143], loss=11.1772
	step [60/143], loss=10.6417
	step [61/143], loss=9.8023
	step [62/143], loss=10.9482
	step [63/143], loss=9.5329
	step [64/143], loss=10.0149
	step [65/143], loss=10.5723
	step [66/143], loss=8.5212
	step [67/143], loss=11.0324
	step [68/143], loss=11.2772
	step [69/143], loss=11.8010
	step [70/143], loss=10.4312
	step [71/143], loss=11.1052
	step [72/143], loss=10.9809
	step [73/143], loss=9.4031
	step [74/143], loss=12.0568
	step [75/143], loss=12.2597
	step [76/143], loss=10.4603
	step [77/143], loss=11.3648
	step [78/143], loss=11.9298
	step [79/143], loss=10.1735
	step [80/143], loss=10.7780
	step [81/143], loss=11.8335
	step [82/143], loss=10.8622
	step [83/143], loss=11.3510
	step [84/143], loss=10.1412
	step [85/143], loss=10.8138
	step [86/143], loss=10.3473
	step [87/143], loss=11.4376
	step [88/143], loss=10.3462
	step [89/143], loss=11.2646
	step [90/143], loss=10.4375
	step [91/143], loss=9.9271
	step [92/143], loss=10.4614
	step [93/143], loss=9.6664
	step [94/143], loss=11.4163
	step [95/143], loss=12.4092
	step [96/143], loss=9.0577
	step [97/143], loss=10.0922
	step [98/143], loss=10.7134
	step [99/143], loss=9.7997
	step [100/143], loss=10.8755
	step [101/143], loss=12.8586
	step [102/143], loss=10.0628
	step [103/143], loss=9.7015
	step [104/143], loss=12.2603
	step [105/143], loss=11.1707
	step [106/143], loss=11.7915
	step [107/143], loss=10.8243
	step [108/143], loss=9.3993
	step [109/143], loss=10.9159
	step [110/143], loss=10.8249
	step [111/143], loss=9.4334
	step [112/143], loss=10.8532
	step [113/143], loss=11.1646
	step [114/143], loss=10.1355
	step [115/143], loss=10.5488
	step [116/143], loss=9.8168
	step [117/143], loss=11.5765
	step [118/143], loss=10.2517
	step [119/143], loss=12.6946
	step [120/143], loss=11.2710
	step [121/143], loss=11.5457
	step [122/143], loss=10.7549
	step [123/143], loss=10.9929
	step [124/143], loss=9.8906
	step [125/143], loss=9.9362
	step [126/143], loss=12.4283
	step [127/143], loss=9.8663
	step [128/143], loss=12.3400
	step [129/143], loss=11.9928
	step [130/143], loss=11.4398
	step [131/143], loss=9.7415
	step [132/143], loss=9.9361
	step [133/143], loss=11.1891
	step [134/143], loss=9.9481
	step [135/143], loss=10.1755
	step [136/143], loss=9.8830
	step [137/143], loss=11.6476
	step [138/143], loss=11.8489
	step [139/143], loss=10.2479
	step [140/143], loss=12.1013
	step [141/143], loss=10.2281
	step [142/143], loss=12.2145
	step [143/143], loss=8.5045
	Evaluating
	loss=0.0355, precision=0.1992, recall=0.9912, f1=0.3317
Training epoch 28
	step [1/143], loss=10.1694
	step [2/143], loss=10.0071
	step [3/143], loss=11.7873
	step [4/143], loss=10.0556
	step [5/143], loss=9.9947
	step [6/143], loss=9.3454
	step [7/143], loss=10.9376
	step [8/143], loss=10.8490
	step [9/143], loss=9.9010
	step [10/143], loss=9.3387
	step [11/143], loss=9.8456
	step [12/143], loss=9.9018
	step [13/143], loss=10.2737
	step [14/143], loss=9.5273
	step [15/143], loss=9.8449
	step [16/143], loss=8.9089
	step [17/143], loss=9.6932
	step [18/143], loss=10.5857
	step [19/143], loss=11.9546
	step [20/143], loss=9.8623
	step [21/143], loss=10.8391
	step [22/143], loss=11.2206
	step [23/143], loss=9.6596
	step [24/143], loss=10.0386
	step [25/143], loss=9.9276
	step [26/143], loss=9.9605
	step [27/143], loss=10.9825
	step [28/143], loss=10.1404
	step [29/143], loss=9.6588
	step [30/143], loss=9.3322
	step [31/143], loss=10.3392
	step [32/143], loss=10.1942
	step [33/143], loss=10.7167
	step [34/143], loss=11.1340
	step [35/143], loss=8.3679
	step [36/143], loss=11.0040
	step [37/143], loss=9.5883
	step [38/143], loss=10.1805
	step [39/143], loss=10.3968
	step [40/143], loss=11.1660
	step [41/143], loss=8.9843
	step [42/143], loss=10.7446
	step [43/143], loss=10.4879
	step [44/143], loss=9.9537
	step [45/143], loss=10.3656
	step [46/143], loss=10.4460
	step [47/143], loss=10.7325
	step [48/143], loss=10.9332
	step [49/143], loss=11.2374
	step [50/143], loss=11.3122
	step [51/143], loss=10.2665
	step [52/143], loss=9.0594
	step [53/143], loss=11.2363
	step [54/143], loss=10.2068
	step [55/143], loss=9.6835
	step [56/143], loss=10.0793
	step [57/143], loss=11.0353
	step [58/143], loss=9.7348
	step [59/143], loss=10.2235
	step [60/143], loss=12.6511
	step [61/143], loss=11.1280
	step [62/143], loss=10.1941
	step [63/143], loss=9.6315
	step [64/143], loss=9.8779
	step [65/143], loss=11.8417
	step [66/143], loss=10.7769
	step [67/143], loss=9.9776
	step [68/143], loss=11.1212
	step [69/143], loss=11.0452
	step [70/143], loss=10.8678
	step [71/143], loss=10.5946
	step [72/143], loss=10.4049
	step [73/143], loss=9.8498
	step [74/143], loss=10.8461
	step [75/143], loss=10.4801
	step [76/143], loss=10.1016
	step [77/143], loss=9.6914
	step [78/143], loss=10.5588
	step [79/143], loss=11.1114
	step [80/143], loss=10.3983
	step [81/143], loss=9.6995
	step [82/143], loss=10.2372
	step [83/143], loss=9.9443
	step [84/143], loss=9.9821
	step [85/143], loss=8.5926
	step [86/143], loss=9.3101
	step [87/143], loss=9.9727
	step [88/143], loss=11.1442
	step [89/143], loss=10.2446
	step [90/143], loss=8.6197
	step [91/143], loss=9.0440
	step [92/143], loss=10.6972
	step [93/143], loss=11.7532
	step [94/143], loss=11.2951
	step [95/143], loss=9.2400
	step [96/143], loss=9.7603
	step [97/143], loss=11.1226
	step [98/143], loss=9.8642
	step [99/143], loss=9.6343
	step [100/143], loss=10.0822
	step [101/143], loss=9.8751
	step [102/143], loss=11.2035
	step [103/143], loss=11.4309
	step [104/143], loss=11.1360
	step [105/143], loss=10.8808
	step [106/143], loss=11.2254
	step [107/143], loss=10.6506
	step [108/143], loss=10.7374
	step [109/143], loss=11.5388
	step [110/143], loss=12.5505
	step [111/143], loss=8.4105
	step [112/143], loss=11.1906
	step [113/143], loss=9.8657
	step [114/143], loss=10.6067
	step [115/143], loss=10.0091
	step [116/143], loss=9.5017
	step [117/143], loss=9.2512
	step [118/143], loss=12.2470
	step [119/143], loss=9.7655
	step [120/143], loss=11.1220
	step [121/143], loss=10.2932
	step [122/143], loss=10.2214
	step [123/143], loss=10.0971
	step [124/143], loss=9.9978
	step [125/143], loss=10.2194
	step [126/143], loss=9.2609
	step [127/143], loss=11.6271
	step [128/143], loss=11.2289
	step [129/143], loss=10.2293
	step [130/143], loss=10.0951
	step [131/143], loss=9.9285
	step [132/143], loss=11.5259
	step [133/143], loss=8.4386
	step [134/143], loss=9.5255
	step [135/143], loss=9.9698
	step [136/143], loss=11.5580
	step [137/143], loss=10.8359
	step [138/143], loss=10.8605
	step [139/143], loss=10.2725
	step [140/143], loss=9.7802
	step [141/143], loss=9.3730
	step [142/143], loss=9.8330
	step [143/143], loss=7.6397
	Evaluating
	loss=0.0285, precision=0.2440, recall=0.9888, f1=0.3914
Training epoch 29
	step [1/143], loss=10.7936
	step [2/143], loss=10.6416
	step [3/143], loss=9.9524
	step [4/143], loss=10.9143
	step [5/143], loss=9.8072
	step [6/143], loss=9.5009
	step [7/143], loss=11.0742
	step [8/143], loss=11.5107
	step [9/143], loss=11.6151
	step [10/143], loss=9.5913
	step [11/143], loss=9.4860
	step [12/143], loss=11.7690
	step [13/143], loss=9.8465
	step [14/143], loss=10.8259
	step [15/143], loss=9.2315
	step [16/143], loss=9.5739
	step [17/143], loss=9.7847
	step [18/143], loss=10.5764
	step [19/143], loss=10.1365
	step [20/143], loss=8.8585
	step [21/143], loss=11.5129
	step [22/143], loss=9.7575
	step [23/143], loss=10.2094
	step [24/143], loss=11.2286
	step [25/143], loss=8.4460
	step [26/143], loss=11.9109
	step [27/143], loss=9.2896
	step [28/143], loss=9.9644
	step [29/143], loss=10.8866
	step [30/143], loss=10.6547
	step [31/143], loss=9.0728
	step [32/143], loss=11.6799
	step [33/143], loss=11.3616
	step [34/143], loss=11.7679
	step [35/143], loss=10.4139
	step [36/143], loss=11.3336
	step [37/143], loss=10.7092
	step [38/143], loss=12.5488
	step [39/143], loss=10.3862
	step [40/143], loss=11.1174
	step [41/143], loss=9.8771
	step [42/143], loss=9.8209
	step [43/143], loss=10.2942
	step [44/143], loss=9.5294
	step [45/143], loss=11.9339
	step [46/143], loss=9.8399
	step [47/143], loss=9.1392
	step [48/143], loss=9.9237
	step [49/143], loss=8.9947
	step [50/143], loss=9.3442
	step [51/143], loss=9.7936
	step [52/143], loss=9.8727
	step [53/143], loss=9.9042
	step [54/143], loss=10.6757
	step [55/143], loss=10.2252
	step [56/143], loss=10.2366
	step [57/143], loss=10.4062
	step [58/143], loss=10.7780
	step [59/143], loss=9.6460
	step [60/143], loss=8.6783
	step [61/143], loss=9.8428
	step [62/143], loss=9.0505
	step [63/143], loss=9.3740
	step [64/143], loss=10.8546
	step [65/143], loss=9.9864
	step [66/143], loss=10.5937
	step [67/143], loss=9.7908
	step [68/143], loss=10.0808
	step [69/143], loss=9.4913
	step [70/143], loss=10.1414
	step [71/143], loss=9.4344
	step [72/143], loss=9.4833
	step [73/143], loss=11.2894
	step [74/143], loss=8.6102
	step [75/143], loss=8.7662
	step [76/143], loss=9.5213
	step [77/143], loss=9.7743
	step [78/143], loss=8.7475
	step [79/143], loss=10.7292
	step [80/143], loss=9.6799
	step [81/143], loss=10.5928
	step [82/143], loss=10.1763
	step [83/143], loss=8.5542
	step [84/143], loss=9.0902
	step [85/143], loss=11.1176
	step [86/143], loss=8.6799
	step [87/143], loss=9.4403
	step [88/143], loss=9.4652
	step [89/143], loss=10.7395
	step [90/143], loss=9.4117
	step [91/143], loss=9.0232
	step [92/143], loss=10.0597
	step [93/143], loss=9.4846
	step [94/143], loss=8.2724
	step [95/143], loss=10.9496
	step [96/143], loss=10.2215
	step [97/143], loss=9.8889
	step [98/143], loss=10.9359
	step [99/143], loss=8.7146
	step [100/143], loss=10.2217
	step [101/143], loss=11.3798
	step [102/143], loss=8.9685
	step [103/143], loss=11.7137
	step [104/143], loss=9.3975
	step [105/143], loss=10.4188
	step [106/143], loss=9.7033
	step [107/143], loss=10.5230
	step [108/143], loss=8.9108
	step [109/143], loss=9.0157
	step [110/143], loss=9.6510
	step [111/143], loss=11.2129
	step [112/143], loss=10.3546
	step [113/143], loss=9.2912
	step [114/143], loss=9.9222
	step [115/143], loss=7.9780
	step [116/143], loss=9.6084
	step [117/143], loss=10.4854
	step [118/143], loss=9.4551
	step [119/143], loss=9.5110
	step [120/143], loss=9.3911
	step [121/143], loss=8.5004
	step [122/143], loss=9.5604
	step [123/143], loss=8.3885
	step [124/143], loss=10.6900
	step [125/143], loss=10.7764
	step [126/143], loss=10.0212
	step [127/143], loss=9.4912
	step [128/143], loss=9.4889
	step [129/143], loss=10.1030
	step [130/143], loss=9.2956
	step [131/143], loss=8.7599
	step [132/143], loss=9.6128
	step [133/143], loss=10.8588
	step [134/143], loss=9.5047
	step [135/143], loss=10.0060
	step [136/143], loss=9.1372
	step [137/143], loss=9.6603
	step [138/143], loss=9.4595
	step [139/143], loss=10.4263
	step [140/143], loss=9.0865
	step [141/143], loss=9.2946
	step [142/143], loss=10.1648
	step [143/143], loss=9.1736
	Evaluating
	loss=0.0213, precision=0.3015, recall=0.9856, f1=0.4617
saving model as: 1_saved_model.pth
Training epoch 30
	step [1/143], loss=8.8871
	step [2/143], loss=12.8081
	step [3/143], loss=9.7559
	step [4/143], loss=8.8730
	step [5/143], loss=9.7705
	step [6/143], loss=9.3984
	step [7/143], loss=9.6959
	step [8/143], loss=11.2844
	step [9/143], loss=10.2751
	step [10/143], loss=9.6897
	step [11/143], loss=8.6736
	step [12/143], loss=10.0612
	step [13/143], loss=10.2044
	step [14/143], loss=9.3369
	step [15/143], loss=10.4860
	step [16/143], loss=10.3702
	step [17/143], loss=12.6237
	step [18/143], loss=11.6282
	step [19/143], loss=9.2031
	step [20/143], loss=8.8902
	step [21/143], loss=10.5783
	step [22/143], loss=10.2461
	step [23/143], loss=10.4101
	step [24/143], loss=9.7879
	step [25/143], loss=9.9022
	step [26/143], loss=10.4458
	step [27/143], loss=10.2532
	step [28/143], loss=8.1616
	step [29/143], loss=9.9890
	step [30/143], loss=9.7586
	step [31/143], loss=9.9459
	step [32/143], loss=9.6024
	step [33/143], loss=8.7517
	step [34/143], loss=9.2885
	step [35/143], loss=10.7224
	step [36/143], loss=12.3271
	step [37/143], loss=8.4341
	step [38/143], loss=9.9064
	step [39/143], loss=9.6106
	step [40/143], loss=10.7285
	step [41/143], loss=9.5782
	step [42/143], loss=9.0413
	step [43/143], loss=10.1211
	step [44/143], loss=8.0650
	step [45/143], loss=9.2288
	step [46/143], loss=9.1783
	step [47/143], loss=9.7998
	step [48/143], loss=8.6889
	step [49/143], loss=10.5740
	step [50/143], loss=9.9886
	step [51/143], loss=10.5846
	step [52/143], loss=8.7153
	step [53/143], loss=10.0859
	step [54/143], loss=9.5931
	step [55/143], loss=9.0251
	step [56/143], loss=12.0606
	step [57/143], loss=8.5172
	step [58/143], loss=9.4269
	step [59/143], loss=9.2037
	step [60/143], loss=8.6347
	step [61/143], loss=8.8194
	step [62/143], loss=8.8052
	step [63/143], loss=10.5733
	step [64/143], loss=11.9204
	step [65/143], loss=9.2420
	step [66/143], loss=9.3587
	step [67/143], loss=9.1693
	step [68/143], loss=9.7733
	step [69/143], loss=9.9331
	step [70/143], loss=9.3222
	step [71/143], loss=9.5014
	step [72/143], loss=9.1073
	step [73/143], loss=9.9616
	step [74/143], loss=9.4750
	step [75/143], loss=10.8628
	step [76/143], loss=9.8805
	step [77/143], loss=9.4447
	step [78/143], loss=9.2508
	step [79/143], loss=9.7517
	step [80/143], loss=8.6349
	step [81/143], loss=9.4218
	step [82/143], loss=9.1608
	step [83/143], loss=8.8732
	step [84/143], loss=10.2068
	step [85/143], loss=9.6079
	step [86/143], loss=10.0481
	step [87/143], loss=9.1936
	step [88/143], loss=9.3998
	step [89/143], loss=11.1529
	step [90/143], loss=10.1618
	step [91/143], loss=8.9319
	step [92/143], loss=9.3209
	step [93/143], loss=9.8205
	step [94/143], loss=9.6582
	step [95/143], loss=10.5681
	step [96/143], loss=9.0167
	step [97/143], loss=10.2844
	step [98/143], loss=10.7466
	step [99/143], loss=9.6584
	step [100/143], loss=9.9074
	step [101/143], loss=9.9811
	step [102/143], loss=10.5019
	step [103/143], loss=8.2957
	step [104/143], loss=10.6401
	step [105/143], loss=9.0484
	step [106/143], loss=9.3673
	step [107/143], loss=9.5546
	step [108/143], loss=9.4315
	step [109/143], loss=9.9890
	step [110/143], loss=9.3320
	step [111/143], loss=9.4547
	step [112/143], loss=8.5231
	step [113/143], loss=7.9182
	step [114/143], loss=7.8004
	step [115/143], loss=10.1102
	step [116/143], loss=9.1149
	step [117/143], loss=9.8289
	step [118/143], loss=9.5906
	step [119/143], loss=8.7579
	step [120/143], loss=9.8017
	step [121/143], loss=8.0913
	step [122/143], loss=9.9873
	step [123/143], loss=9.3986
	step [124/143], loss=8.5368
	step [125/143], loss=10.7862
	step [126/143], loss=10.9862
	step [127/143], loss=9.0417
	step [128/143], loss=8.9219
	step [129/143], loss=9.6366
	step [130/143], loss=10.0294
	step [131/143], loss=9.3133
	step [132/143], loss=10.1736
	step [133/143], loss=9.2247
	step [134/143], loss=11.3281
	step [135/143], loss=11.2448
	step [136/143], loss=9.1227
	step [137/143], loss=9.6491
	step [138/143], loss=9.2323
	step [139/143], loss=8.5351
	step [140/143], loss=9.7180
	step [141/143], loss=11.0409
	step [142/143], loss=9.9319
	step [143/143], loss=8.7033
	Evaluating
	loss=0.0232, precision=0.2791, recall=0.9875, f1=0.4352
Training finished
best_f1: 0.46170137001560607
directing: Y rim_enhanced: False test_id 1
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 15614 # image files with weight 15579
removed wrong scan: weights_Y_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_171_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_299_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_224_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_336_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_350_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_244_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_373_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_308_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_285_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_274_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_188_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_387_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_360_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_372_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_316_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_320_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_253_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_204_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_349_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_149_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_283_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_300_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_333_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_221_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_159_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_363_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_329_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_368_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_211_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_260_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_222_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_313_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_282_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_177_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_190_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_315_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_184_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_312_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_302_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_157_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_236_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_331_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_214_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_212_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_356_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_167_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_383_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_255_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_224_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_240_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_343_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_375_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_318_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_367_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_220_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_163_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_255_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_351_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_305_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_330_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_309_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_225_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_242_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_233_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_346_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_326_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_371_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_275_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_377_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_273_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_197_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_332_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_239_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_307_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_380_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_321_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_265_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_150_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_201_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_291_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_353_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_322_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_310_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_352_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_262_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_297_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_338_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_319_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_179_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_236_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_239_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_218_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_253_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_293_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_303_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_158_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_381_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_317_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_189_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_270_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_294_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_354_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_359_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_266_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_292_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_314_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_218_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_238_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_200_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_262_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_229_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_169_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_288_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_244_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_347_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_323_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_335_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_278_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_306_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_209_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_365_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_259_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_378_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_298_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_256_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_276_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_384_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_208_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_221_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_258_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_339_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_232_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_325_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_217_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_238_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_290_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_166_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_231_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_369_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_280_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_337_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_216_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_304_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_194_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_301_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_324_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_340_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_162_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_295_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_366_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_370_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_362_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_379_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_269_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_180_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_227_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_289_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_296_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_183_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_271_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_226_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_237_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_341_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_161_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_358_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_254_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_235_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_334_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_376_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_287_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_252_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_205_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_228_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_348_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_249_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_251_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_243_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_279_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_268_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_216_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_178_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_230_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_206_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_165_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_207_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_227_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_260_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_311_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_327_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_344_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_256_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_328_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_153_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_257_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_152_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_176_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_219_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_342_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_222_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_223_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_263_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_385_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_156_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_386_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_196_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_243_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_195_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_66_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_277_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_232_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_220_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_214_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_388_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_210_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_241_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_265_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_146_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_266_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_284_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_355_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_174_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_210_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Y_225_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_345_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_272_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Y_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Y_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Y_205_xwqg-B00034_2020-04-03.npy
# all image files: 20333 # all weight files in weight_dir: 4469 # image files with weight 4451
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Y 15579
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/244], loss=424.3583
	step [2/244], loss=232.3296
	step [3/244], loss=198.0383
	step [4/244], loss=173.0310
	step [5/244], loss=172.3901
	step [6/244], loss=165.5435
	step [7/244], loss=160.2231
	step [8/244], loss=161.6947
	step [9/244], loss=160.1628
	step [10/244], loss=156.0733
	step [11/244], loss=155.9874
	step [12/244], loss=154.0053
	step [13/244], loss=151.9302
	step [14/244], loss=150.3721
	step [15/244], loss=150.2920
	step [16/244], loss=145.9954
	step [17/244], loss=147.2809
	step [18/244], loss=143.7503
	step [19/244], loss=144.7565
	step [20/244], loss=144.7772
	step [21/244], loss=143.7349
	step [22/244], loss=139.8114
	step [23/244], loss=139.1415
	step [24/244], loss=138.7612
	step [25/244], loss=134.6946
	step [26/244], loss=134.6512
	step [27/244], loss=134.2041
	step [28/244], loss=134.0418
	step [29/244], loss=131.5464
	step [30/244], loss=130.1851
	step [31/244], loss=131.9790
	step [32/244], loss=128.5435
	step [33/244], loss=128.2999
	step [34/244], loss=129.3645
	step [35/244], loss=126.0612
	step [36/244], loss=126.8104
	step [37/244], loss=126.6685
	step [38/244], loss=122.2423
	step [39/244], loss=121.7541
	step [40/244], loss=124.8219
	step [41/244], loss=121.8818
	step [42/244], loss=120.6020
	step [43/244], loss=120.8460
	step [44/244], loss=119.1172
	step [45/244], loss=118.8615
	step [46/244], loss=120.7703
	step [47/244], loss=116.8912
	step [48/244], loss=118.3685
	step [49/244], loss=117.8608
	step [50/244], loss=114.2313
	step [51/244], loss=118.3583
	step [52/244], loss=116.5347
	step [53/244], loss=113.5028
	step [54/244], loss=110.8250
	step [55/244], loss=112.4148
	step [56/244], loss=112.0333
	step [57/244], loss=109.9313
	step [58/244], loss=110.3727
	step [59/244], loss=108.0718
	step [60/244], loss=110.4214
	step [61/244], loss=111.3839
	step [62/244], loss=107.6322
	step [63/244], loss=105.9329
	step [64/244], loss=105.9792
	step [65/244], loss=106.2354
	step [66/244], loss=104.4940
	step [67/244], loss=105.6390
	step [68/244], loss=103.7271
	step [69/244], loss=106.1119
	step [70/244], loss=106.9897
	step [71/244], loss=102.0505
	step [72/244], loss=105.5214
	step [73/244], loss=103.1057
	step [74/244], loss=102.0370
	step [75/244], loss=100.6926
	step [76/244], loss=100.1134
	step [77/244], loss=101.1155
	step [78/244], loss=100.0655
	step [79/244], loss=98.5486
	step [80/244], loss=98.1681
	step [81/244], loss=102.1483
	step [82/244], loss=96.6492
	step [83/244], loss=98.5673
	step [84/244], loss=98.0733
	step [85/244], loss=95.9841
	step [86/244], loss=97.3781
	step [87/244], loss=95.6523
	step [88/244], loss=97.2203
	step [89/244], loss=94.6970
	step [90/244], loss=95.6383
	step [91/244], loss=94.9816
	step [92/244], loss=92.5628
	step [93/244], loss=93.0474
	step [94/244], loss=93.4365
	step [95/244], loss=92.2152
	step [96/244], loss=92.6394
	step [97/244], loss=93.2800
	step [98/244], loss=91.7227
	step [99/244], loss=92.4344
	step [100/244], loss=90.2485
	step [101/244], loss=91.0758
	step [102/244], loss=89.4662
	step [103/244], loss=89.7193
	step [104/244], loss=91.2556
	step [105/244], loss=89.3047
	step [106/244], loss=92.5615
	step [107/244], loss=91.7471
	step [108/244], loss=90.4146
	step [109/244], loss=91.8834
	step [110/244], loss=89.2083
	step [111/244], loss=87.2603
	step [112/244], loss=92.7230
	step [113/244], loss=90.7404
	step [114/244], loss=88.2825
	step [115/244], loss=88.4971
	step [116/244], loss=87.4521
	step [117/244], loss=85.9615
	step [118/244], loss=88.1261
	step [119/244], loss=84.9387
	step [120/244], loss=88.1771
	step [121/244], loss=84.7746
	step [122/244], loss=86.2943
	step [123/244], loss=82.3345
	step [124/244], loss=86.8172
	step [125/244], loss=86.0434
	step [126/244], loss=82.4162
	step [127/244], loss=81.4761
	step [128/244], loss=88.1721
	step [129/244], loss=83.0457
	step [130/244], loss=84.1840
	step [131/244], loss=84.0874
	step [132/244], loss=83.6556
	step [133/244], loss=84.1864
	step [134/244], loss=82.3382
	step [135/244], loss=84.4076
	step [136/244], loss=82.7962
	step [137/244], loss=82.6470
	step [138/244], loss=80.9074
	step [139/244], loss=84.7157
	step [140/244], loss=82.8596
	step [141/244], loss=82.0012
	step [142/244], loss=83.1372
	step [143/244], loss=79.6287
	step [144/244], loss=82.0016
	step [145/244], loss=83.1041
	step [146/244], loss=81.5357
	step [147/244], loss=80.8065
	step [148/244], loss=80.6954
	step [149/244], loss=83.9947
	step [150/244], loss=79.7124
	step [151/244], loss=78.2029
	step [152/244], loss=80.3383
	step [153/244], loss=78.0356
	step [154/244], loss=78.9175
	step [155/244], loss=80.0788
	step [156/244], loss=80.0738
	step [157/244], loss=79.3644
	step [158/244], loss=78.5509
	step [159/244], loss=80.1881
	step [160/244], loss=78.3096
	step [161/244], loss=78.3581
	step [162/244], loss=77.4934
	step [163/244], loss=77.8759
	step [164/244], loss=78.4206
	step [165/244], loss=76.7720
	step [166/244], loss=75.4392
	step [167/244], loss=76.3263
	step [168/244], loss=78.5480
	step [169/244], loss=76.4283
	step [170/244], loss=76.6701
	step [171/244], loss=75.9793
	step [172/244], loss=76.3365
	step [173/244], loss=76.0914
	step [174/244], loss=75.6140
	step [175/244], loss=74.7034
	step [176/244], loss=75.5735
	step [177/244], loss=77.3951
	step [178/244], loss=74.5610
	step [179/244], loss=74.2458
	step [180/244], loss=76.7489
	step [181/244], loss=73.6141
	step [182/244], loss=73.4795
	step [183/244], loss=74.2525
	step [184/244], loss=75.2846
	step [185/244], loss=73.9919
	step [186/244], loss=71.8890
	step [187/244], loss=73.3195
	step [188/244], loss=76.9997
	step [189/244], loss=72.8860
	step [190/244], loss=73.9860
	step [191/244], loss=72.8303
	step [192/244], loss=74.9452
	step [193/244], loss=74.8280
	step [194/244], loss=74.1753
	step [195/244], loss=73.7758
	step [196/244], loss=71.5165
	step [197/244], loss=71.6888
	step [198/244], loss=69.1228
	step [199/244], loss=70.1711
	step [200/244], loss=72.8025
	step [201/244], loss=71.2135
	step [202/244], loss=72.3995
	step [203/244], loss=71.1192
	step [204/244], loss=71.7001
	step [205/244], loss=70.5366
	step [206/244], loss=70.1945
	step [207/244], loss=70.1654
	step [208/244], loss=70.2313
	step [209/244], loss=74.3268
	step [210/244], loss=71.1948
	step [211/244], loss=68.9198
	step [212/244], loss=69.4020
	step [213/244], loss=69.7036
	step [214/244], loss=70.2246
	step [215/244], loss=73.4022
	step [216/244], loss=70.3548
	step [217/244], loss=68.9746
	step [218/244], loss=67.2518
	step [219/244], loss=69.9449
	step [220/244], loss=68.1449
	step [221/244], loss=69.1278
	step [222/244], loss=69.9886
	step [223/244], loss=69.1316
	step [224/244], loss=70.7822
	step [225/244], loss=72.8045
	step [226/244], loss=71.3268
	step [227/244], loss=67.4571
	step [228/244], loss=68.0567
	step [229/244], loss=71.0562
	step [230/244], loss=69.8848
	step [231/244], loss=68.7996
	step [232/244], loss=69.6476
	step [233/244], loss=68.1663
	step [234/244], loss=68.2139
	step [235/244], loss=69.5638
	step [236/244], loss=69.2569
	step [237/244], loss=70.3569
	step [238/244], loss=66.5623
	step [239/244], loss=66.0433
	step [240/244], loss=67.4445
	step [241/244], loss=67.2916
	step [242/244], loss=66.3021
	step [243/244], loss=67.4896
	step [244/244], loss=29.0166
	Evaluating
	loss=0.2445, precision=0.1797, recall=0.9923, f1=0.3043
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/244], loss=67.4399
	step [2/244], loss=66.9858
	step [3/244], loss=63.6033
	step [4/244], loss=69.8277
	step [5/244], loss=66.3726
	step [6/244], loss=68.1207
	step [7/244], loss=65.6443
	step [8/244], loss=70.3569
	step [9/244], loss=66.8358
	step [10/244], loss=65.7596
	step [11/244], loss=66.2973
	step [12/244], loss=66.1654
	step [13/244], loss=66.3930
	step [14/244], loss=63.9637
	step [15/244], loss=63.2324
	step [16/244], loss=65.5238
	step [17/244], loss=64.8624
	step [18/244], loss=67.2556
	step [19/244], loss=64.1527
	step [20/244], loss=67.7260
	step [21/244], loss=65.9084
	step [22/244], loss=65.6014
	step [23/244], loss=65.0996
	step [24/244], loss=64.0031
	step [25/244], loss=63.2419
	step [26/244], loss=62.8379
	step [27/244], loss=64.6567
	step [28/244], loss=65.5858
	step [29/244], loss=65.5290
	step [30/244], loss=65.4216
	step [31/244], loss=66.2249
	step [32/244], loss=64.1008
	step [33/244], loss=65.8266
	step [34/244], loss=63.1698
	step [35/244], loss=62.0428
	step [36/244], loss=62.6785
	step [37/244], loss=62.6954
	step [38/244], loss=67.9827
	step [39/244], loss=63.1322
	step [40/244], loss=62.5527
	step [41/244], loss=62.4059
	step [42/244], loss=62.0051
	step [43/244], loss=62.3639
	step [44/244], loss=62.4055
	step [45/244], loss=64.3636
	step [46/244], loss=63.9709
	step [47/244], loss=62.6329
	step [48/244], loss=63.7964
	step [49/244], loss=62.4755
	step [50/244], loss=64.8093
	step [51/244], loss=64.1876
	step [52/244], loss=63.6522
	step [53/244], loss=60.3789
	step [54/244], loss=59.7910
	step [55/244], loss=62.5805
	step [56/244], loss=63.2023
	step [57/244], loss=61.6987
	step [58/244], loss=61.2544
	step [59/244], loss=61.9660
	step [60/244], loss=60.4801
	step [61/244], loss=60.7083
	step [62/244], loss=65.0589
	step [63/244], loss=61.6050
	step [64/244], loss=61.0744
	step [65/244], loss=61.6476
	step [66/244], loss=61.8983
	step [67/244], loss=60.8275
	step [68/244], loss=58.6184
	step [69/244], loss=62.0471
	step [70/244], loss=60.7094
	step [71/244], loss=60.1902
	step [72/244], loss=61.6334
	step [73/244], loss=60.2472
	step [74/244], loss=60.0902
	step [75/244], loss=60.0940
	step [76/244], loss=59.9921
	step [77/244], loss=62.2718
	step [78/244], loss=59.4175
	step [79/244], loss=59.5687
	step [80/244], loss=57.5538
	step [81/244], loss=57.9584
	step [82/244], loss=56.6265
	step [83/244], loss=61.0104
	step [84/244], loss=61.4840
	step [85/244], loss=58.2221
	step [86/244], loss=58.8054
	step [87/244], loss=59.6988
	step [88/244], loss=61.0987
	step [89/244], loss=59.1524
	step [90/244], loss=58.0111
	step [91/244], loss=58.0571
	step [92/244], loss=58.2470
	step [93/244], loss=57.8179
	step [94/244], loss=58.7844
	step [95/244], loss=59.0033
	step [96/244], loss=58.6525
	step [97/244], loss=57.8341
	step [98/244], loss=58.5747
	step [99/244], loss=57.9634
	step [100/244], loss=58.8511
	step [101/244], loss=57.1832
	step [102/244], loss=57.9321
	step [103/244], loss=58.4650
	step [104/244], loss=60.7048
	step [105/244], loss=58.2665
	step [106/244], loss=56.8735
	step [107/244], loss=57.6949
	step [108/244], loss=56.8532
	step [109/244], loss=57.1647
	step [110/244], loss=55.7360
	step [111/244], loss=60.9331
	step [112/244], loss=58.0081
	step [113/244], loss=54.8446
	step [114/244], loss=56.0322
	step [115/244], loss=56.8467
	step [116/244], loss=57.7073
	step [117/244], loss=57.9759
	step [118/244], loss=56.8214
	step [119/244], loss=56.6380
	step [120/244], loss=57.1843
	step [121/244], loss=56.1719
	step [122/244], loss=56.3430
	step [123/244], loss=58.3650
	step [124/244], loss=55.8492
	step [125/244], loss=56.6636
	step [126/244], loss=56.3675
	step [127/244], loss=55.9882
	step [128/244], loss=55.2074
	step [129/244], loss=57.9206
	step [130/244], loss=57.9665
	step [131/244], loss=57.8792
	step [132/244], loss=57.0292
	step [133/244], loss=56.2183
	step [134/244], loss=56.9111
	step [135/244], loss=54.2383
	step [136/244], loss=56.4056
	step [137/244], loss=55.2391
	step [138/244], loss=55.8955
	step [139/244], loss=54.4047
	step [140/244], loss=53.4780
	step [141/244], loss=56.0407
	step [142/244], loss=54.6696
	step [143/244], loss=57.5534
	step [144/244], loss=54.2592
	step [145/244], loss=53.8805
	step [146/244], loss=54.8219
	step [147/244], loss=55.2552
	step [148/244], loss=53.9769
	step [149/244], loss=55.0756
	step [150/244], loss=54.4151
	step [151/244], loss=56.4733
	step [152/244], loss=52.9830
	step [153/244], loss=56.3670
	step [154/244], loss=52.5431
	step [155/244], loss=54.2612
	step [156/244], loss=55.3530
	step [157/244], loss=52.0597
	step [158/244], loss=53.5262
	step [159/244], loss=52.9323
	step [160/244], loss=55.2596
	step [161/244], loss=52.1321
	step [162/244], loss=56.8150
	step [163/244], loss=54.7053
	step [164/244], loss=52.0325
	step [165/244], loss=51.8672
	step [166/244], loss=53.8017
	step [167/244], loss=53.8306
	step [168/244], loss=52.6960
	step [169/244], loss=54.2726
	step [170/244], loss=54.5858
	step [171/244], loss=53.5834
	step [172/244], loss=51.9223
	step [173/244], loss=51.3437
	step [174/244], loss=55.0859
	step [175/244], loss=53.0423
	step [176/244], loss=53.6546
	step [177/244], loss=52.5925
	step [178/244], loss=52.3743
	step [179/244], loss=52.4892
	step [180/244], loss=52.5385
	step [181/244], loss=52.0620
	step [182/244], loss=50.1790
	step [183/244], loss=57.1267
	step [184/244], loss=53.1988
	step [185/244], loss=54.3508
	step [186/244], loss=51.9288
	step [187/244], loss=52.6619
	step [188/244], loss=54.4479
	step [189/244], loss=53.9952
	step [190/244], loss=51.1209
	step [191/244], loss=52.0832
	step [192/244], loss=51.3387
	step [193/244], loss=53.8042
	step [194/244], loss=49.6449
	step [195/244], loss=50.3614
	step [196/244], loss=50.9148
	step [197/244], loss=50.0000
	step [198/244], loss=53.3657
	step [199/244], loss=51.4929
	step [200/244], loss=50.4174
	step [201/244], loss=52.9533
	step [202/244], loss=48.6944
	step [203/244], loss=49.8746
	step [204/244], loss=50.8656
	step [205/244], loss=53.8308
	step [206/244], loss=51.3819
	step [207/244], loss=50.5592
	step [208/244], loss=50.2540
	step [209/244], loss=50.5751
	step [210/244], loss=50.7086
	step [211/244], loss=48.6353
	step [212/244], loss=51.3355
	step [213/244], loss=51.4723
	step [214/244], loss=50.3425
	step [215/244], loss=50.6227
	step [216/244], loss=47.5234
	step [217/244], loss=50.2882
	step [218/244], loss=49.4224
	step [219/244], loss=47.9547
	step [220/244], loss=52.7530
	step [221/244], loss=49.8673
	step [222/244], loss=50.8415
	step [223/244], loss=50.1334
	step [224/244], loss=48.4139
	step [225/244], loss=51.3945
	step [226/244], loss=51.1069
	step [227/244], loss=50.9176
	step [228/244], loss=47.1643
	step [229/244], loss=50.1813
	step [230/244], loss=48.8954
	step [231/244], loss=50.7566
	step [232/244], loss=49.2731
	step [233/244], loss=47.7273
	step [234/244], loss=48.9847
	step [235/244], loss=51.2607
	step [236/244], loss=48.7138
	step [237/244], loss=49.0203
	step [238/244], loss=48.6262
	step [239/244], loss=50.2908
	step [240/244], loss=49.2826
	step [241/244], loss=48.3463
	step [242/244], loss=49.6137
	step [243/244], loss=49.1705
	step [244/244], loss=20.8634
	Evaluating
	loss=0.1825, precision=0.2001, recall=0.9918, f1=0.3331
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/244], loss=48.5227
	step [2/244], loss=47.1618
	step [3/244], loss=48.5244
	step [4/244], loss=48.9166
	step [5/244], loss=49.2314
	step [6/244], loss=48.3631
	step [7/244], loss=46.6116
	step [8/244], loss=48.1245
	step [9/244], loss=47.8806
	step [10/244], loss=48.8398
	step [11/244], loss=48.5753
	step [12/244], loss=46.3189
	step [13/244], loss=46.4407
	step [14/244], loss=46.8184
	step [15/244], loss=45.1766
	step [16/244], loss=47.6554
	step [17/244], loss=46.3964
	step [18/244], loss=48.8963
	step [19/244], loss=47.3134
	step [20/244], loss=48.3839
	step [21/244], loss=45.4553
	step [22/244], loss=47.9512
	step [23/244], loss=46.2828
	step [24/244], loss=46.9716
	step [25/244], loss=46.6149
	step [26/244], loss=47.4647
	step [27/244], loss=48.0671
	step [28/244], loss=46.9508
	step [29/244], loss=47.9104
	step [30/244], loss=45.5848
	step [31/244], loss=45.6253
	step [32/244], loss=46.0770
	step [33/244], loss=44.9960
	step [34/244], loss=44.6727
	step [35/244], loss=50.2998
	step [36/244], loss=45.6968
	step [37/244], loss=46.6328
	step [38/244], loss=46.6861
	step [39/244], loss=45.6579
	step [40/244], loss=46.0462
	step [41/244], loss=44.1921
	step [42/244], loss=48.5240
	step [43/244], loss=46.3474
	step [44/244], loss=45.4766
	step [45/244], loss=48.4614
	step [46/244], loss=46.3921
	step [47/244], loss=46.6790
	step [48/244], loss=47.1773
	step [49/244], loss=45.1604
	step [50/244], loss=45.7180
	step [51/244], loss=44.6561
	step [52/244], loss=44.8049
	step [53/244], loss=49.0416
	step [54/244], loss=45.8745
	step [55/244], loss=46.7123
	step [56/244], loss=45.4962
	step [57/244], loss=44.8093
	step [58/244], loss=46.4636
	step [59/244], loss=45.2223
	step [60/244], loss=44.4896
	step [61/244], loss=46.6042
	step [62/244], loss=44.2875
	step [63/244], loss=45.8592
	step [64/244], loss=43.6144
	step [65/244], loss=42.9410
	step [66/244], loss=41.9542
	step [67/244], loss=45.2125
	step [68/244], loss=43.1103
	step [69/244], loss=45.4824
	step [70/244], loss=43.4195
	step [71/244], loss=45.7516
	step [72/244], loss=43.1243
	step [73/244], loss=42.2637
	step [74/244], loss=43.4455
	step [75/244], loss=46.7393
	step [76/244], loss=42.9544
	step [77/244], loss=43.4841
	step [78/244], loss=44.9331
	step [79/244], loss=44.0001
	step [80/244], loss=45.1505
	step [81/244], loss=42.6362
	step [82/244], loss=44.2978
	step [83/244], loss=44.1971
	step [84/244], loss=43.3096
	step [85/244], loss=43.0939
	step [86/244], loss=47.0039
	step [87/244], loss=42.4096
	step [88/244], loss=43.0838
	step [89/244], loss=42.6777
	step [90/244], loss=42.0208
	step [91/244], loss=44.2736
	step [92/244], loss=45.0428
	step [93/244], loss=43.9086
	step [94/244], loss=42.1644
	step [95/244], loss=43.1988
	step [96/244], loss=43.6832
	step [97/244], loss=44.0679
	step [98/244], loss=44.1772
	step [99/244], loss=43.7101
	step [100/244], loss=43.2517
	step [101/244], loss=42.5038
	step [102/244], loss=43.8355
	step [103/244], loss=43.9139
	step [104/244], loss=42.3923
	step [105/244], loss=43.4608
	step [106/244], loss=42.6465
	step [107/244], loss=43.3260
	step [108/244], loss=43.4598
	step [109/244], loss=40.3604
	step [110/244], loss=42.4301
	step [111/244], loss=41.5697
	step [112/244], loss=42.5159
	step [113/244], loss=43.7332
	step [114/244], loss=43.6186
	step [115/244], loss=41.7816
	step [116/244], loss=41.9231
	step [117/244], loss=43.4409
	step [118/244], loss=42.7577
	step [119/244], loss=41.2351
	step [120/244], loss=44.4642
	step [121/244], loss=40.4763
	step [122/244], loss=40.3888
	step [123/244], loss=40.8297
	step [124/244], loss=39.4729
	step [125/244], loss=40.2985
	step [126/244], loss=41.5872
	step [127/244], loss=43.8313
	step [128/244], loss=41.3869
	step [129/244], loss=40.9055
	step [130/244], loss=40.7558
	step [131/244], loss=40.7648
	step [132/244], loss=41.1843
	step [133/244], loss=43.5377
	step [134/244], loss=42.4776
	step [135/244], loss=41.2124
	step [136/244], loss=44.1649
	step [137/244], loss=42.6754
	step [138/244], loss=41.6089
	step [139/244], loss=40.7378
	step [140/244], loss=41.9998
	step [141/244], loss=40.9799
	step [142/244], loss=41.1258
	step [143/244], loss=43.1568
	step [144/244], loss=41.6810
	step [145/244], loss=39.6568
	step [146/244], loss=40.4401
	step [147/244], loss=40.6982
	step [148/244], loss=40.1697
	step [149/244], loss=41.4065
	step [150/244], loss=42.2998
	step [151/244], loss=42.5081
	step [152/244], loss=40.1305
	step [153/244], loss=40.8060
	step [154/244], loss=39.3374
	step [155/244], loss=40.1095
	step [156/244], loss=39.2455
	step [157/244], loss=40.3825
	step [158/244], loss=40.2087
	step [159/244], loss=38.2737
	step [160/244], loss=41.8304
	step [161/244], loss=42.0000
	step [162/244], loss=38.4886
	step [163/244], loss=41.2558
	step [164/244], loss=38.2769
	step [165/244], loss=40.9283
	step [166/244], loss=40.1522
	step [167/244], loss=41.3393
	step [168/244], loss=39.7052
	step [169/244], loss=38.8799
	step [170/244], loss=40.2364
	step [171/244], loss=39.4838
	step [172/244], loss=39.1146
	step [173/244], loss=38.0672
	step [174/244], loss=37.3257
	step [175/244], loss=38.9775
	step [176/244], loss=40.2427
	step [177/244], loss=40.6768
	step [178/244], loss=39.8626
	step [179/244], loss=40.3327
	step [180/244], loss=40.0781
	step [181/244], loss=38.4785
	step [182/244], loss=40.4937
	step [183/244], loss=39.0894
	step [184/244], loss=39.5382
	step [185/244], loss=39.5367
	step [186/244], loss=37.9259
	step [187/244], loss=36.8563
	step [188/244], loss=36.6402
	step [189/244], loss=38.3690
	step [190/244], loss=39.3798
	step [191/244], loss=40.9869
	step [192/244], loss=38.2161
	step [193/244], loss=37.1373
	step [194/244], loss=38.8181
	step [195/244], loss=42.5337
	step [196/244], loss=38.2971
	step [197/244], loss=37.2635
	step [198/244], loss=39.7409
	step [199/244], loss=39.8973
	step [200/244], loss=38.2853
	step [201/244], loss=40.7790
	step [202/244], loss=39.3419
	step [203/244], loss=40.6919
	step [204/244], loss=39.6793
	step [205/244], loss=36.9506
	step [206/244], loss=39.9184
	step [207/244], loss=39.7908
	step [208/244], loss=36.9812
	step [209/244], loss=36.2069
	step [210/244], loss=38.5426
	step [211/244], loss=36.5843
	step [212/244], loss=37.4361
	step [213/244], loss=38.0380
	step [214/244], loss=36.6392
	step [215/244], loss=38.7968
	step [216/244], loss=38.2794
	step [217/244], loss=37.0993
	step [218/244], loss=37.5175
	step [219/244], loss=37.8845
	step [220/244], loss=39.6580
	step [221/244], loss=37.4545
	step [222/244], loss=39.8263
	step [223/244], loss=38.4995
	step [224/244], loss=37.0372
	step [225/244], loss=37.4603
	step [226/244], loss=42.5457
	step [227/244], loss=38.9105
	step [228/244], loss=37.1222
	step [229/244], loss=40.2373
	step [230/244], loss=38.4241
	step [231/244], loss=35.7665
	step [232/244], loss=37.0783
	step [233/244], loss=36.7421
	step [234/244], loss=37.4728
	step [235/244], loss=36.3047
	step [236/244], loss=36.6308
	step [237/244], loss=36.3439
	step [238/244], loss=36.6035
	step [239/244], loss=36.5302
	step [240/244], loss=35.7549
	step [241/244], loss=38.8180
	step [242/244], loss=37.5946
	step [243/244], loss=37.9335
	step [244/244], loss=15.8196
	Evaluating
	loss=0.1350, precision=0.1649, recall=0.9932, f1=0.2828
Training epoch 4
	step [1/244], loss=35.6574
	step [2/244], loss=36.4875
	step [3/244], loss=36.7516
	step [4/244], loss=36.5585
	step [5/244], loss=36.0299
	step [6/244], loss=37.5305
	step [7/244], loss=37.4651
	step [8/244], loss=38.0345
	step [9/244], loss=37.8915
	step [10/244], loss=35.7263
	step [11/244], loss=35.7210
	step [12/244], loss=35.0614
	step [13/244], loss=36.3357
	step [14/244], loss=35.8048
	step [15/244], loss=37.5039
	step [16/244], loss=36.1344
	step [17/244], loss=35.7141
	step [18/244], loss=35.6861
	step [19/244], loss=38.4892
	step [20/244], loss=36.8953
	step [21/244], loss=35.1758
	step [22/244], loss=34.7690
	step [23/244], loss=35.4880
	step [24/244], loss=35.1345
	step [25/244], loss=35.8991
	step [26/244], loss=34.4258
	step [27/244], loss=34.8108
	step [28/244], loss=34.6984
	step [29/244], loss=34.7282
	step [30/244], loss=38.1671
	step [31/244], loss=34.0986
	step [32/244], loss=34.1579
	step [33/244], loss=37.0073
	step [34/244], loss=36.6583
	step [35/244], loss=38.1575
	step [36/244], loss=35.7073
	step [37/244], loss=35.8052
	step [38/244], loss=34.7951
	step [39/244], loss=33.8889
	step [40/244], loss=35.9950
	step [41/244], loss=34.1739
	step [42/244], loss=33.2412
	step [43/244], loss=36.2042
	step [44/244], loss=36.5851
	step [45/244], loss=35.5095
	step [46/244], loss=33.4928
	step [47/244], loss=33.4589
	step [48/244], loss=33.9999
	step [49/244], loss=37.3156
	step [50/244], loss=34.8081
	step [51/244], loss=33.0403
	step [52/244], loss=35.4878
	step [53/244], loss=38.9740
	step [54/244], loss=35.2571
	step [55/244], loss=35.1321
	step [56/244], loss=34.3332
	step [57/244], loss=34.9505
	step [58/244], loss=34.2901
	step [59/244], loss=34.9314
	step [60/244], loss=32.2263
	step [61/244], loss=35.8445
	step [62/244], loss=33.8142
	step [63/244], loss=34.7758
	step [64/244], loss=33.9202
	step [65/244], loss=33.2797
	step [66/244], loss=35.1942
	step [67/244], loss=34.1618
	step [68/244], loss=33.7333
	step [69/244], loss=33.2003
	step [70/244], loss=33.8173
	step [71/244], loss=34.3847
	step [72/244], loss=32.8126
	step [73/244], loss=34.1998
	step [74/244], loss=33.1767
	step [75/244], loss=35.7143
	step [76/244], loss=34.1275
	step [77/244], loss=32.2191
	step [78/244], loss=34.3203
	step [79/244], loss=35.0423
	step [80/244], loss=32.2672
	step [81/244], loss=33.2102
	step [82/244], loss=33.1898
	step [83/244], loss=34.2708
	step [84/244], loss=33.5162
	step [85/244], loss=34.4189
	step [86/244], loss=34.7497
	step [87/244], loss=34.4682
	step [88/244], loss=34.7675
	step [89/244], loss=32.0905
	step [90/244], loss=35.0433
	step [91/244], loss=32.3912
	step [92/244], loss=32.6597
	step [93/244], loss=33.0177
	step [94/244], loss=33.7631
	step [95/244], loss=33.4096
	step [96/244], loss=32.9400
	step [97/244], loss=32.7111
	step [98/244], loss=33.5959
	step [99/244], loss=32.3376
	step [100/244], loss=34.3100
	step [101/244], loss=33.4190
	step [102/244], loss=32.7938
	step [103/244], loss=37.6146
	step [104/244], loss=34.0916
	step [105/244], loss=33.0636
	step [106/244], loss=34.7229
	step [107/244], loss=34.8512
	step [108/244], loss=31.7860
	step [109/244], loss=34.2473
	step [110/244], loss=33.8268
	step [111/244], loss=31.1307
	step [112/244], loss=34.3570
	step [113/244], loss=34.8573
	step [114/244], loss=32.7879
	step [115/244], loss=33.3619
	step [116/244], loss=33.4579
	step [117/244], loss=31.3845
	step [118/244], loss=33.0148
	step [119/244], loss=30.3196
	step [120/244], loss=31.6896
	step [121/244], loss=30.3539
	step [122/244], loss=33.9113
	step [123/244], loss=33.4966
	step [124/244], loss=31.6428
	step [125/244], loss=32.6915
	step [126/244], loss=32.8821
	step [127/244], loss=32.6013
	step [128/244], loss=31.0380
	step [129/244], loss=33.0655
	step [130/244], loss=31.6426
	step [131/244], loss=32.4774
	step [132/244], loss=33.9092
	step [133/244], loss=31.3837
	step [134/244], loss=32.0070
	step [135/244], loss=29.9776
	step [136/244], loss=31.6071
	step [137/244], loss=31.3842
	step [138/244], loss=31.6231
	step [139/244], loss=34.3712
	step [140/244], loss=33.5408
	step [141/244], loss=33.0767
	step [142/244], loss=31.2700
	step [143/244], loss=33.8157
	step [144/244], loss=32.3933
	step [145/244], loss=30.8866
	step [146/244], loss=31.9117
	step [147/244], loss=32.1623
	step [148/244], loss=31.8175
	step [149/244], loss=29.7317
	step [150/244], loss=31.5112
	step [151/244], loss=29.0831
	step [152/244], loss=32.0031
	step [153/244], loss=30.0156
	step [154/244], loss=31.4998
	step [155/244], loss=31.6911
	step [156/244], loss=33.2344
	step [157/244], loss=31.4539
	step [158/244], loss=31.3560
	step [159/244], loss=34.2080
	step [160/244], loss=31.3026
	step [161/244], loss=32.9661
	step [162/244], loss=35.1203
	step [163/244], loss=30.8951
	step [164/244], loss=31.5369
	step [165/244], loss=30.1551
	step [166/244], loss=31.7264
	step [167/244], loss=30.7591
	step [168/244], loss=29.6799
	step [169/244], loss=32.6416
	step [170/244], loss=30.0913
	step [171/244], loss=28.9852
	step [172/244], loss=28.8689
	step [173/244], loss=30.2107
	step [174/244], loss=33.3561
	step [175/244], loss=32.0450
	step [176/244], loss=29.9424
	step [177/244], loss=34.1707
	step [178/244], loss=29.0795
	step [179/244], loss=31.1635
	step [180/244], loss=30.6653
	step [181/244], loss=31.8868
	step [182/244], loss=30.3985
	step [183/244], loss=30.2980
	step [184/244], loss=31.0976
	step [185/244], loss=29.5043
	step [186/244], loss=27.6242
	step [187/244], loss=29.1069
	step [188/244], loss=29.3250
	step [189/244], loss=33.3988
	step [190/244], loss=29.8470
	step [191/244], loss=34.1112
	step [192/244], loss=33.8409
	step [193/244], loss=32.7645
	step [194/244], loss=30.2729
	step [195/244], loss=30.3744
	step [196/244], loss=30.8715
	step [197/244], loss=33.5603
	step [198/244], loss=30.6650
	step [199/244], loss=32.4144
	step [200/244], loss=32.2017
	step [201/244], loss=28.8909
	step [202/244], loss=30.3768
	step [203/244], loss=29.5534
	step [204/244], loss=29.0916
	step [205/244], loss=31.1002
	step [206/244], loss=30.6033
	step [207/244], loss=30.7369
	step [208/244], loss=29.0127
	step [209/244], loss=28.9976
	step [210/244], loss=31.4859
	step [211/244], loss=28.4852
	step [212/244], loss=28.5464
	step [213/244], loss=32.5589
	step [214/244], loss=30.6926
	step [215/244], loss=29.3984
	step [216/244], loss=29.6039
	step [217/244], loss=31.6195
	step [218/244], loss=29.4645
	step [219/244], loss=30.5357
	step [220/244], loss=28.6864
	step [221/244], loss=28.2799
	step [222/244], loss=29.2027
	step [223/244], loss=30.7502
	step [224/244], loss=29.5929
	step [225/244], loss=28.5132
	step [226/244], loss=31.3130
	step [227/244], loss=31.0989
	step [228/244], loss=27.8607
	step [229/244], loss=30.0858
	step [230/244], loss=30.7610
	step [231/244], loss=27.4975
	step [232/244], loss=29.1875
	step [233/244], loss=28.3477
	step [234/244], loss=29.2497
	step [235/244], loss=29.9850
	step [236/244], loss=29.2341
	step [237/244], loss=29.8673
	step [238/244], loss=28.5329
	step [239/244], loss=30.5834
	step [240/244], loss=29.1657
	step [241/244], loss=28.1873
	step [242/244], loss=27.8118
	step [243/244], loss=28.5781
	step [244/244], loss=13.2440
	Evaluating
	loss=0.0973, precision=0.1741, recall=0.9927, f1=0.2962
Training epoch 5
	step [1/244], loss=27.0190
	step [2/244], loss=29.2999
	step [3/244], loss=29.1359
	step [4/244], loss=26.8402
	step [5/244], loss=28.8124
	step [6/244], loss=29.9182
	step [7/244], loss=28.2711
	step [8/244], loss=28.2247
	step [9/244], loss=29.5242
	step [10/244], loss=28.3222
	step [11/244], loss=27.7619
	step [12/244], loss=26.3589
	step [13/244], loss=29.6998
	step [14/244], loss=31.2630
	step [15/244], loss=29.0761
	step [16/244], loss=25.9420
	step [17/244], loss=27.6192
	step [18/244], loss=27.3899
	step [19/244], loss=26.2597
	step [20/244], loss=26.3089
	step [21/244], loss=28.4329
	step [22/244], loss=29.6989
	step [23/244], loss=28.6370
	step [24/244], loss=26.1281
	step [25/244], loss=30.7079
	step [26/244], loss=25.7980
	step [27/244], loss=27.5538
	step [28/244], loss=26.5954
	step [29/244], loss=27.4025
	step [30/244], loss=29.4174
	step [31/244], loss=28.5515
	step [32/244], loss=28.9984
	step [33/244], loss=29.3709
	step [34/244], loss=27.2398
	step [35/244], loss=27.6407
	step [36/244], loss=28.7074
	step [37/244], loss=29.4463
	step [38/244], loss=26.8244
	step [39/244], loss=29.0259
	step [40/244], loss=30.4180
	step [41/244], loss=26.5003
	step [42/244], loss=26.5316
	step [43/244], loss=26.5237
	step [44/244], loss=30.7888
	step [45/244], loss=27.5956
	step [46/244], loss=27.2554
	step [47/244], loss=32.6582
	step [48/244], loss=32.1561
	step [49/244], loss=28.6561
	step [50/244], loss=28.2009
	step [51/244], loss=29.8406
	step [52/244], loss=28.8649
	step [53/244], loss=30.2761
	step [54/244], loss=26.6041
	step [55/244], loss=28.0989
	step [56/244], loss=27.6283
	step [57/244], loss=29.6400
	step [58/244], loss=27.1873
	step [59/244], loss=26.3544
	step [60/244], loss=26.8027
	step [61/244], loss=26.7696
	step [62/244], loss=27.4676
	step [63/244], loss=28.9220
	step [64/244], loss=28.1924
	step [65/244], loss=27.3095
	step [66/244], loss=27.3979
	step [67/244], loss=27.3581
	step [68/244], loss=27.2247
	step [69/244], loss=28.3108
	step [70/244], loss=25.0506
	step [71/244], loss=28.9694
	step [72/244], loss=28.2913
	step [73/244], loss=28.0044
	step [74/244], loss=26.5958
	step [75/244], loss=28.2005
	step [76/244], loss=28.3184
	step [77/244], loss=29.1134
	step [78/244], loss=25.2924
	step [79/244], loss=27.3543
	step [80/244], loss=28.3597
	step [81/244], loss=28.3980
	step [82/244], loss=26.5273
	step [83/244], loss=27.0406
	step [84/244], loss=29.8554
	step [85/244], loss=27.5244
	step [86/244], loss=27.2928
	step [87/244], loss=26.1945
	step [88/244], loss=26.3389
	step [89/244], loss=26.6126
	step [90/244], loss=25.9901
	step [91/244], loss=26.9543
	step [92/244], loss=27.1954
	step [93/244], loss=26.2788
	step [94/244], loss=27.5023
	step [95/244], loss=28.9778
	step [96/244], loss=26.1973
	step [97/244], loss=25.9672
	step [98/244], loss=26.6260
	step [99/244], loss=28.1650
	step [100/244], loss=26.4186
	step [101/244], loss=27.0044
	step [102/244], loss=27.4765
	step [103/244], loss=26.4439
	step [104/244], loss=26.1555
	step [105/244], loss=27.0224
	step [106/244], loss=28.0382
	step [107/244], loss=28.4496
	step [108/244], loss=27.0348
	step [109/244], loss=27.9201
	step [110/244], loss=28.0243
	step [111/244], loss=28.9753
	step [112/244], loss=25.0507
	step [113/244], loss=26.7398
	step [114/244], loss=27.1044
	step [115/244], loss=27.8610
	step [116/244], loss=29.8820
	step [117/244], loss=27.0456
	step [118/244], loss=26.5102
	step [119/244], loss=27.6515
	step [120/244], loss=26.2696
	step [121/244], loss=25.9961
	step [122/244], loss=26.2550
	step [123/244], loss=25.5574
	step [124/244], loss=24.6022
	step [125/244], loss=25.8682
	step [126/244], loss=25.5928
	step [127/244], loss=28.6397
	step [128/244], loss=25.9797
	step [129/244], loss=27.0126
	step [130/244], loss=27.8710
	step [131/244], loss=24.6479
	step [132/244], loss=27.2409
	step [133/244], loss=25.0522
	step [134/244], loss=25.3736
	step [135/244], loss=28.2121
	step [136/244], loss=28.3022
	step [137/244], loss=26.0064
	step [138/244], loss=26.4434
	step [139/244], loss=26.6574
	step [140/244], loss=26.5409
	step [141/244], loss=24.9805
	step [142/244], loss=24.6815
	step [143/244], loss=26.2672
	step [144/244], loss=23.9570
	step [145/244], loss=26.6673
	step [146/244], loss=26.4674
	step [147/244], loss=24.6150
	step [148/244], loss=25.6492
	step [149/244], loss=25.0435
	step [150/244], loss=22.9526
	step [151/244], loss=26.3970
	step [152/244], loss=22.5257
	step [153/244], loss=24.8167
	step [154/244], loss=29.2887
	step [155/244], loss=23.4513
	step [156/244], loss=26.6448
	step [157/244], loss=23.8163
	step [158/244], loss=24.5125
	step [159/244], loss=25.1155
	step [160/244], loss=24.1718
	step [161/244], loss=25.9699
	step [162/244], loss=24.2906
	step [163/244], loss=24.4985
	step [164/244], loss=26.4349
	step [165/244], loss=27.1119
	step [166/244], loss=25.5567
	step [167/244], loss=25.0757
	step [168/244], loss=23.7944
	step [169/244], loss=24.3111
	step [170/244], loss=23.3561
	step [171/244], loss=24.6856
	step [172/244], loss=25.0628
	step [173/244], loss=28.8149
	step [174/244], loss=25.4419
	step [175/244], loss=25.5181
	step [176/244], loss=26.3075
	step [177/244], loss=25.5008
	step [178/244], loss=25.9239
	step [179/244], loss=23.9371
	step [180/244], loss=23.9979
	step [181/244], loss=25.5027
	step [182/244], loss=26.1927
	step [183/244], loss=24.2660
	step [184/244], loss=23.5580
	step [185/244], loss=28.4317
	step [186/244], loss=23.6151
	step [187/244], loss=24.7105
	step [188/244], loss=23.7340
	step [189/244], loss=23.9414
	step [190/244], loss=24.2535
	step [191/244], loss=25.4469
	step [192/244], loss=25.1592
	step [193/244], loss=24.3312
	step [194/244], loss=24.1969
	step [195/244], loss=25.4275
	step [196/244], loss=25.7368
	step [197/244], loss=24.1623
	step [198/244], loss=24.1858
	step [199/244], loss=25.0772
	step [200/244], loss=23.8683
	step [201/244], loss=27.1983
	step [202/244], loss=24.1342
	step [203/244], loss=23.6133
	step [204/244], loss=23.9277
	step [205/244], loss=23.3503
	step [206/244], loss=23.2359
	step [207/244], loss=26.7424
	step [208/244], loss=26.1872
	step [209/244], loss=24.9347
	step [210/244], loss=25.7584
	step [211/244], loss=24.9437
	step [212/244], loss=24.1116
	step [213/244], loss=25.6524
	step [214/244], loss=25.6729
	step [215/244], loss=25.7448
	step [216/244], loss=23.0854
	step [217/244], loss=23.6418
	step [218/244], loss=23.9149
	step [219/244], loss=25.8634
	step [220/244], loss=24.5635
	step [221/244], loss=24.9345
	step [222/244], loss=23.0857
	step [223/244], loss=23.9824
	step [224/244], loss=22.9129
	step [225/244], loss=22.2627
	step [226/244], loss=24.4310
	step [227/244], loss=23.6588
	step [228/244], loss=22.8588
	step [229/244], loss=22.6321
	step [230/244], loss=24.2718
	step [231/244], loss=25.3310
	step [232/244], loss=24.0970
	step [233/244], loss=24.0493
	step [234/244], loss=25.5902
	step [235/244], loss=25.1349
	step [236/244], loss=23.2899
	step [237/244], loss=22.5527
	step [238/244], loss=25.2844
	step [239/244], loss=23.6212
	step [240/244], loss=23.3091
	step [241/244], loss=24.6055
	step [242/244], loss=22.3857
	step [243/244], loss=24.8241
	step [244/244], loss=11.1621
	Evaluating
	loss=0.0774, precision=0.1864, recall=0.9927, f1=0.3139
Training epoch 6
	step [1/244], loss=25.7241
	step [2/244], loss=24.0892
	step [3/244], loss=24.2105
	step [4/244], loss=23.9803
	step [5/244], loss=24.5484
	step [6/244], loss=23.9653
	step [7/244], loss=24.4288
	step [8/244], loss=21.6538
	step [9/244], loss=21.8050
	step [10/244], loss=23.8485
	step [11/244], loss=22.2180
	step [12/244], loss=23.6758
	step [13/244], loss=21.7519
	step [14/244], loss=25.1475
	step [15/244], loss=24.7679
	step [16/244], loss=25.4639
	step [17/244], loss=24.1564
	step [18/244], loss=23.1986
	step [19/244], loss=24.6023
	step [20/244], loss=24.5792
	step [21/244], loss=23.6333
	step [22/244], loss=24.3045
	step [23/244], loss=22.7170
	step [24/244], loss=26.8432
	step [25/244], loss=23.4831
	step [26/244], loss=21.8719
	step [27/244], loss=23.2010
	step [28/244], loss=23.1027
	step [29/244], loss=21.9716
	step [30/244], loss=21.1719
	step [31/244], loss=22.2854
	step [32/244], loss=24.5326
	step [33/244], loss=23.7508
	step [34/244], loss=24.2526
	step [35/244], loss=24.6080
	step [36/244], loss=23.2708
	step [37/244], loss=23.4264
	step [38/244], loss=24.0518
	step [39/244], loss=23.8650
	step [40/244], loss=22.6030
	step [41/244], loss=22.2335
	step [42/244], loss=23.4671
	step [43/244], loss=23.8727
	step [44/244], loss=23.4139
	step [45/244], loss=25.1128
	step [46/244], loss=23.6445
	step [47/244], loss=21.1570
	step [48/244], loss=23.0333
	step [49/244], loss=23.5248
	step [50/244], loss=22.3606
	step [51/244], loss=21.6614
	step [52/244], loss=23.3801
	step [53/244], loss=24.5792
	step [54/244], loss=22.6604
	step [55/244], loss=21.2296
	step [56/244], loss=22.8949
	step [57/244], loss=20.8097
	step [58/244], loss=24.0784
	step [59/244], loss=24.8217
	step [60/244], loss=22.2602
	step [61/244], loss=20.1340
	step [62/244], loss=21.7267
	step [63/244], loss=23.9948
	step [64/244], loss=22.8390
	step [65/244], loss=21.6555
	step [66/244], loss=20.4053
	step [67/244], loss=21.1607
	step [68/244], loss=20.9045
	step [69/244], loss=23.1649
	step [70/244], loss=22.8110
	step [71/244], loss=23.5601
	step [72/244], loss=22.4566
	step [73/244], loss=24.6505
	step [74/244], loss=28.0932
	step [75/244], loss=25.1448
	step [76/244], loss=24.8242
	step [77/244], loss=25.7365
	step [78/244], loss=22.9494
	step [79/244], loss=25.2113
	step [80/244], loss=24.4162
	step [81/244], loss=23.5344
	step [82/244], loss=24.0994
	step [83/244], loss=23.4244
	step [84/244], loss=23.4634
	step [85/244], loss=23.2734
	step [86/244], loss=22.5342
	step [87/244], loss=22.8228
	step [88/244], loss=22.7073
	step [89/244], loss=22.6714
	step [90/244], loss=21.1997
	step [91/244], loss=21.5551
	step [92/244], loss=23.0917
	step [93/244], loss=22.3993
	step [94/244], loss=22.7496
	step [95/244], loss=24.4271
	step [96/244], loss=22.1948
	step [97/244], loss=20.3413
	step [98/244], loss=21.7107
	step [99/244], loss=22.3098
	step [100/244], loss=20.5234
	step [101/244], loss=19.4426
	step [102/244], loss=22.4164
	step [103/244], loss=21.5907
	step [104/244], loss=24.5699
	step [105/244], loss=25.4026
	step [106/244], loss=21.3861
	step [107/244], loss=22.9172
	step [108/244], loss=25.2518
	step [109/244], loss=24.8201
	step [110/244], loss=23.6919
	step [111/244], loss=23.2864
	step [112/244], loss=22.2055
	step [113/244], loss=21.4552
	step [114/244], loss=22.2019
	step [115/244], loss=20.3470
	step [116/244], loss=23.3494
	step [117/244], loss=20.6950
	step [118/244], loss=24.0125
	step [119/244], loss=23.7982
	step [120/244], loss=22.0025
	step [121/244], loss=21.1707
	step [122/244], loss=21.7050
	step [123/244], loss=21.6527
	step [124/244], loss=22.1982
	step [125/244], loss=22.4891
	step [126/244], loss=23.6065
	step [127/244], loss=22.1831
	step [128/244], loss=21.4774
	step [129/244], loss=19.9307
	step [130/244], loss=23.2777
	step [131/244], loss=25.9285
	step [132/244], loss=21.5696
	step [133/244], loss=20.6782
	step [134/244], loss=21.8044
	step [135/244], loss=21.8595
	step [136/244], loss=24.6240
	step [137/244], loss=23.9389
	step [138/244], loss=20.4241
	step [139/244], loss=22.1726
	step [140/244], loss=20.1807
	step [141/244], loss=21.7755
	step [142/244], loss=21.7997
	step [143/244], loss=23.4090
	step [144/244], loss=19.8678
	step [145/244], loss=23.6596
	step [146/244], loss=20.8973
	step [147/244], loss=22.5334
	step [148/244], loss=18.9832
	step [149/244], loss=23.7288
	step [150/244], loss=21.9135
	step [151/244], loss=22.8774
	step [152/244], loss=21.9347
	step [153/244], loss=22.6213
	step [154/244], loss=20.0240
	step [155/244], loss=21.6379
	step [156/244], loss=21.8869
	step [157/244], loss=20.4606
	step [158/244], loss=22.9137
	step [159/244], loss=21.5518
	step [160/244], loss=20.6009
	step [161/244], loss=23.5933
	step [162/244], loss=23.6117
	step [163/244], loss=19.9269
	step [164/244], loss=20.5969
	step [165/244], loss=19.5251
	step [166/244], loss=22.7001
	step [167/244], loss=21.5076
	step [168/244], loss=21.3950
	step [169/244], loss=19.8790
	step [170/244], loss=19.8776
	step [171/244], loss=22.5336
	step [172/244], loss=18.8777
	step [173/244], loss=22.9673
	step [174/244], loss=21.4705
	step [175/244], loss=20.2238
	step [176/244], loss=20.9075
	step [177/244], loss=20.5643
	step [178/244], loss=20.7231
	step [179/244], loss=22.1203
	step [180/244], loss=22.0655
	step [181/244], loss=23.0422
	step [182/244], loss=24.6428
	step [183/244], loss=19.7711
	step [184/244], loss=22.0338
	step [185/244], loss=22.9131
	step [186/244], loss=21.6578
	step [187/244], loss=21.0402
	step [188/244], loss=20.9253
	step [189/244], loss=24.0455
	step [190/244], loss=24.2962
	step [191/244], loss=20.4595
	step [192/244], loss=23.0819
	step [193/244], loss=19.3343
	step [194/244], loss=20.0415
	step [195/244], loss=20.3891
	step [196/244], loss=21.2075
	step [197/244], loss=22.9680
	step [198/244], loss=20.8678
	step [199/244], loss=22.4689
	step [200/244], loss=22.3200
	step [201/244], loss=20.9952
	step [202/244], loss=20.9413
	step [203/244], loss=21.2378
	step [204/244], loss=19.4573
	step [205/244], loss=20.9127
	step [206/244], loss=19.2500
	step [207/244], loss=19.0962
	step [208/244], loss=21.3074
	step [209/244], loss=21.9357
	step [210/244], loss=22.2705
	step [211/244], loss=19.5738
	step [212/244], loss=21.3421
	step [213/244], loss=20.8217
	step [214/244], loss=20.3898
	step [215/244], loss=21.3134
	step [216/244], loss=20.8439
	step [217/244], loss=20.6158
	step [218/244], loss=20.6045
	step [219/244], loss=23.3769
	step [220/244], loss=22.1475
	step [221/244], loss=21.6773
	step [222/244], loss=20.6550
	step [223/244], loss=19.3702
	step [224/244], loss=19.8760
	step [225/244], loss=20.9650
	step [226/244], loss=17.4979
	step [227/244], loss=20.3755
	step [228/244], loss=20.8331
	step [229/244], loss=19.2229
	step [230/244], loss=21.7341
	step [231/244], loss=18.8969
	step [232/244], loss=18.9652
	step [233/244], loss=21.2791
	step [234/244], loss=21.4535
	step [235/244], loss=22.5224
	step [236/244], loss=20.1162
	step [237/244], loss=19.8220
	step [238/244], loss=21.4099
	step [239/244], loss=21.3768
	step [240/244], loss=17.1212
	step [241/244], loss=19.4947
	step [242/244], loss=21.1201
	step [243/244], loss=19.9413
	step [244/244], loss=8.7680
	Evaluating
	loss=0.0645, precision=0.1823, recall=0.9930, f1=0.3080
Training epoch 7
	step [1/244], loss=21.3092
	step [2/244], loss=21.7328
	step [3/244], loss=19.3064
	step [4/244], loss=21.0456
	step [5/244], loss=18.5699
	step [6/244], loss=23.3882
	step [7/244], loss=23.4291
	step [8/244], loss=19.5175
	step [9/244], loss=21.9811
	step [10/244], loss=19.7331
	step [11/244], loss=19.5693
	step [12/244], loss=22.0522
	step [13/244], loss=21.4253
	step [14/244], loss=18.9086
	step [15/244], loss=20.0792
	step [16/244], loss=20.7268
	step [17/244], loss=18.4697
	step [18/244], loss=21.5109
	step [19/244], loss=19.5776
	step [20/244], loss=20.1552
	step [21/244], loss=20.7255
	step [22/244], loss=18.4356
	step [23/244], loss=20.6244
	step [24/244], loss=20.1260
	step [25/244], loss=19.3685
	step [26/244], loss=18.3624
	step [27/244], loss=20.5315
	step [28/244], loss=18.5525
	step [29/244], loss=19.9922
	step [30/244], loss=17.7631
	step [31/244], loss=20.8501
	step [32/244], loss=19.2545
	step [33/244], loss=19.6081
	step [34/244], loss=23.7208
	step [35/244], loss=17.9681
	step [36/244], loss=18.6835
	step [37/244], loss=18.8975
	step [38/244], loss=21.1795
	step [39/244], loss=19.6105
	step [40/244], loss=19.5604
	step [41/244], loss=24.3890
	step [42/244], loss=18.1062
	step [43/244], loss=19.5196
	step [44/244], loss=21.3736
	step [45/244], loss=19.7271
	step [46/244], loss=21.0571
	step [47/244], loss=19.5034
	step [48/244], loss=20.2187
	step [49/244], loss=19.7896
	step [50/244], loss=19.1815
	step [51/244], loss=20.6514
	step [52/244], loss=19.2080
	step [53/244], loss=19.3122
	step [54/244], loss=19.4852
	step [55/244], loss=22.5064
	step [56/244], loss=19.6415
	step [57/244], loss=21.3126
	step [58/244], loss=18.5969
	step [59/244], loss=20.9699
	step [60/244], loss=20.3063
	step [61/244], loss=18.2939
	step [62/244], loss=20.6918
	step [63/244], loss=18.7856
	step [64/244], loss=20.9037
	step [65/244], loss=21.3308
	step [66/244], loss=19.5416
	step [67/244], loss=18.1062
	step [68/244], loss=22.0068
	step [69/244], loss=19.8553
	step [70/244], loss=20.2431
	step [71/244], loss=21.3613
	step [72/244], loss=20.5943
	step [73/244], loss=19.0469
	step [74/244], loss=17.1843
	step [75/244], loss=20.1471
	step [76/244], loss=23.2097
	step [77/244], loss=18.6532
	step [78/244], loss=20.0960
	step [79/244], loss=20.8692
	step [80/244], loss=17.5809
	step [81/244], loss=21.2512
	step [82/244], loss=18.5606
	step [83/244], loss=18.7704
	step [84/244], loss=18.3223
	step [85/244], loss=16.7740
	step [86/244], loss=18.9539
	step [87/244], loss=19.6406
	step [88/244], loss=18.9861
	step [89/244], loss=18.0910
	step [90/244], loss=20.3067
	step [91/244], loss=18.9072
	step [92/244], loss=17.4702
	step [93/244], loss=21.4662
	step [94/244], loss=19.7501
	step [95/244], loss=20.0375
	step [96/244], loss=20.3104
	step [97/244], loss=19.3071
	step [98/244], loss=18.6242
	step [99/244], loss=20.1447
	step [100/244], loss=21.6980
	step [101/244], loss=18.9281
	step [102/244], loss=17.0352
	step [103/244], loss=19.2336
	step [104/244], loss=17.9994
	step [105/244], loss=19.3605
	step [106/244], loss=17.0658
	step [107/244], loss=18.9272
	step [108/244], loss=16.5656
	step [109/244], loss=19.7435
	step [110/244], loss=18.8215
	step [111/244], loss=21.5211
	step [112/244], loss=17.9776
	step [113/244], loss=21.3384
	step [114/244], loss=18.0645
	step [115/244], loss=20.2441
	step [116/244], loss=20.4634
	step [117/244], loss=18.6356
	step [118/244], loss=19.1709
	step [119/244], loss=19.9371
	step [120/244], loss=20.1938
	step [121/244], loss=16.8527
	step [122/244], loss=21.8825
	step [123/244], loss=21.7502
	step [124/244], loss=21.7872
	step [125/244], loss=16.7391
	step [126/244], loss=18.1497
	step [127/244], loss=20.2395
	step [128/244], loss=19.0784
	step [129/244], loss=18.6413
	step [130/244], loss=18.2360
	step [131/244], loss=18.0588
	step [132/244], loss=21.8624
	step [133/244], loss=19.9775
	step [134/244], loss=18.8226
	step [135/244], loss=19.7289
	step [136/244], loss=16.8033
	step [137/244], loss=18.1109
	step [138/244], loss=19.1950
	step [139/244], loss=21.3405
	step [140/244], loss=17.4727
	step [141/244], loss=19.7918
	step [142/244], loss=18.7981
	step [143/244], loss=17.5946
	step [144/244], loss=19.0616
	step [145/244], loss=16.5033
	step [146/244], loss=19.6716
	step [147/244], loss=16.8077
	step [148/244], loss=20.1143
	step [149/244], loss=17.1823
	step [150/244], loss=18.0555
	step [151/244], loss=18.1237
	step [152/244], loss=18.2370
	step [153/244], loss=19.0936
	step [154/244], loss=19.5355
	step [155/244], loss=16.4401
	step [156/244], loss=19.6622
	step [157/244], loss=19.9355
	step [158/244], loss=19.8207
	step [159/244], loss=19.4545
	step [160/244], loss=17.0929
	step [161/244], loss=19.6791
	step [162/244], loss=18.3983
	step [163/244], loss=18.2562
	step [164/244], loss=19.7811
	step [165/244], loss=17.9489
	step [166/244], loss=17.7638
	step [167/244], loss=17.8893
	step [168/244], loss=19.1237
	step [169/244], loss=19.3038
	step [170/244], loss=17.3096
	step [171/244], loss=19.3297
	step [172/244], loss=17.5816
	step [173/244], loss=18.4656
	step [174/244], loss=18.6095
	step [175/244], loss=18.4053
	step [176/244], loss=20.0021
	step [177/244], loss=16.3751
	step [178/244], loss=18.2800
	step [179/244], loss=19.7077
	step [180/244], loss=17.5532
	step [181/244], loss=18.3367
	step [182/244], loss=18.6953
	step [183/244], loss=17.0265
	step [184/244], loss=18.3811
	step [185/244], loss=17.5392
	step [186/244], loss=17.5094
	step [187/244], loss=18.1122
	step [188/244], loss=20.8178
	step [189/244], loss=20.5668
	step [190/244], loss=18.7769
	step [191/244], loss=19.0478
	step [192/244], loss=17.7983
	step [193/244], loss=19.7404
	step [194/244], loss=18.5651
	step [195/244], loss=17.6465
	step [196/244], loss=16.2174
	step [197/244], loss=19.2333
	step [198/244], loss=19.0161
	step [199/244], loss=20.7874
	step [200/244], loss=18.2879
	step [201/244], loss=18.4552
	step [202/244], loss=17.9452
	step [203/244], loss=17.6183
	step [204/244], loss=18.7447
	step [205/244], loss=17.9568
	step [206/244], loss=18.3779
	step [207/244], loss=17.9547
	step [208/244], loss=17.8156
	step [209/244], loss=19.2633
	step [210/244], loss=15.4246
	step [211/244], loss=18.7477
	step [212/244], loss=18.4800
	step [213/244], loss=17.8926
	step [214/244], loss=18.4792
	step [215/244], loss=16.8683
	step [216/244], loss=18.2079
	step [217/244], loss=17.7154
	step [218/244], loss=18.2754
	step [219/244], loss=22.0911
	step [220/244], loss=18.5083
	step [221/244], loss=18.0044
	step [222/244], loss=18.7710
	step [223/244], loss=16.6281
	step [224/244], loss=19.4186
	step [225/244], loss=19.9118
	step [226/244], loss=17.2696
	step [227/244], loss=20.2637
	step [228/244], loss=17.9326
	step [229/244], loss=17.2802
	step [230/244], loss=16.8606
	step [231/244], loss=17.7219
	step [232/244], loss=17.9649
	step [233/244], loss=23.1721
	step [234/244], loss=17.8883
	step [235/244], loss=18.4429
	step [236/244], loss=19.1066
	step [237/244], loss=18.9791
	step [238/244], loss=19.4538
	step [239/244], loss=19.1183
	step [240/244], loss=17.3162
	step [241/244], loss=18.8527
	step [242/244], loss=17.3418
	step [243/244], loss=17.6843
	step [244/244], loss=7.5925
	Evaluating
	loss=0.0533, precision=0.1993, recall=0.9924, f1=0.3320
Training epoch 8
	step [1/244], loss=20.8345
	step [2/244], loss=16.6264
	step [3/244], loss=19.6838
	step [4/244], loss=18.6166
	step [5/244], loss=15.9863
	step [6/244], loss=19.7689
	step [7/244], loss=16.8401
	step [8/244], loss=18.3125
	step [9/244], loss=18.4140
	step [10/244], loss=17.0332
	step [11/244], loss=17.4029
	step [12/244], loss=17.9868
	step [13/244], loss=19.8986
	step [14/244], loss=18.4054
	step [15/244], loss=19.0500
	step [16/244], loss=19.0415
	step [17/244], loss=18.6550
	step [18/244], loss=16.3186
	step [19/244], loss=18.4835
	step [20/244], loss=19.0415
	step [21/244], loss=16.3322
	step [22/244], loss=21.2594
	step [23/244], loss=17.7328
	step [24/244], loss=17.5733
	step [25/244], loss=18.1144
	step [26/244], loss=16.5616
	step [27/244], loss=17.7933
	step [28/244], loss=18.5359
	step [29/244], loss=18.0283
	step [30/244], loss=15.3179
	step [31/244], loss=16.5213
	step [32/244], loss=17.5143
	step [33/244], loss=17.7054
	step [34/244], loss=17.2617
	step [35/244], loss=14.8625
	step [36/244], loss=15.7535
	step [37/244], loss=20.2530
	step [38/244], loss=19.6649
	step [39/244], loss=17.3655
	step [40/244], loss=16.9966
	step [41/244], loss=17.0088
	step [42/244], loss=16.9425
	step [43/244], loss=16.8548
	step [44/244], loss=19.4545
	step [45/244], loss=15.5491
	step [46/244], loss=17.8815
	step [47/244], loss=19.5131
	step [48/244], loss=15.2667
	step [49/244], loss=17.1341
	step [50/244], loss=20.3187
	step [51/244], loss=16.9116
	step [52/244], loss=16.5940
	step [53/244], loss=19.8369
	step [54/244], loss=18.1684
	step [55/244], loss=16.9389
	step [56/244], loss=15.5987
	step [57/244], loss=17.0424
	step [58/244], loss=16.2311
	step [59/244], loss=19.6784
	step [60/244], loss=19.6529
	step [61/244], loss=15.5372
	step [62/244], loss=20.4951
	step [63/244], loss=21.2560
	step [64/244], loss=19.3579
	step [65/244], loss=16.4223
	step [66/244], loss=18.8961
	step [67/244], loss=18.2658
	step [68/244], loss=17.4602
	step [69/244], loss=16.2543
	step [70/244], loss=16.9735
	step [71/244], loss=16.0135
	step [72/244], loss=16.6089
	step [73/244], loss=16.4275
	step [74/244], loss=20.9474
	step [75/244], loss=16.5098
	step [76/244], loss=17.6168
	step [77/244], loss=15.5310
	step [78/244], loss=17.8759
	step [79/244], loss=14.9419
	step [80/244], loss=14.9386
	step [81/244], loss=16.5100
	step [82/244], loss=15.8489
	step [83/244], loss=16.4082
	step [84/244], loss=19.3511
	step [85/244], loss=19.8590
	step [86/244], loss=17.0229
	step [87/244], loss=16.5172
	step [88/244], loss=15.9807
	step [89/244], loss=14.7992
	step [90/244], loss=14.3233
	step [91/244], loss=16.0131
	step [92/244], loss=17.1547
	step [93/244], loss=19.3554
	step [94/244], loss=16.2473
	step [95/244], loss=15.0527
	step [96/244], loss=18.0263
	step [97/244], loss=18.4445
	step [98/244], loss=16.7381
	step [99/244], loss=16.7093
	step [100/244], loss=17.7263
	step [101/244], loss=17.2473
	step [102/244], loss=16.0792
	step [103/244], loss=14.9181
	step [104/244], loss=15.8102
	step [105/244], loss=17.3474
	step [106/244], loss=15.5783
	step [107/244], loss=17.0469
	step [108/244], loss=16.3532
	step [109/244], loss=17.8078
	step [110/244], loss=17.6101
	step [111/244], loss=15.2761
	step [112/244], loss=20.6677
	step [113/244], loss=18.2875
	step [114/244], loss=17.5583
	step [115/244], loss=16.0472
	step [116/244], loss=17.5604
	step [117/244], loss=17.0326
	step [118/244], loss=16.0687
	step [119/244], loss=15.5302
	step [120/244], loss=16.7139
	step [121/244], loss=16.5050
	step [122/244], loss=15.9301
	step [123/244], loss=17.8653
	step [124/244], loss=15.8458
	step [125/244], loss=18.3845
	step [126/244], loss=16.8357
	step [127/244], loss=17.0718
	step [128/244], loss=16.4344
	step [129/244], loss=18.3412
	step [130/244], loss=18.3887
	step [131/244], loss=16.6249
	step [132/244], loss=16.9594
	step [133/244], loss=18.1481
	step [134/244], loss=14.5732
	step [135/244], loss=17.0209
	step [136/244], loss=17.8694
	step [137/244], loss=16.2182
	step [138/244], loss=13.7709
	step [139/244], loss=17.6008
	step [140/244], loss=16.1627
	step [141/244], loss=14.3174
	step [142/244], loss=17.7601
	step [143/244], loss=15.2963
	step [144/244], loss=14.2542
	step [145/244], loss=18.1224
	step [146/244], loss=14.3276
	step [147/244], loss=15.9306
	step [148/244], loss=16.6086
	step [149/244], loss=17.3177
	step [150/244], loss=16.0141
	step [151/244], loss=16.6762
	step [152/244], loss=16.7292
	step [153/244], loss=16.7609
	step [154/244], loss=16.5503
	step [155/244], loss=15.4744
	step [156/244], loss=17.4343
	step [157/244], loss=17.1888
	step [158/244], loss=15.4925
	step [159/244], loss=17.0551
	step [160/244], loss=19.0201
	step [161/244], loss=20.8635
	step [162/244], loss=17.6254
	step [163/244], loss=17.1769
	step [164/244], loss=18.4465
	step [165/244], loss=16.3528
	step [166/244], loss=18.0603
	step [167/244], loss=15.9943
	step [168/244], loss=15.8824
	step [169/244], loss=14.4942
	step [170/244], loss=14.5578
	step [171/244], loss=16.4964
	step [172/244], loss=14.8593
	step [173/244], loss=16.2492
	step [174/244], loss=17.0179
	step [175/244], loss=18.8002
	step [176/244], loss=14.8905
	step [177/244], loss=15.8234
	step [178/244], loss=17.7652
	step [179/244], loss=15.5771
	step [180/244], loss=17.6255
	step [181/244], loss=15.3631
	step [182/244], loss=16.8027
	step [183/244], loss=15.0822
	step [184/244], loss=15.9150
	step [185/244], loss=18.4487
	step [186/244], loss=15.6613
	step [187/244], loss=13.5024
	step [188/244], loss=15.9676
	step [189/244], loss=16.4414
	step [190/244], loss=16.9021
	step [191/244], loss=17.1207
	step [192/244], loss=16.6934
	step [193/244], loss=15.4186
	step [194/244], loss=14.8874
	step [195/244], loss=16.6269
	step [196/244], loss=15.9251
	step [197/244], loss=16.6617
	step [198/244], loss=15.6736
	step [199/244], loss=14.5906
	step [200/244], loss=16.8443
	step [201/244], loss=17.6988
	step [202/244], loss=14.4031
	step [203/244], loss=19.0803
	step [204/244], loss=16.1566
	step [205/244], loss=15.2332
	step [206/244], loss=16.1221
	step [207/244], loss=15.8015
	step [208/244], loss=18.1810
	step [209/244], loss=15.3006
	step [210/244], loss=14.6368
	step [211/244], loss=17.4196
	step [212/244], loss=14.9504
	step [213/244], loss=17.1469
	step [214/244], loss=15.0892
	step [215/244], loss=14.8794
	step [216/244], loss=17.9074
	step [217/244], loss=16.1545
	step [218/244], loss=16.8519
	step [219/244], loss=16.0919
	step [220/244], loss=16.3426
	step [221/244], loss=17.7291
	step [222/244], loss=17.1536
	step [223/244], loss=17.1980
	step [224/244], loss=15.7078
	step [225/244], loss=16.2950
	step [226/244], loss=14.6381
	step [227/244], loss=15.9411
	step [228/244], loss=16.4501
	step [229/244], loss=18.4302
	step [230/244], loss=16.8403
	step [231/244], loss=17.5378
	step [232/244], loss=15.3785
	step [233/244], loss=16.2497
	step [234/244], loss=17.5747
	step [235/244], loss=16.0839
	step [236/244], loss=16.8127
	step [237/244], loss=14.3247
	step [238/244], loss=16.7683
	step [239/244], loss=15.0432
	step [240/244], loss=16.1715
	step [241/244], loss=14.5153
	step [242/244], loss=15.9778
	step [243/244], loss=14.6262
	step [244/244], loss=8.2783
	Evaluating
	loss=0.0431, precision=0.2080, recall=0.9918, f1=0.3439
saving model as: 1_saved_model.pth
Training epoch 9
	step [1/244], loss=15.7285
	step [2/244], loss=17.6512
	step [3/244], loss=15.9915
	step [4/244], loss=14.9623
	step [5/244], loss=17.2749
	step [6/244], loss=16.9836
	step [7/244], loss=15.4403
	step [8/244], loss=15.6810
	step [9/244], loss=14.0093
	step [10/244], loss=14.6293
	step [11/244], loss=16.8379
	step [12/244], loss=18.0356
	step [13/244], loss=14.8127
	step [14/244], loss=14.6698
	step [15/244], loss=15.5004
	step [16/244], loss=16.5126
	step [17/244], loss=16.9768
	step [18/244], loss=17.0969
	step [19/244], loss=16.0722
	step [20/244], loss=19.2149
	step [21/244], loss=16.4910
	step [22/244], loss=15.8759
	step [23/244], loss=16.2568
	step [24/244], loss=15.3100
	step [25/244], loss=16.1665
	step [26/244], loss=16.5513
	step [27/244], loss=14.5521
	step [28/244], loss=13.8412
	step [29/244], loss=14.7653
	step [30/244], loss=14.2826
	step [31/244], loss=20.7494
	step [32/244], loss=15.3327
	step [33/244], loss=15.8757
	step [34/244], loss=14.2867
	step [35/244], loss=15.3919
	step [36/244], loss=17.5843
	step [37/244], loss=17.4508
	step [38/244], loss=15.7001
	step [39/244], loss=15.9582
	step [40/244], loss=15.7699
	step [41/244], loss=14.5778
	step [42/244], loss=15.6808
	step [43/244], loss=14.0807
	step [44/244], loss=14.9254
	step [45/244], loss=13.6015
	step [46/244], loss=14.8959
	step [47/244], loss=16.7996
	step [48/244], loss=18.3841
	step [49/244], loss=17.3690
	step [50/244], loss=14.9881
	step [51/244], loss=15.2760
	step [52/244], loss=14.7935
	step [53/244], loss=17.4142
	step [54/244], loss=13.2759
	step [55/244], loss=15.8827
	step [56/244], loss=13.3099
	step [57/244], loss=13.6430
	step [58/244], loss=15.5424
	step [59/244], loss=15.2543
	step [60/244], loss=16.6972
	step [61/244], loss=13.5062
	step [62/244], loss=16.4204
	step [63/244], loss=14.3501
	step [64/244], loss=19.6847
	step [65/244], loss=16.1583
	step [66/244], loss=15.4646
	step [67/244], loss=16.2376
	step [68/244], loss=17.2902
	step [69/244], loss=14.4270
	step [70/244], loss=14.6963
	step [71/244], loss=14.4845
	step [72/244], loss=13.8678
	step [73/244], loss=14.6124
	step [74/244], loss=16.1714
	step [75/244], loss=13.9295
	step [76/244], loss=15.8649
	step [77/244], loss=15.0890
	step [78/244], loss=15.6655
	step [79/244], loss=14.5467
	step [80/244], loss=14.9229
	step [81/244], loss=15.4735
	step [82/244], loss=15.4246
	step [83/244], loss=16.8354
	step [84/244], loss=15.1260
	step [85/244], loss=17.1941
	step [86/244], loss=14.6418
	step [87/244], loss=14.3102
	step [88/244], loss=15.9556
	step [89/244], loss=15.8691
	step [90/244], loss=17.2689
	step [91/244], loss=16.9649
	step [92/244], loss=13.3684
	step [93/244], loss=15.4132
	step [94/244], loss=14.2587
	step [95/244], loss=15.2343
	step [96/244], loss=16.1780
	step [97/244], loss=15.0633
	step [98/244], loss=13.9194
	step [99/244], loss=15.2086
	step [100/244], loss=14.4053
	step [101/244], loss=14.5174
	step [102/244], loss=14.1250
	step [103/244], loss=12.8745
	step [104/244], loss=15.6993
	step [105/244], loss=15.4512
	step [106/244], loss=17.2796
	step [107/244], loss=13.4326
	step [108/244], loss=15.5367
	step [109/244], loss=14.7668
	step [110/244], loss=16.5536
	step [111/244], loss=17.1973
	step [112/244], loss=15.9412
	step [113/244], loss=15.2601
	step [114/244], loss=14.6861
	step [115/244], loss=12.1381
	step [116/244], loss=16.0282
	step [117/244], loss=14.9446
	step [118/244], loss=13.0961
	step [119/244], loss=14.7767
	step [120/244], loss=14.1678
	step [121/244], loss=15.0341
	step [122/244], loss=12.2346
	step [123/244], loss=14.8169
	step [124/244], loss=12.4296
	step [125/244], loss=13.9205
	step [126/244], loss=13.0100
	step [127/244], loss=16.7096
	step [128/244], loss=14.5372
	step [129/244], loss=21.5539
	step [130/244], loss=15.2957
	step [131/244], loss=15.8750
	step [132/244], loss=16.7727
	step [133/244], loss=15.6931
	step [134/244], loss=15.0008
	step [135/244], loss=15.3113
	step [136/244], loss=16.7619
	step [137/244], loss=16.4398
	step [138/244], loss=12.7748
	step [139/244], loss=17.3887
	step [140/244], loss=16.8404
	step [141/244], loss=14.1611
	step [142/244], loss=15.8698
	step [143/244], loss=14.8625
	step [144/244], loss=18.0101
	step [145/244], loss=14.0603
	step [146/244], loss=16.8357
	step [147/244], loss=17.0493
	step [148/244], loss=14.2858
	step [149/244], loss=17.0458
	step [150/244], loss=14.8429
	step [151/244], loss=13.6233
	step [152/244], loss=13.4531
	step [153/244], loss=17.9038
	step [154/244], loss=16.4802
	step [155/244], loss=13.7385
	step [156/244], loss=11.8758
	step [157/244], loss=14.7506
	step [158/244], loss=16.3773
	step [159/244], loss=12.7013
	step [160/244], loss=15.7108
	step [161/244], loss=15.0841
	step [162/244], loss=13.8086
	step [163/244], loss=17.4990
	step [164/244], loss=16.7349
	step [165/244], loss=14.9631
	step [166/244], loss=16.2014
	step [167/244], loss=16.2690
	step [168/244], loss=14.8709
	step [169/244], loss=14.6421
	step [170/244], loss=16.6341
	step [171/244], loss=15.0199
	step [172/244], loss=14.6809
	step [173/244], loss=14.8350
	step [174/244], loss=13.1673
	step [175/244], loss=13.0322
	step [176/244], loss=14.3876
	step [177/244], loss=14.5586
	step [178/244], loss=13.0688
	step [179/244], loss=18.7607
	step [180/244], loss=13.3876
	step [181/244], loss=15.2874
	step [182/244], loss=14.9223
	step [183/244], loss=16.4584
	step [184/244], loss=14.4156
	step [185/244], loss=15.5426
	step [186/244], loss=16.4346
	step [187/244], loss=15.0221
	step [188/244], loss=14.0672
	step [189/244], loss=13.4198
	step [190/244], loss=14.2778
	step [191/244], loss=12.8218
	step [192/244], loss=14.4693
	step [193/244], loss=14.3543
	step [194/244], loss=13.5484
	step [195/244], loss=15.2120
	step [196/244], loss=13.7814
	step [197/244], loss=13.0388
	step [198/244], loss=14.9892
	step [199/244], loss=15.4315
	step [200/244], loss=14.8746
	step [201/244], loss=14.6983
	step [202/244], loss=13.1661
	step [203/244], loss=13.0994
	step [204/244], loss=15.5355
	step [205/244], loss=13.5776
	step [206/244], loss=15.5080
	step [207/244], loss=14.0302
	step [208/244], loss=15.7873
	step [209/244], loss=15.1641
	step [210/244], loss=14.9339
	step [211/244], loss=13.4891
	step [212/244], loss=15.2280
	step [213/244], loss=11.5889
	step [214/244], loss=12.8288
	step [215/244], loss=16.0619
	step [216/244], loss=17.2965
	step [217/244], loss=14.0445
	step [218/244], loss=15.1643
	step [219/244], loss=15.4448
	step [220/244], loss=12.7401
	step [221/244], loss=16.4079
	step [222/244], loss=13.8593
	step [223/244], loss=13.5448
	step [224/244], loss=15.7672
	step [225/244], loss=15.4538
	step [226/244], loss=12.6660
	step [227/244], loss=16.0981
	step [228/244], loss=12.5322
	step [229/244], loss=15.3540
	step [230/244], loss=13.4052
	step [231/244], loss=13.2043
	step [232/244], loss=15.5975
	step [233/244], loss=14.6273
	step [234/244], loss=15.0346
	step [235/244], loss=13.4606
	step [236/244], loss=15.4758
	step [237/244], loss=14.2594
	step [238/244], loss=11.8144
	step [239/244], loss=18.0672
	step [240/244], loss=14.0319
	step [241/244], loss=13.0228
	step [242/244], loss=14.8909
	step [243/244], loss=12.7383
	step [244/244], loss=6.5234
	Evaluating
	loss=0.0405, precision=0.1730, recall=0.9927, f1=0.2946
Training epoch 10
	step [1/244], loss=15.5586
	step [2/244], loss=12.7082
	step [3/244], loss=12.8769
	step [4/244], loss=13.7937
	step [5/244], loss=11.6486
	step [6/244], loss=16.6224
	step [7/244], loss=14.7002
	step [8/244], loss=14.3669
	step [9/244], loss=13.4587
	step [10/244], loss=12.6157
	step [11/244], loss=15.5280
	step [12/244], loss=13.8404
	step [13/244], loss=16.1197
	step [14/244], loss=13.4096
	step [15/244], loss=13.5529
	step [16/244], loss=12.0346
	step [17/244], loss=13.5412
	step [18/244], loss=12.3548
	step [19/244], loss=13.1240
	step [20/244], loss=15.4829
	step [21/244], loss=15.0940
	step [22/244], loss=14.6577
	step [23/244], loss=12.4939
	step [24/244], loss=12.8300
	step [25/244], loss=12.7676
	step [26/244], loss=13.6491
	step [27/244], loss=15.6008
	step [28/244], loss=15.8635
	step [29/244], loss=13.7761
	step [30/244], loss=15.6856
	step [31/244], loss=14.3092
	step [32/244], loss=15.6712
	step [33/244], loss=14.3589
	step [34/244], loss=15.1223
	step [35/244], loss=16.5796
	step [36/244], loss=14.3754
	step [37/244], loss=13.8776
	step [38/244], loss=13.4942
	step [39/244], loss=13.7671
	step [40/244], loss=14.5575
	step [41/244], loss=13.0325
	step [42/244], loss=15.3457
	step [43/244], loss=13.1660
	step [44/244], loss=14.0542
	step [45/244], loss=15.4155
	step [46/244], loss=13.7239
	step [47/244], loss=14.5223
	step [48/244], loss=13.1529
	step [49/244], loss=13.8871
	step [50/244], loss=15.5835
	step [51/244], loss=15.0977
	step [52/244], loss=15.7772
	step [53/244], loss=14.6627
	step [54/244], loss=15.6046
	step [55/244], loss=13.0734
	step [56/244], loss=14.5317
	step [57/244], loss=15.6817
	step [58/244], loss=15.1416
	step [59/244], loss=14.5236
	step [60/244], loss=17.0825
	step [61/244], loss=15.2486
	step [62/244], loss=11.6770
	step [63/244], loss=14.4638
	step [64/244], loss=12.6541
	step [65/244], loss=12.5311
	step [66/244], loss=13.1424
	step [67/244], loss=13.4193
	step [68/244], loss=11.7999
	step [69/244], loss=15.8963
	step [70/244], loss=12.7696
	step [71/244], loss=13.0895
	step [72/244], loss=12.7033
	step [73/244], loss=12.7261
	step [74/244], loss=17.2484
	step [75/244], loss=12.7427
	step [76/244], loss=13.9365
	step [77/244], loss=15.2400
	step [78/244], loss=13.0978
	step [79/244], loss=12.8359
	step [80/244], loss=16.0907
	step [81/244], loss=14.6483
	step [82/244], loss=13.7545
	step [83/244], loss=13.1822
	step [84/244], loss=14.0022
	step [85/244], loss=12.9059
	step [86/244], loss=12.9509
	step [87/244], loss=13.3786
	step [88/244], loss=12.4467
	step [89/244], loss=13.0679
	step [90/244], loss=13.9197
	step [91/244], loss=12.5381
	step [92/244], loss=14.5421
	step [93/244], loss=13.1732
	step [94/244], loss=12.9446
	step [95/244], loss=16.0610
	step [96/244], loss=15.2708
	step [97/244], loss=11.8559
	step [98/244], loss=13.9402
	step [99/244], loss=14.8959
	step [100/244], loss=13.1044
	step [101/244], loss=15.8061
	step [102/244], loss=14.3065
	step [103/244], loss=11.5898
	step [104/244], loss=13.1938
	step [105/244], loss=14.7193
	step [106/244], loss=17.8342
	step [107/244], loss=14.5460
	step [108/244], loss=14.3080
	step [109/244], loss=12.9236
	step [110/244], loss=12.8600
	step [111/244], loss=15.7289
	step [112/244], loss=15.0433
	step [113/244], loss=14.0521
	step [114/244], loss=14.0580
	step [115/244], loss=14.3136
	step [116/244], loss=13.9126
	step [117/244], loss=17.4006
	step [118/244], loss=13.7165
	step [119/244], loss=12.9846
	step [120/244], loss=16.3816
	step [121/244], loss=14.4484
	step [122/244], loss=12.7960
	step [123/244], loss=13.2927
	step [124/244], loss=15.5299
	step [125/244], loss=14.1675
	step [126/244], loss=11.2566
	step [127/244], loss=14.2681
	step [128/244], loss=12.8990
	step [129/244], loss=14.3402
	step [130/244], loss=14.6775
	step [131/244], loss=12.9446
	step [132/244], loss=13.0687
	step [133/244], loss=11.3291
	step [134/244], loss=13.0219
	step [135/244], loss=13.7417
	step [136/244], loss=13.7186
	step [137/244], loss=14.4886
	step [138/244], loss=14.5179
	step [139/244], loss=13.8271
	step [140/244], loss=12.6924
	step [141/244], loss=13.3293
	step [142/244], loss=11.8544
	step [143/244], loss=13.0483
	step [144/244], loss=15.0981
	step [145/244], loss=13.1855
	step [146/244], loss=13.6275
	step [147/244], loss=12.7573
	step [148/244], loss=13.5548
	step [149/244], loss=12.5766
	step [150/244], loss=15.2603
	step [151/244], loss=12.5292
	step [152/244], loss=11.8551
	step [153/244], loss=13.9798
	step [154/244], loss=13.0927
	step [155/244], loss=12.9731
	step [156/244], loss=15.8913
	step [157/244], loss=12.0971
	step [158/244], loss=13.0737
	step [159/244], loss=14.9436
	step [160/244], loss=14.5323
	step [161/244], loss=11.9229
	step [162/244], loss=13.8239
	step [163/244], loss=13.3267
	step [164/244], loss=12.4699
	step [165/244], loss=13.3104
	step [166/244], loss=12.5135
	step [167/244], loss=12.9756
	step [168/244], loss=12.8620
	step [169/244], loss=14.9987
	step [170/244], loss=13.0482
	step [171/244], loss=12.5010
	step [172/244], loss=16.8026
	step [173/244], loss=14.9866
	step [174/244], loss=14.7641
	step [175/244], loss=14.3784
	step [176/244], loss=12.9194
	step [177/244], loss=11.6632
	step [178/244], loss=13.6690
	step [179/244], loss=12.9212
	step [180/244], loss=12.7309
	step [181/244], loss=16.0355
	step [182/244], loss=12.4439
	step [183/244], loss=14.4310
	step [184/244], loss=13.2064
	step [185/244], loss=12.8166
	step [186/244], loss=18.2483
	step [187/244], loss=13.2312
	step [188/244], loss=15.5472
	step [189/244], loss=13.8228
	step [190/244], loss=12.0892
	step [191/244], loss=13.0232
	step [192/244], loss=13.8344
	step [193/244], loss=12.4837
	step [194/244], loss=15.2683
	step [195/244], loss=12.7383
	step [196/244], loss=14.5020
	step [197/244], loss=14.0021
	step [198/244], loss=11.9912
	step [199/244], loss=13.7549
	step [200/244], loss=13.1571
	step [201/244], loss=11.8075
	step [202/244], loss=15.8298
	step [203/244], loss=13.6468
	step [204/244], loss=12.5422
	step [205/244], loss=13.6185
	step [206/244], loss=13.6297
	step [207/244], loss=13.7421
	step [208/244], loss=11.6181
	step [209/244], loss=12.9296
	step [210/244], loss=14.3843
	step [211/244], loss=12.4025
	step [212/244], loss=13.4382
	step [213/244], loss=14.1048
	step [214/244], loss=14.0032
	step [215/244], loss=12.5729
	step [216/244], loss=12.0078
	step [217/244], loss=13.9093
	step [218/244], loss=13.8748
	step [219/244], loss=12.9652
	step [220/244], loss=14.0648
	step [221/244], loss=14.8253
	step [222/244], loss=13.3639
	step [223/244], loss=12.5782
	step [224/244], loss=12.6720
	step [225/244], loss=14.1477
	step [226/244], loss=10.9474
	step [227/244], loss=12.9612
	step [228/244], loss=13.6596
	step [229/244], loss=10.9022
	step [230/244], loss=13.8176
	step [231/244], loss=11.1599
	step [232/244], loss=15.2273
	step [233/244], loss=12.5134
	step [234/244], loss=11.7620
	step [235/244], loss=12.4464
	step [236/244], loss=13.5711
	step [237/244], loss=15.3127
	step [238/244], loss=13.1695
	step [239/244], loss=16.1344
	step [240/244], loss=11.2400
	step [241/244], loss=12.5717
	step [242/244], loss=15.1162
	step [243/244], loss=12.0645
	step [244/244], loss=5.9426
	Evaluating
	loss=0.0384, precision=0.1514, recall=0.9932, f1=0.2627
Training epoch 11
	step [1/244], loss=13.3129
	step [2/244], loss=12.1147
	step [3/244], loss=13.3082
	step [4/244], loss=13.6340
	step [5/244], loss=12.8964
	step [6/244], loss=13.4266
	step [7/244], loss=15.6925
	step [8/244], loss=13.6426
	step [9/244], loss=12.9358
	step [10/244], loss=13.0580
	step [11/244], loss=12.1683
	step [12/244], loss=15.1450
	step [13/244], loss=12.4767
	step [14/244], loss=13.7019
	step [15/244], loss=13.8828
	step [16/244], loss=12.5791
	step [17/244], loss=11.5997
	step [18/244], loss=13.2839
	step [19/244], loss=11.3159
	step [20/244], loss=11.7751
	step [21/244], loss=12.0536
	step [22/244], loss=16.2865
	step [23/244], loss=11.0607
	step [24/244], loss=12.1201
	step [25/244], loss=12.8958
	step [26/244], loss=12.5059
	step [27/244], loss=10.0961
	step [28/244], loss=13.1621
	step [29/244], loss=12.3011
	step [30/244], loss=13.0460
	step [31/244], loss=14.2241
	step [32/244], loss=14.3520
	step [33/244], loss=11.8415
	step [34/244], loss=11.2957
	step [35/244], loss=11.5903
	step [36/244], loss=13.0766
	step [37/244], loss=16.0059
	step [38/244], loss=14.2151
	step [39/244], loss=15.3803
	step [40/244], loss=18.0066
	step [41/244], loss=12.7966
	step [42/244], loss=12.3546
	step [43/244], loss=14.5169
	step [44/244], loss=13.2165
	step [45/244], loss=15.0992
	step [46/244], loss=14.4777
	step [47/244], loss=12.1019
	step [48/244], loss=13.9776
	step [49/244], loss=14.5010
	step [50/244], loss=13.4372
	step [51/244], loss=12.6140
	step [52/244], loss=13.3049
	step [53/244], loss=12.2259
	step [54/244], loss=12.0772
	step [55/244], loss=11.3725
	step [56/244], loss=14.9537
	step [57/244], loss=12.7117
	step [58/244], loss=11.2145
	step [59/244], loss=14.2414
	step [60/244], loss=12.3548
	step [61/244], loss=14.2555
	step [62/244], loss=13.4878
	step [63/244], loss=11.3322
	step [64/244], loss=13.9517
	step [65/244], loss=11.9591
	step [66/244], loss=14.1676
	step [67/244], loss=14.1534
	step [68/244], loss=12.7218
	step [69/244], loss=13.4536
	step [70/244], loss=12.4466
	step [71/244], loss=11.0193
	step [72/244], loss=12.6321
	step [73/244], loss=12.7562
	step [74/244], loss=12.8098
	step [75/244], loss=11.2061
	step [76/244], loss=13.5886
	step [77/244], loss=11.6994
	step [78/244], loss=13.5250
	step [79/244], loss=12.9178
	step [80/244], loss=12.6139
	step [81/244], loss=13.7112
	step [82/244], loss=14.7832
	step [83/244], loss=10.9105
	step [84/244], loss=13.8663
	step [85/244], loss=14.3784
	step [86/244], loss=13.7335
	step [87/244], loss=11.7549
	step [88/244], loss=9.9588
	step [89/244], loss=11.6593
	step [90/244], loss=12.2320
	step [91/244], loss=11.4734
	step [92/244], loss=11.0148
	step [93/244], loss=11.7046
	step [94/244], loss=11.6792
	step [95/244], loss=13.4409
	step [96/244], loss=12.0737
	step [97/244], loss=12.6862
	step [98/244], loss=12.8460
	step [99/244], loss=16.4819
	step [100/244], loss=13.9799
	step [101/244], loss=12.4544
	step [102/244], loss=14.1928
	step [103/244], loss=12.0315
	step [104/244], loss=12.1315
	step [105/244], loss=11.8384
	step [106/244], loss=15.9387
	step [107/244], loss=14.1508
	step [108/244], loss=12.1779
	step [109/244], loss=12.2602
	step [110/244], loss=12.1953
	step [111/244], loss=14.8879
	step [112/244], loss=12.9912
	step [113/244], loss=12.3966
	step [114/244], loss=10.9029
	step [115/244], loss=12.2802
	step [116/244], loss=12.5999
	step [117/244], loss=12.1180
	step [118/244], loss=13.8465
	step [119/244], loss=11.2049
	step [120/244], loss=10.0358
	step [121/244], loss=11.4912
	step [122/244], loss=13.4099
	step [123/244], loss=11.7879
	step [124/244], loss=14.4922
	step [125/244], loss=14.4278
	step [126/244], loss=11.6785
	step [127/244], loss=11.8746
	step [128/244], loss=12.6351
	step [129/244], loss=11.5971
	step [130/244], loss=9.8841
	step [131/244], loss=12.6879
	step [132/244], loss=10.8078
	step [133/244], loss=13.5930
	step [134/244], loss=13.4135
	step [135/244], loss=12.6078
	step [136/244], loss=12.7594
	step [137/244], loss=12.3890
	step [138/244], loss=12.9194
	step [139/244], loss=12.9428
	step [140/244], loss=11.2704
	step [141/244], loss=13.2517
	step [142/244], loss=12.4069
	step [143/244], loss=11.3631
	step [144/244], loss=11.0483
	step [145/244], loss=16.4783
	step [146/244], loss=12.3864
	step [147/244], loss=13.3642
	step [148/244], loss=11.1070
	step [149/244], loss=14.7130
	step [150/244], loss=14.7924
	step [151/244], loss=13.2320
	step [152/244], loss=14.5363
	step [153/244], loss=13.8996
	step [154/244], loss=13.1609
	step [155/244], loss=12.4345
	step [156/244], loss=12.9458
	step [157/244], loss=9.7561
	step [158/244], loss=11.7507
	step [159/244], loss=11.0643
	step [160/244], loss=12.4131
	step [161/244], loss=15.1394
	step [162/244], loss=12.6636
	step [163/244], loss=11.8711
	step [164/244], loss=14.3209
	step [165/244], loss=11.3634
	step [166/244], loss=12.4131
	step [167/244], loss=11.8033
	step [168/244], loss=10.4057
	step [169/244], loss=11.2014
	step [170/244], loss=11.8878
	step [171/244], loss=13.0828
	step [172/244], loss=11.6212
	step [173/244], loss=14.1142
	step [174/244], loss=12.3802
	step [175/244], loss=11.2764
	step [176/244], loss=10.7260
	step [177/244], loss=13.1238
	step [178/244], loss=12.2900
	step [179/244], loss=13.8846
	step [180/244], loss=13.0592
	step [181/244], loss=13.4718
	step [182/244], loss=13.1700
	step [183/244], loss=13.5658
	step [184/244], loss=12.5850
	step [185/244], loss=11.2105
	step [186/244], loss=13.6389
	step [187/244], loss=12.2816
	step [188/244], loss=11.7464
	step [189/244], loss=10.7726
	step [190/244], loss=13.2675
	step [191/244], loss=12.8854
	step [192/244], loss=13.3001
	step [193/244], loss=11.8611
	step [194/244], loss=10.2055
	step [195/244], loss=13.0150
	step [196/244], loss=13.0461
	step [197/244], loss=11.7546
	step [198/244], loss=11.1653
	step [199/244], loss=11.8922
	step [200/244], loss=12.0337
	step [201/244], loss=10.1451
	step [202/244], loss=10.9657
	step [203/244], loss=11.3995
	step [204/244], loss=12.5315
	step [205/244], loss=13.4062
	step [206/244], loss=12.5463
	step [207/244], loss=11.1392
	step [208/244], loss=12.7310
	step [209/244], loss=12.4229
	step [210/244], loss=12.6258
	step [211/244], loss=12.8951
	step [212/244], loss=12.1915
	step [213/244], loss=11.7377
	step [214/244], loss=12.7588
	step [215/244], loss=12.7712
	step [216/244], loss=13.9629
	step [217/244], loss=10.7906
	step [218/244], loss=11.6985
	step [219/244], loss=12.6128
	step [220/244], loss=14.0678
	step [221/244], loss=13.6472
	step [222/244], loss=11.3565
	step [223/244], loss=14.2476
	step [224/244], loss=13.0400
	step [225/244], loss=16.0153
	step [226/244], loss=12.7643
	step [227/244], loss=11.8632
	step [228/244], loss=12.5910
	step [229/244], loss=11.6727
	step [230/244], loss=12.5128
	step [231/244], loss=12.2326
	step [232/244], loss=9.8199
	step [233/244], loss=12.0089
	step [234/244], loss=11.3599
	step [235/244], loss=12.4705
	step [236/244], loss=11.9717
	step [237/244], loss=14.8214
	step [238/244], loss=12.2298
	step [239/244], loss=11.6054
	step [240/244], loss=11.2110
	step [241/244], loss=12.4061
	step [242/244], loss=11.6424
	step [243/244], loss=13.5130
	step [244/244], loss=5.5675
	Evaluating
	loss=0.0346, precision=0.1637, recall=0.9934, f1=0.2811
Training epoch 12
	step [1/244], loss=12.2513
	step [2/244], loss=12.5398
	step [3/244], loss=17.3508
	step [4/244], loss=12.9497
	step [5/244], loss=11.3481
	step [6/244], loss=9.8455
	step [7/244], loss=12.9226
	step [8/244], loss=11.9256
	step [9/244], loss=10.8971
	step [10/244], loss=13.2175
	step [11/244], loss=12.2034
	step [12/244], loss=11.5826
	step [13/244], loss=10.6712
	step [14/244], loss=13.5049
	step [15/244], loss=10.0513
	step [16/244], loss=14.6608
	step [17/244], loss=13.6792
	step [18/244], loss=12.2094
	step [19/244], loss=14.3583
	step [20/244], loss=10.2797
	step [21/244], loss=11.5911
	step [22/244], loss=12.7771
	step [23/244], loss=13.9351
	step [24/244], loss=12.0231
	step [25/244], loss=11.8598
	step [26/244], loss=11.8157
	step [27/244], loss=11.6900
	step [28/244], loss=12.2733
	step [29/244], loss=13.2553
	step [30/244], loss=12.6753
	step [31/244], loss=11.3174
	step [32/244], loss=11.1860
	step [33/244], loss=13.1138
	step [34/244], loss=9.9221
	step [35/244], loss=11.8096
	step [36/244], loss=11.0798
	step [37/244], loss=14.1268
	step [38/244], loss=12.1836
	step [39/244], loss=12.7992
	step [40/244], loss=11.7469
	step [41/244], loss=11.5586
	step [42/244], loss=14.6935
	step [43/244], loss=16.0291
	step [44/244], loss=10.1996
	step [45/244], loss=11.7775
	step [46/244], loss=14.0380
	step [47/244], loss=11.8915
	step [48/244], loss=12.9423
	step [49/244], loss=10.7096
	step [50/244], loss=13.2870
	step [51/244], loss=11.1043
	step [52/244], loss=10.8382
	step [53/244], loss=11.6632
	step [54/244], loss=11.4548
	step [55/244], loss=13.7069
	step [56/244], loss=10.7791
	step [57/244], loss=10.7407
	step [58/244], loss=12.0195
	step [59/244], loss=12.9638
	step [60/244], loss=11.7449
	step [61/244], loss=12.6931
	step [62/244], loss=12.9408
	step [63/244], loss=13.3129
	step [64/244], loss=10.9614
	step [65/244], loss=11.9478
	step [66/244], loss=10.4205
	step [67/244], loss=12.3514
	step [68/244], loss=14.5789
	step [69/244], loss=11.3992
	step [70/244], loss=10.9074
	step [71/244], loss=10.9703
	step [72/244], loss=12.5106
	step [73/244], loss=14.6926
	step [74/244], loss=12.2739
	step [75/244], loss=12.5722
	step [76/244], loss=12.4278
	step [77/244], loss=11.2165
	step [78/244], loss=12.6846
	step [79/244], loss=10.6334
	step [80/244], loss=12.5237
	step [81/244], loss=13.7266
	step [82/244], loss=12.1473
	step [83/244], loss=11.3753
	step [84/244], loss=14.5324
	step [85/244], loss=12.7857
	step [86/244], loss=14.0433
	step [87/244], loss=13.5373
	step [88/244], loss=11.7982
	step [89/244], loss=11.9247
	step [90/244], loss=11.0279
	step [91/244], loss=12.6319
	step [92/244], loss=9.9472
	step [93/244], loss=9.9032
	step [94/244], loss=14.3655
	step [95/244], loss=10.6733
	step [96/244], loss=11.3407
	step [97/244], loss=11.9742
	step [98/244], loss=11.7117
	step [99/244], loss=12.5261
	step [100/244], loss=12.9885
	step [101/244], loss=12.1284
	step [102/244], loss=10.5990
	step [103/244], loss=12.5199
	step [104/244], loss=13.9760
	step [105/244], loss=12.2037
	step [106/244], loss=10.0260
	step [107/244], loss=12.6413
	step [108/244], loss=11.5852
	step [109/244], loss=13.5331
	step [110/244], loss=12.3304
	step [111/244], loss=12.4795
	step [112/244], loss=11.9944
	step [113/244], loss=17.8630
	step [114/244], loss=10.0712
	step [115/244], loss=12.9875
	step [116/244], loss=12.0053
	step [117/244], loss=12.8286
	step [118/244], loss=11.0625
	step [119/244], loss=10.5403
	step [120/244], loss=10.3665
	step [121/244], loss=12.7145
	step [122/244], loss=11.0472
	step [123/244], loss=11.6800
	step [124/244], loss=11.8626
	step [125/244], loss=11.0667
	step [126/244], loss=12.3839
	step [127/244], loss=14.0635
	step [128/244], loss=12.3612
	step [129/244], loss=11.9875
	step [130/244], loss=10.6669
	step [131/244], loss=11.7634
	step [132/244], loss=11.2334
	step [133/244], loss=12.6466
	step [134/244], loss=11.4357
	step [135/244], loss=11.6441
	step [136/244], loss=11.3801
	step [137/244], loss=10.6060
	step [138/244], loss=13.5678
	step [139/244], loss=12.2606
	step [140/244], loss=12.0222
	step [141/244], loss=10.3610
	step [142/244], loss=11.5698
	step [143/244], loss=10.6221
	step [144/244], loss=12.3062
	step [145/244], loss=11.1264
	step [146/244], loss=10.0485
	step [147/244], loss=9.5279
	step [148/244], loss=10.9109
	step [149/244], loss=12.6336
	step [150/244], loss=12.0226
	step [151/244], loss=13.0943
	step [152/244], loss=14.6361
	step [153/244], loss=9.8469
	step [154/244], loss=11.9311
	step [155/244], loss=14.1159
	step [156/244], loss=10.1799
	step [157/244], loss=13.9930
	step [158/244], loss=11.4421
	step [159/244], loss=12.4684
	step [160/244], loss=11.5786
	step [161/244], loss=12.4925
	step [162/244], loss=9.9956
	step [163/244], loss=10.8317
	step [164/244], loss=13.2798
	step [165/244], loss=11.8628
	step [166/244], loss=11.8393
	step [167/244], loss=10.1745
	step [168/244], loss=11.1074
	step [169/244], loss=10.6946
	step [170/244], loss=10.4302
	step [171/244], loss=13.5591
	step [172/244], loss=10.8772
	step [173/244], loss=11.3432
	step [174/244], loss=12.3542
	step [175/244], loss=11.3876
	step [176/244], loss=11.3421
	step [177/244], loss=10.8557
	step [178/244], loss=11.8801
	step [179/244], loss=12.2753
	step [180/244], loss=11.3538
	step [181/244], loss=11.7701
	step [182/244], loss=10.5330
	step [183/244], loss=11.2686
	step [184/244], loss=15.1803
	step [185/244], loss=12.6849
	step [186/244], loss=11.2764
	step [187/244], loss=13.4536
	step [188/244], loss=12.8694
	step [189/244], loss=10.9691
	step [190/244], loss=11.9361
	step [191/244], loss=11.2080
	step [192/244], loss=14.1793
	step [193/244], loss=10.5057
	step [194/244], loss=12.7632
	step [195/244], loss=11.9393
	step [196/244], loss=11.4319
	step [197/244], loss=10.0755
	step [198/244], loss=11.3351
	step [199/244], loss=14.2777
	step [200/244], loss=11.7318
	step [201/244], loss=13.6315
	step [202/244], loss=13.2976
	step [203/244], loss=10.8963
	step [204/244], loss=9.9924
	step [205/244], loss=13.7306
	step [206/244], loss=12.4460
	step [207/244], loss=10.8124
	step [208/244], loss=12.5090
	step [209/244], loss=10.5793
	step [210/244], loss=12.3034
	step [211/244], loss=9.9236
	step [212/244], loss=10.1012
	step [213/244], loss=11.8883
	step [214/244], loss=14.1140
	step [215/244], loss=10.4861
	step [216/244], loss=10.5299
	step [217/244], loss=11.5364
	step [218/244], loss=9.0924
	step [219/244], loss=11.5052
	step [220/244], loss=8.8601
	step [221/244], loss=12.8280
	step [222/244], loss=11.3850
	step [223/244], loss=11.4188
	step [224/244], loss=11.0322
	step [225/244], loss=13.2038
	step [226/244], loss=12.4435
	step [227/244], loss=13.2789
	step [228/244], loss=11.2794
	step [229/244], loss=10.0561
	step [230/244], loss=11.9659
	step [231/244], loss=10.8094
	step [232/244], loss=11.0475
	step [233/244], loss=11.1125
	step [234/244], loss=15.5322
	step [235/244], loss=12.4589
	step [236/244], loss=13.9906
	step [237/244], loss=11.1188
	step [238/244], loss=10.9719
	step [239/244], loss=9.3444
	step [240/244], loss=12.2186
	step [241/244], loss=10.9294
	step [242/244], loss=11.0544
	step [243/244], loss=11.2018
	step [244/244], loss=6.5827
	Evaluating
	loss=0.0315, precision=0.1758, recall=0.9930, f1=0.2987
Training epoch 13
	step [1/244], loss=12.9882
	step [2/244], loss=10.5617
	step [3/244], loss=11.9056
	step [4/244], loss=13.9726
	step [5/244], loss=11.7377
	step [6/244], loss=9.6447
	step [7/244], loss=13.4966
	step [8/244], loss=11.6406
	step [9/244], loss=11.0303
	step [10/244], loss=9.9636
	step [11/244], loss=12.4952
	step [12/244], loss=11.9490
	step [13/244], loss=11.8238
	step [14/244], loss=11.7860
	step [15/244], loss=11.5232
	step [16/244], loss=12.1827
	step [17/244], loss=10.5839
	step [18/244], loss=10.5187
	step [19/244], loss=10.0805
	step [20/244], loss=9.7554
	step [21/244], loss=12.0713
	step [22/244], loss=14.3017
	step [23/244], loss=10.1540
	step [24/244], loss=10.2607
	step [25/244], loss=9.1639
	step [26/244], loss=11.1043
	step [27/244], loss=9.1393
	step [28/244], loss=10.3666
	step [29/244], loss=12.2331
	step [30/244], loss=12.7067
	step [31/244], loss=10.3114
	step [32/244], loss=9.8112
	step [33/244], loss=11.4139
	step [34/244], loss=11.4986
	step [35/244], loss=10.9649
	step [36/244], loss=11.4142
	step [37/244], loss=11.3129
	step [38/244], loss=11.3410
	step [39/244], loss=11.5329
	step [40/244], loss=10.8265
	step [41/244], loss=10.4177
	step [42/244], loss=11.0545
	step [43/244], loss=11.6970
	step [44/244], loss=12.2053
	step [45/244], loss=11.2843
	step [46/244], loss=9.6794
	step [47/244], loss=10.3558
	step [48/244], loss=9.6775
	step [49/244], loss=10.0048
	step [50/244], loss=10.7058
	step [51/244], loss=10.2678
	step [52/244], loss=13.5557
	step [53/244], loss=11.7683
	step [54/244], loss=12.1460
	step [55/244], loss=10.5025
	step [56/244], loss=11.5222
	step [57/244], loss=12.3905
	step [58/244], loss=11.5900
	step [59/244], loss=12.3569
	step [60/244], loss=11.2692
	step [61/244], loss=11.4587
	step [62/244], loss=10.8274
	step [63/244], loss=11.9451
	step [64/244], loss=11.0585
	step [65/244], loss=11.9515
	step [66/244], loss=11.0758
	step [67/244], loss=13.6894
	step [68/244], loss=13.6640
	step [69/244], loss=10.0984
	step [70/244], loss=14.5467
	step [71/244], loss=10.6411
	step [72/244], loss=12.0033
	step [73/244], loss=11.7605
	step [74/244], loss=10.9164
	step [75/244], loss=9.9700
	step [76/244], loss=12.1546
	step [77/244], loss=9.5099
	step [78/244], loss=10.1931
	step [79/244], loss=10.6950
	step [80/244], loss=10.7228
	step [81/244], loss=10.7398
	step [82/244], loss=14.3565
	step [83/244], loss=12.3943
	step [84/244], loss=12.7432
	step [85/244], loss=11.7570
	step [86/244], loss=11.6508
	step [87/244], loss=11.7885
	step [88/244], loss=10.9308
	step [89/244], loss=11.8605
	step [90/244], loss=12.0102
	step [91/244], loss=12.3955
	step [92/244], loss=12.3742
	step [93/244], loss=11.7683
	step [94/244], loss=11.4825
	step [95/244], loss=12.7495
	step [96/244], loss=12.0409
	step [97/244], loss=10.7829
	step [98/244], loss=11.0415
	step [99/244], loss=11.1311
	step [100/244], loss=11.9797
	step [101/244], loss=11.7643
	step [102/244], loss=9.0746
	step [103/244], loss=9.8337
	step [104/244], loss=11.9567
	step [105/244], loss=10.9128
	step [106/244], loss=10.4228
	step [107/244], loss=10.8018
	step [108/244], loss=11.3467
	step [109/244], loss=12.6227
	step [110/244], loss=10.5826
	step [111/244], loss=14.1063
	step [112/244], loss=10.1218
	step [113/244], loss=10.4384
	step [114/244], loss=8.6206
	step [115/244], loss=13.0976
	step [116/244], loss=13.0075
	step [117/244], loss=12.5177
	step [118/244], loss=13.4534
	step [119/244], loss=12.1545
	step [120/244], loss=11.4332
	step [121/244], loss=9.5100
	step [122/244], loss=10.3792
	step [123/244], loss=11.0266
	step [124/244], loss=11.2732
	step [125/244], loss=10.6042
	step [126/244], loss=9.7628
	step [127/244], loss=9.7927
	step [128/244], loss=9.4741
	step [129/244], loss=9.6621
	step [130/244], loss=10.1534
	step [131/244], loss=11.4994
	step [132/244], loss=9.8902
	step [133/244], loss=13.0308
	step [134/244], loss=11.3315
	step [135/244], loss=12.5302
	step [136/244], loss=10.9449
	step [137/244], loss=10.6249
	step [138/244], loss=10.8201
	step [139/244], loss=10.1492
	step [140/244], loss=9.9194
	step [141/244], loss=11.9189
	step [142/244], loss=11.9449
	step [143/244], loss=11.8779
	step [144/244], loss=11.1592
	step [145/244], loss=11.1437
	step [146/244], loss=12.4461
	step [147/244], loss=13.3939
	step [148/244], loss=11.7856
	step [149/244], loss=10.0239
	step [150/244], loss=10.2262
	step [151/244], loss=9.9366
	step [152/244], loss=10.2722
	step [153/244], loss=9.6542
	step [154/244], loss=8.6602
	step [155/244], loss=9.9891
	step [156/244], loss=10.8980
	step [157/244], loss=10.2632
	step [158/244], loss=13.1889
	step [159/244], loss=12.4764
	step [160/244], loss=12.0672
	step [161/244], loss=11.2640
	step [162/244], loss=12.5093
	step [163/244], loss=12.7149
	step [164/244], loss=11.8367
	step [165/244], loss=9.8831
	step [166/244], loss=13.5581
	step [167/244], loss=11.7931
	step [168/244], loss=9.3912
	step [169/244], loss=10.4617
	step [170/244], loss=9.8537
	step [171/244], loss=9.7839
	step [172/244], loss=10.8860
	step [173/244], loss=11.2236
	step [174/244], loss=11.4716
	step [175/244], loss=12.4051
	step [176/244], loss=10.3030
	step [177/244], loss=13.6772
	step [178/244], loss=11.8380
	step [179/244], loss=11.4335
	step [180/244], loss=9.3516
	step [181/244], loss=10.5774
	step [182/244], loss=10.9236
	step [183/244], loss=11.3234
	step [184/244], loss=11.6670
	step [185/244], loss=14.3583
	step [186/244], loss=11.5545
	step [187/244], loss=10.8376
	step [188/244], loss=9.6727
	step [189/244], loss=11.4842
	step [190/244], loss=13.7081
	step [191/244], loss=11.5903
	step [192/244], loss=13.0021
	step [193/244], loss=9.9749
	step [194/244], loss=11.3189
	step [195/244], loss=8.7817
	step [196/244], loss=11.1790
	step [197/244], loss=10.6790
	step [198/244], loss=10.0657
	step [199/244], loss=11.9662
	step [200/244], loss=10.1094
	step [201/244], loss=11.1839
	step [202/244], loss=14.3076
	step [203/244], loss=10.1894
	step [204/244], loss=8.2902
	step [205/244], loss=10.4492
	step [206/244], loss=9.3766
	step [207/244], loss=11.0991
	step [208/244], loss=11.2381
	step [209/244], loss=10.2227
	step [210/244], loss=10.2085
	step [211/244], loss=9.9761
	step [212/244], loss=10.3702
	step [213/244], loss=9.5164
	step [214/244], loss=11.4408
	step [215/244], loss=10.2333
	step [216/244], loss=11.2008
	step [217/244], loss=10.2641
	step [218/244], loss=11.3616
	step [219/244], loss=10.7806
	step [220/244], loss=10.6087
	step [221/244], loss=9.4706
	step [222/244], loss=10.2575
	step [223/244], loss=9.0572
	step [224/244], loss=11.4062
	step [225/244], loss=10.8572
	step [226/244], loss=14.8035
	step [227/244], loss=12.6903
	step [228/244], loss=10.8932
	step [229/244], loss=10.9036
	step [230/244], loss=10.3163
	step [231/244], loss=13.2087
	step [232/244], loss=9.8755
	step [233/244], loss=12.2706
	step [234/244], loss=10.6622
	step [235/244], loss=10.7831
	step [236/244], loss=9.9753
	step [237/244], loss=10.2591
	step [238/244], loss=13.3640
	step [239/244], loss=12.0488
	step [240/244], loss=12.6388
	step [241/244], loss=11.6963
	step [242/244], loss=10.1892
	step [243/244], loss=10.1832
	step [244/244], loss=5.3012
	Evaluating
	loss=0.0280, precision=0.1920, recall=0.9910, f1=0.3217
Training epoch 14
	step [1/244], loss=12.8443
	step [2/244], loss=9.4339
	step [3/244], loss=10.8928
	step [4/244], loss=10.0689
	step [5/244], loss=7.8221
	step [6/244], loss=10.7688
	step [7/244], loss=15.7484
	step [8/244], loss=10.7633
	step [9/244], loss=12.5307
	step [10/244], loss=12.1282
	step [11/244], loss=9.7451
	step [12/244], loss=11.4817
	step [13/244], loss=11.1504
	step [14/244], loss=10.1243
	step [15/244], loss=9.0839
	step [16/244], loss=9.5615
	step [17/244], loss=11.4987
	step [18/244], loss=11.0102
	step [19/244], loss=13.6586
	step [20/244], loss=8.7954
	step [21/244], loss=10.7502
	step [22/244], loss=12.5078
	step [23/244], loss=11.8821
	step [24/244], loss=9.8137
	step [25/244], loss=13.4755
	step [26/244], loss=10.4286
	step [27/244], loss=11.7349
	step [28/244], loss=11.1782
	step [29/244], loss=10.3197
	step [30/244], loss=9.6370
	step [31/244], loss=10.9392
	step [32/244], loss=9.8844
	step [33/244], loss=10.9503
	step [34/244], loss=11.1911
	step [35/244], loss=10.1586
	step [36/244], loss=9.5708
	step [37/244], loss=11.8711
	step [38/244], loss=13.0426
	step [39/244], loss=11.4475
	step [40/244], loss=11.6905
	step [41/244], loss=10.1103
	step [42/244], loss=10.9720
	step [43/244], loss=9.8749
	step [44/244], loss=10.7105
	step [45/244], loss=10.9580
	step [46/244], loss=13.0867
	step [47/244], loss=9.3127
	step [48/244], loss=13.9049
	step [49/244], loss=9.3213
	step [50/244], loss=10.1228
	step [51/244], loss=10.9337
	step [52/244], loss=10.8489
	step [53/244], loss=10.5200
	step [54/244], loss=10.1814
	step [55/244], loss=10.2772
	step [56/244], loss=9.6906
	step [57/244], loss=10.9588
	step [58/244], loss=8.8107
	step [59/244], loss=11.8804
	step [60/244], loss=10.6666
	step [61/244], loss=11.9429
	step [62/244], loss=9.9536
	step [63/244], loss=10.3741
	step [64/244], loss=9.8071
	step [65/244], loss=10.9031
	step [66/244], loss=10.2857
	step [67/244], loss=10.8397
	step [68/244], loss=11.8545
	step [69/244], loss=9.5635
	step [70/244], loss=12.1670
	step [71/244], loss=9.6750
	step [72/244], loss=12.7374
	step [73/244], loss=9.9033
	step [74/244], loss=10.9013
	step [75/244], loss=8.8944
	step [76/244], loss=12.1382
	step [77/244], loss=11.2582
	step [78/244], loss=11.3904
	step [79/244], loss=10.9336
	step [80/244], loss=12.2966
	step [81/244], loss=9.3690
	step [82/244], loss=12.1381
	step [83/244], loss=12.6662
	step [84/244], loss=10.3923
	step [85/244], loss=10.1633
	step [86/244], loss=9.5652
	step [87/244], loss=9.1642
	step [88/244], loss=10.0186
	step [89/244], loss=11.9280
	step [90/244], loss=9.8288
	step [91/244], loss=10.9421
	step [92/244], loss=10.8254
	step [93/244], loss=10.9206
	step [94/244], loss=10.5341
	step [95/244], loss=10.7153
	step [96/244], loss=9.4306
	step [97/244], loss=10.2761
	step [98/244], loss=11.9230
	step [99/244], loss=12.8154
	step [100/244], loss=11.6762
	step [101/244], loss=11.3181
	step [102/244], loss=9.6715
	step [103/244], loss=13.0385
	step [104/244], loss=9.2647
	step [105/244], loss=12.2307
	step [106/244], loss=9.9369
	step [107/244], loss=9.7647
	step [108/244], loss=9.9952
	step [109/244], loss=10.7225
	step [110/244], loss=12.3789
	step [111/244], loss=9.3531
	step [112/244], loss=10.9322
	step [113/244], loss=9.0701
	step [114/244], loss=10.4555
	step [115/244], loss=9.7044
	step [116/244], loss=11.3259
	step [117/244], loss=11.8481
	step [118/244], loss=11.3538
	step [119/244], loss=10.9910
	step [120/244], loss=11.0185
	step [121/244], loss=11.2170
	step [122/244], loss=10.9082
	step [123/244], loss=9.3784
	step [124/244], loss=12.6114
	step [125/244], loss=11.1837
	step [126/244], loss=9.2339
	step [127/244], loss=9.5644
	step [128/244], loss=12.4570
	step [129/244], loss=9.4032
	step [130/244], loss=9.6104
	step [131/244], loss=10.1973
	step [132/244], loss=9.4059
	step [133/244], loss=9.9956
	step [134/244], loss=10.3265
	step [135/244], loss=11.6331
	step [136/244], loss=12.5232
	step [137/244], loss=10.1869
	step [138/244], loss=9.4670
	step [139/244], loss=9.4245
	step [140/244], loss=12.9033
	step [141/244], loss=11.4383
	step [142/244], loss=11.2998
	step [143/244], loss=10.4309
	step [144/244], loss=10.3945
	step [145/244], loss=9.8470
	step [146/244], loss=9.1499
	step [147/244], loss=10.0885
	step [148/244], loss=10.7682
	step [149/244], loss=12.3471
	step [150/244], loss=11.0368
	step [151/244], loss=10.1813
	step [152/244], loss=9.2625
	step [153/244], loss=11.5270
	step [154/244], loss=10.5466
	step [155/244], loss=12.4957
	step [156/244], loss=10.4441
	step [157/244], loss=10.9797
	step [158/244], loss=10.4740
	step [159/244], loss=9.4058
	step [160/244], loss=8.9391
	step [161/244], loss=10.5318
	step [162/244], loss=7.6024
	step [163/244], loss=12.7480
	step [164/244], loss=10.0901
	step [165/244], loss=9.0073
	step [166/244], loss=9.6355
	step [167/244], loss=9.4347
	step [168/244], loss=8.6421
	step [169/244], loss=12.6002
	step [170/244], loss=9.3466
	step [171/244], loss=13.0843
	step [172/244], loss=11.8842
	step [173/244], loss=10.1175
	step [174/244], loss=11.1140
	step [175/244], loss=9.4875
	step [176/244], loss=10.6296
	step [177/244], loss=10.6862
	step [178/244], loss=9.8396
	step [179/244], loss=9.4055
	step [180/244], loss=9.9090
	step [181/244], loss=10.6208
	step [182/244], loss=9.7738
	step [183/244], loss=10.2961
	step [184/244], loss=8.9065
	step [185/244], loss=10.8047
	step [186/244], loss=10.4897
	step [187/244], loss=9.0708
	step [188/244], loss=9.6975
	step [189/244], loss=10.1747
	step [190/244], loss=9.8559
	step [191/244], loss=9.7274
	step [192/244], loss=9.2827
	step [193/244], loss=10.4341
	step [194/244], loss=9.5543
	step [195/244], loss=8.8897
	step [196/244], loss=10.4741
	step [197/244], loss=10.5434
	step [198/244], loss=9.3044
	step [199/244], loss=8.4386
	step [200/244], loss=14.3909
	step [201/244], loss=11.9241
	step [202/244], loss=9.5675
	step [203/244], loss=9.4216
	step [204/244], loss=9.9593
	step [205/244], loss=10.3817
	step [206/244], loss=10.4039
	step [207/244], loss=12.5189
	step [208/244], loss=11.3031
	step [209/244], loss=12.1766
	step [210/244], loss=10.2118
	step [211/244], loss=10.5843
	step [212/244], loss=9.7164
	step [213/244], loss=11.0788
	step [214/244], loss=11.1787
	step [215/244], loss=10.2926
	step [216/244], loss=10.6367
	step [217/244], loss=9.6638
	step [218/244], loss=10.5777
	step [219/244], loss=12.1800
	step [220/244], loss=12.4313
	step [221/244], loss=10.1882
	step [222/244], loss=10.9849
	step [223/244], loss=11.1920
	step [224/244], loss=9.9303
	step [225/244], loss=8.7202
	step [226/244], loss=9.0766
	step [227/244], loss=9.4499
	step [228/244], loss=9.6953
	step [229/244], loss=9.4534
	step [230/244], loss=10.8604
	step [231/244], loss=12.4721
	step [232/244], loss=11.6775
	step [233/244], loss=8.6912
	step [234/244], loss=12.6959
	step [235/244], loss=12.1642
	step [236/244], loss=11.1753
	step [237/244], loss=8.9691
	step [238/244], loss=11.4092
	step [239/244], loss=11.6424
	step [240/244], loss=13.3725
	step [241/244], loss=9.9512
	step [242/244], loss=11.5762
	step [243/244], loss=9.9811
	step [244/244], loss=4.9065
	Evaluating
	loss=0.0277, precision=0.1767, recall=0.9919, f1=0.2999
Training epoch 15
	step [1/244], loss=10.6114
	step [2/244], loss=9.9881
	step [3/244], loss=9.4622
	step [4/244], loss=11.0785
	step [5/244], loss=13.0990
	step [6/244], loss=11.6206
	step [7/244], loss=10.0288
	step [8/244], loss=10.4742
	step [9/244], loss=8.2398
	step [10/244], loss=11.1647
	step [11/244], loss=12.0399
	step [12/244], loss=10.8981
	step [13/244], loss=8.8634
	step [14/244], loss=12.3989
	step [15/244], loss=8.3288
	step [16/244], loss=9.4477
	step [17/244], loss=9.9557
	step [18/244], loss=10.6021
	step [19/244], loss=8.8516
	step [20/244], loss=10.7114
	step [21/244], loss=9.7626
	step [22/244], loss=9.6686
	step [23/244], loss=9.2994
	step [24/244], loss=9.3093
	step [25/244], loss=9.2441
	step [26/244], loss=8.7311
	step [27/244], loss=8.8919
	step [28/244], loss=11.7564
	step [29/244], loss=11.9833
	step [30/244], loss=9.6660
	step [31/244], loss=10.7054
	step [32/244], loss=8.4361
	step [33/244], loss=10.8071
	step [34/244], loss=10.1737
	step [35/244], loss=12.2736
	step [36/244], loss=10.5512
	step [37/244], loss=8.7492
	step [38/244], loss=9.3448
	step [39/244], loss=9.9754
	step [40/244], loss=10.3492
	step [41/244], loss=7.5678
	step [42/244], loss=9.4413
	step [43/244], loss=11.8379
	step [44/244], loss=9.5672
	step [45/244], loss=9.4119
	step [46/244], loss=9.3160
	step [47/244], loss=10.8718
	step [48/244], loss=10.4303
	step [49/244], loss=12.4098
	step [50/244], loss=10.8425
	step [51/244], loss=10.7844
	step [52/244], loss=10.8470
	step [53/244], loss=11.9040
	step [54/244], loss=12.3564
	step [55/244], loss=12.4427
	step [56/244], loss=9.3487
	step [57/244], loss=10.2901
	step [58/244], loss=10.5413
	step [59/244], loss=11.1249
	step [60/244], loss=8.8045
	step [61/244], loss=11.4429
	step [62/244], loss=9.5263
	step [63/244], loss=10.2302
	step [64/244], loss=8.4699
	step [65/244], loss=9.8720
	step [66/244], loss=10.4243
	step [67/244], loss=9.7250
	step [68/244], loss=11.6543
	step [69/244], loss=8.6589
	step [70/244], loss=9.5258
	step [71/244], loss=9.6073
	step [72/244], loss=9.7771
	step [73/244], loss=9.2794
	step [74/244], loss=10.9472
	step [75/244], loss=10.7515
	step [76/244], loss=8.5903
	step [77/244], loss=12.5449
	step [78/244], loss=10.2107
	step [79/244], loss=8.5987
	step [80/244], loss=8.7520
	step [81/244], loss=9.6524
	step [82/244], loss=12.4167
	step [83/244], loss=9.6431
	step [84/244], loss=9.6042
	step [85/244], loss=10.2463
	step [86/244], loss=8.6238
	step [87/244], loss=9.9604
	step [88/244], loss=10.8753
	step [89/244], loss=11.3548
	step [90/244], loss=10.5860
	step [91/244], loss=14.4821
	step [92/244], loss=10.5968
	step [93/244], loss=8.3061
	step [94/244], loss=8.7443
	step [95/244], loss=9.8374
	step [96/244], loss=10.1556
	step [97/244], loss=10.7797
	step [98/244], loss=11.1656
	step [99/244], loss=11.7453
	step [100/244], loss=11.2118
	step [101/244], loss=10.6558
	step [102/244], loss=11.0653
	step [103/244], loss=10.0347
	step [104/244], loss=8.9578
	step [105/244], loss=10.7433
	step [106/244], loss=10.6208
	step [107/244], loss=10.2065
	step [108/244], loss=8.9472
	step [109/244], loss=9.2635
	step [110/244], loss=9.5155
	step [111/244], loss=9.4432
	step [112/244], loss=8.5375
	step [113/244], loss=9.4185
	step [114/244], loss=8.4738
	step [115/244], loss=8.6937
	step [116/244], loss=12.3121
	step [117/244], loss=11.7824
	step [118/244], loss=10.2599
	step [119/244], loss=9.7418
	step [120/244], loss=10.1609
	step [121/244], loss=8.4599
	step [122/244], loss=11.9432
	step [123/244], loss=10.1110
	step [124/244], loss=10.5582
	step [125/244], loss=13.2064
	step [126/244], loss=7.8099
	step [127/244], loss=8.3966
	step [128/244], loss=10.1574
	step [129/244], loss=10.6860
	step [130/244], loss=9.1569
	step [131/244], loss=8.8426
	step [132/244], loss=8.7287
	step [133/244], loss=11.3214
	step [134/244], loss=11.4897
	step [135/244], loss=9.7802
	step [136/244], loss=10.2092
	step [137/244], loss=12.8757
	step [138/244], loss=11.3916
	step [139/244], loss=10.1203
	step [140/244], loss=9.2289
	step [141/244], loss=9.6762
	step [142/244], loss=10.1795
	step [143/244], loss=11.4851
	step [144/244], loss=8.6568
	step [145/244], loss=9.7267
	step [146/244], loss=9.9727
	step [147/244], loss=10.4293
	step [148/244], loss=11.9810
	step [149/244], loss=11.2265
	step [150/244], loss=10.0445
	step [151/244], loss=11.3251
	step [152/244], loss=8.9145
	step [153/244], loss=9.8319
	step [154/244], loss=9.3323
	step [155/244], loss=9.2425
	step [156/244], loss=8.4712
	step [157/244], loss=11.1159
	step [158/244], loss=9.8670
	step [159/244], loss=8.5033
	step [160/244], loss=10.1192
	step [161/244], loss=9.4590
	step [162/244], loss=9.0862
	step [163/244], loss=10.6222
	step [164/244], loss=8.7547
	step [165/244], loss=9.2392
	step [166/244], loss=10.6362
	step [167/244], loss=10.0021
	step [168/244], loss=10.9851
	step [169/244], loss=10.0309
	step [170/244], loss=8.3253
	step [171/244], loss=8.7485
	step [172/244], loss=9.5929
	step [173/244], loss=9.7657
	step [174/244], loss=9.8295
	step [175/244], loss=9.4814
	step [176/244], loss=9.5535
	step [177/244], loss=8.5840
	step [178/244], loss=8.8198
	step [179/244], loss=11.6796
	step [180/244], loss=9.4920
	step [181/244], loss=9.0329
	step [182/244], loss=10.6604
	step [183/244], loss=9.7032
	step [184/244], loss=10.6440
	step [185/244], loss=10.7284
	step [186/244], loss=12.0412
	step [187/244], loss=8.6373
	step [188/244], loss=9.1312
	step [189/244], loss=9.1859
	step [190/244], loss=11.2908
	step [191/244], loss=11.0914
	step [192/244], loss=9.9035
	step [193/244], loss=8.0152
	step [194/244], loss=8.8266
	step [195/244], loss=11.9532
	step [196/244], loss=10.1585
	step [197/244], loss=9.1183
	step [198/244], loss=9.2176
	step [199/244], loss=13.4163
	step [200/244], loss=9.3498
	step [201/244], loss=9.1364
	step [202/244], loss=9.4778
	step [203/244], loss=10.0926
	step [204/244], loss=10.6345
	step [205/244], loss=11.3721
	step [206/244], loss=11.6255
	step [207/244], loss=10.2336
	step [208/244], loss=9.8735
	step [209/244], loss=9.5756
	step [210/244], loss=9.3351
	step [211/244], loss=11.5901
	step [212/244], loss=7.5393
	step [213/244], loss=7.9779
	step [214/244], loss=10.5925
	step [215/244], loss=9.6889
	step [216/244], loss=11.0615
	step [217/244], loss=13.1036
	step [218/244], loss=9.4167
	step [219/244], loss=9.6621
	step [220/244], loss=11.3465
	step [221/244], loss=11.0056
	step [222/244], loss=9.3570
	step [223/244], loss=8.3313
	step [224/244], loss=12.9386
	step [225/244], loss=9.0395
	step [226/244], loss=8.3408
	step [227/244], loss=10.4200
	step [228/244], loss=9.0080
	step [229/244], loss=8.7348
	step [230/244], loss=12.7712
	step [231/244], loss=9.5005
	step [232/244], loss=11.8499
	step [233/244], loss=10.4976
	step [234/244], loss=11.0794
	step [235/244], loss=9.4979
	step [236/244], loss=9.2420
	step [237/244], loss=11.0735
	step [238/244], loss=7.9922
	step [239/244], loss=10.6776
	step [240/244], loss=8.0002
	step [241/244], loss=9.0565
	step [242/244], loss=9.2115
	step [243/244], loss=9.3726
	step [244/244], loss=3.5076
	Evaluating
	loss=0.0243, precision=0.1988, recall=0.9919, f1=0.3312
Training epoch 16
	step [1/244], loss=10.9695
	step [2/244], loss=8.0491
	step [3/244], loss=8.8326
	step [4/244], loss=9.2299
	step [5/244], loss=9.1841
	step [6/244], loss=9.6628
	step [7/244], loss=10.0274
	step [8/244], loss=11.3987
	step [9/244], loss=9.7647
	step [10/244], loss=11.0183
	step [11/244], loss=11.4792
	step [12/244], loss=11.9504
	step [13/244], loss=9.1976
	step [14/244], loss=8.3068
	step [15/244], loss=9.9198
	step [16/244], loss=8.9162
	step [17/244], loss=8.2052
	step [18/244], loss=7.2932
	step [19/244], loss=11.6623
	step [20/244], loss=9.1742
	step [21/244], loss=7.8263
	step [22/244], loss=9.5144
	step [23/244], loss=11.2119
	step [24/244], loss=10.3137
	step [25/244], loss=9.7309
	step [26/244], loss=11.5850
	step [27/244], loss=10.3935
	step [28/244], loss=10.5578
	step [29/244], loss=10.1189
	step [30/244], loss=10.7698
	step [31/244], loss=8.9039
	step [32/244], loss=7.7531
	step [33/244], loss=9.5300
	step [34/244], loss=8.7884
	step [35/244], loss=9.6527
	step [36/244], loss=9.6919
	step [37/244], loss=10.5683
	step [38/244], loss=11.3965
	step [39/244], loss=10.6593
	step [40/244], loss=9.6309
	step [41/244], loss=9.0029
	step [42/244], loss=9.5300
	step [43/244], loss=9.5218
	step [44/244], loss=12.5105
	step [45/244], loss=11.7040
	step [46/244], loss=10.4984
	step [47/244], loss=9.3497
	step [48/244], loss=8.3380
	step [49/244], loss=10.0753
	step [50/244], loss=9.1399
	step [51/244], loss=9.4775
	step [52/244], loss=8.5768
	step [53/244], loss=8.7992
	step [54/244], loss=9.9896
	step [55/244], loss=9.6572
	step [56/244], loss=10.1486
	step [57/244], loss=10.1045
	step [58/244], loss=7.6769
	step [59/244], loss=9.8287
	step [60/244], loss=9.7722
	step [61/244], loss=9.9604
	step [62/244], loss=9.3740
	step [63/244], loss=9.3617
	step [64/244], loss=9.9542
	step [65/244], loss=9.5490
	step [66/244], loss=8.2100
	step [67/244], loss=11.8489
	step [68/244], loss=8.8573
	step [69/244], loss=10.0920
	step [70/244], loss=9.5680
	step [71/244], loss=7.8910
	step [72/244], loss=13.6393
	step [73/244], loss=10.5195
	step [74/244], loss=10.8792
	step [75/244], loss=8.7200
	step [76/244], loss=11.3129
	step [77/244], loss=11.5796
	step [78/244], loss=10.4896
	step [79/244], loss=8.9947
	step [80/244], loss=10.4701
	step [81/244], loss=11.1582
	step [82/244], loss=10.7293
	step [83/244], loss=8.6418
	step [84/244], loss=8.8155
	step [85/244], loss=10.0893
	step [86/244], loss=9.9036
	step [87/244], loss=9.6922
	step [88/244], loss=7.5565
	step [89/244], loss=9.5330
	step [90/244], loss=12.5049
	step [91/244], loss=8.7751
	step [92/244], loss=9.4560
	step [93/244], loss=9.3105
	step [94/244], loss=8.5693
	step [95/244], loss=9.8490
	step [96/244], loss=9.9732
	step [97/244], loss=11.9387
	step [98/244], loss=10.4076
	step [99/244], loss=9.2996
	step [100/244], loss=8.6914
	step [101/244], loss=11.4603
	step [102/244], loss=10.8895
	step [103/244], loss=8.3852
	step [104/244], loss=9.4996
	step [105/244], loss=8.6404
	step [106/244], loss=7.6566
	step [107/244], loss=12.3530
	step [108/244], loss=9.3415
	step [109/244], loss=9.9872
	step [110/244], loss=13.1865
	step [111/244], loss=11.5691
	step [112/244], loss=8.5331
	step [113/244], loss=10.0329
	step [114/244], loss=10.0294
	step [115/244], loss=10.4956
	step [116/244], loss=9.0749
	step [117/244], loss=9.6054
	step [118/244], loss=8.6904
	step [119/244], loss=7.6857
	step [120/244], loss=9.8283
	step [121/244], loss=9.9206
	step [122/244], loss=9.5612
	step [123/244], loss=9.8476
	step [124/244], loss=7.8983
	step [125/244], loss=7.6594
	step [126/244], loss=10.3331
	step [127/244], loss=11.2087
	step [128/244], loss=8.9948
	step [129/244], loss=9.2825
	step [130/244], loss=8.3246
	step [131/244], loss=9.5518
	step [132/244], loss=9.7597
	step [133/244], loss=9.6935
	step [134/244], loss=8.7775
	step [135/244], loss=9.0267
	step [136/244], loss=9.0482
	step [137/244], loss=8.1329
	step [138/244], loss=8.6753
	step [139/244], loss=10.6526
	step [140/244], loss=9.2915
	step [141/244], loss=7.4822
	step [142/244], loss=7.7115
	step [143/244], loss=8.1996
	step [144/244], loss=9.5116
	step [145/244], loss=10.3385
	step [146/244], loss=8.3377
	step [147/244], loss=9.2717
	step [148/244], loss=8.3263
	step [149/244], loss=12.2074
	step [150/244], loss=9.5837
	step [151/244], loss=9.1477
	step [152/244], loss=9.0258
	step [153/244], loss=10.8336
	step [154/244], loss=9.1553
	step [155/244], loss=9.2385
	step [156/244], loss=9.3843
	step [157/244], loss=12.7752
	step [158/244], loss=10.0176
	step [159/244], loss=9.9485
	step [160/244], loss=9.5437
	step [161/244], loss=10.5831
	step [162/244], loss=10.0636
	step [163/244], loss=9.3210
	step [164/244], loss=9.8384
	step [165/244], loss=11.7461
	step [166/244], loss=8.5898
	step [167/244], loss=8.4760
	step [168/244], loss=10.1014
	step [169/244], loss=10.6728
	step [170/244], loss=10.9055
	step [171/244], loss=8.5361
	step [172/244], loss=9.3143
	step [173/244], loss=8.2574
	step [174/244], loss=8.3341
	step [175/244], loss=10.6557
	step [176/244], loss=9.4628
	step [177/244], loss=8.4269
	step [178/244], loss=7.7699
	step [179/244], loss=10.7318
	step [180/244], loss=9.5267
	step [181/244], loss=7.7188
	step [182/244], loss=9.7430
	step [183/244], loss=9.3033
	step [184/244], loss=9.7284
	step [185/244], loss=9.2197
	step [186/244], loss=8.9649
	step [187/244], loss=12.5553
	step [188/244], loss=10.0519
	step [189/244], loss=11.1359
	step [190/244], loss=9.1156
	step [191/244], loss=10.4771
	step [192/244], loss=9.2392
	step [193/244], loss=11.4057
	step [194/244], loss=8.3732
	step [195/244], loss=8.3530
	step [196/244], loss=11.5795
	step [197/244], loss=10.1532
	step [198/244], loss=8.3724
	step [199/244], loss=9.3001
	step [200/244], loss=9.9985
	step [201/244], loss=7.6521
	step [202/244], loss=10.0539
	step [203/244], loss=7.9171
	step [204/244], loss=8.3884
	step [205/244], loss=8.5508
	step [206/244], loss=9.3532
	step [207/244], loss=8.8154
	step [208/244], loss=8.8072
	step [209/244], loss=9.0406
	step [210/244], loss=7.5142
	step [211/244], loss=8.2554
	step [212/244], loss=10.8317
	step [213/244], loss=10.7455
	step [214/244], loss=8.6324
	step [215/244], loss=10.0062
	step [216/244], loss=8.2601
	step [217/244], loss=10.8763
	step [218/244], loss=8.5557
	step [219/244], loss=8.7430
	step [220/244], loss=8.5973
	step [221/244], loss=9.2492
	step [222/244], loss=8.9359
	step [223/244], loss=10.4877
	step [224/244], loss=8.4276
	step [225/244], loss=8.9237
	step [226/244], loss=8.4182
	step [227/244], loss=8.0430
	step [228/244], loss=7.2783
	step [229/244], loss=9.0623
	step [230/244], loss=11.0633
	step [231/244], loss=7.4928
	step [232/244], loss=9.6853
	step [233/244], loss=10.2614
	step [234/244], loss=8.3771
	step [235/244], loss=9.9936
	step [236/244], loss=8.7057
	step [237/244], loss=10.4753
	step [238/244], loss=10.3372
	step [239/244], loss=10.1494
	step [240/244], loss=9.4802
	step [241/244], loss=11.0737
	step [242/244], loss=12.2096
	step [243/244], loss=8.2306
	step [244/244], loss=3.1716
	Evaluating
	loss=0.0290, precision=0.1453, recall=0.9935, f1=0.2535
Training epoch 17
	step [1/244], loss=9.8507
	step [2/244], loss=10.5752
	step [3/244], loss=9.6436
	step [4/244], loss=10.0734
	step [5/244], loss=9.7929
	step [6/244], loss=9.3458
	step [7/244], loss=8.9581
	step [8/244], loss=10.0190
	step [9/244], loss=9.4300
	step [10/244], loss=8.7664
	step [11/244], loss=8.1578
	step [12/244], loss=10.3875
	step [13/244], loss=11.1396
	step [14/244], loss=6.5982
	step [15/244], loss=9.1896
	step [16/244], loss=8.4591
	step [17/244], loss=9.6387
	step [18/244], loss=9.2524
	step [19/244], loss=8.7204
	step [20/244], loss=10.4608
	step [21/244], loss=6.9980
	step [22/244], loss=7.4024
	step [23/244], loss=9.6109
	step [24/244], loss=10.1061
	step [25/244], loss=9.4175
	step [26/244], loss=10.6025
	step [27/244], loss=11.4982
	step [28/244], loss=9.7765
	step [29/244], loss=9.2843
	step [30/244], loss=9.7651
	step [31/244], loss=9.0597
	step [32/244], loss=9.0791
	step [33/244], loss=9.3013
	step [34/244], loss=11.6454
	step [35/244], loss=11.3164
	step [36/244], loss=9.8624
	step [37/244], loss=10.1883
	step [38/244], loss=11.0273
	step [39/244], loss=9.0633
	step [40/244], loss=8.9633
	step [41/244], loss=10.8407
	step [42/244], loss=9.8549
	step [43/244], loss=10.3202
	step [44/244], loss=9.6393
	step [45/244], loss=8.9816
	step [46/244], loss=8.1350
	step [47/244], loss=9.4628
	step [48/244], loss=7.4834
	step [49/244], loss=9.2412
	step [50/244], loss=7.9770
	step [51/244], loss=10.6272
	step [52/244], loss=10.3986
	step [53/244], loss=11.5415
	step [54/244], loss=9.6654
	step [55/244], loss=7.4639
	step [56/244], loss=8.7187
	step [57/244], loss=9.3504
	step [58/244], loss=8.4544
	step [59/244], loss=8.5443
	step [60/244], loss=8.6009
	step [61/244], loss=10.5266
	step [62/244], loss=9.1658
	step [63/244], loss=9.3903
	step [64/244], loss=11.5045
	step [65/244], loss=8.9988
	step [66/244], loss=8.9546
	step [67/244], loss=8.4257
	step [68/244], loss=11.2376
	step [69/244], loss=7.4662
	step [70/244], loss=8.1968
	step [71/244], loss=8.6687
	step [72/244], loss=7.9836
	step [73/244], loss=8.9624
	step [74/244], loss=9.0576
	step [75/244], loss=8.6356
	step [76/244], loss=8.2440
	step [77/244], loss=9.5898
	step [78/244], loss=9.5092
	step [79/244], loss=8.9376
	step [80/244], loss=6.9536
	step [81/244], loss=7.8210
	step [82/244], loss=8.4061
	step [83/244], loss=7.9515
	step [84/244], loss=11.5242
	step [85/244], loss=10.6533
	step [86/244], loss=9.1414
	step [87/244], loss=9.8757
	step [88/244], loss=11.4540
	step [89/244], loss=8.2632
	step [90/244], loss=9.0035
	step [91/244], loss=8.9543
	step [92/244], loss=10.2719
	step [93/244], loss=8.8976
	step [94/244], loss=9.5069
	step [95/244], loss=11.3004
	step [96/244], loss=10.8046
	step [97/244], loss=9.8932
	step [98/244], loss=7.4177
	step [99/244], loss=10.4144
	step [100/244], loss=9.3873
	step [101/244], loss=9.2633
	step [102/244], loss=10.4621
	step [103/244], loss=9.6253
	step [104/244], loss=9.8659
	step [105/244], loss=9.7170
	step [106/244], loss=10.7033
	step [107/244], loss=9.8868
	step [108/244], loss=9.4938
	step [109/244], loss=10.5092
	step [110/244], loss=10.3270
	step [111/244], loss=8.1691
	step [112/244], loss=7.8925
	step [113/244], loss=10.4328
	step [114/244], loss=7.3899
	step [115/244], loss=8.6746
	step [116/244], loss=11.1979
	step [117/244], loss=8.6869
	step [118/244], loss=9.2703
	step [119/244], loss=10.7207
	step [120/244], loss=8.4340
	step [121/244], loss=6.8795
	step [122/244], loss=7.0700
	step [123/244], loss=9.7424
	step [124/244], loss=12.8299
	step [125/244], loss=9.0072
	step [126/244], loss=10.9738
	step [127/244], loss=9.3690
	step [128/244], loss=9.2439
	step [129/244], loss=8.1957
	step [130/244], loss=9.4250
	step [131/244], loss=10.5215
	step [132/244], loss=6.6409
	step [133/244], loss=10.4549
	step [134/244], loss=8.7087
	step [135/244], loss=7.2709
	step [136/244], loss=8.4670
	step [137/244], loss=8.3453
	step [138/244], loss=9.3858
	step [139/244], loss=9.7344
	step [140/244], loss=8.5631
	step [141/244], loss=8.9475
	step [142/244], loss=9.6951
	step [143/244], loss=9.9235
	step [144/244], loss=8.4200
	step [145/244], loss=9.1593
	step [146/244], loss=7.7890
	step [147/244], loss=8.0129
	step [148/244], loss=9.6848
	step [149/244], loss=10.5396
	step [150/244], loss=8.7855
	step [151/244], loss=8.7393
	step [152/244], loss=8.6147
	step [153/244], loss=10.6390
	step [154/244], loss=6.7545
	step [155/244], loss=9.1125
	step [156/244], loss=9.8678
	step [157/244], loss=9.0285
	step [158/244], loss=7.3601
	step [159/244], loss=7.0327
	step [160/244], loss=8.3038
	step [161/244], loss=8.3655
	step [162/244], loss=7.5763
	step [163/244], loss=6.7996
	step [164/244], loss=11.5432
	step [165/244], loss=10.6004
	step [166/244], loss=11.4315
	step [167/244], loss=11.0099
	step [168/244], loss=8.3722
	step [169/244], loss=9.5094
	step [170/244], loss=9.9098
	step [171/244], loss=9.8145
	step [172/244], loss=11.1038
	step [173/244], loss=8.3027
	step [174/244], loss=9.9583
	step [175/244], loss=9.2453
	step [176/244], loss=9.6766
	step [177/244], loss=8.0562
	step [178/244], loss=9.1177
	step [179/244], loss=9.1402
	step [180/244], loss=9.1801
	step [181/244], loss=9.8922
	step [182/244], loss=8.7736
	step [183/244], loss=8.5247
	step [184/244], loss=9.0563
	step [185/244], loss=11.0713
	step [186/244], loss=8.1882
	step [187/244], loss=8.4748
	step [188/244], loss=9.1959
	step [189/244], loss=9.4545
	step [190/244], loss=11.0707
	step [191/244], loss=10.2169
	step [192/244], loss=9.3728
	step [193/244], loss=8.5947
	step [194/244], loss=10.1900
	step [195/244], loss=9.8778
	step [196/244], loss=9.5429
	step [197/244], loss=9.5289
	step [198/244], loss=10.1089
	step [199/244], loss=7.1923
	step [200/244], loss=8.8196
	step [201/244], loss=9.4286
	step [202/244], loss=10.5837
	step [203/244], loss=8.9585
	step [204/244], loss=9.4398
	step [205/244], loss=7.7148
	step [206/244], loss=9.7106
	step [207/244], loss=7.7932
	step [208/244], loss=9.4059
	step [209/244], loss=11.3621
	step [210/244], loss=11.9602
	step [211/244], loss=8.4046
	step [212/244], loss=8.7113
	step [213/244], loss=7.9848
	step [214/244], loss=9.0629
	step [215/244], loss=8.1652
	step [216/244], loss=7.4918
	step [217/244], loss=8.7026
	step [218/244], loss=7.4844
	step [219/244], loss=11.0514
	step [220/244], loss=8.0965
	step [221/244], loss=8.8949
	step [222/244], loss=9.6766
	step [223/244], loss=9.9498
	step [224/244], loss=9.3650
	step [225/244], loss=8.7106
	step [226/244], loss=10.8419
	step [227/244], loss=9.6359
	step [228/244], loss=7.7466
	step [229/244], loss=8.5583
	step [230/244], loss=8.1458
	step [231/244], loss=9.6742
	step [232/244], loss=8.7070
	step [233/244], loss=7.5365
	step [234/244], loss=8.6712
	step [235/244], loss=10.6633
	step [236/244], loss=8.2393
	step [237/244], loss=9.4936
	step [238/244], loss=7.4750
	step [239/244], loss=10.3095
	step [240/244], loss=8.9489
	step [241/244], loss=8.5465
	step [242/244], loss=9.4939
	step [243/244], loss=8.8291
	step [244/244], loss=4.9344
	Evaluating
	loss=0.0208, precision=0.2084, recall=0.9902, f1=0.3443
saving model as: 1_saved_model.pth
Training epoch 18
	step [1/244], loss=8.8061
	step [2/244], loss=8.3236
	step [3/244], loss=7.8305
	step [4/244], loss=9.4379
	step [5/244], loss=7.3516
	step [6/244], loss=9.1369
	step [7/244], loss=8.2239
	step [8/244], loss=10.1101
	step [9/244], loss=7.9153
	step [10/244], loss=9.9805
	step [11/244], loss=8.4349
	step [12/244], loss=9.7657
	step [13/244], loss=9.1458
	step [14/244], loss=7.2159
	step [15/244], loss=9.9874
	step [16/244], loss=7.5691
	step [17/244], loss=8.7479
	step [18/244], loss=9.4113
	step [19/244], loss=7.7963
	step [20/244], loss=9.2943
	step [21/244], loss=9.0644
	step [22/244], loss=9.8095
	step [23/244], loss=9.3326
	step [24/244], loss=8.2823
	step [25/244], loss=6.1935
	step [26/244], loss=8.5830
	step [27/244], loss=9.0874
	step [28/244], loss=8.6704
	step [29/244], loss=9.0058
	step [30/244], loss=7.3564
	step [31/244], loss=9.2090
	step [32/244], loss=10.2723
	step [33/244], loss=7.3367
	step [34/244], loss=8.4814
	step [35/244], loss=10.6913
	step [36/244], loss=8.1889
	step [37/244], loss=9.0989
	step [38/244], loss=10.3893
	step [39/244], loss=9.8736
	step [40/244], loss=9.2314
	step [41/244], loss=10.0697
	step [42/244], loss=9.0197
	step [43/244], loss=7.4387
	step [44/244], loss=7.6676
	step [45/244], loss=8.7785
	step [46/244], loss=7.6116
	step [47/244], loss=8.8862
	step [48/244], loss=9.5016
	step [49/244], loss=9.5927
	step [50/244], loss=10.6368
	step [51/244], loss=10.0979
	step [52/244], loss=8.4050
	step [53/244], loss=11.6636
	step [54/244], loss=11.0546
	step [55/244], loss=9.7044
	step [56/244], loss=7.9778
	step [57/244], loss=7.8666
	step [58/244], loss=8.3987
	step [59/244], loss=10.1861
	step [60/244], loss=8.7665
	step [61/244], loss=10.1066
	step [62/244], loss=9.4066
	step [63/244], loss=8.6633
	step [64/244], loss=8.2561
	step [65/244], loss=8.9152
	step [66/244], loss=12.5969
	step [67/244], loss=8.9705
	step [68/244], loss=10.4067
	step [69/244], loss=9.3161
	step [70/244], loss=8.6216
	step [71/244], loss=8.2547
	step [72/244], loss=7.0866
	step [73/244], loss=8.8828
	step [74/244], loss=8.1402
	step [75/244], loss=8.0753
	step [76/244], loss=9.4669
	step [77/244], loss=10.0804
	step [78/244], loss=8.1520
	step [79/244], loss=10.6286
	step [80/244], loss=8.5448
	step [81/244], loss=10.4833
	step [82/244], loss=7.6028
	step [83/244], loss=8.4044
	step [84/244], loss=9.5544
	step [85/244], loss=8.2327
	step [86/244], loss=8.3959
	step [87/244], loss=8.1346
	step [88/244], loss=7.9447
	step [89/244], loss=11.1071
	step [90/244], loss=9.5884
	step [91/244], loss=7.6533
	step [92/244], loss=8.5067
	step [93/244], loss=7.5043
	step [94/244], loss=8.3630
	step [95/244], loss=8.8621
	step [96/244], loss=10.0168
	step [97/244], loss=8.5300
	step [98/244], loss=7.9231
	step [99/244], loss=8.0772
	step [100/244], loss=8.9298
	step [101/244], loss=10.5280
	step [102/244], loss=6.6559
	step [103/244], loss=9.6785
	step [104/244], loss=7.6202
	step [105/244], loss=7.8742
	step [106/244], loss=9.2330
	step [107/244], loss=10.0643
	step [108/244], loss=8.4763
	step [109/244], loss=9.0802
	step [110/244], loss=7.1113
	step [111/244], loss=7.1867
	step [112/244], loss=7.5146
	step [113/244], loss=6.8180
	step [114/244], loss=8.8143
	step [115/244], loss=10.1507
	step [116/244], loss=10.2091
	step [117/244], loss=7.6299
	step [118/244], loss=8.3582
	step [119/244], loss=9.7155
	step [120/244], loss=9.0635
	step [121/244], loss=8.4060
	step [122/244], loss=9.7574
	step [123/244], loss=9.1669
	step [124/244], loss=8.9804
	step [125/244], loss=8.6178
	step [126/244], loss=9.6998
	step [127/244], loss=7.1261
	step [128/244], loss=11.1256
	step [129/244], loss=7.4391
	step [130/244], loss=9.4208
	step [131/244], loss=7.6858
	step [132/244], loss=7.9282
	step [133/244], loss=10.8759
	step [134/244], loss=9.0297
	step [135/244], loss=7.6290
	step [136/244], loss=8.9974
	step [137/244], loss=10.4254
	step [138/244], loss=9.5857
	step [139/244], loss=9.4887
	step [140/244], loss=8.4201
	step [141/244], loss=9.0521
	step [142/244], loss=8.2006
	step [143/244], loss=10.3434
	step [144/244], loss=7.4058
	step [145/244], loss=9.6432
	step [146/244], loss=9.6881
	step [147/244], loss=10.0040
	step [148/244], loss=9.1181
	step [149/244], loss=9.5705
	step [150/244], loss=6.9461
	step [151/244], loss=9.2856
	step [152/244], loss=10.2170
	step [153/244], loss=7.8017
	step [154/244], loss=8.0270
	step [155/244], loss=9.1384
	step [156/244], loss=8.5157
	step [157/244], loss=7.3901
	step [158/244], loss=7.0141
	step [159/244], loss=7.6419
	step [160/244], loss=8.9566
	step [161/244], loss=8.3681
	step [162/244], loss=9.2876
	step [163/244], loss=8.6399
	step [164/244], loss=7.2487
	step [165/244], loss=8.3930
	step [166/244], loss=9.5410
	step [167/244], loss=8.6544
	step [168/244], loss=7.6318
	step [169/244], loss=8.6990
	step [170/244], loss=7.1924
	step [171/244], loss=8.8589
	step [172/244], loss=10.2507
	step [173/244], loss=7.4273
	step [174/244], loss=8.4479
	step [175/244], loss=8.0701
	step [176/244], loss=9.4444
	step [177/244], loss=10.2705
	step [178/244], loss=8.8743
	step [179/244], loss=8.9048
	step [180/244], loss=8.5745
	step [181/244], loss=8.3467
	step [182/244], loss=8.6796
	step [183/244], loss=7.7861
	step [184/244], loss=7.9203
	step [185/244], loss=10.8848
	step [186/244], loss=7.9821
	step [187/244], loss=7.0756
	step [188/244], loss=9.8890
	step [189/244], loss=9.8294
	step [190/244], loss=6.5431
	step [191/244], loss=8.7608
	step [192/244], loss=8.7117
	step [193/244], loss=8.4499
	step [194/244], loss=8.3038
	step [195/244], loss=9.7722
	step [196/244], loss=7.7905
	step [197/244], loss=10.8206
	step [198/244], loss=11.4039
	step [199/244], loss=9.0149
	step [200/244], loss=9.3809
	step [201/244], loss=10.5253
	step [202/244], loss=8.7988
	step [203/244], loss=7.8755
	step [204/244], loss=9.1482
	step [205/244], loss=9.7039
	step [206/244], loss=9.4949
	step [207/244], loss=7.0007
	step [208/244], loss=9.5506
	step [209/244], loss=8.6975
	step [210/244], loss=6.9770
	step [211/244], loss=8.2425
	step [212/244], loss=9.9872
	step [213/244], loss=7.9300
	step [214/244], loss=9.1229
	step [215/244], loss=8.4777
	step [216/244], loss=7.8325
	step [217/244], loss=8.0590
	step [218/244], loss=10.2444
	step [219/244], loss=9.2313
	step [220/244], loss=7.8898
	step [221/244], loss=9.6314
	step [222/244], loss=9.0715
	step [223/244], loss=9.9121
	step [224/244], loss=9.1213
	step [225/244], loss=9.6266
	step [226/244], loss=8.8223
	step [227/244], loss=8.1967
	step [228/244], loss=10.7945
	step [229/244], loss=10.2319
	step [230/244], loss=9.2962
	step [231/244], loss=10.7316
	step [232/244], loss=7.5773
	step [233/244], loss=9.6112
	step [234/244], loss=8.0536
	step [235/244], loss=9.4511
	step [236/244], loss=9.2374
	step [237/244], loss=8.8991
	step [238/244], loss=8.6150
	step [239/244], loss=8.7568
	step [240/244], loss=7.7766
	step [241/244], loss=9.3586
	step [242/244], loss=8.5597
	step [243/244], loss=7.9228
	step [244/244], loss=2.6797
	Evaluating
	loss=0.0235, precision=0.1803, recall=0.9921, f1=0.3051
Training epoch 19
	step [1/244], loss=9.5606
	step [2/244], loss=11.1302
	step [3/244], loss=7.9685
	step [4/244], loss=8.5514
	step [5/244], loss=8.6630
	step [6/244], loss=8.3642
	step [7/244], loss=7.7118
	step [8/244], loss=8.4851
	step [9/244], loss=8.2436
	step [10/244], loss=7.3143
	step [11/244], loss=8.4096
	step [12/244], loss=8.6804
	step [13/244], loss=7.4910
	step [14/244], loss=7.8805
	step [15/244], loss=8.2534
	step [16/244], loss=14.4576
	step [17/244], loss=8.2169
	step [18/244], loss=7.4768
	step [19/244], loss=10.0555
	step [20/244], loss=9.9173
	step [21/244], loss=8.6803
	step [22/244], loss=8.9676
	step [23/244], loss=9.0204
	step [24/244], loss=7.8695
	step [25/244], loss=7.4815
	step [26/244], loss=10.3451
	step [27/244], loss=10.1405
	step [28/244], loss=6.5111
	step [29/244], loss=9.2378
	step [30/244], loss=8.9839
	step [31/244], loss=9.7031
	step [32/244], loss=7.6926
	step [33/244], loss=8.1251
	step [34/244], loss=7.6763
	step [35/244], loss=6.2548
	step [36/244], loss=7.7674
	step [37/244], loss=10.1988
	step [38/244], loss=8.8339
	step [39/244], loss=7.8800
	step [40/244], loss=7.8235
	step [41/244], loss=7.6921
	step [42/244], loss=7.6389
	step [43/244], loss=8.3280
	step [44/244], loss=8.4140
	step [45/244], loss=7.2073
	step [46/244], loss=10.5669
	step [47/244], loss=8.8285
	step [48/244], loss=7.3427
	step [49/244], loss=9.7155
	step [50/244], loss=7.3208
	step [51/244], loss=7.9948
	step [52/244], loss=8.6951
	step [53/244], loss=9.0592
	step [54/244], loss=8.6061
	step [55/244], loss=10.4397
	step [56/244], loss=9.5189
	step [57/244], loss=12.5434
	step [58/244], loss=7.9247
	step [59/244], loss=9.5572
	step [60/244], loss=10.0056
	step [61/244], loss=9.1536
	step [62/244], loss=8.9897
	step [63/244], loss=9.7330
	step [64/244], loss=9.4353
	step [65/244], loss=9.6230
	step [66/244], loss=9.7392
	step [67/244], loss=8.5102
	step [68/244], loss=8.1789
	step [69/244], loss=9.2771
	step [70/244], loss=10.3539
	step [71/244], loss=8.0683
	step [72/244], loss=8.7346
	step [73/244], loss=9.0089
	step [74/244], loss=12.3343
	step [75/244], loss=9.2936
	step [76/244], loss=9.0464
	step [77/244], loss=9.0979
	step [78/244], loss=8.0012
	step [79/244], loss=8.8082
	step [80/244], loss=7.2360
	step [81/244], loss=8.2551
	step [82/244], loss=6.3375
	step [83/244], loss=10.5457
	step [84/244], loss=7.2706
	step [85/244], loss=7.7964
	step [86/244], loss=9.4916
	step [87/244], loss=8.6477
	step [88/244], loss=8.0374
	step [89/244], loss=6.7847
	step [90/244], loss=9.1794
	step [91/244], loss=7.4113
	step [92/244], loss=9.2635
	step [93/244], loss=7.9728
	step [94/244], loss=10.0983
	step [95/244], loss=9.9082
	step [96/244], loss=8.2215
	step [97/244], loss=8.8745
	step [98/244], loss=7.9177
	step [99/244], loss=8.7030
	step [100/244], loss=9.8025
	step [101/244], loss=7.6992
	step [102/244], loss=8.6822
	step [103/244], loss=10.0532
	step [104/244], loss=9.9279
	step [105/244], loss=7.2779
	step [106/244], loss=8.7906
	step [107/244], loss=9.0457
	step [108/244], loss=7.7836
	step [109/244], loss=8.6965
	step [110/244], loss=8.2180
	step [111/244], loss=6.4244
	step [112/244], loss=8.0820
	step [113/244], loss=8.4170
	step [114/244], loss=9.9625
	step [115/244], loss=8.3629
	step [116/244], loss=7.3178
	step [117/244], loss=8.1186
	step [118/244], loss=7.9645
	step [119/244], loss=7.4944
	step [120/244], loss=10.7633
	step [121/244], loss=11.4280
	step [122/244], loss=8.2349
	step [123/244], loss=8.0583
	step [124/244], loss=8.2954
	step [125/244], loss=9.1879
	step [126/244], loss=7.3862
	step [127/244], loss=8.5280
	step [128/244], loss=6.8226
	step [129/244], loss=9.7436
	step [130/244], loss=7.5528
	step [131/244], loss=7.9274
	step [132/244], loss=8.2655
	step [133/244], loss=8.3662
	step [134/244], loss=8.2711
	step [135/244], loss=8.5586
	step [136/244], loss=9.0890
	step [137/244], loss=8.9846
	step [138/244], loss=7.0453
	step [139/244], loss=7.4874
	step [140/244], loss=8.2658
	step [141/244], loss=10.6682
	step [142/244], loss=8.5415
	step [143/244], loss=9.3611
	step [144/244], loss=8.1759
	step [145/244], loss=8.7339
	step [146/244], loss=8.8310
	step [147/244], loss=7.7866
	step [148/244], loss=8.8524
	step [149/244], loss=7.7773
	step [150/244], loss=9.6361
	step [151/244], loss=7.4371
	step [152/244], loss=9.1908
	step [153/244], loss=8.6818
	step [154/244], loss=9.1451
	step [155/244], loss=10.3602
	step [156/244], loss=8.3819
	step [157/244], loss=8.7943
	step [158/244], loss=7.0878
	step [159/244], loss=8.9677
	step [160/244], loss=8.1932
	step [161/244], loss=10.2419
	step [162/244], loss=8.6485
	step [163/244], loss=8.7622
	step [164/244], loss=7.0049
	step [165/244], loss=6.4536
	step [166/244], loss=9.0020
	step [167/244], loss=7.9145
	step [168/244], loss=7.9963
	step [169/244], loss=9.2490
	step [170/244], loss=9.9587
	step [171/244], loss=8.5387
	step [172/244], loss=8.3564
	step [173/244], loss=6.9616
	step [174/244], loss=7.7873
	step [175/244], loss=8.3044
	step [176/244], loss=10.2523
	step [177/244], loss=8.0740
	step [178/244], loss=9.3422
	step [179/244], loss=11.2422
	step [180/244], loss=7.2978
	step [181/244], loss=10.2894
	step [182/244], loss=10.2908
	step [183/244], loss=7.5176
	step [184/244], loss=6.9578
	step [185/244], loss=8.9836
	step [186/244], loss=8.4153
	step [187/244], loss=7.9018
	step [188/244], loss=9.6479
	step [189/244], loss=8.4860
	step [190/244], loss=8.0459
	step [191/244], loss=6.9772
	step [192/244], loss=8.1728
	step [193/244], loss=6.2769
	step [194/244], loss=9.0233
	step [195/244], loss=6.8881
	step [196/244], loss=9.4130
	step [197/244], loss=8.0747
	step [198/244], loss=7.7806
	step [199/244], loss=6.3252
	step [200/244], loss=8.2259
	step [201/244], loss=9.2603
	step [202/244], loss=9.0763
	step [203/244], loss=9.8164
	step [204/244], loss=7.9394
	step [205/244], loss=10.4244
	step [206/244], loss=9.4205
	step [207/244], loss=9.8758
	step [208/244], loss=8.2723
	step [209/244], loss=7.0639
	step [210/244], loss=9.0545
	step [211/244], loss=8.6359
	step [212/244], loss=8.2078
	step [213/244], loss=6.7052
	step [214/244], loss=8.7369
	step [215/244], loss=10.3550
	step [216/244], loss=7.9370
	step [217/244], loss=9.1255
	step [218/244], loss=8.4116
	step [219/244], loss=7.4979
	step [220/244], loss=8.5193
	step [221/244], loss=8.3625
	step [222/244], loss=9.5208
	step [223/244], loss=9.7655
	step [224/244], loss=8.5294
	step [225/244], loss=8.3650
	step [226/244], loss=10.6638
	step [227/244], loss=8.1202
	step [228/244], loss=11.1578
	step [229/244], loss=8.5666
	step [230/244], loss=6.8167
	step [231/244], loss=7.7146
	step [232/244], loss=7.2651
	step [233/244], loss=7.2476
	step [234/244], loss=9.6738
	step [235/244], loss=6.9124
	step [236/244], loss=12.3097
	step [237/244], loss=7.5481
	step [238/244], loss=6.8762
	step [239/244], loss=7.8460
	step [240/244], loss=8.2027
	step [241/244], loss=7.5714
	step [242/244], loss=8.3306
	step [243/244], loss=7.3406
	step [244/244], loss=5.3461
	Evaluating
	loss=0.0203, precision=0.2136, recall=0.9906, f1=0.3514
saving model as: 1_saved_model.pth
Training epoch 20
	step [1/244], loss=8.3350
	step [2/244], loss=6.6419
	step [3/244], loss=7.5664
	step [4/244], loss=8.8502
	step [5/244], loss=6.8179
	step [6/244], loss=8.0747
	step [7/244], loss=11.0767
	step [8/244], loss=8.7662
	step [9/244], loss=7.3561
	step [10/244], loss=8.9682
	step [11/244], loss=8.3996
	step [12/244], loss=9.3384
	step [13/244], loss=8.3031
	step [14/244], loss=10.9406
	step [15/244], loss=8.3129
	step [16/244], loss=6.8133
	step [17/244], loss=7.4781
	step [18/244], loss=8.7288
	step [19/244], loss=9.4836
	step [20/244], loss=7.5602
	step [21/244], loss=7.8602
	step [22/244], loss=7.3503
	step [23/244], loss=7.9072
	step [24/244], loss=8.0989
	step [25/244], loss=7.8142
	step [26/244], loss=9.1892
	step [27/244], loss=8.9183
	step [28/244], loss=7.9422
	step [29/244], loss=9.1011
	step [30/244], loss=7.1625
	step [31/244], loss=8.6886
	step [32/244], loss=9.8344
	step [33/244], loss=7.5926
	step [34/244], loss=7.6280
	step [35/244], loss=7.0675
	step [36/244], loss=6.7640
	step [37/244], loss=7.8254
	step [38/244], loss=7.8223
	step [39/244], loss=8.9225
	step [40/244], loss=8.1537
	step [41/244], loss=7.7011
	step [42/244], loss=8.3104
	step [43/244], loss=7.4339
	step [44/244], loss=10.2323
	step [45/244], loss=7.4264
	step [46/244], loss=7.9134
	step [47/244], loss=9.1523
	step [48/244], loss=7.5738
	step [49/244], loss=9.5225
	step [50/244], loss=7.6012
	step [51/244], loss=6.7334
	step [52/244], loss=7.7822
	step [53/244], loss=9.4486
	step [54/244], loss=7.7285
	step [55/244], loss=8.8591
	step [56/244], loss=8.8570
	step [57/244], loss=9.3655
	step [58/244], loss=8.2169
	step [59/244], loss=10.8445
	step [60/244], loss=8.5770
	step [61/244], loss=8.4999
	step [62/244], loss=8.3771
	step [63/244], loss=8.8378
	step [64/244], loss=7.7912
	step [65/244], loss=8.7016
	step [66/244], loss=8.1707
	step [67/244], loss=6.3774
	step [68/244], loss=8.7465
	step [69/244], loss=12.0490
	step [70/244], loss=6.9536
	step [71/244], loss=8.6419
	step [72/244], loss=6.8350
	step [73/244], loss=6.7771
	step [74/244], loss=7.1005
	step [75/244], loss=7.1065
	step [76/244], loss=9.9906
	step [77/244], loss=8.8229
	step [78/244], loss=7.3572
	step [79/244], loss=7.7129
	step [80/244], loss=8.5827
	step [81/244], loss=9.1329
	step [82/244], loss=7.6215
	step [83/244], loss=6.9657
	step [84/244], loss=8.9413
	step [85/244], loss=6.9479
	step [86/244], loss=7.0495
	step [87/244], loss=7.6008
	step [88/244], loss=7.9303
	step [89/244], loss=8.5239
	step [90/244], loss=7.7401
	step [91/244], loss=7.9832
	step [92/244], loss=6.7078
	step [93/244], loss=7.9584
	step [94/244], loss=7.1178
	step [95/244], loss=7.9323
	step [96/244], loss=6.5463
	step [97/244], loss=7.3942
	step [98/244], loss=8.4361
	step [99/244], loss=8.9081
	step [100/244], loss=7.0775
	step [101/244], loss=7.6850
	step [102/244], loss=7.9517
	step [103/244], loss=8.9925
	step [104/244], loss=7.5193
	step [105/244], loss=7.0044
	step [106/244], loss=8.2564
	step [107/244], loss=7.7754
	step [108/244], loss=6.6295
	step [109/244], loss=7.6928
	step [110/244], loss=7.4182
	step [111/244], loss=7.8034
	step [112/244], loss=7.8961
	step [113/244], loss=6.6428
	step [114/244], loss=10.8331
	step [115/244], loss=8.3424
	step [116/244], loss=8.4522
	step [117/244], loss=9.9020
	step [118/244], loss=8.6047
	step [119/244], loss=7.7392
	step [120/244], loss=7.5288
	step [121/244], loss=9.2795
	step [122/244], loss=7.3543
	step [123/244], loss=8.4613
	step [124/244], loss=8.5456
	step [125/244], loss=8.3468
	step [126/244], loss=9.1345
	step [127/244], loss=8.5997
	step [128/244], loss=7.8931
	step [129/244], loss=7.3723
	step [130/244], loss=8.0115
	step [131/244], loss=8.0159
	step [132/244], loss=9.2570
	step [133/244], loss=7.4395
	step [134/244], loss=8.2010
	step [135/244], loss=8.1108
	step [136/244], loss=7.8631
	step [137/244], loss=8.9193
	step [138/244], loss=8.5460
	step [139/244], loss=7.7790
	step [140/244], loss=8.6408
	step [141/244], loss=7.5496
	step [142/244], loss=8.2348
	step [143/244], loss=6.7044
	step [144/244], loss=8.8205
	step [145/244], loss=8.2274
	step [146/244], loss=8.9088
	step [147/244], loss=6.8843
	step [148/244], loss=8.0281
	step [149/244], loss=7.5543
	step [150/244], loss=7.7171
	step [151/244], loss=7.9566
	step [152/244], loss=6.6470
	step [153/244], loss=9.0814
	step [154/244], loss=8.7296
	step [155/244], loss=8.4222
	step [156/244], loss=9.7746
	step [157/244], loss=7.6184
	step [158/244], loss=8.3213
	step [159/244], loss=6.8668
	step [160/244], loss=8.0853
	step [161/244], loss=10.2706
	step [162/244], loss=6.8603
	step [163/244], loss=7.8174
	step [164/244], loss=7.9296
	step [165/244], loss=6.8550
	step [166/244], loss=7.5879
	step [167/244], loss=7.4063
	step [168/244], loss=7.0128
	step [169/244], loss=10.4105
	step [170/244], loss=10.9368
	step [171/244], loss=7.5888
	step [172/244], loss=7.7135
	step [173/244], loss=8.2408
	step [174/244], loss=8.8362
	step [175/244], loss=7.8058
	step [176/244], loss=7.3695
	step [177/244], loss=8.2717
	step [178/244], loss=8.6005
	step [179/244], loss=7.2689
	step [180/244], loss=6.0919
	step [181/244], loss=7.3395
	step [182/244], loss=7.5220
	step [183/244], loss=7.9672
	step [184/244], loss=9.0111
	step [185/244], loss=9.0206
	step [186/244], loss=8.1240
	step [187/244], loss=9.4302
	step [188/244], loss=6.6918
	step [189/244], loss=8.3881
	step [190/244], loss=8.3245
	step [191/244], loss=7.8970
	step [192/244], loss=6.0062
	step [193/244], loss=7.8571
	step [194/244], loss=8.8943
	step [195/244], loss=8.5989
	step [196/244], loss=9.4146
	step [197/244], loss=8.4864
	step [198/244], loss=8.5035
	step [199/244], loss=7.4218
	step [200/244], loss=8.4280
	step [201/244], loss=9.0574
	step [202/244], loss=8.7208
	step [203/244], loss=8.1155
	step [204/244], loss=6.6761
	step [205/244], loss=8.6184
	step [206/244], loss=8.8024
	step [207/244], loss=9.3058
	step [208/244], loss=6.3374
	step [209/244], loss=6.8600
	step [210/244], loss=8.1798
	step [211/244], loss=7.3741
	step [212/244], loss=8.7473
	step [213/244], loss=7.0498
	step [214/244], loss=7.1104
	step [215/244], loss=6.9246
	step [216/244], loss=6.6502
	step [217/244], loss=7.5271
	step [218/244], loss=7.0495
	step [219/244], loss=7.5638
	step [220/244], loss=8.5617
	step [221/244], loss=7.5847
	step [222/244], loss=9.9811
	step [223/244], loss=8.5020
	step [224/244], loss=8.2260
	step [225/244], loss=10.5233
	step [226/244], loss=6.9074
	step [227/244], loss=7.3048
	step [228/244], loss=9.4270
	step [229/244], loss=7.8811
	step [230/244], loss=7.8411
	step [231/244], loss=8.7561
	step [232/244], loss=8.2036
	step [233/244], loss=8.2094
	step [234/244], loss=8.3338
	step [235/244], loss=8.8381
	step [236/244], loss=8.3171
	step [237/244], loss=8.0916
	step [238/244], loss=8.4662
	step [239/244], loss=10.8946
	step [240/244], loss=10.1369
	step [241/244], loss=9.4143
	step [242/244], loss=9.2966
	step [243/244], loss=8.8422
	step [244/244], loss=4.0157
	Evaluating
	loss=0.0226, precision=0.1815, recall=0.9915, f1=0.3069
Training epoch 21
	step [1/244], loss=7.4936
	step [2/244], loss=8.4158
	step [3/244], loss=8.4884
	step [4/244], loss=6.5924
	step [5/244], loss=8.2995
	step [6/244], loss=7.8885
	step [7/244], loss=9.8430
	step [8/244], loss=8.7386
	step [9/244], loss=7.4463
	step [10/244], loss=6.5064
	step [11/244], loss=6.4525
	step [12/244], loss=7.9538
	step [13/244], loss=7.5964
	step [14/244], loss=9.3945
	step [15/244], loss=5.7603
	step [16/244], loss=8.3833
	step [17/244], loss=7.6269
	step [18/244], loss=9.4480
	step [19/244], loss=7.8233
	step [20/244], loss=7.0537
	step [21/244], loss=7.5574
	step [22/244], loss=5.6480
	step [23/244], loss=8.1154
	step [24/244], loss=9.3923
	step [25/244], loss=7.7654
	step [26/244], loss=6.7922
	step [27/244], loss=7.1946
	step [28/244], loss=8.6266
	step [29/244], loss=7.3476
	step [30/244], loss=7.9478
	step [31/244], loss=7.2493
	step [32/244], loss=7.7455
	step [33/244], loss=7.6707
	step [34/244], loss=8.5716
	step [35/244], loss=5.8077
	step [36/244], loss=9.6988
	step [37/244], loss=7.4650
	step [38/244], loss=8.0171
	step [39/244], loss=9.3691
	step [40/244], loss=8.6488
	step [41/244], loss=8.2132
	step [42/244], loss=7.5363
	step [43/244], loss=8.4466
	step [44/244], loss=7.5680
	step [45/244], loss=6.7031
	step [46/244], loss=6.9829
	step [47/244], loss=7.0804
	step [48/244], loss=7.9522
	step [49/244], loss=6.8466
	step [50/244], loss=7.2384
	step [51/244], loss=6.3748
	step [52/244], loss=6.3002
	step [53/244], loss=6.2283
	step [54/244], loss=7.3832
	step [55/244], loss=8.8278
	step [56/244], loss=6.5478
	step [57/244], loss=7.8347
	step [58/244], loss=6.6902
	step [59/244], loss=7.4749
	step [60/244], loss=8.4683
	step [61/244], loss=7.6905
	step [62/244], loss=7.2209
	step [63/244], loss=7.7132
	step [64/244], loss=6.9432
	step [65/244], loss=7.0994
	step [66/244], loss=8.8948
	step [67/244], loss=7.7378
	step [68/244], loss=8.3213
	step [69/244], loss=7.4198
	step [70/244], loss=7.9073
	step [71/244], loss=8.1847
	step [72/244], loss=7.6070
	step [73/244], loss=7.0359
	step [74/244], loss=8.7835
	step [75/244], loss=8.7835
	step [76/244], loss=8.9513
	step [77/244], loss=7.7862
	step [78/244], loss=8.0655
	step [79/244], loss=7.7563
	step [80/244], loss=6.7584
	step [81/244], loss=6.9937
	step [82/244], loss=8.1810
	step [83/244], loss=7.6643
	step [84/244], loss=7.3836
	step [85/244], loss=7.9367
	step [86/244], loss=10.0458
	step [87/244], loss=7.7637
	step [88/244], loss=7.2943
	step [89/244], loss=7.7515
	step [90/244], loss=9.0959
	step [91/244], loss=6.6576
	step [92/244], loss=8.8967
	step [93/244], loss=10.5742
	step [94/244], loss=7.2887
	step [95/244], loss=7.4407
	step [96/244], loss=8.2516
	step [97/244], loss=7.9164
	step [98/244], loss=9.3847
	step [99/244], loss=8.1383
	step [100/244], loss=7.5013
	step [101/244], loss=7.8684
	step [102/244], loss=9.1305
	step [103/244], loss=8.7921
	step [104/244], loss=7.0832
	step [105/244], loss=6.8242
	step [106/244], loss=9.0240
	step [107/244], loss=8.8286
	step [108/244], loss=7.4125
	step [109/244], loss=7.4099
	step [110/244], loss=8.7023
	step [111/244], loss=7.6097
	step [112/244], loss=6.6108
	step [113/244], loss=9.0611
	step [114/244], loss=9.4326
	step [115/244], loss=9.4681
	step [116/244], loss=9.0152
	step [117/244], loss=8.4910
	step [118/244], loss=7.5592
	step [119/244], loss=8.7865
	step [120/244], loss=9.1045
	step [121/244], loss=7.9415
	step [122/244], loss=8.5017
	step [123/244], loss=8.6807
	step [124/244], loss=8.1834
	step [125/244], loss=6.7226
	step [126/244], loss=6.9365
	step [127/244], loss=9.0650
	step [128/244], loss=9.0784
	step [129/244], loss=6.9478
	step [130/244], loss=10.4247
	step [131/244], loss=8.8589
	step [132/244], loss=7.3121
	step [133/244], loss=9.1152
	step [134/244], loss=9.4034
	step [135/244], loss=8.0472
	step [136/244], loss=6.6057
	step [137/244], loss=8.1119
	step [138/244], loss=7.2914
	step [139/244], loss=8.0811
	step [140/244], loss=7.4569
	step [141/244], loss=6.2007
	step [142/244], loss=7.2045
	step [143/244], loss=7.7969
	step [144/244], loss=8.6134
	step [145/244], loss=7.2629
	step [146/244], loss=7.1352
	step [147/244], loss=7.2572
	step [148/244], loss=7.7293
	step [149/244], loss=8.5652
	step [150/244], loss=6.5245
	step [151/244], loss=7.5222
	step [152/244], loss=6.0310
	step [153/244], loss=10.4190
	step [154/244], loss=7.9630
	step [155/244], loss=7.9167
	step [156/244], loss=9.7665
	step [157/244], loss=8.4359
	step [158/244], loss=9.0085
	step [159/244], loss=7.7958
	step [160/244], loss=8.6096
	step [161/244], loss=9.6447
	step [162/244], loss=9.7496
	step [163/244], loss=11.0574
	step [164/244], loss=8.0328
	step [165/244], loss=7.5071
	step [166/244], loss=9.1320
	step [167/244], loss=7.0398
	step [168/244], loss=7.4968
	step [169/244], loss=7.9306
	step [170/244], loss=6.7201
	step [171/244], loss=9.1911
	step [172/244], loss=10.2061
	step [173/244], loss=9.2187
	step [174/244], loss=5.9741
	step [175/244], loss=8.8834
	step [176/244], loss=7.7738
	step [177/244], loss=9.4633
	step [178/244], loss=7.5972
	step [179/244], loss=6.8766
	step [180/244], loss=10.5193
	step [181/244], loss=10.2099
	step [182/244], loss=10.7722
	step [183/244], loss=8.4472
	step [184/244], loss=8.8155
	step [185/244], loss=10.0188
	step [186/244], loss=8.2022
	step [187/244], loss=8.2930
	step [188/244], loss=6.1886
	step [189/244], loss=7.6807
	step [190/244], loss=5.6595
	step [191/244], loss=8.4994
	step [192/244], loss=7.8835
	step [193/244], loss=8.0366
	step [194/244], loss=8.3517
	step [195/244], loss=6.9731
	step [196/244], loss=9.1813
	step [197/244], loss=6.8842
	step [198/244], loss=6.5924
	step [199/244], loss=6.3730
	step [200/244], loss=7.7624
	step [201/244], loss=7.4458
	step [202/244], loss=7.8798
	step [203/244], loss=6.7338
	step [204/244], loss=8.4998
	step [205/244], loss=7.4950
	step [206/244], loss=6.5817
	step [207/244], loss=9.0424
	step [208/244], loss=8.1688
	step [209/244], loss=8.0700
	step [210/244], loss=8.0525
	step [211/244], loss=7.3822
	step [212/244], loss=8.0465
	step [213/244], loss=7.1201
	step [214/244], loss=6.4800
	step [215/244], loss=6.5254
	step [216/244], loss=9.4297
	step [217/244], loss=8.6599
	step [218/244], loss=6.9474
	step [219/244], loss=8.9118
	step [220/244], loss=6.3794
	step [221/244], loss=6.6727
	step [222/244], loss=8.3314
	step [223/244], loss=7.3443
	step [224/244], loss=7.2201
	step [225/244], loss=6.2899
	step [226/244], loss=8.9937
	step [227/244], loss=7.1366
	step [228/244], loss=6.7608
	step [229/244], loss=6.5421
	step [230/244], loss=8.3325
	step [231/244], loss=7.1594
	step [232/244], loss=7.7449
	step [233/244], loss=6.7944
	step [234/244], loss=7.7369
	step [235/244], loss=7.8686
	step [236/244], loss=8.7406
	step [237/244], loss=7.0434
	step [238/244], loss=5.4768
	step [239/244], loss=7.3507
	step [240/244], loss=8.1607
	step [241/244], loss=6.9838
	step [242/244], loss=10.0716
	step [243/244], loss=8.3624
	step [244/244], loss=4.1230
	Evaluating
	loss=0.0200, precision=0.2135, recall=0.9896, f1=0.3512
Training epoch 22
	step [1/244], loss=7.8037
	step [2/244], loss=8.6759
	step [3/244], loss=8.5410
	step [4/244], loss=8.9101
	step [5/244], loss=10.2734
	step [6/244], loss=7.2726
	step [7/244], loss=6.3140
	step [8/244], loss=9.1221
	step [9/244], loss=7.6687
	step [10/244], loss=9.2594
	step [11/244], loss=8.0790
	step [12/244], loss=6.8682
	step [13/244], loss=7.9070
	step [14/244], loss=8.1100
	step [15/244], loss=8.8714
	step [16/244], loss=8.6396
	step [17/244], loss=6.5658
	step [18/244], loss=7.3815
	step [19/244], loss=8.0619
	step [20/244], loss=9.3522
	step [21/244], loss=7.9374
	step [22/244], loss=8.6204
	step [23/244], loss=7.4710
	step [24/244], loss=7.2754
	step [25/244], loss=7.0028
	step [26/244], loss=7.4849
	step [27/244], loss=8.5870
	step [28/244], loss=10.1883
	step [29/244], loss=8.3813
	step [30/244], loss=7.0782
	step [31/244], loss=7.7783
	step [32/244], loss=7.7471
	step [33/244], loss=8.9329
	step [34/244], loss=8.3129
	step [35/244], loss=8.6437
	step [36/244], loss=8.0354
	step [37/244], loss=7.1678
	step [38/244], loss=7.7532
	step [39/244], loss=6.3587
	step [40/244], loss=6.1447
	step [41/244], loss=7.3682
	step [42/244], loss=7.9992
	step [43/244], loss=6.6415
	step [44/244], loss=7.0415
	step [45/244], loss=7.1024
	step [46/244], loss=7.7697
	step [47/244], loss=6.7447
	step [48/244], loss=7.9752
	step [49/244], loss=9.5642
	step [50/244], loss=7.1616
	step [51/244], loss=7.0116
	step [52/244], loss=6.3933
	step [53/244], loss=7.1428
	step [54/244], loss=7.8034
	step [55/244], loss=6.8622
	step [56/244], loss=6.3788
	step [57/244], loss=8.1969
	step [58/244], loss=8.4260
	step [59/244], loss=7.8699
	step [60/244], loss=8.9601
	step [61/244], loss=5.6228
	step [62/244], loss=6.9399
	step [63/244], loss=7.1912
	step [64/244], loss=7.1760
	step [65/244], loss=6.4631
	step [66/244], loss=8.7153
	step [67/244], loss=7.6232
	step [68/244], loss=8.6965
	step [69/244], loss=7.1579
	step [70/244], loss=7.0985
	step [71/244], loss=8.0039
	step [72/244], loss=7.7464
	step [73/244], loss=5.6794
	step [74/244], loss=6.6801
	step [75/244], loss=8.7168
	step [76/244], loss=8.4922
	step [77/244], loss=8.0639
	step [78/244], loss=7.0576
	step [79/244], loss=7.1923
	step [80/244], loss=8.4414
	step [81/244], loss=7.1806
	step [82/244], loss=7.4402
	step [83/244], loss=8.4263
	step [84/244], loss=7.9798
	step [85/244], loss=8.5896
	step [86/244], loss=6.6527
	step [87/244], loss=7.4834
	step [88/244], loss=6.5401
	step [89/244], loss=8.1188
	step [90/244], loss=7.9333
	step [91/244], loss=8.9564
	step [92/244], loss=7.6295
	step [93/244], loss=6.3163
	step [94/244], loss=8.4257
	step [95/244], loss=7.5547
	step [96/244], loss=6.2157
	step [97/244], loss=7.3245
	step [98/244], loss=6.5052
	step [99/244], loss=6.8435
	step [100/244], loss=9.1596
	step [101/244], loss=7.6213
	step [102/244], loss=7.2989
	step [103/244], loss=7.0022
	step [104/244], loss=7.1501
	step [105/244], loss=6.6824
	step [106/244], loss=7.3968
	step [107/244], loss=5.8862
	step [108/244], loss=7.5635
	step [109/244], loss=7.4967
	step [110/244], loss=6.3381
	step [111/244], loss=6.2097
	step [112/244], loss=7.2096
	step [113/244], loss=6.5916
	step [114/244], loss=7.0175
	step [115/244], loss=8.0294
	step [116/244], loss=6.0213
	step [117/244], loss=6.2487
	step [118/244], loss=6.8757
	step [119/244], loss=8.4118
	step [120/244], loss=7.5957
	step [121/244], loss=8.8973
	step [122/244], loss=6.1360
	step [123/244], loss=9.6656
	step [124/244], loss=6.9843
	step [125/244], loss=7.9198
	step [126/244], loss=8.8682
	step [127/244], loss=6.7359
	step [128/244], loss=7.4055
	step [129/244], loss=6.5018
	step [130/244], loss=7.4390
	step [131/244], loss=8.5695
	step [132/244], loss=6.3113
	step [133/244], loss=8.2790
	step [134/244], loss=7.4728
	step [135/244], loss=8.0513
	step [136/244], loss=9.1380
	step [137/244], loss=7.1242
	step [138/244], loss=7.9462
	step [139/244], loss=8.5854
	step [140/244], loss=7.8475
	step [141/244], loss=7.3001
	step [142/244], loss=7.4418
	step [143/244], loss=5.6463
	step [144/244], loss=6.0739
	step [145/244], loss=7.8706
	step [146/244], loss=7.0720
	step [147/244], loss=5.4830
	step [148/244], loss=7.5455
	step [149/244], loss=7.9002
	step [150/244], loss=7.6408
	step [151/244], loss=6.4076
	step [152/244], loss=8.3957
	step [153/244], loss=6.3936
	step [154/244], loss=8.2334
	step [155/244], loss=6.5968
	step [156/244], loss=6.0145
	step [157/244], loss=6.8029
	step [158/244], loss=7.1804
	step [159/244], loss=6.9801
	step [160/244], loss=6.0871
	step [161/244], loss=7.0888
	step [162/244], loss=6.5487
	step [163/244], loss=7.9592
	step [164/244], loss=8.4566
	step [165/244], loss=6.7753
	step [166/244], loss=7.3872
	step [167/244], loss=7.1568
	step [168/244], loss=6.3363
	step [169/244], loss=8.5547
	step [170/244], loss=7.5336
	step [171/244], loss=6.5590
	step [172/244], loss=7.9334
	step [173/244], loss=8.6885
	step [174/244], loss=7.4347
	step [175/244], loss=8.0067
	step [176/244], loss=7.7477
	step [177/244], loss=9.2806
	step [178/244], loss=6.2241
	step [179/244], loss=7.6430
	step [180/244], loss=6.5177
	step [181/244], loss=8.3921
	step [182/244], loss=7.8022
	step [183/244], loss=7.9614
	step [184/244], loss=6.5263
	step [185/244], loss=6.8978
	step [186/244], loss=8.6240
	step [187/244], loss=7.2687
	step [188/244], loss=8.1810
	step [189/244], loss=8.2681
	step [190/244], loss=8.8826
	step [191/244], loss=6.7848
	step [192/244], loss=8.3884
	step [193/244], loss=7.1907
	step [194/244], loss=5.3535
	step [195/244], loss=6.9531
	step [196/244], loss=7.6376
	step [197/244], loss=6.4034
	step [198/244], loss=8.4236
	step [199/244], loss=11.3359
	step [200/244], loss=6.4860
	step [201/244], loss=7.3315
	step [202/244], loss=8.3080
	step [203/244], loss=8.7628
	step [204/244], loss=6.8899
	step [205/244], loss=6.4990
	step [206/244], loss=8.0989
	step [207/244], loss=6.7898
	step [208/244], loss=7.0002
	step [209/244], loss=8.3675
	step [210/244], loss=8.7525
	step [211/244], loss=7.8458
	step [212/244], loss=7.2213
	step [213/244], loss=7.6103
	step [214/244], loss=7.9403
	step [215/244], loss=8.8056
	step [216/244], loss=6.5996
	step [217/244], loss=6.9807
	step [218/244], loss=9.0196
	step [219/244], loss=8.0768
	step [220/244], loss=11.5677
	step [221/244], loss=6.4359
	step [222/244], loss=7.1765
	step [223/244], loss=7.1601
	step [224/244], loss=8.1006
	step [225/244], loss=7.7286
	step [226/244], loss=7.9335
	step [227/244], loss=8.3180
	step [228/244], loss=8.0418
	step [229/244], loss=7.0303
	step [230/244], loss=6.1283
	step [231/244], loss=6.2536
	step [232/244], loss=6.2412
	step [233/244], loss=10.6553
	step [234/244], loss=9.5543
	step [235/244], loss=9.8631
	step [236/244], loss=10.3206
	step [237/244], loss=7.9435
	step [238/244], loss=9.6721
	step [239/244], loss=7.3738
	step [240/244], loss=7.4831
	step [241/244], loss=5.9965
	step [242/244], loss=7.9481
	step [243/244], loss=7.5748
	step [244/244], loss=3.7403
	Evaluating
	loss=0.0208, precision=0.2079, recall=0.9916, f1=0.3438
Training epoch 23
	step [1/244], loss=6.5696
	step [2/244], loss=6.5956
	step [3/244], loss=7.5685
	step [4/244], loss=8.6378
	step [5/244], loss=6.8084
	step [6/244], loss=9.0056
	step [7/244], loss=7.6557
	step [8/244], loss=7.6108
	step [9/244], loss=7.8456
	step [10/244], loss=8.3264
	step [11/244], loss=7.3124
	step [12/244], loss=8.0787
	step [13/244], loss=7.0679
	step [14/244], loss=6.9955
	step [15/244], loss=8.4399
	step [16/244], loss=6.2071
	step [17/244], loss=7.4456
	step [18/244], loss=8.0815
	step [19/244], loss=5.6123
	step [20/244], loss=6.3482
	step [21/244], loss=7.8837
	step [22/244], loss=6.8886
	step [23/244], loss=7.7255
	step [24/244], loss=7.0790
	step [25/244], loss=7.3522
	step [26/244], loss=8.2229
	step [27/244], loss=7.4728
	step [28/244], loss=6.9606
	step [29/244], loss=5.8603
	step [30/244], loss=7.0345
	step [31/244], loss=7.5134
	step [32/244], loss=7.1160
	step [33/244], loss=9.2884
	step [34/244], loss=8.0152
	step [35/244], loss=6.1156
	step [36/244], loss=8.9272
	step [37/244], loss=5.0986
	step [38/244], loss=7.6655
	step [39/244], loss=9.4613
	step [40/244], loss=8.0855
	step [41/244], loss=6.6324
	step [42/244], loss=7.7605
	step [43/244], loss=6.6298
	step [44/244], loss=8.4954
	step [45/244], loss=6.9201
	step [46/244], loss=7.6526
	step [47/244], loss=8.2141
	step [48/244], loss=7.0813
	step [49/244], loss=8.1001
	step [50/244], loss=7.5380
	step [51/244], loss=8.0146
	step [52/244], loss=8.8321
	step [53/244], loss=8.5930
	step [54/244], loss=7.1804
	step [55/244], loss=7.9007
	step [56/244], loss=8.3123
	step [57/244], loss=7.9382
	step [58/244], loss=6.2346
	step [59/244], loss=7.1386
	step [60/244], loss=7.7343
	step [61/244], loss=7.9332
	step [62/244], loss=7.4985
	step [63/244], loss=8.0430
	step [64/244], loss=9.1454
	step [65/244], loss=6.3724
	step [66/244], loss=7.4993
	step [67/244], loss=7.8838
	step [68/244], loss=8.3917
	step [69/244], loss=6.3845
	step [70/244], loss=7.9645
	step [71/244], loss=6.9218
	step [72/244], loss=8.0785
	step [73/244], loss=6.7348
	step [74/244], loss=7.2743
	step [75/244], loss=6.1542
	step [76/244], loss=7.6712
	step [77/244], loss=6.8335
	step [78/244], loss=8.2463
	step [79/244], loss=6.2107
	step [80/244], loss=8.7703
	step [81/244], loss=6.9738
	step [82/244], loss=6.4107
	step [83/244], loss=7.1385
	step [84/244], loss=7.2146
	step [85/244], loss=7.9899
	step [86/244], loss=6.0963
	step [87/244], loss=8.4841
	step [88/244], loss=5.9630
	step [89/244], loss=8.7595
	step [90/244], loss=6.7537
	step [91/244], loss=7.8853
	step [92/244], loss=7.1156
	step [93/244], loss=6.1223
	step [94/244], loss=8.5313
	step [95/244], loss=5.6427
	step [96/244], loss=7.9532
	step [97/244], loss=6.8905
	step [98/244], loss=6.3907
	step [99/244], loss=7.9240
	step [100/244], loss=8.1849
	step [101/244], loss=6.6200
	step [102/244], loss=6.5153
	step [103/244], loss=7.5327
	step [104/244], loss=7.5702
	step [105/244], loss=7.5711
	step [106/244], loss=9.0075
	step [107/244], loss=8.0495
	step [108/244], loss=8.5445
	step [109/244], loss=6.5764
	step [110/244], loss=9.9424
	step [111/244], loss=6.7762
	step [112/244], loss=6.9147
	step [113/244], loss=9.2873
	step [114/244], loss=8.4095
	step [115/244], loss=7.2504
	step [116/244], loss=6.4698
	step [117/244], loss=7.1767
	step [118/244], loss=8.9038
	step [119/244], loss=6.5814
	step [120/244], loss=6.8768
	step [121/244], loss=6.9742
	step [122/244], loss=6.3406
	step [123/244], loss=6.2700
	step [124/244], loss=8.5316
	step [125/244], loss=7.1516
	step [126/244], loss=7.3950
	step [127/244], loss=7.4401
	step [128/244], loss=8.3181
	step [129/244], loss=7.7397
	step [130/244], loss=7.4210
	step [131/244], loss=6.0941
	step [132/244], loss=6.0980
	step [133/244], loss=6.4295
	step [134/244], loss=6.3001
	step [135/244], loss=5.6593
	step [136/244], loss=7.1179
	step [137/244], loss=8.2427
	step [138/244], loss=8.4054
	step [139/244], loss=6.5376
	step [140/244], loss=6.2461
	step [141/244], loss=7.4471
	step [142/244], loss=5.7305
	step [143/244], loss=7.4302
	step [144/244], loss=7.3830
	step [145/244], loss=7.1610
	step [146/244], loss=6.8181
	step [147/244], loss=7.2865
	step [148/244], loss=6.4339
	step [149/244], loss=6.6967
	step [150/244], loss=7.3885
	step [151/244], loss=6.0904
	step [152/244], loss=9.4894
	step [153/244], loss=7.4168
	step [154/244], loss=7.2924
	step [155/244], loss=6.8771
	step [156/244], loss=7.2951
	step [157/244], loss=6.8259
	step [158/244], loss=5.9620
	step [159/244], loss=6.3965
	step [160/244], loss=6.9458
	step [161/244], loss=9.7197
	step [162/244], loss=7.0237
	step [163/244], loss=6.3884
	step [164/244], loss=6.2380
	step [165/244], loss=7.5271
	step [166/244], loss=7.2903
	step [167/244], loss=8.4048
	step [168/244], loss=7.9114
	step [169/244], loss=6.7330
	step [170/244], loss=7.4901
	step [171/244], loss=6.7948
	step [172/244], loss=6.4549
	step [173/244], loss=6.4558
	step [174/244], loss=9.4468
	step [175/244], loss=6.2222
	step [176/244], loss=8.8231
	step [177/244], loss=8.0012
	step [178/244], loss=7.0076
	step [179/244], loss=6.4587
	step [180/244], loss=7.1345
	step [181/244], loss=7.7939
	step [182/244], loss=7.7916
	step [183/244], loss=9.5662
	step [184/244], loss=6.6094
	step [185/244], loss=7.7738
	step [186/244], loss=10.2018
	step [187/244], loss=8.4477
	step [188/244], loss=8.1754
	step [189/244], loss=8.4169
	step [190/244], loss=6.6292
	step [191/244], loss=6.6029
	step [192/244], loss=7.5374
	step [193/244], loss=8.5777
	step [194/244], loss=8.0092
	step [195/244], loss=7.2749
	step [196/244], loss=5.9346
	step [197/244], loss=6.7051
	step [198/244], loss=6.8250
	step [199/244], loss=8.8387
	step [200/244], loss=6.7657
	step [201/244], loss=9.3147
	step [202/244], loss=7.2611
	step [203/244], loss=8.1003
	step [204/244], loss=8.4338
	step [205/244], loss=7.9721
	step [206/244], loss=6.7618
	step [207/244], loss=6.5585
	step [208/244], loss=6.7022
	step [209/244], loss=6.8343
	step [210/244], loss=6.7780
	step [211/244], loss=8.0753
	step [212/244], loss=7.3428
	step [213/244], loss=10.3846
	step [214/244], loss=6.1126
	step [215/244], loss=6.6697
	step [216/244], loss=7.8094
	step [217/244], loss=7.4662
	step [218/244], loss=7.6376
	step [219/244], loss=6.8535
	step [220/244], loss=9.8340
	step [221/244], loss=7.7722
	step [222/244], loss=6.1286
	step [223/244], loss=8.2669
	step [224/244], loss=6.6381
	step [225/244], loss=6.2031
	step [226/244], loss=6.8049
	step [227/244], loss=8.7554
	step [228/244], loss=8.2683
	step [229/244], loss=7.1557
	step [230/244], loss=7.3646
	step [231/244], loss=6.1055
	step [232/244], loss=7.0632
	step [233/244], loss=7.5641
	step [234/244], loss=6.9020
	step [235/244], loss=6.5527
	step [236/244], loss=7.2042
	step [237/244], loss=5.9423
	step [238/244], loss=6.7178
	step [239/244], loss=5.9257
	step [240/244], loss=7.8633
	step [241/244], loss=7.5053
	step [242/244], loss=9.0477
	step [243/244], loss=8.3415
	step [244/244], loss=2.1962
	Evaluating
	loss=0.0198, precision=0.2046, recall=0.9911, f1=0.3392
Training epoch 24
	step [1/244], loss=6.8640
	step [2/244], loss=7.0475
	step [3/244], loss=7.2710
	step [4/244], loss=6.7318
	step [5/244], loss=9.7785
	step [6/244], loss=6.2614
	step [7/244], loss=6.5632
	step [8/244], loss=8.2454
	step [9/244], loss=8.8845
	step [10/244], loss=9.4650
	step [11/244], loss=6.9243
	step [12/244], loss=7.9855
	step [13/244], loss=7.3051
	step [14/244], loss=5.4894
	step [15/244], loss=8.2653
	step [16/244], loss=6.8107
	step [17/244], loss=5.6657
	step [18/244], loss=7.6781
	step [19/244], loss=7.3778
	step [20/244], loss=9.5277
	step [21/244], loss=8.0810
	step [22/244], loss=6.0068
	step [23/244], loss=6.4267
	step [24/244], loss=8.5607
	step [25/244], loss=5.6868
	step [26/244], loss=8.1196
	step [27/244], loss=6.0512
	step [28/244], loss=6.7603
	step [29/244], loss=7.0139
	step [30/244], loss=8.6652
	step [31/244], loss=8.6938
	step [32/244], loss=9.2811
	step [33/244], loss=7.2386
	step [34/244], loss=7.0953
	step [35/244], loss=8.0722
	step [36/244], loss=8.3844
	step [37/244], loss=8.0095
	step [38/244], loss=8.0145
	step [39/244], loss=7.8895
	step [40/244], loss=8.8453
	step [41/244], loss=7.4411
	step [42/244], loss=6.0013
	step [43/244], loss=7.8154
	step [44/244], loss=7.7884
	step [45/244], loss=6.8894
	step [46/244], loss=5.8632
	step [47/244], loss=6.8630
	step [48/244], loss=7.3567
	step [49/244], loss=6.6844
	step [50/244], loss=5.5532
	step [51/244], loss=6.8602
	step [52/244], loss=7.3918
	step [53/244], loss=8.5033
	step [54/244], loss=6.2909
	step [55/244], loss=6.0121
	step [56/244], loss=8.1356
	step [57/244], loss=6.7435
	step [58/244], loss=6.3199
	step [59/244], loss=6.3118
	step [60/244], loss=6.4523
	step [61/244], loss=6.9725
	step [62/244], loss=7.3724
	step [63/244], loss=7.4969
	step [64/244], loss=7.0392
	step [65/244], loss=7.3398
	step [66/244], loss=7.9915
	step [67/244], loss=7.3020
	step [68/244], loss=6.1369
	step [69/244], loss=7.1614
	step [70/244], loss=6.6611
	step [71/244], loss=6.6502
	step [72/244], loss=7.8529
	step [73/244], loss=8.5153
	step [74/244], loss=6.1134
	step [75/244], loss=6.7273
	step [76/244], loss=8.2140
	step [77/244], loss=6.6749
	step [78/244], loss=7.1002
	step [79/244], loss=7.6570
	step [80/244], loss=6.7310
	step [81/244], loss=8.1939
	step [82/244], loss=7.2340
	step [83/244], loss=8.0441
	step [84/244], loss=7.2020
	step [85/244], loss=8.5221
	step [86/244], loss=6.1213
	step [87/244], loss=5.3483
	step [88/244], loss=8.0098
	step [89/244], loss=6.8521
	step [90/244], loss=7.7836
	step [91/244], loss=9.3844
	step [92/244], loss=6.3150
	step [93/244], loss=6.6129
	step [94/244], loss=8.8266
	step [95/244], loss=8.1254
	step [96/244], loss=6.7013
	step [97/244], loss=6.8615
	step [98/244], loss=7.4148
	step [99/244], loss=6.2715
	step [100/244], loss=8.0842
	step [101/244], loss=8.6405
	step [102/244], loss=6.0007
	step [103/244], loss=8.3242
	step [104/244], loss=7.3981
	step [105/244], loss=7.7152
	step [106/244], loss=5.8018
	step [107/244], loss=6.7452
	step [108/244], loss=5.9590
	step [109/244], loss=7.3026
	step [110/244], loss=7.3417
	step [111/244], loss=7.7100
	step [112/244], loss=8.1170
	step [113/244], loss=7.0418
	step [114/244], loss=8.1428
	step [115/244], loss=6.3858
	step [116/244], loss=7.0919
	step [117/244], loss=8.1716
	step [118/244], loss=6.3011
	step [119/244], loss=6.1844
	step [120/244], loss=5.8894
	step [121/244], loss=7.1564
	step [122/244], loss=7.8280
	step [123/244], loss=6.2279
	step [124/244], loss=7.4058
	step [125/244], loss=6.9548
	step [126/244], loss=6.4656
	step [127/244], loss=5.8018
	step [128/244], loss=6.9284
	step [129/244], loss=9.6185
	step [130/244], loss=7.4130
	step [131/244], loss=6.1911
	step [132/244], loss=6.8683
	step [133/244], loss=7.4552
	step [134/244], loss=6.7110
	step [135/244], loss=6.7201
	step [136/244], loss=7.2018
	step [137/244], loss=8.1288
	step [138/244], loss=6.0384
	step [139/244], loss=7.2232
	step [140/244], loss=6.4649
	step [141/244], loss=5.9951
	step [142/244], loss=8.3740
	step [143/244], loss=6.8322
	step [144/244], loss=8.3801
	step [145/244], loss=5.9741
	step [146/244], loss=5.9426
	step [147/244], loss=5.9636
	step [148/244], loss=6.3307
	step [149/244], loss=6.2914
	step [150/244], loss=6.4734
	step [151/244], loss=7.5777
	step [152/244], loss=6.3036
	step [153/244], loss=6.5473
	step [154/244], loss=8.1792
	step [155/244], loss=7.0392
	step [156/244], loss=5.2483
	step [157/244], loss=8.3930
	step [158/244], loss=5.7669
	step [159/244], loss=7.7425
	step [160/244], loss=6.2392
	step [161/244], loss=7.7556
	step [162/244], loss=5.5862
	step [163/244], loss=7.1420
	step [164/244], loss=6.4351
	step [165/244], loss=7.2096
	step [166/244], loss=6.0256
	step [167/244], loss=7.3496
	step [168/244], loss=6.3811
	step [169/244], loss=6.2412
	step [170/244], loss=7.0101
	step [171/244], loss=7.5963
	step [172/244], loss=7.1202
	step [173/244], loss=8.2830
	step [174/244], loss=7.7061
	step [175/244], loss=6.3965
	step [176/244], loss=6.3621
	step [177/244], loss=7.0654
	step [178/244], loss=8.0334
	step [179/244], loss=7.4732
	step [180/244], loss=6.7532
	step [181/244], loss=7.2472
	step [182/244], loss=7.3864
	step [183/244], loss=6.9888
	step [184/244], loss=7.9714
	step [185/244], loss=6.4691
	step [186/244], loss=9.1096
	step [187/244], loss=6.8811
	step [188/244], loss=7.7710
	step [189/244], loss=5.7213
	step [190/244], loss=5.3967
	step [191/244], loss=7.6915
	step [192/244], loss=6.2498
	step [193/244], loss=7.5763
	step [194/244], loss=7.5714
	step [195/244], loss=6.3657
	step [196/244], loss=8.6825
	step [197/244], loss=7.0350
	step [198/244], loss=5.5926
	step [199/244], loss=6.9764
	step [200/244], loss=7.7140
	step [201/244], loss=7.3487
	step [202/244], loss=8.2821
	step [203/244], loss=7.3954
	step [204/244], loss=6.7495
	step [205/244], loss=6.8144
	step [206/244], loss=7.3635
	step [207/244], loss=7.2997
	step [208/244], loss=6.5574
	step [209/244], loss=5.8498
	step [210/244], loss=6.8083
	step [211/244], loss=6.7251
	step [212/244], loss=7.7442
	step [213/244], loss=5.6327
	step [214/244], loss=8.3513
	step [215/244], loss=5.9310
	step [216/244], loss=10.3358
	step [217/244], loss=6.0496
	step [218/244], loss=6.0671
	step [219/244], loss=6.4797
	step [220/244], loss=6.7626
	step [221/244], loss=6.9971
	step [222/244], loss=7.6476
	step [223/244], loss=7.6638
	step [224/244], loss=5.8893
	step [225/244], loss=7.7791
	step [226/244], loss=6.8177
	step [227/244], loss=7.9472
	step [228/244], loss=7.6612
	step [229/244], loss=5.5659
	step [230/244], loss=7.3042
	step [231/244], loss=9.2697
	step [232/244], loss=7.9299
	step [233/244], loss=8.1788
	step [234/244], loss=6.9435
	step [235/244], loss=7.2513
	step [236/244], loss=7.3102
	step [237/244], loss=8.0970
	step [238/244], loss=6.1441
	step [239/244], loss=8.6872
	step [240/244], loss=8.8498
	step [241/244], loss=6.3525
	step [242/244], loss=6.4706
	step [243/244], loss=10.1770
	step [244/244], loss=4.1195
	Evaluating
	loss=0.0186, precision=0.2147, recall=0.9896, f1=0.3528
saving model as: 1_saved_model.pth
Training epoch 25
	step [1/244], loss=5.7255
	step [2/244], loss=6.5625
	step [3/244], loss=8.5933
	step [4/244], loss=6.8402
	step [5/244], loss=5.1518
	step [6/244], loss=5.2249
	step [7/244], loss=6.1133
	step [8/244], loss=7.1420
	step [9/244], loss=7.7661
	step [10/244], loss=5.1315
	step [11/244], loss=6.6418
	step [12/244], loss=11.6382
	step [13/244], loss=6.8914
	step [14/244], loss=7.0895
	step [15/244], loss=8.3115
	step [16/244], loss=8.5183
	step [17/244], loss=7.4633
	step [18/244], loss=5.9825
	step [19/244], loss=6.8196
	step [20/244], loss=8.7374
	step [21/244], loss=6.3255
	step [22/244], loss=9.2642
	step [23/244], loss=7.1187
	step [24/244], loss=6.7883
	step [25/244], loss=7.6538
	step [26/244], loss=6.9783
	step [27/244], loss=6.4893
	step [28/244], loss=7.0313
	step [29/244], loss=5.6293
	step [30/244], loss=7.0113
	step [31/244], loss=6.4539
	step [32/244], loss=5.9554
	step [33/244], loss=7.8692
	step [34/244], loss=7.4294
	step [35/244], loss=8.8532
	step [36/244], loss=6.4356
	step [37/244], loss=7.1332
	step [38/244], loss=7.2290
	step [39/244], loss=6.5612
	step [40/244], loss=6.9156
	step [41/244], loss=8.0238
	step [42/244], loss=6.6249
	step [43/244], loss=6.8595
	step [44/244], loss=6.7384
	step [45/244], loss=6.7361
	step [46/244], loss=6.4381
	step [47/244], loss=8.8958
	step [48/244], loss=6.4052
	step [49/244], loss=7.3059
	step [50/244], loss=6.8276
	step [51/244], loss=7.4671
	step [52/244], loss=6.9913
	step [53/244], loss=5.8985
	step [54/244], loss=6.1678
	step [55/244], loss=6.8335
	step [56/244], loss=6.5283
	step [57/244], loss=7.2375
	step [58/244], loss=8.8036
	step [59/244], loss=7.4183
	step [60/244], loss=5.8121
	step [61/244], loss=6.8468
	step [62/244], loss=6.4227
	step [63/244], loss=7.4386
	step [64/244], loss=7.4982
	step [65/244], loss=6.3808
	step [66/244], loss=8.7930
	step [67/244], loss=6.2900
	step [68/244], loss=8.2682
	step [69/244], loss=7.8467
	step [70/244], loss=6.4671
	step [71/244], loss=7.3021
	step [72/244], loss=8.9029
	step [73/244], loss=6.7805
	step [74/244], loss=7.2585
	step [75/244], loss=8.2729
	step [76/244], loss=6.2533
	step [77/244], loss=6.8039
	step [78/244], loss=8.9390
	step [79/244], loss=8.1918
	step [80/244], loss=7.3569
	step [81/244], loss=5.7587
	step [82/244], loss=6.6160
	step [83/244], loss=6.9709
	step [84/244], loss=5.8446
	step [85/244], loss=8.2137
	step [86/244], loss=9.1921
	step [87/244], loss=6.0167
	step [88/244], loss=7.4710
	step [89/244], loss=5.8965
	step [90/244], loss=7.0918
	step [91/244], loss=6.6643
	step [92/244], loss=7.9770
	step [93/244], loss=6.6497
	step [94/244], loss=8.4573
	step [95/244], loss=8.0566
	step [96/244], loss=6.6760
	step [97/244], loss=6.5460
	step [98/244], loss=6.7001
	step [99/244], loss=7.5826
	step [100/244], loss=6.8078
	step [101/244], loss=7.4542
	step [102/244], loss=5.7222
	step [103/244], loss=7.7327
	step [104/244], loss=6.4717
	step [105/244], loss=6.3251
	step [106/244], loss=7.4620
	step [107/244], loss=6.4536
	step [108/244], loss=7.7467
	step [109/244], loss=7.6015
	step [110/244], loss=7.2694
	step [111/244], loss=8.0410
	step [112/244], loss=7.5287
	step [113/244], loss=6.1333
	step [114/244], loss=6.2135
	step [115/244], loss=5.1708
	step [116/244], loss=6.3042
	step [117/244], loss=7.9962
	step [118/244], loss=6.2336
	step [119/244], loss=6.8549
	step [120/244], loss=6.2771
	step [121/244], loss=7.1434
	step [122/244], loss=8.3571
	step [123/244], loss=7.6106
	step [124/244], loss=6.5442
	step [125/244], loss=6.9534
	step [126/244], loss=6.8785
	step [127/244], loss=6.5076
	step [128/244], loss=7.2479
	step [129/244], loss=5.7681
	step [130/244], loss=7.1526
	step [131/244], loss=6.5321
	step [132/244], loss=5.7346
	step [133/244], loss=6.0101
	step [134/244], loss=6.9453
	step [135/244], loss=6.6570
	step [136/244], loss=6.6283
	step [137/244], loss=7.0316
	step [138/244], loss=7.2583
	step [139/244], loss=7.5721
	step [140/244], loss=6.2713
	step [141/244], loss=6.1876
	step [142/244], loss=6.5305
	step [143/244], loss=6.5644
	step [144/244], loss=7.8374
	step [145/244], loss=5.7113
	step [146/244], loss=6.7563
	step [147/244], loss=6.7949
	step [148/244], loss=6.8929
	step [149/244], loss=6.9461
	step [150/244], loss=9.1905
	step [151/244], loss=6.9876
	step [152/244], loss=6.8614
	step [153/244], loss=6.9019
	step [154/244], loss=6.6194
	step [155/244], loss=7.5553
	step [156/244], loss=6.7172
	step [157/244], loss=7.1381
	step [158/244], loss=7.6601
	step [159/244], loss=6.1058
	step [160/244], loss=6.0868
	step [161/244], loss=9.0165
	step [162/244], loss=7.3064
	step [163/244], loss=6.7461
	step [164/244], loss=7.4292
	step [165/244], loss=7.9758
	step [166/244], loss=5.8050
	step [167/244], loss=7.8841
	step [168/244], loss=6.5487
	step [169/244], loss=6.1311
	step [170/244], loss=6.6194
	step [171/244], loss=7.2064
	step [172/244], loss=7.0154
	step [173/244], loss=6.4515
	step [174/244], loss=5.9594
	step [175/244], loss=5.7627
	step [176/244], loss=6.5296
	step [177/244], loss=6.3408
	step [178/244], loss=6.1728
	step [179/244], loss=5.1298
	step [180/244], loss=6.1092
	step [181/244], loss=6.3089
	step [182/244], loss=8.4877
	step [183/244], loss=6.6614
	step [184/244], loss=6.4822
	step [185/244], loss=7.5366
	step [186/244], loss=6.5836
	step [187/244], loss=7.7874
	step [188/244], loss=6.5843
	step [189/244], loss=6.5727
	step [190/244], loss=5.5684
	step [191/244], loss=5.8244
	step [192/244], loss=7.2064
	step [193/244], loss=5.2139
	step [194/244], loss=8.3041
	step [195/244], loss=6.5371
	step [196/244], loss=6.7698
	step [197/244], loss=6.7948
	step [198/244], loss=8.5689
	step [199/244], loss=6.5720
	step [200/244], loss=6.7189
	step [201/244], loss=6.8640
	step [202/244], loss=7.1160
	step [203/244], loss=6.4429
	step [204/244], loss=6.7045
	step [205/244], loss=7.8807
	step [206/244], loss=6.6222
	step [207/244], loss=6.5533
	step [208/244], loss=5.3698
	step [209/244], loss=5.5613
	step [210/244], loss=6.7776
	step [211/244], loss=8.1675
	step [212/244], loss=7.7731
	step [213/244], loss=5.3387
	step [214/244], loss=8.4453
	step [215/244], loss=6.6932
	step [216/244], loss=6.8528
	step [217/244], loss=7.0016
	step [218/244], loss=7.2679
	step [219/244], loss=7.7364
	step [220/244], loss=5.4057
	step [221/244], loss=6.8827
	step [222/244], loss=8.1245
	step [223/244], loss=5.6993
	step [224/244], loss=7.1131
	step [225/244], loss=5.4139
	step [226/244], loss=7.4733
	step [227/244], loss=6.6914
	step [228/244], loss=8.0240
	step [229/244], loss=7.3583
	step [230/244], loss=5.9521
	step [231/244], loss=6.0093
	step [232/244], loss=8.3106
	step [233/244], loss=8.3457
	step [234/244], loss=6.9983
	step [235/244], loss=6.9850
	step [236/244], loss=7.5198
	step [237/244], loss=7.2139
	step [238/244], loss=7.4815
	step [239/244], loss=7.2657
	step [240/244], loss=6.7592
	step [241/244], loss=6.5348
	step [242/244], loss=6.1001
	step [243/244], loss=8.5197
	step [244/244], loss=3.1526
	Evaluating
	loss=0.0177, precision=0.2154, recall=0.9908, f1=0.3539
saving model as: 1_saved_model.pth
Training epoch 26
	step [1/244], loss=6.3874
	step [2/244], loss=7.2700
	step [3/244], loss=5.8834
	step [4/244], loss=8.4255
	step [5/244], loss=4.8429
	step [6/244], loss=6.1406
	step [7/244], loss=8.8963
	step [8/244], loss=6.4281
	step [9/244], loss=5.5473
	step [10/244], loss=5.3975
	step [11/244], loss=5.6922
	step [12/244], loss=7.8293
	step [13/244], loss=6.5431
	step [14/244], loss=7.0045
	step [15/244], loss=6.8025
	step [16/244], loss=6.1622
	step [17/244], loss=6.2121
	step [18/244], loss=6.4460
	step [19/244], loss=5.6862
	step [20/244], loss=7.1658
	step [21/244], loss=6.5413
	step [22/244], loss=6.5447
	step [23/244], loss=7.0308
	step [24/244], loss=7.0519
	step [25/244], loss=6.1405
	step [26/244], loss=5.9490
	step [27/244], loss=7.6088
	step [28/244], loss=6.6575
	step [29/244], loss=6.6279
	step [30/244], loss=7.3292
	step [31/244], loss=6.5729
	step [32/244], loss=7.2901
	step [33/244], loss=5.4617
	step [34/244], loss=6.3806
	step [35/244], loss=6.5365
	step [36/244], loss=6.7603
	step [37/244], loss=6.2340
	step [38/244], loss=5.5380
	step [39/244], loss=8.2400
	step [40/244], loss=6.4059
	step [41/244], loss=8.1187
	step [42/244], loss=7.5018
	step [43/244], loss=6.3250
	step [44/244], loss=7.1040
	step [45/244], loss=7.2781
	step [46/244], loss=6.0807
	step [47/244], loss=6.1517
	step [48/244], loss=7.2965
	step [49/244], loss=6.2795
	step [50/244], loss=7.9107
	step [51/244], loss=5.9958
	step [52/244], loss=6.3252
	step [53/244], loss=6.2414
	step [54/244], loss=7.6970
	step [55/244], loss=6.4740
	step [56/244], loss=6.1763
	step [57/244], loss=6.2962
	step [58/244], loss=6.7026
	step [59/244], loss=7.5032
	step [60/244], loss=6.3282
	step [61/244], loss=6.0295
	step [62/244], loss=6.4438
	step [63/244], loss=6.7153
	step [64/244], loss=6.8436
	step [65/244], loss=6.7740
	step [66/244], loss=6.1304
	step [67/244], loss=6.0054
	step [68/244], loss=5.8891
	step [69/244], loss=7.9333
	step [70/244], loss=6.8373
	step [71/244], loss=7.1469
	step [72/244], loss=6.7221
	step [73/244], loss=7.4261
	step [74/244], loss=7.7480
	step [75/244], loss=6.1684
	step [76/244], loss=7.3039
	step [77/244], loss=6.3314
	step [78/244], loss=5.1629
	step [79/244], loss=6.2515
	step [80/244], loss=7.9742
	step [81/244], loss=7.3235
	step [82/244], loss=7.5910
	step [83/244], loss=7.3786
	step [84/244], loss=7.4140
	step [85/244], loss=6.9189
	step [86/244], loss=7.1312
	step [87/244], loss=5.5118
	step [88/244], loss=7.5331
	step [89/244], loss=6.8005
	step [90/244], loss=5.4751
	step [91/244], loss=7.0754
	step [92/244], loss=7.2966
	step [93/244], loss=6.8136
	step [94/244], loss=7.3973
	step [95/244], loss=6.3781
	step [96/244], loss=6.0432
	step [97/244], loss=7.1690
	step [98/244], loss=6.5178
	step [99/244], loss=6.4635
	step [100/244], loss=5.3635
	step [101/244], loss=3.9669
	step [102/244], loss=6.9783
	step [103/244], loss=6.4008
	step [104/244], loss=6.9157
	step [105/244], loss=8.0348
	step [106/244], loss=5.6352
	step [107/244], loss=7.8488
	step [108/244], loss=5.8507
	step [109/244], loss=5.7187
	step [110/244], loss=6.8873
	step [111/244], loss=8.4624
	step [112/244], loss=5.8321
	step [113/244], loss=7.0044
	step [114/244], loss=6.6502
	step [115/244], loss=7.8312
	step [116/244], loss=6.3745
	step [117/244], loss=6.9051
	step [118/244], loss=8.9251
	step [119/244], loss=5.5824
	step [120/244], loss=7.0636
	step [121/244], loss=5.3352
	step [122/244], loss=6.0807
	step [123/244], loss=5.9345
	step [124/244], loss=5.4814
	step [125/244], loss=7.4162
	step [126/244], loss=6.1186
	step [127/244], loss=5.0485
	step [128/244], loss=6.2219
	step [129/244], loss=6.1923
	step [130/244], loss=6.2295
	step [131/244], loss=5.8066
	step [132/244], loss=5.9992
	step [133/244], loss=6.7941
	step [134/244], loss=7.1565
	step [135/244], loss=6.8841
	step [136/244], loss=6.8115
	step [137/244], loss=6.1941
	step [138/244], loss=7.0077
	step [139/244], loss=5.8454
	step [140/244], loss=6.4357
	step [141/244], loss=7.3282
	step [142/244], loss=6.0649
	step [143/244], loss=6.4218
	step [144/244], loss=6.4956
	step [145/244], loss=5.9177
	step [146/244], loss=5.3561
	step [147/244], loss=6.6513
	step [148/244], loss=5.5581
	step [149/244], loss=7.1153
	step [150/244], loss=6.3326
	step [151/244], loss=4.6643
	step [152/244], loss=7.0964
	step [153/244], loss=6.1717
	step [154/244], loss=7.1545
	step [155/244], loss=6.6792
	step [156/244], loss=6.2620
	step [157/244], loss=5.4749
	step [158/244], loss=6.2163
	step [159/244], loss=6.5609
	step [160/244], loss=6.3712
	step [161/244], loss=6.6508
	step [162/244], loss=6.9522
	step [163/244], loss=7.0919
	step [164/244], loss=6.5762
	step [165/244], loss=5.0228
	step [166/244], loss=7.1003
	step [167/244], loss=6.2802
	step [168/244], loss=5.4469
	step [169/244], loss=6.9899
	step [170/244], loss=6.4158
	step [171/244], loss=6.9164
	step [172/244], loss=6.4505
	step [173/244], loss=5.9954
	step [174/244], loss=5.4346
	step [175/244], loss=6.9960
	step [176/244], loss=6.6768
	step [177/244], loss=7.7846
	step [178/244], loss=4.3088
	step [179/244], loss=7.1446
	step [180/244], loss=5.7904
	step [181/244], loss=5.3564
	step [182/244], loss=6.2708
	step [183/244], loss=7.5461
	step [184/244], loss=6.5402
	step [185/244], loss=6.2077
	step [186/244], loss=6.4546
	step [187/244], loss=6.5691
	step [188/244], loss=7.7351
	step [189/244], loss=6.0794
	step [190/244], loss=6.1160
	step [191/244], loss=7.6825
	step [192/244], loss=7.9486
	step [193/244], loss=6.1791
	step [194/244], loss=8.1548
	step [195/244], loss=6.4582
	step [196/244], loss=5.7010
	step [197/244], loss=7.1048
	step [198/244], loss=6.1361
	step [199/244], loss=7.0486
	step [200/244], loss=6.4785
	step [201/244], loss=5.8821
	step [202/244], loss=8.3487
	step [203/244], loss=6.5047
	step [204/244], loss=6.1846
	step [205/244], loss=6.4388
	step [206/244], loss=7.0594
	step [207/244], loss=6.4516
	step [208/244], loss=6.4116
	step [209/244], loss=6.5979
	step [210/244], loss=6.8138
	step [211/244], loss=6.5735
	step [212/244], loss=7.0476
	step [213/244], loss=8.4635
	step [214/244], loss=6.6154
	step [215/244], loss=7.2075
	step [216/244], loss=6.7630
	step [217/244], loss=6.7750
	step [218/244], loss=7.9108
	step [219/244], loss=5.8509
	step [220/244], loss=6.2596
	step [221/244], loss=7.0820
	step [222/244], loss=6.7317
	step [223/244], loss=7.6151
	step [224/244], loss=6.3202
	step [225/244], loss=5.9817
	step [226/244], loss=6.2058
	step [227/244], loss=6.9719
	step [228/244], loss=5.8550
	step [229/244], loss=5.8963
	step [230/244], loss=7.2039
	step [231/244], loss=6.9926
	step [232/244], loss=5.9070
	step [233/244], loss=7.0169
	step [234/244], loss=8.5051
	step [235/244], loss=6.5010
	step [236/244], loss=6.6951
	step [237/244], loss=6.1491
	step [238/244], loss=7.8479
	step [239/244], loss=8.0116
	step [240/244], loss=7.3790
	step [241/244], loss=7.5089
	step [242/244], loss=5.7979
	step [243/244], loss=6.6077
	step [244/244], loss=1.9056
	Evaluating
	loss=0.0168, precision=0.2279, recall=0.9891, f1=0.3705
saving model as: 1_saved_model.pth
Training epoch 27
	step [1/244], loss=7.1368
	step [2/244], loss=6.5908
	step [3/244], loss=7.2308
	step [4/244], loss=6.3645
	step [5/244], loss=6.2289
	step [6/244], loss=6.2227
	step [7/244], loss=6.4028
	step [8/244], loss=7.3720
	step [9/244], loss=5.1659
	step [10/244], loss=4.9864
	step [11/244], loss=6.9663
	step [12/244], loss=5.2330
	step [13/244], loss=5.3594
	step [14/244], loss=7.1743
	step [15/244], loss=5.5529
	step [16/244], loss=6.5005
	step [17/244], loss=5.9304
	step [18/244], loss=6.1777
	step [19/244], loss=6.4662
	step [20/244], loss=6.5543
	step [21/244], loss=6.8679
	step [22/244], loss=5.0990
	step [23/244], loss=7.3960
	step [24/244], loss=7.6551
	step [25/244], loss=6.7378
	step [26/244], loss=7.3980
	step [27/244], loss=5.3440
	step [28/244], loss=7.2177
	step [29/244], loss=6.2360
	step [30/244], loss=7.0084
	step [31/244], loss=7.3783
	step [32/244], loss=6.6010
	step [33/244], loss=7.4964
	step [34/244], loss=7.2987
	step [35/244], loss=6.4269
	step [36/244], loss=5.8294
	step [37/244], loss=5.7409
	step [38/244], loss=6.8580
	step [39/244], loss=7.3854
	step [40/244], loss=6.7016
	step [41/244], loss=7.2658
	step [42/244], loss=6.6812
	step [43/244], loss=6.9291
	step [44/244], loss=8.1967
	step [45/244], loss=6.7207
	step [46/244], loss=5.0405
	step [47/244], loss=7.0947
	step [48/244], loss=5.8151
	step [49/244], loss=6.0688
	step [50/244], loss=6.3572
	step [51/244], loss=6.3449
	step [52/244], loss=7.7665
	step [53/244], loss=6.3832
	step [54/244], loss=7.6040
	step [55/244], loss=7.3848
	step [56/244], loss=7.4956
	step [57/244], loss=7.2057
	step [58/244], loss=6.4636
	step [59/244], loss=7.3341
	step [60/244], loss=5.9923
	step [61/244], loss=6.1321
	step [62/244], loss=7.5771
	step [63/244], loss=6.5248
	step [64/244], loss=5.1749
	step [65/244], loss=8.5683
	step [66/244], loss=6.7213
	step [67/244], loss=6.0224
	step [68/244], loss=6.8246
	step [69/244], loss=6.6677
	step [70/244], loss=8.2442
	step [71/244], loss=6.7422
	step [72/244], loss=5.5345
	step [73/244], loss=6.4257
	step [74/244], loss=6.7137
	step [75/244], loss=7.0085
	step [76/244], loss=6.2262
	step [77/244], loss=7.0910
	step [78/244], loss=6.8963
	step [79/244], loss=8.0147
	step [80/244], loss=5.3721
	step [81/244], loss=6.7311
	step [82/244], loss=8.9270
	step [83/244], loss=6.8996
	step [84/244], loss=7.1710
	step [85/244], loss=6.9124
	step [86/244], loss=6.0176
	step [87/244], loss=6.6067
	step [88/244], loss=6.7424
	step [89/244], loss=6.6640
	step [90/244], loss=6.7755
	step [91/244], loss=6.4047
	step [92/244], loss=5.7849
	step [93/244], loss=5.7072
	step [94/244], loss=6.6337
	step [95/244], loss=5.1706
	step [96/244], loss=6.6821
	step [97/244], loss=6.7004
	step [98/244], loss=5.9928
	step [99/244], loss=5.6853
	step [100/244], loss=6.8314
	step [101/244], loss=5.7570
	step [102/244], loss=6.7226
	step [103/244], loss=5.4153
	step [104/244], loss=6.0425
	step [105/244], loss=5.3953
	step [106/244], loss=7.3459
	step [107/244], loss=7.0953
	step [108/244], loss=5.8818
	step [109/244], loss=7.4346
	step [110/244], loss=7.4561
	step [111/244], loss=6.7936
	step [112/244], loss=6.0105
	step [113/244], loss=5.9177
	step [114/244], loss=4.7693
	step [115/244], loss=6.0741
	step [116/244], loss=6.2170
	step [117/244], loss=7.1974
	step [118/244], loss=8.3824
	step [119/244], loss=5.6273
	step [120/244], loss=7.8068
	step [121/244], loss=5.4839
	step [122/244], loss=6.5991
	step [123/244], loss=6.0680
	step [124/244], loss=6.2188
	step [125/244], loss=8.3881
	step [126/244], loss=6.4136
	step [127/244], loss=7.1125
	step [128/244], loss=5.3576
	step [129/244], loss=5.9462
	step [130/244], loss=5.8003
	step [131/244], loss=6.4198
	step [132/244], loss=6.3536
	step [133/244], loss=6.6590
	step [134/244], loss=6.5202
	step [135/244], loss=7.1854
	step [136/244], loss=7.1375
	step [137/244], loss=5.9806
	step [138/244], loss=6.3525
	step [139/244], loss=5.3635
	step [140/244], loss=5.2568
	step [141/244], loss=5.7510
	step [142/244], loss=6.4128
	step [143/244], loss=7.6532
	step [144/244], loss=6.4107
	step [145/244], loss=7.1434
	step [146/244], loss=6.0819
	step [147/244], loss=5.8127
	step [148/244], loss=5.6118
	step [149/244], loss=5.9530
	step [150/244], loss=6.3977
	step [151/244], loss=7.2802
	step [152/244], loss=5.7934
	step [153/244], loss=7.0469
	step [154/244], loss=6.5753
	step [155/244], loss=6.7606
	step [156/244], loss=6.4890
	step [157/244], loss=6.5912
	step [158/244], loss=6.6686
	step [159/244], loss=5.5679
	step [160/244], loss=5.6803
	step [161/244], loss=5.5037
	step [162/244], loss=8.0873
	step [163/244], loss=5.8730
	step [164/244], loss=5.7170
	step [165/244], loss=6.3314
	step [166/244], loss=6.6591
	step [167/244], loss=6.3148
	step [168/244], loss=7.2762
	step [169/244], loss=6.2111
	step [170/244], loss=6.9087
	step [171/244], loss=7.4003
	step [172/244], loss=7.0652
	step [173/244], loss=5.4432
	step [174/244], loss=6.1653
	step [175/244], loss=5.7947
	step [176/244], loss=5.9786
	step [177/244], loss=7.0462
	step [178/244], loss=4.8484
	step [179/244], loss=6.3849
	step [180/244], loss=6.6124
	step [181/244], loss=6.3581
	step [182/244], loss=7.0848
	step [183/244], loss=5.2965
	step [184/244], loss=6.1244
	step [185/244], loss=4.4210
	step [186/244], loss=6.6971
	step [187/244], loss=6.1801
	step [188/244], loss=5.4107
	step [189/244], loss=7.6719
	step [190/244], loss=6.1616
	step [191/244], loss=7.1823
	step [192/244], loss=6.5505
	step [193/244], loss=5.9116
	step [194/244], loss=5.3635
	step [195/244], loss=5.3480
	step [196/244], loss=6.4788
	step [197/244], loss=5.3171
	step [198/244], loss=6.2521
	step [199/244], loss=6.3378
	step [200/244], loss=7.0974
	step [201/244], loss=5.9138
	step [202/244], loss=6.3866
	step [203/244], loss=7.3136
	step [204/244], loss=7.3066
	step [205/244], loss=6.6470
	step [206/244], loss=5.8194
	step [207/244], loss=6.4767
	step [208/244], loss=5.5912
	step [209/244], loss=7.0826
	step [210/244], loss=6.8300
	step [211/244], loss=6.1291
	step [212/244], loss=8.8698
	step [213/244], loss=6.0578
	step [214/244], loss=6.4802
	step [215/244], loss=6.3630
	step [216/244], loss=7.9116
	step [217/244], loss=7.0574
	step [218/244], loss=7.1814
	step [219/244], loss=6.0866
	step [220/244], loss=7.1176
	step [221/244], loss=8.5013
	step [222/244], loss=6.8203
	step [223/244], loss=5.7803
	step [224/244], loss=5.8127
	step [225/244], loss=6.6794
	step [226/244], loss=7.1766
	step [227/244], loss=5.9765
	step [228/244], loss=6.4198
	step [229/244], loss=7.5230
	step [230/244], loss=7.4335
	step [231/244], loss=6.4415
	step [232/244], loss=5.9315
	step [233/244], loss=6.3369
	step [234/244], loss=6.2420
	step [235/244], loss=6.7482
	step [236/244], loss=6.6388
	step [237/244], loss=6.0472
	step [238/244], loss=5.2459
	step [239/244], loss=6.2697
	step [240/244], loss=7.0539
	step [241/244], loss=6.4227
	step [242/244], loss=6.3440
	step [243/244], loss=7.2503
	step [244/244], loss=3.4371
	Evaluating
	loss=0.0161, precision=0.2431, recall=0.9872, f1=0.3901
saving model as: 1_saved_model.pth
Training epoch 28
	step [1/244], loss=5.7353
	step [2/244], loss=5.8073
	step [3/244], loss=6.1834
	step [4/244], loss=6.6050
	step [5/244], loss=6.4659
	step [6/244], loss=5.8320
	step [7/244], loss=6.4991
	step [8/244], loss=7.6097
	step [9/244], loss=5.8135
	step [10/244], loss=5.9919
	step [11/244], loss=7.6671
	step [12/244], loss=7.1253
	step [13/244], loss=5.6719
	step [14/244], loss=6.8477
	step [15/244], loss=6.3990
	step [16/244], loss=6.1143
	step [17/244], loss=6.4002
	step [18/244], loss=6.7288
	step [19/244], loss=5.7354
	step [20/244], loss=5.3527
	step [21/244], loss=5.4404
	step [22/244], loss=5.4672
	step [23/244], loss=5.3874
	step [24/244], loss=8.1015
	step [25/244], loss=5.3907
	step [26/244], loss=7.9511
	step [27/244], loss=6.0917
	step [28/244], loss=6.1068
	step [29/244], loss=6.0146
	step [30/244], loss=6.2204
	step [31/244], loss=5.8723
	step [32/244], loss=7.1712
	step [33/244], loss=6.8667
	step [34/244], loss=4.8755
	step [35/244], loss=7.1328
	step [36/244], loss=5.3422
	step [37/244], loss=5.5930
	step [38/244], loss=6.4408
	step [39/244], loss=5.8080
	step [40/244], loss=8.5478
	step [41/244], loss=6.6846
	step [42/244], loss=4.8292
	step [43/244], loss=7.2590
	step [44/244], loss=6.8822
	step [45/244], loss=5.6344
	step [46/244], loss=6.8950
	step [47/244], loss=11.3694
	step [48/244], loss=6.0705
	step [49/244], loss=6.7722
	step [50/244], loss=8.1272
	step [51/244], loss=7.6593
	step [52/244], loss=4.9394
	step [53/244], loss=5.5804
	step [54/244], loss=7.2358
	step [55/244], loss=6.4623
	step [56/244], loss=7.3224
	step [57/244], loss=5.1752
	step [58/244], loss=7.1361
	step [59/244], loss=8.6745
	step [60/244], loss=7.2305
	step [61/244], loss=6.6024
	step [62/244], loss=5.8240
	step [63/244], loss=7.9142
	step [64/244], loss=7.9080
	step [65/244], loss=6.9761
	step [66/244], loss=7.4204
	step [67/244], loss=8.2645
	step [68/244], loss=7.6951
	step [69/244], loss=6.4366
	step [70/244], loss=6.5165
	step [71/244], loss=5.3529
	step [72/244], loss=6.2526
	step [73/244], loss=4.5848
	step [74/244], loss=6.0663
	step [75/244], loss=7.6245
	step [76/244], loss=6.6993
	step [77/244], loss=6.0722
	step [78/244], loss=6.5750
	step [79/244], loss=6.8084
	step [80/244], loss=6.4095
	step [81/244], loss=7.2167
	step [82/244], loss=5.2822
	step [83/244], loss=5.6456
	step [84/244], loss=7.1945
	step [85/244], loss=6.9376
	step [86/244], loss=5.8280
	step [87/244], loss=5.3954
	step [88/244], loss=7.7485
	step [89/244], loss=6.6348
	step [90/244], loss=7.1065
	step [91/244], loss=6.3447
	step [92/244], loss=5.9170
	step [93/244], loss=5.6634
	step [94/244], loss=6.5353
	step [95/244], loss=6.6673
	step [96/244], loss=5.9013
	step [97/244], loss=5.0576
	step [98/244], loss=5.9228
	step [99/244], loss=6.6816
	step [100/244], loss=5.7244
	step [101/244], loss=5.4262
	step [102/244], loss=6.8518
	step [103/244], loss=6.6144
	step [104/244], loss=4.8378
	step [105/244], loss=6.5721
	step [106/244], loss=6.5099
	step [107/244], loss=6.3922
	step [108/244], loss=5.6726
	step [109/244], loss=5.7717
	step [110/244], loss=6.6213
	step [111/244], loss=6.4829
	step [112/244], loss=5.8702
	step [113/244], loss=4.8422
	step [114/244], loss=6.3205
	step [115/244], loss=7.3615
	step [116/244], loss=7.0271
	step [117/244], loss=5.1013
	step [118/244], loss=5.5595
	step [119/244], loss=6.9180
	step [120/244], loss=6.1014
	step [121/244], loss=7.9361
	step [122/244], loss=5.6724
	step [123/244], loss=6.8240
	step [124/244], loss=5.6733
	step [125/244], loss=5.0609
	step [126/244], loss=7.2350
	step [127/244], loss=5.6491
	step [128/244], loss=6.2435
	step [129/244], loss=7.6847
	step [130/244], loss=5.9771
	step [131/244], loss=8.3423
	step [132/244], loss=7.7398
	step [133/244], loss=5.6875
	step [134/244], loss=6.4779
	step [135/244], loss=7.0224
	step [136/244], loss=5.7980
	step [137/244], loss=6.2053
	step [138/244], loss=4.9963
	step [139/244], loss=6.4445
	step [140/244], loss=9.0949
	step [141/244], loss=5.7688
	step [142/244], loss=6.4893
	step [143/244], loss=5.1234
	step [144/244], loss=6.0648
	step [145/244], loss=6.4751
	step [146/244], loss=6.2728
	step [147/244], loss=7.3560
	step [148/244], loss=6.4116
	step [149/244], loss=7.1744
	step [150/244], loss=5.3570
	step [151/244], loss=6.4470
	step [152/244], loss=6.4607
	step [153/244], loss=6.0224
	step [154/244], loss=6.2154
	step [155/244], loss=5.2336
	step [156/244], loss=6.1023
	step [157/244], loss=6.6113
	step [158/244], loss=6.2521
	step [159/244], loss=5.4149
	step [160/244], loss=5.6621
	step [161/244], loss=7.0462
	step [162/244], loss=6.4556
	step [163/244], loss=9.0752
	step [164/244], loss=6.0377
	step [165/244], loss=8.1483
	step [166/244], loss=5.9124
	step [167/244], loss=6.5657
	step [168/244], loss=7.5814
	step [169/244], loss=6.7325
	step [170/244], loss=6.9266
	step [171/244], loss=5.4749
	step [172/244], loss=5.6769
	step [173/244], loss=6.7778
	step [174/244], loss=5.8472
	step [175/244], loss=6.0929
	step [176/244], loss=6.2633
	step [177/244], loss=7.1376
	step [178/244], loss=8.1988
	step [179/244], loss=6.5396
	step [180/244], loss=7.8999
	step [181/244], loss=6.3037
	step [182/244], loss=6.5325
	step [183/244], loss=7.1164
	step [184/244], loss=7.2890
	step [185/244], loss=5.6755
	step [186/244], loss=6.5406
	step [187/244], loss=7.9423
	step [188/244], loss=7.5497
	step [189/244], loss=6.4619
	step [190/244], loss=7.0598
	step [191/244], loss=6.0475
	step [192/244], loss=7.1761
	step [193/244], loss=6.1397
	step [194/244], loss=5.6075
	step [195/244], loss=6.5188
	step [196/244], loss=6.3686
	step [197/244], loss=6.4714
	step [198/244], loss=5.0457
	step [199/244], loss=6.3100
	step [200/244], loss=6.4776
	step [201/244], loss=6.6424
	step [202/244], loss=5.8388
	step [203/244], loss=5.4383
	step [204/244], loss=6.8944
	step [205/244], loss=6.2712
	step [206/244], loss=6.3448
	step [207/244], loss=6.4500
	step [208/244], loss=4.7716
	step [209/244], loss=6.0588
	step [210/244], loss=9.4576
	step [211/244], loss=5.3423
	step [212/244], loss=5.5382
	step [213/244], loss=4.6970
	step [214/244], loss=5.2377
	step [215/244], loss=8.1286
	step [216/244], loss=5.1718
	step [217/244], loss=6.6356
	step [218/244], loss=5.3655
	step [219/244], loss=5.7332
	step [220/244], loss=5.1559
	step [221/244], loss=6.3745
	step [222/244], loss=6.0970
	step [223/244], loss=5.3347
	step [224/244], loss=5.9259
	step [225/244], loss=6.8608
	step [226/244], loss=5.6542
	step [227/244], loss=5.5514
	step [228/244], loss=5.7727
	step [229/244], loss=6.1528
	step [230/244], loss=6.9919
	step [231/244], loss=6.4809
	step [232/244], loss=6.8301
	step [233/244], loss=5.9678
	step [234/244], loss=6.3260
	step [235/244], loss=6.7740
	step [236/244], loss=6.4876
	step [237/244], loss=6.8424
	step [238/244], loss=6.0735
	step [239/244], loss=6.4091
	step [240/244], loss=7.1857
	step [241/244], loss=6.7378
	step [242/244], loss=8.1516
	step [243/244], loss=5.3341
	step [244/244], loss=2.8783
	Evaluating
	loss=0.0181, precision=0.2186, recall=0.9882, f1=0.3580
Training epoch 29
	step [1/244], loss=5.6968
	step [2/244], loss=5.0304
	step [3/244], loss=6.4625
	step [4/244], loss=6.4246
	step [5/244], loss=6.6127
	step [6/244], loss=6.8660
	step [7/244], loss=6.4768
	step [8/244], loss=6.6432
	step [9/244], loss=5.7475
	step [10/244], loss=6.4025
	step [11/244], loss=7.2131
	step [12/244], loss=6.9727
	step [13/244], loss=5.7254
	step [14/244], loss=6.2147
	step [15/244], loss=6.1830
	step [16/244], loss=6.4869
	step [17/244], loss=5.4692
	step [18/244], loss=5.4409
	step [19/244], loss=5.9074
	step [20/244], loss=6.3286
	step [21/244], loss=5.7014
	step [22/244], loss=6.6934
	step [23/244], loss=6.4391
	step [24/244], loss=6.3995
	step [25/244], loss=6.4419
	step [26/244], loss=5.2442
	step [27/244], loss=5.0703
	step [28/244], loss=5.5719
	step [29/244], loss=6.4442
	step [30/244], loss=5.6970
	step [31/244], loss=6.2586
	step [32/244], loss=9.0706
	step [33/244], loss=7.2564
	step [34/244], loss=6.7077
	step [35/244], loss=5.9661
	step [36/244], loss=6.2472
	step [37/244], loss=6.6866
	step [38/244], loss=7.3570
	step [39/244], loss=5.7929
	step [40/244], loss=4.9165
	step [41/244], loss=6.2972
	step [42/244], loss=6.9237
	step [43/244], loss=5.7335
	step [44/244], loss=6.0503
	step [45/244], loss=4.6499
	step [46/244], loss=5.4815
	step [47/244], loss=6.3748
	step [48/244], loss=6.5403
	step [49/244], loss=4.1180
	step [50/244], loss=5.6530
	step [51/244], loss=7.2270
	step [52/244], loss=6.9987
	step [53/244], loss=7.0185
	step [54/244], loss=6.0643
	step [55/244], loss=7.3964
	step [56/244], loss=5.9640
	step [57/244], loss=6.3783
	step [58/244], loss=6.1792
	step [59/244], loss=7.8070
	step [60/244], loss=5.7544
	step [61/244], loss=7.2511
	step [62/244], loss=5.3419
	step [63/244], loss=5.7369
	step [64/244], loss=6.5963
	step [65/244], loss=5.3451
	step [66/244], loss=5.8883
	step [67/244], loss=5.8198
	step [68/244], loss=5.1036
	step [69/244], loss=6.0957
	step [70/244], loss=5.1680
	step [71/244], loss=5.0544
	step [72/244], loss=6.4157
	step [73/244], loss=6.0895
	step [74/244], loss=5.5737
	step [75/244], loss=5.1566
	step [76/244], loss=6.5909
	step [77/244], loss=5.7416
	step [78/244], loss=6.3727
	step [79/244], loss=7.5609
	step [80/244], loss=5.8017
	step [81/244], loss=6.4536
	step [82/244], loss=5.4653
	step [83/244], loss=5.9273
	step [84/244], loss=8.4475
	step [85/244], loss=6.3482
	step [86/244], loss=7.9115
	step [87/244], loss=5.4964
	step [88/244], loss=6.7286
	step [89/244], loss=5.7368
	step [90/244], loss=5.8722
	step [91/244], loss=8.0974
	step [92/244], loss=5.2331
	step [93/244], loss=5.3480
	step [94/244], loss=6.6660
	step [95/244], loss=5.5159
	step [96/244], loss=6.0474
	step [97/244], loss=6.2882
	step [98/244], loss=6.9200
	step [99/244], loss=5.4154
	step [100/244], loss=7.5825
	step [101/244], loss=6.9418
	step [102/244], loss=8.0167
	step [103/244], loss=6.3499
	step [104/244], loss=6.5173
	step [105/244], loss=5.6004
	step [106/244], loss=7.6746
	step [107/244], loss=5.8285
	step [108/244], loss=4.9069
	step [109/244], loss=5.4305
	step [110/244], loss=5.4518
	step [111/244], loss=6.5463
	step [112/244], loss=7.1738
	step [113/244], loss=5.2061
	step [114/244], loss=6.9891
	step [115/244], loss=5.2139
	step [116/244], loss=5.9031
	step [117/244], loss=6.2679
	step [118/244], loss=6.3125
	step [119/244], loss=5.6683
	step [120/244], loss=6.0763
	step [121/244], loss=6.4382
	step [122/244], loss=5.3734
	step [123/244], loss=6.5588
	step [124/244], loss=6.4053
	step [125/244], loss=5.9137
	step [126/244], loss=6.1765
	step [127/244], loss=6.1612
	step [128/244], loss=5.6583
	step [129/244], loss=6.4375
	step [130/244], loss=5.8684
	step [131/244], loss=5.9461
	step [132/244], loss=7.6436
	step [133/244], loss=5.0759
	step [134/244], loss=5.7255
	step [135/244], loss=7.6507
	step [136/244], loss=6.0085
	step [137/244], loss=6.6185
	step [138/244], loss=6.8455
	step [139/244], loss=6.9795
	step [140/244], loss=6.7358
	step [141/244], loss=5.5681
	step [142/244], loss=5.8537
	step [143/244], loss=6.8146
	step [144/244], loss=6.2798
	step [145/244], loss=5.5915
	step [146/244], loss=5.6583
	step [147/244], loss=4.7377
	step [148/244], loss=6.2880
	step [149/244], loss=6.3839
	step [150/244], loss=7.1869
	step [151/244], loss=5.3956
	step [152/244], loss=4.7958
	step [153/244], loss=6.7198
	step [154/244], loss=7.5281
	step [155/244], loss=7.0304
	step [156/244], loss=5.0550
	step [157/244], loss=6.6689
	step [158/244], loss=4.9887
	step [159/244], loss=6.3054
	step [160/244], loss=8.3725
	step [161/244], loss=5.5617
	step [162/244], loss=6.5121
	step [163/244], loss=5.5597
	step [164/244], loss=6.5958
	step [165/244], loss=7.1589
	step [166/244], loss=6.8503
	step [167/244], loss=5.8862
	step [168/244], loss=6.3937
	step [169/244], loss=6.3913
	step [170/244], loss=6.3320
	step [171/244], loss=4.4754
	step [172/244], loss=5.6057
	step [173/244], loss=6.0237
	step [174/244], loss=5.5206
	step [175/244], loss=6.1725
	step [176/244], loss=6.6987
	step [177/244], loss=6.0109
	step [178/244], loss=8.0166
	step [179/244], loss=6.0324
	step [180/244], loss=4.8131
	step [181/244], loss=4.3439
	step [182/244], loss=5.8814
	step [183/244], loss=5.3819
	step [184/244], loss=5.6367
	step [185/244], loss=6.1608
	step [186/244], loss=6.3848
	step [187/244], loss=6.4402
	step [188/244], loss=5.6525
	step [189/244], loss=6.6056
	step [190/244], loss=7.3157
	step [191/244], loss=5.9037
	step [192/244], loss=5.0492
	step [193/244], loss=5.4259
	step [194/244], loss=7.9590
	step [195/244], loss=5.9697
	step [196/244], loss=6.8496
	step [197/244], loss=5.9403
	step [198/244], loss=7.4200
	step [199/244], loss=6.5466
	step [200/244], loss=6.1230
	step [201/244], loss=5.5430
	step [202/244], loss=4.9108
	step [203/244], loss=6.3922
	step [204/244], loss=6.0252
	step [205/244], loss=5.2294
	step [206/244], loss=5.7765
	step [207/244], loss=6.9383
	step [208/244], loss=7.6395
	step [209/244], loss=6.5208
	step [210/244], loss=6.3464
	step [211/244], loss=6.1421
	step [212/244], loss=5.2489
	step [213/244], loss=6.8001
	step [214/244], loss=7.8779
	step [215/244], loss=5.4690
	step [216/244], loss=7.7087
	step [217/244], loss=5.0890
	step [218/244], loss=5.9401
	step [219/244], loss=8.3965
	step [220/244], loss=5.2469
	step [221/244], loss=6.2124
	step [222/244], loss=6.4392
	step [223/244], loss=5.6214
	step [224/244], loss=5.7811
	step [225/244], loss=5.0509
	step [226/244], loss=6.5130
	step [227/244], loss=7.1209
	step [228/244], loss=4.8427
	step [229/244], loss=5.8719
	step [230/244], loss=5.9717
	step [231/244], loss=5.4041
	step [232/244], loss=7.2166
	step [233/244], loss=7.8299
	step [234/244], loss=6.2692
	step [235/244], loss=6.7162
	step [236/244], loss=6.5912
	step [237/244], loss=5.4358
	step [238/244], loss=6.8761
	step [239/244], loss=5.8509
	step [240/244], loss=5.4363
	step [241/244], loss=5.9780
	step [242/244], loss=6.5930
	step [243/244], loss=5.2183
	step [244/244], loss=2.3093
	Evaluating
	loss=0.0162, precision=0.2474, recall=0.9886, f1=0.3958
saving model as: 1_saved_model.pth
Training epoch 30
	step [1/244], loss=5.8807
	step [2/244], loss=6.7540
	step [3/244], loss=5.7183
	step [4/244], loss=8.5114
	step [5/244], loss=6.3070
	step [6/244], loss=5.3977
	step [7/244], loss=5.3852
	step [8/244], loss=5.1391
	step [9/244], loss=5.3445
	step [10/244], loss=6.6919
	step [11/244], loss=6.2837
	step [12/244], loss=6.4584
	step [13/244], loss=5.3059
	step [14/244], loss=6.2618
	step [15/244], loss=6.3463
	step [16/244], loss=5.5581
	step [17/244], loss=4.9422
	step [18/244], loss=6.2514
	step [19/244], loss=4.9631
	step [20/244], loss=6.9488
	step [21/244], loss=6.8507
	step [22/244], loss=6.1871
	step [23/244], loss=5.1912
	step [24/244], loss=6.8159
	step [25/244], loss=6.7688
	step [26/244], loss=5.8996
	step [27/244], loss=5.6676
	step [28/244], loss=5.8781
	step [29/244], loss=4.0123
	step [30/244], loss=5.1831
	step [31/244], loss=5.7497
	step [32/244], loss=7.1020
	step [33/244], loss=5.2594
	step [34/244], loss=5.0337
	step [35/244], loss=6.7529
	step [36/244], loss=6.4040
	step [37/244], loss=6.0449
	step [38/244], loss=5.9846
	step [39/244], loss=6.4518
	step [40/244], loss=6.7938
	step [41/244], loss=5.9079
	step [42/244], loss=6.7462
	step [43/244], loss=5.4582
	step [44/244], loss=6.7271
	step [45/244], loss=7.1240
	step [46/244], loss=5.2206
	step [47/244], loss=5.3522
	step [48/244], loss=5.5709
	step [49/244], loss=5.8215
	step [50/244], loss=6.1501
	step [51/244], loss=6.7646
	step [52/244], loss=7.9978
	step [53/244], loss=7.5188
	step [54/244], loss=4.8705
	step [55/244], loss=6.2038
	step [56/244], loss=6.2440
	step [57/244], loss=5.3190
	step [58/244], loss=7.0021
	step [59/244], loss=5.7198
	step [60/244], loss=6.2430
	step [61/244], loss=5.4110
	step [62/244], loss=5.8939
	step [63/244], loss=5.4719
	step [64/244], loss=6.5685
	step [65/244], loss=5.8641
	step [66/244], loss=6.4909
	step [67/244], loss=6.3761
	step [68/244], loss=5.0117
	step [69/244], loss=5.8474
	step [70/244], loss=6.1115
	step [71/244], loss=6.0701
	step [72/244], loss=6.0030
	step [73/244], loss=5.8330
	step [74/244], loss=5.8057
	step [75/244], loss=6.5248
	step [76/244], loss=6.7882
	step [77/244], loss=5.5876
	step [78/244], loss=7.5609
	step [79/244], loss=6.1152
	step [80/244], loss=5.1906
	step [81/244], loss=6.2831
	step [82/244], loss=5.1870
	step [83/244], loss=5.5539
	step [84/244], loss=5.4669
	step [85/244], loss=6.9081
	step [86/244], loss=5.2606
	step [87/244], loss=7.2683
	step [88/244], loss=6.2409
	step [89/244], loss=6.4645
	step [90/244], loss=6.0429
	step [91/244], loss=6.7697
	step [92/244], loss=5.3300
	step [93/244], loss=5.9841
	step [94/244], loss=6.8036
	step [95/244], loss=5.8812
	step [96/244], loss=5.7023
	step [97/244], loss=5.4693
	step [98/244], loss=5.5017
	step [99/244], loss=6.4528
	step [100/244], loss=6.4906
	step [101/244], loss=5.4781
	step [102/244], loss=6.2986
	step [103/244], loss=6.7408
	step [104/244], loss=5.1604
	step [105/244], loss=5.0935
	step [106/244], loss=5.4533
	step [107/244], loss=7.3785
	step [108/244], loss=5.5377
	step [109/244], loss=6.9360
	step [110/244], loss=6.5518
	step [111/244], loss=5.1873
	step [112/244], loss=6.9300
	step [113/244], loss=6.0619
	step [114/244], loss=5.7619
	step [115/244], loss=6.4056
	step [116/244], loss=5.8761
	step [117/244], loss=7.1349
	step [118/244], loss=6.7369
	step [119/244], loss=6.4193
	step [120/244], loss=5.1698
	step [121/244], loss=5.0879
	step [122/244], loss=5.1262
	step [123/244], loss=6.0392
	step [124/244], loss=5.3667
	step [125/244], loss=6.3598
	step [126/244], loss=4.8962
	step [127/244], loss=6.3098
	step [128/244], loss=5.3241
	step [129/244], loss=5.7069
	step [130/244], loss=5.2388
	step [131/244], loss=5.3263
	step [132/244], loss=5.0301
	step [133/244], loss=6.8455
	step [134/244], loss=8.2085
	step [135/244], loss=5.1190
	step [136/244], loss=5.0481
	step [137/244], loss=5.1435
	step [138/244], loss=6.0386
	step [139/244], loss=6.7020
	step [140/244], loss=4.6805
	step [141/244], loss=4.9104
	step [142/244], loss=4.6087
	step [143/244], loss=4.9374
	step [144/244], loss=5.1138
	step [145/244], loss=5.9288
	step [146/244], loss=6.2269
	step [147/244], loss=8.0679
	step [148/244], loss=4.8837
	step [149/244], loss=5.7382
	step [150/244], loss=7.6716
	step [151/244], loss=5.6073
	step [152/244], loss=6.6405
	step [153/244], loss=6.1167
	step [154/244], loss=5.3968
	step [155/244], loss=6.0757
	step [156/244], loss=5.2463
	step [157/244], loss=5.5739
	step [158/244], loss=6.3505
	step [159/244], loss=5.8448
	step [160/244], loss=7.4608
	step [161/244], loss=6.3399
	step [162/244], loss=5.0234
	step [163/244], loss=7.0960
	step [164/244], loss=6.4035
	step [165/244], loss=5.7369
	step [166/244], loss=5.7639
	step [167/244], loss=5.0991
	step [168/244], loss=5.9284
	step [169/244], loss=4.5909
	step [170/244], loss=4.8788
	step [171/244], loss=4.8235
	step [172/244], loss=8.3444
	step [173/244], loss=8.1242
	step [174/244], loss=8.5133
	step [175/244], loss=5.5786
	step [176/244], loss=6.7630
	step [177/244], loss=6.0206
	step [178/244], loss=6.7629
	step [179/244], loss=4.2744
	step [180/244], loss=6.1254
	step [181/244], loss=6.3059
	step [182/244], loss=6.6504
	step [183/244], loss=4.7566
	step [184/244], loss=6.1883
	step [185/244], loss=6.4971
	step [186/244], loss=5.5094
	step [187/244], loss=5.7911
	step [188/244], loss=5.9435
	step [189/244], loss=7.4769
	step [190/244], loss=7.0497
	step [191/244], loss=5.6913
	step [192/244], loss=6.0583
	step [193/244], loss=6.3375
	step [194/244], loss=6.2113
	step [195/244], loss=5.6268
	step [196/244], loss=6.2677
	step [197/244], loss=5.6942
	step [198/244], loss=6.6172
	step [199/244], loss=6.2650
	step [200/244], loss=4.9180
	step [201/244], loss=5.6162
	step [202/244], loss=5.3276
	step [203/244], loss=4.4540
	step [204/244], loss=5.9436
	step [205/244], loss=4.8361
	step [206/244], loss=7.5319
	step [207/244], loss=5.6681
	step [208/244], loss=7.2117
	step [209/244], loss=5.8181
	step [210/244], loss=4.9909
	step [211/244], loss=5.8916
	step [212/244], loss=5.7467
	step [213/244], loss=6.3429
	step [214/244], loss=4.9946
	step [215/244], loss=4.8368
	step [216/244], loss=5.8267
	step [217/244], loss=5.2287
	step [218/244], loss=6.0790
	step [219/244], loss=6.9869
	step [220/244], loss=5.7678
	step [221/244], loss=5.0281
	step [222/244], loss=6.3908
	step [223/244], loss=5.7701
	step [224/244], loss=5.4395
	step [225/244], loss=6.0082
	step [226/244], loss=6.0301
	step [227/244], loss=5.1620
	step [228/244], loss=6.4117
	step [229/244], loss=5.2010
	step [230/244], loss=5.6311
	step [231/244], loss=5.1812
	step [232/244], loss=6.0495
	step [233/244], loss=5.2795
	step [234/244], loss=6.2064
	step [235/244], loss=7.4245
	step [236/244], loss=5.4756
	step [237/244], loss=6.6640
	step [238/244], loss=5.5720
	step [239/244], loss=5.9874
	step [240/244], loss=6.2160
	step [241/244], loss=6.0842
	step [242/244], loss=5.8202
	step [243/244], loss=5.5395
	step [244/244], loss=2.3705
	Evaluating
	loss=0.0166, precision=0.2350, recall=0.9888, f1=0.3798
Training finished
best_f1: 0.39578531164919584
directing: Z rim_enhanced: False test_id 1
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 12179 # image files with weight 12153
removed wrong scan: weights_Z_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_165_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_65_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_292_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_106_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_288_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_249_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_239_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_52_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_217_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_196_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_283_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_259_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_299_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_277_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_267_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_250_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_113_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_275_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_280_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_244_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_252_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_58_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_100_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_111_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_281_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_205_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_265_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_244_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_213_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_237_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_271_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_247_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_90_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_289_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_300_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_15_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_93_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_149_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_268_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_227_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_10_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_261_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_250_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_82_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_246_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_230_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_206_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_54_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_220_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_252_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_272_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_294_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_63_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_279_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_81_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_257_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_219_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_291_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_286_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_51_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_204_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_142_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_71_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_142_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_256_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_241_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_251_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_264_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_8_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_224_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_146_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_143_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_298_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_246_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_8_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_233_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_270_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_173_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_211_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_218_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_273_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_238_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_188_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_276_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_249_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_262_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_216_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_293_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_9_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_287_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_284_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_94_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_127_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_295_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_240_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_260_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_92_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_258_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_266_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_274_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_20_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_48_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_263_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_278_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_121_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_140_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_166_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_194_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_232_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_215_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_203_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_234_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_235_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_87_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_214_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_297_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_185_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_212_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_255_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_85_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_285_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_53_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_243_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_223_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_245_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_247_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_38_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_154_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_105_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_290_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_202_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_228_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_269_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_49_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_145_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_253_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_229_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_251_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_73_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_282_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_248_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_Z_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_Z_144_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_Z_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_141_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_Z_254_xwqg-B00034_2020-04-03.npy
# all image files: 15579 # all weight files in weight_dir: 3340 # image files with weight 3331
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/Z 12153
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
	step [1/190], loss=372.0817
	step [2/190], loss=331.5422
	step [3/190], loss=317.6694
	step [4/190], loss=309.4126
	step [5/190], loss=303.9963
	step [6/190], loss=299.2957
	step [7/190], loss=290.6736
	step [8/190], loss=286.6566
	step [9/190], loss=282.6228
	step [10/190], loss=273.7377
	step [11/190], loss=269.7529
	step [12/190], loss=266.0461
	step [13/190], loss=257.7033
	step [14/190], loss=254.6544
	step [15/190], loss=247.4306
	step [16/190], loss=243.4153
	step [17/190], loss=237.2273
	step [18/190], loss=233.9468
	step [19/190], loss=227.7596
	step [20/190], loss=229.1023
	step [21/190], loss=225.3243
	step [22/190], loss=216.7915
	step [23/190], loss=213.5569
	step [24/190], loss=212.4269
	step [25/190], loss=213.2267
	step [26/190], loss=206.5574
	step [27/190], loss=205.1157
	step [28/190], loss=199.3132
	step [29/190], loss=201.7927
	step [30/190], loss=194.9556
	step [31/190], loss=190.6072
	step [32/190], loss=195.6946
	step [33/190], loss=193.0121
	step [34/190], loss=191.7440
	step [35/190], loss=185.8651
	step [36/190], loss=187.0950
	step [37/190], loss=185.1934
	step [38/190], loss=181.2894
	step [39/190], loss=179.0847
	step [40/190], loss=176.1447
	step [41/190], loss=175.8485
	step [42/190], loss=178.1202
	step [43/190], loss=174.9906
	step [44/190], loss=170.6667
	step [45/190], loss=171.0117
	step [46/190], loss=169.1858
	step [47/190], loss=168.3199
	step [48/190], loss=168.2022
	step [49/190], loss=165.8086
	step [50/190], loss=168.5820
	step [51/190], loss=164.4629
	step [52/190], loss=164.2274
	step [53/190], loss=163.0565
	step [54/190], loss=162.4090
	step [55/190], loss=163.3311
	step [56/190], loss=158.9815
	step [57/190], loss=159.7754
	step [58/190], loss=157.1029
	step [59/190], loss=158.5808
	step [60/190], loss=160.2495
	step [61/190], loss=160.0833
	step [62/190], loss=156.0885
	step [63/190], loss=155.3341
	step [64/190], loss=153.5837
	step [65/190], loss=154.2018
	step [66/190], loss=152.8259
	step [67/190], loss=151.9409
	step [68/190], loss=152.2370
	step [69/190], loss=150.9245
	step [70/190], loss=152.1974
	step [71/190], loss=152.4232
	step [72/190], loss=150.3359
	step [73/190], loss=148.9788
	step [74/190], loss=149.8654
	step [75/190], loss=148.9787
	step [76/190], loss=149.9735
	step [77/190], loss=150.6443
	step [78/190], loss=146.5625
	step [79/190], loss=145.5742
	step [80/190], loss=148.7907
	step [81/190], loss=147.1665
	step [82/190], loss=147.2815
	step [83/190], loss=147.2865
	step [84/190], loss=145.0226
	step [85/190], loss=144.2380
	step [86/190], loss=144.5390
	step [87/190], loss=142.9750
	step [88/190], loss=145.1954
	step [89/190], loss=145.5768
	step [90/190], loss=142.1419
	step [91/190], loss=144.2013
	step [92/190], loss=141.5555
	step [93/190], loss=142.2000
	step [94/190], loss=143.8162
	step [95/190], loss=144.0582
	step [96/190], loss=142.1171
	step [97/190], loss=143.3808
	step [98/190], loss=143.6933
	step [99/190], loss=142.4086
	step [100/190], loss=140.2867
	step [101/190], loss=140.6744
	step [102/190], loss=141.8005
	step [103/190], loss=139.7473
	step [104/190], loss=139.9468
	step [105/190], loss=138.5025
	step [106/190], loss=138.7977
	step [107/190], loss=140.5787
	step [108/190], loss=138.6747
	step [109/190], loss=139.2157
	step [110/190], loss=138.4002
	step [111/190], loss=140.3153
	step [112/190], loss=138.1089
	step [113/190], loss=136.5334
	step [114/190], loss=135.7003
	step [115/190], loss=139.1049
	step [116/190], loss=138.2184
	step [117/190], loss=137.1998
	step [118/190], loss=139.1943
	step [119/190], loss=134.7676
	step [120/190], loss=136.7497
	step [121/190], loss=135.6646
	step [122/190], loss=136.8033
	step [123/190], loss=134.9078
	step [124/190], loss=138.0839
	step [125/190], loss=134.2199
	step [126/190], loss=136.4602
	step [127/190], loss=134.8085
	step [128/190], loss=137.0058
	step [129/190], loss=137.6609
	step [130/190], loss=132.8543
	step [131/190], loss=134.4832
	step [132/190], loss=134.1198
	step [133/190], loss=135.4823
	step [134/190], loss=134.0321
	step [135/190], loss=131.5168
	step [136/190], loss=133.4127
	step [137/190], loss=133.8571
	step [138/190], loss=134.2617
	step [139/190], loss=133.2762
	step [140/190], loss=132.3430
	step [141/190], loss=130.6062
	step [142/190], loss=130.4489
	step [143/190], loss=130.7980
	step [144/190], loss=131.3425
	step [145/190], loss=129.3488
	step [146/190], loss=130.3230
	step [147/190], loss=132.0787
	step [148/190], loss=129.7859
	step [149/190], loss=129.0193
	step [150/190], loss=131.4947
	step [151/190], loss=130.9544
	step [152/190], loss=128.9454
	step [153/190], loss=132.1511
	step [154/190], loss=130.9273
	step [155/190], loss=129.2469
	step [156/190], loss=130.1212
	step [157/190], loss=129.6421
	step [158/190], loss=128.2751
	step [159/190], loss=127.0901
	step [160/190], loss=128.0650
	step [161/190], loss=125.8628
	step [162/190], loss=126.7008
	step [163/190], loss=128.3747
	step [164/190], loss=128.5657
	step [165/190], loss=125.6785
	step [166/190], loss=125.6588
	step [167/190], loss=126.6371
	step [168/190], loss=126.3277
	step [169/190], loss=123.7364
	step [170/190], loss=127.6657
	step [171/190], loss=126.9482
	step [172/190], loss=124.7106
	step [173/190], loss=127.0213
	step [174/190], loss=125.9118
	step [175/190], loss=127.9640
	step [176/190], loss=125.8495
	step [177/190], loss=126.1452
	step [178/190], loss=125.4765
	step [179/190], loss=123.2544
	step [180/190], loss=124.6472
	step [181/190], loss=125.3259
	step [182/190], loss=123.4044
	step [183/190], loss=125.4565
	step [184/190], loss=123.7627
	step [185/190], loss=123.6880
	step [186/190], loss=122.5258
	step [187/190], loss=123.0900
	step [188/190], loss=123.8322
	step [189/190], loss=124.3509
	step [190/190], loss=108.0784
	Evaluating
	loss=0.5386, precision=0.0761, recall=0.9916, f1=0.1414
saving model as: 1_saved_model.pth
Training epoch 2
	step [1/190], loss=121.5934
	step [2/190], loss=122.0065
	step [3/190], loss=124.2550
	step [4/190], loss=123.6495
	step [5/190], loss=122.3383
	step [6/190], loss=124.3892
	step [7/190], loss=122.0370
	step [8/190], loss=122.7595
	step [9/190], loss=121.8164
	step [10/190], loss=123.2602
	step [11/190], loss=120.1267
	step [12/190], loss=122.3671
	step [13/190], loss=120.3708
	step [14/190], loss=121.1266
	step [15/190], loss=123.8602
	step [16/190], loss=119.6707
	step [17/190], loss=120.1860
	step [18/190], loss=119.7150
	step [19/190], loss=119.4104
	step [20/190], loss=118.1678
	step [21/190], loss=120.1174
	step [22/190], loss=119.1307
	step [23/190], loss=120.4263
	step [24/190], loss=119.0248
	step [25/190], loss=117.4106
	step [26/190], loss=119.4352
	step [27/190], loss=119.1694
	step [28/190], loss=119.0098
	step [29/190], loss=117.8533
	step [30/190], loss=118.4923
	step [31/190], loss=116.8950
	step [32/190], loss=117.2949
	step [33/190], loss=116.6817
	step [34/190], loss=117.1223
	step [35/190], loss=117.7776
	step [36/190], loss=118.6260
	step [37/190], loss=116.7602
	step [38/190], loss=117.4511
	step [39/190], loss=118.5846
	step [40/190], loss=116.5463
	step [41/190], loss=117.2838
	step [42/190], loss=118.1929
	step [43/190], loss=115.9899
	step [44/190], loss=115.2031
	step [45/190], loss=113.6252
	step [46/190], loss=115.2853
	step [47/190], loss=116.0531
	step [48/190], loss=115.0279
	step [49/190], loss=115.2170
	step [50/190], loss=115.2373
	step [51/190], loss=114.7704
	step [52/190], loss=115.2683
	step [53/190], loss=112.7785
	step [54/190], loss=115.1653
	step [55/190], loss=114.9048
	step [56/190], loss=115.7661
	step [57/190], loss=114.7485
	step [58/190], loss=113.6567
	step [59/190], loss=113.3127
	step [60/190], loss=112.3587
	step [61/190], loss=113.0109
	step [62/190], loss=112.7734
	step [63/190], loss=111.8395
	step [64/190], loss=110.2757
	step [65/190], loss=112.0595
	step [66/190], loss=114.8694
	step [67/190], loss=112.2052
	step [68/190], loss=111.5626
	step [69/190], loss=114.9826
	step [70/190], loss=111.2404
	step [71/190], loss=111.1444
	step [72/190], loss=110.3743
	step [73/190], loss=113.4065
	step [74/190], loss=111.1381
	step [75/190], loss=109.7000
	step [76/190], loss=110.7199
	step [77/190], loss=111.6844
	step [78/190], loss=109.6887
	step [79/190], loss=111.2101
	step [80/190], loss=111.3277
	step [81/190], loss=109.8736
	step [82/190], loss=111.6002
	step [83/190], loss=109.5644
	step [84/190], loss=111.7740
	step [85/190], loss=109.9132
	step [86/190], loss=109.7120
	step [87/190], loss=107.5185
	step [88/190], loss=108.4992
	step [89/190], loss=108.7420
	step [90/190], loss=107.6768
	step [91/190], loss=109.2751
	step [92/190], loss=107.3342
	step [93/190], loss=108.7764
	step [94/190], loss=108.2175
	step [95/190], loss=107.5605
	step [96/190], loss=107.7790
	step [97/190], loss=109.1336
	step [98/190], loss=107.0718
	step [99/190], loss=107.0333
	step [100/190], loss=109.5024
	step [101/190], loss=107.6602
	step [102/190], loss=107.3556
	step [103/190], loss=106.2418
	step [104/190], loss=107.4835
	step [105/190], loss=107.7026
	step [106/190], loss=106.7104
	step [107/190], loss=106.7517
	step [108/190], loss=106.9919
	step [109/190], loss=109.7716
	step [110/190], loss=105.5071
	step [111/190], loss=105.8988
	step [112/190], loss=106.6886
	step [113/190], loss=106.7912
	step [114/190], loss=107.9560
	step [115/190], loss=103.1893
	step [116/190], loss=108.3627
	step [117/190], loss=104.0859
	step [118/190], loss=106.3620
	step [119/190], loss=106.8286
	step [120/190], loss=107.0956
	step [121/190], loss=104.8715
	step [122/190], loss=106.6251
	step [123/190], loss=105.3587
	step [124/190], loss=103.2439
	step [125/190], loss=102.5009
	step [126/190], loss=105.9544
	step [127/190], loss=103.5763
	step [128/190], loss=103.9626
	step [129/190], loss=104.8600
	step [130/190], loss=104.2460
	step [131/190], loss=102.7989
	step [132/190], loss=103.8644
	step [133/190], loss=104.8688
	step [134/190], loss=103.5132
	step [135/190], loss=104.4472
	step [136/190], loss=103.0896
	step [137/190], loss=105.0096
	step [138/190], loss=102.5750
	step [139/190], loss=101.4969
	step [140/190], loss=103.0597
	step [141/190], loss=104.4800
	step [142/190], loss=101.7645
	step [143/190], loss=103.9366
	step [144/190], loss=100.9221
	step [145/190], loss=102.9967
	step [146/190], loss=101.0244
	step [147/190], loss=102.0601
	step [148/190], loss=101.6031
	step [149/190], loss=101.6088
	step [150/190], loss=100.8777
	step [151/190], loss=100.8755
	step [152/190], loss=101.4397
	step [153/190], loss=100.1252
	step [154/190], loss=98.5226
	step [155/190], loss=100.5401
	step [156/190], loss=99.0302
	step [157/190], loss=99.3838
	step [158/190], loss=100.9603
	step [159/190], loss=100.7397
	step [160/190], loss=100.4724
	step [161/190], loss=98.6317
	step [162/190], loss=99.4861
	step [163/190], loss=98.1014
	step [164/190], loss=98.7441
	step [165/190], loss=98.0821
	step [166/190], loss=100.0374
	step [167/190], loss=102.1762
	step [168/190], loss=99.0057
	step [169/190], loss=99.6142
	step [170/190], loss=99.9378
	step [171/190], loss=99.3232
	step [172/190], loss=96.9549
	step [173/190], loss=99.1767
	step [174/190], loss=98.8942
	step [175/190], loss=98.0388
	step [176/190], loss=98.8729
	step [177/190], loss=97.5002
	step [178/190], loss=98.2370
	step [179/190], loss=98.6168
	step [180/190], loss=97.4457
	step [181/190], loss=98.6230
	step [182/190], loss=96.3291
	step [183/190], loss=97.4469
	step [184/190], loss=99.9733
	step [185/190], loss=97.0281
	step [186/190], loss=94.4286
	step [187/190], loss=96.5816
	step [188/190], loss=95.8429
	step [189/190], loss=95.7023
	step [190/190], loss=86.8598
	Evaluating
	loss=0.3588, precision=0.2203, recall=0.9912, f1=0.3605
saving model as: 1_saved_model.pth
Training epoch 3
	step [1/190], loss=95.4032
	step [2/190], loss=95.0549
	step [3/190], loss=97.3055
	step [4/190], loss=94.2190
	step [5/190], loss=97.3882
	step [6/190], loss=95.5611
	step [7/190], loss=97.6865
	step [8/190], loss=97.5157
	step [9/190], loss=96.7393
	step [10/190], loss=94.7994
	step [11/190], loss=94.4612
	step [12/190], loss=96.5581
	step [13/190], loss=96.1727
	step [14/190], loss=94.3636
	step [15/190], loss=93.7986
	step [16/190], loss=94.7463
	step [17/190], loss=94.5165
	step [18/190], loss=95.1251
	step [19/190], loss=94.4987
	step [20/190], loss=94.0387
	step [21/190], loss=93.7431
	step [22/190], loss=92.5840
	step [23/190], loss=94.2394
	step [24/190], loss=92.2933
	step [25/190], loss=92.2534
	step [26/190], loss=92.4933
	step [27/190], loss=93.7952
	step [28/190], loss=94.9811
	step [29/190], loss=92.0913
	step [30/190], loss=92.3556
	step [31/190], loss=91.4126
	step [32/190], loss=91.0265
	step [33/190], loss=90.9470
	step [34/190], loss=91.9929
	step [35/190], loss=93.6321
	step [36/190], loss=93.2838
	step [37/190], loss=90.8970
	step [38/190], loss=91.7616
	step [39/190], loss=89.5886
	step [40/190], loss=90.5916
	step [41/190], loss=91.2469
	step [42/190], loss=92.3829
	step [43/190], loss=91.4523
	step [44/190], loss=92.0568
	step [45/190], loss=90.2099
	step [46/190], loss=92.4742
	step [47/190], loss=89.9566
	step [48/190], loss=90.5328
	step [49/190], loss=90.8735
	step [50/190], loss=89.6900
	step [51/190], loss=89.3638
	step [52/190], loss=89.3342
	step [53/190], loss=87.8855
	step [54/190], loss=91.7009
	step [55/190], loss=91.1002
	step [56/190], loss=89.6086
	step [57/190], loss=92.5264
	step [58/190], loss=89.8674
	step [59/190], loss=89.3538
	step [60/190], loss=88.7398
	step [61/190], loss=90.9460
	step [62/190], loss=87.6817
	step [63/190], loss=91.0406
	step [64/190], loss=90.0424
	step [65/190], loss=88.9541
	step [66/190], loss=88.5307
	step [67/190], loss=88.5926
	step [68/190], loss=87.5010
	step [69/190], loss=88.3368
	step [70/190], loss=88.9893
	step [71/190], loss=87.5447
	step [72/190], loss=89.2002
	step [73/190], loss=87.4866
	step [74/190], loss=87.7798
	step [75/190], loss=89.5055
	step [76/190], loss=87.0434
	step [77/190], loss=87.1906
	step [78/190], loss=86.9643
	step [79/190], loss=87.9156
	step [80/190], loss=86.6168
	step [81/190], loss=85.7018
	step [82/190], loss=87.0942
	step [83/190], loss=87.3335
	step [84/190], loss=86.9425
	step [85/190], loss=85.2697
	step [86/190], loss=85.8834
	step [87/190], loss=85.5636
	step [88/190], loss=86.5314
	step [89/190], loss=85.6510
	step [90/190], loss=85.1253
	step [91/190], loss=86.4068
	step [92/190], loss=86.8402
	step [93/190], loss=86.8403
	step [94/190], loss=87.1473
	step [95/190], loss=85.2153
	step [96/190], loss=85.2450
	step [97/190], loss=87.9338
	step [98/190], loss=86.7433
	step [99/190], loss=84.3194
	step [100/190], loss=85.1361
	step [101/190], loss=85.5450
	step [102/190], loss=84.7824
	step [103/190], loss=83.6457
	step [104/190], loss=85.6898
	step [105/190], loss=83.6674
	step [106/190], loss=85.8180
	step [107/190], loss=83.5950
	step [108/190], loss=82.3394
	step [109/190], loss=82.3262
	step [110/190], loss=84.1706
	step [111/190], loss=84.1670
	step [112/190], loss=84.5287
	step [113/190], loss=81.8661
	step [114/190], loss=83.8033
	step [115/190], loss=83.3109
	step [116/190], loss=82.4729
	step [117/190], loss=83.1222
	step [118/190], loss=84.1736
	step [119/190], loss=82.6352
	step [120/190], loss=82.9500
	step [121/190], loss=85.6579
	step [122/190], loss=83.5197
	step [123/190], loss=84.9268
	step [124/190], loss=82.8963
	step [125/190], loss=82.0526
	step [126/190], loss=83.3291
	step [127/190], loss=80.7867
	step [128/190], loss=82.6501
	step [129/190], loss=82.1811
	step [130/190], loss=79.6511
	step [131/190], loss=83.2503
	step [132/190], loss=80.9985
	step [133/190], loss=81.1785
	step [134/190], loss=79.6977
	step [135/190], loss=81.4845
	step [136/190], loss=83.3979
	step [137/190], loss=81.3550
	step [138/190], loss=81.2409
	step [139/190], loss=82.7649
	step [140/190], loss=82.8745
	step [141/190], loss=80.4155
	step [142/190], loss=81.5447
	step [143/190], loss=80.3104
	step [144/190], loss=79.7290
	step [145/190], loss=80.7168
	step [146/190], loss=81.1055
	step [147/190], loss=80.1916
	step [148/190], loss=79.8076
	step [149/190], loss=79.8331
	step [150/190], loss=81.8872
	step [151/190], loss=78.7214
	step [152/190], loss=80.0319
	step [153/190], loss=79.1426
	step [154/190], loss=78.3006
	step [155/190], loss=81.3577
	step [156/190], loss=81.6067
	step [157/190], loss=79.5948
	step [158/190], loss=79.9664
	step [159/190], loss=77.6180
	step [160/190], loss=79.8200
	step [161/190], loss=78.6991
	step [162/190], loss=78.4533
	step [163/190], loss=79.7298
	step [164/190], loss=77.1517
	step [165/190], loss=78.8237
	step [166/190], loss=78.6829
	step [167/190], loss=80.1815
	step [168/190], loss=77.8444
	step [169/190], loss=78.9847
	step [170/190], loss=77.9302
	step [171/190], loss=78.9932
	step [172/190], loss=79.1582
	step [173/190], loss=75.7409
	step [174/190], loss=77.4338
	step [175/190], loss=77.2578
	step [176/190], loss=79.4589
	step [177/190], loss=77.0757
	step [178/190], loss=75.8946
	step [179/190], loss=79.0331
	step [180/190], loss=78.1231
	step [181/190], loss=75.5757
	step [182/190], loss=77.2343
	step [183/190], loss=80.3808
	step [184/190], loss=77.6625
	step [185/190], loss=77.7108
	step [186/190], loss=75.0405
	step [187/190], loss=76.1737
	step [188/190], loss=74.3675
	step [189/190], loss=75.9874
	step [190/190], loss=67.6787
	Evaluating
	loss=0.2850, precision=0.2111, recall=0.9921, f1=0.3481
Training epoch 4
	step [1/190], loss=75.9209
	step [2/190], loss=75.1959
	step [3/190], loss=74.4018
	step [4/190], loss=77.0562
	step [5/190], loss=76.0365
	step [6/190], loss=77.0583
	step [7/190], loss=76.7415
	step [8/190], loss=74.3275
	step [9/190], loss=75.5457
	step [10/190], loss=74.0992
	step [11/190], loss=75.0018
	step [12/190], loss=75.2608
	step [13/190], loss=74.5126
	step [14/190], loss=74.1781
	step [15/190], loss=73.6120
	step [16/190], loss=75.0591
	step [17/190], loss=75.8743
	step [18/190], loss=73.8332
	step [19/190], loss=75.2316
	step [20/190], loss=73.4066
	step [21/190], loss=76.1870
	step [22/190], loss=73.6214
	step [23/190], loss=74.0864
	step [24/190], loss=72.8769
	step [25/190], loss=75.0473
	step [26/190], loss=74.6824
	step [27/190], loss=73.8944
	step [28/190], loss=71.4371
	step [29/190], loss=73.1162
	step [30/190], loss=76.0769
	step [31/190], loss=73.1274
	step [32/190], loss=72.9400
	step [33/190], loss=74.9902
	step [34/190], loss=73.9766
	step [35/190], loss=72.7849
	step [36/190], loss=73.4313
	step [37/190], loss=71.7860
	step [38/190], loss=74.2298
	step [39/190], loss=74.1538
	step [40/190], loss=72.5373
	step [41/190], loss=73.2430
	step [42/190], loss=71.2341
	step [43/190], loss=72.8229
	step [44/190], loss=72.1505
	step [45/190], loss=71.3610
	step [46/190], loss=71.9138
	step [47/190], loss=73.9184
	step [48/190], loss=72.7455
	step [49/190], loss=72.2318
	step [50/190], loss=72.7332
	step [51/190], loss=70.4040
	step [52/190], loss=72.6169
	step [53/190], loss=71.2083
	step [54/190], loss=72.7621
	step [55/190], loss=71.6738
	step [56/190], loss=72.6133
	step [57/190], loss=70.6409
	step [58/190], loss=71.0885
	step [59/190], loss=72.2619
	step [60/190], loss=72.4499
	step [61/190], loss=69.1446
	step [62/190], loss=71.7282
	step [63/190], loss=71.2401
	step [64/190], loss=68.6785
	step [65/190], loss=71.1712
	step [66/190], loss=69.4482
	step [67/190], loss=71.3275
	step [68/190], loss=71.1645
	step [69/190], loss=73.5350
	step [70/190], loss=71.3589
	step [71/190], loss=69.1659
	step [72/190], loss=68.2827
	step [73/190], loss=70.7203
	step [74/190], loss=69.2309
	step [75/190], loss=69.0682
	step [76/190], loss=67.8541
	step [77/190], loss=70.2150
	step [78/190], loss=68.4317
	step [79/190], loss=68.6168
	step [80/190], loss=68.6086
	step [81/190], loss=70.1413
	step [82/190], loss=69.0769
	step [83/190], loss=69.3498
	step [84/190], loss=69.0800
	step [85/190], loss=68.8151
	step [86/190], loss=68.6586
	step [87/190], loss=67.8666
	step [88/190], loss=68.8691
	step [89/190], loss=67.4199
	step [90/190], loss=68.5393
	step [91/190], loss=67.1340
	step [92/190], loss=68.1721
	step [93/190], loss=68.9207
	step [94/190], loss=67.1564
	step [95/190], loss=70.7140
	step [96/190], loss=67.7260
	step [97/190], loss=69.1022
	step [98/190], loss=68.4707
	step [99/190], loss=68.3724
	step [100/190], loss=67.7493
	step [101/190], loss=69.2365
	step [102/190], loss=67.3306
	step [103/190], loss=67.6300
	step [104/190], loss=67.7641
	step [105/190], loss=67.4863
	step [106/190], loss=65.7026
	step [107/190], loss=69.3048
	step [108/190], loss=67.7162
	step [109/190], loss=68.1424
	step [110/190], loss=67.0001
	step [111/190], loss=67.0956
	step [112/190], loss=68.1811
	step [113/190], loss=67.0306
	step [114/190], loss=67.2765
	step [115/190], loss=66.3611
	step [116/190], loss=66.3784
	step [117/190], loss=63.1809
	step [118/190], loss=66.6002
	step [119/190], loss=68.3985
	step [120/190], loss=67.0935
	step [121/190], loss=67.8874
	step [122/190], loss=66.0371
	step [123/190], loss=66.7062
	step [124/190], loss=67.3513
	step [125/190], loss=64.7617
	step [126/190], loss=65.1276
	step [127/190], loss=65.8733
	step [128/190], loss=65.6974
	step [129/190], loss=65.8560
	step [130/190], loss=65.3828
	step [131/190], loss=70.4648
	step [132/190], loss=65.9901
	step [133/190], loss=64.2029
	step [134/190], loss=66.2303
	step [135/190], loss=63.5921
	step [136/190], loss=64.1092
	step [137/190], loss=62.5561
	step [138/190], loss=66.7739
	step [139/190], loss=66.1873
	step [140/190], loss=64.4465
	step [141/190], loss=64.3994
	step [142/190], loss=64.1301
	step [143/190], loss=65.2635
	step [144/190], loss=66.1169
	step [145/190], loss=64.1797
	step [146/190], loss=64.5381
	step [147/190], loss=65.6532
	step [148/190], loss=63.6172
	step [149/190], loss=64.6415
	step [150/190], loss=63.7415
	step [151/190], loss=62.2644
	step [152/190], loss=65.1998
	step [153/190], loss=64.7171
	step [154/190], loss=64.6084
	step [155/190], loss=60.9449
	step [156/190], loss=66.2361
	step [157/190], loss=62.2127
	step [158/190], loss=60.8179
	step [159/190], loss=63.8856
	step [160/190], loss=66.6535
	step [161/190], loss=63.2757
	step [162/190], loss=63.5951
	step [163/190], loss=62.7269
	step [164/190], loss=66.8800
	step [165/190], loss=61.3419
	step [166/190], loss=63.1030
	step [167/190], loss=62.1066
	step [168/190], loss=62.4715
	step [169/190], loss=64.2615
	step [170/190], loss=61.7265
	step [171/190], loss=61.7736
	step [172/190], loss=61.0318
	step [173/190], loss=60.8808
	step [174/190], loss=61.2392
	step [175/190], loss=61.7620
	step [176/190], loss=60.3941
	step [177/190], loss=61.6982
	step [178/190], loss=62.1663
	step [179/190], loss=61.2982
	step [180/190], loss=61.4358
	step [181/190], loss=61.8743
	step [182/190], loss=60.3403
	step [183/190], loss=60.4522
	step [184/190], loss=61.3775
	step [185/190], loss=61.5277
	step [186/190], loss=61.0989
	step [187/190], loss=60.7286
	step [188/190], loss=61.2269
	step [189/190], loss=60.9043
	step [190/190], loss=53.3010
	Evaluating
	loss=0.2315, precision=0.1881, recall=0.9934, f1=0.3163
Training epoch 5
	step [1/190], loss=58.8316
	step [2/190], loss=63.1219
	step [3/190], loss=60.4516
	step [4/190], loss=60.6824
	step [5/190], loss=62.3822
	step [6/190], loss=60.8489
	step [7/190], loss=60.1108
	step [8/190], loss=60.0327
	step [9/190], loss=60.4149
	step [10/190], loss=59.9942
	step [11/190], loss=59.7093
	step [12/190], loss=61.6566
	step [13/190], loss=61.8223
	step [14/190], loss=57.0191
	step [15/190], loss=58.8883
	step [16/190], loss=57.7624
	step [17/190], loss=59.8695
	step [18/190], loss=59.6800
	step [19/190], loss=61.0043
	step [20/190], loss=59.6320
	step [21/190], loss=60.9824
	step [22/190], loss=59.7844
	step [23/190], loss=59.9601
	step [24/190], loss=58.6887
	step [25/190], loss=59.8870
	step [26/190], loss=58.3482
	step [27/190], loss=61.3245
	step [28/190], loss=58.1704
	step [29/190], loss=59.2325
	step [30/190], loss=59.6703
	step [31/190], loss=57.8734
	step [32/190], loss=60.1721
	step [33/190], loss=60.2961
	step [34/190], loss=58.4404
	step [35/190], loss=57.9492
	step [36/190], loss=59.7046
	step [37/190], loss=58.5153
	step [38/190], loss=59.2310
	step [39/190], loss=58.9487
	step [40/190], loss=57.4408
	step [41/190], loss=60.4078
	step [42/190], loss=61.3496
	step [43/190], loss=57.3718
	step [44/190], loss=57.3057
	step [45/190], loss=56.7620
	step [46/190], loss=56.8424
	step [47/190], loss=56.4392
	step [48/190], loss=55.2554
	step [49/190], loss=58.7559
	step [50/190], loss=56.7573
	step [51/190], loss=58.8886
	step [52/190], loss=56.9437
	step [53/190], loss=56.7391
	step [54/190], loss=57.8618
	step [55/190], loss=58.4772
	step [56/190], loss=60.0574
	step [57/190], loss=56.7228
	step [58/190], loss=56.4503
	step [59/190], loss=59.2851
	step [60/190], loss=57.4036
	step [61/190], loss=58.0834
	step [62/190], loss=56.5243
	step [63/190], loss=57.9846
	step [64/190], loss=54.4666
	step [65/190], loss=54.7946
	step [66/190], loss=57.6878
	step [67/190], loss=57.7050
	step [68/190], loss=58.9030
	step [69/190], loss=57.5739
	step [70/190], loss=57.9507
	step [71/190], loss=57.3756
	step [72/190], loss=56.4204
	step [73/190], loss=55.3388
	step [74/190], loss=56.7873
	step [75/190], loss=56.9300
	step [76/190], loss=55.4830
	step [77/190], loss=56.3178
	step [78/190], loss=57.4962
	step [79/190], loss=55.6278
	step [80/190], loss=54.7399
	step [81/190], loss=55.8222
	step [82/190], loss=54.4034
	step [83/190], loss=55.2394
	step [84/190], loss=56.3982
	step [85/190], loss=56.5386
	step [86/190], loss=53.9368
	step [87/190], loss=57.3781
	step [88/190], loss=55.0975
	step [89/190], loss=54.8708
	step [90/190], loss=55.2778
	step [91/190], loss=55.8764
	step [92/190], loss=56.2003
	step [93/190], loss=53.9817
	step [94/190], loss=53.7637
	step [95/190], loss=53.8120
	step [96/190], loss=54.0172
	step [97/190], loss=55.8637
	step [98/190], loss=53.4583
	step [99/190], loss=56.1683
	step [100/190], loss=52.9717
	step [101/190], loss=56.1276
	step [102/190], loss=53.8504
	step [103/190], loss=54.9030
	step [104/190], loss=58.1255
	step [105/190], loss=54.6147
	step [106/190], loss=54.2150
	step [107/190], loss=53.6308
	step [108/190], loss=55.5045
	step [109/190], loss=55.2501
	step [110/190], loss=55.9820
	step [111/190], loss=53.4400
	step [112/190], loss=59.0193
	step [113/190], loss=56.2696
	step [114/190], loss=53.3984
	step [115/190], loss=53.6317
	step [116/190], loss=52.9887
	step [117/190], loss=53.7502
	step [118/190], loss=56.7160
	step [119/190], loss=54.1400
	step [120/190], loss=52.6526
	step [121/190], loss=51.7164
	step [122/190], loss=53.6693
	step [123/190], loss=56.3232
	step [124/190], loss=53.4124
	step [125/190], loss=53.6608
	step [126/190], loss=53.2610
	step [127/190], loss=52.3880
	step [128/190], loss=54.5803
	step [129/190], loss=53.9268
	step [130/190], loss=53.6258
	step [131/190], loss=52.9707
	step [132/190], loss=52.8646
	step [133/190], loss=56.1407
	step [134/190], loss=51.6945
	step [135/190], loss=52.5119
	step [136/190], loss=51.9535
	step [137/190], loss=55.4541
	step [138/190], loss=52.9711
	step [139/190], loss=52.8937
	step [140/190], loss=53.3226
	step [141/190], loss=52.3909
	step [142/190], loss=52.8646
	step [143/190], loss=51.4371
	step [144/190], loss=52.2548
	step [145/190], loss=51.4443
	step [146/190], loss=53.5570
	step [147/190], loss=51.8389
	step [148/190], loss=52.4914
	step [149/190], loss=51.1931
	step [150/190], loss=53.1935
	step [151/190], loss=50.4747
	step [152/190], loss=53.1150
	step [153/190], loss=50.6018
	step [154/190], loss=50.4906
	step [155/190], loss=50.4881
	step [156/190], loss=50.0500
	step [157/190], loss=50.4057
	step [158/190], loss=51.2971
	step [159/190], loss=51.9744
	step [160/190], loss=50.2286
	step [161/190], loss=49.9644
	step [162/190], loss=52.5051
	step [163/190], loss=50.9871
	step [164/190], loss=52.4161
	step [165/190], loss=50.2639
	step [166/190], loss=50.0362
	step [167/190], loss=50.4410
	step [168/190], loss=51.4675
	step [169/190], loss=49.4997
	step [170/190], loss=50.1859
	step [171/190], loss=51.0676
	step [172/190], loss=50.6147
	step [173/190], loss=53.0674
	step [174/190], loss=51.5552
	step [175/190], loss=53.1159
	step [176/190], loss=51.0973
	step [177/190], loss=50.8729
	step [178/190], loss=50.5714
	step [179/190], loss=49.4140
	step [180/190], loss=49.5312
	step [181/190], loss=48.9049
	step [182/190], loss=49.9715
	step [183/190], loss=52.2967
	step [184/190], loss=48.7483
	step [185/190], loss=50.0950
	step [186/190], loss=50.0670
	step [187/190], loss=51.4560
	step [188/190], loss=48.7523
	step [189/190], loss=48.9836
	step [190/190], loss=42.9873
	Evaluating
	loss=0.1799, precision=0.2032, recall=0.9925, f1=0.3373
Training epoch 6
	step [1/190], loss=49.3841
	step [2/190], loss=48.9525
	step [3/190], loss=48.0768
	step [4/190], loss=50.9471
	step [5/190], loss=49.2171
	step [6/190], loss=48.8409
	step [7/190], loss=52.6560
	step [8/190], loss=48.1231
	step [9/190], loss=48.8277
	step [10/190], loss=49.0593
	step [11/190], loss=50.8335
	step [12/190], loss=48.6069
	step [13/190], loss=48.5662
	step [14/190], loss=47.2009
	step [15/190], loss=50.8636
	step [16/190], loss=48.8957
	step [17/190], loss=49.0171
	step [18/190], loss=48.1542
	step [19/190], loss=49.3743
	step [20/190], loss=50.6278
	step [21/190], loss=50.0012
	step [22/190], loss=49.0449
	step [23/190], loss=49.7064
	step [24/190], loss=47.7556
	step [25/190], loss=49.8269
	step [26/190], loss=47.2350
	step [27/190], loss=50.0625
	step [28/190], loss=46.7490
	step [29/190], loss=49.9101
	step [30/190], loss=45.6659
	step [31/190], loss=47.5636
	step [32/190], loss=47.8579
	step [33/190], loss=48.9239
	step [34/190], loss=46.2088
	step [35/190], loss=47.9023
	step [36/190], loss=47.4070
	step [37/190], loss=47.8659
	step [38/190], loss=48.9230
	step [39/190], loss=47.8374
	step [40/190], loss=49.7164
	step [41/190], loss=46.6203
	step [42/190], loss=47.2977
	step [43/190], loss=46.9942
	step [44/190], loss=47.0627
	step [45/190], loss=46.9348
	step [46/190], loss=48.6226
	step [47/190], loss=46.5731
	step [48/190], loss=49.2051
	step [49/190], loss=45.7606
	step [50/190], loss=48.8900
	step [51/190], loss=46.7638
	step [52/190], loss=47.5448
	step [53/190], loss=47.0090
	step [54/190], loss=48.1439
	step [55/190], loss=46.1244
	step [56/190], loss=44.5833
	step [57/190], loss=46.5530
	step [58/190], loss=45.9781
	step [59/190], loss=47.0669
	step [60/190], loss=47.3102
	step [61/190], loss=48.5370
	step [62/190], loss=48.1075
	step [63/190], loss=47.4110
	step [64/190], loss=46.9023
	step [65/190], loss=46.6775
	step [66/190], loss=46.7386
	step [67/190], loss=47.7319
	step [68/190], loss=46.1031
	step [69/190], loss=45.0051
	step [70/190], loss=46.0960
	step [71/190], loss=45.0995
	step [72/190], loss=44.3127
	step [73/190], loss=45.8763
	step [74/190], loss=46.5047
	step [75/190], loss=47.2131
	step [76/190], loss=46.5323
	step [77/190], loss=47.7032
	step [78/190], loss=48.7546
	step [79/190], loss=46.7397
	step [80/190], loss=45.4141
	step [81/190], loss=45.2317
	step [82/190], loss=45.5897
	step [83/190], loss=45.8342
	step [84/190], loss=44.1002
	step [85/190], loss=43.3701
	step [86/190], loss=44.6068
	step [87/190], loss=46.1639
	step [88/190], loss=48.7749
	step [89/190], loss=43.8713
	step [90/190], loss=45.8972
	step [91/190], loss=45.7505
	step [92/190], loss=43.4233
	step [93/190], loss=45.3740
	step [94/190], loss=44.7723
	step [95/190], loss=44.7193
	step [96/190], loss=43.2624
	step [97/190], loss=45.0210
	step [98/190], loss=44.5694
	step [99/190], loss=44.3511
	step [100/190], loss=45.6067
	step [101/190], loss=46.0754
	step [102/190], loss=44.3382
	step [103/190], loss=44.1351
	step [104/190], loss=46.0273
	step [105/190], loss=44.4100
	step [106/190], loss=42.5382
	step [107/190], loss=43.3342
	step [108/190], loss=46.3737
	step [109/190], loss=45.2437
	step [110/190], loss=45.2807
	step [111/190], loss=45.4493
	step [112/190], loss=42.4281
	step [113/190], loss=44.8168
	step [114/190], loss=44.8464
	step [115/190], loss=47.0625
	step [116/190], loss=44.9661
	step [117/190], loss=46.0511
	step [118/190], loss=45.2339
	step [119/190], loss=44.5115
	step [120/190], loss=44.0092
	step [121/190], loss=45.9238
	step [122/190], loss=46.3265
	step [123/190], loss=43.4231
	step [124/190], loss=43.2857
	step [125/190], loss=45.5871
	step [126/190], loss=45.7421
	step [127/190], loss=45.2596
	step [128/190], loss=44.8308
	step [129/190], loss=43.5930
	step [130/190], loss=44.3111
	step [131/190], loss=44.1388
	step [132/190], loss=42.0778
	step [133/190], loss=42.8636
	step [134/190], loss=45.0862
	step [135/190], loss=43.4579
	step [136/190], loss=46.2253
	step [137/190], loss=44.2854
	step [138/190], loss=44.6336
	step [139/190], loss=42.2823
	step [140/190], loss=42.2707
	step [141/190], loss=46.5315
	step [142/190], loss=41.8408
	step [143/190], loss=44.1911
	step [144/190], loss=42.8159
	step [145/190], loss=43.2948
	step [146/190], loss=43.7105
	step [147/190], loss=41.6983
	step [148/190], loss=41.7850
	step [149/190], loss=43.2141
	step [150/190], loss=43.4349
	step [151/190], loss=44.8218
	step [152/190], loss=41.6339
	step [153/190], loss=41.1369
	step [154/190], loss=43.0653
	step [155/190], loss=42.8701
	step [156/190], loss=42.7606
	step [157/190], loss=41.7623
	step [158/190], loss=41.8720
	step [159/190], loss=42.1994
	step [160/190], loss=44.0364
	step [161/190], loss=46.3528
	step [162/190], loss=42.8265
	step [163/190], loss=40.3553
	step [164/190], loss=41.5045
	step [165/190], loss=43.2217
	step [166/190], loss=43.8114
	step [167/190], loss=42.6296
	step [168/190], loss=41.5913
	step [169/190], loss=41.4182
	step [170/190], loss=44.2700
	step [171/190], loss=42.4804
	step [172/190], loss=41.7830
	step [173/190], loss=42.5477
	step [174/190], loss=41.3363
	step [175/190], loss=42.0665
	step [176/190], loss=43.1927
	step [177/190], loss=43.2968
	step [178/190], loss=42.5024
	step [179/190], loss=41.3549
	step [180/190], loss=41.8793
	step [181/190], loss=44.8816
	step [182/190], loss=41.4876
	step [183/190], loss=41.6247
	step [184/190], loss=38.6531
	step [185/190], loss=39.5205
	step [186/190], loss=42.0710
	step [187/190], loss=40.0002
	step [188/190], loss=40.5788
	step [189/190], loss=40.1854
	step [190/190], loss=37.4293
	Evaluating
	loss=0.1432, precision=0.2418, recall=0.9911, f1=0.3887
saving model as: 1_saved_model.pth
Training epoch 7
	step [1/190], loss=40.7614
	step [2/190], loss=38.6837
	step [3/190], loss=40.0613
	step [4/190], loss=40.4646
	step [5/190], loss=38.4672
	step [6/190], loss=40.9765
	step [7/190], loss=40.8258
	step [8/190], loss=39.8619
	step [9/190], loss=41.6222
	step [10/190], loss=42.2416
	step [11/190], loss=39.8142
	step [12/190], loss=40.6160
	step [13/190], loss=40.2637
	step [14/190], loss=42.4876
	step [15/190], loss=40.6900
	step [16/190], loss=43.0924
	step [17/190], loss=40.1465
	step [18/190], loss=39.7079
	step [19/190], loss=40.1520
	step [20/190], loss=39.6950
	step [21/190], loss=41.0799
	step [22/190], loss=41.1107
	step [23/190], loss=41.8536
	step [24/190], loss=42.7924
	step [25/190], loss=41.3570
	step [26/190], loss=39.1460
	step [27/190], loss=39.1394
	step [28/190], loss=39.6785
	step [29/190], loss=38.7839
	step [30/190], loss=40.5956
	step [31/190], loss=40.6061
	step [32/190], loss=39.8681
	step [33/190], loss=40.7188
	step [34/190], loss=42.0215
	step [35/190], loss=40.7498
	step [36/190], loss=38.8937
	step [37/190], loss=39.5532
	step [38/190], loss=39.3318
	step [39/190], loss=38.0393
	step [40/190], loss=40.9605
	step [41/190], loss=38.5237
	step [42/190], loss=39.9063
	step [43/190], loss=41.2026
	step [44/190], loss=39.6455
	step [45/190], loss=40.2904
	step [46/190], loss=41.1278
	step [47/190], loss=39.7070
	step [48/190], loss=40.4788
	step [49/190], loss=39.7714
	step [50/190], loss=39.1224
	step [51/190], loss=40.0815
	step [52/190], loss=38.5640
	step [53/190], loss=38.9263
	step [54/190], loss=41.0613
	step [55/190], loss=40.2937
	step [56/190], loss=38.4977
	step [57/190], loss=39.1397
	step [58/190], loss=40.9852
	step [59/190], loss=40.4385
	step [60/190], loss=43.4473
	step [61/190], loss=41.0696
	step [62/190], loss=37.9556
	step [63/190], loss=38.6377
	step [64/190], loss=36.6621
	step [65/190], loss=38.8578
	step [66/190], loss=40.6368
	step [67/190], loss=37.5954
	step [68/190], loss=40.5757
	step [69/190], loss=38.1244
	step [70/190], loss=38.3817
	step [71/190], loss=38.4379
	step [72/190], loss=40.6711
	step [73/190], loss=39.1531
	step [74/190], loss=39.8303
	step [75/190], loss=36.7462
	step [76/190], loss=39.2197
	step [77/190], loss=38.8002
	step [78/190], loss=39.2187
	step [79/190], loss=39.1030
	step [80/190], loss=36.7746
	step [81/190], loss=39.6199
	step [82/190], loss=39.3365
	step [83/190], loss=37.9277
	step [84/190], loss=37.2885
	step [85/190], loss=40.3206
	step [86/190], loss=37.8051
	step [87/190], loss=37.3424
	step [88/190], loss=38.4897
	step [89/190], loss=37.1913
	step [90/190], loss=39.1769
	step [91/190], loss=38.8185
	step [92/190], loss=36.9090
	step [93/190], loss=39.3513
	step [94/190], loss=37.7517
	step [95/190], loss=36.7975
	step [96/190], loss=37.5657
	step [97/190], loss=37.0254
	step [98/190], loss=39.7191
	step [99/190], loss=36.2174
	step [100/190], loss=35.5406
	step [101/190], loss=36.7270
	step [102/190], loss=36.9286
	step [103/190], loss=36.6487
	step [104/190], loss=35.8329
	step [105/190], loss=39.0942
	step [106/190], loss=37.9797
	step [107/190], loss=38.8755
	step [108/190], loss=38.4061
	step [109/190], loss=36.7907
	step [110/190], loss=37.5586
	step [111/190], loss=36.7823
	step [112/190], loss=39.4034
	step [113/190], loss=36.3814
	step [114/190], loss=37.7170
	step [115/190], loss=39.1888
	step [116/190], loss=38.4223
	step [117/190], loss=36.3378
	step [118/190], loss=36.0165
	step [119/190], loss=36.9839
	step [120/190], loss=35.3206
	step [121/190], loss=37.2999
	step [122/190], loss=38.1078
	step [123/190], loss=37.0452
	step [124/190], loss=37.5418
	step [125/190], loss=37.6869
	step [126/190], loss=36.4477
	step [127/190], loss=37.4729
	step [128/190], loss=36.2188
	step [129/190], loss=37.4925
	step [130/190], loss=35.3035
	step [131/190], loss=36.2066
	step [132/190], loss=36.3470
	step [133/190], loss=36.5643
	step [134/190], loss=35.0644
	step [135/190], loss=39.4108
	step [136/190], loss=35.6030
	step [137/190], loss=37.2159
	step [138/190], loss=34.5138
	step [139/190], loss=35.5237
	step [140/190], loss=33.6367
	step [141/190], loss=38.8345
	step [142/190], loss=37.7090
	step [143/190], loss=36.0554
	step [144/190], loss=33.7974
	step [145/190], loss=36.1407
	step [146/190], loss=37.2206
	step [147/190], loss=35.6299
	step [148/190], loss=34.7511
	step [149/190], loss=35.4825
	step [150/190], loss=35.7544
	step [151/190], loss=34.8493
	step [152/190], loss=35.0087
	step [153/190], loss=37.8237
	step [154/190], loss=34.1515
	step [155/190], loss=37.1936
	step [156/190], loss=36.7100
	step [157/190], loss=35.6358
	step [158/190], loss=34.4152
	step [159/190], loss=36.8110
	step [160/190], loss=36.2522
	step [161/190], loss=37.0835
	step [162/190], loss=35.2932
	step [163/190], loss=34.4808
	step [164/190], loss=36.5384
	step [165/190], loss=34.8726
	step [166/190], loss=38.5851
	step [167/190], loss=37.2048
	step [168/190], loss=34.3169
	step [169/190], loss=35.9532
	step [170/190], loss=39.3107
	step [171/190], loss=34.2239
	step [172/190], loss=34.9033
	step [173/190], loss=35.1881
	step [174/190], loss=33.4572
	step [175/190], loss=36.7755
	step [176/190], loss=34.6817
	step [177/190], loss=35.2747
	step [178/190], loss=35.2732
	step [179/190], loss=35.7819
	step [180/190], loss=35.8071
	step [181/190], loss=35.4269
	step [182/190], loss=34.6993
	step [183/190], loss=34.7687
	step [184/190], loss=34.3583
	step [185/190], loss=36.6566
	step [186/190], loss=33.7797
	step [187/190], loss=38.3481
	step [188/190], loss=37.1155
	step [189/190], loss=35.7402
	step [190/190], loss=31.9865
	Evaluating
	loss=0.1209, precision=0.2015, recall=0.9928, f1=0.3349
Training epoch 8
	step [1/190], loss=35.2579
	step [2/190], loss=36.5372
	step [3/190], loss=33.9499
	step [4/190], loss=35.0217
	step [5/190], loss=36.1692
	step [6/190], loss=32.7997
	step [7/190], loss=34.6868
	step [8/190], loss=33.7009
	step [9/190], loss=33.3290
	step [10/190], loss=33.6159
	step [11/190], loss=33.3498
	step [12/190], loss=32.1600
	step [13/190], loss=32.5793
	step [14/190], loss=35.6414
	step [15/190], loss=35.2249
	step [16/190], loss=36.1686
	step [17/190], loss=32.2601
	step [18/190], loss=33.6870
	step [19/190], loss=33.0531
	step [20/190], loss=32.8617
	step [21/190], loss=34.9662
	step [22/190], loss=34.9477
	step [23/190], loss=36.4626
	step [24/190], loss=32.8089
	step [25/190], loss=36.5334
	step [26/190], loss=35.7411
	step [27/190], loss=33.5131
	step [28/190], loss=34.2709
	step [29/190], loss=34.8017
	step [30/190], loss=34.6497
	step [31/190], loss=36.4891
	step [32/190], loss=37.0448
	step [33/190], loss=32.8707
	step [34/190], loss=33.7789
	step [35/190], loss=33.6440
	step [36/190], loss=31.9040
	step [37/190], loss=34.2667
	step [38/190], loss=32.8753
	step [39/190], loss=32.6724
	step [40/190], loss=32.6457
	step [41/190], loss=36.5073
	step [42/190], loss=34.4115
	step [43/190], loss=31.9584
	step [44/190], loss=32.9319
	step [45/190], loss=34.6147
	step [46/190], loss=32.0442
	step [47/190], loss=33.6949
	step [48/190], loss=33.5466
	step [49/190], loss=32.8683
	step [50/190], loss=33.6570
	step [51/190], loss=34.9092
	step [52/190], loss=36.0264
	step [53/190], loss=33.8513
	step [54/190], loss=37.3196
	step [55/190], loss=32.9411
	step [56/190], loss=34.3396
	step [57/190], loss=33.0730
	step [58/190], loss=31.8025
	step [59/190], loss=32.8145
	step [60/190], loss=33.3907
	step [61/190], loss=35.5060
	step [62/190], loss=31.5717
	step [63/190], loss=32.1450
	step [64/190], loss=31.3601
	step [65/190], loss=34.6963
	step [66/190], loss=32.2406
	step [67/190], loss=32.9304
	step [68/190], loss=33.3074
	step [69/190], loss=32.3936
	step [70/190], loss=33.5256
	step [71/190], loss=34.7633
	step [72/190], loss=33.0089
	step [73/190], loss=33.8596
	step [74/190], loss=33.2975
	step [75/190], loss=33.3470
	step [76/190], loss=31.4557
	step [77/190], loss=34.3396
	step [78/190], loss=32.5592
	step [79/190], loss=32.1510
	step [80/190], loss=32.8524
	step [81/190], loss=32.1975
	step [82/190], loss=30.8611
	step [83/190], loss=34.2560
	step [84/190], loss=33.4358
	step [85/190], loss=32.3497
	step [86/190], loss=34.6053
	step [87/190], loss=32.6393
	step [88/190], loss=31.0751
	step [89/190], loss=32.0891
	step [90/190], loss=31.9232
	step [91/190], loss=31.4907
	step [92/190], loss=32.4200
	step [93/190], loss=31.3138
	step [94/190], loss=31.6754
	step [95/190], loss=35.3581
	step [96/190], loss=31.3667
	step [97/190], loss=32.1702
	step [98/190], loss=31.1080
	step [99/190], loss=31.8064
	step [100/190], loss=32.4701
	step [101/190], loss=31.3577
	step [102/190], loss=32.8866
	step [103/190], loss=31.4827
	step [104/190], loss=33.5605
	step [105/190], loss=32.3549
	step [106/190], loss=34.4315
	step [107/190], loss=31.8299
	step [108/190], loss=31.6588
	step [109/190], loss=35.0132
	step [110/190], loss=31.5035
	step [111/190], loss=31.3577
	step [112/190], loss=30.8374
	step [113/190], loss=29.8263
	step [114/190], loss=31.3331
	step [115/190], loss=32.0465
	step [116/190], loss=33.8814
	step [117/190], loss=33.7738
	step [118/190], loss=29.2623
	step [119/190], loss=31.8114
	step [120/190], loss=32.5159
	step [121/190], loss=31.5268
	step [122/190], loss=29.0968
	step [123/190], loss=31.4430
	step [124/190], loss=34.4069
	step [125/190], loss=32.3319
	step [126/190], loss=33.2981
	step [127/190], loss=33.8758
	step [128/190], loss=31.1705
	step [129/190], loss=32.7529
	step [130/190], loss=31.4937
	step [131/190], loss=31.7274
	step [132/190], loss=33.9983
	step [133/190], loss=31.2984
	step [134/190], loss=30.8128
	step [135/190], loss=31.9061
	step [136/190], loss=30.3178
	step [137/190], loss=32.9051
	step [138/190], loss=31.7743
	step [139/190], loss=31.1282
	step [140/190], loss=30.7542
	step [141/190], loss=29.4418
	step [142/190], loss=33.6839
	step [143/190], loss=31.0806
	step [144/190], loss=30.8399
	step [145/190], loss=31.1127
	step [146/190], loss=30.2505
	step [147/190], loss=32.0723
	step [148/190], loss=29.5992
	step [149/190], loss=31.0410
	step [150/190], loss=30.6521
	step [151/190], loss=31.4095
	step [152/190], loss=29.7533
	step [153/190], loss=31.3178
	step [154/190], loss=32.7215
	step [155/190], loss=30.4494
	step [156/190], loss=30.9605
	step [157/190], loss=30.2137
	step [158/190], loss=30.4032
	step [159/190], loss=32.0493
	step [160/190], loss=30.5081
	step [161/190], loss=30.4226
	step [162/190], loss=30.5769
	step [163/190], loss=29.4443
	step [164/190], loss=29.9224
	step [165/190], loss=30.0527
	step [166/190], loss=33.2093
	step [167/190], loss=30.5592
	step [168/190], loss=32.2305
	step [169/190], loss=30.3054
	step [170/190], loss=30.4073
	step [171/190], loss=29.9987
	step [172/190], loss=29.4050
	step [173/190], loss=29.0444
	step [174/190], loss=31.1927
	step [175/190], loss=30.1044
	step [176/190], loss=29.0852
	step [177/190], loss=33.2630
	step [178/190], loss=28.5668
	step [179/190], loss=29.1310
	step [180/190], loss=30.7439
	step [181/190], loss=32.5808
	step [182/190], loss=30.7719
	step [183/190], loss=27.8677
	step [184/190], loss=27.6234
	step [185/190], loss=30.1207
	step [186/190], loss=31.0121
	step [187/190], loss=28.9398
	step [188/190], loss=30.3723
	step [189/190], loss=29.6177
	step [190/190], loss=25.3559
	Evaluating
	loss=0.1005, precision=0.2294, recall=0.9917, f1=0.3726
Training epoch 9
	step [1/190], loss=28.7891
	step [2/190], loss=30.9297
	step [3/190], loss=31.1197
	step [4/190], loss=30.1882
	step [5/190], loss=28.7576
	step [6/190], loss=29.1759
	step [7/190], loss=30.9778
	step [8/190], loss=28.6951
	step [9/190], loss=31.2125
	step [10/190], loss=28.8566
	step [11/190], loss=28.6180
	step [12/190], loss=28.4336
	step [13/190], loss=32.6794
	step [14/190], loss=31.8752
	step [15/190], loss=29.5503
	step [16/190], loss=28.0943
	step [17/190], loss=28.6572
	step [18/190], loss=31.5711
	step [19/190], loss=30.3869
	step [20/190], loss=30.7180
	step [21/190], loss=31.0317
	step [22/190], loss=27.7118
	step [23/190], loss=27.3866
	step [24/190], loss=28.7201
	step [25/190], loss=31.5677
	step [26/190], loss=28.5728
	step [27/190], loss=27.1702
	step [28/190], loss=27.7612
	step [29/190], loss=29.7246
	step [30/190], loss=26.9616
	step [31/190], loss=28.4367
	step [32/190], loss=28.9697
	step [33/190], loss=29.9448
	step [34/190], loss=29.7641
	step [35/190], loss=28.8659
	step [36/190], loss=29.3775
	step [37/190], loss=30.6188
	step [38/190], loss=27.5457
	step [39/190], loss=26.5247
	step [40/190], loss=27.7530
	step [41/190], loss=30.0803
	step [42/190], loss=30.0959
	step [43/190], loss=27.2512
	step [44/190], loss=30.5337
	step [45/190], loss=27.0350
	step [46/190], loss=31.0224
	step [47/190], loss=29.1563
	step [48/190], loss=29.5473
	step [49/190], loss=31.8962
	step [50/190], loss=29.9013
	step [51/190], loss=29.0846
	step [52/190], loss=29.4645
	step [53/190], loss=27.9914
	step [54/190], loss=29.6741
	step [55/190], loss=28.3003
	step [56/190], loss=27.8862
	step [57/190], loss=26.7642
	step [58/190], loss=30.3897
	step [59/190], loss=28.7098
	step [60/190], loss=28.9849
	step [61/190], loss=28.5116
	step [62/190], loss=28.5226
	step [63/190], loss=29.1728
	step [64/190], loss=27.0403
	step [65/190], loss=29.9242
	step [66/190], loss=26.2161
	step [67/190], loss=29.0983
	step [68/190], loss=27.9919
	step [69/190], loss=31.5674
	step [70/190], loss=29.9839
	step [71/190], loss=28.9275
	step [72/190], loss=27.6589
	step [73/190], loss=27.2733
	step [74/190], loss=28.0648
	step [75/190], loss=27.5505
	step [76/190], loss=28.2200
	step [77/190], loss=29.0900
	step [78/190], loss=27.2360
	step [79/190], loss=27.5736
	step [80/190], loss=27.4526
	step [81/190], loss=28.3349
	step [82/190], loss=28.0641
	step [83/190], loss=30.1756
	step [84/190], loss=29.0441
	step [85/190], loss=29.9436
	step [86/190], loss=28.7717
	step [87/190], loss=28.4700
	step [88/190], loss=26.3602
	step [89/190], loss=26.6189
	step [90/190], loss=26.0971
	step [91/190], loss=27.6575
	step [92/190], loss=27.4044
	step [93/190], loss=29.4238
	step [94/190], loss=28.7004
	step [95/190], loss=25.9493
	step [96/190], loss=27.0797
	step [97/190], loss=28.1024
	step [98/190], loss=26.1260
	step [99/190], loss=28.1809
	step [100/190], loss=27.7551
	step [101/190], loss=26.9678
	step [102/190], loss=31.2315
	step [103/190], loss=29.4727
	step [104/190], loss=26.4852
	step [105/190], loss=27.5740
	step [106/190], loss=29.4756
	step [107/190], loss=27.3224
	step [108/190], loss=26.9209
	step [109/190], loss=29.2323
	step [110/190], loss=28.5439
	step [111/190], loss=27.3348
	step [112/190], loss=26.2291
	step [113/190], loss=26.4995
	step [114/190], loss=28.3604
	step [115/190], loss=30.8289
	step [116/190], loss=29.4178
	step [117/190], loss=26.8926
	step [118/190], loss=25.4569
	step [119/190], loss=26.6814
	step [120/190], loss=25.1665
	step [121/190], loss=25.6997
	step [122/190], loss=27.4119
	step [123/190], loss=27.8984
	step [124/190], loss=29.5112
	step [125/190], loss=28.1130
	step [126/190], loss=30.4209
	step [127/190], loss=27.1466
	step [128/190], loss=26.8896
	step [129/190], loss=26.6005
	step [130/190], loss=27.2002
	step [131/190], loss=26.0731
	step [132/190], loss=25.7029
	step [133/190], loss=26.2006
	step [134/190], loss=27.9339
	step [135/190], loss=26.5477
	step [136/190], loss=27.9307
	step [137/190], loss=24.8558
	step [138/190], loss=27.1532
	step [139/190], loss=27.4547
	step [140/190], loss=27.0857
	step [141/190], loss=27.3909
	step [142/190], loss=26.2079
	step [143/190], loss=25.4652
	step [144/190], loss=27.0790
	step [145/190], loss=27.0216
	step [146/190], loss=26.7182
	step [147/190], loss=27.2425
	step [148/190], loss=27.1185
	step [149/190], loss=31.6805
	step [150/190], loss=26.7588
	step [151/190], loss=27.0837
	step [152/190], loss=26.0863
	step [153/190], loss=29.0864
	step [154/190], loss=24.9809
	step [155/190], loss=27.7289
	step [156/190], loss=27.2006
	step [157/190], loss=26.3918
	step [158/190], loss=27.2715
	step [159/190], loss=26.1693
	step [160/190], loss=29.3607
	step [161/190], loss=26.4460
	step [162/190], loss=25.0884
	step [163/190], loss=25.3022
	step [164/190], loss=27.8712
	step [165/190], loss=27.5444
	step [166/190], loss=26.6683
	step [167/190], loss=27.9304
	step [168/190], loss=26.8610
	step [169/190], loss=24.7957
	step [170/190], loss=25.8374
	step [171/190], loss=29.0559
	step [172/190], loss=24.3580
	step [173/190], loss=26.9136
	step [174/190], loss=25.7711
	step [175/190], loss=29.1681
	step [176/190], loss=26.6486
	step [177/190], loss=27.2411
	step [178/190], loss=26.2728
	step [179/190], loss=26.5059
	step [180/190], loss=28.4302
	step [181/190], loss=28.6380
	step [182/190], loss=26.2189
	step [183/190], loss=26.4078
	step [184/190], loss=26.0943
	step [185/190], loss=27.4809
	step [186/190], loss=25.9470
	step [187/190], loss=27.8881
	step [188/190], loss=29.1428
	step [189/190], loss=27.3002
	step [190/190], loss=23.2471
	Evaluating
	loss=0.0895, precision=0.2044, recall=0.9925, f1=0.3389
Training epoch 10
	step [1/190], loss=24.8980
	step [2/190], loss=26.1527
	step [3/190], loss=24.9302
	step [4/190], loss=25.7193
	step [5/190], loss=26.6331
	step [6/190], loss=24.3686
	step [7/190], loss=25.7211
	step [8/190], loss=25.5924
	step [9/190], loss=25.7055
	step [10/190], loss=28.8535
	step [11/190], loss=24.0254
	step [12/190], loss=27.2203
	step [13/190], loss=25.1247
	step [14/190], loss=27.3258
	step [15/190], loss=26.3255
	step [16/190], loss=26.3819
	step [17/190], loss=25.5592
	step [18/190], loss=24.2527
	step [19/190], loss=24.2860
	step [20/190], loss=25.3952
	step [21/190], loss=24.8859
	step [22/190], loss=29.3879
	step [23/190], loss=27.1895
	step [24/190], loss=25.6792
	step [25/190], loss=24.6327
	step [26/190], loss=28.0297
	step [27/190], loss=24.9171
	step [28/190], loss=23.6835
	step [29/190], loss=25.2546
	step [30/190], loss=26.3255
	step [31/190], loss=25.2050
	step [32/190], loss=26.9643
	step [33/190], loss=23.6718
	step [34/190], loss=26.3899
	step [35/190], loss=23.9047
	step [36/190], loss=24.6329
	step [37/190], loss=26.8013
	step [38/190], loss=24.9455
	step [39/190], loss=25.4861
	step [40/190], loss=26.6934
	step [41/190], loss=24.0507
	step [42/190], loss=26.2689
	step [43/190], loss=24.7411
	step [44/190], loss=25.9116
	step [45/190], loss=27.4850
	step [46/190], loss=26.9609
	step [47/190], loss=25.1789
	step [48/190], loss=26.2158
	step [49/190], loss=23.2620
	step [50/190], loss=23.8133
	step [51/190], loss=23.8929
	step [52/190], loss=25.5599
	step [53/190], loss=26.2052
	step [54/190], loss=24.6214
	step [55/190], loss=27.1065
	step [56/190], loss=24.9216
	step [57/190], loss=23.8244
	step [58/190], loss=24.4198
	step [59/190], loss=25.5191
	step [60/190], loss=27.5848
	step [61/190], loss=27.3360
	step [62/190], loss=24.9937
	step [63/190], loss=23.8325
	step [64/190], loss=24.1294
	step [65/190], loss=23.4673
	step [66/190], loss=25.3608
	step [67/190], loss=25.3156
	step [68/190], loss=24.8781
	step [69/190], loss=24.9093
	step [70/190], loss=24.8792
	step [71/190], loss=26.3078
	step [72/190], loss=23.6301
	step [73/190], loss=26.6362
	step [74/190], loss=24.6787
	step [75/190], loss=25.4876
	step [76/190], loss=24.1534
	step [77/190], loss=23.7381
	step [78/190], loss=27.3001
	step [79/190], loss=27.3972
	step [80/190], loss=25.7246
	step [81/190], loss=25.4159
	step [82/190], loss=23.8230
	step [83/190], loss=24.9459
	step [84/190], loss=23.6806
	step [85/190], loss=26.0969
	step [86/190], loss=24.7015
	step [87/190], loss=23.7876
	step [88/190], loss=24.6975
	step [89/190], loss=25.1227
	step [90/190], loss=27.3391
	step [91/190], loss=23.1675
	step [92/190], loss=26.1714
	step [93/190], loss=24.9093
	step [94/190], loss=23.2697
	step [95/190], loss=24.8488
	step [96/190], loss=25.1753
	step [97/190], loss=22.9887
	step [98/190], loss=25.9359
	step [99/190], loss=25.6128
	step [100/190], loss=26.2729
	step [101/190], loss=25.3046
	step [102/190], loss=23.8718
	step [103/190], loss=24.3828
	step [104/190], loss=23.5340
	step [105/190], loss=21.7098
	step [106/190], loss=23.7839
	step [107/190], loss=24.9065
	step [108/190], loss=25.4585
	step [109/190], loss=23.1741
	step [110/190], loss=23.0277
	step [111/190], loss=26.0144
	step [112/190], loss=21.4561
	step [113/190], loss=25.7194
	step [114/190], loss=26.8461
	step [115/190], loss=22.6397
	step [116/190], loss=23.2516
	step [117/190], loss=25.5110
	step [118/190], loss=24.1063
	step [119/190], loss=24.4900
	step [120/190], loss=24.2859
	step [121/190], loss=24.4315
	step [122/190], loss=24.4035
	step [123/190], loss=23.8512
	step [124/190], loss=23.3629
	step [125/190], loss=24.1476
	step [126/190], loss=25.0280
	step [127/190], loss=24.8336
	step [128/190], loss=28.4414
	step [129/190], loss=23.6520
	step [130/190], loss=23.5228
	step [131/190], loss=22.8938
	step [132/190], loss=25.6454
	step [133/190], loss=24.0483
	step [134/190], loss=23.8389
	step [135/190], loss=23.7817
	step [136/190], loss=24.4241
	step [137/190], loss=23.8252
	step [138/190], loss=22.3494
	step [139/190], loss=23.9989
	step [140/190], loss=24.8548
	step [141/190], loss=24.4336
	step [142/190], loss=23.2440
	step [143/190], loss=24.8896
	step [144/190], loss=24.4755
	step [145/190], loss=23.2253
	step [146/190], loss=25.0450
	step [147/190], loss=22.6328
	step [148/190], loss=25.7478
	step [149/190], loss=24.7023
	step [150/190], loss=23.6845
	step [151/190], loss=25.1942
	step [152/190], loss=24.3961
	step [153/190], loss=22.4128
	step [154/190], loss=24.6367
	step [155/190], loss=23.0064
	step [156/190], loss=24.2066
	step [157/190], loss=24.5091
	step [158/190], loss=25.9752
	step [159/190], loss=21.9407
	step [160/190], loss=22.0608
	step [161/190], loss=23.8108
	step [162/190], loss=22.8219
	step [163/190], loss=21.4766
	step [164/190], loss=21.2122
	step [165/190], loss=25.3489
	step [166/190], loss=22.1550
	step [167/190], loss=24.8583
	step [168/190], loss=25.2645
	step [169/190], loss=22.3028
	step [170/190], loss=25.1232
	step [171/190], loss=25.2169
	step [172/190], loss=24.7343
	step [173/190], loss=23.2202
	step [174/190], loss=23.3394
	step [175/190], loss=24.1931
	step [176/190], loss=22.7273
	step [177/190], loss=22.9289
	step [178/190], loss=22.7603
	step [179/190], loss=22.4890
	step [180/190], loss=24.2748
	step [181/190], loss=23.4931
	step [182/190], loss=24.8079
	step [183/190], loss=24.6384
	step [184/190], loss=22.8363
	step [185/190], loss=24.9038
	step [186/190], loss=21.7730
	step [187/190], loss=26.2523
	step [188/190], loss=23.1400
	step [189/190], loss=24.7567
	step [190/190], loss=20.7462
	Evaluating
	loss=0.0800, precision=0.1690, recall=0.9930, f1=0.2889
Training epoch 11
	step [1/190], loss=24.4083
	step [2/190], loss=23.3805
	step [3/190], loss=25.9913
	step [4/190], loss=23.4063
	step [5/190], loss=22.2758
	step [6/190], loss=23.3143
	step [7/190], loss=22.4960
	step [8/190], loss=21.8914
	step [9/190], loss=22.0577
	step [10/190], loss=25.8711
	step [11/190], loss=21.8689
	step [12/190], loss=23.6599
	step [13/190], loss=23.1551
	step [14/190], loss=21.0330
	step [15/190], loss=23.9094
	step [16/190], loss=22.2944
	step [17/190], loss=22.3807
	step [18/190], loss=23.1030
	step [19/190], loss=23.1499
	step [20/190], loss=23.1736
	step [21/190], loss=21.8538
	step [22/190], loss=22.5342
	step [23/190], loss=20.5994
	step [24/190], loss=22.9789
	step [25/190], loss=22.0540
	step [26/190], loss=21.5038
	step [27/190], loss=21.3878
	step [28/190], loss=23.2097
	step [29/190], loss=20.9930
	step [30/190], loss=21.8078
	step [31/190], loss=21.1249
	step [32/190], loss=22.4947
	step [33/190], loss=21.9334
	step [34/190], loss=22.7435
	step [35/190], loss=23.4345
	step [36/190], loss=22.0137
	step [37/190], loss=25.8541
	step [38/190], loss=24.1452
	step [39/190], loss=21.6250
	step [40/190], loss=22.2721
	step [41/190], loss=23.0517
	step [42/190], loss=26.4807
	step [43/190], loss=22.9225
	step [44/190], loss=24.9407
	step [45/190], loss=21.7189
	step [46/190], loss=25.3472
	step [47/190], loss=23.1243
	step [48/190], loss=22.8039
	step [49/190], loss=21.6962
	step [50/190], loss=23.9617
	step [51/190], loss=20.8069
	step [52/190], loss=23.3816
	step [53/190], loss=22.6742
	step [54/190], loss=22.6165
	step [55/190], loss=21.7903
	step [56/190], loss=21.7998
	step [57/190], loss=23.5821
	step [58/190], loss=22.3008
	step [59/190], loss=22.0241
	step [60/190], loss=21.1474
	step [61/190], loss=21.1656
	step [62/190], loss=24.4748
	step [63/190], loss=23.1514
	step [64/190], loss=21.1632
	step [65/190], loss=21.0622
	step [66/190], loss=23.3079
	step [67/190], loss=23.1068
	step [68/190], loss=23.7970
	step [69/190], loss=24.3959
	step [70/190], loss=21.5383
	step [71/190], loss=21.2154
	step [72/190], loss=22.1526
	step [73/190], loss=22.3619
	step [74/190], loss=21.1804
	step [75/190], loss=21.6928
	step [76/190], loss=22.7839
	step [77/190], loss=21.2686
	step [78/190], loss=21.8215
	step [79/190], loss=19.5078
	step [80/190], loss=22.2765
	step [81/190], loss=25.2300
	step [82/190], loss=22.8447
	step [83/190], loss=22.5423
	step [84/190], loss=22.4290
	step [85/190], loss=21.6541
	step [86/190], loss=22.9313
	step [87/190], loss=23.3500
	step [88/190], loss=20.8200
	step [89/190], loss=21.4388
	step [90/190], loss=22.0998
	step [91/190], loss=24.8473
	step [92/190], loss=21.5564
	step [93/190], loss=25.2482
	step [94/190], loss=23.8610
	step [95/190], loss=24.4133
	step [96/190], loss=22.2129
	step [97/190], loss=21.7873
	step [98/190], loss=22.6652
	step [99/190], loss=22.1837
	step [100/190], loss=22.8465
	step [101/190], loss=23.0106
	step [102/190], loss=22.2986
	step [103/190], loss=22.5540
	step [104/190], loss=23.7091
	step [105/190], loss=20.8602
	step [106/190], loss=23.9444
	step [107/190], loss=21.1935
	step [108/190], loss=22.2660
	step [109/190], loss=19.5540
	step [110/190], loss=24.1881
	step [111/190], loss=21.8044
	step [112/190], loss=21.7353
	step [113/190], loss=20.8556
	step [114/190], loss=21.5288
	step [115/190], loss=21.9931
	step [116/190], loss=21.1792
	step [117/190], loss=22.3231
	step [118/190], loss=23.5648
	step [119/190], loss=21.9569
	step [120/190], loss=20.4377
	step [121/190], loss=20.3289
	step [122/190], loss=24.7574
	step [123/190], loss=21.3888
	step [124/190], loss=21.9404
	step [125/190], loss=21.8244
	step [126/190], loss=23.0629
	step [127/190], loss=23.3961
	step [128/190], loss=23.0890
	step [129/190], loss=21.4887
	step [130/190], loss=20.4063
	step [131/190], loss=22.1803
	step [132/190], loss=21.4158
	step [133/190], loss=21.1315
	step [134/190], loss=18.4430
	step [135/190], loss=21.2086
	step [136/190], loss=20.1564
	step [137/190], loss=21.7919
	step [138/190], loss=20.3802
	step [139/190], loss=19.5530
	step [140/190], loss=21.6296
	step [141/190], loss=20.2684
	step [142/190], loss=23.8218
	step [143/190], loss=19.6311
	step [144/190], loss=18.9195
	step [145/190], loss=20.9584
	step [146/190], loss=21.9725
	step [147/190], loss=20.3718
	step [148/190], loss=20.2205
	step [149/190], loss=18.6988
	step [150/190], loss=22.1465
	step [151/190], loss=22.9884
	step [152/190], loss=23.6497
	step [153/190], loss=21.5132
	step [154/190], loss=21.1993
	step [155/190], loss=22.5421
	step [156/190], loss=19.6110
	step [157/190], loss=21.7204
	step [158/190], loss=22.8713
	step [159/190], loss=21.8331
	step [160/190], loss=20.2379
	step [161/190], loss=23.5205
	step [162/190], loss=19.6655
	step [163/190], loss=20.0656
	step [164/190], loss=20.3773
	step [165/190], loss=21.6577
	step [166/190], loss=20.0904
	step [167/190], loss=20.5026
	step [168/190], loss=20.2385
	step [169/190], loss=21.0640
	step [170/190], loss=19.6287
	step [171/190], loss=21.1805
	step [172/190], loss=23.9521
	step [173/190], loss=19.6581
	step [174/190], loss=20.5476
	step [175/190], loss=22.6825
	step [176/190], loss=19.9739
	step [177/190], loss=20.7429
	step [178/190], loss=22.1034
	step [179/190], loss=20.7462
	step [180/190], loss=20.6630
	step [181/190], loss=21.1898
	step [182/190], loss=23.2932
	step [183/190], loss=22.8013
	step [184/190], loss=21.2367
	step [185/190], loss=21.2379
	step [186/190], loss=21.3321
	step [187/190], loss=18.9819
	step [188/190], loss=23.4956
	step [189/190], loss=19.1350
	step [190/190], loss=18.2539
	Evaluating
	loss=0.0682, precision=0.2171, recall=0.9917, f1=0.3562
Training epoch 12
	step [1/190], loss=22.5204
	step [2/190], loss=21.5870
	step [3/190], loss=21.6210
	step [4/190], loss=20.6416
	step [5/190], loss=18.4739
	step [6/190], loss=20.5031
	step [7/190], loss=22.7731
	step [8/190], loss=21.0895
	step [9/190], loss=22.4540
	step [10/190], loss=19.8620
	step [11/190], loss=20.5059
	step [12/190], loss=20.8082
	step [13/190], loss=19.8947
	step [14/190], loss=19.8780
	step [15/190], loss=21.0666
	step [16/190], loss=20.9876
	step [17/190], loss=18.4179
	step [18/190], loss=20.4121
	step [19/190], loss=21.2535
	step [20/190], loss=21.5787
	step [21/190], loss=19.9949
	step [22/190], loss=22.1817
	step [23/190], loss=21.9294
	step [24/190], loss=21.4616
	step [25/190], loss=21.5022
	step [26/190], loss=19.9950
	step [27/190], loss=19.7233
	step [28/190], loss=24.2206
	step [29/190], loss=18.7280
	step [30/190], loss=20.3652
	step [31/190], loss=19.1068
	step [32/190], loss=18.2929
	step [33/190], loss=20.8495
	step [34/190], loss=21.9115
	step [35/190], loss=25.3905
	step [36/190], loss=22.7574
	step [37/190], loss=21.8029
	step [38/190], loss=21.1144
	step [39/190], loss=19.8836
	step [40/190], loss=21.1473
	step [41/190], loss=20.0041
	step [42/190], loss=24.7288
	step [43/190], loss=20.8190
	step [44/190], loss=19.0509
	step [45/190], loss=20.6692
	step [46/190], loss=23.0579
	step [47/190], loss=19.8989
	step [48/190], loss=19.1809
	step [49/190], loss=21.7650
	step [50/190], loss=17.4789
	step [51/190], loss=20.2697
	step [52/190], loss=22.1954
	step [53/190], loss=18.5374
	step [54/190], loss=19.1630
	step [55/190], loss=21.2088
	step [56/190], loss=17.7809
	step [57/190], loss=20.4549
	step [58/190], loss=22.1807
	step [59/190], loss=17.4948
	step [60/190], loss=20.6981
	step [61/190], loss=22.6112
	step [62/190], loss=20.3403
	step [63/190], loss=17.7997
	step [64/190], loss=19.2405
	step [65/190], loss=20.2068
	step [66/190], loss=21.3125
	step [67/190], loss=18.6832
	step [68/190], loss=18.8006
	step [69/190], loss=21.3433
	step [70/190], loss=18.8848
	step [71/190], loss=21.0506
	step [72/190], loss=22.0838
	step [73/190], loss=19.1469
	step [74/190], loss=19.4767
	step [75/190], loss=19.5100
	step [76/190], loss=21.4593
	step [77/190], loss=19.0461
	step [78/190], loss=20.1659
	step [79/190], loss=19.8973
	step [80/190], loss=18.1382
	step [81/190], loss=19.8432
	step [82/190], loss=21.4340
	step [83/190], loss=21.2037
	step [84/190], loss=20.4560
	step [85/190], loss=18.5083
	step [86/190], loss=18.4066
	step [87/190], loss=18.7044
	step [88/190], loss=20.6684
	step [89/190], loss=21.2500
	step [90/190], loss=20.9185
	step [91/190], loss=19.0625
	step [92/190], loss=18.2082
	step [93/190], loss=21.7652
	step [94/190], loss=20.1787
	step [95/190], loss=18.7117
	step [96/190], loss=18.4419
	step [97/190], loss=18.1032
	step [98/190], loss=20.9889
	step [99/190], loss=18.1967
	step [100/190], loss=19.7047
	step [101/190], loss=19.4846
	step [102/190], loss=21.4412
	step [103/190], loss=18.2648
	step [104/190], loss=19.8128
	step [105/190], loss=20.4766
	step [106/190], loss=18.8689
	step [107/190], loss=18.6474
	step [108/190], loss=21.1105
	step [109/190], loss=19.7361
	step [110/190], loss=19.2725
	step [111/190], loss=20.9926
	step [112/190], loss=18.5264
	step [113/190], loss=19.0166
	step [114/190], loss=21.0054
	step [115/190], loss=19.5471
	step [116/190], loss=17.7015
	step [117/190], loss=19.7795
	step [118/190], loss=18.4665
	step [119/190], loss=17.2435
	step [120/190], loss=18.1244
	step [121/190], loss=21.5107
	step [122/190], loss=21.6650
	step [123/190], loss=20.9414
	step [124/190], loss=21.1359
	step [125/190], loss=20.8315
	step [126/190], loss=19.9709
	step [127/190], loss=19.7160
	step [128/190], loss=19.8597
	step [129/190], loss=19.0063
	step [130/190], loss=18.6227
	step [131/190], loss=20.2855
	step [132/190], loss=17.2337
	step [133/190], loss=18.7970
	step [134/190], loss=20.2365
	step [135/190], loss=23.2674
	step [136/190], loss=19.4841
	step [137/190], loss=18.7780
	step [138/190], loss=18.8826
	step [139/190], loss=17.3928
	step [140/190], loss=19.0390
	step [141/190], loss=19.9280
	step [142/190], loss=24.3247
	step [143/190], loss=19.8981
	step [144/190], loss=17.8057
	step [145/190], loss=18.4937
	step [146/190], loss=19.6643
	step [147/190], loss=19.2861
	step [148/190], loss=19.1598
	step [149/190], loss=19.8071
	step [150/190], loss=18.5736
	step [151/190], loss=19.7302
	step [152/190], loss=18.5675
	step [153/190], loss=21.2039
	step [154/190], loss=18.2019
	step [155/190], loss=19.4725
	step [156/190], loss=18.1696
	step [157/190], loss=18.3230
	step [158/190], loss=17.6773
	step [159/190], loss=19.2782
	step [160/190], loss=18.5386
	step [161/190], loss=18.0713
	step [162/190], loss=19.3322
	step [163/190], loss=19.7580
	step [164/190], loss=22.4364
	step [165/190], loss=19.4270
	step [166/190], loss=20.2602
	step [167/190], loss=18.6879
	step [168/190], loss=19.5792
	step [169/190], loss=19.6811
	step [170/190], loss=18.7198
	step [171/190], loss=17.3530
	step [172/190], loss=18.1368
	step [173/190], loss=18.6127
	step [174/190], loss=19.0355
	step [175/190], loss=21.1588
	step [176/190], loss=21.3264
	step [177/190], loss=20.2951
	step [178/190], loss=19.6049
	step [179/190], loss=20.1148
	step [180/190], loss=19.3685
	step [181/190], loss=20.7056
	step [182/190], loss=19.9775
	step [183/190], loss=19.2161
	step [184/190], loss=17.1254
	step [185/190], loss=19.8125
	step [186/190], loss=19.7895
	step [187/190], loss=20.2378
	step [188/190], loss=20.1341
	step [189/190], loss=18.1479
	step [190/190], loss=17.0861
	Evaluating
	loss=0.0629, precision=0.2053, recall=0.9929, f1=0.3402
Training epoch 13
	step [1/190], loss=17.2868
	step [2/190], loss=18.3247
	step [3/190], loss=18.5201
	step [4/190], loss=19.0335
	step [5/190], loss=19.6870
	step [6/190], loss=18.3954
	step [7/190], loss=18.1349
	step [8/190], loss=17.1516
	step [9/190], loss=18.7278
	step [10/190], loss=18.2601
	step [11/190], loss=20.5726
	step [12/190], loss=19.3953
	step [13/190], loss=20.6487
	step [14/190], loss=18.9360
	step [15/190], loss=18.0785
	step [16/190], loss=17.5356
	step [17/190], loss=17.7517
	step [18/190], loss=18.6855
	step [19/190], loss=19.5320
	step [20/190], loss=19.9562
	step [21/190], loss=20.6447
	step [22/190], loss=19.1806
	step [23/190], loss=22.0938
	step [24/190], loss=16.1658
	step [25/190], loss=18.1248
	step [26/190], loss=17.8567
	step [27/190], loss=19.2933
	step [28/190], loss=17.9295
	step [29/190], loss=19.1015
	step [30/190], loss=16.6539
	step [31/190], loss=18.0732
	step [32/190], loss=19.4496
	step [33/190], loss=19.1931
	step [34/190], loss=16.8172
	step [35/190], loss=17.7791
	step [36/190], loss=18.3394
	step [37/190], loss=18.8839
	step [38/190], loss=17.5487
	step [39/190], loss=20.6539
	step [40/190], loss=18.1798
	step [41/190], loss=18.9871
	step [42/190], loss=20.4062
	step [43/190], loss=19.4946
	step [44/190], loss=19.5294
	step [45/190], loss=18.2503
	step [46/190], loss=19.5266
	step [47/190], loss=19.4210
	step [48/190], loss=17.7922
	step [49/190], loss=19.7126
	step [50/190], loss=19.8089
	step [51/190], loss=18.7390
	step [52/190], loss=16.8831
	step [53/190], loss=18.9739
	step [54/190], loss=19.0226
	step [55/190], loss=18.6846
	step [56/190], loss=20.5153
	step [57/190], loss=15.6582
	step [58/190], loss=19.5880
	step [59/190], loss=17.5422
	step [60/190], loss=18.2650
	step [61/190], loss=18.4276
	step [62/190], loss=18.3664
	step [63/190], loss=17.8110
	step [64/190], loss=19.9914
	step [65/190], loss=18.3111
	step [66/190], loss=17.4678
	step [67/190], loss=18.6277
	step [68/190], loss=18.0318
	step [69/190], loss=19.3960
	step [70/190], loss=21.4432
	step [71/190], loss=18.1456
	step [72/190], loss=15.9921
	step [73/190], loss=20.5151
	step [74/190], loss=18.7167
	step [75/190], loss=18.5490
	step [76/190], loss=21.0950
	step [77/190], loss=17.6287
	step [78/190], loss=18.1899
	step [79/190], loss=16.4547
	step [80/190], loss=18.7836
	step [81/190], loss=18.5453
	step [82/190], loss=18.4058
	step [83/190], loss=19.1801
	step [84/190], loss=20.3992
	step [85/190], loss=18.2633
	step [86/190], loss=17.4699
	step [87/190], loss=16.4656
	step [88/190], loss=19.1126
	step [89/190], loss=18.3221
	step [90/190], loss=17.8111
	step [91/190], loss=17.5651
	step [92/190], loss=17.8258
	step [93/190], loss=19.5385
	step [94/190], loss=16.3600
	step [95/190], loss=17.8802
	step [96/190], loss=19.4005
	step [97/190], loss=15.9641
	step [98/190], loss=20.2514
	step [99/190], loss=16.4390
	step [100/190], loss=18.9289
	step [101/190], loss=19.2315
	step [102/190], loss=19.0872
	step [103/190], loss=19.1808
	step [104/190], loss=18.4706
	step [105/190], loss=18.1685
	step [106/190], loss=16.4997
	step [107/190], loss=19.6089
	step [108/190], loss=17.1588
	step [109/190], loss=19.0852
	step [110/190], loss=18.4121
	step [111/190], loss=17.5335
	step [112/190], loss=20.5222
	step [113/190], loss=19.2239
	step [114/190], loss=17.3367
	step [115/190], loss=20.8812
	step [116/190], loss=17.1163
	step [117/190], loss=17.2138
	step [118/190], loss=18.8881
	step [119/190], loss=16.1271
	step [120/190], loss=17.3767
	step [121/190], loss=17.5619
	step [122/190], loss=16.4590
	step [123/190], loss=19.3266
	step [124/190], loss=20.7249
	step [125/190], loss=17.0836
	step [126/190], loss=16.2404
	step [127/190], loss=18.9182
	step [128/190], loss=16.8725
	step [129/190], loss=17.4968
	step [130/190], loss=15.1205
	step [131/190], loss=15.8958
	step [132/190], loss=17.4995
	step [133/190], loss=18.8361
	step [134/190], loss=19.1765
	step [135/190], loss=18.8732
	step [136/190], loss=16.6517
	step [137/190], loss=17.7702
	step [138/190], loss=18.0521
	step [139/190], loss=17.3479
	step [140/190], loss=17.6322
	step [141/190], loss=17.5529
	step [142/190], loss=17.5095
	step [143/190], loss=18.2613
	step [144/190], loss=18.1877
	step [145/190], loss=16.5580
	step [146/190], loss=17.2834
	step [147/190], loss=19.6015
	step [148/190], loss=16.3403
	step [149/190], loss=16.8298
	step [150/190], loss=16.1034
	step [151/190], loss=18.0608
	step [152/190], loss=15.9952
	step [153/190], loss=17.5040
	step [154/190], loss=16.2116
	step [155/190], loss=18.4868
	step [156/190], loss=17.5322
	step [157/190], loss=18.1097
	step [158/190], loss=17.0924
	step [159/190], loss=16.7543
	step [160/190], loss=15.8278
	step [161/190], loss=16.5851
	step [162/190], loss=17.0110
	step [163/190], loss=16.6710
	step [164/190], loss=16.5950
	step [165/190], loss=17.5585
	step [166/190], loss=18.6777
	step [167/190], loss=16.5401
	step [168/190], loss=16.7286
	step [169/190], loss=19.0477
	step [170/190], loss=17.1470
	step [171/190], loss=18.7189
	step [172/190], loss=17.3120
	step [173/190], loss=16.9730
	step [174/190], loss=17.2970
	step [175/190], loss=19.1477
	step [176/190], loss=17.0656
	step [177/190], loss=16.6803
	step [178/190], loss=15.7890
	step [179/190], loss=17.8030
	step [180/190], loss=18.5539
	step [181/190], loss=15.5122
	step [182/190], loss=16.7831
	step [183/190], loss=13.6282
	step [184/190], loss=17.7913
	step [185/190], loss=18.8869
	step [186/190], loss=15.8857
	step [187/190], loss=16.2915
	step [188/190], loss=19.1355
	step [189/190], loss=16.9006
	step [190/190], loss=15.1019
	Evaluating
	loss=0.0600, precision=0.1657, recall=0.9941, f1=0.2841
Training epoch 14
	step [1/190], loss=17.5347
	step [2/190], loss=13.7393
	step [3/190], loss=16.7835
	step [4/190], loss=15.8859
	step [5/190], loss=17.4444
	step [6/190], loss=15.4510
	step [7/190], loss=16.6412
	step [8/190], loss=17.3322
	step [9/190], loss=17.1048
	step [10/190], loss=16.1575
	step [11/190], loss=15.3301
	step [12/190], loss=18.1400
	step [13/190], loss=15.2896
	step [14/190], loss=21.4133
	step [15/190], loss=17.3621
	step [16/190], loss=16.8178
	step [17/190], loss=17.6715
	step [18/190], loss=18.0980
	step [19/190], loss=18.3186
	step [20/190], loss=17.1114
	step [21/190], loss=15.6118
	step [22/190], loss=14.8775
	step [23/190], loss=17.4556
	step [24/190], loss=19.1960
	step [25/190], loss=16.4813
	step [26/190], loss=17.1657
	step [27/190], loss=18.7903
	step [28/190], loss=15.3775
	step [29/190], loss=16.5376
	step [30/190], loss=16.4599
	step [31/190], loss=16.7145
	step [32/190], loss=18.8902
	step [33/190], loss=15.7424
	step [34/190], loss=15.3077
	step [35/190], loss=13.9532
	step [36/190], loss=17.0632
	step [37/190], loss=17.0045
	step [38/190], loss=14.5990
	step [39/190], loss=15.8666
	step [40/190], loss=17.9371
	step [41/190], loss=14.9920
	step [42/190], loss=16.8133
	step [43/190], loss=15.6953
	step [44/190], loss=16.8799
	step [45/190], loss=15.5984
	step [46/190], loss=16.1726
	step [47/190], loss=17.1890
	step [48/190], loss=17.4801
	step [49/190], loss=16.8344
	step [50/190], loss=17.1142
	step [51/190], loss=16.0647
	step [52/190], loss=14.3253
	step [53/190], loss=17.0190
	step [54/190], loss=17.8931
	step [55/190], loss=16.3614
	step [56/190], loss=16.9466
	step [57/190], loss=17.5039
	step [58/190], loss=15.6195
	step [59/190], loss=17.1361
	step [60/190], loss=20.0136
	step [61/190], loss=16.8144
	step [62/190], loss=19.9591
	step [63/190], loss=20.0617
	step [64/190], loss=17.3041
	step [65/190], loss=17.9861
	step [66/190], loss=19.5066
	step [67/190], loss=17.8023
	step [68/190], loss=16.4819
	step [69/190], loss=18.2289
	step [70/190], loss=15.4411
	step [71/190], loss=14.5491
	step [72/190], loss=15.4496
	step [73/190], loss=17.2408
	step [74/190], loss=16.3204
	step [75/190], loss=15.8317
	step [76/190], loss=17.9271
	step [77/190], loss=15.1297
	step [78/190], loss=17.3671
	step [79/190], loss=20.0364
	step [80/190], loss=15.7487
	step [81/190], loss=15.0922
	step [82/190], loss=17.5333
	step [83/190], loss=15.9538
	step [84/190], loss=17.5094
	step [85/190], loss=15.8072
	step [86/190], loss=16.4905
	step [87/190], loss=17.5373
	step [88/190], loss=17.9131
	step [89/190], loss=18.8576
	step [90/190], loss=17.2504
	step [91/190], loss=16.6385
	step [92/190], loss=16.7608
	step [93/190], loss=15.6281
	step [94/190], loss=20.0037
	step [95/190], loss=17.0286
	step [96/190], loss=15.3951
	step [97/190], loss=16.2326
	step [98/190], loss=16.1949
	step [99/190], loss=15.5990
	step [100/190], loss=17.6437
	step [101/190], loss=17.7655
	step [102/190], loss=16.3462
	step [103/190], loss=19.4325
	step [104/190], loss=15.1270
	step [105/190], loss=15.6502
	step [106/190], loss=17.5419
	step [107/190], loss=16.0373
	step [108/190], loss=16.9364
	step [109/190], loss=16.8744
	step [110/190], loss=16.1079
	step [111/190], loss=17.7965
	step [112/190], loss=17.0692
	step [113/190], loss=16.8854
	step [114/190], loss=15.5109
	step [115/190], loss=14.3793
	step [116/190], loss=17.8753
	step [117/190], loss=18.9462
	step [118/190], loss=17.9774
	step [119/190], loss=15.3180
	step [120/190], loss=15.6559
	step [121/190], loss=15.4157
	step [122/190], loss=15.5821
	step [123/190], loss=15.8820
	step [124/190], loss=14.0585
	step [125/190], loss=15.3356
	step [126/190], loss=17.4726
	step [127/190], loss=16.7150
	step [128/190], loss=15.8192
	step [129/190], loss=15.2226
	step [130/190], loss=14.4111
	step [131/190], loss=15.3101
	step [132/190], loss=15.2883
	step [133/190], loss=15.2875
	step [134/190], loss=17.9136
	step [135/190], loss=17.6978
	step [136/190], loss=15.4365
	step [137/190], loss=16.9739
	step [138/190], loss=14.9892
	step [139/190], loss=15.8609
	step [140/190], loss=15.0392
	step [141/190], loss=15.0502
	step [142/190], loss=15.6471
	step [143/190], loss=16.7829
	step [144/190], loss=16.5831
	step [145/190], loss=14.3200
	step [146/190], loss=14.5234
	step [147/190], loss=15.4531
	step [148/190], loss=16.3601
	step [149/190], loss=15.3556
	step [150/190], loss=14.6906
	step [151/190], loss=16.6536
	step [152/190], loss=14.8030
	step [153/190], loss=15.8127
	step [154/190], loss=15.4061
	step [155/190], loss=15.5775
	step [156/190], loss=15.6690
	step [157/190], loss=13.3573
	step [158/190], loss=17.6007
	step [159/190], loss=17.7223
	step [160/190], loss=15.5512
	step [161/190], loss=16.6290
	step [162/190], loss=17.9100
	step [163/190], loss=15.5094
	step [164/190], loss=15.9258
	step [165/190], loss=16.7782
	step [166/190], loss=17.9110
	step [167/190], loss=17.7787
	step [168/190], loss=17.1108
	step [169/190], loss=15.8524
	step [170/190], loss=18.8692
	step [171/190], loss=15.1372
	step [172/190], loss=17.0735
	step [173/190], loss=15.7313
	step [174/190], loss=14.5568
	step [175/190], loss=20.8849
	step [176/190], loss=15.0508
	step [177/190], loss=16.0468
	step [178/190], loss=15.7737
	step [179/190], loss=15.3871
	step [180/190], loss=15.7358
	step [181/190], loss=16.9094
	step [182/190], loss=15.6371
	step [183/190], loss=15.2610
	step [184/190], loss=18.1327
	step [185/190], loss=14.3077
	step [186/190], loss=13.6250
	step [187/190], loss=17.3902
	step [188/190], loss=16.3462
	step [189/190], loss=15.8864
	step [190/190], loss=14.4746
	Evaluating
	loss=0.0477, precision=0.2384, recall=0.9910, f1=0.3844
Training epoch 15
	step [1/190], loss=15.8421
	step [2/190], loss=14.7238
	step [3/190], loss=15.5836
	step [4/190], loss=17.0618
	step [5/190], loss=15.1632
	step [6/190], loss=15.8359
	step [7/190], loss=15.9498
	step [8/190], loss=18.5091
	step [9/190], loss=15.4317
	step [10/190], loss=16.0113
	step [11/190], loss=14.9610
	step [12/190], loss=15.9933
	step [13/190], loss=14.8332
	step [14/190], loss=15.1192
	step [15/190], loss=15.1697
	step [16/190], loss=14.5187
	step [17/190], loss=16.5112
	step [18/190], loss=15.2639
	step [19/190], loss=15.1120
	step [20/190], loss=13.0272
	step [21/190], loss=15.7941
	step [22/190], loss=17.4508
	step [23/190], loss=14.7787
	step [24/190], loss=15.1134
	step [25/190], loss=17.2676
	step [26/190], loss=13.2921
	step [27/190], loss=17.4231
	step [28/190], loss=14.0235
	step [29/190], loss=15.5413
	step [30/190], loss=16.2393
	step [31/190], loss=16.4157
	step [32/190], loss=15.5020
	step [33/190], loss=13.9966
	step [34/190], loss=14.6083
	step [35/190], loss=14.7746
	step [36/190], loss=14.4526
	step [37/190], loss=14.1096
	step [38/190], loss=14.2053
	step [39/190], loss=14.7812
	step [40/190], loss=15.5770
	step [41/190], loss=15.4106
	step [42/190], loss=14.9277
	step [43/190], loss=14.3378
	step [44/190], loss=15.4676
	step [45/190], loss=14.7488
	step [46/190], loss=15.6830
	step [47/190], loss=14.5191
	step [48/190], loss=15.1031
	step [49/190], loss=15.9672
	step [50/190], loss=15.1919
	step [51/190], loss=16.4967
	step [52/190], loss=16.5804
	step [53/190], loss=15.8764
	step [54/190], loss=15.6889
	step [55/190], loss=15.5740
	step [56/190], loss=17.0581
	step [57/190], loss=13.5258
	step [58/190], loss=13.7688
	step [59/190], loss=15.0907
	step [60/190], loss=13.9442
	step [61/190], loss=12.7883
	step [62/190], loss=14.5907
	step [63/190], loss=14.9250
	step [64/190], loss=15.5041
	step [65/190], loss=14.9956
	step [66/190], loss=15.1308
	step [67/190], loss=15.7276
	step [68/190], loss=14.1646
	step [69/190], loss=14.8174
	step [70/190], loss=14.7042
	step [71/190], loss=13.6727
	step [72/190], loss=17.1435
	step [73/190], loss=15.7127
	step [74/190], loss=14.3155
	step [75/190], loss=15.1077
	step [76/190], loss=17.7162
	step [77/190], loss=15.7429
	step [78/190], loss=14.1326
	step [79/190], loss=13.2187
	step [80/190], loss=14.8710
	step [81/190], loss=17.7424
	step [82/190], loss=15.7539
	step [83/190], loss=14.2164
	step [84/190], loss=14.3685
	step [85/190], loss=15.7402
	step [86/190], loss=15.8367
	step [87/190], loss=19.1021
	step [88/190], loss=14.0435
	step [89/190], loss=14.6219
	step [90/190], loss=15.5946
	step [91/190], loss=15.0691
	step [92/190], loss=15.5631
	step [93/190], loss=14.4537
	step [94/190], loss=15.1430
	step [95/190], loss=14.5167
	step [96/190], loss=16.6922
	step [97/190], loss=13.7782
	step [98/190], loss=16.2603
	step [99/190], loss=15.3624
	step [100/190], loss=15.3136
	step [101/190], loss=16.4189
	step [102/190], loss=14.4609
	step [103/190], loss=15.7157
	step [104/190], loss=16.3422
	step [105/190], loss=13.9168
	step [106/190], loss=18.4792
	step [107/190], loss=15.7070
	step [108/190], loss=14.2429
	step [109/190], loss=14.6463
	step [110/190], loss=15.9878
	step [111/190], loss=14.9685
	step [112/190], loss=14.1081
	step [113/190], loss=15.0237
	step [114/190], loss=15.0455
	step [115/190], loss=13.5132
	step [116/190], loss=15.0079
	step [117/190], loss=14.6610
	step [118/190], loss=14.7663
	step [119/190], loss=14.8465
	step [120/190], loss=13.5873
	step [121/190], loss=15.0200
	step [122/190], loss=15.8287
	step [123/190], loss=15.6913
	step [124/190], loss=16.3200
	step [125/190], loss=17.1658
	step [126/190], loss=13.4922
	step [127/190], loss=15.6525
	step [128/190], loss=16.3445
	step [129/190], loss=14.0645
	step [130/190], loss=16.0878
	step [131/190], loss=17.8114
	step [132/190], loss=15.5033
	step [133/190], loss=16.0578
	step [134/190], loss=15.7226
	step [135/190], loss=13.9721
	step [136/190], loss=15.4607
	step [137/190], loss=15.7718
	step [138/190], loss=14.7449
	step [139/190], loss=16.2355
	step [140/190], loss=15.1989
	step [141/190], loss=14.0131
	step [142/190], loss=12.7886
	step [143/190], loss=16.9606
	step [144/190], loss=16.0228
	step [145/190], loss=16.2496
	step [146/190], loss=14.1864
	step [147/190], loss=15.5259
	step [148/190], loss=15.3738
	step [149/190], loss=14.8085
	step [150/190], loss=16.2604
	step [151/190], loss=15.2723
	step [152/190], loss=13.7299
	step [153/190], loss=15.8726
	step [154/190], loss=13.4619
	step [155/190], loss=16.4426
	step [156/190], loss=14.5959
	step [157/190], loss=13.2509
	step [158/190], loss=13.7927
	step [159/190], loss=14.7018
	step [160/190], loss=14.5872
	step [161/190], loss=14.8432
	step [162/190], loss=13.9786
	step [163/190], loss=15.6583
	step [164/190], loss=13.9149
	step [165/190], loss=11.4620
	step [166/190], loss=16.8542
	step [167/190], loss=15.2327
	step [168/190], loss=16.3378
	step [169/190], loss=15.2106
	step [170/190], loss=14.9150
	step [171/190], loss=16.9832
	step [172/190], loss=14.0235
	step [173/190], loss=15.1304
	step [174/190], loss=16.8922
	step [175/190], loss=14.1837
	step [176/190], loss=15.3921
	step [177/190], loss=16.0820
	step [178/190], loss=14.2740
	step [179/190], loss=14.3599
	step [180/190], loss=15.0062
	step [181/190], loss=14.8600
	step [182/190], loss=16.7000
	step [183/190], loss=14.0341
	step [184/190], loss=14.8910
	step [185/190], loss=12.4206
	step [186/190], loss=15.1074
	step [187/190], loss=16.0021
	step [188/190], loss=13.4535
	step [189/190], loss=15.0189
	step [190/190], loss=12.7878
	Evaluating
	loss=0.0376, precision=0.2662, recall=0.9892, f1=0.4195
saving model as: 1_saved_model.pth
Training epoch 16
	step [1/190], loss=12.8849
	step [2/190], loss=15.5767
	step [3/190], loss=13.0223
	step [4/190], loss=13.1462
	step [5/190], loss=12.9410
	step [6/190], loss=14.4894
	step [7/190], loss=14.4056
	step [8/190], loss=13.6726
	step [9/190], loss=13.0827
	step [10/190], loss=13.3907
	step [11/190], loss=16.3026
	step [12/190], loss=15.0891
	step [13/190], loss=15.6551
	step [14/190], loss=13.1844
	step [15/190], loss=14.4977
	step [16/190], loss=16.0196
	step [17/190], loss=15.7560
	step [18/190], loss=14.0054
	step [19/190], loss=15.8283
	step [20/190], loss=14.1708
	step [21/190], loss=19.4315
	step [22/190], loss=19.1850
	step [23/190], loss=15.3158
	step [24/190], loss=14.0771
	step [25/190], loss=13.2350
	step [26/190], loss=14.2479
	step [27/190], loss=14.2623
	step [28/190], loss=14.3558
	step [29/190], loss=15.3104
	step [30/190], loss=12.8956
	step [31/190], loss=14.0084
	step [32/190], loss=14.1795
	step [33/190], loss=15.6777
	step [34/190], loss=13.9605
	step [35/190], loss=14.7871
	step [36/190], loss=13.9203
	step [37/190], loss=15.6487
	step [38/190], loss=16.3463
	step [39/190], loss=14.8765
	step [40/190], loss=15.3147
	step [41/190], loss=16.2654
	step [42/190], loss=13.4338
	step [43/190], loss=13.2388
	step [44/190], loss=15.3291
	step [45/190], loss=12.4272
	step [46/190], loss=13.2987
	step [47/190], loss=12.9621
	step [48/190], loss=14.4974
	step [49/190], loss=12.9272
	step [50/190], loss=18.0377
	step [51/190], loss=14.0248
	step [52/190], loss=13.9466
	step [53/190], loss=13.9886
	step [54/190], loss=15.2714
	step [55/190], loss=13.6339
	step [56/190], loss=13.1249
	step [57/190], loss=15.1752
	step [58/190], loss=16.4724
	step [59/190], loss=13.0151
	step [60/190], loss=14.6662
	step [61/190], loss=14.1803
	step [62/190], loss=15.6035
	step [63/190], loss=16.5445
	step [64/190], loss=14.2360
	step [65/190], loss=14.1696
	step [66/190], loss=14.4069
	step [67/190], loss=12.9127
	step [68/190], loss=12.2726
	step [69/190], loss=14.2498
	step [70/190], loss=15.1477
	step [71/190], loss=13.4286
	step [72/190], loss=15.1678
	step [73/190], loss=14.3030
	step [74/190], loss=14.8060
	step [75/190], loss=11.8141
	step [76/190], loss=13.0285
	step [77/190], loss=13.1893
	step [78/190], loss=15.6728
	step [79/190], loss=12.9109
	step [80/190], loss=13.3839
	step [81/190], loss=13.1104
	step [82/190], loss=13.5248
	step [83/190], loss=13.3105
	step [84/190], loss=12.3723
	step [85/190], loss=13.2520
	step [86/190], loss=13.2888
	step [87/190], loss=12.8361
	step [88/190], loss=14.4027
	step [89/190], loss=12.8783
	step [90/190], loss=12.4559
	step [91/190], loss=12.9305
	step [92/190], loss=16.0469
	step [93/190], loss=13.1441
	step [94/190], loss=15.2042
	step [95/190], loss=17.1695
	step [96/190], loss=14.6229
	step [97/190], loss=15.1643
	step [98/190], loss=13.7639
	step [99/190], loss=13.9332
	step [100/190], loss=13.5894
	step [101/190], loss=13.0756
	step [102/190], loss=13.5233
	step [103/190], loss=13.9341
	step [104/190], loss=17.2999
	step [105/190], loss=16.4223
	step [106/190], loss=13.6367
	step [107/190], loss=16.1898
	step [108/190], loss=13.8703
	step [109/190], loss=15.2346
	step [110/190], loss=13.2215
	step [111/190], loss=15.4403
	step [112/190], loss=14.6758
	step [113/190], loss=14.2912
	step [114/190], loss=12.9177
	step [115/190], loss=13.4696
	step [116/190], loss=13.2342
	step [117/190], loss=15.6385
	step [118/190], loss=12.7725
	step [119/190], loss=14.3324
	step [120/190], loss=14.8000
	step [121/190], loss=15.7893
	step [122/190], loss=13.5042
	step [123/190], loss=14.9781
	step [124/190], loss=13.5480
	step [125/190], loss=12.3301
	step [126/190], loss=12.9393
	step [127/190], loss=13.4141
	step [128/190], loss=14.4730
	step [129/190], loss=13.4437
	step [130/190], loss=13.7058
	step [131/190], loss=15.5401
	step [132/190], loss=13.8298
	step [133/190], loss=13.6950
	step [134/190], loss=14.1809
	step [135/190], loss=14.3432
	step [136/190], loss=13.8842
	step [137/190], loss=14.7201
	step [138/190], loss=12.9040
	step [139/190], loss=13.6419
	step [140/190], loss=13.9243
	step [141/190], loss=13.2295
	step [142/190], loss=15.3028
	step [143/190], loss=14.3166
	step [144/190], loss=13.3456
	step [145/190], loss=15.4644
	step [146/190], loss=14.0401
	step [147/190], loss=15.6822
	step [148/190], loss=12.7763
	step [149/190], loss=13.1257
	step [150/190], loss=14.9598
	step [151/190], loss=14.6963
	step [152/190], loss=14.1305
	step [153/190], loss=11.9514
	step [154/190], loss=13.8134
	step [155/190], loss=13.4528
	step [156/190], loss=12.5108
	step [157/190], loss=13.3019
	step [158/190], loss=15.4253
	step [159/190], loss=11.4316
	step [160/190], loss=12.2282
	step [161/190], loss=11.2142
	step [162/190], loss=13.9960
	step [163/190], loss=14.7485
	step [164/190], loss=14.4963
	step [165/190], loss=14.8879
	step [166/190], loss=16.0864
	step [167/190], loss=14.0099
	step [168/190], loss=13.4635
	step [169/190], loss=12.0942
	step [170/190], loss=11.9952
	step [171/190], loss=13.6569
	step [172/190], loss=12.8305
	step [173/190], loss=13.3797
	step [174/190], loss=11.7375
	step [175/190], loss=14.3003
	step [176/190], loss=13.5762
	step [177/190], loss=15.1479
	step [178/190], loss=15.1414
	step [179/190], loss=14.7757
	step [180/190], loss=15.0644
	step [181/190], loss=15.0057
	step [182/190], loss=14.7816
	step [183/190], loss=14.9954
	step [184/190], loss=12.7974
	step [185/190], loss=15.9210
	step [186/190], loss=15.9680
	step [187/190], loss=13.3090
	step [188/190], loss=12.7926
	step [189/190], loss=15.5666
	step [190/190], loss=11.5806
	Evaluating
	loss=0.0453, precision=0.2069, recall=0.9911, f1=0.3424
Training epoch 17
	step [1/190], loss=13.3730
	step [2/190], loss=13.5731
	step [3/190], loss=14.5648
	step [4/190], loss=12.1643
	step [5/190], loss=13.8462
	step [6/190], loss=11.4005
	step [7/190], loss=14.2699
	step [8/190], loss=14.6859
	step [9/190], loss=15.1051
	step [10/190], loss=13.0969
	step [11/190], loss=13.1680
	step [12/190], loss=15.1627
	step [13/190], loss=13.5102
	step [14/190], loss=14.0797
	step [15/190], loss=14.5874
	step [16/190], loss=11.3193
	step [17/190], loss=12.1403
	step [18/190], loss=14.2630
	step [19/190], loss=14.7497
	step [20/190], loss=14.2426
	step [21/190], loss=14.3831
	step [22/190], loss=15.9343
	step [23/190], loss=13.0950
	step [24/190], loss=13.5048
	step [25/190], loss=13.1028
	step [26/190], loss=14.5661
	step [27/190], loss=12.6814
	step [28/190], loss=14.6329
	step [29/190], loss=12.3748
	step [30/190], loss=13.1164
	step [31/190], loss=13.1515
	step [32/190], loss=12.4182
	step [33/190], loss=17.1687
	step [34/190], loss=13.0498
	step [35/190], loss=16.2953
	step [36/190], loss=11.8214
	step [37/190], loss=14.0498
	step [38/190], loss=12.2432
	step [39/190], loss=13.8980
	step [40/190], loss=14.8275
	step [41/190], loss=13.2686
	step [42/190], loss=13.6845
	step [43/190], loss=13.5113
	step [44/190], loss=14.7999
	step [45/190], loss=14.9407
	step [46/190], loss=14.4052
	step [47/190], loss=13.4110
	step [48/190], loss=13.8010
	step [49/190], loss=14.9868
	step [50/190], loss=13.3842
	step [51/190], loss=13.2391
	step [52/190], loss=14.1809
	step [53/190], loss=11.7501
	step [54/190], loss=13.2903
	step [55/190], loss=11.9439
	step [56/190], loss=12.2991
	step [57/190], loss=13.7611
	step [58/190], loss=12.8886
	step [59/190], loss=12.3430
	step [60/190], loss=13.5600
	step [61/190], loss=12.7116
	step [62/190], loss=11.9770
	step [63/190], loss=14.7457
	step [64/190], loss=13.6961
	step [65/190], loss=14.6445
	step [66/190], loss=11.5768
	step [67/190], loss=12.6007
	step [68/190], loss=14.7243
	step [69/190], loss=12.2552
	step [70/190], loss=14.8059
	step [71/190], loss=15.7208
	step [72/190], loss=10.2280
	step [73/190], loss=11.8827
	step [74/190], loss=10.8165
	step [75/190], loss=13.2129
	step [76/190], loss=15.4543
	step [77/190], loss=14.8812
	step [78/190], loss=14.1964
	step [79/190], loss=14.4447
	step [80/190], loss=14.1969
	step [81/190], loss=13.4241
	step [82/190], loss=13.5157
	step [83/190], loss=11.7014
	step [84/190], loss=14.4768
	step [85/190], loss=14.9124
	step [86/190], loss=13.3564
	step [87/190], loss=14.6224
	step [88/190], loss=13.4021
	step [89/190], loss=12.9248
	step [90/190], loss=13.4869
	step [91/190], loss=14.9771
	step [92/190], loss=12.1559
	step [93/190], loss=11.2548
	step [94/190], loss=13.6417
	step [95/190], loss=12.3446
	step [96/190], loss=11.7904
	step [97/190], loss=14.4834
	step [98/190], loss=15.6403
	step [99/190], loss=14.9100
	step [100/190], loss=13.4319
	step [101/190], loss=13.4549
	step [102/190], loss=13.3441
	step [103/190], loss=14.0725
	step [104/190], loss=11.8299
	step [105/190], loss=13.8372
	step [106/190], loss=15.4927
	step [107/190], loss=12.5452
	step [108/190], loss=13.7112
	step [109/190], loss=14.3416
	step [110/190], loss=13.8597
	step [111/190], loss=14.5701
	step [112/190], loss=13.6469
	step [113/190], loss=14.7160
	step [114/190], loss=13.8287
	step [115/190], loss=16.7992
	step [116/190], loss=13.4446
	step [117/190], loss=11.3906
	step [118/190], loss=12.0895
	step [119/190], loss=13.4421
	step [120/190], loss=14.0008
	step [121/190], loss=12.5298
	step [122/190], loss=14.4789
	step [123/190], loss=13.3661
	step [124/190], loss=12.7803
	step [125/190], loss=12.8812
	step [126/190], loss=11.7639
	step [127/190], loss=11.8780
	step [128/190], loss=13.4764
	step [129/190], loss=11.9072
	step [130/190], loss=12.7435
	step [131/190], loss=14.6467
	step [132/190], loss=10.7152
	step [133/190], loss=13.7731
	step [134/190], loss=12.1197
	step [135/190], loss=13.4661
	step [136/190], loss=12.6621
	step [137/190], loss=11.1216
	step [138/190], loss=15.9564
	step [139/190], loss=14.0220
	step [140/190], loss=13.1143
	step [141/190], loss=13.3458
	step [142/190], loss=12.6089
	step [143/190], loss=15.1288
	step [144/190], loss=13.9990
	step [145/190], loss=12.7814
	step [146/190], loss=13.2615
	step [147/190], loss=12.1461
	step [148/190], loss=10.8698
	step [149/190], loss=11.5831
	step [150/190], loss=11.8029
	step [151/190], loss=12.7758
	step [152/190], loss=9.8213
	step [153/190], loss=14.3659
	step [154/190], loss=12.8896
	step [155/190], loss=12.4440
	step [156/190], loss=11.5867
	step [157/190], loss=15.5627
	step [158/190], loss=13.9668
	step [159/190], loss=14.3768
	step [160/190], loss=11.6626
	step [161/190], loss=12.3021
	step [162/190], loss=12.9895
	step [163/190], loss=11.7962
	step [164/190], loss=13.2171
	step [165/190], loss=14.9694
	step [166/190], loss=12.9722
	step [167/190], loss=12.6479
	step [168/190], loss=14.4875
	step [169/190], loss=11.6679
	step [170/190], loss=12.8817
	step [171/190], loss=14.3868
	step [172/190], loss=12.4560
	step [173/190], loss=15.2677
	step [174/190], loss=12.3143
	step [175/190], loss=13.3241
	step [176/190], loss=12.7893
	step [177/190], loss=13.4652
	step [178/190], loss=12.8136
	step [179/190], loss=10.8925
	step [180/190], loss=11.9464
	step [181/190], loss=11.8183
	step [182/190], loss=11.9215
	step [183/190], loss=12.9734
	step [184/190], loss=13.9846
	step [185/190], loss=13.0922
	step [186/190], loss=10.8332
	step [187/190], loss=12.7607
	step [188/190], loss=12.3941
	step [189/190], loss=13.2047
	step [190/190], loss=9.4647
	Evaluating
	loss=0.0337, precision=0.2515, recall=0.9906, f1=0.4012
Training epoch 18
	step [1/190], loss=12.0797
	step [2/190], loss=13.0411
	step [3/190], loss=11.8181
	step [4/190], loss=13.6345
	step [5/190], loss=12.6443
	step [6/190], loss=12.7468
	step [7/190], loss=12.0908
	step [8/190], loss=14.6346
	step [9/190], loss=11.9819
	step [10/190], loss=12.4573
	step [11/190], loss=13.6119
	step [12/190], loss=14.0277
	step [13/190], loss=13.1665
	step [14/190], loss=12.3887
	step [15/190], loss=12.6491
	step [16/190], loss=17.0654
	step [17/190], loss=13.5516
	step [18/190], loss=11.0473
	step [19/190], loss=13.0148
	step [20/190], loss=12.2058
	step [21/190], loss=12.2930
	step [22/190], loss=10.9194
	step [23/190], loss=11.9364
	step [24/190], loss=12.6513
	step [25/190], loss=11.6617
	step [26/190], loss=16.3228
	step [27/190], loss=11.8489
	step [28/190], loss=11.0625
	step [29/190], loss=12.8273
	step [30/190], loss=12.8416
	step [31/190], loss=12.6873
	step [32/190], loss=11.8326
	step [33/190], loss=13.7299
	step [34/190], loss=11.9036
	step [35/190], loss=13.7451
	step [36/190], loss=14.4427
	step [37/190], loss=14.2223
	step [38/190], loss=11.6215
	step [39/190], loss=14.9791
	step [40/190], loss=12.2928
	step [41/190], loss=13.2598
	step [42/190], loss=15.0306
	step [43/190], loss=12.6781
	step [44/190], loss=14.8290
	step [45/190], loss=13.6015
	step [46/190], loss=12.8415
	step [47/190], loss=13.7065
	step [48/190], loss=13.4197
	step [49/190], loss=12.8300
	step [50/190], loss=15.1170
	step [51/190], loss=11.4516
	step [52/190], loss=12.3698
	step [53/190], loss=13.5297
	step [54/190], loss=12.5811
	step [55/190], loss=16.9888
	step [56/190], loss=14.3812
	step [57/190], loss=13.2123
	step [58/190], loss=13.4089
	step [59/190], loss=12.4731
	step [60/190], loss=13.2840
	step [61/190], loss=12.3492
	step [62/190], loss=13.1798
	step [63/190], loss=12.2471
	step [64/190], loss=14.0501
	step [65/190], loss=14.1115
	step [66/190], loss=14.4085
	step [67/190], loss=11.8028
	step [68/190], loss=14.6407
	step [69/190], loss=12.0150
	step [70/190], loss=11.9252
	step [71/190], loss=12.4431
	step [72/190], loss=13.1629
	step [73/190], loss=11.7156
	step [74/190], loss=11.7629
	step [75/190], loss=11.7689
	step [76/190], loss=12.4743
	step [77/190], loss=12.9121
	step [78/190], loss=12.5786
	step [79/190], loss=13.6153
	step [80/190], loss=14.8440
	step [81/190], loss=12.9344
	step [82/190], loss=12.0221
	step [83/190], loss=13.1142
	step [84/190], loss=13.0322
	step [85/190], loss=12.6945
	step [86/190], loss=13.6938
	step [87/190], loss=10.5174
	step [88/190], loss=12.9946
	step [89/190], loss=11.7457
	step [90/190], loss=12.2450
	step [91/190], loss=10.6947
	step [92/190], loss=11.3330
	step [93/190], loss=14.3852
	step [94/190], loss=12.7333
	step [95/190], loss=11.9749
	step [96/190], loss=13.1657
	step [97/190], loss=12.3647
	step [98/190], loss=13.2495
	step [99/190], loss=10.7406
	step [100/190], loss=12.3098
	step [101/190], loss=13.6594
	step [102/190], loss=12.9825
	step [103/190], loss=10.6335
	step [104/190], loss=11.9555
	step [105/190], loss=12.4023
	step [106/190], loss=12.9968
	step [107/190], loss=11.7884
	step [108/190], loss=12.9276
	step [109/190], loss=12.4786
	step [110/190], loss=11.2744
	step [111/190], loss=11.0316
	step [112/190], loss=12.9610
	step [113/190], loss=12.8367
	step [114/190], loss=13.7267
	step [115/190], loss=12.4367
	step [116/190], loss=11.3179
	step [117/190], loss=11.0820
	step [118/190], loss=11.9743
	step [119/190], loss=12.3811
	step [120/190], loss=12.3688
	step [121/190], loss=11.5820
	step [122/190], loss=14.8962
	step [123/190], loss=10.5939
	step [124/190], loss=11.6172
	step [125/190], loss=10.9626
	step [126/190], loss=9.9686
	step [127/190], loss=15.3399
	step [128/190], loss=12.5683
	step [129/190], loss=11.4287
	step [130/190], loss=11.9014
	step [131/190], loss=14.0047
	step [132/190], loss=10.8664
	step [133/190], loss=11.4432
	step [134/190], loss=13.5725
	step [135/190], loss=11.4648
	step [136/190], loss=12.8918
	step [137/190], loss=10.5292
	step [138/190], loss=9.4251
	step [139/190], loss=10.8873
	step [140/190], loss=11.0503
	step [141/190], loss=11.8288
	step [142/190], loss=11.0196
	step [143/190], loss=13.8603
	step [144/190], loss=12.1146
	step [145/190], loss=13.4970
	step [146/190], loss=10.7434
	step [147/190], loss=13.5013
	step [148/190], loss=13.8140
	step [149/190], loss=10.5611
	step [150/190], loss=12.8908
	step [151/190], loss=11.6098
	step [152/190], loss=14.5710
	step [153/190], loss=13.9942
	step [154/190], loss=11.8268
	step [155/190], loss=11.0639
	step [156/190], loss=14.0563
	step [157/190], loss=15.5467
	step [158/190], loss=11.3623
	step [159/190], loss=14.6166
	step [160/190], loss=11.7447
	step [161/190], loss=12.5160
	step [162/190], loss=12.7211
	step [163/190], loss=12.0870
	step [164/190], loss=14.4041
	step [165/190], loss=12.8318
	step [166/190], loss=12.6713
	step [167/190], loss=12.9740
	step [168/190], loss=13.4493
	step [169/190], loss=11.8012
	step [170/190], loss=12.1016
	step [171/190], loss=11.6419
	step [172/190], loss=13.6351
	step [173/190], loss=12.1942
	step [174/190], loss=12.6385
	step [175/190], loss=12.1144
	step [176/190], loss=14.4646
	step [177/190], loss=12.7491
	step [178/190], loss=10.5155
	step [179/190], loss=11.1330
	step [180/190], loss=11.2820
	step [181/190], loss=13.9373
	step [182/190], loss=10.0651
	step [183/190], loss=12.7256
	step [184/190], loss=12.0068
	step [185/190], loss=14.0034
	step [186/190], loss=11.7472
	step [187/190], loss=11.9308
	step [188/190], loss=12.0388
	step [189/190], loss=10.4135
	step [190/190], loss=9.2518
	Evaluating
	loss=0.0335, precision=0.2427, recall=0.9911, f1=0.3899
Training epoch 19
	step [1/190], loss=11.0113
	step [2/190], loss=11.2952
	step [3/190], loss=10.1145
	step [4/190], loss=12.6326
	step [5/190], loss=13.2229
	step [6/190], loss=11.5367
	step [7/190], loss=11.4290
	step [8/190], loss=11.3427
	step [9/190], loss=13.6888
	step [10/190], loss=11.6075
	step [11/190], loss=11.4479
	step [12/190], loss=10.9538
	step [13/190], loss=12.5437
	step [14/190], loss=12.4110
	step [15/190], loss=12.9081
	step [16/190], loss=10.1280
	step [17/190], loss=12.5447
	step [18/190], loss=12.1718
	step [19/190], loss=11.6627
	step [20/190], loss=12.2429
	step [21/190], loss=12.7118
	step [22/190], loss=12.8107
	step [23/190], loss=12.7943
	step [24/190], loss=12.2506
	step [25/190], loss=11.2593
	step [26/190], loss=11.9248
	step [27/190], loss=12.7490
	step [28/190], loss=13.3012
	step [29/190], loss=10.8343
	step [30/190], loss=11.4345
	step [31/190], loss=12.9607
	step [32/190], loss=10.8879
	step [33/190], loss=12.2861
	step [34/190], loss=12.4861
	step [35/190], loss=12.2979
	step [36/190], loss=11.4238
	step [37/190], loss=12.9082
	step [38/190], loss=12.4556
	step [39/190], loss=13.1660
	step [40/190], loss=14.0824
	step [41/190], loss=10.3524
	step [42/190], loss=13.9983
	step [43/190], loss=11.3169
	step [44/190], loss=11.7103
	step [45/190], loss=13.4229
	step [46/190], loss=10.4602
	step [47/190], loss=10.7236
	step [48/190], loss=11.1479
	step [49/190], loss=13.0767
	step [50/190], loss=11.5738
	step [51/190], loss=11.5186
	step [52/190], loss=12.8847
	step [53/190], loss=10.9683
	step [54/190], loss=12.4066
	step [55/190], loss=11.4243
	step [56/190], loss=9.2635
	step [57/190], loss=12.6640
	step [58/190], loss=12.8418
	step [59/190], loss=12.9128
	step [60/190], loss=13.1133
	step [61/190], loss=10.7716
	step [62/190], loss=11.8097
	step [63/190], loss=12.7941
	step [64/190], loss=13.5285
	step [65/190], loss=12.6346
	step [66/190], loss=10.9138
	step [67/190], loss=11.5116
	step [68/190], loss=11.3521
	step [69/190], loss=11.3696
	step [70/190], loss=11.3055
	step [71/190], loss=12.5124
	step [72/190], loss=12.6952
	step [73/190], loss=10.4428
	step [74/190], loss=12.3796
	step [75/190], loss=13.8345
	step [76/190], loss=10.6441
	step [77/190], loss=14.6151
	step [78/190], loss=11.0055
	step [79/190], loss=12.2462
	step [80/190], loss=12.1336
	step [81/190], loss=12.4127
	step [82/190], loss=10.1685
	step [83/190], loss=12.3790
	step [84/190], loss=12.8238
	step [85/190], loss=10.3896
	step [86/190], loss=13.4725
	step [87/190], loss=10.9800
	step [88/190], loss=11.8882
	step [89/190], loss=12.3772
	step [90/190], loss=12.8194
	step [91/190], loss=12.5900
	step [92/190], loss=11.7700
	step [93/190], loss=11.5611
	step [94/190], loss=12.6095
	step [95/190], loss=9.3420
	step [96/190], loss=10.9923
	step [97/190], loss=15.2209
	step [98/190], loss=11.2423
	step [99/190], loss=11.1424
	step [100/190], loss=12.0004
	step [101/190], loss=12.2967
	step [102/190], loss=12.7285
	step [103/190], loss=11.3600
	step [104/190], loss=11.2459
	step [105/190], loss=12.5833
	step [106/190], loss=11.4244
	step [107/190], loss=11.5342
	step [108/190], loss=11.4700
	step [109/190], loss=12.8616
	step [110/190], loss=12.9119
	step [111/190], loss=12.6676
	step [112/190], loss=13.4667
	step [113/190], loss=12.5018
	step [114/190], loss=11.3520
	step [115/190], loss=11.8562
	step [116/190], loss=13.6804
	step [117/190], loss=11.1263
	step [118/190], loss=11.7222
	step [119/190], loss=10.3067
	step [120/190], loss=11.1157
	step [121/190], loss=12.4138
	step [122/190], loss=11.4475
	step [123/190], loss=11.0571
	step [124/190], loss=11.2658
	step [125/190], loss=11.0819
	step [126/190], loss=12.0581
	step [127/190], loss=11.5550
	step [128/190], loss=11.5771
	step [129/190], loss=11.2537
	step [130/190], loss=10.8527
	step [131/190], loss=11.4579
	step [132/190], loss=9.5276
	step [133/190], loss=11.4336
	step [134/190], loss=11.6453
	step [135/190], loss=12.1546
	step [136/190], loss=9.4477
	step [137/190], loss=15.2566
	step [138/190], loss=12.8738
	step [139/190], loss=12.7901
	step [140/190], loss=13.9651
	step [141/190], loss=11.7967
	step [142/190], loss=14.0429
	step [143/190], loss=11.5180
	step [144/190], loss=12.2333
	step [145/190], loss=12.1694
	step [146/190], loss=14.2452
	step [147/190], loss=12.4708
	step [148/190], loss=12.4164
	step [149/190], loss=12.4373
	step [150/190], loss=11.7941
	step [151/190], loss=13.1215
	step [152/190], loss=13.0687
	step [153/190], loss=11.1814
	step [154/190], loss=11.2138
	step [155/190], loss=11.5459
	step [156/190], loss=11.0054
	step [157/190], loss=12.7148
	step [158/190], loss=10.8586
	step [159/190], loss=12.2677
	step [160/190], loss=13.4502
	step [161/190], loss=11.6063
	step [162/190], loss=13.7409
	step [163/190], loss=10.7465
	step [164/190], loss=13.5103
	step [165/190], loss=11.0452
	step [166/190], loss=10.3716
	step [167/190], loss=11.4445
	step [168/190], loss=12.1902
	step [169/190], loss=10.3279
	step [170/190], loss=11.1962
	step [171/190], loss=10.4473
	step [172/190], loss=11.2011
	step [173/190], loss=9.8936
	step [174/190], loss=13.2187
	step [175/190], loss=11.3224
	step [176/190], loss=12.9564
	step [177/190], loss=12.2281
	step [178/190], loss=9.5730
	step [179/190], loss=11.3167
	step [180/190], loss=12.4341
	step [181/190], loss=12.1077
	step [182/190], loss=13.0813
	step [183/190], loss=14.0108
	step [184/190], loss=9.9462
	step [185/190], loss=12.7664
	step [186/190], loss=10.5584
	step [187/190], loss=12.2559
	step [188/190], loss=10.8202
	step [189/190], loss=9.9967
	step [190/190], loss=10.1654
	Evaluating
	loss=0.0350, precision=0.2057, recall=0.9915, f1=0.3406
Training epoch 20
	step [1/190], loss=11.6440
	step [2/190], loss=11.2488
	step [3/190], loss=11.5713
	step [4/190], loss=11.4479
	step [5/190], loss=11.7357
	step [6/190], loss=11.1323
	step [7/190], loss=10.2272
	step [8/190], loss=11.5843
	step [9/190], loss=12.4394
	step [10/190], loss=11.3690
	step [11/190], loss=12.3848
	step [12/190], loss=11.9003
	step [13/190], loss=15.2687
	step [14/190], loss=12.1441
	step [15/190], loss=14.1858
	step [16/190], loss=11.4749
	step [17/190], loss=11.6726
	step [18/190], loss=12.0452
	step [19/190], loss=10.1663
	step [20/190], loss=9.5576
	step [21/190], loss=10.6586
	step [22/190], loss=9.4352
	step [23/190], loss=9.9976
	step [24/190], loss=15.5219
	step [25/190], loss=14.1094
	step [26/190], loss=12.1244
	step [27/190], loss=10.8255
	step [28/190], loss=14.2958
	step [29/190], loss=14.2178
	step [30/190], loss=13.6900
	step [31/190], loss=12.7309
	step [32/190], loss=11.0406
	step [33/190], loss=10.4519
	step [34/190], loss=11.9657
	step [35/190], loss=11.4057
	step [36/190], loss=12.1654
	step [37/190], loss=11.0998
	step [38/190], loss=13.2533
	step [39/190], loss=12.7986
	step [40/190], loss=11.4284
	step [41/190], loss=11.3394
	step [42/190], loss=10.8943
	step [43/190], loss=10.0261
	step [44/190], loss=12.0918
	step [45/190], loss=11.8321
	step [46/190], loss=11.3774
	step [47/190], loss=10.4840
	step [48/190], loss=11.0363
	step [49/190], loss=9.9148
	step [50/190], loss=10.0151
	step [51/190], loss=9.5847
	step [52/190], loss=11.1830
	step [53/190], loss=11.5325
	step [54/190], loss=11.3441
	step [55/190], loss=11.5282
	step [56/190], loss=12.7376
	step [57/190], loss=11.2518
	step [58/190], loss=10.4650
	step [59/190], loss=12.5338
	step [60/190], loss=9.6799
	step [61/190], loss=12.1300
	step [62/190], loss=10.2159
	step [63/190], loss=10.1398
	step [64/190], loss=11.5553
	step [65/190], loss=12.6618
	step [66/190], loss=11.0214
	step [67/190], loss=10.5039
	step [68/190], loss=10.3099
	step [69/190], loss=11.6417
	step [70/190], loss=10.1460
	step [71/190], loss=11.1435
	step [72/190], loss=10.6411
	step [73/190], loss=14.8611
	step [74/190], loss=11.3573
	step [75/190], loss=10.2777
	step [76/190], loss=10.4596
	step [77/190], loss=12.0008
	step [78/190], loss=11.2686
	step [79/190], loss=11.8698
	step [80/190], loss=11.2823
	step [81/190], loss=11.8998
	step [82/190], loss=12.4963
	step [83/190], loss=11.6786
	step [84/190], loss=11.7321
	step [85/190], loss=12.6033
	step [86/190], loss=10.3587
	step [87/190], loss=10.9106
	step [88/190], loss=11.9390
	step [89/190], loss=11.0162
	step [90/190], loss=12.1523
	step [91/190], loss=13.5704
	step [92/190], loss=10.7594
	step [93/190], loss=10.7387
	step [94/190], loss=11.1072
	step [95/190], loss=10.3361
	step [96/190], loss=10.9319
	step [97/190], loss=11.4431
	step [98/190], loss=10.7868
	step [99/190], loss=10.5867
	step [100/190], loss=11.6485
	step [101/190], loss=10.3608
	step [102/190], loss=11.6256
	step [103/190], loss=9.4110
	step [104/190], loss=12.1758
	step [105/190], loss=12.0990
	step [106/190], loss=10.4818
	step [107/190], loss=9.7100
	step [108/190], loss=12.0025
	step [109/190], loss=11.0800
	step [110/190], loss=11.8492
	step [111/190], loss=14.1039
	step [112/190], loss=14.9454
	step [113/190], loss=10.5144
	step [114/190], loss=10.1535
	step [115/190], loss=10.5390
	step [116/190], loss=11.2626
	step [117/190], loss=11.5441
	step [118/190], loss=9.4200
	step [119/190], loss=13.2332
	step [120/190], loss=9.2292
	step [121/190], loss=10.5452
	step [122/190], loss=11.1446
	step [123/190], loss=10.4715
	step [124/190], loss=11.9343
	step [125/190], loss=13.1439
	step [126/190], loss=10.7675
	step [127/190], loss=10.7120
	step [128/190], loss=12.4306
	step [129/190], loss=10.8402
	step [130/190], loss=10.7146
	step [131/190], loss=10.8136
	step [132/190], loss=12.2591
	step [133/190], loss=12.5558
	step [134/190], loss=11.5708
	step [135/190], loss=11.1360
	step [136/190], loss=11.4529
	step [137/190], loss=9.9739
	step [138/190], loss=13.4702
	step [139/190], loss=14.1515
	step [140/190], loss=10.6785
	step [141/190], loss=10.5470
	step [142/190], loss=9.8032
	step [143/190], loss=10.3544
	step [144/190], loss=10.6605
	step [145/190], loss=11.0374
	step [146/190], loss=12.4334
	step [147/190], loss=11.1505
	step [148/190], loss=11.7611
	step [149/190], loss=13.9347
	step [150/190], loss=12.5238
	step [151/190], loss=9.4472
	step [152/190], loss=10.6346
	step [153/190], loss=14.3484
	step [154/190], loss=13.3292
	step [155/190], loss=10.7224
	step [156/190], loss=11.4087
	step [157/190], loss=12.0091
	step [158/190], loss=10.5982
	step [159/190], loss=11.4598
	step [160/190], loss=13.1676
	step [161/190], loss=10.8814
	step [162/190], loss=11.2052
	step [163/190], loss=11.3130
	step [164/190], loss=13.4003
	step [165/190], loss=9.9763
	step [166/190], loss=11.7132
	step [167/190], loss=11.5055
	step [168/190], loss=13.3376
	step [169/190], loss=13.0680
	step [170/190], loss=9.4195
	step [171/190], loss=10.1548
	step [172/190], loss=11.1939
	step [173/190], loss=9.6902
	step [174/190], loss=10.2505
	step [175/190], loss=10.2431
	step [176/190], loss=10.0714
	step [177/190], loss=9.3710
	step [178/190], loss=11.3685
	step [179/190], loss=11.2785
	step [180/190], loss=11.0271
	step [181/190], loss=11.1181
	step [182/190], loss=13.4307
	step [183/190], loss=10.0681
	step [184/190], loss=13.1843
	step [185/190], loss=10.7564
	step [186/190], loss=10.7796
	step [187/190], loss=14.4433
	step [188/190], loss=10.8307
	step [189/190], loss=10.2779
	step [190/190], loss=8.8671
	Evaluating
	loss=0.0336, precision=0.1867, recall=0.9920, f1=0.3142
Training epoch 21
	step [1/190], loss=10.4696
	step [2/190], loss=10.3373
	step [3/190], loss=11.2757
	step [4/190], loss=10.8131
	step [5/190], loss=11.3127
	step [6/190], loss=12.4320
	step [7/190], loss=10.9302
	step [8/190], loss=10.8424
	step [9/190], loss=11.2854
	step [10/190], loss=10.8876
	step [11/190], loss=11.5294
	step [12/190], loss=9.7174
	step [13/190], loss=11.5869
	step [14/190], loss=8.5561
	step [15/190], loss=10.0483
	step [16/190], loss=12.1209
	step [17/190], loss=12.2058
	step [18/190], loss=11.2735
	step [19/190], loss=10.3663
	step [20/190], loss=10.2684
	step [21/190], loss=10.4241
	step [22/190], loss=11.1480
	step [23/190], loss=10.1191
	step [24/190], loss=11.4283
	step [25/190], loss=10.4217
	step [26/190], loss=9.1530
	step [27/190], loss=11.3218
	step [28/190], loss=11.3728
	step [29/190], loss=10.6990
	step [30/190], loss=9.9725
	step [31/190], loss=11.5915
	step [32/190], loss=10.2664
	step [33/190], loss=10.4671
	step [34/190], loss=10.9077
	step [35/190], loss=9.8975
	step [36/190], loss=10.7727
	step [37/190], loss=9.2993
	step [38/190], loss=10.5436
	step [39/190], loss=10.9007
	step [40/190], loss=12.2957
	step [41/190], loss=9.9638
	step [42/190], loss=12.8757
	step [43/190], loss=11.1794
	step [44/190], loss=10.7814
	step [45/190], loss=12.5906
	step [46/190], loss=9.9794
	step [47/190], loss=13.0276
	step [48/190], loss=13.5067
	step [49/190], loss=10.9728
	step [50/190], loss=10.0469
	step [51/190], loss=11.2178
	step [52/190], loss=12.0881
	step [53/190], loss=10.7993
	step [54/190], loss=10.2100
	step [55/190], loss=10.7303
	step [56/190], loss=11.2676
	step [57/190], loss=12.2909
	step [58/190], loss=11.7817
	step [59/190], loss=11.8388
	step [60/190], loss=11.7468
	step [61/190], loss=11.4839
	step [62/190], loss=10.5328
	step [63/190], loss=9.8897
	step [64/190], loss=13.3617
	step [65/190], loss=11.5414
	step [66/190], loss=10.3749
	step [67/190], loss=13.2414
	step [68/190], loss=9.0949
	step [69/190], loss=11.3172
	step [70/190], loss=12.4420
	step [71/190], loss=9.7049
	step [72/190], loss=10.3656
	step [73/190], loss=11.8967
	step [74/190], loss=10.1957
	step [75/190], loss=10.6173
	step [76/190], loss=11.3573
	step [77/190], loss=11.7054
	step [78/190], loss=11.7930
	step [79/190], loss=11.8746
	step [80/190], loss=12.4394
	step [81/190], loss=11.7518
	step [82/190], loss=11.6574
	step [83/190], loss=11.7072
	step [84/190], loss=11.4190
	step [85/190], loss=10.4291
	step [86/190], loss=9.5731
	step [87/190], loss=10.8198
	step [88/190], loss=8.8390
	step [89/190], loss=14.1573
	step [90/190], loss=9.9460
	step [91/190], loss=10.5275
	step [92/190], loss=10.2242
	step [93/190], loss=9.1009
	step [94/190], loss=11.5131
	step [95/190], loss=10.6708
	step [96/190], loss=9.6572
	step [97/190], loss=9.5787
	step [98/190], loss=10.9685
	step [99/190], loss=11.0354
	step [100/190], loss=11.0821
	step [101/190], loss=11.3099
	step [102/190], loss=10.8386
	step [103/190], loss=10.5746
	step [104/190], loss=9.4435
	step [105/190], loss=10.2939
	step [106/190], loss=10.1659
	step [107/190], loss=12.2621
	step [108/190], loss=10.0558
	step [109/190], loss=10.1915
	step [110/190], loss=12.7726
	step [111/190], loss=11.2448
	step [112/190], loss=12.1857
	step [113/190], loss=9.8934
	step [114/190], loss=11.2426
	step [115/190], loss=8.4957
	step [116/190], loss=10.9214
	step [117/190], loss=10.1721
	step [118/190], loss=11.8404
	step [119/190], loss=9.3891
	step [120/190], loss=10.2245
	step [121/190], loss=9.5804
	step [122/190], loss=9.7753
	step [123/190], loss=11.5260
	step [124/190], loss=8.8802
	step [125/190], loss=10.9605
	step [126/190], loss=12.3883
	step [127/190], loss=12.0256
	step [128/190], loss=12.4236
	step [129/190], loss=10.2293
	step [130/190], loss=12.9188
	step [131/190], loss=11.1495
	step [132/190], loss=9.7088
	step [133/190], loss=9.6491
	step [134/190], loss=11.6678
	step [135/190], loss=11.4680
	step [136/190], loss=11.6203
	step [137/190], loss=10.8916
	step [138/190], loss=11.2752
	step [139/190], loss=11.2208
	step [140/190], loss=11.6241
	step [141/190], loss=10.8261
	step [142/190], loss=13.1077
	step [143/190], loss=10.1847
	step [144/190], loss=12.2521
	step [145/190], loss=11.2123
	step [146/190], loss=11.3877
	step [147/190], loss=11.3795
	step [148/190], loss=10.7247
	step [149/190], loss=10.5677
	step [150/190], loss=11.4237
	step [151/190], loss=11.1300
	step [152/190], loss=10.0169
	step [153/190], loss=10.7358
	step [154/190], loss=9.5064
	step [155/190], loss=9.5551
	step [156/190], loss=10.5545
	step [157/190], loss=10.9840
	step [158/190], loss=10.2332
	step [159/190], loss=10.3469
	step [160/190], loss=11.7061
	step [161/190], loss=10.9738
	step [162/190], loss=9.8974
	step [163/190], loss=12.3647
	step [164/190], loss=8.8366
	step [165/190], loss=11.8005
	step [166/190], loss=11.1193
	step [167/190], loss=11.6596
	step [168/190], loss=10.8542
	step [169/190], loss=8.9126
	step [170/190], loss=11.2393
	step [171/190], loss=9.8111
	step [172/190], loss=8.9800
	step [173/190], loss=11.3882
	step [174/190], loss=11.3488
	step [175/190], loss=10.2879
	step [176/190], loss=10.2382
	step [177/190], loss=12.6735
	step [178/190], loss=8.9347
	step [179/190], loss=12.6760
	step [180/190], loss=11.3437
	step [181/190], loss=9.9134
	step [182/190], loss=10.1814
	step [183/190], loss=11.9661
	step [184/190], loss=9.1668
	step [185/190], loss=10.9545
	step [186/190], loss=10.3246
	step [187/190], loss=11.2590
	step [188/190], loss=9.6278
	step [189/190], loss=8.8565
	step [190/190], loss=11.0751
	Evaluating
	loss=0.0263, precision=0.2587, recall=0.9892, f1=0.4102
Training epoch 22
	step [1/190], loss=11.2381
	step [2/190], loss=10.6868
	step [3/190], loss=9.7135
	step [4/190], loss=10.0471
	step [5/190], loss=9.5161
	step [6/190], loss=10.3112
	step [7/190], loss=9.0777
	step [8/190], loss=9.8697
	step [9/190], loss=10.4580
	step [10/190], loss=10.8794
	step [11/190], loss=10.6454
	step [12/190], loss=9.5840
	step [13/190], loss=8.4574
	step [14/190], loss=7.7460
	step [15/190], loss=13.4163
	step [16/190], loss=8.8360
	step [17/190], loss=8.4772
	step [18/190], loss=10.8157
	step [19/190], loss=10.9135
	step [20/190], loss=10.6985
	step [21/190], loss=8.4807
	step [22/190], loss=9.6841
	step [23/190], loss=10.5559
	step [24/190], loss=10.9931
	step [25/190], loss=9.8462
	step [26/190], loss=11.3992
	step [27/190], loss=11.6177
	step [28/190], loss=10.9602
	step [29/190], loss=7.4585
	step [30/190], loss=10.0203
	step [31/190], loss=11.7386
	step [32/190], loss=11.4234
	step [33/190], loss=9.3185
	step [34/190], loss=10.5064
	step [35/190], loss=10.3383
	step [36/190], loss=11.2411
	step [37/190], loss=9.8118
	step [38/190], loss=9.0983
	step [39/190], loss=8.6868
	step [40/190], loss=8.8550
	step [41/190], loss=8.6592
	step [42/190], loss=11.2922
	step [43/190], loss=14.6700
	step [44/190], loss=11.6972
	step [45/190], loss=10.1362
	step [46/190], loss=10.3496
	step [47/190], loss=9.5443
	step [48/190], loss=9.9086
	step [49/190], loss=13.6877
	step [50/190], loss=11.0503
	step [51/190], loss=10.2544
	step [52/190], loss=13.8306
	step [53/190], loss=9.3036
	step [54/190], loss=10.4442
	step [55/190], loss=9.2091
	step [56/190], loss=11.0335
	step [57/190], loss=9.5190
	step [58/190], loss=10.5520
	step [59/190], loss=9.9111
	step [60/190], loss=9.6782
	step [61/190], loss=9.6056
	step [62/190], loss=9.2730
	step [63/190], loss=10.8739
	step [64/190], loss=10.1328
	step [65/190], loss=10.7163
	step [66/190], loss=10.8655
	step [67/190], loss=9.7307
	step [68/190], loss=9.5048
	step [69/190], loss=12.1643
	step [70/190], loss=13.3949
	step [71/190], loss=10.9616
	step [72/190], loss=11.5440
	step [73/190], loss=9.8062
	step [74/190], loss=13.0192
	step [75/190], loss=9.5813
	step [76/190], loss=12.1918
	step [77/190], loss=10.1747
	step [78/190], loss=9.7611
	step [79/190], loss=8.8941
	step [80/190], loss=11.0201
	step [81/190], loss=10.8929
	step [82/190], loss=10.5558
	step [83/190], loss=11.1050
	step [84/190], loss=10.2277
	step [85/190], loss=11.0709
	step [86/190], loss=11.1360
	step [87/190], loss=10.8428
	step [88/190], loss=9.5084
	step [89/190], loss=10.7731
	step [90/190], loss=11.5584
	step [91/190], loss=11.4389
	step [92/190], loss=10.9742
	step [93/190], loss=9.5307
	step [94/190], loss=9.5474
	step [95/190], loss=11.0027
	step [96/190], loss=11.7100
	step [97/190], loss=12.5208
	step [98/190], loss=11.1647
	step [99/190], loss=11.7299
	step [100/190], loss=9.7893
	step [101/190], loss=10.7526
	step [102/190], loss=12.4634
	step [103/190], loss=10.3196
	step [104/190], loss=10.4936
	step [105/190], loss=12.0877
	step [106/190], loss=10.1891
	step [107/190], loss=10.6947
	step [108/190], loss=9.8968
	step [109/190], loss=9.0940
	step [110/190], loss=10.9886
	step [111/190], loss=10.2696
	step [112/190], loss=11.5214
	step [113/190], loss=9.9657
	step [114/190], loss=11.4138
	step [115/190], loss=10.8623
	step [116/190], loss=11.0468
	step [117/190], loss=9.5239
	step [118/190], loss=11.1081
	step [119/190], loss=10.2995
	step [120/190], loss=7.9150
	step [121/190], loss=10.1683
	step [122/190], loss=10.8436
	step [123/190], loss=10.7937
	step [124/190], loss=10.1628
	step [125/190], loss=8.5458
	step [126/190], loss=11.2391
	step [127/190], loss=11.6155
	step [128/190], loss=10.8497
	step [129/190], loss=10.8075
	step [130/190], loss=9.5850
	step [131/190], loss=11.0858
	step [132/190], loss=12.0449
	step [133/190], loss=11.4851
	step [134/190], loss=11.8477
	step [135/190], loss=9.5997
	step [136/190], loss=9.8910
	step [137/190], loss=12.2513
	step [138/190], loss=10.8148
	step [139/190], loss=11.5208
	step [140/190], loss=11.1831
	step [141/190], loss=10.8228
	step [142/190], loss=10.8681
	step [143/190], loss=12.7346
	step [144/190], loss=9.8859
	step [145/190], loss=8.0232
	step [146/190], loss=10.3097
	step [147/190], loss=9.5634
	step [148/190], loss=10.4496
	step [149/190], loss=8.4401
	step [150/190], loss=7.9329
	step [151/190], loss=11.9183
	step [152/190], loss=8.7994
	step [153/190], loss=12.2361
	step [154/190], loss=10.5787
	step [155/190], loss=8.5302
	step [156/190], loss=10.0971
	step [157/190], loss=9.6860
	step [158/190], loss=11.7117
	step [159/190], loss=8.5346
	step [160/190], loss=10.7252
	step [161/190], loss=10.5645
	step [162/190], loss=9.2383
	step [163/190], loss=10.4391
	step [164/190], loss=11.8123
	step [165/190], loss=10.5508
	step [166/190], loss=11.0493
	step [167/190], loss=9.8932
	step [168/190], loss=11.4992
	step [169/190], loss=11.6004
	step [170/190], loss=10.3240
	step [171/190], loss=10.7053
	step [172/190], loss=10.0713
	step [173/190], loss=8.3894
	step [174/190], loss=12.6605
	step [175/190], loss=8.5767
	step [176/190], loss=11.3188
	step [177/190], loss=11.4077
	step [178/190], loss=10.7932
	step [179/190], loss=10.8066
	step [180/190], loss=8.5326
	step [181/190], loss=11.6997
	step [182/190], loss=9.3413
	step [183/190], loss=12.4456
	step [184/190], loss=10.9657
	step [185/190], loss=9.9788
	step [186/190], loss=10.5495
	step [187/190], loss=9.5606
	step [188/190], loss=10.7984
	step [189/190], loss=10.2722
	step [190/190], loss=10.0487
	Evaluating
	loss=0.0264, precision=0.2453, recall=0.9894, f1=0.3931
Training epoch 23
	step [1/190], loss=11.4388
	step [2/190], loss=9.9845
	step [3/190], loss=11.6912
	step [4/190], loss=9.2681
	step [5/190], loss=9.4863
	step [6/190], loss=9.8646
	step [7/190], loss=9.8530
	step [8/190], loss=11.3059
	step [9/190], loss=8.8532
	step [10/190], loss=10.6516
	step [11/190], loss=9.1169
	step [12/190], loss=9.5792
	step [13/190], loss=10.2865
	step [14/190], loss=10.4677
	step [15/190], loss=8.6041
	step [16/190], loss=10.1421
	step [17/190], loss=9.7296
	step [18/190], loss=8.6085
	step [19/190], loss=9.8569
	step [20/190], loss=9.2530
	step [21/190], loss=12.0936
	step [22/190], loss=10.8091
	step [23/190], loss=9.3390
	step [24/190], loss=9.4374
	step [25/190], loss=10.0434
	step [26/190], loss=9.1845
	step [27/190], loss=9.9589
	step [28/190], loss=12.9030
	step [29/190], loss=8.9366
	step [30/190], loss=12.8316
	step [31/190], loss=8.6241
	step [32/190], loss=8.1927
	step [33/190], loss=8.9738
	step [34/190], loss=11.6564
	step [35/190], loss=10.7301
	step [36/190], loss=11.0351
	step [37/190], loss=9.8933
	step [38/190], loss=11.5889
	step [39/190], loss=10.1936
	step [40/190], loss=9.7469
	step [41/190], loss=10.1998
	step [42/190], loss=10.1715
	step [43/190], loss=10.9653
	step [44/190], loss=10.7858
	step [45/190], loss=10.8369
	step [46/190], loss=10.6130
	step [47/190], loss=11.1944
	step [48/190], loss=10.7006
	step [49/190], loss=10.8176
	step [50/190], loss=9.7491
	step [51/190], loss=10.8635
	step [52/190], loss=10.6191
	step [53/190], loss=11.0162
	step [54/190], loss=12.7791
	step [55/190], loss=10.8818
	step [56/190], loss=10.5712
	step [57/190], loss=9.0181
	step [58/190], loss=9.8068
	step [59/190], loss=10.5034
	step [60/190], loss=9.8808
	step [61/190], loss=10.0055
	step [62/190], loss=9.9205
	step [63/190], loss=9.4806
	step [64/190], loss=9.7741
	step [65/190], loss=10.5308
	step [66/190], loss=10.5087
	step [67/190], loss=9.3647
	step [68/190], loss=8.3358
	step [69/190], loss=8.7407
	step [70/190], loss=11.8215
	step [71/190], loss=9.5029
	step [72/190], loss=9.4163
	step [73/190], loss=9.1132
	step [74/190], loss=10.3177
	step [75/190], loss=10.4135
	step [76/190], loss=9.2265
	step [77/190], loss=11.5049
	step [78/190], loss=8.8483
	step [79/190], loss=9.8509
	step [80/190], loss=9.3243
	step [81/190], loss=9.9346
	step [82/190], loss=9.4011
	step [83/190], loss=9.4904
	step [84/190], loss=9.6458
	step [85/190], loss=11.5709
	step [86/190], loss=10.5054
	step [87/190], loss=10.4590
	step [88/190], loss=11.6599
	step [89/190], loss=10.5987
	step [90/190], loss=10.8699
	step [91/190], loss=8.2987
	step [92/190], loss=9.9456
	step [93/190], loss=9.8143
	step [94/190], loss=9.6728
	step [95/190], loss=9.8297
	step [96/190], loss=11.2841
	step [97/190], loss=10.1960
	step [98/190], loss=9.9149
	step [99/190], loss=12.9936
	step [100/190], loss=10.1625
	step [101/190], loss=12.3514
	step [102/190], loss=9.3477
	step [103/190], loss=10.2599
	step [104/190], loss=10.1643
	step [105/190], loss=11.0555
	step [106/190], loss=11.3403
	step [107/190], loss=9.4516
	step [108/190], loss=10.6392
	step [109/190], loss=10.6415
	step [110/190], loss=11.0807
	step [111/190], loss=9.6246
	step [112/190], loss=10.3327
	step [113/190], loss=9.9616
	step [114/190], loss=10.2302
	step [115/190], loss=8.9765
	step [116/190], loss=8.4474
	step [117/190], loss=11.4305
	step [118/190], loss=8.3053
	step [119/190], loss=8.1208
	step [120/190], loss=9.1814
	step [121/190], loss=11.8362
	step [122/190], loss=11.9957
	step [123/190], loss=9.6111
	step [124/190], loss=9.2789
	step [125/190], loss=8.5431
	step [126/190], loss=10.4676
	step [127/190], loss=9.7918
	step [128/190], loss=10.8392
	step [129/190], loss=9.8326
	step [130/190], loss=9.0803
	step [131/190], loss=9.9355
	step [132/190], loss=10.6926
	step [133/190], loss=9.4861
	step [134/190], loss=8.6439
	step [135/190], loss=9.5442
	step [136/190], loss=9.9610
	step [137/190], loss=11.5762
	step [138/190], loss=10.6341
	step [139/190], loss=9.3864
	step [140/190], loss=11.5597
	step [141/190], loss=9.1931
	step [142/190], loss=10.7751
	step [143/190], loss=10.1941
	step [144/190], loss=9.7809
	step [145/190], loss=10.6647
	step [146/190], loss=9.5371
	step [147/190], loss=10.9010
	step [148/190], loss=9.6167
	step [149/190], loss=9.6507
	step [150/190], loss=11.7228
	step [151/190], loss=10.6527
	step [152/190], loss=10.8352
	step [153/190], loss=9.5963
	step [154/190], loss=9.7991
	step [155/190], loss=10.7356
	step [156/190], loss=9.5638
	step [157/190], loss=9.4911
	step [158/190], loss=10.3239
	step [159/190], loss=10.7127
	step [160/190], loss=8.8883
	step [161/190], loss=9.9619
	step [162/190], loss=8.0214
	step [163/190], loss=10.5479
	step [164/190], loss=9.6424
	step [165/190], loss=9.3651
	step [166/190], loss=9.6612
	step [167/190], loss=8.8346
	step [168/190], loss=7.8164
	step [169/190], loss=10.3071
	step [170/190], loss=9.6768
	step [171/190], loss=9.8672
	step [172/190], loss=8.2626
	step [173/190], loss=10.0884
	step [174/190], loss=10.0277
	step [175/190], loss=9.1466
	step [176/190], loss=9.1030
	step [177/190], loss=8.1821
	step [178/190], loss=11.1341
	step [179/190], loss=11.5390
	step [180/190], loss=11.7894
	step [181/190], loss=8.6021
	step [182/190], loss=10.3337
	step [183/190], loss=9.2229
	step [184/190], loss=10.2689
	step [185/190], loss=10.0601
	step [186/190], loss=10.8541
	step [187/190], loss=9.7810
	step [188/190], loss=10.4172
	step [189/190], loss=8.6387
	step [190/190], loss=9.4483
	Evaluating
	loss=0.0325, precision=0.1803, recall=0.9926, f1=0.3052
Training epoch 24
	step [1/190], loss=9.4242
	step [2/190], loss=8.6979
	step [3/190], loss=10.6897
	step [4/190], loss=10.9155
	step [5/190], loss=8.9977
	step [6/190], loss=8.9935
	step [7/190], loss=10.1941
	step [8/190], loss=8.2677
	step [9/190], loss=9.7461
	step [10/190], loss=9.2972
	step [11/190], loss=10.7668
	step [12/190], loss=8.5616
	step [13/190], loss=8.3552
	step [14/190], loss=8.7541
	step [15/190], loss=10.0050
	step [16/190], loss=9.9439
	step [17/190], loss=10.1199
	step [18/190], loss=10.2149
	step [19/190], loss=12.2701
	step [20/190], loss=9.8847
	step [21/190], loss=10.1200
	step [22/190], loss=10.5401
	step [23/190], loss=10.0466
	step [24/190], loss=11.6744
	step [25/190], loss=7.7880
	step [26/190], loss=9.3988
	step [27/190], loss=11.0718
	step [28/190], loss=8.8655
	step [29/190], loss=11.7435
	step [30/190], loss=8.5171
	step [31/190], loss=10.1949
	step [32/190], loss=8.8976
	step [33/190], loss=11.6445
	step [34/190], loss=10.1449
	step [35/190], loss=9.8008
	step [36/190], loss=10.8193
	step [37/190], loss=11.7622
	step [38/190], loss=10.8764
	step [39/190], loss=9.8474
	step [40/190], loss=8.8045
	step [41/190], loss=9.3596
	step [42/190], loss=10.7634
	step [43/190], loss=8.7540
	step [44/190], loss=10.7771
	step [45/190], loss=9.2332
	step [46/190], loss=10.9538
	step [47/190], loss=10.2717
	step [48/190], loss=10.6049
	step [49/190], loss=9.0377
	step [50/190], loss=9.4562
	step [51/190], loss=10.4422
	step [52/190], loss=11.1406
	step [53/190], loss=9.4913
	step [54/190], loss=10.6182
	step [55/190], loss=9.6085
	step [56/190], loss=8.6184
	step [57/190], loss=9.2614
	step [58/190], loss=10.4852
	step [59/190], loss=8.7611
	step [60/190], loss=9.2669
	step [61/190], loss=11.6068
	step [62/190], loss=9.2466
	step [63/190], loss=10.6875
	step [64/190], loss=9.6713
	step [65/190], loss=11.8102
	step [66/190], loss=10.9001
	step [67/190], loss=8.7955
	step [68/190], loss=11.2721
	step [69/190], loss=9.5936
	step [70/190], loss=10.6475
	step [71/190], loss=8.4521
	step [72/190], loss=9.9998
	step [73/190], loss=9.4564
	step [74/190], loss=9.3482
	step [75/190], loss=7.6167
	step [76/190], loss=11.0556
	step [77/190], loss=12.5746
	step [78/190], loss=10.0693
	step [79/190], loss=11.4487
	step [80/190], loss=8.2118
	step [81/190], loss=8.9044
	step [82/190], loss=9.7683
	step [83/190], loss=10.0457
	step [84/190], loss=9.3403
	step [85/190], loss=10.1881
	step [86/190], loss=9.2388
	step [87/190], loss=11.5127
	step [88/190], loss=9.4476
	step [89/190], loss=9.4646
	step [90/190], loss=9.0813
	step [91/190], loss=9.4889
	step [92/190], loss=9.6547
	step [93/190], loss=8.2045
	step [94/190], loss=9.2581
	step [95/190], loss=9.6645
	step [96/190], loss=11.8987
	step [97/190], loss=10.3122
	step [98/190], loss=9.7855
	step [99/190], loss=9.1636
	step [100/190], loss=9.9448
	step [101/190], loss=8.2119
	step [102/190], loss=9.0594
	step [103/190], loss=9.4468
	step [104/190], loss=9.6266
	step [105/190], loss=10.1525
	step [106/190], loss=9.4390
	step [107/190], loss=10.7235
	step [108/190], loss=9.3572
	step [109/190], loss=9.3605
	step [110/190], loss=10.5877
	step [111/190], loss=10.1376
	step [112/190], loss=8.8248
	step [113/190], loss=8.1406
	step [114/190], loss=11.8621
	step [115/190], loss=7.6317
	step [116/190], loss=9.9162
	step [117/190], loss=11.3916
	step [118/190], loss=13.0496
	step [119/190], loss=10.1468
	step [120/190], loss=11.5482
	step [121/190], loss=9.5195
	step [122/190], loss=9.9230
	step [123/190], loss=8.6927
	step [124/190], loss=10.7638
	step [125/190], loss=11.1808
	step [126/190], loss=9.1761
	step [127/190], loss=10.2376
	step [128/190], loss=7.6151
	step [129/190], loss=11.1431
	step [130/190], loss=8.7221
	step [131/190], loss=9.4638
	step [132/190], loss=8.7289
	step [133/190], loss=10.2031
	step [134/190], loss=9.0087
	step [135/190], loss=10.2755
	step [136/190], loss=10.1162
	step [137/190], loss=9.5954
	step [138/190], loss=9.4072
	step [139/190], loss=10.4453
	step [140/190], loss=9.2751
	step [141/190], loss=10.3249
	step [142/190], loss=9.2012
	step [143/190], loss=8.8996
	step [144/190], loss=9.7543
	step [145/190], loss=8.2732
	step [146/190], loss=8.6091
	step [147/190], loss=8.3386
	step [148/190], loss=7.3963
	step [149/190], loss=8.4455
	step [150/190], loss=10.8839
	step [151/190], loss=8.6763
	step [152/190], loss=10.5503
	step [153/190], loss=12.0065
	step [154/190], loss=10.3537
	step [155/190], loss=13.4375
	step [156/190], loss=10.3433
	step [157/190], loss=10.3409
	step [158/190], loss=9.3653
	step [159/190], loss=10.9027
	step [160/190], loss=8.5669
	step [161/190], loss=8.3661
	step [162/190], loss=9.6792
	step [163/190], loss=9.4794
	step [164/190], loss=8.9648
	step [165/190], loss=7.1531
	step [166/190], loss=8.8155
	step [167/190], loss=8.6596
	step [168/190], loss=10.4407
	step [169/190], loss=9.6996
	step [170/190], loss=10.8286
	step [171/190], loss=10.4025
	step [172/190], loss=7.8969
	step [173/190], loss=10.4456
	step [174/190], loss=8.2030
	step [175/190], loss=7.4306
	step [176/190], loss=10.5596
	step [177/190], loss=10.1803
	step [178/190], loss=11.3165
	step [179/190], loss=9.6009
	step [180/190], loss=9.5686
	step [181/190], loss=9.3971
	step [182/190], loss=9.5951
	step [183/190], loss=10.3431
	step [184/190], loss=10.8115
	step [185/190], loss=9.2031
	step [186/190], loss=10.2064
	step [187/190], loss=10.9404
	step [188/190], loss=8.6061
	step [189/190], loss=9.0078
	step [190/190], loss=9.2203
	Evaluating
	loss=0.0247, precision=0.2343, recall=0.9902, f1=0.3789
Training epoch 25
	step [1/190], loss=8.7665
	step [2/190], loss=10.9534
	step [3/190], loss=11.2389
	step [4/190], loss=10.3846
	step [5/190], loss=10.9350
	step [6/190], loss=10.3977
	step [7/190], loss=8.5034
	step [8/190], loss=8.3528
	step [9/190], loss=10.4960
	step [10/190], loss=8.2309
	step [11/190], loss=9.1892
	step [12/190], loss=7.8582
	step [13/190], loss=10.8380
	step [14/190], loss=9.7680
	step [15/190], loss=9.1720
	step [16/190], loss=8.7580
	step [17/190], loss=10.3126
	step [18/190], loss=11.8732
	step [19/190], loss=11.1201
	step [20/190], loss=8.9125
	step [21/190], loss=10.3345
	step [22/190], loss=7.4106
	step [23/190], loss=7.9401
	step [24/190], loss=9.5335
	step [25/190], loss=9.7351
	step [26/190], loss=11.0137
	step [27/190], loss=7.3897
	step [28/190], loss=9.3148
	step [29/190], loss=7.7760
	step [30/190], loss=11.0970
	step [31/190], loss=9.3113
	step [32/190], loss=12.0416
	step [33/190], loss=8.4286
	step [34/190], loss=8.4658
	step [35/190], loss=9.1988
	step [36/190], loss=8.6219
	step [37/190], loss=9.6509
	step [38/190], loss=9.2573
	step [39/190], loss=9.0969
	step [40/190], loss=10.7446
	step [41/190], loss=8.4749
	step [42/190], loss=10.2806
	step [43/190], loss=10.6657
	step [44/190], loss=11.0905
	step [45/190], loss=8.0790
	step [46/190], loss=9.0072
	step [47/190], loss=9.2678
	step [48/190], loss=9.1033
	step [49/190], loss=10.9667
	step [50/190], loss=10.0039
	step [51/190], loss=11.3438
	step [52/190], loss=9.3508
	step [53/190], loss=9.1774
	step [54/190], loss=8.4166
	step [55/190], loss=8.5717
	step [56/190], loss=8.2246
	step [57/190], loss=10.0836
	step [58/190], loss=8.9813
	step [59/190], loss=10.9079
	step [60/190], loss=8.9666
	step [61/190], loss=9.4268
	step [62/190], loss=9.2349
	step [63/190], loss=8.9663
	step [64/190], loss=9.9543
	step [65/190], loss=9.4602
	step [66/190], loss=9.3885
	step [67/190], loss=9.1060
	step [68/190], loss=7.5479
	step [69/190], loss=10.1996
	step [70/190], loss=8.9464
	step [71/190], loss=10.4735
	step [72/190], loss=8.4618
	step [73/190], loss=8.0327
	step [74/190], loss=9.4574
	step [75/190], loss=9.0642
	step [76/190], loss=8.6486
	step [77/190], loss=10.3519
	step [78/190], loss=12.0325
	step [79/190], loss=8.4350
	step [80/190], loss=8.6208
	step [81/190], loss=9.4279
	step [82/190], loss=9.0074
	step [83/190], loss=8.6590
	step [84/190], loss=9.8075
	step [85/190], loss=10.6928
	step [86/190], loss=8.9795
	step [87/190], loss=8.5550
	step [88/190], loss=8.0585
	step [89/190], loss=8.3352
	step [90/190], loss=12.6721
	step [91/190], loss=8.6264
	step [92/190], loss=9.3957
	step [93/190], loss=12.6966
	step [94/190], loss=9.5118
	step [95/190], loss=10.1741
	step [96/190], loss=8.6965
	step [97/190], loss=7.7072
	step [98/190], loss=9.2807
	step [99/190], loss=10.1334
	step [100/190], loss=7.7205
	step [101/190], loss=10.8582
	step [102/190], loss=8.8756
	step [103/190], loss=9.0693
	step [104/190], loss=8.3398
	step [105/190], loss=10.2824
	step [106/190], loss=9.9491
	step [107/190], loss=10.7845
	step [108/190], loss=10.5136
	step [109/190], loss=9.5050
	step [110/190], loss=10.4288
	step [111/190], loss=8.3496
	step [112/190], loss=11.2199
	step [113/190], loss=9.4904
	step [114/190], loss=10.0493
	step [115/190], loss=9.9980
	step [116/190], loss=7.9533
	step [117/190], loss=10.3321
	step [118/190], loss=9.3075
	step [119/190], loss=9.4237
	step [120/190], loss=9.5164
	step [121/190], loss=10.5766
	step [122/190], loss=10.2312
	step [123/190], loss=9.2589
	step [124/190], loss=8.4874
	step [125/190], loss=10.7779
	step [126/190], loss=8.6850
	step [127/190], loss=9.3829
	step [128/190], loss=9.4525
	step [129/190], loss=9.4215
	step [130/190], loss=9.0345
	step [131/190], loss=9.3261
	step [132/190], loss=11.2828
	step [133/190], loss=10.1006
	step [134/190], loss=8.3820
	step [135/190], loss=10.1516
	step [136/190], loss=8.8760
	step [137/190], loss=7.9147
	step [138/190], loss=8.3719
	step [139/190], loss=8.8242
	step [140/190], loss=8.1011
	step [141/190], loss=10.1865
	step [142/190], loss=9.0228
	step [143/190], loss=10.3923
	step [144/190], loss=7.3868
	step [145/190], loss=9.1757
	step [146/190], loss=8.1077
	step [147/190], loss=8.5197
	step [148/190], loss=7.9332
	step [149/190], loss=11.3031
	step [150/190], loss=8.5264
	step [151/190], loss=8.5991
	step [152/190], loss=12.5415
	step [153/190], loss=8.2546
	step [154/190], loss=9.1515
	step [155/190], loss=10.6329
	step [156/190], loss=10.7058
	step [157/190], loss=9.0436
	step [158/190], loss=8.5067
	step [159/190], loss=8.9265
	step [160/190], loss=10.3137
	step [161/190], loss=9.0304
	step [162/190], loss=8.4265
	step [163/190], loss=8.6936
	step [164/190], loss=9.2335
	step [165/190], loss=8.9272
	step [166/190], loss=9.9976
	step [167/190], loss=9.5320
	step [168/190], loss=8.4555
	step [169/190], loss=9.0162
	step [170/190], loss=8.6074
	step [171/190], loss=8.7790
	step [172/190], loss=7.8308
	step [173/190], loss=7.5483
	step [174/190], loss=11.2567
	step [175/190], loss=11.0972
	step [176/190], loss=10.7542
	step [177/190], loss=7.7035
	step [178/190], loss=9.8696
	step [179/190], loss=9.5091
	step [180/190], loss=10.8569
	step [181/190], loss=7.7737
	step [182/190], loss=9.9025
	step [183/190], loss=10.9276
	step [184/190], loss=8.3917
	step [185/190], loss=8.8152
	step [186/190], loss=8.0544
	step [187/190], loss=8.3714
	step [188/190], loss=8.9523
	step [189/190], loss=8.8944
	step [190/190], loss=8.8682
	Evaluating
	loss=0.0209, precision=0.2905, recall=0.9875, f1=0.4489
saving model as: 1_saved_model.pth
Training epoch 26
	step [1/190], loss=10.1421
	step [2/190], loss=9.7405
	step [3/190], loss=10.4223
	step [4/190], loss=9.0808
	step [5/190], loss=8.6655
	step [6/190], loss=7.8479
	step [7/190], loss=8.5579
	step [8/190], loss=9.8805
	step [9/190], loss=8.4876
	step [10/190], loss=8.4468
	step [11/190], loss=7.9905
	step [12/190], loss=10.2976
	step [13/190], loss=9.9622
	step [14/190], loss=8.9286
	step [15/190], loss=10.6129
	step [16/190], loss=10.3376
	step [17/190], loss=10.1157
	step [18/190], loss=11.7781
	step [19/190], loss=9.2657
	step [20/190], loss=9.5647
	step [21/190], loss=10.8805
	step [22/190], loss=9.1249
	step [23/190], loss=12.3919
	step [24/190], loss=9.0610
	step [25/190], loss=9.3733
	step [26/190], loss=11.8353
	step [27/190], loss=11.5812
	step [28/190], loss=9.1185
	step [29/190], loss=10.1491
	step [30/190], loss=10.9078
	step [31/190], loss=9.7880
	step [32/190], loss=9.3378
	step [33/190], loss=8.2462
	step [34/190], loss=8.6886
	step [35/190], loss=9.8246
	step [36/190], loss=9.2410
	step [37/190], loss=8.4223
	step [38/190], loss=8.9083
	step [39/190], loss=8.9925
	step [40/190], loss=10.3707
	step [41/190], loss=10.9972
	step [42/190], loss=9.8953
	step [43/190], loss=12.4873
	step [44/190], loss=9.9934
	step [45/190], loss=9.1963
	step [46/190], loss=9.6621
	step [47/190], loss=7.8169
	step [48/190], loss=11.5355
	step [49/190], loss=9.4258
	step [50/190], loss=12.2297
	step [51/190], loss=8.9887
	step [52/190], loss=8.8182
	step [53/190], loss=10.0958
	step [54/190], loss=9.0765
	step [55/190], loss=9.1569
	step [56/190], loss=9.5321
	step [57/190], loss=9.0327
	step [58/190], loss=11.8178
	step [59/190], loss=9.3683
	step [60/190], loss=10.1744
	step [61/190], loss=9.4043
	step [62/190], loss=10.9357
	step [63/190], loss=10.3625
	step [64/190], loss=9.9034
	step [65/190], loss=10.4100
	step [66/190], loss=8.0798
	step [67/190], loss=7.4821
	step [68/190], loss=11.5640
	step [69/190], loss=10.1765
	step [70/190], loss=9.3835
	step [71/190], loss=11.5466
	step [72/190], loss=7.8730
	step [73/190], loss=8.1770
	step [74/190], loss=9.6293
	step [75/190], loss=8.0734
	step [76/190], loss=9.5962
	step [77/190], loss=7.7864
	step [78/190], loss=10.3271
	step [79/190], loss=10.1963
	step [80/190], loss=8.6113
	step [81/190], loss=10.4918
	step [82/190], loss=9.7513
	step [83/190], loss=8.6464
	step [84/190], loss=11.7154
	step [85/190], loss=8.0481
	step [86/190], loss=7.8333
	step [87/190], loss=8.9000
	step [88/190], loss=9.8106
	step [89/190], loss=9.3345
	step [90/190], loss=11.4321
	step [91/190], loss=8.8152
	step [92/190], loss=9.5472
	step [93/190], loss=8.7384
	step [94/190], loss=9.9891
	step [95/190], loss=8.8262
	step [96/190], loss=9.1446
	step [97/190], loss=8.0751
	step [98/190], loss=9.4291
	step [99/190], loss=9.4169
	step [100/190], loss=7.9734
	step [101/190], loss=8.0649
	step [102/190], loss=8.5864
	step [103/190], loss=9.8082
	step [104/190], loss=8.8420
	step [105/190], loss=7.8704
	step [106/190], loss=10.1351
	step [107/190], loss=10.1079
	step [108/190], loss=8.2231
	step [109/190], loss=8.5481
	step [110/190], loss=8.5812
	step [111/190], loss=7.6703
	step [112/190], loss=8.9370
	step [113/190], loss=12.1243
	step [114/190], loss=10.3564
	step [115/190], loss=9.8666
	step [116/190], loss=8.0736
	step [117/190], loss=10.4339
	step [118/190], loss=9.0410
	step [119/190], loss=9.5396
	step [120/190], loss=8.5205
	step [121/190], loss=9.0146
	step [122/190], loss=9.3750
	step [123/190], loss=11.6832
	step [124/190], loss=9.6797
	step [125/190], loss=7.1561
	step [126/190], loss=8.9616
	step [127/190], loss=6.9918
	step [128/190], loss=6.8135
	step [129/190], loss=8.6757
	step [130/190], loss=7.9076
	step [131/190], loss=7.8832
	step [132/190], loss=7.9561
	step [133/190], loss=10.2480
	step [134/190], loss=9.1849
	step [135/190], loss=9.3613
	step [136/190], loss=9.1536
	step [137/190], loss=8.8411
	step [138/190], loss=9.3494
	step [139/190], loss=9.5155
	step [140/190], loss=9.2186
	step [141/190], loss=9.0928
	step [142/190], loss=11.8197
	step [143/190], loss=9.9777
	step [144/190], loss=8.9807
	step [145/190], loss=9.8327
	step [146/190], loss=9.1767
	step [147/190], loss=9.2084
	step [148/190], loss=9.0739
	step [149/190], loss=9.5521
	step [150/190], loss=8.6387
	step [151/190], loss=9.8657
	step [152/190], loss=9.3377
	step [153/190], loss=8.9039
	step [154/190], loss=9.2388
	step [155/190], loss=8.9768
	step [156/190], loss=7.9616
	step [157/190], loss=8.1524
	step [158/190], loss=9.3823
	step [159/190], loss=11.1331
	step [160/190], loss=8.0660
	step [161/190], loss=9.8640
	step [162/190], loss=10.6400
	step [163/190], loss=11.3780
	step [164/190], loss=10.6922
	step [165/190], loss=9.4497
	step [166/190], loss=8.9288
	step [167/190], loss=8.1906
	step [168/190], loss=8.7378
	step [169/190], loss=9.5692
	step [170/190], loss=7.8049
	step [171/190], loss=10.2079
	step [172/190], loss=9.7660
	step [173/190], loss=9.7290
	step [174/190], loss=10.2918
	step [175/190], loss=9.0006
	step [176/190], loss=7.6122
	step [177/190], loss=8.8401
	step [178/190], loss=10.0628
	step [179/190], loss=9.2972
	step [180/190], loss=8.7372
	step [181/190], loss=8.3979
	step [182/190], loss=8.9802
	step [183/190], loss=8.2413
	step [184/190], loss=9.0034
	step [185/190], loss=11.3564
	step [186/190], loss=11.7618
	step [187/190], loss=8.7292
	step [188/190], loss=9.4455
	step [189/190], loss=10.8866
	step [190/190], loss=9.0129
	Evaluating
	loss=0.0244, precision=0.2266, recall=0.9900, f1=0.3688
Training epoch 27
	step [1/190], loss=8.9766
	step [2/190], loss=7.1992
	step [3/190], loss=8.6668
	step [4/190], loss=8.8436
	step [5/190], loss=9.8395
	step [6/190], loss=8.4989
	step [7/190], loss=8.3825
	step [8/190], loss=10.7060
	step [9/190], loss=8.3691
	step [10/190], loss=9.9178
	step [11/190], loss=7.7057
	step [12/190], loss=8.4158
	step [13/190], loss=10.3889
	step [14/190], loss=9.4745
	step [15/190], loss=8.4262
	step [16/190], loss=7.6556
	step [17/190], loss=9.4341
	step [18/190], loss=7.7499
	step [19/190], loss=8.7227
	step [20/190], loss=6.7089
	step [21/190], loss=7.1153
	step [22/190], loss=11.2347
	step [23/190], loss=9.9351
	step [24/190], loss=8.5949
	step [25/190], loss=8.2582
	step [26/190], loss=10.0217
	step [27/190], loss=9.3605
	step [28/190], loss=9.2320
	step [29/190], loss=8.4216
	step [30/190], loss=7.1095
	step [31/190], loss=10.4921
	step [32/190], loss=9.1361
	step [33/190], loss=9.7477
	step [34/190], loss=9.4066
	step [35/190], loss=9.3049
	step [36/190], loss=9.1977
	step [37/190], loss=7.9612
	step [38/190], loss=7.8672
	step [39/190], loss=9.1810
	step [40/190], loss=9.0041
	step [41/190], loss=8.2614
	step [42/190], loss=8.5659
	step [43/190], loss=8.2248
	step [44/190], loss=8.3736
	step [45/190], loss=9.8549
	step [46/190], loss=8.4833
	step [47/190], loss=11.2202
	step [48/190], loss=7.5663
	step [49/190], loss=7.0841
	step [50/190], loss=8.0052
	step [51/190], loss=7.5507
	step [52/190], loss=7.9829
	step [53/190], loss=9.2673
	step [54/190], loss=9.4416
	step [55/190], loss=8.8723
	step [56/190], loss=7.9202
	step [57/190], loss=9.4720
	step [58/190], loss=8.5466
	step [59/190], loss=7.6116
	step [60/190], loss=8.2142
	step [61/190], loss=8.5535
	step [62/190], loss=9.3475
	step [63/190], loss=9.3039
	step [64/190], loss=8.8598
	step [65/190], loss=9.6209
	step [66/190], loss=9.1269
	step [67/190], loss=7.2920
	step [68/190], loss=9.6895
	step [69/190], loss=8.1704
	step [70/190], loss=8.3634
	step [71/190], loss=8.3446
	step [72/190], loss=9.0087
	step [73/190], loss=9.1862
	step [74/190], loss=8.9182
	step [75/190], loss=10.0279
	step [76/190], loss=8.6073
	step [77/190], loss=9.6362
	step [78/190], loss=9.6196
	step [79/190], loss=7.4640
	step [80/190], loss=9.8705
	step [81/190], loss=9.5156
	step [82/190], loss=10.3342
	step [83/190], loss=8.8148
	step [84/190], loss=11.0560
	step [85/190], loss=10.0010
	step [86/190], loss=9.1521
	step [87/190], loss=8.7487
	step [88/190], loss=10.5188
	step [89/190], loss=9.8722
	step [90/190], loss=8.4058
	step [91/190], loss=9.5313
	step [92/190], loss=9.3206
	step [93/190], loss=9.9181
	step [94/190], loss=9.8327
	step [95/190], loss=9.9378
	step [96/190], loss=8.5485
	step [97/190], loss=8.7057
	step [98/190], loss=7.9896
	step [99/190], loss=10.2495
	step [100/190], loss=8.5595
	step [101/190], loss=8.7450
	step [102/190], loss=9.2196
	step [103/190], loss=9.5233
	step [104/190], loss=8.0092
	step [105/190], loss=10.2351
	step [106/190], loss=8.5231
	step [107/190], loss=9.2990
	step [108/190], loss=7.9070
	step [109/190], loss=9.2481
	step [110/190], loss=9.0987
	step [111/190], loss=8.5649
	step [112/190], loss=8.5716
	step [113/190], loss=9.0860
	step [114/190], loss=10.6427
	step [115/190], loss=7.6685
	step [116/190], loss=10.0175
	step [117/190], loss=8.1561
	step [118/190], loss=10.1919
	step [119/190], loss=9.3660
	step [120/190], loss=8.3671
	step [121/190], loss=7.7462
	step [122/190], loss=8.2249
	step [123/190], loss=9.6018
	step [124/190], loss=8.2344
	step [125/190], loss=9.7779
	step [126/190], loss=8.4969
	step [127/190], loss=8.8542
	step [128/190], loss=8.9417
	step [129/190], loss=7.4112
	step [130/190], loss=7.4751
	step [131/190], loss=10.1845
	step [132/190], loss=7.2575
	step [133/190], loss=8.8097
	step [134/190], loss=8.3283
	step [135/190], loss=8.1110
	step [136/190], loss=8.9760
	step [137/190], loss=9.3939
	step [138/190], loss=10.3399
	step [139/190], loss=9.3322
	step [140/190], loss=7.5102
	step [141/190], loss=8.4275
	step [142/190], loss=9.9387
	step [143/190], loss=9.5833
	step [144/190], loss=8.1601
	step [145/190], loss=7.9346
	step [146/190], loss=8.3309
	step [147/190], loss=7.4750
	step [148/190], loss=7.9240
	step [149/190], loss=9.9004
	step [150/190], loss=8.2592
	step [151/190], loss=7.9790
	step [152/190], loss=8.1937
	step [153/190], loss=10.0013
	step [154/190], loss=9.1799
	step [155/190], loss=7.8199
	step [156/190], loss=7.8926
	step [157/190], loss=9.1134
	step [158/190], loss=10.2262
	step [159/190], loss=8.2712
	step [160/190], loss=8.0618
	step [161/190], loss=9.8534
	step [162/190], loss=8.5941
	step [163/190], loss=10.5908
	step [164/190], loss=8.0664
	step [165/190], loss=10.2116
	step [166/190], loss=9.4153
	step [167/190], loss=8.6993
	step [168/190], loss=11.0840
	step [169/190], loss=7.7525
	step [170/190], loss=9.2393
	step [171/190], loss=7.2134
	step [172/190], loss=8.6861
	step [173/190], loss=9.1862
	step [174/190], loss=9.0795
	step [175/190], loss=8.7207
	step [176/190], loss=7.5370
	step [177/190], loss=9.7874
	step [178/190], loss=8.4515
	step [179/190], loss=7.9879
	step [180/190], loss=8.4342
	step [181/190], loss=9.9450
	step [182/190], loss=11.0574
	step [183/190], loss=9.9139
	step [184/190], loss=7.8834
	step [185/190], loss=10.0077
	step [186/190], loss=8.6885
	step [187/190], loss=9.2165
	step [188/190], loss=7.5525
	step [189/190], loss=8.4068
	step [190/190], loss=7.0852
	Evaluating
	loss=0.0283, precision=0.1969, recall=0.9919, f1=0.3286
Training epoch 28
	step [1/190], loss=9.3460
	step [2/190], loss=9.5254
	step [3/190], loss=9.0616
	step [4/190], loss=7.8931
	step [5/190], loss=8.2109
	step [6/190], loss=8.9441
	step [7/190], loss=9.8886
	step [8/190], loss=9.5686
	step [9/190], loss=9.8615
	step [10/190], loss=9.1110
	step [11/190], loss=8.0183
	step [12/190], loss=8.4329
	step [13/190], loss=10.0392
	step [14/190], loss=8.0013
	step [15/190], loss=8.8496
	step [16/190], loss=8.3506
	step [17/190], loss=9.9945
	step [18/190], loss=7.2098
	step [19/190], loss=10.3038
	step [20/190], loss=7.4953
	step [21/190], loss=6.9735
	step [22/190], loss=9.1449
	step [23/190], loss=10.1036
	step [24/190], loss=10.8585
	step [25/190], loss=9.0196
	step [26/190], loss=9.2008
	step [27/190], loss=8.3826
	step [28/190], loss=11.6004
	step [29/190], loss=9.2013
	step [30/190], loss=8.3715
	step [31/190], loss=7.9151
	step [32/190], loss=8.8294
	step [33/190], loss=7.9336
	step [34/190], loss=10.7753
	step [35/190], loss=8.1420
	step [36/190], loss=7.5134
	step [37/190], loss=8.8894
	step [38/190], loss=6.6252
	step [39/190], loss=9.9626
	step [40/190], loss=10.1629
	step [41/190], loss=8.8371
	step [42/190], loss=7.5880
	step [43/190], loss=8.9590
	step [44/190], loss=8.4636
	step [45/190], loss=9.2761
	step [46/190], loss=8.8438
	step [47/190], loss=6.9030
	step [48/190], loss=8.1193
	step [49/190], loss=7.7297
	step [50/190], loss=7.5678
	step [51/190], loss=9.7752
	step [52/190], loss=9.7156
	step [53/190], loss=8.9216
	step [54/190], loss=8.3956
	step [55/190], loss=7.3247
	step [56/190], loss=7.7557
	step [57/190], loss=7.3531
	step [58/190], loss=7.3506
	step [59/190], loss=8.0819
	step [60/190], loss=7.9445
	step [61/190], loss=9.1911
	step [62/190], loss=10.6187
	step [63/190], loss=9.1121
	step [64/190], loss=10.0344
	step [65/190], loss=8.3130
	step [66/190], loss=11.2432
	step [67/190], loss=8.8568
	step [68/190], loss=9.4863
	step [69/190], loss=10.1962
	step [70/190], loss=8.1400
	step [71/190], loss=9.1418
	step [72/190], loss=8.0527
	step [73/190], loss=9.7070
	step [74/190], loss=8.8541
	step [75/190], loss=7.4017
	step [76/190], loss=10.4947
	step [77/190], loss=8.7368
	step [78/190], loss=8.9485
	step [79/190], loss=8.7226
	step [80/190], loss=8.4675
	step [81/190], loss=8.4224
	step [82/190], loss=8.3568
	step [83/190], loss=9.3392
	step [84/190], loss=7.8103
	step [85/190], loss=8.5969
	step [86/190], loss=8.9958
	step [87/190], loss=8.6537
	step [88/190], loss=7.1854
	step [89/190], loss=8.1976
	step [90/190], loss=9.4601
	step [91/190], loss=8.7382
	step [92/190], loss=8.7162
	step [93/190], loss=7.3222
	step [94/190], loss=7.7340
	step [95/190], loss=8.1559
	step [96/190], loss=8.1851
	step [97/190], loss=10.0469
	step [98/190], loss=8.5052
	step [99/190], loss=7.1099
	step [100/190], loss=7.9373
	step [101/190], loss=9.9754
	step [102/190], loss=9.1662
	step [103/190], loss=8.4491
	step [104/190], loss=7.9474
	step [105/190], loss=7.9806
	step [106/190], loss=9.0017
	step [107/190], loss=7.0462
	step [108/190], loss=8.1285
	step [109/190], loss=7.9418
	step [110/190], loss=11.0658
	step [111/190], loss=9.0428
	step [112/190], loss=9.8345
	step [113/190], loss=7.6933
	step [114/190], loss=7.8309
	step [115/190], loss=10.3810
	step [116/190], loss=8.6034
	step [117/190], loss=9.0160
	step [118/190], loss=9.7579
	step [119/190], loss=8.9522
	step [120/190], loss=7.3937
	step [121/190], loss=9.9778
	step [122/190], loss=7.7492
	step [123/190], loss=8.3333
	step [124/190], loss=9.6248
	step [125/190], loss=7.6260
	step [126/190], loss=8.8107
	step [127/190], loss=9.2832
	step [128/190], loss=7.1239
	step [129/190], loss=9.1413
	step [130/190], loss=9.7631
	step [131/190], loss=8.4252
	step [132/190], loss=9.1050
	step [133/190], loss=6.6534
	step [134/190], loss=12.0366
	step [135/190], loss=7.6926
	step [136/190], loss=7.2591
	step [137/190], loss=8.2580
	step [138/190], loss=7.5447
	step [139/190], loss=8.3995
	step [140/190], loss=7.5070
	step [141/190], loss=8.8898
	step [142/190], loss=8.4150
	step [143/190], loss=9.2780
	step [144/190], loss=8.2846
	step [145/190], loss=9.0301
	step [146/190], loss=8.2539
	step [147/190], loss=8.0284
	step [148/190], loss=8.1021
	step [149/190], loss=8.3014
	step [150/190], loss=8.8910
	step [151/190], loss=7.0783
	step [152/190], loss=8.8658
	step [153/190], loss=8.7244
	step [154/190], loss=7.8347
	step [155/190], loss=9.0722
	step [156/190], loss=8.1578
	step [157/190], loss=7.4818
	step [158/190], loss=7.9413
	step [159/190], loss=6.9740
	step [160/190], loss=7.5574
	step [161/190], loss=8.0570
	step [162/190], loss=9.6046
	step [163/190], loss=7.7453
	step [164/190], loss=7.1138
	step [165/190], loss=10.1102
	step [166/190], loss=8.7878
	step [167/190], loss=6.6413
	step [168/190], loss=9.1378
	step [169/190], loss=8.7199
	step [170/190], loss=8.7477
	step [171/190], loss=7.7139
	step [172/190], loss=8.9108
	step [173/190], loss=7.9873
	step [174/190], loss=10.3854
	step [175/190], loss=8.7045
	step [176/190], loss=8.5527
	step [177/190], loss=7.4833
	step [178/190], loss=8.2539
	step [179/190], loss=9.3694
	step [180/190], loss=9.2851
	step [181/190], loss=8.3796
	step [182/190], loss=10.1373
	step [183/190], loss=7.3013
	step [184/190], loss=9.6841
	step [185/190], loss=8.3028
	step [186/190], loss=8.2896
	step [187/190], loss=7.9803
	step [188/190], loss=7.8700
	step [189/190], loss=8.8002
	step [190/190], loss=8.5335
	Evaluating
	loss=0.0229, precision=0.2373, recall=0.9900, f1=0.3828
Training epoch 29
	step [1/190], loss=7.8948
	step [2/190], loss=7.6738
	step [3/190], loss=8.6668
	step [4/190], loss=8.2131
	step [5/190], loss=8.1970
	step [6/190], loss=7.1914
	step [7/190], loss=7.7957
	step [8/190], loss=7.6096
	step [9/190], loss=7.0116
	step [10/190], loss=7.9544
	step [11/190], loss=9.2509
	step [12/190], loss=9.6072
	step [13/190], loss=8.7758
	step [14/190], loss=7.5441
	step [15/190], loss=8.9138
	step [16/190], loss=8.8747
	step [17/190], loss=10.7439
	step [18/190], loss=7.4900
	step [19/190], loss=8.5693
	step [20/190], loss=9.3032
	step [21/190], loss=10.6729
	step [22/190], loss=7.5653
	step [23/190], loss=7.7792
	step [24/190], loss=7.4403
	step [25/190], loss=8.6508
	step [26/190], loss=9.1219
	step [27/190], loss=8.7538
	step [28/190], loss=9.2739
	step [29/190], loss=7.9548
	step [30/190], loss=7.7733
	step [31/190], loss=9.1034
	step [32/190], loss=7.6843
	step [33/190], loss=8.7692
	step [34/190], loss=7.5973
	step [35/190], loss=8.8629
	step [36/190], loss=7.2613
	step [37/190], loss=8.1417
	step [38/190], loss=9.8748
	step [39/190], loss=8.5987
	step [40/190], loss=9.3061
	step [41/190], loss=6.0332
	step [42/190], loss=9.3415
	step [43/190], loss=7.9530
	step [44/190], loss=8.4552
	step [45/190], loss=7.4310
	step [46/190], loss=7.5380
	step [47/190], loss=7.0488
	step [48/190], loss=7.6853
	step [49/190], loss=6.9960
	step [50/190], loss=9.3966
	step [51/190], loss=8.2742
	step [52/190], loss=8.1862
	step [53/190], loss=8.1703
	step [54/190], loss=8.5639
	step [55/190], loss=9.9539
	step [56/190], loss=8.9215
	step [57/190], loss=8.4490
	step [58/190], loss=7.6231
	step [59/190], loss=6.2029
	step [60/190], loss=8.3426
	step [61/190], loss=7.4470
	step [62/190], loss=9.2183
	step [63/190], loss=7.2266
	step [64/190], loss=8.8453
	step [65/190], loss=8.4686
	step [66/190], loss=9.4642
	step [67/190], loss=9.4603
	step [68/190], loss=8.6628
	step [69/190], loss=8.8186
	step [70/190], loss=7.4119
	step [71/190], loss=6.1663
	step [72/190], loss=8.3191
	step [73/190], loss=7.4605
	step [74/190], loss=8.2524
	step [75/190], loss=9.8453
	step [76/190], loss=8.1786
	step [77/190], loss=7.8401
	step [78/190], loss=7.9499
	step [79/190], loss=8.1237
	step [80/190], loss=8.6784
	step [81/190], loss=7.6045
	step [82/190], loss=6.8872
	step [83/190], loss=10.0217
	step [84/190], loss=6.9435
	step [85/190], loss=9.0277
	step [86/190], loss=10.1030
	step [87/190], loss=7.8436
	step [88/190], loss=6.8010
	step [89/190], loss=11.0583
	step [90/190], loss=8.1841
	step [91/190], loss=7.6964
	step [92/190], loss=11.2441
	step [93/190], loss=8.6341
	step [94/190], loss=10.7007
	step [95/190], loss=7.7577
	step [96/190], loss=8.9175
	step [97/190], loss=7.5937
	step [98/190], loss=8.4161
	step [99/190], loss=8.9224
	step [100/190], loss=8.8924
	step [101/190], loss=8.7080
	step [102/190], loss=9.4184
	step [103/190], loss=10.3193
	step [104/190], loss=9.3264
	step [105/190], loss=7.8234
	step [106/190], loss=8.3733
	step [107/190], loss=6.3588
	step [108/190], loss=6.7360
	step [109/190], loss=7.1960
	step [110/190], loss=9.4271
	step [111/190], loss=9.7040
	step [112/190], loss=8.4437
	step [113/190], loss=8.9402
	step [114/190], loss=9.2432
	step [115/190], loss=10.0869
	step [116/190], loss=8.4319
	step [117/190], loss=9.5171
	step [118/190], loss=9.2309
	step [119/190], loss=8.9451
	step [120/190], loss=10.4244
	step [121/190], loss=8.2863
	step [122/190], loss=7.7292
	step [123/190], loss=9.1611
	step [124/190], loss=7.6536
	step [125/190], loss=11.2295
	step [126/190], loss=7.1707
	step [127/190], loss=7.2450
	step [128/190], loss=9.1119
	step [129/190], loss=10.6118
	step [130/190], loss=11.0115
	step [131/190], loss=8.9827
	step [132/190], loss=6.9747
	step [133/190], loss=7.0283
	step [134/190], loss=9.2662
	step [135/190], loss=8.4812
	step [136/190], loss=8.2175
	step [137/190], loss=6.7523
	step [138/190], loss=8.9551
	step [139/190], loss=8.7566
	step [140/190], loss=8.1856
	step [141/190], loss=7.3070
	step [142/190], loss=7.5406
	step [143/190], loss=8.5705
	step [144/190], loss=7.4295
	step [145/190], loss=9.2946
	step [146/190], loss=8.4690
	step [147/190], loss=8.3538
	step [148/190], loss=10.0891
	step [149/190], loss=7.9551
	step [150/190], loss=9.7935
	step [151/190], loss=7.3386
	step [152/190], loss=8.3210
	step [153/190], loss=7.2123
	step [154/190], loss=8.5350
	step [155/190], loss=8.0725
	step [156/190], loss=8.9151
	step [157/190], loss=7.7983
	step [158/190], loss=8.9347
	step [159/190], loss=8.1255
	step [160/190], loss=8.2406
	step [161/190], loss=7.6769
	step [162/190], loss=8.9981
	step [163/190], loss=9.6401
	step [164/190], loss=7.2959
	step [165/190], loss=9.0687
	step [166/190], loss=6.9922
	step [167/190], loss=10.8956
	step [168/190], loss=8.4260
	step [169/190], loss=8.0027
	step [170/190], loss=7.8491
	step [171/190], loss=7.3130
	step [172/190], loss=11.1879
	step [173/190], loss=8.5880
	step [174/190], loss=7.6780
	step [175/190], loss=6.9754
	step [176/190], loss=10.1335
	step [177/190], loss=7.9028
	step [178/190], loss=6.9895
	step [179/190], loss=8.5733
	step [180/190], loss=8.0601
	step [181/190], loss=9.2094
	step [182/190], loss=8.4793
	step [183/190], loss=8.6348
	step [184/190], loss=6.9212
	step [185/190], loss=9.9821
	step [186/190], loss=8.9808
	step [187/190], loss=10.4128
	step [188/190], loss=7.8449
	step [189/190], loss=7.3512
	step [190/190], loss=7.2001
	Evaluating
	loss=0.0230, precision=0.2323, recall=0.9902, f1=0.3763
Training epoch 30
	step [1/190], loss=6.6195
	step [2/190], loss=7.8545
	step [3/190], loss=8.9232
	step [4/190], loss=8.5063
	step [5/190], loss=7.6809
	step [6/190], loss=8.6857
	step [7/190], loss=7.3385
	step [8/190], loss=6.6585
	step [9/190], loss=7.9574
	step [10/190], loss=8.0461
	step [11/190], loss=9.2330
	step [12/190], loss=8.4520
	step [13/190], loss=6.7408
	step [14/190], loss=7.5241
	step [15/190], loss=8.9768
	step [16/190], loss=8.8048
	step [17/190], loss=7.3898
	step [18/190], loss=7.7851
	step [19/190], loss=7.4159
	step [20/190], loss=7.5348
	step [21/190], loss=8.1373
	step [22/190], loss=7.4289
	step [23/190], loss=7.6281
	step [24/190], loss=6.2633
	step [25/190], loss=7.6244
	step [26/190], loss=6.3044
	step [27/190], loss=9.3234
	step [28/190], loss=5.9123
	step [29/190], loss=7.8601
	step [30/190], loss=7.5762
	step [31/190], loss=8.2241
	step [32/190], loss=9.1624
	step [33/190], loss=7.5501
	step [34/190], loss=8.5259
	step [35/190], loss=6.8673
	step [36/190], loss=7.8436
	step [37/190], loss=8.4938
	step [38/190], loss=6.7374
	step [39/190], loss=8.1619
	step [40/190], loss=9.5715
	step [41/190], loss=7.0957
	step [42/190], loss=8.3678
	step [43/190], loss=8.6868
	step [44/190], loss=7.2854
	step [45/190], loss=9.5875
	step [46/190], loss=8.7106
	step [47/190], loss=8.2552
	step [48/190], loss=8.0857
	step [49/190], loss=8.3866
	step [50/190], loss=7.2535
	step [51/190], loss=10.0028
	step [52/190], loss=8.1624
	step [53/190], loss=10.7754
	step [54/190], loss=8.7328
	step [55/190], loss=6.5735
	step [56/190], loss=7.8130
	step [57/190], loss=8.2849
	step [58/190], loss=6.4524
	step [59/190], loss=8.4808
	step [60/190], loss=11.3527
	step [61/190], loss=8.8436
	step [62/190], loss=8.9867
	step [63/190], loss=7.6868
	step [64/190], loss=8.8780
	step [65/190], loss=7.6645
	step [66/190], loss=7.9349
	step [67/190], loss=8.5773
	step [68/190], loss=7.5463
	step [69/190], loss=9.5819
	step [70/190], loss=6.8261
	step [71/190], loss=7.2914
	step [72/190], loss=9.4578
	step [73/190], loss=10.3782
	step [74/190], loss=8.5384
	step [75/190], loss=9.1745
	step [76/190], loss=6.7560
	step [77/190], loss=6.9194
	step [78/190], loss=7.7154
	step [79/190], loss=7.6731
	step [80/190], loss=7.9648
	step [81/190], loss=7.2364
	step [82/190], loss=10.2499
	step [83/190], loss=8.5523
	step [84/190], loss=7.9234
	step [85/190], loss=8.4833
	step [86/190], loss=9.1187
	step [87/190], loss=8.8747
	step [88/190], loss=6.5181
	step [89/190], loss=8.9551
	step [90/190], loss=7.4834
	step [91/190], loss=6.7815
	step [92/190], loss=7.2991
	step [93/190], loss=8.6168
	step [94/190], loss=9.5338
	step [95/190], loss=9.4922
	step [96/190], loss=7.1133
	step [97/190], loss=8.6399
	step [98/190], loss=7.1352
	step [99/190], loss=8.0155
	step [100/190], loss=8.2379
	step [101/190], loss=9.4337
	step [102/190], loss=8.3021
	step [103/190], loss=7.3603
	step [104/190], loss=7.4124
	step [105/190], loss=9.3301
	step [106/190], loss=6.5759
	step [107/190], loss=7.6594
	step [108/190], loss=8.2631
	step [109/190], loss=9.4242
	step [110/190], loss=7.0454
	step [111/190], loss=8.2620
	step [112/190], loss=7.3096
	step [113/190], loss=7.9088
	step [114/190], loss=9.7015
	step [115/190], loss=8.2084
	step [116/190], loss=9.3079
	step [117/190], loss=7.6349
	step [118/190], loss=8.5977
	step [119/190], loss=8.1958
	step [120/190], loss=8.5758
	step [121/190], loss=7.9085
	step [122/190], loss=8.2024
	step [123/190], loss=6.5491
	step [124/190], loss=7.1890
	step [125/190], loss=8.1878
	step [126/190], loss=7.4019
	step [127/190], loss=7.5191
	step [128/190], loss=8.4464
	step [129/190], loss=7.8735
	step [130/190], loss=7.5896
	step [131/190], loss=10.1030
	step [132/190], loss=7.8767
	step [133/190], loss=7.2198
	step [134/190], loss=8.2892
	step [135/190], loss=6.8261
	step [136/190], loss=5.8789
	step [137/190], loss=7.7570
	step [138/190], loss=8.8675
	step [139/190], loss=7.1623
	step [140/190], loss=9.5709
	step [141/190], loss=9.3373
	step [142/190], loss=8.9170
	step [143/190], loss=8.2361
	step [144/190], loss=7.7790
	step [145/190], loss=7.5834
	step [146/190], loss=7.2336
	step [147/190], loss=8.4006
	step [148/190], loss=8.9522
	step [149/190], loss=8.6670
	step [150/190], loss=9.1392
	step [151/190], loss=9.5345
	step [152/190], loss=7.7309
	step [153/190], loss=8.5329
	step [154/190], loss=7.3881
	step [155/190], loss=7.0981
	step [156/190], loss=9.3018
	step [157/190], loss=9.0803
	step [158/190], loss=7.1653
	step [159/190], loss=8.4801
	step [160/190], loss=9.0332
	step [161/190], loss=8.5659
	step [162/190], loss=8.6366
	step [163/190], loss=8.0818
	step [164/190], loss=8.3922
	step [165/190], loss=6.7531
	step [166/190], loss=8.1362
	step [167/190], loss=7.0293
	step [168/190], loss=7.8916
	step [169/190], loss=9.4075
	step [170/190], loss=7.2577
	step [171/190], loss=8.4611
	step [172/190], loss=9.0524
	step [173/190], loss=9.3456
	step [174/190], loss=8.9672
	step [175/190], loss=7.4437
	step [176/190], loss=8.9183
	step [177/190], loss=7.9126
	step [178/190], loss=7.9521
	step [179/190], loss=7.1937
	step [180/190], loss=8.4381
	step [181/190], loss=6.1981
	step [182/190], loss=8.7346
	step [183/190], loss=8.2600
	step [184/190], loss=9.3606
	step [185/190], loss=8.2886
	step [186/190], loss=8.4482
	step [187/190], loss=10.0811
	step [188/190], loss=7.6680
	step [189/190], loss=7.9994
	step [190/190], loss=6.7333
	Evaluating
	loss=0.0247, precision=0.2161, recall=0.9905, f1=0.3548
Training finished
best_f1: 0.4488964625878502
directing: X rim_enhanced: False test_id 2
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 9348 # image files with weight 9316
removed wrong scan: weights_X_203_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_83_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_45_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_46_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_55_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_70_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_223_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_157_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_42_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_172_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_81_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_78_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_23_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_76_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_25_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_170_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_199_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_112_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_164_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_118_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_198_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_95_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_19_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_56_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_222_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_183_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_239_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_57_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_216_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_235_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_87_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_180_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_5_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_153_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_171_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_161_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_238_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_85_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_105_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_12_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_4_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_196_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_2_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_3_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_43_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_69_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_232_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_101_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_57_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_23_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_183_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_107_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_103_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_65_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_231_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_133_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_143_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_177_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_34_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_221_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_51_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_169_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_103_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_67_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_125_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_211_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_60_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_85_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_25_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_37_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_87_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_99_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_48_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_174_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_149_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_105_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_88_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_15_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_234_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_152_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_227_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_148_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_17_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_209_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_93_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_93_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_52_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_33_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_10_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_110_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_86_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_74_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_160_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_79_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_224_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_159_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_166_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_198_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_194_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_39_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_164_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_98_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_59_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_212_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_32_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_191_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_26_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_93_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_102_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_15_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_86_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_31_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_208_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_187_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_35_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_187_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_4_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_36_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_236_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_74_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_64_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_170_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_184_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_88_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_195_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_203_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_152_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_77_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_190_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_128_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_131_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_175_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_215_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_4_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_22_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_220_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_104_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_142_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_44_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_73_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_138_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_101_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_71_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_131_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_237_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_119_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_12_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_200_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_1_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_89_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_49_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_1_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_128_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_29_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_29_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_230_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_146_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_180_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_46_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_103_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_62_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_112_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_185_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_189_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_34_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_127_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_25_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_145_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_21_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_133_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_90_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_182_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_151_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_94_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_214_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_61_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_84_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_184_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_186_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_116_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_23_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_21_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_217_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_136_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_49_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_211_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_69_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_134_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_31_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_64_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_79_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_22_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_71_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_96_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_109_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_130_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_169_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_130_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_100_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_72_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_82_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_53_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_59_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_178_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_51_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_158_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_97_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_201_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_153_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_94_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_113_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_106_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_170_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_147_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_91_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_114_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_66_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_233_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_36_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_126_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_9_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_87_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_16_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_41_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_186_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_70_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_47_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_124_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_110_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_174_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_124_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_129_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_91_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_41_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_151_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_193_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_121_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_159_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_147_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_77_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_125_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_9_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_56_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_37_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_58_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_127_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_69_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_25_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_13_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_46_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_43_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_197_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_171_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_106_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_38_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_162_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_138_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_206_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_188_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_199_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_126_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_37_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_58_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_132_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_95_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_96_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_20_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_168_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_104_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_177_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_55_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_122_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_168_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_112_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_27_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_32_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_167_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_162_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_75_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_35_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_117_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_65_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_57_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_191_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_163_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_40_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_18_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_107_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_36_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_44_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_139_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_152_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_115_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_45_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_26_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_218_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_169_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_129_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_125_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_38_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_1_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_112_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_229_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_219_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_154_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_181_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_53_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_11_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_48_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_7_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_118_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_122_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_74_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_207_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_90_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_175_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_10_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_196_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_63_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_19_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_200_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_34_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_80_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_168_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_156_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_46_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_128_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_104_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_41_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_123_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_89_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_197_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_42_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_179_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_148_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_105_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_165_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_167_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_202_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_150_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_115_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_68_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_98_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_131_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_173_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_24_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_135_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_121_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_76_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_21_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_188_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_16_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_30_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_47_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_180_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_166_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_129_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_31_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_182_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_92_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_120_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_192_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_139_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_2_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_228_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_201_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_95_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_132_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_133_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_199_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_35_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_241_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_204_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_11_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_67_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_56_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_45_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_116_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_30_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_86_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_78_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_6_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_156_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_50_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_141_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_144_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_13_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_172_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_61_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_130_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_76_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_140_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_202_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_6_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_189_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_210_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_66_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_193_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_159_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_114_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_210_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_104_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_163_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_174_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_40_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_12_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_60_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_120_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_102_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_41_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_83_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_5_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_11_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_116_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_160_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_161_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_197_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_54_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_242_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_117_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_178_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_17_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_198_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_123_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_31_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_108_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_119_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_149_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_28_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_190_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_8_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_118_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_111_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_64_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_97_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_136_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_26_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_226_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_143_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_183_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_212_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_179_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_96_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_137_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_99_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_225_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_72_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_24_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_205_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_80_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_8_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_97_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_7_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_90_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_164_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_66_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_63_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_14_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_88_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_40_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_82_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_107_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_32_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_88_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_137_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_98_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_207_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_30_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_101_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_18_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_14_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_162_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_134_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_109_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_209_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_111_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_181_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_75_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_68_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_50_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_138_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_128_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_177_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_75_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_122_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_78_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_73_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_173_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_27_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_208_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_81_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_108_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_110_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_39_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_3_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_187_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_84_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_133_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_73_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_20_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_59_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_100_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_19_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_28_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_213_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_155_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_240_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_195_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_176_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_33_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_140_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_52_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_42_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_134_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_47_xwqg-B00027_2020-04-30.npy
removed wrong scan: weights_X_157_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_200_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_141_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_150_xwqg-A00121_2019-05-15.npy
removed wrong scan: weights_X_23_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_24_xwqg-B00034_2020-04-03.npy
removed wrong scan: weights_X_79_xwqg-A00085_2019-09-16.npy
removed wrong scan: weights_X_49_xwqg-A00121_2019-05-15.npy
# all image files: 12135 # all weight files in weight_dir: 2534 # image files with weight 2516
train: /ibex/scratch/projects/c2052/air_tube_seg/training_samples_enhanced_one/X 9316
Using 4 GPUs
Going to train epochs [1-30]
Training epoch 1
